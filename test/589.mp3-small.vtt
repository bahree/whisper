WEBVTT

00:00.000 --> 00:12.560
 All right, let's get this started. Hi, Dawa. Hi. So I'm talking to you from my home in

00:12.560 --> 00:17.960
 San Francisco. Where are you? I'm in Palo Alto, not too far away from you, actually.

00:17.960 --> 00:24.440
 Also at your home? Also at my home, yes. The new office. The new office, yeah. So we

00:24.440 --> 00:29.200
 do actually have an office, a hugging face Silicon Valley office in Palo Alto, not too

00:29.200 --> 00:35.680
 far away from here, which we opened recently. But yeah, I'm still getting used to going

00:35.680 --> 00:42.080
 to an actual office. I really like my home office. Yeah, it's kind of here to stay. So

00:42.080 --> 00:49.080
 this is really exciting for me because for a number of reasons. One hugging face is one

00:49.080 --> 00:54.480
 of the most interesting companies today. So especially in the machine learning space,

00:54.480 --> 00:59.960
 but most especially in the natural language processing space, which is where I work. And

00:59.960 --> 01:06.600
 yeah, I saw the tweet in January that you sent out announcing that you were the new

01:06.600 --> 01:11.480
 head of research at hugging face. And I've been dying to talk to you ever since. And

01:11.480 --> 01:16.080
 it's been a good six months. So you've had time to settle in, find your feet, get up

01:16.080 --> 01:21.560
 to speed, actually maybe make an agenda and a plan for yourself at hugging face. So it

01:21.560 --> 01:26.680
 seems like a great time to catch up. And also, a lot of the listeners of this podcast will

01:26.680 --> 01:35.520
 have heard Thomas Wolf from hugging face, one of the founders. Is that right? How would

01:35.520 --> 01:43.040
 you describe Thomas? He's one of the three co founders. And he's our chief science officer.

01:43.040 --> 01:48.960
 So many on this in listening to us right now will have heard Thomas interviewed by Sam

01:48.960 --> 01:55.760
 three months ago. And so this is a, and he had a lot to say about research. And so it's

01:55.760 --> 02:00.480
 a perfect time to dig deeper into some of the things that he got into. And also to just

02:00.480 --> 02:04.440
 open up new territory, find out what's on your mind. How's that sound?

02:04.440 --> 02:09.680
 Yeah, for sure. Yeah, I thought the podcast with Tom was really amazing. So if people

02:09.680 --> 02:13.320
 haven't listened to that, I highly recommend people listen to that too. Yeah.

02:13.320 --> 02:18.520
 You and I spoke briefly a week or two back and I took some notes. And I want to give

02:18.520 --> 02:24.200
 you the end the listeners kind of the menu of things that came to mind for me that we

02:24.200 --> 02:31.200
 could touch on. So big themes. I would love to know more about you as a human and you

02:31.200 --> 02:36.680
 and hugging face. I think a lot of people probably have a name recognition for hugging

02:36.680 --> 02:40.720
 face, but probably don't know really what it is. So it'd be good to dig into that a

02:40.720 --> 02:47.400
 little bit. And then the main dish of the course, let's dig into the future of NLP.

02:47.400 --> 02:53.540
 Yeah. One thing I'd like to emphasize is that hugging face is no longer an NLP company

02:53.540 --> 03:00.880
 per se. So we are doing a lot of very interesting work in computer vision and speech and other

03:00.880 --> 03:05.680
 areas of AI. So I like to think of hugging face as a AI company.

03:05.680 --> 03:09.160
 Yeah. And so that's a perfect segue. Let's dig into that. So hugging face used to be

03:09.160 --> 03:14.640
 an NLP company, I think it's safe to say. And it's really been expanding. I looked on

03:14.640 --> 03:20.200
 Crunchbase just to see what the basic stats are these days. It's like somewhere between

03:20.200 --> 03:28.760
 a hundred and 200 people, series C and based in New York officially, although quite remote

03:28.760 --> 03:36.800
 now like the rest of us. Yeah. And so when you joined, it was already transitioning into

03:36.800 --> 03:41.720
 something bigger than NLP. Yeah. What was your perception of hugging face? How would

03:41.720 --> 03:45.800
 you have described it like before you joined and now that you've joined?

03:45.800 --> 03:53.360
 Yeah. So I've always been impressed by hugging face and how it presents itself to the outside

03:53.360 --> 04:01.000
 world. It's a very open and transparent organization where it really is about a community effort

04:01.000 --> 04:08.000
 to democratize a lot of the tools that everybody uses. So from data sets to models, so Transformers

04:08.000 --> 04:13.640
 library, of course, also the hub, which is really a crucial part of the AI ecosystem

04:13.640 --> 04:20.080
 these days, I think. So I've just always been very impressed by it. And so yeah, that's

04:20.080 --> 04:27.080
 why I chose to join this company. I think it really is a special place and it really

04:27.080 --> 04:31.280
 plays a special role in the community. So I don't think that a company like, I don't

04:31.280 --> 04:36.480
 know, Google or Meta could play the same role that hugging face plays in this ecosystem.

04:36.480 --> 04:41.040
 No, I agree. I agree. It's a pioneer with open source.

04:41.040 --> 04:42.040
 For sure.

04:42.040 --> 04:46.280
 So something else that I really like about hugging face is how European it is and now

04:46.280 --> 04:50.560
 actually very international. The people are just, they come from all over the place. Did

04:50.560 --> 04:54.600
 you know any of the core hugging face people before you joined?

04:54.600 --> 05:01.240
 Yeah. So I mean, I met Tom a few times before and I knew Victor and a bunch of others, Victor

05:01.240 --> 05:07.400
 San. So it's funny actually that you mentioned the Europeanness. So I'm a European as you

05:07.400 --> 05:12.520
 can tell from my accent. I'm originally from Holland, but I live in California and I spent

05:12.520 --> 05:18.000
 some time in the UK and in New York before I moved to California. But Tom, my boss, actually

05:18.000 --> 05:24.200
 lives in Utrecht in the Netherlands, which is where I studied for my undergrad. So Tom

05:24.200 --> 05:25.700
 is not Dutch.

05:25.700 --> 05:29.200
 But you didn't cross paths during your years in the Netherlands.

05:29.200 --> 05:33.920
 No, not at all. I left Holland more than 10 years ago. So I don't think Tom's been living

05:33.920 --> 05:35.640
 there for 10 years.

05:35.640 --> 05:37.840
 So it's not a Dutch mafia. It's a coincidence.

05:37.840 --> 05:46.200
 It's a French mafia, if anything. So the founders are French. So in terms of the company

05:46.200 --> 05:51.360
 at large, what I find fascinating is that we have people, I think in more than 25 countries

05:51.360 --> 05:55.680
 all over the world. So in the science team, we have people on the West Coast, on the East

05:55.680 --> 06:04.560
 Coast of the US and in Canada and lots of different places in Europe and in South Korea.

06:04.560 --> 06:11.360
 And Turkey as well. I have a friend in Turkey based in Istanbul. Let's see. What is your

06:11.360 --> 06:13.360
 job?

06:13.360 --> 06:25.480
 Good question. I wish I knew. So broadly speaking, I'm just trying to help the team

06:25.480 --> 06:31.040
 realize this very ambitious vision that the founders have for the company and for the

06:31.040 --> 06:38.880
 science team inside the company. So yeah, it's not really a well-defined role. I think

06:38.880 --> 06:44.560
 it also kind of depends on what stage we're in in a given research project, for example.

06:44.560 --> 06:47.400
 So I'm kind of discovering that as I go along.

06:47.400 --> 06:50.400
 So the official title is Head of Research.

06:50.400 --> 06:55.440
 And so then comes the question, what is research at Huggingface? How is it different from research

06:55.440 --> 06:59.840
 at a university or research at a big company like Facebook slash Metta, which is where

06:59.840 --> 07:01.440
 you came from before this?

07:01.440 --> 07:05.240
 Yeah. So we're trying to go for a bit of a different model. I think if you want to compare

07:05.240 --> 07:10.840
 it to a single place, then maybe something like DeepMind or OpenAI is closer to what

07:10.840 --> 07:17.240
 we're trying to do than Metta. So yeah, as you mentioned, I've been at FAIR for five

07:17.240 --> 07:23.440
 years and it was a wonderful time. But one of the things that was difficult at FAIR was

07:23.440 --> 07:28.760
 that it's very bottom up, which in theory sounds really nice, but it makes it very difficult

07:28.760 --> 07:35.720
 to do very big ambitious projects. So if you really want to create step change research

07:35.720 --> 07:40.680
 artifacts, which is what we're trying to do, then you need to pull together big groups

07:40.680 --> 07:45.840
 of people and then make sure that they're all aligned in realizing this vision. And

07:45.840 --> 07:51.720
 in a bottom up research organization, that's very difficult to do. So what we're trying

07:51.720 --> 07:56.920
 to do is find the optimal place between the bottom up approach that FAIR and Google Brain

07:56.920 --> 08:02.720
 and places like that have and the top down approach, which DeepMind and OpenAI have,

08:02.720 --> 08:08.320
 where they have a benevolent dictator like Demis or Ilya, basically telling people what

08:08.320 --> 08:12.000
 to do and what the vision is. And we're trying to occupy the middle ground a little bit there

08:12.000 --> 08:16.200
 and really try to use the things that make us special. So that's the ability to move

08:16.200 --> 08:21.800
 fast, the ability to work with the community like we've been doing with projects like Big

08:21.800 --> 08:28.920
 Science and to really exploit the things that make us unique.

08:28.920 --> 08:33.840
 What's the difference between Big Science, which is a project involving lots of external

08:33.840 --> 08:40.280
 people, as many as a thousand are signed up from what I heard from Thomas, probably more

08:40.280 --> 08:46.200
 like hundreds that are active participants on a daily basis, but that's big. And then

08:46.200 --> 08:51.400
 the research team at Hugging Face, describe your actual team, what would you call the

08:51.400 --> 08:56.320
 actual research team at Hugging Face? Is it like 10 people, 20?

08:56.320 --> 08:59.960
 So I think last count was 30, 35 people actually.

08:59.960 --> 09:00.960
 Okay, Big Group.

09:00.960 --> 09:06.160
 Big Science is one of the projects we have going on. So I can tell you a bit about the

09:06.160 --> 09:10.720
 other projects we have going on. So one of the advantages of being at Hugging Face is

09:10.720 --> 09:14.680
 that it's a super transparent and open company. So I can just tell you everything that we're

09:14.680 --> 09:20.000
 doing without feeling bad about it.

09:20.000 --> 09:22.920
 No secret sauce revealed.

09:22.920 --> 09:29.400
 So we have a project around multimodal models. So multimodality, I think everyone agrees

09:29.400 --> 09:32.800
 is very important for the future of AI.

09:32.800 --> 09:37.040
 And when you say multimodality, for those listening in, you're referring to more than

09:37.040 --> 09:42.560
 just text, more than just images, all kinds of sensory, what we would think of as sensory

09:42.560 --> 09:47.040
 modalities or information modalities for humans. You're trying to capture that for models,

09:47.040 --> 09:48.040
 but all at once.

09:48.040 --> 09:53.560
 Yeah, all at once. So I think if you look at more recent multimodal work, it's very often

09:53.560 --> 09:59.280
 just text and images. But there are all kinds of different modalities that you all might

09:59.280 --> 10:01.720
 want to integrate into one single model.

10:01.720 --> 10:05.400
 So how many modalities are you stuffing in?

10:05.400 --> 10:12.360
 So right now it's images, text, videos, and audio, because those are the main ones. And

10:12.360 --> 10:16.440
 then once you have those, then you can start thinking about other specific modalities,

10:16.440 --> 10:22.520
 maybe sort of submodalities. So it's unclear whether code as a modality is a part of text

10:22.520 --> 10:27.040
 or if it's something else. So there's all kinds of interesting questions about what

10:27.040 --> 10:28.320
 the modality really is.

10:28.320 --> 10:32.880
 So my PhD thesis actually was about grounding meaning in perceptual modalities, where I

10:32.880 --> 10:39.480
 also incorporated olfactory semantics. So you can build a bag of chemical compounds model

10:39.480 --> 10:45.760
 and build smell vectors, essentially, and do interesting things with that. So that's

10:45.760 --> 10:49.480
 a long time ago. But yeah, there's a lot of potential there.

10:49.480 --> 10:54.360
 What does the word grounded mean in this context? So let's use NLP. Let's use an example like

10:54.360 --> 11:00.000
 you have a model that, you know, like GPT three. So it's learned how to generate text.

11:00.000 --> 11:01.840
 What does it mean for that model to be grounded?

11:01.840 --> 11:07.400
 Yeah, so, so I was going to say, I think the word grounded isn't really well defined.

11:07.400 --> 11:12.200
 But I'm a philosopher by training originally. So I would argue that most things are not

11:12.200 --> 11:16.980
 well defined. But in my thesis, I make an explicit distinction between referential grounding

11:16.980 --> 11:22.800
 and representational grounding. And so I think referential grounding is what people often

11:22.800 --> 11:28.360
 think about with like referral data sets. So those exist in computer vision, for example,

11:28.360 --> 11:32.560
 where you have to pick out the object. So when someone says banana, then you have to

11:32.560 --> 11:37.120
 be able to point in the image where the banana is. But I think the much more interesting

11:37.120 --> 11:42.560
 type of grounding is representational grounding where you have a holistic meaning representation

11:42.560 --> 11:47.280
 of a concept like elephant and you or violin maybe is a better example. And so you know

11:47.280 --> 11:52.400
 the semantic meaning of violin, you can go to Wikipedia and look up what violin is, what

11:52.400 --> 11:56.520
 that means. But you also have a visual representation of it. And you know what it looks like, you

11:56.520 --> 12:00.400
 know what it sounds like, maybe you know what it smells like, what it feels like, what it's

12:00.400 --> 12:06.840
 like to play it, all of these different modalities are a part of your overarching meaning representation

12:06.840 --> 12:11.400
 of the concept of violin. And I think that is the much more interesting type of meaning

12:11.400 --> 12:16.280
 representation. And so that's the meaning we should try to get into machines if we want

12:16.280 --> 12:19.040
 them to be able to really understand humans.

12:19.040 --> 12:24.840
 So what are some of the problems you see with today's models that reveal that they're insufficiently

12:24.840 --> 12:25.840
 grounded?

12:25.840 --> 12:32.200
 Yeah. So I don't know if we're sure that models are insufficiently grounded. I think that's

12:32.200 --> 12:36.320
 still an empirical question. But my hunch and I think a lot of people in the field share

12:36.320 --> 12:41.000
 that hunch is that you need to have some understanding of the world as humans perceive

12:41.000 --> 12:47.680
 it, if you really want to understand humans. And so there's a lot of communication that

12:47.680 --> 12:54.160
 happens between humans that never really becomes explicit. So people call this common sense,

12:54.160 --> 12:59.360
 for example. So the example I always use is coffee and what coffee smells like, everybody

12:59.360 --> 13:04.360
 knows what coffee smells like. So I never have to explain that to anyone. And so for

13:04.360 --> 13:08.680
 that reason, I also just have no idea how to describe the smell of coffee. I don't know

13:08.680 --> 13:13.560
 if you can try that or like describe the smell of a banana in one sentence. Like you've never

13:13.560 --> 13:17.200
 had to do that because you know that everybody knows what bananas smell like.

13:17.200 --> 13:19.880
 And if you if you could pull it off, we would call you a poet.

13:19.880 --> 13:24.840
 Yeah, exactly. So I think you're totally right. So you have to fall back to associations

13:24.840 --> 13:30.240
 then because there is no descriptive language for this sort of stuff. And I think this happens

13:30.240 --> 13:36.240
 all over the place in natural language communication between humans. And that makes it very hard

13:36.240 --> 13:40.960
 for machines to learn this stuff just from reading Wikipedia or whatever corpus they're

13:40.960 --> 13:42.080
 trained on.

13:42.080 --> 13:45.600
 It's funny, you're very much coming at this as a philosopher, I could see. There's another

13:45.600 --> 13:52.040
 angle which is where I'm coming from. So I'm at a company that is on the applied side.

13:52.040 --> 13:56.840
 So we're using NLP to try and solve problems. And where I see what seems to be the grounded

13:56.840 --> 14:01.800
 problem is the model clearly, if you just poke a little bit, it clearly doesn't understand

14:01.800 --> 14:07.720
 what it's talking about. It'll say all the right things. And then it reveals that it

14:07.720 --> 14:13.200
 actually has no common sense understanding of what coffee is because it'll say something

14:13.200 --> 14:15.720
 that a human would find crazy.

14:15.720 --> 14:21.640
 Yeah. So I think the word understanding, what does understanding even mean there? So I think

14:21.640 --> 14:25.840
 what you're maybe talking about. And so I think there are two main things missing in

14:25.840 --> 14:32.920
 our current paradigm. One is multimodal understanding of concepts. And the other is the intentionality

14:32.920 --> 14:38.560
 with the T of language. So the fact that we use language with an intent to change the

14:38.560 --> 14:43.200
 mental state of whoever we're talking to, right? So I'm using my voice now to change

14:43.200 --> 14:50.680
 your brain, essentially. And so that intent is crucial for real meaning and real understanding.

14:50.680 --> 14:53.320
 And it's something that doesn't exist in language models.

14:53.320 --> 15:01.000
 Do you reckon that we have to give real agency to systems to achieve that? To have them care

15:01.000 --> 15:05.200
 about something? And maybe with reinforcement learning or other paradigms?

15:05.200 --> 15:11.520
 I think so, yeah. So I don't know if agency, I mean, I don't want to keep like going on

15:11.520 --> 15:18.120
 the definitions, but so agency is also a bit unclear, I think. So it's more, yeah, you

15:18.120 --> 15:24.360
 can model the intent of communication when you're trying to model human communication,

15:24.360 --> 15:29.400
 you can try to model the intent as a part of the interaction. So you could think of,

15:29.400 --> 15:32.880
 so the two things I just talked about, you could integrate them in language models pretty

15:32.880 --> 15:39.240
 easily, right? So you could have a language model that also has a multimodal input, maybe

15:39.240 --> 15:44.440
 you can put it in an embodied environment where it can walk around. And then maybe you

15:44.440 --> 15:49.360
 can have multiple of these language models walking around in that world and interacting

15:49.360 --> 15:53.520
 with each other and other humans. So if you put all of that together, then I think you

15:53.520 --> 15:56.760
 get something very close to how humans learn language.

15:56.760 --> 16:01.560
 Is this where you think Huggingface is headed? Is this one of the grand directions?

16:01.560 --> 16:06.000
 This is definitely one of the grand directions. Yeah, so one of our projects is multimodality.

16:06.000 --> 16:11.320
 As I said, another one is about embodied learning. Thomas also talked about this when he spoke

16:11.320 --> 16:12.320
 on this podcast.

16:12.320 --> 16:17.040
 Yeah, the way he described it was maybe we need to teach models language more like we

16:17.040 --> 16:20.680
 teach humans language, which is in the world trying to get things done.

16:20.680 --> 16:26.200
 Exactly, yeah. And that's because we want the models to use the kind of language that's

16:26.200 --> 16:32.360
 useful for interacting with humans. And so people sort of gloss over this, but the reason

16:32.360 --> 16:37.440
 we want to have natural language understanding and natural language generation capabilities

16:37.440 --> 16:43.120
 in these models, because we want them to interact with humans. And so, I mean, one of the other

16:43.120 --> 16:48.160
 things I've been pushing a lot for is a more holistic evaluation of these models where rather

16:48.160 --> 16:53.280
 than just evaluating them on static test sets, we actually expose them to real humans and

16:53.280 --> 16:58.360
 we see how well they do in that setting. And as you mentioned, those models very quickly

16:58.360 --> 17:00.640
 break down if you try to actually do that.

17:00.640 --> 17:06.520
 All right, so different question. I was really curious. So I consider you a very multilingual

17:06.520 --> 17:11.560
 person. I mean, all Dutch people are. If you've ever met a Dutch person, you've met multilingual

17:11.560 --> 17:19.360
 people. And here you are in NLP and adjacent, you know, you're definitely expanding beyond

17:19.360 --> 17:23.800
 that. But you would consider yourself an NLP practitioner. Yeah.

17:23.800 --> 17:28.280
 I think so. Yeah, kind of. I mean, I've been branching out for a long time. So I would

17:28.280 --> 17:34.600
 consider myself an AI person. So a lot of my work is multimodal, but it's language first.

17:34.600 --> 17:38.720
 Yeah, language is my main interest.

17:38.720 --> 17:47.680
 How frustrating or bizarre has it felt to be a deeply multilingual person in like a time

17:47.680 --> 17:53.600
 in science where it's just so English dominated, the research itself, the tools down to the

17:53.600 --> 17:57.200
 very data that we're training these things on. And I'm asking this as an obvious segue

17:57.200 --> 18:02.280
 to this really exciting, you know, project that's underway to perhaps create the first

18:02.280 --> 18:06.760
 truly multilingual based language model as that's my understanding of the project. But

18:06.760 --> 18:12.720
 I first wanted to hear just like you, Dawa, like as a deeply multilingual person, you

18:12.720 --> 18:17.760
 know, like, what does it feel like? What has it felt like to be in this weirdly accidentally

18:17.760 --> 18:19.240
 English dominated space?

18:19.240 --> 18:23.520
 Yeah, so that's a very interesting question. But I don't know if I'm the right person to

18:23.520 --> 18:30.320
 ask it because I moved to the UK for my PhD, and then I moved to the US. And so most Dutch

18:30.320 --> 18:39.880
 people speak pretty decent English, I think. So I think where the accessibility of language

18:39.880 --> 18:44.520
 models and the multilinguality of language models where that really matters is for people

18:44.520 --> 18:52.960
 who are monolingual and who don't speak English. So people who can easily access this technology

18:52.960 --> 18:57.520
 because it's limited only to English. But I think that doesn't really apply to most Dutch

18:57.520 --> 19:02.000
 people because they could very easily switch over, as you mentioned.

19:02.000 --> 19:06.080
 But also like using these things to make sense of the world that's not written in English,

19:06.080 --> 19:09.840
 like, I could tell you how hard it is, because that's my day to day is like dealing with

19:09.840 --> 19:17.400
 Chinese, Russian or other languages, like the tools and the data is far, far weaker.

19:17.400 --> 19:23.040
 Oh, yeah. Yeah, for sure. And I think there's also very interesting underlying questions

19:23.040 --> 19:31.360
 there about the cultural differences that manifest themselves in languages. So English

19:31.360 --> 19:37.760
 as a language is very explicit, so you can be relatively low context in how you communicate.

19:37.760 --> 19:42.360
 So you're just very explicit, or some people would consider Americans relatively blunt,

19:42.360 --> 19:46.600
 I think in how they communicate, same for Dutch people, by the way. But if you think

19:46.600 --> 19:54.000
 about Japanese language, which is very sort of indirect and very different in a sense

19:54.000 --> 19:58.520
 from English, I think that also manifests itself in the culture. So maybe there are

19:58.520 --> 20:02.920
 just things that you can really capture about Japanese culture because you have a specific

20:02.920 --> 20:04.960
 type of language model.

20:04.960 --> 20:09.160
 So tell us a bit about the ongoing experiment to make a truly multilingual model.

20:09.160 --> 20:14.520
 Yeah, so this is the big science model. It has a name now. It's called Bloom, which I

20:14.520 --> 20:21.000
 think is a really nice name because the logo of big science has also always been a flower.

20:21.000 --> 20:26.680
 So the flower is starting to bloom. And so this language model, it's, as you said, the

20:26.680 --> 20:35.120
 first big multilingual language model. And it is only a few weeks away from being done

20:35.120 --> 20:40.360
 training. So it's been very cool. You can just follow it on Twitter. There's a regular

20:40.360 --> 20:46.800
 Twitter update where it's like, we're at, I don't know, like 87% or something now.

20:46.800 --> 20:49.120
 And so have you been playing with checkpoints?

20:49.120 --> 20:54.760
 Yeah, so there's something called the Bloom book where people have been able to just submit

20:54.760 --> 21:01.520
 prompts and then someone would run them and store output somewhere for people to inspect.

21:01.520 --> 21:05.280
 So we're releasing some checkpoints soon as well for people to talk to. And then when

21:05.280 --> 21:09.440
 the final model comes out, it's also going to be released so that people can play with

21:09.440 --> 21:10.440
 it themselves.

21:10.440 --> 21:16.280
 Cool. Is it a basic text-to-text autoregressive model? Same architecture as your typical big

21:16.280 --> 21:17.520
 text-to-text models?

21:17.520 --> 21:24.560
 Yeah, basically, yeah. So I think by design that there hasn't been too much divergence

21:24.560 --> 21:29.920
 from the sort of standard language model that people are used to. But there are some nifty

21:29.920 --> 21:37.000
 new things in there. So it uses like L-O-Y for like how to do the token embeddings and

21:37.000 --> 21:40.760
 things like that. So there are a couple of nice different things in there. But yeah,

21:40.760 --> 21:43.320
 the main architecture is exactly what you would expect.

21:43.320 --> 21:47.720
 Oh, wait, let's dig into that. A lot of people on this call won't really even know what a

21:47.720 --> 21:52.160
 token or a tokenizer is. I think this is a really neat part of NLP. This is very much

21:52.160 --> 21:58.400
 like the tools you use kind of talk. But let's just take a moment. Tell us what is a token,

21:58.400 --> 22:04.920
 what is a tokenizer, and then how did you do it differently with this big bloom model?

22:04.920 --> 22:06.240
 And why did you have to?

22:06.240 --> 22:12.360
 Yeah, so I'm not the right person to really answer detailed questions about the tokenization

22:12.360 --> 22:18.960
 of the language model. But so I can explain what tokenization is. So it's basically just

22:18.960 --> 22:24.840
 how do you cut up your text? So, you know, a sentence consists of words, so you could

22:24.840 --> 22:34.160
 just cut it up in the white space. And just every word is a token. But that is inefficient.

22:34.160 --> 22:38.280
 So what people have been doing is trying to chunk it up in smarter ways.

22:38.280 --> 22:42.800
 Because then you'd have like a vocabulary of millions, right? And with multiple languages,

22:42.800 --> 22:43.800
 it could be huge.

22:43.800 --> 22:48.440
 Yeah. So especially if it's multilingual, maybe you just don't see words often enough

22:48.440 --> 22:54.200
 to really have a very good understanding of their meaning, or a good representation

22:54.200 --> 22:59.160
 of their meaning. And so what you can do is you can chunk different segments of words

22:59.160 --> 23:05.440
 together in smart ways. So this is BPE, by parent coding, and things like that. And

23:05.440 --> 23:12.120
 so there has been a working group in the big science workshop. So it's like a one year workshop

23:12.120 --> 23:17.400
 is how we're thinking about it. And so I think there are 40, 50 different working groups.

23:17.400 --> 23:21.840
 And there was one working group working on tokenization, they wrote a very nice survey

23:21.840 --> 23:27.240
 paper, they did a big analysis of what the right tokenization is. And one of the things

23:27.240 --> 23:32.160
 that they found, I think also together with other like the main model working group is

23:32.160 --> 23:39.520
 that these alibi positional embeddings really help. So this was just an empirical finding.

23:39.520 --> 23:46.080
 And so, you know, there's just a lot of this small research that went into this whole endeavor.

23:46.080 --> 23:50.000
 So why not just go all the way down to the individual character? Why mess with tokens

23:50.000 --> 23:51.000
 at all?

23:51.000 --> 23:55.280
 Yeah, it's a good question. I mean, there, there have been efforts in this direction

23:55.280 --> 24:02.160
 or like, so back in the days, there were like character RNNs like before transformers and

24:02.160 --> 24:06.520
 people were trying to get this to work. It sort of worked, but it didn't really, really

24:06.520 --> 24:07.520
 work.

24:07.520 --> 24:09.560
 It was a great way to generate made up, silly words.

24:09.560 --> 24:15.240
 Yeah. Yeah, for sure. Yeah. And so yeah, I think there's also an interesting possibility

24:15.240 --> 24:21.080
 there where we reduce everything to the bike level. And so when you think about like Unicode

24:21.080 --> 24:27.880
 or UTF-8 or things like that, like in theory, every single character can just be modeled

24:27.880 --> 24:33.440
 at the bike level. And then maybe that's the future. And then maybe you could even put

24:33.440 --> 24:39.120
 images and audio and everything is just bikes. And so basically, you can just have a pre-trained

24:39.120 --> 24:44.520
 bike level model. So I think that's an interesting research direction. And there's been some

24:44.520 --> 24:49.600
 work on that, but so far, it hasn't really proven to be better than just smart ways of

24:49.600 --> 24:51.120
 tokenizing your data.

24:51.120 --> 24:57.200
 So maybe the real explanation for it not working yet is that we haven't used enough data yet.

24:57.200 --> 25:01.320
 So maybe we just need even more data as always then.

25:01.320 --> 25:06.240
 Thomas mentioned 800 gigabytes. What does that actually translate to in terms of like,

25:06.240 --> 25:10.560
 how much of the internet did you grab? I understand you crowdsourced it.

25:10.560 --> 25:18.120
 Yeah, so it was crowdsourced with a big community of collaborators who were part of this big

25:18.120 --> 25:24.440
 science effort. And so it's not really a crawl. So it's very hard to say like, what percentage

25:24.440 --> 25:30.200
 of the internet is this. It really depended on the language and the folks who contributed

25:30.200 --> 25:35.200
 the data for their own language. I think some of them also had different approaches.

25:35.200 --> 25:42.320
 So it's a very kind of targeted way of collecting data. And that's one of the beauties of this

25:42.320 --> 25:43.440
 big science effort.

25:43.440 --> 25:48.280
 So I think there's a lot of emphasis on this bloom model. But what's also very interesting

25:48.280 --> 25:53.840
 about the overarching endeavor is that we have this data set, which is really beautiful

25:53.840 --> 26:00.320
 and curated by experts in those languages. It has a very interesting coverage of different

26:00.320 --> 26:04.840
 languages geographically over the whole world.

26:04.840 --> 26:09.720
 I don't know what the latest numbers know in the 40s or 50s, I think 45.

26:09.720 --> 26:13.120
 Wow. So this is a huge collection of languages.

26:13.120 --> 26:17.760
 And it includes like low resource African languages and things like that. So I think

26:17.760 --> 26:22.560
 that's really great. And so there's the data effort, but then like the legal side of this,

26:22.560 --> 26:26.920
 like how do you distribute the model, the governance side of the data itself, all of

26:26.920 --> 26:31.520
 these super intriguing questions have just been explored by the community completely

26:31.520 --> 26:37.760
 in the open. So it's just fascinating for me. I'm sort of an outsider, right? So yeah,

26:37.760 --> 26:44.440
 so I mean, me too, in a way, like this started about a year ago, I think, or more than that.

26:44.440 --> 26:50.160
 And so I've just been following it from the sidelines and I'm still kind of like not directly

26:50.160 --> 26:54.080
 involved in it that much. And it's just amazing to see.

26:54.080 --> 27:02.200
 Okay, now back to you. So here you are six months into your new role at Huggingface.

27:02.200 --> 27:08.720
 Give us a sense of like what you thought your job would be when you started. And now six

27:08.720 --> 27:13.720
 months later, like, what has changed? What's the newest thing that you've learned about

27:13.720 --> 27:17.720
 yourself and Huggingface and the mission? You know, like, what, what gets you out of

27:17.720 --> 27:19.520
 bed in the morning that's changed?

27:19.520 --> 27:29.560
 Yeah, no, that's a very interesting question. I mean, I think the job has been what I expected

27:29.560 --> 27:34.480
 sort of so so I knew going into this that it's just an amazing team. And we really have

27:34.480 --> 27:40.760
 some some brilliant researchers in this team. So I was very excited about getting to work

27:40.760 --> 27:47.800
 with those folks. And so that's been really awesome. I think one thing that I maybe didn't

27:47.800 --> 27:54.360
 really expect is when you're a company like Huggingface and you're this distributed all

27:54.360 --> 28:00.120
 across the globe, you have to be very decentralized. So a lot of the communication happens asynchronously

28:00.120 --> 28:06.520
 on slack in public channels, which I think is great. And so Huggingface really has a

28:06.520 --> 28:11.760
 unique culture that supports this way of working together. But if you come from a different

28:11.760 --> 28:18.760
 working culture, like me coming from Metta, that that is quite the transition to make.

28:18.760 --> 28:22.400
 And so especially you can't just you can't just go to a whiteboard with people.

28:22.400 --> 28:26.920
 Yeah, so so everything is remote, but it's not even just remote where you're both like

28:26.920 --> 28:33.360
 talking to your computer over zoom. It's like, it's remote also in time. So so one of the

28:33.360 --> 28:38.920
 things I'm struggling with is just time zone. I think so I'm in California, right? So I'm

28:38.920 --> 28:46.160
 sort of trailing the world. And so when when I wake up, or when my my son wakes me up at

28:46.160 --> 28:52.040
 around 7am, then I check my phone and I have like a million slack messages and emails and

28:52.040 --> 28:56.360
 things to read through. And then usually my meetings start at 8am, because I need to make

28:56.360 --> 29:02.680
 sure I can talk to the Europeans. And then they stop working soon after that. And so

29:02.680 --> 29:07.120
 I'm always kind of trailing in time, which is which is not easy.

29:07.120 --> 29:11.840
 You didn't see that coming. I was not prepared for that. Yeah, I'm still still adjusting.

29:11.840 --> 29:16.360
 But I mean, it's an interesting learning experience. And it's just fascinating, I think to see

29:16.360 --> 29:22.160
 like where the world is going with remote work. And so this is the future way, I think,

29:22.160 --> 29:24.800
 in which a lot of companies are going to be doing this.

29:24.800 --> 29:29.880
 So what's it like running and building and nurturing a research team at a startup? I

29:29.880 --> 29:33.240
 think that's something that people will be really curious about. I think a lot of a lot

29:33.240 --> 29:38.560
 of people will be familiar directly or indirectly with how a research group, even 30 strong,

29:38.560 --> 29:43.560
 like you said, at a university works, you know, you've got a PI and that PI's job is

29:43.560 --> 29:47.600
 mostly to get grant money. And then you've got the postdocs who actually run the show.

29:47.600 --> 29:52.120
 And then you've got like grad students who are ranging from miserable to pretty happy.

29:52.120 --> 29:56.760
 And then you've got like interns and undergrads. Does it have anything like that structure?

29:56.760 --> 29:58.680
 Is it just a totally different beast?

29:58.680 --> 30:07.120
 Yeah, it's very different. So I am definitely not the PI. So I'm more more a facilitator,

30:07.120 --> 30:13.000
 I think, or a coordinator. And so we have a very flat, non-hierarchical organization.

30:13.000 --> 30:18.600
 We do have team leads. So those those would be closer to PIs, I think. So we have a multimodal

30:18.600 --> 30:24.840
 project and it has very clear team leads and, you know, things like that. So so my role

30:24.840 --> 30:31.320
 is more like a sort of serving leader where I just try to help people the best way I possibly

30:31.320 --> 30:36.320
 can and to make sure they don't have roadblocks and that people are talking to each other and

30:36.320 --> 30:40.760
 that I'm aware of what's going on and I try to connect people to the right people and

30:40.760 --> 30:42.760
 connect ideas to the right ideas.

30:42.760 --> 30:45.600
 So it sounds like pretty normal management, actually.

30:45.600 --> 30:51.400
 Yeah, but yeah, I guess you could say that, but it's very different from normal management

30:51.400 --> 30:54.880
 at the same time, I think, because of how decentralized the company is and because of

30:54.880 --> 30:59.120
 all of the other things that are just very different from from a traditional management

30:59.120 --> 31:00.800
 role at like a big tech company.

31:00.800 --> 31:04.760
 So well, and also the fact that there's like a thousand strong group of people outside

31:04.760 --> 31:07.560
 the company that you actually have to work with and coordinate with.

31:07.560 --> 31:12.920
 Yeah, but so that's just a big science project. So so I think I mean, you make an interesting

31:12.920 --> 31:19.160
 point that one of the things that makes hugging face so special is that the community plays

31:19.160 --> 31:22.720
 such a big role in the company. And that's not just big science, right?

31:22.720 --> 31:27.840
 So like if you look at Transformers, the library and the open source ecosystem and data sets

31:27.840 --> 31:32.880
 and things like that, that's a huge community. And all of these people are also contributing

31:32.880 --> 31:35.680
 actively to making these tools so awesome.

31:35.680 --> 31:41.760
 Yeah, no, I remember the day we first started using your Transformers library at my company

31:41.760 --> 31:48.680
 Primer. It was a revelation. You just like, I can't under it. I can't say enough about

31:48.680 --> 31:56.240
 how positive the open sourcing of Transformer language models was. And I think hugging face

31:56.240 --> 31:58.640
 deserves most of the credit.

31:58.640 --> 32:05.880
 Just like, yeah, I think one of the reasons that Burke became so popular so quickly was

32:05.880 --> 32:09.800
 because of the Transformers library or the predecessor, right? So PyTorch pre-trained

32:09.800 --> 32:15.880
 Burke. I remember I was at a workshop at the Santa Fe Institute, they do these workshops

32:15.880 --> 32:20.400
 where they invited a bunch of people and they talk about some stuff. And Fernando Pereira

32:20.400 --> 32:26.400
 was there, the Google director of research, I think. And he was saying that we have this

32:26.400 --> 32:31.360
 thing coming out and it's going to like blow everything out of the water. It's amazing.

32:31.360 --> 32:35.680
 It's going to revolutionize NLP. And like, I've heard people say that before and I never

32:35.680 --> 32:42.040
 really believed it. But in this case, he was right. So Bert, yeah, so it dropped like,

32:42.040 --> 32:45.920
 I think two weeks later or something. And then so everyone wanted to play with it and

32:45.920 --> 32:50.640
 being in fairer, obviously PyTorch was the preferred framework. And it took like, I don't

32:50.640 --> 32:54.440
 know, like a week or two before there was this PyTorch pre-trained Burke model that

32:54.440 --> 32:58.120
 everyone was playing with. So it's amazing.

32:58.120 --> 33:02.840
 And so I did some snooping. Your most cited paper, at least according to Google Scholar,

33:02.840 --> 33:09.120
 is this 2017 paper on sentence representations. Why I think that's so notable is that that's

33:09.120 --> 33:17.400
 like just on the before side of Bert. So, you know, Bert comes out in October 2018, something

33:17.400 --> 33:22.640
 like that. And so like, well, a full year before that, you were deep in NLP solving

33:22.640 --> 33:29.240
 hard NLP problems. Do you remember how crazy it was when suddenly, like on the other side

33:29.240 --> 33:33.520
 of that line, when we had language models, all the things in NLP that were really hard

33:33.520 --> 33:39.120
 and tedious, and you needed so much data to even barely get some performance suddenly

33:39.120 --> 33:45.480
 became kind of routine and fun and easy. Like, I'm not, I'm not hiding, I'm not hiding the

33:45.480 --> 33:50.320
 reality that like tons of stuff doesn't work and tons of stuff is still hard. But the things

33:50.320 --> 33:53.800
 that are hard are new things, largely.

33:53.800 --> 34:00.000
 Yeah. So I agree with that. But so to me, as a researcher, it didn't feel like a very

34:00.000 --> 34:05.360
 abrupt transition, actually. So I think that was much more the case for NLP practitioners

34:05.360 --> 34:10.200
 and more applied people trying to use the tools. Yeah. But so for me, as a researcher,

34:10.200 --> 34:15.160
 I think like the transition was, was actually very natural, right? So we were doing things

34:15.160 --> 34:20.080
 with LSTMs and then, okay, transformers. So LSTMs didn't really work. So you needed

34:20.080 --> 34:24.600
 attention. And so there were, so even in in percent, we were also experimenting with

34:24.600 --> 34:28.920
 self attention and things like that. And then what the transformers paper did is it basically

34:28.920 --> 34:33.760
 removed the recurrence. So rather than having an LSTM, we did the forward, just a normal

34:33.760 --> 34:39.480
 MLP feed forward network. And so it turned out that attention on its own is actually

34:39.480 --> 34:46.600
 okay. Right. So from that, it became natural to try to do this on just language modeling

34:46.600 --> 34:51.040
 test. So there's GPT. And then if you can do language modeling, why not do it bidirectionally

34:51.040 --> 34:55.120
 because we were playing with bidirectional LSTMs all the time in percent is a bidirectional

34:55.120 --> 35:02.960
 LSTM. So BERT is just a bidirectional GPT. So it all was very natural, I think, when

35:02.960 --> 35:04.320
 it came up.

35:04.320 --> 35:09.560
 So it felt it felt naturally from the point of view of like no understanding the science.

35:09.560 --> 35:13.560
 But I can tell you from the point of view of people trying to solve, pay, solve problems

35:13.560 --> 35:15.240
 that people will pay you money for.

35:15.240 --> 35:20.960
 Oh yeah, for sure. No, no, no, it changed everything. What there is an aspect of scientifically

35:20.960 --> 35:27.840
 that is new, right? I was delighted when this little cottage industry of Bertology suddenly

35:27.840 --> 35:34.440
 kind of sprouted out of nowhere. So here's the thing, you know, it struck me that deep

35:34.440 --> 35:37.480
 learning used to be very much like a branch of mathematics, right? Because it was part

35:37.480 --> 35:44.040
 of statistics, you know, so like all of ML was just math. And it felt like the math world.

35:44.040 --> 35:48.880
 And then suddenly here we are today with models that are so complicated, they're more like

35:48.880 --> 35:53.920
 biology artifacts. We're like kind of prodding them and probing them and trying to understand

35:53.920 --> 36:00.800
 things like how the heck does Bert, you know, does it understand grammar? To what extent

36:00.800 --> 36:05.000
 does it do it differently than us? Suddenly it's feeling more like an empirical science

36:05.000 --> 36:07.440
 and less like a branch of math.

36:07.440 --> 36:14.080
 Yeah, I'm not sure I'm happy with that, actually. I also think that this, so yeah, I have a

36:14.080 --> 36:18.240
 couple of things to say about that, actually. So I think this cottage industry of Bertology

36:18.240 --> 36:26.320
 is interesting, because a few years before that, we had a cottage industry in Wurtevek.

36:26.320 --> 36:31.480
 So Wurtevek kind of blew everyone away. And then there were a couple of ACLs in the EMNOPs

36:31.480 --> 36:37.080
 where just everything was something to VEC. And it was all just trying to analyze what

36:37.080 --> 36:41.240
 Wurtevek really did. And so I think that's just kind of the progression of science where

36:41.240 --> 36:45.680
 you have a big breakthrough model, and then there's some consolidation, right, in the

36:45.680 --> 36:51.480
 sort of Thomas Kuhn paradigm shift. So there's a real paradigm shifting artifact like Wurtevek

36:51.480 --> 36:56.320
 or Bert, and then there's a lot of consolidation where people try to understand this better.

36:56.320 --> 37:00.800
 So I think that's just the natural progression, and that's just going to continue happening.

37:00.800 --> 37:08.240
 But about Bert specifically, so we just don't have the correct mathematical tools, I think,

37:08.240 --> 37:12.880
 to really understand what it's learning. And so there are some efforts now from Chris

37:12.880 --> 37:18.240
 Ola and trying to understand better what transformers are really learning. But so we have a very

37:18.240 --> 37:24.120
 interesting paper called Mass Language Modeling and the Distribution Hypothesis Order Word

37:24.120 --> 37:29.400
 Does Not Matter Much or something like that. So what we basically show is that you shuffle,

37:29.400 --> 37:34.800
 if you shuffle a corpus, and so all of the sentences are not in the right order anymore,

37:34.800 --> 37:39.760
 and you train a Bert model on it, it just does just as well as a regular Bert. So you

37:39.760 --> 37:47.440
 mentioned does Bert learn grammar? Which weirdly seems to suggest that it doesn't matter, that

37:47.440 --> 37:50.720
 it's clearly doing something differently than humans do. Because if you imagine trying to

37:50.720 --> 37:58.400
 learn language with shuffled language, it'd be a nightmare. Exactly. So I think maybe we're

37:58.400 --> 38:06.720
 also over, I don't know, thinking that Bert is better than it really is or these sorts of models

38:06.720 --> 38:12.800
 that are actually better than they really are. And I think when you think about GPT-3 and how

38:12.800 --> 38:18.720
 much of a splash that made, there's also this element that people just have a natural tendency

38:18.720 --> 38:24.800
 to anthropomorphize everything. You do this to your robot vacuum cleaner and thinks that you

38:24.800 --> 38:29.760
 give it a name. So in the words of Daniel Dennett, the philosopher, you're ascribing

38:29.760 --> 38:37.520
 intentionality. And so I think we do that all the time through everything, and we do it especially

38:37.520 --> 38:41.680
 through things that produce language. Because language is essentially the only thing we know

38:41.680 --> 38:47.440
 that is really, really human only. And so when something produces language, we just go, oh,

38:47.440 --> 38:52.800
 there has to be something brilliant behind that. But very often it's just higher order

38:52.800 --> 38:59.360
 distributional statistics. It's just clever haunts. That's basically, we create these

38:59.360 --> 39:04.480
 benchmark tests. And we watch the performance on these tests going up, up, up, up. And we

39:04.480 --> 39:10.320
 attribute a model like GPT-3 on these language tests as getting truly more clever. It truly

39:10.320 --> 39:14.880
 has a deeper, quote unquote, understanding of the task at hand. But then you do these clever

39:14.880 --> 39:19.120
 experiments like the one you described with scrambling. And it reveals, well, surely actually,

39:19.120 --> 39:24.480
 it's just using distributional tricks. Yeah. So I don't know, I think the jury's still,

39:24.480 --> 39:30.000
 I wouldn't put it that strongly. I think the jury's still out. So I definitely think that there's

39:30.000 --> 39:36.800
 an evaluation crisis in NLP. And I mean, I've been doing a lot of work with lots of folks in

39:36.800 --> 39:44.240
 trying to improve that through things like DynaBench, where we try to rethink benchmarking,

39:44.240 --> 39:49.680
 essentially. But I mean, it's undeniable that progress in NLP has just been insane.

39:49.680 --> 39:57.840
 So if you look at what GPT-3 can do compared to GPT-1, or what we can do now with Dali2 compared

39:57.840 --> 40:04.080
 to, I don't know, the earlier text-to-image synthesis models, it's just crazy how fast

40:04.080 --> 40:09.600
 we're, yeah. So the progress is real. But we should be careful to not kind of over interpret

40:09.600 --> 40:12.560
 what we're seeing. So there's still a lot of stuff that...

40:12.560 --> 40:20.480
 So there's a headline going around just this week about researcher from Google,

40:21.840 --> 40:28.640
 essentially attributing sentience to the Lambda language model. And I think that's really, to

40:28.640 --> 40:35.280
 your point, like what we're talking about. It's these things actually know how to work with language.

40:35.280 --> 40:40.800
 And we humans are language machines. We're completely geared towards understanding and

40:40.800 --> 40:47.600
 transmitting, receiving and sending information with language. That is the most human information.

40:47.600 --> 40:54.720
 Intentionality and understand the world around us, getting stuff done. And so when some mathematical

40:54.720 --> 41:00.800
 object is doing it, I feel it too. I can't help it. Have you ever kind of like had that feeling

41:00.800 --> 41:04.400
 that you just like have to push away of like, man, I'm talking to this thing?

41:04.960 --> 41:08.800
 But what do you mean by a mathematical object? I mean, I think you can argue that your brain is

41:08.800 --> 41:13.280
 also a mathematical object, or at least you can write your brain as a very complicated...

41:13.280 --> 41:17.920
 You took the bait. I was hoping you'd shake the bait. This is like philosopher catnip.

41:20.240 --> 41:26.400
 Well, yeah, I don't know. So I think this... It's very interesting because there's a very nice

41:26.400 --> 41:31.680
 theory of consciousness that says that we take the intentional stance towards ourselves as

41:31.680 --> 41:36.960
 rational agents. And that's what consciousness is. There's a strange loop as Douglas Hofstadter

41:36.960 --> 41:43.840
 calls it. So, yeah, maybe we're evolutionarily hardwired to take an intentional stance towards

41:43.840 --> 41:47.840
 things. And that's why we're so confused by the NLP progress we've been seeing.

41:47.840 --> 41:53.520
 Does it also hint at a way to achieve artificial intelligence of a more AGI flavor?

41:54.400 --> 42:02.000
 Yeah. Again, I don't really know what AGI even means. And I think it's very premature to

42:02.000 --> 42:08.400
 to start thinking about it. And so we have a philosopher slash ethicist who joined Hugging

42:08.400 --> 42:14.640
 Face recently, Jada Bestili. So she has made this point on Twitter too, I think, where

42:15.840 --> 42:20.880
 there are real problems right now with the deployment of AI. And so when we're thinking

42:20.880 --> 42:26.560
 about the applied ethics of these systems, there are just real things we need to fix

42:26.560 --> 42:31.440
 right now. And they are much more salient and much more important right now than

42:31.440 --> 42:35.360
 thinking about AGI and paperclip maximizers and things like that.

42:35.360 --> 42:41.120
 I agree. I agree completely. There's just, I think actually the biggest problem to solve

42:41.120 --> 42:46.560
 from an ethics point of view is these systems not working that well on narrow tasks and people

42:46.560 --> 42:51.520
 over trusting them. That's where a lot of harm can come from not from bias, even that's another

42:51.520 --> 42:55.760
 level of problem, just like over trusting systems misunderstanding their limits.

42:55.760 --> 43:01.680
 Yeah, true. But so there's a trade off here too, right? So we shouldn't hype them up too much,

43:01.680 --> 43:05.920
 because people will just misunderstand what these systems are capable of. But we also

43:05.920 --> 43:10.720
 shouldn't under-hype them too much. So Sam Bowman, professor at NYU is a very nice paper

43:11.520 --> 43:15.360
 where he talks about the dangers of under-hyping. So if we all just pretend that there is no

43:15.360 --> 43:22.000
 progress at all, then at some point we are going to be very surprised when AGI suddenly emerges

43:22.000 --> 43:29.040
 and we have sent you into AI. So we definitely should think about this stuff. And so AI alignment

43:29.040 --> 43:33.760
 is a very active research area and it's a very important research area. But it's all about

43:33.760 --> 43:40.400
 finding that balance, I think. Sure. Okay, lightning round, most exciting things on the horizon

43:41.600 --> 43:47.520
 for research in NLP that you're either working towards now or you'd like to go a little bit

43:47.520 --> 43:52.960
 further than what you're just about to ship, just about to publish. Yeah, so I'm very excited

43:52.960 --> 43:59.040
 in multi-modality, obviously. I think that there's a lot of interesting work coming out in

43:59.040 --> 44:06.000
 semi-parametric models where you have a retriever component and some sort of lightweight reader model

44:06.000 --> 44:10.880
 on top of that retriever. So there was a paper from DeepMind coming out a couple of days that

44:10.880 --> 44:16.400
 came out a couple of days ago. The idea in a nutshell there, is it that these language models

44:16.400 --> 44:22.720
 are basically frozen in time based on the data you give them. And so we need some way to help them

44:22.720 --> 44:26.960
 keep refreshing what they understand about the world. Oh yeah, that's just one application. I

44:26.960 --> 44:36.000
 think it's much more about how you learn different things. So as humans, we have different kinds of

44:36.000 --> 44:42.800
 memory. We have a semantic memory and an episodic memory. And so we also have a library and the

44:42.800 --> 44:48.000
 internet where we can look up stuff. So we don't have to store all of it in our parameters, our

44:48.000 --> 44:54.800
 brain. So I think if you do this with models too, where you have a big index, where you can invest

44:54.800 --> 44:59.760
 a lot of heavy compute in having a very high quality index, then you can have lots of lighter

44:59.760 --> 45:04.160
 weight reader models on top of this. So this is also going to have lots of repercussions. I think

45:04.160 --> 45:10.080
 for industry where if you're a company like Facebook, you want to have a million classifiers

45:10.080 --> 45:14.640
 from all of these different teams all trying to do cool stuff with their classifier. If they have a

45:14.640 --> 45:20.560
 big index that they can rely on, then you kind of do the compute in a much more intelligent way,

45:20.560 --> 45:27.040
 I think. So it's about finding the mix between the retriever, which will be a big sort of language

45:27.040 --> 45:32.240
 model and the reader model on top, which will also be a big sort of language model. And just for

45:32.240 --> 45:38.480
 people who are unfamiliar with the current status quo, what we have right now is basically just

45:38.480 --> 45:43.760
 two kinds of information storage. You've got the model itself, which has been pre-baked with

45:43.760 --> 45:49.200
 just an understanding of language and whatever emerges from that, just basically predicting

45:49.200 --> 45:54.000
 missing words. And then you've got what people usually call the prompt, which is whatever you

45:54.000 --> 45:58.880
 can cram into the attention window at inference time. So you can actually put a whole conversation

45:58.880 --> 46:03.040
 there. You can put example problems. You could do a lot of neat things in the prompt, but it's pretty

46:03.040 --> 46:08.800
 darn limited, right? Aside from your pre-baked knowledge that's crystallized, all these things

46:08.800 --> 46:13.440
 can do is whatever you can cram into the prompt. And what you're suggesting is, hey, maybe we could

46:13.440 --> 46:20.560
 actually build a whole separate system where they could retrieve knowledge at game time.

46:21.200 --> 46:25.680
 Yeah. So there are some interesting models. So I've been involved in a model called REG,

46:25.680 --> 46:31.520
 Retrieve Augmented Generation, and there's also Realm from Google. So the basic idea is that you

46:31.520 --> 46:37.200
 can, so you have your language model, which would be parametric, and then you have your

46:37.200 --> 46:42.000
 K&N, like nearest neighbor search algorithm, essentially, which is non-parametric. And if

46:42.000 --> 46:47.520
 you put those two approaches together, you get a semi-parametric model. And I think there's

46:47.520 --> 46:52.320
 a lot of potential applications for that down the line. So that's one thing. And then the other

46:52.320 --> 46:56.800
 thing, so I said multimodal, semi-parametric. And I think the other thing that's going to be

46:56.800 --> 47:01.440
 interesting, and there's a lot of traction happening there now too, is around data-centric AI.

47:02.560 --> 47:07.360
 So I'm still rooting for things like active learning becoming much more mainstream,

47:08.240 --> 47:14.160
 measuring our data much more carefully. So we have folks like Mick Mitchell in hugging face,

47:14.160 --> 47:18.480
 working on data measurement tools and things like that. So really trying to understand much

47:18.480 --> 47:23.280
 better what's really happening in our data, and trying to do things to the data or curate the

47:23.280 --> 47:30.000
 data in different ways so that we can have better models in the end. Yeah. I quickly, before calling

47:30.000 --> 47:38.240
 you, I actually refreshed my knowledge of what this data tool looks like. It's kind of like an x-ray

47:38.240 --> 47:41.840
 for data sets. And it's a really beautiful idea. I'm surprised that no one, you know,

47:41.840 --> 47:45.200
 you know an idea is a good one when you're like, why haven't we been doing this for ages?

47:45.760 --> 47:51.760
 It's just like all the automatic obvious things you can measure about a data set that's composed

47:51.760 --> 47:56.400
 of language. Like, let's put that all in one toolkit. And then you can keep adding to it and

47:56.400 --> 48:00.880
 make it more and more sophisticated. That's the basic idea, right? Yeah, that's exactly it.

48:00.880 --> 48:06.480
 And so I think that it's just nice that, so this lives in a space, so it has a graphical user

48:06.480 --> 48:12.080
 interface that just exists. Maybe in the longer term, it will be a natural part of the hugging

48:12.080 --> 48:17.760
 face hub where you can just upload any data set to the hub and start measuring what's actually in

48:17.760 --> 48:23.360
 your data. And you have a nice interface where you can just inspect it on the fly. So we have a lot

48:23.360 --> 48:29.200
 of interesting things going on in the direction of evaluation and measurement. So I think what's

48:29.200 --> 48:34.400
 really crucial when you think about the AI pipeline of the future is that you have raw data, which

48:34.400 --> 48:38.800
 you turn into data sets. And those data sets, you turn into models, which you want to measure

48:38.800 --> 48:42.320
 your data sets and your models. And you want to understand what's in there and how well they

48:42.320 --> 48:46.800
 perform. And then you want to, based on your measurement, deploy the best model to production.

48:46.800 --> 48:52.800
 So for making predictions with your model. And so measurement is really absolutely crucial

48:53.520 --> 48:58.080
 in all of the decisions that you're making there. But measurement is also very difficult.

48:58.080 --> 49:03.200
 And so that's something that we're trying to address. So we have an evaluate library that came

49:03.200 --> 49:08.720
 out a couple of weeks ago. We have some very exciting announcements coming up soon. I don't know

49:08.720 --> 49:13.680
 when this podcast comes out, but it might already be out by that time. But we're working on

49:13.680 --> 49:19.920
 evaluation on the hub so that you can essentially evaluate any model on any data set using any

49:19.920 --> 49:24.320
 metric just at the click of a button. So you don't have to do any manual stuff there.

49:25.120 --> 49:30.560
 Surely people are going to miss having to go and copy paste massive chunks of scikit learn code

49:30.560 --> 49:39.040
 into their Jupyter notebooks. Come on, Dawa. Yeah. Yeah. No, I think like lowering the barrier to

49:39.040 --> 49:46.640
 doing proper evaluation according to best practices. I think that's a hugely impactful thing to do.

49:46.640 --> 49:51.280
 And that's something that hugging face is uniquely equipped to do. So that's one of the things I've

49:51.280 --> 49:56.400
 been excited about also in the science team. You mentioned active learning. I'd love to dig into

49:56.400 --> 50:01.360
 that a little bit. That's something I've worked with myself. And it's an enticing idea just for

50:01.360 --> 50:07.280
 those listening at home who aren't familiar. Usually when you want to make a data set to train

50:07.280 --> 50:13.200
 a model, you the human have to select the samples of data from your raw data pool that you're going

50:13.200 --> 50:19.360
 to gold label and train your model on. The idea of active learning in a nutshell is let's put a

50:19.360 --> 50:25.360
 model between you and the data. Sometimes the model you're training and it will decide which

50:25.360 --> 50:31.600
 ones to put in front of you. The human whose time is expensive and whose site is limited and make

50:31.600 --> 50:37.520
 good choices and try and find the most instructive examples from the data to label so that you get

50:37.520 --> 50:41.840
 the most bang for buck because no one wants to spend their whole life labeling data. In fact,

50:41.840 --> 50:46.960
 there's some problems that that's just prohibitive. You literally just can't do it. True positives are

50:46.960 --> 50:53.760
 too rare, et cetera, et cetera. So is there some really exciting new developments in active learning?

50:53.760 --> 50:59.040
 I feel like it's kind of like a done deal. Is there something new and exciting on the horizon,

50:59.040 --> 51:09.360
 do you think? Yeah. So I think it just has a lot of potential. And so what you described is a

51:09.360 --> 51:13.600
 specific kind of active learning, I think, where you have an acquisition function that scores

51:13.600 --> 51:19.120
 examples and you just decide which example you want to label. But I think there are extensions

51:19.120 --> 51:23.840
 of these algorithms where you can think not about the labeling part, but about the pre-training part.

51:23.840 --> 51:30.400
 So maybe I can pick parts of a large corpus that I should be pre-training on now because I know

51:30.400 --> 51:36.240
 what downstream task I care about in the end. So there's a mismatch currently between the

51:36.240 --> 51:40.480
 pre-training phase where we just do language modeling or so causal or mass language modeling.

51:41.120 --> 51:45.120
 And then we fine tune it on something that might be very different from what we're training on.

51:45.120 --> 51:50.160
 So we don't really know what to train on. So I think if we can connect the pre-training phase

51:50.160 --> 51:55.600
 to what we know we are going to care about, then you can do very interesting things. And so the

51:55.600 --> 52:01.840
 way to do that selection, so data selection, is through an acquisition function type of thing.

52:01.840 --> 52:06.560
 So that's why it's related to active learning. Another interesting thing that I've been working

52:06.560 --> 52:11.760
 on as well with some folks is dynamic adversarial data collection where you have a model in the loop

52:11.760 --> 52:18.000
 and a human is trying to fool the model. And so if you take the model fooling examples or

52:18.000 --> 52:21.920
 so if you take all the examples, including the ones that didn't fool the model, but that are still

52:21.920 --> 52:27.920
 kind of intended to try to probe the model for a weakness, if you train on that data and you keep

52:27.920 --> 52:33.760
 updating the model as you're doing the training, so that's the dynamic component, then you get

52:33.760 --> 52:38.880
 a much better model out in the end. So it's really like 10% better. So we have a nice paper

52:38.880 --> 52:43.600
 where we try to do this in the limit over like 20 rounds of natural language inference and you

52:43.600 --> 52:48.080
 just get a much, much better model out in the end. So I think the future of data collection, the way

52:48.080 --> 52:54.160
 we think about it now in the field, is going to be changed a little bit where everything is just

52:54.160 --> 52:59.360
 always going to be with models in the loop. And that maybe ties back to this long term vision

52:59.360 --> 53:04.080
 of having language models interacting with each other in some environment. But if you can have

53:04.080 --> 53:08.880
 humans and models together interacting with each other and learning from each other and maybe

53:08.880 --> 53:13.760
 trying to also kind of probe each other and be on the decision boundaries of certain things,

53:14.400 --> 53:18.800
 then you can learn much more efficiently, I think. It sounds like you're describing education,

53:18.800 --> 53:26.800
 like a school. Exactly, yeah. So in terms of education, one of the important things is also

53:26.800 --> 53:33.360
 a curriculum. So I think one thing you could do with this pre-training and like the active learning

53:33.360 --> 53:37.680
 of pre-training and connecting that to fine tuning is you could try to have a smarter curriculum.

53:37.680 --> 53:43.680
 So if your acquisition function changes over time, essentially you're designing a curriculum or

53:43.680 --> 53:48.800
 you're learning a curriculum on the fly that helps your language model be as good as possible on

53:48.800 --> 53:55.040
 the downstream test that you might care about. Okay. And final question, at least that I can think of,

53:55.760 --> 54:03.040
 is something that Thomas mentioned that intrigued me, which is he said that there seems to be

54:03.040 --> 54:08.800
 something missing with NLP. You know, like we're making these language models bigger and bigger,

54:08.800 --> 54:13.280
 bigger. And yes, we're improving the data and we're getting more data-centric. But

54:14.960 --> 54:19.600
 he gave the impression that he really believes that there's something fundamentally missing. It's

54:19.600 --> 54:24.240
 not just more data. It's not just more text. And you've hinted at this yourself with multimodality

54:24.880 --> 54:32.320
 and interaction, agency, whatever that means. So yeah, if you had to make a guess,

54:32.320 --> 54:39.200
 10 years from now, what do you think the paradigm is going to be? Will we even talk about NLP?

54:39.200 --> 54:46.880
 I suspect NLP will be a historical footnote. They'll just be AI, right? And text will be just one of

54:46.880 --> 54:53.680
 the many crucial developmental raw material for artificial intelligence.

54:53.680 --> 54:58.880
 Yeah, but I still think that text will just remain a dominant modality.

54:58.880 --> 55:02.880
 Well, as it is with humans.

55:02.880 --> 55:10.240
 Exactly. Right. So language really is very crucial. So I don't think NLP itself is going away. But I

55:10.240 --> 55:16.320
 think, yeah, in order to get to real meaning, all of these other fields are probably going to be

55:17.680 --> 55:24.320
 subsumed into NLP when it comes to language understanding. So yeah, I don't know, in 10

55:24.320 --> 55:30.800
 years from now, I think we're going to have very, very different models in a sense. But I think a

55:30.800 --> 55:34.960
 lot of the building blocks that we have now are still going to exist in those models.

55:34.960 --> 55:39.200
 Do you think it'll just be all eventually robotics, either real world and or

55:40.400 --> 55:45.360
 virtual world robots, like taking in all the sensory information?

55:45.360 --> 55:50.320
 Virtual robots, I definitely buy. But so I think for actual physical robots,

55:50.320 --> 55:56.800
 I think like learning from that doesn't really scale that well. So I think one of the big problems

55:56.800 --> 56:01.600
 in robotics is a sim to real, right? How do you transfer from a simulation to a real environment?

56:02.640 --> 56:07.680
 And so as our simulations become reader and reader, that problem is going to become smaller and

56:07.680 --> 56:13.360
 smaller. So I don't think we need physical embodiment in any real sense in order to get to

56:13.360 --> 56:17.680
 meaning, but we'll probably definitely need virtual embodiment where you have an environment

56:17.680 --> 56:22.160
 where you can interact with each other. But maybe that environment already exists, right?

56:22.160 --> 56:27.520
 So maybe that environment is just the internet, or maybe that environment will come into existence

56:27.520 --> 56:32.080
 very soon in the form of the metaverse or whatever you want to call it.

56:32.080 --> 56:35.760
 Any final thoughts you want to share with tens of thousands of people?

56:35.760 --> 56:43.280
 I think one of the things that I think is important, and that's kind of what Huggingface

56:43.280 --> 56:50.400
 also stands for is just open source and open science. And so if I were to give any parting

56:50.400 --> 56:56.480
 thoughts, I would encourage people to always embrace openness, because that really is crucial to

56:57.120 --> 57:01.680
 making progress, but also making sure that the progress that we make doesn't end up in the wrong

57:01.680 --> 57:06.640
 hands or go in the wrong direction. So just for those listening at home, Dawa,

57:06.640 --> 57:09.840
 where can we find out more about you and what you do?

57:09.840 --> 57:16.560
 Yeah, so I have a website. It's dauakila.github.io. It has a couple of links to relevant social

57:16.560 --> 57:24.000
 media profiles. I also have a Twitter account. So that's dauakila, my name. So D-O-U-W-E-K-I-E-L-A.

57:25.040 --> 57:28.880
 And so I'm trying to be more active on Twitter. I'm still working on that.

57:28.880 --> 57:33.680
 And I'm Bohannon Bot on Twitter, and you'll see me probably asking follow up questions

57:34.320 --> 57:39.360
 out in the open following your advice. Dawa, thank you so much. This was a blast.

57:39.360 --> 57:48.880
 Thank you. Thanks for having me.

