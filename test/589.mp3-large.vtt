WEBVTT

00:05.600 --> 00:07.600
 All right, let's get this started.

00:07.600 --> 00:08.840
 Hi Dawah.

00:08.840 --> 00:09.680
 Hi.

00:09.680 --> 00:13.880
 So I'm talking to you from my home in San Francisco.

00:13.880 --> 00:14.700
 Where are you?

00:14.700 --> 00:17.800
 I'm in Palo Alto, not too far away from you actually.

00:17.800 --> 00:19.240
 Also at your home.

00:19.240 --> 00:20.640
 Also at my home, yes.

00:22.200 --> 00:23.240
 The new office.

00:23.240 --> 00:24.240
 The new office, yeah.

00:24.240 --> 00:26.600
 So we do actually have an office,

00:26.600 --> 00:29.000
 a hugging face Silicon Valley office in Palo Alto,

00:29.000 --> 00:32.600
 not too far away from here, which we opened recently.

00:32.600 --> 00:35.080
 But yeah, I'm still getting used

00:35.080 --> 00:36.920
 to going to an actual office.

00:36.920 --> 00:39.160
 I really like my home office.

00:39.160 --> 00:41.600
 Yeah, it's kind of here to stay.

00:41.600 --> 00:44.000
 So this is really exciting for me

00:44.000 --> 00:46.560
 because for a number of reasons.

00:46.560 --> 00:50.040
 One, hugging face is one of the most interesting

00:50.040 --> 00:51.640
 companies today.

00:51.640 --> 00:54.240
 So especially in the machine learning space,

00:54.240 --> 00:56.680
 but most especially in the natural language

00:56.680 --> 00:59.520
 processing space, which is where I work.

00:59.520 --> 01:04.520
 And yeah, I saw the tweet in January that you sent out

01:05.000 --> 01:07.240
 announcing that you were the new head of research

01:07.240 --> 01:08.760
 at hugging face.

01:08.760 --> 01:10.760
 And I've been dying to talk to you ever since.

01:10.760 --> 01:13.480
 And it's been a good six months.

01:13.480 --> 01:15.880
 So you've had time to settle in, find your feet,

01:15.880 --> 01:18.600
 get up to speed, actually maybe make an agenda

01:18.600 --> 01:21.400
 and a plan for yourself at hugging face.

01:21.400 --> 01:23.280
 So it seems like a great time to catch up.

01:23.280 --> 01:26.560
 And also a lot of the listeners of this podcast

01:26.560 --> 01:31.560
 will have heard Thomas Wolf from hugging face,

01:31.680 --> 01:33.080
 one of the founders.

01:33.080 --> 01:33.920
 Is that right?

01:35.320 --> 01:37.560
 How would you describe Thomas?

01:37.560 --> 01:39.960
 He's one of the three co-founders

01:39.960 --> 01:42.920
 and he's our chief science officer.

01:42.920 --> 01:45.800
 So many on this in listening to us right now

01:45.800 --> 01:50.680
 will have heard Thomas interviewed by Sam three months ago.

01:50.680 --> 01:54.840
 And so this is, and he had a lot to say about research.

01:54.840 --> 01:57.080
 And so it's a perfect time to dig deeper

01:57.080 --> 01:59.400
 into some of the things that he got into

01:59.400 --> 02:01.560
 and also to just open up new territory,

02:01.560 --> 02:03.840
 find out what's on your mind.

02:03.840 --> 02:04.880
 How's that sound?

02:04.880 --> 02:05.720
 Yeah, for sure.

02:05.720 --> 02:09.000
 Yeah, I thought the podcast with Tom was really amazing.

02:09.000 --> 02:10.720
 So if people haven't listened to that,

02:10.720 --> 02:13.240
 I highly recommend people listen to that too.

02:13.240 --> 02:16.320
 You and I spoke briefly a week or two back

02:16.320 --> 02:19.240
 and I took some notes and I wanna give you

02:19.240 --> 02:21.800
 and the listeners kind of the menu

02:21.800 --> 02:25.120
 of things that came to mind for me that we could touch on.

02:25.120 --> 02:29.120
 So big themes, I would love to know more about you

02:29.120 --> 02:32.320
 as a human and you and hugging face.

02:32.320 --> 02:36.360
 I think a lot of people probably have a name recognition

02:36.360 --> 02:39.240
 for hugging face, but probably don't know really what it is.

02:39.240 --> 02:42.000
 So it'd be good to dig into that a little bit.

02:42.000 --> 02:44.760
 And then the main dish of the course,

02:44.760 --> 02:47.600
 let's dig into the future of NLP.

02:47.600 --> 02:50.240
 Yeah, one thing I'd like to emphasize

02:50.240 --> 02:54.320
 is that hugging face is no longer an NLP company per se.

02:54.320 --> 02:57.760
 So we are doing a lot of very interesting work

02:57.760 --> 03:01.840
 in computer vision and speech and other areas of AI.

03:01.840 --> 03:05.040
 So I like to think of hugging face as an AI company.

03:05.040 --> 03:07.080
 Yeah, and so that's a perfect segue.

03:07.080 --> 03:07.960
 Let's dig into that.

03:07.960 --> 03:10.280
 So hugging face used to be an NLP company.

03:10.280 --> 03:14.240
 I think it's safe to say, and it's really been expanding.

03:14.240 --> 03:16.160
 I looked on Crunchbase just to see

03:16.160 --> 03:18.400
 what the basic stats are these days.

03:18.400 --> 03:21.960
 It's like somewhere between 100 and 200 people,

03:21.960 --> 03:26.960
 series C and based in New York officially,

03:27.040 --> 03:30.480
 although quite remote now like the rest of us.

03:30.480 --> 03:35.400
 Yeah, and so when you joined,

03:35.400 --> 03:38.800
 it was already transitioning into something bigger than NLP.

03:38.800 --> 03:41.520
 Yeah, what was your perception of hugging face?

03:41.520 --> 03:44.120
 How would you have described it like before you joined

03:44.120 --> 03:45.960
 and now that you've joined?

03:45.960 --> 03:50.960
 Yeah, so I've always been impressed by hugging face

03:51.120 --> 03:53.760
 and how it presents itself to the outside world.

03:53.760 --> 03:56.360
 It's a very open and transparent organization

03:57.720 --> 04:01.080
 where it really is about a community effort

04:01.080 --> 04:05.200
 to democratize a lot of the tools that everybody uses.

04:05.200 --> 04:07.160
 So from datasets to models,

04:07.160 --> 04:10.080
 so Transformers library, of course, also the hub,

04:10.080 --> 04:13.720
 which is really a crucial part of the AI ecosystem

04:13.720 --> 04:14.800
 these days, I think.

04:14.800 --> 04:17.400
 So I've just always been very impressed by it.

04:17.400 --> 04:22.400
 And so yeah, that's why I chose to join this company.

04:22.960 --> 04:25.520
 I think it really is a special place

04:25.520 --> 04:28.040
 and it really plays a special role in the community.

04:28.040 --> 04:31.440
 So I don't think that a company like, I don't know,

04:31.440 --> 04:33.640
 Google or Meta could play the same role

04:33.640 --> 04:36.360
 that hugging face plays in this ecosystem.

04:36.360 --> 04:37.200
 No, I agree.

04:37.200 --> 04:38.040
 I think that's great.

04:38.040 --> 04:38.880
 I agree.

04:38.880 --> 04:40.320
 It's a pioneer with open source.

04:40.320 --> 04:41.160
 For sure.

04:41.160 --> 04:44.040
 So something else that I really like about hugging face

04:44.040 --> 04:45.480
 is how European it is.

04:45.480 --> 04:46.960
 And now actually very international.

04:46.960 --> 04:49.920
 The people are just, they come from all over the place.

04:49.920 --> 04:53.000
 Did you know any of the core hugging face people

04:53.000 --> 04:54.080
 before you joined?

04:54.080 --> 04:57.840
 Yeah, so I mean, I met Tom a few times before

04:57.840 --> 05:01.800
 and I knew Victor and a bunch of others, Victor Senn.

05:01.800 --> 05:05.320
 So it's funny actually that you mentioned the Europeanness.

05:05.320 --> 05:07.880
 So I'm a European, as you can tell from my accent.

05:07.880 --> 05:11.480
 I'm originally from Holland, but I live in California

05:11.480 --> 05:14.520
 and I spent some time in the UK and in New York

05:14.520 --> 05:15.840
 before I moved to California.

05:15.840 --> 05:20.320
 But Tom, my boss actually lives in Utrecht in the Netherlands,

05:20.320 --> 05:22.760
 which is where I studied for my undergrad.

05:22.760 --> 05:25.640
 So, and Tom is not Dutch.

05:25.640 --> 05:26.680
 But you didn't cross paths

05:26.680 --> 05:29.440
 during your years in the Netherlands.

05:29.440 --> 05:30.280
 No, not at all.

05:30.280 --> 05:32.760
 I left Holland more than 10 years ago.

05:32.760 --> 05:35.560
 So I don't think Tom's been living there for 10 years.

05:35.560 --> 05:36.680
 So it's not a Dutch mafia.

05:36.680 --> 05:37.600
 It's a coincidence.

05:37.600 --> 05:39.400
 It's a French mafia, if anything.

05:39.400 --> 05:41.600
 So the founders are French.

05:41.600 --> 05:43.840
 So, but yeah.

05:43.840 --> 05:47.480
 And so in terms of the company at large,

05:47.480 --> 05:49.640
 what I find fascinating is that we have people,

05:49.640 --> 05:52.240
 I think in more than 25 countries all over the world.

05:52.240 --> 05:55.280
 So in the science team, we have people on the West Coast,

05:55.280 --> 05:58.800
 on the East Coast of the US and in Canada

05:58.800 --> 06:02.840
 and lots of different places in Europe and in South Korea.

06:02.840 --> 06:04.480
 And so it's a-

06:04.480 --> 06:05.400
 And Turkey as well.

06:05.400 --> 06:07.640
 I have a friend based in Istanbul.

06:07.640 --> 06:08.480
 Yeah.

06:08.480 --> 06:12.200
 So let's see, what is your job?

06:12.200 --> 06:14.240
 Ha, good question.

06:14.240 --> 06:15.480
 I wish I knew.

06:15.480 --> 06:19.200
 No, it's, I mean, yeah.

06:19.200 --> 06:24.200
 So broadly speaking, I'm just trying to help the team

06:25.560 --> 06:28.200
 realize this very ambitious vision

06:28.200 --> 06:30.640
 that the founders have for the company

06:30.640 --> 06:33.280
 and for the science team inside the company.

06:34.320 --> 06:38.240
 So yeah, it's not really a well-defined role.

06:38.240 --> 06:41.440
 I think it also kind of depends on what stage we're in

06:41.440 --> 06:44.480
 in a given research project, for example.

06:44.480 --> 06:47.280
 So I'm kind of discovering that as I go along.

06:47.280 --> 06:49.560
 So the official title is Head of Research?

06:49.560 --> 06:50.400
 That's right.

06:50.400 --> 06:51.880
 And so then comes the question,

06:51.880 --> 06:53.400
 what is research at Hugging Face?

06:53.400 --> 06:56.440
 How is it different from research at a university

06:56.440 --> 06:59.640
 or research at a big company like Facebook slash Meta,

06:59.640 --> 07:01.680
 which is where you came from before this?

07:01.680 --> 07:04.120
 Yeah, so we're trying to go for a bit of a different model.

07:04.120 --> 07:07.800
 I think if you want to compare it to a single place,

07:07.800 --> 07:10.000
 then maybe something like DeepMind or OpenAI

07:10.000 --> 07:13.840
 is closer to what we're trying to do than Meta.

07:13.840 --> 07:18.000
 So yeah, as you mentioned, I've been at FAIR for five years

07:18.000 --> 07:19.440
 and it was a wonderful time.

07:20.600 --> 07:23.440
 But one of the things that was difficult at FAIR

07:23.440 --> 07:25.920
 was that it's very bottom up,

07:25.920 --> 07:27.640
 which in theory sounds really nice,

07:27.640 --> 07:28.880
 but it makes it very difficult

07:28.880 --> 07:31.960
 to do very big ambitious projects.

07:31.960 --> 07:35.400
 So if you really want to create step change,

07:35.400 --> 07:38.520
 research artifacts, which is what we're trying to do,

07:38.520 --> 07:41.320
 then you need to pull together big groups of people

07:41.320 --> 07:42.920
 and then make sure that they're all aligned

07:42.920 --> 07:45.360
 in realizing this vision.

07:45.360 --> 07:47.720
 And in a bottom up research organization,

07:47.720 --> 07:49.520
 that's very difficult to do.

07:50.560 --> 07:53.960
 So what we're trying to do is find the optimal place

07:53.960 --> 07:57.000
 between the bottom up approach that FAIR and Google Brain

07:57.000 --> 08:00.160
 and places like that have and the top down approach,

08:00.160 --> 08:02.640
 which DeepMind and OpenAI have,

08:02.640 --> 08:05.240
 where they have a benevolent dictator,

08:05.240 --> 08:08.640
 like Demis or Ilya, basically telling people what to do

08:08.640 --> 08:09.480
 and what the vision is.

08:09.480 --> 08:11.520
 And we're trying to occupy the middle ground

08:11.520 --> 08:13.480
 a little bit there and really try to use the things

08:13.480 --> 08:14.680
 that make us special.

08:14.680 --> 08:16.880
 So that's the ability to move fast,

08:18.080 --> 08:20.040
 the ability to work with the community,

08:20.040 --> 08:23.120
 like we've been doing with projects like Big Science

08:23.120 --> 08:26.520
 and to really, yeah, to exploit the things

08:26.520 --> 08:28.760
 that make us unique.

08:28.760 --> 08:31.760
 What's the difference between Big Science,

08:31.760 --> 08:34.800
 which is a project involving lots of external people,

08:34.800 --> 08:36.920
 as many as a thousand are signed up

08:36.920 --> 08:38.360
 from what I heard from Tomas,

08:39.400 --> 08:42.760
 probably more like hundreds that are active participants

08:42.760 --> 08:45.280
 on a daily basis, but that's big.

08:45.280 --> 08:48.560
 And then the research team at Hugging Face,

08:48.560 --> 08:50.600
 describe your actual team,

08:50.600 --> 08:52.360
 what would you call the actual research team

08:52.360 --> 08:53.600
 at Hugging Face?

08:53.600 --> 08:56.240
 Is it like 10 people, 20?

08:56.240 --> 09:00.080
 So I think last count was 30, 35 people actually.

09:00.080 --> 09:01.040
 Okay, big group.

09:01.040 --> 09:04.080
 Big Science is one of the projects we have going on.

09:04.080 --> 09:06.800
 So I can tell you a bit about the other projects

09:06.800 --> 09:07.640
 we have going on.

09:07.640 --> 09:10.680
 So one of the advantages of being at Hugging Face

09:10.680 --> 09:13.200
 is that it's a super transparent and open company.

09:13.200 --> 09:15.320
 So I can just tell you everything that we're doing

09:15.320 --> 09:18.440
 without feeling bad about it.

09:19.920 --> 09:21.440
 No secret sauce revealed.

09:21.440 --> 09:26.440
 Yeah, so we have a project around multimodal models.

09:27.160 --> 09:29.560
 So multimodality, I think everyone agrees

09:29.560 --> 09:32.680
 is very important for the future of AI.

09:32.680 --> 09:35.680
 And when you say multimodality, for those listening in,

09:35.680 --> 09:37.960
 you're referring to more than just text,

09:37.960 --> 09:41.360
 more than just images, all kinds of sensory,

09:41.360 --> 09:43.280
 what we would think of as sensory modalities

09:43.280 --> 09:45.480
 or information modalities for humans.

09:45.480 --> 09:48.320
 You're trying to capture that for models, but all at once.

09:48.320 --> 09:49.160
 Yeah, all at once.

09:49.160 --> 09:53.040
 So I think if you look at more recent multimodal work,

09:53.040 --> 09:55.600
 it's very often just text and images,

09:55.600 --> 09:57.760
 but there are all kinds of different modalities

09:57.760 --> 10:01.600
 that you all might want to integrate into one single model.

10:01.600 --> 10:03.960
 So how many modalities are you stuffing in?

10:05.320 --> 10:10.320
 So right now it's images, text, videos and audio,

10:11.000 --> 10:12.200
 because those are the main ones.

10:12.200 --> 10:13.240
 And then once you have those,

10:13.240 --> 10:16.280
 then you can start thinking about other specific modalities,

10:16.280 --> 10:18.200
 maybe sort of sub modalities, right?

10:18.200 --> 10:22.600
 So it's unclear whether code as a modality is a part of text

10:22.600 --> 10:24.520
 or if it's something else.

10:24.520 --> 10:26.160
 So there's all kinds of interesting questions

10:26.160 --> 10:28.280
 about what the modality really is.

10:28.280 --> 10:31.200
 So my PhD thesis actually was about grounding meaning

10:31.200 --> 10:32.600
 in perceptual modalities,

10:32.600 --> 10:36.080
 where I also incorporated olfactory semantics.

10:36.080 --> 10:39.360
 So you can build a bag of chemical compounds model

10:39.360 --> 10:42.560
 and build smell vectors essentially,

10:42.560 --> 10:45.480
 and do interesting things with that.

10:45.480 --> 10:47.360
 So that's a long time ago,

10:47.360 --> 10:49.360
 but yeah, there's a lot of potential there.

10:49.360 --> 10:52.160
 What does the word grounded mean in this context?

10:52.160 --> 10:53.160
 So let's use NLP.

10:53.160 --> 10:54.000
 Let's use an example.

10:54.000 --> 10:57.720
 Like you have a model that, like GPT-3.

10:57.720 --> 10:59.880
 So it's learned how to generate text.

10:59.880 --> 11:02.200
 What does it mean for that model to be grounded?

11:02.200 --> 11:04.480
 Yeah, so I was going to say,

11:04.480 --> 11:07.320
 I think the word grounded isn't really well-defined,

11:07.320 --> 11:10.280
 but I'm a philosopher by training originally.

11:10.280 --> 11:13.400
 So I would argue that most things are not well-defined,

11:13.400 --> 11:15.720
 but in my thesis, I make an explicit distinction

11:15.720 --> 11:17.120
 between referential grounding

11:17.120 --> 11:19.560
 and representational grounding.

11:19.560 --> 11:21.880
 And so I think referential grounding

11:21.880 --> 11:26.080
 is what people often think about with like referral data sets.

11:26.080 --> 11:28.240
 So those exist in computer vision, for example,

11:28.240 --> 11:29.880
 where you have to pick out the object.

11:29.880 --> 11:32.040
 So when someone says banana,

11:32.040 --> 11:33.920
 then you have to be able to point in the image

11:33.920 --> 11:35.840
 where the banana is.

11:35.840 --> 11:37.840
 But I think the much more interesting type of grounding

11:37.840 --> 11:39.200
 is representational grounding,

11:39.200 --> 11:42.600
 where you have a holistic meaning representation

11:42.600 --> 11:44.440
 of a concept like elephant,

11:44.440 --> 11:46.360
 or violin maybe is a better example.

11:46.360 --> 11:49.200
 And so you know the semantic meaning of violin.

11:49.200 --> 11:52.160
 You can go to Wikipedia and look up what violin is,

11:52.160 --> 11:53.160
 what that means,

11:53.160 --> 11:55.400
 but you also have a visual representation of it,

11:55.400 --> 11:56.440
 and you know what it looks like,

11:56.440 --> 11:57.800
 you know what it sounds like.

11:57.800 --> 12:00.160
 Maybe you know what it smells like, what it feels like,

12:00.160 --> 12:01.360
 what it's like to play it.

12:01.360 --> 12:03.040
 All of these different modalities

12:03.040 --> 12:06.920
 are a part of your overarching meaning representation

12:06.920 --> 12:08.360
 of the concept of violin.

12:08.360 --> 12:10.680
 And I think that is the much more interesting type

12:10.680 --> 12:12.280
 of meaning representation.

12:12.280 --> 12:15.840
 And so that's the meaning we should try to get into machines

12:15.840 --> 12:18.920
 if we want them to be able to really understand humans.

12:18.920 --> 12:21.080
 So what are some of the problems you see

12:21.080 --> 12:23.440
 with today's models that reveal

12:23.440 --> 12:25.760
 that they're insufficiently grounded?

12:25.760 --> 12:29.560
 Yeah, so I don't know if we're sure

12:29.560 --> 12:31.760
 that models are insufficiently grounded.

12:31.760 --> 12:33.920
 I think that's still an empirical question,

12:33.920 --> 12:34.760
 but my hunch,

12:34.760 --> 12:36.920
 and I think a lot of people in the field share that hunch,

12:36.920 --> 12:39.920
 is that you need to have some understanding of the world

12:39.920 --> 12:41.120
 as humans perceive it

12:41.120 --> 12:43.920
 if you really want to understand humans.

12:43.920 --> 12:46.960
 And so there's a lot of communication

12:46.960 --> 12:49.080
 that happens between humans

12:49.080 --> 12:52.720
 that never really becomes explicit.

12:52.720 --> 12:54.640
 So people call this common sense, for example.

12:54.640 --> 12:57.480
 So the example I always use is coffee

12:57.480 --> 12:58.960
 and what coffee smells like.

12:58.960 --> 13:00.680
 Everybody knows what coffee smells like.

13:00.680 --> 13:02.680
 So I never have to explain that to anyone.

13:02.680 --> 13:05.080
 And so for that reason,

13:05.080 --> 13:07.560
 I also just have no idea how to describe

13:07.560 --> 13:08.400
 the smell of coffee.

13:08.400 --> 13:10.040
 I don't know if you can try that

13:10.040 --> 13:12.800
 or describe the smell of a banana in one sentence.

13:12.800 --> 13:14.240
 Like you've never had to do that

13:14.240 --> 13:15.680
 because you know that everybody knows

13:15.680 --> 13:17.120
 what bananas smell like.

13:17.120 --> 13:18.400
 And if you could pull it off,

13:18.400 --> 13:19.720
 we would call you a poet.

13:19.720 --> 13:21.000
 Yeah, exactly.

13:21.000 --> 13:22.920
 So I think you're totally right.

13:22.920 --> 13:25.080
 So you have to fall back to associations then

13:25.080 --> 13:27.560
 because there is no descriptive language

13:27.560 --> 13:28.720
 for this sort of stuff.

13:28.720 --> 13:31.280
 And I think this happens all over the place

13:31.280 --> 13:34.600
 in natural language communication between humans.

13:34.600 --> 13:37.160
 And that makes it very hard for machines

13:37.160 --> 13:39.920
 to learn this stuff just from reading Wikipedia

13:39.920 --> 13:41.960
 or whatever corpus they're trained on.

13:41.960 --> 13:43.480
 It's funny, you're very much coming at this

13:43.480 --> 13:45.080
 as a philosopher, I could see.

13:45.080 --> 13:47.360
 There's another angle, which is where I'm coming from.

13:47.360 --> 13:51.920
 So I'm at a company that is on the applied side.

13:51.920 --> 13:54.920
 So we're using NLP to try and solve problems.

13:54.920 --> 13:57.280
 And where I see what seems to be the grounded problem

13:57.280 --> 14:00.120
 is the model clearly, if you just poke a little bit,

14:00.120 --> 14:04.000
 it clearly doesn't understand what it's talking about.

14:04.000 --> 14:05.760
 It'll say all the right things.

14:05.760 --> 14:08.520
 And then it reveals that it actually

14:08.520 --> 14:12.440
 has no common sense understanding of what coffee is.

14:12.440 --> 14:15.440
 Because it'll say something that a human would find crazy.

14:15.440 --> 14:18.520
 Yeah, so I think the word understanding,

14:18.520 --> 14:21.040
 what does understanding even mean there?

14:21.040 --> 14:23.280
 So I think what you're maybe talking about,

14:23.280 --> 14:25.800
 and that's, so I think there are two main things missing

14:25.800 --> 14:26.920
 in our current paradigm.

14:26.920 --> 14:30.920
 One is multimodal understanding of concepts,

14:30.920 --> 14:32.920
 and the other is the intentionality

14:32.920 --> 14:34.560
 with a T of language.

14:34.560 --> 14:37.880
 So the fact that we use language with an intent

14:37.880 --> 14:41.240
 to change the mental state of whoever we're talking to,

14:41.240 --> 14:44.720
 so I'm using my voice now to change your brain essentially.

14:45.720 --> 14:49.520
 And so that intent is crucial for real meaning

14:49.520 --> 14:51.160
 and real understanding, and it's something

14:51.160 --> 14:53.280
 that doesn't exist in language models.

14:53.280 --> 14:56.240
 Do you reckon that we have to give real agency

14:56.240 --> 15:00.040
 to systems to achieve that?

15:00.040 --> 15:02.280
 To have them care about something?

15:02.280 --> 15:05.280
 Maybe with reinforcement learning or other paradigms?

15:05.280 --> 15:06.160
 I think so, yeah.

15:06.160 --> 15:09.080
 So I don't know if agency, I mean,

15:09.080 --> 15:12.520
 I don't wanna keep going on the definitions,

15:12.520 --> 15:15.320
 but so agency is also a bit unclear, I think.

15:15.320 --> 15:19.800
 So it's more, yeah, you can model the intent

15:19.800 --> 15:21.880
 of communication when you're trying

15:21.880 --> 15:24.360
 to model human communication.

15:24.360 --> 15:25.920
 You can try to model the intent

15:25.920 --> 15:27.880
 as a part of the interaction.

15:27.880 --> 15:30.040
 So you could think of, so the two things

15:30.040 --> 15:31.840
 I just talked about, you could integrate them

15:31.840 --> 15:33.600
 in language models pretty easily, right?

15:33.600 --> 15:36.480
 So you could have a language model

15:36.480 --> 15:39.040
 that also has a multimodal input.

15:39.040 --> 15:41.160
 Maybe you can put it in an embodied environment

15:41.160 --> 15:43.200
 where it can walk around,

15:43.200 --> 15:45.400
 and then maybe you can have multiple

15:45.400 --> 15:48.280
 of these language models walking around

15:48.280 --> 15:50.080
 in that world and interacting with each other

15:50.080 --> 15:51.480
 and other humans.

15:51.480 --> 15:53.000
 So if you put all of that together,

15:53.000 --> 15:54.720
 then I think you get something very close

15:54.720 --> 15:56.720
 to how humans learn language.

15:56.720 --> 15:59.360
 Is this where you think Hugging Face is headed?

15:59.360 --> 16:01.440
 Is this one of the grand directions?

16:01.440 --> 16:03.760
 This is definitely one of the grand directions, yeah.

16:03.760 --> 16:05.960
 So one of our projects is multimodality.

16:05.960 --> 16:08.640
 As I said, another one is about embodied learning.

16:08.640 --> 16:10.320
 Thomas also talked about this

16:10.320 --> 16:12.360
 when he spoke on this podcast.

16:12.360 --> 16:14.000
 Yeah, the way he described it was,

16:14.000 --> 16:16.480
 maybe we need to teach models language

16:16.480 --> 16:18.400
 more like we teach humans language,

16:18.400 --> 16:20.760
 which is in the world trying to get things done.

16:20.760 --> 16:21.600
 Exactly, yeah.

16:21.600 --> 16:24.680
 So, and that's because we want the models

16:24.680 --> 16:26.840
 to use the kind of language that's useful

16:26.840 --> 16:28.720
 for interacting with humans.

16:28.720 --> 16:31.640
 And so people sort of gloss over this,

16:31.640 --> 16:35.560
 but the reason we want to have natural language understanding

16:35.560 --> 16:37.560
 and natural language generation capabilities

16:37.560 --> 16:39.120
 in these models, because we want them

16:39.120 --> 16:41.440
 to interact with humans.

16:41.440 --> 16:43.440
 And so, I mean, one of the other things

16:43.440 --> 16:44.520
 I've been pushing a lot for

16:44.520 --> 16:47.520
 is a more holistic evaluation of these models

16:47.520 --> 16:50.760
 where rather than just evaluating them on static test sets,

16:50.760 --> 16:53.200
 we actually expose them to real humans

16:53.200 --> 16:54.960
 and we see how well they do in that setting.

16:54.960 --> 16:59.000
 And as you mentioned, those models very quickly break down

16:59.000 --> 17:00.880
 if you try to actually do that.

17:00.880 --> 17:03.240
 All right, so different question.

17:03.240 --> 17:04.720
 I was really curious.

17:04.720 --> 17:07.560
 So I consider you a very multilingual person.

17:07.560 --> 17:08.960
 I mean, all Dutch people are.

17:09.880 --> 17:10.840
 If you've ever met a Dutch person,

17:10.840 --> 17:12.680
 you've met multilingual people.

17:12.680 --> 17:17.680
 And here you are in NLP and adjacent,

17:17.880 --> 17:19.640
 you're definitely expanding beyond that,

17:19.640 --> 17:23.880
 but you would consider yourself an NLP practitioner, yeah?

17:23.880 --> 17:25.600
 I think so, yeah, kind of.

17:25.600 --> 17:27.840
 I mean, I've been branching out for a long time.

17:27.840 --> 17:31.560
 So I would consider myself an AI person.

17:31.560 --> 17:34.640
 So a lot of my work is multimodal, but it's language first.

17:34.640 --> 17:38.600
 Yeah, language is my main interest.

17:38.600 --> 17:41.960
 How frustrating or bizarre has it felt

17:41.960 --> 17:45.560
 to be a deeply multilingual person

17:45.560 --> 17:49.160
 in like a time in science where it's just

17:49.160 --> 17:51.800
 so English dominated, the research itself,

17:51.800 --> 17:54.280
 the tools down to the very data

17:54.280 --> 17:55.680
 that we're training these things on.

17:55.680 --> 17:57.320
 And I'm asking this as an obvious seg

17:57.320 --> 18:00.600
 to this really exciting project that's underway

18:00.600 --> 18:03.440
 to perhaps create the first truly multilingual

18:03.440 --> 18:05.000
 based language model.

18:05.000 --> 18:06.680
 That's my understanding of the project.

18:06.680 --> 18:10.400
 But I first wanted to hear just like you, Dawa,

18:10.400 --> 18:13.200
 like as a deeply multilingual person,

18:13.200 --> 18:15.680
 what does it feel like, what has it felt like

18:15.680 --> 18:19.320
 to be in this weirdly accidentally English dominated space?

18:19.320 --> 18:21.560
 Yeah, so that's a very interesting question,

18:21.560 --> 18:24.040
 but I don't know if I'm the right person to ask it

18:24.040 --> 18:27.320
 because I moved to the UK for my PhD

18:27.320 --> 18:28.760
 and then I moved to the US.

18:28.760 --> 18:33.760
 And so most Dutch people speak pretty decent English,

18:34.280 --> 18:35.120
 I think.

18:35.120 --> 18:40.120
 So I think where the accessibility of language models

18:40.320 --> 18:42.720
 and the multilinguality of language models

18:42.720 --> 18:44.560
 where that really matters is for people

18:44.560 --> 18:48.000
 who are monolingual and who don't speak English.

18:48.000 --> 18:52.960
 So people who can't easily access this technology

18:52.960 --> 18:55.360
 because it's limited only to English.

18:55.360 --> 18:57.880
 But I think that doesn't really apply to most Dutch people

18:57.880 --> 19:01.880
 because they could very easily switch over as you mentioned.

19:01.880 --> 19:04.000
 But also like using these things to make sense

19:04.000 --> 19:05.960
 of the world that's not written in English.

19:05.960 --> 19:07.880
 Like I could tell you how hard it is

19:07.880 --> 19:09.680
 because that's my day to day is like dealing

19:09.680 --> 19:12.920
 with Chinese, Russian, or other languages,

19:12.920 --> 19:17.920
 like the tools and the data is far, far weaker.

19:17.920 --> 19:18.760
 Oh, yeah.

19:18.760 --> 19:19.600
 Yeah, for sure.

19:19.600 --> 19:22.080
 And I think there's also very interesting

19:22.080 --> 19:26.840
 underlying questions there about the cultural differences

19:26.840 --> 19:29.200
 that manifest themselves in languages.

19:29.200 --> 19:33.400
 So English as a language is very explicit.

19:33.400 --> 19:35.480
 So you can be relatively low context

19:35.480 --> 19:37.640
 in how you communicate.

19:37.640 --> 19:39.080
 So you're just very explicit

19:39.080 --> 19:42.320
 or some people would consider Americans relatively blunt.

19:42.320 --> 19:44.000
 I think in how they communicate,

19:44.000 --> 19:46.040
 same for Dutch people by the way.

19:46.040 --> 19:48.400
 But if you think about like Japanese language,

19:48.400 --> 19:50.360
 which is very sort of indirect

19:50.360 --> 19:55.280
 and very different in a sense from English,

19:55.280 --> 19:57.320
 I think that also manifests itself in the culture.

19:57.320 --> 19:59.120
 So maybe there are just things

19:59.120 --> 20:01.320
 that you can't really capture about Japanese culture

20:01.320 --> 20:04.760
 because you have a specific type of language model.

20:04.760 --> 20:07.480
 So tell us a bit about the ongoing experiment

20:07.480 --> 20:09.280
 to make a truly multilingual model.

20:09.280 --> 20:11.560
 Yeah, so this is the big science model.

20:11.560 --> 20:14.240
 It has a name now, it's called Bloom,

20:14.240 --> 20:16.520
 which I think is a really nice name

20:16.520 --> 20:18.720
 because the logo of big science

20:18.720 --> 20:20.880
 has also always been a flower.

20:20.880 --> 20:22.880
 So the flower is starting to bloom.

20:22.880 --> 20:26.600
 And so this language model, it's, as you said,

20:26.600 --> 20:29.200
 the first big multilingual language model.

20:29.200 --> 20:34.200
 And it is only a few weeks away from being done training.

20:35.640 --> 20:37.400
 So it's been very cool.

20:37.400 --> 20:38.920
 You can just follow it on Twitter.

20:38.920 --> 20:41.880
 There's a regular Twitter update

20:41.880 --> 20:46.120
 where it's like we're at like 87% or something now.

20:46.120 --> 20:47.400
 And so-

20:47.400 --> 20:49.520
 Have you been playing with checkpoints?

20:49.520 --> 20:52.800
 Yeah, so there's something called the Bloom book

20:52.800 --> 20:55.240
 where people have been able to just submit prompts

20:55.240 --> 20:59.480
 and then someone would run them and store the output

20:59.480 --> 21:01.200
 somewhere for people to inspect.

21:01.200 --> 21:03.960
 And so we're releasing some checkpoints soon as well

21:03.960 --> 21:04.920
 for people to talk to.

21:04.920 --> 21:06.440
 And then when the final model comes out,

21:06.440 --> 21:07.600
 it's also going to be released

21:07.600 --> 21:10.600
 so that people can play with it themselves.

21:10.600 --> 21:14.080
 Cool, is it a basic text-to-text auto-aggressive model,

21:14.080 --> 21:17.720
 same architecture as your typical big text-to-text models?

21:17.720 --> 21:18.680
 Yeah, basically, yeah.

21:18.680 --> 21:23.480
 So it's, I think by design that there hasn't been

21:23.480 --> 21:26.640
 too much divergence from the sort of standard language model

21:26.640 --> 21:28.800
 that people are used to,

21:28.800 --> 21:30.800
 but there are some nifty new things in there.

21:30.800 --> 21:35.800
 So it uses like a L of I for like how to do

21:36.040 --> 21:37.600
 the token embeddings and things like that.

21:37.600 --> 21:40.480
 So there are a couple of nice different things in there,

21:40.480 --> 21:41.720
 but yeah, the main architecture

21:41.720 --> 21:43.480
 is exactly what you would expect.

21:43.480 --> 21:45.280
 Ooh, wait, let's dig into that.

21:45.280 --> 21:47.600
 A lot of people on this call won't really even know

21:47.600 --> 21:49.280
 what a token or a tokenizer is.

21:49.280 --> 21:51.600
 I think this is a really neat part of NLP.

21:51.600 --> 21:54.680
 This is very much like the tools you use kind of talk,

21:54.680 --> 21:56.480
 but let's just like take a moment.

21:56.480 --> 21:58.280
 Tell us, what is a token?

21:58.280 --> 21:59.200
 What is a tokenizer?

21:59.200 --> 22:01.800
 And then like, how did you do it differently

22:01.800 --> 22:04.800
 with this big Bloom model?

22:04.800 --> 22:06.480
 And why did you have to?

22:06.480 --> 22:09.400
 Yeah, so I'm not the right person

22:09.400 --> 22:11.200
 to really answer detailed questions

22:11.200 --> 22:13.640
 about the tokenization of the language model,

22:15.040 --> 22:17.920
 but so I can explain what tokenization is.

22:17.920 --> 22:21.160
 So it's basically just how do you cut up your text?

22:21.160 --> 22:24.440
 So a sentence consists of words,

22:24.440 --> 22:27.480
 so you could just cut it up in the white space

22:27.480 --> 22:30.640
 and just every word is a token,

22:30.640 --> 22:34.080
 but that is inefficient.

22:34.080 --> 22:35.560
 So what people have been doing

22:35.560 --> 22:38.200
 is trying to chunk it up in smarter ways.

22:38.200 --> 22:41.320
 Cause then you'd have like a vocabulary of millions, right?

22:41.320 --> 22:44.080
 And with multiple languages, it could be huge.

22:44.080 --> 22:46.440
 Yeah, so especially if it's multilingual,

22:46.440 --> 22:48.560
 maybe you just don't see words often enough

22:48.560 --> 22:50.840
 to really have a very good understanding

22:50.840 --> 22:55.480
 of their meaning or a good representation of their meaning.

22:55.480 --> 22:58.760
 And so what you can do is you can chunk different segments

22:58.760 --> 23:00.920
 of words together in smart ways.

23:00.920 --> 23:05.240
 So this is BPE, Biparent Coding and things like that.

23:05.240 --> 23:08.560
 And so there has been a working group

23:08.560 --> 23:10.640
 in the big science workshop.

23:10.640 --> 23:12.240
 So it's like a one year workshop

23:12.240 --> 23:13.440
 is how we're thinking about it.

23:13.440 --> 23:17.200
 And so I think there are 40, 50 different working groups

23:17.200 --> 23:20.560
 and there was one working group working on tokenization.

23:20.560 --> 23:22.120
 They wrote a very nice survey paper.

23:22.120 --> 23:25.360
 They did a big analysis of what the right tokenization is.

23:25.360 --> 23:27.720
 And one of the things that they found,

23:27.720 --> 23:29.720
 I think also together with other,

23:29.720 --> 23:32.040
 like the main model working group

23:32.040 --> 23:36.280
 is that these alibi positional embeddings really help.

23:36.280 --> 23:38.240
 So this was just an empirical finding.

23:39.120 --> 23:42.680
 And so there's just a lot of this small research

23:42.680 --> 23:45.960
 that went into this whole endeavor.

23:45.960 --> 23:47.920
 So why not just go all the way down

23:47.920 --> 23:49.120
 to the individual character?

23:49.120 --> 23:50.800
 Why mess with tokens at all?

23:50.800 --> 23:51.680
 Yeah, that's a good question.

23:51.680 --> 23:55.360
 I mean, there had been efforts in this direction

23:55.360 --> 23:58.800
 or like, so back in the days,

23:58.800 --> 24:02.000
 there were like character RNNs like before transformers

24:02.000 --> 24:03.880
 and people were trying to get this to work.

24:03.880 --> 24:06.760
 It sort of worked, but it didn't really, really work.

24:06.760 --> 24:09.720
 It was a great way to generate made up silly words.

24:09.720 --> 24:11.720
 Yeah, yeah, for sure, yeah.

24:11.720 --> 24:14.120
 And so yeah, I think there's also

24:14.120 --> 24:15.680
 an interesting possibility there

24:15.680 --> 24:18.400
 where we reduce everything to the byte level

24:18.400 --> 24:22.640
 and so when you think about like Unicode or UTF-8

24:22.640 --> 24:24.360
 or things like that, like in theory,

24:24.360 --> 24:27.960
 every single character can just be modeled

24:27.960 --> 24:30.200
 at the byte level and then maybe that's the future

24:30.200 --> 24:34.040
 and then maybe you could even put images

24:34.040 --> 24:36.320
 and audio and everything is just bytes.

24:36.320 --> 24:37.720
 And so basically you can just have

24:37.720 --> 24:41.480
 a pre-trained byte level model.

24:41.480 --> 24:43.920
 So I think that's an interesting research direction

24:43.920 --> 24:45.120
 and there's been some work on that,

24:45.120 --> 24:48.240
 but so far it hasn't really proven to be better

24:48.240 --> 24:51.080
 than just smart ways of tokenizing your data.

24:51.080 --> 24:54.600
 So maybe the real explanation for it not working yet

24:54.600 --> 24:57.160
 is that we haven't used enough data yet.

24:57.160 --> 25:01.240
 So maybe we just need even more data as always.

25:01.240 --> 25:03.360
 Thomas mentioned 800 gigabytes.

25:03.360 --> 25:06.160
 What does that actually translate to in terms of like,

25:06.160 --> 25:08.920
 how much of the internet did you grab for this?

25:08.920 --> 25:10.960
 I understand you crowdsourced it.

25:10.960 --> 25:15.120
 Yeah, so it was crowdsourced with a big community

25:15.120 --> 25:19.360
 of collaborators who were a part of this big science effort.

25:19.360 --> 25:22.480
 And so it's not really a crawl.

25:22.480 --> 25:24.520
 So it's very hard to say like what percentage

25:24.520 --> 25:26.240
 of the internet is this.

25:26.240 --> 25:27.680
 It really depended on the language

25:27.680 --> 25:30.640
 and the folks who contributed the data

25:30.640 --> 25:32.200
 for their own language.

25:32.200 --> 25:34.760
 I think some of them also had different approaches.

25:34.760 --> 25:39.760
 So it's a very kind of a targeted way of collecting data.

25:40.920 --> 25:43.360
 And that's one of the beauties of this big science effort.

25:43.360 --> 25:47.040
 And so I think there's a lot of emphasis on this blue model.

25:47.040 --> 25:48.440
 But what's also very interesting

25:48.440 --> 25:52.480
 about the overarching endeavor is that we have this data set

25:52.480 --> 25:55.080
 which is really beautiful hand curated

25:55.080 --> 25:58.320
 by experts in those languages.

25:58.320 --> 26:01.360
 It has a very interesting coverage of different languages

26:01.360 --> 26:03.520
 geographically over the whole world.

26:04.920 --> 26:07.080
 I don't know what the latest number is.

26:07.080 --> 26:10.120
 No, in the 40s or 50s, I think 45.

26:10.120 --> 26:13.040
 Wow, so this is a huge collection of languages.

26:13.040 --> 26:16.000
 And it includes like low resource African languages

26:16.000 --> 26:16.840
 and things like that.

26:16.840 --> 26:18.480
 So I think that's really great.

26:18.480 --> 26:19.960
 And so there's the data effort

26:19.960 --> 26:22.560
 but then like the legal side of this,

26:22.560 --> 26:24.400
 like how do you distribute the model,

26:24.400 --> 26:26.520
 the governance side of the data itself,

26:26.520 --> 26:29.280
 all of these super intriguing questions

26:29.280 --> 26:31.160
 have just been explored by the community

26:31.160 --> 26:33.080
 completely in the open.

26:33.080 --> 26:34.960
 So it's just fascinating for me.

26:34.960 --> 26:36.200
 I'm sort of an outsider, right?

26:36.200 --> 26:39.520
 So just, yeah, so I mean, me too in a way,

26:39.520 --> 26:43.080
 like this started about a year ago, I think,

26:43.080 --> 26:44.320
 or more than that.

26:44.320 --> 26:47.400
 And so I've just been following it from the sidelines

26:47.400 --> 26:51.280
 and I'm still kind of like not directly involved in it

26:51.280 --> 26:54.960
 that much and it's just amazing to see, so.

26:54.960 --> 26:56.320
 Okay, now back to you.

26:56.320 --> 27:00.080
 So here you are six months into your new role

27:00.080 --> 27:01.080
 at Hugging Face.

27:02.120 --> 27:06.960
 Give us a sense of like what you thought your job would be

27:06.960 --> 27:08.120
 when you started.

27:08.120 --> 27:11.200
 And now six months later, like what has changed?

27:11.200 --> 27:13.560
 What's the newest thing that you've learned

27:13.560 --> 27:16.680
 about yourself and Hugging Face and the mission?

27:16.680 --> 27:19.880
 Like what gets you out of bed in the morning that's changed?

27:19.880 --> 27:22.920
 Yeah, no, that's a very interesting question.

27:22.920 --> 27:27.920
 I mean, I think the job has been what I expected sort of.

27:29.880 --> 27:33.400
 So I knew going into this that it's just an amazing team

27:33.400 --> 27:38.160
 and we really have some brilliant researchers in this team.

27:38.160 --> 27:40.840
 So I was very excited about getting to work

27:40.840 --> 27:43.640
 with those folks and so that's been really awesome.

27:44.840 --> 27:48.840
 I think one thing that I maybe didn't really expect

27:48.840 --> 27:52.240
 is when you're a company like Hugging Face

27:52.240 --> 27:55.640
 and you're distributed all across the globe,

27:55.640 --> 27:57.720
 you have to be very decentralized.

27:57.720 --> 28:00.320
 So a lot of the communication happens asynchronously

28:00.320 --> 28:04.800
 on Slack in public channels, which I think is great.

28:04.800 --> 28:07.400
 And so Hugging Face really has a unique culture

28:07.400 --> 28:10.360
 that supports this way of working together.

28:10.360 --> 28:12.560
 But if you come from a different working culture

28:12.560 --> 28:15.880
 like me coming from meta,

28:15.880 --> 28:18.600
 that is quite the transition to make.

28:18.600 --> 28:20.080
 And so especially-

28:20.080 --> 28:22.480
 You can't just go to a whiteboard with people.

28:22.480 --> 28:24.360
 Yeah, so everything is remote,

28:24.360 --> 28:27.320
 but it's not even just remote where you're both like talking

28:27.320 --> 28:29.880
 to your computer over Zoom.

28:29.880 --> 28:32.520
 It's remote also in time.

28:32.520 --> 28:34.920
 So one of the things I'm struggling with

28:34.920 --> 28:36.680
 is just time zone things.

28:36.680 --> 28:38.680
 So I'm in California, right?

28:38.680 --> 28:40.840
 So I'm sort of trailing the world.

28:40.840 --> 28:45.840
 And so when I wake up or when my son wakes me up

28:46.080 --> 28:48.800
 at around 7 a.m., then I check my phone

28:48.800 --> 28:51.440
 and I have like a million Slack messages

28:51.440 --> 28:52.920
 and emails and things to read through.

28:52.920 --> 28:55.640
 And then usually my meetings start at 8 a.m.

28:55.640 --> 28:58.840
 because I need to make sure I can talk to the Europeans.

28:58.840 --> 29:02.560
 And then they stop working soon after that.

29:02.560 --> 29:06.960
 So I'm always kind of trailing in time, which is not easy.

29:06.960 --> 29:08.400
 So you didn't see that coming?

29:08.400 --> 29:09.840
 I was not prepared for that.

29:09.840 --> 29:11.720
 Yeah, I'm still adjusting.

29:11.720 --> 29:14.680
 But I mean, it's an interesting learning experience.

29:14.680 --> 29:16.120
 And it's just fascinating, I think,

29:16.120 --> 29:18.680
 to see where the world is going with remote work.

29:18.680 --> 29:22.080
 And so this is the future way, I think,

29:22.080 --> 29:24.680
 in which a lot of companies are going to be doing this.

29:24.680 --> 29:27.400
 So what's it like running and building

29:27.400 --> 29:29.760
 and nurturing a research team at a startup?

29:29.760 --> 29:30.600
 I think that's something

29:30.600 --> 29:32.400
 that people will be really curious about.

29:32.400 --> 29:35.240
 I think a lot of people will be familiar directly

29:35.240 --> 29:37.480
 or indirectly with how a research group,

29:37.480 --> 29:40.960
 even 30 strong, like you said, at a university works.

29:40.960 --> 29:43.520
 You've got a PI and that PI's job

29:43.520 --> 29:45.240
 is mostly to get grant money.

29:45.240 --> 29:47.520
 And then you've got the postdocs who actually run the show.

29:47.520 --> 29:49.040
 And then you've got grad students

29:49.040 --> 29:52.080
 who are ranging from miserable to pretty happy.

29:52.080 --> 29:55.160
 And then you've got interns and undergrads.

29:55.160 --> 29:56.760
 Does it have anything like that structure?

29:56.760 --> 29:58.800
 Or is it just a totally different beast?

29:58.800 --> 30:00.120
 Yeah, it's very different.

30:00.120 --> 30:02.520
 So I am definitely not a PI.

30:03.560 --> 30:08.560
 So I'm more a facilitator, I think, or a coordinator.

30:08.560 --> 30:12.920
 And so we have a very flat, non-hierarchical organization.

30:12.920 --> 30:14.000
 We do have team leads.

30:14.000 --> 30:16.760
 So those would be closer to PIs, I think.

30:16.760 --> 30:19.080
 So we have a multimodal project

30:19.080 --> 30:22.680
 and it has very clear team leads and things like that.

30:22.680 --> 30:27.680
 So my role is more like a sort of serving leader

30:28.480 --> 30:31.720
 where I just try to help people the best way I possibly can

30:31.720 --> 30:34.200
 and to make sure they don't have roadblocks

30:34.200 --> 30:36.240
 and that people are talking to each other

30:36.240 --> 30:37.840
 and that I'm aware of what's going on.

30:37.840 --> 30:40.560
 And I try to connect people to the right people

30:40.560 --> 30:42.680
 and connect ideas to the right ideas.

30:42.680 --> 30:45.480
 So it sounds like pretty normal management, actually.

30:46.720 --> 30:49.560
 Yeah, I guess you could say that,

30:49.560 --> 30:51.480
 but it's very different from normal management

30:51.480 --> 30:52.320
 at the same time.

30:52.320 --> 30:54.440
 I think because of how decentralized the company is

30:54.440 --> 30:56.400
 and because of all of the other things

30:56.400 --> 30:57.440
 that are just very different

30:57.440 --> 30:59.560
 from a traditional management role

30:59.560 --> 31:01.480
 at like a big tech company.

31:01.480 --> 31:03.760
 Well, and also the fact that there's like a thousand strong

31:03.760 --> 31:05.320
 group of people outside the company

31:05.320 --> 31:07.640
 that you actually have to work with and coordinate with.

31:07.640 --> 31:09.880
 Yeah, but so that's just a big science project, right?

31:09.880 --> 31:13.320
 So I think, I mean, you make an interesting point

31:13.320 --> 31:16.880
 that one of the things that makes HuggingFace so special

31:16.880 --> 31:20.720
 is that the community plays such a big role in the company.

31:20.720 --> 31:22.680
 And that's not just big science, right?

31:22.680 --> 31:25.440
 So like, if you look at Transformers, the library

31:25.440 --> 31:28.000
 and the open source ecosystem and data sets

31:28.000 --> 31:30.800
 and things like that, that's a huge community.

31:30.800 --> 31:33.440
 And all of these people are also contributing actively

31:33.440 --> 31:36.120
 to making these tools so awesome.

31:36.120 --> 31:39.160
 Yeah, no, I remember the day we first started

31:39.160 --> 31:42.560
 using your Transformers library at my company, Primer.

31:42.560 --> 31:43.640
 It was a revelation.

31:43.640 --> 31:51.120
 You're just like, I can't say enough about how positive

31:51.120 --> 31:55.320
 the open sourcing of Transformer language models was.

31:55.320 --> 31:58.480
 And I think HuggingFace deserves most of the credit.

31:58.480 --> 32:00.440
 Just like, yeah.

32:00.440 --> 32:04.720
 I think one of the reasons that BERT became so popular

32:04.720 --> 32:07.720
 so quickly was because of the Transformers library

32:07.720 --> 32:08.760
 or the predecessor, right?

32:08.760 --> 32:10.480
 So PyTorch pre-trained BERT.

32:10.480 --> 32:13.600
 I remember I was at a workshop at this

32:13.600 --> 32:14.680
 Santa Fe Institute.

32:14.680 --> 32:17.120
 They do these workshops where they invite a bunch of people

32:17.120 --> 32:18.400
 and they talk about some stuff.

32:18.400 --> 32:22.960
 And Fernando Pereira was there, the Google director

32:22.960 --> 32:25.000
 of research, I think.

32:25.000 --> 32:27.160
 And he was saying, we have this thing coming out

32:27.160 --> 32:30.800
 and it's going to blow everything out of the water.

32:30.800 --> 32:31.320
 It's amazing.

32:31.320 --> 32:32.880
 It's going to revolutionize NLP.

32:32.880 --> 32:34.560
 And I've heard people say that before.

32:34.560 --> 32:37.000
 And I never really believed it.

32:37.000 --> 32:38.680
 But in this case, he was right.

32:38.680 --> 32:40.720
 So BERT, yeah.

32:40.720 --> 32:43.560
 So it dropped, I think, two weeks later or something.

32:43.560 --> 32:45.760
 And then so everyone wanted to play with it.

32:45.760 --> 32:47.600
 And being fair, obviously PyTorch

32:47.600 --> 32:48.840
 was the preferred framework.

32:48.840 --> 32:51.880
 And it took like, I don't know, like a week or two

32:51.880 --> 32:54.320
 before there was this PyTorch pre-trained BERT model

32:54.320 --> 32:55.560
 that everyone was playing with.

32:55.560 --> 32:57.840
 So it's amazing.

32:57.840 --> 32:59.880
 And so I did some snooping.

32:59.880 --> 33:02.680
 Your most cited paper, at least according to Google Scholar,

33:02.680 --> 33:06.280
 is this 2017 paper on sentence representations.

33:06.280 --> 33:09.160
 Why I think that's so notable is that that's

33:09.160 --> 33:14.280
 like just on the before side of BERT.

33:14.280 --> 33:18.160
 So BERT comes out in October 2018, something like that.

33:18.160 --> 33:20.800
 And so like, well, a full year before that,

33:20.800 --> 33:24.400
 you were deep in NLP, solving hard NLP problems.

33:24.400 --> 33:28.240
 Do you remember how crazy it was when suddenly,

33:28.240 --> 33:29.680
 like on the other side of that line,

33:29.680 --> 33:32.320
 when we had language models, all the things in NLP

33:32.320 --> 33:34.840
 that were really hard and tedious

33:34.840 --> 33:37.280
 and you needed so much data to even barely get

33:37.280 --> 33:39.880
 some performance suddenly became kind

33:39.880 --> 33:42.960
 of routine and fun and easy?

33:42.960 --> 33:47.720
 Like, I'm not hiding the reality that like tons of stuff

33:47.720 --> 33:49.440
 doesn't work and tons of stuff is still hard.

33:49.440 --> 33:53.840
 But the things that are hard are new things, largely.

33:53.840 --> 33:56.440
 Yeah, so I agree with that.

33:56.440 --> 33:58.960
 But so to me, as a researcher, it

33:58.960 --> 34:01.520
 didn't feel like a very abrupt transition, actually.

34:01.520 --> 34:05.440
 So I think that was much more the case for NLP practitioners

34:05.440 --> 34:08.120
 and more applied people trying to use the tools.

34:08.120 --> 34:10.160
 Yeah, but so for me, as a researcher,

34:10.160 --> 34:14.320
 I think like the transition was actually very natural, right?

34:14.320 --> 34:16.640
 So we were doing things with LSTMs and then, OK,

34:16.640 --> 34:18.000
 transformers.

34:18.000 --> 34:20.920
 So LSTMs didn't really work, so you needed attention.

34:20.920 --> 34:23.960
 And so even in InferCent, we were also

34:23.960 --> 34:25.840
 experimenting with self-attention and things

34:25.840 --> 34:26.400
 like that.

34:26.400 --> 34:28.240
 And then what the transformers paper did

34:28.240 --> 34:30.000
 is it basically removed the recurrence.

34:30.000 --> 34:32.960
 So rather than having an LSTM, we did it forward,

34:32.960 --> 34:35.720
 just a normal MLP feed-forward network.

34:35.720 --> 34:38.880
 And so it turned out that attention on its own

34:38.880 --> 34:40.520
 is actually OK.

34:40.520 --> 34:44.720
 So from that, it became natural to try

34:44.720 --> 34:48.120
 to do this on just language modeling tasks, so there's GPT.

34:48.120 --> 34:49.880
 And then if you can do language modeling,

34:49.880 --> 34:51.120
 why not do it bi-directionally?

34:51.120 --> 34:53.280
 Because we were playing with bi-directional LSTMs

34:53.280 --> 34:53.840
 all the time.

34:53.840 --> 34:55.960
 InferCent is a bi-directional LSTM.

34:55.960 --> 34:59.320
 So BERT is just a bi-directional GPT.

34:59.320 --> 35:04.200
 So it all was very natural, I think, when it came up.

35:04.200 --> 35:06.960
 So it felt naturally, from the point of view

35:06.960 --> 35:09.320
 of understanding the science.

35:09.320 --> 35:11.040
 But I can tell you, from the point of view

35:11.040 --> 35:14.120
 of people trying to solve problems that people

35:14.120 --> 35:19.440
 will pay you money for, it changed everything.

35:19.440 --> 35:23.600
 There is an aspect, though, scientifically that is new.

35:23.600 --> 35:27.560
 I was delighted when this little cottage industry of BERTology

35:27.560 --> 35:29.840
 suddenly sprouted out of nowhere.

35:29.840 --> 35:31.640
 So here's the thing.

35:31.640 --> 35:35.000
 It struck me that deep learning used

35:35.000 --> 35:37.120
 to be very much like a branch of mathematics,

35:37.120 --> 35:38.680
 because it was part of statistics.

35:38.680 --> 35:41.360
 So all of ML was just math.

35:41.360 --> 35:43.840
 And it felt like the math world.

35:43.840 --> 35:45.880
 And then suddenly, here we are today

35:45.880 --> 35:48.080
 with models that are so complicated,

35:48.080 --> 35:50.400
 they're more like biology artifacts.

35:50.400 --> 35:52.760
 We're prodding them and probing them

35:52.760 --> 35:54.840
 and trying to understand things like,

35:54.840 --> 36:00.160
 how the heck does BERT understand grammar?

36:00.160 --> 36:00.800
 To what extent?

36:00.800 --> 36:02.360
 Does it do it differently than us?

36:02.360 --> 36:04.920
 Suddenly, it's feeling more like an empirical science

36:04.920 --> 36:07.880
 and less like a branch of math.

36:07.880 --> 36:10.840
 Yeah, I'm not sure I'm happy with that, actually.

36:10.840 --> 36:14.480
 I also think that this is a couple of things

36:14.480 --> 36:15.640
 to say about that, actually.

36:15.640 --> 36:18.360
 So I think this cottage industry of BERTology

36:18.360 --> 36:23.400
 is interesting, because a few years before that,

36:23.400 --> 36:26.240
 we had a cottage industry in Word2Vec.

36:26.240 --> 36:28.200
 So Word2Vec blew everyone away.

36:28.200 --> 36:31.400
 And then there were a couple of ACLs and EMNOPs

36:31.400 --> 36:33.840
 where just everything was something to Vec.

36:33.840 --> 36:36.960
 And it was all just trying to analyze

36:36.960 --> 36:38.200
 what Word2Vec really did.

36:38.200 --> 36:41.280
 And so I think that's just the progression of science, where

36:41.280 --> 36:43.240
 you have a big breakthrough model

36:43.240 --> 36:45.200
 and then there's some consolidation

36:45.200 --> 36:48.280
 in the Thomas Kuhn paradigm shift.

36:48.280 --> 36:50.240
 So there's a real paradigm shifting

36:50.240 --> 36:51.960
 artifact like Word2Vec or BERT.

36:51.960 --> 36:53.600
 And then there's a lot of consolidation

36:53.600 --> 36:56.240
 where people try to understand this better.

36:56.240 --> 36:58.520
 So I think that's just the natural progression.

36:58.520 --> 37:00.680
 And that's just going to continue happening.

37:00.680 --> 37:05.720
 But about BERT specifically, so we just

37:05.720 --> 37:07.920
 don't have the correct mathematical tools,

37:07.920 --> 37:10.520
 I think, to really understand what it's learning.

37:10.520 --> 37:13.280
 And so there are some efforts out from Chris Ola

37:13.280 --> 37:15.760
 in trying to understand better what transformers are really

37:15.760 --> 37:17.560
 learning.

37:17.560 --> 37:19.480
 So we have a very interesting paper

37:19.480 --> 37:21.880
 called Mass Language Modeling and the Distributional

37:21.880 --> 37:25.920
 Hypothesis, order word does not matter much

37:25.920 --> 37:27.080
 or something like that.

37:27.080 --> 37:31.200
 So what we basically show is that if you shuffle a corpus,

37:31.200 --> 37:34.600
 and so all of the sentences are not in the right order anymore

37:34.600 --> 37:36.040
 and you train a BERT model on it,

37:36.040 --> 37:39.440
 it just does just as well as a regular BERT.

37:39.440 --> 37:41.920
 So you mentioned, does BERT learn grammar?

37:41.920 --> 37:46.800
 Which weirdly seems to suggest that it doesn't matter,

37:46.800 --> 37:48.840
 that it's clearly doing something differently

37:48.840 --> 37:49.560
 than humans do.

37:49.560 --> 37:51.640
 Because if you imagine trying to learn language

37:51.640 --> 37:54.440
 with shuffled language, it'd be a nightmare.

37:54.440 --> 37:55.000
 Exactly.

37:55.000 --> 38:02.480
 Yeah, so I think, yeah, maybe we're also over, yeah,

38:02.480 --> 38:05.800
 I don't know, thinking that BERT is better than it really is

38:05.800 --> 38:08.000
 or these sorts of models that are actually better

38:08.000 --> 38:08.880
 than they really are.

38:08.880 --> 38:12.320
 And I think when you think about GPT-3

38:12.320 --> 38:14.720
 and how much of a splash that made,

38:14.720 --> 38:17.560
 there is also this element that people just

38:17.560 --> 38:20.720
 have a natural tendency to anthropomorphize everything.

38:20.720 --> 38:24.320
 Like you do this to your robot vacuum cleaner

38:24.320 --> 38:25.920
 and thinks that you give it a name.

38:25.920 --> 38:28.920
 So that in the words of Daniel Dennett, the philosopher,

38:28.920 --> 38:31.640
 you're ascribing intentionality.

38:31.640 --> 38:36.560
 And so I think we do that all the time to everything.

38:36.560 --> 38:39.080
 And we do it especially to things that produce language.

38:39.080 --> 38:41.320
 Because language is essentially the only thing

38:41.320 --> 38:44.680
 we know that is really, really human only.

38:44.680 --> 38:47.400
 And so when something produces language, we just go,

38:47.400 --> 38:50.320
 oh, there has to be something brilliant behind that,

38:50.320 --> 38:52.920
 but very often it's just higher order

38:52.920 --> 38:54.960
 distributional statistics.

38:54.960 --> 38:57.600
 It's just clever Hans.

38:57.600 --> 39:00.880
 That's basically, we create these benchmark tests

39:00.880 --> 39:03.880
 and we watch the performance on these tests going up, up, up,

39:03.880 --> 39:08.200
 and we attribute a model like GPT-3 on these language tasks

39:08.200 --> 39:09.720
 as getting truly more clever.

39:09.720 --> 39:11.520
 It truly has a deeper, quote unquote,

39:11.520 --> 39:13.640
 understanding of the task at hand.

39:13.640 --> 39:15.400
 But then you do these clever experiments,

39:15.400 --> 39:17.840
 like the one you described with scrambling, and it reveals,

39:17.840 --> 39:20.560
 well, surely, actually, it's just using distributional

39:20.560 --> 39:21.320
 tricks.

39:21.320 --> 39:23.200
 Yeah, but so I don't know.

39:23.200 --> 39:25.880
 I think the jury is still, I wouldn't put it that strongly.

39:25.880 --> 39:27.120
 I think the jury is still out.

39:27.120 --> 39:31.960
 So I definitely think that there is an evaluation crisis

39:31.960 --> 39:32.960
 in NLP.

39:32.960 --> 39:35.520
 And I mean, I've been doing a lot of work

39:35.520 --> 39:39.120
 with lots of folks in trying to improve that through things

39:39.120 --> 39:42.680
 like Dynabench, where we do a different,

39:42.680 --> 39:45.080
 we try to rethink benchmarking essentially.

39:45.080 --> 39:48.640
 But I mean, it's undeniable that progress in NLP

39:48.640 --> 39:50.880
 has just been insane.

39:50.880 --> 39:56.120
 So if you look at what GPT-3 can do compared to GPT-1,

39:56.120 --> 39:59.480
 or what we can do now with DALI-2 compared to,

39:59.480 --> 40:03.080
 I don't know, the earlier text to image synthesis models.

40:03.080 --> 40:06.600
 It's just crazy how fast we're, yeah, so the progress is real.

40:06.600 --> 40:10.440
 But we should be careful to not kind of over interpret

40:10.440 --> 40:11.520
 what we're seeing.

40:11.520 --> 40:13.760
 So there's still a lot of stuff that.

40:13.760 --> 40:18.120
 So there's a headline going around just this week

40:18.120 --> 40:23.640
 about a researcher from Google essentially attributing

40:23.640 --> 40:27.520
 sentience to the lambda language model.

40:27.520 --> 40:29.800
 And I think that's really to your point,

40:29.800 --> 40:31.640
 what we're talking about.

40:31.640 --> 40:35.400
 These things actually know how to work with language.

40:35.400 --> 40:37.800
 And we humans are language machines.

40:37.800 --> 40:41.680
 We're completely geared towards understanding and transmitting,

40:41.680 --> 40:44.040
 receiving, and sending information

40:44.040 --> 40:47.720
 with language that is the most human information,

40:47.720 --> 40:51.120
 intentionality, and understand the world around us,

40:51.120 --> 40:52.640
 getting stuff done.

40:52.640 --> 40:57.080
 And so when some mathematical object is doing it,

40:57.080 --> 40:58.120
 I feel it too.

40:58.120 --> 40:58.800
 I can't help it.

40:58.800 --> 41:00.960
 Have you ever kind of had that feeling

41:00.960 --> 41:03.480
 that you just have to push away of like, man,

41:03.480 --> 41:05.080
 I'm talking to this thing?

41:05.080 --> 41:07.160
 But what do you mean by a mathematical object?

41:07.160 --> 41:09.280
 I mean, I think you can argue that your brain is also

41:09.280 --> 41:12.080
 a mathematical object, or at least you can write

41:12.080 --> 41:13.360
 your brain as a very complicated.

41:13.360 --> 41:14.120
 You took the bait.

41:14.120 --> 41:16.480
 I was hoping you'd take the bait.

41:16.480 --> 41:20.320
 This is like philosopher catnip.

41:20.320 --> 41:21.440
 Well, yeah, I don't know.

41:21.440 --> 41:25.520
 So I think it's very interesting because there

41:25.520 --> 41:27.760
 is a very nice theory of consciousness

41:27.760 --> 41:30.280
 that says that we take the intentional stance

41:30.280 --> 41:32.680
 towards ourselves as rational agents,

41:32.680 --> 41:36.040
 and that's what consciousness is, this strange loop,

41:36.040 --> 41:38.120
 as Douglas Hofstadter calls it.

41:38.120 --> 41:41.560
 So yeah, maybe we're evolutionarily hardwired

41:41.560 --> 41:44.280
 to take an intentional stance towards things,

41:44.280 --> 41:47.040
 and that's why we're so confused by the NOP progress

41:47.040 --> 41:48.000
 we've been seeing.

41:48.000 --> 41:50.480
 Does it also hint at a way to achieve

41:50.480 --> 41:54.480
 artificial intelligence of a more AGI flavor?

41:54.480 --> 42:00.080
 Yeah, so again, I don't really know what AGI even means.

42:00.080 --> 42:04.560
 And I think it's very premature to start thinking about it.

42:04.560 --> 42:07.760
 So we have a philosopher slash ethicist

42:07.760 --> 42:10.280
 who joined Hugging Face recently, Giada Pastilli.

42:10.280 --> 42:14.200
 So she has made this point on Twitter, too,

42:14.200 --> 42:17.720
 I think, where there are real problems right now

42:17.720 --> 42:19.520
 with the deployment of AI.

42:19.520 --> 42:22.280
 And so when we're thinking about the applied ethics

42:22.280 --> 42:25.680
 of these systems, there are just real things

42:25.680 --> 42:27.200
 we need to fix right now.

42:27.200 --> 42:30.480
 And they are much more salient and much more important

42:30.480 --> 42:34.400
 right now than thinking about AGI and paperclip maximizers

42:34.400 --> 42:35.400
 and things like that.

42:35.400 --> 42:36.520
 I agree.

42:36.520 --> 42:38.560
 I agree completely.

42:38.560 --> 42:40.280
 I think, actually, the biggest problem

42:40.280 --> 42:42.200
 to solve from an ethics point of view

42:42.200 --> 42:46.200
 is these systems not working that well on narrow tasks

42:46.200 --> 42:47.800
 and people overtrusting them.

42:47.800 --> 42:49.640
 That's where a lot of harm can come from.

42:49.640 --> 42:50.760
 Not from bias, even.

42:50.760 --> 42:52.680
 That's another level of problem.

42:52.680 --> 42:56.520
 Just like overtrusting systems, misunderstanding their limits.

42:56.520 --> 42:57.600
 Yeah, true.

42:57.600 --> 42:59.840
 But so there's a trade-off here, too, right?

42:59.840 --> 43:01.760
 So we shouldn't hype them up too much

43:01.760 --> 43:03.840
 because people will just misunderstand what

43:03.840 --> 43:05.480
 these systems are capable of.

43:05.480 --> 43:07.720
 But we also shouldn't under-hype them too much.

43:07.720 --> 43:11.640
 So Sam Bowman, professor at NYU, has a very nice paper

43:11.640 --> 43:13.800
 where he talks about the dangers of under-hyping.

43:13.800 --> 43:17.160
 So if we all just pretend that there is no progress at all,

43:17.160 --> 43:20.560
 then at some point we are going to be very surprised when

43:20.560 --> 43:24.000
 AGI suddenly emerges and we have sentient AI.

43:24.000 --> 43:26.840
 So we definitely should think about this stuff.

43:26.840 --> 43:30.600
 And so AI alignment is a very active research area,

43:30.600 --> 43:32.960
 and it's a very important research area.

43:32.960 --> 43:35.320
 But it's all about finding that balance, I think.

43:35.320 --> 43:36.280
 Sure.

43:36.280 --> 43:38.160
 OK, lightning round.

43:38.160 --> 43:41.640
 Most exciting things on the horizon

43:41.640 --> 43:45.680
 for research in NLP that you're either working towards now

43:45.680 --> 43:47.160
 or you'd like to.

43:47.160 --> 43:50.040
 Go a little bit further than what you're just about to ship,

43:50.040 --> 43:51.640
 just about to publish.

43:51.640 --> 43:55.680
 Yeah, so I'm very excited in multimodality, obviously.

43:55.680 --> 43:59.040
 I think that there's a lot of interesting work coming out

43:59.040 --> 44:02.640
 in semi-parametric models where you have a retriever

44:02.640 --> 44:05.840
 component and some sort of lightweight reader

44:05.840 --> 44:07.840
 model on top of that retriever.

44:07.840 --> 44:09.200
 So there is a paper from DeepMind

44:09.200 --> 44:12.400
 coming out a couple of days ago.

44:12.400 --> 44:16.640
 The idea in a nutshell there is that these language models are

44:16.640 --> 44:20.280
 basically frozen in time based on the data you give them.

44:20.280 --> 44:23.960
 And so we need some way to help them keep refreshing what they

44:23.960 --> 44:25.000
 understand about the world.

44:25.000 --> 44:26.840
 Oh, yeah, that's just one application.

44:26.840 --> 44:32.440
 I think it's much more about how you

44:32.440 --> 44:33.720
 learn different things.

44:33.720 --> 44:36.520
 So as humans, we have different kinds of memory.

44:36.520 --> 44:39.160
 We have a semantic memory and an episodic memory.

44:39.160 --> 44:43.280
 And so we also have a library and the internet

44:43.280 --> 44:44.640
 where we can look up stuff.

44:44.640 --> 44:47.720
 So we don't have to store all of it in our parameters,

44:47.720 --> 44:49.320
 our brain.

44:49.320 --> 44:51.520
 So I think if you do this with models, too,

44:51.520 --> 44:54.920
 where you have a big index, where you can invest

44:54.920 --> 44:57.920
 a lot of heavy compute in having a very high quality index,

44:57.920 --> 45:00.440
 then you can have lots of lighter weight reader

45:00.440 --> 45:01.560
 models on top of this.

45:01.560 --> 45:03.880
 So this is also going to have lots of repercussions,

45:03.880 --> 45:07.560
 I think, for industry, where if you're a company like Facebook,

45:07.560 --> 45:10.160
 you want to have a million classifiers

45:10.160 --> 45:11.760
 from all of these different teams

45:11.760 --> 45:14.080
 all trying to do cool stuff with their classifier.

45:14.080 --> 45:16.240
 If they have a big index that they can rely on,

45:16.240 --> 45:20.640
 then you kind of do the compute in a much more intelligent way,

45:20.640 --> 45:21.320
 I think.

45:21.320 --> 45:25.440
 So it's about finding the mix between the retriever, which

45:25.440 --> 45:27.480
 will be a big sort of language model,

45:27.480 --> 45:29.680
 and the reader model on top, which will also

45:29.680 --> 45:31.640
 be a big sort of language model.

45:31.640 --> 45:34.080
 And just for people who are unfamiliar with the current

45:34.080 --> 45:37.320
 status quo, what we have right now

45:37.320 --> 45:40.680
 is basically just two kinds of information storage.

45:40.680 --> 45:42.560
 You've got the model itself, which

45:42.560 --> 45:45.360
 has been pre-baked with just an understanding of language

45:45.360 --> 45:48.400
 and whatever emerges from that, just basically

45:48.400 --> 45:50.560
 predicting missing words.

45:50.560 --> 45:52.360
 And then you've got what people usually

45:52.360 --> 45:54.000
 call the prompt, which is whatever

45:54.000 --> 45:57.000
 you can cram into the attention window at inference time.

45:57.000 --> 45:59.200
 So you can actually put a whole conversation there.

45:59.200 --> 46:00.400
 You can put example problems.

46:00.400 --> 46:02.440
 You could do a lot of neat things in the prompt.

46:02.440 --> 46:05.200
 But it's pretty darn limited, right?

46:05.200 --> 46:08.200
 Aside from your pre-baked knowledge that's crystallized,

46:08.200 --> 46:09.880
 all these things can do is whatever

46:09.880 --> 46:11.240
 you can cram into the prompt.

46:11.240 --> 46:13.440
 And what you're suggesting is, hey, maybe we

46:13.440 --> 46:17.520
 could actually build a whole separate system

46:17.520 --> 46:21.280
 where they could retrieve knowledge at game time.

46:21.280 --> 46:23.680
 Yeah, so there are some interesting models.

46:23.680 --> 46:26.320
 So I've been involved in a model called RAAG, Retrieval

46:26.320 --> 46:27.440
 Augmented Generation.

46:27.440 --> 46:29.560
 And there is also Realm from Google.

46:29.560 --> 46:32.720
 And yeah, so the basic idea is that you can,

46:32.720 --> 46:35.920
 so you have your language model, which would be parametric.

46:35.920 --> 46:39.480
 And then you have your KNN, like a nearest neighbor search

46:39.480 --> 46:41.960
 algorithm, essentially, which is non-parametric.

46:41.960 --> 46:43.800
 And if you put those two approaches together,

46:43.800 --> 46:46.640
 you get a semi-parametric model.

46:46.640 --> 46:49.440
 And I think there's a lot of potential applications

46:49.440 --> 46:50.720
 for that down the line.

46:50.720 --> 46:51.480
 So that's one thing.

46:51.480 --> 46:53.840
 And then the other thing, so I said multimodal,

46:53.840 --> 46:54.760
 semi-parametric.

46:54.760 --> 46:57.480
 And I think the other thing that's going to be interesting,

46:57.480 --> 47:00.040
 and there's a lot of traction happening there now, too,

47:00.040 --> 47:02.600
 is around data-centric AI.

47:02.600 --> 47:06.520
 So I'm still rooting for things like active learning becoming

47:06.520 --> 47:10.640
 much more mainstream, measuring our data much more carefully.

47:10.640 --> 47:14.240
 So we have folks like Mick Mitchell in Huggingface

47:14.240 --> 47:17.120
 working on data measurement tools and things like that.

47:17.120 --> 47:19.160
 So really trying to understand much better what's

47:19.160 --> 47:21.080
 really happening in our data and trying

47:21.080 --> 47:23.680
 to do things to the data or curate the data

47:23.680 --> 47:27.280
 in different ways so that we can have better models in the end.

47:27.280 --> 47:28.360
 Yeah.

47:28.360 --> 47:31.240
 Quickly, before calling you, I actually

47:31.240 --> 47:37.320
 refreshed my knowledge of what this data tool looks like.

47:37.320 --> 47:39.440
 It's kind of like an x-ray for data sets.

47:39.440 --> 47:40.800
 And it's a really beautiful idea.

47:40.800 --> 47:42.160
 I'm surprised that no one had, you

47:42.160 --> 47:44.000
 know an idea is a good one when you're like,

47:44.000 --> 47:45.960
 why haven't we been doing this for ages?

47:45.960 --> 47:49.360
 It's just like all the automatic, obvious things

47:49.360 --> 47:52.800
 you can measure about a data set that's composed of language,

47:52.800 --> 47:54.960
 like let's put that all in one toolkit.

47:54.960 --> 47:56.400
 And then you can keep adding to it

47:56.400 --> 47:58.000
 and make it more and more sophisticated.

47:58.000 --> 47:59.360
 That's the basic idea, right?

47:59.360 --> 48:01.000
 Yeah, that's exactly it.

48:01.000 --> 48:03.040
 And so I think that it's just nice

48:03.040 --> 48:06.600
 that this lives in a space so it has a graphical user

48:06.600 --> 48:09.120
 interface that just exists.

48:09.120 --> 48:10.440
 Maybe in the longer term, it will

48:10.440 --> 48:12.760
 be a natural part of the Huggingface hub

48:12.760 --> 48:15.560
 where you can just upload any data set to the hub

48:15.560 --> 48:18.480
 and start measuring what's actually in your data.

48:18.480 --> 48:20.480
 And you have a nice interface where you can just

48:20.480 --> 48:22.280
 inspect it on the fly.

48:22.280 --> 48:24.800
 So we have a lot of interesting things

48:24.800 --> 48:28.400
 going on in the direction of evaluation and measurement.

48:28.400 --> 48:30.560
 So I think what's really crucial when

48:30.560 --> 48:32.760
 you think about the AI pipeline of the future

48:32.760 --> 48:35.760
 is that you have raw data, which you turn into data sets.

48:35.760 --> 48:37.680
 And those data sets you turn into models,

48:37.680 --> 48:40.240
 but you want to measure your data sets and your models.

48:40.240 --> 48:41.880
 And you want to understand what's in there

48:41.880 --> 48:42.920
 and how well they perform.

48:42.920 --> 48:45.120
 And then you want to, based on your measurement,

48:45.120 --> 48:47.560
 deploy the best model to production,

48:47.560 --> 48:50.360
 so for making predictions with your model.

48:50.360 --> 48:53.160
 And so measurement is really absolutely

48:53.160 --> 48:56.880
 crucial in all of the decisions that you're making there.

48:56.880 --> 48:59.000
 But measurement is also very difficult.

48:59.000 --> 49:02.080
 And so that's something that we're trying to address.

49:02.080 --> 49:04.880
 So we have an evaluate library that came out

49:04.880 --> 49:05.800
 a couple of weeks ago.

49:05.800 --> 49:09.160
 We have some very exciting announcements coming up soon.

49:09.160 --> 49:10.880
 I don't know when this podcast comes out.

49:10.880 --> 49:12.480
 It might already be out by that time.

49:12.480 --> 49:15.800
 But we're working on evaluation on the hub

49:15.800 --> 49:19.280
 so that you can essentially evaluate any model on any data

49:19.280 --> 49:21.840
 set using any metric, just at the click of a button,

49:21.840 --> 49:25.160
 so you don't have to do any manual stuff there.

49:25.160 --> 49:26.720
 Surely people are going to miss having

49:26.720 --> 49:30.720
 to go and copy-paste massive chunks of scikit-learn code

49:30.720 --> 49:32.200
 into their Jupyter notebooks.

49:32.200 --> 49:32.840
 Come on, Dawa.

49:32.840 --> 49:34.840
 I'm sure they will, yeah.

49:34.840 --> 49:35.400
 Yeah.

49:35.400 --> 49:42.400
 No, I think lowering the barrier to doing proper evaluation

49:42.400 --> 49:44.400
 according to best practices, I think

49:44.400 --> 49:46.720
 that that's a hugely impactful thing to do.

49:46.720 --> 49:48.760
 And that's something that Huggingface is uniquely

49:48.760 --> 49:50.240
 quick to do.

49:50.240 --> 49:52.760
 So that's one of the things I've been excited about also

49:52.760 --> 49:53.920
 in the science team.

49:53.920 --> 49:55.360
 You mentioned active learning.

49:55.360 --> 49:57.080
 I'd love to dig into that a little bit.

49:57.080 --> 49:58.760
 That's something I've worked with myself.

49:58.760 --> 50:01.040
 And it's an enticing idea.

50:01.040 --> 50:05.040
 Just for those listening at home who aren't familiar,

50:05.040 --> 50:08.520
 usually when you want to make a data set to train a model,

50:08.520 --> 50:11.200
 you, the human, have to select the samples of data

50:11.200 --> 50:13.160
 from your raw data pool that you're

50:13.160 --> 50:17.160
 going to gold label and train your model on.

50:17.160 --> 50:19.000
 The idea of active learning, in a nutshell,

50:19.000 --> 50:21.200
 is let's put a model between you and the data,

50:21.200 --> 50:23.120
 sometimes the model you're training.

50:23.120 --> 50:26.600
 And it will decide which ones to put in front of you,

50:26.600 --> 50:28.520
 the human, whose time is expensive

50:28.520 --> 50:32.400
 and whose sight is limited, and make good choices

50:32.400 --> 50:35.880
 and try and find the most instructive examples

50:35.880 --> 50:38.800
 from the data to label so that you get the most bang for buck

50:38.800 --> 50:41.520
 because no one wants to spend their whole life labeling data.

50:41.520 --> 50:44.600
 In fact, there's some problems that that's just prohibitive.

50:44.600 --> 50:46.360
 You literally just can't do it.

50:46.360 --> 50:49.880
 True positives are too rare, et cetera, et cetera.

50:49.880 --> 50:53.240
 So is there some really exciting new developments

50:53.240 --> 50:54.040
 in active learning?

50:54.040 --> 50:57.080
 I feel like it's kind of like a done deal.

50:57.080 --> 50:59.160
 Is there something new and exciting on the horizon,

50:59.160 --> 51:00.200
 you think?

51:00.200 --> 51:03.040
 Yeah, I'm not so.

51:03.040 --> 51:06.280
 I think it just has a lot of potential.

51:06.280 --> 51:10.120
 And so what you described is a specific kind

51:10.120 --> 51:12.600
 of active learning, I think, where you have an acquisition

51:12.600 --> 51:14.920
 function that scores examples, and you just

51:14.920 --> 51:16.760
 decide which example you want to label.

51:16.760 --> 51:20.360
 But I think there are extensions of these algorithms

51:20.360 --> 51:22.600
 where you can think not about the labeling part

51:22.600 --> 51:23.960
 but about the pre-training part.

51:23.960 --> 51:28.120
 So maybe I can pick parts of a large corpus

51:28.120 --> 51:29.840
 that I should be pre-training on now

51:29.840 --> 51:33.120
 because I know what downstream task I care about in the end.

51:33.120 --> 51:35.280
 And so there's a mismatch currently

51:35.280 --> 51:37.680
 between the pre-training phase where we just

51:37.680 --> 51:41.160
 do language modeling or causal or mass language modeling.

51:41.160 --> 51:42.760
 And then we fine tune it on something

51:42.760 --> 51:45.200
 that might be very different from what we're training on.

51:45.200 --> 51:47.440
 So we don't really know what to train on.

51:47.440 --> 51:50.720
 So I think if we can connect the pre-training phase to what

51:50.720 --> 51:53.240
 we know we are going to care about,

51:53.240 --> 51:55.000
 then you can do very interesting things.

51:55.000 --> 51:58.920
 And so the way to do that selection, so data selection,

51:58.920 --> 52:02.040
 is through an acquisition function type of thing.

52:02.040 --> 52:04.040
 So that's why it's related to active learning.

52:04.040 --> 52:06.160
 Another interesting thing that I've

52:06.160 --> 52:07.840
 been working on as well with some folks

52:07.840 --> 52:09.920
 is dynamic adversarial data collection,

52:09.920 --> 52:13.520
 where you have a model in the loop and a humanist trying

52:13.520 --> 52:14.760
 to fool the model.

52:14.760 --> 52:17.600
 And so if you take the model fooling examples,

52:17.600 --> 52:19.800
 or so if you take all the examples, including

52:19.800 --> 52:21.200
 the ones that didn't fool the model

52:21.200 --> 52:23.720
 but that are still kind of intended

52:23.720 --> 52:26.440
 to try to probe the model for a weakness,

52:26.440 --> 52:28.920
 if you train on that data and you keep updating the model

52:28.920 --> 52:33.240
 as you're doing the training, so that's the dynamic component,

52:33.240 --> 52:35.480
 then you get a much better model out in the end.

52:35.480 --> 52:37.920
 So it's really like 10% better.

52:37.920 --> 52:40.520
 So we have a nice paper where we try to do this in the limit,

52:40.520 --> 52:43.400
 over like 20 rounds of natural language inference,

52:43.400 --> 52:45.880
 and you just get a much, much better model out in the end.

52:45.880 --> 52:47.960
 So I think the future of data collection,

52:47.960 --> 52:51.160
 the way we think about it now in the field,

52:51.160 --> 52:54.160
 is going to be changed a little bit where everything is just

52:54.160 --> 52:56.360
 always going to be with models in the loop.

52:56.360 --> 52:59.440
 And that maybe ties back to this long-term vision

52:59.440 --> 53:01.680
 of having language models interacting with each other

53:01.680 --> 53:02.640
 in some environment.

53:02.640 --> 53:06.880
 But if you can have humans and models together interacting

53:06.880 --> 53:08.600
 with each other and learning from each other

53:08.600 --> 53:10.840
 and maybe trying to also kind of probe each other

53:10.840 --> 53:14.480
 and be on the decision boundaries of certain things,

53:14.480 --> 53:16.840
 then you can learn much more efficiently, I think.

53:16.840 --> 53:20.680
 It sounds like you're describing education, like a school.

53:20.680 --> 53:21.520
 Exactly, yeah.

53:21.520 --> 53:25.320
 Yeah, yeah, so in terms of education,

53:25.320 --> 53:27.640
 one of the important things is also a curriculum, right?

53:27.640 --> 53:31.840
 So I think one thing you could do with this pre-training,

53:31.840 --> 53:34.600
 like the active learning of pre-training and connecting

53:34.600 --> 53:37.000
 that to fine tuning, is you could try to have a smarter

53:37.000 --> 53:37.840
 curriculum.

53:37.840 --> 53:41.760
 So if your acquisition function changes over time,

53:41.760 --> 53:43.680
 essentially you're designing a curriculum

53:43.680 --> 53:46.240
 or you're learning a curriculum on the fly that

53:46.240 --> 53:48.800
 helps your language model be as good as possible

53:48.800 --> 53:50.880
 on the downstream tasks that you might care about.

53:50.880 --> 53:55.800
 OK, and final question, at least that I can think of,

53:55.800 --> 53:58.720
 is something that Thomas mentioned that intrigued me,

53:58.720 --> 54:02.960
 which is he said that there seems

54:02.960 --> 54:06.480
 to be something missing with NLP.

54:06.480 --> 54:08.840
 Like we're making these language models bigger and bigger

54:08.840 --> 54:11.040
 and bigger, and yes, we're improving the data

54:11.040 --> 54:13.120
 and we're getting more data centric,

54:13.120 --> 54:17.560
 but he gave the impression that he really

54:17.560 --> 54:19.720
 believes that there's something fundamentally missing.

54:19.720 --> 54:22.000
 It's not just more data, it's not just more text.

54:22.000 --> 54:23.400
 And you've hinted at this yourself

54:23.400 --> 54:26.680
 with multimodality and interaction, agency,

54:26.680 --> 54:29.560
 whatever that means.

54:29.560 --> 54:35.640
 So yeah, if you had to make a guess 10 years from now,

54:35.640 --> 54:38.040
 what do you think the paradigm is going to be?

54:38.040 --> 54:39.720
 Will we even talk about NLP?

54:39.720 --> 54:42.880
 I suspect NLP will be a historical footnote.

54:42.880 --> 54:46.040
 They'll just be AI, right?

54:46.040 --> 54:50.880
 And text will be just one of the many crucial developmental raw

54:50.880 --> 54:55.240
 material for artificial intelligence.

54:55.240 --> 54:57.360
 Yeah, but I still think that text will just

54:57.360 --> 55:01.640
 remain a dominant modality.

55:01.640 --> 55:02.840
 Well, as it is with humans.

55:02.840 --> 55:03.960
 Exactly, right.

55:03.960 --> 55:06.480
 So language really is very crucial.

55:06.480 --> 55:09.840
 So I don't think NLP itself is going away,

55:09.840 --> 55:13.720
 but I think in order to get to real meaning,

55:13.720 --> 55:15.880
 all of these other fields are probably

55:15.880 --> 55:19.800
 going to be subsumed into NLP when it comes

55:19.800 --> 55:22.240
 to language understanding.

55:22.240 --> 55:24.120
 So yeah, I don't know.

55:24.120 --> 55:25.680
 In 10 years from now, I think we're

55:25.680 --> 55:30.320
 going to have very, very different models in a sense.

55:30.320 --> 55:32.240
 But I think a lot of the building blocks

55:32.240 --> 55:35.120
 that we have now are still going to exist in those models.

55:35.120 --> 55:37.400
 Do you think it'll just be all eventually robotics,

55:37.400 --> 55:42.360
 either real world and or virtual world robots,

55:42.360 --> 55:45.440
 like taking in all the sensory information?

55:45.440 --> 55:47.560
 Virtual robots I definitely buy.

55:47.560 --> 55:51.440
 So I think for actual physical robots,

55:51.440 --> 55:55.280
 I think learning from that doesn't really scale that well.

55:55.280 --> 55:57.600
 So I think one of the big problems in robotics

55:57.600 --> 55:59.040
 is sim to real, right?

55:59.040 --> 56:00.640
 How do you transfer from a simulation

56:00.640 --> 56:02.840
 to a real environment?

56:02.840 --> 56:05.840
 And so as our simulations become realer and realer,

56:05.840 --> 56:08.760
 that problem is going to become smaller and smaller.

56:08.760 --> 56:10.920
 So I don't think we need physical embodiment

56:10.920 --> 56:13.840
 in any real sense in order to get to meaning,

56:13.840 --> 56:16.480
 but we'll probably definitely need virtual embodiment

56:16.480 --> 56:18.020
 where you have an environment where you

56:18.020 --> 56:20.080
 can interact with each other.

56:20.080 --> 56:22.240
 But maybe that environment already exists, right?

56:22.240 --> 56:25.200
 So maybe that environment is just the internet

56:25.200 --> 56:28.200
 or maybe that environment will come into existence very soon

56:28.200 --> 56:32.160
 in the form of the metaverse or whatever you want to call it.

56:32.160 --> 56:33.640
 Any final thoughts you want to share

56:33.640 --> 56:35.920
 with tens of thousands of people?

56:35.920 --> 56:41.400
 I think one of the things that I think is important

56:41.400 --> 56:44.160
 and that's kind of what Hugging Face also stands for

56:44.160 --> 56:46.640
 is just open source and open science.

56:46.640 --> 56:50.820
 And so if I were to give any parting thoughts,

56:50.820 --> 56:55.080
 I would encourage people to always embrace openness

56:55.080 --> 56:58.040
 because that really is crucial to making progress,

56:58.040 --> 57:00.520
 but also making sure that the progress that we make

57:00.520 --> 57:02.080
 doesn't end up in the wrong hands

57:02.080 --> 57:03.760
 or go in the wrong direction.

57:03.760 --> 57:06.000
 So just for those listening at home,

57:06.000 --> 57:09.920
 Dawah, where can we find out more about you and what you do?

57:09.920 --> 57:11.300
 Yeah, so I have a website.

57:11.300 --> 57:14.120
 It's dawahkiela.github.io.

57:14.120 --> 57:17.560
 It has a couple of links to relevant social media profiles.

57:17.560 --> 57:19.360
 I also have a Twitter account.

57:19.360 --> 57:21.460
 So that's Dawah Kiela, my name.

57:21.460 --> 57:26.080
 So D-O-U-W-E-K-I-E-L-A. And so I'm

57:26.080 --> 57:27.560
 trying to be more active on Twitter.

57:27.560 --> 57:29.040
 I'm still working on that.

57:29.040 --> 57:31.960
 And I'm BohannanBot on Twitter, and you'll

57:31.960 --> 57:35.440
 see me probably asking follow-up questions out in the open

57:35.440 --> 57:36.560
 following your advice.

57:36.560 --> 57:37.760
 Dawah, thank you so much.

57:37.760 --> 57:39.400
 This was a blast.

57:39.400 --> 57:39.920
 Thank you.

57:39.920 --> 57:51.960
 Thanks for having me.

