WEBVTT

00:00.000 --> 00:06.400
 All right.

00:06.400 --> 00:07.400
 Let's get this started.

00:07.400 --> 00:08.400
 Hi, Dawa.

00:08.400 --> 00:09.400
 Hi.

00:09.400 --> 00:13.960
 So, I'm talking to you from my home in San Francisco.

00:13.960 --> 00:14.960
 Where are you?

00:14.960 --> 00:17.960
 I'm in Palo Alto, not too far away from you, actually.

00:17.960 --> 00:18.960
 Also at your home?

00:18.960 --> 00:22.200
 Also at my home, yes.

00:22.200 --> 00:23.280
 The new office.

00:23.280 --> 00:24.280
 The new office, yeah.

00:24.280 --> 00:29.080
 So, we do actually have an office, a hugging face Silicon Valley office in Palo Alto, not

00:29.080 --> 00:32.720
 too far away from here, which we opened recently.

00:32.720 --> 00:36.880
 But yeah, I'm still getting used to going to an actual office.

00:36.880 --> 00:38.880
 I really like my home office.

00:38.880 --> 00:41.680
 Yeah, it's kind of here to stay.

00:41.680 --> 00:47.040
 So, this is really exciting for me because for a number of reasons.

00:47.040 --> 00:53.080
 One, Hugging Face is one of the most interesting companies today, especially in the machine

00:53.080 --> 00:58.080
 learning space, but most especially in the natural language processing space, which is

00:58.080 --> 00:59.840
 where I work.

00:59.840 --> 01:05.880
 And yeah, I saw the tweet in January that you sent out announcing that you were the

01:05.880 --> 01:08.960
 new head of research at Hugging Face.

01:08.960 --> 01:11.020
 And I've been dying to talk to you ever since.

01:11.020 --> 01:16.000
 And it's been a good six months, so you've had time to settle in, find your feet, get

01:16.000 --> 01:21.080
 up to speed, actually maybe make an agenda and a plan for yourself at Hugging Face.

01:21.080 --> 01:23.400
 So, it seems like a great time to catch up.

01:23.400 --> 01:31.720
 Also, a lot of the listeners of this podcast will have heard Tomas Wolf from Hugging Face,

01:31.720 --> 01:32.720
 one of the founders.

01:32.720 --> 01:35.360
 Is that right?

01:35.360 --> 01:37.600
 How would you describe Tomas?

01:37.600 --> 01:43.040
 He's one of the three co-founders and he's our chief science officer.

01:43.040 --> 01:49.320
 So many on this in listening to us right now will have heard Tomas interviewed by Sam three

01:49.320 --> 01:50.900
 months ago.

01:50.900 --> 01:57.120
 And he had a lot to say about research and so it's a perfect time to dig deeper into

01:57.120 --> 02:01.960
 some of the things that he got into and also to just open up new territory, find out what's

02:01.960 --> 02:02.960
 on your mind.

02:02.960 --> 02:03.960
 How's that sound?

02:03.960 --> 02:04.960
 Yeah, for sure.

02:04.960 --> 02:09.040
 Yeah, I thought the podcast with Tom was really amazing.

02:09.040 --> 02:13.320
 So if people haven't listened to that, I highly recommend people listen to that too.

02:13.320 --> 02:18.800
 You and I spoke briefly a week or two back and I took some notes and I want to give you

02:18.800 --> 02:24.480
 and the listeners kind of the menu of things that came to mind for me that we could touch

02:24.480 --> 02:25.480
 on.

02:25.480 --> 02:26.760
 So big themes.

02:26.760 --> 02:32.320
 I would love to know more about you as a human and you and Hugging Face.

02:32.320 --> 02:37.200
 I think a lot of people probably have a name recognition for Hugging Face, but probably

02:37.200 --> 02:39.320
 don't know really what it is.

02:39.320 --> 02:42.120
 So it'd be good to dig into that a little bit.

02:42.120 --> 02:47.720
 And then the main dish of the course, let's dig into the future of NLP.

02:47.720 --> 02:53.960
 Yeah, one thing I'd like to emphasize is that Hugging Face is no longer an NLP company per

02:53.960 --> 02:54.960
 se.

02:54.960 --> 03:00.880
 So we are doing a lot of very interesting work in computer vision and speech and other

03:00.880 --> 03:02.360
 areas of AI.

03:02.360 --> 03:05.240
 So I like to think of Hugging Face as an AI company.

03:05.240 --> 03:06.240
 Yeah.

03:06.240 --> 03:07.240
 And so that's a perfect seg.

03:07.240 --> 03:08.240
 Let's dig into that.

03:08.240 --> 03:12.560
 So Hugging Face used to be an NLP company, I think it's safe to say, and it's really

03:12.560 --> 03:14.240
 been expanding.

03:14.240 --> 03:18.520
 I looked on Crunchbase just to see what the basic stats are these days.

03:18.520 --> 03:27.080
 It's like somewhere between 100 and 200 people, Series C and based in New York officially,

03:27.080 --> 03:31.480
 although quite remote now like the rest of us.

03:31.480 --> 03:33.120
 Yeah.

03:33.120 --> 03:38.520
 So when you joined, it was already transitioning into something bigger than NLP.

03:38.520 --> 03:39.520
 Yeah.

03:39.520 --> 03:41.560
 What was your perception of Hugging Face?

03:41.560 --> 03:45.800
 How would you have described it like before you joined and now that you've joined?

03:45.800 --> 03:46.800
 Yeah.

03:46.800 --> 03:53.480
 So I've always been impressed by Hugging Face and how it presents itself to the outside

03:53.480 --> 03:54.480
 world.

03:54.480 --> 04:01.000
 It's a very open and transparent organization where it really is about a community effort

04:01.000 --> 04:05.280
 to democratize a lot of the tools that everybody uses.

04:05.280 --> 04:10.360
 So from datasets to models, so Transformers library, of course, also the hub, which is

04:10.360 --> 04:15.880
 really a crucial part of the AI ecosystem these days, I think.

04:15.880 --> 04:19.080
 So I've just always been very impressed by it.

04:19.080 --> 04:23.680
 And so that's why I chose to join this company.

04:23.680 --> 04:28.840
 I think it really is a special place and it really plays a special role in the community.

04:28.840 --> 04:34.320
 So I don't think that a company like, I don't know, Google or Meta could play the same role

04:34.320 --> 04:36.520
 that Hugging Face plays in this ecosystem.

04:36.520 --> 04:38.000
 No, I agree.

04:38.000 --> 04:39.200
 I agree.

04:39.200 --> 04:41.080
 It's a pioneer with open source.

04:41.080 --> 04:42.080
 For sure.

04:42.080 --> 04:46.320
 So something else that I really like about Hugging Face is how European it is and now

04:46.320 --> 04:47.640
 actually very international.

04:47.640 --> 04:50.600
 The people are just, they come from all over the place.

04:50.600 --> 04:54.640
 Did you know any of the core Hugging Face people before you joined?

04:54.640 --> 04:55.640
 Yeah.

04:55.640 --> 05:01.360
 So I mean, I met Tom a few times before and I knew Victor and a bunch of others, Victor

05:01.360 --> 05:02.440
 Sen.

05:02.440 --> 05:06.000
 So it's funny actually that you mentioned the European-ness.

05:06.000 --> 05:08.440
 So I'm a European, as you can tell from my accent.

05:08.440 --> 05:13.720
 I'm originally from Holland, but I live in California and I spent some time in the UK

05:13.720 --> 05:15.960
 and in New York before I moved to California.

05:15.960 --> 05:21.360
 But Tom, my boss, actually lives in Utrecht in the Netherlands, which is where I studied

05:21.360 --> 05:23.320
 for my undergrad.

05:23.320 --> 05:25.720
 So Tom is not Dutch.

05:25.720 --> 05:29.040
 But you didn't cross paths during your years in the Netherlands.

05:29.040 --> 05:30.600
 No, not at all.

05:30.600 --> 05:32.840
 I left Holland more than 10 years ago.

05:32.840 --> 05:35.680
 So I don't think Tom's been living there for 10 years.

05:35.680 --> 05:36.800
 So it's not a Dutch mafia.

05:36.800 --> 05:37.880
 It's a coincidence.

05:37.880 --> 05:39.480
 It's a French mafia, if anything.

05:39.480 --> 05:44.360
 So the founders are French.

05:44.360 --> 05:49.600
 So in terms of the company at large, what I find fascinating is that we have people,

05:49.600 --> 05:52.320
 I think, in more than 25 countries all over the world.

05:52.320 --> 05:57.920
 So in the science team, we have people on the West Coast, on the East Coast of the US

05:57.920 --> 06:04.560
 and in Canada and lots of different places in Europe and in South Korea.

06:04.560 --> 06:05.560
 And Turkey as well.

06:05.560 --> 06:07.440
 I have a friend based in Istanbul.

06:07.440 --> 06:08.440
 Yeah.

06:08.440 --> 06:10.520
 Let's see.

06:10.520 --> 06:13.560
 What is your job?

06:13.560 --> 06:14.560
 Good question.

06:14.560 --> 06:17.560
 I wish I knew.

06:17.560 --> 06:27.840
 So broadly speaking, I'm just trying to help the team realize this very ambitious vision

06:27.840 --> 06:34.400
 that the founders have for the company and for the science team inside the company.

06:34.400 --> 06:38.360
 So yeah, it's not really a well-defined role.

06:38.360 --> 06:44.520
 I think it also kind of depends on what stage we're in a given research project, for example.

06:44.520 --> 06:47.360
 So I'm kind of discovering that as I go along.

06:47.360 --> 06:49.640
 So the official title is head of research.

06:49.640 --> 06:50.640
 That's right.

06:50.640 --> 06:53.440
 And so then comes the question, what is research at Hugging Face?

06:53.440 --> 06:58.320
 How is it different from research at a university or research at a big company like Facebook

06:58.320 --> 07:01.400
 slash Metta, which is where you came from before this?

07:01.400 --> 07:02.400
 Yeah.

07:02.400 --> 07:04.200
 So we're trying to go for a bit of a different model.

07:04.200 --> 07:09.280
 I think if you want to compare it to a single place, then maybe something like DeepMind or

07:09.280 --> 07:14.280
 OpenAI is closer to what we're trying to do than Metta.

07:14.280 --> 07:20.740
 So yeah, as you mentioned, I've been at FAIR for five years and it was a wonderful time.

07:20.740 --> 07:26.320
 But one of the things that was difficult at FAIR was that it's very bottom up, which in

07:26.320 --> 07:32.060
 theory sounds really nice, but it makes it very difficult to do very big ambitious projects.

07:32.060 --> 07:37.440
 So if you really want to create step change research artifacts, which is what we're trying

07:37.440 --> 07:42.240
 to do, then you need to pull together big groups of people and then make sure that they're

07:42.240 --> 07:45.720
 all aligned in realizing this vision.

07:45.720 --> 07:50.680
 And in a bottom up research organization, that's very difficult to do.

07:50.680 --> 07:55.440
 So what we're trying to do is find the optimal place between the bottom up approach that

07:55.440 --> 08:00.940
 FAIR and Google Brain and places like that have and the top down approach, which DeepMind

08:00.940 --> 08:07.520
 and OpenAI have, where they have a benevolent dictator like Demis or Ilya basically telling

08:07.520 --> 08:09.480
 people what to do and what the vision is.

08:09.480 --> 08:13.160
 And we're trying to occupy the middle ground a little bit there and really try to use the

08:13.160 --> 08:14.800
 things that make us special.

08:14.800 --> 08:20.520
 So that's the ability to move fast, the ability to work with the community, like we've been

08:20.520 --> 08:28.920
 doing with projects like Big Science, and to exploit the things that make us unique.

08:28.920 --> 08:34.160
 What's the difference between Big Science, which is a project involving lots of external

08:34.160 --> 08:39.920
 people, as many as a thousand are signed up from what I heard from Tomas, probably, you

08:39.920 --> 08:45.420
 know, more like hundreds that are active participants on a daily basis, but that's big.

08:45.420 --> 08:48.960
 And then the research team at Hugging Face.

08:48.960 --> 08:52.600
 Describe your actual team that what would you call the actual research team at Hugging

08:52.600 --> 08:53.600
 Face?

08:53.600 --> 08:56.340
 Is it like 10 people, 20?

08:56.340 --> 09:00.000
 So I think last count was 30, 35 people, actually.

09:00.000 --> 09:01.000
 Okay, big group.

09:01.000 --> 09:04.160
 Big Science is one of the projects we have going on.

09:04.160 --> 09:07.520
 So I can tell you a bit about the other projects we have going on.

09:07.520 --> 09:12.500
 So one of the advantages of being at Hugging Face is that it's a super transparent and

09:12.500 --> 09:13.500
 open company.

09:13.500 --> 09:20.000
 So I can just tell you everything that we're doing without feeling bad about it.

09:20.000 --> 09:21.720
 No secret sauce revealed.

09:21.720 --> 09:27.240
 Yeah, so we have a project around multimodal models.

09:27.240 --> 09:32.800
 So multimodality, I think everyone agrees, is very important for the future of AI.

09:32.800 --> 09:37.040
 And when you say multimodality, for those listening in, you're referring to more than

09:37.040 --> 09:42.580
 just text, more than just images, all kinds of sensory, what we would think of as sensory

09:42.580 --> 09:47.040
 modalities or information modalities for humans, you're trying to capture that for models,

09:47.040 --> 09:48.040
 but all at once.

09:48.040 --> 09:49.360
 Yeah, all at once.

09:49.360 --> 09:55.720
 So I think if you look at more recent multimodal work, it's very often just text and images.

09:55.720 --> 10:00.400
 But there are all kinds of different modalities that you all might want to integrate into

10:00.400 --> 10:01.720
 one single model.

10:01.720 --> 10:05.400
 So how many modalities are you stuffing in?

10:05.400 --> 10:12.320
 So right now it's images, text, videos, and audio, because those are the main ones.

10:12.320 --> 10:16.480
 And then once you have those, then you can start thinking about other specific modalities,

10:16.480 --> 10:18.260
 maybe sort of sub modalities, right?

10:18.260 --> 10:24.600
 So it's unclear whether code as a modality is a part of text or if it's something else.

10:24.600 --> 10:28.360
 So there's all kinds of interesting questions about what the modality really is.

10:28.360 --> 10:32.840
 So my PhD thesis actually was about grounding meaning in perceptual modalities, where I

10:32.840 --> 10:36.140
 also incorporated olfactory semantics.

10:36.140 --> 10:43.000
 So you can build a bag of chemical compounds model and build smell vectors, essentially,

10:43.000 --> 10:45.560
 and do interesting things with that.

10:45.560 --> 10:47.480
 So that's a long time ago.

10:47.480 --> 10:49.480
 But yeah, there's a lot of potential there.

10:49.480 --> 10:52.240
 What does the word grounded mean in this context?

10:52.240 --> 10:53.280
 So let's use NLP.

10:53.280 --> 10:57.760
 Let's use an example like you have a model that, you know, like GPT-3.

10:57.760 --> 10:59.960
 So it's learned how to generate text.

10:59.960 --> 11:01.840
 What does it mean for that model to be grounded?

11:01.840 --> 11:02.840
 Yeah.

11:02.840 --> 11:09.040
 So I was going to say, I think the word grounded isn't really well-defined, but I'm a philosopher

11:09.040 --> 11:13.460
 by training originally, so I would argue that most things are not well-defined.

11:13.460 --> 11:18.080
 But in my thesis, I make an explicit distinction between referential grounding and representational

11:18.080 --> 11:19.680
 grounding.

11:19.680 --> 11:25.440
 And so I think referential grounding is what people often think about with like referral

11:25.440 --> 11:26.440
 data sets.

11:26.440 --> 11:29.920
 So those exist in computer vision, for example, where you have to pick out the object.

11:29.920 --> 11:34.360
 So when someone says banana, then you have to be able to point in the image where the

11:34.360 --> 11:35.900
 banana is.

11:35.900 --> 11:39.280
 But I think the much more interesting type of grounding is representational grounding,

11:39.280 --> 11:45.400
 where you have a holistic meaning representation of a concept like elephant, or violin maybe

11:45.400 --> 11:46.400
 is a better example.

11:46.400 --> 11:49.280
 And so you know the semantic meaning of violin.

11:49.280 --> 11:54.240
 You can go to Wikipedia and look up what violin is, what that means, but you also have a visual

11:54.240 --> 11:58.000
 representation of it, and you know what it looks like, you know what it sounds like.

11:58.000 --> 12:01.480
 Maybe you know what it smells like, what it feels like, what it's like to play it.

12:01.480 --> 12:06.840
 All of these different modalities are a part of your overarching meaning representation

12:06.840 --> 12:08.480
 of the concept of violin.

12:08.480 --> 12:12.400
 And I think that is the much more interesting type of meaning representation.

12:12.400 --> 12:16.840
 And so that's the meaning we should try to get into machines if we want them to be able

12:16.840 --> 12:19.040
 to really understand humans.

12:19.040 --> 12:25.080
 So what are some of the problems you see with today's models that reveal that they're insufficiently

12:25.080 --> 12:26.080
 grounded?

12:26.080 --> 12:27.080
 Yeah.

12:27.080 --> 12:31.720
 So I don't know if we're sure that models are insufficiently grounded.

12:31.720 --> 12:34.040
 I think that's still an empirical question.

12:34.040 --> 12:37.480
 But my hunch, and I think a lot of people in the field share that hunch, is that you need

12:37.480 --> 12:42.360
 to have some understanding of the world as humans perceive it if you really want to understand

12:42.360 --> 12:44.320
 humans.

12:44.320 --> 12:52.040
 And so there's a lot of communication that happens between humans that never really becomes

12:52.040 --> 12:53.040
 explicit.

12:53.040 --> 12:54.680
 So people call this common sense, for example.

12:54.680 --> 12:59.320
 So the example I always use is coffee and what coffee smells like.

12:59.320 --> 13:00.720
 Everybody knows what coffee smells like.

13:00.720 --> 13:03.600
 So I never have to explain that to anyone.

13:03.600 --> 13:08.360
 So for that reason, I also just have no idea how to describe the smell of coffee.

13:08.360 --> 13:13.280
 I don't know if you can try that or describe the smell of a banana in one sentence.

13:13.280 --> 13:17.200
 You've never had to do that because you know that everybody knows what bananas smell like.

13:17.200 --> 13:20.000
 And if you could pull it off, we would call you a poet.

13:20.000 --> 13:21.600
 Yeah, exactly.

13:21.600 --> 13:22.920
 So I think you're totally right.

13:22.920 --> 13:27.640
 So you have to fall back to associations then because there is no descriptive language for

13:27.640 --> 13:28.840
 this sort of stuff.

13:28.840 --> 13:33.640
 And I think this happens all over the place in natural language communication between

13:33.640 --> 13:35.120
 humans.

13:35.120 --> 13:39.880
 And that makes it very hard for machines to learn this stuff just from reading Wikipedia

13:39.880 --> 13:42.040
 or whatever corpus they're trained on.

13:42.040 --> 13:43.040
 It's funny.

13:43.040 --> 13:45.240
 You're very much coming at this as a philosopher, I could see.

13:45.240 --> 13:47.440
 There's another angle, which is where I'm coming from.

13:47.440 --> 13:52.040
 So I'm at a company that is on the applied side.

13:52.040 --> 13:55.060
 So we're using NLP to try and solve problems.

13:55.060 --> 13:59.360
 And where I see what seems to be the grounded problem is the model clearly, if you just

13:59.360 --> 14:04.080
 poke a little bit, it clearly doesn't understand what it's talking about.

14:04.080 --> 14:05.920
 It'll say all the right things.

14:05.920 --> 14:11.600
 And then it reveals that it actually has no common sense understanding of what coffee

14:11.600 --> 14:15.320
 is because it'll say something that a human would find crazy.

14:15.320 --> 14:16.320
 Yeah.

14:16.320 --> 14:21.120
 But so I think the word understanding, what does understanding even mean there?

14:21.120 --> 14:23.840
 So I think what you're maybe talking about.

14:23.840 --> 14:27.040
 So I think there are two main things missing in our current paradigm.

14:27.040 --> 14:31.000
 One is multimodal understanding of concepts.

14:31.000 --> 14:34.640
 And the other is the intentionality with a T of language.

14:34.640 --> 14:40.280
 So the fact that we use language with an intent to change the mental state of whoever we're

14:40.280 --> 14:41.280
 talking to.

14:41.280 --> 14:46.080
 So I'm using my voice now to change your brain, essentially.

14:46.080 --> 14:50.640
 And so that intent is crucial for real meaning and real understanding.

14:50.640 --> 14:53.300
 And it's something that doesn't exist in language models.

14:53.300 --> 15:00.960
 Do you reckon that we have to give real agency to systems to achieve that, to have them care

15:00.960 --> 15:05.200
 about something, maybe with reinforcement learning or other paradigms?

15:05.200 --> 15:06.200
 I think so, yeah.

15:06.200 --> 15:13.040
 So I don't know if agency, I mean, I don't want to keep going on the definitions.

15:13.040 --> 15:15.340
 So agency is also a bit unclear, I think.

15:15.340 --> 15:24.360
 So you can model the intent of communication when you're trying to model human communication.

15:24.360 --> 15:29.400
 You can try to model the intent as a part of the interaction.

15:29.400 --> 15:33.040
 So the two things I just talked about, you could integrate them in language models pretty

15:33.040 --> 15:34.040
 easily.

15:34.040 --> 15:39.200
 So you could have a language model that also has a multimodal input.

15:39.200 --> 15:43.320
 Maybe you can put it in an embodied environment where it can walk around.

15:43.320 --> 15:48.720
 And then maybe you can have multiple of these language models walking around in that world

15:48.720 --> 15:51.520
 and interacting with each other and other humans.

15:51.520 --> 15:55.000
 So if you put all of that together, then I think you get something very close to how

15:55.000 --> 15:56.760
 humans learn language.

15:56.760 --> 15:59.420
 Is this where you think Hugging Face is headed?

15:59.420 --> 16:01.560
 Is this one of the grand directions?

16:01.560 --> 16:03.800
 This is definitely one of the grand directions.

16:03.800 --> 16:04.800
 Yeah.

16:04.800 --> 16:05.800
 So one of our projects is multimodality.

16:05.800 --> 16:08.560
 As I said, another one is about embodied learning.

16:08.560 --> 16:12.080
 Thomas also talked about this when he spoke on this podcast.

16:12.080 --> 16:17.040
 Yeah, the way he described it was maybe we need to teach models language more like we

16:17.040 --> 16:20.960
 teach humans language, which is in the world trying to get things done.

16:20.960 --> 16:21.960
 Exactly.

16:21.960 --> 16:22.960
 Yeah.

16:22.960 --> 16:27.600
 And that's because we want the models to use the kind of language that's useful for interacting

16:27.600 --> 16:29.280
 with humans.

16:29.280 --> 16:31.680
 So people gloss over this.

16:31.680 --> 16:36.840
 But the reason we want to have natural language understanding and natural language generation

16:36.840 --> 16:41.840
 capabilities in these models, because we want them to interact with humans.

16:41.840 --> 16:46.040
 And so I mean, one of the other things I've been pushing a lot for is a more holistic

16:46.040 --> 16:50.840
 evaluation of these models where rather than just evaluating them on static test sets,

16:50.840 --> 16:55.080
 we actually expose them to real humans and we see how well they do in that setting.

16:55.080 --> 17:00.640
 And as you mentioned, those models very quickly break down if you try to actually do that.

17:00.640 --> 17:01.640
 All right.

17:01.640 --> 17:03.200
 So different question.

17:03.200 --> 17:04.800
 I was really curious.

17:04.800 --> 17:07.520
 So I consider you a very multilingual person.

17:07.520 --> 17:09.000
 I mean, all Dutch people are.

17:09.000 --> 17:12.920
 If you've ever met a Dutch person, you've met multilingual people.

17:12.920 --> 17:17.960
 And here you are in NLP and adjacent.

17:17.960 --> 17:19.720
 You're definitely expanding beyond that.

17:19.720 --> 17:23.800
 But you would consider yourself an NLP practitioner, yeah?

17:23.800 --> 17:24.800
 I think so.

17:24.800 --> 17:25.800
 Yeah, kind of.

17:25.800 --> 17:31.600
 I mean, I've been branching out for a long time, so I would consider myself an AI person.

17:31.600 --> 17:34.680
 So a lot of my work is multimodal, but it's language first.

17:34.680 --> 17:38.760
 Yeah, language is my main interest.

17:38.760 --> 17:47.840
 How frustrating or bizarre has it felt to be a deeply multilingual person in a time in

17:47.840 --> 17:53.640
 science where it's just so English dominated, the research itself, the tools down to the

17:53.640 --> 17:55.800
 very data that we're training these things on.

17:55.800 --> 18:00.360
 And I'm asking this as an obvious segue to this really exciting project that's underway

18:00.360 --> 18:05.120
 to perhaps create the first truly multilingual based language model.

18:05.120 --> 18:06.760
 That's my understanding of the project.

18:06.760 --> 18:13.360
 But I first wanted to hear just like you, Dawa, as a deeply multilingual person, what

18:13.360 --> 18:14.360
 does it feel like?

18:14.360 --> 18:19.360
 What has it felt like to be in this weirdly accidentally English dominated space?

18:19.360 --> 18:23.520
 Yeah, so that's a very interesting question, but I don't know if I'm the right person to

18:23.520 --> 18:29.120
 ask it because I moved to the UK for my PhD and then I moved to the US.

18:29.120 --> 18:35.360
 And so most Dutch people speak pretty decent English, I think.

18:35.360 --> 18:42.600
 So I think where the accessibility of language models and the multilinguality of language

18:42.600 --> 18:48.460
 models, where that really matters is for people who are monolingual and who don't speak English.

18:48.460 --> 18:55.520
 So people who can't easily access this technology because it's limited only to English.

18:55.520 --> 18:59.240
 But I think that doesn't really apply to most Dutch people because they could very easily

18:59.240 --> 19:02.040
 switch over, as you mentioned.

19:02.040 --> 19:06.760
 But also using these things to make sense of the world that's not written in English.

19:06.760 --> 19:11.120
 I could tell you how hard it is, because that's my day to day, is dealing with Chinese, Russian,

19:11.120 --> 19:17.560
 or other languages, the tools and the data is far, far weaker.

19:17.560 --> 19:18.560
 Oh, yeah.

19:18.560 --> 19:19.720
 Yeah, for sure.

19:19.720 --> 19:26.120
 And I think there was also very interesting underlying questions there about the cultural

19:26.120 --> 19:29.320
 differences that manifest themselves in languages.

19:29.320 --> 19:33.500
 So English as a language is very explicit.

19:33.500 --> 19:37.760
 So you can be relatively low context in how you communicate.

19:37.760 --> 19:42.360
 So you're just very explicit or some people would consider Americans relatively blunt,

19:42.360 --> 19:44.240
 I think, in how they communicate.

19:44.240 --> 19:46.160
 Same for Dutch people, by the way.

19:46.160 --> 19:53.360
 But if you think about Japanese language, which is very sort of indirect and very different

19:53.360 --> 19:57.480
 in a sense from English, I think that also manifests itself in the culture.

19:57.480 --> 20:01.560
 So maybe there are just things that you can't really capture about Japanese culture because

20:01.560 --> 20:04.960
 you have a specific type of language model.

20:04.960 --> 20:09.240
 So tell us a bit about the ongoing experiment to make a truly multilingual model.

20:09.240 --> 20:11.760
 Yeah, so this is the big science model.

20:11.760 --> 20:12.880
 It has a name now.

20:12.880 --> 20:18.400
 It's called Bloom, which I think is a really nice name because the logo of big science

20:18.400 --> 20:21.000
 has also always been a flower.

20:21.000 --> 20:23.640
 So the flower is starting to bloom.

20:23.640 --> 20:30.440
 And so this language model, it's, as you said, the first big multilingual language model.

20:30.440 --> 20:35.720
 And it is only a few weeks away from being done training.

20:35.720 --> 20:37.480
 So it's been very cool.

20:37.480 --> 20:39.080
 You can just follow it on Twitter.

20:39.080 --> 20:47.560
 There's a regular Twitter update where it's like we're at like 87% or something now.

20:47.560 --> 20:49.040
 Have you been playing with checkpoints?

20:49.040 --> 20:50.040
 Yeah.

20:50.040 --> 20:54.760
 So there's something called the Bloom Book, where people have been able to just submit

20:54.760 --> 21:01.560
 prompts and then someone would run them and sort of output somewhere for people to inspect.

21:01.560 --> 21:05.000
 So we're releasing some checkpoints soon as well for people to talk to.

21:05.000 --> 21:09.080
 And then when the final model comes out, it's also going to be released so that people can

21:09.080 --> 21:10.680
 play with it themselves.

21:10.680 --> 21:11.680
 Cool.

21:11.680 --> 21:14.240
 Is it a basic text-to-text autoregressive model?

21:14.240 --> 21:17.600
 Same architecture as your typical big text-to-text models?

21:17.600 --> 21:19.080
 Yeah, basically.

21:19.080 --> 21:25.920
 So I think by design that there hasn't been too much divergence from the sort of standard

21:25.920 --> 21:28.880
 language model that people are used to.

21:28.880 --> 21:30.880
 But there are some nifty new things in there.

21:30.880 --> 21:37.640
 So it uses like L of I for like how to do the token embeddings and things like that.

21:37.640 --> 21:40.560
 So there are a couple of nice different things in there.

21:40.560 --> 21:43.440
 But yeah, the main architecture is exactly what you would expect.

21:43.440 --> 21:45.280
 Wait, let's dig into that.

21:45.280 --> 21:49.280
 A lot of people on this call won't really even know what a token or a tokenizer is.

21:49.280 --> 21:51.240
 I think this is a really neat part of NLP.

21:51.240 --> 21:56.640
 This is very much a like the tools you use kind of talk, but let's just like take a moment.

21:56.640 --> 22:01.720
 Tell us what is a token, what is a tokenizer, and then like how did you do it differently

22:01.720 --> 22:04.920
 with this big Bloom model?

22:04.920 --> 22:06.120
 And why did you have to?

22:06.120 --> 22:12.320
 Yeah, so I'm not the right person to really answer detailed questions about the tokenization

22:12.320 --> 22:15.160
 of the language model.

22:15.160 --> 22:17.960
 Yeah, so I can explain what tokenization is.

22:17.960 --> 22:21.240
 So it's basically just how do you cut up your text?

22:21.240 --> 22:27.880
 So you know, a sentence consists of words, so you could just cut it up in the white space

22:27.880 --> 22:34.180
 and just every word is a token, but that is inefficient.

22:34.180 --> 22:38.320
 So what people have been doing is trying to chunk it up in smarter ways.

22:38.320 --> 22:41.420
 Because then you'd have like a vocabulary of millions, right?

22:41.420 --> 22:43.880
 And with multiple languages, it could be huge.

22:43.880 --> 22:48.480
 Yeah, so especially if it's multilingual, maybe you just don't see words often enough

22:48.480 --> 22:54.720
 to really have a very good understanding of their meaning, or a good representation of

22:54.720 --> 22:55.720
 their meaning.

22:55.720 --> 23:01.040
 And so what you can do is you can chunk different segments of words together in smart ways.

23:01.040 --> 23:05.440
 So this is BPE, bi-pair encoding, and things like that.

23:05.440 --> 23:10.720
 And so there has been a working group in the big science workshop.

23:10.720 --> 23:13.960
 So it's like a one-year workshop is how we're thinking about it.

23:13.960 --> 23:17.420
 So I think there are 40, 50 different working groups.

23:17.420 --> 23:20.600
 And there was one working group working on tokenization.

23:20.600 --> 23:22.200
 They wrote a very nice survey paper.

23:22.200 --> 23:25.600
 They did a big analysis of what the right tokenization is.

23:25.600 --> 23:31.420
 And one of the things that they found, I think, also together with the main model working

23:31.420 --> 23:36.380
 group, is that these alibi positional embeddings really help.

23:36.380 --> 23:39.560
 So this was just an empirical finding.

23:39.560 --> 23:46.080
 And so there's just a lot of this small research that went into this whole endeavor.

23:46.080 --> 23:49.220
 So why not just go all the way down to the individual character?

23:49.220 --> 23:50.640
 Why mess with tokens at all?

23:50.640 --> 23:52.640
 Yeah, it's a good question.

23:52.640 --> 23:56.920
 There have been efforts in this direction.

23:56.920 --> 24:02.520
 So back in the days, there were character RNNs before transformers, and people were

24:02.520 --> 24:04.000
 trying to get this to work.

24:04.000 --> 24:06.840
 It sort of worked, but it didn't really, really work.

24:06.840 --> 24:09.760
 It was a great way to generate made up silly words.

24:09.760 --> 24:12.480
 Yeah, yeah, for sure.

24:12.480 --> 24:17.160
 So yeah, I think there's also an interesting possibility there where we reduce everything

24:17.160 --> 24:19.320
 to the byte level.

24:19.320 --> 24:25.680
 So when you think about Unicode or UTF-8 or things like that, in theory, every single

24:25.680 --> 24:28.880
 character can just be modeled at the byte level.

24:28.880 --> 24:30.280
 And then maybe that's the future.

24:30.280 --> 24:36.440
 And then maybe you could even put images and audio and everything is just bytes.

24:36.440 --> 24:41.560
 And so basically, you can just have a pre-trained byte level model.

24:41.560 --> 24:44.000
 So I think that's an interesting research direction.

24:44.000 --> 24:45.200
 And there's been some work on that.

24:45.200 --> 24:50.240
 But so far, it hasn't really proven to be better than just smart ways of tokenizing

24:50.240 --> 24:51.240
 your data.

24:51.240 --> 24:57.240
 So maybe the real explanation for it not working yet is that we haven't used enough data yet.

24:57.240 --> 25:01.280
 So maybe we just need even more data, as always.

25:01.280 --> 25:03.440
 Thomas mentioned 800 gigabytes.

25:03.440 --> 25:08.320
 What does that actually translate to in terms of how much of the internet did you grab for

25:08.320 --> 25:09.320
 this?

25:09.320 --> 25:10.640
 I understand you crowdsourced it.

25:10.640 --> 25:11.640
 Yeah.

25:11.640 --> 25:18.120
 So it was crowdsourced with a big community of collaborators who were part of this big

25:18.120 --> 25:20.360
 science effort.

25:20.360 --> 25:22.580
 And so it's not really a crawl.

25:22.580 --> 25:26.320
 So it's very hard to say what percentage of the internet is this.

25:26.320 --> 25:31.320
 It really depended on the language and the folks who contributed the data for their own

25:31.320 --> 25:32.320
 language.

25:32.320 --> 25:35.200
 I think some of them also had different approaches.

25:35.200 --> 25:41.240
 So it's a very targeted way of collecting data.

25:41.240 --> 25:43.480
 And that's one of the beauties of this big science effort.

25:43.480 --> 25:47.100
 So I think there's a lot of emphasis on this blue model.

25:47.100 --> 25:51.720
 But what's also very interesting about the overarching endeavor is that we have this

25:51.720 --> 25:58.400
 data set, which is really beautiful, hand curated by experts in those languages.

25:58.400 --> 26:05.280
 It has a very interesting coverage of different languages geographically over the whole world.

26:05.280 --> 26:07.040
 I don't know what the latest number is.

26:07.040 --> 26:10.000
 No, in the 40s or 50s, I think 45.

26:10.000 --> 26:11.000
 Wow.

26:11.000 --> 26:13.120
 So this is a huge collection of languages.

26:13.120 --> 26:17.240
 And it includes low resource African languages and things like that.

26:17.240 --> 26:18.880
 So I think that's really great.

26:18.880 --> 26:24.400
 So there's the data effort, but then the legal side of this, how do you distribute the model,

26:24.400 --> 26:26.800
 the governance side of the data itself.

26:26.800 --> 26:31.560
 All of these super intriguing questions have just been explored by the community completely

26:31.560 --> 26:33.120
 in the open.

26:33.120 --> 26:34.920
 So it's just fascinating for me.

26:34.920 --> 26:37.720
 I'm sort of an outsider, right?

26:37.720 --> 26:39.920
 So I mean, me too, in a way.

26:39.920 --> 26:44.480
 This started about a year ago, I think, or more than that.

26:44.480 --> 26:50.640
 And so I've just been following it from the sidelines and I'm still not directly involved

26:50.640 --> 26:51.640
 in it that much.

26:51.640 --> 26:54.080
 And it's just amazing to see.

26:54.080 --> 26:57.800
 Okay, now back to you.

26:57.800 --> 27:02.200
 Here you are six months into your new role at Hugging Face.

27:02.200 --> 27:08.200
 Give us a sense of what you thought your job would be when you started.

27:08.200 --> 27:11.320
 And now six months later, what has changed?

27:11.320 --> 27:17.320
 What's the newest thing that you've learned about yourself and Hugging Face and the mission?

27:17.320 --> 27:19.560
 What gets you out of bed in the morning that's changed?

27:19.560 --> 27:22.960
 Yeah, that's a very interesting question.

27:22.960 --> 27:30.240
 I mean, I think the job has been what I expected sort of.

27:30.240 --> 27:35.880
 So I knew going into this that it's just an amazing team and we really have some brilliant

27:35.880 --> 27:38.240
 researchers in this team.

27:38.240 --> 27:41.880
 So I was very excited about getting to work with those folks.

27:41.880 --> 27:44.820
 And so that's been really awesome.

27:44.820 --> 27:51.860
 I think one thing that I maybe didn't really expect is when you're accompanied like Hugging

27:51.860 --> 27:57.800
 Face and you're distributed all across the globe, you have to be very decentralized.

27:57.800 --> 28:04.240
 So a lot of the communication happens asynchronously on Slack in public channels, which I think

28:04.240 --> 28:05.240
 is great.

28:05.240 --> 28:10.440
 And so Hugging Face really has a unique culture that supports this way of working together.

28:10.440 --> 28:16.800
 But if you come from a different working culture, like me coming from Metta, that is quite the

28:16.800 --> 28:18.760
 transition to make.

28:18.760 --> 28:22.400
 And so you can't just go to a whiteboard with people.

28:22.400 --> 28:23.400
 Yeah.

28:23.400 --> 28:27.240
 So everything is remote, but it's not even just remote where you're both like talking

28:27.240 --> 28:31.280
 to your computer over Zoom.

28:31.280 --> 28:33.000
 It's remote also in time.

28:33.000 --> 28:36.760
 So one of the things I'm struggling with is just time zone, I think.

28:36.760 --> 28:38.760
 So I'm in California, right?

28:38.760 --> 28:41.840
 So I'm sort of trailing the world.

28:41.840 --> 28:48.680
 And so when I wake up or when my son wakes me up at around 7 a.m., then I check my phone

28:48.680 --> 28:53.000
 and I have like a million Slack messages and emails and things to read through.

28:53.000 --> 28:57.720
 And then usually my meetings start at 8 a.m. because I need to make sure I can talk to

28:57.720 --> 28:58.960
 the Europeans.

28:58.960 --> 29:02.400
 And then they stop working soon after that.

29:02.400 --> 29:06.760
 And so I'm always kind of trailing in time, which is not easy.

29:06.760 --> 29:08.720
 So you didn't see that coming?

29:08.720 --> 29:10.200
 I was not prepared for that, yeah.

29:10.200 --> 29:11.840
 I'm still adjusting.

29:11.840 --> 29:14.760
 But I mean, it's an interesting learning experience.

29:14.760 --> 29:18.280
 And it's just fascinating, I think, to see like where the world is going with remote

29:18.280 --> 29:19.280
 work.

29:19.280 --> 29:24.800
 And so this is the future way I think in which a lot of companies are going to be doing this.

29:24.800 --> 29:29.760
 So what's it like running and building and nurturing a research team at a startup?

29:29.760 --> 29:32.400
 I think that's something that people will be really curious about.

29:32.400 --> 29:37.680
 I think a lot of people will be familiar directly or indirectly with how a research group, even

29:37.680 --> 29:41.040
 30 strong, like you said, at a university works.

29:41.040 --> 29:45.320
 You've got a PI and that PI's job is mostly to get grant money.

29:45.320 --> 29:47.600
 And then you've got the postdocs who actually run the show.

29:47.600 --> 29:52.120
 And then you've got like grad students who are ranging from miserable to pretty happy.

29:52.120 --> 29:55.280
 And then you've got like interns and undergrads.

29:55.280 --> 29:58.560
 Does it have anything like that structure or is it just a totally different beast?

29:58.560 --> 30:00.520
 Yeah, it's very different.

30:00.520 --> 30:03.640
 So I am definitely not a PI.

30:03.640 --> 30:08.920
 So I'm more a facilitator, I think, or a coordinator.

30:08.920 --> 30:13.000
 So we have a very flat, non-hierarchical organization.

30:13.000 --> 30:14.080
 We do have team leads.

30:14.080 --> 30:16.880
 So those would be closer to PIs, I think.

30:16.880 --> 30:23.040
 So we have a multimodal project and it has very clear team leads and things like that.

30:23.040 --> 30:30.440
 So my role is more like a sort of serving leader where I just try to help people the

30:30.440 --> 30:35.160
 best way I possibly can and to make sure they don't have roadblocks and that people are

30:35.160 --> 30:38.080
 talking to each other and that I'm aware of what's going on.

30:38.080 --> 30:42.780
 And I try to connect people to the right people and connect ideas to the right ideas.

30:42.780 --> 30:45.840
 So it sounds like pretty normal management, actually.

30:45.840 --> 30:49.640
 Yeah, but yeah, I guess you could say that.

30:49.640 --> 30:52.920
 But it's very different from normal management at the same time, I think, because of how

30:52.920 --> 30:56.960
 decentralized the company is and because of all of the other things that are just very

30:56.960 --> 31:01.000
 different from a traditional management role at a big tech company.

31:01.000 --> 31:04.800
 Well, and also the fact that there's like a thousand strong group of people outside

31:04.800 --> 31:07.600
 the company that you actually have to work with and coordinate with.

31:07.600 --> 31:10.240
 Yeah, so that's just a big science project.

31:10.240 --> 31:16.220
 So I think you make an interesting point that one of the things that makes Hugging Face

31:16.220 --> 31:20.800
 so special is that the community plays such a big role in the company.

31:20.800 --> 31:22.720
 And that's not just big science.

31:22.720 --> 31:27.840
 So like if you look at Transformers, the library and the open source ecosystem and data sets

31:27.840 --> 31:31.040
 and things like that, that's a huge community.

31:31.040 --> 31:35.440
 And all of these people are also contributing actively to making these tools so awesome.

31:35.440 --> 31:41.680
 Yeah, no, I remember the day we first started using your Transformers library at my company

31:41.680 --> 31:42.680
 Primer.

31:42.680 --> 31:43.680
 It was a revelation.

31:43.680 --> 31:53.880
 So just like, I can't say enough about how positive the open sourcing of Transformer

31:53.880 --> 31:55.520
 language models was.

31:55.520 --> 32:00.360
 And I think Hugging Face deserves most of the credit, just like, yeah.

32:00.360 --> 32:07.200
 I think one of the reasons that BERT became so popular so quickly was because of the Transformers

32:07.200 --> 32:08.800
 library or the predecessor, right?

32:08.800 --> 32:10.400
 So PyTorch pre-trained BERT.

32:10.400 --> 32:15.040
 I remember I was at a workshop at the Santa Fe Institute.

32:15.040 --> 32:18.120
 They do these workshops where they invite a bunch of people and they talk about some

32:18.120 --> 32:19.120
 stuff.

32:19.120 --> 32:25.160
 And Fernando Pereira was there, the Google director of research, I think.

32:25.160 --> 32:30.240
 And he was saying like, we have this thing coming out and it's going to blow everything

32:30.240 --> 32:31.240
 out of the water.

32:31.240 --> 32:32.240
 It's amazing.

32:32.240 --> 32:33.240
 It's going to revolutionize NLP.

32:33.240 --> 32:37.200
 And I've heard people say that before and I never really believed it.

32:37.200 --> 32:39.120
 But in this case, he was right.

32:39.120 --> 32:40.840
 So BERT, yeah.

32:40.840 --> 32:43.800
 So it dropped like, I think, two weeks later or something.

32:43.800 --> 32:45.920
 And then so everyone wanted to play with it.

32:45.920 --> 32:49.280
 And being fair, obviously PyTorch was the preferred framework.

32:49.280 --> 32:53.840
 And it took like, I don't know, like a week or two before there was this PyTorch pre-trained

32:53.840 --> 32:55.560
 BERT model that everyone was playing with.

32:55.560 --> 32:58.120
 So it's amazing.

32:58.120 --> 33:00.080
 And so I did some snooping.

33:00.080 --> 33:05.560
 Your most cited paper, at least according to Google Scholar, is this 2017 paper on sentence

33:05.560 --> 33:06.560
 representations.

33:06.560 --> 33:14.480
 And I think that's so notable is that that's like just on the before side of BERT.

33:14.480 --> 33:18.280
 So you know, BERT comes out in October 2018, something like that.

33:18.280 --> 33:24.520
 And so like, well, a full year before that, you were deep in NLP, solving hard NLP problems.

33:24.520 --> 33:29.800
 Do you remember how crazy it was when suddenly, like on the other side of that line, when

33:29.800 --> 33:35.040
 we had language models, all the things in NLP that were really hard and tedious and

33:35.040 --> 33:40.380
 you needed so much data to even barely get some performance suddenly became kind of routine

33:40.380 --> 33:43.160
 and fun and easy.

33:43.160 --> 33:48.800
 Like I'm not hiding the reality that like tons of stuff doesn't work and tons of stuff

33:48.800 --> 33:49.800
 is still hard.

33:49.800 --> 33:53.520
 But the things that are hard are new things, largely.

33:53.520 --> 33:54.520
 Yeah.

33:54.520 --> 33:57.320
 So I agree with that.

33:57.320 --> 34:01.540
 So to me as a researcher, it didn't feel like a very abrupt transition, actually.

34:01.540 --> 34:06.560
 So I think that was much more the case for NLP practitioners and more applied people

34:06.560 --> 34:08.120
 trying to use the tools.

34:08.120 --> 34:09.120
 Yeah.

34:09.120 --> 34:14.400
 So for me as a researcher, I think like the transition was actually very natural.

34:14.400 --> 34:18.160
 So we were doing things with LSTMs and then, okay, transformers.

34:18.160 --> 34:19.660
 So LSTMs didn't really work.

34:19.660 --> 34:21.240
 So you needed attention.

34:21.240 --> 34:26.640
 And so even in InfoSend, we were also experimenting with self-attention and things like that.

34:26.640 --> 34:30.100
 And then what the Transformers paper did is it basically removed the recurrence.

34:30.100 --> 34:36.000
 So rather than having an LSTM with it forward, just a normal MLP feed-forward network.

34:36.000 --> 34:40.580
 And so it turned out that attention on its own is actually okay.

34:40.580 --> 34:47.000
 So from that, it became natural to try to do this on just language modeling tasks.

34:47.000 --> 34:48.360
 So there's GPT.

34:48.360 --> 34:51.360
 And then if you can do language modeling, why not do it bidirectionally?

34:51.360 --> 34:53.960
 Because we were playing with bidirectional LSTMs all the time.

34:53.960 --> 34:56.120
 InfoSend is a bidirectional LSTM.

34:56.120 --> 34:59.800
 So BERT is just a bidirectional GPT.

34:59.800 --> 35:04.360
 So it all was very natural, I think, when it came up.

35:04.360 --> 35:10.000
 So it felt naturally from the point of view of understanding the science, but I can tell

35:10.000 --> 35:14.440
 you from the point of view of people trying to solve problems that people will pay you

35:14.440 --> 35:15.440
 money for.

35:15.440 --> 35:16.440
 Oh, yeah.

35:16.440 --> 35:17.440
 For sure.

35:17.440 --> 35:18.440
 No, no.

35:18.440 --> 35:19.440
 It changed everything.

35:19.440 --> 35:23.680
 There is an aspect, though, scientifically that is new, right?

35:23.680 --> 35:28.720
 I was delighted when this little cottage industry of BERTology suddenly kind of sprouted out

35:28.720 --> 35:29.880
 of nowhere.

35:29.880 --> 35:36.240
 So here's the thing, it struck me that deep learning used to be very much like a branch

35:36.240 --> 35:37.240
 of mathematics, right?

35:37.240 --> 35:42.040
 Because it was part of statistics, so all of ML was just math.

35:42.040 --> 35:44.020
 And it felt like the math world.

35:44.020 --> 35:48.400
 And then suddenly, here we are today with models that are so complicated, they're more

35:48.400 --> 35:50.520
 like biology artifacts.

35:50.520 --> 35:55.520
 We're kind of prodding them and probing them and trying to understand things like how the

35:55.520 --> 36:00.240
 heck does BERT understand grammar?

36:00.240 --> 36:01.240
 To what extent?

36:01.240 --> 36:02.640
 Does it do it differently than us?

36:02.640 --> 36:07.440
 Suddenly it's feeling more like an empirical science and less like a branch of math.

36:07.440 --> 36:10.840
 Yeah, I'm not sure I'm happy with that, actually.

36:10.840 --> 36:13.400
 I also think that this...

36:13.400 --> 36:15.600
 So yeah, I have a couple of things to say about that, actually.

36:15.600 --> 36:21.640
 So I think this cottage industry of BERTology is interesting because a few years before

36:21.640 --> 36:26.280
 that, we had a cottage industry in Word2vec.

36:26.280 --> 36:28.320
 So Word2vec kind of blew everyone away.

36:28.320 --> 36:34.800
 And then there were a couple of ACLs and EMLPs where just everything was something to VEC.

36:34.800 --> 36:38.440
 And it was all just trying to analyze what Word2vec really did.

36:38.440 --> 36:42.760
 And so I think that's just kind of the progression of science where you have a big breakthrough

36:42.760 --> 36:48.320
 model and then there's some consolidation in the sort of Thomas Kuhn paradigm shift.

36:48.320 --> 36:52.040
 So there's a real paradigm shifting artifact like Word2vec or BERT.

36:52.040 --> 36:56.280
 And then there's a lot of consolidation where people try to understand this better.

36:56.280 --> 37:00.780
 So I think that's just the natural progression and that's just going to continue happening.

37:00.780 --> 37:08.200
 But about BERT specifically, so we just don't have the correct mathematical tools, I think,

37:08.200 --> 37:10.960
 to really understand what it's learning.

37:10.960 --> 37:15.080
 So there are some efforts now from Chris Ola and trying to understand better what transformers

37:15.080 --> 37:17.620
 are really learning.

37:17.620 --> 37:21.760
 So we have a very interesting paper called Mass Language Modeling and the Distributional

37:21.760 --> 37:27.080
 Hypothesis Order Word Does Not Matter Much or something like that.

37:27.080 --> 37:32.760
 So what we basically show is that if you shuffle a corpus, and so all of the sentences are

37:32.760 --> 37:38.240
 not in the right order anymore, and you train a BERT model on it, it just does just as well

37:38.240 --> 37:39.560
 as a regular BERT.

37:39.560 --> 37:42.080
 So you mentioned like, does BERT learn grammar?

37:42.080 --> 37:48.600
 Which weirdly seems to suggest that it doesn't matter, that it's clearly doing something

37:48.600 --> 37:49.720
 differently than humans do.

37:49.720 --> 37:54.600
 Because if you imagine trying to learn language with shuffled language, it'd be a nightmare.

37:54.600 --> 37:55.600
 Exactly.

37:55.600 --> 37:56.600
 Yeah.

37:56.600 --> 38:06.040
 So I think maybe we're also thinking that BERT is better than it really is or these

38:06.040 --> 38:09.040
 sorts of models, that they're actually better than they really are.

38:09.040 --> 38:14.920
 And I think like when you think about GPT-3 and how much of a splash that made, there's

38:14.920 --> 38:20.920
 also this element that people just have a natural tendency to anthropomorphize everything.

38:20.920 --> 38:25.960
 Like you do this to like your robot vacuum cleaner and thinks that you give it a name,

38:25.960 --> 38:26.960
 right?

38:26.960 --> 38:32.120
 So that in the words of Daniel Dennett, the philosopher, you're ascribing intentionality.

38:32.120 --> 38:38.020
 And so I think we do that all the time to everything, and we do it especially to things

38:38.020 --> 38:39.280
 that produce language.

38:39.280 --> 38:45.120
 Because language is essentially the only thing we know that is really, really human only.

38:45.120 --> 38:49.720
 And so when something produces language, we just go, oh, there has to be something brilliant

38:49.720 --> 38:50.720
 behind that.

38:50.720 --> 38:55.140
 But very often it's just higher order distributional statistics.

38:55.140 --> 38:57.800
 It's just clever Hans.

38:57.800 --> 39:02.380
 That's basically, we create these benchmark tests and we watch the performance on these

39:02.380 --> 39:08.120
 tests going up, up, up, up, and we attribute a model like GPT-3 on these language tests

39:08.120 --> 39:09.800
 as getting truly more clever.

39:09.800 --> 39:14.000
 It truly has a deeper quote unquote understanding of the task at hand.

39:14.000 --> 39:17.200
 But then you do these clever experiments like the one you described with scrambling and

39:17.200 --> 39:21.440
 it reveals, well, surely actually it's just using distributional tricks.

39:21.440 --> 39:22.440
 Yeah.

39:22.440 --> 39:23.440
 So I don't know.

39:23.440 --> 39:25.840
 I think the jury is still, I wouldn't put it that strongly.

39:25.840 --> 39:27.120
 I think the jury is still out.

39:27.120 --> 39:33.200
 So I definitely think that there is an evaluation crisis in NLP.

39:33.200 --> 39:38.760
 And I've been doing a lot of work with lots of folks in trying to improve that through

39:38.760 --> 39:45.240
 things like Dynabench where we do a different, we try to rethink benchmarking essentially.

39:45.240 --> 39:51.000
 But it's undeniable that progress in NLP has just been insane.

39:51.000 --> 39:58.000
 So if you look at like what GPT-3 can do compared to GPT-1, or what we can do now with Dali

39:58.000 --> 40:04.120
 2 compared to, I don't know, the earlier text to image synthesis models, it's just crazy

40:04.120 --> 40:05.520
 how fast we're, yeah.

40:05.520 --> 40:10.920
 So the progress is real, but we should be careful to not kind of over interpret what

40:10.920 --> 40:11.920
 we're seeing.

40:11.920 --> 40:13.960
 So there's still a lot of stuff that...

40:13.960 --> 40:22.840
 So there's a headline going around just this week about a researcher from Google essentially

40:22.840 --> 40:27.720
 attributing sentience to the Lambda language model.

40:27.720 --> 40:32.480
 And I think that's really to your point, like what we're talking about, it's these things

40:32.480 --> 40:35.640
 actually know how to work with language.

40:35.640 --> 40:37.960
 And we humans are language machines.

40:37.960 --> 40:43.940
 We're like completely geared towards understanding and transmitting, receiving and sending information

40:43.940 --> 40:45.320
 with language.

40:45.320 --> 40:51.320
 That is the most human information, intentionality and understand the world around us, getting

40:51.320 --> 40:52.740
 stuff done.

40:52.740 --> 40:58.800
 And so when some mathematical object is doing it, I feel it too, I can't help it.

40:58.800 --> 41:03.240
 Have you ever kind of like had that feeling that you just like have to push away of like,

41:03.240 --> 41:05.280
 man, I'm talking to this thing.

41:05.280 --> 41:07.000
 But what do you mean by a mathematical object?

41:07.000 --> 41:10.680
 I mean, I think you can argue that your brain is also a mathematical object, or at least

41:10.680 --> 41:13.400
 you can write your brain as a very complicated...

41:13.400 --> 41:14.400
 You took the bait.

41:14.400 --> 41:16.600
 I was hoping you'd shake the bait.

41:16.600 --> 41:21.560
 This is like philosopher catnip.

41:21.560 --> 41:28.080
 So I think it's very interesting because there's a very nice theory of consciousness that says

41:28.080 --> 41:32.840
 that we take the intentional stance towards ourselves as rational agents.

41:32.840 --> 41:38.560
 And that's what consciousness is, this strange loop as Douglas Hofstadter calls it.

41:38.560 --> 41:44.480
 So yeah, maybe we're evolutionarily hardwired to take an intentional stance towards things.

41:44.480 --> 41:48.160
 And that's why we're so confused by the NOP progress we've been seeing.

41:48.160 --> 41:55.000
 Does it also hint at a way to achieve artificial intelligence of a more AGI flavor?

41:55.000 --> 42:00.280
 So yeah, again, like I don't really know what AGI even means.

42:00.280 --> 42:04.680
 And I think it's very premature to start thinking about it.

42:04.680 --> 42:10.480
 So we have a philosopher slash ethicist who joined Hugging Face recently, Jada Pastilli.

42:10.480 --> 42:17.460
 So she has made this point on Twitter too, I think, where there are real problems right

42:17.460 --> 42:19.940
 now with the deployment of AI.

42:19.940 --> 42:25.320
 And so when we're thinking about the applied ethics of these systems, there are just real

42:25.320 --> 42:27.480
 things we need to fix right now.

42:27.480 --> 42:33.000
 And they are much more salient and much more important right now than thinking about AGI

42:33.000 --> 42:35.440
 and paperclip maximizers and things like that.

42:35.440 --> 42:36.640
 I agree.

42:36.640 --> 42:37.640
 I agree completely.

42:37.640 --> 42:42.120
 There's just, I think actually the biggest problem to solve from an ethics point of view

42:42.120 --> 42:48.040
 is these systems not working that well on narrow tasks and people over trusting them.

42:48.040 --> 42:50.960
 That's where a lot of harm can come from, not from bias even.

42:50.960 --> 42:55.560
 That's another level of problem, just like over trusting systems, misunderstanding their

42:55.560 --> 42:56.560
 limits.

42:56.560 --> 42:57.560
 Yeah, true.

42:57.560 --> 42:59.920
 But so there's a trade off here too, right?

42:59.920 --> 43:03.980
 So we shouldn't hype them up too much because people will just misunderstand what these

43:03.980 --> 43:07.880
 systems are capable of, but we also shouldn't underhype them too much.

43:07.880 --> 43:12.880
 So Sam Bowman, professor at NYU, has a very nice paper where he talks about the dangers

43:12.880 --> 43:13.880
 of underhyping.

43:13.880 --> 43:18.160
 So if we all just pretend that there is no progress at all, then at some point we are

43:18.160 --> 43:24.160
 going to be very surprised when AGI suddenly emerges and we have sentient AI.

43:24.160 --> 43:30.280
 So we definitely should think about this stuff and so AI alignment is a very active research

43:30.280 --> 43:35.200
 area and it's a very important research area, but it's all about finding that balance, I

43:35.200 --> 43:36.200
 think.

43:36.200 --> 43:37.200
 Sure.

43:37.200 --> 43:38.200
 Okay.

43:38.200 --> 43:39.200
 Lightning round.

43:39.200 --> 43:45.240
 Most exciting things on the horizon for research in NLP that you're either working towards

43:45.240 --> 43:50.240
 now or you'd like to go a little bit further than what you're just about to ship, just

43:50.240 --> 43:51.240
 about to publish.

43:51.240 --> 43:56.080
 Yeah, so I'm very excited in multimodality, obviously.

43:56.080 --> 44:01.360
 I think that there's a lot of interesting work coming out in semi-parametric models

44:01.360 --> 44:06.600
 where you have a retriever component and some sort of lightweight reader model on top of

44:06.600 --> 44:07.600
 that retriever.

44:07.600 --> 44:12.520
 So there was a paper from DeepMind that came out a couple of days ago.

44:12.520 --> 44:18.800
 The idea in a nutshell there is that these language models are basically frozen in time

44:18.800 --> 44:23.680
 based on the data you give them and so we need some way to help them keep refreshing

44:23.680 --> 44:25.080
 what they understand about the world.

44:25.080 --> 44:26.080
 Oh, yeah.

44:26.080 --> 44:27.080
 That's just one application.

44:27.080 --> 44:34.080
 I think it's much more about how you learn different things.

44:34.080 --> 44:36.600
 So as humans, we have different kinds of memory.

44:36.600 --> 44:42.840
 We have a semantic memory and an episodic memory and so we also have a library and the

44:42.840 --> 44:44.680
 internet where we can look up stuff, right?

44:44.680 --> 44:49.460
 So we don't have to store all of it in our parameters, our brain.

44:49.460 --> 44:54.800
 So I think if you do this with models too where you have a big index where you can invest

44:54.800 --> 44:59.720
 a lot of heavy compute and having a very high quality index, then you can have lots of lighter

44:59.720 --> 45:01.480
 weight reader models on top of this.

45:01.480 --> 45:06.760
 So this is also going to have lots of repercussions, I think, for industry where if you're a company

45:06.760 --> 45:11.680
 like Facebook, you want to have a million classifiers from all of these different teams

45:11.680 --> 45:14.280
 all trying to do cool stuff with their classifier.

45:14.280 --> 45:19.600
 If they have a big index that they can rely on, then you kind of do the compute in a much

45:19.600 --> 45:21.480
 more intelligent way, I think.

45:21.480 --> 45:27.320
 So it's about finding the mix between the retriever, which will be a big sort of language

45:27.320 --> 45:31.880
 model, and the reader model on top, which will also be a big sort of language model.

45:31.880 --> 45:37.060
 And just for people who are unfamiliar with the current status quo, what we have right

45:37.060 --> 45:40.800
 now is basically just two kinds of information storage.

45:40.800 --> 45:45.400
 You've got the model itself, which has been pre-baked with just an understanding of language

45:45.400 --> 45:50.800
 and whatever emerges from that, just basically predicting missing words.

45:50.800 --> 45:54.520
 And then you've got what people usually call the prompt, which is whatever you can cram

45:54.520 --> 45:56.920
 into the attention window at inference time.

45:56.920 --> 45:59.280
 So you can actually put a whole conversation there.

45:59.280 --> 46:00.280
 You can put example problems.

46:00.280 --> 46:04.560
 You could do a lot of neat things in the prompt, but it's pretty darn limited, right?

46:04.560 --> 46:09.800
 It's aside from your pre-baked knowledge that's crystallized, all these things can do is whatever

46:09.800 --> 46:11.440
 you can cram into the prompt.

46:11.440 --> 46:17.680
 And what you're suggesting is, hey, maybe we could actually build a whole separate system

46:17.680 --> 46:21.160
 where they could retrieve knowledge at game time.

46:21.160 --> 46:22.160
 Yeah.

46:22.160 --> 46:23.760
 So there are some interesting models.

46:23.760 --> 46:27.680
 So I've been involved in a model called RAG, Retrieval Augmented Generation, and there's

46:27.680 --> 46:30.040
 also Realm from Google.

46:30.040 --> 46:32.840
 And yeah, so the basic idea is that you can...

46:32.840 --> 46:36.040
 So you have your language model, which would be parametric.

46:36.040 --> 46:42.000
 And then you have your KNN, like nearest neighbor search algorithm, essentially, which is non-parametric.

46:42.000 --> 46:46.840
 And if you put those two approaches together, you get a semi-parametric model.

46:46.840 --> 46:50.720
 And I think there's a lot of potential applications for that down the line.

46:50.720 --> 46:51.720
 So that's one thing.

46:51.720 --> 46:52.760
 And then the other thing...

46:52.760 --> 46:54.960
 So I said multimodal, semi-parametric.

46:54.960 --> 46:58.980
 And I think the other thing that's going to be interesting, and there's a lot of traction

46:58.980 --> 47:02.760
 happening there now too, is around data-centric AI.

47:02.760 --> 47:08.740
 So I'm still rooting for things like active learning becoming much more mainstream, measuring

47:08.740 --> 47:11.060
 our data much more carefully.

47:11.060 --> 47:16.640
 So we have folks like Mick Mitchell in Hugging Face working on data measurement tools and

47:16.640 --> 47:17.640
 things like that.

47:17.640 --> 47:21.000
 So really trying to understand much better what's really happening in our data and trying

47:21.000 --> 47:25.640
 to do things to the data or curate the data in different ways so that we can have better

47:25.640 --> 47:27.360
 models in the end.

47:27.360 --> 47:36.520
 Yeah, quickly before calling you, I actually refreshed my knowledge of what this data tool

47:36.520 --> 47:37.520
 looks like.

47:37.520 --> 47:39.680
 It's kind of like an X-ray for data sets.

47:39.680 --> 47:40.680
 And it's a really beautiful idea.

47:40.680 --> 47:42.080
 I'm surprised that no one...

47:42.080 --> 47:46.000
 You know an idea is a good one when you're like, why haven't we been doing this for ages?

47:46.000 --> 47:51.840
 It's just like all the automatic obvious things you can measure about a data set that's composed

47:51.840 --> 47:53.080
 of language.

47:53.080 --> 47:55.160
 Let's put that all in one toolkit.

47:55.160 --> 47:58.160
 And then you can keep adding to it and make it more and more sophisticated.

47:58.160 --> 47:59.160
 That's the basic idea.

47:59.160 --> 48:00.160
 Right?

48:00.160 --> 48:01.160
 Yeah, that's exactly it.

48:01.160 --> 48:04.840
 And so I think that it's just nice that this lives in a space.

48:04.840 --> 48:09.400
 So it has a graphical user interface that just exists.

48:09.400 --> 48:13.080
 Maybe in the longer term, it will be a natural part of the Hugging Face Hub where you can

48:13.080 --> 48:18.600
 just upload any data set to the hub and start measuring what's actually in your data.

48:18.600 --> 48:22.380
 And you have a nice interface where you can just inspect it on the fly.

48:22.380 --> 48:28.480
 So we have a lot of interesting things going on in the direction of evaluation and measurement.

48:28.480 --> 48:32.640
 So I think what's really crucial when you think about the AI pipeline of the future

48:32.640 --> 48:36.880
 is that you have raw data, which you turn into data sets and those data sets you turn

48:36.880 --> 48:37.880
 into models.

48:37.880 --> 48:41.520
 But you want to measure your data sets and your models and you want to understand what's

48:41.520 --> 48:42.960
 in there and how well they perform.

48:42.960 --> 48:47.680
 And then you want to, based on your measurement, deploy the best model to production.

48:47.680 --> 48:50.600
 So for making predictions with your model.

48:50.600 --> 48:56.080
 And so measurement is really absolutely crucial in all of the decisions that you're making

48:56.080 --> 48:57.080
 there.

48:57.080 --> 48:59.400
 But measurement is also very difficult.

48:59.400 --> 49:02.000
 And so that's something that we're trying to address.

49:02.000 --> 49:05.840
 So we have an Evaluate library that came out a couple of weeks ago.

49:05.840 --> 49:09.120
 We have some very exciting announcements coming up soon.

49:09.120 --> 49:12.560
 I don't know when this podcast comes out, but it might already be out by that time.

49:12.560 --> 49:18.480
 But we're working on evaluation on the hub so that you can essentially evaluate any model

49:18.480 --> 49:21.920
 on any data set using any metric, just at the click of a button.

49:21.920 --> 49:24.720
 So you don't have to do any manual stuff there.

49:24.720 --> 49:29.560
 Well, surely people are going to miss having to go and copy paste massive chunks of scikit

49:29.560 --> 49:32.320
 learn code into their Jupyter notebooks.

49:32.320 --> 49:35.320
 Come on, Dawa.

49:35.320 --> 49:36.320
 Yeah.

49:36.320 --> 49:44.000
 No, I think like lowering the barrier to doing proper evaluation according to best practices.

49:44.000 --> 49:46.840
 I think that's a hugely impactful thing to do.

49:46.840 --> 49:50.360
 And that's something that Hugging Face is uniquely equipped to do.

49:50.360 --> 49:54.040
 So that's one of the things I've been excited about also in the science team.

49:54.040 --> 49:55.480
 You mentioned active learning.

49:55.480 --> 49:57.120
 I'd love to dig into that a little bit.

49:57.120 --> 49:58.960
 That's something I've worked with myself.

49:58.960 --> 50:01.220
 And it's an enticing idea.

50:01.220 --> 50:06.520
 Just for those listening at home who aren't familiar, usually when you want to make a

50:06.520 --> 50:11.600
 data set to train a model, you the human have to select the samples of data from your raw

50:11.600 --> 50:17.240
 data pool that you're going to gold label and train your model on.

50:17.240 --> 50:21.360
 The idea of active learning in a nutshell is let's put a model between you and the data,

50:21.360 --> 50:23.520
 sometimes the model you're training.

50:23.520 --> 50:28.440
 And it will decide which ones to put in front of you, the human whose time is expensive

50:28.440 --> 50:35.800
 and whose site is limited and make good choices and try and find the most instructive examples

50:35.800 --> 50:39.400
 from the data to label so that you get the most bang for buck because no one wants to

50:39.400 --> 50:41.680
 spend their whole life labeling data.

50:41.680 --> 50:44.440
 In fact, there's some problems that that's just prohibitive.

50:44.440 --> 50:46.520
 You literally just can't do it.

50:46.520 --> 50:49.940
 True positives are too rare, et cetera, et cetera.

50:49.940 --> 50:53.840
 So is there some like really exciting new developments in active learning?

50:53.840 --> 50:57.400
 I feel like it's kind of like a done deal.

50:57.400 --> 51:00.160
 Is there something new and exciting on the horizon, you think?

51:00.160 --> 51:01.680
 Yeah, I'm not.

51:01.680 --> 51:06.840
 So I think it just has a lot of potential.

51:06.840 --> 51:11.760
 And so what you described is a specific kind of active learning, I think, where you have

51:11.760 --> 51:16.240
 an acquisition function that scores examples and you just decide which example you want

51:16.240 --> 51:17.240
 to label.

51:17.240 --> 51:21.680
 But I think there are extensions of these algorithms where you can think not about the

51:21.680 --> 51:24.000
 labeling part, but about the pre-training part.

51:24.000 --> 51:30.020
 So maybe I can pick parts of a large corpus that I should be pre-training on now because

51:30.020 --> 51:33.680
 I know what downstream task I care about in the end.

51:33.680 --> 51:38.000
 So there's a mismatch currently between the pre-training phase where we just do language

51:38.000 --> 51:41.280
 modeling or causal or mass language modeling.

51:41.280 --> 51:45.120
 And then we fine tune it on something that might be very different from what we're training

51:45.120 --> 51:46.120
 on.

51:46.120 --> 51:47.400
 So we don't really know what to train on.

51:47.400 --> 51:52.000
 So I think if we can connect the pre-training phase to what we know we are going to care

51:52.000 --> 51:55.160
 about, then you can do very interesting things.

51:55.160 --> 52:00.880
 And so the way to do that selection, so data selection, is through an acquisition function

52:00.880 --> 52:02.000
 type of thing.

52:02.000 --> 52:04.320
 So that's why it's related to active learning.

52:04.320 --> 52:09.100
 Another interesting thing that I've been working on as well with some folks is dynamic adversarial

52:09.100 --> 52:13.960
 data collection where you have a model in the loop and a human is trying to fool the

52:13.960 --> 52:14.960
 model.

52:14.960 --> 52:20.000
 And so if you take the model fooling examples or so if you take all the examples, including

52:20.000 --> 52:24.960
 the ones that didn't fool the model, but that are still kind of intended to try to probe

52:24.960 --> 52:28.760
 the model for a weakness, if you train on that data and you keep updating the model

52:28.760 --> 52:34.480
 as you're doing the training, so that's the dynamic component, then you get a much better

52:34.480 --> 52:35.600
 model out in the end.

52:35.600 --> 52:38.060
 So it's really like 10% better.

52:38.060 --> 52:42.600
 So we have a nice paper where we try to do this in the limit over like 20 rounds of natural

52:42.600 --> 52:45.900
 language inference, and you just get a much, much better model out in the end.

52:45.900 --> 52:51.920
 So I think the future of data collection, the way we think about it now in the field,

52:51.920 --> 52:55.200
 is going to be changed a little bit where everything is just always going to be with

52:55.200 --> 52:56.920
 models in the loop.

52:56.920 --> 53:01.000
 And that maybe ties back to this long term vision of having language models interacting

53:01.000 --> 53:02.760
 with each other in some environment.

53:02.760 --> 53:08.120
 But if you can have humans and models together interacting with each other and learning from

53:08.120 --> 53:12.960
 each other and maybe trying to also kind of probe each other and be on the decision boundaries

53:12.960 --> 53:17.000
 of certain things, then you can learn much more efficiently, I think.

53:17.000 --> 53:20.360
 It sounds like you're describing education, like a school.

53:20.360 --> 53:21.360
 Exactly.

53:21.360 --> 53:22.360
 Yeah.

53:22.360 --> 53:27.760
 So in terms of education, one of the important things is also a curriculum, right?

53:27.760 --> 53:33.520
 So I think one thing you could do with this pre-training, like the active learning of

53:33.520 --> 53:37.880
 pre-training and connecting that to fine-tuning is you could try to have a smarter curriculum.

53:37.880 --> 53:43.600
 So if your acquisition function changes over time, essentially you're designing a curriculum

53:43.600 --> 53:48.240
 or you're learning a curriculum on the fly that helps your language model be as good

53:48.240 --> 53:51.080
 as possible on the downstream tasks that you might care about.

53:51.080 --> 53:57.820
 Okay, and final question, at least that I can think of, is something that Thomas mentioned

53:57.820 --> 54:06.640
 that intrigued me, which is he said that there seems to be something missing with NLP.

54:06.640 --> 54:09.800
 Like we're making these language models bigger and bigger and bigger.

54:09.800 --> 54:13.400
 And yes, we're improving the data and we're getting more data centric.

54:13.400 --> 54:19.240
 But he gave the impression that he really believes that there's something fundamentally

54:19.240 --> 54:20.240
 missing.

54:20.240 --> 54:21.240
 There's more data.

54:21.240 --> 54:22.240
 It's not just more text.

54:22.240 --> 54:29.760
 And you've hinted at this yourself with multimodality and interaction, agency, whatever that means.

54:29.760 --> 54:36.780
 So yeah, if you had to make a guess 10 years from now, like what do you think the paradigm

54:36.780 --> 54:38.220
 is going to be?

54:38.220 --> 54:40.040
 Will we even talk about NLP?

54:40.040 --> 54:43.000
 I suspect NLP will be a historical footnote.

54:43.000 --> 54:46.200
 They'll just be AI, right?

54:46.200 --> 54:53.680
 And text will be just one of the many crucial developmental raw material for artificial

54:53.680 --> 54:54.680
 intelligence.

54:54.680 --> 55:01.800
 Yeah, but I still think that text will just remain a dominant modality.

55:01.800 --> 55:03.240
 As it is with humans.

55:03.240 --> 55:04.240
 Exactly.

55:04.240 --> 55:05.240
 Right.

55:05.240 --> 55:06.480
 So language really is very crucial.

55:06.480 --> 55:09.960
 So I don't think NLP itself is going away.

55:09.960 --> 55:15.800
 But I think, yeah, in order to get to real meaning, all of these other fields are probably

55:15.800 --> 55:22.440
 going to be subsumed into NLP when it comes to language understanding.

55:22.440 --> 55:24.160
 So yeah, I don't know.

55:24.160 --> 55:30.400
 In 10 years from now, I think we're going to have very, very different models in a sense.

55:30.400 --> 55:34.560
 But I think a lot of the building blocks that we have now are still going to exist in those

55:34.560 --> 55:35.560
 models.

55:35.560 --> 55:40.800
 Do you think it'll just be all eventually robotics, either real world and or virtual

55:40.800 --> 55:45.800
 world robots, like taking in all the sensory information?

55:45.800 --> 55:47.640
 Virtual robots, I definitely buy.

55:47.640 --> 55:55.040
 So I think for actual physical robots, I think learning from that doesn't really scale that

55:55.040 --> 55:56.040
 well.

55:56.040 --> 55:59.040
 So I think one of the big problems in robotics is sim to real, right?

55:59.040 --> 56:02.960
 How do you transfer from a simulation to a real environment?

56:02.960 --> 56:07.480
 And so as our simulations become realer and realer, that problem is going to become smaller

56:07.480 --> 56:08.840
 and smaller.

56:08.840 --> 56:13.920
 So I don't think we need physical embodiment in any real sense in order to get to meaning.

56:13.920 --> 56:17.840
 But we'll probably definitely need virtual embodiment where you have an environment where

56:17.840 --> 56:20.040
 you can interact with each other.

56:20.040 --> 56:22.300
 But maybe that environment already exists, right?

56:22.300 --> 56:27.520
 So maybe that environment is just the internet, or maybe that environment will come into existence

56:27.520 --> 56:32.280
 very soon in the form of the metaverse or whatever you want to call it.

56:32.280 --> 56:35.880
 Any final thoughts you want to share with tens of thousands of people?

56:35.880 --> 56:43.520
 I think one of the things that I think is important, and that's kind of what Hugging Face also

56:43.520 --> 56:47.240
 stands for, is just open source and open science.

56:47.240 --> 56:53.640
 So if I were to give any parting thoughts, I would encourage people to always embrace

56:53.640 --> 56:59.200
 openness because that really is crucial to making progress, but also making sure that

56:59.200 --> 57:03.800
 the progress that we make doesn't end up in the wrong hands or go in the wrong direction.

57:03.800 --> 57:09.240
 So just for those listening at home, Dauwa, where can we find out more about you and what

57:09.240 --> 57:10.240
 you do?

57:10.240 --> 57:11.240
 Yeah.

57:11.240 --> 57:12.240
 So I have a website.

57:12.240 --> 57:13.240
 It's dauwakila.github.io.

57:13.240 --> 57:17.360
 It has a couple of links to relevant social media profiles.

57:17.360 --> 57:19.400
 I also have a Twitter account.

57:19.400 --> 57:25.720
 So that's Dauwa Kiela, my name, so D-O-U-W-E-K-I-E-L-A.

57:25.720 --> 57:27.560
 So I'm trying to be more active on Twitter.

57:27.560 --> 57:29.160
 I'm still working on that.

57:29.160 --> 57:34.400
 And I'm BohananBot on Twitter, and you'll see me probably asking follow-up questions

57:34.400 --> 57:36.600
 out in the open following your advice.

57:36.600 --> 57:37.880
 Dauwa, thank you so much.

57:37.880 --> 57:38.880
 This was a blast.

57:38.880 --> 57:39.880
 Thank you.

57:39.880 --> 58:00.280
 Thanks for having me.

