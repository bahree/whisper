WEBVTT

00:00.000 --> 00:05.000
All right, everyone, welcome to our AI Trends 2023 series.

00:05.200 --> 00:07.120
Each year, we invite friends of the show

00:07.120 --> 00:10.180
to join us to recap key developments of the year

00:10.180 --> 00:12.080
and anticipate future advancements

00:12.080 --> 00:15.480
in the most interesting subfields in AI.

00:15.480 --> 00:17.680
And today we're joined by Samir Singh.

00:17.680 --> 00:19.600
Samir is an associate professor

00:19.600 --> 00:22.360
in the Department of Computer Science at UC Irvine

00:22.360 --> 00:24.180
and a fellow at the Allen Institute

00:24.180 --> 00:27.000
for Artificial Intelligence, or AI2,

00:27.000 --> 00:29.960
to talk through some of the key research developments

00:29.960 --> 00:32.000
in NLP.

00:32.000 --> 00:34.320
Of course, before we get going,

00:34.320 --> 00:35.960
take a moment to hit that subscribe button

00:35.960 --> 00:38.080
wherever you're listening to today's show,

00:38.080 --> 00:41.960
and you can also follow us on TikTok and Instagram,

00:41.960 --> 00:46.640
at Twiml AI, for highlights from every episode.

00:46.640 --> 00:48.280
All right, let's jump in.

00:48.280 --> 00:53.280
Samir, welcome back to the podcast and our Trends series.

00:53.480 --> 00:54.920
Yeah, thank you for having me, Sam.

00:54.920 --> 00:56.600
It's great to be back.

00:56.600 --> 00:58.320
It's super excited to have you back.

00:58.320 --> 01:00.540
We were joking a little bit before we got rolling

01:00.540 --> 01:04.160
that we picked big years to have you on.

01:04.160 --> 01:07.820
The last one was our 2020,

01:07.820 --> 01:11.260
right in the wake of GPT-3, a big year.

01:11.260 --> 01:15.000
And of course, this has been a huge year for NLP

01:15.000 --> 01:18.560
with the relatively recent release of ChatGPT.

01:18.560 --> 01:21.000
Yeah, it's always kind of crazy

01:21.000 --> 01:23.680
when you have these big changes happening in the year

01:23.680 --> 01:26.760
where there is research still going on in parallel

01:26.760 --> 01:28.800
and people are exploring research questions,

01:28.800 --> 01:32.280
and a lot of them either become obsolete

01:32.280 --> 01:34.760
or have to be revisited and things like that

01:34.760 --> 01:35.640
in the middle of the year.

01:35.640 --> 01:37.800
And in this year, especially,

01:37.800 --> 01:39.760
it was much closer to the end of the year.

01:39.760 --> 01:41.200
So looking back at a year,

01:41.200 --> 01:43.760
it's always interesting to think about the trajectory

01:43.760 --> 01:47.440
and what ideas will still persist and what won't.

01:47.440 --> 01:49.680
Yeah, yeah, a great point.

01:49.680 --> 01:53.120
ChatGPT happened right at the end of the year.

01:53.120 --> 01:55.640
Do you think we'd have the same sense

01:55.640 --> 01:57.600
that this was a huge year in NLP

01:57.600 --> 02:02.320
if it wasn't for that late year release of ChatGPT?

02:02.320 --> 02:03.160
Oh, definitely.

02:03.160 --> 02:06.280
I think this year has been really impressive.

02:06.280 --> 02:08.120
I would say even bigger,

02:08.120 --> 02:10.700
even if you take about ChatGPT,

02:11.520 --> 02:14.000
overall, this year has been really big for NLP,

02:14.000 --> 02:16.880
even compared to sort of the year GPT-3 came out.

02:16.880 --> 02:18.500
So yeah, I think they've been,

02:19.480 --> 02:22.840
I feel like it took us a while to come in terms

02:22.840 --> 02:25.160
with what these large language models are capable of

02:25.160 --> 02:28.880
or what they clearly fail at and what they're good at

02:28.880 --> 02:32.080
and try to sort of build better tooling around it,

02:32.080 --> 02:33.840
build better support systems around it.

02:33.840 --> 02:35.520
And so, yeah, I think this year has been good,

02:35.520 --> 02:38.440
even if you don't take it on ChatGPT, yeah.

02:38.440 --> 02:41.640
Awesome, well, we're gonna dig into ChatGPT

02:41.640 --> 02:43.360
in a fair amount of detail,

02:43.360 --> 02:46.040
as well as some of the other advances you just hinted at.

02:46.040 --> 02:47.240
But before we do,

02:47.240 --> 02:48.720
I'd love to have you take a few minutes

02:48.720 --> 02:52.140
to just kind of introduce yourself to our audience

02:52.140 --> 02:55.680
with a focus on kind of your research focus

02:55.680 --> 02:57.240
and what your interests are.

02:57.240 --> 02:59.760
Cool, yeah, so I've been working in NLP

02:59.760 --> 03:01.600
for a long time now,

03:01.600 --> 03:04.960
but my focus has mostly been looking at

03:04.960 --> 03:07.600
when these language models or machine learning in general

03:07.600 --> 03:09.400
gets interfaced with real users,

03:09.400 --> 03:12.640
what are the needs that sort of are there?

03:12.640 --> 03:14.880
So a lot of my work has been in explanations

03:14.880 --> 03:18.280
and interpretability, but also in robustness,

03:18.280 --> 03:20.080
both from an adversarial perspective,

03:20.080 --> 03:23.360
but also from out of domain generalization perspective.

03:23.360 --> 03:26.240
And also in terms of evaluation,

03:26.240 --> 03:28.720
like how do we know whether the models are doing well,

03:28.720 --> 03:30.160
how well are they doing?

03:30.160 --> 03:33.060
And in general, be able to understand and predict

03:33.060 --> 03:34.280
when the models would work

03:34.280 --> 03:37.280
and when the models would not work.

03:37.280 --> 03:42.280
And I'm imagining that the advent of large language models

03:43.120 --> 03:45.920
and the kind of the dominance of that approach

03:45.920 --> 03:47.900
to NLP modeling is,

03:47.900 --> 03:51.260
well, it certainly changed the tools

03:51.260 --> 03:52.960
and the approach that you take.

03:52.960 --> 03:55.300
Has it changed kind of the fundamental way

03:55.300 --> 03:56.700
that you approach the problem?

03:56.700 --> 03:58.140
To some degree, yes and no.

03:58.140 --> 04:01.140
I think it has made a lot of my work obsolete

04:01.140 --> 04:04.460
in the sense that we were doing a really good job

04:04.460 --> 04:07.420
of finding fundamental faults

04:07.420 --> 04:08.940
in a lot of these language models,

04:08.940 --> 04:10.980
and turns out a lot of them go away

04:10.980 --> 04:14.220
when you have a lot more data or a lot larger size.

04:14.220 --> 04:17.140
And so the specific observations and insights we had,

04:17.140 --> 04:19.700
not all of them have persisted,

04:19.700 --> 04:22.620
but the other differentiation we had in our work

04:22.620 --> 04:25.740
was always being somewhat model agnostic

04:25.740 --> 04:30.020
or try to use a black box approach to the model

04:30.020 --> 04:32.500
rather than looking inside what's going on.

04:32.500 --> 04:34.940
And that is something that you can use

04:34.940 --> 04:37.720
in this world of only access through API,

04:37.720 --> 04:40.100
a lot of those tooling can still work out.

04:40.100 --> 04:41.780
So yeah, so it's been a mix,

04:41.780 --> 04:46.780
but it's been exciting to sort of continue to do that.

04:46.780 --> 04:51.780
Well, you've identified some themes that from your purview

04:52.900 --> 04:57.420
have been some of the key topic areas and research

04:57.420 --> 05:01.260
that have emerged in the field over the past year.

05:02.700 --> 05:04.140
Let's start there.

05:04.140 --> 05:08.700
And maybe before we dive into any of the individual items,

05:08.700 --> 05:13.460
what's your take on 2022 broadly

05:13.460 --> 05:16.420
and some of the areas

05:16.420 --> 05:20.960
that you are most excited about in the year?

05:20.960 --> 05:22.660
Yeah, so I think broadly speaking,

05:22.660 --> 05:25.740
and we will delve deeper into a bunch of these topics,

05:25.740 --> 05:30.140
but broadly speaking, I think the importance of data

05:30.140 --> 05:33.240
and the importance of looking at what might be

05:33.240 --> 05:34.340
in the pre-training data

05:34.340 --> 05:37.660
has sort of brought up back into the focus

05:37.660 --> 05:39.380
in a way that I feel earlier years,

05:39.380 --> 05:40.700
we were a lot more agnostic

05:40.700 --> 05:42.740
of what the model was being trained on

05:42.740 --> 05:46.060
and just more data was better kind of thing.

05:46.060 --> 05:49.540
This year, it's been a lot more sort of thinking

05:49.540 --> 05:51.460
about what goes in the models

05:51.460 --> 05:55.100
and also thinking of ways to use the models,

05:55.100 --> 05:59.220
not just by simply prompting it with a simple thing,

05:59.220 --> 06:00.860
but trying to get it to reason,

06:00.860 --> 06:03.860
trying to get it to break down the problem into pieces

06:03.860 --> 06:06.500
and try to evaluate how much the language models

06:06.500 --> 06:07.940
can do that.

06:07.940 --> 06:09.900
And that I think is key when you start thinking

06:09.900 --> 06:13.940
about taking language models to more higher level

06:13.940 --> 06:16.260
decision-making or higher level reasoning.

06:16.260 --> 06:17.100
Awesome, awesome.

06:17.100 --> 06:20.300
What's the first area you'd like to dig into?

06:20.300 --> 06:24.340
So let's actually start with a chain of thought prompting.

06:24.340 --> 06:27.260
This is work coming out of Google

06:27.260 --> 06:29.740
that came out earlier this year.

06:29.740 --> 06:32.820
And I guess the easiest way to summarize

06:32.820 --> 06:36.220
is to say let's think step-by-step.

06:36.220 --> 06:39.220
The idea here is to have the model

06:39.220 --> 06:42.420
not just generate the answer directly,

06:42.420 --> 06:45.100
but try to have it go through the reasoning process

06:45.100 --> 06:47.740
and then arrive at the answer.

06:47.740 --> 06:50.940
This ended up being quite a strong,

06:52.300 --> 06:54.460
like quite an effective method

06:54.460 --> 06:56.820
to get the model to do a lot of things,

06:56.820 --> 06:59.300
especially when it comes to mathematical reasoning

06:59.300 --> 07:01.580
and sort of where you can break down the problems

07:01.580 --> 07:03.420
into a bunch of these things step.

07:03.420 --> 07:06.860
Chain of thought prompting did extremely well

07:06.860 --> 07:09.380
compared to what we had before.

07:09.380 --> 07:11.300
And part of the difference, I guess,

07:11.300 --> 07:14.380
was you're not just prompting with questions and answers,

07:14.380 --> 07:17.260
but you're also prompting with something

07:17.260 --> 07:18.900
that is much more detailed.

07:18.900 --> 07:21.580
So the prompt itself has a bunch of examples

07:21.580 --> 07:23.220
of breaking the reasoning down,

07:23.220 --> 07:25.820
and then you have the model being able to walk

07:25.820 --> 07:28.100
through that reasoning and get to the answer.

07:28.100 --> 07:33.100
And in that work is the idea that the user of the model

07:33.100 --> 07:37.620
should break the prompts down into more detail,

07:37.620 --> 07:42.420
or that the model should learn how to kind of show its work

07:42.420 --> 07:46.500
and given a coarse-grained prompt,

07:46.500 --> 07:48.700
break the prompt down itself?

07:48.700 --> 07:51.900
Yes, I think the initial paper focused on the user

07:51.900 --> 07:54.820
providing a few examples of this breakdown, right?

07:54.820 --> 07:56.540
So if you're saying like,

07:56.540 --> 07:59.020
here's a mathematical word problem,

07:59.020 --> 08:00.460
you have two apples,

08:00.460 --> 08:03.100
and then somebody gives you double of that,

08:03.100 --> 08:04.700
how many apples do you have?

08:04.700 --> 08:07.060
Breaking it down into a double means times two,

08:07.060 --> 08:08.060
and two times two is four.

08:08.060 --> 08:10.140
I mean, this is a very simple example,

08:10.140 --> 08:12.380
but this kind of giving an example or two

08:12.380 --> 08:16.220
of breaking this down can be quite powerful

08:16.220 --> 08:18.540
for a language model.

08:18.540 --> 08:20.580
Especially, I think, one of the key insights here,

08:20.580 --> 08:23.620
and we can talk about other papers

08:23.620 --> 08:25.700
that sort of showed related things,

08:25.700 --> 08:28.420
but this is a very emergent property

08:28.420 --> 08:31.740
that seems to exist for really large language models.

08:31.740 --> 08:34.140
And if you have smaller language models,

08:34.140 --> 08:36.140
it's kind of difficult to get them to do

08:36.140 --> 08:36.980
this kind of reasoning.

08:36.980 --> 08:39.620
So that's also been exciting to see.

08:39.620 --> 08:42.900
Did the results there, did you find them surprising?

08:42.900 --> 08:47.500
Were they counterintuitive that that would work?

08:47.500 --> 08:50.540
I think how well they worked were,

08:51.700 --> 08:53.500
I think it surprised everyone,

08:53.500 --> 08:54.820
because it's a very simple idea

08:54.820 --> 08:56.620
to just break it down a little bit.

08:56.620 --> 08:58.900
Everybody kind of assumed that the transformers

08:58.900 --> 09:01.060
are sort of either doing this internally

09:01.060 --> 09:03.380
or completely not doing this internally, right?

09:03.380 --> 09:07.700
And by showing you that if you actually write out

09:07.700 --> 09:10.540
a bunch of examples, these transformer models

09:10.540 --> 09:13.540
are able to do this to the extent that they are,

09:13.540 --> 09:17.860
was quite surprising, and the gains were quite impressive.

09:17.860 --> 09:20.580
Can you talk a little bit about the evaluation

09:20.580 --> 09:23.260
of that method?

09:23.260 --> 09:25.180
Yeah, so the evaluation was mostly focused

09:25.180 --> 09:27.540
on mathematical word problems.

09:27.540 --> 09:30.740
So there's this GSM8 data set,

09:30.740 --> 09:34.780
and then there's this MAWPSMOPS, I guess,

09:34.780 --> 09:35.700
data set as well.

09:35.700 --> 09:38.100
These are mathematical word problems.

09:38.100 --> 09:41.420
And this first evaluation was mostly looking at

09:41.420 --> 09:45.140
how well you can do a reason through some of those.

09:45.140 --> 09:48.500
And yeah, it was much, much better

09:48.500 --> 09:51.900
than anything that we had before.

09:51.900 --> 09:53.980
And then they had some evaluations

09:53.980 --> 09:55.620
on symbolic reasoning as well.

09:55.620 --> 09:59.220
So if you give them sort of tasks,

09:59.220 --> 10:04.220
which have like, not have a bunch of,

10:07.620 --> 10:10.260
so like finding a character inside a long string,

10:10.260 --> 10:13.820
like what is the fifth character or something like that,

10:13.820 --> 10:15.700
you can break it down into a bunch of steps.

10:15.700 --> 10:17.780
And if you give it a few examples, it can do it.

10:17.780 --> 10:19.460
If you don't give it a few examples

10:19.460 --> 10:22.220
of how to break it down, the models are very bad.

10:22.220 --> 10:25.820
And have you seen any work that looks to extend this

10:25.820 --> 10:30.180
beyond the kind of math and symbolic domain?

10:30.180 --> 10:33.540
So beyond, I'll talk a little bit about

10:33.540 --> 10:34.900
some of the related ideas

10:34.900 --> 10:37.660
and sort of question answering a little bit later.

10:37.660 --> 10:42.060
But there is one work that is related that I like.

10:42.060 --> 10:45.460
This is called algorithmic prompting.

10:45.460 --> 10:49.060
And this is stuff that came out of Google Brain as well.

10:49.060 --> 10:50.580
So, you know, a lot of the stuff

10:50.580 --> 10:51.820
is coming out of Google Brain

10:51.820 --> 10:54.500
because you need really large language models

10:54.500 --> 10:55.860
to be able to even work with this,

10:55.860 --> 10:58.820
so even bigger than GPT-3, for example.

10:58.820 --> 11:02.020
So in this algorithmic prompting paper,

11:02.020 --> 11:04.620
this was kind of interesting where they had

11:04.620 --> 11:07.180
essentially the same idea as chain of thought,

11:07.180 --> 11:10.020
except that they go really detailed

11:10.020 --> 11:12.860
into what those reasoning steps would be.

11:12.860 --> 11:16.300
So they mostly focus on, you know,

11:16.300 --> 11:19.060
things that can be described more as an algorithm

11:19.060 --> 11:22.460
rather than as just breaking it into a few pieces.

11:22.460 --> 11:23.980
So you can say things like,

11:23.980 --> 11:28.940
if I had to add 12 plus 24, right?

11:28.940 --> 11:30.340
How would you do that?

11:30.340 --> 11:32.420
They literally break it down into digits.

11:32.420 --> 11:34.140
So you take the ones place,

11:34.140 --> 11:37.100
that's two in one case, four in the other.

11:37.100 --> 11:40.500
You add them up, you get six, there is no carry.

11:40.500 --> 11:41.660
Okay, that's one.

11:41.660 --> 11:45.260
The second step is, okay, now look at the next tens place.

11:45.260 --> 11:47.740
It's one and two, add them up, three.

11:47.740 --> 11:49.740
Look at the carry or the carry is zero.

11:49.740 --> 11:51.940
So it's just three and then 36, right?

11:51.940 --> 11:54.660
So all of this, this very detailed breakdown,

11:54.660 --> 11:56.580
which looked like extremely detailed.

11:57.500 --> 12:00.260
But what was really impressive to me about that paper

12:00.260 --> 12:03.620
is they showed that you can give examples

12:03.620 --> 12:06.540
of really low digit operations.

12:06.540 --> 12:08.860
So like maybe two or three digit operations

12:08.860 --> 12:10.340
when you're talking about addition

12:10.340 --> 12:13.540
or multiplication or any of these things.

12:13.540 --> 12:15.380
But at test time, you can,

12:15.380 --> 12:18.180
firstly, even on two and three digit stuff,

12:18.180 --> 12:19.740
it was much, much accurate

12:19.740 --> 12:22.420
compared to regular chain of thought.

12:22.420 --> 12:26.740
Like 20% going from 80% of chain of thought

12:26.740 --> 12:28.100
to something that's 100%.

12:28.100 --> 12:29.940
I'm kind of making up numbers.

12:29.940 --> 12:33.780
And this is relative to asking for the model

12:33.780 --> 12:37.180
to solve the same problem without any intermediate steps.

12:37.180 --> 12:39.900
No, so without intermediate steps is even worse, right?

12:39.900 --> 12:44.220
So this is asking the model to, so like 12 plus 24,

12:44.220 --> 12:46.140
I don't know exactly what the chain of thought would be,

12:46.140 --> 12:47.540
but it would be something

12:47.540 --> 12:49.300
that would be at a higher granularity,

12:49.300 --> 12:51.060
let's just say, right?

12:51.060 --> 12:54.860
And so when you give these detailed prompts,

12:54.860 --> 12:58.300
the models are more accurate, which is not so surprising.

12:58.300 --> 13:01.180
But what was surprising was that they kept increasing

13:01.180 --> 13:05.580
the size of the number at test time.

13:05.580 --> 13:08.660
So started adding more and more digits

13:08.660 --> 13:12.500
and even up to 18 digit numbers.

13:12.500 --> 13:15.740
The model is able to do these operations

13:15.740 --> 13:17.740
much, much more accurately,

13:17.740 --> 13:20.980
even though the prompts were only on two or three digit

13:22.620 --> 13:23.980
sort of numbers, right?

13:23.980 --> 13:28.980
And so does this type of work answer definitively

13:30.140 --> 13:34.220
whether this is already happening inside the model

13:34.220 --> 13:37.260
versus there's some other effects?

13:37.260 --> 13:41.220
Like in a sense, it's really counterintuitive

13:41.220 --> 13:43.020
that it would work at all.

13:43.020 --> 13:45.620
There's no registers inside the model

13:45.620 --> 13:47.460
that are tracking digits,

13:47.460 --> 13:49.460
the ones place and the tens place.

13:49.460 --> 13:51.100
Why should that work?

13:51.100 --> 13:55.340
Yeah, so I think people are still trying to come to terms

13:55.340 --> 13:57.220
with why chain of thought reasoning works.

13:57.220 --> 13:58.860
Is there something in the pre-training data?

13:58.860 --> 14:00.260
Is there something in the model?

14:00.260 --> 14:02.860
And there's been some interesting work there.

14:02.860 --> 14:04.900
But no, I think the tricky thing here

14:04.900 --> 14:07.580
is you're making all of these things explicit.

14:07.580 --> 14:12.460
So you're not relying on the model to keep these bits

14:12.460 --> 14:15.260
somewhere latent in its sort of memory, right?

14:15.260 --> 14:16.540
Like you're making it explicit

14:16.540 --> 14:18.620
and of course it's attending to all of that.

14:18.620 --> 14:22.180
And so the chances of it sort of going away

14:22.180 --> 14:24.820
into a wrong place is much lower.

14:24.820 --> 14:28.340
So, you know, Scratchpad and a bunch of other papers

14:28.340 --> 14:29.820
had similar ideas of like,

14:29.820 --> 14:32.180
hey, let's give some model some space

14:32.180 --> 14:33.940
to think about things, right?

14:33.940 --> 14:38.380
So it's possible that this is just letting the model

14:38.380 --> 14:39.860
actually think things through.

14:39.860 --> 14:43.580
So it's somehow more computation that the model is getting.

14:43.580 --> 14:45.340
And there've been some papers showing that,

14:45.340 --> 14:46.500
yeah, that might be the difference.

14:46.500 --> 14:49.700
The fact that you're generating a single number,

14:49.700 --> 14:50.660
but you're letting,

14:50.660 --> 14:53.100
not just asking the model to give it one shot,

14:53.100 --> 14:55.180
but letting it think about it.

14:55.180 --> 14:57.820
And it's not so much the fact

14:57.820 --> 15:00.260
that you're giving these example breakdowns that helps.

15:00.260 --> 15:02.580
But I think, you know, as many of these things,

15:02.580 --> 15:04.380
I'm sure the answer is complicated

15:04.380 --> 15:06.620
and it's some combination of everything.

15:06.620 --> 15:09.060
The last thing you said almost sounds

15:09.060 --> 15:11.540
like the kind of multitask argument.

15:11.540 --> 15:13.180
It's not that, you know,

15:13.180 --> 15:15.540
the specific other thing that you're asking

15:15.540 --> 15:18.180
the model to do matters,

15:18.180 --> 15:20.340
but that you're asking it to do another thing.

15:20.340 --> 15:24.100
And that kind of, you know, on the traditional side,

15:24.100 --> 15:26.380
like has some kind of regularization effect

15:26.380 --> 15:30.340
or some kind of effect that causes your results to be better

15:30.340 --> 15:32.900
just by overloading the model a little bit.

15:32.900 --> 15:34.540
Yeah, yeah, exactly, right.

15:34.540 --> 15:38.260
You're letting, in some sense, you have more activations,

15:38.260 --> 15:40.980
you have more latent states,

15:40.980 --> 15:44.540
you just are giving model more things to do.

15:44.540 --> 15:48.860
And so it has space to explode through more reasoning.

15:48.860 --> 15:50.420
So maybe that's one explanation

15:50.420 --> 15:53.740
for why this kind of stuff works, but yeah.

15:53.740 --> 15:54.580
Amazing, amazing.

15:54.580 --> 15:57.580
And I should have mentioned earlier on,

15:57.580 --> 15:58.900
but I will mention it now.

15:58.900 --> 16:01.100
All of the papers that we're referring to

16:01.100 --> 16:03.420
will be available on the show notes page.

16:03.420 --> 16:05.940
So folks can check them out.

16:06.820 --> 16:08.900
So the next thing that you had on your list

16:08.900 --> 16:10.860
was decomposed reasoning.

16:10.860 --> 16:13.140
It sounds like it's in a similar vein.

16:13.140 --> 16:14.740
Yeah, so I think this is,

16:14.740 --> 16:16.140
that's why I kind of put them together,

16:16.140 --> 16:17.540
but I think fundamentally,

16:17.540 --> 16:20.340
this is a very different approach to the same idea.

16:20.340 --> 16:22.740
So yes, I think terminology is something

16:22.740 --> 16:24.660
that the field is gonna be revisiting

16:24.660 --> 16:26.740
and decomposed reasoning is kind of something

16:26.740 --> 16:27.900
that I came up with.

16:27.900 --> 16:29.660
I don't even know if people use it.

16:29.660 --> 16:31.020
But the idea here is that

16:31.020 --> 16:32.220
there've been a bunch of papers here

16:32.220 --> 16:35.420
and I'm just gonna sort of quickly run through some of them.

16:35.420 --> 16:38.140
But the idea here is that you shouldn't rely

16:38.140 --> 16:41.460
on the language model alone to do the whole task.

16:41.460 --> 16:44.700
So suppose I give it a mathematical word problem

16:44.700 --> 16:46.980
or if I give it a question answering problem,

16:46.980 --> 16:49.180
that's a lot complicated.

16:49.180 --> 16:53.020
I shouldn't rely on the model and its parameters

16:53.020 --> 16:54.780
to be able to carry everything out.

16:54.780 --> 16:57.260
Maybe the model needs to use a calculator.

16:57.260 --> 17:00.580
Maybe the model needs to do a web search.

17:00.580 --> 17:03.740
Maybe the model needs to even write a small Python script

17:03.740 --> 17:07.700
and actually run it to get the answer that I want.

17:07.700 --> 17:11.020
And so this whole idea and yeah,

17:11.020 --> 17:13.860
of language models getting what you need,

17:13.860 --> 17:16.220
but not just relying on its own parameters,

17:16.220 --> 17:18.780
but breaking down your problem and figuring out,

17:18.780 --> 17:20.500
oh, I need to call something else

17:20.500 --> 17:23.420
and this is what I'm gonna do to call it,

17:23.420 --> 17:25.860
is an idea that sort of came out

17:25.860 --> 17:28.140
post chain of thought sort of middle of the year,

17:28.140 --> 17:29.740
but there've been a bunch of papers

17:29.740 --> 17:31.460
all the way to the end of the year

17:31.460 --> 17:34.740
that have been doing a lot of this.

17:34.740 --> 17:36.940
So yeah, it's kind of been exciting.

17:37.820 --> 17:41.380
A lot of them have been on the QA side of things.

17:41.380 --> 17:44.300
So the two I'll mention is successive prompting

17:44.300 --> 17:45.780
that came out of my group,

17:45.780 --> 17:47.460
but there's also decomposed prompting

17:47.460 --> 17:49.100
that came out of AI2.

17:49.100 --> 17:50.660
And the idea behind both of these

17:50.660 --> 17:52.700
was to take a complex question,

17:52.700 --> 17:55.780
break it down into simpler ones.

17:55.780 --> 17:58.420
And then have the language model

17:58.420 --> 18:01.340
sort of call another language model

18:01.340 --> 18:05.180
that is answering each of these simple questions, right?

18:05.180 --> 18:07.500
So if a simple question is a mathematical operation,

18:07.500 --> 18:08.820
then you would use a calculator.

18:08.820 --> 18:13.100
If a simple question is a very simple lookup question,

18:13.100 --> 18:14.100
then you would use something

18:14.100 --> 18:17.500
that is like a squad style question answering system,

18:17.500 --> 18:18.340
things like that, right?

18:18.340 --> 18:21.900
So being able to take what the user wants

18:21.900 --> 18:24.060
and breaking down into pieces

18:24.060 --> 18:25.780
and then composing the answers together

18:25.780 --> 18:29.140
to give you the actual answer is this kind of stuff.

18:29.140 --> 18:31.700
Can you talk a little bit in a little bit more detail

18:31.700 --> 18:33.860
the difference between successive prompting

18:33.860 --> 18:35.540
and decomposed prompting?

18:35.540 --> 18:37.860
How did the settings for those differ?

18:37.860 --> 18:40.380
They came out pretty much around the same time.

18:40.380 --> 18:42.380
So it's difficult to sort of,

18:42.380 --> 18:46.380
and they sort of appeared at the same conference as well.

18:46.380 --> 18:47.820
I think, yeah.

18:47.820 --> 18:50.020
So I think some of it depended on sort of

18:50.020 --> 18:51.860
which data set they use.

18:51.860 --> 18:54.180
So decomposed prompting used

18:54.180 --> 18:56.060
explicitly multi-hop data sets

18:56.060 --> 18:58.740
and sort of try to decompose it that way.

18:58.740 --> 19:01.580
Successive prompting focused a little bit more

19:01.580 --> 19:04.460
on calculations and symbolic operations as well.

19:04.460 --> 19:06.980
So yeah, I would say the differences between them.

19:06.980 --> 19:09.300
But kind of same idea, different data sets,

19:09.300 --> 19:10.780
slightly different tooling.

19:10.780 --> 19:12.380
Right, right, yeah.

19:12.380 --> 19:14.740
And we'll see in some of these cases,

19:14.740 --> 19:17.340
other pairs of papers also that are very similar

19:17.340 --> 19:18.620
that came out around the same time

19:18.620 --> 19:21.460
because that's where we are, yeah.

19:21.460 --> 19:23.460
How about tool augmented?

19:23.460 --> 19:24.940
So tool augmented stuff.

19:24.940 --> 19:29.500
So there was a paper coming out of Google, I believe,

19:29.500 --> 19:32.100
called Tool Augmented Language Models.

19:32.100 --> 19:33.580
So TAL is the paper.

19:33.580 --> 19:35.060
And this is one of the papers

19:35.060 --> 19:39.340
that was essentially showing that you can have,

19:39.340 --> 19:41.620
instead of just calling a calculator explicitly

19:41.620 --> 19:43.540
or just having a fixed set of things,

19:43.540 --> 19:47.220
you can create a description of APIs

19:47.220 --> 19:49.420
that the language model has access to

19:49.420 --> 19:52.580
and have the language model itself

19:52.580 --> 19:55.420
generate example calls to that API

19:55.420 --> 19:57.180
when it's doing an output, right?

19:57.180 --> 20:01.540
So if I want to say like, hey, GPT-3 or whatever,

20:01.540 --> 20:04.580
how hot is it going to get today, right?

20:04.580 --> 20:08.260
Or how hot is it going to get today in Irvine?

20:08.260 --> 20:09.620
The language model is going to say,

20:09.620 --> 20:13.780
okay, this is a question about the weather in Irvine.

20:13.780 --> 20:18.780
So I'm gonna compose an API call to a weather service, right?

20:18.780 --> 20:21.760
That's going to say, what's the weather in Irvine?

20:21.760 --> 20:24.600
And then it'll return some JSON object that says,

20:24.600 --> 20:26.480
oh, the high is this, low is this,

20:26.480 --> 20:28.160
probability of rain is this.

20:28.160 --> 20:30.760
And then the language model will kick in again

20:30.760 --> 20:32.560
and take that output and say,

20:32.560 --> 20:34.760
oh, it's gonna be pretty hot today

20:35.640 --> 20:37.440
as since it is Southern California.

20:37.440 --> 20:39.320
And yeah, you know, something.

20:39.320 --> 20:41.320
Seems like this research is heading in the direction

20:41.320 --> 20:45.320
of how would you kind of rebuild Siri or Alexa

20:45.320 --> 20:47.240
or something like that with LLMs.

20:47.240 --> 20:49.680
Yes, yeah, and I think this is one of the key

20:49.680 --> 20:51.800
sort of advantages of these language models

20:51.800 --> 20:54.200
is not that they can do additions

20:54.200 --> 20:55.620
and subtractions internally.

20:55.620 --> 20:57.000
Like I think that's interesting

20:57.000 --> 20:58.760
from an intellectual point of view,

20:58.760 --> 21:00.280
but when you're making actual products,

21:00.280 --> 21:02.880
you want this language model to,

21:02.880 --> 21:05.540
language is a way to interface with things

21:05.540 --> 21:07.640
that are external to you, right?

21:07.640 --> 21:10.320
So the language models should take in the user queries,

21:10.320 --> 21:13.640
but also be the interface to other things outside

21:13.640 --> 21:15.620
and be able to query it.

21:15.620 --> 21:17.600
I think we will talk a little bit about that later,

21:17.600 --> 21:19.080
but one of the reasons I like this

21:19.080 --> 21:22.320
is you can also somehow now attribute

21:22.320 --> 21:24.120
the answer that you're getting,

21:24.120 --> 21:26.920
not to some internal parameter in the language model,

21:26.920 --> 21:29.840
but to say, look, this is the API call I made,

21:29.840 --> 21:31.320
and this is the answer I got,

21:31.320 --> 21:34.000
and now that's the answer I gave you.

21:34.000 --> 21:38.000
So in some sense, it becomes a little bit more attributed.

21:38.000 --> 21:43.000
The idea of the language model writing a program

21:43.000 --> 21:48.000
to figure out the answer to a question is a fascinating one,

21:48.920 --> 21:52.800
and it almost feels like if anything that,

21:52.800 --> 21:55.600
anything around LLMs is gonna be the path to AGI,

21:55.600 --> 21:56.680
it's like, it's that.

21:57.720 --> 22:00.360
What was your reaction to that research?

22:01.240 --> 22:03.160
Yeah, I think it seems quite,

22:03.160 --> 22:05.320
like to me from a practical point of view,

22:05.320 --> 22:07.160
it seems quite exciting, right?

22:07.160 --> 22:10.680
Like so, from a core generation point of view

22:10.680 --> 22:12.680
and things like that, it's useful as well,

22:12.680 --> 22:15.120
but the nice thing about the code,

22:15.120 --> 22:17.660
writing code is that it's unambiguous, right?

22:17.660 --> 22:20.660
So it's making some calls to an external database.

22:20.660 --> 22:23.300
If I want to update the language model,

22:23.300 --> 22:25.280
or update this whole system,

22:25.280 --> 22:28.000
I can just update my knowledge directly, right?

22:28.000 --> 22:29.600
The knowledge is external somehow

22:29.600 --> 22:31.960
to the parameterization of the language model.

22:31.960 --> 22:34.040
That makes it super convenient to delete things,

22:34.040 --> 22:36.320
or to add things, or to get attributions,

22:36.320 --> 22:37.520
and all of these things.

22:37.520 --> 22:42.520
And the interface to that data source is always programs.

22:43.640 --> 22:47.200
Either it's like a simple API call or a more complex one.

22:47.200 --> 22:49.400
And I think I really like this idea

22:49.400 --> 22:52.080
because it allows the language models

22:52.080 --> 22:54.080
to do things that it should be doing,

22:54.080 --> 22:55.640
which is to understand language,

22:55.640 --> 22:56.960
or let's not call it understand,

22:56.960 --> 22:58.320
would be able to parse language,

22:58.320 --> 23:01.780
be able to sort of, you know, transform it,

23:02.840 --> 23:05.880
but doesn't necessarily have to know

23:05.880 --> 23:09.240
the temperature of Irvine every day, or things like that.

23:09.240 --> 23:11.160
Like, that's not something I necessarily want

23:11.160 --> 23:13.360
in the parameters of the language model.

23:13.360 --> 23:14.920
Yeah.

23:14.920 --> 23:17.440
So just very subtly in there,

23:17.440 --> 23:22.440
you kind of addressed another big conversation

23:24.120 --> 23:25.960
that's happening in the community now,

23:25.960 --> 23:29.740
and it's this idea of do language models understand?

23:29.740 --> 23:31.480
You call this decomposed reasoning.

23:31.480 --> 23:32.860
The thing is writing programs,

23:32.860 --> 23:35.080
that kind of requires some kind of reasoning.

23:35.080 --> 23:38.120
Like, what's your take on, you know,

23:38.120 --> 23:41.840
these broader questions about, you know,

23:41.840 --> 23:46.040
reasoning and understanding in LLMs?

23:46.040 --> 23:47.800
Or would you like to defer that?

23:47.800 --> 23:51.480
Is there a natural point later for us to talk about that?

23:52.520 --> 23:55.160
Come back to it a little bit later,

23:55.160 --> 23:57.880
maybe even in the next section of sort of,

23:57.880 --> 24:00.400
us trying to sort of question what reasoning is

24:00.400 --> 24:03.640
and trying to evaluate that in some sense.

24:03.640 --> 24:05.360
But yeah.

24:05.360 --> 24:07.820
The semantic argument around understanding,

24:07.820 --> 24:09.380
like that's not that interesting,

24:09.380 --> 24:12.760
but like how a language model can reason,

24:12.760 --> 24:16.280
and, you know, the extent to which it's reasoning

24:16.280 --> 24:19.320
versus like cutting pasting at some, you know,

24:19.320 --> 24:23.880
level beyond, at an impressive, in an impressive way,

24:23.880 --> 24:25.920
like that's kind of really interesting.

24:25.920 --> 24:26.760
Yeah, definitely.

24:26.760 --> 24:30.880
So I would say, and even pushing it a little bit further,

24:30.880 --> 24:33.200
like what are the consequences of the fact

24:33.200 --> 24:35.720
that it is cutting pasting versus it's reasoning, right?

24:35.720 --> 24:38.400
Like, so how should we calibrate

24:38.400 --> 24:40.480
what things these should be deployed for

24:40.480 --> 24:42.720
and what things they should not be deployed for

24:42.720 --> 24:43.920
based on these intuitions?

24:43.920 --> 24:44.840
Those are the kinds of things

24:44.840 --> 24:46.680
that I'm really, really interested in.

24:46.680 --> 24:48.760
Awesome, awesome, awesome.

24:48.760 --> 24:50.040
Is that your next section?

24:50.040 --> 24:52.440
Yes, and that sort of ties in very well

24:52.440 --> 24:55.040
with what I think is exciting next,

24:55.040 --> 24:58.560
which is, I'm gonna call it sort of

24:58.560 --> 25:01.280
understanding the relationship between the data,

25:01.280 --> 25:04.720
the pre-training data and the output of the model.

25:04.720 --> 25:06.360
And I feel like there is, again,

25:06.360 --> 25:08.200
a few different threads here,

25:08.200 --> 25:10.000
but there is one that came out of my group

25:10.000 --> 25:12.880
that I think is a simple idea

25:12.880 --> 25:16.120
that really sort of captures exactly what you said,

25:16.120 --> 25:19.120
the cutting pasting versus a reasoning thing.

25:19.120 --> 25:19.960
So this paper is called

25:19.960 --> 25:22.400
Impact of Pre-training Term Frequencies

25:22.400 --> 25:23.640
on Few-Shot Reasoning.

25:24.520 --> 25:26.680
And the idea here is we were looking

25:26.680 --> 25:28.640
only at numerical reasoning right now.

25:28.640 --> 25:32.040
So we started looking at all of these examples

25:32.040 --> 25:36.520
of, hey, GPT-3 can do addition and multiplication

25:36.520 --> 25:37.760
and things like that.

25:37.760 --> 25:39.680
And we started looking at the instances

25:39.680 --> 25:43.320
and turns out that it doesn't always do it, right?

25:43.320 --> 25:44.720
It's not 100% of those.

25:44.720 --> 25:49.240
It's 80% or 90% or whatever the number is.

25:49.240 --> 25:50.480
So we started looking at, okay,

25:50.480 --> 25:54.840
what differentiates the one it gets correct in

25:54.840 --> 25:57.280
to things that it doesn't get correct in, right?

25:57.280 --> 26:00.400
So for example, we saw that if you ask it,

26:00.400 --> 26:04.240
what is 24 times 18, the model gets it right.

26:04.240 --> 26:05.880
It says 432.

26:05.880 --> 26:09.160
If you say, what is 23 times 18,

26:09.160 --> 26:11.440
the model gets it wrong, right?

26:11.440 --> 26:13.360
So 24 times 18 is correct.

26:13.360 --> 26:15.640
23 times 18 is not correct.

26:15.640 --> 26:16.720
Is this random?

26:16.720 --> 26:18.000
Like, what's going on here?

26:18.000 --> 26:20.000
And did you, just to interrupt there,

26:20.000 --> 26:24.720
did you find that consistent across invocations?

26:24.720 --> 26:27.560
I've run into that kind of thing.

26:27.560 --> 26:28.720
We've all run into that kind of thing

26:28.720 --> 26:30.640
playing with chat GPT and other things.

26:30.640 --> 26:34.120
And sometimes it gets certain things consistently wrong.

26:34.120 --> 26:36.880
Other times it gets the thing wrong sometimes

26:36.880 --> 26:38.120
and not wrong other times,

26:38.120 --> 26:39.720
like it's a random seed kind of thing

26:39.720 --> 26:41.760
or something else going on in the model.

26:41.760 --> 26:43.240
Did you explore that at all?

26:43.240 --> 26:44.640
Yeah, we definitely saw that.

26:44.640 --> 26:47.760
So it's both like, if you're doing few short prompting,

26:47.760 --> 26:49.680
which examples you put in the prompt

26:49.680 --> 26:51.800
would sometimes change to the output

26:51.800 --> 26:52.960
or how you phrase it.

26:52.960 --> 26:55.400
Like, do you say what is 24 times 18

26:55.400 --> 26:58.520
or what is 24 X 18, you know, things like that.

26:58.520 --> 27:00.600
Definitely made a difference.

27:00.600 --> 27:03.120
But even after averaging these things out,

27:04.280 --> 27:07.000
we saw that 24 times 18 was in general

27:07.000 --> 27:09.080
more accurate than 23 times 18.

27:09.920 --> 27:13.480
And even more than that, we did even further analysis.

27:13.480 --> 27:17.320
And it turns out that all of our instances

27:17.320 --> 27:21.720
that involve 24, the model was much more accurate on

27:21.720 --> 27:24.640
than all of the instances that involve 23.

27:24.640 --> 27:28.520
Well, so we decided to do this for everything

27:28.520 --> 27:30.720
from zero to 100.

27:31.600 --> 27:33.400
So all two-digit numbers essentially,

27:33.400 --> 27:35.000
single and two-digit numbers.

27:35.000 --> 27:36.400
And no, it's a whole spectrum.

27:36.400 --> 27:38.040
And we didn't see a clear reason

27:38.040 --> 27:40.560
why some things are low accuracy,

27:40.560 --> 27:42.840
some things are high accuracy.

27:42.840 --> 27:44.960
And so then what we decided to do,

27:44.960 --> 27:48.480
this is the part that I think I got quite excited about.

27:48.480 --> 27:50.760
We started, decided to count how many times

27:50.760 --> 27:52.640
do each number, each of these numbers

27:52.640 --> 27:54.240
appear in the pre-training data.

27:55.360 --> 27:58.720
And turns out, and you can see the plot in the figure,

27:59.760 --> 28:03.360
if you plot the log of the frequency of these terms

28:03.360 --> 28:05.440
and now how accurate the models are,

28:05.440 --> 28:09.880
it is pretty much exactly like a-

28:09.880 --> 28:11.600
Which is intuitive.

28:11.600 --> 28:14.600
The model does better on things that it sees a lot of, right?

28:14.600 --> 28:16.080
Yes, yeah.

28:16.080 --> 28:20.080
But yeah, so it's also expected yet disappointing

28:20.080 --> 28:23.840
because you don't want it to be such a nice, strong curve.

28:23.840 --> 28:25.080
Like you want it to,

28:25.080 --> 28:27.560
like if it's doing mathematical reasoning,

28:27.560 --> 28:30.040
it should know that 23 is one less than 24

28:30.040 --> 28:31.480
and all of these things, right?

28:31.480 --> 28:35.000
So I think it's one of these things

28:35.000 --> 28:38.360
where it was expected that the model would be better

28:38.360 --> 28:40.040
at things it has seen before.

28:40.040 --> 28:43.120
But you also at the same time hold this thing of like,

28:43.120 --> 28:45.560
oh, it is able to reason, it is able to do these things.

28:45.560 --> 28:48.960
And it's kind of difficult to resolve both of those, right?

28:48.960 --> 28:50.960
So this was one example.

28:50.960 --> 28:53.040
I think this, we are barely scratching the surface,

28:53.040 --> 28:54.600
but this was an example of paper

28:54.600 --> 28:56.360
that sort of started looking at

28:56.360 --> 28:59.520
some of these pre-training statistics.

28:59.520 --> 29:01.560
So not just single term frequencies,

29:01.560 --> 29:03.800
but bigram frequencies and things like that

29:03.800 --> 29:07.880
and showed that the model is quite sensitive

29:07.880 --> 29:10.000
to what these things should be, right?

29:10.000 --> 29:11.960
And I don't want to sort of make a claim

29:11.960 --> 29:13.920
that there is cutting and pasting going on

29:13.920 --> 29:16.000
or any of these things,

29:16.000 --> 29:18.000
but this effect is so strong

29:18.000 --> 29:20.240
that at least when we think about reasoning

29:20.240 --> 29:22.840
and when we are evaluating reasoning

29:22.840 --> 29:24.680
in these language models,

29:24.680 --> 29:27.800
we should be taking this effect into account.

29:28.760 --> 29:31.120
And this may be a side note,

29:31.120 --> 29:34.680
it looks like the model that you evaluated with GPTJ

29:34.680 --> 29:39.680
and clearly that's a model,

29:39.960 --> 29:40.840
it's an open source model

29:40.840 --> 29:43.320
that you had access to the pre-training data,

29:43.320 --> 29:45.880
kind of asked questions about

29:45.880 --> 29:49.000
how do you get the same kind of insight

29:49.000 --> 29:52.480
into these models that are behind APIs?

29:52.480 --> 29:54.800
Yeah, so I think the question is like,

29:54.800 --> 29:58.760
I kind of don't mind that models are behind APIs

29:58.760 --> 30:00.160
to some degree that commercially

30:00.160 --> 30:01.720
that that kind of makes sense.

30:01.720 --> 30:04.040
I feel it's a little bit disappointing

30:04.040 --> 30:05.880
that the training data also

30:05.880 --> 30:08.520
is behind sort of closed wall, right?

30:08.520 --> 30:11.360
So like, I know that there is a lot in the training data,

30:11.360 --> 30:13.240
but if you want to be able to understand

30:13.240 --> 30:16.120
like why chat GPT works or why GPT works

30:16.120 --> 30:19.720
or even generally like when do language models work?

30:19.720 --> 30:21.120
When are they safe to deploy?

30:21.120 --> 30:22.680
All of these kind of questions.

30:22.680 --> 30:24.920
I think it's okay if the language model

30:24.920 --> 30:26.920
we only have a black box access to,

30:26.920 --> 30:29.720
but it would be good to have access to the training data.

30:29.720 --> 30:31.080
It would be good to have access

30:31.080 --> 30:32.280
to a bunch of these other things

30:32.280 --> 30:35.240
that can help us sort of do simple kind of analysis

30:35.240 --> 30:37.720
like this and maybe more complex ones

30:37.720 --> 30:41.280
and actually be able to sort of decide

30:41.280 --> 30:42.520
what to do with the model.

30:42.520 --> 30:45.920
So I think this whole direction of trying to understand

30:45.920 --> 30:47.320
what's in the pre-training data,

30:47.320 --> 30:51.200
I think is key and something that will persist

30:51.200 --> 30:52.800
for the next couple of years.

30:52.800 --> 30:57.040
Do you think we have the right tools to do that at scale?

30:57.040 --> 31:00.480
I'm imagining that was not an easy task

31:00.480 --> 31:03.960
to do just for simple mathematical problems.

31:03.960 --> 31:05.320
That's true.

31:05.320 --> 31:09.440
But training GPT-3 is also not a simple problem

31:09.440 --> 31:10.920
and people have solved it, right?

31:10.920 --> 31:12.800
So I think as, yeah.

31:12.800 --> 31:14.360
So I think the tooling is something

31:14.360 --> 31:18.400
that everybody right now is sort of excited

31:18.400 --> 31:21.080
about building tools that actually give information

31:21.080 --> 31:23.000
and insights into these language models.

31:23.000 --> 31:25.760
And I think, even at AI2,

31:25.760 --> 31:27.200
we are at sort of early stages

31:27.200 --> 31:28.320
of trying to do these things

31:28.320 --> 31:30.440
or building some tooling that can support

31:30.440 --> 31:31.680
this kind of analysis.

31:31.680 --> 31:34.040
But if the data set is available,

31:34.040 --> 31:35.800
I think people will do amazing things.

31:35.800 --> 31:39.280
And I thought this would be impossible

31:39.280 --> 31:40.760
and it seemed like crazy.

31:40.760 --> 31:43.400
Like, hey, this is almost a terabyte of text.

31:43.400 --> 31:45.440
Like, how can you do anything with that?

31:45.440 --> 31:49.760
And it was not trivial, but it was easier than impossible.

31:50.720 --> 31:53.440
How do you identify,

31:54.800 --> 31:56.800
so you identified some behavior,

31:58.400 --> 32:00.240
the relationship between accuracy

32:00.240 --> 32:04.440
and frequency in the training data.

32:04.440 --> 32:09.440
How do you identify what that is a consequence of?

32:11.800 --> 32:15.600
Meaning, is it specific to the way GPT-J was trained?

32:15.600 --> 32:18.800
Is it all transformer-based language models?

32:18.800 --> 32:23.040
Is it maybe something about that particular data set?

32:23.040 --> 32:27.000
Like, are you able to say

32:27.000 --> 32:32.000
that it is a broad characteristic of LLMs in general

32:32.000 --> 32:34.800
based on the work that you've done thus far?

32:34.800 --> 32:38.440
That's a little bit difficult to sort of,

32:38.440 --> 32:40.560
yeah, that's a little bit difficult to measure,

32:40.560 --> 32:43.440
partly because we don't have data set available

32:43.440 --> 32:45.280
for too many models, right?

32:45.280 --> 32:49.040
So at least we tried the whole slew of the Luther models

32:49.040 --> 32:50.520
that were trained on the same data set

32:50.520 --> 32:53.440
and we saw similar effect

32:53.440 --> 32:55.640
on different model sizes essentially.

32:56.840 --> 32:58.480
And yeah, as data sets become,

32:58.480 --> 33:00.240
pre-training data sets become more standard,

33:00.240 --> 33:01.520
it's fairly trivial too.

33:01.520 --> 33:04.200
So we'll sort of extend this stuff.

33:04.200 --> 33:07.520
Since this paper, we also have sort of an online demo

33:07.520 --> 33:09.200
where we have a bunch of more tasks

33:09.200 --> 33:12.040
that try to go beyond mathematical reasoning.

33:12.040 --> 33:15.120
It's a little bit difficult to sort of even define

33:15.120 --> 33:16.880
what these sort of terms are

33:16.880 --> 33:19.840
and what you should be computing frequency of.

33:19.840 --> 33:22.520
But yeah, I think we should be able to do this stuff

33:22.520 --> 33:25.120
for other tasks and for other models.

33:25.120 --> 33:29.160
And to me, I think this is somehow a consequence

33:29.160 --> 33:32.560
of a language modeling loss that encourages this

33:32.560 --> 33:33.400
in some sense, right?

33:33.400 --> 33:36.800
So like, yes, the model has seen more

33:36.800 --> 33:38.200
and it'll be more accurate,

33:38.200 --> 33:40.280
but even the ones that it has seen less,

33:40.280 --> 33:42.480
like it has still seen billions of times.

33:42.480 --> 33:45.400
So there is no reason for it to be wrong on it,

33:45.400 --> 33:47.520
except for the fact that the language modeling loss

33:47.520 --> 33:49.960
would sort of want you to be more right

33:49.960 --> 33:51.560
on the ones that has seen more.

33:54.400 --> 33:56.320
You had another paper identified

33:56.320 --> 33:58.600
out of Yav Goldberg's group.

33:58.600 --> 34:01.160
Oh yeah, so this is work led by Yanai.

34:01.160 --> 34:02.640
I think I'll quickly talk about this.

34:02.640 --> 34:07.280
So this had a similar sort of intuition

34:07.280 --> 34:09.800
for like trying to look at things in the data

34:09.800 --> 34:12.400
and trying to figure out like why the model

34:12.400 --> 34:16.960
has certain biases or has certain errors.

34:16.960 --> 34:18.840
And this was sort of a little bit more

34:18.840 --> 34:23.840
on trying to identify when two entities are related, right?

34:23.880 --> 34:27.360
So if you say, where was Barack Obama born?

34:27.360 --> 34:30.080
The model tends to say Chicago

34:31.040 --> 34:33.880
or in some sense it can say Washington

34:33.880 --> 34:37.280
and depending on how you phrase it.

34:37.280 --> 34:39.680
And like, why does it give the wrong answer?

34:39.680 --> 34:41.480
Is kind of the question.

34:41.480 --> 34:45.480
Why does it not say Hawaii or something?

34:45.480 --> 34:49.680
And I think to be able to answer this question,

34:49.680 --> 34:51.560
you have to go back to the pre-training data

34:51.560 --> 34:53.920
and try to see like, okay, what did it even see?

34:53.920 --> 34:55.280
So what I like about this paper

34:55.280 --> 34:59.520
is it kind of tries to build use causality tools

34:59.520 --> 35:01.120
and builds a whole causal graph

35:01.120 --> 35:05.040
for where these kinds of predictions might have come from.

35:05.040 --> 35:07.360
And then tries to estimate all of the edges

35:07.360 --> 35:08.480
in those causality graph

35:08.480 --> 35:10.280
and tries to do some causal inference

35:10.280 --> 35:15.160
to sort of attribute it to specific statistics

35:15.160 --> 35:16.280
of the pre-training data.

35:16.280 --> 35:21.280
So in this causal graph, would each individual document

35:21.280 --> 35:25.640
in the pre-training data be an intervention of sorts?

35:25.640 --> 35:27.240
So they sort of worked,

35:27.240 --> 35:30.360
they worked at the level of, I guess,

35:30.360 --> 35:32.160
triples or something like that, right?

35:32.160 --> 35:35.760
So let's say you see Obama in Chicago

35:35.760 --> 35:37.400
being a Senator there or something, right?

35:37.400 --> 35:40.280
So this is kind of a triple.

35:40.280 --> 35:43.280
And so they work on statistics of those triples

35:43.280 --> 35:45.800
of the pre-training data to sort of make it tractable

35:45.800 --> 35:49.080
and make it sort of allow this inference to work, yeah.

35:49.080 --> 35:52.040
But in applying the causality machinery,

35:52.040 --> 35:54.640
like are each of those interventions

35:54.640 --> 35:59.640
relative to some prior relationship

35:59.760 --> 36:03.320
between the things, the triples?

36:03.320 --> 36:05.520
Yeah, so there is the true relationship

36:05.520 --> 36:06.640
between these triples

36:06.640 --> 36:08.760
and then there is the observed relationship

36:08.760 --> 36:10.400
between these triples.

36:10.400 --> 36:13.000
And how many of these things,

36:13.000 --> 36:16.480
how many times it appeared in the pre-training data.

36:16.480 --> 36:19.280
And so the idea would be when you're doing it

36:19.280 --> 36:22.280
over many different entities and many different relations,

36:23.440 --> 36:25.720
do you, so those kind of become

36:25.720 --> 36:29.000
your whole data set in some sense.

36:29.000 --> 36:32.040
So Obama has appeared in Chicago,

36:32.040 --> 36:34.600
but Hillary Clinton has appeared elsewhere

36:34.600 --> 36:36.000
and on all of these things.

36:36.000 --> 36:39.840
And then together, which of these causal,

36:39.840 --> 36:43.520
which of these relations seem to affect

36:43.520 --> 36:45.600
a specific prediction the most?

36:45.600 --> 36:46.520
That kind of stuff.

36:46.520 --> 36:47.680
Awesome, awesome.

36:48.560 --> 36:52.640
Kind of continuing on in the data theme,

36:52.640 --> 36:55.360
there's been a ton of work looking at

36:55.360 --> 36:58.120
the need for clean data.

36:58.120 --> 37:00.240
I think maybe one of the most surprising things for me

37:00.240 --> 37:05.240
is like the return of supervision at the scale of LLMs.

37:05.560 --> 37:08.040
Talk a little bit about this category.

37:08.040 --> 37:10.800
Yeah, so this was somehow the most surprising category

37:10.800 --> 37:12.680
for me for this year.

37:12.680 --> 37:16.000
I will say that like after GPT came out

37:16.000 --> 37:17.840
and at the end of last year,

37:17.840 --> 37:21.080
everybody was kind of excited about language models,

37:21.080 --> 37:24.160
but the solutions for what's next always seem to be like,

37:24.160 --> 37:27.040
hey, let's get more data and let's get larger models

37:27.040 --> 37:28.680
and let's train, train longer.

37:28.680 --> 37:30.440
And those are still sort of useful things

37:30.440 --> 37:31.760
nobody's denying.

37:31.760 --> 37:33.560
But this year has shown that like,

37:33.560 --> 37:35.560
hey, you can actually do a lot

37:35.560 --> 37:39.120
if you're a little bit careful about your data, right?

37:39.120 --> 37:42.160
And maybe if you start cleaning up your data

37:42.160 --> 37:45.360
and try to think a little bit about where the data,

37:45.360 --> 37:47.120
your pre-training data should come from,

37:47.120 --> 37:49.520
your pre-training data itself,

37:49.520 --> 37:51.560
that could be quite interesting.

37:51.560 --> 37:55.120
So when you think of like RLHF as an example,

37:55.120 --> 37:57.440
do you think of that as fundamentally

37:57.440 --> 37:59.040
just cleaning up your data,

37:59.040 --> 38:01.840
being more careful about your data as opposed to?

38:01.840 --> 38:04.880
Yeah, so no, I think I was thinking more

38:04.880 --> 38:07.640
what happened with the Bloom language model,

38:07.640 --> 38:12.640
which was trained on sort of a lot more thoughtful process

38:12.800 --> 38:14.400
of gathering the data set.

38:14.400 --> 38:16.480
Because I mean, partly because they documented it

38:16.480 --> 38:18.800
and we know what they went through.

38:18.800 --> 38:22.840
But no, like RLHF and those kinds of things,

38:22.840 --> 38:26.800
I think are examples of showing that the language models

38:26.800 --> 38:29.440
are not quite ready for use case,

38:29.440 --> 38:32.680
just based on pre-training on sort of large data

38:32.680 --> 38:34.680
that has been gathered.

38:34.680 --> 38:36.600
You need to reinforce,

38:36.600 --> 38:38.720
like you can call it like, hey, cleaning up the data,

38:38.720 --> 38:41.640
but I think of it as like maybe reinforcing

38:41.640 --> 38:44.480
some of the nice signals in the data

38:44.480 --> 38:46.160
by having these examples,

38:46.160 --> 38:48.320
or in some sense, people have been fine tuning

38:48.320 --> 38:51.280
on this sort of super-based data as well.

38:51.280 --> 38:55.760
And the gains that you get from RLHF

38:55.760 --> 38:58.800
have become extremely evident this year, right?

38:58.800 --> 39:03.160
So somehow that has become the secret sauce of OpenAI

39:03.160 --> 39:04.680
and of all of these companies

39:04.680 --> 39:08.320
that want to have a really strong language models

39:08.320 --> 39:11.960
rather than scale and just raw pre-training.

39:13.080 --> 39:14.320
And for completeness,

39:14.320 --> 39:16.720
we've talked a little bit about RLHF on the show,

39:16.720 --> 39:20.160
but how do you think about it as a researcher?

39:20.160 --> 39:21.320
I think it's quite exciting.

39:21.320 --> 39:22.720
I think it sort of addresses

39:22.720 --> 39:25.600
a lot of my concerns with language models.

39:25.600 --> 39:29.720
I don't think pre-training data can be trusted, right?

39:29.720 --> 39:31.760
And you shouldn't just train something

39:31.760 --> 39:34.680
and expect the model to have clean output

39:34.680 --> 39:39.680
or have your values and any of these kinds of things,

39:41.000 --> 39:43.400
whatever that means in the context of large language models.

39:43.400 --> 39:46.760
But essentially, if you want real users

39:46.760 --> 39:49.800
to be interfacing with language models,

39:49.800 --> 39:54.120
you need to make sure that there is some sort of check.

39:54.120 --> 39:57.280
And RLHF is not the solution, like a full solution,

39:57.280 --> 40:00.720
but at least there is a way to sort of say,

40:00.720 --> 40:02.480
okay, this is the actual task.

40:02.480 --> 40:05.240
Your actual task is to be interfacing with humans,

40:05.240 --> 40:08.080
not just regurgitating what you've seen

40:08.080 --> 40:09.800
in the pre-training corpus, right?

40:09.800 --> 40:11.840
And so that intuition sort of is captured

40:11.840 --> 40:13.960
by using RLHF.

40:13.960 --> 40:17.080
And do you remember offhand any of the,

40:17.080 --> 40:18.320
if they were even published,

40:18.320 --> 40:22.040
the stats in terms of the number of prompts,

40:22.040 --> 40:26.000
like human generated prompts that were used in chat GPT

40:26.000 --> 40:27.640
or in struct GPT?

40:27.640 --> 40:30.320
Yeah, I don't think they were published as far as I know.

40:30.320 --> 40:33.560
Yeah, I don't remember exactly what they are.

40:33.560 --> 40:35.760
I think in struct GPT had the documentation

40:35.760 --> 40:37.560
of sort of how they were gathered,

40:37.560 --> 40:39.880
but the size was like, you know,

40:39.880 --> 40:42.480
how many of them were sort of generation tasks

40:42.480 --> 40:45.400
versus classification tasks, things like that.

40:45.400 --> 40:48.960
But I don't think the exact data set is available.

40:48.960 --> 40:53.960
Do you have a guess as to like the relative cost

40:54.680 --> 40:57.600
of collecting the human feedback

40:57.600 --> 41:00.320
relative to the cost of training the models?

41:00.320 --> 41:02.400
Oh, relative cost of training the models.

41:03.880 --> 41:05.160
I think it's much cheaper.

41:05.160 --> 41:08.400
Order of magnitude, or is it like much, much, much cheaper?

41:08.400 --> 41:10.080
Because we always say like, you know,

41:10.080 --> 41:11.200
collecting the data,

41:11.200 --> 41:13.480
label data is the most expensive part of machine learning.

41:13.480 --> 41:16.800
Is that still true at the scale of LLMs?

41:17.680 --> 41:20.880
Or is it that RLHF is like extremely efficient

41:20.880 --> 41:23.520
and you just need a little bit of guidance

41:23.520 --> 41:27.280
on top of the, you know, the pre-training data?

41:27.280 --> 41:29.560
I feel the true answer is somewhere in between.

41:29.560 --> 41:32.960
So I don't think it's like, it's nowhere very little data.

41:32.960 --> 41:35.680
Like, I think you need a lot of data to be able to do it,

41:35.680 --> 41:37.000
but I don't think it comes close,

41:37.000 --> 41:38.960
at least the way these are trained right now,

41:38.960 --> 41:39.880
I don't think it comes close

41:39.880 --> 41:42.920
to sort of training the model itself, right?

41:42.920 --> 41:45.920
So, but like when you think about, you know,

41:45.920 --> 41:49.920
chat GPT, it's been released publicly

41:49.920 --> 41:51.720
and a lot of people are using it.

41:51.720 --> 41:54.120
A lot of that data is gonna go into,

41:54.120 --> 41:56.960
in some form back into the model and improve it.

41:56.960 --> 42:00.440
So was that expensive to collect?

42:00.440 --> 42:02.560
In some sense, because they had to run chat GPT,

42:02.560 --> 42:05.360
but you know, they'll probably pay some annotators

42:05.360 --> 42:06.200
to clean that up,

42:06.200 --> 42:07.640
but I don't think that's gonna compare

42:07.640 --> 42:09.040
to the actual training.

42:09.040 --> 42:14.040
It's also a really interesting example of like bootstrapping

42:14.640 --> 42:17.640
like there's a certain amount that they collected themselves

42:17.640 --> 42:19.920
you know, they instruct GPT work

42:19.920 --> 42:22.880
and then they, you know, created something

42:22.880 --> 42:25.240
that was good enough to set loose in the world.

42:25.240 --> 42:27.000
And now they've got this virtual cycle

42:27.000 --> 42:29.480
where I'm imagining it's a lot cheaper

42:29.480 --> 42:32.440
for some annotator to clean up what, you know,

42:32.440 --> 42:34.000
millions of people are creating

42:34.000 --> 42:36.880
than for them to create that themselves.

42:36.880 --> 42:39.400
And I think like, I think this year has also shown

42:39.400 --> 42:41.040
maybe even to people at OpenAI

42:41.040 --> 42:43.280
that the value of these things, right?

42:43.280 --> 42:45.120
Like when they released GPT-3,

42:45.120 --> 42:47.120
they probably didn't realize how valuable this would be.

42:47.120 --> 42:48.760
And then they sort of collected data,

42:48.760 --> 42:51.320
released instruct GPT and yeah, on their benchmarks,

42:51.320 --> 42:52.160
it was good.

42:52.160 --> 42:53.480
But once people started using it,

42:53.480 --> 42:55.280
you realize how much better it is.

42:55.280 --> 42:56.960
I think similarly with chat GPT,

42:56.960 --> 42:59.160
they probably knew how good it was,

42:59.160 --> 43:00.600
but they probably didn't realize

43:00.600 --> 43:03.360
how good it actually is, right?

43:03.360 --> 43:07.480
And I think this idea of human feedback

43:07.480 --> 43:10.440
being a secret sauce that is proprietary,

43:10.440 --> 43:15.080
I think can sort of will continue to be a bigger piece

43:15.080 --> 43:15.920
in the future.

43:17.760 --> 43:19.880
Talk a little bit about Roots.

43:19.880 --> 43:22.320
Yeah, so the Roots is this nice dataset

43:22.320 --> 43:25.000
that was gathered by the big science group.

43:25.000 --> 43:27.040
And I've been following the big science group

43:27.040 --> 43:31.440
and they've done a bunch of interesting things there.

43:31.440 --> 43:33.960
I guess I'll jump in to refer to the interview

43:33.960 --> 43:35.360
that I did with Thomas Wolfe

43:35.360 --> 43:38.640
that I don't think Roots came up explicitly,

43:38.640 --> 43:40.000
but we talked about that work

43:40.000 --> 43:43.040
and that eventually resulted in Bloom,

43:43.040 --> 43:45.080
which we'll talk about a little bit more as well.

43:45.080 --> 43:48.400
Yeah, so Roots I like because I think I really like

43:48.400 --> 43:50.640
what Luther have done with the pile dataset

43:50.640 --> 43:52.640
by releasing the dataset that was used to train

43:52.640 --> 43:54.280
all the GPTJ models.

43:54.280 --> 43:57.000
And I think the big science group sort of took that intuition

43:57.000 --> 43:58.560
and sort of went further with it

43:58.560 --> 44:01.080
where they have a really well-documented

44:02.280 --> 44:03.840
and not just well-documented,

44:03.840 --> 44:06.880
I would say a very thoughtful process

44:06.880 --> 44:08.920
of gathering this dataset.

44:08.920 --> 44:12.560
It's multilingual over many, many different languages.

44:12.560 --> 44:14.760
They've been careful about sort of listing

44:14.760 --> 44:16.760
which sources they want to even crawl

44:16.760 --> 44:19.200
in the first place before.

44:19.200 --> 44:21.760
So it's not like a post hoc cleanup of the data.

44:21.760 --> 44:24.600
It's very sort of thinking about it.

44:24.600 --> 44:26.640
They gathered a dataset that is huge

44:26.640 --> 44:29.520
and they have, we talked a little bit about this data also,

44:29.520 --> 44:32.240
but Hugging Face has sort of built tools on top of it

44:32.240 --> 44:34.720
to be able to quickly search it,

44:34.720 --> 44:37.080
to see what's in it and stuff like that.

44:37.080 --> 44:39.880
And I kind of like that approach

44:39.880 --> 44:41.600
to large language models, right?

44:41.600 --> 44:45.880
So I think getting the right dataset

44:45.880 --> 44:47.800
is crucial for these language models

44:47.800 --> 44:50.680
and doing this documentation and stuff

44:50.680 --> 44:53.040
is good for in the long term.

44:53.040 --> 44:57.120
So your next category is decoding only.

44:57.120 --> 44:59.680
Talk a little bit about what that means.

44:59.680 --> 45:02.760
Yeah, so this is a theme that I like

45:02.760 --> 45:06.360
about some of the work that has come out here.

45:06.360 --> 45:09.480
And partly it's because we have these language models

45:09.480 --> 45:13.080
where we have this black box interface to them.

45:13.080 --> 45:15.360
And a lot of it is just prompting.

45:15.360 --> 45:17.200
So changing things on the input side

45:17.200 --> 45:19.160
to see what the model generates.

45:19.160 --> 45:21.760
And the only thing most people are changing

45:21.760 --> 45:23.000
on the output side is like,

45:23.000 --> 45:24.920
oh, let's change the temperature a little bit

45:24.920 --> 45:27.520
and we get a bunch of different things.

45:27.520 --> 45:29.240
But there has been a bunch of work looking at,

45:29.240 --> 45:31.000
okay, let's not just do that.

45:31.000 --> 45:33.960
Let's actually think about what's happening

45:33.960 --> 45:37.880
in the output of the model during decoding of the text.

45:37.880 --> 45:40.000
And maybe we can do smart things there

45:41.120 --> 45:44.640
that actually sort of change the output considerably, right?

45:44.640 --> 45:49.160
So some of these sort of came out sort of late last year.

45:49.160 --> 45:52.160
So there was this work on nucleus sampling,

45:52.160 --> 45:53.560
maybe that's a little bit older,

45:53.560 --> 45:54.600
but then there was this stuff

45:54.600 --> 45:57.520
on sort of constrained decoding as well,

45:57.520 --> 46:00.400
where the constraint decoding paper

46:00.400 --> 46:02.640
came out of semantic machines.

46:02.640 --> 46:06.040
They showed that you can have,

46:06.040 --> 46:08.520
suppose you want the language model

46:08.520 --> 46:10.160
to generate programs, right?

46:10.160 --> 46:13.080
So the programs come with a certain grammar, right?

46:13.080 --> 46:16.080
Like there is a syntax that they need to follow.

46:16.080 --> 46:18.960
So you could actually constrain the output

46:18.960 --> 46:22.720
of the language model as it's generating token by token

46:22.720 --> 46:27.040
to sort of adhere to that syntax in some sense, right?

46:27.920 --> 46:30.320
And just by doing this constraint, you can get,

46:30.320 --> 46:32.360
firstly, obviously you will get programs

46:32.360 --> 46:34.360
that are syntactically correct,

46:34.360 --> 46:37.360
but you can actually get the right things out of the model.

46:38.560 --> 46:42.400
And so there have been a lot of sort of works

46:42.400 --> 46:45.800
looking at how can we decode

46:45.800 --> 46:49.720
by having some constraints on the decoding, right?

46:49.720 --> 46:51.640
So one of the papers that came out this year

46:51.640 --> 46:56.640
that I believe got the best paper award as well

46:56.680 --> 47:01.680
is called Neuralogic A-star, A-star-esque decoding.

47:01.680 --> 47:03.680
And the idea here is that

47:03.680 --> 47:06.120
instead of just doing left to right decoding

47:06.120 --> 47:07.440
where you're being greedy

47:07.440 --> 47:09.920
or where you're being doing some kind of beam search

47:09.920 --> 47:12.040
or sampling or any of these process,

47:12.040 --> 47:15.840
why don't you actually use some of the computer science ideas

47:15.840 --> 47:19.120
that we have like A-star search

47:19.120 --> 47:22.220
and try to find the best possible decoding.

47:24.160 --> 47:25.600
And then when you're doing this kind of thing,

47:25.600 --> 47:27.360
you can also think about constraints

47:27.360 --> 47:29.320
that you might want to put on the decoding.

47:29.320 --> 47:30.800
So you want to say, look,

47:30.800 --> 47:34.960
I want the decoding to have these three words in it, right?

47:35.920 --> 47:38.680
Like A, you're generating a recipe,

47:38.680 --> 47:41.640
make sure that it has these five ingredients, right?

47:41.640 --> 47:43.560
Somewhere in the generated text.

47:44.400 --> 47:45.480
You can also flip it around,

47:45.480 --> 47:48.480
hey, generate whatever text you generate,

47:48.480 --> 47:51.560
make sure it doesn't have these specific words, right?

47:51.560 --> 47:52.800
Like that.

47:52.800 --> 47:57.800
And this paper sort of uses A-star during decoding

47:57.920 --> 48:01.560
to generate that text that sort of,

48:01.560 --> 48:04.060
you know, your constraints are satisfied.

48:05.080 --> 48:06.240
And this paper showed that, yeah,

48:06.240 --> 48:07.360
once you do that properly,

48:07.360 --> 48:10.880
you can actually do a lot of the tasks much better

48:10.880 --> 48:12.840
just by controlling decoding

48:12.840 --> 48:15.400
rather than changing much on the inputs.

48:15.400 --> 48:17.080
It seems like this is another example

48:17.080 --> 48:22.080
where it's predicated on having open access

48:22.440 --> 48:23.800
to the model internals

48:23.800 --> 48:27.900
and you potentially lose a lot if you don't.

48:27.900 --> 48:30.320
Yeah, I think, so from what I understand,

48:30.320 --> 48:32.400
you can still do these kinds of things

48:32.400 --> 48:35.200
with GPT-3 to some degree.

48:35.200 --> 48:37.440
I think what you need, okay,

48:37.440 --> 48:40.960
so you can do this with a black box model

48:40.960 --> 48:43.000
as long as you get the probabilities

48:43.000 --> 48:45.720
of all of the tokens at every step, right?

48:45.720 --> 48:48.680
So I don't think GPT-3 actually does that,

48:48.680 --> 48:51.720
but you know, you could imagine an API that says,

48:51.720 --> 48:53.600
okay, the next, here's the distribution

48:53.600 --> 48:54.960
over all of the tokens.

48:56.280 --> 49:00.400
And you should be still be able to do these kind of things.

49:00.400 --> 49:03.640
So, you know, some of the concerns is like,

49:03.640 --> 49:05.600
if you want decoding to be fast,

49:05.600 --> 49:08.200
then it's difficult to use some of these ideas.

49:08.200 --> 49:11.600
The A star one specificity is a lot slower,

49:12.440 --> 49:14.480
but it's able to satisfy your constraints.

49:14.480 --> 49:18.680
So it can be where you're okay to trade off some time,

49:18.680 --> 49:21.360
but let the model take more time

49:21.360 --> 49:24.280
in making sure the output is clean

49:24.280 --> 49:25.600
and satisfies your constraint.

49:25.600 --> 49:27.300
This could be really, really cool.

49:28.680 --> 49:30.840
And now, yeah, often you see,

49:30.840 --> 49:34.760
hey, we applied one method, A star in this case,

49:34.760 --> 49:37.320
let's go back to the computer science toolkit

49:37.320 --> 49:38.520
and apply everything else.

49:38.520 --> 49:40.440
Have we seen that here?

49:40.440 --> 49:41.280
Not yet.

49:41.280 --> 49:43.880
This, I think came out late enough in the year,

49:43.880 --> 49:46.320
but I guess it came out sort of early in the year,

49:46.320 --> 49:49.160
but yeah, we haven't seen that much yet because,

49:49.160 --> 49:50.000
but I think, yeah,

49:50.000 --> 49:52.400
that's the kind of thing that will happen next is like,

49:52.400 --> 49:54.680
okay, now this is, yeah,

49:54.680 --> 49:57.080
this is attracting a whole different kind of thinking

49:57.080 --> 49:59.880
where people were not thinking about decoding at all,

49:59.880 --> 50:01.920
and now they will be in this light,

50:01.920 --> 50:04.720
which is always a sign of good paper.

50:04.720 --> 50:05.560
Awesome, awesome.

50:05.560 --> 50:09.840
Well, those are great themes to kind of reflect on

50:09.840 --> 50:14.480
as we think about the past year and NLP research.

50:14.480 --> 50:19.160
Our next category is to talk about some of the new tools

50:19.160 --> 50:23.320
and open source projects that we saw in the year.

50:23.320 --> 50:26.400
We've already talked a little bit about datasets,

50:26.400 --> 50:28.060
which is kind of related,

50:29.440 --> 50:33.040
but I think the first thing you have here is OPT.

50:33.040 --> 50:34.680
Tell us about OPT.

50:34.680 --> 50:39.680
So yeah, I think OPT came out fairly early in this year,

50:40.040 --> 50:42.320
and I think it kind of surprised everyone

50:42.320 --> 50:46.920
because the sort of looking back at last year,

50:46.920 --> 50:49.880
there weren't that many open source reproductions

50:49.880 --> 50:51.480
of large sizes, right?

50:51.480 --> 50:55.000
So I think Luther AI was sort of leading it.

50:55.000 --> 50:57.160
GPT-J was 6 billion,

50:57.160 --> 51:00.240
and they were sort of growing it slowly and slowly,

51:00.240 --> 51:02.940
and they had got to 20 billion parameters.

51:02.940 --> 51:06.660
And then OPT sort of came into the scene,

51:06.660 --> 51:09.860
and there were a bunch of nice things.

51:09.860 --> 51:12.660
They documented a lot of their whole training process

51:12.660 --> 51:15.600
in a log book with sort of all kinds of insights

51:15.600 --> 51:17.420
about what training a log.

51:18.420 --> 51:21.100
Yes, it was released by Beta, right?

51:21.100 --> 51:24.420
And that was also, not to say too much against Beta,

51:24.420 --> 51:27.220
but it was also surprising that reproducibility

51:27.220 --> 51:30.740
and open source seemed to be key aspect of OPT as well.

51:30.740 --> 51:33.380
So that was kind of nice.

51:33.380 --> 51:36.940
And they also released a lot of models

51:36.940 --> 51:39.700
and like all different sizes,

51:40.660 --> 51:42.900
including 175 billion,

51:42.900 --> 51:46.780
which hadn't been available at all.

51:46.780 --> 51:47.620
And even right now,

51:47.620 --> 51:50.620
I think it's probably the most useful model

51:50.620 --> 51:53.180
if you want to do stuff with 175 billion

51:53.180 --> 51:55.740
is to use the OPT model, right?

51:55.740 --> 52:00.740
So I think the idea of documenting the whole training data,

52:01.420 --> 52:02.820
gathering process,

52:02.820 --> 52:05.900
documenting the whole training of the model process,

52:05.900 --> 52:07.980
and then releasing all of these models

52:09.620 --> 52:10.900
available for research,

52:10.900 --> 52:13.780
I think has helped the research community a lot.

52:13.780 --> 52:16.460
And I expect that if there are people

52:16.460 --> 52:18.580
who want to build models

52:18.580 --> 52:20.580
and potentially fine tune language models

52:20.580 --> 52:22.580
and do all of these things,

52:22.580 --> 52:24.740
the OPT would be a pretty big resource.

52:24.740 --> 52:27.580
Have you seen much in terms of benchmarking it

52:27.580 --> 52:29.780
against GPT-3?

52:29.780 --> 52:32.780
Yeah, so I think people have been benchmarking it

52:32.780 --> 52:36.060
and I think it performs reasonably well.

52:36.060 --> 52:37.980
The tricky thing is, of course,

52:37.980 --> 52:39.420
there is Instruct GPT,

52:39.420 --> 52:44.220
which is when you call GPT-3 on the API right now,

52:44.220 --> 52:47.100
it's often defaults to the Instruct one.

52:47.100 --> 52:49.340
And that one is a lot more difficult to beat,

52:49.340 --> 52:51.140
but for all of the purposes,

52:51.140 --> 52:52.540
I think of it as like, yeah,

52:52.540 --> 52:55.500
OPT is basically the same as GPT-3.

52:55.500 --> 53:00.500
We talked a little bit about the big science project

53:01.460 --> 53:04.500
and one of its outputs, another is Bloom.

53:05.660 --> 53:08.500
What did you, what was your take on Bloom?

53:08.500 --> 53:12.620
Bloom was again, a really big data model

53:12.620 --> 53:15.220
that was, I think, 180 billion parameters.

53:15.220 --> 53:20.100
So similar sizes, GPT-3 released to be completely open source

53:20.100 --> 53:22.940
it's like we talked about completely well-documented

53:22.940 --> 53:26.660
data process and sort of training process

53:26.660 --> 53:29.100
combined with the fact that this was done

53:29.100 --> 53:31.900
by a group of people just kind of just volunteering

53:31.900 --> 53:33.940
their time to do so.

53:33.940 --> 53:38.180
And then being able to reproduce to a large degree

53:38.180 --> 53:41.740
what OpenAI has done was quite amazing, right?

53:41.740 --> 53:45.180
And then sort of, again, like both OPTs releasing

53:45.180 --> 53:46.980
all of these things is kind of a sign

53:46.980 --> 53:49.660
for other big tech companies to say like,

53:49.660 --> 53:50.660
hey, you can do this

53:50.660 --> 53:52.860
because we have done this kind of thing.

53:52.860 --> 53:56.660
But what Bloom has shown is that a bunch of people

53:56.660 --> 54:00.460
enthusiastic and excited folks that are enterprising

54:00.460 --> 54:04.500
can actually do things that maybe even a year or two ago

54:04.500 --> 54:06.500
would have seemed impossible.

54:06.500 --> 54:11.500
It may have been in our trends conversation from last year

54:11.500 --> 54:16.500
or maybe it was prior, but in these kinds of conversations

54:19.660 --> 54:22.260
there was a point in time where we were lamenting

54:22.260 --> 54:25.020
the kind of the loss on the part

54:25.020 --> 54:28.500
of the individual academic researcher to contribute

54:28.500 --> 54:30.940
to fundamental model research

54:30.940 --> 54:33.580
because of the resources that were required

54:33.580 --> 54:38.580
and to hugging face and the big sciences system

54:38.580 --> 54:43.580
like they showed that, not necessarily, not so fast, right?

54:43.620 --> 54:45.420
Right, right, right, exactly.

54:45.420 --> 54:48.380
And the other thing I like about this, the Bloom effort

54:48.380 --> 54:50.540
is and the corpus that came with it,

54:50.540 --> 54:53.700
they were also focused on being a lot more inclusive

54:53.700 --> 54:56.340
in terms of having a global perspective.

54:56.340 --> 54:59.460
So they would try to cover many, many different languages.

54:59.460 --> 55:02.300
Very principled in the way they pulled the data together.

55:02.300 --> 55:04.020
Yeah, yeah.

55:04.020 --> 55:05.460
And also multilingual in a way

55:05.460 --> 55:08.100
that none of the experts would have been able to do.

55:08.100 --> 55:10.140
None of the existing models have been.

55:10.140 --> 55:12.580
So yeah, it was quite exciting.

55:12.580 --> 55:16.100
And so like conceptually, this is a great example

55:16.100 --> 55:21.100
of how one model at 175 billion parameters

55:23.420 --> 55:26.380
and another model, the same number of parameters

55:26.380 --> 55:29.700
could be very different, at least in the data

55:29.700 --> 55:30.660
that they were trained on.

55:30.660 --> 55:33.100
And you would expect that to result

55:33.100 --> 55:37.140
in very different results using the model.

55:37.140 --> 55:40.300
To what extent have we characterized that?

55:40.300 --> 55:43.420
Like at that scale of data, it's still a lot of data.

55:43.420 --> 55:46.820
It's still a lot of like raw internet data.

55:46.820 --> 55:49.220
Does it all kind of fall out in the wash

55:49.220 --> 55:51.420
and all their efforts at being principled

55:52.980 --> 55:54.540
kind of just get lost?

55:54.540 --> 55:57.780
Or do we know how to compare that?

55:57.780 --> 56:00.220
Yeah, so there have been a bunch of benchmarks

56:00.220 --> 56:02.740
including in their papers, but in general also.

56:02.740 --> 56:07.060
And that's where sort of the, don't hold me to this,

56:07.060 --> 56:11.060
but I would say like Bloom is not the go-to language model

56:11.060 --> 56:14.700
for people if they want to do English things right now.

56:14.700 --> 56:17.340
So I think maybe some of the trade-offs

56:17.340 --> 56:19.060
they made in collecting the data

56:19.060 --> 56:21.780
or even just having more languages

56:22.780 --> 56:24.900
resulted in a model that's definitely really good

56:24.900 --> 56:29.460
for multilingual things, but that's not what our benchmarks

56:29.460 --> 56:32.220
have been designed for, unfortunately.

56:32.220 --> 56:34.900
And so if you just look at the benchmarks,

56:34.900 --> 56:38.540
which are traditionally designed for English,

56:38.540 --> 56:43.540
Bloom, I don't think quite is at par with OPD or GPTC

56:44.780 --> 56:46.460
and definitely not with Instruct.

56:49.020 --> 56:51.260
And when I mentioned benchmark,

56:51.260 --> 56:54.180
there's that aspect of kind of applying

56:54.180 --> 56:58.820
the traditional performance benchmarks for LLMs to Bloom

56:58.820 --> 57:02.180
and comparing their results to the others.

57:02.180 --> 57:07.180
But I'm also curious about how we characterize

57:07.700 --> 57:09.340
like qualitative differences

57:09.340 --> 57:14.340
between the way Bloom responds and the way GPT responds,

57:15.820 --> 57:20.820
for example, in terms of like the kind of fairness

57:22.540 --> 57:27.540
considerations or that kind of thing,

57:27.980 --> 57:30.580
or are there qualitative differences

57:30.580 --> 57:33.980
in the kinds of responses that you get

57:33.980 --> 57:37.380
that aren't picked up by the traditional benchmarks

57:37.380 --> 57:41.540
or are the traditional benchmarks like so expansive

57:41.540 --> 57:44.300
at this point we've kind of characterized

57:44.300 --> 57:46.500
a lot of that stuff explicitly?

57:46.500 --> 57:48.780
Yeah, again, I think the answer is somewhere in between.

57:48.780 --> 57:51.180
So I don't know if people have thoroughly compared the two

57:51.180 --> 57:53.460
to see like, hey, what's the level of toxicity

57:53.460 --> 57:54.620
and things like that.

57:55.820 --> 57:57.060
I think when OPD came out,

57:57.060 --> 57:59.260
they did a lot of this analysis in their paper

57:59.260 --> 58:01.340
of like, hey, how toxic is that model?

58:01.340 --> 58:03.020
How safe is that model?

58:03.020 --> 58:04.620
And they realized that, yeah,

58:04.620 --> 58:06.100
in some things they were worse off

58:06.100 --> 58:10.060
than some of the existing models.

58:10.060 --> 58:14.260
But I think with Bloom specifically,

58:14.260 --> 58:16.300
I don't know off the top of my head

58:16.300 --> 58:21.300
how it's all compared in terms of these other aspects.

58:22.260 --> 58:25.660
Okay, talk about the inverse scaling competition.

58:25.660 --> 58:28.540
Yeah, so this was a pretty nice thing that came out.

58:28.540 --> 58:30.580
And I think, I suppose it's still going on

58:30.580 --> 58:32.060
even though the submissions are down.

58:32.060 --> 58:33.860
So I'm kind of hoping to see

58:33.860 --> 58:36.460
what the actual effect of this was.

58:36.460 --> 58:37.820
But this was sort of introduced

58:37.820 --> 58:40.380
sort of in the middle of the year.

58:40.380 --> 58:44.500
And the idea here is the thinking of things

58:44.500 --> 58:47.300
like what sort of scaling laws was showing, right?

58:47.300 --> 58:49.980
Like when you scale up your models,

58:49.980 --> 58:52.460
performance goes up for everything.

58:52.460 --> 58:53.900
And that's kind of exciting to see,

58:53.900 --> 58:57.180
but it also tells us that, okay,

58:57.180 --> 58:59.340
there are many, many things that just,

58:59.340 --> 59:02.220
the models would just get better on as time goes by

59:02.220 --> 59:05.700
because they'll get bigger, they'll have more data set.

59:05.700 --> 59:08.260
The inverse scaling was this intuition to see,

59:08.260 --> 59:11.260
okay, what are, can we characterize the phenomenas

59:11.260 --> 59:15.860
that don't have the same trend, right?

59:15.860 --> 59:20.020
So other aspects, you create a data set,

59:20.020 --> 59:21.820
which is something everybody will agree

59:21.820 --> 59:23.300
is a reasonable data set.

59:23.300 --> 59:27.260
But when you give them to larger models,

59:27.260 --> 59:30.540
they actually get worse.

59:30.540 --> 59:33.420
And so this prize in this competition

59:33.420 --> 59:38.420
is an effort to identify what those tasks would be

59:39.700 --> 59:43.180
and sort of the better your inverse scaling is.

59:43.180 --> 59:46.260
So the worse, the bigger models are on your,

59:46.260 --> 59:48.540
on the data set that you've contributed,

59:48.540 --> 59:51.940
the more likely you are to win this competition.

59:51.940 --> 59:53.420
And so, yeah, they've had the submissions

59:53.420 --> 59:55.620
and they're kind of evaluating them, I suppose,

59:55.620 --> 59:57.940
and they haven't quite announced it.

59:57.940 --> 01:00:00.380
But I think a lot of the stuff on,

01:00:01.700 --> 01:00:03.060
a lot of the interesting things

01:00:03.060 --> 01:00:06.260
could come out of this effort.

01:00:06.260 --> 01:00:07.900
So one thing I could imagine

01:00:07.900 --> 01:00:11.620
is sort of deeper levels of misinformation

01:00:11.620 --> 01:00:15.020
where the model is relying so much

01:00:15.020 --> 01:00:18.300
on what it has seen in its training data.

01:00:18.300 --> 01:00:19.460
Let's not call it misinformation,

01:00:19.460 --> 01:00:22.740
just not being able to update its information

01:00:22.740 --> 01:00:23.580
in some sense, right?

01:00:23.580 --> 01:00:25.700
So these large language models

01:00:25.700 --> 01:00:28.420
have memorized so much about the pre-training data

01:00:28.420 --> 01:00:32.780
that they kind of reject evidence against that, right?

01:00:32.780 --> 01:00:33.860
Maybe if they're smaller,

01:00:33.860 --> 01:00:35.060
there's less memorization

01:00:35.060 --> 01:00:37.260
and more generalization in some way.

01:00:37.260 --> 01:00:38.700
But I think it could be pretty exciting

01:00:38.700 --> 01:00:39.980
to see what are those things

01:00:39.980 --> 01:00:42.180
that actually get worse with scale.

01:00:42.180 --> 01:00:44.940
I think it's quite an interesting question.

01:00:44.940 --> 01:00:48.380
And next up you have the Galactica,

01:00:48.380 --> 01:00:49.860
can we call it a debacle?

01:00:50.900 --> 01:00:54.820
So Galactica is this LLM that Meta released

01:00:54.820 --> 01:00:59.820
that was tuned to generate scientific and research text.

01:01:02.460 --> 01:01:07.460
And was it even up for three days?

01:01:08.020 --> 01:01:10.100
It got pulled down pretty quickly, right?

01:01:10.100 --> 01:01:12.860
Yep, yeah, I think maybe a little bit more than that,

01:01:12.860 --> 01:01:14.900
but yeah, thereabouts, yeah.

01:01:14.900 --> 01:01:19.300
And I think to me, it's a story

01:01:19.300 --> 01:01:22.060
about how not in anything in terms

01:01:22.060 --> 01:01:24.180
of what the Galactica team did itself, right?

01:01:24.180 --> 01:01:26.420
Like I think the model training it,

01:01:26.420 --> 01:01:29.540
everything was the right thing to be doing.

01:01:30.780 --> 01:01:34.260
The tricky thing was just how it was pitched

01:01:34.260 --> 01:01:39.260
and how there was just not clear caveats

01:01:40.820 --> 01:01:44.300
about what this model is capable of doing

01:01:44.300 --> 01:01:46.820
and what it's not capable of doing

01:01:46.820 --> 01:01:49.620
that led to such a backlash, right?

01:01:49.620 --> 01:01:54.620
So I think it was a language model train

01:01:56.020 --> 01:01:57.660
on a lot of science papers.

01:01:57.660 --> 01:01:59.300
So it's going to produce papers

01:01:59.300 --> 01:02:01.660
that look like scientific text.

01:02:01.660 --> 01:02:04.300
I think that was an expected thing,

01:02:05.220 --> 01:02:08.780
but again, the backlash it got and stuff like that

01:02:08.780 --> 01:02:10.140
essentially tells everyone,

01:02:10.140 --> 01:02:14.860
and I hope the message is not to demo language models

01:02:14.860 --> 01:02:16.860
anymore, but I think the message should be

01:02:16.860 --> 01:02:20.180
how to make sure that you're not hyping things up

01:02:20.180 --> 01:02:21.580
more than they should be.

01:02:21.580 --> 01:02:23.660
If you reflect on ChatGPT,

01:02:23.660 --> 01:02:27.260
which came not very long after Galactica

01:02:27.260 --> 01:02:32.260
and the launches of those respective products,

01:02:34.460 --> 01:02:38.260
are there clear, is there a clear like do don't do list?

01:02:38.260 --> 01:02:40.820
So I will say that ChatGPT itself

01:02:40.820 --> 01:02:45.820
was also not completely without hype attached to it,

01:02:46.100 --> 01:02:47.540
even some sort of how they-

01:02:47.540 --> 01:02:48.380
Right.

01:02:48.380 --> 01:02:49.220
Yeah, right?

01:02:49.220 --> 01:02:50.060
Somehow they managed it.

01:02:50.060 --> 01:02:51.180
There was a lot of hype.

01:02:51.180 --> 01:02:52.500
Right, right, right.

01:02:52.500 --> 01:02:54.700
I will say that they were fairly clear

01:02:54.700 --> 01:02:57.140
about the fact that like, hey, don't trust,

01:02:57.140 --> 01:02:58.660
maybe they could have been clearer,

01:02:58.660 --> 01:03:01.140
but like don't trust the factual stuff

01:03:01.140 --> 01:03:01.980
and things like that.

01:03:01.980 --> 01:03:04.220
Like it's not a lookup engine.

01:03:04.220 --> 01:03:06.740
I think they kind of could have done a lot more of that,

01:03:06.740 --> 01:03:09.740
but they at least had some caveats.

01:03:09.740 --> 01:03:12.940
But more than that, part of their RLHF stuff

01:03:12.940 --> 01:03:16.580
was to make sure that the model is not producing

01:03:16.580 --> 01:03:19.900
at least obviously sexist, don't say-

01:03:19.900 --> 01:03:20.740
Yeah, there was a lot,

01:03:20.740 --> 01:03:22.980
and maybe we're jumping into ChatGPT,

01:03:22.980 --> 01:03:25.220
which actually is the next thing we're gonna talk about,

01:03:25.220 --> 01:03:27.860
but there was definitely a lot of,

01:03:27.860 --> 01:03:29.060
especially early on,

01:03:29.060 --> 01:03:31.620
things that it just would not opine on.

01:03:31.620 --> 01:03:33.580
Like, yeah, no, you're not gonna sucker me in

01:03:33.580 --> 01:03:34.660
to go in there.

01:03:34.660 --> 01:03:35.780
Right, right, right, yeah.

01:03:35.780 --> 01:03:38.020
And I think when you're building something

01:03:38.020 --> 01:03:39.180
that's public-facing,

01:03:39.180 --> 01:03:42.420
that you're selling as a tool, as a product,

01:03:42.420 --> 01:03:44.380
that is necessary, right?

01:03:44.380 --> 01:03:48.780
Like I don't think you should be doing otherwise.

01:03:48.780 --> 01:03:52.380
Galactica should not have been a public-facing tool

01:03:52.380 --> 01:03:55.780
for every scientist to start using to write their papers.

01:03:55.780 --> 01:03:57.580
It should be a language model, right?

01:03:57.580 --> 01:04:00.700
And what the product is or what the tool is

01:04:00.700 --> 01:04:04.140
is a gap that other people can help fill in, right?

01:04:04.140 --> 01:04:06.620
So that was sort of the missing piece

01:04:06.620 --> 01:04:10.060
when I think about ChatGPT versus Galactica.

01:04:10.060 --> 01:04:13.860
It's like, yeah, ChatGPT has some of the caveats

01:04:13.860 --> 01:04:16.140
about what it's doing,

01:04:16.140 --> 01:04:17.420
has some of the caveats about,

01:04:17.420 --> 01:04:20.300
oh, it's a language model, not a product, to some degree,

01:04:21.260 --> 01:04:24.420
and Galactica was missing it, right?

01:04:24.420 --> 01:04:26.020
So, yeah.

01:04:26.020 --> 01:04:27.540
Now, we're there.

01:04:29.380 --> 01:04:31.460
We were talking about open source.

01:04:31.460 --> 01:04:35.340
Next up is kind of commercial developments.

01:04:35.340 --> 01:04:37.220
Top of that list is ChatGPT.

01:04:38.220 --> 01:04:40.540
Yeah, let's talk about it.

01:04:40.540 --> 01:04:44.740
You said early on that, hey, even without ChatGPT,

01:04:44.740 --> 01:04:45.860
this was a huge year.

01:04:45.860 --> 01:04:47.860
That's clearly not to say that ChatGPT

01:04:47.860 --> 01:04:50.940
wasn't a huge contribution to the year.

01:04:50.940 --> 01:04:53.620
I mean, certainly one of the things

01:04:53.620 --> 01:04:56.580
that I found most interesting

01:04:56.580 --> 01:05:01.580
was the degree to which it kind of broke out

01:05:02.460 --> 01:05:07.460
of the MLAI echo chamber to just talking to random friends

01:05:07.540 --> 01:05:10.060
and are like, hey, have you tried this ChatGPT thing?

01:05:10.060 --> 01:05:11.100
Like, yeah, I have.

01:05:12.820 --> 01:05:15.500
Yeah, so that's been, I guess, the most surprising

01:05:15.500 --> 01:05:19.780
and in some sense, the longest-term impact for ChatGPT

01:05:19.780 --> 01:05:23.460
is going to be the fact that it made it commoditized.

01:05:23.460 --> 01:05:27.180
It made it mainstream in a way that nothing before it had.

01:05:27.180 --> 01:05:29.020
And whether it deserved it or not,

01:05:29.020 --> 01:05:32.220
what the actual innovations are and all of these things

01:05:32.220 --> 01:05:34.100
is a different question, right?

01:05:34.100 --> 01:05:37.500
Like it is clearly, even for research point of view,

01:05:37.500 --> 01:05:40.660
qualitatively better than GPT-63.

01:05:40.660 --> 01:05:42.420
Whether it met some threshold

01:05:42.420 --> 01:05:44.900
for becoming the big thing that it did,

01:05:44.900 --> 01:05:48.340
it is sort of difficult to, sort of in hindsight,

01:05:48.340 --> 01:05:50.060
try to evaluate that.

01:05:50.060 --> 01:05:54.380
But I think it was, yeah, it is definitely something

01:05:54.380 --> 01:05:59.380
that became mainstream and everybody's talking about it.

01:05:59.820 --> 01:06:01.580
There is still a question in my mind

01:06:01.580 --> 01:06:05.780
whether that's a good thing or not in the long run,

01:06:05.780 --> 01:06:09.300
because we can talk about some of the problems with ChatGPT,

01:06:09.300 --> 01:06:13.180
the biggest one being, we know it's a language model.

01:06:13.180 --> 01:06:15.220
Like to some degree, we've been figuring out

01:06:15.220 --> 01:06:17.380
last couple of years what these things are capable of

01:06:17.380 --> 01:06:18.740
and what these things are not.

01:06:18.740 --> 01:06:20.900
And I can sit and in like couple of minutes

01:06:20.900 --> 01:06:23.780
come up with tons of examples where it would fail.

01:06:25.060 --> 01:06:27.300
That's not quite the case when you sort of start

01:06:27.300 --> 01:06:28.780
putting it out in the public.

01:06:28.780 --> 01:06:31.740
So most people don't know what a language model is.

01:06:31.740 --> 01:06:33.500
And I've played around with,

01:06:33.500 --> 01:06:35.660
I've got a bunch of my family to try it

01:06:35.660 --> 01:06:36.700
and things like that.

01:06:36.700 --> 01:06:39.100
And the biggest thing I've had,

01:06:39.100 --> 01:06:41.660
the biggest difficulty I've had conveying to them

01:06:41.660 --> 01:06:45.180
is the fact that it's not looking up anything

01:06:45.180 --> 01:06:46.620
when you ask it something, right?

01:06:46.620 --> 01:06:50.540
Like that is a conceptual jump

01:06:50.540 --> 01:06:54.460
that is very, very difficult for people to get over.

01:06:54.460 --> 01:06:55.540
Yeah.

01:06:55.540 --> 01:06:56.820
And so people like,

01:06:56.820 --> 01:06:58.860
yeah, oh, of course it should know about these things

01:06:58.860 --> 01:07:00.780
because it happened yesterday

01:07:00.780 --> 01:07:02.260
and for such a big news items,

01:07:02.260 --> 01:07:03.660
like why would it not know?

01:07:03.660 --> 01:07:06.660
And I'm like, no, actually it doesn't know anything

01:07:06.660 --> 01:07:07.780
beyond a certain time.

01:07:07.780 --> 01:07:10.540
And even saying it knows anything from then

01:07:11.420 --> 01:07:12.860
is a little bit difficult.

01:07:12.860 --> 01:07:17.460
Right, so I think the best analogy that I've,

01:07:17.460 --> 01:07:19.580
you know, this applies to my research as well,

01:07:19.580 --> 01:07:22.380
but the best analogy I've had in trying to explain

01:07:22.380 --> 01:07:24.700
people what chat GPT does

01:07:24.700 --> 01:07:27.860
is to not think of it as a stochastic parrot

01:07:27.860 --> 01:07:28.900
or anything like that.

01:07:28.900 --> 01:07:31.180
But if you have to think in terms of animals,

01:07:31.180 --> 01:07:33.300
think of it as like a chameleon.

01:07:33.300 --> 01:07:36.540
Like it's trying to sort of fit in

01:07:36.540 --> 01:07:39.380
to a bunch of humans, right?

01:07:39.380 --> 01:07:41.100
And it's trying to just write things

01:07:41.100 --> 01:07:43.220
that will make it pass

01:07:43.220 --> 01:07:46.700
as if it sort of knows all of those things, right?

01:07:46.700 --> 01:07:49.980
I was in a Twitter exchange about,

01:07:50.980 --> 01:07:53.980
I had asked chat GPT to explain RLHF

01:07:56.660 --> 01:07:59.620
and it came up with this acronym

01:08:00.900 --> 01:08:02.220
that was like,

01:08:04.740 --> 01:08:05.700
oh, I forget it.

01:08:05.700 --> 01:08:07.020
And it was really funny.

01:08:07.020 --> 01:08:10.260
It was like something leaderboard,

01:08:10.260 --> 01:08:13.180
you know, human something.

01:08:13.180 --> 01:08:16.380
It was so far off.

01:08:16.380 --> 01:08:18.660
Interestingly enough, I'd asked it about,

01:08:18.660 --> 01:08:20.540
I'd had conversations, you know,

01:08:20.540 --> 01:08:22.340
interactions with it about RLHF

01:08:22.340 --> 01:08:24.100
and then it knew what it was.

01:08:24.100 --> 01:08:24.940
Like to your point,

01:08:24.940 --> 01:08:26.780
it's about kind of where it sits

01:08:26.780 --> 01:08:29.180
in the context of the prompt.

01:08:29.180 --> 01:08:30.780
And I just kind of posted, you know,

01:08:30.780 --> 01:08:35.140
is it trolling me or is it like just trying to BS me?

01:08:35.140 --> 01:08:38.340
And one of the responses that I got that,

01:08:38.340 --> 01:08:40.380
you know, and reflecting on is like really insightful.

01:08:40.380 --> 01:08:43.020
Like it's always trying to BS you.

01:08:43.020 --> 01:08:45.740
That's all it's doing is trying to BS you

01:08:45.740 --> 01:08:48.980
to produce some texts that you will think is reasonable.

01:08:48.980 --> 01:08:52.540
And, you know, to its credit,

01:08:52.540 --> 01:08:54.060
a lot of times it's right,

01:08:54.060 --> 01:08:56.100
but that's all it's trying to do.

01:08:56.100 --> 01:08:56.940
Right, right, right.

01:08:56.940 --> 01:08:57.780
Yeah.

01:08:57.780 --> 01:09:00.100
And especially when it comes to factual stuff

01:09:00.100 --> 01:09:01.900
or even like, you know,

01:09:01.900 --> 01:09:05.380
it is a very useful bullshitter in some sense, right?

01:09:05.380 --> 01:09:07.380
So, because when it's right

01:09:07.380 --> 01:09:08.580
or when it's partially right,

01:09:08.580 --> 01:09:10.900
that's still useful because, you know,

01:09:10.900 --> 01:09:12.580
it is what it is.

01:09:12.580 --> 01:09:14.900
But that, when you put that label,

01:09:14.900 --> 01:09:17.260
like if they had sold it as like,

01:09:17.260 --> 01:09:21.340
hey, we have built a really good bullshitter, right?

01:09:21.340 --> 01:09:23.180
Like, and it's sold out as a product,

01:09:23.180 --> 01:09:24.500
then people would know, okay,

01:09:24.500 --> 01:09:26.020
not to use it for a bunch of tasks

01:09:26.020 --> 01:09:28.900
that they're currently thinking of using it, right?

01:09:28.900 --> 01:09:30.420
And so, yeah.

01:09:30.420 --> 01:09:34.020
So that's the sort of divide that in messaging

01:09:34.020 --> 01:09:38.100
that somehow researchers and NLP folks know,

01:09:38.100 --> 01:09:39.460
oh yeah, language model loss,

01:09:39.460 --> 01:09:43.020
obviously all it is doing is blah, blah, blah, right?

01:09:43.020 --> 01:09:45.540
And yes, RLHF can help to some degree,

01:09:45.540 --> 01:09:47.780
but clearly it's not going to be able

01:09:47.780 --> 01:09:51.900
to do these bunch of other things all the time.

01:09:51.900 --> 01:09:56.420
And that kind of thing is missing from general public,

01:09:56.420 --> 01:09:59.420
but also how a lot of people are planning to use it,

01:09:59.420 --> 01:10:00.580
for example, right?

01:10:00.580 --> 01:10:03.300
So I think that aspect is the part

01:10:03.300 --> 01:10:06.780
that we need to think a little bit more about.

01:10:06.780 --> 01:10:10.540
You've got Palm, Minerva, Flan, Down.

01:10:12.500 --> 01:10:15.020
Tell me a little bit more about your take there.

01:10:15.020 --> 01:10:20.020
Cause I hear of them in a vague research context

01:10:20.380 --> 01:10:22.820
cause no one really has access to these,

01:10:22.820 --> 01:10:27.820
but Google as much more so than something

01:10:28.220 --> 01:10:30.780
that is huge from a commercial perspective.

01:10:30.780 --> 01:10:33.660
Is this a prediction or is this a reflection?

01:10:33.660 --> 01:10:35.340
Yeah, so no, I think of this

01:10:35.340 --> 01:10:40.340
as Palm was a huge commercial development this year.

01:10:40.740 --> 01:10:44.260
Like Google built this really, really large model.

01:10:44.260 --> 01:10:46.980
Now there are, obviously they haven't released it, right?

01:10:46.980 --> 01:10:48.620
So what's the ideal situation?

01:10:48.620 --> 01:10:50.380
They completely release it open source.

01:10:50.380 --> 01:10:51.660
Everybody gets access to it.

01:10:51.660 --> 01:10:52.980
That's not gonna happen.

01:10:52.980 --> 01:10:55.740
Another possible thing is they put an API on it

01:10:55.740 --> 01:10:58.780
and charge people from a Google perspective.

01:10:58.780 --> 01:11:00.300
That doesn't make sense, right?

01:11:00.300 --> 01:11:02.140
So it is something that they've built.

01:11:02.140 --> 01:11:03.660
It's valuable internally.

01:11:03.660 --> 01:11:07.020
I'm sure it's sort of has, you know,

01:11:07.020 --> 01:11:09.140
there are reasons not to make it public,

01:11:09.140 --> 01:11:12.940
but it also has a lot of research insights

01:11:12.940 --> 01:11:16.100
because nobody else has such a big language model

01:11:16.100 --> 01:11:17.980
trained in a similar way.

01:11:17.980 --> 01:11:20.620
And I guess I wanna give them props

01:11:20.620 --> 01:11:23.620
for at least publishing and evaluating

01:11:23.620 --> 01:11:26.620
and doing things like that with Palm

01:11:26.620 --> 01:11:29.940
because it is doing,

01:11:29.940 --> 01:11:32.940
it is of a size that we will not see

01:11:32.940 --> 01:11:35.820
for maybe another year or two

01:11:35.820 --> 01:11:38.260
to be sort of publicly available,

01:11:38.260 --> 01:11:40.980
but yet we get to hear about some insights,

01:11:40.980 --> 01:11:43.660
what to expect, what are the emergence behaviors

01:11:44.980 --> 01:11:46.540
coming out of those language models, right?

01:11:46.540 --> 01:11:48.940
So yeah, it would be ideal if you could audit it

01:11:48.940 --> 01:11:51.140
and all of us and I could contribute

01:11:51.140 --> 01:11:52.780
in finding out what the problems are

01:11:52.780 --> 01:11:54.940
and when it works and when it doesn't work.

01:11:54.940 --> 01:11:58.140
But given that, I think they did a good job.

01:11:58.140 --> 01:12:00.060
Specifically, what I will say is that

01:12:00.060 --> 01:12:03.940
that size has brought up a bunch of capabilities

01:12:03.940 --> 01:12:05.740
like the whole chain of thought thing

01:12:05.740 --> 01:12:08.180
that we talked about at the beginning

01:12:08.180 --> 01:12:11.380
that somehow became possible at that size,

01:12:11.380 --> 01:12:15.500
but wasn't possible at other sites, right?

01:12:15.500 --> 01:12:20.220
So that's why that research is also all coming out of Google

01:12:20.220 --> 01:12:25.220
because it applies mostly to LLMs of that size.

01:12:27.820 --> 01:12:30.980
Palm is 540 billion parameters?

01:12:30.980 --> 01:12:33.180
Something, yeah, 540, yeah.

01:12:33.180 --> 01:12:35.740
So I think they have access to it

01:12:35.740 --> 01:12:38.260
and they can produce a string of papers

01:12:38.260 --> 01:12:40.500
and yes, nobody else can write those papers,

01:12:40.500 --> 01:12:44.380
but from a consumer of research as well as producer, right?

01:12:44.380 --> 01:12:47.820
So from my consumer side, I love to read research

01:12:47.820 --> 01:12:50.460
and I'm glad that they're writing those papers

01:12:50.460 --> 01:12:52.980
because there's a lot of interesting stuff

01:12:52.980 --> 01:12:54.100
in all of the papers.

01:12:54.100 --> 01:12:55.980
So yeah, there's a whole string of papers

01:12:55.980 --> 01:12:56.820
that I would recommend

01:12:56.820 --> 01:12:59.060
and I can point you to them offline.

01:12:59.060 --> 01:13:04.060
But yeah, there's stuff that we'll see happen publicly

01:13:04.140 --> 01:13:06.100
next year or maybe another year after that

01:13:06.100 --> 01:13:08.780
when those models become commercialized.

01:13:08.780 --> 01:13:13.300
So yeah, no, I think that that's been kind of key.

01:13:13.300 --> 01:13:18.300
So I'd say for that commercial, but not commercialized.

01:13:18.580 --> 01:13:21.460
Yes, right, right, right, yeah, yeah, yeah.

01:13:21.460 --> 01:13:23.220
Or soon to be commercialized, I'm sure,

01:13:23.220 --> 01:13:24.860
but maybe not by Google.

01:13:24.860 --> 01:13:27.020
Yeah, awesome.

01:13:27.020 --> 01:13:31.900
Next up, kind of the intersection between search and LLMs.

01:13:31.900 --> 01:13:33.060
What are you seeing there?

01:13:33.060 --> 01:13:36.260
Yeah, so I think that's been kind of an interesting,

01:13:36.260 --> 01:13:37.740
it's been a commercial development,

01:13:37.740 --> 01:13:40.180
again, questionable to some degree,

01:13:40.180 --> 01:13:42.260
but because I don't think the research is,

01:13:42.260 --> 01:13:44.020
and these models are quite up to snuff,

01:13:44.020 --> 01:13:48.580
but this somewhat coincided with Chad GPT.

01:13:49.700 --> 01:13:50.540
I think-

01:13:50.540 --> 01:13:52.460
Well, Chad GPT certainly raised a ton of questions

01:13:52.460 --> 01:13:54.420
about, hey, is this a Google killer?

01:13:54.420 --> 01:13:56.060
Right, right, right, right.

01:13:56.060 --> 01:13:57.300
Yes, exactly.

01:13:57.300 --> 01:13:58.420
And along the same time,

01:13:58.420 --> 01:14:02.140
there were at least three search engines that I know of.

01:14:02.140 --> 01:14:05.940
There was perplexity.ai that I don't think existed before

01:14:07.100 --> 01:14:09.980
what the product they came up with,

01:14:09.980 --> 01:14:12.740
which is a search engine which sort of gathers

01:14:12.740 --> 01:14:15.540
all of the results from a typical search engine,

01:14:15.540 --> 01:14:18.740
but then uses GPT-3-like models

01:14:18.740 --> 01:14:21.660
to summarize the content of those links

01:14:21.660 --> 01:14:24.660
and produces a paragraph that actually answers your query.

01:14:25.580 --> 01:14:27.380
You.com is, again, a search engine

01:14:27.380 --> 01:14:28.580
that has been around for a while,

01:14:28.580 --> 01:14:32.380
but they brought this whole chat aspect to their search

01:14:32.380 --> 01:14:34.940
where you're sort of chatting

01:14:34.940 --> 01:14:37.020
and trying to come up with an answer.

01:14:37.020 --> 01:14:39.260
And again, it's sort of not just showing you

01:14:39.260 --> 01:14:43.780
a bunch of links, but composing information into text.

01:14:43.780 --> 01:14:45.580
That's Richard Socher's company,

01:14:45.580 --> 01:14:48.940
and we'll drop a link to my interview with him

01:14:48.940 --> 01:14:50.420
in the show notes as well.

01:14:50.420 --> 01:14:51.820
Okay, cool, yeah, yeah, yeah.

01:14:51.820 --> 01:14:55.180
And Neva is another, you know,

01:14:55.180 --> 01:14:58.060
it's a private search company.

01:14:58.060 --> 01:15:00.740
It's a startup that also has an AI agent

01:15:00.740 --> 01:15:02.940
that you can talk to it and things like that, right?

01:15:02.940 --> 01:15:04.820
So I haven't played around with all of them.

01:15:04.820 --> 01:15:07.260
I've played around with them a little bit.

01:15:07.260 --> 01:15:11.180
And again, it's very easy to find problems

01:15:11.180 --> 01:15:13.580
and sort of realize that, okay,

01:15:13.580 --> 01:15:15.860
these language models are, you know,

01:15:15.860 --> 01:15:17.060
this interface is great,

01:15:17.060 --> 01:15:19.420
and it would be great to get the right paragraph

01:15:19.420 --> 01:15:21.180
if it could get there.

01:15:21.180 --> 01:15:23.420
But oftentimes they don't quite work

01:15:23.420 --> 01:15:25.620
because of sort of fundamental issues with language models.

01:15:25.620 --> 01:15:28.220
But I think from a commercial development,

01:15:28.220 --> 01:15:30.700
I'm pretty excited about what search would look like

01:15:30.700 --> 01:15:33.100
in the future and where language models

01:15:33.100 --> 01:15:36.580
would fit into that whole product.

01:15:36.580 --> 01:15:41.580
Yeah, one of my thought experiments with this

01:15:41.780 --> 01:15:44.260
in the context of ChatGPT,

01:15:44.260 --> 01:15:45.780
not that it was particularly deep,

01:15:45.780 --> 01:15:50.780
but like there was this early meme,

01:15:50.780 --> 01:15:52.460
you know, along the lines of,

01:15:52.460 --> 01:15:55.620
hey, Google search is crap now, it's all ads.

01:15:57.020 --> 01:16:00.420
ChatGPT, you know, I love this interface,

01:16:00.420 --> 01:16:02.060
you know, it's gonna kill Google.

01:16:02.060 --> 01:16:07.060
And so I asked ChatGPT to basically build a response

01:16:07.180 --> 01:16:08.740
with an ad in it.

01:16:08.740 --> 01:16:10.940
It works, it can do it.

01:16:10.940 --> 01:16:13.060
I wouldn't be so sure that your, you know,

01:16:13.060 --> 01:16:15.820
LLM-based search won't have any ads.

01:16:15.820 --> 01:16:17.620
Right, right, right, yeah.

01:16:17.620 --> 01:16:18.820
Yeah, no, I think, yeah.

01:16:19.900 --> 01:16:21.820
Where the ads would come in

01:16:21.820 --> 01:16:23.660
and how subtle the ads will be

01:16:23.660 --> 01:16:25.260
once you throw in a language model into it.

01:16:25.260 --> 01:16:28.260
Yeah, that's kind of interesting to think about.

01:16:28.260 --> 01:16:31.460
And I guess next up on your list of commercial developments

01:16:31.460 --> 01:16:35.340
is what I might call the LLM-ing of all the things.

01:16:35.340 --> 01:16:36.660
Yeah, so I think, you know,

01:16:36.660 --> 01:16:40.860
it's been two years or so since GPT-3 came out

01:16:41.820 --> 01:16:45.340
and it's the question of like, okay,

01:16:45.340 --> 01:16:48.220
where is the world changing products

01:16:48.220 --> 01:16:49.900
that are using GPT-3?

01:16:49.900 --> 01:16:52.620
When it came out, hey, it was gonna change everything.

01:16:52.620 --> 01:16:54.100
Has it changed everything?

01:16:54.100 --> 01:16:57.620
And I would say like for the most part, no.

01:16:57.620 --> 01:17:01.180
The products that did seem to show some promise

01:17:01.180 --> 01:17:03.820
and some of these are ones that will appear in the future

01:17:03.820 --> 01:17:06.380
but have been kind of semi-announced

01:17:06.380 --> 01:17:08.460
is the notion of writing assistantships, right?

01:17:08.460 --> 01:17:12.620
So I think Notion AI is the one I think about

01:17:12.620 --> 01:17:15.180
where a lot of people, it's a mainstream product,

01:17:15.180 --> 01:17:16.940
anybody can use Notion

01:17:16.940 --> 01:17:19.940
and Notion has this GPT-3 thing built in

01:17:19.940 --> 01:17:23.140
where it can write to-do lists for you

01:17:23.140 --> 01:17:23.980
and things like that.

01:17:23.980 --> 01:17:28.980
So I think that is a pretty strong first version of GPT-3

01:17:28.980 --> 01:17:32.220
as a commercial product that anybody can use

01:17:32.220 --> 01:17:34.060
that I'm quite excited about.

01:17:34.060 --> 01:17:38.100
I feel like the timing there is very ChatGPT influence.

01:17:38.100 --> 01:17:39.940
Obviously they've been working on it.

01:17:39.940 --> 01:17:43.460
You know, they saw it when GPT-3 came out

01:17:43.460 --> 01:17:47.500
but I think they made it available right after ChatGPT

01:17:47.500 --> 01:17:52.220
and a lot of these, you know,

01:17:52.220 --> 01:17:54.580
Jasper's been around for a while

01:17:54.580 --> 01:17:59.180
but there's a lot of new kind of writing assistant

01:18:00.260 --> 01:18:01.100
types of things.

01:18:01.100 --> 01:18:03.500
And it just does seem like there's a step function increase

01:18:03.500 --> 01:18:07.500
in kind of energy in the space of using LLMs

01:18:09.180 --> 01:18:13.140
since ChatGPT, even though they're all based on GPT-3

01:18:13.140 --> 01:18:15.660
which has been around for two years, right?

01:18:15.660 --> 01:18:17.100
Yeah, so I don't know exactly

01:18:17.100 --> 01:18:20.580
why that thing seemed to align well, right?

01:18:20.580 --> 01:18:22.660
So it's like, yeah, GPT-3 was announced

01:18:22.660 --> 01:18:24.860
but it was a while before the API was rolled out

01:18:24.860 --> 01:18:27.620
to everybody and, you know, and maybe after that

01:18:27.620 --> 01:18:29.420
it takes a while to make the business case

01:18:29.420 --> 01:18:30.260
for these things.

01:18:30.260 --> 01:18:33.380
So yeah, maybe it's just timing of why it worked

01:18:33.380 --> 01:18:36.140
or there were people like already kind of working on it

01:18:36.140 --> 01:18:38.340
in a sort of on the side and they were like,

01:18:38.340 --> 01:18:41.620
hey, no, we gotta sort of ride this wave

01:18:41.620 --> 01:18:43.180
and sort of introduce things, right?

01:18:43.180 --> 01:18:46.060
So I don't know exactly what that looks like,

01:18:46.060 --> 01:18:48.380
but yeah, no, I think the fact that it aligns

01:18:48.380 --> 01:18:51.140
also gets a lot more excitement and people know,

01:18:51.140 --> 01:18:54.380
oh, okay, ChatGPT is something I've played around with.

01:18:54.380 --> 01:18:58.100
This is now ChatGPT that's working on something that I do

01:18:58.100 --> 01:18:59.740
and there is a lot of value in that.

01:18:59.740 --> 01:19:04.740
Am I detecting an underlying pessimism maybe

01:19:05.980 --> 01:19:10.900
about like, you know, kind of, you know,

01:19:10.900 --> 01:19:13.020
where's the flying car that I was promised?

01:19:13.020 --> 01:19:15.180
All I have is this GPT-3 thing.

01:19:15.180 --> 01:19:16.660
Well, it's not so much the pessimism

01:19:16.660 --> 01:19:20.140
because when I saw GPT-3, it became evident to me

01:19:20.140 --> 01:19:22.180
that this is a great language model,

01:19:22.180 --> 01:19:26.060
but it's not clear as it is how it can be made

01:19:26.060 --> 01:19:27.980
into a product, right?

01:19:27.980 --> 01:19:29.860
But it still came with a lot of hype

01:19:29.860 --> 01:19:31.860
and yeah, it can generate a bunch of things,

01:19:31.860 --> 01:19:34.940
but we quite haven't quite seen

01:19:34.940 --> 01:19:36.860
what the product version of those look like.

01:19:36.860 --> 01:19:40.260
I think the language models are extremely powerful,

01:19:40.260 --> 01:19:41.940
not just as language models,

01:19:41.940 --> 01:19:45.140
but they can be converted into products.

01:19:45.140 --> 01:19:47.900
I don't quite feel like we are at a stage

01:19:47.900 --> 01:19:49.980
where it's just going to be through prompting

01:19:49.980 --> 01:19:53.020
and, you know, let's just tweak it a little bit.

01:19:53.020 --> 01:19:54.620
I think there are a bunch of products

01:19:54.620 --> 01:19:57.100
that'll come out of just by doing that,

01:19:57.100 --> 01:19:58.700
but there's a whole slew of product

01:19:58.700 --> 01:20:01.900
where the language models need to know a lot more

01:20:01.900 --> 01:20:04.820
about the context where they're going to be

01:20:05.700 --> 01:20:09.940
to be able to be effective, yeah, effective tools.

01:20:10.900 --> 01:20:12.580
And you mentioned Microsoft.

01:20:12.580 --> 01:20:14.100
What did you have in mind there?

01:20:14.100 --> 01:20:16.540
Yeah, this was sort of a news that came out recently

01:20:16.540 --> 01:20:20.300
where they're trying to have a bigger stake in open AI,

01:20:20.300 --> 01:20:22.540
but also just generally thinking

01:20:22.540 --> 01:20:26.620
of having open AI-like tools available in Word,

01:20:26.620 --> 01:20:29.740
available in PowerPoint and all of these things.

01:20:29.740 --> 01:20:30.580
They don't have it yet,

01:20:30.580 --> 01:20:31.900
but I think those kinds of things

01:20:31.900 --> 01:20:33.980
are just sort of coming as well.

01:20:33.980 --> 01:20:38.100
Do you think a chat GPT-based Bing is a Google killer?

01:20:38.100 --> 01:20:41.340
Oh, I don't think with that branding,

01:20:41.340 --> 01:20:44.300
they would have to call it something else or yeah.

01:20:44.300 --> 01:20:46.660
At this point, yeah.

01:20:46.660 --> 01:20:49.300
I mean, that seemed to be the suggestion, right?

01:20:49.300 --> 01:20:54.300
Chat GPT comes out, they're gonna take a big stake.

01:20:54.500 --> 01:20:58.780
And it was mentioned, if not in the official announcement,

01:20:58.780 --> 01:21:00.060
it seemed to be the conjecture

01:21:00.060 --> 01:21:03.980
that it was gonna be some tie up with Bing,

01:21:05.300 --> 01:21:08.300
explicitly to target search, right?

01:21:08.300 --> 01:21:11.620
I think there needs to be a lot more fundamental work

01:21:11.620 --> 01:21:13.460
and we can talk about this in the future predictions,

01:21:13.460 --> 01:21:16.620
but there needs to be a lot more fundamental work

01:21:16.620 --> 01:21:20.060
before we sort of are able to kill search

01:21:20.060 --> 01:21:21.700
just by putting a language model, right?

01:21:21.700 --> 01:21:23.860
Like I think that gap is not as simple

01:21:23.860 --> 01:21:28.020
as replacing something or just augmenting existing search.

01:21:28.020 --> 01:21:29.940
I think you would have to think about

01:21:29.940 --> 01:21:32.980
what kind of things can language models actually do

01:21:32.980 --> 01:21:36.500
and you still want to rely on sources and things like that.

01:21:36.500 --> 01:21:40.860
But yeah, so I think it's going to happen at some point,

01:21:40.860 --> 01:21:44.220
but it's going to be like search as,

01:21:44.220 --> 01:21:45.420
it won't be replacing search

01:21:45.420 --> 01:21:47.140
because it'll be a different thing, right?

01:21:47.140 --> 01:21:49.540
Like it'll be, it's not gonna be search because-

01:21:49.540 --> 01:21:50.980
Not in the way we think about search right now.

01:21:50.980 --> 01:21:53.540
Search literally means, yeah, exactly.

01:21:53.540 --> 01:21:54.860
It'll be question answering

01:21:54.860 --> 01:21:56.220
or it'll be something else, right?

01:21:56.220 --> 01:21:58.900
Like it'll be a helper or whatever,

01:21:58.900 --> 01:22:01.580
but search is maybe not the right thing to do.

01:22:01.580 --> 01:22:04.820
Well, one quick thing before we jump into predictions,

01:22:04.820 --> 01:22:09.540
you kind of reflected on your top use case for the year

01:22:09.540 --> 01:22:12.900
and that was CodePilot.

01:22:12.900 --> 01:22:14.500
Tell me a little bit more about

01:22:14.500 --> 01:22:15.860
how you're thinking about that.

01:22:15.860 --> 01:22:17.300
I think CodePilot came out

01:22:17.300 --> 01:22:19.460
probably not exactly in this calendar year,

01:22:19.460 --> 01:22:23.380
but I feel like it got a lot more adoption this year

01:22:23.380 --> 01:22:25.980
and started becoming part of the tools

01:22:25.980 --> 01:22:27.540
where people are coding.

01:22:27.540 --> 01:22:30.100
And personally, I started using CodePilot this year,

01:22:30.100 --> 01:22:33.260
so I'm gonna put it in a top use case this year.

01:22:33.260 --> 01:22:36.540
And I will say before Notion.ai,

01:22:36.540 --> 01:22:40.980
CodePilot was probably the only use of large language models

01:22:40.980 --> 01:22:43.140
that I saw anywhere.

01:22:43.140 --> 01:22:45.860
So from that point of view, it was interesting

01:22:45.860 --> 01:22:47.180
that GPT-3 came out

01:22:47.180 --> 01:22:50.620
and then nothing, nothing that can build CodePilot.

01:22:50.620 --> 01:22:52.460
But from a use case point of view,

01:22:52.460 --> 01:22:54.700
it has been incredibly useful, right?

01:22:54.700 --> 01:22:58.740
So I've been able to do things.

01:22:58.740 --> 01:23:02.180
It has made me a lot more effective as a coder,

01:23:02.180 --> 01:23:03.020
not that I code much,

01:23:03.020 --> 01:23:05.180
but when I do, I want to do a lot

01:23:05.180 --> 01:23:08.500
and CodePilot has let me sort of do that.

01:23:08.500 --> 01:23:10.620
And that's been amazing.

01:23:10.620 --> 01:23:13.220
I feel the right combination of,

01:23:13.220 --> 01:23:15.940
hey, having a nice user interface,

01:23:15.940 --> 01:23:19.460
having the right data that is trained on

01:23:19.460 --> 01:23:22.260
to be able to sort of really help people

01:23:22.260 --> 01:23:23.460
in what they want to do.

01:23:23.460 --> 01:23:25.380
Now, of course, CodePilot has issues.

01:23:25.380 --> 01:23:30.380
It's producing code that can be dangerous,

01:23:30.860 --> 01:23:32.340
that can be buggy.

01:23:32.340 --> 01:23:34.860
And of course, there are the questions of copyright

01:23:34.860 --> 01:23:37.420
and plagiarism, exactly.

01:23:37.420 --> 01:23:41.420
So I feel like, I hope those things will get resolved,

01:23:41.420 --> 01:23:42.780
but those are, again,

01:23:42.780 --> 01:23:44.500
when you start using a language model,

01:23:44.500 --> 01:23:47.940
these are the issues that you have to solve.

01:23:47.940 --> 01:23:50.060
And then I'm glad that CodePilot is bringing

01:23:50.060 --> 01:23:51.980
all of these things into the discussion

01:23:51.980 --> 01:23:54.380
by it being out there.

01:23:55.340 --> 01:23:57.140
Yeah, I've had the same experience with it.

01:23:57.140 --> 01:24:00.620
I think I've shared this on social

01:24:00.620 --> 01:24:04.540
or in the podcast in a conversation.

01:24:04.540 --> 01:24:08.580
I saw all the CodePilot demos,

01:24:08.580 --> 01:24:11.660
played around with it with kind of the toy problem things,

01:24:11.660 --> 01:24:14.780
but I don't do a lot of coding necessarily,

01:24:14.780 --> 01:24:17.460
but I do tend to binge on coding every once in a while.

01:24:17.460 --> 01:24:19.940
Like, and usually like that end of year holiday thing,

01:24:19.940 --> 01:24:21.700
I'll have some project.

01:24:21.700 --> 01:24:24.700
And I did that this year and use CodePilot.

01:24:24.700 --> 01:24:25.620
It was amazing.

01:24:25.620 --> 01:24:27.860
Like the productivity you can,

01:24:27.860 --> 01:24:31.620
it helps, the productivity helps create for you

01:24:31.620 --> 01:24:36.620
attacking a new problem with new tools

01:24:37.260 --> 01:24:40.100
without the context switching of going to Google

01:24:40.100 --> 01:24:40.980
and Stack Overflow.

01:24:40.980 --> 01:24:42.700
Like, it's incredible.

01:24:42.700 --> 01:24:44.620
I'm a total believer.

01:24:44.620 --> 01:24:47.900
Yeah, and I think that exactly is the kind of thing

01:24:47.900 --> 01:24:50.540
I expect language models to be useful for.

01:24:50.540 --> 01:24:51.620
They are not going to,

01:24:51.620 --> 01:24:53.620
and with chat GPT going back a little bit,

01:24:53.620 --> 01:24:54.460
people are talking about,

01:24:54.460 --> 01:24:56.100
hey, people are gonna lose jobs

01:24:56.100 --> 01:24:57.900
and it's gonna change everything.

01:24:57.900 --> 01:25:01.180
And we'll replace X, Y, Z with chat GPT.

01:25:01.180 --> 01:25:04.780
And I don't quite see that happening,

01:25:04.780 --> 01:25:07.740
but I do expect a lot of people in many different areas

01:25:07.740 --> 01:25:11.460
becoming a lot more productive because of chat GPT.

01:25:11.460 --> 01:25:14.980
And CodePilot is an example of how language models

01:25:14.980 --> 01:25:19.140
can make you a lot more productive without replacing,

01:25:19.140 --> 01:25:21.300
I don't think it's replacing specific programmers.

01:25:21.300 --> 01:25:24.660
It's just making, allowing them to do a lot more.

01:25:24.660 --> 01:25:27.420
And that I think is the best use of technology.

01:25:27.420 --> 01:25:28.260
Awesome, awesome.

01:25:28.260 --> 01:25:31.540
Well, let's jump into predictions.

01:25:31.540 --> 01:25:34.340
What are you most excited about

01:25:34.340 --> 01:25:37.100
kind of looking into your crystal ball?

01:25:37.100 --> 01:25:40.860
So I think the chat GPT is the one that sort of,

01:25:40.860 --> 01:25:42.860
everybody knew language models,

01:25:42.860 --> 01:25:45.540
they're just trained on data and making predictions.

01:25:45.540 --> 01:25:49.700
What chat GPT really did was remind everyone like,

01:25:49.700 --> 01:25:52.060
okay, even if the language modeling part

01:25:52.060 --> 01:25:54.100
is quote unquote solved, right?

01:25:54.100 --> 01:25:56.940
Even if you get a really, really large language model,

01:25:56.940 --> 01:26:00.340
that doesn't mean you're done, right?

01:26:00.340 --> 01:26:02.860
And I think one of the biggest aspects of that

01:26:02.860 --> 01:26:07.860
was making sure that what you're generating is not just BS,

01:26:08.780 --> 01:26:12.380
it's somehow valid, somehow the truth,

01:26:12.380 --> 01:26:16.420
somehow something that you can cite and rely on, right?

01:26:16.420 --> 01:26:20.060
They definitely shine a light on how challenging that is.

01:26:20.060 --> 01:26:21.740
Right, exactly, yeah.

01:26:21.740 --> 01:26:23.780
So I don't think this is gonna be a prediction

01:26:23.780 --> 01:26:25.420
necessarily for 2023,

01:26:25.420 --> 01:26:27.860
maybe 2023 is when we'll start seeing

01:26:27.860 --> 01:26:29.660
the first attempts at this,

01:26:29.660 --> 01:26:32.260
but being able to generate text

01:26:32.260 --> 01:26:35.060
that does not have misinformation,

01:26:35.060 --> 01:26:40.060
that differentiates factual from creative hallucinations,

01:26:40.500 --> 01:26:43.220
that is able to cite its sources

01:26:43.220 --> 01:26:44.620
and sort of point to like,

01:26:44.620 --> 01:26:46.940
look, this is the piece of paragraph

01:26:46.940 --> 01:26:50.260
that I'm based on which I'm generating a piece of text.

01:26:50.260 --> 01:26:52.980
I think those things are needed

01:26:52.980 --> 01:26:54.820
and it's probably going to be

01:26:54.820 --> 01:26:58.380
the next aspect of language models.

01:26:58.380 --> 01:27:01.660
That's gonna be a big topic of research.

01:27:01.660 --> 01:27:05.260
Do you have a sense for where, how we get there?

01:27:05.260 --> 01:27:09.260
Is it kind of applying the same tools,

01:27:09.260 --> 01:27:13.620
RLHF for example, attacking this specific problem

01:27:13.620 --> 01:27:16.260
or do you think there's, you know,

01:27:16.260 --> 01:27:18.460
we don't have the tools and it's gonna need to be

01:27:18.460 --> 01:27:22.340
kind of new invention that gets us there?

01:27:22.340 --> 01:27:24.940
I think it's going to have to be new inventions

01:27:24.940 --> 01:27:27.580
and I want to sort of think of it as not just,

01:27:27.580 --> 01:27:29.660
you know, how do we attribute it

01:27:29.660 --> 01:27:31.420
to specific pieces of text,

01:27:31.420 --> 01:27:33.580
but I kind of think of it as like

01:27:33.580 --> 01:27:35.660
being able to use other tools,

01:27:35.660 --> 01:27:38.980
being able to use other things available

01:27:38.980 --> 01:27:40.860
to the language model

01:27:40.860 --> 01:27:42.540
when it's being trained as well, right?

01:27:42.540 --> 01:27:47.340
So it should not rely on memorizing facts to any degree.

01:27:47.340 --> 01:27:51.220
It should just rely on using existing tools,

01:27:51.220 --> 01:27:54.500
including search, including maybe calculations,

01:27:54.500 --> 01:27:56.700
maybe even a Python interpreter,

01:27:56.700 --> 01:27:59.180
whatever else it needs to do,

01:27:59.180 --> 01:28:02.180
but still be able to do the language modeling task, right?

01:28:02.180 --> 01:28:04.220
So I think there is some combination

01:28:04.220 --> 01:28:06.820
of being able to refer to external stuff

01:28:06.820 --> 01:28:08.340
and still do language modeling

01:28:08.340 --> 01:28:10.860
that we haven't quite tracked it

01:28:11.780 --> 01:28:14.620
and that would be something that I think

01:28:14.620 --> 01:28:15.980
will come into picture.

01:28:15.980 --> 01:28:17.660
I'll give you an example of how

01:28:19.140 --> 01:28:20.860
sort of some people have been thinking about it.

01:28:20.860 --> 01:28:21.820
There's this whole idea

01:28:21.820 --> 01:28:24.820
of retrieval-based language modeling

01:28:24.820 --> 01:28:28.380
where you're still generating the text token by token,

01:28:28.380 --> 01:28:31.620
but you're always retrieving some set of documents

01:28:31.620 --> 01:28:33.020
and you're conditioning on them

01:28:33.020 --> 01:28:35.380
when you're generating each token.

01:28:35.380 --> 01:28:39.940
That's sort of one step towards what I'm talking about,

01:28:39.940 --> 01:28:41.700
where at least you're trying to look

01:28:41.700 --> 01:28:44.460
at retrieved documents when you're generating,

01:28:45.380 --> 01:28:47.140
but that doesn't guarantee what you're generating

01:28:47.140 --> 01:28:48.620
is actually based on.

01:28:48.620 --> 01:28:53.620
So you just spoke earlier about the decomposed reasoning.

01:28:55.100 --> 01:29:00.020
Is this prediction that those ideas

01:29:00.020 --> 01:29:03.620
become more real in some way in 23-24

01:29:03.620 --> 01:29:08.620
or is it that what we're doing with a trained model

01:29:10.300 --> 01:29:13.180
to kind of get decomposed reasoning,

01:29:13.180 --> 01:29:14.580
we're gonna push even deeper

01:29:14.580 --> 01:29:17.260
into the fundamental creation of the model

01:29:17.260 --> 01:29:19.540
like at train time and other things?

01:29:19.540 --> 01:29:21.820
Yeah, so more of the latter, right?

01:29:21.820 --> 01:29:23.740
So right now we are expecting the model

01:29:23.740 --> 01:29:25.660
to be able to do decomposed reasoning,

01:29:25.660 --> 01:29:28.780
but we only do it at test time in some sense, right?

01:29:28.780 --> 01:29:31.100
Let's actually try to start thinking,

01:29:31.100 --> 01:29:33.060
putting that stuff during training, right?

01:29:33.060 --> 01:29:36.180
So like, again, I don't want to make this analogy too much,

01:29:36.180 --> 01:29:39.940
but when you think about when you're training a human

01:29:39.940 --> 01:29:41.140
on how to do things,

01:29:41.140 --> 01:29:44.540
you don't just give it pairs of input and output.

01:29:44.540 --> 01:29:47.620
You give it a little bit more of a decomposition

01:29:47.620 --> 01:29:50.780
and then based on that, they're able to do what they do.

01:29:50.780 --> 01:29:53.380
If you want them to use the Python interpreter,

01:29:53.380 --> 01:29:56.740
you don't just expect them to finish it on their own.

01:29:56.740 --> 01:29:58.820
They can use the interpreter when needed kind of thing,

01:29:58.820 --> 01:29:59.660
right?

01:29:59.660 --> 01:30:01.660
So I just think of language models as,

01:30:01.660 --> 01:30:03.980
yeah, maybe they're still doing the language modeling tasks,

01:30:03.980 --> 01:30:06.580
but they have access to a bunch of other tools.

01:30:07.500 --> 01:30:09.860
And maybe this is more far-fetched than 2023,

01:30:09.860 --> 01:30:11.780
but I think in the long run,

01:30:11.780 --> 01:30:15.060
you want a system that's able to do those things.

01:30:15.060 --> 01:30:15.900
You got it.

01:30:15.900 --> 01:30:17.580
Your next prediction is around diffusion models.

01:30:17.580 --> 01:30:18.420
It's kind of surprising

01:30:18.420 --> 01:30:20.580
that that term hasn't come up yet so far.

01:30:20.580 --> 01:30:23.020
Yeah, I guess it is surprising,

01:30:23.020 --> 01:30:24.580
but also in NLP in general,

01:30:24.580 --> 01:30:27.100
I feel like we are barely scratching the surface

01:30:27.100 --> 01:30:30.540
of what diffusion models can do.

01:30:30.540 --> 01:30:34.020
So yeah, I think clearly in the image generation space,

01:30:34.020 --> 01:30:37.140
we've seen a lot of progress with diffusion models

01:30:37.140 --> 01:30:40.820
and we've seen some in NLP, but not enough.

01:30:40.820 --> 01:30:44.180
I guess what I find attractive about diffusion model

01:30:44.180 --> 01:30:46.940
is that it's trying to generate more

01:30:46.940 --> 01:30:49.380
than just a single thing at one point, right?

01:30:49.380 --> 01:30:52.500
So when diffusion models are applied to text,

01:30:52.500 --> 01:30:53.620
the way it would look like

01:30:53.620 --> 01:30:56.460
is not just producing one token at a time.

01:30:56.460 --> 01:30:58.980
It will try to produce a whole sentence

01:30:58.980 --> 01:31:02.340
or whatever we decide is the right granularity.

01:31:02.340 --> 01:31:05.900
And that idea of a model that is trained

01:31:05.900 --> 01:31:08.380
not to do one token at a time,

01:31:08.380 --> 01:31:11.820
but to do something bigger really appeals to me

01:31:11.820 --> 01:31:13.740
because I feel like a lot of the issues

01:31:13.740 --> 01:31:16.180
we talk about with language models

01:31:16.180 --> 01:31:17.700
fundamentally come from the fact

01:31:17.700 --> 01:31:20.820
that it's trained to do one token at a time

01:31:20.820 --> 01:31:24.340
and sort of, and that's kind of the loss, right?

01:31:24.340 --> 01:31:29.340
So if we can have the model be trained to generate more

01:31:29.380 --> 01:31:31.140
and then give it a loss,

01:31:31.140 --> 01:31:33.100
I think that's fundamentally interesting

01:31:33.100 --> 01:31:35.780
and diffusion models sort of provide one way of doing that.

01:31:35.780 --> 01:31:40.780
Do you, would you kind of visualize this as a model

01:31:42.660 --> 01:31:46.980
like in a first iteration spitting out bullshit

01:31:46.980 --> 01:31:50.780
and then successively like iterating towards truth?

01:31:50.780 --> 01:31:53.540
Like, is that one way that this could play out?

01:31:53.540 --> 01:31:54.580
Yes.

01:31:54.580 --> 01:31:57.900
Well, I mean, probably not.

01:31:57.900 --> 01:32:01.260
Probably it's gonna be somewhere in the latent space,

01:32:01.260 --> 01:32:05.820
but I think the way I think about it is like,

01:32:05.820 --> 01:32:08.900
if we were doing this token by token thing for images,

01:32:08.900 --> 01:32:10.500
it just wouldn't make sense, right?

01:32:10.500 --> 01:32:12.300
Like predict-

01:32:12.300 --> 01:32:14.060
Certainly wouldn't produce the images

01:32:14.060 --> 01:32:17.980
that we see coming out of stable diffusion and yeah.

01:32:17.980 --> 01:32:19.980
Or even what it's going to learn

01:32:19.980 --> 01:32:21.380
is going to be something different.

01:32:21.380 --> 01:32:24.020
What it's going to learn is given the image

01:32:24.020 --> 01:32:25.500
that I've seen so far,

01:32:25.500 --> 01:32:28.980
let me predict the next pixel or the next piece, right?

01:32:28.980 --> 01:32:32.300
That somehow feels like a fundamentally different task

01:32:32.300 --> 01:32:34.900
than being able to generate an image fully, right?

01:32:35.900 --> 01:32:40.860
And so I feel like thinking about the same idea for text

01:32:40.860 --> 01:32:41.820
just kind of makes sense.

01:32:41.820 --> 01:32:44.780
Like you write the summary in one shot

01:32:44.780 --> 01:32:47.980
and realize how wrong it is.

01:32:47.980 --> 01:32:50.380
It feels like there's something fundamentally different

01:32:50.380 --> 01:32:53.100
than, hey, you got a bunch of tokens correct,

01:32:53.100 --> 01:32:54.340
but you also got a bunch.

01:32:54.340 --> 01:32:57.780
And in some sense, there are some analogies to RLHF

01:32:57.780 --> 01:33:00.580
and using PPO for training, for example,

01:33:00.580 --> 01:33:04.140
where you try to make sure it's fluent

01:33:04.140 --> 01:33:05.180
and things like that.

01:33:05.180 --> 01:33:07.340
These are all losses designed on

01:33:07.340 --> 01:33:09.100
not just a token by token basis,

01:33:09.100 --> 01:33:10.780
but something that's longer.

01:33:10.780 --> 01:33:13.420
And so we've known how useful they've been.

01:33:13.420 --> 01:33:15.220
So I feel like there may be something

01:33:15.220 --> 01:33:18.140
in taking that idea and applying it to pre-shading

01:33:18.140 --> 01:33:19.780
is something that we'll use.

01:33:19.780 --> 01:33:21.060
Interesting, interesting.

01:33:21.060 --> 01:33:23.260
I expect a lot of people will be wanting

01:33:23.260 --> 01:33:25.140
to figure out how to do that.

01:33:25.140 --> 01:33:26.180
Great.

01:33:26.180 --> 01:33:29.220
And online updates to models.

01:33:29.220 --> 01:33:32.100
Yeah, so I think one of the problems with language models,

01:33:32.100 --> 01:33:34.300
so let's keep aside the grand vision

01:33:34.300 --> 01:33:36.860
of how language models will use search

01:33:36.860 --> 01:33:38.140
and all of these other things.

01:33:38.140 --> 01:33:40.340
But one of the fundamental problems with language models

01:33:40.340 --> 01:33:43.620
is that the world changes, but they don't.

01:33:44.500 --> 01:33:47.140
And this seems to be a fundamental sort of issue

01:33:47.140 --> 01:33:49.500
with language models.

01:33:49.500 --> 01:33:54.500
So I think thinking about how we can update language models

01:33:54.500 --> 01:33:59.180
every month or every week or every day, right?

01:33:59.180 --> 01:34:03.300
I think is an interesting problem to be thinking about

01:34:03.300 --> 01:34:04.900
and becomes increasingly relevant

01:34:04.900 --> 01:34:08.220
where BERT doesn't know anything about COVID.

01:34:08.220 --> 01:34:10.780
So it's not useful for a bunch of applications,

01:34:10.780 --> 01:34:12.980
even though otherwise fundamentally

01:34:12.980 --> 01:34:15.180
there's nothing wrong with it, right?

01:34:15.180 --> 01:34:19.660
That kind of stuff is just not fun.

01:34:19.660 --> 01:34:21.100
And I think there'll be research

01:34:21.100 --> 01:34:22.860
on trying to sort of fix that.

01:34:22.860 --> 01:34:27.780
What's the current, not necessarily state of the art,

01:34:27.780 --> 01:34:31.620
but kind of current approach for doing this

01:34:31.620 --> 01:34:33.660
at the scale of a GPT-3?

01:34:33.660 --> 01:34:36.460
Like, is it collect more data and retrain from scratch

01:34:36.460 --> 01:34:41.460
or how did they approximate

01:34:41.860 --> 01:34:45.460
or approach some kind of incremental training ability,

01:34:45.460 --> 01:34:46.340
if at all?

01:34:46.340 --> 01:34:51.340
Yeah, so there hasn't been that much work on that front.

01:34:51.340 --> 01:34:54.100
I would say this is something that's, yeah,

01:34:54.100 --> 01:34:56.340
needs a lot more attention.

01:34:56.340 --> 01:35:00.220
Yeah, but I think there'd been parameter efficient training

01:35:00.220 --> 01:35:05.220
on sort of how can we slightly improve the,

01:35:07.060 --> 01:35:10.180
like change the model, but not completely change it.

01:35:10.180 --> 01:35:12.300
Find the set of parameters that we should update

01:35:12.300 --> 01:35:14.900
so that it's not updating the whole parameters,

01:35:14.900 --> 01:35:16.540
but updating a little bit of it.

01:35:16.540 --> 01:35:19.060
Things like that I feel are around,

01:35:19.060 --> 01:35:20.980
but needs a lot more work.

01:35:20.980 --> 01:35:23.900
One way to think about the fundamental problem

01:35:23.900 --> 01:35:25.380
is with the transformer,

01:35:25.380 --> 01:35:28.580
it's not like a layered architecture like a CNN

01:35:28.580 --> 01:35:31.580
where you can just chop off the end layers

01:35:31.580 --> 01:35:33.740
and retrain from that point.

01:35:33.740 --> 01:35:37.300
It's just a much more complex and interconnected model.

01:35:37.300 --> 01:35:42.060
So that kind of incremental updating doesn't work.

01:35:42.060 --> 01:35:43.220
Not so easily, yeah.

01:35:43.220 --> 01:35:46.780
I think there's been some work on sort of taking like,

01:35:46.780 --> 01:35:49.260
hey, 1% of the parameters sort of spread

01:35:49.260 --> 01:35:52.820
over the transformer and updating them with new text.

01:35:52.820 --> 01:35:55.540
But I think, yeah, solving this problem

01:35:55.540 --> 01:35:59.380
is going to be something that needs to happen pretty quick.

01:36:00.540 --> 01:36:03.020
And so to be clear, taking a step back,

01:36:03.020 --> 01:36:06.620
like this is all the looking forward section,

01:36:06.620 --> 01:36:10.620
those three things, kind of misinformation

01:36:10.620 --> 01:36:13.300
and attributable generations, diffusion models

01:36:13.300 --> 01:36:16.180
and online updates were specifically in your category

01:36:16.180 --> 01:36:21.180
of the greatest, most exciting opportunities in the field.

01:36:21.500 --> 01:36:25.980
Areas where we're likely to see a lot of research attention

01:36:27.460 --> 01:36:30.580
and possibly some really interesting results

01:36:30.580 --> 01:36:32.340
coming up in the next year or two.

01:36:32.340 --> 01:36:35.300
And also sort of fundamental problems

01:36:35.300 --> 01:36:37.540
that need to be addressed by language models.

01:36:39.180 --> 01:36:42.620
And so that brings us to your top three predictions

01:36:42.620 --> 01:36:44.780
for the field proper.

01:36:44.780 --> 01:36:46.940
What do you see there?

01:36:46.940 --> 01:36:48.940
Yeah, so I think, and maybe some of it

01:36:48.940 --> 01:36:51.780
is riddled with disappointment as well.

01:36:51.780 --> 01:36:55.500
So the first one here is multiple modalities.

01:36:55.500 --> 01:36:57.020
I think there's been a lot of exciting work,

01:36:57.020 --> 01:36:59.180
so I don't want to sort of take that away.

01:36:59.180 --> 01:37:02.380
But to me, after GPT-3 came out

01:37:02.380 --> 01:37:05.660
and then you saw Clip and DALI and Whisper

01:37:05.660 --> 01:37:09.660
and now there's video models and things like that.

01:37:10.660 --> 01:37:14.460
To me, fundamentally, I understand technically

01:37:14.460 --> 01:37:16.260
why they're not the same model,

01:37:16.260 --> 01:37:17.900
but it's still a little bit disappointing

01:37:17.900 --> 01:37:19.980
that they're not the same model.

01:37:19.980 --> 01:37:22.660
Why is there not the same model

01:37:22.660 --> 01:37:26.660
that trains over the same data GPT-3 is trained on,

01:37:26.660 --> 01:37:31.060
but also on the Lion dataset that does all the images

01:37:31.060 --> 01:37:35.340
and text and audio and video and stuff like that.

01:37:35.340 --> 01:37:39.380
And I think this is a sort of near future prediction

01:37:39.380 --> 01:37:42.820
is that we are going to see ways for pre-training models

01:37:42.820 --> 01:37:45.300
that cuts across multiple modalities.

01:37:45.300 --> 01:37:47.180
And I think Clip was a good example,

01:37:47.180 --> 01:37:49.140
sort of early example of what you can do

01:37:49.140 --> 01:37:51.060
when you have a lot of text and images.

01:37:51.060 --> 01:37:53.980
But I think it still didn't have access

01:37:53.980 --> 01:37:57.300
to a lot of text-only data.

01:37:57.300 --> 01:38:00.820
And I want a model that can do chat GPT-like things,

01:38:00.820 --> 01:38:04.100
but also generate images for me

01:38:04.100 --> 01:38:06.820
and maybe read them out and things like that.

01:38:06.820 --> 01:38:10.220
So I feel like multiple modalities is an exciting

01:38:10.220 --> 01:38:13.380
sort of kind of an opportunity,

01:38:13.380 --> 01:38:16.100
but definitely something that's going to happen soon.

01:38:16.100 --> 01:38:16.940
Mm-hmm.

01:38:16.940 --> 01:38:19.140
Yeah, when I first heard you describe this,

01:38:19.140 --> 01:38:20.260
I thought, well, multimodal,

01:38:20.260 --> 01:38:23.180
like that was the big thing we were talking about

01:38:23.180 --> 01:38:25.940
in these trends conversations last year,

01:38:25.940 --> 01:38:27.860
but you're going a level deeper.

01:38:27.860 --> 01:38:31.940
You don't want multimodal use cases or outputs.

01:38:31.940 --> 01:38:36.220
You want a single architecture to do multimodal things.

01:38:36.220 --> 01:38:37.420
That's what I want.

01:38:37.420 --> 01:38:40.500
My prediction is maybe going to be a little bit more

01:38:40.500 --> 01:38:42.700
grounded, so to say.

01:38:42.700 --> 01:38:46.060
But yeah, you know, like video, for example,

01:38:46.060 --> 01:38:48.020
is a more concrete one, like text to video.

01:38:48.020 --> 01:38:50.620
We've seen some initial versions of those.

01:38:50.620 --> 01:38:54.020
That's probably where a lot of initial stuff would go in.

01:38:54.020 --> 01:38:55.980
But when, and you know, I've been really excited

01:38:55.980 --> 01:38:58.740
about sort of the mind dojo world

01:38:58.740 --> 01:39:01.260
of like playing with text and Minecraft

01:39:01.260 --> 01:39:02.540
and having an agent that can do

01:39:02.540 --> 01:39:04.140
a bunch of things in Minecraft.

01:39:04.140 --> 01:39:07.620
I feel like there are things that models can learn

01:39:07.620 --> 01:39:11.260
from images, even for language modeling,

01:39:11.260 --> 01:39:14.180
it would benefit to see a lot of images in some sense.

01:39:14.180 --> 01:39:16.460
Like there are just a bunch of things in images

01:39:16.460 --> 01:39:19.060
that we never talk about in text.

01:39:19.060 --> 01:39:24.060
And so from an AI agent,

01:39:24.580 --> 01:39:27.180
I think it's useful to think about something

01:39:27.180 --> 01:39:28.300
that has access to everything.

01:39:28.300 --> 01:39:29.820
But yeah, more concretely,

01:39:29.820 --> 01:39:32.100
we're just going to be pushing them sort of pair-wise.

01:39:32.100 --> 01:39:34.340
Yeah, it's going to be audio and images,

01:39:34.340 --> 01:39:36.220
and there's going to be a bunch of other pairs

01:39:36.220 --> 01:39:37.820
that will happen first.

01:39:37.820 --> 01:39:40.900
But eventually I think having multiple,

01:39:40.900 --> 01:39:42.100
actual multiple modalities,

01:39:42.100 --> 01:39:46.180
not just greater than one modalities would be exciting.

01:39:48.020 --> 01:39:48.860
Awesome, awesome.

01:39:48.860 --> 01:39:49.900
Next up.

01:39:49.900 --> 01:39:54.100
Next, I'm kind of excited about better training

01:39:54.100 --> 01:39:56.980
and better inference and better in the sense

01:39:56.980 --> 01:39:59.300
of being more computationally efficient.

01:39:59.300 --> 01:40:02.460
I think this is an exciting work that,

01:40:02.460 --> 01:40:04.020
a bunch of people are already doing,

01:40:04.020 --> 01:40:05.860
but I think this is just going to become

01:40:05.860 --> 01:40:09.980
increasingly important from a sustainability point of view,

01:40:09.980 --> 01:40:13.060
but also from like universities surviving

01:40:13.060 --> 01:40:14.380
and doing interesting things

01:40:14.380 --> 01:40:17.700
and small companies contributing to research.

01:40:17.700 --> 01:40:20.380
I think it's important to be able to train these models,

01:40:20.380 --> 01:40:22.060
to be able to run these models,

01:40:22.980 --> 01:40:25.140
and there's going to be a lot of research

01:40:25.140 --> 01:40:27.100
in trying to do those kinds of things.

01:40:27.100 --> 01:40:30.060
And you've got a few examples that we'll link to

01:40:30.060 --> 01:40:30.900
in the show notes.

01:40:30.900 --> 01:40:34.220
Anything that you want to point out?

01:40:34.220 --> 01:40:38.020
Yeah, so let me mention two that I saw recently.

01:40:38.020 --> 01:40:41.980
One of them is this paper called cramming.

01:40:41.980 --> 01:40:44.540
And the idea here is to,

01:40:44.540 --> 01:40:47.460
they think about the scaling laws paper,

01:40:47.460 --> 01:40:49.980
like, hey, what can you do when the models get larger

01:40:49.980 --> 01:40:51.220
and stuff like that?

01:40:51.220 --> 01:40:54.140
The cramming paper sort of turns it on his head

01:40:54.140 --> 01:40:58.740
and decides, okay, what if I have just one GPU for one day?

01:40:58.740 --> 01:41:00.780
What's the most I can do with that?

01:41:01.700 --> 01:41:03.780
And it's a very sort of different question,

01:41:03.780 --> 01:41:07.500
but it somehow is a lot more relevant to many more people,

01:41:07.500 --> 01:41:11.420
because a lot more people have a single GPU for a single day

01:41:11.420 --> 01:41:13.340
and they show that you can get almost

01:41:13.340 --> 01:41:15.180
sort of bird level performance

01:41:15.180 --> 01:41:16.420
if you make the right choices

01:41:16.420 --> 01:41:19.820
and they sort of detail what those choices might look like.

01:41:19.820 --> 01:41:22.700
It's a paper, but I think I like that idea of like,

01:41:22.700 --> 01:41:27.020
hey, what if we were scrappy about training these models?

01:41:27.020 --> 01:41:28.020
How far can we get?

01:41:28.020 --> 01:41:30.220
I think that's a very interesting question

01:41:30.220 --> 01:41:34.420
that Google and OpenAI is not gonna be asking,

01:41:34.420 --> 01:41:37.100
but might be relevant for a lot of research.

01:41:37.100 --> 01:41:40.900
The other one I want to talk about is this Petals work

01:41:40.900 --> 01:41:42.700
that came out of the big science thing.

01:41:42.700 --> 01:41:44.220
I haven't read too much about this,

01:41:44.220 --> 01:41:46.820
but it seems like a really interesting idea

01:41:46.820 --> 01:41:50.900
of the problem of running really large language models.

01:41:50.900 --> 01:41:54.860
So even if OPT releases 175 billion model,

01:41:54.860 --> 01:41:56.420
how do you actually run it?

01:41:56.420 --> 01:41:58.460
It doesn't really help most people,

01:41:58.460 --> 01:42:00.180
even if you have a big cluster,

01:42:00.180 --> 01:42:02.180
it's kind of difficult to run it.

01:42:02.180 --> 01:42:06.660
So what this Petals does is they're building this framework

01:42:06.660 --> 01:42:09.420
for using the ideas behind BitTorrent

01:42:10.300 --> 01:42:12.780
of sort of distributed computing

01:42:12.780 --> 01:42:14.380
and bringing it to language models.

01:42:14.380 --> 01:42:15.980
So like, hey, you should be able to run

01:42:15.980 --> 01:42:19.340
these 100 billion size language models,

01:42:19.340 --> 01:42:24.060
language models distributed over a bunch of commodity

01:42:24.060 --> 01:42:27.620
sort of consumer computers.

01:42:27.620 --> 01:42:29.380
So yeah, I think this is an interesting idea.

01:42:29.380 --> 01:42:30.460
I haven't played around with it

01:42:30.460 --> 01:42:33.020
and see how far you can push it.

01:42:33.020 --> 01:42:35.180
That's partly, you need a bunch of people

01:42:35.180 --> 01:42:38.420
also running Petals, but once we get there,

01:42:38.420 --> 01:42:40.380
I think that could be a pretty exciting way

01:42:40.380 --> 01:42:42.060
to run language models.

01:42:42.060 --> 01:42:44.180
Interesting, interesting.

01:42:44.180 --> 01:42:48.860
So your third prediction is editing

01:42:48.860 --> 01:42:50.460
and revising models.

01:42:50.460 --> 01:42:51.660
What do you mean there?

01:42:51.660 --> 01:42:55.020
So these are these family of models

01:42:55.020 --> 01:42:59.540
that are not so much interested in generating text,

01:42:59.540 --> 01:43:02.620
but taking existing text and editing it.

01:43:03.660 --> 01:43:06.460
And I think this is a very interesting idea

01:43:06.460 --> 01:43:08.540
that can become increasingly important.

01:43:08.540 --> 01:43:11.940
And in some sense, this could be the way

01:43:11.940 --> 01:43:14.940
you fix language model output potentially,

01:43:14.940 --> 01:43:17.700
is to have another model that takes the output

01:43:17.700 --> 01:43:19.500
of the language model and fixes it.

01:43:20.940 --> 01:43:22.660
So some of the work here,

01:43:22.660 --> 01:43:27.580
there was a paper out of Julia's group from YouTube now,

01:43:27.580 --> 01:43:30.820
that sort of looked at summarization.

01:43:30.820 --> 01:43:33.380
And there are systems that generate summaries.

01:43:33.380 --> 01:43:35.260
How can you take that generated summaries

01:43:35.260 --> 01:43:37.820
and edit it to correct all the factual mistakes

01:43:37.820 --> 01:43:38.660
it has made?

01:43:39.740 --> 01:43:42.460
And editing is somehow a much,

01:43:42.460 --> 01:43:44.380
let's not say definitely a simpler problem,

01:43:44.380 --> 01:43:47.340
but in some sense, it could be a simpler problem

01:43:47.340 --> 01:43:50.100
than writing the whole summary from scratch,

01:43:50.100 --> 01:43:51.700
especially when you do the writing,

01:43:51.700 --> 01:43:53.620
you do left to right generation.

01:43:53.620 --> 01:43:55.980
So you can't go back and revisit something

01:43:55.980 --> 01:43:57.220
that you've done before.

01:43:57.220 --> 01:43:58.620
With these editing models,

01:43:58.620 --> 01:44:01.700
they have the whole picture to some degree,

01:44:01.700 --> 01:44:03.900
and all they have to do is fix it

01:44:03.900 --> 01:44:05.580
so that the picture is consistent.

01:44:06.780 --> 01:44:10.380
And so this idea seems like potentially simpler

01:44:10.380 --> 01:44:11.900
than generation.

01:44:11.900 --> 01:44:14.500
So you could generate something,

01:44:14.500 --> 01:44:16.660
and maybe this is also attached to diffusion models

01:44:16.660 --> 01:44:19.500
where you write something that's maybe not so correct,

01:44:19.500 --> 01:44:21.900
but you revise it and it becomes better.

01:44:21.900 --> 01:44:24.620
So there is a bunch of work along these directions

01:44:24.620 --> 01:44:27.300
that came out essentially this year,

01:44:27.300 --> 01:44:29.420
maybe second half of this year,

01:44:29.420 --> 01:44:31.140
some of it early on,

01:44:31.140 --> 01:44:35.580
that tries to gather data sets where you have edits,

01:44:35.580 --> 01:44:37.380
or try to maybe even generate data sets

01:44:37.380 --> 01:44:38.700
where you have edits,

01:44:38.700 --> 01:44:42.380
and create these models that are able to fix those edits.

01:44:42.380 --> 01:44:46.260
And so the prediction specifically is that

01:44:47.780 --> 01:44:50.660
teams will build on this and produce models

01:44:50.660 --> 01:44:53.780
that can actually kind of deliver on

01:44:54.940 --> 01:44:58.980
the ability to do editing and revising.

01:44:58.980 --> 01:45:02.300
And I think this could be, for example,

01:45:02.300 --> 01:45:05.620
there'll be an editing model that can fix bias issues.

01:45:05.620 --> 01:45:08.220
There'll be an editing model that fixes toxicity.

01:45:08.220 --> 01:45:11.460
There'll be an editing model that fix factuality.

01:45:11.460 --> 01:45:14.740
And these editing models can make web searches

01:45:14.740 --> 01:45:18.140
and sort of take that information and edit the output.

01:45:18.140 --> 01:45:23.140
So I could imagine that this could be a practical way

01:45:23.500 --> 01:45:26.060
of solving many of the issues in language modeling.

01:45:26.060 --> 01:45:29.180
It is a really interesting idea that,

01:45:29.180 --> 01:45:31.100
I don't know if it's like a separation of concerns

01:45:31.100 --> 01:45:33.980
or something like the language model

01:45:33.980 --> 01:45:36.380
doesn't necessarily need to do everything

01:45:36.380 --> 01:45:38.700
if we can compensate.

01:45:38.700 --> 01:45:41.380
So in a way, it's like decomposition as well.

01:45:41.380 --> 01:45:46.180
Like let it generate if the way to get

01:45:47.460 --> 01:45:49.380
something that's not toxic that's accurate

01:45:49.380 --> 01:45:53.620
is to have another type of model support it.

01:45:53.620 --> 01:45:54.460
Great.

01:45:54.460 --> 01:45:56.780
Yeah, I think, yeah, that's right.

01:45:56.780 --> 01:45:59.700
And then for, at least for summarization

01:45:59.700 --> 01:46:01.700
and things where it's supposed to be factual

01:46:01.700 --> 01:46:02.540
and stuff like that,

01:46:02.540 --> 01:46:05.660
I could see it sort of addressing those problems.

01:46:05.660 --> 01:46:08.100
Of course, if it's generating a long text

01:46:08.100 --> 01:46:12.500
and there are longer range sort of consistency issues

01:46:12.500 --> 01:46:13.340
and stuff like that,

01:46:13.340 --> 01:46:14.380
it might be a little bit difficult

01:46:14.380 --> 01:46:17.420
for editing models to come into picture there.

01:46:17.420 --> 01:46:19.420
What I like about editing is also,

01:46:19.420 --> 01:46:21.380
it's something that we can imagine

01:46:21.380 --> 01:46:23.420
not only working on language model output,

01:46:23.420 --> 01:46:25.540
but working on a human output

01:46:25.540 --> 01:46:28.820
or text that's been written with the writing assistant

01:46:28.820 --> 01:46:29.660
and things like that, right?

01:46:29.660 --> 01:46:30.780
Like you can still go back

01:46:30.780 --> 01:46:33.780
and do a post-processing editing step to polish it up.

01:46:33.780 --> 01:46:35.580
And I think that could be very useful as well.

01:46:35.580 --> 01:46:40.580
So our last category in the NLP predictions

01:46:41.420 --> 01:46:46.180
is top people, companies, organizations, teams

01:46:46.180 --> 01:46:49.460
to watch in the field 2023.

01:46:49.460 --> 01:46:54.260
Of course, the caveat of you're not,

01:46:55.540 --> 01:46:58.820
any omissions here are not to slight the work

01:46:58.820 --> 01:47:00.020
of any particular team,

01:47:00.020 --> 01:47:02.900
but like who's got your mind share

01:47:02.900 --> 01:47:07.220
and who are you expecting to see interesting things

01:47:07.220 --> 01:47:09.980
from in the upcoming year?

01:47:09.980 --> 01:47:13.380
Yeah, so this has been a little bit difficult question,

01:47:13.380 --> 01:47:14.220
I think every year,

01:47:14.220 --> 01:47:15.380
but one thing I will say,

01:47:15.380 --> 01:47:17.300
and this is maybe the most obvious answer

01:47:17.300 --> 01:47:20.100
is to sort of keep an eye on open AI

01:47:20.100 --> 01:47:21.900
and what they're up to, right?

01:47:21.900 --> 01:47:24.900
I think people, once they do something,

01:47:24.900 --> 01:47:26.220
people always come back and say,

01:47:26.220 --> 01:47:28.300
look, what they've done is not so exciting.

01:47:28.300 --> 01:47:29.660
Oh, they only scaled it up

01:47:29.660 --> 01:47:32.860
or oh, they only did this additional thing.

01:47:32.860 --> 01:47:36.020
But the fact is that they are the first ones to do it.

01:47:36.020 --> 01:47:38.900
They're the first ones to bring it out, make it available.

01:47:38.900 --> 01:47:43.900
And that is, and get people excited about language models

01:47:44.380 --> 01:47:45.780
in a way that they weren't before.

01:47:45.780 --> 01:47:49.900
So that happened with GPT-2, GPT-3 and chat-GPT.

01:47:49.900 --> 01:47:53.060
And I'm sure GPT-4 will have the same thing.

01:47:53.060 --> 01:47:55.220
I'm sure retroactively,

01:47:55.220 --> 01:47:58.420
we will all talk about what the problems with GPT-4 are

01:47:58.420 --> 01:48:02.420
and how it's incrementally only training on more data

01:48:02.420 --> 01:48:05.620
or has more parameters or whatever it is.

01:48:05.620 --> 01:48:07.060
But I think qualitatively,

01:48:07.060 --> 01:48:09.420
it'll bring something interesting to the table.

01:48:09.420 --> 01:48:10.980
And I'm really curious about

01:48:10.980 --> 01:48:14.500
what that next interesting thing is going to be.

01:48:15.460 --> 01:48:20.300
Do you think the general predictions

01:48:20.300 --> 01:48:22.260
that are kind of floating around,

01:48:22.260 --> 01:48:27.260
basically spring and 100 trillion parameters,

01:48:30.500 --> 01:48:32.060
is your money on those?

01:48:32.060 --> 01:48:36.900
I mean, to sort of have a completely different perspective,

01:48:36.900 --> 01:48:40.780
I think this is also a nice model that came out,

01:48:40.780 --> 01:48:42.580
this nice paper that came out a little earlier

01:48:42.580 --> 01:48:44.780
called the Chinchilla paper.

01:48:44.780 --> 01:48:46.940
This was a paper that show that these models

01:48:46.940 --> 01:48:51.420
are extremely under-trained and they are data hungry.

01:48:51.420 --> 01:48:55.780
So one version of GPT-4 could be potentially

01:48:55.780 --> 01:48:57.420
not even a different architecture,

01:48:57.420 --> 01:48:59.060
not even more parameters.

01:48:59.060 --> 01:49:02.100
Like exactly, let's keep it 175 billion

01:49:03.020 --> 01:49:06.380
and let's just somehow get 10 times the data

01:49:06.380 --> 01:49:09.700
if you can potentially get that spot somewhere, right?

01:49:09.700 --> 01:49:10.540
I could totally-

01:49:10.540 --> 01:49:14.940
But then everybody that shared the image

01:49:14.940 --> 01:49:17.620
with the little dot and the big dot would be totally wrong.

01:49:17.620 --> 01:49:21.900
Well, yeah, they'll just sort of replace that with data

01:49:21.900 --> 01:49:24.100
and it might still work, right?

01:49:24.100 --> 01:49:25.580
For those not on Twitter,

01:49:25.580 --> 01:49:30.580
that has dominated LLM Twitter over the past couple of days.

01:49:30.900 --> 01:49:32.340
Is there even...

01:49:34.140 --> 01:49:36.460
I think that when GPT-3 came out,

01:49:36.460 --> 01:49:41.460
the kind of colloquial articulation of what they did

01:49:42.220 --> 01:49:45.660
was like train this language model on the entire internet.

01:49:45.660 --> 01:49:49.140
Like, is there 10X more data to train on?

01:49:49.140 --> 01:49:49.980
Yeah, I don't know.

01:49:49.980 --> 01:49:51.500
I don't know how much they've trained on

01:49:51.500 --> 01:49:52.420
and how much there is.

01:49:52.420 --> 01:49:55.260
I mean, there's definitely 10X more data.

01:49:55.260 --> 01:49:58.980
There is a lot of stuff that's proprietary, right?

01:49:58.980 --> 01:50:00.100
Proprietary?

01:50:00.100 --> 01:50:01.780
Maybe even proprietary, right?

01:50:01.780 --> 01:50:02.620
Like-

01:50:02.620 --> 01:50:05.780
Transcribe a bunch of videos and audio and books

01:50:05.780 --> 01:50:07.100
and I guess, yeah.

01:50:07.100 --> 01:50:10.900
They do have that Whisper model that does, yeah,

01:50:10.900 --> 01:50:12.820
that does really good transcribing.

01:50:12.820 --> 01:50:13.660
So they could use that.

01:50:13.660 --> 01:50:14.500
Good point.

01:50:14.500 --> 01:50:15.580
They didn't create that for no reason.

01:50:15.580 --> 01:50:16.980
Right, yeah.

01:50:16.980 --> 01:50:19.780
They also can go into scientific papers

01:50:19.780 --> 01:50:23.260
and I don't think the 48 million papers

01:50:23.260 --> 01:50:26.780
that Galactica was trained on was something

01:50:26.780 --> 01:50:27.940
GPT-3 was trained on.

01:50:27.940 --> 01:50:29.900
And I think that is a pretty valuable resource.

01:50:29.900 --> 01:50:31.420
That Galactica paper also showed

01:50:31.420 --> 01:50:33.740
that even on mathematical reasoning and things like that,

01:50:33.740 --> 01:50:35.660
they were actually better.

01:50:35.660 --> 01:50:37.660
So these scientific papers may be useful

01:50:37.660 --> 01:50:39.900
for a bunch of other things that we don't realize.

01:50:39.900 --> 01:50:42.300
So yeah, I think where that data comes from

01:50:42.300 --> 01:50:43.140
is unclear to me,

01:50:43.140 --> 01:50:46.340
but it's clear that more data is somehow

01:50:46.340 --> 01:50:49.260
maybe even more interesting than more parameters.

01:50:49.260 --> 01:50:54.220
And more data could include more RLHF style things, right?

01:50:54.220 --> 01:50:56.260
Like, I don't know what to open it.

01:50:56.260 --> 01:50:57.140
Okay.

01:50:57.140 --> 01:51:00.700
The other top company to, I would say again,

01:51:00.700 --> 01:51:03.660
continue taking a look at is Hugging Face.

01:51:03.660 --> 01:51:06.460
I've been constantly sort of amazed

01:51:06.460 --> 01:51:09.580
by how much they've been doing.

01:51:09.580 --> 01:51:13.460
One of the sort of key insights is like EMNLP,

01:51:13.460 --> 01:51:15.460
which is this top conference in NLP,

01:51:15.460 --> 01:51:18.260
has this demo track where they highlight

01:51:18.260 --> 01:51:19.820
sort of not research papers,

01:51:19.820 --> 01:51:24.380
but products sort of demos that are relevant for research.

01:51:24.380 --> 01:51:28.340
And for the last three years, I think at EMNLP,

01:51:28.340 --> 01:51:31.860
Hugging Face has got the best demo paper award.

01:51:31.860 --> 01:51:35.100
And that kind of thing sort of shows

01:51:36.020 --> 01:51:38.060
how they've been doing very different things,

01:51:38.060 --> 01:51:41.380
but also doing things that are impactful and interesting.

01:51:41.380 --> 01:51:43.940
So the two I want to highlight this year is,

01:51:43.940 --> 01:51:45.660
again, they've done many, many things,

01:51:45.660 --> 01:51:47.260
but the one I want to highlight

01:51:47.260 --> 01:51:50.100
is the Evaluate system,

01:51:50.100 --> 01:51:53.660
where they had this whole evaluation framework

01:51:53.660 --> 01:51:56.940
for reproducing evaluations and evaluating models

01:51:56.940 --> 01:51:59.060
and making all of this stuff really easy.

01:51:59.060 --> 01:52:00.820
So you can introduce a new metric

01:52:00.820 --> 01:52:03.860
and evaluate it on thousands of models, things like that,

01:52:03.860 --> 01:52:05.780
make it really easy to compare models,

01:52:05.780 --> 01:52:09.060
make it really easy to reproduce papers.

01:52:09.060 --> 01:52:11.980
And I think that that's a really valuable service

01:52:11.980 --> 01:52:13.980
to do research.

01:52:13.980 --> 01:52:15.660
And the other one that I sort of,

01:52:15.660 --> 01:52:17.580
we also started with this of like,

01:52:17.580 --> 01:52:20.260
hey, what's happening inside the pre-training data?

01:52:20.260 --> 01:52:23.700
One of the tools they have is this Roots search tool

01:52:23.700 --> 01:52:26.740
that takes the Roots pre-training data,

01:52:26.740 --> 01:52:29.580
but allows you to search it and find all kinds of things

01:52:29.580 --> 01:52:31.500
that are happening inside that pre-training data.

01:52:31.500 --> 01:52:33.660
So if you have a specific prediction,

01:52:33.660 --> 01:52:34.500
then you want to be like,

01:52:34.500 --> 01:52:36.180
hey, is there anything in the training data

01:52:36.180 --> 01:52:38.260
that looks exactly like this?

01:52:38.260 --> 01:52:41.060
You can do that search and get some results.

01:52:41.060 --> 01:52:45.300
So I think they're just being pretty creative

01:52:45.300 --> 01:52:48.940
and thoughtful about what is useful and building tools,

01:52:48.940 --> 01:52:50.060
and that's been exciting.

01:52:50.060 --> 01:52:51.900
And the last one that I'll bring up,

01:52:51.900 --> 01:52:56.060
and this is something that was on top of my head this week,

01:52:56.060 --> 01:52:57.620
but it can change.

01:52:57.620 --> 01:53:02.620
It's a group called OUGHT, it's O-U-G-H-T,

01:53:03.140 --> 01:53:07.820
I believe it's OUGHT.org, it's a website.

01:53:07.820 --> 01:53:11.500
And this is sort of a research nonprofit.

01:53:11.500 --> 01:53:15.980
And they've been doing sort of interesting things

01:53:18.260 --> 01:53:20.060
related to sort of building tools.

01:53:20.060 --> 01:53:22.900
So they have this tool called Primer,

01:53:22.900 --> 01:53:26.540
and this is going back to decomposed reasoning.

01:53:26.540 --> 01:53:29.340
This tool called Primer, you can give it a question

01:53:29.340 --> 01:53:31.380
and it tries to come up with an answer,

01:53:31.380 --> 01:53:33.660
but in the process of coming up with an answer,

01:53:33.660 --> 01:53:37.900
it can do a web search, or it can write a small program,

01:53:37.900 --> 01:53:39.340
and it can do all of these things,

01:53:39.340 --> 01:53:41.700
and they've built a sort of nice tool

01:53:41.700 --> 01:53:44.300
to be able to visualize what the decompositions are

01:53:44.300 --> 01:53:46.460
and what sort of things are being done.

01:53:46.460 --> 01:53:49.940
So it's a really interesting use case of language models.

01:53:50.980 --> 01:53:55.060
And then they also have another tool called Ellicit,

01:53:55.060 --> 01:53:58.300
which is, in some sense, it's a little bit like Galactica,

01:53:58.300 --> 01:54:02.900
but it's not so much interested in generating papers for you

01:54:02.900 --> 01:54:05.900
but helping you do research for your paper, right?

01:54:05.900 --> 01:54:08.500
So you have a specific question,

01:54:08.500 --> 01:54:11.180
it's going to find a bunch of relevant papers,

01:54:11.180 --> 01:54:13.460
take out snippets from those papers,

01:54:13.460 --> 01:54:16.140
and be able to do that.

01:54:16.140 --> 01:54:19.700
So I don't know, they've had to have a bunch of tools

01:54:19.700 --> 01:54:22.660
that when I'm looking at decomposed reasoning,

01:54:22.660 --> 01:54:24.620
it comes up, and I'm looking at,

01:54:24.620 --> 01:54:27.300
okay, research assistance, it sort of comes up,

01:54:27.300 --> 01:54:29.340
and so it's been interesting to see,

01:54:29.340 --> 01:54:31.180
and I'm curious what they'll do next.

01:54:31.180 --> 01:54:33.100
I'm really curious about that,

01:54:33.100 --> 01:54:35.300
and I'm gonna look into that in more detail.

01:54:35.300 --> 01:54:37.900
So, awesome, awesome.

01:54:39.740 --> 01:54:42.180
Well, I think we are done.

01:54:42.180 --> 01:54:45.140
Like, you've been a champ, this has been awesome.

01:54:46.460 --> 01:54:47.620
It's been fun, yeah.

01:54:48.660 --> 01:54:53.100
Yeah, no, I mean, you rose to the occasion

01:54:53.100 --> 01:54:57.980
of kind of capturing an amazing year in NLP, for sure.

01:54:57.980 --> 01:55:01.740
So thanks so much for joining us.

01:55:01.740 --> 01:55:02.860
Yeah, thanks for inviting me.

01:55:02.860 --> 01:55:05.740
I think the time sort of justifies

01:55:05.740 --> 01:55:09.220
how much this year had in NLP this year,

01:55:09.220 --> 01:55:12.220
and I'm really curious to see where NLP is going to go.

01:55:12.220 --> 01:55:15.820
I will mention that ChatGPT came out right before,

01:55:15.820 --> 01:55:17.860
or I think maybe even during Eurips.

01:55:17.860 --> 01:55:22.020
So I attended Eurips, and I saw the firsthand experience

01:55:22.020 --> 01:55:24.220
of the whole machine learning community there.

01:55:24.220 --> 01:55:27.180
Then I flew to Abu Dhabi to attend EMNLP,

01:55:27.180 --> 01:55:28.660
and that's where I saw the reaction

01:55:28.660 --> 01:55:31.020
of the whole NLP community.

01:55:31.020 --> 01:55:33.220
And it's been interesting to see sort of

01:55:33.220 --> 01:55:36.420
how the reactions have sort of spanned

01:55:37.300 --> 01:55:39.020
both optimism and excitement,

01:55:39.020 --> 01:55:40.540
which is kind of where I am,

01:55:40.540 --> 01:55:44.260
like to see like, hey, what can we build with this stuff?

01:55:44.260 --> 01:55:45.940
To pessimism where they're like,

01:55:45.940 --> 01:55:48.260
oh, you know, it doesn't really,

01:55:48.260 --> 01:55:49.820
yeah, it's not gonna change anything,

01:55:49.820 --> 01:55:52.540
it's just a bigger language model,

01:55:52.540 --> 01:55:54.260
all the way to, essentially,

01:55:54.260 --> 01:55:56.540
I want to say some form of denial,

01:55:56.540 --> 01:55:57.860
where it's like, look,

01:55:57.860 --> 01:56:01.660
it's behind propriety, closed off system,

01:56:01.660 --> 01:56:05.260
and therefore it doesn't matter to do research,

01:56:05.260 --> 01:56:08.460
and that's definitely not a take I agree with.

01:56:08.460 --> 01:56:10.460
So yeah, it's been exciting.

01:56:10.460 --> 01:56:13.180
And there's also a fourth, which maybe is less so,

01:56:13.180 --> 01:56:15.940
and I don't, maybe less so in the research community

01:56:15.940 --> 01:56:17.660
than in the general sphere,

01:56:17.660 --> 01:56:21.020
which is fear of the implications of it.

01:56:23.540 --> 01:56:26.060
Did you find that less so on the research side?

01:56:26.060 --> 01:56:29.580
I guess less so, definitely less so on the, yeah,

01:56:29.580 --> 01:56:31.020
because I think we've been,

01:56:31.900 --> 01:56:33.340
there is a little bit of fear

01:56:33.340 --> 01:56:35.140
becoming a little bit more obvious,

01:56:35.140 --> 01:56:36.980
but I think the community,

01:56:36.980 --> 01:56:38.060
because of a lot of people

01:56:38.060 --> 01:56:39.620
who've been sort of pointing out problems

01:56:39.620 --> 01:56:41.500
with large language models for a while,

01:56:41.500 --> 01:56:45.420
we are kind of, we know what not to,

01:56:45.420 --> 01:56:47.940
as a community, we should know what not to do,

01:56:47.940 --> 01:56:49.620
but it is a little bit scary

01:56:49.620 --> 01:56:51.220
when people are using it for things

01:56:51.220 --> 01:56:54.020
that clearly, at the onset, should be like,

01:56:54.020 --> 01:56:56.180
hey, why are you doing this, right?

01:56:56.180 --> 01:56:57.620
Yeah, yeah, yeah.

01:56:57.620 --> 01:56:58.700
Awesome.

01:56:58.700 --> 01:57:01.700
Well, once again, Samir, thanks so much.

01:57:01.700 --> 01:57:03.460
Really great session and conversation

01:57:03.460 --> 01:57:06.460
and appreciate all the work you put into prepping for it.

01:57:06.460 --> 01:57:07.300
Yeah, thank you, Sam.

01:57:07.300 --> 01:57:24.300
It was fun.

