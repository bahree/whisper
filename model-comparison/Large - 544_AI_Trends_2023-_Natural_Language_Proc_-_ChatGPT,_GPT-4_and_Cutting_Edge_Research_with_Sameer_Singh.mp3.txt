All right, everyone, welcome to our AI Trends 2023 series.
Each year, we invite friends of the show
to join us to recap key developments of the year
and anticipate future advancements
in the most interesting subfields in AI.
And today we're joined by Samir Singh.
Samir is an associate professor
in the Department of Computer Science at UC Irvine
and a fellow at the Allen Institute
for Artificial Intelligence, or AI2,
to talk through some of the key research developments
in NLP.
Of course, before we get going,
take a moment to hit that subscribe button
wherever you're listening to today's show,
and you can also follow us on TikTok and Instagram,
at Twiml AI, for highlights from every episode.
All right, let's jump in.
Samir, welcome back to the podcast and our Trends series.
Yeah, thank you for having me, Sam.
It's great to be back.
It's super excited to have you back.
We were joking a little bit before we got rolling
that we picked big years to have you on.
The last one was our 2020,
right in the wake of GPT-3, a big year.
And of course, this has been a huge year for NLP
with the relatively recent release of ChatGPT.
Yeah, it's always kind of crazy
when you have these big changes happening in the year
where there is research still going on in parallel
and people are exploring research questions,
and a lot of them either become obsolete
or have to be revisited and things like that
in the middle of the year.
And in this year, especially,
it was much closer to the end of the year.
So looking back at a year,
it's always interesting to think about the trajectory
and what ideas will still persist and what won't.
Yeah, yeah, a great point.
ChatGPT happened right at the end of the year.
Do you think we'd have the same sense
that this was a huge year in NLP
if it wasn't for that late year release of ChatGPT?
Oh, definitely.
I think this year has been really impressive.
I would say even bigger,
even if you take about ChatGPT,
overall, this year has been really big for NLP,
even compared to sort of the year GPT-3 came out.
So yeah, I think they've been,
I feel like it took us a while to come in terms
with what these large language models are capable of
or what they clearly fail at and what they're good at
and try to sort of build better tooling around it,
build better support systems around it.
And so, yeah, I think this year has been good,
even if you don't take it on ChatGPT, yeah.
Awesome, well, we're gonna dig into ChatGPT
in a fair amount of detail,
as well as some of the other advances you just hinted at.
But before we do,
I'd love to have you take a few minutes
to just kind of introduce yourself to our audience
with a focus on kind of your research focus
and what your interests are.
Cool, yeah, so I've been working in NLP
for a long time now,
but my focus has mostly been looking at
when these language models or machine learning in general
gets interfaced with real users,
what are the needs that sort of are there?
So a lot of my work has been in explanations
and interpretability, but also in robustness,
both from an adversarial perspective,
but also from out of domain generalization perspective.
And also in terms of evaluation,
like how do we know whether the models are doing well,
how well are they doing?
And in general, be able to understand and predict
when the models would work
and when the models would not work.
And I'm imagining that the advent of large language models
and the kind of the dominance of that approach
to NLP modeling is,
well, it certainly changed the tools
and the approach that you take.
Has it changed kind of the fundamental way
that you approach the problem?
To some degree, yes and no.
I think it has made a lot of my work obsolete
in the sense that we were doing a really good job
of finding fundamental faults
in a lot of these language models,
and turns out a lot of them go away
when you have a lot more data or a lot larger size.
And so the specific observations and insights we had,
not all of them have persisted,
but the other differentiation we had in our work
was always being somewhat model agnostic
or try to use a black box approach to the model
rather than looking inside what's going on.
And that is something that you can use
in this world of only access through API,
a lot of those tooling can still work out.
So yeah, so it's been a mix,
but it's been exciting to sort of continue to do that.
Well, you've identified some themes that from your purview
have been some of the key topic areas and research
that have emerged in the field over the past year.
Let's start there.
And maybe before we dive into any of the individual items,
what's your take on 2022 broadly
and some of the areas
that you are most excited about in the year?
Yeah, so I think broadly speaking,
and we will delve deeper into a bunch of these topics,
but broadly speaking, I think the importance of data
and the importance of looking at what might be
in the pre-training data
has sort of brought up back into the focus
in a way that I feel earlier years,
we were a lot more agnostic
of what the model was being trained on
and just more data was better kind of thing.
This year, it's been a lot more sort of thinking
about what goes in the models
and also thinking of ways to use the models,
not just by simply prompting it with a simple thing,
but trying to get it to reason,
trying to get it to break down the problem into pieces
and try to evaluate how much the language models
can do that.
And that I think is key when you start thinking
about taking language models to more higher level
decision-making or higher level reasoning.
Awesome, awesome.
What's the first area you'd like to dig into?
So let's actually start with a chain of thought prompting.
This is work coming out of Google
that came out earlier this year.
And I guess the easiest way to summarize
is to say let's think step-by-step.
The idea here is to have the model
not just generate the answer directly,
but try to have it go through the reasoning process
and then arrive at the answer.
This ended up being quite a strong,
like quite an effective method
to get the model to do a lot of things,
especially when it comes to mathematical reasoning
and sort of where you can break down the problems
into a bunch of these things step.
Chain of thought prompting did extremely well
compared to what we had before.
And part of the difference, I guess,
was you're not just prompting with questions and answers,
but you're also prompting with something
that is much more detailed.
So the prompt itself has a bunch of examples
of breaking the reasoning down,
and then you have the model being able to walk
through that reasoning and get to the answer.
And in that work is the idea that the user of the model
should break the prompts down into more detail,
or that the model should learn how to kind of show its work
and given a coarse-grained prompt,
break the prompt down itself?
Yes, I think the initial paper focused on the user
providing a few examples of this breakdown, right?
So if you're saying like,
here's a mathematical word problem,
you have two apples,
and then somebody gives you double of that,
how many apples do you have?
Breaking it down into a double means times two,
and two times two is four.
I mean, this is a very simple example,
but this kind of giving an example or two
of breaking this down can be quite powerful
for a language model.
Especially, I think, one of the key insights here,
and we can talk about other papers
that sort of showed related things,
but this is a very emergent property
that seems to exist for really large language models.
And if you have smaller language models,
it's kind of difficult to get them to do
this kind of reasoning.
So that's also been exciting to see.
Did the results there, did you find them surprising?
Were they counterintuitive that that would work?
I think how well they worked were,
I think it surprised everyone,
because it's a very simple idea
to just break it down a little bit.
Everybody kind of assumed that the transformers
are sort of either doing this internally
or completely not doing this internally, right?
And by showing you that if you actually write out
a bunch of examples, these transformer models
are able to do this to the extent that they are,
was quite surprising, and the gains were quite impressive.
Can you talk a little bit about the evaluation
of that method?
Yeah, so the evaluation was mostly focused
on mathematical word problems.
So there's this GSM8 data set,
and then there's this MAWPSMOPS, I guess,
data set as well.
These are mathematical word problems.
And this first evaluation was mostly looking at
how well you can do a reason through some of those.
And yeah, it was much, much better
than anything that we had before.
And then they had some evaluations
on symbolic reasoning as well.
So if you give them sort of tasks,
which have like, not have a bunch of,
so like finding a character inside a long string,
like what is the fifth character or something like that,
you can break it down into a bunch of steps.
And if you give it a few examples, it can do it.
If you don't give it a few examples
of how to break it down, the models are very bad.
And have you seen any work that looks to extend this
beyond the kind of math and symbolic domain?
So beyond, I'll talk a little bit about
some of the related ideas
and sort of question answering a little bit later.
But there is one work that is related that I like.
This is called algorithmic prompting.
And this is stuff that came out of Google Brain as well.
So, you know, a lot of the stuff
is coming out of Google Brain
because you need really large language models
to be able to even work with this,
so even bigger than GPT-3, for example.
So in this algorithmic prompting paper,
this was kind of interesting where they had
essentially the same idea as chain of thought,
except that they go really detailed
into what those reasoning steps would be.
So they mostly focus on, you know,
things that can be described more as an algorithm
rather than as just breaking it into a few pieces.
So you can say things like,
if I had to add 12 plus 24, right?
How would you do that?
They literally break it down into digits.
So you take the ones place,
that's two in one case, four in the other.
You add them up, you get six, there is no carry.
Okay, that's one.
The second step is, okay, now look at the next tens place.
It's one and two, add them up, three.
Look at the carry or the carry is zero.
So it's just three and then 36, right?
So all of this, this very detailed breakdown,
which looked like extremely detailed.
But what was really impressive to me about that paper
is they showed that you can give examples
of really low digit operations.
So like maybe two or three digit operations
when you're talking about addition
or multiplication or any of these things.
But at test time, you can,
firstly, even on two and three digit stuff,
it was much, much accurate
compared to regular chain of thought.
Like 20% going from 80% of chain of thought
to something that's 100%.
I'm kind of making up numbers.
And this is relative to asking for the model
to solve the same problem without any intermediate steps.
No, so without intermediate steps is even worse, right?
So this is asking the model to, so like 12 plus 24,
I don't know exactly what the chain of thought would be,
but it would be something
that would be at a higher granularity,
let's just say, right?
And so when you give these detailed prompts,
the models are more accurate, which is not so surprising.
But what was surprising was that they kept increasing
the size of the number at test time.
So started adding more and more digits
and even up to 18 digit numbers.
The model is able to do these operations
much, much more accurately,
even though the prompts were only on two or three digit
sort of numbers, right?
And so does this type of work answer definitively
whether this is already happening inside the model
versus there's some other effects?
Like in a sense, it's really counterintuitive
that it would work at all.
There's no registers inside the model
that are tracking digits,
the ones place and the tens place.
Why should that work?
Yeah, so I think people are still trying to come to terms
with why chain of thought reasoning works.
Is there something in the pre-training data?
Is there something in the model?
And there's been some interesting work there.
But no, I think the tricky thing here
is you're making all of these things explicit.
So you're not relying on the model to keep these bits
somewhere latent in its sort of memory, right?
Like you're making it explicit
and of course it's attending to all of that.
And so the chances of it sort of going away
into a wrong place is much lower.
So, you know, Scratchpad and a bunch of other papers
had similar ideas of like,
hey, let's give some model some space
to think about things, right?
So it's possible that this is just letting the model
actually think things through.
So it's somehow more computation that the model is getting.
And there've been some papers showing that,
yeah, that might be the difference.
The fact that you're generating a single number,
but you're letting,
not just asking the model to give it one shot,
but letting it think about it.
And it's not so much the fact
that you're giving these example breakdowns that helps.
But I think, you know, as many of these things,
I'm sure the answer is complicated
and it's some combination of everything.
The last thing you said almost sounds
like the kind of multitask argument.
It's not that, you know,
the specific other thing that you're asking
the model to do matters,
but that you're asking it to do another thing.
And that kind of, you know, on the traditional side,
like has some kind of regularization effect
or some kind of effect that causes your results to be better
just by overloading the model a little bit.
Yeah, yeah, exactly, right.
You're letting, in some sense, you have more activations,
you have more latent states,
you just are giving model more things to do.
And so it has space to explode through more reasoning.
So maybe that's one explanation
for why this kind of stuff works, but yeah.
Amazing, amazing.
And I should have mentioned earlier on,
but I will mention it now.
All of the papers that we're referring to
will be available on the show notes page.
So folks can check them out.
So the next thing that you had on your list
was decomposed reasoning.
It sounds like it's in a similar vein.
Yeah, so I think this is,
that's why I kind of put them together,
but I think fundamentally,
this is a very different approach to the same idea.
So yes, I think terminology is something
that the field is gonna be revisiting
and decomposed reasoning is kind of something
that I came up with.
I don't even know if people use it.
But the idea here is that
there've been a bunch of papers here
and I'm just gonna sort of quickly run through some of them.
But the idea here is that you shouldn't rely
on the language model alone to do the whole task.
So suppose I give it a mathematical word problem
or if I give it a question answering problem,
that's a lot complicated.
I shouldn't rely on the model and its parameters
to be able to carry everything out.
Maybe the model needs to use a calculator.
Maybe the model needs to do a web search.
Maybe the model needs to even write a small Python script
and actually run it to get the answer that I want.
And so this whole idea and yeah,
of language models getting what you need,
but not just relying on its own parameters,
but breaking down your problem and figuring out,
oh, I need to call something else
and this is what I'm gonna do to call it,
is an idea that sort of came out
post chain of thought sort of middle of the year,
but there've been a bunch of papers
all the way to the end of the year
that have been doing a lot of this.
So yeah, it's kind of been exciting.
A lot of them have been on the QA side of things.
So the two I'll mention is successive prompting
that came out of my group,
but there's also decomposed prompting
that came out of AI2.
And the idea behind both of these
was to take a complex question,
break it down into simpler ones.
And then have the language model
sort of call another language model
that is answering each of these simple questions, right?
So if a simple question is a mathematical operation,
then you would use a calculator.
If a simple question is a very simple lookup question,
then you would use something
that is like a squad style question answering system,
things like that, right?
So being able to take what the user wants
and breaking down into pieces
and then composing the answers together
to give you the actual answer is this kind of stuff.
Can you talk a little bit in a little bit more detail
the difference between successive prompting
and decomposed prompting?
How did the settings for those differ?
They came out pretty much around the same time.
So it's difficult to sort of,
and they sort of appeared at the same conference as well.
I think, yeah.
So I think some of it depended on sort of
which data set they use.
So decomposed prompting used
explicitly multi-hop data sets
and sort of try to decompose it that way.
Successive prompting focused a little bit more
on calculations and symbolic operations as well.
So yeah, I would say the differences between them.
But kind of same idea, different data sets,
slightly different tooling.
Right, right, yeah.
And we'll see in some of these cases,
other pairs of papers also that are very similar
that came out around the same time
because that's where we are, yeah.
How about tool augmented?
So tool augmented stuff.
So there was a paper coming out of Google, I believe,
called Tool Augmented Language Models.
So TAL is the paper.
And this is one of the papers
that was essentially showing that you can have,
instead of just calling a calculator explicitly
or just having a fixed set of things,
you can create a description of APIs
that the language model has access to
and have the language model itself
generate example calls to that API
when it's doing an output, right?
So if I want to say like, hey, GPT-3 or whatever,
how hot is it going to get today, right?
Or how hot is it going to get today in Irvine?
The language model is going to say,
okay, this is a question about the weather in Irvine.
So I'm gonna compose an API call to a weather service, right?
That's going to say, what's the weather in Irvine?
And then it'll return some JSON object that says,
oh, the high is this, low is this,
probability of rain is this.
And then the language model will kick in again
and take that output and say,
oh, it's gonna be pretty hot today
as since it is Southern California.
And yeah, you know, something.
Seems like this research is heading in the direction
of how would you kind of rebuild Siri or Alexa
or something like that with LLMs.
Yes, yeah, and I think this is one of the key
sort of advantages of these language models
is not that they can do additions
and subtractions internally.
Like I think that's interesting
from an intellectual point of view,
but when you're making actual products,
you want this language model to,
language is a way to interface with things
that are external to you, right?
So the language models should take in the user queries,
but also be the interface to other things outside
and be able to query it.
I think we will talk a little bit about that later,
but one of the reasons I like this
is you can also somehow now attribute
the answer that you're getting,
not to some internal parameter in the language model,
but to say, look, this is the API call I made,
and this is the answer I got,
and now that's the answer I gave you.
So in some sense, it becomes a little bit more attributed.
The idea of the language model writing a program
to figure out the answer to a question is a fascinating one,
and it almost feels like if anything that,
anything around LLMs is gonna be the path to AGI,
it's like, it's that.
What was your reaction to that research?
Yeah, I think it seems quite,
like to me from a practical point of view,
it seems quite exciting, right?
Like so, from a core generation point of view
and things like that, it's useful as well,
but the nice thing about the code,
writing code is that it's unambiguous, right?
So it's making some calls to an external database.
If I want to update the language model,
or update this whole system,
I can just update my knowledge directly, right?
The knowledge is external somehow
to the parameterization of the language model.
That makes it super convenient to delete things,
or to add things, or to get attributions,
and all of these things.
And the interface to that data source is always programs.
Either it's like a simple API call or a more complex one.
And I think I really like this idea
because it allows the language models
to do things that it should be doing,
which is to understand language,
or let's not call it understand,
would be able to parse language,
be able to sort of, you know, transform it,
but doesn't necessarily have to know
the temperature of Irvine every day, or things like that.
Like, that's not something I necessarily want
in the parameters of the language model.
Yeah.
So just very subtly in there,
you kind of addressed another big conversation
that's happening in the community now,
and it's this idea of do language models understand?
You call this decomposed reasoning.
The thing is writing programs,
that kind of requires some kind of reasoning.
Like, what's your take on, you know,
these broader questions about, you know,
reasoning and understanding in LLMs?
Or would you like to defer that?
Is there a natural point later for us to talk about that?
Come back to it a little bit later,
maybe even in the next section of sort of,
us trying to sort of question what reasoning is
and trying to evaluate that in some sense.
But yeah.
The semantic argument around understanding,
like that's not that interesting,
but like how a language model can reason,
and, you know, the extent to which it's reasoning
versus like cutting pasting at some, you know,
level beyond, at an impressive, in an impressive way,
like that's kind of really interesting.
Yeah, definitely.
So I would say, and even pushing it a little bit further,
like what are the consequences of the fact
that it is cutting pasting versus it's reasoning, right?
Like, so how should we calibrate
what things these should be deployed for
and what things they should not be deployed for
based on these intuitions?
Those are the kinds of things
that I'm really, really interested in.
Awesome, awesome, awesome.
Is that your next section?
Yes, and that sort of ties in very well
with what I think is exciting next,
which is, I'm gonna call it sort of
understanding the relationship between the data,
the pre-training data and the output of the model.
And I feel like there is, again,
a few different threads here,
but there is one that came out of my group
that I think is a simple idea
that really sort of captures exactly what you said,
the cutting pasting versus a reasoning thing.
So this paper is called
Impact of Pre-training Term Frequencies
on Few-Shot Reasoning.
And the idea here is we were looking
only at numerical reasoning right now.
So we started looking at all of these examples
of, hey, GPT-3 can do addition and multiplication
and things like that.
And we started looking at the instances
and turns out that it doesn't always do it, right?
It's not 100% of those.
It's 80% or 90% or whatever the number is.
So we started looking at, okay,
what differentiates the one it gets correct in
to things that it doesn't get correct in, right?
So for example, we saw that if you ask it,
what is 24 times 18, the model gets it right.
It says 432.
If you say, what is 23 times 18,
the model gets it wrong, right?
So 24 times 18 is correct.
23 times 18 is not correct.
Is this random?
Like, what's going on here?
And did you, just to interrupt there,
did you find that consistent across invocations?
I've run into that kind of thing.
We've all run into that kind of thing
playing with chat GPT and other things.
And sometimes it gets certain things consistently wrong.
Other times it gets the thing wrong sometimes
and not wrong other times,
like it's a random seed kind of thing
or something else going on in the model.
Did you explore that at all?
Yeah, we definitely saw that.
So it's both like, if you're doing few short prompting,
which examples you put in the prompt
would sometimes change to the output
or how you phrase it.
Like, do you say what is 24 times 18
or what is 24 X 18, you know, things like that.
Definitely made a difference.
But even after averaging these things out,
we saw that 24 times 18 was in general
more accurate than 23 times 18.
And even more than that, we did even further analysis.
And it turns out that all of our instances
that involve 24, the model was much more accurate on
than all of the instances that involve 23.
Well, so we decided to do this for everything
from zero to 100.
So all two-digit numbers essentially,
single and two-digit numbers.
And no, it's a whole spectrum.
And we didn't see a clear reason
why some things are low accuracy,
some things are high accuracy.
And so then what we decided to do,
this is the part that I think I got quite excited about.
We started, decided to count how many times
do each number, each of these numbers
appear in the pre-training data.
And turns out, and you can see the plot in the figure,
if you plot the log of the frequency of these terms
and now how accurate the models are,
it is pretty much exactly like a-
Which is intuitive.
The model does better on things that it sees a lot of, right?
Yes, yeah.
But yeah, so it's also expected yet disappointing
because you don't want it to be such a nice, strong curve.
Like you want it to,
like if it's doing mathematical reasoning,
it should know that 23 is one less than 24
and all of these things, right?
So I think it's one of these things
where it was expected that the model would be better
at things it has seen before.
But you also at the same time hold this thing of like,
oh, it is able to reason, it is able to do these things.
And it's kind of difficult to resolve both of those, right?
So this was one example.
I think this, we are barely scratching the surface,
but this was an example of paper
that sort of started looking at
some of these pre-training statistics.
So not just single term frequencies,
but bigram frequencies and things like that
and showed that the model is quite sensitive
to what these things should be, right?
And I don't want to sort of make a claim
that there is cutting and pasting going on
or any of these things,
but this effect is so strong
that at least when we think about reasoning
and when we are evaluating reasoning
in these language models,
we should be taking this effect into account.
And this may be a side note,
it looks like the model that you evaluated with GPTJ
and clearly that's a model,
it's an open source model
that you had access to the pre-training data,
kind of asked questions about
how do you get the same kind of insight
into these models that are behind APIs?
Yeah, so I think the question is like,
I kind of don't mind that models are behind APIs
to some degree that commercially
that that kind of makes sense.
I feel it's a little bit disappointing
that the training data also
is behind sort of closed wall, right?
So like, I know that there is a lot in the training data,
but if you want to be able to understand
like why chat GPT works or why GPT works
or even generally like when do language models work?
When are they safe to deploy?
All of these kind of questions.
I think it's okay if the language model
we only have a black box access to,
but it would be good to have access to the training data.
It would be good to have access
to a bunch of these other things
that can help us sort of do simple kind of analysis
like this and maybe more complex ones
and actually be able to sort of decide
what to do with the model.
So I think this whole direction of trying to understand
what's in the pre-training data,
I think is key and something that will persist
for the next couple of years.
Do you think we have the right tools to do that at scale?
I'm imagining that was not an easy task
to do just for simple mathematical problems.
That's true.
But training GPT-3 is also not a simple problem
and people have solved it, right?
So I think as, yeah.
So I think the tooling is something
that everybody right now is sort of excited
about building tools that actually give information
and insights into these language models.
And I think, even at AI2,
we are at sort of early stages
of trying to do these things
or building some tooling that can support
this kind of analysis.
But if the data set is available,
I think people will do amazing things.
And I thought this would be impossible
and it seemed like crazy.
Like, hey, this is almost a terabyte of text.
Like, how can you do anything with that?
And it was not trivial, but it was easier than impossible.
How do you identify,
so you identified some behavior,
the relationship between accuracy
and frequency in the training data.
How do you identify what that is a consequence of?
Meaning, is it specific to the way GPT-J was trained?
Is it all transformer-based language models?
Is it maybe something about that particular data set?
Like, are you able to say
that it is a broad characteristic of LLMs in general
based on the work that you've done thus far?
That's a little bit difficult to sort of,
yeah, that's a little bit difficult to measure,
partly because we don't have data set available
for too many models, right?
So at least we tried the whole slew of the Luther models
that were trained on the same data set
and we saw similar effect
on different model sizes essentially.
And yeah, as data sets become,
pre-training data sets become more standard,
it's fairly trivial too.
So we'll sort of extend this stuff.
Since this paper, we also have sort of an online demo
where we have a bunch of more tasks
that try to go beyond mathematical reasoning.
It's a little bit difficult to sort of even define
what these sort of terms are
and what you should be computing frequency of.
But yeah, I think we should be able to do this stuff
for other tasks and for other models.
And to me, I think this is somehow a consequence
of a language modeling loss that encourages this
in some sense, right?
So like, yes, the model has seen more
and it'll be more accurate,
but even the ones that it has seen less,
like it has still seen billions of times.
So there is no reason for it to be wrong on it,
except for the fact that the language modeling loss
would sort of want you to be more right
on the ones that has seen more.
You had another paper identified
out of Yav Goldberg's group.
Oh yeah, so this is work led by Yanai.
I think I'll quickly talk about this.
So this had a similar sort of intuition
for like trying to look at things in the data
and trying to figure out like why the model
has certain biases or has certain errors.
And this was sort of a little bit more
on trying to identify when two entities are related, right?
So if you say, where was Barack Obama born?
The model tends to say Chicago
or in some sense it can say Washington
and depending on how you phrase it.
And like, why does it give the wrong answer?
Is kind of the question.
Why does it not say Hawaii or something?
And I think to be able to answer this question,
you have to go back to the pre-training data
and try to see like, okay, what did it even see?
So what I like about this paper
is it kind of tries to build use causality tools
and builds a whole causal graph
for where these kinds of predictions might have come from.
And then tries to estimate all of the edges
in those causality graph
and tries to do some causal inference
to sort of attribute it to specific statistics
of the pre-training data.
So in this causal graph, would each individual document
in the pre-training data be an intervention of sorts?
So they sort of worked,
they worked at the level of, I guess,
triples or something like that, right?
So let's say you see Obama in Chicago
being a Senator there or something, right?
So this is kind of a triple.
And so they work on statistics of those triples
of the pre-training data to sort of make it tractable
and make it sort of allow this inference to work, yeah.
But in applying the causality machinery,
like are each of those interventions
relative to some prior relationship
between the things, the triples?
Yeah, so there is the true relationship
between these triples
and then there is the observed relationship
between these triples.
And how many of these things,
how many times it appeared in the pre-training data.
And so the idea would be when you're doing it
over many different entities and many different relations,
do you, so those kind of become
your whole data set in some sense.
So Obama has appeared in Chicago,
but Hillary Clinton has appeared elsewhere
and on all of these things.
And then together, which of these causal,
which of these relations seem to affect
a specific prediction the most?
That kind of stuff.
Awesome, awesome.
Kind of continuing on in the data theme,
there's been a ton of work looking at
the need for clean data.
I think maybe one of the most surprising things for me
is like the return of supervision at the scale of LLMs.
Talk a little bit about this category.
Yeah, so this was somehow the most surprising category
for me for this year.
I will say that like after GPT came out
and at the end of last year,
everybody was kind of excited about language models,
but the solutions for what's next always seem to be like,
hey, let's get more data and let's get larger models
and let's train, train longer.
And those are still sort of useful things
nobody's denying.
But this year has shown that like,
hey, you can actually do a lot
if you're a little bit careful about your data, right?
And maybe if you start cleaning up your data
and try to think a little bit about where the data,
your pre-training data should come from,
your pre-training data itself,
that could be quite interesting.
So when you think of like RLHF as an example,
do you think of that as fundamentally
just cleaning up your data,
being more careful about your data as opposed to?
Yeah, so no, I think I was thinking more
what happened with the Bloom language model,
which was trained on sort of a lot more thoughtful process
of gathering the data set.
Because I mean, partly because they documented it
and we know what they went through.
But no, like RLHF and those kinds of things,
I think are examples of showing that the language models
are not quite ready for use case,
just based on pre-training on sort of large data
that has been gathered.
You need to reinforce,
like you can call it like, hey, cleaning up the data,
but I think of it as like maybe reinforcing
some of the nice signals in the data
by having these examples,
or in some sense, people have been fine tuning
on this sort of super-based data as well.
And the gains that you get from RLHF
have become extremely evident this year, right?
So somehow that has become the secret sauce of OpenAI
and of all of these companies
that want to have a really strong language models
rather than scale and just raw pre-training.
And for completeness,
we've talked a little bit about RLHF on the show,
but how do you think about it as a researcher?
I think it's quite exciting.
I think it sort of addresses
a lot of my concerns with language models.
I don't think pre-training data can be trusted, right?
And you shouldn't just train something
and expect the model to have clean output
or have your values and any of these kinds of things,
whatever that means in the context of large language models.
But essentially, if you want real users
to be interfacing with language models,
you need to make sure that there is some sort of check.
And RLHF is not the solution, like a full solution,
but at least there is a way to sort of say,
okay, this is the actual task.
Your actual task is to be interfacing with humans,
not just regurgitating what you've seen
in the pre-training corpus, right?
And so that intuition sort of is captured
by using RLHF.
And do you remember offhand any of the,
if they were even published,
the stats in terms of the number of prompts,
like human generated prompts that were used in chat GPT
or in struct GPT?
Yeah, I don't think they were published as far as I know.
Yeah, I don't remember exactly what they are.
I think in struct GPT had the documentation
of sort of how they were gathered,
but the size was like, you know,
how many of them were sort of generation tasks
versus classification tasks, things like that.
But I don't think the exact data set is available.
Do you have a guess as to like the relative cost
of collecting the human feedback
relative to the cost of training the models?
Oh, relative cost of training the models.
I think it's much cheaper.
Order of magnitude, or is it like much, much, much cheaper?
Because we always say like, you know,
collecting the data,
label data is the most expensive part of machine learning.
Is that still true at the scale of LLMs?
Or is it that RLHF is like extremely efficient
and you just need a little bit of guidance
on top of the, you know, the pre-training data?
I feel the true answer is somewhere in between.
So I don't think it's like, it's nowhere very little data.
Like, I think you need a lot of data to be able to do it,
but I don't think it comes close,
at least the way these are trained right now,
I don't think it comes close
to sort of training the model itself, right?
So, but like when you think about, you know,
chat GPT, it's been released publicly
and a lot of people are using it.
A lot of that data is gonna go into,
in some form back into the model and improve it.
So was that expensive to collect?
In some sense, because they had to run chat GPT,
but you know, they'll probably pay some annotators
to clean that up,
but I don't think that's gonna compare
to the actual training.
It's also a really interesting example of like bootstrapping
like there's a certain amount that they collected themselves
you know, they instruct GPT work
and then they, you know, created something
that was good enough to set loose in the world.
And now they've got this virtual cycle
where I'm imagining it's a lot cheaper
for some annotator to clean up what, you know,
millions of people are creating
than for them to create that themselves.
And I think like, I think this year has also shown
maybe even to people at OpenAI
that the value of these things, right?
Like when they released GPT-3,
they probably didn't realize how valuable this would be.
And then they sort of collected data,
released instruct GPT and yeah, on their benchmarks,
it was good.
But once people started using it,
you realize how much better it is.
I think similarly with chat GPT,
they probably knew how good it was,
but they probably didn't realize
how good it actually is, right?
And I think this idea of human feedback
being a secret sauce that is proprietary,
I think can sort of will continue to be a bigger piece
in the future.
Talk a little bit about Roots.
Yeah, so the Roots is this nice dataset
that was gathered by the big science group.
And I've been following the big science group
and they've done a bunch of interesting things there.
I guess I'll jump in to refer to the interview
that I did with Thomas Wolfe
that I don't think Roots came up explicitly,
but we talked about that work
and that eventually resulted in Bloom,
which we'll talk about a little bit more as well.
Yeah, so Roots I like because I think I really like
what Luther have done with the pile dataset
by releasing the dataset that was used to train
all the GPTJ models.
And I think the big science group sort of took that intuition
and sort of went further with it
where they have a really well-documented
and not just well-documented,
I would say a very thoughtful process
of gathering this dataset.
It's multilingual over many, many different languages.
They've been careful about sort of listing
which sources they want to even crawl
in the first place before.
So it's not like a post hoc cleanup of the data.
It's very sort of thinking about it.
They gathered a dataset that is huge
and they have, we talked a little bit about this data also,
but Hugging Face has sort of built tools on top of it
to be able to quickly search it,
to see what's in it and stuff like that.
And I kind of like that approach
to large language models, right?
So I think getting the right dataset
is crucial for these language models
and doing this documentation and stuff
is good for in the long term.
So your next category is decoding only.
Talk a little bit about what that means.
Yeah, so this is a theme that I like
about some of the work that has come out here.
And partly it's because we have these language models
where we have this black box interface to them.
And a lot of it is just prompting.
So changing things on the input side
to see what the model generates.
And the only thing most people are changing
on the output side is like,
oh, let's change the temperature a little bit
and we get a bunch of different things.
But there has been a bunch of work looking at,
okay, let's not just do that.
Let's actually think about what's happening
in the output of the model during decoding of the text.
And maybe we can do smart things there
that actually sort of change the output considerably, right?
So some of these sort of came out sort of late last year.
So there was this work on nucleus sampling,
maybe that's a little bit older,
but then there was this stuff
on sort of constrained decoding as well,
where the constraint decoding paper
came out of semantic machines.
They showed that you can have,
suppose you want the language model
to generate programs, right?
So the programs come with a certain grammar, right?
Like there is a syntax that they need to follow.
So you could actually constrain the output
of the language model as it's generating token by token
to sort of adhere to that syntax in some sense, right?
And just by doing this constraint, you can get,
firstly, obviously you will get programs
that are syntactically correct,
but you can actually get the right things out of the model.
And so there have been a lot of sort of works
looking at how can we decode
by having some constraints on the decoding, right?
So one of the papers that came out this year
that I believe got the best paper award as well
is called Neuralogic A-star, A-star-esque decoding.
And the idea here is that
instead of just doing left to right decoding
where you're being greedy
or where you're being doing some kind of beam search
or sampling or any of these process,
why don't you actually use some of the computer science ideas
that we have like A-star search
and try to find the best possible decoding.
And then when you're doing this kind of thing,
you can also think about constraints
that you might want to put on the decoding.
So you want to say, look,
I want the decoding to have these three words in it, right?
Like A, you're generating a recipe,
make sure that it has these five ingredients, right?
Somewhere in the generated text.
You can also flip it around,
hey, generate whatever text you generate,
make sure it doesn't have these specific words, right?
Like that.
And this paper sort of uses A-star during decoding
to generate that text that sort of,
you know, your constraints are satisfied.
And this paper showed that, yeah,
once you do that properly,
you can actually do a lot of the tasks much better
just by controlling decoding
rather than changing much on the inputs.
It seems like this is another example
where it's predicated on having open access
to the model internals
and you potentially lose a lot if you don't.
Yeah, I think, so from what I understand,
you can still do these kinds of things
with GPT-3 to some degree.
I think what you need, okay,
so you can do this with a black box model
as long as you get the probabilities
of all of the tokens at every step, right?
So I don't think GPT-3 actually does that,
but you know, you could imagine an API that says,
okay, the next, here's the distribution
over all of the tokens.
And you should be still be able to do these kind of things.
So, you know, some of the concerns is like,
if you want decoding to be fast,
then it's difficult to use some of these ideas.
The A star one specificity is a lot slower,
but it's able to satisfy your constraints.
So it can be where you're okay to trade off some time,
but let the model take more time
in making sure the output is clean
and satisfies your constraint.
This could be really, really cool.
And now, yeah, often you see,
hey, we applied one method, A star in this case,
let's go back to the computer science toolkit
and apply everything else.
Have we seen that here?
Not yet.
This, I think came out late enough in the year,
but I guess it came out sort of early in the year,
but yeah, we haven't seen that much yet because,
but I think, yeah,
that's the kind of thing that will happen next is like,
okay, now this is, yeah,
this is attracting a whole different kind of thinking
where people were not thinking about decoding at all,
and now they will be in this light,
which is always a sign of good paper.
Awesome, awesome.
Well, those are great themes to kind of reflect on
as we think about the past year and NLP research.
Our next category is to talk about some of the new tools
and open source projects that we saw in the year.
We've already talked a little bit about datasets,
which is kind of related,
but I think the first thing you have here is OPT.
Tell us about OPT.
So yeah, I think OPT came out fairly early in this year,
and I think it kind of surprised everyone
because the sort of looking back at last year,
there weren't that many open source reproductions
of large sizes, right?
So I think Luther AI was sort of leading it.
GPT-J was 6 billion,
and they were sort of growing it slowly and slowly,
and they had got to 20 billion parameters.
And then OPT sort of came into the scene,
and there were a bunch of nice things.
They documented a lot of their whole training process
in a log book with sort of all kinds of insights
about what training a log.
Yes, it was released by Beta, right?
And that was also, not to say too much against Beta,
but it was also surprising that reproducibility
and open source seemed to be key aspect of OPT as well.
So that was kind of nice.
And they also released a lot of models
and like all different sizes,
including 175 billion,
which hadn't been available at all.
And even right now,
I think it's probably the most useful model
if you want to do stuff with 175 billion
is to use the OPT model, right?
So I think the idea of documenting the whole training data,
gathering process,
documenting the whole training of the model process,
and then releasing all of these models
available for research,
I think has helped the research community a lot.
And I expect that if there are people
who want to build models
and potentially fine tune language models
and do all of these things,
the OPT would be a pretty big resource.
Have you seen much in terms of benchmarking it
against GPT-3?
Yeah, so I think people have been benchmarking it
and I think it performs reasonably well.
The tricky thing is, of course,
there is Instruct GPT,
which is when you call GPT-3 on the API right now,
it's often defaults to the Instruct one.
And that one is a lot more difficult to beat,
but for all of the purposes,
I think of it as like, yeah,
OPT is basically the same as GPT-3.
We talked a little bit about the big science project
and one of its outputs, another is Bloom.
What did you, what was your take on Bloom?
Bloom was again, a really big data model
that was, I think, 180 billion parameters.
So similar sizes, GPT-3 released to be completely open source
it's like we talked about completely well-documented
data process and sort of training process
combined with the fact that this was done
by a group of people just kind of just volunteering
their time to do so.
And then being able to reproduce to a large degree
what OpenAI has done was quite amazing, right?
And then sort of, again, like both OPTs releasing
all of these things is kind of a sign
for other big tech companies to say like,
hey, you can do this
because we have done this kind of thing.
But what Bloom has shown is that a bunch of people
enthusiastic and excited folks that are enterprising
can actually do things that maybe even a year or two ago
would have seemed impossible.
It may have been in our trends conversation from last year
or maybe it was prior, but in these kinds of conversations
there was a point in time where we were lamenting
the kind of the loss on the part
of the individual academic researcher to contribute
to fundamental model research
because of the resources that were required
and to hugging face and the big sciences system
like they showed that, not necessarily, not so fast, right?
Right, right, right, exactly.
And the other thing I like about this, the Bloom effort
is and the corpus that came with it,
they were also focused on being a lot more inclusive
in terms of having a global perspective.
So they would try to cover many, many different languages.
Very principled in the way they pulled the data together.
Yeah, yeah.
And also multilingual in a way
that none of the experts would have been able to do.
None of the existing models have been.
So yeah, it was quite exciting.
And so like conceptually, this is a great example
of how one model at 175 billion parameters
and another model, the same number of parameters
could be very different, at least in the data
that they were trained on.
And you would expect that to result
in very different results using the model.
To what extent have we characterized that?
Like at that scale of data, it's still a lot of data.
It's still a lot of like raw internet data.
Does it all kind of fall out in the wash
and all their efforts at being principled
kind of just get lost?
Or do we know how to compare that?
Yeah, so there have been a bunch of benchmarks
including in their papers, but in general also.
And that's where sort of the, don't hold me to this,
but I would say like Bloom is not the go-to language model
for people if they want to do English things right now.
So I think maybe some of the trade-offs
they made in collecting the data
or even just having more languages
resulted in a model that's definitely really good
for multilingual things, but that's not what our benchmarks
have been designed for, unfortunately.
And so if you just look at the benchmarks,
which are traditionally designed for English,
Bloom, I don't think quite is at par with OPD or GPTC
and definitely not with Instruct.
And when I mentioned benchmark,
there's that aspect of kind of applying
the traditional performance benchmarks for LLMs to Bloom
and comparing their results to the others.
But I'm also curious about how we characterize
like qualitative differences
between the way Bloom responds and the way GPT responds,
for example, in terms of like the kind of fairness
considerations or that kind of thing,
or are there qualitative differences
in the kinds of responses that you get
that aren't picked up by the traditional benchmarks
or are the traditional benchmarks like so expansive
at this point we've kind of characterized
a lot of that stuff explicitly?
Yeah, again, I think the answer is somewhere in between.
So I don't know if people have thoroughly compared the two
to see like, hey, what's the level of toxicity
and things like that.
I think when OPD came out,
they did a lot of this analysis in their paper
of like, hey, how toxic is that model?
How safe is that model?
And they realized that, yeah,
in some things they were worse off
than some of the existing models.
But I think with Bloom specifically,
I don't know off the top of my head
how it's all compared in terms of these other aspects.
Okay, talk about the inverse scaling competition.
Yeah, so this was a pretty nice thing that came out.
And I think, I suppose it's still going on
even though the submissions are down.
So I'm kind of hoping to see
what the actual effect of this was.
But this was sort of introduced
sort of in the middle of the year.
And the idea here is the thinking of things
like what sort of scaling laws was showing, right?
Like when you scale up your models,
performance goes up for everything.
And that's kind of exciting to see,
but it also tells us that, okay,
there are many, many things that just,
the models would just get better on as time goes by
because they'll get bigger, they'll have more data set.
The inverse scaling was this intuition to see,
okay, what are, can we characterize the phenomenas
that don't have the same trend, right?
So other aspects, you create a data set,
which is something everybody will agree
is a reasonable data set.
But when you give them to larger models,
they actually get worse.
And so this prize in this competition
is an effort to identify what those tasks would be
and sort of the better your inverse scaling is.
So the worse, the bigger models are on your,
on the data set that you've contributed,
the more likely you are to win this competition.
And so, yeah, they've had the submissions
and they're kind of evaluating them, I suppose,
and they haven't quite announced it.
But I think a lot of the stuff on,
a lot of the interesting things
could come out of this effort.
So one thing I could imagine
is sort of deeper levels of misinformation
where the model is relying so much
on what it has seen in its training data.
Let's not call it misinformation,
just not being able to update its information
in some sense, right?
So these large language models
have memorized so much about the pre-training data
that they kind of reject evidence against that, right?
Maybe if they're smaller,
there's less memorization
and more generalization in some way.
But I think it could be pretty exciting
to see what are those things
that actually get worse with scale.
I think it's quite an interesting question.
And next up you have the Galactica,
can we call it a debacle?
So Galactica is this LLM that Meta released
that was tuned to generate scientific and research text.
And was it even up for three days?
It got pulled down pretty quickly, right?
Yep, yeah, I think maybe a little bit more than that,
but yeah, thereabouts, yeah.
And I think to me, it's a story
about how not in anything in terms
of what the Galactica team did itself, right?
Like I think the model training it,
everything was the right thing to be doing.
The tricky thing was just how it was pitched
and how there was just not clear caveats
about what this model is capable of doing
and what it's not capable of doing
that led to such a backlash, right?
So I think it was a language model train
on a lot of science papers.
So it's going to produce papers
that look like scientific text.
I think that was an expected thing,
but again, the backlash it got and stuff like that
essentially tells everyone,
and I hope the message is not to demo language models
anymore, but I think the message should be
how to make sure that you're not hyping things up
more than they should be.
If you reflect on ChatGPT,
which came not very long after Galactica
and the launches of those respective products,
are there clear, is there a clear like do don't do list?
So I will say that ChatGPT itself
was also not completely without hype attached to it,
even some sort of how they-
Right.
Yeah, right?
Somehow they managed it.
There was a lot of hype.
Right, right, right.
I will say that they were fairly clear
about the fact that like, hey, don't trust,
maybe they could have been clearer,
but like don't trust the factual stuff
and things like that.
Like it's not a lookup engine.
I think they kind of could have done a lot more of that,
but they at least had some caveats.
But more than that, part of their RLHF stuff
was to make sure that the model is not producing
at least obviously sexist, don't say-
Yeah, there was a lot,
and maybe we're jumping into ChatGPT,
which actually is the next thing we're gonna talk about,
but there was definitely a lot of,
especially early on,
things that it just would not opine on.
Like, yeah, no, you're not gonna sucker me in
to go in there.
Right, right, right, yeah.
And I think when you're building something
that's public-facing,
that you're selling as a tool, as a product,
that is necessary, right?
Like I don't think you should be doing otherwise.
Galactica should not have been a public-facing tool
for every scientist to start using to write their papers.
It should be a language model, right?
And what the product is or what the tool is
is a gap that other people can help fill in, right?
So that was sort of the missing piece
when I think about ChatGPT versus Galactica.
It's like, yeah, ChatGPT has some of the caveats
about what it's doing,
has some of the caveats about,
oh, it's a language model, not a product, to some degree,
and Galactica was missing it, right?
So, yeah.
Now, we're there.
We were talking about open source.
Next up is kind of commercial developments.
Top of that list is ChatGPT.
Yeah, let's talk about it.
You said early on that, hey, even without ChatGPT,
this was a huge year.
That's clearly not to say that ChatGPT
wasn't a huge contribution to the year.
I mean, certainly one of the things
that I found most interesting
was the degree to which it kind of broke out
of the MLAI echo chamber to just talking to random friends
and are like, hey, have you tried this ChatGPT thing?
Like, yeah, I have.
Yeah, so that's been, I guess, the most surprising
and in some sense, the longest-term impact for ChatGPT
is going to be the fact that it made it commoditized.
It made it mainstream in a way that nothing before it had.
And whether it deserved it or not,
what the actual innovations are and all of these things
is a different question, right?
Like it is clearly, even for research point of view,
qualitatively better than GPT-63.
Whether it met some threshold
for becoming the big thing that it did,
it is sort of difficult to, sort of in hindsight,
try to evaluate that.
But I think it was, yeah, it is definitely something
that became mainstream and everybody's talking about it.
There is still a question in my mind
whether that's a good thing or not in the long run,
because we can talk about some of the problems with ChatGPT,
the biggest one being, we know it's a language model.
Like to some degree, we've been figuring out
last couple of years what these things are capable of
and what these things are not.
And I can sit and in like couple of minutes
come up with tons of examples where it would fail.
That's not quite the case when you sort of start
putting it out in the public.
So most people don't know what a language model is.
And I've played around with,
I've got a bunch of my family to try it
and things like that.
And the biggest thing I've had,
the biggest difficulty I've had conveying to them
is the fact that it's not looking up anything
when you ask it something, right?
Like that is a conceptual jump
that is very, very difficult for people to get over.
Yeah.
And so people like,
yeah, oh, of course it should know about these things
because it happened yesterday
and for such a big news items,
like why would it not know?
And I'm like, no, actually it doesn't know anything
beyond a certain time.
And even saying it knows anything from then
is a little bit difficult.
Right, so I think the best analogy that I've,
you know, this applies to my research as well,
but the best analogy I've had in trying to explain
people what chat GPT does
is to not think of it as a stochastic parrot
or anything like that.
But if you have to think in terms of animals,
think of it as like a chameleon.
Like it's trying to sort of fit in
to a bunch of humans, right?
And it's trying to just write things
that will make it pass
as if it sort of knows all of those things, right?
I was in a Twitter exchange about,
I had asked chat GPT to explain RLHF
and it came up with this acronym
that was like,
oh, I forget it.
And it was really funny.
It was like something leaderboard,
you know, human something.
It was so far off.
Interestingly enough, I'd asked it about,
I'd had conversations, you know,
interactions with it about RLHF
and then it knew what it was.
Like to your point,
it's about kind of where it sits
in the context of the prompt.
And I just kind of posted, you know,
is it trolling me or is it like just trying to BS me?
And one of the responses that I got that,
you know, and reflecting on is like really insightful.
Like it's always trying to BS you.
That's all it's doing is trying to BS you
to produce some texts that you will think is reasonable.
And, you know, to its credit,
a lot of times it's right,
but that's all it's trying to do.
Right, right, right.
Yeah.
And especially when it comes to factual stuff
or even like, you know,
it is a very useful bullshitter in some sense, right?
So, because when it's right
or when it's partially right,
that's still useful because, you know,
it is what it is.
But that, when you put that label,
like if they had sold it as like,
hey, we have built a really good bullshitter, right?
Like, and it's sold out as a product,
then people would know, okay,
not to use it for a bunch of tasks
that they're currently thinking of using it, right?
And so, yeah.
So that's the sort of divide that in messaging
that somehow researchers and NLP folks know,
oh yeah, language model loss,
obviously all it is doing is blah, blah, blah, right?
And yes, RLHF can help to some degree,
but clearly it's not going to be able
to do these bunch of other things all the time.
And that kind of thing is missing from general public,
but also how a lot of people are planning to use it,
for example, right?
So I think that aspect is the part
that we need to think a little bit more about.
You've got Palm, Minerva, Flan, Down.
Tell me a little bit more about your take there.
Cause I hear of them in a vague research context
cause no one really has access to these,
but Google as much more so than something
that is huge from a commercial perspective.
Is this a prediction or is this a reflection?
Yeah, so no, I think of this
as Palm was a huge commercial development this year.
Like Google built this really, really large model.
Now there are, obviously they haven't released it, right?
So what's the ideal situation?
They completely release it open source.
Everybody gets access to it.
That's not gonna happen.
Another possible thing is they put an API on it
and charge people from a Google perspective.
That doesn't make sense, right?
So it is something that they've built.
It's valuable internally.
I'm sure it's sort of has, you know,
there are reasons not to make it public,
but it also has a lot of research insights
because nobody else has such a big language model
trained in a similar way.
And I guess I wanna give them props
for at least publishing and evaluating
and doing things like that with Palm
because it is doing,
it is of a size that we will not see
for maybe another year or two
to be sort of publicly available,
but yet we get to hear about some insights,
what to expect, what are the emergence behaviors
coming out of those language models, right?
So yeah, it would be ideal if you could audit it
and all of us and I could contribute
in finding out what the problems are
and when it works and when it doesn't work.
But given that, I think they did a good job.
Specifically, what I will say is that
that size has brought up a bunch of capabilities
like the whole chain of thought thing
that we talked about at the beginning
that somehow became possible at that size,
but wasn't possible at other sites, right?
So that's why that research is also all coming out of Google
because it applies mostly to LLMs of that size.
Palm is 540 billion parameters?
Something, yeah, 540, yeah.
So I think they have access to it
and they can produce a string of papers
and yes, nobody else can write those papers,
but from a consumer of research as well as producer, right?
So from my consumer side, I love to read research
and I'm glad that they're writing those papers
because there's a lot of interesting stuff
in all of the papers.
So yeah, there's a whole string of papers
that I would recommend
and I can point you to them offline.
But yeah, there's stuff that we'll see happen publicly
next year or maybe another year after that
when those models become commercialized.
So yeah, no, I think that that's been kind of key.
So I'd say for that commercial, but not commercialized.
Yes, right, right, right, yeah, yeah, yeah.
Or soon to be commercialized, I'm sure,
but maybe not by Google.
Yeah, awesome.
Next up, kind of the intersection between search and LLMs.
What are you seeing there?
Yeah, so I think that's been kind of an interesting,
it's been a commercial development,
again, questionable to some degree,
but because I don't think the research is,
and these models are quite up to snuff,
but this somewhat coincided with Chad GPT.
I think-
Well, Chad GPT certainly raised a ton of questions
about, hey, is this a Google killer?
Right, right, right, right.
Yes, exactly.
And along the same time,
there were at least three search engines that I know of.
There was perplexity.ai that I don't think existed before
what the product they came up with,
which is a search engine which sort of gathers
all of the results from a typical search engine,
but then uses GPT-3-like models
to summarize the content of those links
and produces a paragraph that actually answers your query.
You.com is, again, a search engine
that has been around for a while,
but they brought this whole chat aspect to their search
where you're sort of chatting
and trying to come up with an answer.
And again, it's sort of not just showing you
a bunch of links, but composing information into text.
That's Richard Socher's company,
and we'll drop a link to my interview with him
in the show notes as well.
Okay, cool, yeah, yeah, yeah.
And Neva is another, you know,
it's a private search company.
It's a startup that also has an AI agent
that you can talk to it and things like that, right?
So I haven't played around with all of them.
I've played around with them a little bit.
And again, it's very easy to find problems
and sort of realize that, okay,
these language models are, you know,
this interface is great,
and it would be great to get the right paragraph
if it could get there.
But oftentimes they don't quite work
because of sort of fundamental issues with language models.
But I think from a commercial development,
I'm pretty excited about what search would look like
in the future and where language models
would fit into that whole product.
Yeah, one of my thought experiments with this
in the context of ChatGPT,
not that it was particularly deep,
but like there was this early meme,
you know, along the lines of,
hey, Google search is crap now, it's all ads.
ChatGPT, you know, I love this interface,
you know, it's gonna kill Google.
And so I asked ChatGPT to basically build a response
with an ad in it.
It works, it can do it.
I wouldn't be so sure that your, you know,
LLM-based search won't have any ads.
Right, right, right, yeah.
Yeah, no, I think, yeah.
Where the ads would come in
and how subtle the ads will be
once you throw in a language model into it.
Yeah, that's kind of interesting to think about.
And I guess next up on your list of commercial developments
is what I might call the LLM-ing of all the things.
Yeah, so I think, you know,
it's been two years or so since GPT-3 came out
and it's the question of like, okay,
where is the world changing products
that are using GPT-3?
When it came out, hey, it was gonna change everything.
Has it changed everything?
And I would say like for the most part, no.
The products that did seem to show some promise
and some of these are ones that will appear in the future
but have been kind of semi-announced
is the notion of writing assistantships, right?
So I think Notion AI is the one I think about
where a lot of people, it's a mainstream product,
anybody can use Notion
and Notion has this GPT-3 thing built in
where it can write to-do lists for you
and things like that.
So I think that is a pretty strong first version of GPT-3
as a commercial product that anybody can use
that I'm quite excited about.
I feel like the timing there is very ChatGPT influence.
Obviously they've been working on it.
You know, they saw it when GPT-3 came out
but I think they made it available right after ChatGPT
and a lot of these, you know,
Jasper's been around for a while
but there's a lot of new kind of writing assistant
types of things.
And it just does seem like there's a step function increase
in kind of energy in the space of using LLMs
since ChatGPT, even though they're all based on GPT-3
which has been around for two years, right?
Yeah, so I don't know exactly
why that thing seemed to align well, right?
So it's like, yeah, GPT-3 was announced
but it was a while before the API was rolled out
to everybody and, you know, and maybe after that
it takes a while to make the business case
for these things.
So yeah, maybe it's just timing of why it worked
or there were people like already kind of working on it
in a sort of on the side and they were like,
hey, no, we gotta sort of ride this wave
and sort of introduce things, right?
So I don't know exactly what that looks like,
but yeah, no, I think the fact that it aligns
also gets a lot more excitement and people know,
oh, okay, ChatGPT is something I've played around with.
This is now ChatGPT that's working on something that I do
and there is a lot of value in that.
Am I detecting an underlying pessimism maybe
about like, you know, kind of, you know,
where's the flying car that I was promised?
All I have is this GPT-3 thing.
Well, it's not so much the pessimism
because when I saw GPT-3, it became evident to me
that this is a great language model,
but it's not clear as it is how it can be made
into a product, right?
But it still came with a lot of hype
and yeah, it can generate a bunch of things,
but we quite haven't quite seen
what the product version of those look like.
I think the language models are extremely powerful,
not just as language models,
but they can be converted into products.
I don't quite feel like we are at a stage
where it's just going to be through prompting
and, you know, let's just tweak it a little bit.
I think there are a bunch of products
that'll come out of just by doing that,
but there's a whole slew of product
where the language models need to know a lot more
about the context where they're going to be
to be able to be effective, yeah, effective tools.
And you mentioned Microsoft.
What did you have in mind there?
Yeah, this was sort of a news that came out recently
where they're trying to have a bigger stake in open AI,
but also just generally thinking
of having open AI-like tools available in Word,
available in PowerPoint and all of these things.
They don't have it yet,
but I think those kinds of things
are just sort of coming as well.
Do you think a chat GPT-based Bing is a Google killer?
Oh, I don't think with that branding,
they would have to call it something else or yeah.
At this point, yeah.
I mean, that seemed to be the suggestion, right?
Chat GPT comes out, they're gonna take a big stake.
And it was mentioned, if not in the official announcement,
it seemed to be the conjecture
that it was gonna be some tie up with Bing,
explicitly to target search, right?
I think there needs to be a lot more fundamental work
and we can talk about this in the future predictions,
but there needs to be a lot more fundamental work
before we sort of are able to kill search
just by putting a language model, right?
Like I think that gap is not as simple
as replacing something or just augmenting existing search.
I think you would have to think about
what kind of things can language models actually do
and you still want to rely on sources and things like that.
But yeah, so I think it's going to happen at some point,
but it's going to be like search as,
it won't be replacing search
because it'll be a different thing, right?
Like it'll be, it's not gonna be search because-
Not in the way we think about search right now.
Search literally means, yeah, exactly.
It'll be question answering
or it'll be something else, right?
Like it'll be a helper or whatever,
but search is maybe not the right thing to do.
Well, one quick thing before we jump into predictions,
you kind of reflected on your top use case for the year
and that was CodePilot.
Tell me a little bit more about
how you're thinking about that.
I think CodePilot came out
probably not exactly in this calendar year,
but I feel like it got a lot more adoption this year
and started becoming part of the tools
where people are coding.
And personally, I started using CodePilot this year,
so I'm gonna put it in a top use case this year.
And I will say before Notion.ai,
CodePilot was probably the only use of large language models
that I saw anywhere.
So from that point of view, it was interesting
that GPT-3 came out
and then nothing, nothing that can build CodePilot.
But from a use case point of view,
it has been incredibly useful, right?
So I've been able to do things.
It has made me a lot more effective as a coder,
not that I code much,
but when I do, I want to do a lot
and CodePilot has let me sort of do that.
And that's been amazing.
I feel the right combination of,
hey, having a nice user interface,
having the right data that is trained on
to be able to sort of really help people
in what they want to do.
Now, of course, CodePilot has issues.
It's producing code that can be dangerous,
that can be buggy.
And of course, there are the questions of copyright
and plagiarism, exactly.
So I feel like, I hope those things will get resolved,
but those are, again,
when you start using a language model,
these are the issues that you have to solve.
And then I'm glad that CodePilot is bringing
all of these things into the discussion
by it being out there.
Yeah, I've had the same experience with it.
I think I've shared this on social
or in the podcast in a conversation.
I saw all the CodePilot demos,
played around with it with kind of the toy problem things,
but I don't do a lot of coding necessarily,
but I do tend to binge on coding every once in a while.
Like, and usually like that end of year holiday thing,
I'll have some project.
And I did that this year and use CodePilot.
It was amazing.
Like the productivity you can,
it helps, the productivity helps create for you
attacking a new problem with new tools
without the context switching of going to Google
and Stack Overflow.
Like, it's incredible.
I'm a total believer.
Yeah, and I think that exactly is the kind of thing
I expect language models to be useful for.
They are not going to,
and with chat GPT going back a little bit,
people are talking about,
hey, people are gonna lose jobs
and it's gonna change everything.
And we'll replace X, Y, Z with chat GPT.
And I don't quite see that happening,
but I do expect a lot of people in many different areas
becoming a lot more productive because of chat GPT.
And CodePilot is an example of how language models
can make you a lot more productive without replacing,
I don't think it's replacing specific programmers.
It's just making, allowing them to do a lot more.
And that I think is the best use of technology.
Awesome, awesome.
Well, let's jump into predictions.
What are you most excited about
kind of looking into your crystal ball?
So I think the chat GPT is the one that sort of,
everybody knew language models,
they're just trained on data and making predictions.
What chat GPT really did was remind everyone like,
okay, even if the language modeling part
is quote unquote solved, right?
Even if you get a really, really large language model,
that doesn't mean you're done, right?
And I think one of the biggest aspects of that
was making sure that what you're generating is not just BS,
it's somehow valid, somehow the truth,
somehow something that you can cite and rely on, right?
They definitely shine a light on how challenging that is.
Right, exactly, yeah.
So I don't think this is gonna be a prediction
necessarily for 2023,
maybe 2023 is when we'll start seeing
the first attempts at this,
but being able to generate text
that does not have misinformation,
that differentiates factual from creative hallucinations,
that is able to cite its sources
and sort of point to like,
look, this is the piece of paragraph
that I'm based on which I'm generating a piece of text.
I think those things are needed
and it's probably going to be
the next aspect of language models.
That's gonna be a big topic of research.
Do you have a sense for where, how we get there?
Is it kind of applying the same tools,
RLHF for example, attacking this specific problem
or do you think there's, you know,
we don't have the tools and it's gonna need to be
kind of new invention that gets us there?
I think it's going to have to be new inventions
and I want to sort of think of it as not just,
you know, how do we attribute it
to specific pieces of text,
but I kind of think of it as like
being able to use other tools,
being able to use other things available
to the language model
when it's being trained as well, right?
So it should not rely on memorizing facts to any degree.
It should just rely on using existing tools,
including search, including maybe calculations,
maybe even a Python interpreter,
whatever else it needs to do,
but still be able to do the language modeling task, right?
So I think there is some combination
of being able to refer to external stuff
and still do language modeling
that we haven't quite tracked it
and that would be something that I think
will come into picture.
I'll give you an example of how
sort of some people have been thinking about it.
There's this whole idea
of retrieval-based language modeling
where you're still generating the text token by token,
but you're always retrieving some set of documents
and you're conditioning on them
when you're generating each token.
That's sort of one step towards what I'm talking about,
where at least you're trying to look
at retrieved documents when you're generating,
but that doesn't guarantee what you're generating
is actually based on.
So you just spoke earlier about the decomposed reasoning.
Is this prediction that those ideas
become more real in some way in 23-24
or is it that what we're doing with a trained model
to kind of get decomposed reasoning,
we're gonna push even deeper
into the fundamental creation of the model
like at train time and other things?
Yeah, so more of the latter, right?
So right now we are expecting the model
to be able to do decomposed reasoning,
but we only do it at test time in some sense, right?
Let's actually try to start thinking,
putting that stuff during training, right?
So like, again, I don't want to make this analogy too much,
but when you think about when you're training a human
on how to do things,
you don't just give it pairs of input and output.
You give it a little bit more of a decomposition
and then based on that, they're able to do what they do.
If you want them to use the Python interpreter,
you don't just expect them to finish it on their own.
They can use the interpreter when needed kind of thing,
right?
So I just think of language models as,
yeah, maybe they're still doing the language modeling tasks,
but they have access to a bunch of other tools.
And maybe this is more far-fetched than 2023,
but I think in the long run,
you want a system that's able to do those things.
You got it.
Your next prediction is around diffusion models.
It's kind of surprising
that that term hasn't come up yet so far.
Yeah, I guess it is surprising,
but also in NLP in general,
I feel like we are barely scratching the surface
of what diffusion models can do.
So yeah, I think clearly in the image generation space,
we've seen a lot of progress with diffusion models
and we've seen some in NLP, but not enough.
I guess what I find attractive about diffusion model
is that it's trying to generate more
than just a single thing at one point, right?
So when diffusion models are applied to text,
the way it would look like
is not just producing one token at a time.
It will try to produce a whole sentence
or whatever we decide is the right granularity.
And that idea of a model that is trained
not to do one token at a time,
but to do something bigger really appeals to me
because I feel like a lot of the issues
we talk about with language models
fundamentally come from the fact
that it's trained to do one token at a time
and sort of, and that's kind of the loss, right?
So if we can have the model be trained to generate more
and then give it a loss,
I think that's fundamentally interesting
and diffusion models sort of provide one way of doing that.
Do you, would you kind of visualize this as a model
like in a first iteration spitting out bullshit
and then successively like iterating towards truth?
Like, is that one way that this could play out?
Yes.
Well, I mean, probably not.
Probably it's gonna be somewhere in the latent space,
but I think the way I think about it is like,
if we were doing this token by token thing for images,
it just wouldn't make sense, right?
Like predict-
Certainly wouldn't produce the images
that we see coming out of stable diffusion and yeah.
Or even what it's going to learn
is going to be something different.
What it's going to learn is given the image
that I've seen so far,
let me predict the next pixel or the next piece, right?
That somehow feels like a fundamentally different task
than being able to generate an image fully, right?
And so I feel like thinking about the same idea for text
just kind of makes sense.
Like you write the summary in one shot
and realize how wrong it is.
It feels like there's something fundamentally different
than, hey, you got a bunch of tokens correct,
but you also got a bunch.
And in some sense, there are some analogies to RLHF
and using PPO for training, for example,
where you try to make sure it's fluent
and things like that.
These are all losses designed on
not just a token by token basis,
but something that's longer.
And so we've known how useful they've been.
So I feel like there may be something
in taking that idea and applying it to pre-shading
is something that we'll use.
Interesting, interesting.
I expect a lot of people will be wanting
to figure out how to do that.
Great.
And online updates to models.
Yeah, so I think one of the problems with language models,
so let's keep aside the grand vision
of how language models will use search
and all of these other things.
But one of the fundamental problems with language models
is that the world changes, but they don't.
And this seems to be a fundamental sort of issue
with language models.
So I think thinking about how we can update language models
every month or every week or every day, right?
I think is an interesting problem to be thinking about
and becomes increasingly relevant
where BERT doesn't know anything about COVID.
So it's not useful for a bunch of applications,
even though otherwise fundamentally
there's nothing wrong with it, right?
That kind of stuff is just not fun.
And I think there'll be research
on trying to sort of fix that.
What's the current, not necessarily state of the art,
but kind of current approach for doing this
at the scale of a GPT-3?
Like, is it collect more data and retrain from scratch
or how did they approximate
or approach some kind of incremental training ability,
if at all?
Yeah, so there hasn't been that much work on that front.
I would say this is something that's, yeah,
needs a lot more attention.
Yeah, but I think there'd been parameter efficient training
on sort of how can we slightly improve the,
like change the model, but not completely change it.
Find the set of parameters that we should update
so that it's not updating the whole parameters,
but updating a little bit of it.
Things like that I feel are around,
but needs a lot more work.
One way to think about the fundamental problem
is with the transformer,
it's not like a layered architecture like a CNN
where you can just chop off the end layers
and retrain from that point.
It's just a much more complex and interconnected model.
So that kind of incremental updating doesn't work.
Not so easily, yeah.
I think there's been some work on sort of taking like,
hey, 1% of the parameters sort of spread
over the transformer and updating them with new text.
But I think, yeah, solving this problem
is going to be something that needs to happen pretty quick.
And so to be clear, taking a step back,
like this is all the looking forward section,
those three things, kind of misinformation
and attributable generations, diffusion models
and online updates were specifically in your category
of the greatest, most exciting opportunities in the field.
Areas where we're likely to see a lot of research attention
and possibly some really interesting results
coming up in the next year or two.
And also sort of fundamental problems
that need to be addressed by language models.
And so that brings us to your top three predictions
for the field proper.
What do you see there?
Yeah, so I think, and maybe some of it
is riddled with disappointment as well.
So the first one here is multiple modalities.
I think there's been a lot of exciting work,
so I don't want to sort of take that away.
But to me, after GPT-3 came out
and then you saw Clip and DALI and Whisper
and now there's video models and things like that.
To me, fundamentally, I understand technically
why they're not the same model,
but it's still a little bit disappointing
that they're not the same model.
Why is there not the same model
that trains over the same data GPT-3 is trained on,
but also on the Lion dataset that does all the images
and text and audio and video and stuff like that.
And I think this is a sort of near future prediction
is that we are going to see ways for pre-training models
that cuts across multiple modalities.
And I think Clip was a good example,
sort of early example of what you can do
when you have a lot of text and images.
But I think it still didn't have access
to a lot of text-only data.
And I want a model that can do chat GPT-like things,
but also generate images for me
and maybe read them out and things like that.
So I feel like multiple modalities is an exciting
sort of kind of an opportunity,
but definitely something that's going to happen soon.
Mm-hmm.
Yeah, when I first heard you describe this,
I thought, well, multimodal,
like that was the big thing we were talking about
in these trends conversations last year,
but you're going a level deeper.
You don't want multimodal use cases or outputs.
You want a single architecture to do multimodal things.
That's what I want.
My prediction is maybe going to be a little bit more
grounded, so to say.
But yeah, you know, like video, for example,
is a more concrete one, like text to video.
We've seen some initial versions of those.
That's probably where a lot of initial stuff would go in.
But when, and you know, I've been really excited
about sort of the mind dojo world
of like playing with text and Minecraft
and having an agent that can do
a bunch of things in Minecraft.
I feel like there are things that models can learn
from images, even for language modeling,
it would benefit to see a lot of images in some sense.
Like there are just a bunch of things in images
that we never talk about in text.
And so from an AI agent,
I think it's useful to think about something
that has access to everything.
But yeah, more concretely,
we're just going to be pushing them sort of pair-wise.
Yeah, it's going to be audio and images,
and there's going to be a bunch of other pairs
that will happen first.
But eventually I think having multiple,
actual multiple modalities,
not just greater than one modalities would be exciting.
Awesome, awesome.
Next up.
Next, I'm kind of excited about better training
and better inference and better in the sense
of being more computationally efficient.
I think this is an exciting work that,
a bunch of people are already doing,
but I think this is just going to become
increasingly important from a sustainability point of view,
but also from like universities surviving
and doing interesting things
and small companies contributing to research.
I think it's important to be able to train these models,
to be able to run these models,
and there's going to be a lot of research
in trying to do those kinds of things.
And you've got a few examples that we'll link to
in the show notes.
Anything that you want to point out?
Yeah, so let me mention two that I saw recently.
One of them is this paper called cramming.
And the idea here is to,
they think about the scaling laws paper,
like, hey, what can you do when the models get larger
and stuff like that?
The cramming paper sort of turns it on his head
and decides, okay, what if I have just one GPU for one day?
What's the most I can do with that?
And it's a very sort of different question,
but it somehow is a lot more relevant to many more people,
because a lot more people have a single GPU for a single day
and they show that you can get almost
sort of bird level performance
if you make the right choices
and they sort of detail what those choices might look like.
It's a paper, but I think I like that idea of like,
hey, what if we were scrappy about training these models?
How far can we get?
I think that's a very interesting question
that Google and OpenAI is not gonna be asking,
but might be relevant for a lot of research.
The other one I want to talk about is this Petals work
that came out of the big science thing.
I haven't read too much about this,
but it seems like a really interesting idea
of the problem of running really large language models.
So even if OPT releases 175 billion model,
how do you actually run it?
It doesn't really help most people,
even if you have a big cluster,
it's kind of difficult to run it.
So what this Petals does is they're building this framework
for using the ideas behind BitTorrent
of sort of distributed computing
and bringing it to language models.
So like, hey, you should be able to run
these 100 billion size language models,
language models distributed over a bunch of commodity
sort of consumer computers.
So yeah, I think this is an interesting idea.
I haven't played around with it
and see how far you can push it.
That's partly, you need a bunch of people
also running Petals, but once we get there,
I think that could be a pretty exciting way
to run language models.
Interesting, interesting.
So your third prediction is editing
and revising models.
What do you mean there?
So these are these family of models
that are not so much interested in generating text,
but taking existing text and editing it.
And I think this is a very interesting idea
that can become increasingly important.
And in some sense, this could be the way
you fix language model output potentially,
is to have another model that takes the output
of the language model and fixes it.
So some of the work here,
there was a paper out of Julia's group from YouTube now,
that sort of looked at summarization.
And there are systems that generate summaries.
How can you take that generated summaries
and edit it to correct all the factual mistakes
it has made?
And editing is somehow a much,
let's not say definitely a simpler problem,
but in some sense, it could be a simpler problem
than writing the whole summary from scratch,
especially when you do the writing,
you do left to right generation.
So you can't go back and revisit something
that you've done before.
With these editing models,
they have the whole picture to some degree,
and all they have to do is fix it
so that the picture is consistent.
And so this idea seems like potentially simpler
than generation.
So you could generate something,
and maybe this is also attached to diffusion models
where you write something that's maybe not so correct,
but you revise it and it becomes better.
So there is a bunch of work along these directions
that came out essentially this year,
maybe second half of this year,
some of it early on,
that tries to gather data sets where you have edits,
or try to maybe even generate data sets
where you have edits,
and create these models that are able to fix those edits.
And so the prediction specifically is that
teams will build on this and produce models
that can actually kind of deliver on
the ability to do editing and revising.
And I think this could be, for example,
there'll be an editing model that can fix bias issues.
There'll be an editing model that fixes toxicity.
There'll be an editing model that fix factuality.
And these editing models can make web searches
and sort of take that information and edit the output.
So I could imagine that this could be a practical way
of solving many of the issues in language modeling.
It is a really interesting idea that,
I don't know if it's like a separation of concerns
or something like the language model
doesn't necessarily need to do everything
if we can compensate.
So in a way, it's like decomposition as well.
Like let it generate if the way to get
something that's not toxic that's accurate
is to have another type of model support it.
Great.
Yeah, I think, yeah, that's right.
And then for, at least for summarization
and things where it's supposed to be factual
and stuff like that,
I could see it sort of addressing those problems.
Of course, if it's generating a long text
and there are longer range sort of consistency issues
and stuff like that,
it might be a little bit difficult
for editing models to come into picture there.
What I like about editing is also,
it's something that we can imagine
not only working on language model output,
but working on a human output
or text that's been written with the writing assistant
and things like that, right?
Like you can still go back
and do a post-processing editing step to polish it up.
And I think that could be very useful as well.
So our last category in the NLP predictions
is top people, companies, organizations, teams
to watch in the field 2023.
Of course, the caveat of you're not,
any omissions here are not to slight the work
of any particular team,
but like who's got your mind share
and who are you expecting to see interesting things
from in the upcoming year?
Yeah, so this has been a little bit difficult question,
I think every year,
but one thing I will say,
and this is maybe the most obvious answer
is to sort of keep an eye on open AI
and what they're up to, right?
I think people, once they do something,
people always come back and say,
look, what they've done is not so exciting.
Oh, they only scaled it up
or oh, they only did this additional thing.
But the fact is that they are the first ones to do it.
They're the first ones to bring it out, make it available.
And that is, and get people excited about language models
in a way that they weren't before.
So that happened with GPT-2, GPT-3 and chat-GPT.
And I'm sure GPT-4 will have the same thing.
I'm sure retroactively,
we will all talk about what the problems with GPT-4 are
and how it's incrementally only training on more data
or has more parameters or whatever it is.
But I think qualitatively,
it'll bring something interesting to the table.
And I'm really curious about
what that next interesting thing is going to be.
Do you think the general predictions
that are kind of floating around,
basically spring and 100 trillion parameters,
is your money on those?
I mean, to sort of have a completely different perspective,
I think this is also a nice model that came out,
this nice paper that came out a little earlier
called the Chinchilla paper.
This was a paper that show that these models
are extremely under-trained and they are data hungry.
So one version of GPT-4 could be potentially
not even a different architecture,
not even more parameters.
Like exactly, let's keep it 175 billion
and let's just somehow get 10 times the data
if you can potentially get that spot somewhere, right?
I could totally-
But then everybody that shared the image
with the little dot and the big dot would be totally wrong.
Well, yeah, they'll just sort of replace that with data
and it might still work, right?
For those not on Twitter,
that has dominated LLM Twitter over the past couple of days.
Is there even...
I think that when GPT-3 came out,
the kind of colloquial articulation of what they did
was like train this language model on the entire internet.
Like, is there 10X more data to train on?
Yeah, I don't know.
I don't know how much they've trained on
and how much there is.
I mean, there's definitely 10X more data.
There is a lot of stuff that's proprietary, right?
Proprietary?
Maybe even proprietary, right?
Like-
Transcribe a bunch of videos and audio and books
and I guess, yeah.
They do have that Whisper model that does, yeah,
that does really good transcribing.
So they could use that.
Good point.
They didn't create that for no reason.
Right, yeah.
They also can go into scientific papers
and I don't think the 48 million papers
that Galactica was trained on was something
GPT-3 was trained on.
And I think that is a pretty valuable resource.
That Galactica paper also showed
that even on mathematical reasoning and things like that,
they were actually better.
So these scientific papers may be useful
for a bunch of other things that we don't realize.
So yeah, I think where that data comes from
is unclear to me,
but it's clear that more data is somehow
maybe even more interesting than more parameters.
And more data could include more RLHF style things, right?
Like, I don't know what to open it.
Okay.
The other top company to, I would say again,
continue taking a look at is Hugging Face.
I've been constantly sort of amazed
by how much they've been doing.
One of the sort of key insights is like EMNLP,
which is this top conference in NLP,
has this demo track where they highlight
sort of not research papers,
but products sort of demos that are relevant for research.
And for the last three years, I think at EMNLP,
Hugging Face has got the best demo paper award.
And that kind of thing sort of shows
how they've been doing very different things,
but also doing things that are impactful and interesting.
So the two I want to highlight this year is,
again, they've done many, many things,
but the one I want to highlight
is the Evaluate system,
where they had this whole evaluation framework
for reproducing evaluations and evaluating models
and making all of this stuff really easy.
So you can introduce a new metric
and evaluate it on thousands of models, things like that,
make it really easy to compare models,
make it really easy to reproduce papers.
And I think that that's a really valuable service
to do research.
And the other one that I sort of,
we also started with this of like,
hey, what's happening inside the pre-training data?
One of the tools they have is this Roots search tool
that takes the Roots pre-training data,
but allows you to search it and find all kinds of things
that are happening inside that pre-training data.
So if you have a specific prediction,
then you want to be like,
hey, is there anything in the training data
that looks exactly like this?
You can do that search and get some results.
So I think they're just being pretty creative
and thoughtful about what is useful and building tools,
and that's been exciting.
And the last one that I'll bring up,
and this is something that was on top of my head this week,
but it can change.
It's a group called OUGHT, it's O-U-G-H-T,
I believe it's OUGHT.org, it's a website.
And this is sort of a research nonprofit.
And they've been doing sort of interesting things
related to sort of building tools.
So they have this tool called Primer,
and this is going back to decomposed reasoning.
This tool called Primer, you can give it a question
and it tries to come up with an answer,
but in the process of coming up with an answer,
it can do a web search, or it can write a small program,
and it can do all of these things,
and they've built a sort of nice tool
to be able to visualize what the decompositions are
and what sort of things are being done.
So it's a really interesting use case of language models.
And then they also have another tool called Ellicit,
which is, in some sense, it's a little bit like Galactica,
but it's not so much interested in generating papers for you
but helping you do research for your paper, right?
So you have a specific question,
it's going to find a bunch of relevant papers,
take out snippets from those papers,
and be able to do that.
So I don't know, they've had to have a bunch of tools
that when I'm looking at decomposed reasoning,
it comes up, and I'm looking at,
okay, research assistance, it sort of comes up,
and so it's been interesting to see,
and I'm curious what they'll do next.
I'm really curious about that,
and I'm gonna look into that in more detail.
So, awesome, awesome.
Well, I think we are done.
Like, you've been a champ, this has been awesome.
It's been fun, yeah.
Yeah, no, I mean, you rose to the occasion
of kind of capturing an amazing year in NLP, for sure.
So thanks so much for joining us.
Yeah, thanks for inviting me.
I think the time sort of justifies
how much this year had in NLP this year,
and I'm really curious to see where NLP is going to go.
I will mention that ChatGPT came out right before,
or I think maybe even during Eurips.
So I attended Eurips, and I saw the firsthand experience
of the whole machine learning community there.
Then I flew to Abu Dhabi to attend EMNLP,
and that's where I saw the reaction
of the whole NLP community.
And it's been interesting to see sort of
how the reactions have sort of spanned
both optimism and excitement,
which is kind of where I am,
like to see like, hey, what can we build with this stuff?
To pessimism where they're like,
oh, you know, it doesn't really,
yeah, it's not gonna change anything,
it's just a bigger language model,
all the way to, essentially,
I want to say some form of denial,
where it's like, look,
it's behind propriety, closed off system,
and therefore it doesn't matter to do research,
and that's definitely not a take I agree with.
So yeah, it's been exciting.
And there's also a fourth, which maybe is less so,
and I don't, maybe less so in the research community
than in the general sphere,
which is fear of the implications of it.
Did you find that less so on the research side?
I guess less so, definitely less so on the, yeah,
because I think we've been,
there is a little bit of fear
becoming a little bit more obvious,
but I think the community,
because of a lot of people
who've been sort of pointing out problems
with large language models for a while,
we are kind of, we know what not to,
as a community, we should know what not to do,
but it is a little bit scary
when people are using it for things
that clearly, at the onset, should be like,
hey, why are you doing this, right?
Yeah, yeah, yeah.
Awesome.
Well, once again, Samir, thanks so much.
Really great session and conversation
and appreciate all the work you put into prepping for it.
Yeah, thank you, Sam.
It was fun.
