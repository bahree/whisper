1
00:00:00,000 --> 00:00:05,000
All right, everyone, welcome to our AI Trends 2023 series.

2
00:00:05,200 --> 00:00:07,120
Each year, we invite friends of the show

3
00:00:07,120 --> 00:00:10,180
to join us to recap key developments of the year

4
00:00:10,180 --> 00:00:12,080
and anticipate future advancements

5
00:00:12,080 --> 00:00:15,480
in the most interesting subfields in AI.

6
00:00:15,480 --> 00:00:17,680
And today we're joined by Samir Singh.

7
00:00:17,680 --> 00:00:19,600
Samir is an associate professor

8
00:00:19,600 --> 00:00:22,360
in the Department of Computer Science at UC Irvine

9
00:00:22,360 --> 00:00:24,180
and a fellow at the Allen Institute

10
00:00:24,180 --> 00:00:27,000
for Artificial Intelligence, or AI2,

11
00:00:27,000 --> 00:00:29,960
to talk through some of the key research developments

12
00:00:29,960 --> 00:00:32,000
in NLP.

13
00:00:32,000 --> 00:00:34,320
Of course, before we get going,

14
00:00:34,320 --> 00:00:35,960
take a moment to hit that subscribe button

15
00:00:35,960 --> 00:00:38,080
wherever you're listening to today's show,

16
00:00:38,080 --> 00:00:41,960
and you can also follow us on TikTok and Instagram,

17
00:00:41,960 --> 00:00:46,640
at Twiml AI, for highlights from every episode.

18
00:00:46,640 --> 00:00:48,280
All right, let's jump in.

19
00:00:48,280 --> 00:00:53,280
Samir, welcome back to the podcast and our Trends series.

20
00:00:53,480 --> 00:00:54,920
Yeah, thank you for having me, Sam.

21
00:00:54,920 --> 00:00:56,600
It's great to be back.

22
00:00:56,600 --> 00:00:58,320
It's super excited to have you back.

23
00:00:58,320 --> 00:01:00,540
We were joking a little bit before we got rolling

24
00:01:00,540 --> 00:01:04,160
that we picked big years to have you on.

25
00:01:04,160 --> 00:01:07,820
The last one was our 2020,

26
00:01:07,820 --> 00:01:11,260
right in the wake of GPT-3, a big year.

27
00:01:11,260 --> 00:01:15,000
And of course, this has been a huge year for NLP

28
00:01:15,000 --> 00:01:18,560
with the relatively recent release of ChatGPT.

29
00:01:18,560 --> 00:01:21,000
Yeah, it's always kind of crazy

30
00:01:21,000 --> 00:01:23,680
when you have these big changes happening in the year

31
00:01:23,680 --> 00:01:26,760
where there is research still going on in parallel

32
00:01:26,760 --> 00:01:28,800
and people are exploring research questions,

33
00:01:28,800 --> 00:01:32,280
and a lot of them either become obsolete

34
00:01:32,280 --> 00:01:34,760
or have to be revisited and things like that

35
00:01:34,760 --> 00:01:35,640
in the middle of the year.

36
00:01:35,640 --> 00:01:37,800
And in this year, especially,

37
00:01:37,800 --> 00:01:39,760
it was much closer to the end of the year.

38
00:01:39,760 --> 00:01:41,200
So looking back at a year,

39
00:01:41,200 --> 00:01:43,760
it's always interesting to think about the trajectory

40
00:01:43,760 --> 00:01:47,440
and what ideas will still persist and what won't.

41
00:01:47,440 --> 00:01:49,680
Yeah, yeah, a great point.

42
00:01:49,680 --> 00:01:53,120
ChatGPT happened right at the end of the year.

43
00:01:53,120 --> 00:01:55,640
Do you think we'd have the same sense

44
00:01:55,640 --> 00:01:57,600
that this was a huge year in NLP

45
00:01:57,600 --> 00:02:02,320
if it wasn't for that late year release of ChatGPT?

46
00:02:02,320 --> 00:02:03,160
Oh, definitely.

47
00:02:03,160 --> 00:02:06,280
I think this year has been really impressive.

48
00:02:06,280 --> 00:02:08,120
I would say even bigger,

49
00:02:08,120 --> 00:02:10,700
even if you take about ChatGPT,

50
00:02:11,520 --> 00:02:14,000
overall, this year has been really big for NLP,

51
00:02:14,000 --> 00:02:16,880
even compared to sort of the year GPT-3 came out.

52
00:02:16,880 --> 00:02:18,500
So yeah, I think they've been,

53
00:02:19,480 --> 00:02:22,840
I feel like it took us a while to come in terms

54
00:02:22,840 --> 00:02:25,160
with what these large language models are capable of

55
00:02:25,160 --> 00:02:28,880
or what they clearly fail at and what they're good at

56
00:02:28,880 --> 00:02:32,080
and try to sort of build better tooling around it,

57
00:02:32,080 --> 00:02:33,840
build better support systems around it.

58
00:02:33,840 --> 00:02:35,520
And so, yeah, I think this year has been good,

59
00:02:35,520 --> 00:02:38,440
even if you don't take it on ChatGPT, yeah.

60
00:02:38,440 --> 00:02:41,640
Awesome, well, we're gonna dig into ChatGPT

61
00:02:41,640 --> 00:02:43,360
in a fair amount of detail,

62
00:02:43,360 --> 00:02:46,040
as well as some of the other advances you just hinted at.

63
00:02:46,040 --> 00:02:47,240
But before we do,

64
00:02:47,240 --> 00:02:48,720
I'd love to have you take a few minutes

65
00:02:48,720 --> 00:02:52,140
to just kind of introduce yourself to our audience

66
00:02:52,140 --> 00:02:55,680
with a focus on kind of your research focus

67
00:02:55,680 --> 00:02:57,240
and what your interests are.

68
00:02:57,240 --> 00:02:59,760
Cool, yeah, so I've been working in NLP

69
00:02:59,760 --> 00:03:01,600
for a long time now,

70
00:03:01,600 --> 00:03:04,960
but my focus has mostly been looking at

71
00:03:04,960 --> 00:03:07,600
when these language models or machine learning in general

72
00:03:07,600 --> 00:03:09,400
gets interfaced with real users,

73
00:03:09,400 --> 00:03:12,640
what are the needs that sort of are there?

74
00:03:12,640 --> 00:03:14,880
So a lot of my work has been in explanations

75
00:03:14,880 --> 00:03:18,280
and interpretability, but also in robustness,

76
00:03:18,280 --> 00:03:20,080
both from an adversarial perspective,

77
00:03:20,080 --> 00:03:23,360
but also from out of domain generalization perspective.

78
00:03:23,360 --> 00:03:26,240
And also in terms of evaluation,

79
00:03:26,240 --> 00:03:28,720
like how do we know whether the models are doing well,

80
00:03:28,720 --> 00:03:30,160
how well are they doing?

81
00:03:30,160 --> 00:03:33,060
And in general, be able to understand and predict

82
00:03:33,060 --> 00:03:34,280
when the models would work

83
00:03:34,280 --> 00:03:37,280
and when the models would not work.

84
00:03:37,280 --> 00:03:42,280
And I'm imagining that the advent of large language models

85
00:03:43,120 --> 00:03:45,920
and the kind of the dominance of that approach

86
00:03:45,920 --> 00:03:47,900
to NLP modeling is,

87
00:03:47,900 --> 00:03:51,260
well, it certainly changed the tools

88
00:03:51,260 --> 00:03:52,960
and the approach that you take.

89
00:03:52,960 --> 00:03:55,300
Has it changed kind of the fundamental way

90
00:03:55,300 --> 00:03:56,700
that you approach the problem?

91
00:03:56,700 --> 00:03:58,140
To some degree, yes and no.

92
00:03:58,140 --> 00:04:01,140
I think it has made a lot of my work obsolete

93
00:04:01,140 --> 00:04:04,460
in the sense that we were doing a really good job

94
00:04:04,460 --> 00:04:07,420
of finding fundamental faults

95
00:04:07,420 --> 00:04:08,940
in a lot of these language models,

96
00:04:08,940 --> 00:04:10,980
and turns out a lot of them go away

97
00:04:10,980 --> 00:04:14,220
when you have a lot more data or a lot larger size.

98
00:04:14,220 --> 00:04:17,140
And so the specific observations and insights we had,

99
00:04:17,140 --> 00:04:19,700
not all of them have persisted,

100
00:04:19,700 --> 00:04:22,620
but the other differentiation we had in our work

101
00:04:22,620 --> 00:04:25,740
was always being somewhat model agnostic

102
00:04:25,740 --> 00:04:30,020
or try to use a black box approach to the model

103
00:04:30,020 --> 00:04:32,500
rather than looking inside what's going on.

104
00:04:32,500 --> 00:04:34,940
And that is something that you can use

105
00:04:34,940 --> 00:04:37,720
in this world of only access through API,

106
00:04:37,720 --> 00:04:40,100
a lot of those tooling can still work out.

107
00:04:40,100 --> 00:04:41,780
So yeah, so it's been a mix,

108
00:04:41,780 --> 00:04:46,780
but it's been exciting to sort of continue to do that.

109
00:04:46,780 --> 00:04:51,780
Well, you've identified some themes that from your purview

110
00:04:52,900 --> 00:04:57,420
have been some of the key topic areas and research

111
00:04:57,420 --> 00:05:01,260
that have emerged in the field over the past year.

112
00:05:02,700 --> 00:05:04,140
Let's start there.

113
00:05:04,140 --> 00:05:08,700
And maybe before we dive into any of the individual items,

114
00:05:08,700 --> 00:05:13,460
what's your take on 2022 broadly

115
00:05:13,460 --> 00:05:16,420
and some of the areas

116
00:05:16,420 --> 00:05:20,960
that you are most excited about in the year?

117
00:05:20,960 --> 00:05:22,660
Yeah, so I think broadly speaking,

118
00:05:22,660 --> 00:05:25,740
and we will delve deeper into a bunch of these topics,

119
00:05:25,740 --> 00:05:30,140
but broadly speaking, I think the importance of data

120
00:05:30,140 --> 00:05:33,240
and the importance of looking at what might be

121
00:05:33,240 --> 00:05:34,340
in the pre-training data

122
00:05:34,340 --> 00:05:37,660
has sort of brought up back into the focus

123
00:05:37,660 --> 00:05:39,380
in a way that I feel earlier years,

124
00:05:39,380 --> 00:05:40,700
we were a lot more agnostic

125
00:05:40,700 --> 00:05:42,740
of what the model was being trained on

126
00:05:42,740 --> 00:05:46,060
and just more data was better kind of thing.

127
00:05:46,060 --> 00:05:49,540
This year, it's been a lot more sort of thinking

128
00:05:49,540 --> 00:05:51,460
about what goes in the models

129
00:05:51,460 --> 00:05:55,100
and also thinking of ways to use the models,

130
00:05:55,100 --> 00:05:59,220
not just by simply prompting it with a simple thing,

131
00:05:59,220 --> 00:06:00,860
but trying to get it to reason,

132
00:06:00,860 --> 00:06:03,860
trying to get it to break down the problem into pieces

133
00:06:03,860 --> 00:06:06,500
and try to evaluate how much the language models

134
00:06:06,500 --> 00:06:07,940
can do that.

135
00:06:07,940 --> 00:06:09,900
And that I think is key when you start thinking

136
00:06:09,900 --> 00:06:13,940
about taking language models to more higher level

137
00:06:13,940 --> 00:06:16,260
decision-making or higher level reasoning.

138
00:06:16,260 --> 00:06:17,100
Awesome, awesome.

139
00:06:17,100 --> 00:06:20,300
What's the first area you'd like to dig into?

140
00:06:20,300 --> 00:06:24,340
So let's actually start with a chain of thought prompting.

141
00:06:24,340 --> 00:06:27,260
This is work coming out of Google

142
00:06:27,260 --> 00:06:29,740
that came out earlier this year.

143
00:06:29,740 --> 00:06:32,820
And I guess the easiest way to summarize

144
00:06:32,820 --> 00:06:36,220
is to say let's think step-by-step.

145
00:06:36,220 --> 00:06:39,220
The idea here is to have the model

146
00:06:39,220 --> 00:06:42,420
not just generate the answer directly,

147
00:06:42,420 --> 00:06:45,100
but try to have it go through the reasoning process

148
00:06:45,100 --> 00:06:47,740
and then arrive at the answer.

149
00:06:47,740 --> 00:06:50,940
This ended up being quite a strong,

150
00:06:52,300 --> 00:06:54,460
like quite an effective method

151
00:06:54,460 --> 00:06:56,820
to get the model to do a lot of things,

152
00:06:56,820 --> 00:06:59,300
especially when it comes to mathematical reasoning

153
00:06:59,300 --> 00:07:01,580
and sort of where you can break down the problems

154
00:07:01,580 --> 00:07:03,420
into a bunch of these things step.

155
00:07:03,420 --> 00:07:06,860
Chain of thought prompting did extremely well

156
00:07:06,860 --> 00:07:09,380
compared to what we had before.

157
00:07:09,380 --> 00:07:11,300
And part of the difference, I guess,

158
00:07:11,300 --> 00:07:14,380
was you're not just prompting with questions and answers,

159
00:07:14,380 --> 00:07:17,260
but you're also prompting with something

160
00:07:17,260 --> 00:07:18,900
that is much more detailed.

161
00:07:18,900 --> 00:07:21,580
So the prompt itself has a bunch of examples

162
00:07:21,580 --> 00:07:23,220
of breaking the reasoning down,

163
00:07:23,220 --> 00:07:25,820
and then you have the model being able to walk

164
00:07:25,820 --> 00:07:28,100
through that reasoning and get to the answer.

165
00:07:28,100 --> 00:07:33,100
And in that work is the idea that the user of the model

166
00:07:33,100 --> 00:07:37,620
should break the prompts down into more detail,

167
00:07:37,620 --> 00:07:42,420
or that the model should learn how to kind of show its work

168
00:07:42,420 --> 00:07:46,500
and given a coarse-grained prompt,

169
00:07:46,500 --> 00:07:48,700
break the prompt down itself?

170
00:07:48,700 --> 00:07:51,900
Yes, I think the initial paper focused on the user

171
00:07:51,900 --> 00:07:54,820
providing a few examples of this breakdown, right?

172
00:07:54,820 --> 00:07:56,540
So if you're saying like,

173
00:07:56,540 --> 00:07:59,020
here's a mathematical word problem,

174
00:07:59,020 --> 00:08:00,460
you have two apples,

175
00:08:00,460 --> 00:08:03,100
and then somebody gives you double of that,

176
00:08:03,100 --> 00:08:04,700
how many apples do you have?

177
00:08:04,700 --> 00:08:07,060
Breaking it down into a double means times two,

178
00:08:07,060 --> 00:08:08,060
and two times two is four.

179
00:08:08,060 --> 00:08:10,140
I mean, this is a very simple example,

180
00:08:10,140 --> 00:08:12,380
but this kind of giving an example or two

181
00:08:12,380 --> 00:08:16,220
of breaking this down can be quite powerful

182
00:08:16,220 --> 00:08:18,540
for a language model.

183
00:08:18,540 --> 00:08:20,580
Especially, I think, one of the key insights here,

184
00:08:20,580 --> 00:08:23,620
and we can talk about other papers

185
00:08:23,620 --> 00:08:25,700
that sort of showed related things,

186
00:08:25,700 --> 00:08:28,420
but this is a very emergent property

187
00:08:28,420 --> 00:08:31,740
that seems to exist for really large language models.

188
00:08:31,740 --> 00:08:34,140
And if you have smaller language models,

189
00:08:34,140 --> 00:08:36,140
it's kind of difficult to get them to do

190
00:08:36,140 --> 00:08:36,980
this kind of reasoning.

191
00:08:36,980 --> 00:08:39,620
So that's also been exciting to see.

192
00:08:39,620 --> 00:08:42,900
Did the results there, did you find them surprising?

193
00:08:42,900 --> 00:08:47,500
Were they counterintuitive that that would work?

194
00:08:47,500 --> 00:08:50,540
I think how well they worked were,

195
00:08:51,700 --> 00:08:53,500
I think it surprised everyone,

196
00:08:53,500 --> 00:08:54,820
because it's a very simple idea

197
00:08:54,820 --> 00:08:56,620
to just break it down a little bit.

198
00:08:56,620 --> 00:08:58,900
Everybody kind of assumed that the transformers

199
00:08:58,900 --> 00:09:01,060
are sort of either doing this internally

200
00:09:01,060 --> 00:09:03,380
or completely not doing this internally, right?

201
00:09:03,380 --> 00:09:07,700
And by showing you that if you actually write out

202
00:09:07,700 --> 00:09:10,540
a bunch of examples, these transformer models

203
00:09:10,540 --> 00:09:13,540
are able to do this to the extent that they are,

204
00:09:13,540 --> 00:09:17,860
was quite surprising, and the gains were quite impressive.

205
00:09:17,860 --> 00:09:20,580
Can you talk a little bit about the evaluation

206
00:09:20,580 --> 00:09:23,260
of that method?

207
00:09:23,260 --> 00:09:25,180
Yeah, so the evaluation was mostly focused

208
00:09:25,180 --> 00:09:27,540
on mathematical word problems.

209
00:09:27,540 --> 00:09:30,740
So there's this GSM8 data set,

210
00:09:30,740 --> 00:09:34,780
and then there's this MAWPSMOPS, I guess,

211
00:09:34,780 --> 00:09:35,700
data set as well.

212
00:09:35,700 --> 00:09:38,100
These are mathematical word problems.

213
00:09:38,100 --> 00:09:41,420
And this first evaluation was mostly looking at

214
00:09:41,420 --> 00:09:45,140
how well you can do a reason through some of those.

215
00:09:45,140 --> 00:09:48,500
And yeah, it was much, much better

216
00:09:48,500 --> 00:09:51,900
than anything that we had before.

217
00:09:51,900 --> 00:09:53,980
And then they had some evaluations

218
00:09:53,980 --> 00:09:55,620
on symbolic reasoning as well.

219
00:09:55,620 --> 00:09:59,220
So if you give them sort of tasks,

220
00:09:59,220 --> 00:10:04,220
which have like, not have a bunch of,

221
00:10:07,620 --> 00:10:10,260
so like finding a character inside a long string,

222
00:10:10,260 --> 00:10:13,820
like what is the fifth character or something like that,

223
00:10:13,820 --> 00:10:15,700
you can break it down into a bunch of steps.

224
00:10:15,700 --> 00:10:17,780
And if you give it a few examples, it can do it.

225
00:10:17,780 --> 00:10:19,460
If you don't give it a few examples

226
00:10:19,460 --> 00:10:22,220
of how to break it down, the models are very bad.

227
00:10:22,220 --> 00:10:25,820
And have you seen any work that looks to extend this

228
00:10:25,820 --> 00:10:30,180
beyond the kind of math and symbolic domain?

229
00:10:30,180 --> 00:10:33,540
So beyond, I'll talk a little bit about

230
00:10:33,540 --> 00:10:34,900
some of the related ideas

231
00:10:34,900 --> 00:10:37,660
and sort of question answering a little bit later.

232
00:10:37,660 --> 00:10:42,060
But there is one work that is related that I like.

233
00:10:42,060 --> 00:10:45,460
This is called algorithmic prompting.

234
00:10:45,460 --> 00:10:49,060
And this is stuff that came out of Google Brain as well.

235
00:10:49,060 --> 00:10:50,580
So, you know, a lot of the stuff

236
00:10:50,580 --> 00:10:51,820
is coming out of Google Brain

237
00:10:51,820 --> 00:10:54,500
because you need really large language models

238
00:10:54,500 --> 00:10:55,860
to be able to even work with this,

239
00:10:55,860 --> 00:10:58,820
so even bigger than GPT-3, for example.

240
00:10:58,820 --> 00:11:02,020
So in this algorithmic prompting paper,

241
00:11:02,020 --> 00:11:04,620
this was kind of interesting where they had

242
00:11:04,620 --> 00:11:07,180
essentially the same idea as chain of thought,

243
00:11:07,180 --> 00:11:10,020
except that they go really detailed

244
00:11:10,020 --> 00:11:12,860
into what those reasoning steps would be.

245
00:11:12,860 --> 00:11:16,300
So they mostly focus on, you know,

246
00:11:16,300 --> 00:11:19,060
things that can be described more as an algorithm

247
00:11:19,060 --> 00:11:22,460
rather than as just breaking it into a few pieces.

248
00:11:22,460 --> 00:11:23,980
So you can say things like,

249
00:11:23,980 --> 00:11:28,940
if I had to add 12 plus 24, right?

250
00:11:28,940 --> 00:11:30,340
How would you do that?

251
00:11:30,340 --> 00:11:32,420
They literally break it down into digits.

252
00:11:32,420 --> 00:11:34,140
So you take the ones place,

253
00:11:34,140 --> 00:11:37,100
that's two in one case, four in the other.

254
00:11:37,100 --> 00:11:40,500
You add them up, you get six, there is no carry.

255
00:11:40,500 --> 00:11:41,660
Okay, that's one.

256
00:11:41,660 --> 00:11:45,260
The second step is, okay, now look at the next tens place.

257
00:11:45,260 --> 00:11:47,740
It's one and two, add them up, three.

258
00:11:47,740 --> 00:11:49,740
Look at the carry or the carry is zero.

259
00:11:49,740 --> 00:11:51,940
So it's just three and then 36, right?

260
00:11:51,940 --> 00:11:54,660
So all of this, this very detailed breakdown,

261
00:11:54,660 --> 00:11:56,580
which looked like extremely detailed.

262
00:11:57,500 --> 00:12:00,260
But what was really impressive to me about that paper

263
00:12:00,260 --> 00:12:03,620
is they showed that you can give examples

264
00:12:03,620 --> 00:12:06,540
of really low digit operations.

265
00:12:06,540 --> 00:12:08,860
So like maybe two or three digit operations

266
00:12:08,860 --> 00:12:10,340
when you're talking about addition

267
00:12:10,340 --> 00:12:13,540
or multiplication or any of these things.

268
00:12:13,540 --> 00:12:15,380
But at test time, you can,

269
00:12:15,380 --> 00:12:18,180
firstly, even on two and three digit stuff,

270
00:12:18,180 --> 00:12:19,740
it was much, much accurate

271
00:12:19,740 --> 00:12:22,420
compared to regular chain of thought.

272
00:12:22,420 --> 00:12:26,740
Like 20% going from 80% of chain of thought

273
00:12:26,740 --> 00:12:28,100
to something that's 100%.

274
00:12:28,100 --> 00:12:29,940
I'm kind of making up numbers.

275
00:12:29,940 --> 00:12:33,780
And this is relative to asking for the model

276
00:12:33,780 --> 00:12:37,180
to solve the same problem without any intermediate steps.

277
00:12:37,180 --> 00:12:39,900
No, so without intermediate steps is even worse, right?

278
00:12:39,900 --> 00:12:44,220
So this is asking the model to, so like 12 plus 24,

279
00:12:44,220 --> 00:12:46,140
I don't know exactly what the chain of thought would be,

280
00:12:46,140 --> 00:12:47,540
but it would be something

281
00:12:47,540 --> 00:12:49,300
that would be at a higher granularity,

282
00:12:49,300 --> 00:12:51,060
let's just say, right?

283
00:12:51,060 --> 00:12:54,860
And so when you give these detailed prompts,

284
00:12:54,860 --> 00:12:58,300
the models are more accurate, which is not so surprising.

285
00:12:58,300 --> 00:13:01,180
But what was surprising was that they kept increasing

286
00:13:01,180 --> 00:13:05,580
the size of the number at test time.

287
00:13:05,580 --> 00:13:08,660
So started adding more and more digits

288
00:13:08,660 --> 00:13:12,500
and even up to 18 digit numbers.

289
00:13:12,500 --> 00:13:15,740
The model is able to do these operations

290
00:13:15,740 --> 00:13:17,740
much, much more accurately,

291
00:13:17,740 --> 00:13:20,980
even though the prompts were only on two or three digit

292
00:13:22,620 --> 00:13:23,980
sort of numbers, right?

293
00:13:23,980 --> 00:13:28,980
And so does this type of work answer definitively

294
00:13:30,140 --> 00:13:34,220
whether this is already happening inside the model

295
00:13:34,220 --> 00:13:37,260
versus there's some other effects?

296
00:13:37,260 --> 00:13:41,220
Like in a sense, it's really counterintuitive

297
00:13:41,220 --> 00:13:43,020
that it would work at all.

298
00:13:43,020 --> 00:13:45,620
There's no registers inside the model

299
00:13:45,620 --> 00:13:47,460
that are tracking digits,

300
00:13:47,460 --> 00:13:49,460
the ones place and the tens place.

301
00:13:49,460 --> 00:13:51,100
Why should that work?

302
00:13:51,100 --> 00:13:55,340
Yeah, so I think people are still trying to come to terms

303
00:13:55,340 --> 00:13:57,220
with why chain of thought reasoning works.

304
00:13:57,220 --> 00:13:58,860
Is there something in the pre-training data?

305
00:13:58,860 --> 00:14:00,260
Is there something in the model?

306
00:14:00,260 --> 00:14:02,860
And there's been some interesting work there.

307
00:14:02,860 --> 00:14:04,900
But no, I think the tricky thing here

308
00:14:04,900 --> 00:14:07,580
is you're making all of these things explicit.

309
00:14:07,580 --> 00:14:12,460
So you're not relying on the model to keep these bits

310
00:14:12,460 --> 00:14:15,260
somewhere latent in its sort of memory, right?

311
00:14:15,260 --> 00:14:16,540
Like you're making it explicit

312
00:14:16,540 --> 00:14:18,620
and of course it's attending to all of that.

313
00:14:18,620 --> 00:14:22,180
And so the chances of it sort of going away

314
00:14:22,180 --> 00:14:24,820
into a wrong place is much lower.

315
00:14:24,820 --> 00:14:28,340
So, you know, Scratchpad and a bunch of other papers

316
00:14:28,340 --> 00:14:29,820
had similar ideas of like,

317
00:14:29,820 --> 00:14:32,180
hey, let's give some model some space

318
00:14:32,180 --> 00:14:33,940
to think about things, right?

319
00:14:33,940 --> 00:14:38,380
So it's possible that this is just letting the model

320
00:14:38,380 --> 00:14:39,860
actually think things through.

321
00:14:39,860 --> 00:14:43,580
So it's somehow more computation that the model is getting.

322
00:14:43,580 --> 00:14:45,340
And there've been some papers showing that,

323
00:14:45,340 --> 00:14:46,500
yeah, that might be the difference.

324
00:14:46,500 --> 00:14:49,700
The fact that you're generating a single number,

325
00:14:49,700 --> 00:14:50,660
but you're letting,

326
00:14:50,660 --> 00:14:53,100
not just asking the model to give it one shot,

327
00:14:53,100 --> 00:14:55,180
but letting it think about it.

328
00:14:55,180 --> 00:14:57,820
And it's not so much the fact

329
00:14:57,820 --> 00:15:00,260
that you're giving these example breakdowns that helps.

330
00:15:00,260 --> 00:15:02,580
But I think, you know, as many of these things,

331
00:15:02,580 --> 00:15:04,380
I'm sure the answer is complicated

332
00:15:04,380 --> 00:15:06,620
and it's some combination of everything.

333
00:15:06,620 --> 00:15:09,060
The last thing you said almost sounds

334
00:15:09,060 --> 00:15:11,540
like the kind of multitask argument.

335
00:15:11,540 --> 00:15:13,180
It's not that, you know,

336
00:15:13,180 --> 00:15:15,540
the specific other thing that you're asking

337
00:15:15,540 --> 00:15:18,180
the model to do matters,

338
00:15:18,180 --> 00:15:20,340
but that you're asking it to do another thing.

339
00:15:20,340 --> 00:15:24,100
And that kind of, you know, on the traditional side,

340
00:15:24,100 --> 00:15:26,380
like has some kind of regularization effect

341
00:15:26,380 --> 00:15:30,340
or some kind of effect that causes your results to be better

342
00:15:30,340 --> 00:15:32,900
just by overloading the model a little bit.

343
00:15:32,900 --> 00:15:34,540
Yeah, yeah, exactly, right.

344
00:15:34,540 --> 00:15:38,260
You're letting, in some sense, you have more activations,

345
00:15:38,260 --> 00:15:40,980
you have more latent states,

346
00:15:40,980 --> 00:15:44,540
you just are giving model more things to do.

347
00:15:44,540 --> 00:15:48,860
And so it has space to explode through more reasoning.

348
00:15:48,860 --> 00:15:50,420
So maybe that's one explanation

349
00:15:50,420 --> 00:15:53,740
for why this kind of stuff works, but yeah.

350
00:15:53,740 --> 00:15:54,580
Amazing, amazing.

351
00:15:54,580 --> 00:15:57,580
And I should have mentioned earlier on,

352
00:15:57,580 --> 00:15:58,900
but I will mention it now.

353
00:15:58,900 --> 00:16:01,100
All of the papers that we're referring to

354
00:16:01,100 --> 00:16:03,420
will be available on the show notes page.

355
00:16:03,420 --> 00:16:05,940
So folks can check them out.

356
00:16:06,820 --> 00:16:08,900
So the next thing that you had on your list

357
00:16:08,900 --> 00:16:10,860
was decomposed reasoning.

358
00:16:10,860 --> 00:16:13,140
It sounds like it's in a similar vein.

359
00:16:13,140 --> 00:16:14,740
Yeah, so I think this is,

360
00:16:14,740 --> 00:16:16,140
that's why I kind of put them together,

361
00:16:16,140 --> 00:16:17,540
but I think fundamentally,

362
00:16:17,540 --> 00:16:20,340
this is a very different approach to the same idea.

363
00:16:20,340 --> 00:16:22,740
So yes, I think terminology is something

364
00:16:22,740 --> 00:16:24,660
that the field is gonna be revisiting

365
00:16:24,660 --> 00:16:26,740
and decomposed reasoning is kind of something

366
00:16:26,740 --> 00:16:27,900
that I came up with.

367
00:16:27,900 --> 00:16:29,660
I don't even know if people use it.

368
00:16:29,660 --> 00:16:31,020
But the idea here is that

369
00:16:31,020 --> 00:16:32,220
there've been a bunch of papers here

370
00:16:32,220 --> 00:16:35,420
and I'm just gonna sort of quickly run through some of them.

371
00:16:35,420 --> 00:16:38,140
But the idea here is that you shouldn't rely

372
00:16:38,140 --> 00:16:41,460
on the language model alone to do the whole task.

373
00:16:41,460 --> 00:16:44,700
So suppose I give it a mathematical word problem

374
00:16:44,700 --> 00:16:46,980
or if I give it a question answering problem,

375
00:16:46,980 --> 00:16:49,180
that's a lot complicated.

376
00:16:49,180 --> 00:16:53,020
I shouldn't rely on the model and its parameters

377
00:16:53,020 --> 00:16:54,780
to be able to carry everything out.

378
00:16:54,780 --> 00:16:57,260
Maybe the model needs to use a calculator.

379
00:16:57,260 --> 00:17:00,580
Maybe the model needs to do a web search.

380
00:17:00,580 --> 00:17:03,740
Maybe the model needs to even write a small Python script

381
00:17:03,740 --> 00:17:07,700
and actually run it to get the answer that I want.

382
00:17:07,700 --> 00:17:11,020
And so this whole idea and yeah,

383
00:17:11,020 --> 00:17:13,860
of language models getting what you need,

384
00:17:13,860 --> 00:17:16,220
but not just relying on its own parameters,

385
00:17:16,220 --> 00:17:18,780
but breaking down your problem and figuring out,

386
00:17:18,780 --> 00:17:20,500
oh, I need to call something else

387
00:17:20,500 --> 00:17:23,420
and this is what I'm gonna do to call it,

388
00:17:23,420 --> 00:17:25,860
is an idea that sort of came out

389
00:17:25,860 --> 00:17:28,140
post chain of thought sort of middle of the year,

390
00:17:28,140 --> 00:17:29,740
but there've been a bunch of papers

391
00:17:29,740 --> 00:17:31,460
all the way to the end of the year

392
00:17:31,460 --> 00:17:34,740
that have been doing a lot of this.

393
00:17:34,740 --> 00:17:36,940
So yeah, it's kind of been exciting.

394
00:17:37,820 --> 00:17:41,380
A lot of them have been on the QA side of things.

395
00:17:41,380 --> 00:17:44,300
So the two I'll mention is successive prompting

396
00:17:44,300 --> 00:17:45,780
that came out of my group,

397
00:17:45,780 --> 00:17:47,460
but there's also decomposed prompting

398
00:17:47,460 --> 00:17:49,100
that came out of AI2.

399
00:17:49,100 --> 00:17:50,660
And the idea behind both of these

400
00:17:50,660 --> 00:17:52,700
was to take a complex question,

401
00:17:52,700 --> 00:17:55,780
break it down into simpler ones.

402
00:17:55,780 --> 00:17:58,420
And then have the language model

403
00:17:58,420 --> 00:18:01,340
sort of call another language model

404
00:18:01,340 --> 00:18:05,180
that is answering each of these simple questions, right?

405
00:18:05,180 --> 00:18:07,500
So if a simple question is a mathematical operation,

406
00:18:07,500 --> 00:18:08,820
then you would use a calculator.

407
00:18:08,820 --> 00:18:13,100
If a simple question is a very simple lookup question,

408
00:18:13,100 --> 00:18:14,100
then you would use something

409
00:18:14,100 --> 00:18:17,500
that is like a squad style question answering system,

410
00:18:17,500 --> 00:18:18,340
things like that, right?

411
00:18:18,340 --> 00:18:21,900
So being able to take what the user wants

412
00:18:21,900 --> 00:18:24,060
and breaking down into pieces

413
00:18:24,060 --> 00:18:25,780
and then composing the answers together

414
00:18:25,780 --> 00:18:29,140
to give you the actual answer is this kind of stuff.

415
00:18:29,140 --> 00:18:31,700
Can you talk a little bit in a little bit more detail

416
00:18:31,700 --> 00:18:33,860
the difference between successive prompting

417
00:18:33,860 --> 00:18:35,540
and decomposed prompting?

418
00:18:35,540 --> 00:18:37,860
How did the settings for those differ?

419
00:18:37,860 --> 00:18:40,380
They came out pretty much around the same time.

420
00:18:40,380 --> 00:18:42,380
So it's difficult to sort of,

421
00:18:42,380 --> 00:18:46,380
and they sort of appeared at the same conference as well.

422
00:18:46,380 --> 00:18:47,820
I think, yeah.

423
00:18:47,820 --> 00:18:50,020
So I think some of it depended on sort of

424
00:18:50,020 --> 00:18:51,860
which data set they use.

425
00:18:51,860 --> 00:18:54,180
So decomposed prompting used

426
00:18:54,180 --> 00:18:56,060
explicitly multi-hop data sets

427
00:18:56,060 --> 00:18:58,740
and sort of try to decompose it that way.

428
00:18:58,740 --> 00:19:01,580
Successive prompting focused a little bit more

429
00:19:01,580 --> 00:19:04,460
on calculations and symbolic operations as well.

430
00:19:04,460 --> 00:19:06,980
So yeah, I would say the differences between them.

431
00:19:06,980 --> 00:19:09,300
But kind of same idea, different data sets,

432
00:19:09,300 --> 00:19:10,780
slightly different tooling.

433
00:19:10,780 --> 00:19:12,380
Right, right, yeah.

434
00:19:12,380 --> 00:19:14,740
And we'll see in some of these cases,

435
00:19:14,740 --> 00:19:17,340
other pairs of papers also that are very similar

436
00:19:17,340 --> 00:19:18,620
that came out around the same time

437
00:19:18,620 --> 00:19:21,460
because that's where we are, yeah.

438
00:19:21,460 --> 00:19:23,460
How about tool augmented?

439
00:19:23,460 --> 00:19:24,940
So tool augmented stuff.

440
00:19:24,940 --> 00:19:29,500
So there was a paper coming out of Google, I believe,

441
00:19:29,500 --> 00:19:32,100
called Tool Augmented Language Models.

442
00:19:32,100 --> 00:19:33,580
So TAL is the paper.

443
00:19:33,580 --> 00:19:35,060
And this is one of the papers

444
00:19:35,060 --> 00:19:39,340
that was essentially showing that you can have,

445
00:19:39,340 --> 00:19:41,620
instead of just calling a calculator explicitly

446
00:19:41,620 --> 00:19:43,540
or just having a fixed set of things,

447
00:19:43,540 --> 00:19:47,220
you can create a description of APIs

448
00:19:47,220 --> 00:19:49,420
that the language model has access to

449
00:19:49,420 --> 00:19:52,580
and have the language model itself

450
00:19:52,580 --> 00:19:55,420
generate example calls to that API

451
00:19:55,420 --> 00:19:57,180
when it's doing an output, right?

452
00:19:57,180 --> 00:20:01,540
So if I want to say like, hey, GPT-3 or whatever,

453
00:20:01,540 --> 00:20:04,580
how hot is it going to get today, right?

454
00:20:04,580 --> 00:20:08,260
Or how hot is it going to get today in Irvine?

455
00:20:08,260 --> 00:20:09,620
The language model is going to say,

456
00:20:09,620 --> 00:20:13,780
okay, this is a question about the weather in Irvine.

457
00:20:13,780 --> 00:20:18,780
So I'm gonna compose an API call to a weather service, right?

458
00:20:18,780 --> 00:20:21,760
That's going to say, what's the weather in Irvine?

459
00:20:21,760 --> 00:20:24,600
And then it'll return some JSON object that says,

460
00:20:24,600 --> 00:20:26,480
oh, the high is this, low is this,

461
00:20:26,480 --> 00:20:28,160
probability of rain is this.

462
00:20:28,160 --> 00:20:30,760
And then the language model will kick in again

463
00:20:30,760 --> 00:20:32,560
and take that output and say,

464
00:20:32,560 --> 00:20:34,760
oh, it's gonna be pretty hot today

465
00:20:35,640 --> 00:20:37,440
as since it is Southern California.

466
00:20:37,440 --> 00:20:39,320
And yeah, you know, something.

467
00:20:39,320 --> 00:20:41,320
Seems like this research is heading in the direction

468
00:20:41,320 --> 00:20:45,320
of how would you kind of rebuild Siri or Alexa

469
00:20:45,320 --> 00:20:47,240
or something like that with LLMs.

470
00:20:47,240 --> 00:20:49,680
Yes, yeah, and I think this is one of the key

471
00:20:49,680 --> 00:20:51,800
sort of advantages of these language models

472
00:20:51,800 --> 00:20:54,200
is not that they can do additions

473
00:20:54,200 --> 00:20:55,620
and subtractions internally.

474
00:20:55,620 --> 00:20:57,000
Like I think that's interesting

475
00:20:57,000 --> 00:20:58,760
from an intellectual point of view,

476
00:20:58,760 --> 00:21:00,280
but when you're making actual products,

477
00:21:00,280 --> 00:21:02,880
you want this language model to,

478
00:21:02,880 --> 00:21:05,540
language is a way to interface with things

479
00:21:05,540 --> 00:21:07,640
that are external to you, right?

480
00:21:07,640 --> 00:21:10,320
So the language models should take in the user queries,

481
00:21:10,320 --> 00:21:13,640
but also be the interface to other things outside

482
00:21:13,640 --> 00:21:15,620
and be able to query it.

483
00:21:15,620 --> 00:21:17,600
I think we will talk a little bit about that later,

484
00:21:17,600 --> 00:21:19,080
but one of the reasons I like this

485
00:21:19,080 --> 00:21:22,320
is you can also somehow now attribute

486
00:21:22,320 --> 00:21:24,120
the answer that you're getting,

487
00:21:24,120 --> 00:21:26,920
not to some internal parameter in the language model,

488
00:21:26,920 --> 00:21:29,840
but to say, look, this is the API call I made,

489
00:21:29,840 --> 00:21:31,320
and this is the answer I got,

490
00:21:31,320 --> 00:21:34,000
and now that's the answer I gave you.

491
00:21:34,000 --> 00:21:38,000
So in some sense, it becomes a little bit more attributed.

492
00:21:38,000 --> 00:21:43,000
The idea of the language model writing a program

493
00:21:43,000 --> 00:21:48,000
to figure out the answer to a question is a fascinating one,

494
00:21:48,920 --> 00:21:52,800
and it almost feels like if anything that,

495
00:21:52,800 --> 00:21:55,600
anything around LLMs is gonna be the path to AGI,

496
00:21:55,600 --> 00:21:56,680
it's like, it's that.

497
00:21:57,720 --> 00:22:00,360
What was your reaction to that research?

498
00:22:01,240 --> 00:22:03,160
Yeah, I think it seems quite,

499
00:22:03,160 --> 00:22:05,320
like to me from a practical point of view,

500
00:22:05,320 --> 00:22:07,160
it seems quite exciting, right?

501
00:22:07,160 --> 00:22:10,680
Like so, from a core generation point of view

502
00:22:10,680 --> 00:22:12,680
and things like that, it's useful as well,

503
00:22:12,680 --> 00:22:15,120
but the nice thing about the code,

504
00:22:15,120 --> 00:22:17,660
writing code is that it's unambiguous, right?

505
00:22:17,660 --> 00:22:20,660
So it's making some calls to an external database.

506
00:22:20,660 --> 00:22:23,300
If I want to update the language model,

507
00:22:23,300 --> 00:22:25,280
or update this whole system,

508
00:22:25,280 --> 00:22:28,000
I can just update my knowledge directly, right?

509
00:22:28,000 --> 00:22:29,600
The knowledge is external somehow

510
00:22:29,600 --> 00:22:31,960
to the parameterization of the language model.

511
00:22:31,960 --> 00:22:34,040
That makes it super convenient to delete things,

512
00:22:34,040 --> 00:22:36,320
or to add things, or to get attributions,

513
00:22:36,320 --> 00:22:37,520
and all of these things.

514
00:22:37,520 --> 00:22:42,520
And the interface to that data source is always programs.

515
00:22:43,640 --> 00:22:47,200
Either it's like a simple API call or a more complex one.

516
00:22:47,200 --> 00:22:49,400
And I think I really like this idea

517
00:22:49,400 --> 00:22:52,080
because it allows the language models

518
00:22:52,080 --> 00:22:54,080
to do things that it should be doing,

519
00:22:54,080 --> 00:22:55,640
which is to understand language,

520
00:22:55,640 --> 00:22:56,960
or let's not call it understand,

521
00:22:56,960 --> 00:22:58,320
would be able to parse language,

522
00:22:58,320 --> 00:23:01,780
be able to sort of, you know, transform it,

523
00:23:02,840 --> 00:23:05,880
but doesn't necessarily have to know

524
00:23:05,880 --> 00:23:09,240
the temperature of Irvine every day, or things like that.

525
00:23:09,240 --> 00:23:11,160
Like, that's not something I necessarily want

526
00:23:11,160 --> 00:23:13,360
in the parameters of the language model.

527
00:23:13,360 --> 00:23:14,920
Yeah.

528
00:23:14,920 --> 00:23:17,440
So just very subtly in there,

529
00:23:17,440 --> 00:23:22,440
you kind of addressed another big conversation

530
00:23:24,120 --> 00:23:25,960
that's happening in the community now,

531
00:23:25,960 --> 00:23:29,740
and it's this idea of do language models understand?

532
00:23:29,740 --> 00:23:31,480
You call this decomposed reasoning.

533
00:23:31,480 --> 00:23:32,860
The thing is writing programs,

534
00:23:32,860 --> 00:23:35,080
that kind of requires some kind of reasoning.

535
00:23:35,080 --> 00:23:38,120
Like, what's your take on, you know,

536
00:23:38,120 --> 00:23:41,840
these broader questions about, you know,

537
00:23:41,840 --> 00:23:46,040
reasoning and understanding in LLMs?

538
00:23:46,040 --> 00:23:47,800
Or would you like to defer that?

539
00:23:47,800 --> 00:23:51,480
Is there a natural point later for us to talk about that?

540
00:23:52,520 --> 00:23:55,160
Come back to it a little bit later,

541
00:23:55,160 --> 00:23:57,880
maybe even in the next section of sort of,

542
00:23:57,880 --> 00:24:00,400
us trying to sort of question what reasoning is

543
00:24:00,400 --> 00:24:03,640
and trying to evaluate that in some sense.

544
00:24:03,640 --> 00:24:05,360
But yeah.

545
00:24:05,360 --> 00:24:07,820
The semantic argument around understanding,

546
00:24:07,820 --> 00:24:09,380
like that's not that interesting,

547
00:24:09,380 --> 00:24:12,760
but like how a language model can reason,

548
00:24:12,760 --> 00:24:16,280
and, you know, the extent to which it's reasoning

549
00:24:16,280 --> 00:24:19,320
versus like cutting pasting at some, you know,

550
00:24:19,320 --> 00:24:23,880
level beyond, at an impressive, in an impressive way,

551
00:24:23,880 --> 00:24:25,920
like that's kind of really interesting.

552
00:24:25,920 --> 00:24:26,760
Yeah, definitely.

553
00:24:26,760 --> 00:24:30,880
So I would say, and even pushing it a little bit further,

554
00:24:30,880 --> 00:24:33,200
like what are the consequences of the fact

555
00:24:33,200 --> 00:24:35,720
that it is cutting pasting versus it's reasoning, right?

556
00:24:35,720 --> 00:24:38,400
Like, so how should we calibrate

557
00:24:38,400 --> 00:24:40,480
what things these should be deployed for

558
00:24:40,480 --> 00:24:42,720
and what things they should not be deployed for

559
00:24:42,720 --> 00:24:43,920
based on these intuitions?

560
00:24:43,920 --> 00:24:44,840
Those are the kinds of things

561
00:24:44,840 --> 00:24:46,680
that I'm really, really interested in.

562
00:24:46,680 --> 00:24:48,760
Awesome, awesome, awesome.

563
00:24:48,760 --> 00:24:50,040
Is that your next section?

564
00:24:50,040 --> 00:24:52,440
Yes, and that sort of ties in very well

565
00:24:52,440 --> 00:24:55,040
with what I think is exciting next,

566
00:24:55,040 --> 00:24:58,560
which is, I'm gonna call it sort of

567
00:24:58,560 --> 00:25:01,280
understanding the relationship between the data,

568
00:25:01,280 --> 00:25:04,720
the pre-training data and the output of the model.

569
00:25:04,720 --> 00:25:06,360
And I feel like there is, again,

570
00:25:06,360 --> 00:25:08,200
a few different threads here,

571
00:25:08,200 --> 00:25:10,000
but there is one that came out of my group

572
00:25:10,000 --> 00:25:12,880
that I think is a simple idea

573
00:25:12,880 --> 00:25:16,120
that really sort of captures exactly what you said,

574
00:25:16,120 --> 00:25:19,120
the cutting pasting versus a reasoning thing.

575
00:25:19,120 --> 00:25:19,960
So this paper is called

576
00:25:19,960 --> 00:25:22,400
Impact of Pre-training Term Frequencies

577
00:25:22,400 --> 00:25:23,640
on Few-Shot Reasoning.

578
00:25:24,520 --> 00:25:26,680
And the idea here is we were looking

579
00:25:26,680 --> 00:25:28,640
only at numerical reasoning right now.

580
00:25:28,640 --> 00:25:32,040
So we started looking at all of these examples

581
00:25:32,040 --> 00:25:36,520
of, hey, GPT-3 can do addition and multiplication

582
00:25:36,520 --> 00:25:37,760
and things like that.

583
00:25:37,760 --> 00:25:39,680
And we started looking at the instances

584
00:25:39,680 --> 00:25:43,320
and turns out that it doesn't always do it, right?

585
00:25:43,320 --> 00:25:44,720
It's not 100% of those.

586
00:25:44,720 --> 00:25:49,240
It's 80% or 90% or whatever the number is.

587
00:25:49,240 --> 00:25:50,480
So we started looking at, okay,

588
00:25:50,480 --> 00:25:54,840
what differentiates the one it gets correct in

589
00:25:54,840 --> 00:25:57,280
to things that it doesn't get correct in, right?

590
00:25:57,280 --> 00:26:00,400
So for example, we saw that if you ask it,

591
00:26:00,400 --> 00:26:04,240
what is 24 times 18, the model gets it right.

592
00:26:04,240 --> 00:26:05,880
It says 432.

593
00:26:05,880 --> 00:26:09,160
If you say, what is 23 times 18,

594
00:26:09,160 --> 00:26:11,440
the model gets it wrong, right?

595
00:26:11,440 --> 00:26:13,360
So 24 times 18 is correct.

596
00:26:13,360 --> 00:26:15,640
23 times 18 is not correct.

597
00:26:15,640 --> 00:26:16,720
Is this random?

598
00:26:16,720 --> 00:26:18,000
Like, what's going on here?

599
00:26:18,000 --> 00:26:20,000
And did you, just to interrupt there,

600
00:26:20,000 --> 00:26:24,720
did you find that consistent across invocations?

601
00:26:24,720 --> 00:26:27,560
I've run into that kind of thing.

602
00:26:27,560 --> 00:26:28,720
We've all run into that kind of thing

603
00:26:28,720 --> 00:26:30,640
playing with chat GPT and other things.

604
00:26:30,640 --> 00:26:34,120
And sometimes it gets certain things consistently wrong.

605
00:26:34,120 --> 00:26:36,880
Other times it gets the thing wrong sometimes

606
00:26:36,880 --> 00:26:38,120
and not wrong other times,

607
00:26:38,120 --> 00:26:39,720
like it's a random seed kind of thing

608
00:26:39,720 --> 00:26:41,760
or something else going on in the model.

609
00:26:41,760 --> 00:26:43,240
Did you explore that at all?

610
00:26:43,240 --> 00:26:44,640
Yeah, we definitely saw that.

611
00:26:44,640 --> 00:26:47,760
So it's both like, if you're doing few short prompting,

612
00:26:47,760 --> 00:26:49,680
which examples you put in the prompt

613
00:26:49,680 --> 00:26:51,800
would sometimes change to the output

614
00:26:51,800 --> 00:26:52,960
or how you phrase it.

615
00:26:52,960 --> 00:26:55,400
Like, do you say what is 24 times 18

616
00:26:55,400 --> 00:26:58,520
or what is 24 X 18, you know, things like that.

617
00:26:58,520 --> 00:27:00,600
Definitely made a difference.

618
00:27:00,600 --> 00:27:03,120
But even after averaging these things out,

619
00:27:04,280 --> 00:27:07,000
we saw that 24 times 18 was in general

620
00:27:07,000 --> 00:27:09,080
more accurate than 23 times 18.

621
00:27:09,920 --> 00:27:13,480
And even more than that, we did even further analysis.

622
00:27:13,480 --> 00:27:17,320
And it turns out that all of our instances

623
00:27:17,320 --> 00:27:21,720
that involve 24, the model was much more accurate on

624
00:27:21,720 --> 00:27:24,640
than all of the instances that involve 23.

625
00:27:24,640 --> 00:27:28,520
Well, so we decided to do this for everything

626
00:27:28,520 --> 00:27:30,720
from zero to 100.

627
00:27:31,600 --> 00:27:33,400
So all two-digit numbers essentially,

628
00:27:33,400 --> 00:27:35,000
single and two-digit numbers.

629
00:27:35,000 --> 00:27:36,400
And no, it's a whole spectrum.

630
00:27:36,400 --> 00:27:38,040
And we didn't see a clear reason

631
00:27:38,040 --> 00:27:40,560
why some things are low accuracy,

632
00:27:40,560 --> 00:27:42,840
some things are high accuracy.

633
00:27:42,840 --> 00:27:44,960
And so then what we decided to do,

634
00:27:44,960 --> 00:27:48,480
this is the part that I think I got quite excited about.

635
00:27:48,480 --> 00:27:50,760
We started, decided to count how many times

636
00:27:50,760 --> 00:27:52,640
do each number, each of these numbers

637
00:27:52,640 --> 00:27:54,240
appear in the pre-training data.

638
00:27:55,360 --> 00:27:58,720
And turns out, and you can see the plot in the figure,

639
00:27:59,760 --> 00:28:03,360
if you plot the log of the frequency of these terms

640
00:28:03,360 --> 00:28:05,440
and now how accurate the models are,

641
00:28:05,440 --> 00:28:09,880
it is pretty much exactly like a-

642
00:28:09,880 --> 00:28:11,600
Which is intuitive.

643
00:28:11,600 --> 00:28:14,600
The model does better on things that it sees a lot of, right?

644
00:28:14,600 --> 00:28:16,080
Yes, yeah.

645
00:28:16,080 --> 00:28:20,080
But yeah, so it's also expected yet disappointing

646
00:28:20,080 --> 00:28:23,840
because you don't want it to be such a nice, strong curve.

647
00:28:23,840 --> 00:28:25,080
Like you want it to,

648
00:28:25,080 --> 00:28:27,560
like if it's doing mathematical reasoning,

649
00:28:27,560 --> 00:28:30,040
it should know that 23 is one less than 24

650
00:28:30,040 --> 00:28:31,480
and all of these things, right?

651
00:28:31,480 --> 00:28:35,000
So I think it's one of these things

652
00:28:35,000 --> 00:28:38,360
where it was expected that the model would be better

653
00:28:38,360 --> 00:28:40,040
at things it has seen before.

654
00:28:40,040 --> 00:28:43,120
But you also at the same time hold this thing of like,

655
00:28:43,120 --> 00:28:45,560
oh, it is able to reason, it is able to do these things.

656
00:28:45,560 --> 00:28:48,960
And it's kind of difficult to resolve both of those, right?

657
00:28:48,960 --> 00:28:50,960
So this was one example.

658
00:28:50,960 --> 00:28:53,040
I think this, we are barely scratching the surface,

659
00:28:53,040 --> 00:28:54,600
but this was an example of paper

660
00:28:54,600 --> 00:28:56,360
that sort of started looking at

661
00:28:56,360 --> 00:28:59,520
some of these pre-training statistics.

662
00:28:59,520 --> 00:29:01,560
So not just single term frequencies,

663
00:29:01,560 --> 00:29:03,800
but bigram frequencies and things like that

664
00:29:03,800 --> 00:29:07,880
and showed that the model is quite sensitive

665
00:29:07,880 --> 00:29:10,000
to what these things should be, right?

666
00:29:10,000 --> 00:29:11,960
And I don't want to sort of make a claim

667
00:29:11,960 --> 00:29:13,920
that there is cutting and pasting going on

668
00:29:13,920 --> 00:29:16,000
or any of these things,

669
00:29:16,000 --> 00:29:18,000
but this effect is so strong

670
00:29:18,000 --> 00:29:20,240
that at least when we think about reasoning

671
00:29:20,240 --> 00:29:22,840
and when we are evaluating reasoning

672
00:29:22,840 --> 00:29:24,680
in these language models,

673
00:29:24,680 --> 00:29:27,800
we should be taking this effect into account.

674
00:29:28,760 --> 00:29:31,120
And this may be a side note,

675
00:29:31,120 --> 00:29:34,680
it looks like the model that you evaluated with GPTJ

676
00:29:34,680 --> 00:29:39,680
and clearly that's a model,

677
00:29:39,960 --> 00:29:40,840
it's an open source model

678
00:29:40,840 --> 00:29:43,320
that you had access to the pre-training data,

679
00:29:43,320 --> 00:29:45,880
kind of asked questions about

680
00:29:45,880 --> 00:29:49,000
how do you get the same kind of insight

681
00:29:49,000 --> 00:29:52,480
into these models that are behind APIs?

682
00:29:52,480 --> 00:29:54,800
Yeah, so I think the question is like,

683
00:29:54,800 --> 00:29:58,760
I kind of don't mind that models are behind APIs

684
00:29:58,760 --> 00:30:00,160
to some degree that commercially

685
00:30:00,160 --> 00:30:01,720
that that kind of makes sense.

686
00:30:01,720 --> 00:30:04,040
I feel it's a little bit disappointing

687
00:30:04,040 --> 00:30:05,880
that the training data also

688
00:30:05,880 --> 00:30:08,520
is behind sort of closed wall, right?

689
00:30:08,520 --> 00:30:11,360
So like, I know that there is a lot in the training data,

690
00:30:11,360 --> 00:30:13,240
but if you want to be able to understand

691
00:30:13,240 --> 00:30:16,120
like why chat GPT works or why GPT works

692
00:30:16,120 --> 00:30:19,720
or even generally like when do language models work?

693
00:30:19,720 --> 00:30:21,120
When are they safe to deploy?

694
00:30:21,120 --> 00:30:22,680
All of these kind of questions.

695
00:30:22,680 --> 00:30:24,920
I think it's okay if the language model

696
00:30:24,920 --> 00:30:26,920
we only have a black box access to,

697
00:30:26,920 --> 00:30:29,720
but it would be good to have access to the training data.

698
00:30:29,720 --> 00:30:31,080
It would be good to have access

699
00:30:31,080 --> 00:30:32,280
to a bunch of these other things

700
00:30:32,280 --> 00:30:35,240
that can help us sort of do simple kind of analysis

701
00:30:35,240 --> 00:30:37,720
like this and maybe more complex ones

702
00:30:37,720 --> 00:30:41,280
and actually be able to sort of decide

703
00:30:41,280 --> 00:30:42,520
what to do with the model.

704
00:30:42,520 --> 00:30:45,920
So I think this whole direction of trying to understand

705
00:30:45,920 --> 00:30:47,320
what's in the pre-training data,

706
00:30:47,320 --> 00:30:51,200
I think is key and something that will persist

707
00:30:51,200 --> 00:30:52,800
for the next couple of years.

708
00:30:52,800 --> 00:30:57,040
Do you think we have the right tools to do that at scale?

709
00:30:57,040 --> 00:31:00,480
I'm imagining that was not an easy task

710
00:31:00,480 --> 00:31:03,960
to do just for simple mathematical problems.

711
00:31:03,960 --> 00:31:05,320
That's true.

712
00:31:05,320 --> 00:31:09,440
But training GPT-3 is also not a simple problem

713
00:31:09,440 --> 00:31:10,920
and people have solved it, right?

714
00:31:10,920 --> 00:31:12,800
So I think as, yeah.

715
00:31:12,800 --> 00:31:14,360
So I think the tooling is something

716
00:31:14,360 --> 00:31:18,400
that everybody right now is sort of excited

717
00:31:18,400 --> 00:31:21,080
about building tools that actually give information

718
00:31:21,080 --> 00:31:23,000
and insights into these language models.

719
00:31:23,000 --> 00:31:25,760
And I think, even at AI2,

720
00:31:25,760 --> 00:31:27,200
we are at sort of early stages

721
00:31:27,200 --> 00:31:28,320
of trying to do these things

722
00:31:28,320 --> 00:31:30,440
or building some tooling that can support

723
00:31:30,440 --> 00:31:31,680
this kind of analysis.

724
00:31:31,680 --> 00:31:34,040
But if the data set is available,

725
00:31:34,040 --> 00:31:35,800
I think people will do amazing things.

726
00:31:35,800 --> 00:31:39,280
And I thought this would be impossible

727
00:31:39,280 --> 00:31:40,760
and it seemed like crazy.

728
00:31:40,760 --> 00:31:43,400
Like, hey, this is almost a terabyte of text.

729
00:31:43,400 --> 00:31:45,440
Like, how can you do anything with that?

730
00:31:45,440 --> 00:31:49,760
And it was not trivial, but it was easier than impossible.

731
00:31:50,720 --> 00:31:53,440
How do you identify,

732
00:31:54,800 --> 00:31:56,800
so you identified some behavior,

733
00:31:58,400 --> 00:32:00,240
the relationship between accuracy

734
00:32:00,240 --> 00:32:04,440
and frequency in the training data.

735
00:32:04,440 --> 00:32:09,440
How do you identify what that is a consequence of?

736
00:32:11,800 --> 00:32:15,600
Meaning, is it specific to the way GPT-J was trained?

737
00:32:15,600 --> 00:32:18,800
Is it all transformer-based language models?

738
00:32:18,800 --> 00:32:23,040
Is it maybe something about that particular data set?

739
00:32:23,040 --> 00:32:27,000
Like, are you able to say

740
00:32:27,000 --> 00:32:32,000
that it is a broad characteristic of LLMs in general

741
00:32:32,000 --> 00:32:34,800
based on the work that you've done thus far?

742
00:32:34,800 --> 00:32:38,440
That's a little bit difficult to sort of,

743
00:32:38,440 --> 00:32:40,560
yeah, that's a little bit difficult to measure,

744
00:32:40,560 --> 00:32:43,440
partly because we don't have data set available

745
00:32:43,440 --> 00:32:45,280
for too many models, right?

746
00:32:45,280 --> 00:32:49,040
So at least we tried the whole slew of the Luther models

747
00:32:49,040 --> 00:32:50,520
that were trained on the same data set

748
00:32:50,520 --> 00:32:53,440
and we saw similar effect

749
00:32:53,440 --> 00:32:55,640
on different model sizes essentially.

750
00:32:56,840 --> 00:32:58,480
And yeah, as data sets become,

751
00:32:58,480 --> 00:33:00,240
pre-training data sets become more standard,

752
00:33:00,240 --> 00:33:01,520
it's fairly trivial too.

753
00:33:01,520 --> 00:33:04,200
So we'll sort of extend this stuff.

754
00:33:04,200 --> 00:33:07,520
Since this paper, we also have sort of an online demo

755
00:33:07,520 --> 00:33:09,200
where we have a bunch of more tasks

756
00:33:09,200 --> 00:33:12,040
that try to go beyond mathematical reasoning.

757
00:33:12,040 --> 00:33:15,120
It's a little bit difficult to sort of even define

758
00:33:15,120 --> 00:33:16,880
what these sort of terms are

759
00:33:16,880 --> 00:33:19,840
and what you should be computing frequency of.

760
00:33:19,840 --> 00:33:22,520
But yeah, I think we should be able to do this stuff

761
00:33:22,520 --> 00:33:25,120
for other tasks and for other models.

762
00:33:25,120 --> 00:33:29,160
And to me, I think this is somehow a consequence

763
00:33:29,160 --> 00:33:32,560
of a language modeling loss that encourages this

764
00:33:32,560 --> 00:33:33,400
in some sense, right?

765
00:33:33,400 --> 00:33:36,800
So like, yes, the model has seen more

766
00:33:36,800 --> 00:33:38,200
and it'll be more accurate,

767
00:33:38,200 --> 00:33:40,280
but even the ones that it has seen less,

768
00:33:40,280 --> 00:33:42,480
like it has still seen billions of times.

769
00:33:42,480 --> 00:33:45,400
So there is no reason for it to be wrong on it,

770
00:33:45,400 --> 00:33:47,520
except for the fact that the language modeling loss

771
00:33:47,520 --> 00:33:49,960
would sort of want you to be more right

772
00:33:49,960 --> 00:33:51,560
on the ones that has seen more.

773
00:33:54,400 --> 00:33:56,320
You had another paper identified

774
00:33:56,320 --> 00:33:58,600
out of Yav Goldberg's group.

775
00:33:58,600 --> 00:34:01,160
Oh yeah, so this is work led by Yanai.

776
00:34:01,160 --> 00:34:02,640
I think I'll quickly talk about this.

777
00:34:02,640 --> 00:34:07,280
So this had a similar sort of intuition

778
00:34:07,280 --> 00:34:09,800
for like trying to look at things in the data

779
00:34:09,800 --> 00:34:12,400
and trying to figure out like why the model

780
00:34:12,400 --> 00:34:16,960
has certain biases or has certain errors.

781
00:34:16,960 --> 00:34:18,840
And this was sort of a little bit more

782
00:34:18,840 --> 00:34:23,840
on trying to identify when two entities are related, right?

783
00:34:23,880 --> 00:34:27,360
So if you say, where was Barack Obama born?

784
00:34:27,360 --> 00:34:30,080
The model tends to say Chicago

785
00:34:31,040 --> 00:34:33,880
or in some sense it can say Washington

786
00:34:33,880 --> 00:34:37,280
and depending on how you phrase it.

787
00:34:37,280 --> 00:34:39,680
And like, why does it give the wrong answer?

788
00:34:39,680 --> 00:34:41,480
Is kind of the question.

789
00:34:41,480 --> 00:34:45,480
Why does it not say Hawaii or something?

790
00:34:45,480 --> 00:34:49,680
And I think to be able to answer this question,

791
00:34:49,680 --> 00:34:51,560
you have to go back to the pre-training data

792
00:34:51,560 --> 00:34:53,920
and try to see like, okay, what did it even see?

793
00:34:53,920 --> 00:34:55,280
So what I like about this paper

794
00:34:55,280 --> 00:34:59,520
is it kind of tries to build use causality tools

795
00:34:59,520 --> 00:35:01,120
and builds a whole causal graph

796
00:35:01,120 --> 00:35:05,040
for where these kinds of predictions might have come from.

797
00:35:05,040 --> 00:35:07,360
And then tries to estimate all of the edges

798
00:35:07,360 --> 00:35:08,480
in those causality graph

799
00:35:08,480 --> 00:35:10,280
and tries to do some causal inference

800
00:35:10,280 --> 00:35:15,160
to sort of attribute it to specific statistics

801
00:35:15,160 --> 00:35:16,280
of the pre-training data.

802
00:35:16,280 --> 00:35:21,280
So in this causal graph, would each individual document

803
00:35:21,280 --> 00:35:25,640
in the pre-training data be an intervention of sorts?

804
00:35:25,640 --> 00:35:27,240
So they sort of worked,

805
00:35:27,240 --> 00:35:30,360
they worked at the level of, I guess,

806
00:35:30,360 --> 00:35:32,160
triples or something like that, right?

807
00:35:32,160 --> 00:35:35,760
So let's say you see Obama in Chicago

808
00:35:35,760 --> 00:35:37,400
being a Senator there or something, right?

809
00:35:37,400 --> 00:35:40,280
So this is kind of a triple.

810
00:35:40,280 --> 00:35:43,280
And so they work on statistics of those triples

811
00:35:43,280 --> 00:35:45,800
of the pre-training data to sort of make it tractable

812
00:35:45,800 --> 00:35:49,080
and make it sort of allow this inference to work, yeah.

813
00:35:49,080 --> 00:35:52,040
But in applying the causality machinery,

814
00:35:52,040 --> 00:35:54,640
like are each of those interventions

815
00:35:54,640 --> 00:35:59,640
relative to some prior relationship

816
00:35:59,760 --> 00:36:03,320
between the things, the triples?

817
00:36:03,320 --> 00:36:05,520
Yeah, so there is the true relationship

818
00:36:05,520 --> 00:36:06,640
between these triples

819
00:36:06,640 --> 00:36:08,760
and then there is the observed relationship

820
00:36:08,760 --> 00:36:10,400
between these triples.

821
00:36:10,400 --> 00:36:13,000
And how many of these things,

822
00:36:13,000 --> 00:36:16,480
how many times it appeared in the pre-training data.

823
00:36:16,480 --> 00:36:19,280
And so the idea would be when you're doing it

824
00:36:19,280 --> 00:36:22,280
over many different entities and many different relations,

825
00:36:23,440 --> 00:36:25,720
do you, so those kind of become

826
00:36:25,720 --> 00:36:29,000
your whole data set in some sense.

827
00:36:29,000 --> 00:36:32,040
So Obama has appeared in Chicago,

828
00:36:32,040 --> 00:36:34,600
but Hillary Clinton has appeared elsewhere

829
00:36:34,600 --> 00:36:36,000
and on all of these things.

830
00:36:36,000 --> 00:36:39,840
And then together, which of these causal,

831
00:36:39,840 --> 00:36:43,520
which of these relations seem to affect

832
00:36:43,520 --> 00:36:45,600
a specific prediction the most?

833
00:36:45,600 --> 00:36:46,520
That kind of stuff.

834
00:36:46,520 --> 00:36:47,680
Awesome, awesome.

835
00:36:48,560 --> 00:36:52,640
Kind of continuing on in the data theme,

836
00:36:52,640 --> 00:36:55,360
there's been a ton of work looking at

837
00:36:55,360 --> 00:36:58,120
the need for clean data.

838
00:36:58,120 --> 00:37:00,240
I think maybe one of the most surprising things for me

839
00:37:00,240 --> 00:37:05,240
is like the return of supervision at the scale of LLMs.

840
00:37:05,560 --> 00:37:08,040
Talk a little bit about this category.

841
00:37:08,040 --> 00:37:10,800
Yeah, so this was somehow the most surprising category

842
00:37:10,800 --> 00:37:12,680
for me for this year.

843
00:37:12,680 --> 00:37:16,000
I will say that like after GPT came out

844
00:37:16,000 --> 00:37:17,840
and at the end of last year,

845
00:37:17,840 --> 00:37:21,080
everybody was kind of excited about language models,

846
00:37:21,080 --> 00:37:24,160
but the solutions for what's next always seem to be like,

847
00:37:24,160 --> 00:37:27,040
hey, let's get more data and let's get larger models

848
00:37:27,040 --> 00:37:28,680
and let's train, train longer.

849
00:37:28,680 --> 00:37:30,440
And those are still sort of useful things

850
00:37:30,440 --> 00:37:31,760
nobody's denying.

851
00:37:31,760 --> 00:37:33,560
But this year has shown that like,

852
00:37:33,560 --> 00:37:35,560
hey, you can actually do a lot

853
00:37:35,560 --> 00:37:39,120
if you're a little bit careful about your data, right?

854
00:37:39,120 --> 00:37:42,160
And maybe if you start cleaning up your data

855
00:37:42,160 --> 00:37:45,360
and try to think a little bit about where the data,

856
00:37:45,360 --> 00:37:47,120
your pre-training data should come from,

857
00:37:47,120 --> 00:37:49,520
your pre-training data itself,

858
00:37:49,520 --> 00:37:51,560
that could be quite interesting.

859
00:37:51,560 --> 00:37:55,120
So when you think of like RLHF as an example,

860
00:37:55,120 --> 00:37:57,440
do you think of that as fundamentally

861
00:37:57,440 --> 00:37:59,040
just cleaning up your data,

862
00:37:59,040 --> 00:38:01,840
being more careful about your data as opposed to?

863
00:38:01,840 --> 00:38:04,880
Yeah, so no, I think I was thinking more

864
00:38:04,880 --> 00:38:07,640
what happened with the Bloom language model,

865
00:38:07,640 --> 00:38:12,640
which was trained on sort of a lot more thoughtful process

866
00:38:12,800 --> 00:38:14,400
of gathering the data set.

867
00:38:14,400 --> 00:38:16,480
Because I mean, partly because they documented it

868
00:38:16,480 --> 00:38:18,800
and we know what they went through.

869
00:38:18,800 --> 00:38:22,840
But no, like RLHF and those kinds of things,

870
00:38:22,840 --> 00:38:26,800
I think are examples of showing that the language models

871
00:38:26,800 --> 00:38:29,440
are not quite ready for use case,

872
00:38:29,440 --> 00:38:32,680
just based on pre-training on sort of large data

873
00:38:32,680 --> 00:38:34,680
that has been gathered.

874
00:38:34,680 --> 00:38:36,600
You need to reinforce,

875
00:38:36,600 --> 00:38:38,720
like you can call it like, hey, cleaning up the data,

876
00:38:38,720 --> 00:38:41,640
but I think of it as like maybe reinforcing

877
00:38:41,640 --> 00:38:44,480
some of the nice signals in the data

878
00:38:44,480 --> 00:38:46,160
by having these examples,

879
00:38:46,160 --> 00:38:48,320
or in some sense, people have been fine tuning

880
00:38:48,320 --> 00:38:51,280
on this sort of super-based data as well.

881
00:38:51,280 --> 00:38:55,760
And the gains that you get from RLHF

882
00:38:55,760 --> 00:38:58,800
have become extremely evident this year, right?

883
00:38:58,800 --> 00:39:03,160
So somehow that has become the secret sauce of OpenAI

884
00:39:03,160 --> 00:39:04,680
and of all of these companies

885
00:39:04,680 --> 00:39:08,320
that want to have a really strong language models

886
00:39:08,320 --> 00:39:11,960
rather than scale and just raw pre-training.

887
00:39:13,080 --> 00:39:14,320
And for completeness,

888
00:39:14,320 --> 00:39:16,720
we've talked a little bit about RLHF on the show,

889
00:39:16,720 --> 00:39:20,160
but how do you think about it as a researcher?

890
00:39:20,160 --> 00:39:21,320
I think it's quite exciting.

891
00:39:21,320 --> 00:39:22,720
I think it sort of addresses

892
00:39:22,720 --> 00:39:25,600
a lot of my concerns with language models.

893
00:39:25,600 --> 00:39:29,720
I don't think pre-training data can be trusted, right?

894
00:39:29,720 --> 00:39:31,760
And you shouldn't just train something

895
00:39:31,760 --> 00:39:34,680
and expect the model to have clean output

896
00:39:34,680 --> 00:39:39,680
or have your values and any of these kinds of things,

897
00:39:41,000 --> 00:39:43,400
whatever that means in the context of large language models.

898
00:39:43,400 --> 00:39:46,760
But essentially, if you want real users

899
00:39:46,760 --> 00:39:49,800
to be interfacing with language models,

900
00:39:49,800 --> 00:39:54,120
you need to make sure that there is some sort of check.

901
00:39:54,120 --> 00:39:57,280
And RLHF is not the solution, like a full solution,

902
00:39:57,280 --> 00:40:00,720
but at least there is a way to sort of say,

903
00:40:00,720 --> 00:40:02,480
okay, this is the actual task.

904
00:40:02,480 --> 00:40:05,240
Your actual task is to be interfacing with humans,

905
00:40:05,240 --> 00:40:08,080
not just regurgitating what you've seen

906
00:40:08,080 --> 00:40:09,800
in the pre-training corpus, right?

907
00:40:09,800 --> 00:40:11,840
And so that intuition sort of is captured

908
00:40:11,840 --> 00:40:13,960
by using RLHF.

909
00:40:13,960 --> 00:40:17,080
And do you remember offhand any of the,

910
00:40:17,080 --> 00:40:18,320
if they were even published,

911
00:40:18,320 --> 00:40:22,040
the stats in terms of the number of prompts,

912
00:40:22,040 --> 00:40:26,000
like human generated prompts that were used in chat GPT

913
00:40:26,000 --> 00:40:27,640
or in struct GPT?

914
00:40:27,640 --> 00:40:30,320
Yeah, I don't think they were published as far as I know.

915
00:40:30,320 --> 00:40:33,560
Yeah, I don't remember exactly what they are.

916
00:40:33,560 --> 00:40:35,760
I think in struct GPT had the documentation

917
00:40:35,760 --> 00:40:37,560
of sort of how they were gathered,

918
00:40:37,560 --> 00:40:39,880
but the size was like, you know,

919
00:40:39,880 --> 00:40:42,480
how many of them were sort of generation tasks

920
00:40:42,480 --> 00:40:45,400
versus classification tasks, things like that.

921
00:40:45,400 --> 00:40:48,960
But I don't think the exact data set is available.

922
00:40:48,960 --> 00:40:53,960
Do you have a guess as to like the relative cost

923
00:40:54,680 --> 00:40:57,600
of collecting the human feedback

924
00:40:57,600 --> 00:41:00,320
relative to the cost of training the models?

925
00:41:00,320 --> 00:41:02,400
Oh, relative cost of training the models.

926
00:41:03,880 --> 00:41:05,160
I think it's much cheaper.

927
00:41:05,160 --> 00:41:08,400
Order of magnitude, or is it like much, much, much cheaper?

928
00:41:08,400 --> 00:41:10,080
Because we always say like, you know,

929
00:41:10,080 --> 00:41:11,200
collecting the data,

930
00:41:11,200 --> 00:41:13,480
label data is the most expensive part of machine learning.

931
00:41:13,480 --> 00:41:16,800
Is that still true at the scale of LLMs?

932
00:41:17,680 --> 00:41:20,880
Or is it that RLHF is like extremely efficient

933
00:41:20,880 --> 00:41:23,520
and you just need a little bit of guidance

934
00:41:23,520 --> 00:41:27,280
on top of the, you know, the pre-training data?

935
00:41:27,280 --> 00:41:29,560
I feel the true answer is somewhere in between.

936
00:41:29,560 --> 00:41:32,960
So I don't think it's like, it's nowhere very little data.

937
00:41:32,960 --> 00:41:35,680
Like, I think you need a lot of data to be able to do it,

938
00:41:35,680 --> 00:41:37,000
but I don't think it comes close,

939
00:41:37,000 --> 00:41:38,960
at least the way these are trained right now,

940
00:41:38,960 --> 00:41:39,880
I don't think it comes close

941
00:41:39,880 --> 00:41:42,920
to sort of training the model itself, right?

942
00:41:42,920 --> 00:41:45,920
So, but like when you think about, you know,

943
00:41:45,920 --> 00:41:49,920
chat GPT, it's been released publicly

944
00:41:49,920 --> 00:41:51,720
and a lot of people are using it.

945
00:41:51,720 --> 00:41:54,120
A lot of that data is gonna go into,

946
00:41:54,120 --> 00:41:56,960
in some form back into the model and improve it.

947
00:41:56,960 --> 00:42:00,440
So was that expensive to collect?

948
00:42:00,440 --> 00:42:02,560
In some sense, because they had to run chat GPT,

949
00:42:02,560 --> 00:42:05,360
but you know, they'll probably pay some annotators

950
00:42:05,360 --> 00:42:06,200
to clean that up,

951
00:42:06,200 --> 00:42:07,640
but I don't think that's gonna compare

952
00:42:07,640 --> 00:42:09,040
to the actual training.

953
00:42:09,040 --> 00:42:14,040
It's also a really interesting example of like bootstrapping

954
00:42:14,640 --> 00:42:17,640
like there's a certain amount that they collected themselves

955
00:42:17,640 --> 00:42:19,920
you know, they instruct GPT work

956
00:42:19,920 --> 00:42:22,880
and then they, you know, created something

957
00:42:22,880 --> 00:42:25,240
that was good enough to set loose in the world.

958
00:42:25,240 --> 00:42:27,000
And now they've got this virtual cycle

959
00:42:27,000 --> 00:42:29,480
where I'm imagining it's a lot cheaper

960
00:42:29,480 --> 00:42:32,440
for some annotator to clean up what, you know,

961
00:42:32,440 --> 00:42:34,000
millions of people are creating

962
00:42:34,000 --> 00:42:36,880
than for them to create that themselves.

963
00:42:36,880 --> 00:42:39,400
And I think like, I think this year has also shown

964
00:42:39,400 --> 00:42:41,040
maybe even to people at OpenAI

965
00:42:41,040 --> 00:42:43,280
that the value of these things, right?

966
00:42:43,280 --> 00:42:45,120
Like when they released GPT-3,

967
00:42:45,120 --> 00:42:47,120
they probably didn't realize how valuable this would be.

968
00:42:47,120 --> 00:42:48,760
And then they sort of collected data,

969
00:42:48,760 --> 00:42:51,320
released instruct GPT and yeah, on their benchmarks,

970
00:42:51,320 --> 00:42:52,160
it was good.

971
00:42:52,160 --> 00:42:53,480
But once people started using it,

972
00:42:53,480 --> 00:42:55,280
you realize how much better it is.

973
00:42:55,280 --> 00:42:56,960
I think similarly with chat GPT,

974
00:42:56,960 --> 00:42:59,160
they probably knew how good it was,

975
00:42:59,160 --> 00:43:00,600
but they probably didn't realize

976
00:43:00,600 --> 00:43:03,360
how good it actually is, right?

977
00:43:03,360 --> 00:43:07,480
And I think this idea of human feedback

978
00:43:07,480 --> 00:43:10,440
being a secret sauce that is proprietary,

979
00:43:10,440 --> 00:43:15,080
I think can sort of will continue to be a bigger piece

980
00:43:15,080 --> 00:43:15,920
in the future.

981
00:43:17,760 --> 00:43:19,880
Talk a little bit about Roots.

982
00:43:19,880 --> 00:43:22,320
Yeah, so the Roots is this nice dataset

983
00:43:22,320 --> 00:43:25,000
that was gathered by the big science group.

984
00:43:25,000 --> 00:43:27,040
And I've been following the big science group

985
00:43:27,040 --> 00:43:31,440
and they've done a bunch of interesting things there.

986
00:43:31,440 --> 00:43:33,960
I guess I'll jump in to refer to the interview

987
00:43:33,960 --> 00:43:35,360
that I did with Thomas Wolfe

988
00:43:35,360 --> 00:43:38,640
that I don't think Roots came up explicitly,

989
00:43:38,640 --> 00:43:40,000
but we talked about that work

990
00:43:40,000 --> 00:43:43,040
and that eventually resulted in Bloom,

991
00:43:43,040 --> 00:43:45,080
which we'll talk about a little bit more as well.

992
00:43:45,080 --> 00:43:48,400
Yeah, so Roots I like because I think I really like

993
00:43:48,400 --> 00:43:50,640
what Luther have done with the pile dataset

994
00:43:50,640 --> 00:43:52,640
by releasing the dataset that was used to train

995
00:43:52,640 --> 00:43:54,280
all the GPTJ models.

996
00:43:54,280 --> 00:43:57,000
And I think the big science group sort of took that intuition

997
00:43:57,000 --> 00:43:58,560
and sort of went further with it

998
00:43:58,560 --> 00:44:01,080
where they have a really well-documented

999
00:44:02,280 --> 00:44:03,840
and not just well-documented,

1000
00:44:03,840 --> 00:44:06,880
I would say a very thoughtful process

1001
00:44:06,880 --> 00:44:08,920
of gathering this dataset.

1002
00:44:08,920 --> 00:44:12,560
It's multilingual over many, many different languages.

1003
00:44:12,560 --> 00:44:14,760
They've been careful about sort of listing

1004
00:44:14,760 --> 00:44:16,760
which sources they want to even crawl

1005
00:44:16,760 --> 00:44:19,200
in the first place before.

1006
00:44:19,200 --> 00:44:21,760
So it's not like a post hoc cleanup of the data.

1007
00:44:21,760 --> 00:44:24,600
It's very sort of thinking about it.

1008
00:44:24,600 --> 00:44:26,640
They gathered a dataset that is huge

1009
00:44:26,640 --> 00:44:29,520
and they have, we talked a little bit about this data also,

1010
00:44:29,520 --> 00:44:32,240
but Hugging Face has sort of built tools on top of it

1011
00:44:32,240 --> 00:44:34,720
to be able to quickly search it,

1012
00:44:34,720 --> 00:44:37,080
to see what's in it and stuff like that.

1013
00:44:37,080 --> 00:44:39,880
And I kind of like that approach

1014
00:44:39,880 --> 00:44:41,600
to large language models, right?

1015
00:44:41,600 --> 00:44:45,880
So I think getting the right dataset

1016
00:44:45,880 --> 00:44:47,800
is crucial for these language models

1017
00:44:47,800 --> 00:44:50,680
and doing this documentation and stuff

1018
00:44:50,680 --> 00:44:53,040
is good for in the long term.

1019
00:44:53,040 --> 00:44:57,120
So your next category is decoding only.

1020
00:44:57,120 --> 00:44:59,680
Talk a little bit about what that means.

1021
00:44:59,680 --> 00:45:02,760
Yeah, so this is a theme that I like

1022
00:45:02,760 --> 00:45:06,360
about some of the work that has come out here.

1023
00:45:06,360 --> 00:45:09,480
And partly it's because we have these language models

1024
00:45:09,480 --> 00:45:13,080
where we have this black box interface to them.

1025
00:45:13,080 --> 00:45:15,360
And a lot of it is just prompting.

1026
00:45:15,360 --> 00:45:17,200
So changing things on the input side

1027
00:45:17,200 --> 00:45:19,160
to see what the model generates.

1028
00:45:19,160 --> 00:45:21,760
And the only thing most people are changing

1029
00:45:21,760 --> 00:45:23,000
on the output side is like,

1030
00:45:23,000 --> 00:45:24,920
oh, let's change the temperature a little bit

1031
00:45:24,920 --> 00:45:27,520
and we get a bunch of different things.

1032
00:45:27,520 --> 00:45:29,240
But there has been a bunch of work looking at,

1033
00:45:29,240 --> 00:45:31,000
okay, let's not just do that.

1034
00:45:31,000 --> 00:45:33,960
Let's actually think about what's happening

1035
00:45:33,960 --> 00:45:37,880
in the output of the model during decoding of the text.

1036
00:45:37,880 --> 00:45:40,000
And maybe we can do smart things there

1037
00:45:41,120 --> 00:45:44,640
that actually sort of change the output considerably, right?

1038
00:45:44,640 --> 00:45:49,160
So some of these sort of came out sort of late last year.

1039
00:45:49,160 --> 00:45:52,160
So there was this work on nucleus sampling,

1040
00:45:52,160 --> 00:45:53,560
maybe that's a little bit older,

1041
00:45:53,560 --> 00:45:54,600
but then there was this stuff

1042
00:45:54,600 --> 00:45:57,520
on sort of constrained decoding as well,

1043
00:45:57,520 --> 00:46:00,400
where the constraint decoding paper

1044
00:46:00,400 --> 00:46:02,640
came out of semantic machines.

1045
00:46:02,640 --> 00:46:06,040
They showed that you can have,

1046
00:46:06,040 --> 00:46:08,520
suppose you want the language model

1047
00:46:08,520 --> 00:46:10,160
to generate programs, right?

1048
00:46:10,160 --> 00:46:13,080
So the programs come with a certain grammar, right?

1049
00:46:13,080 --> 00:46:16,080
Like there is a syntax that they need to follow.

1050
00:46:16,080 --> 00:46:18,960
So you could actually constrain the output

1051
00:46:18,960 --> 00:46:22,720
of the language model as it's generating token by token

1052
00:46:22,720 --> 00:46:27,040
to sort of adhere to that syntax in some sense, right?

1053
00:46:27,920 --> 00:46:30,320
And just by doing this constraint, you can get,

1054
00:46:30,320 --> 00:46:32,360
firstly, obviously you will get programs

1055
00:46:32,360 --> 00:46:34,360
that are syntactically correct,

1056
00:46:34,360 --> 00:46:37,360
but you can actually get the right things out of the model.

1057
00:46:38,560 --> 00:46:42,400
And so there have been a lot of sort of works

1058
00:46:42,400 --> 00:46:45,800
looking at how can we decode

1059
00:46:45,800 --> 00:46:49,720
by having some constraints on the decoding, right?

1060
00:46:49,720 --> 00:46:51,640
So one of the papers that came out this year

1061
00:46:51,640 --> 00:46:56,640
that I believe got the best paper award as well

1062
00:46:56,680 --> 00:47:01,680
is called Neuralogic A-star, A-star-esque decoding.

1063
00:47:01,680 --> 00:47:03,680
And the idea here is that

1064
00:47:03,680 --> 00:47:06,120
instead of just doing left to right decoding

1065
00:47:06,120 --> 00:47:07,440
where you're being greedy

1066
00:47:07,440 --> 00:47:09,920
or where you're being doing some kind of beam search

1067
00:47:09,920 --> 00:47:12,040
or sampling or any of these process,

1068
00:47:12,040 --> 00:47:15,840
why don't you actually use some of the computer science ideas

1069
00:47:15,840 --> 00:47:19,120
that we have like A-star search

1070
00:47:19,120 --> 00:47:22,220
and try to find the best possible decoding.

1071
00:47:24,160 --> 00:47:25,600
And then when you're doing this kind of thing,

1072
00:47:25,600 --> 00:47:27,360
you can also think about constraints

1073
00:47:27,360 --> 00:47:29,320
that you might want to put on the decoding.

1074
00:47:29,320 --> 00:47:30,800
So you want to say, look,

1075
00:47:30,800 --> 00:47:34,960
I want the decoding to have these three words in it, right?

1076
00:47:35,920 --> 00:47:38,680
Like A, you're generating a recipe,

1077
00:47:38,680 --> 00:47:41,640
make sure that it has these five ingredients, right?

1078
00:47:41,640 --> 00:47:43,560
Somewhere in the generated text.

1079
00:47:44,400 --> 00:47:45,480
You can also flip it around,

1080
00:47:45,480 --> 00:47:48,480
hey, generate whatever text you generate,

1081
00:47:48,480 --> 00:47:51,560
make sure it doesn't have these specific words, right?

1082
00:47:51,560 --> 00:47:52,800
Like that.

1083
00:47:52,800 --> 00:47:57,800
And this paper sort of uses A-star during decoding

1084
00:47:57,920 --> 00:48:01,560
to generate that text that sort of,

1085
00:48:01,560 --> 00:48:04,060
you know, your constraints are satisfied.

1086
00:48:05,080 --> 00:48:06,240
And this paper showed that, yeah,

1087
00:48:06,240 --> 00:48:07,360
once you do that properly,

1088
00:48:07,360 --> 00:48:10,880
you can actually do a lot of the tasks much better

1089
00:48:10,880 --> 00:48:12,840
just by controlling decoding

1090
00:48:12,840 --> 00:48:15,400
rather than changing much on the inputs.

1091
00:48:15,400 --> 00:48:17,080
It seems like this is another example

1092
00:48:17,080 --> 00:48:22,080
where it's predicated on having open access

1093
00:48:22,440 --> 00:48:23,800
to the model internals

1094
00:48:23,800 --> 00:48:27,900
and you potentially lose a lot if you don't.

1095
00:48:27,900 --> 00:48:30,320
Yeah, I think, so from what I understand,

1096
00:48:30,320 --> 00:48:32,400
you can still do these kinds of things

1097
00:48:32,400 --> 00:48:35,200
with GPT-3 to some degree.

1098
00:48:35,200 --> 00:48:37,440
I think what you need, okay,

1099
00:48:37,440 --> 00:48:40,960
so you can do this with a black box model

1100
00:48:40,960 --> 00:48:43,000
as long as you get the probabilities

1101
00:48:43,000 --> 00:48:45,720
of all of the tokens at every step, right?

1102
00:48:45,720 --> 00:48:48,680
So I don't think GPT-3 actually does that,

1103
00:48:48,680 --> 00:48:51,720
but you know, you could imagine an API that says,

1104
00:48:51,720 --> 00:48:53,600
okay, the next, here's the distribution

1105
00:48:53,600 --> 00:48:54,960
over all of the tokens.

1106
00:48:56,280 --> 00:49:00,400
And you should be still be able to do these kind of things.

1107
00:49:00,400 --> 00:49:03,640
So, you know, some of the concerns is like,

1108
00:49:03,640 --> 00:49:05,600
if you want decoding to be fast,

1109
00:49:05,600 --> 00:49:08,200
then it's difficult to use some of these ideas.

1110
00:49:08,200 --> 00:49:11,600
The A star one specificity is a lot slower,

1111
00:49:12,440 --> 00:49:14,480
but it's able to satisfy your constraints.

1112
00:49:14,480 --> 00:49:18,680
So it can be where you're okay to trade off some time,

1113
00:49:18,680 --> 00:49:21,360
but let the model take more time

1114
00:49:21,360 --> 00:49:24,280
in making sure the output is clean

1115
00:49:24,280 --> 00:49:25,600
and satisfies your constraint.

1116
00:49:25,600 --> 00:49:27,300
This could be really, really cool.

1117
00:49:28,680 --> 00:49:30,840
And now, yeah, often you see,

1118
00:49:30,840 --> 00:49:34,760
hey, we applied one method, A star in this case,

1119
00:49:34,760 --> 00:49:37,320
let's go back to the computer science toolkit

1120
00:49:37,320 --> 00:49:38,520
and apply everything else.

1121
00:49:38,520 --> 00:49:40,440
Have we seen that here?

1122
00:49:40,440 --> 00:49:41,280
Not yet.

1123
00:49:41,280 --> 00:49:43,880
This, I think came out late enough in the year,

1124
00:49:43,880 --> 00:49:46,320
but I guess it came out sort of early in the year,

1125
00:49:46,320 --> 00:49:49,160
but yeah, we haven't seen that much yet because,

1126
00:49:49,160 --> 00:49:50,000
but I think, yeah,

1127
00:49:50,000 --> 00:49:52,400
that's the kind of thing that will happen next is like,

1128
00:49:52,400 --> 00:49:54,680
okay, now this is, yeah,

1129
00:49:54,680 --> 00:49:57,080
this is attracting a whole different kind of thinking

1130
00:49:57,080 --> 00:49:59,880
where people were not thinking about decoding at all,

1131
00:49:59,880 --> 00:50:01,920
and now they will be in this light,

1132
00:50:01,920 --> 00:50:04,720
which is always a sign of good paper.

1133
00:50:04,720 --> 00:50:05,560
Awesome, awesome.

1134
00:50:05,560 --> 00:50:09,840
Well, those are great themes to kind of reflect on

1135
00:50:09,840 --> 00:50:14,480
as we think about the past year and NLP research.

1136
00:50:14,480 --> 00:50:19,160
Our next category is to talk about some of the new tools

1137
00:50:19,160 --> 00:50:23,320
and open source projects that we saw in the year.

1138
00:50:23,320 --> 00:50:26,400
We've already talked a little bit about datasets,

1139
00:50:26,400 --> 00:50:28,060
which is kind of related,

1140
00:50:29,440 --> 00:50:33,040
but I think the first thing you have here is OPT.

1141
00:50:33,040 --> 00:50:34,680
Tell us about OPT.

1142
00:50:34,680 --> 00:50:39,680
So yeah, I think OPT came out fairly early in this year,

1143
00:50:40,040 --> 00:50:42,320
and I think it kind of surprised everyone

1144
00:50:42,320 --> 00:50:46,920
because the sort of looking back at last year,

1145
00:50:46,920 --> 00:50:49,880
there weren't that many open source reproductions

1146
00:50:49,880 --> 00:50:51,480
of large sizes, right?

1147
00:50:51,480 --> 00:50:55,000
So I think Luther AI was sort of leading it.

1148
00:50:55,000 --> 00:50:57,160
GPT-J was 6 billion,

1149
00:50:57,160 --> 00:51:00,240
and they were sort of growing it slowly and slowly,

1150
00:51:00,240 --> 00:51:02,940
and they had got to 20 billion parameters.

1151
00:51:02,940 --> 00:51:06,660
And then OPT sort of came into the scene,

1152
00:51:06,660 --> 00:51:09,860
and there were a bunch of nice things.

1153
00:51:09,860 --> 00:51:12,660
They documented a lot of their whole training process

1154
00:51:12,660 --> 00:51:15,600
in a log book with sort of all kinds of insights

1155
00:51:15,600 --> 00:51:17,420
about what training a log.

1156
00:51:18,420 --> 00:51:21,100
Yes, it was released by Beta, right?

1157
00:51:21,100 --> 00:51:24,420
And that was also, not to say too much against Beta,

1158
00:51:24,420 --> 00:51:27,220
but it was also surprising that reproducibility

1159
00:51:27,220 --> 00:51:30,740
and open source seemed to be key aspect of OPT as well.

1160
00:51:30,740 --> 00:51:33,380
So that was kind of nice.

1161
00:51:33,380 --> 00:51:36,940
And they also released a lot of models

1162
00:51:36,940 --> 00:51:39,700
and like all different sizes,

1163
00:51:40,660 --> 00:51:42,900
including 175 billion,

1164
00:51:42,900 --> 00:51:46,780
which hadn't been available at all.

1165
00:51:46,780 --> 00:51:47,620
And even right now,

1166
00:51:47,620 --> 00:51:50,620
I think it's probably the most useful model

1167
00:51:50,620 --> 00:51:53,180
if you want to do stuff with 175 billion

1168
00:51:53,180 --> 00:51:55,740
is to use the OPT model, right?

1169
00:51:55,740 --> 00:52:00,740
So I think the idea of documenting the whole training data,

1170
00:52:01,420 --> 00:52:02,820
gathering process,

1171
00:52:02,820 --> 00:52:05,900
documenting the whole training of the model process,

1172
00:52:05,900 --> 00:52:07,980
and then releasing all of these models

1173
00:52:09,620 --> 00:52:10,900
available for research,

1174
00:52:10,900 --> 00:52:13,780
I think has helped the research community a lot.

1175
00:52:13,780 --> 00:52:16,460
And I expect that if there are people

1176
00:52:16,460 --> 00:52:18,580
who want to build models

1177
00:52:18,580 --> 00:52:20,580
and potentially fine tune language models

1178
00:52:20,580 --> 00:52:22,580
and do all of these things,

1179
00:52:22,580 --> 00:52:24,740
the OPT would be a pretty big resource.

1180
00:52:24,740 --> 00:52:27,580
Have you seen much in terms of benchmarking it

1181
00:52:27,580 --> 00:52:29,780
against GPT-3?

1182
00:52:29,780 --> 00:52:32,780
Yeah, so I think people have been benchmarking it

1183
00:52:32,780 --> 00:52:36,060
and I think it performs reasonably well.

1184
00:52:36,060 --> 00:52:37,980
The tricky thing is, of course,

1185
00:52:37,980 --> 00:52:39,420
there is Instruct GPT,

1186
00:52:39,420 --> 00:52:44,220
which is when you call GPT-3 on the API right now,

1187
00:52:44,220 --> 00:52:47,100
it's often defaults to the Instruct one.

1188
00:52:47,100 --> 00:52:49,340
And that one is a lot more difficult to beat,

1189
00:52:49,340 --> 00:52:51,140
but for all of the purposes,

1190
00:52:51,140 --> 00:52:52,540
I think of it as like, yeah,

1191
00:52:52,540 --> 00:52:55,500
OPT is basically the same as GPT-3.

1192
00:52:55,500 --> 00:53:00,500
We talked a little bit about the big science project

1193
00:53:01,460 --> 00:53:04,500
and one of its outputs, another is Bloom.

1194
00:53:05,660 --> 00:53:08,500
What did you, what was your take on Bloom?

1195
00:53:08,500 --> 00:53:12,620
Bloom was again, a really big data model

1196
00:53:12,620 --> 00:53:15,220
that was, I think, 180 billion parameters.

1197
00:53:15,220 --> 00:53:20,100
So similar sizes, GPT-3 released to be completely open source

1198
00:53:20,100 --> 00:53:22,940
it's like we talked about completely well-documented

1199
00:53:22,940 --> 00:53:26,660
data process and sort of training process

1200
00:53:26,660 --> 00:53:29,100
combined with the fact that this was done

1201
00:53:29,100 --> 00:53:31,900
by a group of people just kind of just volunteering

1202
00:53:31,900 --> 00:53:33,940
their time to do so.

1203
00:53:33,940 --> 00:53:38,180
And then being able to reproduce to a large degree

1204
00:53:38,180 --> 00:53:41,740
what OpenAI has done was quite amazing, right?

1205
00:53:41,740 --> 00:53:45,180
And then sort of, again, like both OPTs releasing

1206
00:53:45,180 --> 00:53:46,980
all of these things is kind of a sign

1207
00:53:46,980 --> 00:53:49,660
for other big tech companies to say like,

1208
00:53:49,660 --> 00:53:50,660
hey, you can do this

1209
00:53:50,660 --> 00:53:52,860
because we have done this kind of thing.

1210
00:53:52,860 --> 00:53:56,660
But what Bloom has shown is that a bunch of people

1211
00:53:56,660 --> 00:54:00,460
enthusiastic and excited folks that are enterprising

1212
00:54:00,460 --> 00:54:04,500
can actually do things that maybe even a year or two ago

1213
00:54:04,500 --> 00:54:06,500
would have seemed impossible.

1214
00:54:06,500 --> 00:54:11,500
It may have been in our trends conversation from last year

1215
00:54:11,500 --> 00:54:16,500
or maybe it was prior, but in these kinds of conversations

1216
00:54:19,660 --> 00:54:22,260
there was a point in time where we were lamenting

1217
00:54:22,260 --> 00:54:25,020
the kind of the loss on the part

1218
00:54:25,020 --> 00:54:28,500
of the individual academic researcher to contribute

1219
00:54:28,500 --> 00:54:30,940
to fundamental model research

1220
00:54:30,940 --> 00:54:33,580
because of the resources that were required

1221
00:54:33,580 --> 00:54:38,580
and to hugging face and the big sciences system

1222
00:54:38,580 --> 00:54:43,580
like they showed that, not necessarily, not so fast, right?

1223
00:54:43,620 --> 00:54:45,420
Right, right, right, exactly.

1224
00:54:45,420 --> 00:54:48,380
And the other thing I like about this, the Bloom effort

1225
00:54:48,380 --> 00:54:50,540
is and the corpus that came with it,

1226
00:54:50,540 --> 00:54:53,700
they were also focused on being a lot more inclusive

1227
00:54:53,700 --> 00:54:56,340
in terms of having a global perspective.

1228
00:54:56,340 --> 00:54:59,460
So they would try to cover many, many different languages.

1229
00:54:59,460 --> 00:55:02,300
Very principled in the way they pulled the data together.

1230
00:55:02,300 --> 00:55:04,020
Yeah, yeah.

1231
00:55:04,020 --> 00:55:05,460
And also multilingual in a way

1232
00:55:05,460 --> 00:55:08,100
that none of the experts would have been able to do.

1233
00:55:08,100 --> 00:55:10,140
None of the existing models have been.

1234
00:55:10,140 --> 00:55:12,580
So yeah, it was quite exciting.

1235
00:55:12,580 --> 00:55:16,100
And so like conceptually, this is a great example

1236
00:55:16,100 --> 00:55:21,100
of how one model at 175 billion parameters

1237
00:55:23,420 --> 00:55:26,380
and another model, the same number of parameters

1238
00:55:26,380 --> 00:55:29,700
could be very different, at least in the data

1239
00:55:29,700 --> 00:55:30,660
that they were trained on.

1240
00:55:30,660 --> 00:55:33,100
And you would expect that to result

1241
00:55:33,100 --> 00:55:37,140
in very different results using the model.

1242
00:55:37,140 --> 00:55:40,300
To what extent have we characterized that?

1243
00:55:40,300 --> 00:55:43,420
Like at that scale of data, it's still a lot of data.

1244
00:55:43,420 --> 00:55:46,820
It's still a lot of like raw internet data.

1245
00:55:46,820 --> 00:55:49,220
Does it all kind of fall out in the wash

1246
00:55:49,220 --> 00:55:51,420
and all their efforts at being principled

1247
00:55:52,980 --> 00:55:54,540
kind of just get lost?

1248
00:55:54,540 --> 00:55:57,780
Or do we know how to compare that?

1249
00:55:57,780 --> 00:56:00,220
Yeah, so there have been a bunch of benchmarks

1250
00:56:00,220 --> 00:56:02,740
including in their papers, but in general also.

1251
00:56:02,740 --> 00:56:07,060
And that's where sort of the, don't hold me to this,

1252
00:56:07,060 --> 00:56:11,060
but I would say like Bloom is not the go-to language model

1253
00:56:11,060 --> 00:56:14,700
for people if they want to do English things right now.

1254
00:56:14,700 --> 00:56:17,340
So I think maybe some of the trade-offs

1255
00:56:17,340 --> 00:56:19,060
they made in collecting the data

1256
00:56:19,060 --> 00:56:21,780
or even just having more languages

1257
00:56:22,780 --> 00:56:24,900
resulted in a model that's definitely really good

1258
00:56:24,900 --> 00:56:29,460
for multilingual things, but that's not what our benchmarks

1259
00:56:29,460 --> 00:56:32,220
have been designed for, unfortunately.

1260
00:56:32,220 --> 00:56:34,900
And so if you just look at the benchmarks,

1261
00:56:34,900 --> 00:56:38,540
which are traditionally designed for English,

1262
00:56:38,540 --> 00:56:43,540
Bloom, I don't think quite is at par with OPD or GPTC

1263
00:56:44,780 --> 00:56:46,460
and definitely not with Instruct.

1264
00:56:49,020 --> 00:56:51,260
And when I mentioned benchmark,

1265
00:56:51,260 --> 00:56:54,180
there's that aspect of kind of applying

1266
00:56:54,180 --> 00:56:58,820
the traditional performance benchmarks for LLMs to Bloom

1267
00:56:58,820 --> 00:57:02,180
and comparing their results to the others.

1268
00:57:02,180 --> 00:57:07,180
But I'm also curious about how we characterize

1269
00:57:07,700 --> 00:57:09,340
like qualitative differences

1270
00:57:09,340 --> 00:57:14,340
between the way Bloom responds and the way GPT responds,

1271
00:57:15,820 --> 00:57:20,820
for example, in terms of like the kind of fairness

1272
00:57:22,540 --> 00:57:27,540
considerations or that kind of thing,

1273
00:57:27,980 --> 00:57:30,580
or are there qualitative differences

1274
00:57:30,580 --> 00:57:33,980
in the kinds of responses that you get

1275
00:57:33,980 --> 00:57:37,380
that aren't picked up by the traditional benchmarks

1276
00:57:37,380 --> 00:57:41,540
or are the traditional benchmarks like so expansive

1277
00:57:41,540 --> 00:57:44,300
at this point we've kind of characterized

1278
00:57:44,300 --> 00:57:46,500
a lot of that stuff explicitly?

1279
00:57:46,500 --> 00:57:48,780
Yeah, again, I think the answer is somewhere in between.

1280
00:57:48,780 --> 00:57:51,180
So I don't know if people have thoroughly compared the two

1281
00:57:51,180 --> 00:57:53,460
to see like, hey, what's the level of toxicity

1282
00:57:53,460 --> 00:57:54,620
and things like that.

1283
00:57:55,820 --> 00:57:57,060
I think when OPD came out,

1284
00:57:57,060 --> 00:57:59,260
they did a lot of this analysis in their paper

1285
00:57:59,260 --> 00:58:01,340
of like, hey, how toxic is that model?

1286
00:58:01,340 --> 00:58:03,020
How safe is that model?

1287
00:58:03,020 --> 00:58:04,620
And they realized that, yeah,

1288
00:58:04,620 --> 00:58:06,100
in some things they were worse off

1289
00:58:06,100 --> 00:58:10,060
than some of the existing models.

1290
00:58:10,060 --> 00:58:14,260
But I think with Bloom specifically,

1291
00:58:14,260 --> 00:58:16,300
I don't know off the top of my head

1292
00:58:16,300 --> 00:58:21,300
how it's all compared in terms of these other aspects.

1293
00:58:22,260 --> 00:58:25,660
Okay, talk about the inverse scaling competition.

1294
00:58:25,660 --> 00:58:28,540
Yeah, so this was a pretty nice thing that came out.

1295
00:58:28,540 --> 00:58:30,580
And I think, I suppose it's still going on

1296
00:58:30,580 --> 00:58:32,060
even though the submissions are down.

1297
00:58:32,060 --> 00:58:33,860
So I'm kind of hoping to see

1298
00:58:33,860 --> 00:58:36,460
what the actual effect of this was.

1299
00:58:36,460 --> 00:58:37,820
But this was sort of introduced

1300
00:58:37,820 --> 00:58:40,380
sort of in the middle of the year.

1301
00:58:40,380 --> 00:58:44,500
And the idea here is the thinking of things

1302
00:58:44,500 --> 00:58:47,300
like what sort of scaling laws was showing, right?

1303
00:58:47,300 --> 00:58:49,980
Like when you scale up your models,

1304
00:58:49,980 --> 00:58:52,460
performance goes up for everything.

1305
00:58:52,460 --> 00:58:53,900
And that's kind of exciting to see,

1306
00:58:53,900 --> 00:58:57,180
but it also tells us that, okay,

1307
00:58:57,180 --> 00:58:59,340
there are many, many things that just,

1308
00:58:59,340 --> 00:59:02,220
the models would just get better on as time goes by

1309
00:59:02,220 --> 00:59:05,700
because they'll get bigger, they'll have more data set.

1310
00:59:05,700 --> 00:59:08,260
The inverse scaling was this intuition to see,

1311
00:59:08,260 --> 00:59:11,260
okay, what are, can we characterize the phenomenas

1312
00:59:11,260 --> 00:59:15,860
that don't have the same trend, right?

1313
00:59:15,860 --> 00:59:20,020
So other aspects, you create a data set,

1314
00:59:20,020 --> 00:59:21,820
which is something everybody will agree

1315
00:59:21,820 --> 00:59:23,300
is a reasonable data set.

1316
00:59:23,300 --> 00:59:27,260
But when you give them to larger models,

1317
00:59:27,260 --> 00:59:30,540
they actually get worse.

1318
00:59:30,540 --> 00:59:33,420
And so this prize in this competition

1319
00:59:33,420 --> 00:59:38,420
is an effort to identify what those tasks would be

1320
00:59:39,700 --> 00:59:43,180
and sort of the better your inverse scaling is.

1321
00:59:43,180 --> 00:59:46,260
So the worse, the bigger models are on your,

1322
00:59:46,260 --> 00:59:48,540
on the data set that you've contributed,

1323
00:59:48,540 --> 00:59:51,940
the more likely you are to win this competition.

1324
00:59:51,940 --> 00:59:53,420
And so, yeah, they've had the submissions

1325
00:59:53,420 --> 00:59:55,620
and they're kind of evaluating them, I suppose,

1326
00:59:55,620 --> 00:59:57,940
and they haven't quite announced it.

1327
00:59:57,940 --> 01:00:00,380
But I think a lot of the stuff on,

1328
01:00:01,700 --> 01:00:03,060
a lot of the interesting things

1329
01:00:03,060 --> 01:00:06,260
could come out of this effort.

1330
01:00:06,260 --> 01:00:07,900
So one thing I could imagine

1331
01:00:07,900 --> 01:00:11,620
is sort of deeper levels of misinformation

1332
01:00:11,620 --> 01:00:15,020
where the model is relying so much

1333
01:00:15,020 --> 01:00:18,300
on what it has seen in its training data.

1334
01:00:18,300 --> 01:00:19,460
Let's not call it misinformation,

1335
01:00:19,460 --> 01:00:22,740
just not being able to update its information

1336
01:00:22,740 --> 01:00:23,580
in some sense, right?

1337
01:00:23,580 --> 01:00:25,700
So these large language models

1338
01:00:25,700 --> 01:00:28,420
have memorized so much about the pre-training data

1339
01:00:28,420 --> 01:00:32,780
that they kind of reject evidence against that, right?

1340
01:00:32,780 --> 01:00:33,860
Maybe if they're smaller,

1341
01:00:33,860 --> 01:00:35,060
there's less memorization

1342
01:00:35,060 --> 01:00:37,260
and more generalization in some way.

1343
01:00:37,260 --> 01:00:38,700
But I think it could be pretty exciting

1344
01:00:38,700 --> 01:00:39,980
to see what are those things

1345
01:00:39,980 --> 01:00:42,180
that actually get worse with scale.

1346
01:00:42,180 --> 01:00:44,940
I think it's quite an interesting question.

1347
01:00:44,940 --> 01:00:48,380
And next up you have the Galactica,

1348
01:00:48,380 --> 01:00:49,860
can we call it a debacle?

1349
01:00:50,900 --> 01:00:54,820
So Galactica is this LLM that Meta released

1350
01:00:54,820 --> 01:00:59,820
that was tuned to generate scientific and research text.

1351
01:01:02,460 --> 01:01:07,460
And was it even up for three days?

1352
01:01:08,020 --> 01:01:10,100
It got pulled down pretty quickly, right?

1353
01:01:10,100 --> 01:01:12,860
Yep, yeah, I think maybe a little bit more than that,

1354
01:01:12,860 --> 01:01:14,900
but yeah, thereabouts, yeah.

1355
01:01:14,900 --> 01:01:19,300
And I think to me, it's a story

1356
01:01:19,300 --> 01:01:22,060
about how not in anything in terms

1357
01:01:22,060 --> 01:01:24,180
of what the Galactica team did itself, right?

1358
01:01:24,180 --> 01:01:26,420
Like I think the model training it,

1359
01:01:26,420 --> 01:01:29,540
everything was the right thing to be doing.

1360
01:01:30,780 --> 01:01:34,260
The tricky thing was just how it was pitched

1361
01:01:34,260 --> 01:01:39,260
and how there was just not clear caveats

1362
01:01:40,820 --> 01:01:44,300
about what this model is capable of doing

1363
01:01:44,300 --> 01:01:46,820
and what it's not capable of doing

1364
01:01:46,820 --> 01:01:49,620
that led to such a backlash, right?

1365
01:01:49,620 --> 01:01:54,620
So I think it was a language model train

1366
01:01:56,020 --> 01:01:57,660
on a lot of science papers.

1367
01:01:57,660 --> 01:01:59,300
So it's going to produce papers

1368
01:01:59,300 --> 01:02:01,660
that look like scientific text.

1369
01:02:01,660 --> 01:02:04,300
I think that was an expected thing,

1370
01:02:05,220 --> 01:02:08,780
but again, the backlash it got and stuff like that

1371
01:02:08,780 --> 01:02:10,140
essentially tells everyone,

1372
01:02:10,140 --> 01:02:14,860
and I hope the message is not to demo language models

1373
01:02:14,860 --> 01:02:16,860
anymore, but I think the message should be

1374
01:02:16,860 --> 01:02:20,180
how to make sure that you're not hyping things up

1375
01:02:20,180 --> 01:02:21,580
more than they should be.

1376
01:02:21,580 --> 01:02:23,660
If you reflect on ChatGPT,

1377
01:02:23,660 --> 01:02:27,260
which came not very long after Galactica

1378
01:02:27,260 --> 01:02:32,260
and the launches of those respective products,

1379
01:02:34,460 --> 01:02:38,260
are there clear, is there a clear like do don't do list?

1380
01:02:38,260 --> 01:02:40,820
So I will say that ChatGPT itself

1381
01:02:40,820 --> 01:02:45,820
was also not completely without hype attached to it,

1382
01:02:46,100 --> 01:02:47,540
even some sort of how they-

1383
01:02:47,540 --> 01:02:48,380
Right.

1384
01:02:48,380 --> 01:02:49,220
Yeah, right?

1385
01:02:49,220 --> 01:02:50,060
Somehow they managed it.

1386
01:02:50,060 --> 01:02:51,180
There was a lot of hype.

1387
01:02:51,180 --> 01:02:52,500
Right, right, right.

1388
01:02:52,500 --> 01:02:54,700
I will say that they were fairly clear

1389
01:02:54,700 --> 01:02:57,140
about the fact that like, hey, don't trust,

1390
01:02:57,140 --> 01:02:58,660
maybe they could have been clearer,

1391
01:02:58,660 --> 01:03:01,140
but like don't trust the factual stuff

1392
01:03:01,140 --> 01:03:01,980
and things like that.

1393
01:03:01,980 --> 01:03:04,220
Like it's not a lookup engine.

1394
01:03:04,220 --> 01:03:06,740
I think they kind of could have done a lot more of that,

1395
01:03:06,740 --> 01:03:09,740
but they at least had some caveats.

1396
01:03:09,740 --> 01:03:12,940
But more than that, part of their RLHF stuff

1397
01:03:12,940 --> 01:03:16,580
was to make sure that the model is not producing

1398
01:03:16,580 --> 01:03:19,900
at least obviously sexist, don't say-

1399
01:03:19,900 --> 01:03:20,740
Yeah, there was a lot,

1400
01:03:20,740 --> 01:03:22,980
and maybe we're jumping into ChatGPT,

1401
01:03:22,980 --> 01:03:25,220
which actually is the next thing we're gonna talk about,

1402
01:03:25,220 --> 01:03:27,860
but there was definitely a lot of,

1403
01:03:27,860 --> 01:03:29,060
especially early on,

1404
01:03:29,060 --> 01:03:31,620
things that it just would not opine on.

1405
01:03:31,620 --> 01:03:33,580
Like, yeah, no, you're not gonna sucker me in

1406
01:03:33,580 --> 01:03:34,660
to go in there.

1407
01:03:34,660 --> 01:03:35,780
Right, right, right, yeah.

1408
01:03:35,780 --> 01:03:38,020
And I think when you're building something

1409
01:03:38,020 --> 01:03:39,180
that's public-facing,

1410
01:03:39,180 --> 01:03:42,420
that you're selling as a tool, as a product,

1411
01:03:42,420 --> 01:03:44,380
that is necessary, right?

1412
01:03:44,380 --> 01:03:48,780
Like I don't think you should be doing otherwise.

1413
01:03:48,780 --> 01:03:52,380
Galactica should not have been a public-facing tool

1414
01:03:52,380 --> 01:03:55,780
for every scientist to start using to write their papers.

1415
01:03:55,780 --> 01:03:57,580
It should be a language model, right?

1416
01:03:57,580 --> 01:04:00,700
And what the product is or what the tool is

1417
01:04:00,700 --> 01:04:04,140
is a gap that other people can help fill in, right?

1418
01:04:04,140 --> 01:04:06,620
So that was sort of the missing piece

1419
01:04:06,620 --> 01:04:10,060
when I think about ChatGPT versus Galactica.

1420
01:04:10,060 --> 01:04:13,860
It's like, yeah, ChatGPT has some of the caveats

1421
01:04:13,860 --> 01:04:16,140
about what it's doing,

1422
01:04:16,140 --> 01:04:17,420
has some of the caveats about,

1423
01:04:17,420 --> 01:04:20,300
oh, it's a language model, not a product, to some degree,

1424
01:04:21,260 --> 01:04:24,420
and Galactica was missing it, right?

1425
01:04:24,420 --> 01:04:26,020
So, yeah.

1426
01:04:26,020 --> 01:04:27,540
Now, we're there.

1427
01:04:29,380 --> 01:04:31,460
We were talking about open source.

1428
01:04:31,460 --> 01:04:35,340
Next up is kind of commercial developments.

1429
01:04:35,340 --> 01:04:37,220
Top of that list is ChatGPT.

1430
01:04:38,220 --> 01:04:40,540
Yeah, let's talk about it.

1431
01:04:40,540 --> 01:04:44,740
You said early on that, hey, even without ChatGPT,

1432
01:04:44,740 --> 01:04:45,860
this was a huge year.

1433
01:04:45,860 --> 01:04:47,860
That's clearly not to say that ChatGPT

1434
01:04:47,860 --> 01:04:50,940
wasn't a huge contribution to the year.

1435
01:04:50,940 --> 01:04:53,620
I mean, certainly one of the things

1436
01:04:53,620 --> 01:04:56,580
that I found most interesting

1437
01:04:56,580 --> 01:05:01,580
was the degree to which it kind of broke out

1438
01:05:02,460 --> 01:05:07,460
of the MLAI echo chamber to just talking to random friends

1439
01:05:07,540 --> 01:05:10,060
and are like, hey, have you tried this ChatGPT thing?

1440
01:05:10,060 --> 01:05:11,100
Like, yeah, I have.

1441
01:05:12,820 --> 01:05:15,500
Yeah, so that's been, I guess, the most surprising

1442
01:05:15,500 --> 01:05:19,780
and in some sense, the longest-term impact for ChatGPT

1443
01:05:19,780 --> 01:05:23,460
is going to be the fact that it made it commoditized.

1444
01:05:23,460 --> 01:05:27,180
It made it mainstream in a way that nothing before it had.

1445
01:05:27,180 --> 01:05:29,020
And whether it deserved it or not,

1446
01:05:29,020 --> 01:05:32,220
what the actual innovations are and all of these things

1447
01:05:32,220 --> 01:05:34,100
is a different question, right?

1448
01:05:34,100 --> 01:05:37,500
Like it is clearly, even for research point of view,

1449
01:05:37,500 --> 01:05:40,660
qualitatively better than GPT-63.

1450
01:05:40,660 --> 01:05:42,420
Whether it met some threshold

1451
01:05:42,420 --> 01:05:44,900
for becoming the big thing that it did,

1452
01:05:44,900 --> 01:05:48,340
it is sort of difficult to, sort of in hindsight,

1453
01:05:48,340 --> 01:05:50,060
try to evaluate that.

1454
01:05:50,060 --> 01:05:54,380
But I think it was, yeah, it is definitely something

1455
01:05:54,380 --> 01:05:59,380
that became mainstream and everybody's talking about it.

1456
01:05:59,820 --> 01:06:01,580
There is still a question in my mind

1457
01:06:01,580 --> 01:06:05,780
whether that's a good thing or not in the long run,

1458
01:06:05,780 --> 01:06:09,300
because we can talk about some of the problems with ChatGPT,

1459
01:06:09,300 --> 01:06:13,180
the biggest one being, we know it's a language model.

1460
01:06:13,180 --> 01:06:15,220
Like to some degree, we've been figuring out

1461
01:06:15,220 --> 01:06:17,380
last couple of years what these things are capable of

1462
01:06:17,380 --> 01:06:18,740
and what these things are not.

1463
01:06:18,740 --> 01:06:20,900
And I can sit and in like couple of minutes

1464
01:06:20,900 --> 01:06:23,780
come up with tons of examples where it would fail.

1465
01:06:25,060 --> 01:06:27,300
That's not quite the case when you sort of start

1466
01:06:27,300 --> 01:06:28,780
putting it out in the public.

1467
01:06:28,780 --> 01:06:31,740
So most people don't know what a language model is.

1468
01:06:31,740 --> 01:06:33,500
And I've played around with,

1469
01:06:33,500 --> 01:06:35,660
I've got a bunch of my family to try it

1470
01:06:35,660 --> 01:06:36,700
and things like that.

1471
01:06:36,700 --> 01:06:39,100
And the biggest thing I've had,

1472
01:06:39,100 --> 01:06:41,660
the biggest difficulty I've had conveying to them

1473
01:06:41,660 --> 01:06:45,180
is the fact that it's not looking up anything

1474
01:06:45,180 --> 01:06:46,620
when you ask it something, right?

1475
01:06:46,620 --> 01:06:50,540
Like that is a conceptual jump

1476
01:06:50,540 --> 01:06:54,460
that is very, very difficult for people to get over.

1477
01:06:54,460 --> 01:06:55,540
Yeah.

1478
01:06:55,540 --> 01:06:56,820
And so people like,

1479
01:06:56,820 --> 01:06:58,860
yeah, oh, of course it should know about these things

1480
01:06:58,860 --> 01:07:00,780
because it happened yesterday

1481
01:07:00,780 --> 01:07:02,260
and for such a big news items,

1482
01:07:02,260 --> 01:07:03,660
like why would it not know?

1483
01:07:03,660 --> 01:07:06,660
And I'm like, no, actually it doesn't know anything

1484
01:07:06,660 --> 01:07:07,780
beyond a certain time.

1485
01:07:07,780 --> 01:07:10,540
And even saying it knows anything from then

1486
01:07:11,420 --> 01:07:12,860
is a little bit difficult.

1487
01:07:12,860 --> 01:07:17,460
Right, so I think the best analogy that I've,

1488
01:07:17,460 --> 01:07:19,580
you know, this applies to my research as well,

1489
01:07:19,580 --> 01:07:22,380
but the best analogy I've had in trying to explain

1490
01:07:22,380 --> 01:07:24,700
people what chat GPT does

1491
01:07:24,700 --> 01:07:27,860
is to not think of it as a stochastic parrot

1492
01:07:27,860 --> 01:07:28,900
or anything like that.

1493
01:07:28,900 --> 01:07:31,180
But if you have to think in terms of animals,

1494
01:07:31,180 --> 01:07:33,300
think of it as like a chameleon.

1495
01:07:33,300 --> 01:07:36,540
Like it's trying to sort of fit in

1496
01:07:36,540 --> 01:07:39,380
to a bunch of humans, right?

1497
01:07:39,380 --> 01:07:41,100
And it's trying to just write things

1498
01:07:41,100 --> 01:07:43,220
that will make it pass

1499
01:07:43,220 --> 01:07:46,700
as if it sort of knows all of those things, right?

1500
01:07:46,700 --> 01:07:49,980
I was in a Twitter exchange about,

1501
01:07:50,980 --> 01:07:53,980
I had asked chat GPT to explain RLHF

1502
01:07:56,660 --> 01:07:59,620
and it came up with this acronym

1503
01:08:00,900 --> 01:08:02,220
that was like,

1504
01:08:04,740 --> 01:08:05,700
oh, I forget it.

1505
01:08:05,700 --> 01:08:07,020
And it was really funny.

1506
01:08:07,020 --> 01:08:10,260
It was like something leaderboard,

1507
01:08:10,260 --> 01:08:13,180
you know, human something.

1508
01:08:13,180 --> 01:08:16,380
It was so far off.

1509
01:08:16,380 --> 01:08:18,660
Interestingly enough, I'd asked it about,

1510
01:08:18,660 --> 01:08:20,540
I'd had conversations, you know,

1511
01:08:20,540 --> 01:08:22,340
interactions with it about RLHF

1512
01:08:22,340 --> 01:08:24,100
and then it knew what it was.

1513
01:08:24,100 --> 01:08:24,940
Like to your point,

1514
01:08:24,940 --> 01:08:26,780
it's about kind of where it sits

1515
01:08:26,780 --> 01:08:29,180
in the context of the prompt.

1516
01:08:29,180 --> 01:08:30,780
And I just kind of posted, you know,

1517
01:08:30,780 --> 01:08:35,140
is it trolling me or is it like just trying to BS me?

1518
01:08:35,140 --> 01:08:38,340
And one of the responses that I got that,

1519
01:08:38,340 --> 01:08:40,380
you know, and reflecting on is like really insightful.

1520
01:08:40,380 --> 01:08:43,020
Like it's always trying to BS you.

1521
01:08:43,020 --> 01:08:45,740
That's all it's doing is trying to BS you

1522
01:08:45,740 --> 01:08:48,980
to produce some texts that you will think is reasonable.

1523
01:08:48,980 --> 01:08:52,540
And, you know, to its credit,

1524
01:08:52,540 --> 01:08:54,060
a lot of times it's right,

1525
01:08:54,060 --> 01:08:56,100
but that's all it's trying to do.

1526
01:08:56,100 --> 01:08:56,940
Right, right, right.

1527
01:08:56,940 --> 01:08:57,780
Yeah.

1528
01:08:57,780 --> 01:09:00,100
And especially when it comes to factual stuff

1529
01:09:00,100 --> 01:09:01,900
or even like, you know,

1530
01:09:01,900 --> 01:09:05,380
it is a very useful bullshitter in some sense, right?

1531
01:09:05,380 --> 01:09:07,380
So, because when it's right

1532
01:09:07,380 --> 01:09:08,580
or when it's partially right,

1533
01:09:08,580 --> 01:09:10,900
that's still useful because, you know,

1534
01:09:10,900 --> 01:09:12,580
it is what it is.

1535
01:09:12,580 --> 01:09:14,900
But that, when you put that label,

1536
01:09:14,900 --> 01:09:17,260
like if they had sold it as like,

1537
01:09:17,260 --> 01:09:21,340
hey, we have built a really good bullshitter, right?

1538
01:09:21,340 --> 01:09:23,180
Like, and it's sold out as a product,

1539
01:09:23,180 --> 01:09:24,500
then people would know, okay,

1540
01:09:24,500 --> 01:09:26,020
not to use it for a bunch of tasks

1541
01:09:26,020 --> 01:09:28,900
that they're currently thinking of using it, right?

1542
01:09:28,900 --> 01:09:30,420
And so, yeah.

1543
01:09:30,420 --> 01:09:34,020
So that's the sort of divide that in messaging

1544
01:09:34,020 --> 01:09:38,100
that somehow researchers and NLP folks know,

1545
01:09:38,100 --> 01:09:39,460
oh yeah, language model loss,

1546
01:09:39,460 --> 01:09:43,020
obviously all it is doing is blah, blah, blah, right?

1547
01:09:43,020 --> 01:09:45,540
And yes, RLHF can help to some degree,

1548
01:09:45,540 --> 01:09:47,780
but clearly it's not going to be able

1549
01:09:47,780 --> 01:09:51,900
to do these bunch of other things all the time.

1550
01:09:51,900 --> 01:09:56,420
And that kind of thing is missing from general public,

1551
01:09:56,420 --> 01:09:59,420
but also how a lot of people are planning to use it,

1552
01:09:59,420 --> 01:10:00,580
for example, right?

1553
01:10:00,580 --> 01:10:03,300
So I think that aspect is the part

1554
01:10:03,300 --> 01:10:06,780
that we need to think a little bit more about.

1555
01:10:06,780 --> 01:10:10,540
You've got Palm, Minerva, Flan, Down.

1556
01:10:12,500 --> 01:10:15,020
Tell me a little bit more about your take there.

1557
01:10:15,020 --> 01:10:20,020
Cause I hear of them in a vague research context

1558
01:10:20,380 --> 01:10:22,820
cause no one really has access to these,

1559
01:10:22,820 --> 01:10:27,820
but Google as much more so than something

1560
01:10:28,220 --> 01:10:30,780
that is huge from a commercial perspective.

1561
01:10:30,780 --> 01:10:33,660
Is this a prediction or is this a reflection?

1562
01:10:33,660 --> 01:10:35,340
Yeah, so no, I think of this

1563
01:10:35,340 --> 01:10:40,340
as Palm was a huge commercial development this year.

1564
01:10:40,740 --> 01:10:44,260
Like Google built this really, really large model.

1565
01:10:44,260 --> 01:10:46,980
Now there are, obviously they haven't released it, right?

1566
01:10:46,980 --> 01:10:48,620
So what's the ideal situation?

1567
01:10:48,620 --> 01:10:50,380
They completely release it open source.

1568
01:10:50,380 --> 01:10:51,660
Everybody gets access to it.

1569
01:10:51,660 --> 01:10:52,980
That's not gonna happen.

1570
01:10:52,980 --> 01:10:55,740
Another possible thing is they put an API on it

1571
01:10:55,740 --> 01:10:58,780
and charge people from a Google perspective.

1572
01:10:58,780 --> 01:11:00,300
That doesn't make sense, right?

1573
01:11:00,300 --> 01:11:02,140
So it is something that they've built.

1574
01:11:02,140 --> 01:11:03,660
It's valuable internally.

1575
01:11:03,660 --> 01:11:07,020
I'm sure it's sort of has, you know,

1576
01:11:07,020 --> 01:11:09,140
there are reasons not to make it public,

1577
01:11:09,140 --> 01:11:12,940
but it also has a lot of research insights

1578
01:11:12,940 --> 01:11:16,100
because nobody else has such a big language model

1579
01:11:16,100 --> 01:11:17,980
trained in a similar way.

1580
01:11:17,980 --> 01:11:20,620
And I guess I wanna give them props

1581
01:11:20,620 --> 01:11:23,620
for at least publishing and evaluating

1582
01:11:23,620 --> 01:11:26,620
and doing things like that with Palm

1583
01:11:26,620 --> 01:11:29,940
because it is doing,

1584
01:11:29,940 --> 01:11:32,940
it is of a size that we will not see

1585
01:11:32,940 --> 01:11:35,820
for maybe another year or two

1586
01:11:35,820 --> 01:11:38,260
to be sort of publicly available,

1587
01:11:38,260 --> 01:11:40,980
but yet we get to hear about some insights,

1588
01:11:40,980 --> 01:11:43,660
what to expect, what are the emergence behaviors

1589
01:11:44,980 --> 01:11:46,540
coming out of those language models, right?

1590
01:11:46,540 --> 01:11:48,940
So yeah, it would be ideal if you could audit it

1591
01:11:48,940 --> 01:11:51,140
and all of us and I could contribute

1592
01:11:51,140 --> 01:11:52,780
in finding out what the problems are

1593
01:11:52,780 --> 01:11:54,940
and when it works and when it doesn't work.

1594
01:11:54,940 --> 01:11:58,140
But given that, I think they did a good job.

1595
01:11:58,140 --> 01:12:00,060
Specifically, what I will say is that

1596
01:12:00,060 --> 01:12:03,940
that size has brought up a bunch of capabilities

1597
01:12:03,940 --> 01:12:05,740
like the whole chain of thought thing

1598
01:12:05,740 --> 01:12:08,180
that we talked about at the beginning

1599
01:12:08,180 --> 01:12:11,380
that somehow became possible at that size,

1600
01:12:11,380 --> 01:12:15,500
but wasn't possible at other sites, right?

1601
01:12:15,500 --> 01:12:20,220
So that's why that research is also all coming out of Google

1602
01:12:20,220 --> 01:12:25,220
because it applies mostly to LLMs of that size.

1603
01:12:27,820 --> 01:12:30,980
Palm is 540 billion parameters?

1604
01:12:30,980 --> 01:12:33,180
Something, yeah, 540, yeah.

1605
01:12:33,180 --> 01:12:35,740
So I think they have access to it

1606
01:12:35,740 --> 01:12:38,260
and they can produce a string of papers

1607
01:12:38,260 --> 01:12:40,500
and yes, nobody else can write those papers,

1608
01:12:40,500 --> 01:12:44,380
but from a consumer of research as well as producer, right?

1609
01:12:44,380 --> 01:12:47,820
So from my consumer side, I love to read research

1610
01:12:47,820 --> 01:12:50,460
and I'm glad that they're writing those papers

1611
01:12:50,460 --> 01:12:52,980
because there's a lot of interesting stuff

1612
01:12:52,980 --> 01:12:54,100
in all of the papers.

1613
01:12:54,100 --> 01:12:55,980
So yeah, there's a whole string of papers

1614
01:12:55,980 --> 01:12:56,820
that I would recommend

1615
01:12:56,820 --> 01:12:59,060
and I can point you to them offline.

1616
01:12:59,060 --> 01:13:04,060
But yeah, there's stuff that we'll see happen publicly

1617
01:13:04,140 --> 01:13:06,100
next year or maybe another year after that

1618
01:13:06,100 --> 01:13:08,780
when those models become commercialized.

1619
01:13:08,780 --> 01:13:13,300
So yeah, no, I think that that's been kind of key.

1620
01:13:13,300 --> 01:13:18,300
So I'd say for that commercial, but not commercialized.

1621
01:13:18,580 --> 01:13:21,460
Yes, right, right, right, yeah, yeah, yeah.

1622
01:13:21,460 --> 01:13:23,220
Or soon to be commercialized, I'm sure,

1623
01:13:23,220 --> 01:13:24,860
but maybe not by Google.

1624
01:13:24,860 --> 01:13:27,020
Yeah, awesome.

1625
01:13:27,020 --> 01:13:31,900
Next up, kind of the intersection between search and LLMs.

1626
01:13:31,900 --> 01:13:33,060
What are you seeing there?

1627
01:13:33,060 --> 01:13:36,260
Yeah, so I think that's been kind of an interesting,

1628
01:13:36,260 --> 01:13:37,740
it's been a commercial development,

1629
01:13:37,740 --> 01:13:40,180
again, questionable to some degree,

1630
01:13:40,180 --> 01:13:42,260
but because I don't think the research is,

1631
01:13:42,260 --> 01:13:44,020
and these models are quite up to snuff,

1632
01:13:44,020 --> 01:13:48,580
but this somewhat coincided with Chad GPT.

1633
01:13:49,700 --> 01:13:50,540
I think-

1634
01:13:50,540 --> 01:13:52,460
Well, Chad GPT certainly raised a ton of questions

1635
01:13:52,460 --> 01:13:54,420
about, hey, is this a Google killer?

1636
01:13:54,420 --> 01:13:56,060
Right, right, right, right.

1637
01:13:56,060 --> 01:13:57,300
Yes, exactly.

1638
01:13:57,300 --> 01:13:58,420
And along the same time,

1639
01:13:58,420 --> 01:14:02,140
there were at least three search engines that I know of.

1640
01:14:02,140 --> 01:14:05,940
There was perplexity.ai that I don't think existed before

1641
01:14:07,100 --> 01:14:09,980
what the product they came up with,

1642
01:14:09,980 --> 01:14:12,740
which is a search engine which sort of gathers

1643
01:14:12,740 --> 01:14:15,540
all of the results from a typical search engine,

1644
01:14:15,540 --> 01:14:18,740
but then uses GPT-3-like models

1645
01:14:18,740 --> 01:14:21,660
to summarize the content of those links

1646
01:14:21,660 --> 01:14:24,660
and produces a paragraph that actually answers your query.

1647
01:14:25,580 --> 01:14:27,380
You.com is, again, a search engine

1648
01:14:27,380 --> 01:14:28,580
that has been around for a while,

1649
01:14:28,580 --> 01:14:32,380
but they brought this whole chat aspect to their search

1650
01:14:32,380 --> 01:14:34,940
where you're sort of chatting

1651
01:14:34,940 --> 01:14:37,020
and trying to come up with an answer.

1652
01:14:37,020 --> 01:14:39,260
And again, it's sort of not just showing you

1653
01:14:39,260 --> 01:14:43,780
a bunch of links, but composing information into text.

1654
01:14:43,780 --> 01:14:45,580
That's Richard Socher's company,

1655
01:14:45,580 --> 01:14:48,940
and we'll drop a link to my interview with him

1656
01:14:48,940 --> 01:14:50,420
in the show notes as well.

1657
01:14:50,420 --> 01:14:51,820
Okay, cool, yeah, yeah, yeah.

1658
01:14:51,820 --> 01:14:55,180
And Neva is another, you know,

1659
01:14:55,180 --> 01:14:58,060
it's a private search company.

1660
01:14:58,060 --> 01:15:00,740
It's a startup that also has an AI agent

1661
01:15:00,740 --> 01:15:02,940
that you can talk to it and things like that, right?

1662
01:15:02,940 --> 01:15:04,820
So I haven't played around with all of them.

1663
01:15:04,820 --> 01:15:07,260
I've played around with them a little bit.

1664
01:15:07,260 --> 01:15:11,180
And again, it's very easy to find problems

1665
01:15:11,180 --> 01:15:13,580
and sort of realize that, okay,

1666
01:15:13,580 --> 01:15:15,860
these language models are, you know,

1667
01:15:15,860 --> 01:15:17,060
this interface is great,

1668
01:15:17,060 --> 01:15:19,420
and it would be great to get the right paragraph

1669
01:15:19,420 --> 01:15:21,180
if it could get there.

1670
01:15:21,180 --> 01:15:23,420
But oftentimes they don't quite work

1671
01:15:23,420 --> 01:15:25,620
because of sort of fundamental issues with language models.

1672
01:15:25,620 --> 01:15:28,220
But I think from a commercial development,

1673
01:15:28,220 --> 01:15:30,700
I'm pretty excited about what search would look like

1674
01:15:30,700 --> 01:15:33,100
in the future and where language models

1675
01:15:33,100 --> 01:15:36,580
would fit into that whole product.

1676
01:15:36,580 --> 01:15:41,580
Yeah, one of my thought experiments with this

1677
01:15:41,780 --> 01:15:44,260
in the context of ChatGPT,

1678
01:15:44,260 --> 01:15:45,780
not that it was particularly deep,

1679
01:15:45,780 --> 01:15:50,780
but like there was this early meme,

1680
01:15:50,780 --> 01:15:52,460
you know, along the lines of,

1681
01:15:52,460 --> 01:15:55,620
hey, Google search is crap now, it's all ads.

1682
01:15:57,020 --> 01:16:00,420
ChatGPT, you know, I love this interface,

1683
01:16:00,420 --> 01:16:02,060
you know, it's gonna kill Google.

1684
01:16:02,060 --> 01:16:07,060
And so I asked ChatGPT to basically build a response

1685
01:16:07,180 --> 01:16:08,740
with an ad in it.

1686
01:16:08,740 --> 01:16:10,940
It works, it can do it.

1687
01:16:10,940 --> 01:16:13,060
I wouldn't be so sure that your, you know,

1688
01:16:13,060 --> 01:16:15,820
LLM-based search won't have any ads.

1689
01:16:15,820 --> 01:16:17,620
Right, right, right, yeah.

1690
01:16:17,620 --> 01:16:18,820
Yeah, no, I think, yeah.

1691
01:16:19,900 --> 01:16:21,820
Where the ads would come in

1692
01:16:21,820 --> 01:16:23,660
and how subtle the ads will be

1693
01:16:23,660 --> 01:16:25,260
once you throw in a language model into it.

1694
01:16:25,260 --> 01:16:28,260
Yeah, that's kind of interesting to think about.

1695
01:16:28,260 --> 01:16:31,460
And I guess next up on your list of commercial developments

1696
01:16:31,460 --> 01:16:35,340
is what I might call the LLM-ing of all the things.

1697
01:16:35,340 --> 01:16:36,660
Yeah, so I think, you know,

1698
01:16:36,660 --> 01:16:40,860
it's been two years or so since GPT-3 came out

1699
01:16:41,820 --> 01:16:45,340
and it's the question of like, okay,

1700
01:16:45,340 --> 01:16:48,220
where is the world changing products

1701
01:16:48,220 --> 01:16:49,900
that are using GPT-3?

1702
01:16:49,900 --> 01:16:52,620
When it came out, hey, it was gonna change everything.

1703
01:16:52,620 --> 01:16:54,100
Has it changed everything?

1704
01:16:54,100 --> 01:16:57,620
And I would say like for the most part, no.

1705
01:16:57,620 --> 01:17:01,180
The products that did seem to show some promise

1706
01:17:01,180 --> 01:17:03,820
and some of these are ones that will appear in the future

1707
01:17:03,820 --> 01:17:06,380
but have been kind of semi-announced

1708
01:17:06,380 --> 01:17:08,460
is the notion of writing assistantships, right?

1709
01:17:08,460 --> 01:17:12,620
So I think Notion AI is the one I think about

1710
01:17:12,620 --> 01:17:15,180
where a lot of people, it's a mainstream product,

1711
01:17:15,180 --> 01:17:16,940
anybody can use Notion

1712
01:17:16,940 --> 01:17:19,940
and Notion has this GPT-3 thing built in

1713
01:17:19,940 --> 01:17:23,140
where it can write to-do lists for you

1714
01:17:23,140 --> 01:17:23,980
and things like that.

1715
01:17:23,980 --> 01:17:28,980
So I think that is a pretty strong first version of GPT-3

1716
01:17:28,980 --> 01:17:32,220
as a commercial product that anybody can use

1717
01:17:32,220 --> 01:17:34,060
that I'm quite excited about.

1718
01:17:34,060 --> 01:17:38,100
I feel like the timing there is very ChatGPT influence.

1719
01:17:38,100 --> 01:17:39,940
Obviously they've been working on it.

1720
01:17:39,940 --> 01:17:43,460
You know, they saw it when GPT-3 came out

1721
01:17:43,460 --> 01:17:47,500
but I think they made it available right after ChatGPT

1722
01:17:47,500 --> 01:17:52,220
and a lot of these, you know,

1723
01:17:52,220 --> 01:17:54,580
Jasper's been around for a while

1724
01:17:54,580 --> 01:17:59,180
but there's a lot of new kind of writing assistant

1725
01:18:00,260 --> 01:18:01,100
types of things.

1726
01:18:01,100 --> 01:18:03,500
And it just does seem like there's a step function increase

1727
01:18:03,500 --> 01:18:07,500
in kind of energy in the space of using LLMs

1728
01:18:09,180 --> 01:18:13,140
since ChatGPT, even though they're all based on GPT-3

1729
01:18:13,140 --> 01:18:15,660
which has been around for two years, right?

1730
01:18:15,660 --> 01:18:17,100
Yeah, so I don't know exactly

1731
01:18:17,100 --> 01:18:20,580
why that thing seemed to align well, right?

1732
01:18:20,580 --> 01:18:22,660
So it's like, yeah, GPT-3 was announced

1733
01:18:22,660 --> 01:18:24,860
but it was a while before the API was rolled out

1734
01:18:24,860 --> 01:18:27,620
to everybody and, you know, and maybe after that

1735
01:18:27,620 --> 01:18:29,420
it takes a while to make the business case

1736
01:18:29,420 --> 01:18:30,260
for these things.

1737
01:18:30,260 --> 01:18:33,380
So yeah, maybe it's just timing of why it worked

1738
01:18:33,380 --> 01:18:36,140
or there were people like already kind of working on it

1739
01:18:36,140 --> 01:18:38,340
in a sort of on the side and they were like,

1740
01:18:38,340 --> 01:18:41,620
hey, no, we gotta sort of ride this wave

1741
01:18:41,620 --> 01:18:43,180
and sort of introduce things, right?

1742
01:18:43,180 --> 01:18:46,060
So I don't know exactly what that looks like,

1743
01:18:46,060 --> 01:18:48,380
but yeah, no, I think the fact that it aligns

1744
01:18:48,380 --> 01:18:51,140
also gets a lot more excitement and people know,

1745
01:18:51,140 --> 01:18:54,380
oh, okay, ChatGPT is something I've played around with.

1746
01:18:54,380 --> 01:18:58,100
This is now ChatGPT that's working on something that I do

1747
01:18:58,100 --> 01:18:59,740
and there is a lot of value in that.

1748
01:18:59,740 --> 01:19:04,740
Am I detecting an underlying pessimism maybe

1749
01:19:05,980 --> 01:19:10,900
about like, you know, kind of, you know,

1750
01:19:10,900 --> 01:19:13,020
where's the flying car that I was promised?

1751
01:19:13,020 --> 01:19:15,180
All I have is this GPT-3 thing.

1752
01:19:15,180 --> 01:19:16,660
Well, it's not so much the pessimism

1753
01:19:16,660 --> 01:19:20,140
because when I saw GPT-3, it became evident to me

1754
01:19:20,140 --> 01:19:22,180
that this is a great language model,

1755
01:19:22,180 --> 01:19:26,060
but it's not clear as it is how it can be made

1756
01:19:26,060 --> 01:19:27,980
into a product, right?

1757
01:19:27,980 --> 01:19:29,860
But it still came with a lot of hype

1758
01:19:29,860 --> 01:19:31,860
and yeah, it can generate a bunch of things,

1759
01:19:31,860 --> 01:19:34,940
but we quite haven't quite seen

1760
01:19:34,940 --> 01:19:36,860
what the product version of those look like.

1761
01:19:36,860 --> 01:19:40,260
I think the language models are extremely powerful,

1762
01:19:40,260 --> 01:19:41,940
not just as language models,

1763
01:19:41,940 --> 01:19:45,140
but they can be converted into products.

1764
01:19:45,140 --> 01:19:47,900
I don't quite feel like we are at a stage

1765
01:19:47,900 --> 01:19:49,980
where it's just going to be through prompting

1766
01:19:49,980 --> 01:19:53,020
and, you know, let's just tweak it a little bit.

1767
01:19:53,020 --> 01:19:54,620
I think there are a bunch of products

1768
01:19:54,620 --> 01:19:57,100
that'll come out of just by doing that,

1769
01:19:57,100 --> 01:19:58,700
but there's a whole slew of product

1770
01:19:58,700 --> 01:20:01,900
where the language models need to know a lot more

1771
01:20:01,900 --> 01:20:04,820
about the context where they're going to be

1772
01:20:05,700 --> 01:20:09,940
to be able to be effective, yeah, effective tools.

1773
01:20:10,900 --> 01:20:12,580
And you mentioned Microsoft.

1774
01:20:12,580 --> 01:20:14,100
What did you have in mind there?

1775
01:20:14,100 --> 01:20:16,540
Yeah, this was sort of a news that came out recently

1776
01:20:16,540 --> 01:20:20,300
where they're trying to have a bigger stake in open AI,

1777
01:20:20,300 --> 01:20:22,540
but also just generally thinking

1778
01:20:22,540 --> 01:20:26,620
of having open AI-like tools available in Word,

1779
01:20:26,620 --> 01:20:29,740
available in PowerPoint and all of these things.

1780
01:20:29,740 --> 01:20:30,580
They don't have it yet,

1781
01:20:30,580 --> 01:20:31,900
but I think those kinds of things

1782
01:20:31,900 --> 01:20:33,980
are just sort of coming as well.

1783
01:20:33,980 --> 01:20:38,100
Do you think a chat GPT-based Bing is a Google killer?

1784
01:20:38,100 --> 01:20:41,340
Oh, I don't think with that branding,

1785
01:20:41,340 --> 01:20:44,300
they would have to call it something else or yeah.

1786
01:20:44,300 --> 01:20:46,660
At this point, yeah.

1787
01:20:46,660 --> 01:20:49,300
I mean, that seemed to be the suggestion, right?

1788
01:20:49,300 --> 01:20:54,300
Chat GPT comes out, they're gonna take a big stake.

1789
01:20:54,500 --> 01:20:58,780
And it was mentioned, if not in the official announcement,

1790
01:20:58,780 --> 01:21:00,060
it seemed to be the conjecture

1791
01:21:00,060 --> 01:21:03,980
that it was gonna be some tie up with Bing,

1792
01:21:05,300 --> 01:21:08,300
explicitly to target search, right?

1793
01:21:08,300 --> 01:21:11,620
I think there needs to be a lot more fundamental work

1794
01:21:11,620 --> 01:21:13,460
and we can talk about this in the future predictions,

1795
01:21:13,460 --> 01:21:16,620
but there needs to be a lot more fundamental work

1796
01:21:16,620 --> 01:21:20,060
before we sort of are able to kill search

1797
01:21:20,060 --> 01:21:21,700
just by putting a language model, right?

1798
01:21:21,700 --> 01:21:23,860
Like I think that gap is not as simple

1799
01:21:23,860 --> 01:21:28,020
as replacing something or just augmenting existing search.

1800
01:21:28,020 --> 01:21:29,940
I think you would have to think about

1801
01:21:29,940 --> 01:21:32,980
what kind of things can language models actually do

1802
01:21:32,980 --> 01:21:36,500
and you still want to rely on sources and things like that.

1803
01:21:36,500 --> 01:21:40,860
But yeah, so I think it's going to happen at some point,

1804
01:21:40,860 --> 01:21:44,220
but it's going to be like search as,

1805
01:21:44,220 --> 01:21:45,420
it won't be replacing search

1806
01:21:45,420 --> 01:21:47,140
because it'll be a different thing, right?

1807
01:21:47,140 --> 01:21:49,540
Like it'll be, it's not gonna be search because-

1808
01:21:49,540 --> 01:21:50,980
Not in the way we think about search right now.

1809
01:21:50,980 --> 01:21:53,540
Search literally means, yeah, exactly.

1810
01:21:53,540 --> 01:21:54,860
It'll be question answering

1811
01:21:54,860 --> 01:21:56,220
or it'll be something else, right?

1812
01:21:56,220 --> 01:21:58,900
Like it'll be a helper or whatever,

1813
01:21:58,900 --> 01:22:01,580
but search is maybe not the right thing to do.

1814
01:22:01,580 --> 01:22:04,820
Well, one quick thing before we jump into predictions,

1815
01:22:04,820 --> 01:22:09,540
you kind of reflected on your top use case for the year

1816
01:22:09,540 --> 01:22:12,900
and that was CodePilot.

1817
01:22:12,900 --> 01:22:14,500
Tell me a little bit more about

1818
01:22:14,500 --> 01:22:15,860
how you're thinking about that.

1819
01:22:15,860 --> 01:22:17,300
I think CodePilot came out

1820
01:22:17,300 --> 01:22:19,460
probably not exactly in this calendar year,

1821
01:22:19,460 --> 01:22:23,380
but I feel like it got a lot more adoption this year

1822
01:22:23,380 --> 01:22:25,980
and started becoming part of the tools

1823
01:22:25,980 --> 01:22:27,540
where people are coding.

1824
01:22:27,540 --> 01:22:30,100
And personally, I started using CodePilot this year,

1825
01:22:30,100 --> 01:22:33,260
so I'm gonna put it in a top use case this year.

1826
01:22:33,260 --> 01:22:36,540
And I will say before Notion.ai,

1827
01:22:36,540 --> 01:22:40,980
CodePilot was probably the only use of large language models

1828
01:22:40,980 --> 01:22:43,140
that I saw anywhere.

1829
01:22:43,140 --> 01:22:45,860
So from that point of view, it was interesting

1830
01:22:45,860 --> 01:22:47,180
that GPT-3 came out

1831
01:22:47,180 --> 01:22:50,620
and then nothing, nothing that can build CodePilot.

1832
01:22:50,620 --> 01:22:52,460
But from a use case point of view,

1833
01:22:52,460 --> 01:22:54,700
it has been incredibly useful, right?

1834
01:22:54,700 --> 01:22:58,740
So I've been able to do things.

1835
01:22:58,740 --> 01:23:02,180
It has made me a lot more effective as a coder,

1836
01:23:02,180 --> 01:23:03,020
not that I code much,

1837
01:23:03,020 --> 01:23:05,180
but when I do, I want to do a lot

1838
01:23:05,180 --> 01:23:08,500
and CodePilot has let me sort of do that.

1839
01:23:08,500 --> 01:23:10,620
And that's been amazing.

1840
01:23:10,620 --> 01:23:13,220
I feel the right combination of,

1841
01:23:13,220 --> 01:23:15,940
hey, having a nice user interface,

1842
01:23:15,940 --> 01:23:19,460
having the right data that is trained on

1843
01:23:19,460 --> 01:23:22,260
to be able to sort of really help people

1844
01:23:22,260 --> 01:23:23,460
in what they want to do.

1845
01:23:23,460 --> 01:23:25,380
Now, of course, CodePilot has issues.

1846
01:23:25,380 --> 01:23:30,380
It's producing code that can be dangerous,

1847
01:23:30,860 --> 01:23:32,340
that can be buggy.

1848
01:23:32,340 --> 01:23:34,860
And of course, there are the questions of copyright

1849
01:23:34,860 --> 01:23:37,420
and plagiarism, exactly.

1850
01:23:37,420 --> 01:23:41,420
So I feel like, I hope those things will get resolved,

1851
01:23:41,420 --> 01:23:42,780
but those are, again,

1852
01:23:42,780 --> 01:23:44,500
when you start using a language model,

1853
01:23:44,500 --> 01:23:47,940
these are the issues that you have to solve.

1854
01:23:47,940 --> 01:23:50,060
And then I'm glad that CodePilot is bringing

1855
01:23:50,060 --> 01:23:51,980
all of these things into the discussion

1856
01:23:51,980 --> 01:23:54,380
by it being out there.

1857
01:23:55,340 --> 01:23:57,140
Yeah, I've had the same experience with it.

1858
01:23:57,140 --> 01:24:00,620
I think I've shared this on social

1859
01:24:00,620 --> 01:24:04,540
or in the podcast in a conversation.

1860
01:24:04,540 --> 01:24:08,580
I saw all the CodePilot demos,

1861
01:24:08,580 --> 01:24:11,660
played around with it with kind of the toy problem things,

1862
01:24:11,660 --> 01:24:14,780
but I don't do a lot of coding necessarily,

1863
01:24:14,780 --> 01:24:17,460
but I do tend to binge on coding every once in a while.

1864
01:24:17,460 --> 01:24:19,940
Like, and usually like that end of year holiday thing,

1865
01:24:19,940 --> 01:24:21,700
I'll have some project.

1866
01:24:21,700 --> 01:24:24,700
And I did that this year and use CodePilot.

1867
01:24:24,700 --> 01:24:25,620
It was amazing.

1868
01:24:25,620 --> 01:24:27,860
Like the productivity you can,

1869
01:24:27,860 --> 01:24:31,620
it helps, the productivity helps create for you

1870
01:24:31,620 --> 01:24:36,620
attacking a new problem with new tools

1871
01:24:37,260 --> 01:24:40,100
without the context switching of going to Google

1872
01:24:40,100 --> 01:24:40,980
and Stack Overflow.

1873
01:24:40,980 --> 01:24:42,700
Like, it's incredible.

1874
01:24:42,700 --> 01:24:44,620
I'm a total believer.

1875
01:24:44,620 --> 01:24:47,900
Yeah, and I think that exactly is the kind of thing

1876
01:24:47,900 --> 01:24:50,540
I expect language models to be useful for.

1877
01:24:50,540 --> 01:24:51,620
They are not going to,

1878
01:24:51,620 --> 01:24:53,620
and with chat GPT going back a little bit,

1879
01:24:53,620 --> 01:24:54,460
people are talking about,

1880
01:24:54,460 --> 01:24:56,100
hey, people are gonna lose jobs

1881
01:24:56,100 --> 01:24:57,900
and it's gonna change everything.

1882
01:24:57,900 --> 01:25:01,180
And we'll replace X, Y, Z with chat GPT.

1883
01:25:01,180 --> 01:25:04,780
And I don't quite see that happening,

1884
01:25:04,780 --> 01:25:07,740
but I do expect a lot of people in many different areas

1885
01:25:07,740 --> 01:25:11,460
becoming a lot more productive because of chat GPT.

1886
01:25:11,460 --> 01:25:14,980
And CodePilot is an example of how language models

1887
01:25:14,980 --> 01:25:19,140
can make you a lot more productive without replacing,

1888
01:25:19,140 --> 01:25:21,300
I don't think it's replacing specific programmers.

1889
01:25:21,300 --> 01:25:24,660
It's just making, allowing them to do a lot more.

1890
01:25:24,660 --> 01:25:27,420
And that I think is the best use of technology.

1891
01:25:27,420 --> 01:25:28,260
Awesome, awesome.

1892
01:25:28,260 --> 01:25:31,540
Well, let's jump into predictions.

1893
01:25:31,540 --> 01:25:34,340
What are you most excited about

1894
01:25:34,340 --> 01:25:37,100
kind of looking into your crystal ball?

1895
01:25:37,100 --> 01:25:40,860
So I think the chat GPT is the one that sort of,

1896
01:25:40,860 --> 01:25:42,860
everybody knew language models,

1897
01:25:42,860 --> 01:25:45,540
they're just trained on data and making predictions.

1898
01:25:45,540 --> 01:25:49,700
What chat GPT really did was remind everyone like,

1899
01:25:49,700 --> 01:25:52,060
okay, even if the language modeling part

1900
01:25:52,060 --> 01:25:54,100
is quote unquote solved, right?

1901
01:25:54,100 --> 01:25:56,940
Even if you get a really, really large language model,

1902
01:25:56,940 --> 01:26:00,340
that doesn't mean you're done, right?

1903
01:26:00,340 --> 01:26:02,860
And I think one of the biggest aspects of that

1904
01:26:02,860 --> 01:26:07,860
was making sure that what you're generating is not just BS,

1905
01:26:08,780 --> 01:26:12,380
it's somehow valid, somehow the truth,

1906
01:26:12,380 --> 01:26:16,420
somehow something that you can cite and rely on, right?

1907
01:26:16,420 --> 01:26:20,060
They definitely shine a light on how challenging that is.

1908
01:26:20,060 --> 01:26:21,740
Right, exactly, yeah.

1909
01:26:21,740 --> 01:26:23,780
So I don't think this is gonna be a prediction

1910
01:26:23,780 --> 01:26:25,420
necessarily for 2023,

1911
01:26:25,420 --> 01:26:27,860
maybe 2023 is when we'll start seeing

1912
01:26:27,860 --> 01:26:29,660
the first attempts at this,

1913
01:26:29,660 --> 01:26:32,260
but being able to generate text

1914
01:26:32,260 --> 01:26:35,060
that does not have misinformation,

1915
01:26:35,060 --> 01:26:40,060
that differentiates factual from creative hallucinations,

1916
01:26:40,500 --> 01:26:43,220
that is able to cite its sources

1917
01:26:43,220 --> 01:26:44,620
and sort of point to like,

1918
01:26:44,620 --> 01:26:46,940
look, this is the piece of paragraph

1919
01:26:46,940 --> 01:26:50,260
that I'm based on which I'm generating a piece of text.

1920
01:26:50,260 --> 01:26:52,980
I think those things are needed

1921
01:26:52,980 --> 01:26:54,820
and it's probably going to be

1922
01:26:54,820 --> 01:26:58,380
the next aspect of language models.

1923
01:26:58,380 --> 01:27:01,660
That's gonna be a big topic of research.

1924
01:27:01,660 --> 01:27:05,260
Do you have a sense for where, how we get there?

1925
01:27:05,260 --> 01:27:09,260
Is it kind of applying the same tools,

1926
01:27:09,260 --> 01:27:13,620
RLHF for example, attacking this specific problem

1927
01:27:13,620 --> 01:27:16,260
or do you think there's, you know,

1928
01:27:16,260 --> 01:27:18,460
we don't have the tools and it's gonna need to be

1929
01:27:18,460 --> 01:27:22,340
kind of new invention that gets us there?

1930
01:27:22,340 --> 01:27:24,940
I think it's going to have to be new inventions

1931
01:27:24,940 --> 01:27:27,580
and I want to sort of think of it as not just,

1932
01:27:27,580 --> 01:27:29,660
you know, how do we attribute it

1933
01:27:29,660 --> 01:27:31,420
to specific pieces of text,

1934
01:27:31,420 --> 01:27:33,580
but I kind of think of it as like

1935
01:27:33,580 --> 01:27:35,660
being able to use other tools,

1936
01:27:35,660 --> 01:27:38,980
being able to use other things available

1937
01:27:38,980 --> 01:27:40,860
to the language model

1938
01:27:40,860 --> 01:27:42,540
when it's being trained as well, right?

1939
01:27:42,540 --> 01:27:47,340
So it should not rely on memorizing facts to any degree.

1940
01:27:47,340 --> 01:27:51,220
It should just rely on using existing tools,

1941
01:27:51,220 --> 01:27:54,500
including search, including maybe calculations,

1942
01:27:54,500 --> 01:27:56,700
maybe even a Python interpreter,

1943
01:27:56,700 --> 01:27:59,180
whatever else it needs to do,

1944
01:27:59,180 --> 01:28:02,180
but still be able to do the language modeling task, right?

1945
01:28:02,180 --> 01:28:04,220
So I think there is some combination

1946
01:28:04,220 --> 01:28:06,820
of being able to refer to external stuff

1947
01:28:06,820 --> 01:28:08,340
and still do language modeling

1948
01:28:08,340 --> 01:28:10,860
that we haven't quite tracked it

1949
01:28:11,780 --> 01:28:14,620
and that would be something that I think

1950
01:28:14,620 --> 01:28:15,980
will come into picture.

1951
01:28:15,980 --> 01:28:17,660
I'll give you an example of how

1952
01:28:19,140 --> 01:28:20,860
sort of some people have been thinking about it.

1953
01:28:20,860 --> 01:28:21,820
There's this whole idea

1954
01:28:21,820 --> 01:28:24,820
of retrieval-based language modeling

1955
01:28:24,820 --> 01:28:28,380
where you're still generating the text token by token,

1956
01:28:28,380 --> 01:28:31,620
but you're always retrieving some set of documents

1957
01:28:31,620 --> 01:28:33,020
and you're conditioning on them

1958
01:28:33,020 --> 01:28:35,380
when you're generating each token.

1959
01:28:35,380 --> 01:28:39,940
That's sort of one step towards what I'm talking about,

1960
01:28:39,940 --> 01:28:41,700
where at least you're trying to look

1961
01:28:41,700 --> 01:28:44,460
at retrieved documents when you're generating,

1962
01:28:45,380 --> 01:28:47,140
but that doesn't guarantee what you're generating

1963
01:28:47,140 --> 01:28:48,620
is actually based on.

1964
01:28:48,620 --> 01:28:53,620
So you just spoke earlier about the decomposed reasoning.

1965
01:28:55,100 --> 01:29:00,020
Is this prediction that those ideas

1966
01:29:00,020 --> 01:29:03,620
become more real in some way in 23-24

1967
01:29:03,620 --> 01:29:08,620
or is it that what we're doing with a trained model

1968
01:29:10,300 --> 01:29:13,180
to kind of get decomposed reasoning,

1969
01:29:13,180 --> 01:29:14,580
we're gonna push even deeper

1970
01:29:14,580 --> 01:29:17,260
into the fundamental creation of the model

1971
01:29:17,260 --> 01:29:19,540
like at train time and other things?

1972
01:29:19,540 --> 01:29:21,820
Yeah, so more of the latter, right?

1973
01:29:21,820 --> 01:29:23,740
So right now we are expecting the model

1974
01:29:23,740 --> 01:29:25,660
to be able to do decomposed reasoning,

1975
01:29:25,660 --> 01:29:28,780
but we only do it at test time in some sense, right?

1976
01:29:28,780 --> 01:29:31,100
Let's actually try to start thinking,

1977
01:29:31,100 --> 01:29:33,060
putting that stuff during training, right?

1978
01:29:33,060 --> 01:29:36,180
So like, again, I don't want to make this analogy too much,

1979
01:29:36,180 --> 01:29:39,940
but when you think about when you're training a human

1980
01:29:39,940 --> 01:29:41,140
on how to do things,

1981
01:29:41,140 --> 01:29:44,540
you don't just give it pairs of input and output.

1982
01:29:44,540 --> 01:29:47,620
You give it a little bit more of a decomposition

1983
01:29:47,620 --> 01:29:50,780
and then based on that, they're able to do what they do.

1984
01:29:50,780 --> 01:29:53,380
If you want them to use the Python interpreter,

1985
01:29:53,380 --> 01:29:56,740
you don't just expect them to finish it on their own.

1986
01:29:56,740 --> 01:29:58,820
They can use the interpreter when needed kind of thing,

1987
01:29:58,820 --> 01:29:59,660
right?

1988
01:29:59,660 --> 01:30:01,660
So I just think of language models as,

1989
01:30:01,660 --> 01:30:03,980
yeah, maybe they're still doing the language modeling tasks,

1990
01:30:03,980 --> 01:30:06,580
but they have access to a bunch of other tools.

1991
01:30:07,500 --> 01:30:09,860
And maybe this is more far-fetched than 2023,

1992
01:30:09,860 --> 01:30:11,780
but I think in the long run,

1993
01:30:11,780 --> 01:30:15,060
you want a system that's able to do those things.

1994
01:30:15,060 --> 01:30:15,900
You got it.

1995
01:30:15,900 --> 01:30:17,580
Your next prediction is around diffusion models.

1996
01:30:17,580 --> 01:30:18,420
It's kind of surprising

1997
01:30:18,420 --> 01:30:20,580
that that term hasn't come up yet so far.

1998
01:30:20,580 --> 01:30:23,020
Yeah, I guess it is surprising,

1999
01:30:23,020 --> 01:30:24,580
but also in NLP in general,

2000
01:30:24,580 --> 01:30:27,100
I feel like we are barely scratching the surface

2001
01:30:27,100 --> 01:30:30,540
of what diffusion models can do.

2002
01:30:30,540 --> 01:30:34,020
So yeah, I think clearly in the image generation space,

2003
01:30:34,020 --> 01:30:37,140
we've seen a lot of progress with diffusion models

2004
01:30:37,140 --> 01:30:40,820
and we've seen some in NLP, but not enough.

2005
01:30:40,820 --> 01:30:44,180
I guess what I find attractive about diffusion model

2006
01:30:44,180 --> 01:30:46,940
is that it's trying to generate more

2007
01:30:46,940 --> 01:30:49,380
than just a single thing at one point, right?

2008
01:30:49,380 --> 01:30:52,500
So when diffusion models are applied to text,

2009
01:30:52,500 --> 01:30:53,620
the way it would look like

2010
01:30:53,620 --> 01:30:56,460
is not just producing one token at a time.

2011
01:30:56,460 --> 01:30:58,980
It will try to produce a whole sentence

2012
01:30:58,980 --> 01:31:02,340
or whatever we decide is the right granularity.

2013
01:31:02,340 --> 01:31:05,900
And that idea of a model that is trained

2014
01:31:05,900 --> 01:31:08,380
not to do one token at a time,

2015
01:31:08,380 --> 01:31:11,820
but to do something bigger really appeals to me

2016
01:31:11,820 --> 01:31:13,740
because I feel like a lot of the issues

2017
01:31:13,740 --> 01:31:16,180
we talk about with language models

2018
01:31:16,180 --> 01:31:17,700
fundamentally come from the fact

2019
01:31:17,700 --> 01:31:20,820
that it's trained to do one token at a time

2020
01:31:20,820 --> 01:31:24,340
and sort of, and that's kind of the loss, right?

2021
01:31:24,340 --> 01:31:29,340
So if we can have the model be trained to generate more

2022
01:31:29,380 --> 01:31:31,140
and then give it a loss,

2023
01:31:31,140 --> 01:31:33,100
I think that's fundamentally interesting

2024
01:31:33,100 --> 01:31:35,780
and diffusion models sort of provide one way of doing that.

2025
01:31:35,780 --> 01:31:40,780
Do you, would you kind of visualize this as a model

2026
01:31:42,660 --> 01:31:46,980
like in a first iteration spitting out bullshit

2027
01:31:46,980 --> 01:31:50,780
and then successively like iterating towards truth?

2028
01:31:50,780 --> 01:31:53,540
Like, is that one way that this could play out?

2029
01:31:53,540 --> 01:31:54,580
Yes.

2030
01:31:54,580 --> 01:31:57,900
Well, I mean, probably not.

2031
01:31:57,900 --> 01:32:01,260
Probably it's gonna be somewhere in the latent space,

2032
01:32:01,260 --> 01:32:05,820
but I think the way I think about it is like,

2033
01:32:05,820 --> 01:32:08,900
if we were doing this token by token thing for images,

2034
01:32:08,900 --> 01:32:10,500
it just wouldn't make sense, right?

2035
01:32:10,500 --> 01:32:12,300
Like predict-

2036
01:32:12,300 --> 01:32:14,060
Certainly wouldn't produce the images

2037
01:32:14,060 --> 01:32:17,980
that we see coming out of stable diffusion and yeah.

2038
01:32:17,980 --> 01:32:19,980
Or even what it's going to learn

2039
01:32:19,980 --> 01:32:21,380
is going to be something different.

2040
01:32:21,380 --> 01:32:24,020
What it's going to learn is given the image

2041
01:32:24,020 --> 01:32:25,500
that I've seen so far,

2042
01:32:25,500 --> 01:32:28,980
let me predict the next pixel or the next piece, right?

2043
01:32:28,980 --> 01:32:32,300
That somehow feels like a fundamentally different task

2044
01:32:32,300 --> 01:32:34,900
than being able to generate an image fully, right?

2045
01:32:35,900 --> 01:32:40,860
And so I feel like thinking about the same idea for text

2046
01:32:40,860 --> 01:32:41,820
just kind of makes sense.

2047
01:32:41,820 --> 01:32:44,780
Like you write the summary in one shot

2048
01:32:44,780 --> 01:32:47,980
and realize how wrong it is.

2049
01:32:47,980 --> 01:32:50,380
It feels like there's something fundamentally different

2050
01:32:50,380 --> 01:32:53,100
than, hey, you got a bunch of tokens correct,

2051
01:32:53,100 --> 01:32:54,340
but you also got a bunch.

2052
01:32:54,340 --> 01:32:57,780
And in some sense, there are some analogies to RLHF

2053
01:32:57,780 --> 01:33:00,580
and using PPO for training, for example,

2054
01:33:00,580 --> 01:33:04,140
where you try to make sure it's fluent

2055
01:33:04,140 --> 01:33:05,180
and things like that.

2056
01:33:05,180 --> 01:33:07,340
These are all losses designed on

2057
01:33:07,340 --> 01:33:09,100
not just a token by token basis,

2058
01:33:09,100 --> 01:33:10,780
but something that's longer.

2059
01:33:10,780 --> 01:33:13,420
And so we've known how useful they've been.

2060
01:33:13,420 --> 01:33:15,220
So I feel like there may be something

2061
01:33:15,220 --> 01:33:18,140
in taking that idea and applying it to pre-shading

2062
01:33:18,140 --> 01:33:19,780
is something that we'll use.

2063
01:33:19,780 --> 01:33:21,060
Interesting, interesting.

2064
01:33:21,060 --> 01:33:23,260
I expect a lot of people will be wanting

2065
01:33:23,260 --> 01:33:25,140
to figure out how to do that.

2066
01:33:25,140 --> 01:33:26,180
Great.

2067
01:33:26,180 --> 01:33:29,220
And online updates to models.

2068
01:33:29,220 --> 01:33:32,100
Yeah, so I think one of the problems with language models,

2069
01:33:32,100 --> 01:33:34,300
so let's keep aside the grand vision

2070
01:33:34,300 --> 01:33:36,860
of how language models will use search

2071
01:33:36,860 --> 01:33:38,140
and all of these other things.

2072
01:33:38,140 --> 01:33:40,340
But one of the fundamental problems with language models

2073
01:33:40,340 --> 01:33:43,620
is that the world changes, but they don't.

2074
01:33:44,500 --> 01:33:47,140
And this seems to be a fundamental sort of issue

2075
01:33:47,140 --> 01:33:49,500
with language models.

2076
01:33:49,500 --> 01:33:54,500
So I think thinking about how we can update language models

2077
01:33:54,500 --> 01:33:59,180
every month or every week or every day, right?

2078
01:33:59,180 --> 01:34:03,300
I think is an interesting problem to be thinking about

2079
01:34:03,300 --> 01:34:04,900
and becomes increasingly relevant

2080
01:34:04,900 --> 01:34:08,220
where BERT doesn't know anything about COVID.

2081
01:34:08,220 --> 01:34:10,780
So it's not useful for a bunch of applications,

2082
01:34:10,780 --> 01:34:12,980
even though otherwise fundamentally

2083
01:34:12,980 --> 01:34:15,180
there's nothing wrong with it, right?

2084
01:34:15,180 --> 01:34:19,660
That kind of stuff is just not fun.

2085
01:34:19,660 --> 01:34:21,100
And I think there'll be research

2086
01:34:21,100 --> 01:34:22,860
on trying to sort of fix that.

2087
01:34:22,860 --> 01:34:27,780
What's the current, not necessarily state of the art,

2088
01:34:27,780 --> 01:34:31,620
but kind of current approach for doing this

2089
01:34:31,620 --> 01:34:33,660
at the scale of a GPT-3?

2090
01:34:33,660 --> 01:34:36,460
Like, is it collect more data and retrain from scratch

2091
01:34:36,460 --> 01:34:41,460
or how did they approximate

2092
01:34:41,860 --> 01:34:45,460
or approach some kind of incremental training ability,

2093
01:34:45,460 --> 01:34:46,340
if at all?

2094
01:34:46,340 --> 01:34:51,340
Yeah, so there hasn't been that much work on that front.

2095
01:34:51,340 --> 01:34:54,100
I would say this is something that's, yeah,

2096
01:34:54,100 --> 01:34:56,340
needs a lot more attention.

2097
01:34:56,340 --> 01:35:00,220
Yeah, but I think there'd been parameter efficient training

2098
01:35:00,220 --> 01:35:05,220
on sort of how can we slightly improve the,

2099
01:35:07,060 --> 01:35:10,180
like change the model, but not completely change it.

2100
01:35:10,180 --> 01:35:12,300
Find the set of parameters that we should update

2101
01:35:12,300 --> 01:35:14,900
so that it's not updating the whole parameters,

2102
01:35:14,900 --> 01:35:16,540
but updating a little bit of it.

2103
01:35:16,540 --> 01:35:19,060
Things like that I feel are around,

2104
01:35:19,060 --> 01:35:20,980
but needs a lot more work.

2105
01:35:20,980 --> 01:35:23,900
One way to think about the fundamental problem

2106
01:35:23,900 --> 01:35:25,380
is with the transformer,

2107
01:35:25,380 --> 01:35:28,580
it's not like a layered architecture like a CNN

2108
01:35:28,580 --> 01:35:31,580
where you can just chop off the end layers

2109
01:35:31,580 --> 01:35:33,740
and retrain from that point.

2110
01:35:33,740 --> 01:35:37,300
It's just a much more complex and interconnected model.

2111
01:35:37,300 --> 01:35:42,060
So that kind of incremental updating doesn't work.

2112
01:35:42,060 --> 01:35:43,220
Not so easily, yeah.

2113
01:35:43,220 --> 01:35:46,780
I think there's been some work on sort of taking like,

2114
01:35:46,780 --> 01:35:49,260
hey, 1% of the parameters sort of spread

2115
01:35:49,260 --> 01:35:52,820
over the transformer and updating them with new text.

2116
01:35:52,820 --> 01:35:55,540
But I think, yeah, solving this problem

2117
01:35:55,540 --> 01:35:59,380
is going to be something that needs to happen pretty quick.

2118
01:36:00,540 --> 01:36:03,020
And so to be clear, taking a step back,

2119
01:36:03,020 --> 01:36:06,620
like this is all the looking forward section,

2120
01:36:06,620 --> 01:36:10,620
those three things, kind of misinformation

2121
01:36:10,620 --> 01:36:13,300
and attributable generations, diffusion models

2122
01:36:13,300 --> 01:36:16,180
and online updates were specifically in your category

2123
01:36:16,180 --> 01:36:21,180
of the greatest, most exciting opportunities in the field.

2124
01:36:21,500 --> 01:36:25,980
Areas where we're likely to see a lot of research attention

2125
01:36:27,460 --> 01:36:30,580
and possibly some really interesting results

2126
01:36:30,580 --> 01:36:32,340
coming up in the next year or two.

2127
01:36:32,340 --> 01:36:35,300
And also sort of fundamental problems

2128
01:36:35,300 --> 01:36:37,540
that need to be addressed by language models.

2129
01:36:39,180 --> 01:36:42,620
And so that brings us to your top three predictions

2130
01:36:42,620 --> 01:36:44,780
for the field proper.

2131
01:36:44,780 --> 01:36:46,940
What do you see there?

2132
01:36:46,940 --> 01:36:48,940
Yeah, so I think, and maybe some of it

2133
01:36:48,940 --> 01:36:51,780
is riddled with disappointment as well.

2134
01:36:51,780 --> 01:36:55,500
So the first one here is multiple modalities.

2135
01:36:55,500 --> 01:36:57,020
I think there's been a lot of exciting work,

2136
01:36:57,020 --> 01:36:59,180
so I don't want to sort of take that away.

2137
01:36:59,180 --> 01:37:02,380
But to me, after GPT-3 came out

2138
01:37:02,380 --> 01:37:05,660
and then you saw Clip and DALI and Whisper

2139
01:37:05,660 --> 01:37:09,660
and now there's video models and things like that.

2140
01:37:10,660 --> 01:37:14,460
To me, fundamentally, I understand technically

2141
01:37:14,460 --> 01:37:16,260
why they're not the same model,

2142
01:37:16,260 --> 01:37:17,900
but it's still a little bit disappointing

2143
01:37:17,900 --> 01:37:19,980
that they're not the same model.

2144
01:37:19,980 --> 01:37:22,660
Why is there not the same model

2145
01:37:22,660 --> 01:37:26,660
that trains over the same data GPT-3 is trained on,

2146
01:37:26,660 --> 01:37:31,060
but also on the Lion dataset that does all the images

2147
01:37:31,060 --> 01:37:35,340
and text and audio and video and stuff like that.

2148
01:37:35,340 --> 01:37:39,380
And I think this is a sort of near future prediction

2149
01:37:39,380 --> 01:37:42,820
is that we are going to see ways for pre-training models

2150
01:37:42,820 --> 01:37:45,300
that cuts across multiple modalities.

2151
01:37:45,300 --> 01:37:47,180
And I think Clip was a good example,

2152
01:37:47,180 --> 01:37:49,140
sort of early example of what you can do

2153
01:37:49,140 --> 01:37:51,060
when you have a lot of text and images.

2154
01:37:51,060 --> 01:37:53,980
But I think it still didn't have access

2155
01:37:53,980 --> 01:37:57,300
to a lot of text-only data.

2156
01:37:57,300 --> 01:38:00,820
And I want a model that can do chat GPT-like things,

2157
01:38:00,820 --> 01:38:04,100
but also generate images for me

2158
01:38:04,100 --> 01:38:06,820
and maybe read them out and things like that.

2159
01:38:06,820 --> 01:38:10,220
So I feel like multiple modalities is an exciting

2160
01:38:10,220 --> 01:38:13,380
sort of kind of an opportunity,

2161
01:38:13,380 --> 01:38:16,100
but definitely something that's going to happen soon.

2162
01:38:16,100 --> 01:38:16,940
Mm-hmm.

2163
01:38:16,940 --> 01:38:19,140
Yeah, when I first heard you describe this,

2164
01:38:19,140 --> 01:38:20,260
I thought, well, multimodal,

2165
01:38:20,260 --> 01:38:23,180
like that was the big thing we were talking about

2166
01:38:23,180 --> 01:38:25,940
in these trends conversations last year,

2167
01:38:25,940 --> 01:38:27,860
but you're going a level deeper.

2168
01:38:27,860 --> 01:38:31,940
You don't want multimodal use cases or outputs.

2169
01:38:31,940 --> 01:38:36,220
You want a single architecture to do multimodal things.

2170
01:38:36,220 --> 01:38:37,420
That's what I want.

2171
01:38:37,420 --> 01:38:40,500
My prediction is maybe going to be a little bit more

2172
01:38:40,500 --> 01:38:42,700
grounded, so to say.

2173
01:38:42,700 --> 01:38:46,060
But yeah, you know, like video, for example,

2174
01:38:46,060 --> 01:38:48,020
is a more concrete one, like text to video.

2175
01:38:48,020 --> 01:38:50,620
We've seen some initial versions of those.

2176
01:38:50,620 --> 01:38:54,020
That's probably where a lot of initial stuff would go in.

2177
01:38:54,020 --> 01:38:55,980
But when, and you know, I've been really excited

2178
01:38:55,980 --> 01:38:58,740
about sort of the mind dojo world

2179
01:38:58,740 --> 01:39:01,260
of like playing with text and Minecraft

2180
01:39:01,260 --> 01:39:02,540
and having an agent that can do

2181
01:39:02,540 --> 01:39:04,140
a bunch of things in Minecraft.

2182
01:39:04,140 --> 01:39:07,620
I feel like there are things that models can learn

2183
01:39:07,620 --> 01:39:11,260
from images, even for language modeling,

2184
01:39:11,260 --> 01:39:14,180
it would benefit to see a lot of images in some sense.

2185
01:39:14,180 --> 01:39:16,460
Like there are just a bunch of things in images

2186
01:39:16,460 --> 01:39:19,060
that we never talk about in text.

2187
01:39:19,060 --> 01:39:24,060
And so from an AI agent,

2188
01:39:24,580 --> 01:39:27,180
I think it's useful to think about something

2189
01:39:27,180 --> 01:39:28,300
that has access to everything.

2190
01:39:28,300 --> 01:39:29,820
But yeah, more concretely,

2191
01:39:29,820 --> 01:39:32,100
we're just going to be pushing them sort of pair-wise.

2192
01:39:32,100 --> 01:39:34,340
Yeah, it's going to be audio and images,

2193
01:39:34,340 --> 01:39:36,220
and there's going to be a bunch of other pairs

2194
01:39:36,220 --> 01:39:37,820
that will happen first.

2195
01:39:37,820 --> 01:39:40,900
But eventually I think having multiple,

2196
01:39:40,900 --> 01:39:42,100
actual multiple modalities,

2197
01:39:42,100 --> 01:39:46,180
not just greater than one modalities would be exciting.

2198
01:39:48,020 --> 01:39:48,860
Awesome, awesome.

2199
01:39:48,860 --> 01:39:49,900
Next up.

2200
01:39:49,900 --> 01:39:54,100
Next, I'm kind of excited about better training

2201
01:39:54,100 --> 01:39:56,980
and better inference and better in the sense

2202
01:39:56,980 --> 01:39:59,300
of being more computationally efficient.

2203
01:39:59,300 --> 01:40:02,460
I think this is an exciting work that,

2204
01:40:02,460 --> 01:40:04,020
a bunch of people are already doing,

2205
01:40:04,020 --> 01:40:05,860
but I think this is just going to become

2206
01:40:05,860 --> 01:40:09,980
increasingly important from a sustainability point of view,

2207
01:40:09,980 --> 01:40:13,060
but also from like universities surviving

2208
01:40:13,060 --> 01:40:14,380
and doing interesting things

2209
01:40:14,380 --> 01:40:17,700
and small companies contributing to research.

2210
01:40:17,700 --> 01:40:20,380
I think it's important to be able to train these models,

2211
01:40:20,380 --> 01:40:22,060
to be able to run these models,

2212
01:40:22,980 --> 01:40:25,140
and there's going to be a lot of research

2213
01:40:25,140 --> 01:40:27,100
in trying to do those kinds of things.

2214
01:40:27,100 --> 01:40:30,060
And you've got a few examples that we'll link to

2215
01:40:30,060 --> 01:40:30,900
in the show notes.

2216
01:40:30,900 --> 01:40:34,220
Anything that you want to point out?

2217
01:40:34,220 --> 01:40:38,020
Yeah, so let me mention two that I saw recently.

2218
01:40:38,020 --> 01:40:41,980
One of them is this paper called cramming.

2219
01:40:41,980 --> 01:40:44,540
And the idea here is to,

2220
01:40:44,540 --> 01:40:47,460
they think about the scaling laws paper,

2221
01:40:47,460 --> 01:40:49,980
like, hey, what can you do when the models get larger

2222
01:40:49,980 --> 01:40:51,220
and stuff like that?

2223
01:40:51,220 --> 01:40:54,140
The cramming paper sort of turns it on his head

2224
01:40:54,140 --> 01:40:58,740
and decides, okay, what if I have just one GPU for one day?

2225
01:40:58,740 --> 01:41:00,780
What's the most I can do with that?

2226
01:41:01,700 --> 01:41:03,780
And it's a very sort of different question,

2227
01:41:03,780 --> 01:41:07,500
but it somehow is a lot more relevant to many more people,

2228
01:41:07,500 --> 01:41:11,420
because a lot more people have a single GPU for a single day

2229
01:41:11,420 --> 01:41:13,340
and they show that you can get almost

2230
01:41:13,340 --> 01:41:15,180
sort of bird level performance

2231
01:41:15,180 --> 01:41:16,420
if you make the right choices

2232
01:41:16,420 --> 01:41:19,820
and they sort of detail what those choices might look like.

2233
01:41:19,820 --> 01:41:22,700
It's a paper, but I think I like that idea of like,

2234
01:41:22,700 --> 01:41:27,020
hey, what if we were scrappy about training these models?

2235
01:41:27,020 --> 01:41:28,020
How far can we get?

2236
01:41:28,020 --> 01:41:30,220
I think that's a very interesting question

2237
01:41:30,220 --> 01:41:34,420
that Google and OpenAI is not gonna be asking,

2238
01:41:34,420 --> 01:41:37,100
but might be relevant for a lot of research.

2239
01:41:37,100 --> 01:41:40,900
The other one I want to talk about is this Petals work

2240
01:41:40,900 --> 01:41:42,700
that came out of the big science thing.

2241
01:41:42,700 --> 01:41:44,220
I haven't read too much about this,

2242
01:41:44,220 --> 01:41:46,820
but it seems like a really interesting idea

2243
01:41:46,820 --> 01:41:50,900
of the problem of running really large language models.

2244
01:41:50,900 --> 01:41:54,860
So even if OPT releases 175 billion model,

2245
01:41:54,860 --> 01:41:56,420
how do you actually run it?

2246
01:41:56,420 --> 01:41:58,460
It doesn't really help most people,

2247
01:41:58,460 --> 01:42:00,180
even if you have a big cluster,

2248
01:42:00,180 --> 01:42:02,180
it's kind of difficult to run it.

2249
01:42:02,180 --> 01:42:06,660
So what this Petals does is they're building this framework

2250
01:42:06,660 --> 01:42:09,420
for using the ideas behind BitTorrent

2251
01:42:10,300 --> 01:42:12,780
of sort of distributed computing

2252
01:42:12,780 --> 01:42:14,380
and bringing it to language models.

2253
01:42:14,380 --> 01:42:15,980
So like, hey, you should be able to run

2254
01:42:15,980 --> 01:42:19,340
these 100 billion size language models,

2255
01:42:19,340 --> 01:42:24,060
language models distributed over a bunch of commodity

2256
01:42:24,060 --> 01:42:27,620
sort of consumer computers.

2257
01:42:27,620 --> 01:42:29,380
So yeah, I think this is an interesting idea.

2258
01:42:29,380 --> 01:42:30,460
I haven't played around with it

2259
01:42:30,460 --> 01:42:33,020
and see how far you can push it.

2260
01:42:33,020 --> 01:42:35,180
That's partly, you need a bunch of people

2261
01:42:35,180 --> 01:42:38,420
also running Petals, but once we get there,

2262
01:42:38,420 --> 01:42:40,380
I think that could be a pretty exciting way

2263
01:42:40,380 --> 01:42:42,060
to run language models.

2264
01:42:42,060 --> 01:42:44,180
Interesting, interesting.

2265
01:42:44,180 --> 01:42:48,860
So your third prediction is editing

2266
01:42:48,860 --> 01:42:50,460
and revising models.

2267
01:42:50,460 --> 01:42:51,660
What do you mean there?

2268
01:42:51,660 --> 01:42:55,020
So these are these family of models

2269
01:42:55,020 --> 01:42:59,540
that are not so much interested in generating text,

2270
01:42:59,540 --> 01:43:02,620
but taking existing text and editing it.

2271
01:43:03,660 --> 01:43:06,460
And I think this is a very interesting idea

2272
01:43:06,460 --> 01:43:08,540
that can become increasingly important.

2273
01:43:08,540 --> 01:43:11,940
And in some sense, this could be the way

2274
01:43:11,940 --> 01:43:14,940
you fix language model output potentially,

2275
01:43:14,940 --> 01:43:17,700
is to have another model that takes the output

2276
01:43:17,700 --> 01:43:19,500
of the language model and fixes it.

2277
01:43:20,940 --> 01:43:22,660
So some of the work here,

2278
01:43:22,660 --> 01:43:27,580
there was a paper out of Julia's group from YouTube now,

2279
01:43:27,580 --> 01:43:30,820
that sort of looked at summarization.

2280
01:43:30,820 --> 01:43:33,380
And there are systems that generate summaries.

2281
01:43:33,380 --> 01:43:35,260
How can you take that generated summaries

2282
01:43:35,260 --> 01:43:37,820
and edit it to correct all the factual mistakes

2283
01:43:37,820 --> 01:43:38,660
it has made?

2284
01:43:39,740 --> 01:43:42,460
And editing is somehow a much,

2285
01:43:42,460 --> 01:43:44,380
let's not say definitely a simpler problem,

2286
01:43:44,380 --> 01:43:47,340
but in some sense, it could be a simpler problem

2287
01:43:47,340 --> 01:43:50,100
than writing the whole summary from scratch,

2288
01:43:50,100 --> 01:43:51,700
especially when you do the writing,

2289
01:43:51,700 --> 01:43:53,620
you do left to right generation.

2290
01:43:53,620 --> 01:43:55,980
So you can't go back and revisit something

2291
01:43:55,980 --> 01:43:57,220
that you've done before.

2292
01:43:57,220 --> 01:43:58,620
With these editing models,

2293
01:43:58,620 --> 01:44:01,700
they have the whole picture to some degree,

2294
01:44:01,700 --> 01:44:03,900
and all they have to do is fix it

2295
01:44:03,900 --> 01:44:05,580
so that the picture is consistent.

2296
01:44:06,780 --> 01:44:10,380
And so this idea seems like potentially simpler

2297
01:44:10,380 --> 01:44:11,900
than generation.

2298
01:44:11,900 --> 01:44:14,500
So you could generate something,

2299
01:44:14,500 --> 01:44:16,660
and maybe this is also attached to diffusion models

2300
01:44:16,660 --> 01:44:19,500
where you write something that's maybe not so correct,

2301
01:44:19,500 --> 01:44:21,900
but you revise it and it becomes better.

2302
01:44:21,900 --> 01:44:24,620
So there is a bunch of work along these directions

2303
01:44:24,620 --> 01:44:27,300
that came out essentially this year,

2304
01:44:27,300 --> 01:44:29,420
maybe second half of this year,

2305
01:44:29,420 --> 01:44:31,140
some of it early on,

2306
01:44:31,140 --> 01:44:35,580
that tries to gather data sets where you have edits,

2307
01:44:35,580 --> 01:44:37,380
or try to maybe even generate data sets

2308
01:44:37,380 --> 01:44:38,700
where you have edits,

2309
01:44:38,700 --> 01:44:42,380
and create these models that are able to fix those edits.

2310
01:44:42,380 --> 01:44:46,260
And so the prediction specifically is that

2311
01:44:47,780 --> 01:44:50,660
teams will build on this and produce models

2312
01:44:50,660 --> 01:44:53,780
that can actually kind of deliver on

2313
01:44:54,940 --> 01:44:58,980
the ability to do editing and revising.

2314
01:44:58,980 --> 01:45:02,300
And I think this could be, for example,

2315
01:45:02,300 --> 01:45:05,620
there'll be an editing model that can fix bias issues.

2316
01:45:05,620 --> 01:45:08,220
There'll be an editing model that fixes toxicity.

2317
01:45:08,220 --> 01:45:11,460
There'll be an editing model that fix factuality.

2318
01:45:11,460 --> 01:45:14,740
And these editing models can make web searches

2319
01:45:14,740 --> 01:45:18,140
and sort of take that information and edit the output.

2320
01:45:18,140 --> 01:45:23,140
So I could imagine that this could be a practical way

2321
01:45:23,500 --> 01:45:26,060
of solving many of the issues in language modeling.

2322
01:45:26,060 --> 01:45:29,180
It is a really interesting idea that,

2323
01:45:29,180 --> 01:45:31,100
I don't know if it's like a separation of concerns

2324
01:45:31,100 --> 01:45:33,980
or something like the language model

2325
01:45:33,980 --> 01:45:36,380
doesn't necessarily need to do everything

2326
01:45:36,380 --> 01:45:38,700
if we can compensate.

2327
01:45:38,700 --> 01:45:41,380
So in a way, it's like decomposition as well.

2328
01:45:41,380 --> 01:45:46,180
Like let it generate if the way to get

2329
01:45:47,460 --> 01:45:49,380
something that's not toxic that's accurate

2330
01:45:49,380 --> 01:45:53,620
is to have another type of model support it.

2331
01:45:53,620 --> 01:45:54,460
Great.

2332
01:45:54,460 --> 01:45:56,780
Yeah, I think, yeah, that's right.

2333
01:45:56,780 --> 01:45:59,700
And then for, at least for summarization

2334
01:45:59,700 --> 01:46:01,700
and things where it's supposed to be factual

2335
01:46:01,700 --> 01:46:02,540
and stuff like that,

2336
01:46:02,540 --> 01:46:05,660
I could see it sort of addressing those problems.

2337
01:46:05,660 --> 01:46:08,100
Of course, if it's generating a long text

2338
01:46:08,100 --> 01:46:12,500
and there are longer range sort of consistency issues

2339
01:46:12,500 --> 01:46:13,340
and stuff like that,

2340
01:46:13,340 --> 01:46:14,380
it might be a little bit difficult

2341
01:46:14,380 --> 01:46:17,420
for editing models to come into picture there.

2342
01:46:17,420 --> 01:46:19,420
What I like about editing is also,

2343
01:46:19,420 --> 01:46:21,380
it's something that we can imagine

2344
01:46:21,380 --> 01:46:23,420
not only working on language model output,

2345
01:46:23,420 --> 01:46:25,540
but working on a human output

2346
01:46:25,540 --> 01:46:28,820
or text that's been written with the writing assistant

2347
01:46:28,820 --> 01:46:29,660
and things like that, right?

2348
01:46:29,660 --> 01:46:30,780
Like you can still go back

2349
01:46:30,780 --> 01:46:33,780
and do a post-processing editing step to polish it up.

2350
01:46:33,780 --> 01:46:35,580
And I think that could be very useful as well.

2351
01:46:35,580 --> 01:46:40,580
So our last category in the NLP predictions

2352
01:46:41,420 --> 01:46:46,180
is top people, companies, organizations, teams

2353
01:46:46,180 --> 01:46:49,460
to watch in the field 2023.

2354
01:46:49,460 --> 01:46:54,260
Of course, the caveat of you're not,

2355
01:46:55,540 --> 01:46:58,820
any omissions here are not to slight the work

2356
01:46:58,820 --> 01:47:00,020
of any particular team,

2357
01:47:00,020 --> 01:47:02,900
but like who's got your mind share

2358
01:47:02,900 --> 01:47:07,220
and who are you expecting to see interesting things

2359
01:47:07,220 --> 01:47:09,980
from in the upcoming year?

2360
01:47:09,980 --> 01:47:13,380
Yeah, so this has been a little bit difficult question,

2361
01:47:13,380 --> 01:47:14,220
I think every year,

2362
01:47:14,220 --> 01:47:15,380
but one thing I will say,

2363
01:47:15,380 --> 01:47:17,300
and this is maybe the most obvious answer

2364
01:47:17,300 --> 01:47:20,100
is to sort of keep an eye on open AI

2365
01:47:20,100 --> 01:47:21,900
and what they're up to, right?

2366
01:47:21,900 --> 01:47:24,900
I think people, once they do something,

2367
01:47:24,900 --> 01:47:26,220
people always come back and say,

2368
01:47:26,220 --> 01:47:28,300
look, what they've done is not so exciting.

2369
01:47:28,300 --> 01:47:29,660
Oh, they only scaled it up

2370
01:47:29,660 --> 01:47:32,860
or oh, they only did this additional thing.

2371
01:47:32,860 --> 01:47:36,020
But the fact is that they are the first ones to do it.

2372
01:47:36,020 --> 01:47:38,900
They're the first ones to bring it out, make it available.

2373
01:47:38,900 --> 01:47:43,900
And that is, and get people excited about language models

2374
01:47:44,380 --> 01:47:45,780
in a way that they weren't before.

2375
01:47:45,780 --> 01:47:49,900
So that happened with GPT-2, GPT-3 and chat-GPT.

2376
01:47:49,900 --> 01:47:53,060
And I'm sure GPT-4 will have the same thing.

2377
01:47:53,060 --> 01:47:55,220
I'm sure retroactively,

2378
01:47:55,220 --> 01:47:58,420
we will all talk about what the problems with GPT-4 are

2379
01:47:58,420 --> 01:48:02,420
and how it's incrementally only training on more data

2380
01:48:02,420 --> 01:48:05,620
or has more parameters or whatever it is.

2381
01:48:05,620 --> 01:48:07,060
But I think qualitatively,

2382
01:48:07,060 --> 01:48:09,420
it'll bring something interesting to the table.

2383
01:48:09,420 --> 01:48:10,980
And I'm really curious about

2384
01:48:10,980 --> 01:48:14,500
what that next interesting thing is going to be.

2385
01:48:15,460 --> 01:48:20,300
Do you think the general predictions

2386
01:48:20,300 --> 01:48:22,260
that are kind of floating around,

2387
01:48:22,260 --> 01:48:27,260
basically spring and 100 trillion parameters,

2388
01:48:30,500 --> 01:48:32,060
is your money on those?

2389
01:48:32,060 --> 01:48:36,900
I mean, to sort of have a completely different perspective,

2390
01:48:36,900 --> 01:48:40,780
I think this is also a nice model that came out,

2391
01:48:40,780 --> 01:48:42,580
this nice paper that came out a little earlier

2392
01:48:42,580 --> 01:48:44,780
called the Chinchilla paper.

2393
01:48:44,780 --> 01:48:46,940
This was a paper that show that these models

2394
01:48:46,940 --> 01:48:51,420
are extremely under-trained and they are data hungry.

2395
01:48:51,420 --> 01:48:55,780
So one version of GPT-4 could be potentially

2396
01:48:55,780 --> 01:48:57,420
not even a different architecture,

2397
01:48:57,420 --> 01:48:59,060
not even more parameters.

2398
01:48:59,060 --> 01:49:02,100
Like exactly, let's keep it 175 billion

2399
01:49:03,020 --> 01:49:06,380
and let's just somehow get 10 times the data

2400
01:49:06,380 --> 01:49:09,700
if you can potentially get that spot somewhere, right?

2401
01:49:09,700 --> 01:49:10,540
I could totally-

2402
01:49:10,540 --> 01:49:14,940
But then everybody that shared the image

2403
01:49:14,940 --> 01:49:17,620
with the little dot and the big dot would be totally wrong.

2404
01:49:17,620 --> 01:49:21,900
Well, yeah, they'll just sort of replace that with data

2405
01:49:21,900 --> 01:49:24,100
and it might still work, right?

2406
01:49:24,100 --> 01:49:25,580
For those not on Twitter,

2407
01:49:25,580 --> 01:49:30,580
that has dominated LLM Twitter over the past couple of days.

2408
01:49:30,900 --> 01:49:32,340
Is there even...

2409
01:49:34,140 --> 01:49:36,460
I think that when GPT-3 came out,

2410
01:49:36,460 --> 01:49:41,460
the kind of colloquial articulation of what they did

2411
01:49:42,220 --> 01:49:45,660
was like train this language model on the entire internet.

2412
01:49:45,660 --> 01:49:49,140
Like, is there 10X more data to train on?

2413
01:49:49,140 --> 01:49:49,980
Yeah, I don't know.

2414
01:49:49,980 --> 01:49:51,500
I don't know how much they've trained on

2415
01:49:51,500 --> 01:49:52,420
and how much there is.

2416
01:49:52,420 --> 01:49:55,260
I mean, there's definitely 10X more data.

2417
01:49:55,260 --> 01:49:58,980
There is a lot of stuff that's proprietary, right?

2418
01:49:58,980 --> 01:50:00,100
Proprietary?

2419
01:50:00,100 --> 01:50:01,780
Maybe even proprietary, right?

2420
01:50:01,780 --> 01:50:02,620
Like-

2421
01:50:02,620 --> 01:50:05,780
Transcribe a bunch of videos and audio and books

2422
01:50:05,780 --> 01:50:07,100
and I guess, yeah.

2423
01:50:07,100 --> 01:50:10,900
They do have that Whisper model that does, yeah,

2424
01:50:10,900 --> 01:50:12,820
that does really good transcribing.

2425
01:50:12,820 --> 01:50:13,660
So they could use that.

2426
01:50:13,660 --> 01:50:14,500
Good point.

2427
01:50:14,500 --> 01:50:15,580
They didn't create that for no reason.

2428
01:50:15,580 --> 01:50:16,980
Right, yeah.

2429
01:50:16,980 --> 01:50:19,780
They also can go into scientific papers

2430
01:50:19,780 --> 01:50:23,260
and I don't think the 48 million papers

2431
01:50:23,260 --> 01:50:26,780
that Galactica was trained on was something

2432
01:50:26,780 --> 01:50:27,940
GPT-3 was trained on.

2433
01:50:27,940 --> 01:50:29,900
And I think that is a pretty valuable resource.

2434
01:50:29,900 --> 01:50:31,420
That Galactica paper also showed

2435
01:50:31,420 --> 01:50:33,740
that even on mathematical reasoning and things like that,

2436
01:50:33,740 --> 01:50:35,660
they were actually better.

2437
01:50:35,660 --> 01:50:37,660
So these scientific papers may be useful

2438
01:50:37,660 --> 01:50:39,900
for a bunch of other things that we don't realize.

2439
01:50:39,900 --> 01:50:42,300
So yeah, I think where that data comes from

2440
01:50:42,300 --> 01:50:43,140
is unclear to me,

2441
01:50:43,140 --> 01:50:46,340
but it's clear that more data is somehow

2442
01:50:46,340 --> 01:50:49,260
maybe even more interesting than more parameters.

2443
01:50:49,260 --> 01:50:54,220
And more data could include more RLHF style things, right?

2444
01:50:54,220 --> 01:50:56,260
Like, I don't know what to open it.

2445
01:50:56,260 --> 01:50:57,140
Okay.

2446
01:50:57,140 --> 01:51:00,700
The other top company to, I would say again,

2447
01:51:00,700 --> 01:51:03,660
continue taking a look at is Hugging Face.

2448
01:51:03,660 --> 01:51:06,460
I've been constantly sort of amazed

2449
01:51:06,460 --> 01:51:09,580
by how much they've been doing.

2450
01:51:09,580 --> 01:51:13,460
One of the sort of key insights is like EMNLP,

2451
01:51:13,460 --> 01:51:15,460
which is this top conference in NLP,

2452
01:51:15,460 --> 01:51:18,260
has this demo track where they highlight

2453
01:51:18,260 --> 01:51:19,820
sort of not research papers,

2454
01:51:19,820 --> 01:51:24,380
but products sort of demos that are relevant for research.

2455
01:51:24,380 --> 01:51:28,340
And for the last three years, I think at EMNLP,

2456
01:51:28,340 --> 01:51:31,860
Hugging Face has got the best demo paper award.

2457
01:51:31,860 --> 01:51:35,100
And that kind of thing sort of shows

2458
01:51:36,020 --> 01:51:38,060
how they've been doing very different things,

2459
01:51:38,060 --> 01:51:41,380
but also doing things that are impactful and interesting.

2460
01:51:41,380 --> 01:51:43,940
So the two I want to highlight this year is,

2461
01:51:43,940 --> 01:51:45,660
again, they've done many, many things,

2462
01:51:45,660 --> 01:51:47,260
but the one I want to highlight

2463
01:51:47,260 --> 01:51:50,100
is the Evaluate system,

2464
01:51:50,100 --> 01:51:53,660
where they had this whole evaluation framework

2465
01:51:53,660 --> 01:51:56,940
for reproducing evaluations and evaluating models

2466
01:51:56,940 --> 01:51:59,060
and making all of this stuff really easy.

2467
01:51:59,060 --> 01:52:00,820
So you can introduce a new metric

2468
01:52:00,820 --> 01:52:03,860
and evaluate it on thousands of models, things like that,

2469
01:52:03,860 --> 01:52:05,780
make it really easy to compare models,

2470
01:52:05,780 --> 01:52:09,060
make it really easy to reproduce papers.

2471
01:52:09,060 --> 01:52:11,980
And I think that that's a really valuable service

2472
01:52:11,980 --> 01:52:13,980
to do research.

2473
01:52:13,980 --> 01:52:15,660
And the other one that I sort of,

2474
01:52:15,660 --> 01:52:17,580
we also started with this of like,

2475
01:52:17,580 --> 01:52:20,260
hey, what's happening inside the pre-training data?

2476
01:52:20,260 --> 01:52:23,700
One of the tools they have is this Roots search tool

2477
01:52:23,700 --> 01:52:26,740
that takes the Roots pre-training data,

2478
01:52:26,740 --> 01:52:29,580
but allows you to search it and find all kinds of things

2479
01:52:29,580 --> 01:52:31,500
that are happening inside that pre-training data.

2480
01:52:31,500 --> 01:52:33,660
So if you have a specific prediction,

2481
01:52:33,660 --> 01:52:34,500
then you want to be like,

2482
01:52:34,500 --> 01:52:36,180
hey, is there anything in the training data

2483
01:52:36,180 --> 01:52:38,260
that looks exactly like this?

2484
01:52:38,260 --> 01:52:41,060
You can do that search and get some results.

2485
01:52:41,060 --> 01:52:45,300
So I think they're just being pretty creative

2486
01:52:45,300 --> 01:52:48,940
and thoughtful about what is useful and building tools,

2487
01:52:48,940 --> 01:52:50,060
and that's been exciting.

2488
01:52:50,060 --> 01:52:51,900
And the last one that I'll bring up,

2489
01:52:51,900 --> 01:52:56,060
and this is something that was on top of my head this week,

2490
01:52:56,060 --> 01:52:57,620
but it can change.

2491
01:52:57,620 --> 01:53:02,620
It's a group called OUGHT, it's O-U-G-H-T,

2492
01:53:03,140 --> 01:53:07,820
I believe it's OUGHT.org, it's a website.

2493
01:53:07,820 --> 01:53:11,500
And this is sort of a research nonprofit.

2494
01:53:11,500 --> 01:53:15,980
And they've been doing sort of interesting things

2495
01:53:18,260 --> 01:53:20,060
related to sort of building tools.

2496
01:53:20,060 --> 01:53:22,900
So they have this tool called Primer,

2497
01:53:22,900 --> 01:53:26,540
and this is going back to decomposed reasoning.

2498
01:53:26,540 --> 01:53:29,340
This tool called Primer, you can give it a question

2499
01:53:29,340 --> 01:53:31,380
and it tries to come up with an answer,

2500
01:53:31,380 --> 01:53:33,660
but in the process of coming up with an answer,

2501
01:53:33,660 --> 01:53:37,900
it can do a web search, or it can write a small program,

2502
01:53:37,900 --> 01:53:39,340
and it can do all of these things,

2503
01:53:39,340 --> 01:53:41,700
and they've built a sort of nice tool

2504
01:53:41,700 --> 01:53:44,300
to be able to visualize what the decompositions are

2505
01:53:44,300 --> 01:53:46,460
and what sort of things are being done.

2506
01:53:46,460 --> 01:53:49,940
So it's a really interesting use case of language models.

2507
01:53:50,980 --> 01:53:55,060
And then they also have another tool called Ellicit,

2508
01:53:55,060 --> 01:53:58,300
which is, in some sense, it's a little bit like Galactica,

2509
01:53:58,300 --> 01:54:02,900
but it's not so much interested in generating papers for you

2510
01:54:02,900 --> 01:54:05,900
but helping you do research for your paper, right?

2511
01:54:05,900 --> 01:54:08,500
So you have a specific question,

2512
01:54:08,500 --> 01:54:11,180
it's going to find a bunch of relevant papers,

2513
01:54:11,180 --> 01:54:13,460
take out snippets from those papers,

2514
01:54:13,460 --> 01:54:16,140
and be able to do that.

2515
01:54:16,140 --> 01:54:19,700
So I don't know, they've had to have a bunch of tools

2516
01:54:19,700 --> 01:54:22,660
that when I'm looking at decomposed reasoning,

2517
01:54:22,660 --> 01:54:24,620
it comes up, and I'm looking at,

2518
01:54:24,620 --> 01:54:27,300
okay, research assistance, it sort of comes up,

2519
01:54:27,300 --> 01:54:29,340
and so it's been interesting to see,

2520
01:54:29,340 --> 01:54:31,180
and I'm curious what they'll do next.

2521
01:54:31,180 --> 01:54:33,100
I'm really curious about that,

2522
01:54:33,100 --> 01:54:35,300
and I'm gonna look into that in more detail.

2523
01:54:35,300 --> 01:54:37,900
So, awesome, awesome.

2524
01:54:39,740 --> 01:54:42,180
Well, I think we are done.

2525
01:54:42,180 --> 01:54:45,140
Like, you've been a champ, this has been awesome.

2526
01:54:46,460 --> 01:54:47,620
It's been fun, yeah.

2527
01:54:48,660 --> 01:54:53,100
Yeah, no, I mean, you rose to the occasion

2528
01:54:53,100 --> 01:54:57,980
of kind of capturing an amazing year in NLP, for sure.

2529
01:54:57,980 --> 01:55:01,740
So thanks so much for joining us.

2530
01:55:01,740 --> 01:55:02,860
Yeah, thanks for inviting me.

2531
01:55:02,860 --> 01:55:05,740
I think the time sort of justifies

2532
01:55:05,740 --> 01:55:09,220
how much this year had in NLP this year,

2533
01:55:09,220 --> 01:55:12,220
and I'm really curious to see where NLP is going to go.

2534
01:55:12,220 --> 01:55:15,820
I will mention that ChatGPT came out right before,

2535
01:55:15,820 --> 01:55:17,860
or I think maybe even during Eurips.

2536
01:55:17,860 --> 01:55:22,020
So I attended Eurips, and I saw the firsthand experience

2537
01:55:22,020 --> 01:55:24,220
of the whole machine learning community there.

2538
01:55:24,220 --> 01:55:27,180
Then I flew to Abu Dhabi to attend EMNLP,

2539
01:55:27,180 --> 01:55:28,660
and that's where I saw the reaction

2540
01:55:28,660 --> 01:55:31,020
of the whole NLP community.

2541
01:55:31,020 --> 01:55:33,220
And it's been interesting to see sort of

2542
01:55:33,220 --> 01:55:36,420
how the reactions have sort of spanned

2543
01:55:37,300 --> 01:55:39,020
both optimism and excitement,

2544
01:55:39,020 --> 01:55:40,540
which is kind of where I am,

2545
01:55:40,540 --> 01:55:44,260
like to see like, hey, what can we build with this stuff?

2546
01:55:44,260 --> 01:55:45,940
To pessimism where they're like,

2547
01:55:45,940 --> 01:55:48,260
oh, you know, it doesn't really,

2548
01:55:48,260 --> 01:55:49,820
yeah, it's not gonna change anything,

2549
01:55:49,820 --> 01:55:52,540
it's just a bigger language model,

2550
01:55:52,540 --> 01:55:54,260
all the way to, essentially,

2551
01:55:54,260 --> 01:55:56,540
I want to say some form of denial,

2552
01:55:56,540 --> 01:55:57,860
where it's like, look,

2553
01:55:57,860 --> 01:56:01,660
it's behind propriety, closed off system,

2554
01:56:01,660 --> 01:56:05,260
and therefore it doesn't matter to do research,

2555
01:56:05,260 --> 01:56:08,460
and that's definitely not a take I agree with.

2556
01:56:08,460 --> 01:56:10,460
So yeah, it's been exciting.

2557
01:56:10,460 --> 01:56:13,180
And there's also a fourth, which maybe is less so,

2558
01:56:13,180 --> 01:56:15,940
and I don't, maybe less so in the research community

2559
01:56:15,940 --> 01:56:17,660
than in the general sphere,

2560
01:56:17,660 --> 01:56:21,020
which is fear of the implications of it.

2561
01:56:23,540 --> 01:56:26,060
Did you find that less so on the research side?

2562
01:56:26,060 --> 01:56:29,580
I guess less so, definitely less so on the, yeah,

2563
01:56:29,580 --> 01:56:31,020
because I think we've been,

2564
01:56:31,900 --> 01:56:33,340
there is a little bit of fear

2565
01:56:33,340 --> 01:56:35,140
becoming a little bit more obvious,

2566
01:56:35,140 --> 01:56:36,980
but I think the community,

2567
01:56:36,980 --> 01:56:38,060
because of a lot of people

2568
01:56:38,060 --> 01:56:39,620
who've been sort of pointing out problems

2569
01:56:39,620 --> 01:56:41,500
with large language models for a while,

2570
01:56:41,500 --> 01:56:45,420
we are kind of, we know what not to,

2571
01:56:45,420 --> 01:56:47,940
as a community, we should know what not to do,

2572
01:56:47,940 --> 01:56:49,620
but it is a little bit scary

2573
01:56:49,620 --> 01:56:51,220
when people are using it for things

2574
01:56:51,220 --> 01:56:54,020
that clearly, at the onset, should be like,

2575
01:56:54,020 --> 01:56:56,180
hey, why are you doing this, right?

2576
01:56:56,180 --> 01:56:57,620
Yeah, yeah, yeah.

2577
01:56:57,620 --> 01:56:58,700
Awesome.

2578
01:56:58,700 --> 01:57:01,700
Well, once again, Samir, thanks so much.

2579
01:57:01,700 --> 01:57:03,460
Really great session and conversation

2580
01:57:03,460 --> 01:57:06,460
and appreciate all the work you put into prepping for it.

2581
01:57:06,460 --> 01:57:07,300
Yeah, thank you, Sam.

2582
01:57:07,300 --> 01:57:24,300
It was fun.

