WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.960
I'm your host, Sam Charrington, in this episode I'm joined by Google Research Scientist

00:32.960 --> 00:38.640
Ryan Poplin, who recently co-authored the paper, prediction of cardiovascular risk factors

00:38.640 --> 00:43.040
from retinal, fundous photographs via deep learning.

00:43.040 --> 00:47.600
In our conversation, Ryan details his work training a deep learning model to predict various

00:47.600 --> 00:53.240
patient risk factors for heart disease, including some surprising ones like age and gender.

00:53.240 --> 00:57.240
We also dive into some interesting findings he discovered with regards to multitask

00:57.240 --> 01:02.800
learning, as well as his use of attention mechanisms to provide explainability.

01:02.800 --> 01:07.760
This was a really interesting discussion that I'm sure you'll enjoy.

01:07.760 --> 01:12.240
Before we get to the show, this past Saturday I spent some time at the scaled machine learning

01:12.240 --> 01:15.760
conference which was held on the Stanford campus in Palo Alto.

01:15.760 --> 01:21.520
I had a great time, heard some great speakers, met a few listeners, and tweeted a ton.

01:21.520 --> 01:26.280
So for more of my thoughts from the conference, head on over to my Twitter feed at at Sam

01:26.280 --> 01:28.280
Charrington.

01:28.280 --> 01:32.680
This week I'm at Nvidia's GTC conference, so be sure to hit that follow button for more

01:32.680 --> 01:34.280
news and views.

01:34.280 --> 01:37.240
And now on to the show.

01:37.240 --> 01:48.000
All right, everyone, I've got Ryan Poplin on the line, Ryan is a research scientist

01:48.000 --> 01:54.000
at Google and the lead author on the recent paper, prediction of cardiovascular risk factors

01:54.000 --> 01:57.360
from retinal fundous photographs via deep learning.

01:57.360 --> 02:00.600
Ryan, welcome to this week in machine learning and AI.

02:00.600 --> 02:01.600
Thank you so much.

02:01.600 --> 02:02.600
Thank you for having me.

02:02.600 --> 02:03.600
Absolutely.

02:03.600 --> 02:08.960
So tell us a little bit about how you got involved and interested in machine learning.

02:08.960 --> 02:09.960
What's your background?

02:09.960 --> 02:13.640
Sure, I'm a computer scientist by training.

02:13.640 --> 02:18.640
And what really drew me to machine learning was this idea that I could have an impact in

02:18.640 --> 02:25.840
a variety of scientific domains through statistics and through knowing about computer science

02:25.840 --> 02:27.040
and machine learning.

02:27.040 --> 02:32.720
And it allows me to sort of apply these methods to a variety of domains without having

02:32.720 --> 02:38.440
to sort of derive, you know, domain expert features about these problems.

02:38.440 --> 02:44.360
And so did you have a formal education in AI, where did you, how did you kind of come

02:44.360 --> 02:48.760
up to speed on the tools and techniques that you use today?

02:48.760 --> 02:49.760
Right.

02:49.760 --> 02:53.240
So I did a bachelor's degree in computer science in Indiana.

02:53.240 --> 02:57.080
This was at the Rose Home and Institute of Technology where I studied computer science

02:57.080 --> 02:58.560
and mathematics.

02:58.560 --> 03:04.120
And then for graduate work, I was at Carnegie Mellon University, where I was in a pretty

03:04.120 --> 03:09.920
fun computational neuroscience program, which was a fun way to combine sort of stats, computer

03:09.920 --> 03:12.760
science, and the study of neuroscience.

03:12.760 --> 03:16.080
That's sort of where I picked up the statistical machine learning techniques.

03:16.080 --> 03:17.080
Awesome.

03:17.080 --> 03:23.680
And does your work today, do you spend a lot of time applying the machine learning in neuroscience

03:23.680 --> 03:29.040
and in related fields, or do you work pretty broadly, you know, across domains?

03:29.040 --> 03:30.040
Right.

03:30.040 --> 03:32.240
So I like to work broadly across a variety of domains.

03:32.240 --> 03:36.160
So I've been having fun, you know, applying these, these methods, which are pretty general

03:36.160 --> 03:40.880
to problems in genomics, problems in medical imaging, which is a subject of this paper

03:40.880 --> 03:43.120
on a variety of other scientific domains.

03:43.120 --> 03:44.120
Awesome.

03:44.120 --> 03:45.120
Awesome.

03:45.120 --> 03:48.360
So why don't we get started by having you tell us a little bit about this paper?

03:48.360 --> 03:49.360
Sure.

03:49.360 --> 03:55.920
So this is, this was an effort to using the, the UK biobank, which is the set of fundus

03:55.920 --> 04:01.880
photography images that were annotated with patient metadata, try to take those images,

04:01.880 --> 04:05.280
send them through a neural network, and try to see what we could predict from it.

04:05.280 --> 04:07.280
And what we found was actually quite surprising.

04:07.280 --> 04:12.040
You could predict quite a wide variety of aspects of a person's health just by looking at this

04:12.040 --> 04:13.040
fundus photo.

04:13.040 --> 04:16.160
Why don't we get started by talking about what is a fundus?

04:16.160 --> 04:17.160
Yeah.

04:17.160 --> 04:18.160
Sure.

04:18.160 --> 04:24.080
So a fundus photograph is, is a pretty routine thing actually that I'm sure most of the listeners

04:24.080 --> 04:27.520
of this, of this show have actually had taken themselves.

04:27.520 --> 04:29.960
So it's simply a picture of the back of the eye.

04:29.960 --> 04:34.840
So, you know, a specialist would use a camera called a fundoscope, they, they take a picture

04:34.840 --> 04:38.600
of the back of the eye, and that's, and that's the retina.

04:38.600 --> 04:42.920
So if you go to the ophthalmologist, you probably have this taken, you're to routinely kind

04:42.920 --> 04:44.400
of once a year.

04:44.400 --> 04:48.520
So is this the thing that happens when they, like, blow the burst of air in your eye?

04:48.520 --> 04:49.520
Not quite.

04:49.520 --> 04:50.520
It is a camera.

04:50.520 --> 04:52.600
So you, you know, it's just a normal, you know, picture.

04:52.600 --> 04:55.680
It's actually, you know, very pleasant and non-invasive.

04:55.680 --> 04:57.360
You simply take a picture of the back of the eye.

04:57.360 --> 05:02.040
You can see in the image, you can see things like the optic disc and blood vessels sort

05:02.040 --> 05:08.080
of emanating out of that, and doctors use this to look for a wide variety of eye diseases.

05:08.080 --> 05:11.880
And what we're showing in this paper is that you can look at other diseases as well.

05:11.880 --> 05:12.880
Awesome.

05:12.880 --> 05:19.320
One of the things that I gleaned from the paper was that, you know, I've seen a bunch

05:19.320 --> 05:25.400
of articles about the paper and they all kind of focus on your ability to predict some

05:25.400 --> 05:30.080
interesting cardiovascular risk factors.

05:30.080 --> 05:35.320
But one of the things that's seen most interesting was that, and that didn't get picked up in

05:35.320 --> 05:40.760
a lot of these articles is that you didn't really go in with a direct correlation between

05:40.760 --> 05:45.120
a lot of the things that you were able to predict in the data set meaning, you know,

05:45.120 --> 05:50.840
it wasn't already known that looking at these retinal images, you could predict things

05:50.840 --> 05:56.360
like age and gender and stuff like that, but you discovered that through the research.

05:56.360 --> 05:57.960
Is that the correct interpretation?

05:57.960 --> 05:59.800
Yeah, that's exactly right.

05:59.800 --> 06:05.640
So we started, you know, the background work from my current team is that we've been

06:05.640 --> 06:08.800
using these images to look at a variety of eye diseases.

06:08.800 --> 06:14.200
So predicting diabetic retinopathy, for example, at the level of accuracy of a doctor.

06:14.200 --> 06:17.960
And so when we started using this data set, that's exactly where we started was trying

06:17.960 --> 06:20.000
to predict, look at eye diseases.

06:20.000 --> 06:24.360
And then what we noticed is that there's a wide variety of metadata about the patient

06:24.360 --> 06:26.200
available in this data set.

06:26.200 --> 06:31.240
And so we sort of added those variables to the model in really kind of a diagnostic kind

06:31.240 --> 06:32.240
of way.

06:32.240 --> 06:36.040
So these are things like age and gender and all kinds of other things.

06:36.040 --> 06:40.520
And we felt like those variables, they should be a, you know, very high fidelity in the

06:40.520 --> 06:45.040
data set because it's very easy to measure someone's age or ask their gender.

06:45.040 --> 06:48.680
And so we felt like it was a great sort of control or ground truth that we could add

06:48.680 --> 06:50.280
to the model.

06:50.280 --> 06:53.680
But then what we discovered as we were training the model was actually we were able to

06:53.680 --> 06:56.600
predict these things with remarkably high accuracy.

06:56.600 --> 07:00.640
And in fact, in the beginning, we thought it was a bug in the model, it was a problem

07:00.640 --> 07:07.560
because, you know, you look at a gender AUC of.97 and you show that to someone and they

07:07.560 --> 07:11.040
say to you, it must be, you must have a bug in your model because there's no way you

07:11.040 --> 07:14.360
can predict that with such high accuracy, right?

07:14.360 --> 07:18.240
And you can't ignore an AUC of.97, especially when the classes are balanced.

07:18.240 --> 07:20.600
This is quite a robust prediction from the model.

07:20.600 --> 07:24.720
And in fact, as we dug more and more into it, we discovered that this wasn't a bug in

07:24.720 --> 07:26.840
the model, it was actually a real prediction.

07:26.840 --> 07:32.240
And so drilling a little bit more on the kind of that, the realization, I'm imagining

07:32.240 --> 07:39.200
that you're at what I'm hearing you say is that you started by actually using this, you

07:39.200 --> 07:43.080
know, the things that you ultimately found you could predict as features of your model

07:43.080 --> 07:44.680
so as inputs.

07:44.680 --> 07:48.960
And then, you know, at some point along the way, you found that they were highly dependent

07:48.960 --> 07:50.520
on the images themselves.

07:50.520 --> 07:52.880
Is that kind of the way it came about?

07:52.880 --> 07:58.480
Sort of, so the input to the model was always simply the images in the fundus, so always

07:58.480 --> 08:01.040
using the pixels of the image.

08:01.040 --> 08:05.400
And then alongside it though, you can predict many things simultaneously.

08:05.400 --> 08:10.480
And actually what we found just through our research is that when you give the model many,

08:10.480 --> 08:13.960
many things to predict simultaneously, it actually does better overall.

08:13.960 --> 08:17.320
It's sort of like giving the model something to do with all of its capacity.

08:17.320 --> 08:21.040
And so as a result, we would throw in things like, why don't you try to predict the age,

08:21.040 --> 08:24.360
the gender, and actually all kinds of other things that we thought it probably couldn't

08:24.360 --> 08:27.360
predict, so it would serve as a nice control.

08:27.360 --> 08:30.720
But in reality, what we found is it's doing quite well.

08:30.720 --> 08:34.920
And so we started with gender, that was the most, that was a very surprising one.

08:34.920 --> 08:37.640
And we realized that it, in fact, was a real prediction.

08:37.640 --> 08:39.240
It was actually doing such a good job.

08:39.240 --> 08:40.800
And then we discovered age.

08:40.800 --> 08:44.680
And here, here it's slightly a different prediction, you're, you're regressing to a floating

08:44.680 --> 08:45.680
point value.

08:45.680 --> 08:50.360
And what we found is that age you were actually correct within something like three years

08:50.360 --> 08:54.800
up the person's actual age, which was just remarkable, because when we talked to doctors,

08:54.800 --> 08:59.240
they would tell us, sure, I can tell the difference between an old eye or a young eye.

08:59.240 --> 09:02.960
But then we tell them we're actually able to tell you the age within three years.

09:02.960 --> 09:05.120
And they're quite surprised by that.

09:05.120 --> 09:06.120
Yeah.

09:06.120 --> 09:10.840
So you said something in passing that I wanted to drill in on a bit.

09:10.840 --> 09:17.280
You found that the model was, you, you improve the model, maybe you can be more specific

09:17.280 --> 09:20.320
or kind of restate the, what you said.

09:20.320 --> 09:24.920
But it sounded like you were saying you improved the model's overall performance across a bunch

09:24.920 --> 09:25.920
of measures.

09:25.920 --> 09:30.880
Like each time you asked it to do something new, it got better overall.

09:30.880 --> 09:31.880
Is that what you're saying?

09:31.880 --> 09:32.880
Yeah, that's right.

09:32.880 --> 09:35.520
So, so this is called multi-task learning.

09:35.520 --> 09:40.720
What we found is that, so given a fixed model, as you add more and more features that

09:40.720 --> 09:44.440
you wanted to predict, you're sort of giving, giving the model something to do or letting

09:44.440 --> 09:45.800
it use its capacity.

09:45.800 --> 09:51.440
And so it has a tendency, you, it can't possibly overfit because it's being asked to predict

09:51.440 --> 09:52.760
so many things.

09:52.760 --> 09:57.120
And as a result, you sort of improve the performance of the model overall.

09:57.120 --> 09:58.520
Oh, interesting.

09:58.520 --> 10:04.600
And you didn't apply necessarily or go out of your way to apply kind of multi-task learning,

10:04.600 --> 10:08.200
you know, techniques or anything like that to try to improve the performance.

10:08.200 --> 10:12.360
But rather, this was just, you know, you got better performance for free by asking the

10:12.360 --> 10:14.080
model to do more stuff.

10:14.080 --> 10:15.080
That's right.

10:15.080 --> 10:18.920
And, and that's sort of a feature of this particular data set in which there are many,

10:18.920 --> 10:24.680
many metadata values that we could try to predict, you know, it probably may not work

10:24.680 --> 10:28.160
in every possible problem or every domain, but this was one and where to, and actually

10:28.160 --> 10:29.160
worked.

10:29.160 --> 10:36.040
Now that you have seen this, if you're working on a problem, you would, you know, would

10:36.040 --> 10:40.000
one of your steps in optimization be just trying to come up with other stuff for the network

10:40.000 --> 10:41.000
to do?

10:41.000 --> 10:43.040
Like is that a rational way of thinking about it?

10:43.040 --> 10:44.040
It is absolutely.

10:44.040 --> 10:46.920
This is a technique we're applying to a lots of different problems, actually.

10:46.920 --> 10:51.720
So another example where this worked really well was in our efforts to predict diabetic

10:51.720 --> 10:52.720
retinopathy.

10:52.720 --> 10:57.440
So in this case, you sort of have this grade that's given by the doctors, are you one of

10:57.440 --> 11:01.640
these five classes of the disease diabetic retinopathy, but there's all kinds of other

11:01.640 --> 11:05.320
things you can predict about the image, such as, is it the image quality?

11:05.320 --> 11:10.320
So would the grader say that this is a high quality image or a low or an unusable image,

11:10.320 --> 11:14.920
or things like, is this a left eye or a right eye, which is, you know, not that useful

11:14.920 --> 11:20.400
of a prediction, but adding this extra extra stuff into the into the model actually helped

11:20.400 --> 11:21.640
it a little quite a bit.

11:21.640 --> 11:22.640
Mm-hmm.

11:22.640 --> 11:27.160
I had a note to ask about kind of right versus left eye, and I didn't see that mentioned

11:27.160 --> 11:29.200
in the paper at all.

11:29.200 --> 11:34.000
Did that come into play at all in this recent paper?

11:34.000 --> 11:35.000
Yeah.

11:35.000 --> 11:39.640
So that's another feature that we try to predict alongside of, you know, age and gender

11:39.640 --> 11:43.560
and smoking status, you also predicted this image of a left eye or a right eye.

11:43.560 --> 11:44.560
Okay.

11:44.560 --> 11:50.440
And now how, you know, how arbitrary can you, can you get with this, like, can you, you

11:50.440 --> 11:56.440
know, could you, for example, I guess you'd need a label, but could you come up with, you

11:56.440 --> 12:03.440
know, some arbitrary metric like the, you know, the vascular density of a retina image

12:03.440 --> 12:09.400
or, you know, the color shading or something like that and try to, you know, maybe find

12:09.400 --> 12:16.320
some mechanical way to produce labels and then ask the network to try to match to that

12:16.320 --> 12:17.320
as well.

12:17.320 --> 12:18.320
Absolutely.

12:18.320 --> 12:22.400
I think it's a very fruitful avenue of research for when you have a problem when you're

12:22.400 --> 12:27.160
trying to optimize your model to do better, I think this is one way to try that.

12:27.160 --> 12:32.320
It doesn't work in every case, like the features need to be sort of informative or correlative

12:32.320 --> 12:38.400
or interesting, like I think creating synthetic features that actually aren't correlated

12:38.400 --> 12:42.200
with what you're trying to predict probably won't help as much.

12:42.200 --> 12:44.920
But it's definitely a fruitful array of research for sure.

12:44.920 --> 12:48.240
Can you give me a sense for like the effect that this had, you know, what are we talking

12:48.240 --> 12:54.880
about in terms of performance lifts ultimately, you know, is this, you know, a few percent

12:54.880 --> 12:57.800
or is this, you know, how significant does it?

12:57.800 --> 12:58.800
Right.

12:58.800 --> 12:59.800
It's something like a few percent.

12:59.800 --> 13:03.000
These are the kind of things where when you're, when you're sort of at the middle to

13:03.000 --> 13:06.480
the end of a project and you're trying to eke out, you know, all the possible gains

13:06.480 --> 13:09.640
in a data set, this would be a strategy to try to use.

13:09.640 --> 13:14.000
You know, it's not going to transform the problem because ultimately all your learning

13:14.000 --> 13:17.320
are sort of the correlations between the features anyway.

13:17.320 --> 13:20.360
But it does, you know, help a couple percent.

13:20.360 --> 13:24.120
And there are many other techniques that we try to use to try to eke out that kind of

13:24.120 --> 13:28.520
last percent in model performance.

13:28.520 --> 13:33.960
So I guess as often happens here, I kind of, you know, argued into a particular interesting

13:33.960 --> 13:37.960
detail, but we got a little bit off of track and talking about the project as a whole.

13:37.960 --> 13:43.920
So you, one of the things that I noticed in addition to predicting, you know, developing

13:43.920 --> 13:49.080
this model that predicted all of these factors, as well as the kind of core factors that you

13:49.080 --> 13:58.920
were looking at the cardiovascular risk factors, you also tried to address the explainability

13:58.920 --> 14:03.320
challenge that is often faced by deep learning models.

14:03.320 --> 14:05.760
Can you talk a little bit about how you did that?

14:05.760 --> 14:06.760
Yeah, absolutely.

14:06.760 --> 14:12.120
You know, so as you mentioned, it's often the case that criticism of these kinds of models

14:12.120 --> 14:14.440
is that they're so-called black boxes.

14:14.440 --> 14:20.920
And I think there's a very long and growing body of research showing that these models

14:20.920 --> 14:23.560
are actually shouldn't be called black boxes.

14:23.560 --> 14:27.800
They're actually quite explainable in many different ways.

14:27.800 --> 14:30.960
And the technique that we used here is called soft attention.

14:30.960 --> 14:36.600
And so the idea is to create a network such that you're sort of successively zooming in

14:36.600 --> 14:37.880
on the pixels.

14:37.880 --> 14:44.680
And then you kind of asked the model which were the pixels that made you make that prediction.

14:44.680 --> 14:49.080
And then you, then you can project those pixels onto the original image and get an idea

14:49.080 --> 14:53.400
for why the model made the choice that it did.

14:53.400 --> 14:57.800
And so when we did that with this paper, we looked at a variety of cardiovascular risk

14:57.800 --> 15:03.480
factors and created these heat maps of what were the pixels in the image that led you

15:03.480 --> 15:05.080
to make that prediction.

15:05.080 --> 15:06.600
And what we found was actually pretty cool.

15:06.600 --> 15:12.520
So there are some predictions in which the predominantly the blood vessels were where

15:12.520 --> 15:13.840
the informative features.

15:13.840 --> 15:19.120
And so you can see these as these, in the paper, we use these green heat maps and they

15:19.120 --> 15:24.680
sort of snake along and follow the blood vessels that they're emanating out of the altidisk.

15:24.680 --> 15:30.040
And so a few of those predictions in particular are the prediction of the age and the prediction

15:30.040 --> 15:31.040
of the blood pressure.

15:31.040 --> 15:32.560
And so the blood pressure makes a lot of sense.

15:32.560 --> 15:38.160
If you want to predict the blood pressure, you need to look at the blood vessels.

15:38.160 --> 15:40.000
And then the other predictions.

15:40.000 --> 15:44.160
So for example, the gender prediction were actually focused on quite different features.

15:44.160 --> 15:49.360
So there we saw things like the macula and the size of the altidisk.

15:49.360 --> 15:53.800
And when you go back and like show these images to doctors, they can sort of, you know,

15:53.800 --> 15:56.800
provide feedback and sort of back up some of these findings.

15:56.800 --> 16:02.000
So when you show these heat maps to doctors about gender, they say, oh, yeah, there's

16:02.000 --> 16:06.640
this research about the ratio of the sizes of the altidisk and the caliber of the blood

16:06.640 --> 16:07.840
vessels as they emanate out.

16:07.840 --> 16:09.920
And that's predictive of gender.

16:09.920 --> 16:14.680
And so this sort of gives a bit of validity to the predictions and also makes people feel

16:14.680 --> 16:17.040
much more comfortable.

16:17.040 --> 16:23.360
When you're applying this soft attention mechanism, do you, are you ultimately training kind

16:23.360 --> 16:28.960
of multiple models in parallel, or is it, you know, one big network that has a bunch

16:28.960 --> 16:30.680
of different outputs?

16:30.680 --> 16:31.680
Yes.

16:31.680 --> 16:34.200
So in this particular case, it's two separate networks.

16:34.200 --> 16:38.920
We had, we used the inception V3 architecture to do the majority of the predictions.

16:38.920 --> 16:43.160
And that network has the highest AUCs in the best accuracy.

16:43.160 --> 16:47.480
And then we trained a separate model, which is this smaller soft attention model to do

16:47.480 --> 16:49.680
the model explanation.

16:49.680 --> 16:53.120
For this particular paper, that was done just because the inception architecture had

16:53.120 --> 16:56.920
had a better performance and we wanted to report the best performance.

16:56.920 --> 17:00.960
But it could, it could have been the same model doing both actually.

17:00.960 --> 17:01.960
Okay.

17:01.960 --> 17:02.960
Yeah.

17:02.960 --> 17:07.640
So it is sort of a question of like what the goal of the, of the prediction is.

17:07.640 --> 17:12.240
So if your goal is to be able to show these heat maps and sort of give an idea to someone

17:12.240 --> 17:15.720
of why you made these predictions that maybe a network like the soft attention network is

17:15.720 --> 17:16.800
the way to go.

17:16.800 --> 17:20.360
And the, and the accuracy is actually only slightly worse than if you use a much bigger model

17:20.360 --> 17:23.360
like the inception V3.

17:23.360 --> 17:24.360
Okay.

17:24.360 --> 17:28.280
And did you, did you test a bunch of different models or did you start with, you know, start

17:28.280 --> 17:33.720
and finish with inception, you know, with some prior knowledge that it would probably

17:33.720 --> 17:35.120
perform the best?

17:35.120 --> 17:36.120
Right.

17:36.120 --> 17:42.600
So in terms of broad like architecture, what we found is that pretty much in every problem

17:42.600 --> 17:46.400
that we tried, the inception architecture simply works the best.

17:46.400 --> 17:50.320
And so it's hard to move away from it because it, it seems to predominantly always

17:50.320 --> 17:51.320
work the best.

17:51.320 --> 17:57.240
You know, and this is the network that's, you know, running lots of products all around

17:57.240 --> 17:59.280
Google, such as Google Photos and things.

17:59.280 --> 18:04.240
And so it's simply the case that when you send it a lot of images, that architecture seems

18:04.240 --> 18:05.560
to do the best.

18:05.560 --> 18:10.240
Now there's lots and lots of hyper parameter optimizations that can be done to that network.

18:10.240 --> 18:14.520
Things like, you know, the sizes of things as you grow out the layers and all kinds of

18:14.520 --> 18:18.760
things that we definitely explored as part of this, as part of this work.

18:18.760 --> 18:23.080
And predominantly, the inception architecture seems to work the best across a wide variety

18:23.080 --> 18:24.080
of problems.

18:24.080 --> 18:28.000
And that's true, not only in medical imaging, it's also working for us in genomics and

18:28.000 --> 18:31.360
radiology and other other things.

18:31.360 --> 18:34.800
One of the other things I noticed was that you use transfer learning here.

18:34.800 --> 18:36.480
Can you talk a little bit about that?

18:36.480 --> 18:37.480
Yeah.

18:37.480 --> 18:43.240
So it's a pretty standard technique nowadays for when you don't have as much data as you

18:43.240 --> 18:44.720
would training data as you would like.

18:44.720 --> 18:50.680
And so there's actually a benefit to training this model on cats and dogs and pictures of

18:50.680 --> 18:53.280
flowers and buildings and things.

18:53.280 --> 18:58.040
And then starting from those weights and telling the model, you know, forget about cats and

18:58.040 --> 19:00.440
dogs now and actually learn about retinas.

19:00.440 --> 19:05.320
And now you're going to start from those weights, but still update as you look at millions

19:05.320 --> 19:07.320
and millions of retinas.

19:07.320 --> 19:12.760
It sounds like the fact that that, you know, helped you, you know, bootstrap your training

19:12.760 --> 19:13.760
process.

19:13.760 --> 19:14.760
That's not a surprise.

19:14.760 --> 19:15.760
You do that a lot as well.

19:15.760 --> 19:16.760
Yeah.

19:16.760 --> 19:22.080
So that technique is used actually quite a bit across products here and across products

19:22.080 --> 19:23.720
basically everywhere now.

19:23.720 --> 19:28.720
And it's a, it's a small boost, certainly having more retinas pictures of retinas would

19:28.720 --> 19:33.240
be way better than having pictures of cats and dogs, but it is the fact that these retinas

19:33.240 --> 19:38.120
are actually closer to natural images than they are to random images.

19:38.120 --> 19:43.400
And so you do get, you do get some boost by training on doing this pre-training on other

19:43.400 --> 19:44.400
data sets.

19:44.400 --> 19:52.640
Yeah, I had a conversation with someone recently, I forget, I forget the specific context.

19:52.640 --> 19:57.160
We didn't have a chance to go into a lot of detail, but he made kind of a passing comment

19:57.160 --> 20:01.680
that transfer learning, you know, almost like transfer learning had been debunked or something

20:01.680 --> 20:04.400
like that or it doesn't work in practice.

20:04.400 --> 20:08.760
Clearly, you know, it does for these kinds of problems and you see it a lot.

20:08.760 --> 20:13.120
Do you have any, you know, care to guess at what that might be referring to?

20:13.120 --> 20:18.960
Have you seen, you know, frustrations in applying it to certain types of problems or anything

20:18.960 --> 20:19.960
like that?

20:19.960 --> 20:20.960
Yeah.

20:20.960 --> 20:21.960
A couple of things.

20:21.960 --> 20:25.440
So one is that the benefit you're going to get is actually much, much smaller than you

20:25.440 --> 20:26.440
would like.

20:26.440 --> 20:31.560
You know, it's not like you're adding millions of more training points to your data

20:31.560 --> 20:35.880
set, you're actually, it's a much smaller factor because it is the fact that you want these

20:35.880 --> 20:40.200
models to be optimized to learn about retinas, right, and the patterns in retinas.

20:40.200 --> 20:45.800
But it's just a matter of sort of bootstrapping those image like features and maybe the lower

20:45.800 --> 20:49.120
layers of the network and you just sort of get some benefit from that.

20:49.120 --> 20:52.720
So that's the first thing is that the benefits actually much smaller than you would hope for.

20:52.720 --> 20:58.440
The next thing is, it kind of depends on how close your problem is to the, to the problem

20:58.440 --> 21:00.520
in the of these natural images.

21:00.520 --> 21:06.760
So for example, if you're, if your data set is sort of synthetic images or experimental

21:06.760 --> 21:12.360
images or, or things that don't look like pictures of cats and dogs, then it will help

21:12.360 --> 21:14.520
much, much less.

21:14.520 --> 21:17.760
And that, I would expect that to be the case with these retinal images.

21:17.760 --> 21:24.400
But you, you kind of characterize them as, as being similar in some ways is, is it just

21:24.400 --> 21:28.360
kind of the naturalness of the image as opposed to, for example, like a computer generated

21:28.360 --> 21:29.360
image?

21:29.360 --> 21:30.360
Exactly.

21:30.360 --> 21:32.440
These definitely aren't random images.

21:32.440 --> 21:37.640
These are, they have lighting features that you could maybe learn, they have, they certainly

21:37.640 --> 21:43.000
have color correlations and spatial correlations that you could learn.

21:43.000 --> 21:47.720
So I do think it's the case that these images are closer to natural images than to just sort

21:47.720 --> 21:50.320
of starting from random weights, yeah.

21:50.320 --> 21:59.000
And when you say writing features, do you mean like metadata, like capture time and maybe

21:59.000 --> 22:03.400
capture location or things like that that are kind of burned into the image?

22:03.400 --> 22:07.360
You know, it could be because these are, you know, these are cameras, a fundoscope is

22:07.360 --> 22:08.360
a camera.

22:08.360 --> 22:13.040
And so there, there are definitely a camera related artifacts that you could probably

22:13.040 --> 22:17.560
learn and pick up on, even if they were, you know, this camera versus, versus other

22:17.560 --> 22:20.480
camera images of natural images.

22:20.480 --> 22:23.720
Maybe I, maybe I misheard you when you said writing.

22:23.720 --> 22:28.680
If in fact that, you know, there were like optical, you know, characters writing on these

22:28.680 --> 22:32.920
images like the, you know, patient name or whatever, you know, would you, would you have

22:32.920 --> 22:38.280
thought to do, you know, either mask those or, you know, do you worry about the network

22:38.280 --> 22:46.680
learning like, I don't know, you know, times, time of year, you know, types of names,

22:46.680 --> 22:47.680
that kind of thing.

22:47.680 --> 22:48.680
Yeah.

22:48.680 --> 22:52.640
So definitely when you're working with medical images, that's something to definitely worry

22:52.640 --> 22:53.640
about.

22:53.640 --> 22:57.800
So we have this process by which we try to de-identify those images.

22:57.800 --> 23:03.640
So we would first run it through a process to sort of ask, is there, is there writing

23:03.640 --> 23:04.640
on the images?

23:04.640 --> 23:06.080
Is one common way to do it?

23:06.080 --> 23:07.080
So.

23:07.080 --> 23:08.080
Okay.

23:08.080 --> 23:12.480
And these are standard sort of like OCR kind of techniques like, do you see any kind of

23:12.480 --> 23:13.480
writing?

23:13.480 --> 23:14.480
What is the writing?

23:14.480 --> 23:16.480
And if there is, we would mess that out.

23:16.480 --> 23:17.480
Yeah.

23:17.480 --> 23:18.480
Interesting.

23:18.480 --> 23:25.160
So do the, the transfer learning you mentioned kind of using the, using a network trained

23:25.160 --> 23:30.320
on image net and kind of bootstrapping with the, the weights from that network.

23:30.320 --> 23:34.560
I've also talked to folks about kind of a, you know, what I guess is another kind of

23:34.560 --> 23:38.240
transfer learning where you're kind of training a network and then kind of chopping off the

23:38.240 --> 23:44.520
last layers and replacing the last layers with something else that's more specific to

23:44.520 --> 23:45.520
your domain.

23:45.520 --> 23:48.280
Do you, I guess this is a bit of an aside.

23:48.280 --> 23:51.840
Are there kind of more specific names for each of those two things or would you, would

23:51.840 --> 23:54.320
you call them both transfer learning?

23:54.320 --> 23:57.920
I think they're both called transfer learning.

23:57.920 --> 24:02.320
We, we do that procedure as well where we're chopping off, you know, if you have a network

24:02.320 --> 24:06.760
trained on thousands of classes such as cats and dogs and buildings and flowers, then

24:06.760 --> 24:12.200
it's natural to sort of chop off that last layer and add sort of, you know, predictions

24:12.200 --> 24:18.320
for, you know, gender, which is, you know, a classifier prediction and then age, which

24:18.320 --> 24:22.160
is a floating point regression prediction, prediction, and you sort of add these extra layers

24:22.160 --> 24:23.600
to the top of the network.

24:23.600 --> 24:26.960
And then those layers have to be pre-initialized with sort of random weights.

24:26.960 --> 24:31.720
So those things definitely need to be learned as you optimize the, as you optimize the network.

24:31.720 --> 24:34.600
But I think in both cases, it's sort of called transfer learning.

24:34.600 --> 24:39.640
And there are lots of other machine learning techniques we try to do to augment the data.

24:39.640 --> 24:45.200
There's like random flipping of the images and, and brightness, random brightness changes

24:45.200 --> 24:46.200
and things.

24:46.200 --> 24:50.880
And these are ways to sort of augment your training data in small ways.

24:50.880 --> 24:58.440
And I imagine that you have a standard kind of script or, you know, or a pipeline that

24:58.440 --> 25:04.280
you send images through that does all this or do you, more kind of hand apply these things

25:04.280 --> 25:06.920
on a problem by problem basis.

25:06.920 --> 25:13.000
But now it's somewhat standard because these techniques like, you know, flipping and rotations

25:13.000 --> 25:17.720
and brightness changes, those are, those are can be applied to any kind of image, whether

25:17.720 --> 25:20.560
it's an image of a retina or other things.

25:20.560 --> 25:24.440
And so those techniques are pretty standard and everything we've done is built on top of

25:24.440 --> 25:25.440
TensorFlow.

25:25.440 --> 25:29.800
And these are using TensorFlow ops to do those image manipulations.

25:29.800 --> 25:32.760
But the code is shared amongst many, many teams here now.

25:32.760 --> 25:37.960
And so if things like flipping and changing brightness and like, are you also doing like

25:37.960 --> 25:43.440
kind of random crops and that kind of stuff is, you know, how, how many or how big is

25:43.440 --> 25:46.640
this pipeline of kind of augmentation operations?

25:46.640 --> 25:49.960
Is it a handful or is it a bunch of things?

25:49.960 --> 25:51.800
It's quite extensive.

25:51.800 --> 25:53.600
It can be, it can be a bunch of things.

25:53.600 --> 25:56.600
It sort of depends a little bit on the problem domain.

25:56.600 --> 26:01.000
So just one example is for us, the crop, the random crops and things didn't quite work

26:01.000 --> 26:05.720
out as well because the camera image is actually pretty standard in terms of its size and

26:05.720 --> 26:10.600
its field of view and things because, you know, it's a patient setting where the patient

26:10.600 --> 26:12.120
puts their face into the camera.

26:12.120 --> 26:15.760
And so the field of view is actually pretty, pretty set.

26:15.760 --> 26:19.240
And these images are taken by, you know, professional ophthalmologist.

26:19.240 --> 26:24.560
And so that kind of stuff, so adding crops doesn't quite help as much.

26:24.560 --> 26:29.480
So it's a matter of sort of capturing the natural variability of the training data.

26:29.480 --> 26:34.560
And so those sort of augmentations can be turned on or off depending on what your problem

26:34.560 --> 26:36.280
domain is.

26:36.280 --> 26:43.280
You briefly mentioned the data sets and kind of how that data was sourced.

26:43.280 --> 26:48.720
And the paper, you, there, I recall you mentioning one of the data sets, the UK data

26:48.720 --> 26:49.720
source.

26:49.720 --> 26:52.960
But in the paper, you mentioned two, one from the UK and one from the US.

26:52.960 --> 26:53.960
That's right.

26:53.960 --> 26:54.960
Yeah.

26:54.960 --> 27:00.040
Um, we started primarily by looking at the UK biobank because this is, this was a pretty

27:00.040 --> 27:03.800
fun data set that had lots and lots of metadata about the patient.

27:03.800 --> 27:08.800
And then when the, when we wanted to sort of validate these predictions, we used another

27:08.800 --> 27:13.840
data set called the I-PACS data set, which is a set of images from a US tele-optimology

27:13.840 --> 27:17.840
service where, um, you know, these images are sent out.

27:17.840 --> 27:20.040
They're graded by professional ophthalmologists.

27:20.040 --> 27:23.840
And they also had, they also had some of the same metadata associated with it.

27:23.840 --> 27:26.560
So we could validate some of these predictions.

27:26.560 --> 27:32.240
One of the things that I noticed in the paper was that you, in reporting performance and

27:32.240 --> 27:38.920
result and the like, you, uh, always treated these data sets separately.

27:38.920 --> 27:45.520
And I was wondering whether, uh, you, for example, combined the data sets and evaluated

27:45.520 --> 27:50.600
performance on or trained on the combined data set and evaluated performance on kind of

27:50.600 --> 27:55.400
a randomized sample from these data sets, you know, you know, and or kind of what your

27:55.400 --> 27:58.560
general thinking is about that whole line of thought.

27:58.560 --> 28:04.000
Yeah, so we had to be a little bit careful here because some of the data sets had some

28:04.000 --> 28:07.200
of the, uh, some of the variables and others didn't.

28:07.200 --> 28:12.240
So just for an example, systolic and diastolic blood pressure, those were available in UK

28:12.240 --> 28:13.240
biobank.

28:13.240 --> 28:15.680
And so we trained on those, but they aren't available in I-PACS.

28:15.680 --> 28:19.200
And so we couldn't report the results there.

28:19.200 --> 28:22.800
We actually, you know, we couldn't train the model on that because that data wasn't available.

28:22.800 --> 28:27.560
Um, and so that's one reason why it's sort of very carefully split between the two things.

28:27.560 --> 28:33.400
Um, there are actually looks like only ages common to the two data sets and gender, yeah,

28:33.400 --> 28:34.400
age and gender.

28:34.400 --> 28:35.400
Okay.

28:35.400 --> 28:36.400
Yeah, yeah.

28:36.400 --> 28:39.800
And so, um, yeah, that's just an unfortunate feature of the data.

28:39.800 --> 28:44.200
We were still working on trying to collect other data sets, which have these other metadata

28:44.200 --> 28:45.200
available.

28:45.200 --> 28:50.400
Um, one cool thing about I put the I-PACS data set was they had HBA 1C level measured

28:50.400 --> 28:51.400
for their patients.

28:51.400 --> 28:52.400
Mm-hmm.

28:52.400 --> 28:53.560
And so this is a measure of blood glucose.

28:53.560 --> 28:58.000
Um, and so it was really cool to be able to try to predict that, but that, but that,

28:58.000 --> 29:00.880
uh, feature is not available in UK biobank.

29:00.880 --> 29:03.640
So we had to play a little, a little bit there.

29:03.640 --> 29:04.640
Mm-hmm.

29:04.640 --> 29:11.720
Uh, and it looks like, uh, that was the, the only feature that, you know, for what you

29:11.720 --> 29:15.120
didn't consistently outperform the baseline.

29:15.120 --> 29:16.120
Is that right?

29:16.120 --> 29:18.480
And what, what's your intuition around that?

29:18.480 --> 29:19.480
Yeah.

29:19.480 --> 29:23.440
So I think the, uh, the mean is sort of slightly better than the baseline, but if you look

29:23.440 --> 29:27.320
at, um, 90% conference intervals, they're, they're basically overlapping.

29:27.320 --> 29:31.040
And so what I think it's saying is that there are some features in the images, which are

29:31.040 --> 29:36.480
correlated with HBA 1C, but, you know, it's not, it's not a slam dunk prediction that

29:36.480 --> 29:39.200
you would want to sort of, uh, bet the house on.

29:39.200 --> 29:44.560
Mm-hmm, as is the case with, you know, age where you've got this, you know, very accurate,

29:44.560 --> 29:45.960
uh, set of predictions.

29:45.960 --> 29:46.960
Right.

29:46.960 --> 29:47.960
Exactly.

29:47.960 --> 29:52.240
And we tried to be very careful about producing the 95% conference intervals to give someone

29:52.240 --> 29:56.960
an idea of how, how accurate we thought these predictions were and whether they were

29:56.960 --> 30:02.000
actually, um, you know, predicting something real or just sort of regressing to the, to

30:02.000 --> 30:03.720
the mean values in the data set.

30:03.720 --> 30:04.720
Mm-hmm.

30:04.720 --> 30:09.720
So how did you produce the confidence intervals and the p values and the statistical measures

30:09.720 --> 30:12.840
like that that you, um, that you mentioned in the paper?

30:12.840 --> 30:13.840
Mm-hmm.

30:13.840 --> 30:17.040
So all the confidence intervals are done through bootstrapping of the eVAL sets.

30:17.040 --> 30:21.080
You sort of randomly select from that eVAL set many, many times and then from that you

30:21.080 --> 30:23.640
have a distribution of AUC values.

30:23.640 --> 30:26.760
And then you can report the 95% confidence interval of those.

30:26.760 --> 30:32.960
And that's true for, um, the AUCs and, and, and also the mean squared errors and things.

30:32.960 --> 30:41.320
Um, um, you, you, and, and Google more broadly alone are doing a ton of work in this area

30:41.320 --> 30:48.120
specifically applying deep learning to not just image-based predictions, but, you know,

30:48.120 --> 30:50.320
healthcare predictions more broadly.

30:50.320 --> 30:56.600
But in the, you know, maybe starting specifically with the domain of image-based, uh, predict,

30:56.600 --> 31:01.480
predictions and diagnostics, you know, what are your, what are your, what's your sense

31:01.480 --> 31:06.000
of what the, you know, where are we, I guess, is the question, what are the limitations?

31:06.000 --> 31:10.040
What, what are we able to do very well right now and, and kind of, what do you think the

31:10.040 --> 31:16.040
path is to making this kind of a standard tool in the physicians toolkit?

31:16.040 --> 31:17.040
Mm-hmm.

31:17.040 --> 31:22.000
So one thing that I think that we're able to do very well right now in the medical imaging

31:22.000 --> 31:30.640
domain is to automate the diagnoses of diseases in which doctors can sort of look at an image,

31:30.640 --> 31:33.760
give you a diagnosis and be very confident in their diagnosis.

31:33.760 --> 31:39.520
So from that data, we can sort of collect many, many diagnoses from doctors, train models

31:39.520 --> 31:45.680
to replicate that performance and, and sort of automate that, that doctor-level diagnosis

31:45.680 --> 31:50.040
and make it sort of radically available, you know, throughout the world in which the people

31:50.040 --> 31:53.240
may not have access to that level of expert care.

31:53.240 --> 32:00.920
The places where we're still need work are in the sort of more researchy or experimental

32:00.920 --> 32:04.360
type predictions like the ones we're talking about here, in which we've shown that there's

32:04.360 --> 32:09.440
like some tantalizing correlations, but the data sets, they only have a couple hundred

32:09.440 --> 32:13.920
cardiovascular events, for example, and so we weren't able to prove that we're able

32:13.920 --> 32:17.880
to predict these things at high accuracy, but they're sort of tantalizing evidence that

32:17.880 --> 32:19.560
maybe there's something there.

32:19.560 --> 32:25.400
And so you could imagine a future in which this, this fundus image is actually, you know,

32:25.400 --> 32:30.000
taken like more like a, a vital sign like, like your, your, your blood pressure when you

32:30.000 --> 32:34.480
go into the doctor's office where you take this snapshot of the eye, it's a very non-invasive

32:34.480 --> 32:38.680
thing, very easy to do, and you get this sort of overall view of someone's health.

32:38.680 --> 32:42.320
And so we need, there's lots of work that needs to be done to sort of validate those predictions

32:42.320 --> 32:47.120
and those ideas, but that that's one, one area that I could go.

32:47.120 --> 32:52.320
And in terms of making this more sort of broadly accepted or available, I think one way to

32:52.320 --> 32:57.000
do this is to focus on this explanation of the predictions, and so it gives sort of

32:57.000 --> 33:03.720
get doctors more like on your side about the prediction, like by providing an explanation,

33:03.720 --> 33:10.240
they sort of can trust it, they can believe in what the model is doing.

33:10.240 --> 33:15.560
Did you out of curiosity, did you interface directly with the doctors for this study?

33:15.560 --> 33:20.600
Absolutely, yeah, we work with, we have lots of doctors that we work with through our,

33:20.600 --> 33:25.640
our eye disease initiatives, and we also work with cardiologists as well to sort of bounce

33:25.640 --> 33:31.600
ideas off of them, show them, and early results, and sort of get their feedback on the predictions,

33:31.600 --> 33:32.600
yeah.

33:32.600 --> 33:39.160
And were there, were there any particular, you know, conversations or reactions that

33:39.160 --> 33:46.400
jumped out at you for, you know, when you presented, maybe, you know, with and without presenting,

33:46.400 --> 33:53.000
you know, the, the soft attention results like, did you have those the entire time or

33:53.000 --> 33:58.560
did you have some conversations with doctors, you know, before you had the explanations

33:58.560 --> 34:03.280
from the attention mechanisms and some after and like, can you, did you see a market difference

34:03.280 --> 34:05.120
in kind of the way they react?

34:05.120 --> 34:06.640
Yeah, absolutely.

34:06.640 --> 34:11.760
So you can imagine trying to show a doctor, tell them that you have an AUC of.97 for

34:11.760 --> 34:15.480
gender, and they kind of, they kind of laugh at you, they don't believe it.

34:15.480 --> 34:19.880
But then when you show them that heat map and sort of show that it's focusing on the

34:19.880 --> 34:23.720
optic disc or maybe features around the optic disc, then they say, oh, yeah, of course we

34:23.720 --> 34:27.000
knew that, that, that, of course you can see that.

34:27.000 --> 34:33.440
And so it really does, by, by, by showing where in the image, the model is using to make

34:33.440 --> 34:38.840
this prediction, it really does provide a level of, of trust and also, you know, a level

34:38.840 --> 34:41.400
of validity to the results.

34:41.400 --> 34:45.880
Can you tell us a little bit about the, the broader research landscape in this area?

34:45.880 --> 34:50.040
What are, you know, you've mentioned a bunch of the research that you're doing.

34:50.040 --> 34:54.480
Where do you think the most interesting and important activity in this space is happening

34:54.480 --> 34:56.480
right now?

34:56.480 --> 35:02.360
So the future work for, for the cardiovascular effort is, is certainly in validating

35:02.360 --> 35:09.600
in more data, so more data sets, trying to find data sets which have more cardiovascular

35:09.600 --> 35:12.520
events so we can sort of validate those approaches.

35:12.520 --> 35:17.240
And I think as we gather together more and more data, that we'll be able to, you know,

35:17.240 --> 35:21.280
narrow those, those confidence intervals and try to decide if this is actually working.

35:21.280 --> 35:25.800
So that's the, so the future in that respect is certainly gathering more data.

35:25.800 --> 35:29.680
And that's true across a wide variety of domains actually.

35:29.680 --> 35:34.280
We have a lot of, you know, initial predictions and initial results, but we need lots and

35:34.280 --> 35:37.280
lots more data in order to validate those things.

35:37.280 --> 35:41.680
You know, that almost makes me think that, that the, the take is that, you know, we've

35:41.680 --> 35:45.840
kind of solves a lot of the machine learning bits of this.

35:45.840 --> 35:50.200
You know, we've got inception, we've got, you know, transfer learning from ImageNet,

35:50.200 --> 35:56.560
we've got, you know, attention mechanisms and the like to, to help with explainability.

35:56.560 --> 36:01.360
It sounds like you're, you're saying that, you know, in terms of kind of core, you

36:01.360 --> 36:06.360
know, evolving the way we think about our capabilities on the machine learning side,

36:06.360 --> 36:11.120
you know, we're kind of as far as we need to be and now we just need more data.

36:11.120 --> 36:14.560
Is that taking it too far or do you agree with that?

36:14.560 --> 36:21.560
I think in terms of medical imaging, I think that's more or less true.

36:21.560 --> 36:25.840
There's still research that's happening for sure on better ways, better and more valid

36:25.840 --> 36:31.520
ways to do the model explanation, ways to incorporate many more predictions simultaneously

36:31.520 --> 36:32.920
and things like this.

36:32.920 --> 36:38.080
But I think for, for medical imaging, I think there's definitely a path forward that we see

36:38.080 --> 36:41.280
that is more or less guaranteed to work at this point.

36:41.280 --> 36:43.600
We've done the early work to show that it's going to work.

36:43.600 --> 36:45.360
We just need to execute.

36:45.360 --> 36:51.520
I think the areas in which it's less clear are in other domains, so things like working

36:51.520 --> 36:55.880
with medical records and things like working with the genomics data and in other sort

36:55.880 --> 36:59.280
of health, health biomedicine data.

36:59.280 --> 37:05.760
There, there's still some more fundamental work that needs to happen around the network

37:05.760 --> 37:08.840
architectures and machine learning models and alike.

37:08.840 --> 37:09.840
That's right.

37:09.840 --> 37:10.840
Exactly.

37:10.840 --> 37:14.080
Because there, for example, on the medical record space, you're working with sort of time

37:14.080 --> 37:17.480
as a dimension that you need to add to the model in some way.

37:17.480 --> 37:19.640
There's a lot of unstructured notes and things.

37:19.640 --> 37:24.680
So there's sort of a much wider variety of model architectures that are needed to be

37:24.680 --> 37:27.200
explored there, for sure.

37:27.200 --> 37:28.200
Interesting.

37:28.200 --> 37:33.480
In terms of that data challenge, how involved were you in that?

37:33.480 --> 37:37.640
Did you come on this project and the data was there and it sounds like you've used this

37:37.640 --> 37:39.680
data set quite a bit.

37:39.680 --> 37:45.400
But do you have any experience trying to source this data and does that lead you, leave

37:45.400 --> 37:51.240
you with any advice for folks that are interested in doing work in this field, but need to get

37:51.240 --> 37:52.840
their hands on a data set?

37:52.840 --> 37:53.840
Yeah.

37:53.840 --> 37:56.960
So data is definitely the key.

37:56.960 --> 38:00.240
You can't really, can't do anything in this data.

38:00.240 --> 38:05.240
And so, you know, when this project started, it started with literally the word UK biobank.

38:05.240 --> 38:06.640
Is there something interesting there?

38:06.640 --> 38:07.640
Yes or no?

38:07.640 --> 38:08.640
We'd have no idea.

38:08.640 --> 38:13.440
And so it was a matter of sort of starting from that, with that data from scratch and sort

38:13.440 --> 38:18.920
of deciding what was in it and learning how all the different fields are encoded and

38:18.920 --> 38:23.640
learning like when the data was trustworthy and which fields were encoded in different

38:23.640 --> 38:24.640
ways and things.

38:24.640 --> 38:29.200
And so there's definitely a lot of work in terms of cleaning up the data, parsing and

38:29.200 --> 38:33.480
downloading and making it widely available to the team and things.

38:33.480 --> 38:37.760
So there's a lot of engineering work and data science work that goes into that for sure.

38:37.760 --> 38:43.800
Can you maybe give us a sense of the, you know, the timeframes involved in a project like

38:43.800 --> 38:44.800
this?

38:44.800 --> 38:48.920
It sounds like you didn't start from a cold start, so it may be a little bit difficult

38:48.920 --> 38:53.800
because you've got, you know, certainly a tool chain in place, this data set you've

38:53.800 --> 38:55.080
worked with for a while.

38:55.080 --> 39:02.600
But in terms of, you know, starting from, you know, the idea to research this project

39:02.600 --> 39:06.760
to publishing the paper recently, like, what was that time frame like?

39:06.760 --> 39:07.760
Mm-hmm.

39:07.760 --> 39:10.720
So it's something, I think, a little over a year.

39:10.720 --> 39:17.480
So it's roughly, you know, four to six months of, you know, digging through the data and

39:17.480 --> 39:24.560
deciding what's there, you know, understanding why when I load up the JPEG, it looks, you

39:24.560 --> 39:28.920
know, reversed or whatever and learning the specifics of the encoding and things like

39:28.920 --> 39:33.600
this, you know, that takes time diving into the data.

39:33.600 --> 39:38.280
And so that's, you know, roughly four to six months of digging in and trying things.

39:38.280 --> 39:45.000
I think our earliest predictions on the gender and AUC were within maybe three to four months.

39:45.000 --> 39:48.240
And then you dive into, then you branch out and look at these other predictions like

39:48.240 --> 39:51.840
blood pressure and BMI and cardiovascular events and things.

39:51.840 --> 39:56.120
And, you know, and doing this sort of statistical analysis to prove that that was actually working,

39:56.120 --> 39:59.400
that probably took another three to four months or something.

39:59.400 --> 40:04.480
And then there's, of course, a lot of work that goes into publishing manuscript.

40:04.480 --> 40:08.120
And so, you know, we went through rounds of peer reviews and things to get that out

40:08.120 --> 40:09.120
there.

40:09.120 --> 40:10.120
Sure.

40:10.120 --> 40:11.120
Sure.

40:11.120 --> 40:14.320
And this particular data set, the Biobank data set, or either of them, really, are they

40:14.320 --> 40:18.720
publicly available or are there hoops that you needed to jump through to get access to

40:18.720 --> 40:19.720
them?

40:19.720 --> 40:22.320
They're publicly available to qualified researchers.

40:22.320 --> 40:29.200
It's a process of applying for access and basically stating that you have a stated

40:29.200 --> 40:34.200
research goal in mind and then they decide to grant you access to that data and you can

40:34.200 --> 40:35.200
use it.

40:35.200 --> 40:36.200
Yeah.

40:36.200 --> 40:37.200
Interesting.

40:37.200 --> 40:42.200
And are there other interesting data sets on your radar?

40:42.200 --> 40:43.200
Oh, okay.

40:43.200 --> 40:50.520
So, just to talk about the UK Biobank again, they're adding, currently adding genomics data

40:50.520 --> 40:51.520
to the data set.

40:51.520 --> 40:57.520
And that'll be really exciting to see what's there and what we can predict from it.

40:57.520 --> 41:03.040
And in terms of fundus imagery, there are other data collections that are available.

41:03.040 --> 41:07.960
You know, they're these things like sort of Kaggle-like competitions or machine learning

41:07.960 --> 41:13.080
competitions and like a few of them are on fundus imagery and so those data sets are

41:13.080 --> 41:14.080
available.

41:14.080 --> 41:15.080
Awesome.

41:15.080 --> 41:18.360
Well, Ryan, this has been super interesting.

41:18.360 --> 41:23.760
Do you have any final thoughts or words for folks or anything that we didn't cover?

41:23.760 --> 41:25.960
Any questions that I should have asked?

41:25.960 --> 41:26.960
Yes.

41:26.960 --> 41:27.960
It's been super fun.

41:27.960 --> 41:28.960
Thank you so much.

41:28.960 --> 41:34.600
One parting thing maybe is I'm super excited about the fact that this work was done in

41:34.600 --> 41:41.240
sort of feature lists kind of unbiased way in which I am certainly not an ophthalmologist

41:41.240 --> 41:46.520
and don't know anything about retinus, but we were able to sort of tell the model to

41:46.520 --> 41:49.760
use its features to learn about a patient's health.

41:49.760 --> 41:54.760
And then we surprisingly learned about a bunch of different statistical correlations that

41:54.760 --> 41:59.960
were in the data that we wouldn't have found if we had sort of went into it like sort

41:59.960 --> 42:04.640
of presupposing what the features might be and looking at things like the width of the

42:04.640 --> 42:06.160
blood vessels or things.

42:06.160 --> 42:12.160
And so I do like that this is potentially a method of scientific discovery that could

42:12.160 --> 42:16.000
be used in the future and people should think about trying that.

42:16.000 --> 42:22.720
Yeah, that sounds like a general advertisement for deep learning and you know kind of a data

42:22.720 --> 42:26.080
first as opposed to a feature first approach.

42:26.080 --> 42:27.920
Is that is that kind of where you're headed?

42:27.920 --> 42:28.920
Yeah, that's right.

42:28.920 --> 42:29.920
Awesome.

42:29.920 --> 42:30.920
Awesome.

42:30.920 --> 42:31.920
Well, once again, Ryan.

42:31.920 --> 42:32.920
Thank you so much.

42:32.920 --> 42:33.920
I appreciate you taking the time.

42:33.920 --> 42:34.920
Thank you.

42:34.920 --> 42:38.920
All right, everyone.

42:38.920 --> 42:40.640
That's our show for today.

42:40.640 --> 42:45.240
For more information on Ryan or any of the topics covered in this episode, you'll find

42:45.240 --> 42:51.120
the show notes at twimmolai.com slash talk slash 122.

42:51.120 --> 42:55.560
We heard from a lot of you over the weekend about our last episode on the reproducibility

42:55.560 --> 43:00.400
crisis and the philosophy of data and we really appreciate your comments.

43:00.400 --> 43:04.880
If you want to get in on the conversation, be sure to hit us up at Sam Charrington or

43:04.880 --> 43:10.260
at twimmolai on Twitter or via the show notes page, which we'll link to in the notes for

43:10.260 --> 43:11.960
this show.

43:11.960 --> 43:22.600
Thanks once again for listening and catch you next time.

