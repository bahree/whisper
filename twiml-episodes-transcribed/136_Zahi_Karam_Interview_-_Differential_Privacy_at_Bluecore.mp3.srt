1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,720
I'm your host Sam Charrington.

4
00:00:31,720 --> 00:00:36,480
This week on the podcast I am excited to present a series of interviews exploring the emerging

5
00:00:36,480 --> 00:00:39,280
field of differential privacy.

6
00:00:39,280 --> 00:00:43,040
Over the course of the week we'll dig into some of the very exciting research and application

7
00:00:43,040 --> 00:00:46,160
work happening right now in the field.

8
00:00:46,160 --> 00:00:51,600
In this episode I'm joined by Zahi Karam, director of data science at Bluecore, whose

9
00:00:51,600 --> 00:00:57,000
retail marketing platform specializes in personalized email marketing.

10
00:00:57,000 --> 00:01:01,720
I sat down with Zahi at the George and Partners portfolio conference last year, where he gave

11
00:01:01,720 --> 00:01:07,120
me my initial exposure to the field of differential privacy, ultimately leading to this podcast

12
00:01:07,120 --> 00:01:08,800
series.

13
00:01:08,800 --> 00:01:13,280
Zahi shared his insights into how differential privacy can be deployed in the real world

14
00:01:13,280 --> 00:01:17,680
and some of the technical and cultural challenges to doing so.

15
00:01:17,680 --> 00:01:23,080
We discussed the Bluecore use case in depth, including why and for whom they build differentially

16
00:01:23,080 --> 00:01:25,800
private machine learning models.

17
00:01:25,800 --> 00:01:29,840
Thanks to George and Partners for their continued support of the podcast and for sponsoring

18
00:01:29,840 --> 00:01:32,400
this series.

19
00:01:32,400 --> 00:01:37,200
George and Partners is a venture capital firm that invests in growth stage business software

20
00:01:37,200 --> 00:01:40,440
companies in the US and Canada.

21
00:01:40,440 --> 00:01:44,920
Most investment, George and works closely with portfolio companies to accelerate adoption

22
00:01:44,920 --> 00:01:50,120
of key technologies, including machine learning and differential privacy.

23
00:01:50,120 --> 00:01:54,680
To help their portfolio companies provide privacy guarantees to their customers, George

24
00:01:54,680 --> 00:02:00,440
and recently launched its first software product, Epsilon, which is a differentially private

25
00:02:00,440 --> 00:02:02,720
machine learning solution.

26
00:02:02,720 --> 00:02:07,600
You'll learn more about Epsilon in my interview with George and Chang Liu later this week,

27
00:02:07,600 --> 00:02:11,600
and if you find this field interesting, I'd encourage you to visit the differential

28
00:02:11,600 --> 00:02:20,320
privacy resource center they've set up at gptrs.vc slash twimmelai.

29
00:02:20,320 --> 00:02:25,440
And now on to the show.

30
00:02:25,440 --> 00:02:32,240
Alright everyone, I am here at the George and Partners portfolio conference and I am

31
00:02:32,240 --> 00:02:38,800
with Zahi Karam, Zahi is the director of data science at BluCore.

32
00:02:38,800 --> 00:02:41,600
Zahi, welcome to this week in machine learning and AI.

33
00:02:41,600 --> 00:02:44,240
It's great to be here, great, great.

34
00:02:44,240 --> 00:02:48,040
Why don't we get started by having you tell us a little bit about your background and

35
00:02:48,040 --> 00:02:52,200
how you got involved in data science and machine learning.

36
00:02:52,200 --> 00:02:59,240
So my background is a bit of an interesting one for somebody in machine learning and data

37
00:02:59,240 --> 00:03:02,920
science, I started off in electrical engineering.

38
00:03:02,920 --> 00:03:03,920
That was my undergrad.

39
00:03:03,920 --> 00:03:04,920
Go AA.

40
00:03:04,920 --> 00:03:05,920
AA?

41
00:03:05,920 --> 00:03:08,680
Yes, amazing.

42
00:03:08,680 --> 00:03:15,000
So I was in AA and I got very excited about digital signal processing and so that was

43
00:03:15,000 --> 00:03:18,040
my focus for my undergrad.

44
00:03:18,040 --> 00:03:24,840
Then for my masters, I stuck with digital signal processing, which is still very exciting.

45
00:03:24,840 --> 00:03:31,680
And then I got into speech, speech is an interesting field because it started off purely in the

46
00:03:31,680 --> 00:03:37,400
electrical engineering realm and now it has gotten into the computer science realm with

47
00:03:37,400 --> 00:03:44,600
the advent of algorithms such as deep learning.

48
00:03:44,600 --> 00:03:51,640
So I got into speech and that's how I got into machine learning, specifically my PhD was

49
00:03:51,640 --> 00:04:02,440
focused on semi-supervised methods to improve on speaker identification, okay.

50
00:04:02,440 --> 00:04:07,000
Meaning identifying or differentiating one speaker from another in a noisy kind of

51
00:04:07,000 --> 00:04:14,520
the party scenario or not in a noisy environment, specifically how can we, specifically the

52
00:04:14,520 --> 00:04:22,840
problem we were trying to solve is, can I tell given a 30 second speech utterance, can

53
00:04:22,840 --> 00:04:25,520
I tell who it is, okay.

54
00:04:25,520 --> 00:04:31,640
And in a setting like that, you've got somewhat, you've got maybe two or three utterances

55
00:04:31,640 --> 00:04:36,320
from the target speaker that you cover, but you also have a lot of additional data that

56
00:04:36,320 --> 00:04:41,320
is unlabeled, right, because it's not hard to collect speech, right, it's expensive to

57
00:04:41,320 --> 00:04:43,360
label speech, right, right.

58
00:04:43,360 --> 00:04:49,880
So how do we leverage that additional data to do a better job at speaker identification

59
00:04:49,880 --> 00:04:54,080
and really it's, it extends beyond speaker identification, that's just the application.

60
00:04:54,080 --> 00:04:55,080
Okay.

61
00:04:55,080 --> 00:04:59,640
Well, if you're gotten pretty good at that, now you can talk to your Google home or your

62
00:04:59,640 --> 00:05:04,880
Alexa and the training is, you know, just saying, okay, Google like three or four times

63
00:05:04,880 --> 00:05:08,560
and then it can differentiate the different people in the family.

64
00:05:08,560 --> 00:05:18,440
This work in general is, has benefited from these competitions that provide a data set,

65
00:05:18,440 --> 00:05:22,760
that companies and research labs from around the world compete.

66
00:05:22,760 --> 00:05:25,520
So it's very easy to get started with it.

67
00:05:25,520 --> 00:05:30,880
You've got the data, just usually a tough part, right, and then you've got a highly collaborative

68
00:05:30,880 --> 00:05:36,360
environment where everybody is, you know, there's yearly conferences and really competitions.

69
00:05:36,360 --> 00:05:39,680
So it makes for a field that evolves very quickly.

70
00:05:39,680 --> 00:05:40,680
Yeah.

71
00:05:40,680 --> 00:05:43,200
So I was fortunate to be part of that experience.

72
00:05:43,200 --> 00:05:44,200
That's great.

73
00:05:44,200 --> 00:05:46,880
Yeah, and it was a highly collaborative experience.

74
00:05:46,880 --> 00:05:55,960
So that was my PhD, then I went to University of Michigan for a postdoc and there I wanted

75
00:05:55,960 --> 00:06:02,880
to stay in the machine learning world but move away from speech and get into medical applications

76
00:06:02,880 --> 00:06:11,600
unfortunately, speech pulled me back and it turns out there's an interesting application

77
00:06:11,600 --> 00:06:20,840
in psychiatry where bipolar patients, if you are a person with bipolar disorder, you

78
00:06:20,840 --> 00:06:27,640
can be, have, be normal most of the time but you could also go manic and when you go

79
00:06:27,640 --> 00:06:37,720
manic, you'll do crazy stuff like why Mercedes, quit your job, that's naked on tables and

80
00:06:37,720 --> 00:06:41,440
you're not aware that you're doing these things, you know.

81
00:06:41,440 --> 00:06:47,640
And so that could have a very negative effect on your life, you could, it takes on average

82
00:06:47,640 --> 00:06:53,960
two years to get back to your normal life after a manic episode and on the others and you

83
00:06:53,960 --> 00:07:01,600
could also go get depressed and then we all know the negative side of depression.

84
00:07:01,600 --> 00:07:06,480
So you've got this spectrum and the problem with bipolar disorder, when you're depressed,

85
00:07:06,480 --> 00:07:09,240
you don't want to seek help because you're depressed and when you're manic, you don't

86
00:07:09,240 --> 00:07:11,920
know you're manic, so you don't seek help.

87
00:07:11,920 --> 00:07:17,000
So how do we identify ahead of time that somebody's going manic with the press, so that's

88
00:07:17,000 --> 00:07:24,200
a problem and it turns out that speech is an interesting, let's say, biomarker for that.

89
00:07:24,200 --> 00:07:31,320
The psychiatry we worked with left to say that I'll have family members of people with

90
00:07:31,320 --> 00:07:37,520
bipolar disorder tell me, doctor, I knew he was going to go manic because I heard it in

91
00:07:37,520 --> 00:07:42,880
his voice or I called her on the phone and she sounded depressed.

92
00:07:42,880 --> 00:07:43,880
So I knew she was good.

93
00:07:43,880 --> 00:07:47,760
Not just an indicator, but a predictive indicator.

94
00:07:47,760 --> 00:07:54,640
So that's what we based the research on and so we spent about three years between writing

95
00:07:54,640 --> 00:07:59,760
an application, a cell phone application that recorded only your side of the conversation

96
00:07:59,760 --> 00:08:04,520
because it's illegal in some states to record both sides of that consent.

97
00:08:04,520 --> 00:08:10,280
So only record your side of the conversation and analyze it for mood symptoms and then

98
00:08:10,280 --> 00:08:19,040
we gave those cell phones to bipolar patients and tracked the phone calls over a year and

99
00:08:19,040 --> 00:08:25,760
then also tracked the mood symptoms over that year and then build a system that could

100
00:08:25,760 --> 00:08:29,760
recognize when somebody was going manic or depressive.

101
00:08:29,760 --> 00:08:30,760
Interesting.

102
00:08:30,760 --> 00:08:32,440
So that was that work.

103
00:08:32,440 --> 00:08:39,680
And are the, you know, we're still like not at the core topic yet, but I'm super interested

104
00:08:39,680 --> 00:08:40,680
in this.

105
00:08:40,680 --> 00:08:47,200
Is this where the signals, the things that they said or things like pitch in tempo and

106
00:08:47,200 --> 00:08:53,520
kind of vocal characteristics or something that's like even, you know, lower level than

107
00:08:53,520 --> 00:08:54,520
that?

108
00:08:54,520 --> 00:08:55,520
Yeah.

109
00:08:55,520 --> 00:09:02,680
So it's very hard to design experiments that protect the patients.

110
00:09:02,680 --> 00:09:07,240
And that is the most important thing that you have to do when you design an experiment.

111
00:09:07,240 --> 00:09:12,920
Make sure that you're protecting the patient, especially a vulnerable population, like

112
00:09:12,920 --> 00:09:14,400
people with bipolar disorder.

113
00:09:14,400 --> 00:09:23,400
And so we had to make sure that the features that came out of the system could not be used

114
00:09:23,400 --> 00:09:28,400
to reconstruct what was said in these phone calls.

115
00:09:28,400 --> 00:09:34,000
So our feature extraction was designed in a way to make sure that you couldn't recover

116
00:09:34,000 --> 00:09:35,240
what was said.

117
00:09:35,240 --> 00:09:42,920
And that's important because let's say somebody goes manic and does something, then we have

118
00:09:42,920 --> 00:09:46,880
all the phone recordings and that's right.

119
00:09:46,880 --> 00:09:50,840
And what if we knew ahead of time that something was going to happen?

120
00:09:50,840 --> 00:09:56,040
It gets into a very first, you're not protecting the patient, but there's also the whole legal

121
00:09:56,040 --> 00:09:57,040
amplification.

122
00:09:57,040 --> 00:09:58,040
Right.

123
00:09:58,040 --> 00:09:59,040
Right.

124
00:09:59,040 --> 00:10:00,040
Interesting.

125
00:10:00,040 --> 00:10:01,040
Interesting.

126
00:10:01,040 --> 00:10:06,040
Okay.

127
00:10:06,040 --> 00:10:07,040
Nice.

128
00:10:07,040 --> 00:10:08,520
And so what does BluCore do?

129
00:10:08,520 --> 00:10:12,400
BluCore is a decisioning platform for commerce.

130
00:10:12,400 --> 00:10:18,120
We help marketers find their best customers and keep them for life.

131
00:10:18,120 --> 00:10:27,600
We are a SaaS company, so essentially we provide service for commerce companies and essentially

132
00:10:27,600 --> 00:10:34,520
we help their marketing team identify who to send a message to and what is the best message

133
00:10:34,520 --> 00:10:38,680
and at what time to send it and on which channel?

134
00:10:38,680 --> 00:10:49,080
And the experience, it sounds like the experience you had working on the postdoc in terms of the

135
00:10:49,080 --> 00:10:56,440
focus that needed to be placed on ensuring the privacy of the patients has had kind of

136
00:10:56,440 --> 00:11:03,080
immediate ramifications on your work at BluCore and has led you to doing some pretty interesting

137
00:11:03,080 --> 00:11:05,280
things around privacy.

138
00:11:05,280 --> 00:11:06,280
Yes.

139
00:11:06,280 --> 00:11:13,160
I wouldn't say there's a direct connection between the two, but certainly I think that we

140
00:11:13,160 --> 00:11:19,000
should always be thinking about how can we make sure that we're preserving privacy in

141
00:11:19,000 --> 00:11:26,000
the data that we collect in a way that protects our customers, commerce companies and protects

142
00:11:26,000 --> 00:11:27,800
their customers, right?

143
00:11:27,800 --> 00:11:33,960
And in the medical field, it's obviously even you could argue more important to protect

144
00:11:33,960 --> 00:11:37,880
the patients and the vulnerable population that we are trying to help.

145
00:11:37,880 --> 00:11:40,840
So yes, there's definitely that parallel there.

146
00:11:40,840 --> 00:11:43,960
The approaches are very different in each of them.

147
00:11:43,960 --> 00:11:49,440
In the medical field, we were using a lot of encryption, a lot of feature design that

148
00:11:49,440 --> 00:11:58,520
protected the content, what we said, at BluCore, we are using the financial privacy to try

149
00:11:58,520 --> 00:12:08,160
and build models that do not leak any information about the customers that we use to build those

150
00:12:08,160 --> 00:12:09,160
models with.

151
00:12:09,160 --> 00:12:10,160
Okay.

152
00:12:10,160 --> 00:12:15,320
And when you say customers, you're referring to your customers or your customers' customers

153
00:12:15,320 --> 00:12:16,320
are both.

154
00:12:16,320 --> 00:12:17,320
Both.

155
00:12:17,320 --> 00:12:18,320
Okay.

156
00:12:18,320 --> 00:12:19,320
Both.

157
00:12:19,320 --> 00:12:25,440
But it's a great question and it's a very interesting question and one that we are figuring

158
00:12:25,440 --> 00:12:31,680
out as we go through this process, but ultimately we want to protect both, right?

159
00:12:31,680 --> 00:12:40,520
Where we really see the value of the financial privacy at BluCore is that we have multiple

160
00:12:40,520 --> 00:12:41,520
partners.

161
00:12:41,520 --> 00:12:46,640
We've got about 400 partners to be clear partners are our customers are the commerce

162
00:12:46,640 --> 00:12:49,200
companies that buy us.

163
00:12:49,200 --> 00:12:57,040
And each of these partners, we silo their data so that we are essentially protecting each

164
00:12:57,040 --> 00:13:02,800
commerce company's data from the other commerce company's data, right?

165
00:13:02,800 --> 00:13:09,760
Because we don't want necessarily to give, you know, if you had, let's say, to apparel

166
00:13:09,760 --> 00:13:16,080
companies that are competitors, you don't want to give, I'm going to throw out names

167
00:13:16,080 --> 00:13:21,520
like, gap, you don't want to give it expresses data, right, and vice versa, right?

168
00:13:21,520 --> 00:13:28,120
So we keep all of those kind of, there's, you know, like this base level security of you

169
00:13:28,120 --> 00:13:35,880
don't want the data leaking over from one customer to the next, but you also don't, unless

170
00:13:35,880 --> 00:13:40,560
you're, this is part of your value proposition to these customers and it's what they're buying.

171
00:13:40,560 --> 00:13:45,000
You don't want to give their competitors an advantage based on data that you've collected

172
00:13:45,000 --> 00:13:47,080
from the one partner.

173
00:13:47,080 --> 00:13:48,080
Exactly.

174
00:13:48,080 --> 00:13:50,400
We want to be very careful.

175
00:13:50,400 --> 00:13:54,880
We don't want to give their competitors an advantage throughout platform, right?

176
00:13:54,880 --> 00:13:55,880
Right.

177
00:13:55,880 --> 00:13:59,280
Essentially, you want to be fair at all of our partners, right?

178
00:13:59,280 --> 00:14:05,360
And obviously, in the contracts we have with our partners, all of that is in there, right?

179
00:14:05,360 --> 00:14:11,240
What gets interesting though is, when you're building machine learning models or data science

180
00:14:11,240 --> 00:14:17,320
models or whatever you want to call it, the more data you have, the better the model is.

181
00:14:17,320 --> 00:14:25,960
And you could argue that there is an advantage if you could break down these silos.

182
00:14:25,960 --> 00:14:33,280
And there is an advantage that data asset becomes much more powerful if you can break down

183
00:14:33,280 --> 00:14:34,280
these silos.

184
00:14:34,280 --> 00:14:43,000
Now, how can we do it and is there a way to do it in a way that doesn't give gap up on

185
00:14:43,000 --> 00:14:46,200
express, right, or express a leg up on gap?

186
00:14:46,200 --> 00:14:54,320
Is there a way to break down these silos and get better models for everyone?

187
00:14:54,320 --> 00:15:03,280
But in a way where if you took out one of the partners' data sets from that pool of data,

188
00:15:03,280 --> 00:15:05,960
the model would not be affected.

189
00:15:05,960 --> 00:15:14,200
So that means now, each partner doesn't contribute enough to that aggregate model for them

190
00:15:14,200 --> 00:15:18,000
to be leaking any of the information to the others, right?

191
00:15:18,000 --> 00:15:19,000
Interesting.

192
00:15:19,000 --> 00:15:27,720
So it's a way to think about the maybe exemplar of the problem as being, you know, if the

193
00:15:27,720 --> 00:15:35,000
model is impacted proportionally by the amount of data I'm bringing to the table, then the

194
00:15:35,000 --> 00:15:41,760
larger customers or partners are, you know, basically subsidizing the model performance

195
00:15:41,760 --> 00:15:43,880
of the smaller ones, right?

196
00:15:43,880 --> 00:15:50,360
And so is what you're saying that differential privacy solves that, helps you solve that

197
00:15:50,360 --> 00:15:51,360
problem?

198
00:15:51,360 --> 00:15:56,480
That is what we hope, differential privacy will help us solve.

199
00:15:56,480 --> 00:16:01,840
You're very right, if you think about, because the other way of doing this is to get everybody

200
00:16:01,840 --> 00:16:04,760
to sign up for a data co-op, right?

201
00:16:04,760 --> 00:16:08,360
Where you explicitly saying, I'm going to share my data with everybody else, and I get

202
00:16:08,360 --> 00:16:09,600
everybody else's data.

203
00:16:09,600 --> 00:16:12,400
And that makes a lot of sense if you've got a lot of small players.

204
00:16:12,400 --> 00:16:17,840
But at BlueCore, we've got massive enterprise customers, and then we've got smaller SMB

205
00:16:17,840 --> 00:16:18,840
customers.

206
00:16:18,840 --> 00:16:24,640
So the enterprise customer might not want to share that data and give an advantage to

207
00:16:24,640 --> 00:16:26,720
SMB customers, right?

208
00:16:26,720 --> 00:16:34,000
So how can we make it so that that is a non-issue, right?

209
00:16:34,000 --> 00:16:35,640
And it's very tricky.

210
00:16:35,640 --> 00:16:40,640
And there's this whole concept of, well, you could do data anonymization, right?

211
00:16:40,640 --> 00:16:44,760
This is how it's typically been done in the past, where you obfuscate some part of the

212
00:16:44,760 --> 00:16:45,760
data.

213
00:16:45,760 --> 00:16:47,720
And that's not quite enough, right?

214
00:16:47,720 --> 00:16:52,040
Well, this is the problem, anonymization doesn't guarantee privacy, right?

215
00:16:52,040 --> 00:16:56,800
And we've seen that over and over with examples like the release of the Netflix data and people

216
00:16:56,800 --> 00:17:01,600
were able to, even though the Netflix data had removed, are you familiar with the Netflix

217
00:17:01,600 --> 00:17:02,600
data?

218
00:17:02,600 --> 00:17:04,760
I am vaguely familiar.

219
00:17:04,760 --> 00:17:12,640
So there was a data set that was, quote unquote, anonymized, but you can infer individually

220
00:17:12,640 --> 00:17:17,160
identifiable information from the data set, was it there?

221
00:17:17,160 --> 00:17:18,160
Yeah.

222
00:17:18,160 --> 00:17:22,320
Netflix had this grand challenge that said we'll give them a million dollars to the first

223
00:17:22,320 --> 00:17:28,920
percent that can improve our recommendations by X, right?

224
00:17:28,920 --> 00:17:34,120
And so they released this data set where they had some data set of people waiting real

225
00:17:34,120 --> 00:17:43,640
data of people, their customers waiting movies, and they removed the identifier of the customer,

226
00:17:43,640 --> 00:17:44,640
right?

227
00:17:44,640 --> 00:17:47,560
So you didn't know which customer waited for that movie, right?

228
00:17:47,560 --> 00:17:51,640
So they released that and that's an anonymized data set.

229
00:17:51,640 --> 00:17:57,760
The thing is some clever researchers said, well, you know what, that's not really anonymous

230
00:17:57,760 --> 00:18:04,360
because I can use this external data source, which is IMDB, and correlate the ratings

231
00:18:04,360 --> 00:18:06,680
from IMDB with the Netflix data.

232
00:18:06,680 --> 00:18:11,560
And they were able to identify certain people on the Netflix data, right?

233
00:18:11,560 --> 00:18:14,960
So anonymization doesn't guarantee privacy, got it?

234
00:18:14,960 --> 00:18:20,120
So this recent field coming up, that's been coming up in the last 10 years, that is around

235
00:18:20,120 --> 00:18:29,120
differential privacy, whereas how can you mathematically prove privacy guarantees, right?

236
00:18:29,120 --> 00:18:35,120
And the basic concept behind this whole field is essentially you add noise.

237
00:18:35,120 --> 00:18:42,160
You add some noise that makes it hard to recover the underlying data that trained the model.

238
00:18:42,160 --> 00:18:49,840
And the basic idea behind it is, I need to be able to have two data sets that differ

239
00:18:49,840 --> 00:18:52,800
by one data point.

240
00:18:52,800 --> 00:18:56,440
One data set has that data point, the other data set does not have that data point.

241
00:18:56,440 --> 00:19:02,120
And I should not be able to tell from the resultant models that I trained using these two separate

242
00:19:02,120 --> 00:19:08,520
data sets, which of the two data sets the model was generated from, right?

243
00:19:08,520 --> 00:19:16,280
So if I have a model A, let's say logistic regression, and I use the data set that is missing

244
00:19:16,280 --> 00:19:22,120
at that point of interest, and I train logistic regression model with that, and then I use the

245
00:19:22,120 --> 00:19:25,880
data set with the point of interest, and I train logistic regression model with that.

246
00:19:25,880 --> 00:19:30,520
I should not be able to tell which of the two data sets I used, right?

247
00:19:30,520 --> 00:19:33,760
And that's the basic idea behind the financial privacy.

248
00:19:33,760 --> 00:19:41,280
When I hear a differential privacy explained, folks usually come at it from the opposite direction,

249
00:19:41,280 --> 00:19:47,480
which is, you don't want me to be able to interrogate a model and determine that individual

250
00:19:47,480 --> 00:19:50,040
X was somehow used in its training, right?

251
00:19:50,040 --> 00:19:53,240
It's kind of two sides of the same coin.

252
00:19:53,240 --> 00:19:54,240
Exactly.

253
00:19:54,240 --> 00:19:55,240
Exactly.

254
00:19:55,240 --> 00:19:58,240
It's just another way of explaining, yeah.

255
00:19:58,240 --> 00:20:03,920
So the basic premise is you inject noise at some point in the model training, whether that's

256
00:20:03,920 --> 00:20:09,960
at the feature level, at the model training level, at the output of the models.

257
00:20:09,960 --> 00:20:15,600
So that's essentially the basic concept at a very high level behind the financial privacy.

258
00:20:15,600 --> 00:20:20,480
Yeah, conceptually, I'm struggling with the idea that tens or hundreds of millions of

259
00:20:20,480 --> 00:20:26,480
people are going to have their data points in this model, and that even without differential

260
00:20:26,480 --> 00:20:33,320
privacy, I'd be able to learn anything from about one individual person with how huge the

261
00:20:33,320 --> 00:20:34,320
data set is.

262
00:20:34,320 --> 00:20:37,160
It's kind of counterintuitive for me.

263
00:20:37,160 --> 00:20:41,560
It is, I agree that it's counterintuitive, but a lot of the models that are being used

264
00:20:41,560 --> 00:20:46,720
right now have thousands of parameters in them that get tweaked.

265
00:20:46,720 --> 00:20:50,560
So for example, if you look at deep learning models between all the layers and all the

266
00:20:50,560 --> 00:20:53,360
weights, you end up with thousands of parameters, right?

267
00:20:53,360 --> 00:21:02,440
And so it is possible, for example, if you build an image recognition system and then using

268
00:21:02,440 --> 00:21:09,080
images to train that model, it is possible for some of these images that used to train

269
00:21:09,080 --> 00:21:15,880
to be for the model parameters to actually capture that image and you would be able to

270
00:21:15,880 --> 00:21:24,320
interrogate that model and get back that image or exactly the model parameters and get back

271
00:21:24,320 --> 00:21:25,320
that image.

272
00:21:25,320 --> 00:21:27,000
So while the likelihood of that is low, it's a risk.

273
00:21:27,000 --> 00:21:28,000
It's a risk.

274
00:21:28,000 --> 00:21:36,400
And the only way to gain T that can't happen is through this approach of the financial

275
00:21:36,400 --> 00:21:42,480
privacy where you inject noise and how much noise you inject and where you inject it

276
00:21:42,480 --> 00:21:49,800
is the eye of it and then you can prove to a certain bound, right?

277
00:21:49,800 --> 00:21:56,680
That this model is eventually private and it will not give up your data.

278
00:21:56,680 --> 00:22:05,040
The other thing that's a little counterintuitive for me is generally these models are, the models

279
00:22:05,040 --> 00:22:06,040
are closed.

280
00:22:06,040 --> 00:22:09,960
I don't have access to the models, the model parameters, things like that.

281
00:22:09,960 --> 00:22:19,880
Is it that and maybe it's both of these that AI can learn by interrogating the model

282
00:22:19,880 --> 00:22:28,840
just via regular inference and or B that the model parameters are leakier than I might

283
00:22:28,840 --> 00:22:29,840
otherwise think.

284
00:22:29,840 --> 00:22:33,880
I can look in the memory of my phone and see the model parameters or something like that.

285
00:22:33,880 --> 00:22:39,560
So the leakiness of the model parameters really comes up when people open source a lot

286
00:22:39,560 --> 00:22:40,880
of these models, right?

287
00:22:40,880 --> 00:22:46,840
Like now when Google might open source the image recognition stack, right?

288
00:22:46,840 --> 00:22:50,280
And then that deep learning model is essentially what the open source thing is, is the way

289
00:22:50,280 --> 00:22:51,280
to.

290
00:22:51,280 --> 00:22:52,280
Okay.

291
00:22:52,280 --> 00:22:53,280
And someone's picture might be in there.

292
00:22:53,280 --> 00:22:54,280
Yeah.

293
00:22:54,280 --> 00:22:55,280
Okay.

294
00:22:55,280 --> 00:23:01,440
So, but the alternative is somebody could also interrogate that model and through that

295
00:23:01,440 --> 00:23:09,520
interrogation, identify, uncover some of the training data that was used.

296
00:23:09,520 --> 00:23:16,600
Being a sophisticated enough approach that interrogates the model enough and you know

297
00:23:16,600 --> 00:23:23,760
with a, you can design a coverage, I guess, of a way to interrogate the model to isolate.

298
00:23:23,760 --> 00:23:25,280
Exactly.

299
00:23:25,280 --> 00:23:30,960
And you can also, the other goal of the French privacy is it's not just interrogating that

300
00:23:30,960 --> 00:23:42,040
model but also a malicious party might use external data side information to know how to design

301
00:23:42,040 --> 00:23:48,160
a way to interrogate that model to figure out some, if somebody was in the data set or

302
00:23:48,160 --> 00:23:51,720
what that person's features were in that data set.

303
00:23:51,720 --> 00:23:53,640
So it's, it's three ways.

304
00:23:53,640 --> 00:23:58,040
You can either look under the hood and the parameters might leak some information.

305
00:23:58,040 --> 00:24:03,360
You can just directly interrogate the model or you can leverage side data, right?

306
00:24:03,360 --> 00:24:07,760
So for example, in the Netflix example, if you just looked at the Netflix data by itself,

307
00:24:07,760 --> 00:24:09,640
you couldn't uncover anybody.

308
00:24:09,640 --> 00:24:16,520
But then when you, a malicious agent could go to IMDB and then use that to figure out,

309
00:24:16,520 --> 00:24:17,520
right?

310
00:24:17,520 --> 00:24:21,080
So there's that aspect too.

311
00:24:21,080 --> 00:24:22,080
Okay.

312
00:24:22,080 --> 00:24:26,440
And this is the tricky part and you're asking the right questions, right?

313
00:24:26,440 --> 00:24:32,840
We can't protect against everything and we can't foresee all the ways that a malicious

314
00:24:32,840 --> 00:24:37,040
agent could try and uncover this information.

315
00:24:37,040 --> 00:24:43,200
So all these protections we try and put, whether it's anonymization or black boxing

316
00:24:43,200 --> 00:24:44,200
get, right?

317
00:24:44,200 --> 00:24:45,840
So you can't look at the parameters.

318
00:24:45,840 --> 00:24:52,480
All of these are good, they're useful, but they don't guarantee and the way to guarantee

319
00:24:52,480 --> 00:24:55,280
is again through this concept of the French privacy, right?

320
00:24:55,280 --> 00:24:58,320
So you are asking the right questions.

321
00:24:58,320 --> 00:25:03,920
In most cases, especially smaller models with fewer parameters and if you have a lot of

322
00:25:03,920 --> 00:25:10,200
training data, the likelihood that somebody could get at the individual training samples,

323
00:25:10,200 --> 00:25:15,040
this is pretty low, sure, but the French privacy gives us a guarantee.

324
00:25:15,040 --> 00:25:17,520
So now we are, we are comfortable.

325
00:25:17,520 --> 00:25:22,120
We can say I am comfortable that this model is not going to be information.

326
00:25:22,120 --> 00:25:29,040
Is differential privacy or at least the kind of the mathematical framework that has been

327
00:25:29,040 --> 00:25:37,240
developed around it also allow us to assess a model and put some bounds around kind of

328
00:25:37,240 --> 00:25:41,600
the risk for a model that for what, you know, pre differential privacy?

329
00:25:41,600 --> 00:25:47,520
It's, it's very much focused on the, once you decide how and where you want to inject

330
00:25:47,520 --> 00:25:50,800
noise, what are the, what are the guarantees after that?

331
00:25:50,800 --> 00:25:51,800
Okay.

332
00:25:51,800 --> 00:25:57,320
So how do you see that fitting into the mix at BluCore?

333
00:25:57,320 --> 00:26:02,160
So at BluCore, we have a, there's a bit of a difference between how we're trying to

334
00:26:02,160 --> 00:26:07,560
use differential privacy and how it's typically thought of, right?

335
00:26:07,560 --> 00:26:14,720
Because there is, our goal is to make sure that we can, or to find a way to aggregate data

336
00:26:14,720 --> 00:26:21,360
across these different silos in a way that protects each of our partners data, right?

337
00:26:21,360 --> 00:26:27,720
And, and that's where, that's why it's, it's a bit interesting, right?

338
00:26:27,720 --> 00:26:30,760
It's, it's a space that isn't that explored.

339
00:26:30,760 --> 00:26:35,960
All of the whole differential privacy space is young and then this is an application of

340
00:26:35,960 --> 00:26:39,840
the differential privacy that is a bit novel, right?

341
00:26:39,840 --> 00:26:46,200
So it's a, it's an interesting one and, and if, and if we are able to do aggregation

342
00:26:46,200 --> 00:26:52,240
in a differential private manner, then that will make our models that much better.

343
00:26:52,240 --> 00:26:57,560
It will make the value of our data that much higher and not at the expense of any individual

344
00:26:57,560 --> 00:26:58,560
partner.

345
00:26:58,560 --> 00:26:59,560
Exactly.

346
00:26:59,560 --> 00:27:03,720
And ultimately our partners will benefit, you know, and in the end, we exist to serve

347
00:27:03,720 --> 00:27:04,720
our partners.

348
00:27:04,720 --> 00:27:05,720
Right.

349
00:27:05,720 --> 00:27:08,240
That's why they do business with us.

350
00:27:08,240 --> 00:27:14,480
And so how can we make our models better, provide them value without violating any, any

351
00:27:14,480 --> 00:27:20,960
contracts, we have them, any, any, whether they are written contracts or, or, or trust,

352
00:27:20,960 --> 00:27:21,960
right?

353
00:27:21,960 --> 00:27:26,240
In the end, there's a legal aspect of what you can do with the data, but there's also

354
00:27:26,240 --> 00:27:30,560
the partner's willingness to, to share that data, right?

355
00:27:30,560 --> 00:27:35,080
And, and in the end, we want to be forthcoming with our partners with how we use their data

356
00:27:35,080 --> 00:27:38,960
and, and how we combine it with other data sets, right?

357
00:27:38,960 --> 00:27:42,200
So this is, this is very much an exploratory phase.

358
00:27:42,200 --> 00:27:47,960
We are trying to understand what we can do, what we can guarantee and what are the benefits

359
00:27:47,960 --> 00:27:49,440
of it.

360
00:27:49,440 --> 00:27:55,720
And through that, we will understand what the value of it is and then go to our partners

361
00:27:55,720 --> 00:28:01,760
with that value and articulate that value and get them excited about what differential

362
00:28:01,760 --> 00:28:04,840
privacy can bring to them, right?

363
00:28:04,840 --> 00:28:06,400
And how it can protect them.

364
00:28:06,400 --> 00:28:07,400
Yeah.

365
00:28:07,400 --> 00:28:13,440
Are you, have you engaged with any of your partners around the topic yet?

366
00:28:13,440 --> 00:28:19,520
I'm imagining the, you know, getting a partner up to speed on kind of the, you know, even

367
00:28:19,520 --> 00:28:20,840
what you're trying to do.

368
00:28:20,840 --> 00:28:24,000
Like there's, you're trying to do something that's pretty innovative and you're trying

369
00:28:24,000 --> 00:28:30,320
to do something that's, you know, trying to do that thing using, you know, innovative

370
00:28:30,320 --> 00:28:37,200
and, I mean, not to overuse that word, but like it, it's cutting edge stuff.

371
00:28:37,200 --> 00:28:39,880
I guess replace one cliche with another.

372
00:28:39,880 --> 00:28:48,120
Well, it is cutting edge and this is where our partnership with Georgian has been extremely

373
00:28:48,120 --> 00:28:55,200
valuable because we are a startup, we're growing fast, we have products we have to build.

374
00:28:55,200 --> 00:29:02,000
And yet we have to make time to explore these, these, these novel and cutting edge avenues

375
00:29:02,000 --> 00:29:06,560
that could down the line provide us significant value, right?

376
00:29:06,560 --> 00:29:12,520
And our team is growing, but we're not necessarily big enough to be able to support all of these

377
00:29:12,520 --> 00:29:18,480
pie in the sky or, you know, futuristic models and ideas alone.

378
00:29:18,480 --> 00:29:25,840
So the partnership with Georgian, partners has been crucial for us exploring this space.

379
00:29:25,840 --> 00:29:27,440
The partnership is, it's twofold.

380
00:29:27,440 --> 00:29:35,120
The partnership is from a technical aspect where we've been working with great researchers

381
00:29:35,120 --> 00:29:44,840
on the Georgian impact team and they are providing a lot of technical guidance, running a lot

382
00:29:44,840 --> 00:29:45,840
of experiments.

383
00:29:45,840 --> 00:29:51,360
It's a full on engagement, it's not engagement where one party is removed, so one party

384
00:29:51,360 --> 00:29:52,800
is removed from the equation, right?

385
00:29:52,800 --> 00:29:56,000
It's a full on engagement, it's down in the weeds.

386
00:29:56,000 --> 00:30:02,240
We've had some of our data scientists come and spend time in Toronto and embed with

387
00:30:02,240 --> 00:30:04,040
the team for a few days.

388
00:30:04,040 --> 00:30:10,480
So it's a full on engagement with the Georgian partners team from a technical standpoint,

389
00:30:10,480 --> 00:30:19,040
but then there's also the legal standpoint of it all and the communication standpoint.

390
00:30:19,040 --> 00:30:24,120
How do you understand this landscape of what you can do with the data, what you can do

391
00:30:24,120 --> 00:30:29,920
with the data given the constraints of the contracts you have, and most importantly

392
00:30:29,920 --> 00:30:36,520
is how do you message what the financial privacy is to your partners, to your customers,

393
00:30:36,520 --> 00:30:41,680
so that they buy into the idea and sign off on the idea, right?

394
00:30:41,680 --> 00:30:48,040
Because even if your contracts allow for the concept of the financial private aggregation

395
00:30:48,040 --> 00:30:55,480
of data and your partners don't necessarily buy into this idea, then we can't go live

396
00:30:55,480 --> 00:30:56,480
with that.

397
00:30:56,480 --> 00:30:57,480
Right.

398
00:30:57,480 --> 00:31:01,360
So we don't want to do our partners, right?

399
00:31:01,360 --> 00:31:06,760
Our goal is to create something that's good for them, not create something that would

400
00:31:06,760 --> 00:31:10,640
lose us our trust.

401
00:31:10,640 --> 00:31:19,400
So just to run that off, Georgian has been working hard on understanding that legal landscape

402
00:31:19,400 --> 00:31:28,480
and understanding how to communicate what the financial privacy is to partners that might

403
00:31:28,480 --> 00:31:34,080
not have the technical depth to fully understand what it is and how it's different from anonymization.

404
00:31:34,080 --> 00:31:35,080
Right.

405
00:31:35,080 --> 00:31:39,640
People understand anonymization, they don't understand anonymization and they might not

406
00:31:39,640 --> 00:31:42,240
understand the risk of anonymization, right?

407
00:31:42,240 --> 00:31:45,200
And then you're telling them, well, there's this other thing called the financial privacy,

408
00:31:45,200 --> 00:31:46,200
right?

409
00:31:46,200 --> 00:31:53,600
And it's just, it took us a while to understand what it brings to the table and so you can

410
00:31:53,600 --> 00:31:59,840
imagine people that aren't technically versed in the space, it's hard to explain these things,

411
00:31:59,840 --> 00:32:00,840
right?

412
00:32:00,840 --> 00:32:08,280
I'm trying to envision kind of the process that you're pursuing to explore this.

413
00:32:08,280 --> 00:32:13,720
Are you, I mean, it sounds like it's kind of going straight from, you know, academic

414
00:32:13,720 --> 00:32:21,280
research papers to running experiments locally to, yeah, this is, this is it, we're, we're

415
00:32:21,280 --> 00:32:25,800
essentially, in everything we do, we try not to reinvent the wheel, right?

416
00:32:25,800 --> 00:32:29,440
There's, there's a lot of great academic literature out there.

417
00:32:29,440 --> 00:32:34,800
Unfortunately, the financial privacy is, is a weird one because it's a relatively young

418
00:32:34,800 --> 00:32:35,800
field.

419
00:32:35,800 --> 00:32:39,680
Ten years old is relatively young and most of the work that's happening in that field

420
00:32:39,680 --> 00:32:41,320
is happening now.

421
00:32:41,320 --> 00:32:46,360
So a lot of the papers that come out, you know, it's, it's, we have to constantly keep

422
00:32:46,360 --> 00:32:51,200
checking the literature to make sure we're up to date on, on the latest and greatest.

423
00:32:51,200 --> 00:32:55,880
And at the same time, a lot of the papers are coming out from companies and so they might

424
00:32:55,880 --> 00:33:01,320
not be fully transparent in everything they have done, right, to allow us to replicate

425
00:33:01,320 --> 00:33:02,320
what's going on.

426
00:33:02,320 --> 00:33:03,320
Okay.

427
00:33:03,320 --> 00:33:09,240
So it's, it's an interesting world where we're trying to follow what's out there, but at the

428
00:33:09,240 --> 00:33:16,680
same time, figure out where we need to innovate to make it work for us, right?

429
00:33:16,680 --> 00:33:22,600
Which is again, somewhat of a different use case than the traditional differential privacy

430
00:33:22,600 --> 00:33:25,320
use case, or at least different concerns.

431
00:33:25,320 --> 00:33:30,280
Well, there's, there's that, but there's also differential privacy is one of these things

432
00:33:30,280 --> 00:33:35,240
where, especially if you're, if you're applying, you know, I said you're, you're injecting

433
00:33:35,240 --> 00:33:36,800
noise in certain places, right?

434
00:33:36,800 --> 00:33:39,920
If you're applying, if you're injecting that noise at the model level, at the output of

435
00:33:39,920 --> 00:33:46,640
the model, then the choice of the model directly affects how you bring differential privacy

436
00:33:46,640 --> 00:33:47,640
into it.

437
00:33:47,640 --> 00:33:52,080
So while the literature covers some models, it doesn't cover all models.

438
00:33:52,080 --> 00:33:53,080
Okay.

439
00:33:53,080 --> 00:33:55,240
So finding out what is the best way to do that.

440
00:33:55,240 --> 00:34:00,800
And then the, the, the ultimate challenge that we have is that just because we can get

441
00:34:00,800 --> 00:34:05,920
a given model to be defensively private, so logistic regression, there's some good literature

442
00:34:05,920 --> 00:34:11,440
about how to get that to be defensive private, that doesn't necessarily mean that it will

443
00:34:11,440 --> 00:34:16,720
provide value when you aggregate data across multiple partners.

444
00:34:16,720 --> 00:34:22,880
So that's where we found that we've had where there isn't much precedent around it.

445
00:34:22,880 --> 00:34:25,040
So we've figuring it out as we go along.

446
00:34:25,040 --> 00:34:29,320
So how do you take a model that you can make defensive private through through what exists

447
00:34:29,320 --> 00:34:34,560
in the literature, but then get value when you aggregate data across multiple partners?

448
00:34:34,560 --> 00:34:37,800
Yes, I think where that's where it's been exciting for us, right?

449
00:34:37,800 --> 00:34:43,360
And that's where the engagement with Georgian partners becomes extremely valuable.

450
00:34:43,360 --> 00:34:49,520
And you plan to publish on any of this or is your focus more kind of on the commercial

451
00:34:49,520 --> 00:34:54,560
application and you'll kind of let the research catch up if you in the areas you may have

452
00:34:54,560 --> 00:34:56,520
jumped ahead?

453
00:34:56,520 --> 00:34:58,480
That's a very good question.

454
00:34:58,480 --> 00:35:04,360
We, I think if there is something that is worthwhile to publish.

455
00:35:04,360 --> 00:35:10,320
I think it would be a great opportunity to share that with the community because it's

456
00:35:10,320 --> 00:35:12,920
a community that's growing fast, right?

457
00:35:12,920 --> 00:35:17,800
It's a community that's still young, so it would be good to give back to that community.

458
00:35:17,800 --> 00:35:24,760
And at the same time, I think it would be good to highlight the work that Georgian is doing

459
00:35:24,760 --> 00:35:33,160
and that we are doing in this space because I do think it's exciting that we are able

460
00:35:33,160 --> 00:35:42,520
to, as a startup, do this sort of cutting edge work and find some time and especially with

461
00:35:42,520 --> 00:35:49,160
the help of Georgian, gather, combine those resources and make progress on a problem that

462
00:35:49,160 --> 00:35:54,680
is at the sort of cutting edge of what's happening right now.

463
00:35:54,680 --> 00:36:01,480
So it's nice coming from an academic background and the people we work with at Georgian

464
00:36:01,480 --> 00:36:07,160
also come from an academic background, it's nice to be able to keep playing in that

465
00:36:07,160 --> 00:36:09,080
cutting edge space, right?

466
00:36:09,080 --> 00:36:17,240
Even when you're in a high growth startup that needs to deliver products and sell products

467
00:36:17,240 --> 00:36:18,240
and right now.

468
00:36:18,240 --> 00:36:19,240
All right.

469
00:36:19,240 --> 00:36:24,240
So to wrap things up for folks that want to dig into differential privacy a bit more

470
00:36:24,240 --> 00:36:30,000
is there canonical reference or two or three that they should be looking for or start

471
00:36:30,000 --> 00:36:32,960
with a Google search?

472
00:36:32,960 --> 00:36:37,200
There isn't a good book on the subject matter yet.

473
00:36:37,200 --> 00:36:40,320
I wish there was, it would make our life a lot easier.

474
00:36:40,320 --> 00:36:46,200
There's a Google search, Google's color search would get you, you'd pretty much find everything

475
00:36:46,200 --> 00:36:47,200
that's out there.

476
00:36:47,200 --> 00:36:50,480
You know, there's not, the thing is there isn't that much out there.

477
00:36:50,480 --> 00:36:55,400
So it doesn't take much digging to get to the canonical papers, man.

478
00:36:55,400 --> 00:36:56,400
Got it.

479
00:36:56,400 --> 00:36:57,400
Yeah.

480
00:36:57,400 --> 00:36:58,400
Awesome.

481
00:36:58,400 --> 00:36:59,400
Anything else you'd like to leave us with?

482
00:36:59,400 --> 00:37:03,240
No, I guess nothing on the technical side.

483
00:37:03,240 --> 00:37:09,560
I think that it's very interesting, this is my first experience at a startup.

484
00:37:09,560 --> 00:37:16,800
It's very exciting for me that we have partners that we can collaborate with, you know, in

485
00:37:16,800 --> 00:37:22,360
academia, you know, you always try and find other research you can collaborate with, that

486
00:37:22,360 --> 00:37:24,560
brings something new to the equation.

487
00:37:24,560 --> 00:37:32,280
And so it was very refreshing to be at the startup, at BluCore and find partners with our

488
00:37:32,280 --> 00:37:40,760
VCs, within our VCs that we can collaborate with on a problem that is very exciting, right?

489
00:37:40,760 --> 00:37:47,000
And while I'm new to the space, I don't think this is a very common theme.

490
00:37:47,000 --> 00:37:53,200
So I think it's, you know, it's a great opportunity for us as BluCore.

491
00:37:53,200 --> 00:38:01,480
And I think it's also a great opportunity for Georgian partners to work on this very

492
00:38:01,480 --> 00:38:09,440
exciting work together and explore it from the legal aspect of it, the customer sentiment

493
00:38:09,440 --> 00:38:13,640
side, the technical side, and ultimately the application.

494
00:38:13,640 --> 00:38:14,640
Right.

495
00:38:14,640 --> 00:38:15,640
Well, Zahee, thank you so much.

496
00:38:15,640 --> 00:38:20,720
I really enjoyed the conversation and learned a ton about differential privacy and how

497
00:38:20,720 --> 00:38:24,000
you're looking to use it.

498
00:38:24,000 --> 00:38:25,000
It was a pleasure.

499
00:38:25,000 --> 00:38:26,000
Thank you.

500
00:38:26,000 --> 00:38:32,360
All right, everyone, that's our show for today.

501
00:38:32,360 --> 00:38:37,880
For more information on Zahee or any of the topics covered in this episode, head on over

502
00:38:37,880 --> 00:38:43,400
to twimla.com slash talk slash 133.

503
00:38:43,400 --> 00:38:47,760
Thanks again to our friends at Georgian for sponsoring this series and be sure to visit

504
00:38:47,760 --> 00:38:55,560
their differential privacy resource center at gptrs.vc slash twimla for more information

505
00:38:55,560 --> 00:38:58,760
on the field and what they're up to.

506
00:38:58,760 --> 00:39:26,760
Of course, thanks so much for listening and catch you next time.

