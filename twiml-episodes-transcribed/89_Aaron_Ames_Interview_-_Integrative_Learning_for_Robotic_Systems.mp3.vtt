WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.560
I'm your host Sam Charrington.

00:23.560 --> 00:28.120
This week on the podcast we're featuring a series of conversations from the AWS re-invent

00:28.120 --> 00:30.200
conference in Las Vegas.

00:30.200 --> 00:34.560
I had a great time at this event, getting caught up on the new machine learning and AI products

00:34.560 --> 00:38.520
and services announced by AWS and its partners.

00:38.520 --> 00:42.720
If you missed the news coming out of re-invent and want to know more about what one of the

00:42.720 --> 00:48.060
biggest AI platform providers is up to, make sure you check out Monday's show, Twimble

00:48.060 --> 00:50.200
Talk number 83.

00:50.200 --> 00:54.040
Around table discussion I held with Dave McCrory and Lawrence Chung.

00:54.040 --> 01:00.400
We cover all of AWS's most important news, including the new SageMaker, DeepLens, Recognition

01:00.400 --> 01:06.440
Video, Transcription, Alexa for Business, Greengrass ML inference and more.

01:06.440 --> 01:11.800
This week we're also running a special listener appreciation contest to celebrate hitting

01:11.800 --> 01:18.400
1 million listens here on the podcast and to thank you all for being so awesome.

01:18.400 --> 01:25.440
Tweet to us using the hashtag Twimble1Mill to enter. Every entry gets a fly Twimble1Mill

01:25.440 --> 01:31.280
sticker plus a chance to win a limited run t-shirt commemorating the occasion.

01:31.280 --> 01:35.400
We'll be digging into the magic Twimble swag bag and giving away some other mystery prizes

01:35.400 --> 01:39.560
as well so you definitely don't want to miss this.

01:39.560 --> 01:47.000
If you're not on Twitter or you want more ways to enter, visit twimbleai.com slash Twimble1Mill

01:47.000 --> 01:49.400
for the full rundown.

01:49.400 --> 01:53.760
Before we dive in, I'd like to thank our good friends over at Intel Nirvana for their

01:53.760 --> 01:57.880
sponsorship of this podcast and our reinvent series.

01:57.880 --> 02:02.640
One of the big announcements at reinvent this year was the release of Amazon DeepLens,

02:02.640 --> 02:08.320
a fully programmable deep learning enabled wireless video camera designed to help developers

02:08.320 --> 02:13.920
learn and experiment with AI both in the cloud and at the edge.

02:13.920 --> 02:19.640
This is powered by an Intel Atom X5 processor, which delivers up to 100 gigaflops of processing

02:19.640 --> 02:22.480
power to onboard applications.

02:22.480 --> 02:26.440
To learn more about DeepLens and the other interesting things Intel's been up to in the

02:26.440 --> 02:30.840
AI space, check out intelnervana.com.

02:30.840 --> 02:36.800
Today we're joined by Aaron Ames, Professor of Mechanical and Civil Engineering at Caltech.

02:36.800 --> 02:42.040
Aaron joined me before his talk at the reinvent deep learning summit on iRobot, Computer

02:42.040 --> 02:45.120
Vision and Autonomous Robotics.

02:45.120 --> 02:49.840
While Aaron considers himself a hardware guy, we got into a great discussion centered

02:49.840 --> 02:53.040
around the intersection of robotics and machine learning.

02:53.040 --> 02:59.000
We cover a range of topics, including Boston Dynamics's Backflipping Robot and how a system

02:59.000 --> 03:01.360
like that actually works.

03:01.360 --> 03:07.120
As well as the various humanoid robots his own lab is created and more broadly, his views

03:07.120 --> 03:10.840
on the role of end-to-end deep learning in robotics.

03:10.840 --> 03:15.040
I had a blast with this interview and I think you will too.

03:15.040 --> 03:17.840
And now on to the show.

03:17.840 --> 03:29.400
Alright everyone, I am here in Las Vegas at Amazon reinvent and I have the pleasure of

03:29.400 --> 03:31.040
being seated with Aaron Ames.

03:31.040 --> 03:36.920
Aaron is a professor of mechanical and civil engineering at Caltech and he's going to

03:36.920 --> 03:41.200
be speaking here at reinvent tomorrow as part of their deep learning summit.

03:41.200 --> 03:46.040
But as you can tell from his department affiliation, he's not a deep learning guy, he is a robotics

03:46.040 --> 03:49.040
guy, a hardware, a self-professed hardware guy.

03:49.040 --> 03:52.040
Aaron, welcome to this week in machine learning and AI.

03:52.040 --> 03:53.040
Pleasure to be here.

03:53.040 --> 03:54.040
Thanks for having me on the interview.

03:54.040 --> 04:00.480
Yeah, I come from actually algorithms and mathematics is sort of my background and putting

04:00.480 --> 04:02.480
it on hardware.

04:02.480 --> 04:06.880
And it's funny because what I do, I'm speaking at the deep learning summit, I think partially

04:06.880 --> 04:12.280
to give this perspective of how learning and AI algorithms will play out on hardware

04:12.280 --> 04:15.320
platforms and what that connection will be.

04:15.320 --> 04:19.760
So I mean, historically in my research and in research like Boston Dynamics and all

04:19.760 --> 04:23.560
these cool things like the backflip video recently appeared, that was incredible.

04:23.560 --> 04:27.080
It was incredible, but guess how much learning is on those platforms?

04:27.080 --> 04:28.720
Yeah, I imagine not much.

04:28.720 --> 04:29.720
No, zero.

04:29.720 --> 04:30.720
Okay.

04:30.720 --> 04:34.920
I mean, the core in doing these things is to take the dynamics of the system, right?

04:34.920 --> 04:39.640
There's physics driving it and then you develop algorithms using something called control

04:39.640 --> 04:44.800
theory to determine how to move the robot, how to make the actuators move in specific

04:44.800 --> 04:47.000
patterns so that you get these dynamic behaviors.

04:47.000 --> 04:48.000
Right.

04:48.000 --> 04:49.480
So it's heavily tied with the physics, right?

04:49.480 --> 04:53.280
You have the physics, you make decisions based on the physics and it's all very deterministic

04:53.280 --> 04:54.280
and pre-programmed.

04:54.280 --> 04:55.280
Yeah.

04:55.280 --> 04:58.240
So AI on the other hand is a totally different animal.

04:58.240 --> 04:59.240
You start with data.

04:59.240 --> 05:00.880
It's all data driven data centric.

05:00.880 --> 05:05.600
You take examples of things that work well, you know, you label images and then you plug

05:05.600 --> 05:07.240
it into a deep neural network.

05:07.240 --> 05:10.520
There's other variants of learning as well that are a little more mathematical and then

05:10.520 --> 05:14.800
the back end is that it sort of learns or identifies these patterns.

05:14.800 --> 05:15.800
Right.

05:15.800 --> 05:19.600
And that's really exciting because it's computers doing things that we don't always expect

05:19.600 --> 05:23.680
they'll do and they can deal with highly unstructured and data driven approaches.

05:23.680 --> 05:24.680
Yeah.

05:24.680 --> 05:29.480
But it sort of runs contrary to this whole hardware and theoretical approach that's

05:29.480 --> 05:33.160
often taken in robotics because, you know, we need to know everything about the robot.

05:33.160 --> 05:37.800
It has to be very pre-programmed and pre-planned in a specific way.

05:37.800 --> 05:41.480
So the question of the deep learning summit that I'm addressing in my talk is, is what

05:41.480 --> 05:45.680
would this integration kind of look like or a view towards this integration of learning

05:45.680 --> 05:49.440
with hardware and robotic systems?

05:49.440 --> 05:55.320
Something that has yet to actually be done in a good way because they're such different

05:55.320 --> 05:56.320
worlds.

05:56.320 --> 05:58.240
And what's missing from the learning community?

05:58.240 --> 05:59.760
What's missing from the robotics community?

05:59.760 --> 06:01.120
How could they benefit each other?

06:01.120 --> 06:02.120
Right.

06:02.120 --> 06:03.120
Right.

06:03.120 --> 06:04.120
Let's dig into that.

06:04.120 --> 06:06.880
But before we do, I want to make sure that the audience gets to know you a little bit.

06:06.880 --> 06:07.880
Absolutely.

06:07.880 --> 06:12.560
So how did you get interested in algorithms, math, hardware and the way you've put them

06:12.560 --> 06:13.560
all together?

06:13.560 --> 06:14.560
Right.

06:14.560 --> 06:15.560
Science fiction in short.

06:15.560 --> 06:16.560
Okay.

06:16.560 --> 06:17.560
That was driven by science fiction.

06:17.560 --> 06:21.040
When I was in undergrad, all I did was read sci-fi all the time.

06:21.040 --> 06:22.280
Any favorites?

06:22.280 --> 06:24.560
Asmob is guy that go to classic, right?

06:24.560 --> 06:25.560
Yeah.

06:25.560 --> 06:28.040
There's some other great ones, you know, high line, you know, these.

06:28.040 --> 06:31.240
Like the classic authors, I think, had a really unique perspective.

06:31.240 --> 06:36.680
Actually, coming from the authors that sort of started in the 50s, 60s, there was such

06:36.680 --> 06:43.160
imagination to where we'd be, unconstrained by the problems that would later be confronted

06:43.160 --> 06:46.600
in robotics and in all these other things that I think it painted a picture of the world

06:46.600 --> 06:50.840
that was really enticing, of how robots can really work amongst us.

06:50.840 --> 06:54.560
And that's driven me for a long time as this sort of internal fascination.

06:54.560 --> 06:58.920
I can't really explain it on my borderline or maybe not even borderline obsessed with

06:58.920 --> 06:59.920
it.

06:59.920 --> 07:02.280
How do we make robots move like us and do things like us?

07:02.280 --> 07:03.280
Yeah.

07:03.280 --> 07:04.800
So that's driven me for a really long time.

07:04.800 --> 07:06.560
And I wanted to delve into that.

07:06.560 --> 07:09.160
So my background is actually highly theoretic.

07:09.160 --> 07:13.240
I didn't touch hardware until actually after my PhD.

07:13.240 --> 07:16.000
I studied walking, but from a theoretic perspective.

07:16.000 --> 07:20.480
So I really wanted to understand the mathematical underpinnings of locomotion.

07:20.480 --> 07:25.200
And as you start by looking at that from a robotics, a hardware perspective or like human

07:25.200 --> 07:26.200
walking.

07:26.200 --> 07:32.480
I started it from a robotics perspective, but not so much hardware as the mathematics

07:32.480 --> 07:35.800
underlying movement, right, underlying dynamics, right?

07:35.800 --> 07:37.040
And how do we understand that?

07:37.040 --> 07:43.400
How do we even model or formulate a mathematical model of walking and running and doing dynamic

07:43.400 --> 07:44.400
things on robots?

07:44.400 --> 07:48.840
So I delved into that, you know, I proved a lot of theorems on what this might look

07:48.840 --> 07:50.800
like, well, how do we quantify what this is?

07:50.800 --> 07:51.880
How do we characterize it?

07:51.880 --> 07:57.320
So really that, you know, that's sort of the basic science of dynamic robotic movement.

07:57.320 --> 07:58.920
How far have we come in that?

07:58.920 --> 08:03.840
Do we have a strong analytical foundation in locomotion, or is it, you know, do we bump

08:03.840 --> 08:08.680
up against, you know, an edge analytically and have to apply computation to it?

08:08.680 --> 08:09.680
That's an interesting question.

08:09.680 --> 08:14.680
And actually, we have a really strong analytic foundation now for locomotion that's been

08:14.680 --> 08:19.320
developed over the last, I mean, I guess 20 years.

08:19.320 --> 08:20.320
Okay.

08:20.320 --> 08:25.000
Starting sort of a little before I was a grad student, it was in its infancy and I started

08:25.000 --> 08:26.000
working with it.

08:26.000 --> 08:29.800
Other people, great people at other universities have developed these frameworks.

08:29.800 --> 08:32.840
Jesse Grizzell at University of Michigan is one example.

08:32.840 --> 08:36.760
There's lots of people, but we've come a long way in our mathematical understanding of

08:36.760 --> 08:37.760
locomotion.

08:37.760 --> 08:38.760
Yeah.

08:38.760 --> 08:39.760
What the models are.

08:39.760 --> 08:40.760
How do we quantify that behavior?

08:40.760 --> 08:43.040
And there's a lot of papers on this too.

08:43.040 --> 08:45.960
You can understand it mathematically, but what's interesting is we have this mathematical

08:45.960 --> 08:52.880
understanding a while ago now, but computationally realizing that math on hardware was a huge

08:52.880 --> 08:53.880
problem.

08:53.880 --> 08:57.840
So that's where the sort of blockade came is we could write a theorem and we could say,

08:57.840 --> 09:00.520
if this thing exists, then we have walking, right?

09:00.520 --> 09:02.600
But how do you find the thing that exists?

09:02.600 --> 09:04.160
And that's a computational question.

09:04.160 --> 09:08.680
In the end, every mathematical thing you do has to be translated to algorithms on the

09:08.680 --> 09:09.680
robot.

09:09.680 --> 09:10.680
And how do you do that?

09:10.680 --> 09:16.120
It turns out that recently there's been a huge surge in this computation area, huge breakthroughs,

09:16.120 --> 09:20.840
mainly due to the computation breakthroughs that have happened.

09:20.840 --> 09:25.120
So the prevalence of cheap and vast computation.

09:25.120 --> 09:29.760
So it turns out that there's a lot of analogies between making a robot move dynamically and

09:29.760 --> 09:30.760
learn.

09:30.760 --> 09:36.200
What I mean is, in essence, it's a large optimization problem, right?

09:36.200 --> 09:38.200
And that's what the math boils down to.

09:38.200 --> 09:42.440
And how do you solve large-scale optimization problems efficiently?

09:42.440 --> 09:46.360
And there's been some great results recently, some of which have come out of my lab, some

09:46.360 --> 09:51.960
of other labs, a bunch of people collaborating where we can now solve these orders of magnitude

09:51.960 --> 09:54.160
faster than we could 10 years ago.

09:54.160 --> 10:00.280
I mean, it used to be it'd take a day plus to generate one walking behavior for a humanoid

10:00.280 --> 10:01.280
robot, right?

10:01.280 --> 10:06.080
Over a day of computation, if it converged, and as a walking behavior, what does that mean

10:06.080 --> 10:07.320
specifically?

10:07.320 --> 10:14.400
So the way you can think about a walking behavior is a periodic motion that's stable.

10:14.400 --> 10:15.400
Okay.

10:15.400 --> 10:21.040
Given a set of parameters that define us, you know, the hardware or what the, you know,

10:21.040 --> 10:23.080
the joints, the angles, the lengths of the legs.

10:23.080 --> 10:25.440
The masses, the inertia, all that stuff.

10:25.440 --> 10:26.440
So what happens?

10:26.440 --> 10:27.440
That's even lower level, then.

10:27.440 --> 10:28.440
Yeah, yeah.

10:28.440 --> 10:31.120
So you pull all those things together to create a mathematical model, right?

10:31.120 --> 10:35.040
And you get a differential equation, if you like, the technical term.

10:35.040 --> 10:39.480
And it's actually a hybrid system too, meaning there's continuous dynamics.

10:39.480 --> 10:41.440
Think about just a bouncing ball, right?

10:41.440 --> 10:44.960
As almost the simplest example of a walking gate, okay?

10:44.960 --> 10:46.840
What I mean is a periodic motion.

10:46.840 --> 10:49.480
So it falls through the air until it hits the ground.

10:49.480 --> 10:52.600
And then there's a discrete impact that pops it back up.

10:52.600 --> 10:56.000
Now imagine you have a little actuation with that ball, like a little spring, and you

10:56.000 --> 11:01.840
can, you know, then the goal of locomotion is to create a stable, periodic motion.

11:01.840 --> 11:04.600
So the ball bounces at the same height all the time, okay?

11:04.600 --> 11:06.520
That's a very low dimensional example.

11:06.520 --> 11:10.880
Now take a humanoid robot, you take all the physics that go into it, right?

11:10.880 --> 11:15.320
So you have something like, you know, let's say 25 degrees of freedom.

11:15.320 --> 11:20.200
What that means is 25 joints that you can actuate, or other joints that you can't actuate.

11:20.200 --> 11:23.200
The point is 25 things they can move, yeah, all right?

11:23.200 --> 11:26.760
And then you have to take that and you get some mathematical representation of it as a

11:26.760 --> 11:30.200
hybrid system, because there's this, you know, when the legs swinging forward, that's

11:30.200 --> 11:31.200
a continuous dynamic.

11:31.200 --> 11:34.120
It's just like when the ball's falling, then the foot strikes the ground and you get these

11:34.120 --> 11:35.120
impacts.

11:35.120 --> 11:40.040
And then you have to create a periodic motion that coordinates all of those, you know, 25

11:40.040 --> 11:42.960
degrees of freedom together in a synchronous way.

11:42.960 --> 11:47.960
And so do these 25 degrees of freedom translate into, you know, some series of hundreds of

11:47.960 --> 11:49.480
differential equations that you're trying to say?

11:49.480 --> 11:53.520
Yeah, so you actually get two times the number of degrees of freedom, because usually we're

11:53.520 --> 11:54.960
dealing with second order systems.

11:54.960 --> 12:01.080
So you end up with, let's say, 50 equations, 50 ordinary differential equations, right?

12:01.080 --> 12:05.400
Or what, technically, an ordinary differential equation that's 50-dimensional, okay?

12:05.400 --> 12:06.400
Okay.

12:06.400 --> 12:09.400
So you're dealing on some 50-dimensional space of evolution.

12:09.400 --> 12:10.400
Yeah.

12:10.400 --> 12:11.400
So it's a very high-dimensional space.

12:11.400 --> 12:12.400
And that's for simple ones.

12:12.400 --> 12:13.400
It gets even higher.

12:13.400 --> 12:14.560
I mean, take a full humanoid with hands and everything.

12:14.560 --> 12:19.960
You could be dealing with a hundred-dimensional system, plus, and this is all very nonlinear.

12:19.960 --> 12:23.680
And more importantly, it's because you're generating these periodic motions, you have to

12:23.680 --> 12:28.200
really utilize these nonlinear dynamics, the inherent dynamics of the system to generate

12:28.200 --> 12:29.200
these behaviors.

12:29.200 --> 12:30.200
Okay.

12:30.200 --> 12:32.080
And is that mean specifically?

12:32.080 --> 12:36.680
What that means is you can't just to provide an example.

12:36.680 --> 12:37.680
You know, it's funny.

12:37.680 --> 12:40.960
We get a lot of comments on YouTube for our videos, right?

12:40.960 --> 12:45.440
And these are, I actually read them sometimes as I enjoy reading them, although I never respond.

12:45.440 --> 12:48.080
I think that's the key is never responding to comments.

12:48.080 --> 12:49.080
But I like reading them.

12:49.080 --> 12:53.880
And a lot of questions revolve around, well, why not just take a human walking trajectory,

12:53.880 --> 12:54.880
right?

12:54.880 --> 12:57.760
Just record a human walking and pop it on the robot, right?

12:57.760 --> 13:01.120
Well, it's because the physics are different from the human and the robot, right?

13:01.120 --> 13:02.120
Right.

13:02.120 --> 13:06.000
So, what I mean by the nonlinear dynamics is there's only certain trajectories that work

13:06.000 --> 13:10.280
for the system, that make sense, that are consistent with its dynamics.

13:10.280 --> 13:12.280
And those are the ones you have to find.

13:12.280 --> 13:15.880
And you can't just put a human trajectory on, you'd have to modify it so it'd be consistent

13:15.880 --> 13:16.880
with the dynamics.

13:16.880 --> 13:19.000
The robot would have to basically have human physical properties.

13:19.000 --> 13:23.560
Yeah, and human actuation and everything else, it'd have to be perfectly human in some

13:23.560 --> 13:25.560
way, and that's not going to happen.

13:25.560 --> 13:26.560
And it shouldn't.

13:26.560 --> 13:29.760
It's like when we have planes that fly, they don't flap their wings.

13:29.760 --> 13:33.240
You have to exploit the, you know, you want to be inspired by flight.

13:33.240 --> 13:34.240
You want to create lift.

13:34.240 --> 13:35.240
Right.

13:35.240 --> 13:37.240
But you want to do it in a way that's consistent with what we can build.

13:37.240 --> 13:38.240
Right.

13:38.240 --> 13:41.360
So you take all these dynamics just not to get too far in the weeds.

13:41.360 --> 13:46.040
And then you have to create these periodic motions, which again, result in these optimization

13:46.040 --> 13:47.040
problems.

13:47.040 --> 13:48.040
Okay.

13:48.040 --> 13:51.840
You know, you could imagine what these might be, you know, in the sense of, even with

13:51.840 --> 13:54.080
the bouncing ball, you want to create a periodic trajectory.

13:54.080 --> 13:56.440
So start at some point and end at some point.

13:56.440 --> 13:58.880
Those are some constraints on the system, right?

13:58.880 --> 14:01.960
Another constraint, it has to satisfy the dynamics of the ball falling.

14:01.960 --> 14:05.480
So you put all those together and you crunch it into this big optimization.

14:05.480 --> 14:09.840
And we, like I mentioned, we've gone from maybe a day plus to solve these, maybe to down

14:09.840 --> 14:15.440
to a couple of minutes, even faster, sub-second, meaning almost real time, which means we

14:15.440 --> 14:18.560
can generate a gate in really fast, right?

14:18.560 --> 14:20.120
And by a gate, I mean a periodic motion.

14:20.120 --> 14:21.120
Yeah.

14:21.120 --> 14:24.040
And then you can start putting all those together and create advanced behaviors, right?

14:24.040 --> 14:28.280
So that's kind of the paradigm for how you create walking gates or any kind of behavior.

14:28.280 --> 14:30.320
You tell, you know, there was the backflip we mentioned earlier.

14:30.320 --> 14:31.320
How would you do that?

14:31.320 --> 14:33.800
Well, you can actually take the dynamics of that system.

14:33.800 --> 14:38.560
You can set that up as an optimization problem where you go through this motion of flippy.

14:38.560 --> 14:42.280
And then you can crunch it into an optimization problem and generate those motions.

14:42.280 --> 14:44.640
And then pop that on the robot.

14:44.640 --> 14:45.640
And that sounds easy.

14:45.640 --> 14:47.520
There's a lot of difficulty in doing that.

14:47.520 --> 14:50.160
It's a non-trivial adventure to actually implement that.

14:50.160 --> 14:52.200
But that's the general trend.

14:52.200 --> 14:53.480
And there's lots of ways that people do this.

14:53.480 --> 14:56.160
I mean, I don't want to go through all the background, but there's lots of ways people

14:56.160 --> 14:57.600
can generate these periodic motions.

14:57.600 --> 15:02.600
They can have reduced dimensional representations of the system that make it a little faster.

15:02.600 --> 15:04.240
There's a lot of tricks and all this stuff.

15:04.240 --> 15:08.600
But in the end, what you're fundamentally doing, the point of this discussion is to kind

15:08.600 --> 15:15.800
of say, this is the way the robotics community by and large approaches the problem of generating

15:15.800 --> 15:16.800
behaviors on robots.

15:16.800 --> 15:18.440
So they start with the physics.

15:18.440 --> 15:23.080
They set up some sort of optimization problem, computation problem, which generates the,

15:23.080 --> 15:26.200
gives you this periodic motion, which you then put on the robot.

15:26.200 --> 15:32.240
Now, when we see a video like the, you know, people will be familiar with the Boston Dynamics

15:32.240 --> 15:34.480
and the Backflip one that came out recently.

15:34.480 --> 15:39.400
And in fact, you showed me a really interesting one on your YouTube of a robot called Doris,

15:39.400 --> 15:41.000
just part of the interview.

15:41.000 --> 15:45.280
When we see videos like that, and let's take the Backfliping one because it was.

15:45.280 --> 15:46.280
I think everyone's seen it.

15:46.280 --> 15:47.280
So that's fine.

15:47.280 --> 15:48.800
It was, I have to say, it was very impressive.

15:48.800 --> 15:51.880
I mean, Boston Dynamics is always raising the bar for our academics.

15:51.880 --> 15:52.880
Yeah.

15:52.880 --> 15:56.960
You think, you know, you mentioned the Doris, you know, if you look at Doris walking on my

15:56.960 --> 16:01.160
YouTube page and then compare it with some of the walking that Boston Dynamics has, they're

16:01.160 --> 16:02.160
in the ballpark, right?

16:02.160 --> 16:03.160
They are.

16:03.160 --> 16:04.680
I mean, which is something I'm very proud of.

16:04.680 --> 16:07.800
But, you know, they had that stuff a couple of years ago and we now we have the mathematics

16:07.800 --> 16:08.800
to understand it.

16:08.800 --> 16:09.800
And then they do the Backflip.

16:09.800 --> 16:10.800
I'm right.

16:10.800 --> 16:14.560
So now we got to do the Backflip or some variants of that that matches it.

16:14.560 --> 16:17.440
So they raise the bar and they push us, which I think is great, but it's a very impressive

16:17.440 --> 16:18.440
behavior.

16:18.440 --> 16:19.440
Yeah.

16:19.440 --> 16:20.440
And so there's no doubt about it.

16:20.440 --> 16:25.720
But when we look at that, are we seeing a robot, you know, basically performing a script

16:25.720 --> 16:30.600
and it can do, you know, just what we see starting from where it started, you know, going

16:30.600 --> 16:33.400
through every point and space that it saw or is there.

16:33.400 --> 16:34.400
That's exactly right.

16:34.400 --> 16:36.800
That's some parametric thing where there's some variability.

16:36.800 --> 16:37.800
It's a script.

16:37.800 --> 16:38.800
It's a script.

16:38.800 --> 16:39.800
Okay.

16:39.800 --> 16:40.800
And you're exactly right.

16:40.800 --> 16:42.800
And here comes the crux of the problem, right?

16:42.800 --> 16:45.560
And the exciting opportunities.

16:45.560 --> 16:50.040
So, you know, when you, let's now talk about the Backflip with Boston Dynamics, when you

16:50.040 --> 16:53.360
look at that video, I mean, everyone's like, oh, SkyNets coming, the robots are taking

16:53.360 --> 16:54.360
over.

16:54.360 --> 16:55.360
Oh, my God.

16:55.360 --> 16:57.920
And again, we mentioned earlier, and now we don't know exactly what's on the robot, just

16:57.920 --> 17:00.560
to be clear, they don't publicly release anything that's on there.

17:00.560 --> 17:01.560
Right.

17:01.560 --> 17:04.600
Although I've, I know Boston Dynamics well enough to make a very educated guess.

17:04.600 --> 17:09.040
And the guess is based on their past stuff, too, is exactly what you said.

17:09.040 --> 17:11.240
It's a pre-plan behavior.

17:11.240 --> 17:15.360
So this robot has no knowledge of its environment in the sense that it's not observing where

17:15.360 --> 17:22.000
those blocks are, and in real time, adjusting its behavior, and learning how to do this behavior.

17:22.000 --> 17:25.160
They put those obstacles in the memory of the computer.

17:25.160 --> 17:26.800
They pre-plan those behaviors.

17:26.800 --> 17:29.640
They do a bunch of experiments till they get the right behavior.

17:29.640 --> 17:30.680
They take a bunch of videos.

17:30.680 --> 17:35.000
What I loved about that, Blackfoot videos, after the Backflip, they showed failure cases.

17:35.000 --> 17:38.080
Which I thought was, which is important to show, because anytime you see a video of something

17:38.080 --> 17:39.080
working, right?

17:39.080 --> 17:40.080
Right.

17:40.080 --> 17:42.600
There's a thousand or 10,000 cases where it didn't work at all.

17:42.600 --> 17:43.600
Right.

17:43.600 --> 17:45.680
And everything tuned in just right?

17:45.680 --> 17:49.840
As hardware is not quite as clean as the math I talked about.

17:49.840 --> 17:50.840
And then it goes.

17:50.840 --> 17:56.120
So it's a pre-plan behavior that robot has no awareness of what it's doing in a broader

17:56.120 --> 17:57.120
sense.

17:57.120 --> 18:04.360
Even beyond the awareness and learning and ability to, the robot's ability to adjust, you

18:04.360 --> 18:08.520
know, I think when it, you know, it's easy to envision something like that where I guess

18:08.520 --> 18:12.240
I'm thinking of it from like a computer, you know, a computer program perspective.

18:12.240 --> 18:17.480
There's no function that says, you know, Backflip, you know, start from X equals whatever.

18:17.480 --> 18:22.160
It is like a vector of points that it is just following, right?

18:22.160 --> 18:23.160
Yeah, yeah.

18:23.160 --> 18:24.160
Effectively, yeah.

18:24.160 --> 18:28.120
I mean, you can represent these behaviors as sort of modules, right?

18:28.120 --> 18:29.720
If you'd like, like Backflip.

18:29.720 --> 18:30.720
It itself.

18:30.720 --> 18:34.480
I mean, so the thing about dynamic Backflip, like with how many, you know, how much freedom,

18:34.480 --> 18:37.560
like Backflip and you can give it a height and it would on the fly.

18:37.560 --> 18:39.400
And that's, and that's a good question.

18:39.400 --> 18:40.400
Exactly.

18:40.400 --> 18:41.400
Right.

18:41.400 --> 18:45.880
So far, you know, before we get to any conversation about awareness and learning, you're absolutely

18:45.880 --> 18:46.880
right.

18:46.880 --> 18:50.320
And I think this is a very astute question and comment is right now.

18:50.320 --> 18:54.000
I mean, I don't know their exact capabilities, but typically it's Backflip from a height

18:54.000 --> 18:57.200
of this high, maybe with some small variations, right?

18:57.200 --> 19:01.520
But if you, if you change the terrain it was on or change the box or change any of those

19:01.520 --> 19:03.640
parameters, it wouldn't do it.

19:03.640 --> 19:04.640
Right.

19:04.640 --> 19:07.560
In fact, if you look at that video, you look at what it's landing on, it's kind of a somewhat

19:07.560 --> 19:12.600
absorbed, you know, a pad that looks like it has, that it's very carefully constructed.

19:12.600 --> 19:14.080
It's not just a random floor.

19:14.080 --> 19:17.640
There was something special about what they were landing on, partially to absorb the shock

19:17.640 --> 19:18.640
of impact.

19:18.640 --> 19:19.640
I'm sure.

19:19.640 --> 19:23.080
But the point is, as you said, yeah, so there's Backflip as a canonical unit, but that

19:23.080 --> 19:26.120
is very constrained in the environments it can work on.

19:26.120 --> 19:27.120
So you're right.

19:27.120 --> 19:30.160
And this is something that's very important for people to understand, I think, from the

19:30.160 --> 19:36.120
learning perspective, is that we're not like one step away from, you know, self-aware

19:36.120 --> 19:37.760
machines in this context.

19:37.760 --> 19:43.280
I mean, we can do pre-programmed behaviors in environments we completely understand and

19:43.280 --> 19:47.720
have characterized within a small window of perturbation.

19:47.720 --> 19:49.480
And that's what we're getting really good at that.

19:49.480 --> 19:52.800
We couldn't do that 10 years ago, 20 years ago, we couldn't even do that, right?

19:52.800 --> 19:54.680
We couldn't, a robot couldn't do a backflip.

19:54.680 --> 19:58.360
You know, I mean, in, or at least not a humanoid, although Raybert has some great stuff from

19:58.360 --> 20:03.040
the 80s where he had two-legged sort of polo-step type robots that could do backflips back then.

20:03.040 --> 20:06.080
You should go check out some of this stuff from when he was a professor at MIT.

20:06.080 --> 20:07.840
I mean, his old, the 1980s.

20:07.840 --> 20:08.840
Mark Raybert.

20:08.840 --> 20:09.840
Mark Raybert.

20:09.840 --> 20:10.840
Yeah.

20:10.840 --> 20:11.840
He's the head of Boston Dynamics.

20:11.840 --> 20:12.840
He's, it's his brainchild.

20:12.840 --> 20:13.840
Got it.

20:13.840 --> 20:15.320
And he went and founded it from MIT.

20:15.320 --> 20:19.560
He actually started out at JPL, which is part of Caltech, and then Carnegie Mellon, then

20:19.560 --> 20:22.320
MIT, then started Boston Dynamics has been doing that forever.

20:22.320 --> 20:25.960
But if you look at it stuff in the 80s, it has the same characteristic.

20:25.960 --> 20:29.840
If you watch those videos, you see how the backflip came to be on Atlas, right?

20:29.840 --> 20:30.840
Yeah.

20:30.840 --> 20:32.000
And, you know, you can really see the trend.

20:32.000 --> 20:35.720
But again, it's all this very structured thing.

20:35.720 --> 20:40.040
And that leads to what do we do about unstructured environments?

20:40.040 --> 20:41.040
Right.

20:41.040 --> 20:43.960
And this is actually the kind of the core of what I'm going to talk about tomorrow, is

20:43.960 --> 20:46.160
I want to set the stage with just like we had this conversation.

20:46.160 --> 20:49.480
I want to explain what it takes to make a robot walk, because I think when you understand

20:49.480 --> 20:55.400
that or make a robot backflip, you realize how much machinery is there, and how first

20:55.400 --> 20:58.200
you're not going to learn how to do that, really.

20:58.200 --> 20:59.200
Right.

20:59.200 --> 21:01.120
I mean, it's too high-dimensional of a problem for learning.

21:01.120 --> 21:06.840
You're not just going to plug in the actuators and whatever into some deep neural network

21:06.840 --> 21:09.320
and expect the outcome to be a backflip.

21:09.320 --> 21:10.320
You need to...

21:10.320 --> 21:12.640
There's some structure in the system that has to be exploited, right?

21:12.640 --> 21:16.760
So there's a place for control and dynamics, but there's a place for learning too, right?

21:16.760 --> 21:20.040
You know, and I think that's the thing is to understand the right context for it.

21:20.040 --> 21:21.040
Right.

21:21.040 --> 21:23.720
You know, learning will not take over the world and learning will not solve all problems.

21:23.720 --> 21:24.720
Right.

21:24.720 --> 21:27.880
But learning can handle unknown and unforeseen things.

21:27.880 --> 21:28.880
Yeah.

21:28.880 --> 21:32.520
So that's the role it can play in the context of like a backflip.

21:32.520 --> 21:34.960
So how do we go about getting to that?

21:34.960 --> 21:41.720
I mean, it seems like it strikes me that there are lots of, you know, umpteen ways of

21:41.720 --> 21:43.120
kind of attacking that problem.

21:43.120 --> 21:46.880
Like I'm thinking that the thought that comes to mind is like, you know, one approach

21:46.880 --> 21:51.560
might be defining levels of abstraction or primitives or something like that, trying

21:51.560 --> 21:57.000
to figure those out and, you know, I'll have the learning, the intelligence kind of select

21:57.000 --> 21:58.000
those on the fly.

21:58.000 --> 22:02.920
Like, what are the, what's the, you need to come, you need to come, you need to come join

22:02.920 --> 22:05.760
the robotics walking community because you nailed it.

22:05.760 --> 22:10.000
No, the current perspective, I, you know, I even have some papers on this from about

22:10.000 --> 22:14.640
five years ago, now, okay, where we call it motion primitives and transitions, right?

22:14.640 --> 22:18.320
And so basically what you do is you create primitives for all these different behaviors.

22:18.320 --> 22:21.040
Because you're going to have to create the behaviors and now the computation has gotten

22:21.040 --> 22:23.920
better, you can imagine, think about a graph, right?

22:23.920 --> 22:27.760
You know, where every node of the graph, so every point is a behavior.

22:27.760 --> 22:28.760
Okay.

22:28.760 --> 22:30.880
And then you have transitions between those behaviors, right?

22:30.880 --> 22:31.880
Which are admissible.

22:31.880 --> 22:36.440
So you might have backflip followed by walking, followed by going up and down stairs.

22:36.440 --> 22:40.120
You might have a bunch of different primitives for different backflips from different heights

22:40.120 --> 22:44.240
and different terrain types and you can build up this entire compendium.

22:44.240 --> 22:50.440
And then you could supervise how you pick an individual behavior with a learning element.

22:50.440 --> 22:53.080
And this has started to be done that I mentioned Jesse Grisel earlier.

22:53.080 --> 22:55.320
I worked closely with him and have for a really long time.

22:55.320 --> 22:58.960
And he started to play with some of these ideas, putting learning on top of that sort

22:58.960 --> 23:04.000
of motion primitive and transition framework where you decide how what gate to do at any

23:04.000 --> 23:07.680
given time based on what the environment is doing and learning algorithm could do that

23:07.680 --> 23:08.680
really well.

23:08.680 --> 23:10.240
And that's a great place for learning.

23:10.240 --> 23:14.080
And that's a great way of thinking about learning in the sense that you've sort of taken

23:14.080 --> 23:15.560
the dynamics into account.

23:15.560 --> 23:23.080
You've taken the mathematical representation of dynamic motions to their sort of extreme.

23:23.080 --> 23:27.000
You've utilized them all the way and then you let learning do what learning does best,

23:27.000 --> 23:31.640
which is based on input data, the environment, decide what to do on an output, but at a

23:31.640 --> 23:34.160
very high level.

23:34.160 --> 23:38.120
And if you look at the way the human brain works, this is very analogous to how the human

23:38.120 --> 23:39.200
brain and body work.

23:39.200 --> 23:43.160
So, you know, there's not thinking about moving my legs and my legs.

23:43.160 --> 23:44.160
Exactly.

23:44.160 --> 23:45.160
Yeah.

23:45.160 --> 23:46.160
So we have motion primitive.

23:46.160 --> 23:47.160
And we transition between them.

23:47.160 --> 23:48.480
More importantly, the architecture of the human body.

23:48.480 --> 23:52.880
Now I'm not an expert here, but I've certainly read a bit about it.

23:52.880 --> 23:55.480
Just with this paradigm, you know, some people say we should just learn everything because

23:55.480 --> 23:56.960
our brain learns everything.

23:56.960 --> 23:58.520
It's not actually true.

23:58.520 --> 24:02.240
Our brain is responsible for some of our motions, but in our spinal cord, we have a separate

24:02.240 --> 24:03.240
brain.

24:03.240 --> 24:06.960
I mean, in essence, we have patterns that are generated.

24:06.960 --> 24:08.440
Those are your locomotion patterns.

24:08.440 --> 24:09.760
Those are your primitives.

24:09.760 --> 24:13.960
So you couldn't walk essentially with very little cognitive load.

24:13.960 --> 24:16.760
I mean, think about walking and texting on your phone.

24:16.760 --> 24:17.880
You don't have to think about it, right?

24:17.880 --> 24:19.240
So that's the motion primitive acting.

24:19.240 --> 24:22.800
And what happens when you get to a stair, you have to look up from your phone.

24:22.800 --> 24:24.080
You have a cognitive load.

24:24.080 --> 24:25.920
For a second, you have to think, what am I going to do next?

24:25.920 --> 24:28.120
You decide, you go in, you're back to your phone.

24:28.120 --> 24:29.120
Yeah.

24:29.120 --> 24:32.760
So think about any time you could be doing something while on your phone.

24:32.760 --> 24:36.720
That's where dynamics control and all those classic approaches would be used.

24:36.720 --> 24:40.000
Any time you have to look up and see something, that's where machine learning would play

24:40.000 --> 24:41.000
role.

24:41.000 --> 24:42.000
Right?

24:42.000 --> 24:44.600
The real point here is it's not one area or the other.

24:44.600 --> 24:45.600
Right.

24:45.600 --> 24:49.440
We really need to understand the intersection of these two domains.

24:49.440 --> 24:53.440
And that's really the challenge problem of the next decade in my opinion.

24:53.440 --> 24:56.040
Because there's a lot of people that do learning, a lot of people that do robotics.

24:56.040 --> 24:59.640
And there's the beginnings of connecting these up a little bit.

24:59.640 --> 25:03.040
But we need to make a concerted effort to really understand this connection point, because

25:03.040 --> 25:05.240
I think that's sort of the key.

25:05.240 --> 25:13.520
So the kind of using a learning system to plan a goal achievement across some set of

25:13.520 --> 25:15.840
primitives is one approach.

25:15.840 --> 25:18.480
We've already talked about the, you know, we've already thrown out the window.

25:18.480 --> 25:22.640
The idea of learning the motion primitives, you know, from the ground up.

25:22.640 --> 25:27.440
But is there a role in learning and making them more robust?

25:27.440 --> 25:28.440
Yes, absolutely.

25:28.440 --> 25:29.440
Right.

25:29.440 --> 25:30.920
That's the second place where learning comes in.

25:30.920 --> 25:31.920
Okay.

25:31.920 --> 25:32.920
You think we'd have a script?

25:32.920 --> 25:37.760
We had a script for this interview, because you're feeding right into my, my talking research.

25:37.760 --> 25:42.040
So the second place, yes, is where learning plays, will play a big role is not at the level

25:42.040 --> 25:48.400
of planning, but at the level of unknown, unforeseen environments and influences.

25:48.400 --> 25:49.400
Right.

25:49.400 --> 25:51.640
So the simplest example is walking on different terrain.

25:51.640 --> 25:52.640
Yeah.

25:52.640 --> 25:55.600
So when you're walking on flat hard ground, we have a perfect model that, remember that everything

25:55.600 --> 26:00.040
we talked about with generating models was built on the premise of having a model.

26:00.040 --> 26:01.040
Right.

26:01.040 --> 26:05.040
So now if you walk on standard dirt, it turns out there are no models of this.

26:05.040 --> 26:06.200
There's no simple models at least.

26:06.200 --> 26:11.320
I mean, there's people make them make their entire careers about modeling and simulating

26:11.320 --> 26:14.200
granular media, deforming and moving around.

26:14.200 --> 26:18.520
So standing dirt, crunching down, it's a really, really hard problem.

26:18.520 --> 26:23.040
So there won't be some computationally as difficult to just render it as a picture.

26:23.040 --> 26:24.040
Exactly.

26:24.040 --> 26:25.040
Let's try to figure out its physics.

26:25.040 --> 26:26.040
Exactly.

26:26.040 --> 26:27.040
Exactly.

26:27.040 --> 26:31.520
So you could imagine days to generate physics simulations of, of sand movie.

26:31.520 --> 26:35.640
Now if it takes a day to generate a physics simulation of, of a robot putting its foot in

26:35.640 --> 26:38.920
sand, you're probably not going to be real time using those things, right?

26:38.920 --> 26:39.920
Yeah.

26:39.920 --> 26:40.920
So how do we bring it together?

26:40.920 --> 26:43.880
Well, so we have some initial work with some colleagues I have at Georgia Tech on

26:43.880 --> 26:48.400
this idea where we learn how to hop in granular terrain.

26:48.400 --> 26:50.080
And so we don't use neural nets there.

26:50.080 --> 26:53.800
We use something called Gaussian processes, which are another way of learning, but it's

26:53.800 --> 26:56.080
not the neural net way.

26:56.080 --> 27:02.000
It's a variant that basically deals with some initial guess on the model and then you update

27:02.000 --> 27:04.880
that model as new data comes in.

27:04.880 --> 27:10.400
And we were able to not know what the tray model was, but have a guess based on some physics.

27:10.400 --> 27:14.880
And then every time the robot would hop in the, in the train, we'd take that data in and

27:14.880 --> 27:18.680
update the model of what the terrain forces look like.

27:18.680 --> 27:21.040
We iterated that through the optimization problem.

27:21.040 --> 27:24.960
So we'd update that every time we, we had a new hop, we'd take this new information,

27:24.960 --> 27:29.880
we generate a new model of the physics interaction with the world, and then we'd run the optimization

27:29.880 --> 27:30.880
with that new model.

27:30.880 --> 27:35.640
And now are we talking about something that's done in simulation as far as learning process

27:35.640 --> 27:36.640
or?

27:36.640 --> 27:38.040
No, we did this on hardware.

27:38.040 --> 27:39.040
Okay.

27:39.040 --> 27:43.520
I mean, we verified in simulation, but really, you, this would have to be on the hardware

27:43.520 --> 27:45.360
because you need the data.

27:45.360 --> 27:46.360
Okay.

27:46.360 --> 27:47.360
Right.

27:47.360 --> 27:48.960
You need to sense what's happening with the environment.

27:48.960 --> 27:49.960
Right.

27:49.960 --> 27:52.000
Because we don't have the models to put in the simulation.

27:52.000 --> 27:54.680
So this is now a data driven modification.

27:54.680 --> 28:01.600
So where we can combine the data with learning that model of the environment with the optimization

28:01.600 --> 28:05.120
framework that I discussed earlier in a feedback loop.

28:05.120 --> 28:08.480
And we did that and we were able to actually, so we wanted to hop, I mean, this is a very

28:08.480 --> 28:09.480
simple robot.

28:09.480 --> 28:12.320
So this is not a walking robot, but it's kind of like, think about the bouncing ball again

28:12.320 --> 28:14.000
where we're going back to the basics.

28:14.000 --> 28:16.880
We wanted to make this thing hop at a specific height.

28:16.880 --> 28:21.640
We'd first tried without having a model of the granular train, and it wouldn't do it.

28:21.640 --> 28:25.560
Is this a, I'm getting hung up on the form factor here.

28:25.560 --> 28:30.720
Is this like a standalone robot that is, you know, what does this thing look like?

28:30.720 --> 28:31.720
All right.

28:31.720 --> 28:32.720
So that's a good question.

28:32.720 --> 28:33.720
Yeah.

28:33.720 --> 28:34.720
And is it suspended?

28:34.720 --> 28:35.720
Yeah.

28:35.720 --> 28:36.720
And some, you know, is it fixed in somewhere?

28:36.720 --> 28:37.720
Yeah.

28:37.720 --> 28:42.760
So this is a very, this was a very simple test bed meant to generate physics of terrain

28:42.760 --> 28:43.760
interaction.

28:43.760 --> 28:47.080
It was actually developed by a colleague of mine, Dan Goldman at Georgia Tech, who is a

28:47.080 --> 28:48.080
physicist.

28:48.080 --> 28:51.280
And this is work with Patricia Vella as well at Georgia Tech, who does machine learning

28:51.280 --> 28:52.280
stuff.

28:52.280 --> 28:53.840
So it's, it's a very simple thing.

28:53.840 --> 28:59.960
It's a motor with a spring between the, basically, the motor and the world.

28:59.960 --> 29:00.960
Okay.

29:00.960 --> 29:01.960
Okay.

29:01.960 --> 29:05.960
So basically it can, it can move a mass up and down to make this thing move on top of

29:05.960 --> 29:06.960
the spring.

29:06.960 --> 29:07.960
And then there's a spring.

29:07.960 --> 29:11.880
And then there's like a foot, right, between the granular terrain.

29:11.880 --> 29:12.880
Right.

29:12.880 --> 29:15.760
So it's, so this thing is only moving one dimension, it's a one dimensional, it's a one

29:15.760 --> 29:16.760
dimensional hopper.

29:16.760 --> 29:17.760
Yeah.

29:17.760 --> 29:18.760
It's one dimensional hopper.

29:18.760 --> 29:19.760
That's right.

29:19.760 --> 29:22.840
So again, any, any, what it does is it sits in a bed of poppy seeds.

29:22.840 --> 29:25.880
His poppy seeds are actually a really good model of different sand and dirt, but they

29:25.880 --> 29:28.720
don't get caught up in the actuator because they're big enough that they don't get into

29:28.720 --> 29:29.720
all the things.

29:29.720 --> 29:30.720
Okay.

29:30.720 --> 29:31.720
And the, and the details.

29:31.720 --> 29:32.720
Exactly.

29:32.720 --> 29:36.640
And what actually you do is is there's this bed of poppy seeds, but you don't want to

29:36.640 --> 29:40.120
have it be changing every time you hop on it because you can compact them.

29:40.120 --> 29:41.520
So it'd be more like hard terrain.

29:41.520 --> 29:44.280
So it actually aerates the bed between every hop.

29:44.280 --> 29:49.240
So you sort of move the poppy seeds, let them settle hop, let them settle hop.

29:49.240 --> 29:54.680
So you, you do a successive experiments where you have the same kind of initial condition

29:54.680 --> 29:58.640
in the poppy seeds just to make sure you have a consistent model you're learning.

29:58.640 --> 29:59.640
So that's a setup.

29:59.640 --> 30:01.480
So it's a very isolated little box.

30:01.480 --> 30:06.120
And then you can use vision or anything else to, to kind of identify what the forces are

30:06.120 --> 30:07.960
between the robot and the terrain.

30:07.960 --> 30:08.960
Okay.

30:08.960 --> 30:09.960
And so, and then we ran this experiment.

30:09.960 --> 30:12.240
Again, this was a proof of concept.

30:12.240 --> 30:15.600
It's probably one of the first examples of putting all these pieces together.

30:15.600 --> 30:18.200
And it kind of shows you, and this is an important point.

30:18.200 --> 30:23.080
It shows you where we're at in, you know, find learning and control and dynamics.

30:23.080 --> 30:27.680
If we have to go back to a 1D hopper and we're, and we publish a paper on it because it's

30:27.680 --> 30:32.440
new and interesting, right, as opposed to a humanoid robot, which is, you know, so you

30:32.440 --> 30:35.360
can imagine that we're just beginning this process and there's some other people working

30:35.360 --> 30:36.360
on this domain as well.

30:36.360 --> 30:41.200
But it's kind of, isn't it scary if you can just watch a beach and just say, yeah, exactly.

30:41.200 --> 30:42.200
Yes.

30:42.200 --> 30:43.680
And that's my advice to anybody.

30:43.680 --> 30:47.480
If a robot has changed and you try to kill it, you just run to a beach and you're going

30:47.480 --> 30:48.800
to be fun.

30:48.800 --> 30:53.720
So, or go on some ice or snow or something, you know, and it'll fall over.

30:53.720 --> 30:54.720
You'll be safe.

30:54.720 --> 30:55.720
Yeah.

30:55.720 --> 30:56.720
That's right.

30:56.720 --> 30:57.720
Huh.

30:57.720 --> 31:01.560
So we, we got those two pieces of learning like what's, what's next?

31:01.560 --> 31:09.480
I think it's, I mean, I think you did a good job exactly parsing the two forefronts is

31:09.480 --> 31:13.360
that we need to push in terms of machine learning on robotic systems.

31:13.360 --> 31:19.000
It's really, this is the key point is there's a lot of work right now on learning as a stand

31:19.000 --> 31:20.600
alone entity, right?

31:20.600 --> 31:21.600
Learn everything.

31:21.600 --> 31:22.600
Right.

31:22.600 --> 31:26.480
And with this, if I can interrupt, as a point of reference for folks that are listening

31:26.480 --> 31:30.320
that want to hear a little bit more about that perspective, a good place to start would

31:30.320 --> 31:35.720
be the interview I did with Peter Abiel, you would be, yes, I know Peter, well, I know.

31:35.720 --> 31:37.680
So Peter is precise, he does fantastic work.

31:37.680 --> 31:43.720
I really respect the work he does, but the idea there is to learn everything.

31:43.720 --> 31:44.720
Right.

31:44.720 --> 31:45.720
And we talked about that.

31:45.720 --> 31:46.720
Yeah.

31:46.720 --> 31:50.520
And I am very strong and adamant is that that is not the right answer.

31:50.520 --> 31:52.720
Now, people will disagree with me and that's okay.

31:52.720 --> 31:54.360
It's okay to disagree in academia.

31:54.360 --> 31:55.960
It shows that the problems aren't solved.

31:55.960 --> 32:00.280
There's no, you know, physical systems, robot arms, even, you know, he does manipulation.

32:00.280 --> 32:01.280
And stuff.

32:01.280 --> 32:03.200
Have this wonderful structure we can exploit.

32:03.200 --> 32:04.200
Yeah.

32:04.200 --> 32:08.000
They might be high dimensional, but they live on manifolds, which are low dimensional

32:08.000 --> 32:10.640
surfaces in this high dimensional space.

32:10.640 --> 32:15.440
Let's use physics and control to push the system to this low dimensional space and then

32:15.440 --> 32:16.840
learn on that space.

32:16.840 --> 32:20.120
It will work orders of magnitude better, I promise, right?

32:20.120 --> 32:25.040
Well, I promise in that, I think I'm right with my opinion, which I may be wrong, and that

32:25.040 --> 32:26.040
would be great.

32:26.040 --> 32:31.160
And at the point when, when a robot does a back flip with no knowledge of his physics

32:31.160 --> 32:36.160
or dynamics, a humanoid robot, then I'll be like, okay, I am ready to listen to the pure

32:36.160 --> 32:37.160
learning approach.

32:37.160 --> 32:41.000
But, you know, then the robotics community, the proof is in the pudding, right?

32:41.000 --> 32:44.000
You know, as they sometimes say, the proof is on the robot.

32:44.000 --> 32:49.760
So, and the reality is we can do so much more with zero learning, model based approaches

32:49.760 --> 32:53.040
on physical hardware than we can with learning.

32:53.040 --> 32:55.960
Now that being said, I'm not trying to advocate that that's the end of the story.

32:55.960 --> 32:59.840
Like I've said through this whole interview, there's limitations to this.

32:59.840 --> 33:02.280
And that's where learning will play a huge role.

33:02.280 --> 33:05.600
So I think the forefront is don't learn everything.

33:05.600 --> 33:09.440
Don't fall into this trap of a big shiny black box that you put in what you want, and it

33:09.440 --> 33:12.040
spits out the right answer.

33:12.040 --> 33:16.280
If only because from a scientific perspective, I find that very unsatisfying too, that's

33:16.280 --> 33:17.960
a separate point.

33:17.960 --> 33:24.120
But I, my opinion is don't fall into that paradigm because you're, you're restricting,

33:24.120 --> 33:29.080
or you're limiting yourself in your worldview instead, unify, unify, unify, unify.

33:29.080 --> 33:33.160
So that's my argument on the forefront of learning is what, what else does unify

33:33.160 --> 33:34.160
mean for you?

33:34.160 --> 33:41.880
I mean, bring the learning, physics, dynamics, computation, control, mechanical design, actuators,

33:41.880 --> 33:42.880
all those pieces.

33:42.880 --> 33:47.680
What's beautiful about robots is they are a microcosm of the universe in some way.

33:47.680 --> 33:52.680
They're completely self-consistent systems that you have complete control and you create.

33:52.680 --> 33:53.680
Yeah.

33:53.680 --> 33:57.240
You have computation, you have actuators, you have all these wonderful things.

33:57.240 --> 34:02.600
So use that, you know, and understand those different pieces at a deep level, and then

34:02.600 --> 34:04.360
you'll really understand how to put them together.

34:04.360 --> 34:10.280
So that's what I mean, unify, unify, all of computation, control, design, and learning.

34:10.280 --> 34:15.680
Understand where they all fit together relative, use the strengths of each, you know, to exploit

34:15.680 --> 34:16.680
them to their maximum.

34:16.680 --> 34:22.200
And that's where I think the really fun stuff is going to come in the next couple years,

34:22.200 --> 34:23.200
in my opinion.

34:23.200 --> 34:27.120
You know, I could be dead wrong in a year from now, if I am, I'm happy to admit it.

34:27.120 --> 34:31.080
But I think that's really where the future is in terms of learning and roboticism.

34:31.080 --> 34:34.360
And do you have any predictions in terms of what that really fun stuff is going to look

34:34.360 --> 34:35.640
like for us?

34:35.640 --> 34:39.480
The forefront is the following, in my opinion, it's getting robots out of the lab into

34:39.480 --> 34:40.480
the lab.

34:40.480 --> 34:42.840
You know, there's been a couple examples of that.

34:42.840 --> 34:45.440
We've taken some of our robots out of the lab in limited context.

34:45.440 --> 34:48.480
Boston and Amick still has some of the best videos where I was walking around kind of

34:48.480 --> 34:49.720
in snow and stuff like that, right?

34:49.720 --> 34:50.720
Yeah.

34:50.720 --> 34:51.720
That was purely a result of that.

34:51.720 --> 34:56.760
Actually, the four, the four-legged ones are the, they had a biped outside in the snow

34:56.760 --> 34:57.760
in one video.

34:57.760 --> 34:59.680
It was about a year or two years ago now.

34:59.680 --> 35:01.640
And again, remember, that's purely reactive.

35:01.640 --> 35:02.640
There was no learning there.

35:02.640 --> 35:06.680
It was walking in snow and on uneven terrain, only through the robustness of the algorithms

35:06.680 --> 35:08.000
that are on there, right?

35:08.000 --> 35:12.960
So, but I think that's really the forefront is, you know, get out of structured environments,

35:12.960 --> 35:17.400
get out of the lab and get on dynamic systems.

35:17.400 --> 35:24.960
So I'm very, very pure, for example, manipulation tasks as they're deceptive in their simplicity.

35:24.960 --> 35:29.200
You know, when a robot can't fall over, you can always correct, right?

35:29.200 --> 35:35.160
And there's a robustness there that when you go to humanoid robots and dynamic robots,

35:35.160 --> 35:36.480
you don't have anymore.

35:36.480 --> 35:41.000
So get a dynamic robotic system, whatever that is, a four-legged robot, a two-legged robot,

35:41.000 --> 35:45.760
a hopping robot, whatever it happens to be out into the real world and make it do cool

35:45.760 --> 35:46.920
stuff.

35:46.920 --> 35:51.560
And that's to me the way of really proving that you understand what's going on, you know,

35:51.560 --> 35:56.400
because that will take all these things we mentioned, especially in an autonomous context.

35:56.400 --> 35:57.840
So this is the second point.

35:57.840 --> 36:01.880
We actually just started a center for autonomy at Caltech called Cast.

36:01.880 --> 36:08.000
And it's really aimed at doing these things is how do we get robots into the wild and not

36:08.000 --> 36:10.080
prescript them all the way, right?

36:10.080 --> 36:14.400
And that's really what I mean by getting into the wild is tell a robot go from A to B outside,

36:14.400 --> 36:15.400
right?

36:15.400 --> 36:19.400
Walking robot or humanoid robot and by A to B it might be over a beach.

36:19.400 --> 36:22.000
It might be, you know, through some ice and snow.

36:22.000 --> 36:23.760
How do we do that?

36:23.760 --> 36:24.760
What's that?

36:24.760 --> 36:29.080
So we actually frame these questions in the context of moonshots for Cast.

36:29.080 --> 36:30.080
Okay.

36:30.080 --> 36:35.920
So just to give us a sense of how hard they are and by moonshot it really a lot of these

36:35.920 --> 36:40.320
are moonshots, meaning we could do it if we had massive amount of resources and people

36:40.320 --> 36:45.760
concerted, but one example of a moonshot is have a robot walk the Pacific Crest Trail.

36:45.760 --> 36:51.360
So this is a trail that goes from Mexico to Canada and have it do it autonomously.

36:51.360 --> 36:56.880
So what would it take to do that, you know, and that exactly would require all the pieces

36:56.880 --> 37:00.520
we've discussed today, plus many more that we don't even know, we don't know yet.

37:00.520 --> 37:05.560
But that's the kind of thing we need to be thinking about I think is pushing these boundaries

37:05.560 --> 37:07.440
of what we can do with robotic systems.

37:07.440 --> 37:11.800
And in an autonomous way, so bringing autonomy in it and bringing them and understanding

37:11.800 --> 37:15.720
how that fits with both the mathematical representation of behaviors and learning and

37:15.720 --> 37:17.640
where those each can play a role.

37:17.640 --> 37:21.360
So to me, that's the direction of push.

37:21.360 --> 37:24.520
It's challenging, but fun, but it's time for the real world.

37:24.520 --> 37:25.520
It's kind of where we're at.

37:25.520 --> 37:28.560
For a long time, we couldn't even get robust to do stuff in our labs that was all that

37:28.560 --> 37:29.560
interesting.

37:29.560 --> 37:33.440
And now we're at the point where we can do some cool stuff in our labs, so leave the lab.

37:33.440 --> 37:36.760
When I'm struggling with a little bit and trying to bridge our early conversation about

37:36.760 --> 37:44.440
these very scripted, rigid tasks, and even some of the stuff that the Boston Dynamics

37:44.440 --> 37:49.560
kind of walking in snow, we have to be incorporating in sensors.

37:49.560 --> 37:55.000
Is even the most primitive stuff is that I'm thinking of that as just kind of the actuators

37:55.000 --> 37:57.840
and not sensors or is that not the way to think about it?

37:57.840 --> 38:01.840
I mean, they all have sensors, the question is, what are the sensors doing and what information

38:01.840 --> 38:02.840
are they taking?

38:02.840 --> 38:03.840
Yeah.

38:03.840 --> 38:06.080
And how are they fit into it?

38:06.080 --> 38:08.200
These sensors are part of all of this.

38:08.200 --> 38:10.960
The question is, what sensors and what are they sensing?

38:10.960 --> 38:15.800
So for all the, from the back flip on, again, I can't speak, I know they're hardware all

38:15.800 --> 38:19.800
the way in, but roughly speaking, you have encoders that look at the angles of all the

38:19.800 --> 38:20.800
joints.

38:20.800 --> 38:24.160
You have an IMU, a inertial measurement unit that tells you the global orientation of the

38:24.160 --> 38:25.160
robot, right?

38:25.160 --> 38:26.160
Some accelerometers.

38:26.160 --> 38:27.160
Some accelerometers in that case.

38:27.160 --> 38:28.160
Exactly.

38:28.160 --> 38:29.160
Exactly.

38:29.160 --> 38:31.760
And then there's sometimes some sort of sensing of the environment, has your flip touched

38:31.760 --> 38:33.400
down, and that's a pretty essential one.

38:33.400 --> 38:35.120
So when we walk with Doris, I can tell you that.

38:35.120 --> 38:37.720
But we need are those three main components.

38:37.720 --> 38:39.400
We need to know when the foot's on the ground.

38:39.400 --> 38:43.480
So since the impact with the ground, we need to know what the angles are on all the joints.

38:43.480 --> 38:49.880
And we need to know, again, an IMU, an accelerometer, we need to know the global orientation of

38:49.880 --> 38:51.840
the robot relative to the robot.

38:51.840 --> 38:58.040
Those three pieces of his information are all you really need to have a robot do a dynamic

38:58.040 --> 39:00.600
thing in a constrained context.

39:00.600 --> 39:01.600
Right?

39:01.600 --> 39:02.920
That means you know the environment, right?

39:02.920 --> 39:07.320
You don't need computer vision if you know how high the blocks are and where they're located

39:07.320 --> 39:08.320
relative to the robot.

39:08.320 --> 39:10.880
And you set the robot up in the same spot every time, right?

39:10.880 --> 39:14.440
So obviously when you start to do more unstructured things, you're going to have to bring in other

39:14.440 --> 39:17.000
sensors, cameras, of course.

39:17.000 --> 39:20.840
And that's one, you know, Pietro Perona's talking, another professor at Caltech is talking

39:20.840 --> 39:24.360
with me at the machine learning summit, and he does computer vision.

39:24.360 --> 39:27.000
And we've started to say, how can we integrate these two pieces together?

39:27.000 --> 39:30.840
So we actually have a robot in our lab called Cassie, where we, he did his algorithms to

39:30.840 --> 39:34.800
parse, you know, where they can determine the pose of people.

39:34.800 --> 39:37.760
And we want to use those pose information to have the robot do something.

39:37.760 --> 39:42.520
So we want the vision of the robot, what the robot's seen, to feed into its behaviors.

39:42.520 --> 39:43.520
Right?

39:43.520 --> 39:45.280
And we're just starting this track, but that gives you an idea.

39:45.280 --> 39:47.200
So in this case, we need a vision system.

39:47.200 --> 39:48.200
You might need four sensors.

39:48.200 --> 39:51.440
If you're going to observe the environment for the hopping behaviors we discussed, you're

39:51.440 --> 39:55.680
going to need to have a really nice notion of what the forces are on the system.

39:55.680 --> 39:58.360
So the more, the more you want to do, the more sensors you need.

39:58.360 --> 39:59.360
Yeah.

39:59.360 --> 40:04.120
And then going back in our conversation, this is very consistent with the perspective

40:04.120 --> 40:05.880
of the human body.

40:05.880 --> 40:10.800
So if you want to walk on flat ground with no obstacles, you can actually, you need very

40:10.800 --> 40:12.280
little sensors, right?

40:12.280 --> 40:16.120
You kind of know when your foot strikes the ground and the rest is pretty, right?

40:16.120 --> 40:20.120
If you've never been on ice before and you take somebody on ice, look at the way they

40:20.120 --> 40:21.120
have sensing.

40:21.120 --> 40:22.120
They're doing a lot of sensing.

40:22.120 --> 40:24.640
They're doing a lot of computation because they're learning, but then watch a couple

40:24.640 --> 40:29.840
minutes on ice and people kind of settle in and they're clearly, they've whittled down.

40:29.840 --> 40:32.880
They've taken all their sensor information and they've decided which sensor information

40:32.880 --> 40:34.120
is important.

40:34.120 --> 40:35.320
And they're using that.

40:35.320 --> 40:37.920
And that's part of the problem, too, is how do you take all this information in with

40:37.920 --> 40:41.280
your environment and whittle out the stuff that's actually relevant to what you're trying

40:41.280 --> 40:43.200
to do and an individual motion permit?

40:43.200 --> 40:47.680
I think that's a great example or a great way of articulating it that really gets at what

40:47.680 --> 40:49.200
what I was struggling with.

40:49.200 --> 40:52.600
It's the, you know, when you think about the human on the ice, there's, you know, there's

40:52.600 --> 40:58.440
that bit of learning and, you know, if you take that to, I'm trying to, trying to reconcile

40:58.440 --> 41:03.480
that with, you know, the Boston, the Boston time I was walking in the snow.

41:03.480 --> 41:08.080
You know, that, you're, we're saying that that robot is not learning like what is it doing

41:08.080 --> 41:12.200
with that sensor data that's allowing it to be more robust than, you know, what we

41:12.200 --> 41:13.200
do.

41:13.200 --> 41:14.200
Well, okay.

41:14.200 --> 41:18.720
So, you notice it was walking in snow and up, you know, small terrain differences, but

41:18.720 --> 41:20.080
not ice.

41:20.080 --> 41:23.320
So the deciding factor here is friction.

41:23.320 --> 41:27.560
So as long as it has sufficient friction when the foot touches down, you can do the same

41:27.560 --> 41:32.680
behavior you do on firm ground on non-firm ground as long as it's reasonably close, right?

41:32.680 --> 41:37.080
As long as the foot doesn't slip as long as, or not too much, or if it slips, you can

41:37.080 --> 41:38.080
catch yourself.

41:38.080 --> 41:42.840
I may be confusing videos in MMI's, but there's one where like it's winning up here.

41:42.840 --> 41:43.840
Oh, it slips on ice.

41:43.840 --> 41:44.840
Yeah.

41:44.840 --> 41:47.080
And there's one where it even slips on icing and catches itself, right?

41:47.080 --> 41:51.720
And again, it slips on a small patch of ice and catches itself and then it's off the

41:51.720 --> 41:52.720
ice.

41:52.720 --> 41:53.720
Yeah.

41:53.720 --> 41:56.720
This is, I think, what your question is and where we can separate is that there's a difference

41:56.720 --> 42:03.360
between reactive behavior that's robust enough to handle terrain differences with learning

42:03.360 --> 42:05.320
a new behavior itself.

42:05.320 --> 42:06.320
Yes.

42:06.320 --> 42:07.320
And that's the difference.

42:07.320 --> 42:12.640
So if you're walking along and you slip on a little puddle, right?

42:12.640 --> 42:16.360
Watch a person when they slip, they go, right, and they catch themselves and then they

42:16.360 --> 42:17.360
keep walking.

42:17.360 --> 42:18.360
That's not a learned behavior.

42:18.360 --> 42:20.320
That's a reactive behavior, right?

42:20.320 --> 42:21.680
Or you miss a step.

42:21.680 --> 42:22.680
That's a great one.

42:22.680 --> 42:25.520
When people don't know a step's coming and there's a step, and then you see them fall,

42:25.520 --> 42:27.600
and then you know this feeling too, right?

42:27.600 --> 42:32.120
You're already falling and catching yourself when you realize, oh, I just fell down a step,

42:32.120 --> 42:33.120
right?

42:33.120 --> 42:36.080
Your body's doing stuff before you even realize what's happened, right?

42:36.080 --> 42:37.080
So that's a great example.

42:37.080 --> 42:41.960
So next time you fall, please, I mean, hopefully I don't fall on purpose, but after you kind

42:41.960 --> 42:45.440
of catch yourself from falling, think back and realize you did all that stuff before

42:45.440 --> 42:47.400
you even thought about what you were doing.

42:47.400 --> 42:48.400
That's reactive.

42:48.400 --> 42:49.920
So that's what Boston Dynamics does.

42:49.920 --> 42:54.120
Their controllers are so robust and they're very impressively robust that they can react

42:54.120 --> 42:56.280
to all these different things and be robust to it.

42:56.280 --> 43:02.040
It's not fair to say that, you know, it's simply kind of actuating, you know, these motors

43:02.040 --> 43:07.200
through a series of pre-plan points and, you know, that's how it's doing, you know, walking

43:07.200 --> 43:11.960
or doing flips or something like, it's more, it's more robust, it's more hierarchical.

43:11.960 --> 43:12.960
It is.

43:12.960 --> 43:13.960
There is more.

43:13.960 --> 43:14.960
Yeah.

43:14.960 --> 43:15.960
And the same with our robots.

43:15.960 --> 43:20.000
My description of moving the robot through a series of pre-plan points or trajectories

43:20.000 --> 43:24.560
as we call it is a simplistic representation of what actually goes on the hard one.

43:24.560 --> 43:25.560
That's part of it.

43:25.560 --> 43:26.720
That's actually mathematically.

43:26.720 --> 43:30.480
That's what we call sort of the feed-forward term or the nominal behavior.

43:30.480 --> 43:33.080
So assuming everything's perfect, that's what it will do.

43:33.080 --> 43:35.200
But we do, we need to stabilize.

43:35.200 --> 43:37.000
We talked about stable periodic motions.

43:37.000 --> 43:38.000
We need to stabilize.

43:38.000 --> 43:39.640
We need to robustify that.

43:39.640 --> 43:40.640
Okay.

43:40.640 --> 43:44.480
And so for that, you add things that bring you back to that orbit if needed.

43:44.480 --> 43:47.760
And that's where this robustness and reactive behavior comes from.

43:47.760 --> 43:49.400
There is a hierarchy.

43:49.400 --> 43:53.280
And so for Boston and Amics, that hierarchy is based on foot placement, typically based

43:53.280 --> 43:57.720
on assuming based on the rubber papers from the 80s and all this, is that they sort of

43:57.720 --> 44:02.360
based on, you know, the orientation of the robot at a high level, it'll place its foot

44:02.360 --> 44:03.520
in different locations.

44:03.520 --> 44:04.520
It still has the nominal.

44:04.520 --> 44:06.120
Listen to a Boston and Amics video.

44:06.120 --> 44:07.120
Yeah.

44:07.120 --> 44:08.720
And you'll notice it's very time-based.

44:08.720 --> 44:10.960
That's because that's the nominal trajectories.

44:10.960 --> 44:11.960
You hear the consistent.

44:11.960 --> 44:12.960
I'm hearing that in my head.

44:12.960 --> 44:15.640
And it doesn't change, right?

44:15.640 --> 44:16.640
So all that's changing.

44:16.640 --> 44:18.040
So that's the trajectory points.

44:18.040 --> 44:19.040
Okay.

44:19.040 --> 44:21.720
But then on top of that, there's a layer where it says, well, if I'm leaning too far

44:21.720 --> 44:25.800
to my left, I mean, this is a simplification, put my foot out here.

44:25.800 --> 44:26.800
Okay.

44:26.800 --> 44:27.800
React to that motion.

44:27.800 --> 44:28.800
Okay.

44:28.800 --> 44:30.080
And so there's that reactive layer as well.

44:30.080 --> 44:31.240
So that's the robustness.

44:31.240 --> 44:35.960
And that's what you see acting when the big dogs on ice, when it's walking in snow.

44:35.960 --> 44:40.080
That's a reactive behavior that's all just a hierarchical algorithm, right?

44:40.080 --> 44:41.720
Mathematical algorithm.

44:41.720 --> 44:42.720
It's not learning.

44:42.720 --> 44:44.280
That's what happens when you go on ice, right?

44:44.280 --> 44:49.080
So just, and so in terms of ice, it's not a single perturbation to your behavior.

44:49.080 --> 44:50.080
Yeah.

44:50.080 --> 44:51.920
It's an entirely new behavior you have to come up with.

44:51.920 --> 44:53.080
So that's a new motion primitive.

44:53.080 --> 44:54.080
Right.

44:54.080 --> 44:55.400
So you have to learn that.

44:55.400 --> 44:58.240
And by learning that, I don't necessarily mean machine learning that primitive.

44:58.240 --> 45:02.200
What I mean is you'd have to learn the fact that on ice, the friction model is different.

45:02.200 --> 45:03.200
Yeah.

45:03.200 --> 45:04.200
Learn that friction model.

45:04.200 --> 45:08.040
Put it back into the mathematical algorithms, modify the nominal behaviors, and then

45:08.040 --> 45:09.760
make yourself robust.

45:09.760 --> 45:11.440
So that's where this feedback loop comes in.

45:11.440 --> 45:13.920
And that's where learning will play a role.

45:13.920 --> 45:14.920
And that's kind of what you do.

45:14.920 --> 45:17.880
I mean, if you look at a human, you go on ice, and basically you're shuffling your feet

45:17.880 --> 45:18.880
around.

45:18.880 --> 45:20.120
You're learning the friction properties of ice.

45:20.120 --> 45:22.680
Once you have a pretty good model of those friction properties, you plug it into your

45:22.680 --> 45:27.560
nominal sort of optimization method, which sits at your spinal cord.

45:27.560 --> 45:31.160
And then once you've got that down, you can walk fairly normally because you've learned

45:31.160 --> 45:32.480
the thing you didn't know.

45:32.480 --> 45:36.120
And then you go back to doing the thing that you always do with a slight modification

45:36.120 --> 45:38.640
based on the different physical model of the world.

45:38.640 --> 45:40.240
So that's kind of the way we work, right?

45:40.240 --> 45:41.240
Right.

45:41.240 --> 45:42.240
It's funny.

45:42.240 --> 45:45.440
The human systems, I think, are great inspiration at every level because it completely mirrors

45:45.440 --> 45:49.200
what we're finding on robotic systems inspirationally.

45:49.200 --> 45:52.960
Again, not in terms of we need to mimic what actually is happening in the human body.

45:52.960 --> 45:57.840
But the higher arcies, where learning plays a role, where dynamics plays a role is really

45:57.840 --> 46:01.800
clear on the human in the human body, I think it's great inspiration for robotic systems.

46:01.800 --> 46:04.320
And the same parallels are happening on the neural network side.

46:04.320 --> 46:07.480
Like we're taking inspiration from these things, bringing them in and try to evolve the

46:07.480 --> 46:09.360
way we think about the learning side.

46:09.360 --> 46:11.440
So definitely inspiration is huge.

46:11.440 --> 46:15.160
But again, a word of caution is stay away from mimicry, right?

46:15.160 --> 46:19.000
Don't just try to create the exact same thing on a robot or an AI, right?

46:19.000 --> 46:23.280
Oh, well, there's, you know, X number of neurons in the human mind.

46:23.280 --> 46:26.080
So if we can hit that neuron, we'll have a smart robot, right?

46:26.080 --> 46:27.080
Right.

46:27.080 --> 46:29.240
No, it's not, that's not the way it's just like if you flap something, it won't necessarily

46:29.240 --> 46:30.240
fly.

46:30.240 --> 46:32.600
But yes, look at the structures.

46:32.600 --> 46:38.400
The really the structures are our key and try to understand what they mean and then realize

46:38.400 --> 46:39.400
them on robotic systems.

46:39.400 --> 46:40.400
Great.

46:40.400 --> 46:41.400
Great.

46:41.400 --> 46:42.400
Well, I really enjoy this conversation.

46:42.400 --> 46:46.600
Any final words for folks or how can folks find you learn more about your work?

46:46.600 --> 46:49.840
The internet is available to find my stuff.

46:49.840 --> 46:54.640
My lab website is bipedorobotics.com, just a simple name.

46:54.640 --> 46:57.960
You can find me on the Caltech website, just Google AirNames and there should be enough

46:57.960 --> 46:58.960
stuff.

46:58.960 --> 46:59.960
My students are on there too.

46:59.960 --> 47:00.960
They do amazing stuff.

47:00.960 --> 47:03.560
A lot of the videos you see are from my grad students.

47:03.560 --> 47:09.720
If you're interested in robotics, please come to grad school somewhere.

47:09.720 --> 47:14.120
If you feel free to ping us, if you're really interested in Caltech.

47:14.120 --> 47:17.080
And in general, keep studying these problems.

47:17.080 --> 47:20.480
This is, we're at a fascinating point right now.

47:20.480 --> 47:22.280
And I think that's amazing.

47:22.280 --> 47:27.120
My couple of final closing statements are, this is massively exciting, but be careful

47:27.120 --> 47:28.600
of the hype.

47:28.600 --> 47:31.840
Instead of just going for the hype, think about these things.

47:31.840 --> 47:36.960
Think about where learning will play a role, look to unification, because we can achieve

47:36.960 --> 47:42.440
these promises that are being made, but we have to be very smart about how we approach

47:42.440 --> 47:43.440
the problem.

47:43.440 --> 47:45.000
And that's what makes it fun right now.

47:45.000 --> 47:46.280
These are not solved problems.

47:46.280 --> 47:50.880
And anybody that says they're solved, I think doesn't know what they're talking about.

47:50.880 --> 47:55.280
We were at this point where we're really trying to understand how learning and, for example,

47:55.280 --> 47:57.880
robotics systems work together.

47:57.880 --> 48:03.440
And it's an exciting time to be doing this, so I encourage everyone to really dig into

48:03.440 --> 48:05.840
it and see what they can learn.

48:05.840 --> 48:06.840
Oh, thanks Aaron.

48:06.840 --> 48:07.840
Thanks a lot.

48:07.840 --> 48:15.520
Alright everyone, that's our show for today.

48:15.520 --> 48:20.440
Thanks so much for listening, and for your continued feedback and support.

48:20.440 --> 48:24.840
For more information on Aaron, or any of the topics covered in this episode, head on

48:24.840 --> 48:29.760
over to twimlai.com slash talk slash 87.

48:29.760 --> 48:36.680
To follow along with the AWS re-invent series, visit twimlai.com slash re-invent.

48:36.680 --> 48:43.840
To enter our Twimlai 1 mil contest, visit twimlai.com slash twimlai 1 mil.

48:43.840 --> 48:48.680
Of course, we'd be delighted to hear from you, either via a comment on the show notes

48:48.680 --> 48:55.040
page or via Twitter to add Twimlai or add Sam Charington.

48:55.040 --> 48:58.800
Thanks again to Intel Nirvana for their sponsorship of this series.

48:58.800 --> 49:03.440
To learn more about their role in deep lens and the other things they've been up to, visit

49:03.440 --> 49:06.040
intelnervana.com.

49:06.040 --> 49:19.000
And of course, thanks once again to you for listening, and catch you next time.

