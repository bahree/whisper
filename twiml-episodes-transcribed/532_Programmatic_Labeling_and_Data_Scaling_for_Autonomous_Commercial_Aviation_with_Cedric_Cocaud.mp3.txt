All right, everyone.
Welcome to another episode of the Twimmel AI podcast.
I am your host, Sam Charrington,
and today I'm joined by
Cedric Coco, Chief Engineer of
the Wayfinder Group at Acubed.
Cedric, welcome to the podcast.
Thank you for having me.
I'm looking forward to digging into our conversation.
We'll be talking about a lot of
cool things that you are doing with
data-centric AI in particular,
your journey with data labeling for
your specific use case,
to get us started.
I'd love to have you share a little bit about your background
and how you came to work in ML&AI.
Yes, thank you.
My name is Cedric Coco.
I'm the Chief Engineer of the Wayfinder Group at Acubed,
and that is Airbus Innovation Center in Silicon Valley.
My background, I have a PhD in computer vision navigation,
and I used to come from the space sector,
so I spent part of my life trying to land spacecraft
autonomously using one camera on near-Earth asteroids.
I lowered my altitude of interest in 2016
when I joined Airbus and we started to work
on aviation rather than spacecraft.
The thing that really attracted me there
was the UAM concept, the urban air mobility or air taxis.
At the time, there was this new project
that started called Bahana.
It was a technology demonstrator,
basically a first prototype of one of those air taxis.
The whole goal is to provide a completely new way
of commuting people within urban areas and suburban areas,
and this time at scale in the sense of really having thousands
of vehicles in a city trying to have passenger
going from one area to another to solve the congestion problem
that we have in all big urban centers.
I was part of the sense and avoid crew back in the days,
and the central focus was basically developing machine learning
and vision for the sense avoid system for urban air mobility.
As we moved forward, we realized that this technology
was really applicable to a larger range of application
for aviation and airbus,
and we started to apply the same technology
to commercial aircraft.
This is where we are now,
the focus of Wayfinder is to build more autonomy
into commercial aircraft.
So you mentioned you got started in this air taxi project,
Bahana, what were the key technology innovations
that drove that project that ultimately led to what you're working on now?
Yeah, so air taxis is a completely new breed of aircraft.
We're looking at an electric propulsion system.
We're looking at basically a vertical take-off and landing system
that can be converted into a fixed wing
during the cruise phase for efficiency purposes.
But the other aspect that is driving this innovation
and this revolution in the aviation sector
is how you build them.
Currently commercial aircraft are built
in the hundreds every year.
You could hit the 1000, but we're not at the 10,000 or even more
in terms of scale.
So one of the key concepts that most urban air mobility companies
in the area and across the world are thinking about it
as how to combine forces with the automotive industry
and with their skills and their production capabilities
to be able to scale up production
while having the safety standard
and the reliability standard of aviation.
So this is a huge challenge and on the technology side
there's two aspects that are really important.
One of them is the electric propulsion system
which is quite new and the other one is autonomy.
So one of the key and fundamental of air taxis to make it viable
is to have a fully autonomous solution down the road.
Many are taking roads where they go through an intermediate step
where it's going to be piloted.
But the end goal is clear for everybody
we need to go for a fully autonomous system.
And so that has multiple objectives.
One of them is how do you handle traffic at scale
over a city with all those aircraft?
The other one is when that system fails to ensure
the proper distances between all the vehicle,
what do you have on board to detect
that such distances have been so violated
and how do you respond adequately
so that you don't create more risks to other vehicles.
So you have this layered approach between air traffic control,
between deconfliction, reserving the airspace
and then ultimately your sense and avoid
on the aircraft itself,
and then you have to have the ability to perceive your environment
on the aircraft to understand what it is
and to make the proper decisions that minimize risks
and ensure that the passenger is delivered safely
to its destination.
So my role and the role that the team I was in at the time
was ready to develop that sense and avoid system
that last piece of this onion model
for ensuring the safety of passengers.
And I imagine that translating that
into the commercial side of things,
there are some foundational elements that translate,
but it sounds like a very different problem.
It is a very different problem
because commercial aircraft operates
in a completely different airspace
and that airspace is highly controlled.
So from the time that you take off from the airport
where the tower helps you and guides you,
you have already corridors that are reserved for you.
So this is a very interesting problem
because bringing autonomy to commercial aircraft
when you really look at it
is probably not as hard as autonomous or self-driving cars.
Our environment is a lot more structured.
We have autopilot that can fly the aircraft
when you're in the air during the cruise phase
and we had them for decades now.
And they are up to the safety level
and the reliability level that you need
to be able to transport 350 passengers.
The other aspect is our pilot
are actually skilled, trained professionals
not to criticize anybody driving in the area,
but depending on our expertise,
our level of being tired or not,
let's say drivers are more at risk for this.
So we have those very skilled pilots.
We have this aircraft that already has
a good level of autonomy built in.
But now what we need to tackle
is when you increase the scale of this air traffic,
you decrease the distances
and especially in terminal areas,
how many aircraft can you actually land
at an airport and how many can take off
within this small volume
without creating risks for anyone.
And there is certain volume
that manual operation can control
and can tackle.
But if you want to go beyond that,
then autonomy is a must go
to be able to maintain the same level of safety
while increasing volume, while increasing air traffic.
And this is where we come into play
because we are looking at a range of autonomous functions
to be able to take that leap toward this next generation aircraft
and this future where basically
the air traffic would increase significantly.
And just to give you some numbers,
based on the trends that we've seen before COVID,
the air traffic doubled every 15 years.
And right in 2019,
commercial aircraft carried 4.9 billion passengers.
So the application that we're working on
is technically going to touch about 60% of the planet
if you look at the numbers in 2019.
And then if you look at doubling that,
meaning that roughly you would touch 9 to 10 billion people a year,
the need in terms of safety,
the needs in terms of making sure
that this fully-autonomated system works well
is orders of magnitude beyond
most of the other application that we see there.
And when you talk about increasing the volume
and the requirements that that places
especially around the terminal,
the terminals in the terminal area,
I'm thinking that one of the major factors we're talking about
is like the spacing and approaches and landings
and things like that.
And there are parameters around where you want that to be
if it's being done manually.
And the idea is that if it's fully autonomous
that you can kind of shrink that spacing, is that the idea?
So there's two things that we're really trying to achieve.
One of them is that there is a lot of things
that the pilot must do when he approaches the terminal,
the airport, and when he navigates either.
So either when he's doing,
he's landing the aircraft, he's in the taxi phase, or he's taking off.
And what we want to build is enough autonomy in those aircraft
so that the pilot stops being worried
about all those small details plus the strategic aspects
and can really focus only on the strategic part.
For that, there's a range of functions
that needs to be built in.
So for example, autonomous landing
so that the person can focus on other aspects
than just controlling the aircraft toward the right light slope
to be able to touch the runway at the right point.
All of that can be abstracted out.
The aircraft can take care of it
and the pilot can focus his attention
on more strategic aspects of the flight.
One of them is, for example,
what is the route that he will need to take
to confirm that this is the right runway
that he needs to land at, for instance.
And during the taxi phase, especially,
it's pretty crowded on the runway
and especially when you're getting close to the gates.
So being able to have a system that keeps an eye
on all that traffic around you
and alerts you when really your attention is needed
for one particular aspect
is really going to help lower the workload of the pilots
and help the overall operation to be more efficient
and safer as well.
I'm going to say this is where ML becomes
a critical component of the system
because doing all of that autonomous detection
of the runway, autonomous assessment of where your aircraft is
during that landing segment
and during the taxi phase,
making sure that the aircraft is really
where it's supposed to be on the runway
that there's no object around.
So the collision protection function is key.
So all of those requires a heavy dose
of perception decision making.
And the range of things that you see
in the complexity of the situation
makes it hard for classical approaches to work
and really requires the power of ML
to be able to ensure the performance
and the reliability that we expect out of that function.
Yeah, let's dig into that a little bit more deeply.
You reference autonomous vehicles earlier
as kind of a comparison in urban taxi contexts.
There's obviously a lot of investment happening
in that type of autonomy right now
from an ML and AI perspective.
But again, this sounds like a very different problem.
Can you kind of elaborate on some of the ways
that from a technical perspective,
the problem that you're dealing with
differs from what someone who's working on AV cars
is worth thinking about?
Yeah, that's a very interesting question
because one of the strategies here
is to leverage a lot of the work
that has been done in the self-driving car industry.
They've been starting this decades ago.
And so there's a lot of legacy, a lot of system
that is available.
And so for us to be able to speed up our development,
being able to leverage all of that work
is to our benefit.
However, there's a certain limit
at which we can transfer and re-adapt that technology
without tuning it and actually
or changing it from the bottom up.
So for instance, cars needs to see in 2D plus heights,
but it's a 2.5D problem.
They need to see, you know, 200, 300 meters in front of them.
And maybe the autonomous trucks needs to see a little further.
But in our case, when the aircraft gets close to an airport,
we need to see kilometers ahead of time.
And the object that we're trying to see
are actually very small in the image.
And we need very high-resolution image
to be able to have just enough pixel enough information
to give to the ML or other algorithms
to be able to make heads and tails
of everything that they see in there.
So typically, the cameras that we need to use
to be able to do those functions
are in the order of 10 to 15 megapixels
versus in the car industry in the order of two,
maybe five megapixels for very high-resolution.
And then in our case, because we operate in 3D,
we need more, of course, than one camera.
We need cameras to cover the full space ahead of us.
So that is one differentiator between our use case
and the self-driving car use case
is just the sheer number of pixels
that we need to deal with in real time.
The other aspect is real-time performance.
We need to guarantee that our algorithm
perform adequately at the frame rate that we have.
So basically, we need to guarantee real-time performances.
And for that, the level of guarantee that we need to provide
depends on what we call the design assurance level,
which is basically a gauge on the safety
and the process that we've put behind
to guarantee the safety of those systems.
And the autonomous, the aviation industry
actually has very stringent standards on that.
And what happens is before we can even use an aircraft,
the certification authorities need to stamp our system.
So they need to look at all the details.
Because of all the guarantees that we need to give
to certification authorities,
and because of the fact that they need to stamp our design
and they need to stamp the way we are writing the software
before we can even start using the aircraft,
there's a lot more that we need to put
into those developments and a lot more proof
on the safety and the performance of those algorithms
that we need to provide than any other application.
I was just going to jump in there.
I remember hearing presentations around
the kind of verification and validation that happens,
for example, by NASA software engineers
building those kinds of systems and classical,
aeronautic systems.
And that seems, I guess, like a culture clash,
when you're thinking about machine learning
and probabilistic types of systems.
And I'm wondering how you bring those two worlds together
in areas like this.
That is a fundamental challenge for us.
So our team is at the crossroad
between two very different worlds.
Our team is drawing people from software companies
such as Google, Facebook, the sell-driving companies
in the area, they're used to create software
in a certain way, a very rapid way,
and exactly, exactly.
That doesn't work when you're carrying 300 people.
This is exactly the problem is destructive flight test
is not really an option for us.
And then on the aerospace side, the people that you have there
are used to do things right the first time.
And to have everything deterministic,
everything planned, and everything verified.
And so what we're doing right now is
we are proposing those new algorithms based on machine learning
that are completely probabilistic based.
And we're saying it's going to work.
We can predict that it will work to a certain performance
standard, but we don't have any formal proof for you.
We can only do that statistically.
What does that even mean now?
Are you referring to the application of formal methods
to provide performance envelopes and guarantees to your system?
Exactly.
So right now, there's a lot of work that
is being done in formal methods.
And those methods have proven to be very useful for,
let's say, algorithms of low to medium complexity.
But when we're tackling autonomous landing,
autonomous taxi function, for example,
the sheer number of pixels and the image size
that we need to deal with and the speed at which we need
to deal with them makes those applications
for now barely usable.
We basically don't have the right tool
to be able to apply it on very complex neural network
that would be based on architectures like ResNet or VGG
or all the newest flavor as well.
So at that point, since we can't rely on those formal methods,
what we're left with is those statistical methods.
And one of the things that makes it really hard
is that we don't know the distribution of the events
beforehand.
So we need to be able to do those data collection
to be able to understand what is our environment,
as far as our function, our concerns,
what is the, let's call it for a Gaussian distribution,
what is the nominal case that gives you your 60%,
but also what is the long tail events?
And one of the interesting examples
that we have is we've captured on video and elephant
walking on taxiways in Africa just behind
some commercial aircraft at an airport.
And there's no way any of our engineers at least
in Silicon Valley could have figured this one out.
But these are cases that our system for object detection,
for example, will have to tackle.
So this is the challenge is that we
can't use those formal approaches
because we don't know what the world is made of
until you've done that data collection.
And that data collection really gives you just the statistical
the means to do a statistical approach.
And in terms of that statistical approach,
what are some of the tools that you're using?
Early on when we started to do flight tests
and one of the greatest achievements that we've done
in our team is actually work on the AI
that completed the very first autonomous landing
of an A350 with a camera system.
And to give you an idea that the A350
is this aircraft that carries between 300 and 350 passengers.
So it's a fairly big beast to land.
Now, when we did that, one of the first things
that we noticed is flying an A350 is quite costly.
It's time consuming and to ensure safety,
it takes quite a bit of time to go through all the hoops
to have the authorization to fly it.
So doing data collection using flight test aircraft
is a lengthy and time consuming process.
So what we have developed here in Silicon Valley
is this hybrid approach where we said,
well, the software engineers here
are used to those two weeks development cycle
or those very short development cycle
where there's a new release every couple weeks.
And how can we do that for aerospace?
So what we did is we acquired our own aircraft
that we own and operate.
And we've modified that aircraft to be equipped
with all the sensors that we need to perform those functions.
And we are now flying this aircraft throughout the area
to be able to do data collection in those different conditions
that we're gonna see in the field.
So it can range from clear conditions during daytime
to night conditions, but also what we call the graded conditions.
So fog, low ceiling, rain, and all those.
And to be able to have real data
and to be able to acquire the volume necessary
to start doing those statistical proofs
of reliability and performance is essential for us
to push the maturity of those functions.
Are you saying, or is it the case that you have to fly?
If you want to do this on A350s,
you have to fly A350s for data collection
or are you flying cheaper vehicles for data collection?
And in our case, when we started in flight test aircraft,
we started doing data collection on A350s
and other big commercial aircraft.
But it's quite costly to do so.
And after analyzing the problem,
we saw that we don't actually need to use such an aircraft
to collect data that is applicable and usable
to develop those functions.
And the reason is, in our case,
we're using a beachcraft barren
to collect the data for the visual landing system.
And the barren, although less stable than an A350,
can replicate the approach that an A350 would do,
can explore the full service volume that we need to explore
and can cover pretty much all of the conditions
that an A350 would encounter.
And even to a certain extent,
because the beachcraft barren is less stable than an A350,
we cover a wider range of conditions
than what we would be able to do with an A350.
So on that respect, it's actually a way
of building a more robust system using a cheaper aircraft
that is available for us anytime.
Yeah, yeah.
Going back to the previous question
about statistical tools,
I'm trying to get a little bit more clarity
on what that looks like.
Like I'm envisioning you're using something akin
to like, you know, confidence testing
or something like that.
But it's not clear to me how you ever have confidence
if you don't know what your plane is going to be able to see.
So I'm looking for kind of names of,
oh, we use this statistical method or this
or that to produce these guarantees.
Is that something that you can elaborate on?
Yes.
So first thing is we need to identify
what are the dimensions of the problem
that affects the performance of,
in this case, the ML model or the perception stack.
So one of the things that we've observed
is the configuration of the airport is critical.
If you have only one runway,
it's easier to identify than when you're in Chicago
where you have like seven with runways
that are crossing each other
and taxiways that are confusing everything.
So the airport configuration is, for example,
one of those dimensions.
The other one is lighting conditions,
weather conditions.
So and you have also those,
I'm going to call it corner cases.
So when you have the sun in view
or when you have the moon in view
and you're trying to perform this function,
that will affect the performance of the ML
if it has never seen such occurrences.
Now, once we have identified those dimensions,
we need to understand what is the amount of data
that we need to be able to be representative
of our environment for each of those dimensions.
And then what we can do is using a representative statistical set.
So a number of population of images
that represent our space.
We can take the ML,
do the test over a percentage of that
with a holdout at the end.
And then by having multiple folds
and basically reshuffling this,
we can understand what is the degree of generalization
of the ML model.
And using that statistic,
we can derive from there how many airports do we need to see
to guarantee to a certain probability
that we will meet our performance.
So to give you an example,
let's say that we've collected data on 100 different airports.
We can use that 100 airports and say,
let's try to train only on 30
and check how we perform on the rest.
And then we do that again saying we take 40, 50, et cetera.
From there you can draw a curve.
And at some point that curve will hit an asymptote
that tells you I've reached my target performance
and any additional airport on top of that
will increase or improve it.
But you're already there.
So from there we can backtrack how many airports
or how many samples we need in each of those dimensions.
And so that's one of the approach that we're exploring
to be able to provide this statistical proof.
Thus far in the conversation you've kind of connected
the safety, critical nature of what you're doing
and the need to provide these performance guarantees
to kind of this very fundamental task
that all ML practitioners are worried about data collection.
And you've talked a little bit about how you collect
the data.
There's also the aspect of labeling the data
which is a big deal as well.
Can you talk a little bit about some of the challenges
of labeling for your use case?
Absolutely.
And I think anybody that has worked in ML
fully understand that getting the right data
and getting the good labels is half of the work.
And the other half being able to train it
and test it properly.
So labeling and acquiring that data
is one of the major challenges for our industry.
The car industry has the luxury of being able to collect data
in a fairly cheap way because they need to equip a car
that's certainly an expensive piece of equipment
but after that driving around is not an expensive operation.
In our case, flying is an expensive operation
meaning that getting data is time consuming.
It's we need to put a lot of resources behind it.
And then after that, we need to label that data
and we need to label to a level of precision
that has rarely been seen in other use cases.
And I'm going to give you an example
of the typical classifier and bounding box sort of ML model
that you have out there.
So for detecting cats and dogs.
So most of the time to be able to do
those typical classification problem
and to put a bounding box on it.
Using, for example, the typical YOLO architecture,
your labeling needs is basically you need to fit a box
on an image that mainly contains the object of interest.
So for example, your dog may be half of the image
or may be 40% of all your pixels.
And so that means at the end, if you put a bounding box on it
as a manual operator would, even if the bounding box
is not well fitted over the object
and you're taking another 70 pixels on the side
of your image for, for example, an HD image
that's still more or less within the 2% or 5% margin.
And your ML model will be able to generalize
over all of those samples assuming
that you don't have an inherent bias into your images.
So the classical approach makes it, if you forgive the term,
makes sloppy labels still OK to achieve the right performance.
Now, in our case, if you take a 12 megapixel image,
which is the typical range of resolution
that we need for lending an aircraft
and being able to provide the navigational parameter for it,
the airport can be as small as 20 pixel by 20 pixel in this image.
So at the end, if you want to keep your error bounded,
your bounding box on that object needs
to be within maybe 5 pixels or 3 pixels.
So what does it mean in terms of labeling
is that the manual labeler needs to take that 12 megapixel
or even 15 megapixel image, needs to zoom in as soon
as he has identified where the object and the tiny object
is in the image, and then needs to work
on putting this bounding box as tight as possible around it
within a 3 pixel accuracy.
And so that is quite a challenge as anybody
has worked with manual labeling knows,
and it's also time-consuming.
And I'm going to add that most of our crowd sourcing
strategies to be able to do that,
so meaning gathering a large population of manual annotator
is not a good method to do this because the level of accuracy
required requires a lot more time than they will spend
because of the money that they're getting out of it.
I mean, there are even simpler challenges
like most of the folks that we're paying to do manual labeling.
I'm imagining a 12 megapixel image
is going to take a long time to download
and is going to choke their machine.
That is absolutely right.
The whole pipeline behind it of being able to transfer
those data to the annotator is a challenge in itself.
We could, and one of the strategies, for example,
to break it down into a sub-grid, but it gives you
more images at the end, more images to annotate,
still meaning more time to do so.
And the problem is, at scale, it doesn't work at all.
If you're at a point where you're collecting
literally millions of images every week,
it is totally impossible to break down those images
into those 500 by 500, for example, pixel images
and have this army of human annotators looking at each of them.
The throughput is just going to break the whole chain,
which forces us at that point to think
about auto labeling approach or semi-automated approach.
And I'm going to say this is not a binary problem,
in the sense you can think about it as a spectrum.
In the very first stage, when your ML models are immature
and you're just learning how to do this,
you're going to need a human annotator
for pretty much every frame.
But then, as you're starting to build some autonomy
on top of it, in the sense, you have algorithms,
heuristics that can start to make sense of the data
and give you a first guess of what you need to annotate,
then you can increase the number of images
that you're automatically annotating
and have the human annotator only checking every one image
out of 10.
And then, as those algorithms, again, mature,
you can push the cursor further
and have the annotator looking at one image over 100,
over 1,000, et cetera.
And then the ultimate goal for us
is when you reach the millions, one image out of 1 million
is still something that a human annotator can do.
But how do you ensure the quality and the consistency
of the million of images behind it?
And that is the fundamental challenge
for the labeling parts that we are addressing.
And before we jump into the automatic labeling,
automated labeling, and more detail,
speaking to the previous comment,
do you, both from a labeling perspective,
as well as from a model perspective?
Do you tile the images?
Or do you keep the images whole?
So for the labeling, this, I'm going to use an expression
that everybody hates.
It depends.
So for the images, the concept that we've seen
is because our use case for commercial aviation
is within a very structured environment
compared to others, we have a pre-array knowledge
on the environment that we can leverage
to actually break it down.
So in the sense, to be more practical here,
we know exactly where the airport is
because we have the lat long coordinates,
and this is in a database.
Now, where should it be in the image is the question.
So as soon as we know the GPS coordinate of the aircraft,
we know its attitude, heading, and all that stuff, yeah.
Exactly.
And then if we have the camera calibrated,
and we know exactly where it is,
and where it's pointing at on the aircraft,
then we roughly know where the airport
is supposed to be within the image.
Now, there's still sufficient amount of errors in there
so that we can't just rely on that system
to guide the aircraft all the way to the runway.
Many, you can't do a full geometric solution
because there's a lot of noise and uncertainty in the system.
Exactly.
This is where the ML component becomes necessary
because we need to go from a rough region of interest
to the exact position of the runway within the image.
But that gives us a first guess,
and this is a region of interest that we can cut out
from that 12-megapixel image,
and then have the annotators annotate
instead of looking at the whole image.
So that is one technique to reduce the amount of pixels
that we need to process when we're labeling things.
So in other words, what you primarily care about
for the task that we're discussing,
eG landing is the airport and the runways,
and a lot of your images are gonna have,
by definition, a lot of sky, for example,
and you don't really care about having that annotated
so you can kind of crop that out.
Exactly.
One additional thing is, as you're pointing out,
during crews where you can only see sky,
this is something that we're not even recording
because there is no task to be done when you're in crews.
It's all the airspace is controlled,
so you know that your separation is guaranteed.
So on that case, it's a very safe environment.
So you need to start recording as soon as you are
in the terminal area and you're in the last stretch
to be able to land that aircraft on the airport.
And even in there, as you said,
there's a large portion that might be the sky of the beginning.
As you get closer, the scale of the runway will change
and the runway will take the full field of view of your image.
And this is also one of the challenges
for the ML on the ML side, as a side note is,
you start looking at the tiny little box
in the middle of your image,
and the ML has to recognize that as a runway
and then do the math behind it
to be able to give you exactly your position
with respect to it.
And then you have to go all the way
to the point that you don't even see the full runway,
you only see a portion of it.
And yet, all the way to the landing,
your ML model still needs to take that partial image
and compute the exact same thing
of where you are with respect to it
to be able to land your aircraft.
So the scaling factor is a major challenge,
both for the ML part but also for the annotation.
Because we need to ensure that our error on the labels
are tightly controlled over this entire range of scales.
And then the other side of my question
was with regard to building and training the model,
your typical computer vision networks
or hundreds of pixel-wide images
is kind of what they're tuned for.
Are you tiling or are you using techniques
that work at full-scale images?
So this is a very good question
because it taps into the real-time performance
with respect to the onboard compute power
that we have available.
And I'm going to...
The flip side of that is inference, right?
Exactly, exactly.
And I'm going to make a little digression here
just to understand the fundamental problem.
So on an aircraft, everything needs to be certified
and everything needs to be deterministic,
which means that the type of computer
that we can use to run the ML algorithms
are not the typical GPU that you use on your gaming laptop.
They're much less powerful and there's redundancy
built into it.
And so essentially, it's a big constraint
to be able to achieve the real-time performance
that we need to have.
So having said that processing 12 megapixel
or 15 megapixel images in real time
when you have three cameras, for example,
processing it at the same time,
is a major challenge for us.
So there's different approaches of doing this.
Tiling is definitely one of the first methods
that we are using.
But also, the other aspect is to exploit the scale.
So if we can shrink the image and we can use the image
at different scales, it also provides you a filter
that lets information at different frequencies pop out.
So for example, when you're pretty far from the runway,
if you shrink the image, it actually
blurs out the high frequency item that are actually noise
and that you don't really care about.
And you can see more clearly the runway down the road.
Now, that's one approach.
The other approach is really to take the full image,
just cut it in grids that overlap each other
and to basically brute force it over the entire thing.
But again, like here, this approach
where the bottleneck is the compute power that is available.
And we don't have the luxury of having
a full server of GPUs in the trunk.
Like, we would another application.
And so kind of getting back to the labeling task,
to the label, I guess the labels would translate
from you're essentially down sampling your images.
You know the label is, it's kind of a one-way,
one-to-one mapping.
So that's not, you don't have to manually label
down-sampled images separate from the full resolution
images.
No, that's right.
You would sample the images only once
at the highest resolution.
And then you could reuse that as you down sample it.
However, I have to say you can down-sample it
so that basically you have this filtering effect
that happens, and that enables you
to go faster in processing the image.
But it's not sufficient, and you can't just
use the output of that, because the resolution
and the precision of the output parameter that we need
is such that we also need the full resolution imagery
and to have a pixel-level accuracy
in the detection at the end.
So I guess what I'm saying is, you
can use those different techniques of, for example,
cutting out a region of interest out of your image,
of down-sampling it as well.
But it's not sufficient.
You need at the end, once you've defined
really the region of interest, to get back
to the original image with the highest resolution possible,
to get that last piece of precision
that you need for your autopilot, for instance.
I think what I'm hearing you say is that you
use these tricks like the down-sampling
more to identify regions of interest,
and then you pass full resolution images
to your models, as opposed to some funky model
that knows how to deal with both full resolution
and down-sample images or something like that.
So we're exploring different ways of doing that.
But essentially, what I'm saying is,
those are a range of techniques that you can use.
And then there's different architectures behind it,
where you can paralyze some of those from that processing.
And that is the way for us to achieve both the precision
that we need at the frame rates that we need
to process all of that information.
So that means that, and this is the second big challenge
for us, we can't just reuse ML models out of the shelf
from the fresh from the press, from Google or Facebook,
because they're essentially on fit for our application.
So there's definitely some big segments that we can reuse
that is very useful for us, but at the end,
because of the performance constraints
that we have, the compute constraints,
the huge images, we need to significantly rethink
the architecture and create sort of novel architectures
that really matches our needs.
We started talking a bit about automated labeling,
and you mentioned the kind of using the geometry
to identify where an airport is.
Is there more to the automated labeling?
Is that automated labeling, or is that just focusing
on what needs to be labeled, and then you're doing more
from an automated labeling perspective?
The way I'm going to answer the question is,
it's an evolution of our pipeline.
At the very beginning, as I said,
everything was labeled manually,
actually, everyone's going to say, in-house,
until we ran out in interns, no, this is a joke,
until basically people said that it's completely
unfeasible to scale it up.
So after that, this is, well, actually soon after that,
we started looking into those heuristics
and those semi-automated processes
to be able to label the images
with as few human or manual label as possible.
And we are building the stack
so that we use more of those algorithms,
more of those heuristics to be able to increase
the precision of our label,
to be able to increase the consistency of them,
and to be able to do that at greater scale
with fewer human supervision.
So the middle portion of that journey
is what we call weak supervision, for example,
or pragmatic means.
So we're starting to have
some of those heuristics applied,
and it gives us enough consistency
and enough accuracy so that we can meet
the performance requirements that we have.
But again, there's a scaling challenge here
because, for example, along one lending,
we might have to pick 20 or 50 images
that are manually labeled
so that we can recalibrate all of the entire sequence.
So that would be a mix of programmatic aspect
or weak supervision with the manual labelers
sort of going back at it
and sort of giving you some pointers to the algorithm
so that everything is re-adjusted
and all the errors are minimized in your sequence.
Can you elaborate on that recalibration?
One of the things is there's a lot of vibration,
for instance, on an aircraft,
and what you need to know to be able to label things properly
is what is your camera position on the aircraft
and what is this orientation with respect
to your frame of reference?
And that thing moves over time.
So when we're doing data collection,
our aircraft will do maybe 50 landings
during that data collection.
And the exact position and the exact calibration matrix
that we need to apply for that camera
will not be the same between the first flight test
and the last flight test.
The only way to compensate for that
is to, in retrospect, analyze the full sequence
and do what we call the bundle adjustment over it.
So recalculate what would be the calibration
for that sequence and then using that optimized calibration
matrix, recompute where all the labels should be.
And that is one way of minimizing the error
over those labels.
And this is for the programmatic labels.
That's correct.
So that is our way of using heuristics
to be able to automatically label those images.
But unlike a programmatic labeling task on NLP
where you've got your heuristics,
you apply them to your data and you get some labels,
you've got this loop where just that one shot data point
isn't sufficient because you think you know where your camera
is but you don't really until you analyze a sequence
of images and then you have to go back and correct.
Exactly.
And this is why we often have the question
saying, if you can auto label your images,
why don't you use your auto labeling algorithm
in inference to actually do the function?
And the simple answer is, well, that algorithm only works
once you have the full sequence and you've already landed
and you can sort of recalculate all of it
and then subtract the error out of your sequence.
So essentially, it's completely inapplicable
to the inference case.
But one additional thing is the heuristics
have some limitations, especially when
you're starting to look at long tail events.
This is where typically they fail.
And also at larger scales to be able to ensure
the consistency and the quality of your labels
using those heuristics might prove of a challenge.
And this is where the journey that I was talking about
keeps on going because at some point,
if you have enough data and your ML is mature enough,
you can start to use your ML to go back into your data set
and start doing auto labeling.
And using the output of your ML with the uncertainty
or the confidence depending on the metrics
and how you factor in those output for your ML,
you can identify pieces or groups of data
that needs further attention for labeling.
And so this is the entire journey
from your very first image that you hand label
to the batch of images that you're programmatically labeling
with some human supervision all the way to the grail
where you have your ML automatically annotating
your data sets with very specialized ML trained for that
that wouldn't run on your aircraft.
And the journey here is that we're trying to get
to that point where ML is usable in that way,
while guaranteeing again the precision and accuracy
that we need for our use case, which is extremely high.
Is it fair to characterize that last stage
as kind of a hybrid of programmatic labeling
and active learning?
Exactly.
And the active learning is a very interesting approach
which has been implemented in various flavors
across the industry, especially in industries
where you have a vast amount of data
or when the data is cheap to acquire
and you have more data on your hand than you actually need
and you want to sort out which one is useful
and which one is not.
In our case, we're not in this data rich environment,
data is hard to get, data is costly to get.
So active learning in our case
means something slightly different.
It's like active labeling in a sense.
That would be a new label that would really fit our purpose.
Active labeling, it's true that during one sequence,
during one lending, if you're capturing images
at, for example, 20 frame per second,
your frame one will not be very different from frame two.
So in that respect, active learning,
if you follow the concept, would say, well,
sub-sample your images because what you want is diversity.
But you also need to guarantee volume in a certain way
and this one is also kind of a challenge for us to get.
So we have to have this balance between labeling
enough to get the critical volume,
but then ensuring diversity by looking at which data
provides the most learning value over it.
So our implementation is going to be slightly different
from other industries, but it's still very relevant
to be able to pick the right data
that you want to have in your data set at the end.
Part of what I heard when you describe
the way you apply programmatic labeling is that
you're applying these heuristics to these images,
you're kind of going back through a second time
or end time in a loop to calibrate or recalibrate.
But it almost sounded like then you have these labels
which you have a very high degree of confidence about.
I guess that's prompting the thought,
are you then when you're training,
do you consider that weak supervision in the sense
that the labels are noisy or are you still worried
about noise and if so, how do you deal with that?
So noise is still going to be in there.
And but to a degree that we have control over.
So essentially the whole point is to be able
to quantify the error and understand that
at the end of labels are going to have some amount of error
but not beyond a certain threshold that we've specified.
Now this is extremely hard to do
because when you're meeting new conditions
that you haven't encountered in the past,
you're not sure that your labeling pipeline
and your heuristics are going to provide
the same level of quality and consistency
as known environments, known conditions
and images coming from this operational domain.
So to give you a concrete example,
labeling daytime images for landing
is something that we understand how to do
and we understand how to provide
the sufficient precision and consistency on it.
But nighttime is an entirely new ball game here.
So this is an exercise that we've done as well.
And what we've seen is that our labeling pipeline
for daytime doesn't work during nighttime.
You don't see the same features.
And for example, one of the things is trying to put
a, trying to identify the corner points of the runway
is not feasible during nighttime
because you don't actually see it.
All you see is the lights around.
So as soon as it's going to be an iterative process,
each time we're pushing further,
the operational domain that our function needs to operate in,
we're going to encounter those new conditions,
those new images and we will have to assess
whether the current labeling pipeline
can still provide the same quality
and basically the same bound on the error.
And most likely, we're going to have to iterate on this.
And so in that way, I guess the point
that I'm trying to make here is that it's a full loop.
You are using your labeling pipeline to provide labels
with a certain error, a certain consistency.
But then at the same time, you need to test your labeling
pipeline to see what kind of error it induces
in your labels based on those conditions.
And for that, you need reliable data.
So it's kind of a chicken and a neck problem.
You need well-labeled data to test your labeling pipeline
and you need a good labeling pipeline
to provide you right labels and right images.
So that is the challenge that we're facing.
I'm also wondering about the use of or thoughts
on the role of synthetic data for your use case.
In a sense, I guess the thought is coming from,
I would think there are fairly small number of airports
that can accommodate an Airbus 350.
Why don't you send someone around?
And in fact, there may be survey data
that knows where the corner points are
once you can localize the airport.
Can you then just generate synthetically
all the training data that you need?
That is a very good point.
And I'm going to say you're ties into two things.
One is we can absolutely replicate all the airport
across the world because there's database of them.
So this is a capability that we have.
And the other point is we, it's going
to be very challenging to be able to collect real data
at all the airports in the world that we operate at
or that we want to operate at.
So this is where it really is helpful
because you can collect data on a sub portion
of those airport.
You can generate the synthetic data for all of them
and using those two data sets and verifying
that your synthetic data is actually representative
of your real data, you can create a very extensive data
set to train the neural network, but also to test it.
And this is the point that I want to make
is the difficulty in the challenges
in improving the safety and the reliability of the algorithm
requires a large data set that needs to be shown
as representative of your environment
and dense enough to be able to give you
the statistical proof at the end.
And this is extremely hard to do with real data
because of the challenge of collecting it.
And this is where synthetic data can be very useful
because from a sparse population of samples,
you can recreate that population with synthetic data,
make it a very dense population
and make your case based on that population
with a mix of real and synthetic data.
So this is one of the avenue that self-driving cars
have taken is, for example, for one mile
that a typical autonomous car is driving,
they're probably generating a 1,000 mile of simulated driving
and they're testing their algorithm on all of that.
So that gives you a 1,000 to 1,000 leverage
and that enables you to have the statistical
meaningfulness that you need to start to trust your system.
And so in our case, we are looking at the same approach
of leveraging synthetic data not only for the training
but also for the testing.
Are there other techniques or approaches or ways
that you see your pipeline evolving
that we haven't touched on?
So one of the other alternative to purity synthetic data
and real data is data augmentation.
And that is a very nice, and I'm gonna say,
cheap way of reusing your real data
and creating new data that has learning values.
So you've got daytime images,
make them look like nighttime images
and put them through the pipeline.
Exactly.
And one of the challenges as well that might not necessarily
come to mind is you're using one type of camera
when you're recording the imagery.
It might not be the same that you're using at inference
and actually as the generation of aircraft goes on
and your new cameras comes in,
it might be a completely different camera
that are gonna be using 10 years or 15 years from now
but you still wanna be able to use the data
that you've collected because of all the energy
and the resources that you've put into it.
And so being able to reuse that real data
post-processing it so that the noise, for example,
looks different and matches your new camera.
You can induce chromatic aberration,
so change and shifts in RGB.
And you can also warp those images
so that they look different in terms of perspective.
There is some studies to put rain on top of good
and clear condition imagery.
And then you can put some fog, et cetera.
Now the challenge at some point is
you can do all those fancy things on top
but you always need to validate
that they are representative of the real data.
And this is where it, you cannot just start
with synthetic and stick in synthetic
and then deploy it in the real world.
This validation is the thing that is somewhat taxing
because you need all that real data
to prove that you're still in the real world.
Well, Cedric, lots of exciting stuff there, sounds like
you've got enough to keep you busy for quite a while.
We are indeed.
And I must add, this is a very exciting time for us
because in contrast with the car industry,
we're really at the beginning of this.
And this is a brand new revolution of autonomy in aviation.
And so we are building the foundation block
of how to use ML and how to use
artificial intelligence into not only commercial aircraft
but a wide range of aviation products.
And so the other mission that for me is really exciting
at A Cube in particular is we're drawing all those engineers
from other fields such as Google, Facebook,
the self-driving car industry that have a big knowledge
and a significant amount of experience in those industries
and we're pulling them into the aviation industry
and we're saying, OK, so you have
those 10-year cycle to produce a new aircraft
but we want to use all those new techniques that Silicon Valley
has created to be able to do, let's say, one new release
every three weeks.
And we're going to be able to push that to an aircraft
and test those things.
And so just the pipeline and the processes
are going to be revolutioning the aviation industry.
It's basically a brand new world for us
so it's quite exciting to be in this year.
And I'm going to say as well, just as a last note,
A Cube and Wayfinder is actively recruiting.
So if you're interested in facing all those challenges
and working with us, it will be a pleasure
to have your application.
Where should they go?
Well, make sure to include it in the show notes page.
Yes, so A Cube as its own websites
and if you actually type Wayfinder.error,
it will redirect you directly to our page with our blogs
and the join the team section.
Awesome, awesome.
Well, Cedric, thanks so much for joining
and sharing a bit about what you're up to, very cool stuff.
Thank you very much for inviting me.
