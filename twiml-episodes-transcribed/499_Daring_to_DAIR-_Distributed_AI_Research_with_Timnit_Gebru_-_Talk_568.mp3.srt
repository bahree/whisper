1
00:00:00,000 --> 00:00:05,120
Like I said, your experience teaches you a lot more than what anybody else writes or says.

2
00:00:05,760 --> 00:00:10,960
Is that if you don't have the right institution and the right structure,

3
00:00:11,920 --> 00:00:15,440
there's just no way that you can do things fairly.

4
00:00:21,280 --> 00:00:25,520
All right, everyone. Welcome to another episode of the Twomo AI podcast.

5
00:00:25,520 --> 00:00:31,280
I am, of course, your host, Sam Charrington. And today, I'm joined by a very special guest,

6
00:00:31,280 --> 00:00:38,080
none other than Timnett Gebru, founder and executive director of Dare, the Distributed Artificial

7
00:00:38,080 --> 00:00:43,680
Intelligence Research Institute, and of course, a great friend of the show. Before we dive into

8
00:00:43,680 --> 00:00:47,840
today's conversation, be sure to take a moment to head over to Apple Podcast or your listening

9
00:00:47,840 --> 00:00:53,760
platform of choice. And if you enjoy the show, please leave us a five star rating and review.

10
00:00:53,760 --> 00:00:58,720
Timnett, it is wonderful to have you back on the show. It has been a bit.

11
00:00:59,440 --> 00:01:06,720
I think this is actually your fourth time ish because you did a meetup that you probably don't

12
00:01:06,720 --> 00:01:15,600
remember back in January 17 about your Google Street View work. And then your first time on

13
00:01:15,600 --> 00:01:23,920
the show is in January of 18 episode number 88. We're probably at five 88 or something like that now.

14
00:01:23,920 --> 00:01:32,080
And of course, you helped us cover trends in fairness and AI ethics in January of 20, kind of

15
00:01:32,080 --> 00:01:44,000
looking back on 19. Wow, it's been a long two and a half years. Why don't we get started by

16
00:01:44,000 --> 00:01:49,040
having you share a little bit about what's been going on for you? Welcome back.

17
00:01:50,000 --> 00:01:56,720
Yeah, I can't even, it's really interesting being back, you know, because I remember our first

18
00:01:56,720 --> 00:02:04,400
black night workshop, you all had, you were at like a hotel room, you had a whole set off. It was just

19
00:02:04,400 --> 00:02:11,120
like, it just feels like such a long time ago. Yeah, that was long beach. Yeah, yeah, yeah.

20
00:02:11,120 --> 00:02:18,480
And it's very interesting. It's kind of like chronicling a journey, you know, every time I come back

21
00:02:18,480 --> 00:02:27,200
here. Well, I have to say that, you know, right now, I'm focused on dare, as you mentioned.

22
00:02:28,880 --> 00:02:40,000
And I'm trying to take the time to calm down a little bit. And also think about, you know,

23
00:02:40,000 --> 00:02:45,440
just take, take a step back. So one of the things I wanted to do was think about, you know,

24
00:02:45,440 --> 00:02:50,960
there are all of these issues that we're talking about, right fairness, ethics, labor issues, etc.

25
00:02:50,960 --> 00:02:56,800
And but what does the right model for doing things look like, right? What does the right

26
00:02:56,800 --> 00:03:01,680
institute look like? What do the right incentive structures look like? How should we approach

27
00:03:01,680 --> 00:03:08,000
the way we do research and what we build, what we don't build? And I am just kind of trying to

28
00:03:08,000 --> 00:03:16,640
take the time to figure those out at this right now with there. Is it fair to ask you to give a 30,000

29
00:03:16,640 --> 00:03:24,400
foot 30-second overview of your recent experiences to get some at Google to help folks get some

30
00:03:24,400 --> 00:03:34,800
context if they've not heard any of the previous, where do we start? Well, so well, I got fired from

31
00:03:34,800 --> 00:03:43,520
Google or as some of my former teammates have called it, actually Sami Bianjo. He coined the term

32
00:03:43,520 --> 00:03:49,360
being resonated. He was like, in French, he said in French, you know, you have this word where like

33
00:03:49,360 --> 00:03:59,360
someone resigns you. And so like they call it being resonated. So I was resonated from Google. And

34
00:03:59,360 --> 00:04:07,600
it was a whole, to be honest with you, I still have not processed it because I don't, you know,

35
00:04:08,960 --> 00:04:16,400
it was in the middle of a pandemic, in the middle of, you know, a war that just started in Ethiopia,

36
00:04:16,400 --> 00:04:24,240
the most horrible war I have ever seen that is not really being talked about, that also gets us,

37
00:04:24,240 --> 00:04:30,960
has gotten me to see all of the issues on social media. And in a way that I've never seen before,

38
00:04:30,960 --> 00:04:37,120
you know, people talk about these issues. And it's like you never learn about it as much as when

39
00:04:37,120 --> 00:04:45,840
you experience it. And so in the middle of that whole thing, and I wrote, you know, this paper on

40
00:04:45,840 --> 00:04:51,760
the dangers of large language models. And the way this actually happened, believe it or not,

41
00:04:51,760 --> 00:05:00,640
was not because I wanted to write a paper. But I saw that people at Google were basically saying,

42
00:05:00,640 --> 00:05:05,200
you know, why are we not the leaders in like large language models? You know, this is, we should

43
00:05:05,200 --> 00:05:11,920
be the ones doing these giant models. And you know, you see this race, just people are so fixated

44
00:05:11,920 --> 00:05:18,000
on having larger and larger models. And I was, I was very worried about that because it seemed to

45
00:05:18,000 --> 00:05:24,800
be this rush to a bigger thing without clarity on like why that bigger thing and also what are the

46
00:05:24,800 --> 00:05:33,680
issues. And so I asked Emily Bender and I said, hey, you know, do you have papers on this that you've

47
00:05:33,680 --> 00:05:40,640
written before that I can cite because right now I'm citing your tweets. And if I could cite a

48
00:05:40,640 --> 00:05:45,120
paper that you've written that I can send to people because people are also internally at Google

49
00:05:45,120 --> 00:05:49,280
asking me what are things we should worry about. And so she said, hey, why don't we write something

50
00:05:49,280 --> 00:05:53,760
together? And I'm like, well, I don't know what I'd contribute, you know. And so then I, and we

51
00:05:53,760 --> 00:05:58,400
each pulled in other people, I pulled in Meg and other people from that play team and we wrote this

52
00:05:58,400 --> 00:06:04,320
paper. And honestly, I never thought it would be controversial. It was it, you know, I, I just

53
00:06:04,320 --> 00:06:09,120
thought it was just going to be this paper. And that's it, right? I didn't think they would love,

54
00:06:09,120 --> 00:06:12,880
I didn't think the Google people were going to be like super happy about it. But I didn't think

55
00:06:12,880 --> 00:06:20,240
they were going to just, you know, do what they did, obviously. And so long story short, I found

56
00:06:20,240 --> 00:06:29,200
myself basically disconnected from my corporate account in the middle of my supposed vacation.

57
00:06:29,200 --> 00:06:36,480
And I found out from my direct report that I had apparently sent in my resignation. And that's

58
00:06:36,480 --> 00:06:43,920
sort of a whole, you know, very, very stressful few months because then, you know, there was all

59
00:06:43,920 --> 00:06:48,320
this harassment online. There was all of this, you know, you have to make sure you're safe.

60
00:06:49,360 --> 00:06:58,240
There are literally like people from the dark web who made it a point, like a point to to

61
00:06:58,240 --> 00:07:03,840
harass me come to all the talks I'm giving. And you know, just kind of harass anybody who is

62
00:07:03,840 --> 00:07:08,400
coming to my defense, you know, a lot of other people found themselves writing documents,

63
00:07:08,400 --> 00:07:12,240
having to talk to lawyers and things like that. People who don't even know me, by the way,

64
00:07:12,240 --> 00:07:17,840
just because people just, you know, were coming to my defense on Twitter or something like that,

65
00:07:17,840 --> 00:07:25,040
just because of that, I found myself being a thirst thrust into the public space. And so then that

66
00:07:25,040 --> 00:07:33,680
also just that fact itself brings in more attention for more people. And then I was like really

67
00:07:33,680 --> 00:07:38,800
worried about my team and what was going to happen to them. But then, you know, my co-lead

68
00:07:38,800 --> 00:07:46,400
Mitchell was also fired. So it was a whole few months. It was a whole thing. And, you know,

69
00:07:46,400 --> 00:07:51,680
that's what I mean, but I didn't have a chance to really process what has happened. In the midst of

70
00:07:51,680 --> 00:07:56,960
that, of course, I was thinking, what is the next, what is the thing I could do next? Because I really,

71
00:07:57,600 --> 00:08:02,400
you know, couldn't get myself to think about being at another large tech company and do that

72
00:08:02,400 --> 00:08:10,240
fight again. I also know that I would not, there would be some companies that would be unwilling

73
00:08:10,240 --> 00:08:15,680
to hire somebody like me after all of that. There's, you know, some members of my former team,

74
00:08:15,680 --> 00:08:22,960
their office were rescinded from some places like after this publicity. And it's real, you know,

75
00:08:22,960 --> 00:08:29,840
it really is real that people can, you know, by speaking up, just destroy their entire careers

76
00:08:29,840 --> 00:08:36,960
and any options. But, you know, I had been thinking about creating a distributed independent

77
00:08:36,960 --> 00:08:41,920
research institute. I didn't even think about like creating a university. Why can't we have a

78
00:08:41,920 --> 00:08:46,560
distributed kind of different kind of, you know, I've been thinking about these things. But

79
00:08:47,200 --> 00:08:53,440
if I hadn't been fired, probably what I would have done is slowly start something, you know,

80
00:08:53,440 --> 00:08:58,640
maybe start something from on the side and grow it there, be very slowly, not like the way,

81
00:08:58,640 --> 00:09:04,640
you know, we just started this. So anyhow, and after that, I decided to start there, the

82
00:09:04,640 --> 00:09:10,720
distributed air research institute. That's awesome. And so what's the, how do you think about the

83
00:09:10,720 --> 00:09:16,720
charter for dare? What's kind of in the, in the zone, in the scope versus out of scope?

84
00:09:17,520 --> 00:09:23,760
Yeah. So, you know, dare is the air research institute, like, you know, like any other research

85
00:09:23,760 --> 00:09:29,520
institute that you can think of. The thing that we are is we're an interdisciplinary research

86
00:09:29,520 --> 00:09:35,520
institute. So, you know, Alexandra recently joined as our director of research. She's a sociologist.

87
00:09:36,720 --> 00:09:43,360
And the distributed aspect was very important for me because I saw it even at Google in the

88
00:09:43,360 --> 00:09:51,120
ethical AI team, you know, Meg was very good at retaining a distributed team. And, you know,

89
00:09:51,120 --> 00:09:58,560
one of the last people we hired was Mahdi, who's a Moroccan. And he was raising the alarm

90
00:09:58,560 --> 00:10:04,080
on social media, like no other person. And he was doing all this research, his friends were in

91
00:10:04,080 --> 00:10:11,120
jail, their journalists. And I could see that nobody, you know, even the people in ethics or

92
00:10:11,120 --> 00:10:18,240
whatever could not really grasp the gravity of the situation. And if you didn't have that person,

93
00:10:18,240 --> 00:10:22,320
with that experience, there was no way you would, you would, you know, find out about that issue

94
00:10:22,320 --> 00:10:28,080
and look into it, right? And that showed me the importance of, of having people, you know,

95
00:10:28,080 --> 00:10:33,680
like that and not forcing them to move to Silicon Valley or whatever. I don't want to, you know,

96
00:10:33,680 --> 00:10:39,200
I'm, what I'm thinking about is how not to consolidate power, right? Not how to, far,

97
00:10:39,200 --> 00:10:46,400
there kind of contribute to the brain drain of different other locations. So, so that's why the

98
00:10:46,400 --> 00:10:51,440
first word that came to my mind was distributed. And I called, you know, I told Eric Sears,

99
00:10:51,440 --> 00:10:56,640
who's a program officer, a director at MacArthur, the MacArthur Foundation, I was like, hey,

100
00:10:56,640 --> 00:11:00,240
look, you know, the first word that came to my mind is distributed. I want to call it dare,

101
00:11:00,240 --> 00:11:07,280
like does it sound weird, you know? It's like, no, it's, it's cool. And so, so that's, that's dare.

102
00:11:07,280 --> 00:11:11,840
And so when you say what's in scope versus out of scope, you know, that's honestly something

103
00:11:11,840 --> 00:11:17,440
that we're still trying to figure out because it, I'd like it to be kind of a combination of,

104
00:11:17,440 --> 00:11:23,760
of course, we have a few top-down directions, but I really feel strongly that it's, it's very

105
00:11:23,760 --> 00:11:29,600
important to have a bottoms-up approach to research because you can't be the all-knowing person

106
00:11:29,600 --> 00:11:34,960
who knows like what the next important thing is, right? So it's important to let other people

107
00:11:34,960 --> 00:11:43,120
drive that too. But the thing we're focused on right now is, you know, what is our research

108
00:11:43,120 --> 00:11:49,760
philosophy? And what, what do we care about, right? And so first of all, we care very much about

109
00:11:49,760 --> 00:11:56,320
not exploiting people in the research process. One of the most, one of the things that is super

110
00:11:56,320 --> 00:12:01,680
clear in research in general, and especially when you look at this field where you, you know,

111
00:12:01,680 --> 00:12:06,560
there's a lot of knowledge that's extracted from people, a lot of data in different forms

112
00:12:07,120 --> 00:12:14,560
that's extracted from people without compensation, without, you know, acknowledgement, etc, right?

113
00:12:14,560 --> 00:12:20,000
Like you have that also in the fairness space. For instance, you have a group of researchers,

114
00:12:20,000 --> 00:12:25,040
you know, they get tenure and they're ascending based on work on fairness or something.

115
00:12:25,040 --> 00:12:29,760
And who are the subjects that they talk about? Oh, they'll talk about formerly incarcerated people

116
00:12:29,760 --> 00:12:34,000
or people in prison currently. They'll talk about like different groups of people who are

117
00:12:34,000 --> 00:12:40,480
harmed by this technology who are not, you know, getting the money, you know, for the research or

118
00:12:40,480 --> 00:12:46,800
the fame or, or, you know, many times their lives are not changing because of this work, but they're

119
00:12:46,800 --> 00:12:52,400
subjects of it, right? And so we're trying to figure out how do we not do that? You know, how do we

120
00:12:52,400 --> 00:12:58,160
do the opposite of that? What does it mean to have research that that that incorporates

121
00:12:58,160 --> 00:13:04,400
these people and actually is led by many times people like that? And how do you funnel resources?

122
00:13:05,440 --> 00:13:12,480
And so one of our research fellows who just joined Meela is actually one of the things she's

123
00:13:12,480 --> 00:13:15,920
doing is helping us figure that out, right? What is our research philosophy and how do we

124
00:13:15,920 --> 00:13:23,840
operationalize it? So in terms of, you know, what's in scope and out of scope, so there's a self-selection

125
00:13:23,840 --> 00:13:30,960
going on there where the people, you know, who do want to do research at their are people who

126
00:13:30,960 --> 00:13:35,760
care about these kinds of things are somehow embedded in community building, not just, you know,

127
00:13:37,520 --> 00:13:45,440
like research that has nothing to do with that. And, you know, like, for instance, if you want to work

128
00:13:45,440 --> 00:13:51,600
on, you know, low, you know, so I'm, I'm, I'm advising on a workshop, which I had

129
00:13:51,600 --> 00:13:57,200
coordinated before on practical machine learning and, you know, for developing countries or

130
00:13:57,200 --> 00:14:02,000
practical machine learning in low resource scenarios. So if you want to, you know, kind of think

131
00:14:02,000 --> 00:14:08,880
about what about, like, small data and small compute, right? Like that, I think you might want to

132
00:14:08,880 --> 00:14:12,640
join, you know, we might want to think about working out there. But if you're interested in, like,

133
00:14:12,640 --> 00:14:17,520
even larger models and even larger or something, then I don't understand what we would, you know,

134
00:14:17,520 --> 00:14:22,560
provide in that sense. So that's kind of how I'm thinking about it right now. What I'm hearing in

135
00:14:22,560 --> 00:14:30,720
part is that the, the areas that you've traditionally been working in and a researcher, ethics,

136
00:14:30,720 --> 00:14:38,880
fairness, and that you're probably best known for, that is not necessarily a research focus for

137
00:14:38,880 --> 00:14:47,840
Dare, but more like a undercurrent or a foundation. And Dare is going to be broader and encompass,

138
00:14:47,840 --> 00:14:53,600
you know, like you said, all the things that another research institute might, like Amila might

139
00:14:53,600 --> 00:14:58,720
be interested in depending on, you know, who it is that comes and starts out research programs

140
00:14:58,720 --> 00:15:04,800
there. Exactly. So like, a lot, some people describe Dare as like an AI ethics research institute,

141
00:15:04,800 --> 00:15:10,880
right? And I'm like, no, it's like, yeah, that's not what we're, we're hoping to do. And by,

142
00:15:10,880 --> 00:15:17,760
by virtue of who we are, we will, so there's two ends of the spectrum that we were looking at,

143
00:15:17,760 --> 00:15:22,560
right? And I think our advisory committee members, when you look at Safiya Noble and Shira,

144
00:15:23,440 --> 00:15:30,960
Meina, they encompass those two ends of the spectrum. So the first end is how do you develop,

145
00:15:30,960 --> 00:15:35,840
how do you do this research in a way that we think is beneficial to the groups of people that we

146
00:15:35,840 --> 00:15:40,320
care about? And actually, when you say what's in scope and out of scope, our focus is, you know,

147
00:15:40,320 --> 00:15:46,160
we're starting with thinking about, you know, people in, in Africa and the African diaspora, right?

148
00:15:46,160 --> 00:15:52,160
Like so, you know, you know, you know, there's no kind of question. Like I don't have, I don't know

149
00:15:52,160 --> 00:15:57,760
if I have to explain why, but like, you know, black people in general around the world who are

150
00:15:57,760 --> 00:16:03,680
very much harmed by this technology and not necessarily benefiting from it. So when you look at

151
00:16:03,680 --> 00:16:10,560
Shira, he's, in the area he's in, he's in Kenya, and a lot of his work is on how to, you know,

152
00:16:10,560 --> 00:16:16,800
work on climate change and data science, right? He analyzes bird migration patterns to, to,

153
00:16:16,800 --> 00:16:21,760
that tells you something about the climate and how it's changing. He, he was at the first black

154
00:16:21,760 --> 00:16:28,560
name workshop. He probably covered his work, food security and conservation. He works on stuff

155
00:16:28,560 --> 00:16:33,280
like the he co-founded data science Africa, right? So it's kind of like, you know, how to work on

156
00:16:33,280 --> 00:16:38,160
data, quote unquote, data science or related fields in a way that is beneficial to start, you know,

157
00:16:38,160 --> 00:16:44,880
to the groups of people that he cares about. On the other end, you have Sophia who's in, you know,

158
00:16:44,880 --> 00:16:51,920
in, in the US, and she is more on the other end of the spectrum, how to, you know, raise the alarm

159
00:16:51,920 --> 00:16:59,840
when we know there are issues that with technology that's already been built, right? So we, and,

160
00:16:59,840 --> 00:17:06,640
you know, she's more from the social sciences side, right? So like, for me, that encompasses sort

161
00:17:06,640 --> 00:17:11,680
of what I want to build with dare, right? Interdisciplinary have different groups of people

162
00:17:11,680 --> 00:17:18,400
to, to be able to work on research that, you know, we think is beneficial to our communities.

163
00:17:19,280 --> 00:17:24,320
And, you know, way that's not exploiting the people who are actually, you know, who might not

164
00:17:24,320 --> 00:17:29,440
have PhDs or whatever, but have a lot of knowledge about the, the systems and how they're impacting

165
00:17:29,440 --> 00:17:35,040
them. So I like what you said. Yeah, it is an undercurrent, right? Of like, how do we do this work?

166
00:17:35,040 --> 00:17:39,040
Is, is, is that's how we're building this foundation? I mean, this institute.

167
00:17:39,040 --> 00:17:46,560
One of the things that we chatted about before we started recording was that a lot of your focus

168
00:17:46,560 --> 00:17:51,200
right now is on institution building. For obvious reasons, you're building an institution. Like,

169
00:17:52,240 --> 00:18:00,800
I'm curious what that means for you. And also, well, afterwards, I want to relate that back to

170
00:18:00,800 --> 00:18:08,960
your experience at Google and, and the, the idea around, you know, how to, how to ethics organizations

171
00:18:08,960 --> 00:18:12,960
inside large companies? Like, how do we build those so that they have teeth, so to speak, so that

172
00:18:12,960 --> 00:18:19,120
they can be effective? Yeah, that's a very good question. And so I've been going on this fairness

173
00:18:19,120 --> 00:18:24,560
rabbit hole, as you know, and, you know, I've been like, I've worked on things related to math

174
00:18:24,560 --> 00:18:31,520
and or documentation or auditing, community building, like Black Day Eye, Power Building, you know,

175
00:18:31,520 --> 00:18:38,400
met all the different kind of ways in which I think you can attack the problem. And I have kind

176
00:18:38,400 --> 00:18:43,600
of just kind of come to the conclusion, like many. And of course, this is not something new that

177
00:18:43,600 --> 00:18:49,120
I'm saying. It's just like I said, your experience teaches you a lot more than what anybody else writes

178
00:18:49,120 --> 00:18:57,920
or says is that if you don't have the right institution and the right structure, there's just no way

179
00:18:57,920 --> 00:19:04,080
that you can do things, quote unquote, fairly, right? So, um, so that's why I'm, I'm kind of working

180
00:19:04,080 --> 00:19:09,200
on institution building, right? I've, I've had experiences in academia. I've had experiences in

181
00:19:09,200 --> 00:19:13,120
industry. And when I, after I got fired from Google, I was thinking, you know, a lot of people

182
00:19:13,120 --> 00:19:18,160
were saying, well, you obviously won't have academic freedom in industry. If you want that, you should

183
00:19:18,160 --> 00:19:24,320
go to academia. And I'll say, that's not true, right? To me, it's a pyramid scheme up here at the

184
00:19:24,320 --> 00:19:30,160
top of the, you know, somebody just tweeted the other day that graduate students make $36,000 a year,

185
00:19:30,160 --> 00:19:36,080
perhaps, right? And, you know, it's like they're in this weird position. Are they students? Are they,

186
00:19:36,080 --> 00:19:41,680
are they workers? Like do they get vacation or not? But they're in this situation for years, right?

187
00:19:41,680 --> 00:19:48,720
Very similar to college athletes. Oh, apps 100%, which also should get paid exactly. So that's

188
00:19:48,720 --> 00:19:55,120
where we are, right? And so, um, yes, and it makes absolutely no sense. It's, I think it's very,

189
00:19:55,120 --> 00:20:00,800
very exploitative. And so imagine you're doing that work as a graduate student at your advisor

190
00:20:00,800 --> 00:20:05,680
controls your life. And then you're going to tell them, you know, whatever research they're doing

191
00:20:05,680 --> 00:20:10,880
is not fair. You should have a different sort of direction. You, you're, you should stop. How are

192
00:20:10,880 --> 00:20:16,000
you going to do that? You, you will lose your, your money. You will lose your, um, career,

193
00:20:16,000 --> 00:20:20,080
like your future prospects, because they won't write you a recommendation. If people are on

194
00:20:20,080 --> 00:20:26,560
visas, you will lose your visa. So, so, so, um, how are we telling people to do the right thing

195
00:20:26,560 --> 00:20:31,520
when we know we're not setting them up, right? With the incentive structure to do the right thing.

196
00:20:31,520 --> 00:20:38,080
And it's the same thing at work too, right? Like, um, again, what did I, I spoke up? I got fired.

197
00:20:38,080 --> 00:20:44,400
So, um, then why, why would anybody do something differently then, right? Like, and so, so, that's

198
00:20:44,400 --> 00:20:50,320
why I really believe we have to think about, um, the, um, incentive structures. And it's not just

199
00:20:50,320 --> 00:20:56,800
about, for instance, labor practices that we're talking about, right? It's about what kind of work

200
00:20:56,800 --> 00:21:02,800
is valued and what kind of work is not valued. Um, you know, so I, I think you have Mariel Gray.

201
00:21:02,800 --> 00:21:08,160
Well, so her and Sudarsasri have this book called Ghost Work, um, how Silicon Valley is creating a

202
00:21:08,160 --> 00:21:13,760
global underclass, and they're talking about data labor, right? So all of this automation that we

203
00:21:13,760 --> 00:21:18,960
talk about is sort of pseudo, it's not, you know, real automation is that there's a lot of people

204
00:21:18,960 --> 00:21:24,160
behind it labeling data, you know, doing all sorts of things, but they're being exploited. They're

205
00:21:24,160 --> 00:21:31,280
not being paid, right? Um, and so in, in, in, in graduate school, if you're telling your PhD

206
00:21:31,280 --> 00:21:36,720
student that they should spend all of this time working on data related work, data labor, that's

207
00:21:36,720 --> 00:21:41,840
the very, the most important thing you should think about how you're gathering and annotating data,

208
00:21:41,840 --> 00:21:47,760
take the time to do this right. But then they can't publish their work or they, it's not valued

209
00:21:47,760 --> 00:21:52,720
or they can't get a job after they graduate. Again, that's an incentive structure and institution

210
00:21:52,720 --> 00:21:58,800
building issue, right? So now, there's some people working on journals, for instance, to be able to,

211
00:21:58,800 --> 00:22:05,680
for people to be able to publish on data. And there was this new rips, this new, new rips,

212
00:22:06,320 --> 00:22:12,160
data sets and benchmarks to act where we actually published a paper too for dare. So that's what I

213
00:22:12,160 --> 00:22:18,480
mean, like, this is exactly why I'm thinking about the, the, the incentive structures, right? Because

214
00:22:18,480 --> 00:22:22,960
there's no way you could, you know, do quote unquote the right thing if you're in the wrong

215
00:22:22,960 --> 00:22:30,000
incentive structure. Yeah, yeah, we, I did an interview with Safe Savage, who researches

216
00:22:30,000 --> 00:22:37,520
that area as well. That was a future of work for the invisible workers in AI. Exactly.

217
00:22:37,520 --> 00:22:43,840
I can episode 447. You know, if you kind of, you know, chart your path as

218
00:22:45,360 --> 00:22:49,600
experimenting with different institutional structures to try to see what works,

219
00:22:49,600 --> 00:23:00,320
um, is your decision to start dare, uh, you know, can we infer from that that support organizations

220
00:23:00,320 --> 00:23:05,040
aren't enough, internal organizations aren't enough. There needs to be just an independent,

221
00:23:05,520 --> 00:23:11,840
uh, alternative to kind of traditional research structures. What I'm exactly, what I'm thinking is,

222
00:23:11,840 --> 00:23:16,640
so let's say if I went to academia, I'm, I told you, I'm trying to spend the time to think about

223
00:23:16,640 --> 00:23:21,440
the meta questions. How do we build something, et cetera? How am I going to survive? Like,

224
00:23:21,440 --> 00:23:26,080
I have to publish tomorrow. My students have to, you know, I'm, oh, no 10 year old, sorry,

225
00:23:26,080 --> 00:23:33,600
what are you working on? Like, it's not even an option, right? So, so my, so my hope is that, yes,

226
00:23:33,600 --> 00:23:39,600
um, we start these smaller independent institutes where we can actually say stuff. Um, Alex was

227
00:23:39,600 --> 00:23:45,360
telling me about a talk that she gave the other day. And that might, might not have made a number

228
00:23:45,360 --> 00:23:49,120
of people happy, but she's like, well, that's fine because I'm not looking to get tenure. And I'm like,

229
00:23:49,120 --> 00:23:53,840
yeah, that's the kind of stuff you can do when you're not looking to get tenure. So I think, you

230
00:23:53,840 --> 00:23:59,440
know, it gives us the opportunity to actually advocate for things that we think are important. And

231
00:23:59,440 --> 00:24:06,960
maybe, um, slowly, those other larger institutions might change or, you know, have pressure to,

232
00:24:06,960 --> 00:24:12,000
to do things differently. If they know that there are different options, like, like our institute,

233
00:24:12,000 --> 00:24:18,160
and if other people create other institutes. And honestly, um, if you look at, you know,

234
00:24:18,160 --> 00:24:24,880
even how I started Black Neye, before Black Neye, I had been involved in a lot of other organizations,

235
00:24:24,880 --> 00:24:30,640
like for diversity or for this or for that. And I was like, you know, there's no way I can convince

236
00:24:30,640 --> 00:24:36,800
you all to do the things that I think we need to do for Black people, you know, I just, I fought,

237
00:24:36,800 --> 00:24:41,360
I tried at this and that. And I was like, let's start something new and do it the way we think it

238
00:24:41,360 --> 00:24:46,880
should be done. And this is kind of similar to that, right? I tried this. I tried that. I tried

239
00:24:46,880 --> 00:24:54,000
inside the organizations. I tried appealing to, you know, higher ups. I feel whatever. But, you know,

240
00:24:54,000 --> 00:25:01,360
I found that that's not, you know, it's not working. And so what I want is to have an alternative.

241
00:25:01,360 --> 00:25:06,960
And even when you look at Black Neye, right? What has, you know, there's now, um, Latinx,

242
00:25:06,960 --> 00:25:12,080
and AI, queer, and AI, indigenous, and AI, disabilities, and AI, you also have a lot of Black

243
00:25:12,080 --> 00:25:17,360
in X, I'm Black in robotics, Black in neuro, Black in physics, Black, I don't even know like there's

244
00:25:17,360 --> 00:25:24,800
so many of them, right? And we can then, you know, build, there's like a network. Now you have,

245
00:25:24,800 --> 00:25:30,400
you build power and you can advocate for things collectively. Um, and so hopefully that's what I'm

246
00:25:30,400 --> 00:25:37,840
hoping with Dare, right? It's kind of an alternative to what we have right now. Um, and hopefully,

247
00:25:37,840 --> 00:25:43,520
you know, other people can kind of replicate it in a way that not exactly replicated, but,

248
00:25:43,520 --> 00:25:50,080
you know, in a way that works for their context. Um, and, you know, so with Dare, I can do things like,

249
00:25:50,080 --> 00:25:54,960
you know, think about funding. Right? Where is our funding coming from? You know, honestly,

250
00:25:54,960 --> 00:26:00,800
sometimes it feels like pick your poison, like there's no really clean money, like, you know, I'm learning

251
00:26:00,800 --> 00:26:06,720
all those things, but, you know, you, I'm thinking about, right, like, again, like I said,

252
00:26:06,720 --> 00:26:11,920
the meta questions, right? If we're thinking about AI and where the money comes from, you think

253
00:26:11,920 --> 00:26:18,000
about technology in general, when the government really invests in technology, right? Um, it's doing

254
00:26:18,000 --> 00:26:22,800
warfare, or when they're interested in something to do with warfare, like so the transistors and

255
00:26:22,800 --> 00:26:28,160
silicon valley, right? Um, in World War II, you think about machine translation, why people

256
00:26:28,160 --> 00:26:33,040
were interested in their research, has to do with Russia, Cold War, you think about DARPA

257
00:26:33,040 --> 00:26:38,160
and self-driving cars. It wasn't because they were like, oh, we need, you know, to make cars more

258
00:26:38,160 --> 00:26:43,680
accessible. You know, we need, we need to make sure that blind people can very freely move around.

259
00:26:43,680 --> 00:26:51,920
So let's build that's not what they said. They said we, we care about, you know, autonomous warfare,

260
00:26:51,920 --> 00:26:58,480
right? And so how do we expect to come to a different conclusion when from the very beginning,

261
00:26:58,480 --> 00:27:02,960
our funding, our incentive structures, every, the paradigm that we're using has something to do

262
00:27:02,960 --> 00:27:08,480
with warfare. And it's the same with industry too, like if, if all you're thinking about is how to

263
00:27:08,480 --> 00:27:14,880
make money for this large, huge humongous company that, that, you know, affects the entire world,

264
00:27:14,880 --> 00:27:19,600
controls the entire world, how do we have the space to think about a different paradigm? Right?

265
00:27:19,600 --> 00:27:24,960
So like we're hoping to think about a different paradigm. I'm sure like, you know,

266
00:27:24,960 --> 00:27:29,600
not that these paradigms don't exist. Other people are doing it too. But like, you know, kind of

267
00:27:30,240 --> 00:27:34,240
take the time for ourselves to think about what paradigm should we follow starting from the funding

268
00:27:35,200 --> 00:27:39,360
to how we do research to, you know, who we are hoping to serve.

269
00:27:40,320 --> 00:27:45,920
You know, we, we both have a lot of kind of colleagues in the industry that are working within

270
00:27:45,920 --> 00:27:53,280
larger organizations trying to help them use AI responsibly. You know, what does it say about

271
00:27:53,280 --> 00:27:59,280
that work? Is it, you know, futile? Is it for not? Is it, you know, a pessimistic view? Or is it,

272
00:28:00,560 --> 00:28:06,720
you know, do you, do you have examples of that process working correctly that you refer to?

273
00:28:07,440 --> 00:28:12,080
And, you know, you're just offering an alternative or do you think that that is,

274
00:28:12,080 --> 00:28:22,240
um, you know, yeah, you know, there's, um, there's this paper called, um, what is it? The Grey Hoodie

275
00:28:22,240 --> 00:28:26,960
project from the university project? Yeah, from people at the University of Toronto. And, um,

276
00:28:27,760 --> 00:28:34,720
and they talk about, they say how big text tactics are close to big tobacco. So they talk

277
00:28:34,720 --> 00:28:40,400
about how, um, they give examples of how like, you know, the tobacco industries would give lots of

278
00:28:40,400 --> 00:28:46,160
money to certain academics who talk at who write about how, well, you know, it's not, it's unclear

279
00:28:46,160 --> 00:28:52,160
if smoking causes cancer or something like that. Or they would then internally retaliate against

280
00:28:52,160 --> 00:28:57,760
people who actually have those kinds of funding, I mean, of conclusions, right? Or fossil

281
00:28:57,760 --> 00:29:02,320
fuel industry who's scientists knew about climate change way back, but they were suppressing it.

282
00:29:02,880 --> 00:29:09,200
And so why, you know, why wouldn't big tech be like that? I mean, what is their incentive not

283
00:29:09,200 --> 00:29:14,880
to be like that? So I, I have seen it myself, how they capture, how they, you know, people

284
00:29:14,880 --> 00:29:21,440
talk about industry capture, how they use, um, research in order to like, fight back against

285
00:29:21,440 --> 00:29:27,520
regulation. So I do believe, honestly, that the number one, um, reason that these large tech

286
00:29:27,520 --> 00:29:34,880
companies want to have these clinical ethics teams is to, um, in order to like, fight back against

287
00:29:34,880 --> 00:29:41,280
regulation. So after I got fired, you know, uh, members of Congress and representatives sent a

288
00:29:41,280 --> 00:29:45,440
letter to Google, and there was a number of letters they sent. First of all, they sent letters

289
00:29:45,440 --> 00:29:51,760
about, you know, um, you know, the, the number of black people they have in these AI divisions,

290
00:29:51,760 --> 00:29:58,640
do they have special like, uh, training, uh, in AI, except, you know, racial equity or in

291
00:29:58,640 --> 00:30:03,280
the training or impacts that or anything. And they write back and they say, oh, we have, you know,

292
00:30:03,280 --> 00:30:08,320
these, um, ERGs or whatever, you know, employee resource groups and we have lots of black people,

293
00:30:08,320 --> 00:30:13,600
we have this event, that event. And similarly, um, they are, uh, wrote a letter to them about

294
00:30:13,600 --> 00:30:17,840
Lara's English models and their impacts, et cetera. And they're like, we have had hundreds of,

295
00:30:17,840 --> 00:30:23,520
uh, papers in ethics and fairness. You know what I mean? But I know for a fact, they are actually

296
00:30:23,520 --> 00:30:30,080
suppressing more papers about the dangers of Lara's language models. You would think that they've

297
00:30:30,080 --> 00:30:35,680
learned from, uh, their lessons. So when they're doing this, they, they are freely allowed to suppress

298
00:30:35,680 --> 00:30:41,520
and persecute people with certain kinds of works, but not others, you have to ask why. And that's,

299
00:30:41,520 --> 00:30:46,960
that becomes more of a propaganda than research, right? So I do think that this is their goal, but

300
00:30:46,960 --> 00:30:53,760
so the people inside can know that and try to fight that, right? And, um, I think the way they can

301
00:30:53,760 --> 00:30:58,960
fight that is through collective organizing. Like, people before them have done, right? Um,

302
00:30:58,960 --> 00:31:04,160
Polaroid or a workers organized against Polaroid's, um, partnership with apartheid South Africa,

303
00:31:04,160 --> 00:31:09,680
right? And that was, that had a, a big impact. So it's not that I don't think that people in the

304
00:31:09,680 --> 00:31:16,880
inside cannot, um, cannot change things, but it's that they have to be vigilant to understand

305
00:31:16,880 --> 00:31:22,880
why they want them there and how their work is being used, right? If your Meg Mitchell used to

306
00:31:22,880 --> 00:31:27,440
call it fig lift, right? She's like, oh, fig leaf. She's like, I don't want to do a fig leaf work,

307
00:31:27,440 --> 00:31:32,880
you know, because like they already do everything else. And you do, you do the fig leaf work,

308
00:31:32,880 --> 00:31:36,560
right? Like you're like, just stamping what they say, oh, we're going to write your ethical

309
00:31:36,560 --> 00:31:40,960
consideration section or we're just not changing the course, the direction that you're doing.

310
00:31:41,840 --> 00:31:46,960
But we'll, we'll sort of do a fig leaf thing. That can do much more harm than good.

311
00:31:46,960 --> 00:31:54,560
Do you, are there examples in the industry that you look to as you're creating there?

312
00:31:54,560 --> 00:32:01,760
Um, well, I mean, there's examples of what I don't want it to be like and what that was a huge

313
00:32:01,760 --> 00:32:08,160
motivation for, you know, like the open AI type stuff is not what I want. Like when open AI was

314
00:32:08,160 --> 00:32:14,320
announced, and I've been so clear about this, I've never had kind of, um, I remember when open AI

315
00:32:14,320 --> 00:32:19,600
was announced, I was at New York's. And I think it was in 2015. And that was before the name change.

316
00:32:19,600 --> 00:32:29,600
And I had just gotten harassed at some, you know, bro, like, at a Google party. I was harassed.

317
00:32:29,600 --> 00:32:34,080
I was having a horrible time. I just like, I don't ever want to come back to this conference.

318
00:32:34,080 --> 00:32:42,400
And, you know, and it was after the whole Google gorillas incident. And they announced this company

319
00:32:42,400 --> 00:32:49,680
that said this, that's supposed to save humanity from AI, like the whole world. It's all about

320
00:32:49,680 --> 00:32:56,720
Elon Musk and Peter teal and all these people. They had 10 deep learning people 100% no interdiscipline

321
00:32:56,720 --> 00:33:04,720
or whatever. Eight of them white men. Uh, one white woman, one Asian woman. Um, and I know for,

322
00:33:04,720 --> 00:33:11,280
I like, I knew for a fact, this was not going to save humanity. And fast forward, what are we talking

323
00:33:11,280 --> 00:33:16,080
about? We're talking about GPT three. We're talking about the dangers of large language models.

324
00:33:16,080 --> 00:33:19,920
We're talking about how we're worried about things. We're talking about how, you know, there's

325
00:33:19,920 --> 00:33:25,840
unsafe products out there, etc. Right. So of course, like, you start at the insect. This is exactly

326
00:33:25,840 --> 00:33:30,160
what I'm thinking about institution building. Right. So you start at the inception. Who was at the

327
00:33:30,160 --> 00:33:35,840
table? Where did the funding come from? Where were, you know, and so unless you think about that,

328
00:33:35,840 --> 00:33:42,720
you can't not arrive at the kind of the state that we're in today. There are a lot of those

329
00:33:42,720 --> 00:33:48,880
kinds of models. I am finding out about institutes left and right right now. One working on AGI,

330
00:33:48,880 --> 00:33:53,840
one working on some other thing that has like 50 million dollar endowment or whatever.

331
00:33:54,640 --> 00:33:58,880
I get irritated. I'm like, where is that 50 million dollars coming from? I wanted to

332
00:33:58,880 --> 00:34:04,560
dominate. So I don't have to think, but you know, but then I would have to compromise on of course,

333
00:34:04,560 --> 00:34:09,600
probably where we get the money or something like that. So that is, you know, on the one hand,

334
00:34:09,600 --> 00:34:15,520
I have models of what I don't want, but I do have models of the kinds of grassroots organizing

335
00:34:15,520 --> 00:34:20,800
that I've seen that I'm really excited about. Right. So for instance, I gave you an example of

336
00:34:20,800 --> 00:34:27,440
Masakani, which is a network. And it was really beautiful to see, right, because it grew up

337
00:34:27,440 --> 00:34:33,920
of a grew out of the deep learning in Daba, which is a convene, right. And so then they create a

338
00:34:33,920 --> 00:34:39,680
people there who met there created Masakani network. And it's really cool. It's a whole bunch of

339
00:34:39,680 --> 00:34:45,840
people focused on working on natural language processing tools for African languages. And

340
00:34:46,400 --> 00:34:51,600
their values are very much kind of in line with the kinds of values I'm thinking about for

341
00:34:51,600 --> 00:34:57,360
dare. And they grew it super slowly, you know. And I think now they have a foundation. I don't think

342
00:34:57,360 --> 00:35:05,280
they have any full-time people. Before the show, I was talking to you about this article, this

343
00:35:05,280 --> 00:35:12,320
wired article I had read about the Maori who created speech-related technology to benefit

344
00:35:12,320 --> 00:35:21,360
their community. So they had this competition using their local radio for people to send in

345
00:35:21,360 --> 00:35:27,120
kind of annotated speech for speech-to-text and other kinds of speech-related, you know,

346
00:35:27,120 --> 00:35:33,520
language-related technology. And they had like hundreds of hours of data, right. And then all of

347
00:35:33,520 --> 00:35:38,160
a sudden, this company, I think, was called Lion Bridge or something in the American company,

348
00:35:38,160 --> 00:35:44,400
wanted to license their data. And they, you know, said no. And they published their reasoning. And

349
00:35:44,400 --> 00:35:49,680
they said, you know, we think this is the last frontier for colonization. They beat the language

350
00:35:49,680 --> 00:35:53,520
out of our grandparents. Literally, we're not allowed to speak this language. And they were

351
00:35:53,520 --> 00:35:58,960
beat up for speaking it. And why is this company interested in like, you know, buying stuff for

352
00:35:58,960 --> 00:36:03,120
now? It's not, it's obviously not because they want to benefit this community. So they're like,

353
00:36:03,120 --> 00:36:07,440
we want to make sure that whatever we do with this data and how, you know, it's something that

354
00:36:07,440 --> 00:36:11,760
benefits us. So those are the kinds of models I'm looking at. I'm like, oh, that's awesome. Like,

355
00:36:11,760 --> 00:36:16,560
I like that, you know, how they're doing data stewardship, you know. And, you know,

356
00:36:16,560 --> 00:36:22,240
unless it can't, I like their approach for grassroots organizing. Mijante, right? It's another

357
00:36:22,240 --> 00:36:27,600
grassroots organizing. They're doing such great work. I just read their report on, for instance,

358
00:36:27,600 --> 00:36:34,560
border technology. And they're talking, they're educating people about what are the different

359
00:36:34,560 --> 00:36:40,240
companies involved in this like digital border, digital border walls? How should we organize?

360
00:36:40,240 --> 00:36:46,720
They drove Palantir out of Palo Alto, right? And they had this no tech for ice campaigns. So, so I'm

361
00:36:46,720 --> 00:36:53,040
looking at that them too. And how they're, they've been able to be so successful with their grassroots

362
00:36:53,040 --> 00:36:59,360
organizing. So I'm looking at different kind of, you know, different kind of models to see what,

363
00:36:59,360 --> 00:37:03,680
what it is that I like about each of these models and what makes sense for dare.

364
00:37:03,680 --> 00:37:12,160
Yeah, yeah. One of the things that you mentioned, as we were chatting before, getting started was

365
00:37:12,160 --> 00:37:18,160
this realization that you had that you can't reduce fairness to a mathematical problem. Like,

366
00:37:19,120 --> 00:37:25,360
I think you saw that experience that. I see a ton of that elaborate on that a bit and kind of

367
00:37:25,360 --> 00:37:32,160
your journey to realizing that and where you see it. How it occurs for you out in the industry?

368
00:37:32,160 --> 00:37:39,520
You know, I mean, like my sisters have been saying, you know, doctors of Vienna was been saying this

369
00:37:39,520 --> 00:37:45,360
forever. Dr. Ruha Benjamin has been saying this forever is Simone Brown. And, you know, like

370
00:37:46,160 --> 00:37:52,000
the people not trained as engineers and computer scientists have been saying this for a long,

371
00:37:52,000 --> 00:37:59,360
long time. And unfortunately, I was reading Philip Agres. It was the most depressing thing. In the

372
00:37:59,360 --> 00:38:08,560
90s, he wrote, so there was even actually a Washington Post article about him. He was in AI and then

373
00:38:08,560 --> 00:38:16,960
kind of became much more of a critic of it. He became a professor and he was like talking about a

374
00:38:16,960 --> 00:38:21,360
number of issues that, for instance, like people are going to share their data much more

375
00:38:21,360 --> 00:38:28,640
freely with, you know, for various applications, right? This is way before social media and stuff.

376
00:38:28,640 --> 00:38:32,400
And he was like, it's not going to be so much of a big brother kind of thing, but people are

377
00:38:32,400 --> 00:38:37,280
just going to like share it without knowing, without thinking carefully, talked about face recognition.

378
00:38:37,840 --> 00:38:42,240
And so I was reading this, like, lessons from trying to reform AI or something like that. I'm like,

379
00:38:42,240 --> 00:38:47,600
you got to be kidding me. What year was this in the 90s?

380
00:38:47,600 --> 00:38:52,880
I don't remember when. I mean, but it was like lessons from trying to reform AI talks about how

381
00:38:52,880 --> 00:38:57,680
the field is not reflective, how it's arrogant. You can't get people to think about disciplinary

382
00:38:57,680 --> 00:39:03,680
norms and whatever. And I'm like, oh my god, like it's true, right? And that's what it is. It's

383
00:39:03,680 --> 00:39:10,640
that when the field feels like it's better than other fields has a lot of power and money

384
00:39:10,640 --> 00:39:15,280
thrown at it. At this point, you have money from the government, money from industry, money from

385
00:39:15,280 --> 00:39:20,400
everywhere. You don't have to think about what anybody else says, even though these people have

386
00:39:20,400 --> 00:39:29,760
been saying this stuff forever. And so what my own experience showed me, of course, we can talk

387
00:39:29,760 --> 00:39:38,640
all we want about, we want to reduce fairness to mathematical equations, because first,

388
00:39:38,640 --> 00:39:44,640
there's again, it takes me back to that incentive structure. When you think about how you ascend

389
00:39:44,640 --> 00:39:51,680
in the academic world in computer science or in engineering in general, there is this hierarchy

390
00:39:51,680 --> 00:39:55,680
of knowledge. I gave a whole talk about the hierarchy of knowledge, right? Certain kinds of knowledge

391
00:39:55,680 --> 00:40:01,440
and contributions are valued. So if you spend five years working on data-related stuff, first of all,

392
00:40:01,440 --> 00:40:06,400
in these conferences, if you have a data track, it's already inferior. Oh, it's just a data set paper

393
00:40:06,400 --> 00:40:12,960
or whatever. Where's the engineer? That's how they talk. And actually, Kiri Wack staff gave

394
00:40:12,960 --> 00:40:19,440
a keynote at ICML 2012 called machine learning that matters, which was basically about this kind

395
00:40:19,440 --> 00:40:26,960
of stuff and how what conferences are valuing versus not. So that, to me, makes it such that you

396
00:40:26,960 --> 00:40:32,560
want to reduce everything to the algorithm, to the math, to whatever. You want to not look at it

397
00:40:32,560 --> 00:40:39,840
as a sociotechnical problem as many people in SDS have said. And when you do that, like the paper,

398
00:40:39,840 --> 00:40:46,000
there's a paper called fairness and abstraction. They do a really good job of giving examples of

399
00:40:46,000 --> 00:40:50,560
what kind of issues might arise when you do that, right? You're just like looking at the system

400
00:40:50,560 --> 00:40:58,720
in isolation, not thinking about it as, you know, as part of a larger system, which is like, how

401
00:40:58,720 --> 00:41:04,000
is it being used? What domain is it being used in? Who is using it against whom, et cetera? Then

402
00:41:04,000 --> 00:41:09,040
your analysis becomes very different and much more complex. But you're not incentivized to do that.

403
00:41:09,040 --> 00:41:14,000
Where you're not going to ascend, the people who want to give you tenure are going to be like,

404
00:41:14,000 --> 00:41:18,800
oh, whatever, that's just data, or that's just fluffy, whatever. You know, that's how I'm telling

405
00:41:18,800 --> 00:41:25,360
you what I talk. And so because of that, you're incentivized to be like, oh, you know, what does

406
00:41:25,360 --> 00:41:31,920
fairness mean? I have no idea what fairness means, right? And, you know, Mimi Anoha, actually,

407
00:41:31,920 --> 00:41:38,640
was Sita Pena, I think, who said she came to fact and she gave a talk about some of her

408
00:41:38,640 --> 00:41:46,400
observations there at a some other workshop. And she said, what, why, you know, what does it

409
00:41:46,400 --> 00:41:51,920
need to make systems fair that are punitive? That their job is to just be punitive, right? So,

410
00:41:53,360 --> 00:41:58,960
for instance, when people talk about risk assessment, they jump to, you know, using the

411
00:41:58,960 --> 00:42:06,080
compass data set and then, you know, Christian Lam and others, like, have written about all the

412
00:42:06,080 --> 00:42:11,440
issues with that data set and why people shouldn't just jump to use it. And then they, they say,

413
00:42:11,440 --> 00:42:17,360
okay, like, you know, we, we looked at that data set and we have this new algorithm and it makes,

414
00:42:17,360 --> 00:42:22,960
you know, this other metric higher by x percent, right? What does exactly, what does that mean in

415
00:42:22,960 --> 00:42:28,960
reality, right? Like what you're doing is you're still locking people up, like, you know, and you're

416
00:42:28,960 --> 00:42:32,800
trying to figure out how, you know, what does fair mean in this case? Like, you're locking this

417
00:42:32,800 --> 00:42:37,360
other person out of the same map. So a lot of people don't, you know, abolitionists don't even think

418
00:42:37,360 --> 00:42:42,240
that whole system should exist, right? So when you're not looking at the entire system and you're

419
00:42:42,240 --> 00:42:50,160
just focusing on this map, it's even, it's unclear like what, you know, if that's even something that

420
00:42:50,160 --> 00:42:55,200
will help or not, and many times it can be very harmful. I've seen this in the face recognition

421
00:42:55,200 --> 00:43:01,920
discussion, you know, after Joy and I wrote the paper, like gender shades, a lot of people were like,

422
00:43:01,920 --> 00:43:06,080
oh, okay, you know, Microsoft came out with an announcement saying now that, like, they've changed

423
00:43:06,080 --> 00:43:10,560
their training data, you know, data, and now it's much better. Now it's all, you know, accurate.

424
00:43:10,560 --> 00:43:17,600
It doesn't have, you know, the, for darker skin women, the air rates are not as high,

425
00:43:17,600 --> 00:43:22,880
but then when you look at all of this scholarship from especially trans and non-binary scholars,

426
00:43:22,880 --> 00:43:27,680
they talk about how automatic gender recognition should not even exist. It shouldn't even be a thing.

427
00:43:27,680 --> 00:43:32,880
So why are you jumping to making it go unfair, right? That's because you're not incentivized to

428
00:43:32,880 --> 00:43:39,200
look at the whole system. So, you know, and then even if you want to do the right thing, even if you

429
00:43:39,200 --> 00:43:44,800
want to do the right thing, like I tried to do at Google, if you're, you're going to get fired,

430
00:43:45,440 --> 00:43:50,400
then what, what does that mean, right? Like, where are we writing papers about what to do if everybody's

431
00:43:50,400 --> 00:43:54,800
going to get fired if they try to do the right thing? So that's what I mean. You just cannot

432
00:43:54,800 --> 00:44:00,720
reduce it to this mathematical thing, but you could, you keep on doing it and our field people

433
00:44:00,720 --> 00:44:04,960
keep on doing it because that's what they're incentivized to do and that's what they're rewarded for.

434
00:44:04,960 --> 00:44:12,320
I think what I love so much about talking to you is that you have this very clear view of all of

435
00:44:12,320 --> 00:44:18,240
the challenges, the systemic systematic challenges that are kind of inherent and baked into,

436
00:44:18,240 --> 00:44:26,640
you know, all of these systems that, you know, we struggle against and, you know, that doesn't,

437
00:44:28,480 --> 00:44:31,920
you don't get jaded. You just, oh, let me try something else. Let me try something else.

438
00:44:32,800 --> 00:44:33,760
I'll try something else.

439
00:44:38,560 --> 00:44:42,480
Honestly, I think sometimes it's because I don't really think about, you know,

440
00:44:42,480 --> 00:44:49,840
the other option, I think, is way too depressing is what I think, right? The other option of, like,

441
00:44:51,280 --> 00:44:56,880
you know, being like, I guess this is too big of a problem. We can't do anything. I just, you know,

442
00:44:56,880 --> 00:45:03,120
it's too depressing, right? And then sometimes, like, when I started there, now I think about,

443
00:45:03,120 --> 00:45:09,360
oh, my God, I was doing, I was doing this visualization of the plots of, like, how much money you have,

444
00:45:09,360 --> 00:45:16,960
your burn rate, when there's a red line, when you run out of money. And I made a mistake,

445
00:45:16,960 --> 00:45:22,640
and that red line was like literally like this October. And I just, my heart was just like,

446
00:45:23,440 --> 00:45:27,920
and I knew I made a mistake, but I'm like, oh, my God, I'm doing something right now where this could

447
00:45:27,920 --> 00:45:33,920
be, this could be the scenario. And it's not just my job. It's like all of these people's jobs

448
00:45:33,920 --> 00:45:38,960
that are on the line, you know? And so when I think about those things, I'm like, oh, my God,

449
00:45:38,960 --> 00:45:47,280
what am I doing? But you know, if I don't really, if I don't really, then, yeah, we have to be,

450
00:45:47,280 --> 00:45:53,360
and I love this quote by Maryam Kaba, and I heard it from Ruha Benjamin saying it,

451
00:45:53,360 --> 00:45:59,920
that hope is a discipline. It's, you know, we have to, yeah, what's the alternative? What do we,

452
00:45:59,920 --> 00:46:05,600
you know, and it's not like a lot of times, I think we think that things are so far away,

453
00:46:05,600 --> 00:46:11,040
they're not going to touch us, but we're seeing that that is not true, right? With the pandemic,

454
00:46:11,040 --> 00:46:16,240
with the wildfires in California, with, you know, all of the ways in which we're all connected

455
00:46:16,240 --> 00:46:23,040
in the world, like, you know, if you want a, just even a better, a better world for ourselves,

456
00:46:23,040 --> 00:46:28,080
I'm not even thinking about the next generation, who should sue the hell out of all of the prior

457
00:46:28,080 --> 00:46:35,120
generations for living them, like a world with, you know, with the climate catastrophe.

458
00:46:36,080 --> 00:46:40,400
Even if we want a different alternative, you know, I think we should work for it, and for me,

459
00:46:41,920 --> 00:46:46,560
I, it makes me feel better to do that, right? To feel like, at least, you know, I'm trying this

460
00:46:46,560 --> 00:46:54,000
other thing, you know, this other alternative, you know, otherwise it's just too depressing.

461
00:46:54,000 --> 00:47:01,520
I don't, I don't know how to not do that, you know? Yeah, yeah, yeah. So the initial funding

462
00:47:01,520 --> 00:47:08,160
for Dare came through MacArthur, have you identified funding sources beyond that, or how that

463
00:47:08,160 --> 00:47:13,200
all is going to work? Yeah, that's the big question that I'm working on right now. So the initial

464
00:47:13,200 --> 00:47:19,680
funding came from MacArthur and Ford, and also the Kapoor Center gave us a gift, and I'm Rocket

465
00:47:19,680 --> 00:47:25,360
Feller Foundation and Open Society Foundation. So it's all these foundations right now. And so

466
00:47:25,360 --> 00:47:30,640
now we're applying for, you know, project-based funding for grants, like based on specific projects

467
00:47:30,640 --> 00:47:35,440
that we're working on. And just like other people, you know, if there's an NSF grant, we're looking

468
00:47:35,440 --> 00:47:42,160
to that and, you know, see if we can apply. But I am extremely worried about having a whole

469
00:47:42,160 --> 00:47:46,480
institute that is only based on grants. So one of the things I'm doing right now is trying to

470
00:47:46,480 --> 00:47:52,320
figure out how do we have our own revenue stream, and what does that look like? And really hoping

471
00:47:52,320 --> 00:47:57,360
to have some things that we can experiment with in the next few months, because, you know,

472
00:47:57,360 --> 00:48:02,320
we have a bunch of people with expertise, and I think we can provide that expertise in different

473
00:48:02,320 --> 00:48:08,720
ways that are valuable to people, and that help us kind of generate revenue for our institute,

474
00:48:08,720 --> 00:48:13,040
in a way that gives us a little bit more freedom of independence and flexibility, right?

475
00:48:13,040 --> 00:48:20,320
Imagine right now, I say something wrong that one of the funders doesn't like, and they're all

476
00:48:20,320 --> 00:48:25,440
know each other, and then everybody can just be like, sorry, bye. Like, you know, that can happen.

477
00:48:27,280 --> 00:48:33,680
And it's really interesting, you know, the nonprofit world. You realize, you know, I mean,

478
00:48:33,680 --> 00:48:38,320
it is because of wealth inequality that this world even exists. It's actually really sad.

479
00:48:38,320 --> 00:48:44,560
And it's all the people I ran away from, like, you know, Eric Schmidt, you know, Chan Zuckerberg

480
00:48:44,560 --> 00:48:49,840
Basils, and these are all the people who have these large foundations that want to fund tech-related

481
00:48:49,840 --> 00:48:55,600
stuff. So it's, you know, so that's kind of what I'm thinking about right now. Like, we're

482
00:48:55,600 --> 00:49:02,320
identifying different funding sources, thinking about how to diversify our funding sources,

483
00:49:02,320 --> 00:49:09,360
what would our own revenue stream look like? And once I figure it, especially the revenue

484
00:49:09,360 --> 00:49:16,640
stream part, and we have a few things to experiment with, I'll be much happier. I'll be, you know,

485
00:49:16,640 --> 00:49:25,120
I'll feel much better about it. Nice, nice. Now, how far along are you? Are there folks that are

486
00:49:25,120 --> 00:49:32,560
dare-affiliated that have research projects that are spun up and things that you can talk to?

487
00:49:32,560 --> 00:49:38,800
So we have, we have Alex Hanna as a director of research, Dylan Baker, who used to be

488
00:49:38,800 --> 00:49:47,200
under me at Google 2. As a, as a research research and such engineer, we have, I think, two research

489
00:49:47,200 --> 00:49:55,920
fellows, Milla and Raseja. Milla just joined, like, this week. And one person who's probably going

490
00:49:55,920 --> 00:50:01,840
to join us full-time in the next month or so. And, yeah, so we have, for instance, one of the

491
00:50:01,840 --> 00:50:09,600
projects that I had been working on with, we've been working on with Raseja is this project to

492
00:50:09,600 --> 00:50:15,520
analyze spatial apartheid, the impacts of spatial apartheid using satellite images and computer

493
00:50:15,520 --> 00:50:20,960
vision techniques. And that's a project where, again, all the issues I talked about appear,

494
00:50:20,960 --> 00:50:26,320
like, it takes you a long, long time to, the innovation is on figuring out the data, right? Like,

495
00:50:27,440 --> 00:50:32,480
how to get the data and how to process it, how to annotate it. That's very hard to-

496
00:50:32,480 --> 00:50:34,720
What does that mean, spatial apartheid?

497
00:50:34,720 --> 00:50:42,240
Oh, spatial apartheid is basically, like, segregation, but it was mandated in 1950 by the

498
00:50:42,240 --> 00:50:51,120
Group Areas Act in South Africa. So it's a big, like, it's a feature of apartheid, you know, and so

499
00:50:53,280 --> 00:50:58,320
people of European descent could live in certain areas and everybody else had to live in,

500
00:50:58,320 --> 00:51:03,680
and, you know, other areas like townships. And the budget application was a lot lower for townships,

501
00:51:03,680 --> 00:51:09,920
of course. And so the question is, you know, supposedly apartheid has ended legally, right?

502
00:51:09,920 --> 00:51:16,160
But when you look at these aerial images, it's so clear. Like, the delineation is so

503
00:51:16,160 --> 00:51:22,080
clear. And so the question is, can we analyze the evolution of these neighborhoods and how things

504
00:51:22,080 --> 00:51:27,840
are changing? Because we know, right? It passes the smell test in that you can look at these things

505
00:51:27,840 --> 00:51:35,520
visually and do an analysis. We're not just trying to do magic, right? So the question is,

506
00:51:35,520 --> 00:51:42,240
you know, how can we use computer vision techniques to do that? So Rassadja, when speaking of,

507
00:51:42,240 --> 00:51:47,200
you know, exploitation versus not, et cetera, Rassadja, someone who grew up in a township,

508
00:51:47,200 --> 00:51:51,440
I mean, so this is a very personal project for her. So it's like she's, you know,

509
00:51:51,440 --> 00:51:56,000
investigating her own, you know, like stuff that's related to her. It's not like this,

510
00:51:56,000 --> 00:52:00,720
what people say, parachute science, right? So that's one of the projects we're working on. We just

511
00:52:00,720 --> 00:52:06,880
had a paper on Neuritz. We're working on releasing the data. That's one of the things I like

512
00:52:06,880 --> 00:52:12,880
about being a dare because I, you know, we didn't just stop, you know, publish the paper and like,

513
00:52:12,880 --> 00:52:17,040
really quickly release the data and we're done. We're like, okay, how do we release the data? How

514
00:52:17,040 --> 00:52:21,440
do we create visualizations? How do we allow people to interact with the data? What art,

515
00:52:21,440 --> 00:52:25,920
what follow up work are, you know, we're writing an article for Africa as a country. I don't know if

516
00:52:25,920 --> 00:52:31,120
you know the outlet. It's it's one of my favorite outlets about about the work and one of the

517
00:52:31,120 --> 00:52:37,600
things we want to say is that actually the South African government has to label townships

518
00:52:37,600 --> 00:52:42,320
if they want to analyze the impacts of spatial apartheid. What they do is they end up in the census,

519
00:52:42,320 --> 00:52:50,800
they lump it with just suburbs as formal residential areas. But, you know, that doesn't allow you to,

520
00:52:50,800 --> 00:52:56,160
to because townships were created as because of apartheid that doesn't allow you to. And this is

521
00:52:56,160 --> 00:53:01,040
interesting. It's, it's part of a larger kind of issue. Mimio Nguho was talking about how

522
00:53:01,760 --> 00:53:06,480
some of her work, I think it's called Data Voids or something like that talks about how,

523
00:53:06,480 --> 00:53:12,080
for instance, Google Maps didn't have Fabela in Brazil, you know, right? That's a huge, huge,

524
00:53:12,080 --> 00:53:19,120
huge community of people. So it's part of this larger thing about, you know, who's data is, is

525
00:53:19,120 --> 00:53:26,080
visible anyhow. But yeah, like, but that's an example of a project that we're working on and

526
00:53:26,080 --> 00:53:30,720
there's a few others too. Well, how, how can the community support what you're doing?

527
00:53:31,680 --> 00:53:38,880
Well, you know, follow us a dare. We're going to, you know, on Twitter, I think we're going to

528
00:53:38,880 --> 00:53:45,840
also have more stuff on our website, just about more stuff we're working on. And you can donate

529
00:53:45,840 --> 00:53:50,960
to dare if you're interested. We're going to have, you know, fellowships for people that we're,

530
00:53:50,960 --> 00:53:57,840
you know, we have to think through how to do these fellowships too. And yeah, I think that's it,

531
00:53:57,840 --> 00:54:03,040
you know, and advocate for more funding for these kinds of independent research institutes.

532
00:54:03,040 --> 00:54:09,760
I don't want to have to cater to like a billionaire to get, you know, funding for our institute.

533
00:54:09,760 --> 00:54:16,000
I'd rather apply for a grant that comes from, you know, public, you know, taxpayers and, you know,

534
00:54:16,000 --> 00:54:21,440
be accountable to that. So that's another way I think in which people can advocate for these things.

535
00:54:21,440 --> 00:54:24,640
And are you still hiring? Are you bringing on additional researchers?

536
00:54:24,640 --> 00:54:29,600
Yeah, I mean, we have a lot of requests for hiring. And so we have to figure out, like I said,

537
00:54:29,600 --> 00:54:36,800
we have to first build the initial foundational team. And so before I, we open it up for like

538
00:54:36,800 --> 00:54:42,480
applications that will fly like that we've had like hundreds of people asking about internships

539
00:54:42,480 --> 00:54:49,360
and volunteer and, you know, full-time jobs. So after we set up the initial team, then we're

540
00:54:49,360 --> 00:54:53,040
going to be thinking, you know, thinking very carefully about what kind of internship fellowship

541
00:54:53,040 --> 00:54:58,000
opportunities will have, what kind of, you know, other full-time opportunities will have.

542
00:54:58,000 --> 00:55:02,160
I mean, that's the thing about having a small research institute and having to think about

543
00:55:02,160 --> 00:55:07,680
funding sources as I can't grow it really fast, right? I can't, like, so that's the sad part. But,

544
00:55:08,320 --> 00:55:12,960
and the thing about volunteer opportunities that I'm thinking about very carefully is,

545
00:55:12,960 --> 00:55:19,120
who does that prioritize? Right? A lot of people can't do volunteer stuff because they have to

546
00:55:19,120 --> 00:55:24,080
work. So I think I feel strongly about people being compensated for their work.

547
00:55:24,080 --> 00:55:31,680
Very cool. Very cool. Well, Timnett, it has been wonderful, as always,

548
00:55:32,480 --> 00:55:36,000
connecting, reconnecting with you and learning a little bit about

549
00:55:36,000 --> 00:55:42,640
dare and what you're building there. Thank you for having me. It's a lot of fun to

550
00:55:42,640 --> 00:55:48,320
come back periodically and kind of reminisce on like how much stuff has changed, you know.

551
00:55:48,320 --> 00:55:54,480
Yeah. Yeah, we'll have to be sure to schedule the next one not quite as far out.

