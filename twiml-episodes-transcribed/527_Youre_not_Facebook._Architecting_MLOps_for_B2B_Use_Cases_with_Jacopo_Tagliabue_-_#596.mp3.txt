All right, without further ado, I am super excited to jump into our keynote interview for this morning. Allow me to welcome up Yakapo Tagliaboo, former director of Kobio and member of South Park Commons.
Ciao, how are you? Good, good and you. Thanks so much for having me. I am wonderful. I am super excited for the interview. Let's just get started by having you share a little bit about your background and your recent experience. You're in the middle of a transition now.
Exactly. Up until recently, I was a director of AI at Kobio, a public Canadian company in AI SaaS, which I guess would feature prominently today in our chat. Before that, I joined Kobio because Kobio acquired my own startup, which was called Tuzo, which was a startup in Silicon Valley doing natural language processing for e-commerce.
It wasn't a glorious day before MLOPS snowflake and all the fancy things. So it was when you still have a lot of stuff to do yourself, basically. And then I enjoy my my role at Kobio in the last three years doing both AI and the research level system and also MLOPS and actually, you know, productionized those insights for hundreds of customers at the same time.
And now I'm building a new company in the data space, but that's a different story. That's awesome. So you've been a very outspoken, shall we say, critic of the way folks, some folks are approaching MLOPS? I think, you know, that may be too harsh, but I think you maybe a realist is a great way to characterize your perspective.
You've taken a pragmatic approach to MLOPS. Talk a little bit about the way you see folks pursuing MLOPS and some of the ways that that kind of varies from your experience.
Yeah, I mean, critic may be too harsh. I would say just Italians, you know, as we like to criticize everything, just as our default mode. But definitely more than a critic, I think that what we did, not just mean, but my team at large, kind of identified this gap like 18 months ago, starting something like that with the bigger boat repo.
And then if I dig up in the market, not just the market of tools, almost like in the market of ideas, I seen a lot of people were talking about MLOPS, either by this what we did at Uber or Pinterest, which is amazing.
And then this is a flash tutorial to put a cycle model online. So there was these two, these two words, it's like the hyper simple thing, the toy thing, which is good to start, you know, my students and why you do that.
It's awesome. You need to start somewhere. And then there's what people do at company with a limited budget, a limited talent, a limited resources and so on and so forth.
But the truth of the matter is most people are actually in the middle of this, you know, this distribution. Most people, you know, unfortunately for a living, they cannot just put a flash apple line. We need to do a bit more than that.
You know, how easy would it be my job if that was just a job. So it's a bit harder than the tutorial that you see online. But honestly, it's not, I wouldn't say maybe it's easier, but it's not the same type of difficulty that you find when we have this, you know, a planetary scale infrastructure, where everything customized your need.
And so our idea as a team was, well, we did this before. We made all possible mistakes in this field, because we did a garage scale at scale up scale and IPO scale. So we did this, this mistake a different scale. Why don't we tell people what we did? So maybe we saved them some time.
And so that's how we kind of the reasonable scale things started. And, you know, and all of that.
Yeah, yeah, I, you know, and hearing you describe this, I'm thinking that in large part, the, you know, an important part of this is kind of the distinction between, you know, where we were several years ago and where we are today, right.
And so several years ago, it was important to take a look at what Google and Facebook and Uber were doing because they were the ones that were being successful at getting models in a production. And they had identified a lot of best practices that we could take, you know, the fact that you need tooling and you need platform, you can't just do it all in a notebook, like that was an important realization at the time.
But now the opportunities are much broader. There's a lot more to learn from a lot, you know, a lot more companies that are having success and doing it in more of a scale down way.
So to your point, not everyone has to try to be Facebook now.
But I think there's still a lot of value into seeing what, what Uber or Pinterest or whatever Google does. In the same sense, I always use my tennis method, in sense that, you know, I really enjoy watching, you know, Roger, Roger Federer doesn't play anymore, but like, you know,
you know, you know, you know, you know, you're going to compete at the level or similar to the level. And so it's very good to know where is the North and start. But then when you go and you train, like myself, I don't train like Rafa Nadal, let's be honest, like, you know, I'm just trying to get the ball on the other side.
You know, my teacher, the math and the teacher would, you know, would teach me like the teacher, Rafa, it will actually be completely, you know, would be completely oblivious to the situation.
You know, the context in which you're in, make some things, you know, more important and something less. And I think it's very important to know, go, you know, all big tech, kind of like fetish of like, oh, the only good things in ML ops, cannot be done in that scale, because you think that's false.
There's a lot of super interesting ML that can be done in scale, actually. Yeah, yeah. Another dimension to the conversation is a lot of the folks that we point to, you know, the Facebooks and the Uber's.
They're serving these broad B2C audiences, whereas much of the opportunity, or there's a tremendous amount of opportunity for B2B companies. And, you know, they don't have the same sets of issues or problems or, you know, the same sets of data, the same sets of, they're not set up in the same way as B2C, you know, companies tend to be.
Yeah, I'd like to maybe dig into that a little bit and talk a little bit about some of the ways that B2B is, is different.
Absolutely. So, first of all, I think they're like two type of B2Bs that are very relevant for ML and that are really, really at the beginning of their journey. One is, B2B company like Tuzo, Coveo, and so on, which are really ML companies, as in we serve businesses with ML.
Models is what we do. And so, as you can guess, ML opting this company is super important, because what you really do is serve model a scale for hundreds of thousands of customers.
So, that's one important type of B2B, and of course, you know, most of my experience direct speed has come from that.
But there's a lot of other super important potential in B2B app, the expensive file, this word, the gust of this word, you know, the SaaS that actually are eating the word in many sense inside the SMB's or enterprises.
So, small models are part of their offering. I don't know, expensive I may have a machine learning model identifying the type of food that you are expensive or something like that.
And these two, these type of companies are very different challenges than the people building a recommender system for a B&B, for example.
And I like to think about usually three dimensions of differences. One is data, which is most, you know, quality, quantity, variety, and so on and so forth.
Another is modeling, like the actual model that actually will do the job and a type of effort that is required for the model to be successful and third one is tooling.
Okay, let's say you fix the data with a lot of work. Let's say we fixed the model and we can discuss a lot of work.
But the last part is how do you make that productive at scale?
But at scale, it doesn't mean one single website visited by 100 million people like Amazon every day. It may mean a hundred enterprises with, you know, 2000 access each.
They used to have to run in parallel. And of course, as you can imagine, tooling and automation is super important.
So, these three dimensions are very different between the typical B2C use cases and all the B2B use cases that have seen in my life.
So, your first set of B2B companies are folks that are creating models. The second are, you know, B2B, the example you gave was a B2B software company.
To what extent do the differences that you see also apply to kind of individual B2B companies that are trying to take advantage of ML to serve their users, their customers.
So, I think it's, for them, the sophistication of model compared to Acobay or Tuzo this word is going to be a bit less simple.
If we go to these access, so the data party may be easier because the data that this company uses are data that they own generating their own platform.
So, in some sense, they control that part, which is not true for model providers.
But the modeling aspect, maybe even if we can say maybe even less sophisticated because at the end of the day, that model is really part of a bigger value proposition to serve the customer.
While if you are a company that sells models as an API, you literally sell models. Your model better be good.
So, that I think that's a fundamental aspect there in terms of the evaluation of the company or the marketing or differentiating.
When you think holistically, not just the ML, but our ML play a role in the company's success. People that do models for a living still have a higher bar to somehow execute in the ML part.
Then people that build B2B SaaS, whose component may be powered by ML in some sense.
The still a lot of challenges there, especially on the tooling side if you go around the three dimension.
But I think it's fair to say that people don't buy expense because of their ML models. They buy expense because of the experience, because it's so well thought and so on and so forth.
But there's an argument to make the people buy Coveo or Brumic or whatever because of their model. That's literally what this company sells.
So, I think even the type of people that are going to hire, the type of resource that you're going to invest, they kind of change a bit between these two types of B2B companies.
And again, B2C is a completely different game, in this sense.
Yeah, before we dig into data modeling and tooling, I'd love to dig into the value proposition that you alluded to a second ago.
The way folks in B2B think about ROI and how that differs from folks on the B2B side. What's your experience there?
So, I think a huge, well, a huge part of being good at ML is connecting ML to something that the company will work for care about.
I know that sounds like a clich√© now everybody says that, but it really, it really is.
And the crucial thing when you consider a recommender system at Airbnb versus a recommender system in B2B is that there's a much harder to detect and make the case or even to somehow orient yourself in the B2B case than in the B2C case.
What do I mean by that? You work for a recommender system, let's say Amazon, even a super clear case.
You make a 1% improvement of the recommender system that translates in a building dollar at the end of the year.
Everybody claps your hands, you know, you're paying for the size of your team, you know, Jeff is happy, Jeff is not there anymore, but whatever, like everybody's happy and that's fine.
It's not that easy, of course, is a simplified, you know, stripped on version of the entire story.
But there's a clear sense in which what you do is immediately somehow available to the rest of the company to build upon and have success.
And that's a very clear case. But now think if you're building a recommender system for a hundred e-commerce that are not yours.
You don't owe the e-commerce, you don't own the data, you don't know the final shoppers.
So let's say you make a 1% improvement in your recommender system. What happens to your company? Well, nothing.
Why? What? For two reasons. One is because your 100 customer, your 100 enterprise customer you're serving are not Amazon, you know, each of one of them are not Amazon, right? So it means that 1% improvement for that is not going to result unfortunately in a billion dollar of revving it more.
But even more subtly, even if that was the case, your typical business model being a SaaS provider is not related to improvement directly.
So even if you made a 1 billion dollar for them more, you're not going to pocket the money directly. The business model of SaaS is typically, you know, a consumption or, you know, or whatever is, but it's something along that line, which means that improving the model per se is not as no short term game.
Okay, of course, if your model is not going to buy your part, so you're going to go out of business, but it's a much indirect relationship between what you're doing right now and what the product needs. That's even harder to measure in the expensive I case, for example, you don't even sell a model.
So how can we measure if the reccom, if the vision system that I specified is built is truly is truly aligned with the company, maybe savings and cost, that's an easy way doesn't measure, but how much are customers that appear because of that that's that's much harder problem to solve.
So one thing that I find in practitioner in the B2B space that are very good, the people that I look up to is the ability to still make a good case of why we should invest in that and to find some proxy or what it means for us to be successful.
Okay, even if we can make this a bit testing to recognize comparison like Amazon would. Yeah, what are some examples of those kinds of proxies. One that I really like is well robustness, which I think is super important robustness in two senses, one is the amount of maintenance and kind of like manual work that having this model in production required.
Itan yesterday said the child itan yesterday said you know that's you know the training model, you know, it's easy if you take up the arpegg.
Imagine doing that for 600 clients at the same time, so you really need to get your stuff together to be able to do that scale.
So the ability of you to go to retraining and and you know all the all the deployment phases very smoothly is a very important part of how you build a solution.
So one of course is metrics right as in you need to be able to monitor what you're doing and to make sure that in some sense you're providing a good service to your client, which may be bottom line, you know, you're.
MMR should should always go should always be be in a boat park or whatever or you can be more fancy and do you know be able to testing all sorts of tests that are like you know very important for people to make sure you don't do stupid stuff on somebody else's property.
So let's dig into the first of the kind of those tangible differences that you mentioned and talk a little bit about some of the ways that data is different in B to B context.
Imagine that typically one issues that there's a lot less of it.
Is that the case?
Yeah, in my experience, there's a lot of that or it's even softly. So there's a lot of them cumulative. So there's there's a lot of you know if you take together all the customer that are big to be company may have you made up with a pretty significant chunk of data.
But then when you go and look at the ML deployment, ML deployment are always per customer of course for all sorts of you know common sense but also legal and privacy, you know, complication that you can all imagine.
So at the end of the day, you end up instead of having one big client, you're having like a hundred of them which are big issues sort of that but they're not as you know as big as you may as you may have with you know with an Amazon or more or something like that.
So that's problem number one of course which is well which make some impact on the modeling that you can do as everybody knows there are some models that are you know more prone to you know to work very well with a lot of data but it don't really work you know with small data and vice versa.
So the data constraints somehow the second aspect which is the modeling aspect of course but the second part which is mixed which I think is really the hardest part is that it makes it very hard to scale the you know the data part without making you know without making unnecessarily costly or for the company.
What do I mean by that if you have one one data source is like your Amazon recommender and people click on Amazon then every time you make an improvement even when you collect some new data that goes fed in into one big data distribution.
The model is trying to capture in some way but if you have a hundred of them every time somebody clicks on something that just impact one of these are hundreds more distribution doesn't translate to any to any of the other one.
Which is one of the reason why building an out company at scale as a very different unit economics that building sus at scale because there are some fixed costs that don't really go down the more customer you have because you still have to chase this long tail of prediction every time you collect new data points.
So that would say it is a you know major major point when you plan a company thinking like an entrepreneur when you plan a company in this space need to be aware the B2BML company are very different beast than normal B2B software.
And of course another one is data quality right in data and data data standardization in general.
If you own your website if you are Amazon you can interact literally with every part of the system to make sure you know to some extent that everything flows seamlessly or at least you can ping somebody in your company and tell into fix it.
If you are downstream from that like if you provide the commerce with a recommender system the website is theirs so even this relationship of data feeding is way less direct of course you can ask them you can ask them to be better.
But it's not that you can literally ping somebody in your same company to fix that they may have a different timeline that you have so that makes your job much much harder because again this is one customer think of like a hundred different customer with a hundred different.
Data in some sense and you all need to be standardized to be working with one model type it's it's really challenging in that sense and involves a lot of processes not just code you know it's one of those problems that cannot be solved just by having good code.
So the problem that is involved actually people and you know the people problems are the artists to solve in general.
So it sounds like one of the implications of this idea of kind of not owning the the full stack to the customer is that if you have some cool idea for wanting to collect more data to enhance your model you might not be able to execute that or not easily.
So yeah in most cases you may not because you have to go back to your customer to the ones the front end to maybe change instrumentation or at something or convincing that this experiment is worth doing right but that of course you know length and the feedback and the cycle of operation which is why my suggestion is always when you start logging things try to log as much as you can from day one in that domain because truly you never know when that is going to be is going to be useful.
Like I remember like my back in the Tuesdays the 2017 when we started a company we started geo coding to a rough extent for example the city level where our shopper was doing was doing shopper like completely anonymous but at that time.
We didn't even have any geographical feature in our model whatsoever it turns out that we never add one even when we sell when we sold the company so we never use that but we plan for it in advance so that's here after when we just wanted to do some analysis.
To ask ourselves how much weather in that case was atmospheric weather influence shopping we could ask the question without going back to our clients because we have a couple of years of data that at that time we didn't know it was useful but we just thought you may
got to be useful but at that time you know they basically give us like this this treasure that we can just go in mind for new insight so my suggestion is always complying with you know laws and privacy and so
ever but log as much as you can because you never know what the new feature is going to be or you never know what you knew cool ML idea you're going to have.
Now does that practically speaking did you have a process where if anyone came up with an idea that was logged immediately or you know was there discussion about it and just the bar was very high to say no we're not going to log that.
Yeah I think more of a set was ever came up with logged.
The cool thing about it commerce that it commerce is a very advanced field compared to other type of verticals because because it comes like some of the best they are company in the planet that it commerce of course and even so everybody commerce is kind of.
Kind of understand the idea you need to log a lot of stuff tools like Google analytics for example that almost everybody uses already put their idea into our you know into retailers head so when you come in as a provider of
recommendation is say I want to log as much as Google analytics does everybody is typically happy to kind of like consider that and Google analytics local lot of stuff so the truth of the matter is that we are basically a protocol to copy in a good sense is like hey these will Google logs to provide people feedback and this is condense years of analytics in retail so if we start from this list we can be that wrong and it was actually you know it was a very good list.
It's also a lot of stuff but again there's a press conference you can point to and say hey if Google did it and you're happy with it as a provider you know let us do the same basically.
Yeah yeah I want to take a second here to encourage folks in our audience to chime in with questions via the chat on the platform I will be working those into our conversation looks like we've got a couple of folks.
In the chat that are working on B2B or I've mentioned that they're working on B2B there I'm sure there are others but it also strikes me that a lot of this conversation while it doesn't necessarily or well while it may not apply to kind of the.
The team working on the biggest problems at Facebook or Uber there probably are teams in a Facebook in a Uber that have a lot of these same challenges.
Absolutely the enterprise facing all the enterprise facing ML application even a Netflix or Uber or Google they resemble for many of these reason way more the B2B word or practitioner then their counterparts in the same company.
A discussion that I have all the time with the creator of metaflow like Bill and Sabine came from Netflix is exactly that like metaflow is an open source framework that is born at a big tech.
Mostly to satisfy the use not with the people building the recommender system but literally everybody else that in that case you know resembles way more you know my recommender system life than their own that they're more similar to me in some sense then to their own colleagues for that for that in that respect totally totally and I think there's an argument to be made.
That's in the next 10 years the biggest but or deep most deployments of ML solution are going to be inside of enterprises all sizes more than you know does the B2B the B2C or consumer facing so this is a huge amount of potential there if you get better at this everybody's going to benefit not just the companies but also of course you know the consumers in the end you know and all the process that gets in line thanks to the college.
Yeah yeah so maybe a question that folks can reflect on or uses a prompt for their questions are you know how can you scale down machine learning ML ops to meet the smaller use cases or or these B2B use cases.
But let's jump into some of the differences on the modeling side between what a Facebook might do and what B2B company can do.
So I think a lot of the a lot of what we said before it's kind of a super relevant for model like a lot of like of course we all like to build model like model is the could part like model is you know it's the shiny thing that gets on the on the CEO you know slides or like you know on you know on whatever or whatever covert or whatever you care about.
And of course models you know are very important and nobody nobody saying nobody saying they're not. But one thing that we realize is a field especially you know with a bunch of innovation the last couple of years is that model up to a certain extent are getting commoditized.
As in there's a bunch of back practice and architecture we know they kind of work you can kind of throw you know you cannot throw tabular data to gibberish you can kind of throw you know language data to you know to a transformer and you know it's going to be it's going to be fine you know it may not be the best thing ever
but it's going to be fine enough for you to start iterating which again is the most important thing like getting fast to production.
I think that that makes people realize that unless your model is mission critical and tied to revenue in the same sense that we discussed before so less 1% of better modeling makes you a billion dollar.
I think that people then realize and when modeling is solved there's the entire thing around models before and after which is I think is conventionally what ML ops refer to there is actually the hard part.
Like we thought for some reason what we thought was the cool PhD stuff is the one that we saw before because it's mostly a code problem not a people or a process problem so we kind of saw modeling in some sense.
But now we're left with it with the master models do before and after and so I think that that's an important lesson to be taken here so at the reasonable scale as we as we say models are important up for certain extent but after you reach that level of performance the software use cases that's typically not much of an interest or an ROI in spending another months you know tuning that or tweaking that or you know trying another architecture and so on and so forth again that's very different from.
The advertising machine learning model at Google which I don't know what it does but you know if you improve that by 0.1% that's going to be a success for the company so at some point also the type of people the main joy working in these two teams maybe different somebody is the guy that wants to build stuff from 0 to 1.
And solve the end to end use cases the other guy is the ultra optimizer you may be the guy that for 10 years just does the ranking optimization just because it does that and I think there's also reflect you know different personality of different choices that company to be successful.
And so you're saying that B2B it B2B might not be the place for that ultra optimizer.
I thought I think it's not I think what you like to do is you know is shaving off the last 0.5% of emerald yes you're probably better off at amazon then thank you absolutely.
We had a great conversation about build versus buy yesterday afternoon and there's some implications here it sounds like you are your proponent of build probably more so than you know folks at Facebook or Uber.
For B2B companies sorry buy.
Yeah I'm a huge yes I'm kind of like a huge a huge buy person in that sense where buy where buy and I think it reflects also some discussion that you have in yesterday buy doesn't literally mean just you know buy and forget.
But you of course means buy me also mean using off the shelf open source you know like the metaphor of my example before like so buy is in you use something that a lot of people did.
But I don't really don't build exactly but you have to take the you know the you know the the working yourself to kind of make it work with the rest of your stock and that's not really that's not the case.
I think the value is in the B2B especially is understand your.
What what your company what your company needs from email and I said what makes you unique and what's your unique take on it.
In our case a cover has been for example session based recommendation that recommendation the work without assuming to know anybody from the user which was a huge innovation the B2B space is you know it's the only is the only offering of the kind available still today.
And then everything else buy or you know quote unquote buy like don't build my customer doesn't care if I'm using metaphor if I manually spin up GPUs my customer doesn't care if I use you know snowflake or you know if I manually you know.
Wrangle CSV files so for me I just buy everything that makes me productive and it makes me get the result as fast as possible in most efficient ways possible my customer cares about recommendation though so I need to be good at that so once once in your mind is is clear what adds value to your company.
Optimize for that and everything else buy at least in the beginning because you don't know you know what you don't know in the first buy and use what other people have been doing and then you can always go back and change your mind is much harder to do the other around spending six month you know building something you don't realize you don't need.
Yeah yeah Dan asks in the chat one way that that his company's approached this is to really focus on reusability so for example with document understanding try to build tools that can be reused reused across different document understanding use cases I guess this is a an example of build or sorry buy.
You're kind of buying the thing that you built previously maybe.
Do you see reuse is playing a big role in efficiency for BDB teams when you can that's that's a good strategy for for the unit economics problem we mentioned before but mind you while English is English everywhere so sure I learned something about English with my model I can reuse in an angling
English document recommendation on this case for example because if I recommend an electronics shop or recommend something in a shoe shop of course I can reuse what I learned about electronics to sell you bags so depending on what you actually end up doing there may be somebody
usability to it which is great if you can please do that.
However it turns out that the word is a such that a lot of very successful application of a mail at scale I actually per deployment so there's not much you can use aside from the code of course that you can just bought from one to the other.
John asks about referring back to your comment about logging information that you've got even if you don't need it what about the notion that if you ask for data that you obviously don't need a proportion of that data will be junk data you run into that.
What's the problem I will ask back what's the problem with junk let's say let's say we ingest data I obviously don't need the need today like let's say what's what's the worst that can happen is just going to see that in my stuff like a nobody's not going to fit right I I think I'm not suggesting wasting money on on storage you don't need that's that's not an
message here by generally speaking we storage becoming exponentially cheaper over the years the cost of your missing a use cases because you didn't lock something and now you have to go back and log it and wait six months to have enough data is way more than logging up the B a bit more
generous in the first place and pay a couple of hundred dollars more to snowflake every month that's for sure that's for sure the worst thing can happen in people building a
small feature is having an idea that will put your company ahead and realize you need to wait three months to collect enough data to even see if that works.
That's really the worst thing I can.
So the next differentiator on your list is tooling we talked a little bit about this you know from a build and buy perspective are there
other differences in the way you see folks tooling for B2B. So I think the bit so the bit the planetary scale comes comes with a couple of
cavet right first of course a lot of custom system because they need to play nicely with other custom system so at some point the custom system just you
know ask for another custom system and the other thing that you may have latency or scale or security whatever constraint that I'm not just there for the
official market. On the on the on the smaller side the reasonable scale side there are two things first you're not as unique as you think as well as you're not
as special as you think meaning that in a good set meaning that the problem that you have a problem already been solved by somebody because you are in the
proportion of the distribution that is fairly that where that there's a few companies there. So if there's a tool that is being proven successful for you know small
enterprises chances is it's going to it's going to work for you it's going to you for you as well so that's that's one part and the second part is you don't have
them many people lying around probably like you don't have like a hundred engineers to build a custom tool over a custom tool so buying is also a way for you
to use the people that you have in the most effective way meaning that a they don't do frustrating work like glowing stuff or fixing
pipelines or stuff that honestly ML people don't want to do and they're very expensive you know and you know and you know and they're very in demand. So that's one point and second again you get the most
marginal value from that like if if I pay an ML people to the recommender system I don't want you to to end the infrastructure or to fix my
Kubernetes setup like you know I want you just a proper abstraction that allows that person to be to be in that sense to be productive a bigger
question would be is only sauce when you say don't buy when she don't build it's literally buy as offloading entirely what you need to do with a
platform or it may be an intermediate solution when you reuse some tools but you actually hosted yourself and that is more subtle is in the maybe
initial scale when you have to offload everything like even an orchestrator for example like typical example say airflow very popular orchestrator in the open source world but is also at least too very famous platform you know AWS and
astronomer that you can buy. Then after a while if you decide well you want to know what.
This is everything I want to orchestrate myself because I'm big enough it's important enough you can always bring it back to in house and somehow do like in between you
have to build and buy the open source to me is like an in between you have some cost of the building because they have to maintain it and integrated but also some
future of the buying as in this is a proven tool that has been used a hundreds of time before so if I use it again I'm likely to be within the 90%
range of of you know of capabilities that I actually need to do my job.
I thought that was a really interesting point about building in particular you kind of reference this hitting cost of building is that it predisposes you to
needing to build the next thing and the next thing and the next thing after that because you can't you're not using off the shelf things and they're not easily
integrated or it requires work to integrate you you're not selecting from a vendor's partners of all the things that you know it could integrate to fill in a particular slot.
Yeah and there's also incentives in this network FF for vendors to integrate with other vendors right so if I started with a vendor and open solution I'm likely to have like you know again if you start with
metaflow you know they're going to have an air flow integration right because because there are two popular tools if I'm an orchestrator Jacob of law whatever it is now I have to do you know that's not do that.
But now I have to do that myself so that's not it's not a reasonable point I think there's a lot of cost like especially small scale especially startup scale that I know well
opportunity cost is what's going to kill you like what's going to kill you is not moving fast enough to build a new thing and by definition if you're just building infrastructure that you could have bought you're not building the next cool thing.
One of the themes that played very heavily in that bill versus by conversation yesterday was fear of lock in do you have how do you parse that on this I'm really okay I'm really a contrarian and I really don't care.
I used for my last five years as a career one cloud provider and I'm and I'm fine like this speed and and you know reliability that I get by kind of using the past solution is way more than anything else that we need to do if I go you know if I if I kind of move out of that.
So I think people say sometimes well I may move out or WS or Azure or whatever honestly how many times did it happen like I certainly I certainly never did it and I don't plan on doing it again I think and I think this applied to every color provide like their solution when you when you when you start in the past approach are kind of so well integrated typically that at some point is an ecosystem that you know well when you are somebody you have best practices so I understand the general concern as a concern.
As a conceptual thing but I think at the scaling which I believe my most of my life in the last five years the gain of of loading all the problem to AWS like significant this or passes the question of like well what if AWS is the price is for 10% more next year.
So the one thing that wasn't on your list you know again data modeling tooling was team and I imagine the team set up is going to be different between B to B and B to see you've talked about one of those ways.
You know not you're not likely to find those kind of optimizers fraction of percent optimizers are there other ways the teams differ.
Well absolutely so I think there are there's a bunch of things that we learn at B to B to be productive they may not necessarily translate to B to see but also they may not necessarily be the norm at B to B.
So one thing that we would really like is the idea of the end to end ML person which again it's kind of a controversial a controversial idea which is the idea that your your ML person is responsible for everything going from the data in snowflake to shipping the model into production and making sure that works.
So it needs to be comfortable with SQL to extract the data and make sure that it is okay needs to be comfortable with building the model pipelines needs to be comfortable with your shipping into production.
Why because well first we want to enforce ownership and then proud of your work so no end off no I build the model and then I give you the notebook to somebody else and then is his fault in somebody goes better no it's your entire thing.
And second because again of kind of efficiency right as a smaller scale we can have that many people around like how many people do we even need to start the project.
Now if I needed that engineer a data analyst an ML person a DevOps person just to ship one model in production time for salaries and I don't even know if they need them.
So our and I think that ties back back to the build part our job as leader in B to B context.
Is to make sure that if you hired this end to end ML people people that really care about building a product a feature by themselves.
You abstract away all the you know chores infrastructure not needed problems they may have and that's why I want to build everything to buy everything.
I don't want my data sign to to to be preoccupied with Kubernetes or to scaling spark or to you know or to deploy manually into a pod.
All of this can be abstracted away from that but I wanted to be able to take ownership of the business logic that takes the data into a model into prediction and to feedback.
That's what I pay them for and my job as a leader and other senior people it to give them the building blocks and the legal blocks that they can assemble together to get the job done.
And me as a leader the best way for me to do it myself stop building it myself is picking from the vendors or the open source project that I like what I believe will make the experience of my team the best possible.
And that I think is a for us is really a great recipe of how small be to be team can actually be very productive.
Of course at Facebook scale that doesn't apply Google scale that doesn't apply like this entire teams to build the infrastructure for you but again you have different constraints.
But the reasonable scale we find the job of leaders at least in our in our case to be give you the tools and then get out the way and then hold you responsible and accountable for the entire thing not just for one time it is.
Yeah yeah tomorrow morning this time on our agenda we've got a fun debate plan to dig into and the end versus specialized tools I'm sure you have an opinion on that as well but I brought that up to reference the debate we had last time last year's Tom O'Conn and the debate question was should every data scientist and ML engineer learn Kubernetes.
Absolutely no absolutely no but they should understand like you know they should they should ship that model to Kubernetes or such maker or whatever you're using because somebody put the abstraction in place for them and that they should follow the life cycle of that to make sure that the model does you know what what I think you do so they need to understand the system perspective but I don't want them to do YAML file.
Exactly.
Exactly.
Yeah they are there are there are there are there for me like you know I know I don't I don't know what the team to do that but by the really to become one with the data like this old idea the now just people that are custodian of the data or custodian of deployment.
And then this data scientist the middle doing notebooks is againaring is enough is really not a good productive system for company again that they have you know they have a limited amount of resources in that sense.
So the kind of team member that you describe sounds like what you might call a full stack ML person.
That's, you know, they're working on a stack that you've provided for them through buying.
But from the perspective of your, you know, product or workflow, they're kind of going in to end.
Are there, do you have you found challenges in, you know, hiring that full stack person, you know, especially, you know, given that you're not Facebook.
Yeah, I mean, like staffing is a, of course, the elephant.
Everybody outside of five companies.
And honestly, even in fine companies, like, because they're keeping on stealing each other people.
It's a, it's a, it's a, it's a, it's a problem.
One thing that I, at least in my experience is that young folks with proper guidance may actually be surprisingly quick to get the, to get up to speed to, to this way of working.
Again, because you're not teaching them Kubernetes or, you know, scaling spark.
But because you're teaching, you know, logic that it will make them more productive.
So in my experience, you know, that, that's one way of being productive. Another one is, well, you need less people.
Like, if you set up your team properly.
Two end to end ML person will do the job of 10.
There are like, kind of continuously ending off stuff.
And there's a notebook, the notebook breaks, the dependency doesn't work today.
It is not clean. Nobody knows what in the table.
So at the end of the day, I know it sounds counterintuitive.
One more expensive person will actually cost you way less than five cheap one.
Just because the five cheap one now, now they're not going to even compare to the one that you have.
One, one, one thing that I noticed is that good ML team outside of big tech always look understaffed from the outside.
When you look at it and people told you how many people did that, you're always surprised like, well, that's, that's very, that's very few.
But the secret is, you know, if you take away the bottom lack of infrastructure, you need surprising this more
amount of people to build great things. I think that's been our experience.
You know, there's still, I imagine the, you know, you've hired engineers, even though you've only hired two.
They're still going to have that desire to run off and scratching it, right?
You know, there's this hole in this thing that you bought me. I love it. You know, can I spend three months building?
You know, the thing that would perfectly fit in that hole to automate everything and make everything wonderful.
Like, how do you, how do you deal with that?
That's, that's, that's a, that's a very good point.
Well, the general thing is that if that's not cool, like if that whole, like, you know, if it's in perfect, it is not really what we need.
Like, you know, let's use our time to do, to do better stuff. Like there's no shortage of cool things to get.
My point is, there's a shortage of cool things to do in, you know, in the M L word.
So let's not touch what's working with us. But if at some point is an existential piece, you know, by all means, let's, let's do that, especially if it's open source.
Like, you know, building a such platform is typically not a good investment of anybody's time.
But if it's an open source tool and you can fork it or extend it in some ways, some, some tools are better than others in that sense and they make it easier.
That's totally fine. Like, you know, I, I completely, we as a team, like, extended functionality of metaflow several times with our own custom stuff.
We still have the core of the platform, however, on custom stuff on top.
But the platform design is such a way that, you know, that we don't kind of like, you know, destroy the good just to have the perfect fit with our, without use cases.
So one thing that I would say to leaders, picking the tools, keeping this, you know, keeping these use cases in mind as in what happens if it doesn't really fit, maybe today fits 100%.
But what happens if tomorrow is 90? How easy will be for my team without disrupting everything to get the last 10%.
And I think that's an important, you know, variable when you decide, you know, which tool to pick.
Of course, again, with Suspac, it's much harder to do this.
Yeah, yeah. Granted that hiring is, is always difficult.
Samir is curious if you could comment a little bit more on, you know, what you've done to improve hiring.
Samir observes that adding humanity to the recruiting process has had an impact, but it's not something that many seem to do.
Are there things that have worked for you?
Being very outspoken and public about what we do, it's been very helpful, not just in a, you know, influencer sense.
I like literally doing a lot of open source, publishing our research ideas in the public for everybody to read, going to conference, going to event, be very vocal about the type of problems that we add and be very, you know, very honest.
And the type of what you're going to do with us, right? You're going to do cool ML stuff at this scale solving this problem. So if you want something else, we're not for you.
So that's point number one, like, you know, you need to have a brand in the, in the, in the good sense, as in you need to be, you know, a person that is distinguishable the community that does X and Y.
And so people will come to you. So that's, that's number one.
Number two, give back to the community.
Like a lot of the young contacts that we have, even for open source project or for research contact, are just because we are there.
Like we are in cheap discord discussing ML ops, we are in the materials.
It's like helping out people when they asked to, you know, we are here today.
Thanks much for having us and another occasion to talk about our experience.
And this is a lot of work, like being present for communities is a lot of work.
But it's also pays off to the community because you get back.
But also the community at some point is going to be there for you because, you know, the next time you build a company, you go and say, hey, I'm building a new company.
Hopefully some people, you know, will be already trust you and know you for what you can do.
And so it would be easier for you to attract talent.
So for us being member of the community has been a huge, but the research part.
So going to conferences and so on and the ML ops and on part, like the open source part.
These two communities for us are a never ending source of joy and also, and also pipeline people.
So Fahad had a question that I think goes back to our earlier conversation about kind of the nuance nature of ROI for B2B teams.
You mentioned the bottlenecks faced by small companies in terms of, you know, achieving sufficient data acquisition and model optimization.
So how do those companies achieve the kind of results that are important from a business perspective?
I think that's kind of getting at, you know, if it's hard to collect data, you know, you don't want to over invest in optimizing models.
Like, how do you really identify where the value is for your team? Are there things that you've, you know, tips that you've identified for really focusing in on that?
Yes, the first thing is put things in production as fast as possible as, you know, as cheaply as possible, possibly without a mail.
The first to do a mail is always never use a mail, right?
Let's say you're building a classification model for sentiment analysis, right? You don't have any data and it's totally fine.
Let's start with building a model that says if the word super cool is in the tweet, it's going to be positive.
If the word bad is in the tweet is going to be negative, okay?
Let's try to figure out for precision. We don't care of cloud.
A lot of tweets are going to go unclassified, but we just want to get a hundred of them every day classified and see how the people react.
See if you can type some value into that because to build that lambda function is going to cost you three minutes.
And then if people never even click on the feature you're building, there's no reason for you to build entire pipeline.
So try always to be incremental. So if you have a sophisticated organization where building your pipelines that are too easy, you know, you can start with a simple model.
But sometimes a day one in an organization, there's no model. There's no blueprint. There's no template. Start with something that doesn't require a mail.
And once you build the case that that thing is useful when maybe people click on it or people complain about it because it's not accurate, which means that people care.
So it's great. Then go back to then go back to you know to everybody else and say, hey, we may have something to it.
Come over over engineers. Again, exactly. Don't try to beat data sparsity, which is an important problem of itself, you know, before knowing if it's worthwhile.
Okay. So that's I think would be my trick to, you know, to try and innovate in that sense.
But the first thing that you mentioned is key also it's what I heard was about iterating quickly and, you know, not just trying things from a modeling perspective,
kind of end-to-end, you know, idea, iterate quickly, get things out in the market and see what matters, what has impact.
Yeah, absolutely. Speed of iteration, bit sophistication like every day of the year. And ML is still a very practical.
Like it's more art and science, you know, don't get fooled by the PhDs. Like ML is really more art and science.
So the more things you try, the higher the chance of success, almost by definition.
Of course, again, with the experience and with sophisticated issue, you get some boundaries and you get some intuition what's going to work and what doesn't.
But there's no substitution for just trying it out, see how people react and kind of iterate on that.
So the faster you can do that, the better it is bonus point.
If you build a good pipelines, you can swap in the modeling part by taking everything else when the time comes.
Or we can improve the data part and keep in the model constant.
Like building good pipelines with good legal groups will allow you to select to be improved.
When time comes, what you need without starting from scratch again.
So you're describing experimentation. One of the big challenges with doing experimentation in a B2B context is the, you know, the signal that you have to compare your results is is, you know, it's less.
So with regards to AB testing and things like that, it takes longer to converge.
Are you, do you, you find yourself making judgment calls on, on less data?
Or are there things that you do to, to collect data more, more quickly?
Or make those decisions more scientifically, I guess.
So some B2B use cases are so heavily, like for the purpose of a T-test to see if I have a lot of it, like e-commerce is a good example.
Even as moly commerce, which generates millions of data every day.
So typically that would be enough to do like a standard key test or whatever you're doing in your, in your, in your, like in your AB testing.
So that's typically fine.
In other cases, is more of a question of like, well, people seem to be engaged, but we want to invest more in that on that.
And that's, that's more of a judgment call with other business folks and so on and so forth, right?
There are some features that, even if they're not needed, they may be needed for the business for other reasons.
Because they're differentiator, because all of your competitors have it.
Even if just three people click on it, you need to have it, because that's, you know, that's the definition of what means to be a platform in some sense.
And so you just build it.
It's a very nuanced, like the point is like, it's what to build is always the hard discussion. How to build it tends to be after a certain level of experience, tends to be the easy part.
But what to build is what we decided together as a team is an organization to focus our effort on is always the hard discussion.
So there's no fast answer to that.
But yeah, sometimes there's data, sometimes they just you and the vision that you have for whatever the company's doing.
Yeah, yeah. I think Andreas in the chat wants to pick a fight with you about recording all the data.
That's something that I'll let the two of you take offline. Andreas, reach out via the platform and schedule a meeting with Jakobo to run that one into the ground.
We're going to log everything during the meeting. So just to make sure.
Also, well, thanks again, Jakobo for a great conversation. It's wonderful to have you on the at the conference today.
Thanks so much for having me and it's been super fun. Guys, if you want to chat more, please reach out on the platform link in the very easy to reach. So thanks again.
Thank you.
