1
00:00:00,000 --> 00:00:11,720
All right, everyone. I am here with Pavan Taraga. Pavan is jointly appointed in the schools

2
00:00:11,720 --> 00:00:18,240
of electrical engineering and arts, media, and engineering at Arizona State University.

3
00:00:18,240 --> 00:00:22,640
Pavan, welcome to the Twomo AI podcast. Thank you, Sam. It's a pleasure and an honor

4
00:00:22,640 --> 00:00:27,680
to be here. Thank you. It is my pleasure to host you here. I'm looking forward to digging

5
00:00:27,680 --> 00:00:34,320
into your paper that will be presented at CVPR, revisiting invariance with geometry

6
00:00:34,320 --> 00:00:38,880
and deep learning. But before we do that, I'd love for you to share a little bit about your

7
00:00:38,880 --> 00:00:46,720
background and how you came to work in computer vision and ML. Sounds great. So my beginnings

8
00:00:46,720 --> 00:00:54,480
in this area started as a senior in my undergraduate years. I was looking at problems like

9
00:00:54,480 --> 00:01:00,160
face recognition, face tracking from video, track to do a senior design project really. And

10
00:01:00,160 --> 00:01:07,760
as I started digging more, this was in the early or yeah, early 2000s. And it was a very exciting

11
00:01:07,760 --> 00:01:14,400
time to be in the field of computer vision because the problem statement in those days would

12
00:01:14,400 --> 00:01:20,560
be presented as giving a computer the ability to see and that felt like wow, that's a frontier

13
00:01:20,560 --> 00:01:27,200
topic, right? Yeah. And I said, you know, looks like I'm sufficiently invested in this, looks

14
00:01:27,200 --> 00:01:32,240
like it intersects with things like neuroscience, perception that is interesting mathematics,

15
00:01:32,240 --> 00:01:38,080
that is interesting computing happening, very highly interdisciplinary and it felt like there

16
00:01:38,080 --> 00:01:44,880
was much work to be done. So I decided to go to school in 2000, I mean grad school in 2004

17
00:01:44,880 --> 00:01:52,400
to the University of Maryland studying with Professor Ramachalappa who is a pioneer in this field

18
00:01:52,400 --> 00:01:58,080
and very well known for face recognition techniques and a lot of interesting things in vision at the

19
00:01:58,080 --> 00:02:06,640
time. And as I started my work in the lab, I broadened from just face recognition to other things

20
00:02:06,640 --> 00:02:13,280
like understanding video, understanding time series, understanding the role of light, you know,

21
00:02:13,280 --> 00:02:21,280
geometry, elimination, reflectance, all these physics-based concepts and how they interface with

22
00:02:21,280 --> 00:02:27,120
pattern recognition methods or machine learning methods. And as I went deeper and deeper into it,

23
00:02:27,120 --> 00:02:34,160
I felt like there was a big disconnect between the methods of physics of image formation

24
00:02:35,040 --> 00:02:41,120
and the methods that are used in machine learning where it's just purely a driven and statistical

25
00:02:41,120 --> 00:02:47,280
techniques. And I was trying to find some middle ground where I could inject physical knowledge

26
00:02:47,280 --> 00:02:51,360
into certain structures that could blend well with machine learning techniques. And one thing

27
00:02:51,360 --> 00:02:57,120
led to another, I started getting interested in this area of mathematics called Rimanian geometry

28
00:02:57,120 --> 00:03:03,360
and then topology as a means to express these intuitions and these constraints from physics and

29
00:03:03,360 --> 00:03:09,520
interface them with deep learning or machine learning in before deep learning. So that's the theme of

30
00:03:09,520 --> 00:03:16,720
my work over the past decade, which is try to understand basic phenomena, whether it's, you know,

31
00:03:16,720 --> 00:03:23,920
images or video or human activity. And in recent years, we've also broadened our investigation

32
00:03:23,920 --> 00:03:29,520
beyond computer vision to include things like wearable devices and physiological monitoring where

33
00:03:29,520 --> 00:03:35,440
the phenomena under study is basic human movement and other things. Try to understand it from

34
00:03:35,440 --> 00:03:42,960
first principles and try to express that knowledge in a way that constrains conditions

35
00:03:43,760 --> 00:03:46,960
machine learning. So that's the general intersection of topics I've been looking at.

36
00:03:47,920 --> 00:03:53,200
Okay, and I mentioned in introducing you that one of your appointments is with a school that

37
00:03:53,200 --> 00:03:59,440
has arts in the title. How does that come up in your work and research?

38
00:03:59,440 --> 00:04:06,080
So you're particularly connected to that particular piece of that school.

39
00:04:06,080 --> 00:04:13,120
So that's a whole story by itself. And I can go very organically about how it all began or I can go

40
00:04:13,120 --> 00:04:23,920
in hindsight, this is how it went organically. This is how it began. Our school was founded in 2009

41
00:04:23,920 --> 00:04:30,080
and you know, I graduated from grad school in 2009. But then that was 2009, very similar to 2020,

42
00:04:30,080 --> 00:04:35,680
Wall Street collapsing and you can lose. So I stayed back when I posed or for a couple of years.

43
00:04:35,680 --> 00:04:42,160
And when I went interviewing in 2011, this position opened up in this school under the title

44
00:04:42,160 --> 00:04:49,520
of Assistant Professor in Human Activity Analysis. It was very intriguing. And I had been doing

45
00:04:49,520 --> 00:04:56,480
human activity analysis as far as, you know, understanding a video-based gate analysis.

46
00:04:56,480 --> 00:05:02,320
Okay, so I'm like fitness trackers and quantified self and all that kind of stuff.

47
00:05:02,320 --> 00:05:05,440
Right. So this was a little before all that stuff happened in particular.

48
00:05:06,640 --> 00:05:10,720
And I came in and I visited and there was there was a mind-boggling sense of

49
00:05:11,280 --> 00:05:16,240
interdisciplinary that I saw in the school people in music looking at stuff like that, right?

50
00:05:16,240 --> 00:05:22,000
accelerometers, variables, driving, interactive performance with it. There was a group doing

51
00:05:22,000 --> 00:05:26,960
stroke rehabilitation where you attract human movements through motion capture and

52
00:05:27,680 --> 00:05:33,760
sonify the qualities of your movements. So smoothness or jerkiness would be converted to sound

53
00:05:33,760 --> 00:05:39,440
and you can hear yourself. And that provided additional feedback for you to correct your movements.

54
00:05:39,440 --> 00:05:46,080
Oh wow. It was, yeah, it was a very mind-boggling experience. And even now,

55
00:05:46,800 --> 00:05:52,560
people find it very interviewing when I mentioned these things. So the human activity analysis component

56
00:05:53,920 --> 00:05:59,600
was a way for the school to address, you know, slightly more formal ways to think about human

57
00:05:59,600 --> 00:06:03,600
movement, whether it's sense from a camera, whether it's sense from a variable device, whether

58
00:06:03,600 --> 00:06:08,640
it's motion capture, try to come up with techniques to represent human movement. And then

59
00:06:08,640 --> 00:06:14,080
do machine learning on it or feed into these other applications. And it seemed like I was

60
00:06:14,080 --> 00:06:19,360
like Fritt at the time and I got a job. And I've been here for eight years and it has led me into

61
00:06:19,360 --> 00:06:25,680
very interesting collaborations with, yeah, media artists and health scientists who are interested

62
00:06:25,680 --> 00:06:32,560
in the intersection of computing art and things like health promotion. So that's the other side.

63
00:06:32,560 --> 00:06:41,520
Got it. Got it. You mentioned that one of the big themes of your work is been

64
00:06:43,520 --> 00:06:49,360
integrating physical, physics-based principles and computer vision.

65
00:06:51,360 --> 00:06:57,840
Can you maybe talk a little bit about that in the, you know, broader context of the

66
00:06:57,840 --> 00:07:01,840
computer vision landscape? We've talked about this quite a bit on the podcast and I kind of

67
00:07:01,840 --> 00:07:07,200
described it as a pendulum that's kind of swung from physics-based models to statistical and

68
00:07:07,200 --> 00:07:10,560
kind of is settling somewhere in the middle now. But it sounds like you've been working at this

69
00:07:10,560 --> 00:07:16,720
for a while. I'm curious how, how you think of it? Yes, I mean, you're absolutely right. It's

70
00:07:16,720 --> 00:07:20,960
been a pendulum that keeps swinging and I don't think it settles anywhere. I mean, that's

71
00:07:20,960 --> 00:07:32,000
interesting problem about computer vision. Every time someone thinks it settles, right? Everybody thinks it's

72
00:07:32,000 --> 00:07:36,240
settled now in the soonest chips. So, I mean, yes, it's been going on and off. This idea about,

73
00:07:36,240 --> 00:07:41,040
I mean, I like to think of it as model-based vision versus purely data-driven, you know,

74
00:07:41,040 --> 00:07:48,640
methods of thinking of vision. Yeah. The way the pendulum swings in my opinion is not that

75
00:07:48,640 --> 00:07:55,280
the problems are changing. It's just the language that goes into talking about it and the tools that

76
00:07:55,280 --> 00:08:02,400
go into addressing it keeps shifting, but the problems have remained mostly the same and the

77
00:08:02,400 --> 00:08:09,040
problems are the following in my opinion. So vision is a very unique, you know, some people think

78
00:08:09,040 --> 00:08:14,480
of it as an application of machine learning, which is a reductionistic way to think about it.

79
00:08:14,480 --> 00:08:19,600
Sure, everything is data and everything is fed into a model and outcomes of decision, but vision

80
00:08:19,600 --> 00:08:25,920
and any perceptual, you know, field of inquiry, that can include visions, sound,

81
00:08:25,920 --> 00:08:30,960
haptics, any of these things which have to do with perception, I feel are fundamentally different

82
00:08:31,680 --> 00:08:39,040
than any other application of data analysis. The way we perceive the world is not the way,

83
00:08:39,040 --> 00:08:44,160
let's say, back transactions are processed by a machine learning computer, you know, a machine

84
00:08:44,160 --> 00:08:51,520
learning technique. There is a huge amount of variability is the way I'd call it that exhibits

85
00:08:51,520 --> 00:09:00,560
in the natural world, which is somehow either discarded or properly parsed out by whatever's

86
00:09:00,560 --> 00:09:07,200
happening in our brains and the sources of variability are physics-based to a large extent, you know,

87
00:09:07,200 --> 00:09:11,120
the same picture under a different lighting condition looks different, the same picture under,

88
00:09:12,000 --> 00:09:17,280
you know, a different slightly changed viewpoint looks different. So that looking different part

89
00:09:17,280 --> 00:09:23,280
is what gives rise to statistical variability. And the statistical ways of thinking are, well,

90
00:09:23,280 --> 00:09:28,800
let's just fill up the observation space with more data points and we'll figure out what the

91
00:09:28,800 --> 00:09:35,440
shape of the distribution is from data, which is okay in as intense to infinity, I guess that's fine,

92
00:09:35,440 --> 00:09:40,640
but when data augmentation approaches and domain adaptation and that kind of thing,

93
00:09:40,640 --> 00:09:46,640
yeah, yeah, but under and not tend to infinity, if you only have a few data samples,

94
00:09:47,280 --> 00:09:54,000
you are better off trying to understand how the physics around you affects the observed

95
00:09:54,000 --> 00:10:01,040
imagery. And that's where I think the methods have shifted. So, you know, in the keynote that I have

96
00:10:01,040 --> 00:10:05,520
had CBPR, you know, at the workshop called differential geometry and computer vision, I go through

97
00:10:05,520 --> 00:10:11,280
some of these historical trends. And one of the core themes that brings it all together, the word

98
00:10:11,280 --> 00:10:17,280
that I use is called invariance, which is when you look around and try to classify objects,

99
00:10:17,280 --> 00:10:24,080
we are able to do this in a way that is invariant to a lot of nuisance variables. That is light,

100
00:10:24,080 --> 00:10:29,040
you know, shading viewpoint and also it's of interesting effects that are hard to describe.

101
00:10:29,040 --> 00:10:37,440
When you say invariance, there's two ways to think about it. The physics-based ways to think about

102
00:10:37,440 --> 00:10:43,200
it are, let's say I am looking at this scene, I know everything about this scene, including its

103
00:10:43,200 --> 00:10:48,640
3D geometry, including how the paint reflects off light, including the wavelengths in the incoming

104
00:10:48,640 --> 00:10:55,920
radiation. If I have full knowledge of all of this, then I can re-render a scene, let's say,

105
00:10:55,920 --> 00:11:03,040
I can, just like how it happens in graphics, I can create so many different versions of the same

106
00:11:03,040 --> 00:11:09,280
picture, if I had full knowledge of everything, simply through a forward rendering process and construct

107
00:11:09,280 --> 00:11:15,120
variability. The data driven ways of thinking about it say, if you don't have access to everything

108
00:11:15,120 --> 00:11:20,000
that you need to understand the phenomena, what is the minimal set, what is the minimal piece of

109
00:11:20,000 --> 00:11:26,880
information that is needed to get a job done. That's the dichotomy in the physics-based ways of

110
00:11:26,880 --> 00:11:35,440
thinking and the statistical ways of thinking. When you use this term invariant is the invariant

111
00:11:35,440 --> 00:11:41,440
referring to, say we're talking about a scene with an object in it that might render differently

112
00:11:41,440 --> 00:11:48,560
in different lighting conditions, etc. is the invariant that object that is definitively in the

113
00:11:48,560 --> 00:11:55,360
scene and then we've got all these other effects, or does the invariant refer to something else,

114
00:11:55,360 --> 00:11:58,800
maybe something more in the mathematical from a mathematical perspective.

115
00:12:00,480 --> 00:12:05,600
I mean, the word invariant, of course, will depend upon what the end task is. If it is object

116
00:12:05,600 --> 00:12:12,320
recognition, yes, something intrinsic to the object is the invariant. It is not always as simple

117
00:12:12,320 --> 00:12:17,120
as saying the invariant is the color of the object because that changes. It is often not

118
00:12:17,120 --> 00:12:22,160
same as saying the invariant is the edge map of the object or certain corners of the object

119
00:12:22,160 --> 00:12:28,800
because they go in and out of view. So there's not easy ways of describing what that invariant

120
00:12:28,800 --> 00:12:32,800
actually physically means. So it becomes mathematical at some level. There is no linguistic equivalent

121
00:12:32,800 --> 00:12:40,240
that I can come up with. But if you look at this rendering ways of thinking, if you can render

122
00:12:40,240 --> 00:12:44,240
this object that you're interested to recognize, in all possible wing conditions, all possible

123
00:12:44,240 --> 00:12:49,360
lighting conditions, and you have this huge set of pictures, that huge set of pictures

124
00:12:50,240 --> 00:12:56,640
could be called, you know, the word sometimes that gets used is equivalent class,

125
00:12:56,640 --> 00:13:02,080
or sometimes they call it an orbit. So this object that you're trying to recognize manifests

126
00:13:02,080 --> 00:13:08,080
itself in all these different ways. If you have a handle on that set, you are in good shape.

127
00:13:08,720 --> 00:13:13,200
If you have a different object and you place it in the same scene and you render it in all these

128
00:13:13,200 --> 00:13:19,280
different variations, and you have its own different set, then the invariant that separates these

129
00:13:19,280 --> 00:13:28,640
two is some measure of difference between these two sets of pictures. And sometimes that set of

130
00:13:28,640 --> 00:13:34,720
pictures can have a nice structure which can allow you to compute it in closed form and sometimes not.

131
00:13:35,600 --> 00:13:42,240
So the way we have been trying to express, you know, sometimes delumination is complicated. And

132
00:13:42,240 --> 00:13:47,920
if, in the most general case, we don't know how to describe this full set of pictures,

133
00:13:48,640 --> 00:13:54,880
under some simplifying assumptions, which is rooted in some old work from the 90s,

134
00:13:54,880 --> 00:13:59,600
Bell Humor and Krigman wrote a very famous paper, and I said, what is the set of all faces

135
00:13:59,600 --> 00:14:04,800
under any given illumination condition? And they made some simplifying assumptions of what

136
00:14:04,800 --> 00:14:10,240
of faces, I mean, if I ask you define a face, it's part of how to define a face, right? So how do you

137
00:14:10,240 --> 00:14:16,000
even think? And this is why we've tended towards deep learning and statistical methods over the

138
00:14:16,000 --> 00:14:21,840
past few years because we don't know how to define these things. So exactly. But here is what they

139
00:14:21,840 --> 00:14:27,600
found. There's a, if you define it in some sub-linguistic ways, let's say it's a comics object,

140
00:14:27,600 --> 00:14:32,480
and let's say it's an object which has reflectance defined by some lambershield properties,

141
00:14:32,480 --> 00:14:37,920
then you can actually write down what the set looks like. Now comes deep learning. It says,

142
00:14:37,920 --> 00:14:43,040
I can't define these things, give me data. And the more data you have, the better it is. But

143
00:14:43,040 --> 00:14:46,720
no one knows how much data is enough, right? I mean, the more is better is the answer,

144
00:14:46,720 --> 00:14:52,320
but how much is enough is never known? And we have been positioning ourselves at that intersection,

145
00:14:52,320 --> 00:14:58,000
where we say, look, if I know that I'm looking at faces, I'm going to weaken the structure a little

146
00:14:58,000 --> 00:15:04,320
bit. I'm going to say that, yeah, these objects that we're looking at have some characterization

147
00:15:04,320 --> 00:15:10,000
under these assumptions of simplicity. But then comes deep learning, which allows me to fit

148
00:15:10,000 --> 00:15:15,200
those other degrees, which I'm not able to specify analytically. So we're trying to reduce the

149
00:15:15,200 --> 00:15:20,560
need for larger and larger training sets by restricting the deep net layers somehow that are

150
00:15:20,560 --> 00:15:28,400
motivated by the knowledge of that physics of image formation. Sure, we don't know how much

151
00:15:28,400 --> 00:15:35,040
data we need to get the full specification, but we're saying this will reduce the need for more

152
00:15:35,040 --> 00:15:40,720
and more data. And all things being the same with the same amount of training sets, the same

153
00:15:40,720 --> 00:15:45,600
complexity of the deep architecture, adding these constraints known from the physics of image

154
00:15:45,600 --> 00:15:53,440
formation improves performance. And it also stabilizes performance against degradation of inputs.

155
00:15:53,440 --> 00:15:57,840
You know, typically if you blur a picture, if you surrender a picture in slightly different ways,

156
00:15:57,840 --> 00:16:03,120
performance drops pretty dramatically, we are able to avoid that. It's a middle ground, I'd say.

157
00:16:03,120 --> 00:16:10,400
We are not being super specific about defining objects, nor are we saying more data is good.

158
00:16:10,400 --> 00:16:15,600
We are saying something in the middle, that is, we are trying to come up with some description

159
00:16:15,600 --> 00:16:21,840
of that equivalence class under simplifying assumptions and then let data fill in the rest.

160
00:16:21,840 --> 00:16:27,840
So that's the way we're trying to marry the two things. And is the result

161
00:16:29,200 --> 00:16:36,800
a mathematical analysis in closed form, or is it experimental results on data sets?

162
00:16:39,280 --> 00:16:43,840
I mean, most of the things we do is we end up having constraints which are closed for

163
00:16:43,840 --> 00:16:49,600
mathematical equations. Maybe one layer in the deep net is expected to be orthonormal because

164
00:16:49,600 --> 00:16:54,960
the physics of image formation says that certain variables under certain lighting additions

165
00:16:54,960 --> 00:16:59,120
will have an orthonormal structure. Okay, that's the way we impose the constraint that certain

166
00:16:59,120 --> 00:17:04,560
layers might have an orthonormal constraint put in. But then the network itself has to be learned

167
00:17:04,560 --> 00:17:11,360
end to end with data sets. And the performance has to be validated empirically on data sets.

168
00:17:11,360 --> 00:17:17,200
Okay. One of the interesting things we found is this concept of orthonormality which seems to

169
00:17:17,200 --> 00:17:25,040
have some very special power. What we're finding is whether, you know, think of it. So we played

170
00:17:25,040 --> 00:17:32,800
with this idea in a paper for BMVC last year where we took some classic, you know, disentangling

171
00:17:32,800 --> 00:17:41,280
autoencoder kind of networks. And we had good reason to impose an orthonormality constraint on

172
00:17:41,280 --> 00:17:48,080
the latent blocks of these disentangling autoencoders. By orthonormality, we had to write up a whole,

173
00:17:48,960 --> 00:17:56,400
you know, theory around we expect these factors to represent either movement or lighting conditions

174
00:17:56,400 --> 00:18:03,280
or deformations. And under appropriate relaxation, they all become orthogonal to each other and

175
00:18:03,280 --> 00:18:08,800
they all have this spherical structures. So looking at all of this, it looks like orthonormality

176
00:18:08,800 --> 00:18:14,960
is a trade off, which is coming close to what the physics is telling us to do. And but also being

177
00:18:14,960 --> 00:18:20,160
sensitive to the idea that it has to be implemented easily. We don't want to over complicate things.

178
00:18:20,160 --> 00:18:27,120
And we want our constraints to be differentiable. So it's a design process. So with through in these

179
00:18:27,120 --> 00:18:32,320
orthonormality constraints on disentangling autoencoders and boom, the numbers of disentangling

180
00:18:32,320 --> 00:18:39,200
quality just went up quite significantly. And it's. And so in the case of an autoencoder or in

181
00:18:39,200 --> 00:18:48,320
the layer of a deep network, what does it mean to impose that kind of constraint? Is it, you know,

182
00:18:48,320 --> 00:18:54,480
architectural and does it mean that you're diverging from kind of your CNN, resonant kind of

183
00:18:54,480 --> 00:18:59,680
tried and true architectures or is it, you know, loss function based or something totally different?

184
00:18:59,680 --> 00:19:05,440
How do you impose those constraints? There's a there's two or three ways in which it's happened.

185
00:19:05,440 --> 00:19:09,680
One is we stick with architectures as is don't mess with the architectures, but add-and-loss

186
00:19:09,680 --> 00:19:15,200
functions that works when the constraints are actually expressable as a closed form equation

187
00:19:16,000 --> 00:19:22,160
like spherical losses or, you know, orthonormality. Those can be written on as a closed form equation.

188
00:19:22,800 --> 00:19:27,840
Sometimes the constraints we arrive at do not have an equation, but they have what is called

189
00:19:27,840 --> 00:19:34,480
a manifold structure. And why manifolds arise is very closely related to invariance.

190
00:19:35,520 --> 00:19:41,120
Here is an example, you know, if I say, so the idea of invariance is this, right? I mean,

191
00:19:41,120 --> 00:19:45,280
you have a feature space. Let's say the feature space is something. You have this feature space

192
00:19:45,280 --> 00:19:52,960
in RN and reports specific than something, so it's easier to follow. So let's say it's the, you know,

193
00:19:52,960 --> 00:19:58,080
latent space of AlexNet, okay? Some features come from the latent space of AlexNet,

194
00:19:58,080 --> 00:20:05,520
which is embedded in RN, right? So it's a vector in RN. Yep. And then we come in and we say,

195
00:20:05,520 --> 00:20:10,160
look, I want to impose a slight equivalence here, which is if I rotate my picture,

196
00:20:10,160 --> 00:20:14,240
looks like the features are also changing somehow. I mean, the features are not always

197
00:20:14,240 --> 00:20:19,200
invariant to physical variables like this, right? But if you're able to say that this feature,

198
00:20:19,200 --> 00:20:24,800
this feature, this feature in RN actually represent the same picture, just that they're rotated.

199
00:20:24,800 --> 00:20:32,880
We are trying to basically paste the features and thereby the underlying space into something else

200
00:20:32,880 --> 00:20:39,280
to express that concept of equivalence. And sometimes when that ways of expressing these equivalences

201
00:20:39,280 --> 00:20:46,160
is well defined, what happens is the space gets crumpled. You hope that the neural net learns

202
00:20:46,160 --> 00:20:51,600
to crumple the space all of its own. That is one of the overarching hopes in deep learning

203
00:20:51,600 --> 00:20:56,720
that as you go through the layers of deep learning, the deep learning is learning to squish and crumple

204
00:20:56,720 --> 00:21:04,400
the original feature space into interesting ways to get a job done. But what it's doing is it's not

205
00:21:04,400 --> 00:21:08,720
always getting the job done the right way because you'd never have enough data. But if I explicitly

206
00:21:08,720 --> 00:21:13,760
tell it that here is how rotation affects the features and here is how you paste them together

207
00:21:13,760 --> 00:21:19,040
through whatever mathematics that's needed, then we get some manifold structures. You know,

208
00:21:19,040 --> 00:21:25,760
this crumpling can sometimes be expressed as a manifold. If you want to say in the concept of,

209
00:21:26,320 --> 00:21:30,640
you know, in the paradigm of loss functions, how do you express a manifold as a loss function?

210
00:21:30,640 --> 00:21:40,320
Sometimes you cannot. What you can do instead is, you know, manifolds are basically crumpled spaces

211
00:21:40,320 --> 00:21:47,360
and they have ideas associated with them which are analogous to how we think of maps and,

212
00:21:47,360 --> 00:21:53,520
you know, the earth itself is composed of the earth is a manifold, but then it's also a sphere

213
00:21:53,520 --> 00:21:58,560
approximately. So if you forget the idea that it's a sphere, but if it were a general weirdly shaped

214
00:21:58,560 --> 00:22:05,280
blob, you would represent it by a series of charts and you would explain how the charts connect

215
00:22:05,280 --> 00:22:11,360
and that's the way you specify a manifold through things called charts. And charts are also sometimes,

216
00:22:11,360 --> 00:22:17,040
you know, they have a thing which is similar called tangent spaces. So you sort of flatten the

217
00:22:17,040 --> 00:22:23,600
manifold in local coordinate charts and you can express that tangent space as a vector space.

218
00:22:24,640 --> 00:22:29,680
So once in a while, we have run into these conditions where we have a constraint which was

219
00:22:29,680 --> 00:22:34,720
expressed as a manifold which could not be written down as an equation, but whose tangent space

220
00:22:34,720 --> 00:22:41,360
could be written down. So we were able to enforce conditions of that tangent Z and said,

221
00:22:41,360 --> 00:22:47,840
I want this layer in my deep net to represent coordinates of a manifold on a specific tangent space

222
00:22:47,840 --> 00:22:51,840
and the mapping from that back to the manifold could be written down in closed form. So it depends.

223
00:22:54,160 --> 00:23:02,480
Let me see if I can upload this anywhere close to what you just said. What the way I'm kind of

224
00:23:02,480 --> 00:23:10,960
hearing this is that you've got some problem. Say you've got some object and you apply some simple

225
00:23:10,960 --> 00:23:17,280
transformation to that object, maybe you rotate it. If you've got a deep neural network that is

226
00:23:17,280 --> 00:23:26,160
trying to detect that object, for example, and you've trained it, there may be some feature space

227
00:23:26,160 --> 00:23:36,880
or some representation of that object in the different layers of the neural net. In a oversimplified

228
00:23:36,880 --> 00:23:44,000
world, you'd kind of want there to be a relationship between the rotation of the object itself and

229
00:23:44,000 --> 00:23:48,080
the rotation of the features. Maybe you could apply some simple transformation of the features,

230
00:23:48,080 --> 00:23:54,160
but the world isn't networks aren't that simple. But it turns out there is a relationship

231
00:23:54,160 --> 00:24:00,800
between the features. It's just more like this crumbly manifold thing and you found a way to

232
00:24:02,800 --> 00:24:07,840
express that using the mathematical language of these manifolds that allow you to

233
00:24:08,960 --> 00:24:14,560
detect the actual invariance of the object. That is very correct.

234
00:24:17,120 --> 00:24:18,560
Thank you for giving me those lines.

235
00:24:18,560 --> 00:24:28,640
The only disclaimer is we have been able to do this for a few common sources of physical

236
00:24:28,640 --> 00:24:36,000
variability and that includes things like rotations of objects and deformations of moving parts

237
00:24:36,000 --> 00:24:43,280
in certain cases, lighting conditions, just to be very clear. We haven't been able to do this

238
00:24:43,280 --> 00:24:49,440
across the board for every possible thing. So simple, maybe not three-point studio lighting,

239
00:24:49,440 --> 00:24:57,760
but a simple radio rotation or something like that, but clearly there's lots of things you can

240
00:24:57,760 --> 00:25:04,880
do in the physical world that aren't amenable to that representation.

241
00:25:04,880 --> 00:25:06,000
Absolutely. Yes.

242
00:25:06,000 --> 00:25:15,600
Okay. Okay. Cool. One of the things that comes to mind in thinking about this and in your work,

243
00:25:15,600 --> 00:25:26,000
you fall back on or maybe ground yourself in what you call pragmatic choices of deep architectures,

244
00:25:26,000 --> 00:25:33,360
meaning the popular stuff, the way we're doing things today. I think of Jeff Hinton's

245
00:25:33,360 --> 00:25:40,160
capsule networks is trying to come at some of the same ideas or same problems. Are you familiar

246
00:25:40,160 --> 00:25:47,440
with that work and compare contrast? I mean, we've tracked that body of work also. Again,

247
00:25:47,440 --> 00:25:57,600
he's with all due respect, ACM Turing, I can't do that easily, but it's an over-complication.

248
00:25:57,600 --> 00:26:07,840
I mean, it's ignoring so much. The basic laws of rotations are not that hard if you understand

249
00:26:07,840 --> 00:26:15,200
how to express rotations and we're taking out that invariance is doable without that level of

250
00:26:15,200 --> 00:26:20,000
combination. So I'm correct that you're trying to come at some of the same problems at least.

251
00:26:20,000 --> 00:26:27,920
Right. Okay. In it may be the case that if that enterprise succeeds, if that capsule network

252
00:26:27,920 --> 00:26:37,600
enterprise succeeds, it may be a more general solution to everything, maybe, but if you want to

253
00:26:37,600 --> 00:26:43,440
be a bit specific about understood factors of variation, I feel that's an over-complication.

254
00:26:43,440 --> 00:26:48,560
And there are nicer ways to do that. And I think we're able to do that in a better way.

255
00:26:48,560 --> 00:27:00,960
Yeah. Okay. Yeah. Cool. So you've developed this approach and you mentioned that you've

256
00:27:02,400 --> 00:27:08,160
got some experimental results as well. Can you talk a little bit about how you frame the question

257
00:27:08,160 --> 00:27:18,240
experimentally and what you've seen? Sure. I think the way we frame it is we want to keep

258
00:27:18,240 --> 00:27:24,320
a few things fixed. And the way we keep a few things fixed is we say pick an architecture first

259
00:27:24,320 --> 00:27:29,280
and that can be allocated. We're looking at things like dense net, point net, all those

260
00:27:29,280 --> 00:27:34,480
your architectures which are known to work well for certain databases. Keep the architecture

261
00:27:34,480 --> 00:27:41,840
more or less fixed. Keep the training set more or less fixed. The only thing that varies is,

262
00:27:41,840 --> 00:27:47,440
you know, don't play too much with new fancier data augmentation methods. The only thing we are

263
00:27:47,440 --> 00:27:53,840
doing is adding in constraints, either in some latent variables or we're adding in certain

264
00:27:53,840 --> 00:28:00,960
augmented loss functions. So most of the additional thing that we're doing is a mathematical expression

265
00:28:00,960 --> 00:28:07,840
of some kind. And keeping in, otherwise, it's hard to compare. I mean, if you say, let me train it

266
00:28:07,840 --> 00:28:15,120
for more iterations, but not add a constraint, can you compare it? So keeping mostly the computational

267
00:28:15,120 --> 00:28:20,960
resources fixed. We're asking if this additional mathematical knowledge pushes on the low.

268
00:28:20,960 --> 00:28:25,600
And we've been finding that it does. We have done that for image classification. We've done that for

269
00:28:26,400 --> 00:28:31,680
disentangling networks. We've done that for time series problems recently. And some of our

270
00:28:31,680 --> 00:28:37,600
compelling results are indeed from time series modeling where, you know, we've applied this to

271
00:28:37,600 --> 00:28:42,880
human activity like ignition data sets with either stick figures or wearable devices.

272
00:28:42,880 --> 00:28:49,600
And the kind of factor that we're trying to factor out in human movement is not light and shape

273
00:28:49,600 --> 00:28:55,840
and geometry, but it's time series variability issues, which is, you know, the same action when

274
00:28:55,840 --> 00:29:01,200
performed by the same person, but at a slightly different time will give rise to slightly different

275
00:29:01,200 --> 00:29:06,560
traces because people have intrinsic variability and how they move. And oftentimes that variability

276
00:29:06,560 --> 00:29:12,000
gets expressed through some time warping kinds of relationships. What we've done is express the

277
00:29:12,000 --> 00:29:18,560
time warping property as a constraint, which can be forced by the network to be factored out

278
00:29:18,560 --> 00:29:23,920
in a latent variable if we just throw in that constrictive the loss function. And we've found that

279
00:29:23,920 --> 00:29:29,440
it improves numbers significantly just like that without any additional training, without any

280
00:29:29,440 --> 00:29:37,920
additional data requirements. So the pictures or what I think are the pictures of this if I'm

281
00:29:37,920 --> 00:29:47,040
understanding the problem is along the lines of, you know, start from your seat in the living room,

282
00:29:47,040 --> 00:29:53,760
go to the refrigerator, grab a drink, you know, take off the cover and drop the cover in the trash can,

283
00:29:53,760 --> 00:30:01,920
and you've got this kind of two-dimensional plot of the path that the person might take in doing

284
00:30:01,920 --> 00:30:10,480
all that. And your argument is that the path is an invariant because the task is the same.

285
00:30:10,480 --> 00:30:21,600
It's, you know, do X and Y. And what you're trying to do is identify, well, what is the fundamental

286
00:30:21,600 --> 00:30:30,800
that they said, identifying the path? Is it somehow in a data set with lots of these, you know,

287
00:30:30,800 --> 00:30:37,920
traces or paths, identify which ones correspond to the same actions? It's close. I mean, we haven't

288
00:30:37,920 --> 00:30:43,760
looked at paths in that way, but we've looked at traces of stick figures, you know, so you have

289
00:30:43,760 --> 00:30:49,360
like 50 joints being tracked and you have the full-time series of 50 joints evolving in space

290
00:30:49,360 --> 00:30:55,280
and in three dimensions that comes from motion capture, say. Okay. Yes, the actions not really

291
00:30:55,280 --> 00:30:59,840
unlike what you're seeing, actions in a kitchen, actions in a room, actions in an office, picking

292
00:30:59,840 --> 00:31:06,080
up objects, placing them here and there. And the feature that is invariant, of course, is hard

293
00:31:06,080 --> 00:31:11,520
to linguistically describe, but one of the variables that gives rise to confusion is that

294
00:31:12,320 --> 00:31:17,120
people sometimes take longer to do the same thing. People sometimes are fast in certain phases of

295
00:31:17,120 --> 00:31:22,320
the movement, slow in certain phases of the movement, right? Or there is asymmetry in the body,

296
00:31:22,320 --> 00:31:27,600
you know, the left, the left arm swings more than the right arm. You know, there's all these

297
00:31:27,600 --> 00:31:32,880
interesting sources of variability which are hard to and the only way deep learning will be robust

298
00:31:32,880 --> 00:31:37,440
to that is if you augment it with all these variables, all these sources of variation.

299
00:31:38,320 --> 00:31:43,680
The way we think of it is that the variability here is expressible as a warping of the time

300
00:31:43,680 --> 00:31:50,000
axis, whether it's short versus long or speeding up versus slowing down or if it's one side faster

301
00:31:50,000 --> 00:31:56,160
than the other side or the swings are smaller than the other. It's all a time warp. Sometimes it

302
00:31:56,160 --> 00:32:02,560
can be constant. Sometimes it can be non-constant. So that brings up an interesting question. Do you

303
00:32:03,440 --> 00:32:10,400
assume in your work throughout a single source of invariance or do you also

304
00:32:11,920 --> 00:32:17,120
conceive of multiple sources of invariance? Like, you know, there's a time invariance, but there's

305
00:32:17,120 --> 00:32:23,840
also the left arm swing invariance factor. I mean, that is the, I mean, we are headed in that

306
00:32:23,840 --> 00:32:27,520
direction. I mean, right now our investigations have been affected. That would be the answer.

307
00:32:27,520 --> 00:32:33,680
The goal is to be able to have almost like a linear combination of known invariances that,

308
00:32:33,680 --> 00:32:39,680
you know, you can account for. Right. I mean, at this time, we have been playing it very carefully

309
00:32:39,680 --> 00:32:43,360
that let's take this one source of variable. Let's see if that can be factored out. Let's see if

310
00:32:43,360 --> 00:32:48,640
we can get invaders to that. And we have had success in many different applications.

311
00:32:48,640 --> 00:32:57,840
It sounds like you're further saying, though, that in the case of at least this motion capture

312
00:32:57,840 --> 00:33:11,360
type of a data set that maybe time becomes kind of a meta-invariance that can account for multiple

313
00:33:11,360 --> 00:33:19,440
physical characteristics. Am I hearing that correctly in there? It can. It's hard to write that out

314
00:33:19,440 --> 00:33:27,040
clearly, but it does. Like, for instance, if you had like load bearing, you know, if you were

315
00:33:27,040 --> 00:33:34,480
carrying a heavy bag on your bag, it will have an interesting effect on the time series of your

316
00:33:34,480 --> 00:33:41,040
joins, which is not that easy to explain, but it will sort of stretch out certain phases of a movement

317
00:33:41,040 --> 00:33:47,200
and shrink certain phases of your movement. It does. So, yeah, the stretchings and shrinkings of the

318
00:33:47,200 --> 00:33:54,160
time axis are the key to finding what that invariant is for the lack. Yeah.

319
00:33:56,480 --> 00:34:03,120
And so are there well-established benchmark data sets for these types of tasks? Or are you

320
00:34:03,120 --> 00:34:10,080
rolling your own to explore these methods? No, for motion capture, there are benchmark data sets.

321
00:34:10,080 --> 00:34:15,840
There are, you know, Microsoft has a, it used to have a RGB data set. I mean, the go-by-the-RGBD

322
00:34:16,720 --> 00:34:20,800
activity sort of, you know, keywords. And there's a few out there. There's a few benchmark

323
00:34:20,800 --> 00:34:27,920
data sets out there. NTU has one. MSR is one. And sometimes even, you know, the video data sets

324
00:34:27,920 --> 00:34:34,720
like HMDB have stick figures available through other methods like PostNet, for instance. So, yeah,

325
00:34:34,720 --> 00:34:40,640
there are well-established data sets that we experiment with. And is the task that's posed by

326
00:34:40,640 --> 00:34:50,720
these data sets one of predicting the action that the, it's activity classification and prediction

327
00:34:50,720 --> 00:34:59,440
by and large. Yeah. Okay. Okay. And so what's the kind of state of the art for that kind of

328
00:34:59,440 --> 00:35:06,400
activity detection and how does your method compare to it? So most of the time series in the deep

329
00:35:06,400 --> 00:35:13,120
learning world, most time series things are either a combination of 1D, CNNs or, you know, LSTM models.

330
00:35:13,120 --> 00:35:18,640
So depending upon the data set, the way our process goes is we say, let's find the latest, you know,

331
00:35:18,640 --> 00:35:24,640
benchmarks and we'll improve on those through these mathematical techniques. So, a recent paper

332
00:35:24,640 --> 00:35:32,320
we did in CVPR 2019 used LSTMs as the benchmark data, you know, the technique. And the data sets were

333
00:35:32,320 --> 00:35:40,400
NTU 3D data set and a few others like that motion capture. The tunable parameter in LSTMs is oftentimes

334
00:35:40,400 --> 00:35:46,320
the hidden layers, how many hidden units do you have? And of course, if you scroll through it,

335
00:35:46,320 --> 00:35:51,360
the numbers keep going better and better. The way we've done it is we kept things the same. We say,

336
00:35:51,360 --> 00:35:57,840
let's say 16 hidden units or 32 hidden units. Keep that the same. The only thing will change is add

337
00:35:57,840 --> 00:36:03,520
in this additional module that either disentangles the time or function or adds in as a constraint

338
00:36:03,520 --> 00:36:09,360
and numbers always go out. So in the way we thought about it, if my number, if memory is right,

339
00:36:09,360 --> 00:36:19,840
the NTU RGB data set had like, you know, 80% roughly accuracy with a very fancy LSTM with 200 hidden

340
00:36:19,840 --> 00:36:24,960
units and stuff like that. And we were able to improve it by four five percentage points easy without

341
00:36:24,960 --> 00:36:31,360
any changes to anything, but just this additional constraint added in. So if you find unit more,

342
00:36:31,360 --> 00:36:36,400
sure, there's more things to be squeezed out, but we were able to consistently improve the

343
00:36:36,400 --> 00:36:41,520
performance of LSTM by easy five percentage points and times six eight percentage points with no

344
00:36:41,520 --> 00:36:48,240
change, but a simple constraint on time warping. Yeah. So those are the kinds of results that we've

345
00:36:48,240 --> 00:36:54,160
been finding, which is if you rethink what the constraints should be through understanding the

346
00:36:54,160 --> 00:37:00,080
phenomena first, the payoffs are actually quite significant without any additional requirements on

347
00:37:00,080 --> 00:37:06,080
data or network architecture complexity or training strategies. They can all be very basic.

348
00:37:06,080 --> 00:37:12,080
Mm-hmm. And so now we've talked about a couple of, you know, very different types of problems. One,

349
00:37:12,080 --> 00:37:22,640
kind of a, you know, computer, a very visual type of task in one of this more time series to

350
00:37:22,640 --> 00:37:28,960
apply this to different settings. How much hand crafting needs to go into the loss functions and

351
00:37:28,960 --> 00:37:36,000
the, you know, the different constraints that you're applying to the network? That is where the big

352
00:37:36,000 --> 00:37:43,760
work is. So I think the pendulum is swinging to that level of hand crafting, you know,

353
00:37:43,760 --> 00:37:47,840
moving away from features to architectures and loss functions, right? That's where the pendulum is.

354
00:37:47,840 --> 00:38:00,240
Yep. And the amount of work that goes into hand crafting is a lot of, I would say, studying basically

355
00:38:01,040 --> 00:38:09,120
understanding how these variables actually affect the observed data and try to express it in a way

356
00:38:09,120 --> 00:38:15,440
that is amenable to fusion with the deep net. The beauty is physics is not one way, you know, light

357
00:38:15,440 --> 00:38:19,520
is, there is no single model for expressing how light and surfaces interact. There's layers and

358
00:38:19,520 --> 00:38:25,680
layers and layers to it. Yeah. And you have to know all of that or at least as much as, you know,

359
00:38:25,680 --> 00:38:32,080
as much as you can learn. And then the hand crafting is where in this spectrum of sophistication,

360
00:38:32,080 --> 00:38:41,200
do I stop in a way that I actually have a pragmatic effect on performance without changing anything

361
00:38:41,200 --> 00:38:45,840
else? And that's where a lot of intuition is, you know, you cannot get away from this intuitive

362
00:38:45,840 --> 00:38:52,720
exercise. Despite all the progress of machine learning and deep learning, the networks are arguably

363
00:38:52,720 --> 00:38:57,120
both intuitive and highly unintuitive. I mean, some people have an insight about why a network

364
00:38:57,120 --> 00:39:03,280
works, but presented to someone else, it's mysterious. And the same thing is true of the loss

365
00:39:03,280 --> 00:39:08,960
functions business. Sometimes we can motivate it very easily through simple things like, well,

366
00:39:08,960 --> 00:39:17,280
yeah, cross entropy means where to make sense. Physics is where some of the unintuitive stuff lies.

367
00:39:18,560 --> 00:39:23,920
It's, that's where a lot of design thinking exists and we are doing that. So yes, that's

368
00:39:23,920 --> 00:39:31,440
where much of the work is understanding that when you approach the n plus 1th problem that's

369
00:39:31,440 --> 00:39:38,000
different from the ones that you've looked at previously that you're starting from scratch,

370
00:39:38,000 --> 00:39:47,120
or are there some principles that give you a foothold when trying to apply this method to the

371
00:39:47,120 --> 00:39:52,320
new area? And if so, what are those principles? The principles, I mean, the details, of course,

372
00:39:52,320 --> 00:39:56,240
have to be looked at from scratch, but the principles that we bring to the table are

373
00:39:58,160 --> 00:40:03,280
ideas of geometry and, you know, this idea that look, whatever it is that you're observing,

374
00:40:03,280 --> 00:40:09,760
whatever is the raw space, that is not the space on which you want your analysis to occur. You want

375
00:40:09,760 --> 00:40:17,040
the analysis to occur in a space that is crumpled. And the generalizable knowledge that we bring to

376
00:40:17,040 --> 00:40:22,560
the table is how do we represent these crumpled spaces? And that's the mathematics of humanian

377
00:40:22,560 --> 00:40:28,000
geometry, entropology, group theory. Those are all the new mathematics. It's not new mathematics at

378
00:40:28,000 --> 00:40:34,160
all. It's mathematics of the past two centuries, but in the realm of machine learning, that mathematics

379
00:40:34,160 --> 00:40:41,760
has not made its way in a systematic way. So that's the generalizable knowledge. We bring in group

380
00:40:41,760 --> 00:40:46,560
theory, geometry, differential geometry, topology. That's the way we think about it. But then the

381
00:40:46,560 --> 00:40:53,360
specifics, the problem specifics have to be studied from scratch, but then that knowledge can often

382
00:40:53,360 --> 00:40:58,800
be expressed in the constraints of geometry, entropology, and group theory. And that's where we

383
00:40:58,800 --> 00:41:03,440
specialize. How do we take this domain specific knowledge and look at it through the lens of

384
00:41:03,440 --> 00:41:10,400
groups and invariance? That's a different kind of generalizable knowledge. It's really a way of

385
00:41:10,400 --> 00:41:19,440
thinking about phenomena rather than thinking about data. And going back to your keynote,

386
00:41:19,440 --> 00:41:27,920
are there, do you kind of take a step back and kind of apply this broadly to computer vision,

387
00:41:27,920 --> 00:41:36,000
machine learning? Do you kind of offer any thoughts for where this is all going?

388
00:41:38,960 --> 00:41:40,160
Not. Let's make some up.

389
00:41:40,160 --> 00:41:51,840
So data constraints scenarios, that's where this is all going. Machine learning with unconstrained

390
00:41:51,840 --> 00:41:59,760
amounts of training data is what the last 10 years were about. And we are finding that it's a nice

391
00:41:59,760 --> 00:42:06,720
goal, but there are no guarantees to be ever had, even if you train it forever with as much amount

392
00:42:06,720 --> 00:42:12,960
of data that you've got. If any mission critical deployment requires a guaranteed robustness of

393
00:42:12,960 --> 00:42:19,680
some kind, there is nothing to be given other than, yeah, this is what my numbers are on some data

394
00:42:19,680 --> 00:42:24,960
set. That's all you have. And now if I can just hit pause there, you throughout our conversation,

395
00:42:24,960 --> 00:42:29,520
you've talked about constraints, you've talked about constraints on the network, and

396
00:42:29,520 --> 00:42:37,760
then you've talked about constraints on loss functions, you've talked about constraints on

397
00:42:38,320 --> 00:42:43,680
architectures and not changing architectures. And those have implications on compute constraints.

398
00:42:43,680 --> 00:42:49,280
And you haven't really explicitly talked about constraints on data. How does that fit into

399
00:42:49,280 --> 00:42:56,720
all this other stuff we've talked about? I mean, the way I think about it is if you don't have

400
00:42:56,720 --> 00:43:02,560
access to additional data, you get more bang for your buck by adding these additional

401
00:43:02,560 --> 00:43:07,920
constraints that we were talking about. If you have access to more data and you can collect as much

402
00:43:07,920 --> 00:43:16,080
as you want, you always should. I mean, that's undeniable, but it's becoming more and more clear

403
00:43:16,080 --> 00:43:23,200
that that's not where the future is headed. We are not able to keep training bigger and bigger,

404
00:43:23,200 --> 00:43:27,760
you know, it's an unsustainable path. I mean, there is enough energy going in that direction

405
00:43:27,760 --> 00:43:32,720
anyway, whether or not we like it or I like it, but it's not a sustainable path of progress.

406
00:43:32,720 --> 00:43:38,000
It's smaller and smaller, you know, diminishing returns. I'm with increasing resources.

407
00:43:38,800 --> 00:43:45,920
So that's clear on the margins, but is part of your work trying to get at, you know,

408
00:43:45,920 --> 00:43:54,800
one shot, few shot types of problems or no? We are, I mean, that would be an extreme case.

409
00:43:55,840 --> 00:44:01,520
Yes. I mean, we are thinking more along the lines of if I had to collect more data,

410
00:44:02,800 --> 00:44:11,600
can I first pause before collecting any more data and robustify what I've got with domain

411
00:44:11,600 --> 00:44:17,440
knowledge? That's the way I think about it. One shot and few shot, it's a whole different ball

412
00:44:17,440 --> 00:44:24,000
game. I mean, it's like the wide west of, you know, machine learning and I wouldn't go that far yet,

413
00:44:24,000 --> 00:44:30,800
but will it be applicable? I mean, sure, I don't see why not, but I wouldn't make big claims

414
00:44:30,800 --> 00:44:37,920
of getting one shot performance, but it should definitely help. If not, you know, make it more

415
00:44:37,920 --> 00:44:43,680
amenable to, you know, less training sets. Yeah. Okay. So I interrupted you were talking about

416
00:44:44,320 --> 00:44:50,560
essentially that the data, you know, data collection is always going to be expensive and,

417
00:44:51,920 --> 00:44:58,640
you know, thinking about the problem space, you know, can provide, provide these benefits.

418
00:44:58,640 --> 00:45:05,360
Yeah. Yeah. I mean, unfortunately, human in the loop can't go away. I mean, there is

419
00:45:05,360 --> 00:45:10,480
neural architecture search. Yes. I mean, again, will this succeed? They will succeed at developing

420
00:45:10,480 --> 00:45:15,360
some representations that get a job done. And when you layer in questions of interpretation,

421
00:45:15,360 --> 00:45:20,000
explanation, which everybody's talking about, I have a much simpler take on it, which is,

422
00:45:20,800 --> 00:45:25,920
if you don't have robustness to even simple physical variables, how will you even explain it in

423
00:45:25,920 --> 00:45:31,120
your hand? I mean, if your classification shifts simply because I rotate a picture and you're

424
00:45:31,120 --> 00:45:37,840
asking me to explain it, I think you're asking the wrong question. If you're at least asking me,

425
00:45:37,840 --> 00:45:44,000
can you be first be robust slash invariant to simple things? And then explain it to me,

426
00:45:44,000 --> 00:45:49,840
that's a more well-posed question, but these are premature questions to ask. And some

427
00:45:49,840 --> 00:45:55,360
colleagues of mine have gone so far to say, repeatability, if your machine learning technique is not

428
00:45:55,360 --> 00:46:00,160
repeatable. And by that, things like this, yeah, if I click this picture at a slightly different

429
00:46:00,160 --> 00:46:03,920
time of day, and I think it's really changed, it takes up the time of day, and the decision has

430
00:46:03,920 --> 00:46:10,160
flipped. It's not, the process is not even repeatable. So don't even go to the extent of

431
00:46:10,800 --> 00:46:16,160
explaining a non-repeatable process or trying to interpret a non-repeatable process. Those are all

432
00:46:17,040 --> 00:46:23,280
questions that should come later. So if you think of repeatability, you do an experiment,

433
00:46:23,280 --> 00:46:31,360
you get the same result over and over again. As far as the big things are controlled, machine learning

434
00:46:31,360 --> 00:46:40,080
hasn't yet delivered that even. So we're trying to bring in that level of robustness, I call it

435
00:46:40,080 --> 00:46:44,800
robustness slash invariance. Some people have called it repeatability, simple and repeatability

436
00:46:44,800 --> 00:46:50,560
sounds shinier, and it sounds like the stakes are much higher, but I'm happy to just call it

437
00:46:50,560 --> 00:46:58,000
invariance. Nice, nice. Well, Paven, thanks so much for taking the time to share what you're up to,

438
00:46:58,000 --> 00:47:05,840
and provide us some context for your CVPR keynote. Very cool stuff. Thank you so much Sam. This has

439
00:47:05,840 --> 00:47:21,280
been a pleasure and you've been great. Thank you so much.

