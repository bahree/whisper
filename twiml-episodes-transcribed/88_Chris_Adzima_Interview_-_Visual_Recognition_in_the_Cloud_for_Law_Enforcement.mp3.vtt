WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:21.280
people doing interesting things in machine learning and artificial intelligence. I'm your

00:21.280 --> 00:26.440
host Sam Charrington. This week on the podcast, we're featuring a series of conversations

00:26.440 --> 00:32.600
from the AWS Reinvent Conference in Las Vegas. I had a great time at this event, getting caught

00:32.600 --> 00:38.520
up on the new machine learning and AI products and services announced by AWS and its partners.

00:38.520 --> 00:42.760
If you missed the news coming out of Reinvent and want to know more about what one of the

00:42.760 --> 00:48.320
biggest AI platform providers is up to, make sure you check out Monday's show, Twimble Talk

00:48.320 --> 00:54.360
number 83. A roundtable discussion I held with Dave McCrory and Lawrence Chung. We cover

00:54.360 --> 01:01.080
all of AWS's most important news, including the new SageMaker, DeepLens, Recognition Video,

01:01.080 --> 01:07.720
Transcription, Alexa for Business, Greengrass ML inference and more. This week, we're also

01:07.720 --> 01:13.560
running a special listener appreciation contest to celebrate hitting 1 million listens here on

01:13.560 --> 01:21.400
the podcast and to thank you all for being so awesome. Tweet to us using the hashtag Twimble 1

01:21.400 --> 01:27.960
Mill to enter. Every entry gets a fly Twimble 1 Mill sticker, plus a chance to win a limited

01:27.960 --> 01:33.640
run t-shirt commemorating the occasion. We'll be digging into the Magic Twimble swag bag and

01:33.640 --> 01:38.440
giving away some other mystery prizes as well, so you definitely don't want to miss this.

01:39.320 --> 01:46.920
If you're not on Twitter or you want more ways to enter, visit Twimbleai.com slash Twimble 1 Mill

01:46.920 --> 01:53.000
for the full rundown. Before we dive in, I'd like to thank our good friends over at Intel

01:53.000 --> 01:58.760
Nirvana for their sponsorship of this podcast and our reinvent series. One of the big announcements

01:58.760 --> 02:04.520
at reinvent this year was the release of Amazon DeepLens, a fully programmable, deep learning

02:04.520 --> 02:10.520
enabled wireless video camera designed to help developers learn and experiment with AI

02:10.520 --> 02:17.080
both in the cloud and at the edge. DeepLens is powered by an Intel Atom X5 processor, which

02:17.080 --> 02:23.160
delivers up to 100 gigaflops of processing power to onboard applications. To learn more about

02:23.160 --> 02:29.960
DeepLens and the other interesting things Intel's been up to in the AI space, check out intel Nirvana.com.

02:31.000 --> 02:37.960
Okay, in this episode, we're joined by Chris Adzima, senior information analyst for the Washington

02:37.960 --> 02:44.520
County Sheriff's Department. Chris joined me to discuss his very interesting use case for AWS

02:44.520 --> 02:51.080
recognition, their image object detection system, which he uses to help his agency identify criminal

02:51.080 --> 02:57.720
suspects in the Portland area by matching photos to mug shots. We discuss how bias affects the

02:57.720 --> 03:02.920
work he's doing and how they try to remove it from their process, as well as what his next steps

03:02.920 --> 03:07.960
are with the recognition service. This was a pretty interesting discussion and I'm sure you'll enjoy

03:07.960 --> 03:10.520
it. And now on to the show.

03:18.360 --> 03:23.800
All right, everyone. I am here at the AWS Reunment Conference and I've got the pleasure of being

03:23.800 --> 03:29.000
seated with Chris Adzima. Chris is a senior information systems analyst with the Washington

03:29.000 --> 03:33.480
County Sheriff's Office and Washington County, if you don't know is in the Portland area. Is that

03:33.480 --> 03:38.600
right, Chris? Chris, welcome to this week in machine learning and AI. Well, thanks for having me.

03:38.600 --> 03:43.160
It's great to have you here. I am looking forward to learning a little bit about the talk that you

03:43.160 --> 03:48.920
gave yesterday. What was the topic of that one? So it was called the unusual suspects and essentially,

03:49.640 --> 03:58.600
it is about how I used AWS recognition to attempt to identify unknown suspects who committed

03:58.600 --> 04:06.760
crimes. And what I used is previous booking photos or mug shots to upload the recognition

04:06.760 --> 04:13.880
created collection. And now we can search surveillance footage or pictures taken by eyewitnesses

04:14.600 --> 04:19.480
against those booking photos and attempt to identify those people based on if they had stayed

04:19.480 --> 04:24.280
in our jail already. Oh, wow. Oh, wow. Before we get into that, why don't we spend a little bit of

04:24.280 --> 04:29.560
time having you share with us a little bit about your background and how you got interested in

04:29.560 --> 04:34.920
the machine learning and AI stuff to begin with? Sure. I have a pretty extensive background,

04:34.920 --> 04:41.880
goes back about 15 years now where I started at eBay working in fraud detection. And there we

04:42.520 --> 04:47.160
utilized machine learning. I didn't ever make any models because I'm not a data scientist,

04:47.160 --> 04:52.280
but I utilized the models to detect things like people taking over accounts or posting fraudulent

04:52.280 --> 05:01.400
items. Okay. So back then it was very interesting to me. And I kind of grew up with that as part of

05:01.400 --> 05:08.760
my background. When I moved into public service, I started working at the sheriff's office. I wanted

05:08.760 --> 05:15.720
to bring some of that interest into the sheriff's office. And so I started looking at ways we could

05:15.720 --> 05:22.840
utilize machine learning or anything like that in order to help our deputies do their job.

05:23.320 --> 05:28.120
And so one of the biggest areas of opportunity was the fact that we have all of these

05:28.920 --> 05:33.960
videos, all of these pictures of people who committed crimes that we can't identify. And we also

05:33.960 --> 05:38.920
have all of these pictures of people who have been booked into our jail. And I thought there's

05:38.920 --> 05:46.280
got to be a way that we can marry those two situations and come up with a very interesting way to

05:46.280 --> 05:51.560
solve that problem. And so that's kind of where how I grew up and got interested into it.

05:51.560 --> 05:58.760
Okay. Great. Is this the first machine learning project that you've done at the sheriff's office?

05:58.760 --> 06:06.120
Yes. This is the first one. We started looking into other machine learning things and we're

06:06.120 --> 06:12.760
slowly getting into other things like crime detection. And by crime detection, I don't mean

06:13.960 --> 06:18.840
somebody, one specific person committing a crime. I mean, minority report, not like minority report.

06:20.120 --> 06:26.920
More specifically, we noticed that as crime happens in one area, it migrates to another area

06:26.920 --> 06:32.840
over time. And we can utilize that data to predict that there's a possibility that a crime is going

06:32.840 --> 06:39.080
to occur in an area subsequent or bordering. And therefore, we can send out patrols to that area.

06:39.080 --> 06:43.080
Okay. Not to attempt to catch the person, but to attempt to prevent the crime from happening

06:43.080 --> 06:48.040
in the first place. Because that's honestly what our goal is, right? We don't want to catch the

06:48.040 --> 06:51.400
people doing the crime. We want the people to not do the crime in the first place. So that's what

06:51.400 --> 06:56.200
we're hopes are for that. So an application like that is essentially like, you know, how you got this

06:57.160 --> 07:02.200
you know, fleet of cars and officers and other vehicles. Like how do you deploy them?

07:02.200 --> 07:06.840
Right. Right. How do you how do you put them in the in the areas that are going to get the most

07:08.280 --> 07:11.160
prevention for where they are? Okay.

07:12.680 --> 07:16.840
How long have you been at the sheriff's office? Just about two years now. In fact,

07:16.840 --> 07:22.600
is it the 29th? It's the 29th. Yes, it'll be two years tomorrow. Oh wow. Wow.

07:22.600 --> 07:32.440
That'd be anniversary. Yes. Did the I guess I'm curious. Did the you know, so you you used

07:32.440 --> 07:41.080
Amazon's recognition product to do this first application to what degree did you know, having access

07:41.080 --> 07:47.480
to this via an API as opposed to needing to build your own models contribute to your ability to

07:47.480 --> 07:54.520
actually do it? 100%. I guess I kind of do the answer to that question, but I'm not a data scientist.

07:54.520 --> 08:00.920
I don't know how any of the stuff works in the back end. And I'm okay with that. What I what it

08:00.920 --> 08:07.160
really did was allow me somebody who's very big into coding. Some I know I know how to code.

08:07.880 --> 08:11.880
Now I'm able to utilize these these machine learnings, these deep learning techniques

08:11.880 --> 08:19.000
to help me do my job. And I didn't have to engage a data scientist. I didn't have to even

08:19.000 --> 08:25.000
know the model. I just was able to tie into an API and utilize their model that they already

08:25.000 --> 08:30.760
built. And it has been immensely beneficial because of that. Can you talk a little bit about your

08:30.760 --> 08:37.800
process for developing the system? Yeah. So in my talk yesterday, I talked about how quickly

08:37.800 --> 08:42.680
this happened. This was right around this time last year at Reinventing Announced Recognition.

08:43.480 --> 08:52.840
And by mid-November, sorry mid-December, I had had a prototype up and running already,

08:52.840 --> 08:57.640
basically. That's incredible. Yeah, it was really, really amazing. Once I found out

08:57.640 --> 09:03.560
that recognition existed and I did a quick, I think it only took me about a day to go in and

09:03.560 --> 09:08.920
read the documentation because it's not an overly complicated API. I think there's only

09:08.920 --> 09:14.200
something like 20 calls in total. So once I read the documentation, I was able to go in and

09:14.200 --> 09:19.720
realize that, okay, the first thing I need to do is get my mug shots available to recognition,

09:19.720 --> 09:27.960
uploaded 300,000 mug shots into S3. I did it manually. I didn't realize there was an API I could

09:27.960 --> 09:34.120
use. This was when I was really green with AWS. So literally I used the web for my drag and draw

09:34.120 --> 09:42.280
300,000 increments of a thousand out into the web form. Okay. So that took me about five days.

09:43.400 --> 09:48.840
Had I realized now that then that what I know now about the API for S3, I would have just written

09:48.840 --> 09:53.880
the script, uploaded them that way. That's how we keep our mug shots up to date now. I have a daily

09:53.880 --> 10:00.120
script that runs, throws them into S3 and indexes. But at least in mug shots were digital.

10:00.120 --> 10:05.560
In the beginning, in the beginning, it was a manual process. Once I got those up into S3, though,

10:06.280 --> 10:12.040
I did write a script because then I knew that I could wrote a script to loop through all 300,000

10:12.040 --> 10:20.600
indexed them into recognition, which is just one simple API call. Once they were indexed,

10:20.600 --> 10:25.480
I essentially had everything I needed to do to get up and running. Once you have that collection,

10:25.480 --> 10:34.680
you can just do an API call to their search faces API. And you send it the binary data from the

10:34.680 --> 10:39.720
image that you want to search from into the collection and it returns the results.

10:40.440 --> 10:43.800
And what are the images that you want to search from? What are those input images?

10:43.800 --> 10:54.840
So they come from multiple different ways. The web form that I created will post them to S3 and

10:54.840 --> 11:00.680
then S3 will do the search that way. I'll then delete that one because we don't save any of the

11:00.680 --> 11:09.480
images we search for. But then I also have a mobile app. Those will go directly into the API

11:09.480 --> 11:15.000
that way. The source images, though, are these like, is this surveillance video or is this like

11:15.000 --> 11:20.200
an officer on a street with your mobile app taking a picture of someone or in my talk yesterday,

11:20.200 --> 11:24.920
I should three different examples. One of them was from a surveillance. I don't know if people

11:24.920 --> 11:30.040
know this or not, but there are surveillance cameras on those little self-check out things at most

11:30.040 --> 11:36.200
of those department stores. And that was the surveillance camera from that. Yeah. So that was a good

11:36.200 --> 11:41.720
hit. Meaning like the credit card swiper thing? Well, not the self-check out kiosks.

11:41.720 --> 11:46.760
You know, we're going to scan it yourself. So in that case, the guy was scanning the items,

11:46.760 --> 11:50.520
but he didn't actually pay. He just put them back in the cart and walked out, made it seem like

11:50.520 --> 11:56.200
it was legitimate. Okay. So we got a surveillance shot off of that. The second example I gave

11:56.200 --> 12:02.280
was actually a cell phone picture. And I witnessed took a picture of somebody with their cell phone

12:02.280 --> 12:08.120
and then called the deputies. The deputies showed up. And the deputy took a picture of the cell phone

12:08.120 --> 12:13.560
picture. So it was like a second generation with all the glare on the phone and all.

12:13.560 --> 12:19.400
But recognition still found the face and ran that. And then the third example, which is the example

12:19.400 --> 12:27.000
that I am most pleased with, was an artist's rendition from an eyewitness. So a sketch.

12:27.000 --> 12:33.000
Wow. We ran a sketch through recognition and it pulled back a legitimate result. Wow.

12:33.000 --> 12:39.000
So when you say, well, we're running, we're running anything we can. If it has a face on it,

12:39.000 --> 12:46.440
it doesn't matter if it's a drawing or an image of an image of an image, we're trying

12:46.440 --> 12:56.120
because we want to identify these people. And how do you characterize the performance of

12:56.120 --> 13:01.400
recognition for your use case? Do you have specific ways you think about that?

13:02.200 --> 13:09.880
So as far as metrics go not yet because it's still really early. It's only been a year since I

13:09.880 --> 13:14.520
even started. And it's only been about six months that the deputies have been using it in full force.

13:15.720 --> 13:21.400
But I can tell you anecdotally, every single deputy, every single law enforcement officer who's

13:21.400 --> 13:30.120
used this has said, wow, I can't believe that how good this is doing. And it's also difficult

13:30.120 --> 13:35.880
to quantify it because I'd love to get up here and say, you know, this tool has led to

13:35.880 --> 13:41.880
X amount of convictions, right? But it doesn't quite work like that, right? The deputies use it as

13:41.880 --> 13:47.960
a tool. It's not their one and only thing they do. So while a deputy might put something into

13:47.960 --> 13:55.480
the tool, get a result. It may or may not be the person in the picture, but it could lead them

13:55.480 --> 14:01.560
to a relative. Like with facial features being similar, you might get somebody's father return

14:01.560 --> 14:07.320
in the result. You go talk to the father, realize it was the son who was the person you were looking

14:07.320 --> 14:14.760
for. So is that a good hit for us? I don't know how to quantify that yet. But I can tell you that all

14:14.760 --> 14:21.960
of these situations have occurred. So it's working well. Everybody likes it. And I think that's really

14:21.960 --> 14:26.920
all that matters when I'm putting a tool in the hands of a deputy, saving them time so that they can

14:26.920 --> 14:33.480
go out there and keep ourselves safe. When you're thinking about it from the perspective of a developer

14:33.480 --> 14:39.720
or a technologist using this service, how about the performance of recognition itself? Have you?

14:39.720 --> 14:45.000
And you know, I guess you would have to like, you put in an image, I guess you would want to know

14:45.000 --> 14:51.080
like what percent of the time, you know, is the image found when it's in the database. Do you

14:51.080 --> 14:56.600
track that? We did some benchmarking at the very beginning about that because we had images that

14:56.600 --> 15:02.600
were not mug shots, but we knew those people in those images had been booked into our jail at some

15:02.600 --> 15:11.160
point. So we created, I don't want to call it a blind case study, but I didn't know which of the

15:11.160 --> 15:14.440
group had been in our jail and which of the group had not been in our jail. And I didn't know the

15:14.440 --> 15:21.480
identities of the people. Okay. So one of the deputies sent me over about a hundred images.

15:22.280 --> 15:28.200
And of the images of the people who had been in our jail, I was able to correctly identify 75

15:28.200 --> 15:35.560
percent of them. And of the people who had not been in our jail, but results were returned,

15:35.560 --> 15:42.280
I was able to say that if 50 percent of those people, I was able to say, no, we don't have a good

15:42.280 --> 15:46.680
idea for these people. So about 50 percent of the people who had never been in our jail, I said,

15:46.680 --> 15:52.280
oh, I think it's this person and it actually was. But I think when it comes down to a 75 percent

15:52.280 --> 15:56.920
accuracy saying that there's a 75 percent chance that if they had been booked in our jail,

15:56.920 --> 16:02.040
I'm going to be able to tell you who they are. That's tremendous. So yeah, I think it's,

16:03.080 --> 16:08.920
you know, it's like what you said previously, it's a tool, right? And if you think about it in

16:08.920 --> 16:15.480
that context and not like recognition is going to replace policing, then it's helpful.

16:15.480 --> 16:20.920
Yeah. And exactly. And giving the deputies as many tools as possible to go out and do their

16:20.920 --> 16:30.760
jobs is exactly what my job is. And so I don't necessarily want to do their job for them,

16:30.760 --> 16:36.920
although I'd love to do their job. But I don't necessarily want to do their job for them. I want

16:36.920 --> 16:43.240
them to have a tool that they can not have to be sitting in behind a computer doing research,

16:43.240 --> 16:52.040
looking at thousands of mug shots that possibly fit. Instead, they get a likely choice of five or

16:52.040 --> 16:57.800
six and they can go out and take action on those follow up on leads as opposed to being tied to

16:57.800 --> 17:06.280
a computer. Are there things that you need to think about as a developer when using these APIs

17:06.280 --> 17:10.840
that are different from, you know, the traditional ways you might develop applications?

17:10.840 --> 17:19.160
As far as being in law enforcement, yes. I have to concern myself with different levels

17:19.160 --> 17:28.760
of data classifications. So for instance, I, there's laws that prevent us from sending images

17:28.760 --> 17:35.800
of juveniles over the internet without certain security in place. And while those securities do

17:35.800 --> 17:41.480
exist with Amazon, I have to ensure that they are in place before I send a juvenile's image. So

17:42.120 --> 17:46.520
in that case, we just have a policy that we don't run recognition off of juvenile images.

17:47.080 --> 17:53.640
And as far as just a regular old developer that I have been for a while now thinking about these

17:53.640 --> 18:00.920
APIs, I have to think about them in a different way because kind of like what we were just talking

18:00.920 --> 18:08.600
about the effectiveness of the API. Usually if you were to come with me and say this API is 75%

18:08.600 --> 18:15.160
effective, I wouldn't even look at because that's just not beneficial. But when you think about

18:15.160 --> 18:22.840
what it's doing and the fact that 75% beneficial is way better than 10% beneficial, which is where

18:22.840 --> 18:32.440
we were at with scraping through mug shots using a Boolean search essentially. It's way beneficial.

18:32.440 --> 18:35.720
And that's something that you have to wrap your head around as a developer saying that

18:35.720 --> 18:41.480
don't just throw it out because you want to see in high 90s. But elaborate a little bit on

18:42.200 --> 18:45.720
what you were doing before. Previous to having this recognition

18:47.000 --> 18:52.600
product in place, we had a web form where you would go in and you would type in demographics

18:52.600 --> 19:01.560
like I'm looking for a mail between the ages of 4050, height of 5 foot 10, black hair and

19:01.560 --> 19:05.880
down the list of anything that you can search for. And then it would then search for all of the

19:05.880 --> 19:12.360
inmates that meet those requirements and return thousands of mug shots that you would have to

19:12.360 --> 19:21.640
then search through by your eyes to determine. And where those that metadata was all manually

19:21.640 --> 19:27.960
exactly. So it's all based on when you get booked in, ask you questions, how old are you?

19:28.680 --> 19:32.200
Sometimes we have all that information because they give us a driver's license and we can put

19:32.200 --> 19:37.960
all that in. But in some cases, it's all based on what the inmates tell us. And I can tell you

19:37.960 --> 19:42.200
that they foreshore give us different names. They foreshore give us different birthdays.

19:42.200 --> 19:51.000
So yeah, it was very, while it was somewhat accurate, it definitely did have a tilt to it

19:51.000 --> 19:58.200
because of incorrect entered data. Okay. And so you get this list of your thousand search results

19:58.200 --> 20:02.600
and you know, for whatever reason, I guess it's TV. But I'm imagining like these big binders of

20:02.600 --> 20:08.920
mug shots, you're not doing it like that anymore. No, not anymore. Obviously before we had a website

20:08.920 --> 20:13.560
that the deputies could utilize, that's exactly how they did it. And there was binders full of

20:13.560 --> 20:19.480
mug shots. I think it's still like that on TV most years. On TV. And when they when they give

20:20.280 --> 20:26.760
what we call a six pack or a lineup card, it's still right there laid on the table where the

20:26.760 --> 20:32.280
guy points at the picture. Yeah, it's on the computer now too. Oh, really? Yeah. So, but

20:32.600 --> 20:36.600
while it's very different than that, it's actually very similar at the same time where yeah,

20:36.600 --> 20:41.960
you're just slogging through picture after picture. And anybody who knows anything about like

20:41.960 --> 20:47.000
assembly line hypnosis or anything like that, after the fifth page, you're not seeing those

20:47.000 --> 20:52.920
faces like you should anymore. You know, you're just, you know, hoping that there's some huge

20:52.920 --> 20:57.320
like mole on the guy's forehead or something because aside from that, you're not going to see it.

20:57.320 --> 21:07.240
Which is where the you mentioned 10% accuracy rate before where that comes from. So, when I think

21:07.240 --> 21:14.760
about the, you know, some of the issues associated with associated with applying machine learning

21:14.760 --> 21:21.400
in AI to one of the things that comes up for me pretty quickly or some of the, you know, incidents

21:21.400 --> 21:27.880
and stories we've seen around just the, you know, the bias that's injected into these types of

21:27.880 --> 21:34.520
algorithms and how that can kind of play out in, you know, these which are, you know, you know,

21:34.520 --> 21:40.520
really critical, you know, situations that involve human lives. Like, what kind of, how do you

21:40.520 --> 21:46.200
think about that and what kind of experience have you had with those kinds of issues? So, luckily,

21:46.200 --> 21:52.920
we, I'm happy to work for a sheriff who is very conscious about bias in policing.

21:54.040 --> 22:01.560
Well, before any of the other counties around this were thinking about that, he was thinking about

22:01.560 --> 22:06.840
it and running reports to make sure that his deputies weren't biased. I'm happy to say that

22:06.840 --> 22:16.200
all reports look good for us. So, as far as personal or very specific to what I've done,

22:16.200 --> 22:21.480
I don't see it, unfortunately, to say that like, this is how I would handle it or how we do him.

22:21.480 --> 22:29.320
Fortunately, we don't see it, but I do keep that in mind because these algorithms do tend to

22:29.320 --> 22:35.080
to have a little bit of bias. And that bias is based on when the algorithms are trained,

22:35.080 --> 22:40.680
they tend to be trained by the developers. So, the developers are going to, you know,

22:40.680 --> 22:47.800
whoever developed it is going to have their ethnicity, their gender more likely to be detected

22:47.800 --> 22:54.280
than others. So, that's what's the specific effects things that you've seen in your work with

22:54.280 --> 23:02.440
recognition? I haven't necessarily seen a lot of, I don't know, what I would call a misidentity

23:02.440 --> 23:07.800
based on something that I would say that, oh, this is definitely because this person is one

23:07.800 --> 23:12.600
ethnicity, but it's biased towards another ethnicity. I really haven't seen that much.

23:12.600 --> 23:17.320
That being said, we have come across a couple of situations where I would run,

23:17.320 --> 23:23.160
we would run an image through recognition. And it would give us a ethnicity, the results would

23:23.160 --> 23:27.480
be an ethnicity that was contrary to what we personally thought just looking at the image.

23:27.480 --> 23:36.600
But in that case, we still haven't had a situation where we were either proven wrong or right

23:36.600 --> 23:39.720
based on our previous thing, because we haven't identified that person yet.

23:41.240 --> 23:47.240
Do you find that is it equally as effective in identifying women

23:47.240 --> 23:53.960
in, you know, out of the pictures that you've identified or equally as effective in identifying,

23:53.960 --> 23:59.000
you know, all ethnicities or are there kind of shifts and biases in that part?

23:59.000 --> 24:06.040
I would say that when I see results that, for instance, if a result set has multiple

24:06.040 --> 24:13.320
genders in that result set, I will see males put in with a picture of a female, more than I will

24:13.320 --> 24:20.680
see females returned when I give it a picture of a male. So it definitely is more,

24:20.680 --> 24:27.000
I hate to use the word bias because, you know, I don't know how to say this except for that,

24:27.000 --> 24:31.960
when it's all set and done, because it's just a tool that we've created.

24:31.960 --> 24:37.800
If it gives us five results of people who are completely off, then we just kind of

24:37.800 --> 24:46.200
sweep those results aside. But yes, I know you can bias in some ways as a kind of a loaded term,

24:46.200 --> 24:53.880
and especially in my field. But statistically, if we kind of separate those kind of issues

24:53.880 --> 25:01.160
that you are seeing where it is more likely to return male pictures than female pictures,

25:01.160 --> 25:03.560
even when you give it a female picture, for example.

25:03.560 --> 25:10.680
Right. And I find that I've almost looked at those results and saw that the results return.

25:10.680 --> 25:15.960
So if I give recognition of female picture and it returns five results and two of them

25:15.960 --> 25:23.160
are males, I find that those males tend to have more generic features. You know, when you look at

25:23.160 --> 25:28.280
them, you don't see anything that stands out specifically. And so that makes me wonder if that those

25:28.280 --> 25:35.240
people's facial architecture is just simply generic in themselves. And that's why they're getting

25:35.240 --> 25:40.840
returned. I haven't done deep enough into looking at their specific facial analysis, which is one

25:40.840 --> 25:45.320
thing that recognition would allow me to do is I could send that one picture through and it could

25:45.320 --> 25:53.160
tell me what it's likelihood of it being male female, likelihood of it being one age group or

25:53.160 --> 25:59.720
another. I haven't really done that much of a deep dive into those outliers that I've seen

25:59.720 --> 26:04.200
to see if maybe that's what it is, maybe that recognition thinks that this picture of a male

26:04.200 --> 26:09.960
is actually a picture of a female and that's why it's being returned. But I can say that as far as

26:09.960 --> 26:17.720
age goes, I am a 35-year-old guy and every single time I run my picture through it, it thinks I'm 50.

26:21.800 --> 26:29.240
So there's that. And I think that if I maybe lost a beard and maybe didn't have as much

26:29.240 --> 26:40.120
little gray hairs, maybe it wouldn't think of me so old, but yeah, interesting. Are there kind of

26:40.120 --> 26:50.520
continuing on the, are there ways, I'm thinking the right way to get at this question? I'm curious

26:50.520 --> 26:56.840
about you're getting going back to the fact that recognition in a lot of ways is a black box for

26:56.840 --> 27:03.000
you, right? Are there things that you maybe kind of, you know, blind to as a developer that you

27:03.000 --> 27:09.800
might want to have more information about that you've run into? Well, obviously, I'm pretty much

27:09.800 --> 27:17.240
blind to the entire recognition of how it works. I send it faces, it sends me results and I have no

27:17.240 --> 27:27.400
idea how it gets to be, but I would like a way to a train recognition, if not, if they're just for

27:27.400 --> 27:34.040
me, you know, not having training my own collection, but train it by some sort of feedback loop,

27:34.040 --> 27:40.440
say that indeed this was a good hit, but indeed this wasn't a good hit, so that, you know, it can

27:40.440 --> 27:48.280
get smarter. I'd love to see that. I'd love to see a way for me to train it in different ways,

27:48.280 --> 27:55.320
because while I only use one slice of recognition, there are other bits that would be hugely

27:55.320 --> 28:00.760
beneficial to us if they worked in the way we needed them. A great example of that is tattoo

28:00.760 --> 28:09.160
recognition. We have, as well as a catalog of faces, we have a catalog of scars marks and tattoos.

28:09.160 --> 28:14.920
I've seen this on TV also. This is on TV as well. You know, on TV, they're able to say,

28:14.920 --> 28:19.720
oh, here's a picture of his skull tattoo, and here's him over here with his skull tattoo.

28:19.720 --> 28:31.080
Right. That doesn't exist in reality and be in recognition. It doesn't, the API isn't detailed

28:31.080 --> 28:36.520
enough to tell me this is a tattoo of a skull. It can say it's a tattoo, which is great,

28:36.520 --> 28:43.640
but it would be even better if I could take all of my pictures of tattoos, feed them into

28:44.840 --> 28:54.760
the collections in recognition, and then auto-tag those, so that we could have a more standardized

28:54.760 --> 29:01.880
list of tattoos. So if recognition saw a skull, it would always return it as the same spelling of

29:01.880 --> 29:09.880
skull as the same. It would generate some kind of taxonomy of tattoos. Somebody would say skull,

29:09.880 --> 29:15.320
somebody makes a crossbone, somebody may put an eye in there somewhere. So we don't have an easy

29:15.320 --> 29:24.440
way to textually search for if a victim comes in and says their attacker had a skull tattoo on

29:24.440 --> 29:32.280
their chest. We could, if we had recognition already auto-tagging these tattoos, I could go

29:32.280 --> 29:37.880
in search skull and get a list of everybody who has a skull tattoo on their chest that has been

29:37.880 --> 29:44.040
through our jail, possibly ideing them simply by knowing that they had a tattoo, and that would be

29:44.040 --> 29:51.240
obviously immensely, I can see how that'd be powerful, and I can tell you that I was watching a

29:51.240 --> 29:58.440
TV show a couple of days ago, and they took a picture of a guy's side of his head, and they said,

29:58.440 --> 30:05.000
oh, look at this guy's ear, ears are biometrically as identical to fingerprints, so we're just going

30:05.000 --> 30:11.400
to take a picture of his ear and run it through ear recognition. I mean, obviously if they could do

30:11.400 --> 30:17.560
that, I'd love recognition to do that. I brought that up because this is the, again, this is the

30:17.560 --> 30:25.160
thing I come across with the public is they assume we already can do this stuff. They assume, like,

30:25.160 --> 30:33.480
when I talk about recognition with the citizens in our area, half of them didn't realize we couldn't

30:33.480 --> 30:39.000
do facial recognition before. They assume that when we send a picture to the news and say, can you

30:39.000 --> 30:45.320
help me identify this person, that we've already done that part, right? Now luckily we have already

30:45.320 --> 30:53.160
done that part, but before this year, we just didn't exist, and it's pretty funny. In my mind,

30:53.160 --> 30:57.720
like, you know, in my mind, you're still going through these bindings of things, and the public

30:57.720 --> 31:02.920
thinks that you have a, you know, a minority report already established. Exactly. That's interesting.

31:02.920 --> 31:11.160
So that's another thing is I want to, I want to get us to the point where we are at where the

31:11.160 --> 31:16.760
citizens already think we are, right? So that when they watch TV, they watch a TV show like APB,

31:16.760 --> 31:22.280
and they see all of this stuff that doesn't exist yet, that I can at least say, well, I'm working

31:22.280 --> 31:28.360
towards it and getting us there. And that's what, that's my goal. And so what do you,

31:29.080 --> 31:36.200
what's next to get you there? Is it, you know, are you kind of waiting for, are you stuck waiting for

31:36.200 --> 31:41.400
recognition to build out all these features, or is this like inspiration and justification for you

31:41.400 --> 31:46.200
to, you know, go find a data scientist to partner with, or something like that. How do you proceed?

31:47.000 --> 31:53.640
Many different ways. This is definitely not me sitting waiting for something to happen.

31:53.640 --> 31:59.880
This has really energized me and energized everybody on my team to go out and innovate and find

31:59.880 --> 32:04.600
new ways that we can assist the deputies and other law enforcement officers doing their job.

32:04.600 --> 32:09.240
So if it means that we know we want to do something and the only way we're going to be able to do it

32:09.240 --> 32:15.480
is finding a data scientist to partner with, then we'll do that. If it means, you know, talking with

32:15.480 --> 32:21.320
Amazon over and over again until we get something into the product that we need, then that's what it

32:21.320 --> 32:27.720
means. If it means finding an interesting way to use another one of their machine learning tools

32:28.760 --> 32:33.080
that they didn't intend, because I can honestly tell you all of the talks I've had with Amazon,

32:33.080 --> 32:38.520
when they first put out recognition, nobody thought this would be great for recognizing criminals

32:38.520 --> 32:44.680
from mug shots. But now that I do it, it's something that they absolutely think about the law

32:44.680 --> 32:52.360
enforcement applications for all of their future machine learning stuff. So yeah, it's just going

32:52.360 --> 32:57.080
out there and trying to find interesting ways to use the things that we already have access to

32:57.080 --> 33:04.280
and possibly driving that thing into what we need it to be, which is kind of what we've done

33:04.280 --> 33:09.880
with recognition. Now, I won't take any credit for what the AWS guys do because they do great work

33:09.880 --> 33:15.320
and they do it on their own. I'm just saying that once they see that we have a use case there,

33:15.320 --> 33:18.920
they will take that into account and move for us.

33:18.920 --> 33:34.600
When you think about the vast array of services that AWS offers, or when you think about the array

33:34.600 --> 33:41.000
of machine learning and AI services that are offered by not only AWS, but Google and Microsoft,

33:41.000 --> 33:49.560
and a host of other players, do you have a short list of things that you're excited about,

33:49.560 --> 33:56.280
you know, tinkering with and putting the use in your service office?

33:56.280 --> 34:01.960
Absolutely, absolutely. I mean, just this morning, the keynote announced recognition for video.

34:03.960 --> 34:06.920
I don't even know if I'm going to wait till I get home to play with that.

34:06.920 --> 34:16.120
We have so much video in the way of surveillance of crimes and the way of, you know,

34:17.160 --> 34:23.960
video sent in by eyewitnesses that we could utilize to determine, you know, who that person is

34:23.960 --> 34:28.680
and doing that thing. And then there are other use cases that just quickly going through my head,

34:28.680 --> 34:34.360
you know, one of the things that recognition would possibly allow us to do is determine intent.

34:34.360 --> 34:39.000
You know, is somebody intending to do harm? Is somebody intending to do somebody else?

34:39.000 --> 34:45.480
Maybe there's a way to get notifications if a camera sees that.

34:45.480 --> 34:54.840
So just many things I want to play around with that. There's the new, I think it's called deep lens,

34:54.840 --> 35:00.920
which allows you to train a model based on what you show it. Obviously, you can use that

35:00.920 --> 35:07.080
for anything from training it to determine products to training it to determine. I think they showed

35:08.520 --> 35:15.240
record labels, things like that. I think I could train it to do better at finding

35:16.040 --> 35:21.080
the faces of people in those surveillance cameras. Or maybe like we were just talking about,

35:21.080 --> 35:28.360
maybe I can train it to do tattoo detection, right? So that's something I want to play around with.

35:28.360 --> 35:36.680
Then they talked about, I can't remember the name of it now, but the new, they have a new

35:36.680 --> 35:41.000
machine learning platform that allows you to build the models without having to know anything

35:41.000 --> 35:46.440
about data science. And so I'm going to play around with that obviously and see if I can

35:46.440 --> 35:49.720
get a model up and running and maybe talk, like when we were talking about

35:51.320 --> 35:57.000
predictive policing to know where to send deputies because of, you know, maybe on the Fourth of July,

35:57.000 --> 36:01.880
we see a lot of illegal fireworks being set off in a certain area and get a notification. Hey,

36:01.880 --> 36:07.160
why don't you head over to that area? That kind of thing. So lots of stuff I'm excited to play with.

36:07.800 --> 36:12.760
Awesome. Awesome. Well, Chris, thanks so much for taking the time out to chat with me. I enjoyed

36:12.760 --> 36:18.600
learning a bit about your use case and, you know, hearing about this kind of dealing with AI

36:18.600 --> 36:22.840
services from a developer's perspective. I enjoyed it too. Thank you very much for having me.

36:22.840 --> 36:32.520
Thanks. All right, everyone. That's our show for today. Thanks so much for listening and for your

36:32.520 --> 36:38.520
continued feedback and support. For more information on Chris or any of the topics covered in this

36:38.520 --> 36:46.280
episode, head on over to twimmelaii.com slash talk slash 86. To follow along with the AWS

36:46.280 --> 36:54.200
Reinvent series, visit twimmelaii.com slash reinvent. To enter our twimmel one mill contest,

36:54.200 --> 37:01.560
visit twimmelaii.com slash twimmel one mill. Of course, we'd be delighted to hear from you,

37:01.560 --> 37:08.360
either via a comment on the show notes page or via Twitter to at twimmelaii or at Sam Charrington.

37:09.480 --> 37:14.360
Thanks again to until Nirvana for their sponsorship of this series. To learn more about their

37:14.360 --> 37:21.720
role in deep lens and the other things they've been up to, visit intelnervana.com. And of course,

37:21.720 --> 37:51.560
thanks once again to you for listening and catch you next time.

