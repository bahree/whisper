1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:26,160
I'm your host Sam Charrington.

3
00:00:26,160 --> 00:00:30,320
Have you been enjoying our Twamokon coverage, but want more?

4
00:00:30,320 --> 00:00:35,640
Twamokon video packages are now available for advanced purchase over at twamokon.com

5
00:00:35,640 --> 00:00:37,160
slash videos.

6
00:00:37,160 --> 00:00:42,560
The package features over 13 hours of content, including all the awesome keynotes and panels

7
00:00:42,560 --> 00:00:49,120
you've heard on the podcast, all of the breakout sessions, including 9K study and 8 tech track

8
00:00:49,120 --> 00:00:56,000
sessions, as well as the highly regarded team tear down panels with Airbnb and SurveyMonkey.

9
00:00:56,000 --> 00:01:01,680
Again, visit twamokon.com slash videos for more info.

10
00:01:01,680 --> 00:01:06,720
And now on to the show.

11
00:01:06,720 --> 00:01:11,560
Alright everyone, I am on the line with Phoebe DeVries and Brendan Mead.

12
00:01:11,560 --> 00:01:16,160
Phoebe is a postdoctorate fellow with the Department of Earth and Planetary Sciences

13
00:01:16,160 --> 00:01:20,720
at Harvard, as well as an assistant faculty at the University of Connecticut.

14
00:01:20,720 --> 00:01:26,480
And Brendan is professor of Earth and Planetary Sciences at Harvard, and an affiliate faculty

15
00:01:26,480 --> 00:01:28,840
in computer science as well.

16
00:01:28,840 --> 00:01:32,640
Phoebe and Brendan, welcome to this week in machine learning and AI.

17
00:01:32,640 --> 00:01:34,480
Thanks so much for having us Sam.

18
00:01:34,480 --> 00:01:35,480
Thank you.

19
00:01:35,480 --> 00:01:40,520
Let's get started by having the two of you introduce yourselves.

20
00:01:40,520 --> 00:01:42,280
Brendan wanted to go first.

21
00:01:42,280 --> 00:01:43,280
Thanks Sam.

22
00:01:43,280 --> 00:01:49,000
Yeah, I'm a professor of Earth and Planetary Sciences at Harvard, where I've been for the

23
00:01:49,000 --> 00:01:51,000
past 13 years prior to that.

24
00:01:51,000 --> 00:01:55,080
I got undergraduate degree in history of science from Johns Hopkins University, and I got

25
00:01:55,080 --> 00:02:00,840
my PhD in Earth and Planetary Sciences from MIT.

26
00:02:00,840 --> 00:02:05,000
And here at Harvard, we do a lot of work on computational science with a particular

27
00:02:05,000 --> 00:02:07,840
emphasis on understanding earthquakes.

28
00:02:07,840 --> 00:02:08,840
Fantastic.

29
00:02:08,840 --> 00:02:09,840
And Phoebe?

30
00:02:09,840 --> 00:02:10,840
Yeah.

31
00:02:10,840 --> 00:02:15,160
Well, I did my undergrad at Harvard and applied math.

32
00:02:15,160 --> 00:02:20,400
And then I spent a year at Cambridge doing a master's degree in glaciology, and then came

33
00:02:20,400 --> 00:02:24,720
back to work in Brendan's research group for my PhD.

34
00:02:24,720 --> 00:02:27,520
And now I'm a postdoc with him.

35
00:02:27,520 --> 00:02:32,400
And for most of my PhD, we worked on sort of large-scale forward modeling problems of

36
00:02:32,400 --> 00:02:36,800
time-dependent stress changes after large earthquakes, but now we're really focusing

37
00:02:36,800 --> 00:02:39,560
on machine learning for earthquake science.

38
00:02:39,560 --> 00:02:46,160
So Brendan, the description of your lab is that you are focusing on deconvolving, tectonic

39
00:02:46,160 --> 00:02:52,320
and earthquake cycle signals across Japanese islands to identify the coupled, subduction

40
00:02:52,320 --> 00:02:59,000
zone interface that ruptured during the Great to Hoku-Oki earthquake of 2011.

41
00:02:59,000 --> 00:03:02,000
Yeah, that's true.

42
00:03:02,000 --> 00:03:04,920
That's certainly something that we worked on.

43
00:03:04,920 --> 00:03:07,200
Let me tell you how we got into this problem.

44
00:03:07,200 --> 00:03:08,200
Okay.

45
00:03:08,200 --> 00:03:12,640
And our lab, what we've been really interested in, is trying to say as much as we can about

46
00:03:12,640 --> 00:03:15,680
earthquakes before they happen.

47
00:03:15,680 --> 00:03:20,360
And one of the great revolutions of the past 20 to 25 years now has been the ability to

48
00:03:20,360 --> 00:03:28,360
measure using very high precision GPS measurements how the earth's surface moves at scales where

49
00:03:28,360 --> 00:03:33,160
we can actually see the strain accumulation in the Earth's crust, the elastic strain that

50
00:03:33,160 --> 00:03:35,840
will be released in a future-grade earthquake.

51
00:03:35,840 --> 00:03:40,320
And so for a long time, our goal has been to try and map this type of behavior around

52
00:03:40,320 --> 00:03:45,000
the globe, and try and use this information to say something about the future sizes and

53
00:03:45,000 --> 00:03:47,760
locations of large earthquakes.

54
00:03:47,760 --> 00:03:55,680
To do this, we have employed a lot of HPC approaches ranging from visco-elastic modeling to boundary

55
00:03:55,680 --> 00:04:02,760
element modeling, and more recently, our interests have drifted more towards the more direct

56
00:04:02,760 --> 00:04:09,240
applications of ML and AI to these problems where there are vast data sets waiting to be

57
00:04:09,240 --> 00:04:10,240
explored.

58
00:04:10,240 --> 00:04:12,720
And that's what got us to the paper we wrote recently.

59
00:04:12,720 --> 00:04:13,720
Awesome.

60
00:04:13,720 --> 00:04:17,760
And Phoebe, you were the lead author on this paper, which was called Deep Learning of

61
00:04:17,760 --> 00:04:21,400
Aftershock Patterns Following Large Earthquakes.

62
00:04:21,400 --> 00:04:22,400
Tell us about the paper.

63
00:04:22,400 --> 00:04:24,840
What was the main thrust of the work?

64
00:04:24,840 --> 00:04:30,800
Well, we were interested in Aftershock's from a machine learning perspective, or Aftershock

65
00:04:30,800 --> 00:04:37,080
locations specifically, because there are these very well-established empirical relationships

66
00:04:37,080 --> 00:04:47,440
that can describe the time decay of Aftershock frequency, and also the likely maximum magnitude

67
00:04:47,440 --> 00:04:49,640
of Aftershocks as well.

68
00:04:49,640 --> 00:04:55,000
But the locations of Aftershocks are a lot more difficult to explain.

69
00:04:55,000 --> 00:05:00,960
So that's really what got us interested in this problem from a machine learning perspective.

70
00:05:00,960 --> 00:05:03,440
There are a lot of data about Aftershocks.

71
00:05:03,440 --> 00:05:10,480
So it's really a sort of good, you know, it's a very good application for machine learning

72
00:05:10,480 --> 00:05:12,840
just off the bat.

73
00:05:12,840 --> 00:05:14,960
What are some of the available data sources?

74
00:05:14,960 --> 00:05:20,280
Is it primarily the GPS data that Brendan was describing?

75
00:05:20,280 --> 00:05:26,800
Yes, there are a lot of earthquake catalogs, just records of where and when earthquakes

76
00:05:26,800 --> 00:05:30,680
have occurred, just thousands and thousands of them.

77
00:05:30,680 --> 00:05:35,520
And what we did in this study was we combined data from two separate catalogs.

78
00:05:35,520 --> 00:05:40,440
One was a catalog of very large earthquakes, most of them larger than six or so.

79
00:05:40,440 --> 00:05:44,040
And the other was a catalog of Aftershock's following those two.

80
00:05:44,040 --> 00:05:48,400
And it was the combination of those two catalogs that enabled us to analyze the relationship

81
00:05:48,400 --> 00:05:52,640
between the main shocks themselves and the Aftershocks that followed.

82
00:05:52,640 --> 00:05:59,400
And is there, is it a challenge to differentiate earthquakes from Aftershocks?

83
00:05:59,400 --> 00:06:03,160
Is that an issue or not really?

84
00:06:03,160 --> 00:06:05,560
That is discussed a lot in the literature.

85
00:06:05,560 --> 00:06:10,760
We decided to take a very sort of broad or simple approach to it.

86
00:06:10,760 --> 00:06:17,760
So we just defined Aftershocks as the earthquakes that take place sort of within a year after

87
00:06:17,760 --> 00:06:23,120
these large main shocks and within 100 kilometers horizontally and down to 50 kilometers depth,

88
00:06:23,120 --> 00:06:26,960
just to keep things simple for this kind of first approach, first try.

89
00:06:26,960 --> 00:06:28,640
Yeah, I think Phoebe is exactly right.

90
00:06:28,640 --> 00:06:34,080
In the literature, you can find arguments that Aftershocks are extremely well-defined phenomena.

91
00:06:34,080 --> 00:06:39,120
And you can find arguments that Aftershocks are nothing more than another part of a generic

92
00:06:39,120 --> 00:06:43,720
earthquake sequence that has been ongoing for hundreds of not thousands of years.

93
00:06:43,720 --> 00:06:48,720
And I absolutely agree with Phoebe that we took a very simplified approach to classifying

94
00:06:48,720 --> 00:06:50,280
them on this study.

95
00:06:50,280 --> 00:06:58,200
You mentioned that your lab has previously spent a lot of time on high-performance computing

96
00:06:58,200 --> 00:07:03,400
based analysis of these earthquakes and Aftershocks.

97
00:07:03,400 --> 00:07:09,560
I'm curious if you can describe that a little bit more as a segue to talking about what's

98
00:07:09,560 --> 00:07:14,440
been new and working with machine learning based approaches.

99
00:07:14,440 --> 00:07:19,000
Phoebe, do you want to talk about your thesis?

100
00:07:19,000 --> 00:07:26,200
Well, I could talk about the visco-elastic modeling piece of what Brendan's group has been

101
00:07:26,200 --> 00:07:27,200
doing.

102
00:07:27,200 --> 00:07:35,640
So we implemented a three-dimensional code to calculate time-dependent stress changes

103
00:07:35,640 --> 00:07:42,480
in the crust after large earthquakes due to visco-elastic relaxation.

104
00:07:42,480 --> 00:07:51,600
And we were using these models to try to get at questions of whether or not or look at

105
00:07:51,600 --> 00:07:56,040
questions of possible delayed earthquake triggering.

106
00:07:56,040 --> 00:08:01,080
One of the applications we are one of the examples that we looked at was the North Anatolian

107
00:08:01,080 --> 00:08:06,160
fault in Turkey where there's been this remarkable sequence of large earthquakes that have

108
00:08:06,160 --> 00:08:11,080
just been marching to the west over time since 1939.

109
00:08:11,080 --> 00:08:20,400
And so we used these visco-elastic models to try to look at what effects visco-elastic

110
00:08:20,400 --> 00:08:29,240
relaxation may have had in terms of loading the hypersenters of the subsequent earthquakes

111
00:08:29,240 --> 00:08:30,840
in the sequence.

112
00:08:30,840 --> 00:08:36,200
So not to get too deep into the science, but this visco-elastic study is essentially modeling

113
00:08:36,200 --> 00:08:42,160
the Earth or maybe the surface of the Earth as kind of using fluid dynamics types of

114
00:08:42,160 --> 00:08:46,160
approaches to try to anticipate earthquakes and aftershocks?

115
00:08:46,160 --> 00:08:49,760
Yeah, so it's a quasi-static model.

116
00:08:49,760 --> 00:08:57,040
So we used Berger's realities to model the behavior of the lower crust and upper mantle.

117
00:08:57,040 --> 00:09:02,120
And it was sort of purely phenomenological, it just seems to explain the data really

118
00:09:02,120 --> 00:09:05,240
well and that's what motivated the models.

119
00:09:05,240 --> 00:09:13,160
The previous approach to calculating these models or doing this analysis was based on

120
00:09:13,160 --> 00:09:14,480
high-performance computing.

121
00:09:14,480 --> 00:09:17,080
What did that entail?

122
00:09:17,080 --> 00:09:20,920
In terms of the high-performance computing behind the visco-elastic models?

123
00:09:20,920 --> 00:09:26,920
Or I guess I'm trying to get at ways that the modeling process may have differed between

124
00:09:26,920 --> 00:09:35,320
what you did before in machine learning and was it this specific problem that said,

125
00:09:35,320 --> 00:09:41,120
hey, this needs to be machine learning and not high-performance computing or for the

126
00:09:41,120 --> 00:09:47,520
class of problems that you tend to see in this space, could you choose either?

127
00:09:47,520 --> 00:09:51,040
And machine learning was kind of the new fancy thing, so we try that.

128
00:09:51,040 --> 00:09:55,800
How do they qualitatively differ in terms of solutions to these kinds of problems?

129
00:09:55,800 --> 00:09:57,680
Those are the kinds of things I'm curious about.

130
00:09:57,680 --> 00:09:59,640
Yeah, Sam, that's a great question.

131
00:09:59,640 --> 00:10:02,760
It turns out one led to the other.

132
00:10:02,760 --> 00:10:10,840
So when Phoebe did the work using this HPC code to try to see if we could explain the

133
00:10:10,840 --> 00:10:15,280
delayed triggering of earthquakes along the North and Italian fault, that ended up taking

134
00:10:15,280 --> 00:10:18,840
about 2.5 million CPU hours.

135
00:10:18,840 --> 00:10:19,840
Wow.

136
00:10:19,840 --> 00:10:25,600
And that was how long it took to try and explain the triggering of about seven or eight

137
00:10:25,600 --> 00:10:26,600
earthquakes.

138
00:10:26,600 --> 00:10:30,880
And we said, we'd like to do this elsewhere, but that's taking a long time.

139
00:10:30,880 --> 00:10:38,080
And so together we came up with the idea to see if we could train a neural network to emulate

140
00:10:38,080 --> 00:10:40,240
our HPC code.

141
00:10:40,240 --> 00:10:46,240
And so what we did was with the HPC code, we were able to generate lots of training data.

142
00:10:46,240 --> 00:10:51,160
And then we trained a neural network to simply map inputs to outputs, all the nonlinear

143
00:10:51,160 --> 00:10:52,160
ones.

144
00:10:52,160 --> 00:10:56,720
And then once we had done all the nonlinear ones, we could just combine the linear part

145
00:10:56,720 --> 00:10:57,720
easily.

146
00:10:57,720 --> 00:11:03,480
And so when we did that, we were able to replace this HPC code with an incredibly compact

147
00:11:03,480 --> 00:11:09,680
neural network, which is able to give almost exactly the same answer and run in 500 times

148
00:11:09,680 --> 00:11:10,680
faster.

149
00:11:10,680 --> 00:11:12,720
Oh, that's awesome.

150
00:11:12,720 --> 00:11:19,720
And that just one of the reasons why the description of your lab that I saw on your Harvard

151
00:11:19,720 --> 00:11:23,800
page caught my interest was it mentioned deconvolving.

152
00:11:23,800 --> 00:11:29,680
I'm assuming that that is referring to motion of tectonic plates and not convolutional

153
00:11:29,680 --> 00:11:30,680
neural nets.

154
00:11:30,680 --> 00:11:34,920
And there's an overlap there.

155
00:11:34,920 --> 00:11:39,920
I would have been way ahead of my time if I had been not clever.

156
00:11:39,920 --> 00:11:40,920
Yeah.

157
00:11:40,920 --> 00:11:47,960
So when we got into the work of interpreting these GPS data, the world was kind of divided

158
00:11:47,960 --> 00:11:48,960
into two halves.

159
00:11:48,960 --> 00:11:53,240
There were the people who looked at the data and thought it told you about strain accumulation

160
00:11:53,240 --> 00:11:55,240
in the earthquake cycle.

161
00:11:55,240 --> 00:11:59,120
And people who thought it told you about long-term tectonic motions.

162
00:11:59,120 --> 00:12:00,600
Neither group was wrong.

163
00:12:00,600 --> 00:12:03,360
In fact, both groups were right.

164
00:12:03,360 --> 00:12:09,120
And it was the ability to integrate and deconvolve those two signals together that really

165
00:12:09,120 --> 00:12:10,760
allowed for progress to be made.

166
00:12:10,760 --> 00:12:15,280
And that involved developing a class of large scale models.

167
00:12:15,280 --> 00:12:18,920
And that's what allowed us to really isolate the part we care about, which is the

168
00:12:18,920 --> 00:12:20,600
strain accumulation signal.

169
00:12:20,600 --> 00:12:23,400
So let's talk a little bit about that modeling process.

170
00:12:23,400 --> 00:12:25,200
How did you go about that?

171
00:12:25,200 --> 00:12:26,200
Yeah.

172
00:12:26,200 --> 00:12:34,400
So the core idea was to think about earthquakes, not as random phenomena, not as things that

173
00:12:34,400 --> 00:12:38,000
popped up in the middle of nowhere, but rather to think about earthquakes as a byproduct

174
00:12:38,000 --> 00:12:39,880
of plate tectonics.

175
00:12:39,880 --> 00:12:46,240
So most earthquakes occur simply because large pieces of the earth's rocky crust are

176
00:12:46,240 --> 00:12:50,440
moving past each other, and they're temporarily stuck together, and they're stuck together

177
00:12:50,440 --> 00:12:54,640
along these localized surfaces called faults that occasionally fail.

178
00:12:54,640 --> 00:12:59,720
And when they do fail, because the frictional resistance to motion is overcome by the stresses

179
00:12:59,720 --> 00:13:03,280
that are accumulated, they rupture in large earthquakes.

180
00:13:03,280 --> 00:13:09,480
So to make progress, what we found we had to do was we had to integrate both the physics

181
00:13:09,480 --> 00:13:13,760
of what was going on at large scales, that is, the motion of tectonic plates, with the

182
00:13:13,760 --> 00:13:18,240
physics of what was going on around faults, that is earthquake cycle physics.

183
00:13:18,240 --> 00:13:22,320
And we were able to do that and link them together, and by linking them together, we finally

184
00:13:22,320 --> 00:13:28,240
had a computational tool that allowed us to tease apart the competing effects due to

185
00:13:28,240 --> 00:13:30,080
do the both sets of physics.

186
00:13:30,080 --> 00:13:33,240
And with that, we were able to isolate the strain accumulation signature that we were

187
00:13:33,240 --> 00:13:34,240
looking for.

188
00:13:34,240 --> 00:13:35,240
Okay.

189
00:13:35,240 --> 00:13:39,760
So you started that by saying, something to get left me with the impression that the

190
00:13:39,760 --> 00:13:47,360
idea that earthquakes are caused by these tectonic shifts or plates was, you know, contentious

191
00:13:47,360 --> 00:13:55,760
in some way, or not, let's start with contentious in some way in the field.

192
00:13:55,760 --> 00:14:02,000
And yeah, it sounds like there's, so one model is this tectonic plate model, and the

193
00:14:02,000 --> 00:14:06,240
other is more this local phenomenon?

194
00:14:06,240 --> 00:14:10,120
I would say it was more just a case of what people were interested in.

195
00:14:10,120 --> 00:14:13,600
They were interested in the tectonic problem, or they were interested in the earthquake

196
00:14:13,600 --> 00:14:14,600
problem.

197
00:14:14,600 --> 00:14:20,440
And I think the realization that a lot of people had, and this started back in the 80s,

198
00:14:20,440 --> 00:14:24,680
and the late 80s and through the 90s and early 2000s, was that those two scales had to

199
00:14:24,680 --> 00:14:25,920
be linked.

200
00:14:25,920 --> 00:14:30,240
The small scale and the large scale had to be linked to really make sense of either.

201
00:14:30,240 --> 00:14:35,820
And that was a computational challenge, in particular, due to the complex geometry of

202
00:14:35,820 --> 00:14:39,520
fault systems of plate boundaries, that that was where the real challenge was.

203
00:14:39,520 --> 00:14:43,600
How did you represent those geometries and integrate them into a computational model

204
00:14:43,600 --> 00:14:45,760
that included the physics for both?

205
00:14:45,760 --> 00:14:52,520
And the specifics of that model, how do you go about developing that?

206
00:14:52,520 --> 00:15:03,040
Well, that takes us back a long time, that takes us back to my PhD, and so the ideas have

207
00:15:03,040 --> 00:15:10,000
been in the air that these scales had to be linked for more than a decade.

208
00:15:10,000 --> 00:15:13,760
And people had done a very nice local scale work on it.

209
00:15:13,760 --> 00:15:19,040
And what I attempted to do was to generalize that work so that we could put in high fidelity,

210
00:15:19,040 --> 00:15:24,480
very geometrically complex representations of fault systems, like those in Southern California,

211
00:15:24,480 --> 00:15:26,480
which are unbelievably complex.

212
00:15:26,480 --> 00:15:32,680
And yet at the same time, have all the motions on the faults in Southern California be consistent

213
00:15:32,680 --> 00:15:36,360
with what's going on between the two large plates, the North American and the Pacific

214
00:15:36,360 --> 00:15:40,240
plate, on either side of the Southern California fault system?

215
00:15:40,240 --> 00:15:45,680
And the hardest part of that was actually not any of the physics part, but the most interesting

216
00:15:45,680 --> 00:15:53,440
part was thinking about the problem very algorithmically, and figuring out how to efficiently

217
00:15:53,440 --> 00:16:00,880
specify how the fault system geometry would be related to the motions of the plates on

218
00:16:00,880 --> 00:16:04,160
either side of these faults.

219
00:16:04,160 --> 00:16:10,120
It was an extremely geometrically complex problem involved mapping back and forth between

220
00:16:10,120 --> 00:16:12,960
four different reference frames.

221
00:16:12,960 --> 00:16:18,320
And I'm really glad I finished it so that I don't have to work on that again.

222
00:16:18,320 --> 00:16:20,840
That's the macro scale piece.

223
00:16:20,840 --> 00:16:27,080
How is that then linked to the micro scale piece, which is if I caught you correctly, that's

224
00:16:27,080 --> 00:16:29,560
more looking at these earthquakes as a time series?

225
00:16:29,560 --> 00:16:30,560
Is that right?

226
00:16:30,560 --> 00:16:31,560
What do you think about it?

227
00:16:31,560 --> 00:16:39,040
Yeah, I think Phoebe's work is what really linked that in, because she added time-dependent

228
00:16:39,040 --> 00:16:44,080
evolving motions due to the coupling between the Earth's crust and the mantle.

229
00:16:44,080 --> 00:16:47,600
And so I think Phoebe can speak to the time-dependent part of that story.

230
00:16:47,600 --> 00:16:48,600
Awesome.

231
00:16:48,600 --> 00:16:49,600
Well, yeah.

232
00:16:49,600 --> 00:16:55,600
Well, we haven't actually totally linked Brendan's block models.

233
00:16:55,600 --> 00:17:04,160
Well, yeah, so we built in some time dependence to the framework that Brendan built over his

234
00:17:04,160 --> 00:17:05,160
PhD.

235
00:17:05,160 --> 00:17:10,400
And since then, using these visco-elastic models.

236
00:17:10,400 --> 00:17:14,800
And that just allowed us to sort of add a time perturbation throughout the earthquake cycle

237
00:17:14,800 --> 00:17:21,280
to sort of incorporate information about when the most recent earthquake had occurred on

238
00:17:21,280 --> 00:17:22,280
that fault.

239
00:17:22,280 --> 00:17:27,840
All right, so the picture of starting to emerge from me, you've got this model that kind

240
00:17:27,840 --> 00:17:35,960
of depends on two time scales, this physical geometric perspective, and this more time-oriented

241
00:17:35,960 --> 00:17:38,560
perspective or local perspective.

242
00:17:38,560 --> 00:17:46,560
And one big challenge is linking these two and kind of driving some consistency between

243
00:17:46,560 --> 00:17:48,040
them.

244
00:17:48,040 --> 00:17:54,840
And then, once you've got this model, you identified the computational complexity of actually

245
00:17:54,840 --> 00:18:03,000
using it and set out to build a deep learning-based approach that would essentially model your

246
00:18:03,000 --> 00:18:04,000
model.

247
00:18:04,000 --> 00:18:07,080
Is that a decent summary?

248
00:18:07,080 --> 00:18:12,360
Sam, that's like the best abstract of what we've done for the past 10 or 15 years I've

249
00:18:12,360 --> 00:18:13,360
ever heard.

250
00:18:13,360 --> 00:18:14,360
That's fantastic.

251
00:18:14,360 --> 00:18:18,000
I really like that way of describing it.

252
00:18:18,000 --> 00:18:25,040
And that was our entry into the machine learning and the machine learning world.

253
00:18:25,040 --> 00:18:32,760
And from that, we eventually found these datasets that we could start making slightly U.S.

254
00:18:32,760 --> 00:18:36,520
model-dependent predictions with, and that's what led us to the paper.

255
00:18:36,520 --> 00:18:37,520
Nice, nice.

256
00:18:37,520 --> 00:18:43,280
So Phoebe, as you set out to explore this machine learning, deep learning world, what were

257
00:18:43,280 --> 00:18:49,200
the things that you found that you were able to apply to your particular problem?

258
00:18:49,200 --> 00:18:53,880
We were just, well, so after we worked on sort of accelerating this computationally intensive

259
00:18:53,880 --> 00:18:59,920
code using a train neural network, we, well, at least from my perspective, I just started

260
00:18:59,920 --> 00:19:05,000
to think of all the different problems in Earth science that involve sort of inputs and

261
00:19:05,000 --> 00:19:10,800
outputs and figuring out what the relationship is between those inputs and outputs.

262
00:19:10,800 --> 00:19:17,200
And so that's really what got us thinking about this aftershock problem because we could

263
00:19:17,200 --> 00:19:23,520
calculate stress changes in the crust and upper mantle after large earthquakes and then

264
00:19:23,520 --> 00:19:30,360
see if we could use a neural network to map those stress changes to aftershock locations.

265
00:19:30,360 --> 00:19:33,680
So it just got us thinking about all these different problems.

266
00:19:33,680 --> 00:19:42,080
What was the nature of the deep learning model that you used for these types of problems?

267
00:19:42,080 --> 00:19:47,720
Did you, was it kind of an off-the-shelf type of a model or was it something that you

268
00:19:47,720 --> 00:19:52,760
crafted from scratch based on the specifics of your problem?

269
00:19:52,760 --> 00:19:55,840
No, it was very, very simple.

270
00:19:55,840 --> 00:19:57,360
We implemented it in Keras.

271
00:19:57,360 --> 00:20:01,880
It was a fully connected network about as simple as you can get.

272
00:20:01,880 --> 00:20:04,240
And it just seemed to work really well.

273
00:20:04,240 --> 00:20:10,240
And do you have any intuition about the fact that it works so well, so quickly give you

274
00:20:10,240 --> 00:20:16,280
any intuition about the problem, like does it say anything about latent characteristics

275
00:20:16,280 --> 00:20:23,800
of the problem or the relationships between the data that you had and the actual physical

276
00:20:23,800 --> 00:20:28,400
phenomena or is it, it just happens to work well because these things are good at picking

277
00:20:28,400 --> 00:20:29,720
out patterns and data.

278
00:20:29,720 --> 00:20:35,320
It turns out for this problem there's a remarkable amount of physical insight that we could

279
00:20:35,320 --> 00:20:36,760
gain from this.

280
00:20:36,760 --> 00:20:39,280
So let me set the stage a little bit.

281
00:20:39,280 --> 00:20:44,720
When we modeled the aftershocks in the study, we didn't just take the location of the

282
00:20:44,720 --> 00:20:48,840
main shock and say go predict the aftershocks.

283
00:20:48,840 --> 00:20:52,280
Instead what we did was we took the location of the main shock and we took information

284
00:20:52,280 --> 00:20:57,280
about large earthquakes in particular how much they slipped and where they slipped.

285
00:20:57,280 --> 00:21:03,200
And we put that information through a forward model which predicted stresses, the changing

286
00:21:03,200 --> 00:21:07,720
stress everywhere in the earth's crust as a result of the large earthquake.

287
00:21:07,720 --> 00:21:13,680
Those were the raw inputs that we actually put into that served as essentially the features

288
00:21:13,680 --> 00:21:15,760
for a machine learning model.

289
00:21:15,760 --> 00:21:20,880
And then the labels in our machine learning model were the locations of the earthquakes.

290
00:21:20,880 --> 00:21:27,880
And so what this did was we provided a sort of physics-based regularization of the features

291
00:21:27,880 --> 00:21:32,360
in a way that ensures that conservation of mass is satisfied, conservation of linear

292
00:21:32,360 --> 00:21:34,640
momentum is satisfied.

293
00:21:34,640 --> 00:21:39,520
And because of this, when we were able to look at the neural network and do inference

294
00:21:39,520 --> 00:21:45,000
with it after we had already trained in everything, we were able to look at synthetic examples

295
00:21:45,000 --> 00:21:49,680
and we were able to interpret those synthetic examples in the context of some of the basic

296
00:21:49,680 --> 00:21:51,560
physics that we had put in.

297
00:21:51,560 --> 00:21:56,120
And what it led us to discover was that in fact the neural network had not learned some

298
00:21:56,120 --> 00:21:59,280
absolutely out there pattern.

299
00:21:59,280 --> 00:22:04,200
The neural network had actually come very close to finding a physical quantity that we had

300
00:22:04,200 --> 00:22:05,840
heard of before.

301
00:22:05,840 --> 00:22:11,040
And that was essentially something called the Von Mises yield criterion.

302
00:22:11,040 --> 00:22:17,240
And this is a metric that exists in the literature and is very commonly used to explain the transition

303
00:22:17,240 --> 00:22:20,600
from elastic to plastic behavior in particular.

304
00:22:20,600 --> 00:22:23,400
And so that's one of the things that was really interesting about this study.

305
00:22:23,400 --> 00:22:26,880
We didn't just get a neural network, we didn't just develop a neural network that had greater

306
00:22:26,880 --> 00:22:27,880
predictive power.

307
00:22:27,880 --> 00:22:33,320
And it turned out we learned that the physics that might be controlling the triggering

308
00:22:33,320 --> 00:22:37,080
of aftershocks was different from what we thought it was before.

309
00:22:37,080 --> 00:22:39,480
That's really interesting.

310
00:22:39,480 --> 00:22:44,360
Well how did you go from the predictions that you were seeing, the inferences that you

311
00:22:44,360 --> 00:22:49,240
were seeing, to backing that into this Von Mises yield criterion?

312
00:22:49,240 --> 00:22:52,200
How did you see that that was there?

313
00:22:52,200 --> 00:22:55,080
Yeah, go ahead Phoebe.

314
00:22:55,080 --> 00:22:56,080
Oh yeah.

315
00:22:56,080 --> 00:23:01,280
So we looked at synthetic examples because it was kind of daunting to think about interpreting

316
00:23:01,280 --> 00:23:06,760
the predictions of the neural network for these very complex slip distributions that

317
00:23:06,760 --> 00:23:09,040
we got from these catalogs.

318
00:23:09,040 --> 00:23:14,920
So we looked at just for idealized earthquakes, what the neural network was predicting.

319
00:23:14,920 --> 00:23:22,040
And then we just simply compared that spatial pattern to the spatial patterns of a big suite

320
00:23:22,040 --> 00:23:29,000
of different stress metrics to see sort of which ones it was most highly correlated with.

321
00:23:29,000 --> 00:23:34,120
And it turned out that something like 98% of the variance in the neural network output

322
00:23:34,120 --> 00:23:37,080
could be explained by the Von Mises yield criterion.

323
00:23:37,080 --> 00:23:45,800
Okay, so you already had this criterion in mind as a contributor or a way to model these

324
00:23:45,800 --> 00:23:50,560
aftershocks going into this problem.

325
00:23:50,560 --> 00:23:56,760
I would say that there were a huge number of candidate functions that we would have

326
00:23:56,760 --> 00:23:57,760
considered.

327
00:23:57,760 --> 00:24:03,040
And my bet, I don't know about Phoebe, but my bet was that it was going to be some really

328
00:24:03,040 --> 00:24:09,240
complicated combination of four or five things that we're going to have to be combined

329
00:24:09,240 --> 00:24:15,200
in some weird nonlinear way to mimic what the neural network was doing.

330
00:24:15,200 --> 00:24:19,840
And as Phoebe mentioned to our surprise, what we found was that there was essentially

331
00:24:19,840 --> 00:24:24,880
one quantity that the neural network was coming close to discovering on its own.

332
00:24:24,880 --> 00:24:30,040
And in that sense, it's really interesting because a neural network is obviously incredibly

333
00:24:30,040 --> 00:24:34,120
good at finding very complicated nonlinear functions.

334
00:24:34,120 --> 00:24:39,920
And what it told us in this case was that to the extent that we trained it, it couldn't

335
00:24:39,920 --> 00:24:45,280
find anything that did too, too, too much better than a physical, a single physical quantity

336
00:24:45,280 --> 00:24:48,920
that actually exists.

337
00:24:48,920 --> 00:24:53,600
So Phoebe, what was the most challenging aspect of applying neural networks to this

338
00:24:53,600 --> 00:24:54,600
problem?

339
00:24:54,600 --> 00:25:02,400
The neural network really wasn't that challenging, it was the sort of data assembly, which was

340
00:25:02,400 --> 00:25:04,640
really, really a lot.

341
00:25:04,640 --> 00:25:10,560
And I think the generation of that, the generation of the data set was really the heavy lift

342
00:25:10,560 --> 00:25:18,560
of this because we had to read in all these complex slip distributions from this online

343
00:25:18,560 --> 00:25:21,640
catalog that many different authors had contributed to.

344
00:25:21,640 --> 00:25:28,720
So there were lots of sort of slight inconsistencies in the way that these data files were written.

345
00:25:28,720 --> 00:25:33,200
And so it had to be sort of very robust to those little inconsistencies.

346
00:25:33,200 --> 00:25:38,680
And then for each slip distribution, we had to calculate the stress changes in the vicinity

347
00:25:38,680 --> 00:25:44,040
and within 100 kilometers of each of these large main shocks and then incorporate another

348
00:25:44,040 --> 00:25:47,200
catalog of the aftershocks.

349
00:25:47,200 --> 00:25:51,800
So it was just a very large scale data assembly problem.

350
00:25:51,800 --> 00:25:59,040
The aftershocks that you were predicting, you were predicting those in two dimensional

351
00:25:59,040 --> 00:26:05,160
space, as opposed to kind of radio distance from the earthquake, presumably.

352
00:26:05,160 --> 00:26:09,920
And were you also trying to predict the magnitude of the aftershocks as well?

353
00:26:09,920 --> 00:26:11,880
No, that's what we're working on now.

354
00:26:11,880 --> 00:26:19,600
But we were doing it in the volume around the fault.

355
00:26:19,600 --> 00:26:24,280
So we actually, we decided to make this problem very easily tractable to start out with.

356
00:26:24,280 --> 00:26:30,240
We decided to frame this problem of explaining aftershock locations as a binary classification

357
00:26:30,240 --> 00:26:31,240
problem.

358
00:26:31,240 --> 00:26:37,600
And the way that we did that was we discretized the volume around each of these main shocks.

359
00:26:37,600 --> 00:26:42,200
And then framed to the problem is trying to classify each grid cell as either containing

360
00:26:42,200 --> 00:26:46,080
or not containing aftershocks.

361
00:26:46,080 --> 00:26:51,000
So that is the sort of way that we approached it, but we did do it not in two dimensions.

362
00:26:51,000 --> 00:26:53,200
We did it in the whole volume around the fault.

363
00:26:53,200 --> 00:26:55,440
Was that binary classification approach?

364
00:26:55,440 --> 00:26:59,840
Was that the first thing you decided to try and it worked or did you kind of iterate

365
00:26:59,840 --> 00:27:01,400
or evolve to that?

366
00:27:01,400 --> 00:27:02,880
That's the first thing we tried.

367
00:27:02,880 --> 00:27:05,160
And the results were just so interesting that we went with it.

368
00:27:05,160 --> 00:27:10,440
But we're certainly now trying lots of different other approaches to see what we can learn.

369
00:27:10,440 --> 00:27:12,000
And what are some of those?

370
00:27:12,000 --> 00:27:17,200
Well, we're looking at aftershock magnitudes right now.

371
00:27:17,200 --> 00:27:21,480
And it's all very preliminary, but it's really interesting to look at the kind of patterns

372
00:27:21,480 --> 00:27:26,840
that the neural network outputs and sort of use these approaches as kind of pattern synthesizers

373
00:27:26,840 --> 00:27:28,720
almost.

374
00:27:28,720 --> 00:27:34,600
And then I think going forward, also looking at aftershock density and eventually aftershock

375
00:27:34,600 --> 00:27:35,600
timing as well.

376
00:27:35,600 --> 00:27:44,080
I'm so curious about this, about how you incorporated the Von Mise's yield criterion.

377
00:27:44,080 --> 00:27:50,520
Was that incorporated into the neural network itself or the machine learning aspect of

378
00:27:50,520 --> 00:27:57,480
this project or was that secondary analysis that you just apply to the results?

379
00:27:57,480 --> 00:27:58,480
Option two.

380
00:27:58,480 --> 00:27:59,480
Got it.

381
00:27:59,480 --> 00:28:00,480
Okay.

382
00:28:00,480 --> 00:28:04,560
And that's why it's interesting because it's not a quantity that we gave it.

383
00:28:04,560 --> 00:28:09,120
And one could compute that quantity from the inputs, you essentially have to solve an

384
00:28:09,120 --> 00:28:15,040
eigenvalue problem, but it's that's why it was such a surprise to us.

385
00:28:15,040 --> 00:28:22,760
Yeah, it reminded me a little bit of a conversation with I'm forgetting his name.

386
00:28:22,760 --> 00:28:29,240
I believe at the University of Washington, where he was basically the outputs of this network

387
00:28:29,240 --> 00:28:35,680
were coefficients of a bunch of factors, so kind of linear kind of what you thought you

388
00:28:35,680 --> 00:28:39,360
would see like a linear combination of a bunch of different factors.

389
00:28:39,360 --> 00:28:46,720
And he used that to basically to derive mathematical equations for physical input parameter.

390
00:28:46,720 --> 00:28:47,720
Input parameters.

391
00:28:47,720 --> 00:28:48,720
Yeah.

392
00:28:48,720 --> 00:28:50,360
It was being used for laser tuning.

393
00:28:50,360 --> 00:28:51,360
That's great work.

394
00:28:51,360 --> 00:28:56,240
It's coming out of the University of Washington, Brunton is one of the people who's doing it.

395
00:28:56,240 --> 00:29:01,680
And that offers the prospect of using machine learning based techniques to constrain the

396
00:29:01,680 --> 00:29:06,160
physics of systems, even when we don't know the physics of a priori.

397
00:29:06,160 --> 00:29:10,280
I think that's a really promising technique.

398
00:29:10,280 --> 00:29:17,040
Not maybe for earthquake problems, but in particular for a lot of problems involving movement

399
00:29:17,040 --> 00:29:24,040
on the Earth's surface, whether it's groundwater or ice or things like that that are very complex

400
00:29:24,040 --> 00:29:25,040
media problems.

401
00:29:25,040 --> 00:29:28,800
I think the techniques, those sorts of techniques are going to be deeply powerful.

402
00:29:28,800 --> 00:29:29,800
Yeah.

403
00:29:29,800 --> 00:29:30,800
Yeah.

404
00:29:30,800 --> 00:29:37,120
It was Nathan Coots at a Coots at University of Washington for that particular one.

405
00:29:37,120 --> 00:29:38,360
Where do you see this going?

406
00:29:38,360 --> 00:29:44,960
Phoebe, you're continuing this looking at, you mentioned the magnitude of the aftershocks.

407
00:29:44,960 --> 00:29:50,480
You mentioned as well the time dependency of the aftershocks.

408
00:29:50,480 --> 00:29:56,320
Do you have a sense for how the modeling approach will need to change as you take on these

409
00:29:56,320 --> 00:29:57,320
new problems?

410
00:29:57,320 --> 00:30:02,000
I mean, we're very much in exploration mode right now.

411
00:30:02,000 --> 00:30:08,760
So I think we'll go to more complex network architectures if we need to.

412
00:30:08,760 --> 00:30:12,160
But yeah, it's just exciting to think about.

413
00:30:12,160 --> 00:30:17,680
And aftershock forecasting as a problem is sort of exciting to think about from a machine

414
00:30:17,680 --> 00:30:23,800
learning perspective because we've taken into account only static elastic stress changes.

415
00:30:23,800 --> 00:30:27,880
These sort of features of the network are only these static stress changes due to the

416
00:30:27,880 --> 00:30:29,040
main shocks.

417
00:30:29,040 --> 00:30:33,480
But there are a lot of other phenomena that may affect aftershock behavior, everything

418
00:30:33,480 --> 00:30:38,680
from the locations of existing geological structures in the region to poor or elastic

419
00:30:38,680 --> 00:30:41,760
stress changes, visco-elastic stress changes.

420
00:30:41,760 --> 00:30:46,440
So I think, I don't know if I'm going to agree with this, but I feel like the most important

421
00:30:46,440 --> 00:30:50,680
implication of this paper from my perspective is kind of the approach.

422
00:30:50,680 --> 00:30:54,600
Yeah, I think I agree with that entirely.

423
00:30:54,600 --> 00:30:59,680
I think I even zoom out a little bit and I think the approach is really important in

424
00:30:59,680 --> 00:31:06,680
so much as it provides a window into how we can rebuild a lot of earth science and potentially

425
00:31:06,680 --> 00:31:13,680
a lot of other sciences of complex systems in a way that doesn't require us to know everything

426
00:31:13,680 --> 00:31:15,880
about those systems beforehand.

427
00:31:15,880 --> 00:31:22,360
And what I mean is we spend a lot of time doing bottom-up modeling of all sorts of phenomena.

428
00:31:22,360 --> 00:31:25,960
And that's an excellent approach, but it requires that we know what's going on from the bottom

429
00:31:25,960 --> 00:31:26,960
up.

430
00:31:26,960 --> 00:31:29,800
And I think for a lot of problems that we face in earth science and in the world, what

431
00:31:29,800 --> 00:31:34,120
we really want to do first is try to make predictions.

432
00:31:34,120 --> 00:31:36,760
And that's where we can make a lot of progress here.

433
00:31:36,760 --> 00:31:40,640
We can try to make predictions, improve predictions of weather systems, climate systems, the

434
00:31:40,640 --> 00:31:44,800
earthquake system, environmental systems.

435
00:31:44,800 --> 00:31:51,440
And if we can exploit the ability to make predictions even in the absence of knowing what the

436
00:31:51,440 --> 00:31:57,360
first principles are, I think that deeply motivates us to go try and find out what those

437
00:31:57,360 --> 00:31:59,080
first principles are.

438
00:31:59,080 --> 00:32:01,280
That's one of the jobs of the scientist.

439
00:32:01,280 --> 00:32:06,360
And what we now have are tools that enable us to do that, not just by sitting around

440
00:32:06,360 --> 00:32:10,360
and thinking about what the first principles ought to be, but by giving us these networks

441
00:32:10,360 --> 00:32:14,560
that we can probe into and try to understand why they're predicting what they are.

442
00:32:14,560 --> 00:32:22,120
One of the questions that remains for me is in using this neural network approach, you've

443
00:32:22,120 --> 00:32:29,720
been able to pretty significantly reduce the time it takes to get new predictions to make

444
00:32:29,720 --> 00:32:37,680
new results by training a network to emulate the output of this traditional model that

445
00:32:37,680 --> 00:32:41,920
was hard to build and expensive to run.

446
00:32:41,920 --> 00:32:52,600
So you're using neural networks to model a model, so you still have to have the model.

447
00:32:52,600 --> 00:32:58,680
And I guess the question is, do you still need to develop these traditional models for

448
00:32:58,680 --> 00:33:02,280
each of the new problems that you want to try and solve?

449
00:33:02,280 --> 00:33:09,360
And does that remain a limiting factor in your ability to accelerate innovation as you

450
00:33:09,360 --> 00:33:11,000
were describing?

451
00:33:11,000 --> 00:33:14,960
I think that's a great question, and we've done it both ways.

452
00:33:14,960 --> 00:33:21,360
So when we accelerated this big HPC code, that did require Ford model.

453
00:33:21,360 --> 00:33:27,560
And in that sense, its primary utility is not insight, but simply rather speed.

454
00:33:27,560 --> 00:33:31,920
And it's nice too, because anyone can download this model and run it kind of on a laptop,

455
00:33:31,920 --> 00:33:35,400
instead of needing a data center to run it in, which is nice.

456
00:33:35,400 --> 00:33:39,720
But for the aftershock study that we did more recently, in that case, we didn't have

457
00:33:39,720 --> 00:33:43,520
to develop anything other than a kind of a classical machine learning model.

458
00:33:43,520 --> 00:33:48,600
And so I think insights are going to be gained in both ways, and it's going to help more

459
00:33:48,600 --> 00:33:50,800
people do a lot of science in both ways.

460
00:33:50,800 --> 00:33:58,560
Okay, and so just so that I understand that last point, the inputs to the aftershock model,

461
00:33:58,560 --> 00:34:03,440
I thought those features came from the traditional HPC model.

462
00:34:03,440 --> 00:34:05,200
Did I misunderstand that?

463
00:34:05,200 --> 00:34:08,240
Yeah, those are two separate studies by large.

464
00:34:08,240 --> 00:34:10,600
So got it, okay.

465
00:34:10,600 --> 00:34:14,600
One could have put in that HPC model, that's a really good idea, Sam.

466
00:34:14,600 --> 00:34:17,400
We should probably do that study.

467
00:34:17,400 --> 00:34:20,440
Hold on, I got to write that down.

468
00:34:20,440 --> 00:34:25,920
But we used a simpler version of it for this particular study, which was not time dependent.

469
00:34:25,920 --> 00:34:29,480
It was just a step function change in stress due to the earthquake itself.

470
00:34:29,480 --> 00:34:34,640
I'm sure I'll make even more sense when I listen to it again.

471
00:34:34,640 --> 00:34:40,960
But this has been really great. I appreciate you taking the time to share with me a bit

472
00:34:40,960 --> 00:34:42,560
of what you're working on.

473
00:34:42,560 --> 00:34:45,160
Any final words or thoughts?

474
00:34:45,160 --> 00:34:46,160
Not for me.

475
00:34:46,160 --> 00:34:47,160
Thank you so much for having us.

476
00:34:47,160 --> 00:34:48,760
Oh, Sam, this has been great.

477
00:34:48,760 --> 00:34:49,960
Thanks for having us.

478
00:34:49,960 --> 00:34:54,600
And I hope for a lot of earth and environmental science problems, there can be continued and

479
00:34:54,600 --> 00:35:00,440
greater dialogue between the ML community and the earth science community, because we

480
00:35:00,440 --> 00:35:04,120
have a lot of data from complex systems that we don't understand, and we really want

481
00:35:04,120 --> 00:35:06,880
to predict those to live in a better world.

482
00:35:06,880 --> 00:35:07,880
Awesome.

483
00:35:07,880 --> 00:35:09,880
Thanks so much, Phoebe and Brendan.

484
00:35:09,880 --> 00:35:10,880
Thanks, Sam.

485
00:35:10,880 --> 00:35:11,880
Thanks.

486
00:35:11,880 --> 00:35:17,800
All right, that's our show for today.

487
00:35:17,800 --> 00:35:23,800
To learn more about today's show, visit twomolai.com slash shows.

488
00:35:23,800 --> 00:35:29,800
Make sure you visit twomolcon.com slash videos to secure your access to Twomolcon video content

489
00:35:29,800 --> 00:35:30,800
now.

490
00:35:30,800 --> 00:35:40,800
Peace.

