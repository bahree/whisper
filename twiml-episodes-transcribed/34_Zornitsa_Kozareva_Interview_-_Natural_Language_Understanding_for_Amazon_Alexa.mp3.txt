Hello and welcome to another episode of Tumultalk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington.
I apologize in advance for a longer than usual intro, but we've got a bunch of news and
announcements that we wanted to share this week.
Thanks to everyone that listened to, shared, and commented on last week's show.
Based on your feedback so far, it's pretty clear you're enjoying both the Industrial
AI series as well as our more technical Nerd Alert shows.
I am too, so you'll definitely see more of both.
This week though, we're starting a two week break from the Industrial AI series.
I've got a great show for you today and then next week the week of July 3rd will
be foregoing our usual Friday release and experimenting with a shift to a Monday release
schedule for at least the rest of the summer.
When this podcast drops, I'll be in New York City for the O'Reilly AI conference where
I'll be interviewing speakers like Douglas Eck from Google Brain, Rana Alcaloubi from
Effectiva, Ben Vigota from Gamalon, and Naveen Rao of Intel Nirvana.
Our O'Reilly AI series will be posted on Monday July 10th for your binge listening pleasure.
The following week I'm in Germany, in Hamburg, Berlin, and possibly Munich.
If you're in or near one of those cities and you'd like to connect, definitely give me
a shout out.
As we've mentioned over the past few weeks, we've been planning and now have finalized
our very first Twimmel Happy Hour.
We've partnered with our friends from the NYAI meetup group and we'd love for you to
meet us at Amesworth Midtown and New York City on Thursday, June 29th, starting at 6pm
right after the O'Reilly conference for a few hours of drinks, conversations, and networking.
I'm looking forward to being back in my hometown and sharing a drink with those of you who
can make it.
Make sure to RSVP at twimmelai.com slash NY meetup to let us know you're coming.
And now about today's show.
Our guest this week is Zornizza Kozarva, Manager of Machine Learning with Amazon Web
Services Deep Learning, where she leads a group focused on natural language processing
and dialogue systems for products like Alexa and Lex, the latter of which we discussed
in the podcast.
We spend most of our time talking through the architecture of modern natural language
understanding systems, including the role of deep learning, and some of the various ways
folks are working to overcome the challenges in the field, such as understanding human
intent.
If you're interested in this field, she mentions the AWS chatbot challenge, which you've
still got a couple more weeks to participate in, of course, a link will be in the show notes,
which will be posted at twimmelai.com slash talk slash 30.
I had a ton of fun chatting with Zornizza and learned a bunch, and we couldn't wait
to share this conversation with you.
Enjoy.
All right, everyone, I am on the line with Zornizza Kozarva, Zornizza is a manager with
AWS deep learning, and we are going to be talking about deep learning and natural language
understanding.
And I'm super excited to have her on the line.
How are you Zornizza?
Likewise, thank you Sam, it's a great pleasure to be here and to be part of the show.
I'm doing really well.
Awesome, awesome.
Well, why don't we get started by having you tell us a little bit about your background
and how you got to where you are?
Yes, so currently I'm a manager of the AWS deep learning group at Amazon that focuses
on natural language processing and dialogue systems.
My background and PhDs are in the field of natural language processing, which focuses
on doing systems that understand what humans mean.
For a couple of years, I wore an academic hat at the University of South and California.
I was an assistant professor there for the computer science department, and I focused
on different types of research funded by DARPA and IRPA.
And after that, I moved to industry where I decided to tackle the same challenges, but
at a much larger scale and with a bigger impact to humans and society.
Hmm, do you currently do research or are you primarily focused on product oriented work?
I'm focused on both, like we do a lot of product development inside the Amazon, but
at the same time, I'm making sure that I continue participating and serving the scientific
community.
We do have scientific work as well as I regularly serve on the program committees and
I'm an area chair.
So I'm trying to do both, but at work, definitely the focus is on building products.
Got it.
Got it.
You're currently working on the deep learning and natural language understanding systems
that power Amazon Alexa and Amazon Lex.
I'm pretty sure everyone knows what Amazon Alexa is.
We've talked about it a bunch of times here on the show, and in fact, I demonstrated a
few times how to access the show on Alexa.
So folks are familiar with that, but I'm not sure everyone knows what Amazon Lex is.
So can you maybe give us a high level overview of that service?
It was just announced last year at ReInvent, right?
That is absolutely right.
Well, let me walk you through like how we ended up with Amazon Lex.
So if you think about it, we live in the artificial intelligence era, and we see the development
of very smart systems from self-driving cars to Internet of things.
But at the core, since this conversation was system, that enabled the communications
between machines and humans.
And it has been a dream for developers and for sciences in general to be able to build
such assistance.
But if you take any developer and they have to build such kind of a system, it requires
them to know a lot about natural language understanding and speech recognition, which
are very tough.
And you either should have like a PhD or should have spent significant amount in these
areas.
Developer had to worry about how do I build systems that are really scalable, how do I
enable testing and make sure that my bots are going to be working what the users need,
how to do authentication, how to integrate the business logic, and all of these challenges
were kind of blocking the development at a faster space of all these applications.
So we introduce Amazon Lex, which is a new service for building conversation interfaces
for your apps using voice and text.
And there are multiple benefits to that.
One is like Amazon Lex is very easy to use.
It's built for developers.
And we at Amazon do all the heavy lifting in terms of the infrastructure we take care
of the models.
And the developer just has to focus on what is their customer use case and how they want
these applications to look.
The best part is like we have very high quality text and speech language understanding.
And they are powered by the same deep learning technology as Alexa.
So that's why we have the short legs, it's short for Alexa.
The good part is also that any developer can seamlessly deploy and scale.
If you build your application, for example, for specific platform, like let's say, Facebook
Messenger, you can very easily port it to many other platforms and you don't have to worry
about that.
And at the same time, we have like AWS mobile hub integration that allows you to do many
things like simple, nice data, analyze user behavior, track retention, integrate your
bot and so many, many things.
So this new service Amazon Lex allows people to focus on what matters to them and literally
like so of a particular user need, whether it's booking a hotel or it's opening a bank
account.
And the most exciting part is like we're organizing a challenge right now.
We started in April and it will end in July.
So those folks that are really passionate and want to build their bots, I encourage them
to have a look at our web page and just register for the challenge.
Oh, is that some kind of is it like a prize for the best Amazon Lex app or something?
Yeah, there are different rewards, there's the monetary rewards, there's the AWS credits
and also folks can come to reinvent 2017, there will be ticket.
And as I say, the focus is like build a chatbot that engages users and at the same time fulfills
a specific need that you have like booking a hotel or any other thing that you might
have in mind.
Okay.
Nice.
So with that in mind, let's maybe talk about what are some of the biggest challenges
in working on systems like this, like what are you currently researching?
Well, my focus is in natural language processing and the most hard part is to build systems
that actually understand what the humans mean.
This involves like can we understand what intense people have in mind, can we identify the
slots that enable us and understand how these intense should be fulfilled.
And for most people that haven't worked in this area, it sounds like oh, that's pretty
straightforward, but actually language is very ambiguous and very, very hard to understand.
If we deal with very explicit intents, let's say cancel travel to Miami, I just literally
said every single thing that I'm planning to do, cancel with my intent and Miami is my destination.
But imagine I'm chatting with someone and they're asking me, are you coming to the board
a party?
And I suddenly say, I'm on my way.
If the conversational system pops in and says, hey, Zornica, should I send an Uber your
way?
That's amazing.
So, building such applications is really hard.
It requires a system to understand implicit intents, which are very hard to detect.
It requires the system to know what your user preferences are.
Maybe I'm using specific means of transportation.
The system has learned it over time.
And it's making automatically the recommendation.
And the ability to generate such kind of replies automatically instead of using templates
that are already pre-specified, these are all very, very complex problems that are open
ended.
And we are continuing to invest both in terms of sciences as well as in industry.
How do you solve and make systems being able to handle all this complexity?
Hmm.
So maybe what's a specific area that you've been focusing on there in terms of your research?
My research focuses on building this natural language understanding capability.
Anyone, anytime an utterance that comes in, we focus on extracting those slots to fill
in that template as well as understanding what these intents are, such that when you pass
that information to a dialogue manager or a component that communicates with the backends,
we understand like what the human meant and the more correctly you extract such information
and the most accurately you populate it in these specific templates, then the better your
system will be.
And humans don't have to repeat the same question over and over again.
You mentioned a dialogue manager.
Did these systems have standard components, independent of the implementation to all of
these systems have dialogue managers and what is the general architecture of these types
of systems?
Yeah, that's an excellent question.
While a conversational assistant can live in different shapes and forms like either on
your phone or in your home device, even in your car, the common part is like they have
very standard, like processing blocks, input is like either speech or text.
And the first component that gets hit is the so-called natural language understanding component.
That is the piece that focuses on understanding the intents and the slots of the users.
Once that information is extracted, it gets passed to the dialogue manager.
The task is to take these pieces of information, send them to the backends and a backend
think about it.
If you want to book, let's say, a flight.
Maybe I just say book me a flight to Miami.
Once extracted the Miami term and you pass it to the dialogue manager, it communicates
to the backend, let's say your favorite travel website.
And it says, well, now I need to know what date you will be traveling, do you have any
price restrictions, do you want it to be like a direct flight or not?
It sends back all this information to the dialogue manager says, hey, I need to have all
this information filled.
The dialogue manager passes the slots and then they hit a natural language generation
component that says back to the user, hey, can you tell me like, do you want a direct flight
and do you have any price constraints?
That output could be both text or it could be speech.
It gets sends back to the user and then the user says, well, actually, I don't have
any constraints.
Find me, let's say, the cheapest flight.
So this loop, constant loop between these three components, natural language understanding,
dialogue manager, natural language generation is what drives the whole conversation between
a system and a human.
How you implement them, it depends on you as a policy developer.
How you decide what models to include.
But on the high level, these are kind of like the three building blocks that act like
everyone has to focus us on building.
Okay.
Okay.
And then even within the NLU component there, we can even break that down further.
I know or I recall that the Google a while ago, I guess, around a year ago announced like
an open source parser, their parsing MacPars face rule.
And that's just one of the different pieces of the NLU component itself.
Like what does that part look like?
Well, think about it that the natural language understanding components there, as I said,
they're different and depends what your need is.
Okay.
And the tools that you quoted, they're kind of like a high level dependency parser or
just a parser.
You can use that information to facilitate how to build this kind of natural language understanding
components.
But at the core, it's like you have to think about how to define your slots or how do
you define your semantics, how do you define the space over which your system will operate
and would semantically understand what a human means.
And these things typically are called slots, they're the entities and semantic bits of information.
That capture what our request is.
You can build them from scratch.
You just need to know like what are the right machine learning models for that.
Or if you want, as I say in Amazon Lex, we have like pre-built capabilities such that
people can just choose from a drop-down menu.
So that enables like a person who knows nothing about machine learning to be able to build
them.
For anyone that is from natural language crossing field and has dealt and worked in semantics
and information extraction, it is much easier for people with such kind of skill set to
be able to build their component from scratch and to know how to define these semantics
or how to define the space that the machine learning system should operate over and make predictions.
And how do you characterize that space?
You know, you're starting with typically some audio waveform, like what's the process
for getting that into some parsed set of slots that you can then operate on further
back in the system.
Uh-huh, yeah.
Once you have the speech, as I said, the input could be both speech that you transfer from
speech to text or it could be the text itself.
Think about it like a text messaging.
Once you have that text in, what you do is like you have to make the design of what the
intents and the slots are.
And I know sometimes people come and ask me, well, where do these intents and slots come
from?
And the answer is they're the designer's choice.
You can take them from an existing large ontology, just pick the pieces that you care
about.
Let's say in travel, you might care about cities and countries and so on.
You can automatically learn them from a lot of unlabeled text or you can even manually
create them.
So how do these slots look like?
Let's say we're building a shopping bot.
Then we can decide that our slots are the things that are most important, products and vendors
and brands and models and product families.
And the intents or which are the actions on top of which we can do operations are buying
them, selling them, recommending them, tracking them.
What we do is like we formulate this type of problems as a sequence prediction.
Like if I give you these categories, the products and the brands and I give you some text of
length M, like can you find segments inside the text that can be labeled with these specific
categories?
And the moment you do that and you're having this rich representation that you're caring
about and that your dialogue system can ingest and actually build on top of.
So you have to annotate your data in the correct way, meaning if I say purchase added
as rules, quick to and Nike Pegasus, each word or how's it segments of words, they get
tacked with this specific information.
And then the machines just ingest it such that they can build prediction models on top
of that.
And that's one way you can do it.
Yeah.
Okay.
So one thing I'm inferring from your description is that you consider or it's generally
considered that the speech recognition component of, you know, a broader system like Lex and
the NLU component of a broader system like, you know, system are like two different things.
Is that the way you tend to think of it?
Like this, you're getting speech that's already, you know, turned from audio signals to some
set of symbols, whether that's, you know, language or something else.
And then that's what you're working to understand.
Is that right?
That is correct.
Yes.
You think of them like different building blocks that you need to piece together in the
right way, such that you can do the complex system that is going to be able to drive
this dialogue.
So in my case, I just focus on the natural language processing component.
And we have amazing colleagues that do the same thing.
They focus on speech or they focus on generating from speech to text, yes.
So when you were talking about the intense and the slots and looking at that problem as
predicting a sequence that makes me think of deep learning models like LSTMs, do those
would come into play there, talk about the impact of deep learning on all this.
Yes.
Absolutely.
You have very good intuition and LSTMs are very useful in solving such type of a problem.
It's a structure prediction problem.
And as I say, until recently, people used to employ a lot of the traditional supervised
learning.
They picked their favorite algorithm being a CRF dagger or being like learning to search
algorithm.
And most of the focus was on how to do feature engineering and find what are the right
features and how do we iterate over those features.
But now with the big wave of deep learning, we see that one can build such kind of systems.
If you're an expert in the field, one can do them much faster in terms of like picking
the right algorithm, you can train your word embeddings on a much larger, unlabeled data
and also get much more powerful results.
So LSTM is great for solving this kind of structure prediction problem.
And typically we see very good results if you have the last layer with conditional random
fields.
Can you elaborate on that?
What are conditional random fields?
Yes, it's very, very standard.
They're like discriminative classifiers.
It's very, very standard built for many decades and using this kind of task.
Their use, as I say, for sequence prediction, can encode relationships between the observed
data, like typically the current and the previous word, does not model long-range dependencies.
While in the LSTM, you have these deep learning models, which are more powerful, as I said,
they can learn powerful representation given enough data.
They can capture the long-term dependencies, not only in a sentence, but even they can
span between sentences and paragraphs.
And as I said, the best part is like you don't have to focus on feature engineering.
You can just pass the raw data as input.
So if you're looking to build a system like this, what are the primary considerations that
you need to think about to architect it correctly and to build a system that meets a specific
set of needs?
I would say if you are familiar with machine learning, if you use these sets of methods or
methods that are appropriate for structure prediction, your problem will be kind of solved.
But the part that one really needs to focus on is the data.
Because the data is going to drive the quality of how your system performs and behaves.
And the semantic representation, this means as like how are these labeled spaces going
to look like?
In terms of like the machine learning algorithm themselves, there's like plenty of open-source
platforms that people can use at Amazon, we have the MXNet platform, which is an open-source
machine learning platform, we have TensorFlow.
People have a wide variety of choosing which machine learning toolbox they can use.
And as I said, that's pretty easy to pick up.
But the hardest part is like if you don't know how to get your data, if you don't know
how to represent it right, then even if you have the best toolbox on earth, you're
only able to build your applications in the right way.
So I encourage people to look at both things, not only at the machine learning side, but
also on the side of like, how do I design my system?
How do I collect the data?
How do I drive all of these processes together?
And you said making sure you represent your data right is one of the big challenges.
What exactly does that mean and what are the considerations there?
Yes, well, as I said, the biggest problem is like semantic understanding of like what
humans mean and how should the computer in cold and understand that.
And that's a challenge.
And it's been a challenge for many years.
There are theories, but yet building such a system that understands us humans is very
challenging.
So we have these basic representations that can, to some extent, work, but that doesn't
mean that really understand what a human means.
It's very hard for the systems to pick up like sarcasm.
How do you take it, represent it and even like model it?
These are much, much harder things we have to account for.
Right now we have these basic, more flatter or slightly more structured representations,
but that's definitely not the way to go forward.
And I'm trying to put that into context.
If I'm thinking through building out a system to understand language and you're advising
me to start with the data collection and representation, I get collection, obviously it's going
to be, or depending on the situation, it may be difficult to collect the data, but assume
that I can collect a bunch of data, say, it's assumed it's in text form from bot requests
or something like that.
The next step you're saying is the representation.
Where does representation come in before I start throwing my data at the machine learning
algorithms and put another way, like part of what I'm expecting or hoping the machine
learning algorithms to do is to kind of parse the data and help me figure out how to represent
it.
Is that right?
Well, there are different ways one can do it.
I'm also hearing a good thought from your side, which is like, can you even build the machines
or can you build a machine, machine learning algorithms that can, given just some chat,
but can automatically infer what the slots should be, what the intent should be.
And definitely that's possible.
It's not going to be very accurate because it's almost like when you do clustering over
documents, right, you can end up with these different representations.
And the same thing is going to happen here.
So that's why typically you start with the notions of you defining your, your slots
and intents.
So when you build the machine learning applications, typically you have the classes, right, that
you want to output, meaning if I give you a large document collection and I say, well,
I just want to know is this low, is this medicine and so on.
Okay.
You already have these categories.
So it's the same thing for building these natural language understanding components.
You need to know what categories or what is your space over which you're going to operate.
If you try to learn that automatically, as I say, you're going to end up with these
coarse green, very high level, you'll define like meanings and they won't be sufficient
for you to build a meaningful application, but if you sit down and actually write it yourself,
you have much higher chance because think about it just to build a flight booking system.
If you open the different travel website and you see like what are the minimum sets of
inputs they require you from the destination to destination, time, location, and so on.
And you just define literally that to be your space or the slots over which the system
would operate.
You're going to be able to build much, much better system.
And to be honest, yes, one of the of the pushes in science, both in terms of industries,
like can you build systems or conversational assistance that can learn from human interactions
and that you can teach right now.
If you ask something to a system to tell you, I'm sorry, I'm not quite sure about that
or like I'm still learning.
In reality, it will be really awesome if you make those system learn such that anything
you say and based on how people have replied, plus like request for new, let's say for
new information that you haven't seen before, if the system can learn it and start asking
about it or see many people are talking about it, then this is the place where we want
to go because humans cannot encode all the knowledge in the world.
And where I'm headed at is like, if you look at how all of these applications are built,
they are operated over different domains, which means you have a movie domain, you have
like music domain, shopping domain and so on.
And each of these domains have their semantic representations or so called slots and templates
that you can call and you can fill in.
But a human cannot sit down and represents the whole world in these frames, right?
It's right.
I'm consuming its labor intensive.
It doesn't scale.
And most importantly, it's like we humans don't operate for domains, right?
I can say order my pizza and book a flight, right?
I just request two different domains and express multiple intents.
And that's the main reasons why it's, as I say, like the current technology is great
and it's all real use cases.
But there's like many, many more things that have to be done and we have to focus on figuring
out how do you build an end-to-end system that can operate for any domain that can operate
for different languages and that can continue to help assist in us.
And so what's the state of the art there are people building a bunch of individual domain
knowledge and then using some deep neural network to kind of identify which domain is
being asked about in a particular utterance or are they building kind of bigger, flatter
models?
Is there any particular direction now?
I think we have one of the most common trends that I have been seeing over time in this
field is for people to try to focus on the different domains and for each domain focus
on the slots.
And sometimes I've seen people trying to transfer knowledge from similar domains, let's
say I know everything about reserving restaurants and I have a little bit of data for hotels.
Now can I learn how to reserve hotels by knowing how to reserve restaurants?
So that's the most common approach.
And as I said, it's a challenge because it doesn't scale, it just doesn't work.
The ideal scenario is like, can you build a system that doesn't have these boundaries
and can just like operate over anything that we have in mind?
Yes, in terms of like what algorithms, there's a lot of people do use deep learning as
I say, different parts of the components.
Yeah.
Yeah, and I guess deep learning isn't really fundamental to my question as much as is
the best practice to or you know, is the research frontier.
If I'm trying to build a system that can handle hotel reservations and pizza orders and
buying cars and you know, multiple things, am I likely to be better off getting a bunch
of data for each of those building out specialized representations for each of those and training
models that perform well for each and then use some kind of discriminator that can figure
out which model to use or is there some other kind of technique that at that metal level,
you know, to pull it all together?
Yeah, if you're building a production system that has to work and satisfy user and customer
needs, then that's the right way to go.
It will guarantee you one that your system is going to be doing the right things and
it will have higher precision, which is what's very important for users.
Nobody likes to hear the same question twice like, what did you mean?
I didn't get your question.
In terms of research, I think people can have much more flexibility and they can use
all kinds of techniques.
They can use like transfer learning to see like, how do I transfer my bot that I built?
Let's say for restaurants into flights or into something else and I think these kind of
transfers are more successful for things that are closer to each other.
There are some people that I have just seen on general like entity recognition and sentiment
analysis, but the thing is like they've been evaluated on much smaller data.
So ideally what I really would like to see is like people doing it on a larger scale with
lots of data and having many of these domains and actually showing like how possible it is
and where are going to be like the biggest challenges.
Also you can build the system in a different way like if you want, you can do these individual
components and then you have something on the top that actually ranks your results and
tells you what's the most likely domain sets of intents and also slots or you can build
one big giant neural network architecture that you can incorporate.
Let's say if you have your LSTM, you can make it learn both your slots.
It can learn your intent as well as the domain and over time with a lot of data, you can
learn these constraints that in certain domain, let's say like movies, it's very less likely
that I do an action that is let's say possible for a different.
I would say that if you are building a real application, then stick to what works.
If you are passionate about exploring and pushing the frontiers and doing like a lot of research,
then definitely there are many more like open doors in terms of like trying to see like
where is the boundaries and how far can we get.
Hmm, great, great.
Let's talk about the data a little bit.
Are there standard data sets that folks are using for these kinds of problems like there
are in the image recognition side of things?
I've seen like there is some challenges that are called the dialog state tracking challenge.
They do focus a lot on the dialog manager component and there are also these attempts
with these different same data challenge that people can do the knowledge transfer that
I was talking about.
I've seen people use different types like Ubuntu chats and that's like if you just want
to build like a high level chatbot system that doesn't, it's not go oriented.
And then I've seen a lot of work from Facebook AI that drives in that area and they have
a data set that's called BABY.
It's called what?
I think BABY.
Okay.
I'm not sure how the right pronunciation is BABY, but it has like different tasks like
20 tasks and it's annotated and people do use it for conducting research.
So if somebody wants to like start in that area and wants to play around, maybe these
are good places.
As I said, it depends like you just want to build like a chatbot that doesn't have any
goal for a few months or do you want to focus on building a real working system that is
going to fulfill your goals for you as a human.
Hmm.
And now if you're building a system like this and none of these data sets work for you,
you've got to figure out some place to find that data.
And I noted that you've got some research experience into large scale knowledge extraction
from the web.
Are there any techniques there that you might use to help you collect the data set?
Well, you can do different things.
People have invested a lot of effort in building this wizard of all systems.
It means like you can build very quick application that simulates a scenario.
Let's say one person will pretend he is the booking expert and the other one will be
the customer and they can drive the conversation on their own and the booking agent will just
try to fulfill that intent.
So if you create such scenarios and record all the data with different people interacting,
that could be one easy quick way to start building your application.
Another way could be that you can do that.
And just to drill into that, what you're suggesting there is that you define out these
scenarios and then are you thinking like go to some place like a mechanical Turk and
say, hey, you know, you play the booking agent and you play the customer and you get kind
of random people to explore these scenarios or do you mean something else by that?
Yeah, it's perfectly feasible.
Yes, you can exactly do that.
Like kind of decide what you what the application you want to build.
You can set up everything being on Amazon Turk or being even let's say you're a startup
and you don't want this information to be leaking.
Then you can set it up.
Even build your own very quick in-house application that just can record these conversations.
And once you have that data, then you can start building, let's say, a first prototype.
You can even launch it and then test it on real users.
When the data comes back, you can start iterating.
And once you have this kind of nice loop, you can keep iterating and improving what you
have done.
That's one way.
Like completely even focus on, let's say, like just Amazon Mechanical Turk.
You can either have predefined questions, but then the conversation will become very artificial
or you can play the game that you have just explained.
And then the on your questions, like, can we do something if I give you the whole web?
What can I do to facilitate the building of such systems?
Like at the core of these systems is like a knowledge component.
You have to have knowledge bases that help you extract this information much more accurately
that help you get world knowledge.
And one way is that you can get that world knowledge if you don't have your own knowledge
graph.
You can get that world knowledge by literally extracting all of that information from tons
of web pages on different topics.
And you can integrate it inside your models, right?
Either as additional features or additional signal and the help drive these applications
to be more precise and more accurate.
Interesting, interesting.
If you did a talk on dialogue systems, actually that's coming up.
It looks like is that in the same kind of domain of the things that we've been talking about
as far or there are different aspects to that?
Yes, that was like, have the core component of like, if people know machine learning and
they want to build everything from scratch, their cells is like, what components should
they focus on?
What machine learning algorithm they should work like pick up and then how to get the
data?
And at the same time, it's like, what's the expectation?
Some people have the expectations that this system should be like 99% accurate.
And I was like, oh, not really, we're doing these baby steps in the field for some very
specific basic things.
You have high accuracy, but that doesn't mean that you have solved the problem.
It just shows that the solution of such task is possible, but doesn't mean that it's a
solve problem.
So, you bring up accuracy and that raises some interesting questions, I think, on the
speech recognition side, you know, there are some standardized measures of accuracy and
you have folks like, you know, Microsoft and others reporting there, their accuracy
and, in fact, Microsoft recently reported, or they're, I think, a number of organizations
have recently reported human parity and recognition.
It seems like it's maybe not quite as straightforward to report accuracy on the NLU side of things.
Is that true or no?
Well, actually, no, think about it like when we annotate the data, you typically have
your training set, your development set, and you have your test set.
And for each of those, we always measure what is your precision, what is your recall,
what's your F score?
We go on the level of each of these slots, and we also record other things like, how hard
is the task for a human?
We measure the cap agreement, the Krippendorf Alpha, and the idea is that if you see how hard
the task is for a human, then expect your system to be 10% less than that, right?
So if two humans have really hard time annotating your data with your representations, this
means that it will be extremely hard also for that system to learn about it.
My point was that building the natural language understanding systems is much harder than
building a, let's say, image understanding system.
And the challenge is like language is very ambiguous.
We have to deal with a lot of slang that people might be using, or specific like metaphors
that people have in mind.
And so the building of a system that can understand and operate on top of that, right?
It's very hard.
That's why I was saying that it's important to have your knowledge basis hooked up so
that the system can get, think about like a cheat sheet and some extra information that
can help it facilitate, understand better, like what we mean.
Right.
Yeah.
I think that's what I was getting at with the accuracy question.
I can think of many occasions often happening right here at home where I definitely understand
the words, but I'm not sure I totally understand the meaning.
And while I can certainly grade my system's accuracy relative to some label set, it's
harder to capture even the right metrics in terms of, you mentioned this earlier in
the conversation, like, do we even have representation for nuance and sarcasm and things
like that, that, you know, if I had some uber metric of, hey, does, you know, does the system
understand what was being said, it just seems way more difficult to really capture what
that even means.
Yes.
And, you know, like back in the day when I was like with my academic head, my head that
grant from my ARPA, that focused on, can you build systems that understand metaphors
for four different languages, it was Arabic, Russian, Spanish and English, and what
I mean, metaphors.
Metaphors.
Metaphors, got it.
Yes.
And you have these four different languages like Spanish and Russian and Arabic and English.
And the idea is like, we just pass any text, could be news, can you find those metaphors,
can you interpret them and also assign the sentiment that the person had when he said that
metaphor, right?
So, if I say my lawyer is a shark, so you know that, yeah, sharks are vicious, but it's
really good for me to have a lawyer like that because it means that that the lawyer is
going to do the right thing and they're going to protect you.
But if I'm saying this to you, then for you, that is going to have a negative connotation,
right?
So, it was really hard to build systems that just given any three texts in these languages,
it can identify the metaphoric expression interpret what they mean.
And yeah, we've focused like two years on just building and trying to solve it.
And unlike other tasks in each language processing that have matured over time and that have
like significantly higher performance, building such kind of system that understand metaphors
was very hard.
It's like in the 50 percent, it's really challenging.
Yeah, again, going back to my previous point, it's even hard to, before we get to building
the system to think about what performance even means in this context, right?
I think even that statement about my lawyer as a shark can be, you know, can probably have
either positive or negative sentiment depending on the circumstance.
That is correct, exactly.
But you know, the best part is like, as I say, language is very hard and there are a lot
of people trying to solve these challenging tasks for some of them, we've made tremendous
progress, others are open-ended and we're still working on them.
And that's what excites me because it means that we have a lot of work to do, a lot of
efforts to put into building their systems.
Absolutely, absolutely.
What are some of the other areas that you're tracking that you're seeing exciting work
happening?
In natural language processing, there is like a lot of focus on question answering.
And the building, both type of systems, like if I give you a knowledge base, can you
do question answering over this knowledge base and do inferences on top of that?
And we have seen open-ended question answering where you have a document and just somebody
comes in and types the question like this on the spur of the moment, then can we find
the corresponding answer in that document?
They're both very challenging and exciting.
So this year I will area chair for ACA, which is association of computational linguistics
conference, our top tier one conference and like the biggest natural language processing
conferences and the areas that we're training a lot for information extraction and question
answering.
And there's like a lot of effort, some challenges that are coming up and people have just
been given the data and given the ability to think about how to innovate and how to solve
them.
I know Kora is among the folks that have data sets for question answering and there's
another popular one, at least another popular one that I'm forgetting the name of right
now.
Yes, that's absolutely correct.
Like Kora has a nice data set.
You have squat, who is coming, does a data set developed by University of Stanford,
Percy Liang's group?
And that's one of the, it's a very good data set.
It has a lot of training data, magnitude's larger than previous data set.
And it has different types of categories of questions.
So that kind of mostly simulates or approximate real case scenario.
And that data set focuses on the second question answering type that I was talking about.
You have like documents and then you have a question like can you find the answer in
the document?
And it's much challenging because people can ask the question in any form using different
part of phrases.
And then finding these pants with the correct answer is way harder than traversing a
real knowledge base.
And so are the core techniques that are used for these two different types of question
answering tasks the same or are they dramatically different?
Well, I've seen people that use, let's say, knowledge graphs to answer questions to
use more graph based algorithms.
And I'm seeing a lot of trends with the deep learning for the second kind of problem
with this squat data set, which is more like machine comprehension from text.
And there are people have different architectures of how they solve it.
But let's say for the machine comprehension one, I've seen very common is like people
try to represent the question into some kind of an embedding or vector space.
And then they have the document they try to use like attention mechanism on top to pick
up entities or pick up spans that could be good match for the answer.
And then they kind of try to have some similarity between the question and the answer.
So yes, both of those different types of question answering have wide variety of methods
have been employed.
But these are two things that I'm noticing are kind of trending when people publish their
work.
Hmm.
So for the knowledge graphs, I'm imagining there that you're using some kind of technique
to identify, well, maybe a precursor to that is what types of representations are using
typically on your documents that you're trying to do this question answering for.
Are you doing things like trying to do semantics and identify nouns and verbs and that kind
of structure?
Or are you operating on a lower level than that?
Oh, actually, yeah, that's what I was saying that this data set, I like it because one,
the magnitude is much larger than any previous data set and two, you have to focus on extracting
different types of bits and pieces.
And sometimes they could be just a word of phrase.
They could be combinations of like just like a non phrase or much harder.
It's not like a single answer like when was Barack Obama born and you just say the year,
right?
That's what a knowledge like a question answering over knowledge base does.
This one is more open and that if I say like, hey, what were the symptoms for people who
have cardiac arrest, maybe the answers were contained in different paragraphs and you
have to find those different paragraphs and the exact spans with the answer.
And that's what makes it much more challenging, but at the same time, much more useful because
most of the information that we need and the questions that we have, they lay in this unlabeled
documents that are being on the web or that you as a corporation might have and you might
want to just search for the information.
So I personally prefer this type of question answering work because it's, as I say, more
real case scenario.
And second, it's like more useful to us.
And so just to take a step back.
So with these question answering data sets in particular, the Stanford one, the data set
includes the base document and then a set of questions and answers from that document
and are there multiple answers for each question and to what degree to the questions overlap?
Yes, that's a great question.
So typically you have something like one question like, I'm just reading from the official
paper that was published, which governing bodies have veto power and then you have a whole
document that, let's say, is talking about something, right?
There is one specific sentence that can have this answer, for example, the European Parliament
and the Council of the European Union have powers over the month and veto and so on.
So given that question, then the idea and this document idea is like, can you find the
paragraph and more specifically the sentence that contains this specific type of an answer?
And I haven't like the deeper in terms of like understanding like how many of the question
the exact, let's say, question could be found with the exact answer inside.
But I do know that the creators made sure that when that data was annotated, that they
asked people is like, if you read this article, can you ask a question using paraphrases,
which means like different words or different ways that you can ask about it.
So it doesn't have to be the exact same, how to say exact same phrasing.
If you had the exact same phrasing, then the problem is much more simple, right?
But definitely that's how to say much more challenging dataset, the way it was created
and the way it's annotated.
So I think they did a good job on making sure it's created the right way.
It has different complexity, yeah.
So that helps me, that helps clarify for me like what you're fundamentally trying to
do is you're given a question and you're trying to essentially index into this document,
the sentence, the particular sentence that answers it, which is a totally different
problem than, or at least it is a more narrow problem than synthesizing an answer based
on multiple sentences in the document or a summary type of problem or trying to pull
pieces from two different sentences that are required to formulate an answer.
Yes, that's more like a summarization what you are describing.
And there the goal is a little bit different if I give you one or multiple news articles
about the same topic is like can you find sentences such that you can summarize the text
in a much more compact fashion.
And for a human, the moment they read it, they get the gist of the facts and at the same
time the whole summary is coherent and has natural reading flow.
There are dataset that has been created on that area is just the problem is called more
like summarization, right?
Right, but there's some overlap, right?
So if you just to construct a simple example, if I've got a document that says somewhere
in it, you know, my favorite color is red, you know, and then the next sentence is, you
know, but I also like blue and yellow, right?
And if you ask a question, what colors do I like?
There's got to be some synthesis or summarization in there somewhere and, you know, A, is that,
you know, typically part of the question answering, is there a, you know, a set of work in question
answering that's looking at those kind of more complex scenarios or are there folks that
are trying to combine the question answering and summarization pieces to, you know, answer
these more complex questions.
Yeah, if I'm not mistaken, there are people who are trying to do both.
I've seen this a mostly like focusing on one, but noted the two, but I'm sure that there
are people who work on that.
I'm just not aware at the moment of, of, or I don't have them on top of my head for such
paper, but like, there's a researcher called Sasha Rush, like he has a paper on like,
how do you do this summarization that you were describing with attention mechanisms?
So I'm sure that they're likely there could be a work combining both of the things that
you're describing.
So if someone wants to dig into this more, do you have any go-to resources for getting
started?
Yes.
For example, depends on what your aim is.
If your aim is to stay on top of the Neatronic Processing field, a great place is just to
go to the ACO ontology and all the different NLP conferences, their index there.
You can see the latest papers organized by yours and by tracks.
So you can pick your favorite track, being question answering, being parsing, being
machine translation, whatever excites you.
And it's great to just sit down and like read through these papers and search that you
can get on a high level.
What is it that people are working on and how far they have advanced?
If somebody's just a beginner and they're trying to learn about the field, I encourage
you.
You can look at the Stanford's class on deep learning, like Neatronic's first thing
with deep learning.
It's taught by Chris Manning and Richard Zohar, both of them are like leaders in the field.
And it's a good place to start and just kind of learn both about what are the problems
in NLP, like the basic ones, and then how do you solve them using deep learning?
And there are a lot of books people can just take and read specific chapters.
So it really depends on what is your end goal, is it learning or is it just download and
build?
And you have open source code that people can use as well, just to run something basic
and understand what's happening.
And to bring us full circle, if all you want to do is build systems that use this stuff,
you can use services like the stuff you're working on at Amazon.
Yes, you can do that.
And as I say, one is if you know nothing about machine learning, but yet you want to build
such systems, you can use a lot of the pre-built things and we've done the heavy lifting
for you.
If you are passionate about machine learning, you know the algorithms and you love implementing
from scratch.
You can use like open source like MXNet, which we are supporting.
You can use TensorFlow.
You can use any of those tools.
It's just depending on like what's your level and also like are you on a deadline or are
you in this like learning exploration mode?
And do you foresee a future that allows folks to combine elements of both and in particular
what I'm getting at is, you know, now your choice has seemed to be, you know, I can use
a service like Lex that's, you know, pre-trained or I can, you know, roll my own and, you know,
train using my own data and, you know, the presumption being I'm trying to get more accuracy
by, you know, training on a more limited, you know, the more limited corpus that I'm concerned
about.
Do you see over time the kind of AI as a service offerings allowing you to upload your
own data and train on it and somehow augment, you know, their pre-trained models?
Yeah, I think that's exactly where like future is headed.
Definitely people have their own data sets, their own requirements.
If they have the data scientists also like available, doing exactly what you describe is
possible.
And I think that's fantastic way to take advantage of your in-house data, take advantage
of your engineers and put them on a mission or put them on a task, which is like, okay,
improve these services, augment them, make them better.
So definitely I think that's a great place to be and in a great area to invest and focus
on.
Great.
Well, I really appreciated all the time you spent with us today and I really enjoyed
the conversation.
Thanks so much.
Zora Nitsa.
Likewise, thank you very much Sam.
Bye.
Bye.
Bye.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued support, comments and feedback.
This is your last reminder.
If you are in New York today, June 29th, Thursday, join me this evening at the happy hour.
I'm hosting with the NYAI Meetup.
If you'd like more details, please sign up using the form at twimlai.com slash NY Meetup.
The notes for this episode can be found at twimlai.com slash talk slash 30.
For more information on industrial AI, my report on the topic or the industrial AI podcast
series, visit twimlai.com slash industrial AI.
As always, remember to post your favorite quote or takeaway from this episode and we'll
send you a laptop sticker.
You can post them as comments to the show notes page via Twitter mentioning at twimlai or via
our Facebook page.
Thanks so much for listening and catch you next time.
