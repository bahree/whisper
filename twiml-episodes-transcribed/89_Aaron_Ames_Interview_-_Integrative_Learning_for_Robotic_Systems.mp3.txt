Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast we're featuring a series of conversations from the AWS re-invent
conference in Las Vegas.
I had a great time at this event, getting caught up on the new machine learning and AI products
and services announced by AWS and its partners.
If you missed the news coming out of re-invent and want to know more about what one of the
biggest AI platform providers is up to, make sure you check out Monday's show, Twimble
Talk number 83.
Around table discussion I held with Dave McCrory and Lawrence Chung.
We cover all of AWS's most important news, including the new SageMaker, DeepLens, Recognition
Video, Transcription, Alexa for Business, Greengrass ML inference and more.
This week we're also running a special listener appreciation contest to celebrate hitting
1 million listens here on the podcast and to thank you all for being so awesome.
Tweet to us using the hashtag Twimble1Mill to enter. Every entry gets a fly Twimble1Mill
sticker plus a chance to win a limited run t-shirt commemorating the occasion.
We'll be digging into the magic Twimble swag bag and giving away some other mystery prizes
as well so you definitely don't want to miss this.
If you're not on Twitter or you want more ways to enter, visit twimbleai.com slash Twimble1Mill
for the full rundown.
Before we dive in, I'd like to thank our good friends over at Intel Nirvana for their
sponsorship of this podcast and our reinvent series.
One of the big announcements at reinvent this year was the release of Amazon DeepLens,
a fully programmable deep learning enabled wireless video camera designed to help developers
learn and experiment with AI both in the cloud and at the edge.
This is powered by an Intel Atom X5 processor, which delivers up to 100 gigaflops of processing
power to onboard applications.
To learn more about DeepLens and the other interesting things Intel's been up to in the
AI space, check out intelnervana.com.
Today we're joined by Aaron Ames, Professor of Mechanical and Civil Engineering at Caltech.
Aaron joined me before his talk at the reinvent deep learning summit on iRobot, Computer
Vision and Autonomous Robotics.
While Aaron considers himself a hardware guy, we got into a great discussion centered
around the intersection of robotics and machine learning.
We cover a range of topics, including Boston Dynamics's Backflipping Robot and how a system
like that actually works.
As well as the various humanoid robots his own lab is created and more broadly, his views
on the role of end-to-end deep learning in robotics.
I had a blast with this interview and I think you will too.
And now on to the show.
Alright everyone, I am here in Las Vegas at Amazon reinvent and I have the pleasure of
being seated with Aaron Ames.
Aaron is a professor of mechanical and civil engineering at Caltech and he's going to
be speaking here at reinvent tomorrow as part of their deep learning summit.
But as you can tell from his department affiliation, he's not a deep learning guy, he is a robotics
guy, a hardware, a self-professed hardware guy.
Aaron, welcome to this week in machine learning and AI.
Pleasure to be here.
Thanks for having me on the interview.
Yeah, I come from actually algorithms and mathematics is sort of my background and putting
it on hardware.
And it's funny because what I do, I'm speaking at the deep learning summit, I think partially
to give this perspective of how learning and AI algorithms will play out on hardware
platforms and what that connection will be.
So I mean, historically in my research and in research like Boston Dynamics and all
these cool things like the backflip video recently appeared, that was incredible.
It was incredible, but guess how much learning is on those platforms?
Yeah, I imagine not much.
No, zero.
Okay.
I mean, the core in doing these things is to take the dynamics of the system, right?
There's physics driving it and then you develop algorithms using something called control
theory to determine how to move the robot, how to make the actuators move in specific
patterns so that you get these dynamic behaviors.
Right.
So it's heavily tied with the physics, right?
You have the physics, you make decisions based on the physics and it's all very deterministic
and pre-programmed.
Yeah.
So AI on the other hand is a totally different animal.
You start with data.
It's all data driven data centric.
You take examples of things that work well, you know, you label images and then you plug
it into a deep neural network.
There's other variants of learning as well that are a little more mathematical and then
the back end is that it sort of learns or identifies these patterns.
Right.
And that's really exciting because it's computers doing things that we don't always expect
they'll do and they can deal with highly unstructured and data driven approaches.
Yeah.
But it sort of runs contrary to this whole hardware and theoretical approach that's
often taken in robotics because, you know, we need to know everything about the robot.
It has to be very pre-programmed and pre-planned in a specific way.
So the question of the deep learning summit that I'm addressing in my talk is, is what
would this integration kind of look like or a view towards this integration of learning
with hardware and robotic systems?
Something that has yet to actually be done in a good way because they're such different
worlds.
And what's missing from the learning community?
What's missing from the robotics community?
How could they benefit each other?
Right.
Right.
Let's dig into that.
But before we do, I want to make sure that the audience gets to know you a little bit.
Absolutely.
So how did you get interested in algorithms, math, hardware and the way you've put them
all together?
Right.
Science fiction in short.
Okay.
That was driven by science fiction.
When I was in undergrad, all I did was read sci-fi all the time.
Any favorites?
Asmob is guy that go to classic, right?
Yeah.
There's some other great ones, you know, high line, you know, these.
Like the classic authors, I think, had a really unique perspective.
Actually, coming from the authors that sort of started in the 50s, 60s, there was such
imagination to where we'd be, unconstrained by the problems that would later be confronted
in robotics and in all these other things that I think it painted a picture of the world
that was really enticing, of how robots can really work amongst us.
And that's driven me for a long time as this sort of internal fascination.
I can't really explain it on my borderline or maybe not even borderline obsessed with
it.
How do we make robots move like us and do things like us?
Yeah.
So that's driven me for a really long time.
And I wanted to delve into that.
So my background is actually highly theoretic.
I didn't touch hardware until actually after my PhD.
I studied walking, but from a theoretic perspective.
So I really wanted to understand the mathematical underpinnings of locomotion.
And as you start by looking at that from a robotics, a hardware perspective or like human
walking.
I started it from a robotics perspective, but not so much hardware as the mathematics
underlying movement, right, underlying dynamics, right?
And how do we understand that?
How do we even model or formulate a mathematical model of walking and running and doing dynamic
things on robots?
So I delved into that, you know, I proved a lot of theorems on what this might look
like, well, how do we quantify what this is?
How do we characterize it?
So really that, you know, that's sort of the basic science of dynamic robotic movement.
How far have we come in that?
Do we have a strong analytical foundation in locomotion, or is it, you know, do we bump
up against, you know, an edge analytically and have to apply computation to it?
That's an interesting question.
And actually, we have a really strong analytic foundation now for locomotion that's been
developed over the last, I mean, I guess 20 years.
Okay.
Starting sort of a little before I was a grad student, it was in its infancy and I started
working with it.
Other people, great people at other universities have developed these frameworks.
Jesse Grizzell at University of Michigan is one example.
There's lots of people, but we've come a long way in our mathematical understanding of
locomotion.
Yeah.
What the models are.
How do we quantify that behavior?
And there's a lot of papers on this too.
You can understand it mathematically, but what's interesting is we have this mathematical
understanding a while ago now, but computationally realizing that math on hardware was a huge
problem.
So that's where the sort of blockade came is we could write a theorem and we could say,
if this thing exists, then we have walking, right?
But how do you find the thing that exists?
And that's a computational question.
In the end, every mathematical thing you do has to be translated to algorithms on the
robot.
And how do you do that?
It turns out that recently there's been a huge surge in this computation area, huge breakthroughs,
mainly due to the computation breakthroughs that have happened.
So the prevalence of cheap and vast computation.
So it turns out that there's a lot of analogies between making a robot move dynamically and
learn.
What I mean is, in essence, it's a large optimization problem, right?
And that's what the math boils down to.
And how do you solve large-scale optimization problems efficiently?
And there's been some great results recently, some of which have come out of my lab, some
of other labs, a bunch of people collaborating where we can now solve these orders of magnitude
faster than we could 10 years ago.
I mean, it used to be it'd take a day plus to generate one walking behavior for a humanoid
robot, right?
Over a day of computation, if it converged, and as a walking behavior, what does that mean
specifically?
So the way you can think about a walking behavior is a periodic motion that's stable.
Okay.
Given a set of parameters that define us, you know, the hardware or what the, you know,
the joints, the angles, the lengths of the legs.
The masses, the inertia, all that stuff.
So what happens?
That's even lower level, then.
Yeah, yeah.
So you pull all those things together to create a mathematical model, right?
And you get a differential equation, if you like, the technical term.
And it's actually a hybrid system too, meaning there's continuous dynamics.
Think about just a bouncing ball, right?
As almost the simplest example of a walking gate, okay?
What I mean is a periodic motion.
So it falls through the air until it hits the ground.
And then there's a discrete impact that pops it back up.
Now imagine you have a little actuation with that ball, like a little spring, and you
can, you know, then the goal of locomotion is to create a stable, periodic motion.
So the ball bounces at the same height all the time, okay?
That's a very low dimensional example.
Now take a humanoid robot, you take all the physics that go into it, right?
So you have something like, you know, let's say 25 degrees of freedom.
What that means is 25 joints that you can actuate, or other joints that you can't actuate.
The point is 25 things they can move, yeah, all right?
And then you have to take that and you get some mathematical representation of it as a
hybrid system, because there's this, you know, when the legs swinging forward, that's
a continuous dynamic.
It's just like when the ball's falling, then the foot strikes the ground and you get these
impacts.
And then you have to create a periodic motion that coordinates all of those, you know, 25
degrees of freedom together in a synchronous way.
And so do these 25 degrees of freedom translate into, you know, some series of hundreds of
differential equations that you're trying to say?
Yeah, so you actually get two times the number of degrees of freedom, because usually we're
dealing with second order systems.
So you end up with, let's say, 50 equations, 50 ordinary differential equations, right?
Or what, technically, an ordinary differential equation that's 50-dimensional, okay?
Okay.
So you're dealing on some 50-dimensional space of evolution.
Yeah.
So it's a very high-dimensional space.
And that's for simple ones.
It gets even higher.
I mean, take a full humanoid with hands and everything.
You could be dealing with a hundred-dimensional system, plus, and this is all very nonlinear.
And more importantly, it's because you're generating these periodic motions, you have to
really utilize these nonlinear dynamics, the inherent dynamics of the system to generate
these behaviors.
Okay.
And is that mean specifically?
What that means is you can't just to provide an example.
You know, it's funny.
We get a lot of comments on YouTube for our videos, right?
And these are, I actually read them sometimes as I enjoy reading them, although I never respond.
I think that's the key is never responding to comments.
But I like reading them.
And a lot of questions revolve around, well, why not just take a human walking trajectory,
right?
Just record a human walking and pop it on the robot, right?
Well, it's because the physics are different from the human and the robot, right?
Right.
So, what I mean by the nonlinear dynamics is there's only certain trajectories that work
for the system, that make sense, that are consistent with its dynamics.
And those are the ones you have to find.
And you can't just put a human trajectory on, you'd have to modify it so it'd be consistent
with the dynamics.
The robot would have to basically have human physical properties.
Yeah, and human actuation and everything else, it'd have to be perfectly human in some
way, and that's not going to happen.
And it shouldn't.
It's like when we have planes that fly, they don't flap their wings.
You have to exploit the, you know, you want to be inspired by flight.
You want to create lift.
Right.
But you want to do it in a way that's consistent with what we can build.
Right.
So you take all these dynamics just not to get too far in the weeds.
And then you have to create these periodic motions, which again, result in these optimization
problems.
Okay.
You know, you could imagine what these might be, you know, in the sense of, even with
the bouncing ball, you want to create a periodic trajectory.
So start at some point and end at some point.
Those are some constraints on the system, right?
Another constraint, it has to satisfy the dynamics of the ball falling.
So you put all those together and you crunch it into this big optimization.
And we, like I mentioned, we've gone from maybe a day plus to solve these, maybe to down
to a couple of minutes, even faster, sub-second, meaning almost real time, which means we
can generate a gate in really fast, right?
And by a gate, I mean a periodic motion.
Yeah.
And then you can start putting all those together and create advanced behaviors, right?
So that's kind of the paradigm for how you create walking gates or any kind of behavior.
You tell, you know, there was the backflip we mentioned earlier.
How would you do that?
Well, you can actually take the dynamics of that system.
You can set that up as an optimization problem where you go through this motion of flippy.
And then you can crunch it into an optimization problem and generate those motions.
And then pop that on the robot.
And that sounds easy.
There's a lot of difficulty in doing that.
It's a non-trivial adventure to actually implement that.
But that's the general trend.
And there's lots of ways that people do this.
I mean, I don't want to go through all the background, but there's lots of ways people
can generate these periodic motions.
They can have reduced dimensional representations of the system that make it a little faster.
There's a lot of tricks and all this stuff.
But in the end, what you're fundamentally doing, the point of this discussion is to kind
of say, this is the way the robotics community by and large approaches the problem of generating
behaviors on robots.
So they start with the physics.
They set up some sort of optimization problem, computation problem, which generates the,
gives you this periodic motion, which you then put on the robot.
Now, when we see a video like the, you know, people will be familiar with the Boston Dynamics
and the Backflip one that came out recently.
And in fact, you showed me a really interesting one on your YouTube of a robot called Doris,
just part of the interview.
When we see videos like that, and let's take the Backfliping one because it was.
I think everyone's seen it.
So that's fine.
It was, I have to say, it was very impressive.
I mean, Boston Dynamics is always raising the bar for our academics.
Yeah.
You think, you know, you mentioned the Doris, you know, if you look at Doris walking on my
YouTube page and then compare it with some of the walking that Boston Dynamics has, they're
in the ballpark, right?
They are.
I mean, which is something I'm very proud of.
But, you know, they had that stuff a couple of years ago and we now we have the mathematics
to understand it.
And then they do the Backflip.
I'm right.
So now we got to do the Backflip or some variants of that that matches it.
So they raise the bar and they push us, which I think is great, but it's a very impressive
behavior.
Yeah.
And so there's no doubt about it.
But when we look at that, are we seeing a robot, you know, basically performing a script
and it can do, you know, just what we see starting from where it started, you know, going
through every point and space that it saw or is there.
That's exactly right.
That's some parametric thing where there's some variability.
It's a script.
It's a script.
Okay.
And you're exactly right.
And here comes the crux of the problem, right?
And the exciting opportunities.
So, you know, when you, let's now talk about the Backflip with Boston Dynamics, when you
look at that video, I mean, everyone's like, oh, SkyNets coming, the robots are taking
over.
Oh, my God.
And again, we mentioned earlier, and now we don't know exactly what's on the robot, just
to be clear, they don't publicly release anything that's on there.
Right.
Although I've, I know Boston Dynamics well enough to make a very educated guess.
And the guess is based on their past stuff, too, is exactly what you said.
It's a pre-plan behavior.
So this robot has no knowledge of its environment in the sense that it's not observing where
those blocks are, and in real time, adjusting its behavior, and learning how to do this behavior.
They put those obstacles in the memory of the computer.
They pre-plan those behaviors.
They do a bunch of experiments till they get the right behavior.
They take a bunch of videos.
What I loved about that, Blackfoot videos, after the Backflip, they showed failure cases.
Which I thought was, which is important to show, because anytime you see a video of something
working, right?
Right.
There's a thousand or 10,000 cases where it didn't work at all.
Right.
And everything tuned in just right?
As hardware is not quite as clean as the math I talked about.
And then it goes.
So it's a pre-plan behavior that robot has no awareness of what it's doing in a broader
sense.
Even beyond the awareness and learning and ability to, the robot's ability to adjust, you
know, I think when it, you know, it's easy to envision something like that where I guess
I'm thinking of it from like a computer, you know, a computer program perspective.
There's no function that says, you know, Backflip, you know, start from X equals whatever.
It is like a vector of points that it is just following, right?
Yeah, yeah.
Effectively, yeah.
I mean, you can represent these behaviors as sort of modules, right?
If you'd like, like Backflip.
It itself.
I mean, so the thing about dynamic Backflip, like with how many, you know, how much freedom,
like Backflip and you can give it a height and it would on the fly.
And that's, and that's a good question.
Exactly.
Right.
So far, you know, before we get to any conversation about awareness and learning, you're absolutely
right.
And I think this is a very astute question and comment is right now.
I mean, I don't know their exact capabilities, but typically it's Backflip from a height
of this high, maybe with some small variations, right?
But if you, if you change the terrain it was on or change the box or change any of those
parameters, it wouldn't do it.
Right.
In fact, if you look at that video, you look at what it's landing on, it's kind of a somewhat
absorbed, you know, a pad that looks like it has, that it's very carefully constructed.
It's not just a random floor.
There was something special about what they were landing on, partially to absorb the shock
of impact.
I'm sure.
But the point is, as you said, yeah, so there's Backflip as a canonical unit, but that
is very constrained in the environments it can work on.
So you're right.
And this is something that's very important for people to understand, I think, from the
learning perspective, is that we're not like one step away from, you know, self-aware
machines in this context.
I mean, we can do pre-programmed behaviors in environments we completely understand and
have characterized within a small window of perturbation.
And that's what we're getting really good at that.
We couldn't do that 10 years ago, 20 years ago, we couldn't even do that, right?
We couldn't, a robot couldn't do a backflip.
You know, I mean, in, or at least not a humanoid, although Raybert has some great stuff from
the 80s where he had two-legged sort of polo-step type robots that could do backflips back then.
You should go check out some of this stuff from when he was a professor at MIT.
I mean, his old, the 1980s.
Mark Raybert.
Mark Raybert.
Yeah.
He's the head of Boston Dynamics.
He's, it's his brainchild.
Got it.
And he went and founded it from MIT.
He actually started out at JPL, which is part of Caltech, and then Carnegie Mellon, then
MIT, then started Boston Dynamics has been doing that forever.
But if you look at it stuff in the 80s, it has the same characteristic.
If you watch those videos, you see how the backflip came to be on Atlas, right?
Yeah.
And, you know, you can really see the trend.
But again, it's all this very structured thing.
And that leads to what do we do about unstructured environments?
Right.
And this is actually the kind of the core of what I'm going to talk about tomorrow, is
I want to set the stage with just like we had this conversation.
I want to explain what it takes to make a robot walk, because I think when you understand
that or make a robot backflip, you realize how much machinery is there, and how first
you're not going to learn how to do that, really.
Right.
I mean, it's too high-dimensional of a problem for learning.
You're not just going to plug in the actuators and whatever into some deep neural network
and expect the outcome to be a backflip.
You need to...
There's some structure in the system that has to be exploited, right?
So there's a place for control and dynamics, but there's a place for learning too, right?
You know, and I think that's the thing is to understand the right context for it.
Right.
You know, learning will not take over the world and learning will not solve all problems.
Right.
But learning can handle unknown and unforeseen things.
Yeah.
So that's the role it can play in the context of like a backflip.
So how do we go about getting to that?
I mean, it seems like it strikes me that there are lots of, you know, umpteen ways of
kind of attacking that problem.
Like I'm thinking that the thought that comes to mind is like, you know, one approach
might be defining levels of abstraction or primitives or something like that, trying
to figure those out and, you know, I'll have the learning, the intelligence kind of select
those on the fly.
Like, what are the, what's the, you need to come, you need to come, you need to come join
the robotics walking community because you nailed it.
No, the current perspective, I, you know, I even have some papers on this from about
five years ago, now, okay, where we call it motion primitives and transitions, right?
And so basically what you do is you create primitives for all these different behaviors.
Because you're going to have to create the behaviors and now the computation has gotten
better, you can imagine, think about a graph, right?
You know, where every node of the graph, so every point is a behavior.
Okay.
And then you have transitions between those behaviors, right?
Which are admissible.
So you might have backflip followed by walking, followed by going up and down stairs.
You might have a bunch of different primitives for different backflips from different heights
and different terrain types and you can build up this entire compendium.
And then you could supervise how you pick an individual behavior with a learning element.
And this has started to be done that I mentioned Jesse Grisel earlier.
I worked closely with him and have for a really long time.
And he started to play with some of these ideas, putting learning on top of that sort
of motion primitive and transition framework where you decide how what gate to do at any
given time based on what the environment is doing and learning algorithm could do that
really well.
And that's a great place for learning.
And that's a great way of thinking about learning in the sense that you've sort of taken
the dynamics into account.
You've taken the mathematical representation of dynamic motions to their sort of extreme.
You've utilized them all the way and then you let learning do what learning does best,
which is based on input data, the environment, decide what to do on an output, but at a
very high level.
And if you look at the way the human brain works, this is very analogous to how the human
brain and body work.
So, you know, there's not thinking about moving my legs and my legs.
Exactly.
Yeah.
So we have motion primitive.
And we transition between them.
More importantly, the architecture of the human body.
Now I'm not an expert here, but I've certainly read a bit about it.
Just with this paradigm, you know, some people say we should just learn everything because
our brain learns everything.
It's not actually true.
Our brain is responsible for some of our motions, but in our spinal cord, we have a separate
brain.
I mean, in essence, we have patterns that are generated.
Those are your locomotion patterns.
Those are your primitives.
So you couldn't walk essentially with very little cognitive load.
I mean, think about walking and texting on your phone.
You don't have to think about it, right?
So that's the motion primitive acting.
And what happens when you get to a stair, you have to look up from your phone.
You have a cognitive load.
For a second, you have to think, what am I going to do next?
You decide, you go in, you're back to your phone.
Yeah.
So think about any time you could be doing something while on your phone.
That's where dynamics control and all those classic approaches would be used.
Any time you have to look up and see something, that's where machine learning would play
role.
Right?
The real point here is it's not one area or the other.
Right.
We really need to understand the intersection of these two domains.
And that's really the challenge problem of the next decade in my opinion.
Because there's a lot of people that do learning, a lot of people that do robotics.
And there's the beginnings of connecting these up a little bit.
But we need to make a concerted effort to really understand this connection point, because
I think that's sort of the key.
So the kind of using a learning system to plan a goal achievement across some set of
primitives is one approach.
We've already talked about the, you know, we've already thrown out the window.
The idea of learning the motion primitives, you know, from the ground up.
But is there a role in learning and making them more robust?
Yes, absolutely.
Right.
That's the second place where learning comes in.
Okay.
You think we'd have a script?
We had a script for this interview, because you're feeding right into my, my talking research.
So the second place, yes, is where learning plays, will play a big role is not at the level
of planning, but at the level of unknown, unforeseen environments and influences.
Right.
So the simplest example is walking on different terrain.
Yeah.
So when you're walking on flat hard ground, we have a perfect model that, remember that everything
we talked about with generating models was built on the premise of having a model.
Right.
So now if you walk on standard dirt, it turns out there are no models of this.
There's no simple models at least.
I mean, there's people make them make their entire careers about modeling and simulating
granular media, deforming and moving around.
So standing dirt, crunching down, it's a really, really hard problem.
So there won't be some computationally as difficult to just render it as a picture.
Exactly.
Let's try to figure out its physics.
Exactly.
Exactly.
So you could imagine days to generate physics simulations of, of sand movie.
Now if it takes a day to generate a physics simulation of, of a robot putting its foot in
sand, you're probably not going to be real time using those things, right?
Yeah.
So how do we bring it together?
Well, so we have some initial work with some colleagues I have at Georgia Tech on
this idea where we learn how to hop in granular terrain.
And so we don't use neural nets there.
We use something called Gaussian processes, which are another way of learning, but it's
not the neural net way.
It's a variant that basically deals with some initial guess on the model and then you update
that model as new data comes in.
And we were able to not know what the tray model was, but have a guess based on some physics.
And then every time the robot would hop in the, in the train, we'd take that data in and
update the model of what the terrain forces look like.
We iterated that through the optimization problem.
So we'd update that every time we, we had a new hop, we'd take this new information,
we generate a new model of the physics interaction with the world, and then we'd run the optimization
with that new model.
And now are we talking about something that's done in simulation as far as learning process
or?
No, we did this on hardware.
Okay.
I mean, we verified in simulation, but really, you, this would have to be on the hardware
because you need the data.
Okay.
Right.
You need to sense what's happening with the environment.
Right.
Because we don't have the models to put in the simulation.
So this is now a data driven modification.
So where we can combine the data with learning that model of the environment with the optimization
framework that I discussed earlier in a feedback loop.
And we did that and we were able to actually, so we wanted to hop, I mean, this is a very
simple robot.
So this is not a walking robot, but it's kind of like, think about the bouncing ball again
where we're going back to the basics.
We wanted to make this thing hop at a specific height.
We'd first tried without having a model of the granular train, and it wouldn't do it.
Is this a, I'm getting hung up on the form factor here.
Is this like a standalone robot that is, you know, what does this thing look like?
All right.
So that's a good question.
Yeah.
And is it suspended?
Yeah.
And some, you know, is it fixed in somewhere?
Yeah.
So this is a very, this was a very simple test bed meant to generate physics of terrain
interaction.
It was actually developed by a colleague of mine, Dan Goldman at Georgia Tech, who is a
physicist.
And this is work with Patricia Vella as well at Georgia Tech, who does machine learning
stuff.
So it's, it's a very simple thing.
It's a motor with a spring between the, basically, the motor and the world.
Okay.
Okay.
So basically it can, it can move a mass up and down to make this thing move on top of
the spring.
And then there's a spring.
And then there's like a foot, right, between the granular terrain.
Right.
So it's, so this thing is only moving one dimension, it's a one dimensional, it's a one
dimensional hopper.
Yeah.
It's one dimensional hopper.
That's right.
So again, any, any, what it does is it sits in a bed of poppy seeds.
His poppy seeds are actually a really good model of different sand and dirt, but they
don't get caught up in the actuator because they're big enough that they don't get into
all the things.
Okay.
And the, and the details.
Exactly.
And what actually you do is is there's this bed of poppy seeds, but you don't want to
have it be changing every time you hop on it because you can compact them.
So it'd be more like hard terrain.
So it actually aerates the bed between every hop.
So you sort of move the poppy seeds, let them settle hop, let them settle hop.
So you, you do a successive experiments where you have the same kind of initial condition
in the poppy seeds just to make sure you have a consistent model you're learning.
So that's a setup.
So it's a very isolated little box.
And then you can use vision or anything else to, to kind of identify what the forces are
between the robot and the terrain.
Okay.
And so, and then we ran this experiment.
Again, this was a proof of concept.
It's probably one of the first examples of putting all these pieces together.
And it kind of shows you, and this is an important point.
It shows you where we're at in, you know, find learning and control and dynamics.
If we have to go back to a 1D hopper and we're, and we publish a paper on it because it's
new and interesting, right, as opposed to a humanoid robot, which is, you know, so you
can imagine that we're just beginning this process and there's some other people working
on this domain as well.
But it's kind of, isn't it scary if you can just watch a beach and just say, yeah, exactly.
Yes.
And that's my advice to anybody.
If a robot has changed and you try to kill it, you just run to a beach and you're going
to be fun.
So, or go on some ice or snow or something, you know, and it'll fall over.
You'll be safe.
Yeah.
That's right.
Huh.
So we, we got those two pieces of learning like what's, what's next?
I think it's, I mean, I think you did a good job exactly parsing the two forefronts is
that we need to push in terms of machine learning on robotic systems.
It's really, this is the key point is there's a lot of work right now on learning as a stand
alone entity, right?
Learn everything.
Right.
And with this, if I can interrupt, as a point of reference for folks that are listening
that want to hear a little bit more about that perspective, a good place to start would
be the interview I did with Peter Abiel, you would be, yes, I know Peter, well, I know.
So Peter is precise, he does fantastic work.
I really respect the work he does, but the idea there is to learn everything.
Right.
And we talked about that.
Yeah.
And I am very strong and adamant is that that is not the right answer.
Now, people will disagree with me and that's okay.
It's okay to disagree in academia.
It shows that the problems aren't solved.
There's no, you know, physical systems, robot arms, even, you know, he does manipulation.
And stuff.
Have this wonderful structure we can exploit.
Yeah.
They might be high dimensional, but they live on manifolds, which are low dimensional
surfaces in this high dimensional space.
Let's use physics and control to push the system to this low dimensional space and then
learn on that space.
It will work orders of magnitude better, I promise, right?
Well, I promise in that, I think I'm right with my opinion, which I may be wrong, and that
would be great.
And at the point when, when a robot does a back flip with no knowledge of his physics
or dynamics, a humanoid robot, then I'll be like, okay, I am ready to listen to the pure
learning approach.
But, you know, then the robotics community, the proof is in the pudding, right?
You know, as they sometimes say, the proof is on the robot.
So, and the reality is we can do so much more with zero learning, model based approaches
on physical hardware than we can with learning.
Now that being said, I'm not trying to advocate that that's the end of the story.
Like I've said through this whole interview, there's limitations to this.
And that's where learning will play a huge role.
So I think the forefront is don't learn everything.
Don't fall into this trap of a big shiny black box that you put in what you want, and it
spits out the right answer.
If only because from a scientific perspective, I find that very unsatisfying too, that's
a separate point.
But I, my opinion is don't fall into that paradigm because you're, you're restricting,
or you're limiting yourself in your worldview instead, unify, unify, unify, unify.
So that's my argument on the forefront of learning is what, what else does unify
mean for you?
I mean, bring the learning, physics, dynamics, computation, control, mechanical design, actuators,
all those pieces.
What's beautiful about robots is they are a microcosm of the universe in some way.
They're completely self-consistent systems that you have complete control and you create.
Yeah.
You have computation, you have actuators, you have all these wonderful things.
So use that, you know, and understand those different pieces at a deep level, and then
you'll really understand how to put them together.
So that's what I mean, unify, unify, all of computation, control, design, and learning.
Understand where they all fit together relative, use the strengths of each, you know, to exploit
them to their maximum.
And that's where I think the really fun stuff is going to come in the next couple years,
in my opinion.
You know, I could be dead wrong in a year from now, if I am, I'm happy to admit it.
But I think that's really where the future is in terms of learning and roboticism.
And do you have any predictions in terms of what that really fun stuff is going to look
like for us?
The forefront is the following, in my opinion, it's getting robots out of the lab into
the lab.
You know, there's been a couple examples of that.
We've taken some of our robots out of the lab in limited context.
Boston and Amick still has some of the best videos where I was walking around kind of
in snow and stuff like that, right?
Yeah.
That was purely a result of that.
Actually, the four, the four-legged ones are the, they had a biped outside in the snow
in one video.
It was about a year or two years ago now.
And again, remember, that's purely reactive.
There was no learning there.
It was walking in snow and on uneven terrain, only through the robustness of the algorithms
that are on there, right?
So, but I think that's really the forefront is, you know, get out of structured environments,
get out of the lab and get on dynamic systems.
So I'm very, very pure, for example, manipulation tasks as they're deceptive in their simplicity.
You know, when a robot can't fall over, you can always correct, right?
And there's a robustness there that when you go to humanoid robots and dynamic robots,
you don't have anymore.
So get a dynamic robotic system, whatever that is, a four-legged robot, a two-legged robot,
a hopping robot, whatever it happens to be out into the real world and make it do cool
stuff.
And that's to me the way of really proving that you understand what's going on, you know,
because that will take all these things we mentioned, especially in an autonomous context.
So this is the second point.
We actually just started a center for autonomy at Caltech called Cast.
And it's really aimed at doing these things is how do we get robots into the wild and not
prescript them all the way, right?
And that's really what I mean by getting into the wild is tell a robot go from A to B outside,
right?
Walking robot or humanoid robot and by A to B it might be over a beach.
It might be, you know, through some ice and snow.
How do we do that?
What's that?
So we actually frame these questions in the context of moonshots for Cast.
Okay.
So just to give us a sense of how hard they are and by moonshot it really a lot of these
are moonshots, meaning we could do it if we had massive amount of resources and people
concerted, but one example of a moonshot is have a robot walk the Pacific Crest Trail.
So this is a trail that goes from Mexico to Canada and have it do it autonomously.
So what would it take to do that, you know, and that exactly would require all the pieces
we've discussed today, plus many more that we don't even know, we don't know yet.
But that's the kind of thing we need to be thinking about I think is pushing these boundaries
of what we can do with robotic systems.
And in an autonomous way, so bringing autonomy in it and bringing them and understanding
how that fits with both the mathematical representation of behaviors and learning and
where those each can play a role.
So to me, that's the direction of push.
It's challenging, but fun, but it's time for the real world.
It's kind of where we're at.
For a long time, we couldn't even get robust to do stuff in our labs that was all that
interesting.
And now we're at the point where we can do some cool stuff in our labs, so leave the lab.
When I'm struggling with a little bit and trying to bridge our early conversation about
these very scripted, rigid tasks, and even some of the stuff that the Boston Dynamics
kind of walking in snow, we have to be incorporating in sensors.
Is even the most primitive stuff is that I'm thinking of that as just kind of the actuators
and not sensors or is that not the way to think about it?
I mean, they all have sensors, the question is, what are the sensors doing and what information
are they taking?
Yeah.
And how are they fit into it?
These sensors are part of all of this.
The question is, what sensors and what are they sensing?
So for all the, from the back flip on, again, I can't speak, I know they're hardware all
the way in, but roughly speaking, you have encoders that look at the angles of all the
joints.
You have an IMU, a inertial measurement unit that tells you the global orientation of the
robot, right?
Some accelerometers.
Some accelerometers in that case.
Exactly.
Exactly.
And then there's sometimes some sort of sensing of the environment, has your flip touched
down, and that's a pretty essential one.
So when we walk with Doris, I can tell you that.
But we need are those three main components.
We need to know when the foot's on the ground.
So since the impact with the ground, we need to know what the angles are on all the joints.
And we need to know, again, an IMU, an accelerometer, we need to know the global orientation of
the robot relative to the robot.
Those three pieces of his information are all you really need to have a robot do a dynamic
thing in a constrained context.
Right?
That means you know the environment, right?
You don't need computer vision if you know how high the blocks are and where they're located
relative to the robot.
And you set the robot up in the same spot every time, right?
So obviously when you start to do more unstructured things, you're going to have to bring in other
sensors, cameras, of course.
And that's one, you know, Pietro Perona's talking, another professor at Caltech is talking
with me at the machine learning summit, and he does computer vision.
And we've started to say, how can we integrate these two pieces together?
So we actually have a robot in our lab called Cassie, where we, he did his algorithms to
parse, you know, where they can determine the pose of people.
And we want to use those pose information to have the robot do something.
So we want the vision of the robot, what the robot's seen, to feed into its behaviors.
Right?
And we're just starting this track, but that gives you an idea.
So in this case, we need a vision system.
You might need four sensors.
If you're going to observe the environment for the hopping behaviors we discussed, you're
going to need to have a really nice notion of what the forces are on the system.
So the more, the more you want to do, the more sensors you need.
Yeah.
And then going back in our conversation, this is very consistent with the perspective
of the human body.
So if you want to walk on flat ground with no obstacles, you can actually, you need very
little sensors, right?
You kind of know when your foot strikes the ground and the rest is pretty, right?
If you've never been on ice before and you take somebody on ice, look at the way they
have sensing.
They're doing a lot of sensing.
They're doing a lot of computation because they're learning, but then watch a couple
minutes on ice and people kind of settle in and they're clearly, they've whittled down.
They've taken all their sensor information and they've decided which sensor information
is important.
And they're using that.
And that's part of the problem, too, is how do you take all this information in with
your environment and whittle out the stuff that's actually relevant to what you're trying
to do and an individual motion permit?
I think that's a great example or a great way of articulating it that really gets at what
what I was struggling with.
It's the, you know, when you think about the human on the ice, there's, you know, there's
that bit of learning and, you know, if you take that to, I'm trying to, trying to reconcile
that with, you know, the Boston, the Boston time I was walking in the snow.
You know, that, you're, we're saying that that robot is not learning like what is it doing
with that sensor data that's allowing it to be more robust than, you know, what we
do.
Well, okay.
So, you notice it was walking in snow and up, you know, small terrain differences, but
not ice.
So the deciding factor here is friction.
So as long as it has sufficient friction when the foot touches down, you can do the same
behavior you do on firm ground on non-firm ground as long as it's reasonably close, right?
As long as the foot doesn't slip as long as, or not too much, or if it slips, you can
catch yourself.
I may be confusing videos in MMI's, but there's one where like it's winning up here.
Oh, it slips on ice.
Yeah.
And there's one where it even slips on icing and catches itself, right?
And again, it slips on a small patch of ice and catches itself and then it's off the
ice.
Yeah.
This is, I think, what your question is and where we can separate is that there's a difference
between reactive behavior that's robust enough to handle terrain differences with learning
a new behavior itself.
Yes.
And that's the difference.
So if you're walking along and you slip on a little puddle, right?
Watch a person when they slip, they go, right, and they catch themselves and then they
keep walking.
That's not a learned behavior.
That's a reactive behavior, right?
Or you miss a step.
That's a great one.
When people don't know a step's coming and there's a step, and then you see them fall,
and then you know this feeling too, right?
You're already falling and catching yourself when you realize, oh, I just fell down a step,
right?
Your body's doing stuff before you even realize what's happened, right?
So that's a great example.
So next time you fall, please, I mean, hopefully I don't fall on purpose, but after you kind
of catch yourself from falling, think back and realize you did all that stuff before
you even thought about what you were doing.
That's reactive.
So that's what Boston Dynamics does.
Their controllers are so robust and they're very impressively robust that they can react
to all these different things and be robust to it.
It's not fair to say that, you know, it's simply kind of actuating, you know, these motors
through a series of pre-plan points and, you know, that's how it's doing, you know, walking
or doing flips or something like, it's more, it's more robust, it's more hierarchical.
It is.
There is more.
Yeah.
And the same with our robots.
My description of moving the robot through a series of pre-plan points or trajectories
as we call it is a simplistic representation of what actually goes on the hard one.
That's part of it.
That's actually mathematically.
That's what we call sort of the feed-forward term or the nominal behavior.
So assuming everything's perfect, that's what it will do.
But we do, we need to stabilize.
We talked about stable periodic motions.
We need to stabilize.
We need to robustify that.
Okay.
And so for that, you add things that bring you back to that orbit if needed.
And that's where this robustness and reactive behavior comes from.
There is a hierarchy.
And so for Boston and Amics, that hierarchy is based on foot placement, typically based
on assuming based on the rubber papers from the 80s and all this, is that they sort of
based on, you know, the orientation of the robot at a high level, it'll place its foot
in different locations.
It still has the nominal.
Listen to a Boston and Amics video.
Yeah.
And you'll notice it's very time-based.
That's because that's the nominal trajectories.
You hear the consistent.
I'm hearing that in my head.
And it doesn't change, right?
So all that's changing.
So that's the trajectory points.
Okay.
But then on top of that, there's a layer where it says, well, if I'm leaning too far
to my left, I mean, this is a simplification, put my foot out here.
Okay.
React to that motion.
Okay.
And so there's that reactive layer as well.
So that's the robustness.
And that's what you see acting when the big dogs on ice, when it's walking in snow.
That's a reactive behavior that's all just a hierarchical algorithm, right?
Mathematical algorithm.
It's not learning.
That's what happens when you go on ice, right?
So just, and so in terms of ice, it's not a single perturbation to your behavior.
Yeah.
It's an entirely new behavior you have to come up with.
So that's a new motion primitive.
Right.
So you have to learn that.
And by learning that, I don't necessarily mean machine learning that primitive.
What I mean is you'd have to learn the fact that on ice, the friction model is different.
Yeah.
Learn that friction model.
Put it back into the mathematical algorithms, modify the nominal behaviors, and then
make yourself robust.
So that's where this feedback loop comes in.
And that's where learning will play a role.
And that's kind of what you do.
I mean, if you look at a human, you go on ice, and basically you're shuffling your feet
around.
You're learning the friction properties of ice.
Once you have a pretty good model of those friction properties, you plug it into your
nominal sort of optimization method, which sits at your spinal cord.
And then once you've got that down, you can walk fairly normally because you've learned
the thing you didn't know.
And then you go back to doing the thing that you always do with a slight modification
based on the different physical model of the world.
So that's kind of the way we work, right?
Right.
It's funny.
The human systems, I think, are great inspiration at every level because it completely mirrors
what we're finding on robotic systems inspirationally.
Again, not in terms of we need to mimic what actually is happening in the human body.
But the higher arcies, where learning plays a role, where dynamics plays a role is really
clear on the human in the human body, I think it's great inspiration for robotic systems.
And the same parallels are happening on the neural network side.
Like we're taking inspiration from these things, bringing them in and try to evolve the
way we think about the learning side.
So definitely inspiration is huge.
But again, a word of caution is stay away from mimicry, right?
Don't just try to create the exact same thing on a robot or an AI, right?
Oh, well, there's, you know, X number of neurons in the human mind.
So if we can hit that neuron, we'll have a smart robot, right?
Right.
No, it's not, that's not the way it's just like if you flap something, it won't necessarily
fly.
But yes, look at the structures.
The really the structures are our key and try to understand what they mean and then realize
them on robotic systems.
Great.
Great.
Well, I really enjoy this conversation.
Any final words for folks or how can folks find you learn more about your work?
The internet is available to find my stuff.
My lab website is bipedorobotics.com, just a simple name.
You can find me on the Caltech website, just Google AirNames and there should be enough
stuff.
My students are on there too.
They do amazing stuff.
A lot of the videos you see are from my grad students.
If you're interested in robotics, please come to grad school somewhere.
If you feel free to ping us, if you're really interested in Caltech.
And in general, keep studying these problems.
This is, we're at a fascinating point right now.
And I think that's amazing.
My couple of final closing statements are, this is massively exciting, but be careful
of the hype.
Instead of just going for the hype, think about these things.
Think about where learning will play a role, look to unification, because we can achieve
these promises that are being made, but we have to be very smart about how we approach
the problem.
And that's what makes it fun right now.
These are not solved problems.
And anybody that says they're solved, I think doesn't know what they're talking about.
We were at this point where we're really trying to understand how learning and, for example,
robotics systems work together.
And it's an exciting time to be doing this, so I encourage everyone to really dig into
it and see what they can learn.
Oh, thanks Aaron.
Thanks a lot.
Alright everyone, that's our show for today.
Thanks so much for listening, and for your continued feedback and support.
For more information on Aaron, or any of the topics covered in this episode, head on
over to twimlai.com slash talk slash 87.
To follow along with the AWS re-invent series, visit twimlai.com slash re-invent.
To enter our Twimlai 1 mil contest, visit twimlai.com slash twimlai 1 mil.
Of course, we'd be delighted to hear from you, either via a comment on the show notes
page or via Twitter to add Twimlai or add Sam Charington.
Thanks again to Intel Nirvana for their sponsorship of this series.
To learn more about their role in deep lens and the other things they've been up to, visit
intelnervana.com.
And of course, thanks once again to you for listening, and catch you next time.
