WEBVTT

00:00.000 --> 00:18.000
All right, without further ado, I am super excited to jump into our keynote interview for this morning. Allow me to welcome up Yakapo Tagliaboo, former director of Kobio and member of South Park Commons.

00:18.000 --> 00:40.000
Ciao, how are you? Good, good and you. Thanks so much for having me. I am wonderful. I am super excited for the interview. Let's just get started by having you share a little bit about your background and your recent experience. You're in the middle of a transition now.

00:40.000 --> 01:00.000
Exactly. Up until recently, I was a director of AI at Kobio, a public Canadian company in AI SaaS, which I guess would feature prominently today in our chat. Before that, I joined Kobio because Kobio acquired my own startup, which was called Tuzo, which was a startup in Silicon Valley doing natural language processing for e-commerce.

01:00.000 --> 01:22.000
It wasn't a glorious day before MLOPS snowflake and all the fancy things. So it was when you still have a lot of stuff to do yourself, basically. And then I enjoy my my role at Kobio in the last three years doing both AI and the research level system and also MLOPS and actually, you know, productionized those insights for hundreds of customers at the same time.

01:22.000 --> 01:48.000
And now I'm building a new company in the data space, but that's a different story. That's awesome. So you've been a very outspoken, shall we say, critic of the way folks, some folks are approaching MLOPS? I think, you know, that may be too harsh, but I think you maybe a realist is a great way to characterize your perspective.

01:48.000 --> 02:03.000
You've taken a pragmatic approach to MLOPS. Talk a little bit about the way you see folks pursuing MLOPS and some of the ways that that kind of varies from your experience.

02:03.000 --> 02:23.000
Yeah, I mean, critic may be too harsh. I would say just Italians, you know, as we like to criticize everything, just as our default mode. But definitely more than a critic, I think that what we did, not just mean, but my team at large, kind of identified this gap like 18 months ago, starting something like that with the bigger boat repo.

02:23.000 --> 02:36.000
And then if I dig up in the market, not just the market of tools, almost like in the market of ideas, I seen a lot of people were talking about MLOPS, either by this what we did at Uber or Pinterest, which is amazing.

02:36.000 --> 02:47.000
And then this is a flash tutorial to put a cycle model online. So there was these two, these two words, it's like the hyper simple thing, the toy thing, which is good to start, you know, my students and why you do that.

02:47.000 --> 02:55.000
It's awesome. You need to start somewhere. And then there's what people do at company with a limited budget, a limited talent, a limited resources and so on and so forth.

02:55.000 --> 03:06.000
But the truth of the matter is most people are actually in the middle of this, you know, this distribution. Most people, you know, unfortunately for a living, they cannot just put a flash apple line. We need to do a bit more than that.

03:06.000 --> 03:23.000
You know, how easy would it be my job if that was just a job. So it's a bit harder than the tutorial that you see online. But honestly, it's not, I wouldn't say maybe it's easier, but it's not the same type of difficulty that you find when we have this, you know, a planetary scale infrastructure, where everything customized your need.

03:23.000 --> 03:38.000
And so our idea as a team was, well, we did this before. We made all possible mistakes in this field, because we did a garage scale at scale up scale and IPO scale. So we did this, this mistake a different scale. Why don't we tell people what we did? So maybe we saved them some time.

03:38.000 --> 03:43.000
And so that's how we kind of the reasonable scale things started. And, you know, and all of that.

03:43.000 --> 04:00.000
Yeah, yeah, I, you know, and hearing you describe this, I'm thinking that in large part, the, you know, an important part of this is kind of the distinction between, you know, where we were several years ago and where we are today, right.

04:00.000 --> 04:26.000
And so several years ago, it was important to take a look at what Google and Facebook and Uber were doing because they were the ones that were being successful at getting models in a production. And they had identified a lot of best practices that we could take, you know, the fact that you need tooling and you need platform, you can't just do it all in a notebook, like that was an important realization at the time.

04:26.000 --> 04:39.000
But now the opportunities are much broader. There's a lot more to learn from a lot, you know, a lot more companies that are having success and doing it in more of a scale down way.

04:39.000 --> 04:43.000
So to your point, not everyone has to try to be Facebook now.

04:43.000 --> 04:58.000
But I think there's still a lot of value into seeing what, what Uber or Pinterest or whatever Google does. In the same sense, I always use my tennis method, in sense that, you know, I really enjoy watching, you know, Roger, Roger Federer doesn't play anymore, but like, you know,

04:58.000 --> 05:23.000
you know, you know, you know, you know, you're going to compete at the level or similar to the level. And so it's very good to know where is the North and start. But then when you go and you train, like myself, I don't train like Rafa Nadal, let's be honest, like, you know, I'm just trying to get the ball on the other side.

05:23.000 --> 05:31.000
You know, my teacher, the math and the teacher would, you know, would teach me like the teacher, Rafa, it will actually be completely, you know, would be completely oblivious to the situation.

05:31.000 --> 05:44.000
You know, the context in which you're in, make some things, you know, more important and something less. And I think it's very important to know, go, you know, all big tech, kind of like fetish of like, oh, the only good things in ML ops, cannot be done in that scale, because you think that's false.

05:44.000 --> 05:58.000
There's a lot of super interesting ML that can be done in scale, actually. Yeah, yeah. Another dimension to the conversation is a lot of the folks that we point to, you know, the Facebooks and the Uber's.

05:58.000 --> 06:25.000
They're serving these broad B2C audiences, whereas much of the opportunity, or there's a tremendous amount of opportunity for B2B companies. And, you know, they don't have the same sets of issues or problems or, you know, the same sets of data, the same sets of, they're not set up in the same way as B2C, you know, companies tend to be.

06:25.000 --> 06:33.000
Yeah, I'd like to maybe dig into that a little bit and talk a little bit about some of the ways that B2B is, is different.

06:33.000 --> 06:50.000
Absolutely. So, first of all, I think they're like two type of B2Bs that are very relevant for ML and that are really, really at the beginning of their journey. One is, B2B company like Tuzo, Coveo, and so on, which are really ML companies, as in we serve businesses with ML.

06:50.000 --> 07:00.000
Models is what we do. And so, as you can guess, ML opting this company is super important, because what you really do is serve model a scale for hundreds of thousands of customers.

07:00.000 --> 07:06.000
So, that's one important type of B2B, and of course, you know, most of my experience direct speed has come from that.

07:06.000 --> 07:19.000
But there's a lot of other super important potential in B2B app, the expensive file, this word, the gust of this word, you know, the SaaS that actually are eating the word in many sense inside the SMB's or enterprises.

07:19.000 --> 07:30.000
So, small models are part of their offering. I don't know, expensive I may have a machine learning model identifying the type of food that you are expensive or something like that.

07:30.000 --> 07:38.000
And these two, these type of companies are very different challenges than the people building a recommender system for a B&B, for example.

07:38.000 --> 07:47.000
And I like to think about usually three dimensions of differences. One is data, which is most, you know, quality, quantity, variety, and so on and so forth.

07:47.000 --> 07:57.000
Another is modeling, like the actual model that actually will do the job and a type of effort that is required for the model to be successful and third one is tooling.

07:57.000 --> 08:02.000
Okay, let's say you fix the data with a lot of work. Let's say we fixed the model and we can discuss a lot of work.

08:02.000 --> 08:06.000
But the last part is how do you make that productive at scale?

08:06.000 --> 08:18.000
But at scale, it doesn't mean one single website visited by 100 million people like Amazon every day. It may mean a hundred enterprises with, you know, 2000 access each.

08:18.000 --> 08:23.000
They used to have to run in parallel. And of course, as you can imagine, tooling and automation is super important.

08:23.000 --> 08:30.000
So, these three dimensions are very different between the typical B2C use cases and all the B2B use cases that have seen in my life.

08:30.000 --> 08:44.000
So, your first set of B2B companies are folks that are creating models. The second are, you know, B2B, the example you gave was a B2B software company.

08:44.000 --> 08:57.000
To what extent do the differences that you see also apply to kind of individual B2B companies that are trying to take advantage of ML to serve their users, their customers.

08:57.000 --> 09:07.000
So, I think it's, for them, the sophistication of model compared to Acobay or Tuzo this word is going to be a bit less simple.

09:07.000 --> 09:14.000
If we go to these access, so the data party may be easier because the data that this company uses are data that they own generating their own platform.

09:14.000 --> 09:20.000
So, in some sense, they control that part, which is not true for model providers.

09:20.000 --> 09:30.000
But the modeling aspect, maybe even if we can say maybe even less sophisticated because at the end of the day, that model is really part of a bigger value proposition to serve the customer.

09:30.000 --> 09:36.000
While if you are a company that sells models as an API, you literally sell models. Your model better be good.

09:36.000 --> 09:43.000
So, that I think that's a fundamental aspect there in terms of the evaluation of the company or the marketing or differentiating.

09:43.000 --> 09:55.000
When you think holistically, not just the ML, but our ML play a role in the company's success. People that do models for a living still have a higher bar to somehow execute in the ML part.

09:55.000 --> 10:01.000
Then people that build B2B SaaS, whose component may be powered by ML in some sense.

10:01.000 --> 10:07.000
The still a lot of challenges there, especially on the tooling side if you go around the three dimension.

10:07.000 --> 10:15.000
But I think it's fair to say that people don't buy expense because of their ML models. They buy expense because of the experience, because it's so well thought and so on and so forth.

10:15.000 --> 10:22.000
But there's an argument to make the people buy Coveo or Brumic or whatever because of their model. That's literally what this company sells.

10:22.000 --> 10:30.000
So, I think even the type of people that are going to hire, the type of resource that you're going to invest, they kind of change a bit between these two types of B2B companies.

10:30.000 --> 10:34.000
And again, B2C is a completely different game, in this sense.

10:34.000 --> 10:46.000
Yeah, before we dig into data modeling and tooling, I'd love to dig into the value proposition that you alluded to a second ago.

10:46.000 --> 10:56.000
The way folks in B2B think about ROI and how that differs from folks on the B2B side. What's your experience there?

10:56.000 --> 11:05.000
So, I think a huge, well, a huge part of being good at ML is connecting ML to something that the company will work for care about.

11:05.000 --> 11:10.000
I know that sounds like a cliché now everybody says that, but it really, it really is.

11:10.000 --> 11:27.000
And the crucial thing when you consider a recommender system at Airbnb versus a recommender system in B2B is that there's a much harder to detect and make the case or even to somehow orient yourself in the B2B case than in the B2C case.

11:27.000 --> 11:33.000
What do I mean by that? You work for a recommender system, let's say Amazon, even a super clear case.

11:33.000 --> 11:40.000
You make a 1% improvement of the recommender system that translates in a building dollar at the end of the year.

11:40.000 --> 11:49.000
Everybody claps your hands, you know, you're paying for the size of your team, you know, Jeff is happy, Jeff is not there anymore, but whatever, like everybody's happy and that's fine.

11:49.000 --> 11:53.000
It's not that easy, of course, is a simplified, you know, stripped on version of the entire story.

11:53.000 --> 12:01.000
But there's a clear sense in which what you do is immediately somehow available to the rest of the company to build upon and have success.

12:01.000 --> 12:07.000
And that's a very clear case. But now think if you're building a recommender system for a hundred e-commerce that are not yours.

12:07.000 --> 12:11.000
You don't owe the e-commerce, you don't own the data, you don't know the final shoppers.

12:11.000 --> 12:19.000
So let's say you make a 1% improvement in your recommender system. What happens to your company? Well, nothing.

12:19.000 --> 12:34.000
Why? What? For two reasons. One is because your 100 customer, your 100 enterprise customer you're serving are not Amazon, you know, each of one of them are not Amazon, right? So it means that 1% improvement for that is not going to result unfortunately in a billion dollar of revving it more.

12:34.000 --> 12:42.000
But even more subtly, even if that was the case, your typical business model being a SaaS provider is not related to improvement directly.

12:42.000 --> 12:59.000
So even if you made a 1 billion dollar for them more, you're not going to pocket the money directly. The business model of SaaS is typically, you know, a consumption or, you know, or whatever is, but it's something along that line, which means that improving the model per se is not as no short term game.

12:59.000 --> 13:14.000
Okay, of course, if your model is not going to buy your part, so you're going to go out of business, but it's a much indirect relationship between what you're doing right now and what the product needs. That's even harder to measure in the expensive I case, for example, you don't even sell a model.

13:14.000 --> 13:31.000
So how can we measure if the reccom, if the vision system that I specified is built is truly is truly aligned with the company, maybe savings and cost, that's an easy way doesn't measure, but how much are customers that appear because of that that's that's much harder problem to solve.

13:31.000 --> 13:45.000
So one thing that I find in practitioner in the B2B space that are very good, the people that I look up to is the ability to still make a good case of why we should invest in that and to find some proxy or what it means for us to be successful.

13:45.000 --> 14:09.000
Okay, even if we can make this a bit testing to recognize comparison like Amazon would. Yeah, what are some examples of those kinds of proxies. One that I really like is well robustness, which I think is super important robustness in two senses, one is the amount of maintenance and kind of like manual work that having this model in production required.

14:09.000 --> 14:16.000
Itan yesterday said the child itan yesterday said you know that's you know the training model, you know, it's easy if you take up the arpegg.

14:16.000 --> 14:23.000
Imagine doing that for 600 clients at the same time, so you really need to get your stuff together to be able to do that scale.

14:23.000 --> 14:32.000
So the ability of you to go to retraining and and you know all the all the deployment phases very smoothly is a very important part of how you build a solution.

14:32.000 --> 14:44.000
So one of course is metrics right as in you need to be able to monitor what you're doing and to make sure that in some sense you're providing a good service to your client, which may be bottom line, you know, you're.

14:44.000 --> 14:59.000
MMR should should always go should always be be in a boat park or whatever or you can be more fancy and do you know be able to testing all sorts of tests that are like you know very important for people to make sure you don't do stupid stuff on somebody else's property.

14:59.000 --> 15:13.000
So let's dig into the first of the kind of those tangible differences that you mentioned and talk a little bit about some of the ways that data is different in B to B context.

15:13.000 --> 15:17.000
Imagine that typically one issues that there's a lot less of it.

15:17.000 --> 15:19.000
Is that the case?

15:19.000 --> 15:33.000
Yeah, in my experience, there's a lot of that or it's even softly. So there's a lot of them cumulative. So there's there's a lot of you know if you take together all the customer that are big to be company may have you made up with a pretty significant chunk of data.

15:33.000 --> 15:45.000
But then when you go and look at the ML deployment, ML deployment are always per customer of course for all sorts of you know common sense but also legal and privacy, you know, complication that you can all imagine.

15:45.000 --> 15:57.000
So at the end of the day, you end up instead of having one big client, you're having like a hundred of them which are big issues sort of that but they're not as you know as big as you may as you may have with you know with an Amazon or more or something like that.

15:57.000 --> 16:13.000
So that's problem number one of course which is well which make some impact on the modeling that you can do as everybody knows there are some models that are you know more prone to you know to work very well with a lot of data but it don't really work you know with small data and vice versa.

16:13.000 --> 16:31.000
So the data constraints somehow the second aspect which is the modeling aspect of course but the second part which is mixed which I think is really the hardest part is that it makes it very hard to scale the you know the data part without making you know without making unnecessarily costly or for the company.

16:31.000 --> 16:43.000
What do I mean by that if you have one one data source is like your Amazon recommender and people click on Amazon then every time you make an improvement even when you collect some new data that goes fed in into one big data distribution.

16:43.000 --> 16:55.000
The model is trying to capture in some way but if you have a hundred of them every time somebody clicks on something that just impact one of these are hundreds more distribution doesn't translate to any to any of the other one.

16:55.000 --> 17:11.000
Which is one of the reason why building an out company at scale as a very different unit economics that building sus at scale because there are some fixed costs that don't really go down the more customer you have because you still have to chase this long tail of prediction every time you collect new data points.

17:11.000 --> 17:24.000
So that would say it is a you know major major point when you plan a company thinking like an entrepreneur when you plan a company in this space need to be aware the B2BML company are very different beast than normal B2B software.

17:24.000 --> 17:30.000
And of course another one is data quality right in data and data data standardization in general.

17:30.000 --> 17:44.000
If you own your website if you are Amazon you can interact literally with every part of the system to make sure you know to some extent that everything flows seamlessly or at least you can ping somebody in your company and tell into fix it.

17:44.000 --> 17:57.000
If you are downstream from that like if you provide the commerce with a recommender system the website is theirs so even this relationship of data feeding is way less direct of course you can ask them you can ask them to be better.

17:57.000 --> 18:10.000
But it's not that you can literally ping somebody in your same company to fix that they may have a different timeline that you have so that makes your job much much harder because again this is one customer think of like a hundred different customer with a hundred different.

18:10.000 --> 18:24.000
Data in some sense and you all need to be standardized to be working with one model type it's it's really challenging in that sense and involves a lot of processes not just code you know it's one of those problems that cannot be solved just by having good code.

18:24.000 --> 18:31.000
So the problem that is involved actually people and you know the people problems are the artists to solve in general.

18:31.000 --> 18:51.000
So it sounds like one of the implications of this idea of kind of not owning the the full stack to the customer is that if you have some cool idea for wanting to collect more data to enhance your model you might not be able to execute that or not easily.

18:51.000 --> 19:20.000
So yeah in most cases you may not because you have to go back to your customer to the ones the front end to maybe change instrumentation or at something or convincing that this experiment is worth doing right but that of course you know length and the feedback and the cycle of operation which is why my suggestion is always when you start logging things try to log as much as you can from day one in that domain because truly you never know when that is going to be is going to be useful.

19:20.000 --> 19:35.000
Like I remember like my back in the Tuesdays the 2017 when we started a company we started geo coding to a rough extent for example the city level where our shopper was doing was doing shopper like completely anonymous but at that time.

19:35.000 --> 19:49.000
We didn't even have any geographical feature in our model whatsoever it turns out that we never add one even when we sell when we sold the company so we never use that but we plan for it in advance so that's here after when we just wanted to do some analysis.

19:49.000 --> 20:05.000
To ask ourselves how much weather in that case was atmospheric weather influence shopping we could ask the question without going back to our clients because we have a couple of years of data that at that time we didn't know it was useful but we just thought you may

20:05.000 --> 20:17.000
got to be useful but at that time you know they basically give us like this this treasure that we can just go in mind for new insight so my suggestion is always complying with you know laws and privacy and so

20:17.000 --> 20:26.000
ever but log as much as you can because you never know what the new feature is going to be or you never know what you knew cool ML idea you're going to have.

20:26.000 --> 20:41.000
Now does that practically speaking did you have a process where if anyone came up with an idea that was logged immediately or you know was there discussion about it and just the bar was very high to say no we're not going to log that.

20:41.000 --> 20:45.000
Yeah I think more of a set was ever came up with logged.

20:45.000 --> 20:59.000
The cool thing about it commerce that it commerce is a very advanced field compared to other type of verticals because because it comes like some of the best they are company in the planet that it commerce of course and even so everybody commerce is kind of.

20:59.000 --> 21:12.000
Kind of understand the idea you need to log a lot of stuff tools like Google analytics for example that almost everybody uses already put their idea into our you know into retailers head so when you come in as a provider of

21:12.000 --> 21:39.000
recommendation is say I want to log as much as Google analytics does everybody is typically happy to kind of like consider that and Google analytics local lot of stuff so the truth of the matter is that we are basically a protocol to copy in a good sense is like hey these will Google logs to provide people feedback and this is condense years of analytics in retail so if we start from this list we can be that wrong and it was actually you know it was a very good list.

21:39.000 --> 21:50.000
It's also a lot of stuff but again there's a press conference you can point to and say hey if Google did it and you're happy with it as a provider you know let us do the same basically.

21:50.000 --> 22:04.000
Yeah yeah I want to take a second here to encourage folks in our audience to chime in with questions via the chat on the platform I will be working those into our conversation looks like we've got a couple of folks.

22:04.000 --> 22:21.000
In the chat that are working on B2B or I've mentioned that they're working on B2B there I'm sure there are others but it also strikes me that a lot of this conversation while it doesn't necessarily or well while it may not apply to kind of the.

22:21.000 --> 22:32.000
The team working on the biggest problems at Facebook or Uber there probably are teams in a Facebook in a Uber that have a lot of these same challenges.

22:32.000 --> 22:46.000
Absolutely the enterprise facing all the enterprise facing ML application even a Netflix or Uber or Google they resemble for many of these reason way more the B2B word or practitioner then their counterparts in the same company.

22:46.000 --> 22:57.000
A discussion that I have all the time with the creator of metaflow like Bill and Sabine came from Netflix is exactly that like metaflow is an open source framework that is born at a big tech.

22:57.000 --> 23:19.000
Mostly to satisfy the use not with the people building the recommender system but literally everybody else that in that case you know resembles way more you know my recommender system life than their own that they're more similar to me in some sense then to their own colleagues for that for that in that respect totally totally and I think there's an argument to be made.

23:19.000 --> 23:46.000
That's in the next 10 years the biggest but or deep most deployments of ML solution are going to be inside of enterprises all sizes more than you know does the B2B the B2C or consumer facing so this is a huge amount of potential there if you get better at this everybody's going to benefit not just the companies but also of course you know the consumers in the end you know and all the process that gets in line thanks to the college.

23:46.000 --> 24:02.000
Yeah yeah so maybe a question that folks can reflect on or uses a prompt for their questions are you know how can you scale down machine learning ML ops to meet the smaller use cases or or these B2B use cases.

24:02.000 --> 24:12.000
But let's jump into some of the differences on the modeling side between what a Facebook might do and what B2B company can do.

24:12.000 --> 24:31.000
So I think a lot of the a lot of what we said before it's kind of a super relevant for model like a lot of like of course we all like to build model like model is the could part like model is you know it's the shiny thing that gets on the on the CEO you know slides or like you know on you know on whatever or whatever covert or whatever you care about.

24:31.000 --> 24:45.000
And of course models you know are very important and nobody nobody saying nobody saying they're not. But one thing that we realize is a field especially you know with a bunch of innovation the last couple of years is that model up to a certain extent are getting commoditized.

24:45.000 --> 25:00.000
As in there's a bunch of back practice and architecture we know they kind of work you can kind of throw you know you cannot throw tabular data to gibberish you can kind of throw you know language data to you know to a transformer and you know it's going to be it's going to be fine you know it may not be the best thing ever

25:00.000 --> 25:07.000
but it's going to be fine enough for you to start iterating which again is the most important thing like getting fast to production.

25:07.000 --> 25:19.000
I think that that makes people realize that unless your model is mission critical and tied to revenue in the same sense that we discussed before so less 1% of better modeling makes you a billion dollar.

25:19.000 --> 25:30.000
I think that people then realize and when modeling is solved there's the entire thing around models before and after which is I think is conventionally what ML ops refer to there is actually the hard part.

25:30.000 --> 25:42.000
Like we thought for some reason what we thought was the cool PhD stuff is the one that we saw before because it's mostly a code problem not a people or a process problem so we kind of saw modeling in some sense.

25:42.000 --> 26:11.000
But now we're left with it with the master models do before and after and so I think that that's an important lesson to be taken here so at the reasonable scale as we as we say models are important up for certain extent but after you reach that level of performance the software use cases that's typically not much of an interest or an ROI in spending another months you know tuning that or tweaking that or you know trying another architecture and so on and so forth again that's very different from.

26:11.000 --> 26:29.000
The advertising machine learning model at Google which I don't know what it does but you know if you improve that by 0.1% that's going to be a success for the company so at some point also the type of people the main joy working in these two teams maybe different somebody is the guy that wants to build stuff from 0 to 1.

26:29.000 --> 26:44.000
And solve the end to end use cases the other guy is the ultra optimizer you may be the guy that for 10 years just does the ranking optimization just because it does that and I think there's also reflect you know different personality of different choices that company to be successful.

26:44.000 --> 26:51.000
And so you're saying that B2B it B2B might not be the place for that ultra optimizer.

26:51.000 --> 27:03.000
I thought I think it's not I think what you like to do is you know is shaving off the last 0.5% of emerald yes you're probably better off at amazon then thank you absolutely.

27:03.000 --> 27:20.000
We had a great conversation about build versus buy yesterday afternoon and there's some implications here it sounds like you are your proponent of build probably more so than you know folks at Facebook or Uber.

27:20.000 --> 27:24.000
For B2B companies sorry buy.

27:24.000 --> 27:38.000
Yeah I'm a huge yes I'm kind of like a huge a huge buy person in that sense where buy where buy and I think it reflects also some discussion that you have in yesterday buy doesn't literally mean just you know buy and forget.

27:38.000 --> 27:47.000
But you of course means buy me also mean using off the shelf open source you know like the metaphor of my example before like so buy is in you use something that a lot of people did.

27:47.000 --> 27:57.000
But I don't really don't build exactly but you have to take the you know the you know the the working yourself to kind of make it work with the rest of your stock and that's not really that's not the case.

27:57.000 --> 28:03.000
I think the value is in the B2B especially is understand your.

28:03.000 --> 28:09.000
What what your company what your company needs from email and I said what makes you unique and what's your unique take on it.

28:09.000 --> 28:23.000
In our case a cover has been for example session based recommendation that recommendation the work without assuming to know anybody from the user which was a huge innovation the B2B space is you know it's the only is the only offering of the kind available still today.

28:23.000 --> 28:38.000
And then everything else buy or you know quote unquote buy like don't build my customer doesn't care if I'm using metaphor if I manually spin up GPUs my customer doesn't care if I use you know snowflake or you know if I manually you know.

28:38.000 --> 28:57.000
Wrangle CSV files so for me I just buy everything that makes me productive and it makes me get the result as fast as possible in most efficient ways possible my customer cares about recommendation though so I need to be good at that so once once in your mind is is clear what adds value to your company.

28:57.000 --> 29:15.000
Optimize for that and everything else buy at least in the beginning because you don't know you know what you don't know in the first buy and use what other people have been doing and then you can always go back and change your mind is much harder to do the other around spending six month you know building something you don't realize you don't need.

29:15.000 --> 29:43.000
Yeah yeah Dan asks in the chat one way that that his company's approached this is to really focus on reusability so for example with document understanding try to build tools that can be reused reused across different document understanding use cases I guess this is a an example of build or sorry buy.

29:43.000 --> 29:48.000
You're kind of buying the thing that you built previously maybe.

29:48.000 --> 30:05.000
Do you see reuse is playing a big role in efficiency for BDB teams when you can that's that's a good strategy for for the unit economics problem we mentioned before but mind you while English is English everywhere so sure I learned something about English with my model I can reuse in an angling

30:05.000 --> 30:21.000
English document recommendation on this case for example because if I recommend an electronics shop or recommend something in a shoe shop of course I can reuse what I learned about electronics to sell you bags so depending on what you actually end up doing there may be somebody

30:21.000 --> 30:24.000
usability to it which is great if you can please do that.

30:24.000 --> 30:38.000
However it turns out that the word is a such that a lot of very successful application of a mail at scale I actually per deployment so there's not much you can use aside from the code of course that you can just bought from one to the other.

30:38.000 --> 30:58.000
John asks about referring back to your comment about logging information that you've got even if you don't need it what about the notion that if you ask for data that you obviously don't need a proportion of that data will be junk data you run into that.

30:58.000 --> 31:17.560
What's the problem I will ask back what's the problem with junk let's say let's say we ingest data I obviously don't need the need today like let's say what's what's the worst that can happen is just going to see that in my stuff like a nobody's not going to fit right I I think I'm not suggesting wasting money on on storage you don't need that's that's not an

31:17.560 --> 31:33.560
message here by generally speaking we storage becoming exponentially cheaper over the years the cost of your missing a use cases because you didn't lock something and now you have to go back and log it and wait six months to have enough data is way more than logging up the B a bit more

31:33.560 --> 31:41.560
generous in the first place and pay a couple of hundred dollars more to snowflake every month that's for sure that's for sure the worst thing can happen in people building a

31:41.560 --> 31:48.560
small feature is having an idea that will put your company ahead and realize you need to wait three months to collect enough data to even see if that works.

31:48.560 --> 31:53.560
That's really the worst thing I can.

31:53.560 --> 32:02.560
So the next differentiator on your list is tooling we talked a little bit about this you know from a build and buy perspective are there

32:02.560 --> 32:11.560
other differences in the way you see folks tooling for B2B. So I think the bit so the bit the planetary scale comes comes with a couple of

32:11.560 --> 32:18.560
cavet right first of course a lot of custom system because they need to play nicely with other custom system so at some point the custom system just you

32:18.560 --> 32:26.560
know ask for another custom system and the other thing that you may have latency or scale or security whatever constraint that I'm not just there for the

32:26.560 --> 32:35.560
official market. On the on the on the smaller side the reasonable scale side there are two things first you're not as unique as you think as well as you're not

32:35.560 --> 32:43.560
as special as you think meaning that in a good set meaning that the problem that you have a problem already been solved by somebody because you are in the

32:43.560 --> 32:51.560
proportion of the distribution that is fairly that where that there's a few companies there. So if there's a tool that is being proven successful for you know small

32:51.560 --> 32:59.560
enterprises chances is it's going to it's going to work for you it's going to you for you as well so that's that's one part and the second part is you don't have

32:59.560 --> 33:06.560
them many people lying around probably like you don't have like a hundred engineers to build a custom tool over a custom tool so buying is also a way for you

33:06.560 --> 33:13.560
to use the people that you have in the most effective way meaning that a they don't do frustrating work like glowing stuff or fixing

33:13.560 --> 33:22.560
pipelines or stuff that honestly ML people don't want to do and they're very expensive you know and you know and you know and they're very in demand. So that's one point and second again you get the most

33:22.560 --> 33:29.560
marginal value from that like if if I pay an ML people to the recommender system I don't want you to to end the infrastructure or to fix my

33:29.560 --> 33:36.560
Kubernetes setup like you know I want you just a proper abstraction that allows that person to be to be in that sense to be productive a bigger

33:36.560 --> 33:46.560
question would be is only sauce when you say don't buy when she don't build it's literally buy as offloading entirely what you need to do with a

33:46.560 --> 33:55.560
platform or it may be an intermediate solution when you reuse some tools but you actually hosted yourself and that is more subtle is in the maybe

33:55.560 --> 34:09.560
initial scale when you have to offload everything like even an orchestrator for example like typical example say airflow very popular orchestrator in the open source world but is also at least too very famous platform you know AWS and

34:09.560 --> 34:14.560
astronomer that you can buy. Then after a while if you decide well you want to know what.

34:14.560 --> 34:22.560
This is everything I want to orchestrate myself because I'm big enough it's important enough you can always bring it back to in house and somehow do like in between you

34:22.560 --> 34:29.560
have to build and buy the open source to me is like an in between you have some cost of the building because they have to maintain it and integrated but also some

34:29.560 --> 34:38.560
future of the buying as in this is a proven tool that has been used a hundreds of time before so if I use it again I'm likely to be within the 90%

34:38.560 --> 34:43.560
range of of you know of capabilities that I actually need to do my job.

34:43.560 --> 34:54.560
I thought that was a really interesting point about building in particular you kind of reference this hitting cost of building is that it predisposes you to

34:54.560 --> 35:01.560
needing to build the next thing and the next thing and the next thing after that because you can't you're not using off the shelf things and they're not easily

35:01.560 --> 35:15.560
integrated or it requires work to integrate you you're not selecting from a vendor's partners of all the things that you know it could integrate to fill in a particular slot.

35:15.560 --> 35:25.560
Yeah and there's also incentives in this network FF for vendors to integrate with other vendors right so if I started with a vendor and open solution I'm likely to have like you know again if you start with

35:25.560 --> 35:36.560
metaflow you know they're going to have an air flow integration right because because there are two popular tools if I'm an orchestrator Jacob of law whatever it is now I have to do you know that's not do that.

35:36.560 --> 35:46.560
But now I have to do that myself so that's not it's not a reasonable point I think there's a lot of cost like especially small scale especially startup scale that I know well

35:46.560 --> 35:59.560
opportunity cost is what's going to kill you like what's going to kill you is not moving fast enough to build a new thing and by definition if you're just building infrastructure that you could have bought you're not building the next cool thing.

35:59.560 --> 36:20.560
One of the themes that played very heavily in that bill versus by conversation yesterday was fear of lock in do you have how do you parse that on this I'm really okay I'm really a contrarian and I really don't care.

36:20.560 --> 36:41.560
I used for my last five years as a career one cloud provider and I'm and I'm fine like this speed and and you know reliability that I get by kind of using the past solution is way more than anything else that we need to do if I go you know if I if I kind of move out of that.

36:41.560 --> 37:10.560
So I think people say sometimes well I may move out or WS or Azure or whatever honestly how many times did it happen like I certainly I certainly never did it and I don't plan on doing it again I think and I think this applied to every color provide like their solution when you when you when you start in the past approach are kind of so well integrated typically that at some point is an ecosystem that you know well when you are somebody you have best practices so I understand the general concern as a concern.

37:10.560 --> 37:30.560
As a conceptual thing but I think at the scaling which I believe my most of my life in the last five years the gain of of loading all the problem to AWS like significant this or passes the question of like well what if AWS is the price is for 10% more next year.

37:30.560 --> 37:47.560
So the one thing that wasn't on your list you know again data modeling tooling was team and I imagine the team set up is going to be different between B to B and B to see you've talked about one of those ways.

37:47.560 --> 37:58.560
You know not you're not likely to find those kind of optimizers fraction of percent optimizers are there other ways the teams differ.

37:58.560 --> 38:09.560
Well absolutely so I think there are there's a bunch of things that we learn at B to B to be productive they may not necessarily translate to B to see but also they may not necessarily be the norm at B to B.

38:09.560 --> 38:27.560
So one thing that we would really like is the idea of the end to end ML person which again it's kind of a controversial a controversial idea which is the idea that your your ML person is responsible for everything going from the data in snowflake to shipping the model into production and making sure that works.

38:27.560 --> 38:37.560
So it needs to be comfortable with SQL to extract the data and make sure that it is okay needs to be comfortable with building the model pipelines needs to be comfortable with your shipping into production.

38:37.560 --> 38:50.560
Why because well first we want to enforce ownership and then proud of your work so no end off no I build the model and then I give you the notebook to somebody else and then is his fault in somebody goes better no it's your entire thing.

38:50.560 --> 38:59.560
And second because again of kind of efficiency right as a smaller scale we can have that many people around like how many people do we even need to start the project.

38:59.560 --> 39:07.560
Now if I needed that engineer a data analyst an ML person a DevOps person just to ship one model in production time for salaries and I don't even know if they need them.

39:07.560 --> 39:14.560
So our and I think that ties back back to the build part our job as leader in B to B context.

39:14.560 --> 39:22.560
Is to make sure that if you hired this end to end ML people people that really care about building a product a feature by themselves.

39:22.560 --> 39:32.560
You abstract away all the you know chores infrastructure not needed problems they may have and that's why I want to build everything to buy everything.

39:32.560 --> 39:41.560
I don't want my data sign to to to be preoccupied with Kubernetes or to scaling spark or to you know or to deploy manually into a pod.

39:41.560 --> 39:52.560
All of this can be abstracted away from that but I wanted to be able to take ownership of the business logic that takes the data into a model into prediction and to feedback.

39:52.560 --> 40:03.560
That's what I pay them for and my job as a leader and other senior people it to give them the building blocks and the legal blocks that they can assemble together to get the job done.

40:03.560 --> 40:14.560
And me as a leader the best way for me to do it myself stop building it myself is picking from the vendors or the open source project that I like what I believe will make the experience of my team the best possible.

40:14.560 --> 40:21.560
And that I think is a for us is really a great recipe of how small be to be team can actually be very productive.

40:21.560 --> 40:29.560
Of course at Facebook scale that doesn't apply Google scale that doesn't apply like this entire teams to build the infrastructure for you but again you have different constraints.

40:29.560 --> 40:44.560
But the reasonable scale we find the job of leaders at least in our in our case to be give you the tools and then get out the way and then hold you responsible and accountable for the entire thing not just for one time it is.

40:44.560 --> 41:10.560
Yeah yeah tomorrow morning this time on our agenda we've got a fun debate plan to dig into and the end versus specialized tools I'm sure you have an opinion on that as well but I brought that up to reference the debate we had last time last year's Tom O'Conn and the debate question was should every data scientist and ML engineer learn Kubernetes.

41:10.560 --> 41:38.560
Absolutely no absolutely no but they should understand like you know they should they should ship that model to Kubernetes or such maker or whatever you're using because somebody put the abstraction in place for them and that they should follow the life cycle of that to make sure that the model does you know what what I think you do so they need to understand the system perspective but I don't want them to do YAML file.

41:38.560 --> 41:41.360
Exactly.

41:41.360 --> 41:41.360
Exactly.

41:41.360 --> 41:53.560
Yeah they are there are there are there are there for me like you know I know I don't I don't know what the team to do that but by the really to become one with the data like this old idea the now just people that are custodian of the data or custodian of deployment.

41:53.560 --> 42:07.560
And then this data scientist the middle doing notebooks is againaring is enough is really not a good productive system for company again that they have you know they have a limited amount of resources in that sense.

42:07.560 --> 42:16.560
So the kind of team member that you describe sounds like what you might call a full stack ML person.

42:16.560 --> 42:22.560
That's, you know, they're working on a stack that you've provided for them through buying.

42:22.560 --> 42:30.560
But from the perspective of your, you know, product or workflow, they're kind of going in to end.

42:30.560 --> 42:41.560
Are there, do you have you found challenges in, you know, hiring that full stack person, you know, especially, you know, given that you're not Facebook.

42:41.560 --> 42:45.560
Yeah, I mean, like staffing is a, of course, the elephant.

42:45.560 --> 42:47.560
Everybody outside of five companies.

42:47.560 --> 42:52.560
And honestly, even in fine companies, like, because they're keeping on stealing each other people.

42:52.560 --> 42:55.560
It's a, it's a, it's a, it's a, it's a problem.

42:55.560 --> 43:10.560
One thing that I, at least in my experience is that young folks with proper guidance may actually be surprisingly quick to get the, to get up to speed to, to this way of working.

43:10.560 --> 43:16.560
Again, because you're not teaching them Kubernetes or, you know, scaling spark.

43:16.560 --> 43:20.560
But because you're teaching, you know, logic that it will make them more productive.

43:20.560 --> 43:27.560
So in my experience, you know, that, that's one way of being productive. Another one is, well, you need less people.

43:27.560 --> 43:30.560
Like, if you set up your team properly.

43:30.560 --> 43:35.560
Two end to end ML person will do the job of 10.

43:35.560 --> 43:38.560
There are like, kind of continuously ending off stuff.

43:38.560 --> 43:41.560
And there's a notebook, the notebook breaks, the dependency doesn't work today.

43:41.560 --> 43:43.560
It is not clean. Nobody knows what in the table.

43:43.560 --> 43:46.560
So at the end of the day, I know it sounds counterintuitive.

43:46.560 --> 43:52.560
One more expensive person will actually cost you way less than five cheap one.

43:52.560 --> 43:57.560
Just because the five cheap one now, now they're not going to even compare to the one that you have.

43:57.560 --> 44:06.560
One, one, one thing that I noticed is that good ML team outside of big tech always look understaffed from the outside.

44:06.560 --> 44:12.560
When you look at it and people told you how many people did that, you're always surprised like, well, that's, that's very, that's very few.

44:12.560 --> 44:18.560
But the secret is, you know, if you take away the bottom lack of infrastructure, you need surprising this more

44:18.560 --> 44:23.560
amount of people to build great things. I think that's been our experience.

44:23.560 --> 44:31.560
You know, there's still, I imagine the, you know, you've hired engineers, even though you've only hired two.

44:31.560 --> 44:37.560
They're still going to have that desire to run off and scratching it, right?

44:37.560 --> 44:44.560
You know, there's this hole in this thing that you bought me. I love it. You know, can I spend three months building?

44:44.560 --> 44:49.560
You know, the thing that would perfectly fit in that hole to automate everything and make everything wonderful.

44:49.560 --> 44:52.560
Like, how do you, how do you deal with that?

44:52.560 --> 44:55.560
That's, that's, that's a, that's a very good point.

44:55.560 --> 45:02.560
Well, the general thing is that if that's not cool, like if that whole, like, you know, if it's in perfect, it is not really what we need.

45:02.560 --> 45:07.560
Like, you know, let's use our time to do, to do better stuff. Like there's no shortage of cool things to get.

45:07.560 --> 45:12.560
My point is, there's a shortage of cool things to do in, you know, in the M L word.

45:12.560 --> 45:21.560
So let's not touch what's working with us. But if at some point is an existential piece, you know, by all means, let's, let's do that, especially if it's open source.

45:21.560 --> 45:25.560
Like, you know, building a such platform is typically not a good investment of anybody's time.

45:25.560 --> 45:33.560
But if it's an open source tool and you can fork it or extend it in some ways, some, some tools are better than others in that sense and they make it easier.

45:33.560 --> 45:42.560
That's totally fine. Like, you know, I, I completely, we as a team, like, extended functionality of metaflow several times with our own custom stuff.

45:42.560 --> 45:46.560
We still have the core of the platform, however, on custom stuff on top.

45:46.560 --> 45:54.560
But the platform design is such a way that, you know, that we don't kind of like, you know, destroy the good just to have the perfect fit with our, without use cases.

45:54.560 --> 46:04.560
So one thing that I would say to leaders, picking the tools, keeping this, you know, keeping these use cases in mind as in what happens if it doesn't really fit, maybe today fits 100%.

46:04.560 --> 46:11.560
But what happens if tomorrow is 90? How easy will be for my team without disrupting everything to get the last 10%.

46:11.560 --> 46:15.560
And I think that's an important, you know, variable when you decide, you know, which tool to pick.

46:15.560 --> 46:18.560
Of course, again, with Suspac, it's much harder to do this.

46:18.560 --> 46:24.560
Yeah, yeah. Granted that hiring is, is always difficult.

46:24.560 --> 46:32.560
Samir is curious if you could comment a little bit more on, you know, what you've done to improve hiring.

46:32.560 --> 46:40.560
Samir observes that adding humanity to the recruiting process has had an impact, but it's not something that many seem to do.

46:40.560 --> 46:44.560
Are there things that have worked for you?

46:44.560 --> 46:54.560
Being very outspoken and public about what we do, it's been very helpful, not just in a, you know, influencer sense.

46:54.560 --> 47:07.560
I like literally doing a lot of open source, publishing our research ideas in the public for everybody to read, going to conference, going to event, be very vocal about the type of problems that we add and be very, you know, very honest.

47:07.560 --> 47:14.560
And the type of what you're going to do with us, right? You're going to do cool ML stuff at this scale solving this problem. So if you want something else, we're not for you.

47:14.560 --> 47:24.560
So that's point number one, like, you know, you need to have a brand in the, in the, in the good sense, as in you need to be, you know, a person that is distinguishable the community that does X and Y.

47:24.560 --> 47:27.560
And so people will come to you. So that's, that's number one.

47:27.560 --> 47:30.560
Number two, give back to the community.

47:30.560 --> 47:37.560
Like a lot of the young contacts that we have, even for open source project or for research contact, are just because we are there.

47:37.560 --> 47:40.560
Like we are in cheap discord discussing ML ops, we are in the materials.

47:40.560 --> 47:44.560
It's like helping out people when they asked to, you know, we are here today.

47:44.560 --> 47:48.560
Thanks much for having us and another occasion to talk about our experience.

47:48.560 --> 47:53.560
And this is a lot of work, like being present for communities is a lot of work.

47:53.560 --> 47:56.560
But it's also pays off to the community because you get back.

47:56.560 --> 48:03.560
But also the community at some point is going to be there for you because, you know, the next time you build a company, you go and say, hey, I'm building a new company.

48:03.560 --> 48:08.560
Hopefully some people, you know, will be already trust you and know you for what you can do.

48:08.560 --> 48:10.560
And so it would be easier for you to attract talent.

48:10.560 --> 48:14.560
So for us being member of the community has been a huge, but the research part.

48:14.560 --> 48:19.560
So going to conferences and so on and the ML ops and on part, like the open source part.

48:19.560 --> 48:28.560
These two communities for us are a never ending source of joy and also, and also pipeline people.

48:28.560 --> 48:40.560
So Fahad had a question that I think goes back to our earlier conversation about kind of the nuance nature of ROI for B2B teams.

48:40.560 --> 48:50.560
You mentioned the bottlenecks faced by small companies in terms of, you know, achieving sufficient data acquisition and model optimization.

48:50.560 --> 48:56.560
So how do those companies achieve the kind of results that are important from a business perspective?

48:56.560 --> 49:06.560
I think that's kind of getting at, you know, if it's hard to collect data, you know, you don't want to over invest in optimizing models.

49:06.560 --> 49:17.560
Like, how do you really identify where the value is for your team? Are there things that you've, you know, tips that you've identified for really focusing in on that?

49:17.560 --> 49:27.560
Yes, the first thing is put things in production as fast as possible as, you know, as cheaply as possible, possibly without a mail.

49:27.560 --> 49:30.560
The first to do a mail is always never use a mail, right?

49:30.560 --> 49:39.560
Let's say you're building a classification model for sentiment analysis, right? You don't have any data and it's totally fine.

49:39.560 --> 49:45.560
Let's start with building a model that says if the word super cool is in the tweet, it's going to be positive.

49:45.560 --> 49:48.560
If the word bad is in the tweet is going to be negative, okay?

49:48.560 --> 49:50.560
Let's try to figure out for precision. We don't care of cloud.

49:50.560 --> 49:57.560
A lot of tweets are going to go unclassified, but we just want to get a hundred of them every day classified and see how the people react.

49:57.560 --> 50:03.560
See if you can type some value into that because to build that lambda function is going to cost you three minutes.

50:03.560 --> 50:09.560
And then if people never even click on the feature you're building, there's no reason for you to build entire pipeline.

50:09.560 --> 50:17.560
So try always to be incremental. So if you have a sophisticated organization where building your pipelines that are too easy, you know, you can start with a simple model.

50:17.560 --> 50:27.560
But sometimes a day one in an organization, there's no model. There's no blueprint. There's no template. Start with something that doesn't require a mail.

50:27.560 --> 50:34.560
And once you build the case that that thing is useful when maybe people click on it or people complain about it because it's not accurate, which means that people care.

50:34.560 --> 50:40.560
So it's great. Then go back to then go back to you know to everybody else and say, hey, we may have something to it.

50:40.560 --> 50:49.560
Come over over engineers. Again, exactly. Don't try to beat data sparsity, which is an important problem of itself, you know, before knowing if it's worthwhile.

50:49.560 --> 50:54.560
Okay. So that's I think would be my trick to, you know, to try and innovate in that sense.

50:54.560 --> 51:08.560
But the first thing that you mentioned is key also it's what I heard was about iterating quickly and, you know, not just trying things from a modeling perspective,

51:08.560 --> 51:15.560
kind of end-to-end, you know, idea, iterate quickly, get things out in the market and see what matters, what has impact.

51:15.560 --> 51:22.560
Yeah, absolutely. Speed of iteration, bit sophistication like every day of the year. And ML is still a very practical.

51:22.560 --> 51:28.560
Like it's more art and science, you know, don't get fooled by the PhDs. Like ML is really more art and science.

51:28.560 --> 51:32.560
So the more things you try, the higher the chance of success, almost by definition.

51:32.560 --> 51:38.560
Of course, again, with the experience and with sophisticated issue, you get some boundaries and you get some intuition what's going to work and what doesn't.

51:38.560 --> 51:43.560
But there's no substitution for just trying it out, see how people react and kind of iterate on that.

51:43.560 --> 51:47.560
So the faster you can do that, the better it is bonus point.

51:47.560 --> 51:53.560
If you build a good pipelines, you can swap in the modeling part by taking everything else when the time comes.

51:53.560 --> 51:56.560
Or we can improve the data part and keep in the model constant.

51:56.560 --> 52:02.560
Like building good pipelines with good legal groups will allow you to select to be improved.

52:02.560 --> 52:07.560
When time comes, what you need without starting from scratch again.

52:07.560 --> 52:22.560
So you're describing experimentation. One of the big challenges with doing experimentation in a B2B context is the, you know, the signal that you have to compare your results is is, you know, it's less.

52:22.560 --> 52:28.560
So with regards to AB testing and things like that, it takes longer to converge.

52:28.560 --> 52:34.560
Are you, do you, you find yourself making judgment calls on, on less data?

52:34.560 --> 52:40.560
Or are there things that you do to, to collect data more, more quickly?

52:40.560 --> 52:43.560
Or make those decisions more scientifically, I guess.

52:43.560 --> 52:52.560
So some B2B use cases are so heavily, like for the purpose of a T-test to see if I have a lot of it, like e-commerce is a good example.

52:52.560 --> 52:55.560
Even as moly commerce, which generates millions of data every day.

52:55.560 --> 53:01.560
So typically that would be enough to do like a standard key test or whatever you're doing in your, in your, in your, like in your AB testing.

53:01.560 --> 53:03.560
So that's typically fine.

53:03.560 --> 53:10.560
In other cases, is more of a question of like, well, people seem to be engaged, but we want to invest more in that on that.

53:10.560 --> 53:14.560
And that's, that's more of a judgment call with other business folks and so on and so forth, right?

53:14.560 --> 53:19.560
There are some features that, even if they're not needed, they may be needed for the business for other reasons.

53:19.560 --> 53:22.560
Because they're differentiator, because all of your competitors have it.

53:22.560 --> 53:29.560
Even if just three people click on it, you need to have it, because that's, you know, that's the definition of what means to be a platform in some sense.

53:29.560 --> 53:30.560
And so you just build it.

53:30.560 --> 53:41.560
It's a very nuanced, like the point is like, it's what to build is always the hard discussion. How to build it tends to be after a certain level of experience, tends to be the easy part.

53:41.560 --> 53:48.560
But what to build is what we decided together as a team is an organization to focus our effort on is always the hard discussion.

53:48.560 --> 53:50.560
So there's no fast answer to that.

53:50.560 --> 53:56.560
But yeah, sometimes there's data, sometimes they just you and the vision that you have for whatever the company's doing.

53:56.560 --> 54:04.560
Yeah, yeah. I think Andreas in the chat wants to pick a fight with you about recording all the data.

54:04.560 --> 54:15.560
That's something that I'll let the two of you take offline. Andreas, reach out via the platform and schedule a meeting with Jakobo to run that one into the ground.

54:15.560 --> 54:20.560
We're going to log everything during the meeting. So just to make sure.

54:20.560 --> 54:27.560
Also, well, thanks again, Jakobo for a great conversation. It's wonderful to have you on the at the conference today.

54:27.560 --> 54:35.560
Thanks so much for having me and it's been super fun. Guys, if you want to chat more, please reach out on the platform link in the very easy to reach. So thanks again.

54:35.560 --> 54:50.560
Thank you.

