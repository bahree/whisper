WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.640
I'm your host Sam Charrington.

00:32.640 --> 00:36.920
This week on the podcast, I'm excited to present a series of interviews exploring the emerging

00:36.920 --> 00:40.160
field of differential privacy.

00:40.160 --> 00:44.200
Over the course of the week, we'll dig into some of the very exciting research and application

00:44.200 --> 00:47.800
work happening right now in the field.

00:47.800 --> 00:53.120
In this, our first episode of the series, I'm joined by Aaron Roth, Associate Professor

00:53.120 --> 00:58.400
of Computer Science and Information Science at the University of Pennsylvania.

00:58.400 --> 01:03.200
Aaron is first and foremost a theoretician, and our conversation starts with him helping

01:03.200 --> 01:07.800
us understand the context and theory behind differential privacy, a research area he

01:07.800 --> 01:11.240
was fortunate to begin pursuing at its very inception.

01:11.240 --> 01:15.560
We explore the application of differential privacy to machine learning systems, including

01:15.560 --> 01:19.000
the costs and challenges of doing so.

01:19.000 --> 01:23.720
Aaron discusses as well quite a few examples of differential privacy in action, including

01:23.720 --> 01:28.560
work being done at Google, Apple, and the US Census Bureau, along with some of the major

01:28.560 --> 01:34.160
research directions currently being pursued in the field.

01:34.160 --> 01:38.080
Thanks to George and Partners for their continued support of the podcast and for sponsoring

01:38.080 --> 01:39.680
this series.

01:39.680 --> 01:44.160
George and Partners is a venture capital firm that invests in growth stage companies in

01:44.160 --> 01:46.400
the US and Canada.

01:46.400 --> 01:50.960
Post investment, George and works closely with portfolio companies to accelerate adoption

01:50.960 --> 01:56.720
of key technologies, including machine learning and differential privacy.

01:56.720 --> 02:01.400
To help their portfolio companies provide privacy guarantees to their customers, George

02:01.400 --> 02:07.080
and recently launched its first software product, Epsilon, which is a differentially private

02:07.080 --> 02:09.360
machine learning solution.

02:09.360 --> 02:14.120
You'll learn more about Epsilon in my interview with George and Chang Liu later this week,

02:14.120 --> 02:18.120
but if you find this field interesting, I'd encourage you to visit the differential

02:18.120 --> 02:26.880
privacy resource center they've set up at gptrs.vc slash twimmel ai.

02:26.880 --> 02:30.040
And now on to the show.

02:30.040 --> 02:35.600
Alright everyone, I am on the line with Aaron Roth.

02:35.600 --> 02:40.600
Aaron is Associate Professor of Computer Science and Information Science at the University of

02:40.600 --> 02:41.600
Pennsylvania.

02:41.600 --> 02:44.400
Aaron, welcome to this week in machine learning and AI.

02:44.400 --> 02:45.920
Thanks, I'm glad to be here.

02:45.920 --> 02:49.120
Why don't we get started by having you tell us a little bit about your background and

02:49.120 --> 02:53.400
how you got involved in the machine learning field?

02:53.400 --> 03:00.200
Sure, so I'm a computer scientist and I guess I would describe my background as really

03:00.200 --> 03:02.720
coming from theoretical computer science.

03:02.720 --> 03:10.080
So as someone who sits down and tries to understand things by thinking mathematically and proving

03:10.080 --> 03:19.000
theorems and the way I came to machine learning in general is well from a background in learning

03:19.000 --> 03:27.600
theory and in particular the flavor of problems that I've studied both sort of historically

03:27.600 --> 03:34.920
and now have to do with the way that machine learning as a technology interacts with things

03:34.920 --> 03:38.840
that you might more normally think of as societal concerns.

03:38.840 --> 03:45.240
So things like privacy, things like fairness and things that maybe more typically an economist

03:45.240 --> 03:51.600
would think about like how do machine learning algorithms work in strategic situations?

03:51.600 --> 03:58.720
You're also very involved in the work happening around the application of differential privacy

03:58.720 --> 04:01.880
to machine learning.

04:01.880 --> 04:04.920
How did you get started down that route?

04:04.920 --> 04:12.040
Well, so I started my PhD in 2006 which is the same year that the first paper on differential

04:12.040 --> 04:18.200
privacy was published by Cynthia Dwork and Cobine Sim and Adam Smith and Frank McCherry.

04:18.200 --> 04:24.480
And so this was a very new topic at the time that I was just starting to think about

04:24.480 --> 04:34.320
research and it struck me as timely and important and at the same time when I was just starting

04:34.320 --> 04:36.800
to think about it, not much was known.

04:36.800 --> 04:42.440
So it was sort of at the sweet spot for PhD thesis where you can study an important problem

04:42.440 --> 04:46.320
and have a lot of impact without having to do anything to clever.

04:46.320 --> 04:53.320
Well, maybe a good place to start in our exploration of differential privacy and machine learning

04:53.320 --> 04:59.000
would be to have you define differential privacy and tell us a little bit about the context

04:59.000 --> 05:01.120
in which it was created.

05:01.120 --> 05:10.800
Sure, so privacy has a relatively long history in computer science, but until people started

05:10.800 --> 05:16.880
thinking about differential privacy, what people meant when they said privacy, with some

05:16.880 --> 05:23.680
kind of syntactic constraints on what the output of a computation could look like.

05:23.680 --> 05:32.920
And it turns out these kinds of syntactic privacy guarantees don't have a strong meaning

05:32.920 --> 05:39.440
in terms of privacy and there was sort of a cat and mouse game in which people would

05:39.440 --> 05:45.280
attempt to share data sets with some kind of privacy protections and then some clever

05:45.280 --> 05:50.040
person would come around and figure out how to get around those privacy protections and

05:50.040 --> 05:51.360
this would iterate.

05:51.360 --> 05:57.800
Can you give us an example of those types of syntactic constraints and a little bit of

05:57.800 --> 05:59.960
how that cat and mouse game evolved?

05:59.960 --> 06:06.000
Sure, yeah, so maybe the simplest thing that you might imagine is you might think to

06:06.000 --> 06:13.520
yourself, well, if I just remove any identifying attributes from a data set.

06:13.520 --> 06:19.520
So for example, if I've got a data set of medical records, if I just remove things like

06:19.520 --> 06:25.400
name and zip code and maybe a few others, that'll anonymize the data and it'll be safe

06:25.400 --> 06:28.280
to release the data set in the clear.

06:28.280 --> 06:32.240
And unfortunately, that turns out not to be the case.

06:32.240 --> 06:39.120
So there's a bunch of examples of the sort, but maybe the first one was a demonstration

06:39.120 --> 06:41.120
by Latanya Sweeney.

06:41.120 --> 06:45.760
At the time she was a PhD student at MIT, now she's a professor at Harvard.

06:45.760 --> 06:52.840
And the state of Massachusetts had released a supposedly anonymized data set of patient

06:52.840 --> 06:53.840
medical records.

06:53.840 --> 06:57.840
So it didn't have people's names attached.

06:57.840 --> 07:03.680
But what Latanya figured out was that there was another data set that was publicly available.

07:03.680 --> 07:11.080
That was the voter registration records in Cambridge, Massachusetts.

07:11.080 --> 07:17.920
And when you've got two data sets and you know something about an individual, for example,

07:17.920 --> 07:24.000
Latanya knew that the governor of Massachusetts at the time, William Weld, lived in Cambridge.

07:24.000 --> 07:26.360
I knew a few other things about him.

07:26.360 --> 07:32.760
You can basically cross-reference these two data sets and reattach names.

07:32.760 --> 07:40.400
So that's sort of the simplest example for why attempting to remove identifying attributes

07:40.400 --> 07:41.400
doesn't work.

07:41.400 --> 07:45.160
You know, it seems like a good idea in isolation, but in the real world, there's all of this

07:45.160 --> 07:49.240
information out there that you can attempt to cross-reference with existing data sets.

07:49.240 --> 07:55.880
I think another example along those lines was when Netflix released their anonymized

07:55.880 --> 08:02.520
recommendation data set for I think it was the Netflix prize, someone or a set of people

08:02.520 --> 08:07.880
cross-reference that to IMDB and found that they were able to de-anonymize a pretty significant

08:07.880 --> 08:09.400
portion of those records.

08:09.400 --> 08:14.040
Yeah, that was another high-profile example using a more sophisticated technique that

08:14.040 --> 08:18.640
was done by Arvind Narayanaan, who is now a professor at Princeton and Videlish Mottikov,

08:18.640 --> 08:21.000
who's at Cornell Tech.

08:21.000 --> 08:26.960
And that was another example where names were removed, so all that was made available was

08:26.960 --> 08:33.200
sort of a big data set where for each person now identified by supposedly anonymous numeric

08:33.200 --> 08:38.960
identifier, all you saw about them were which movies they watched, what they rated them,

08:38.960 --> 08:40.640
and approximately when they watched them.

08:40.640 --> 08:46.440
And as you say, by cross-referencing this data set with IMDB, they were able to reattach

08:46.440 --> 08:47.440
names.

08:47.440 --> 08:53.680
So differential privacy came about kind of in the wake of the broader realization of

08:53.680 --> 08:56.520
the failure of anonymization, it sounds like?

08:56.520 --> 08:57.520
Exactly.

08:57.520 --> 09:02.320
So I think the key insight that the creators of differential privacy had was that, you

09:02.320 --> 09:08.320
know, if you want to speak rigorously about what someone can infer about an individual

09:08.320 --> 09:13.960
and given what they observe about an algorithm, you shouldn't be trying to put syntactic

09:13.960 --> 09:18.720
constraints on the output of that algorithm, but rather you should be putting information

09:18.720 --> 09:23.560
theoretic constraints on the algorithmic process itself on the computation.

09:23.560 --> 09:27.160
And so that's exactly what differential privacy does.

09:27.160 --> 09:32.320
What differential privacy means, informally, it's a constraint on an algorithm, and it

09:32.320 --> 09:39.880
says small changes in the input to an algorithm, for example, adding or removing the record

09:39.880 --> 09:46.920
of a single individual should have only small changes on the distribution of outputs that

09:46.920 --> 09:48.480
the algorithm produces.

09:48.480 --> 09:55.680
So if I remove your record entirely from a data set, that shouldn't cause any observable

09:55.680 --> 10:00.760
event, anything that the algorithm might do when computing on the data set to become too

10:00.760 --> 10:03.400
much more or less likely.

10:03.400 --> 10:09.960
And this kind of probabilistic information theoretic constraint turns out to have a really strong

10:09.960 --> 10:16.760
semantics about what, you know, an adversary, an attacker can infer about your data.

10:16.760 --> 10:21.880
One of the subtleties in the way you describe that is that different, and maybe it's not

10:21.880 --> 10:27.600
so subtle, but differential privacy isn't an algorithm itself, it's a constraint on an

10:27.600 --> 10:28.600
algorithm.

10:28.600 --> 10:30.960
Am I hearing that correctly?

10:30.960 --> 10:31.960
That's right.

10:31.960 --> 10:32.960
Yeah.

10:32.960 --> 10:37.200
So differential privacy is a property that an algorithm might or might not have, any particular

10:37.200 --> 10:41.040
algorithm might be differentially private or might not be.

10:41.040 --> 10:46.640
And a lot of the definition of the constraint, it's relatively simple, but a lot of the

10:46.640 --> 10:53.000
science that goes into studying differential privacy asks the question, you know, if I've

10:53.000 --> 10:58.480
tied my hands in this way in what kinds of algorithms I can use, what type of algorithm

10:58.480 --> 11:00.640
tasks can I still perform?

11:00.640 --> 11:05.280
And as you say, differential privacy is a family of, you know, it's a constraint, it's

11:05.280 --> 11:06.280
not an algorithm.

11:06.280 --> 11:12.240
So to show that you can do something subject to differential privacy, it's sufficient

11:12.240 --> 11:17.480
to exhibit a differentially private algorithm that does that thing, but to prove lower bounds

11:17.480 --> 11:21.640
to show that for some problem, you cannot solve it subject to differential privacy.

11:21.640 --> 11:22.840
That's another matter entirely.

11:22.840 --> 11:28.040
You have to write down a mathematical proof that no algorithm could solve it subject to

11:28.040 --> 11:29.040
the constraint.

11:29.040 --> 11:30.040
Hmm.

11:30.040 --> 11:37.680
So how does that relatively simple sounding constraint lead to the benefits of privacy?

11:37.680 --> 11:44.480
I guess most basically it provides a guarantee of plausible deniability.

11:44.480 --> 11:50.080
So let me, maybe to make things less abstract, let me give you an example of a very simple,

11:50.080 --> 11:52.960
intuitive, differentially private algorithm.

11:52.960 --> 12:00.320
So suppose that I want to conduct a poll and I want to find out amongst all of the citizens

12:00.320 --> 12:04.880
in Philadelphia, how many of them voted for Donald Trump in the last election?

12:04.880 --> 12:05.880
Okay.

12:05.880 --> 12:11.200
You know, the most straightforward way to do this is I would call up some random sample

12:11.200 --> 12:13.120
of individuals on the phone.

12:13.120 --> 12:18.640
And I'd ask them, you know, did you vote for Donald Trump in the 2016 election?

12:18.640 --> 12:22.440
And I'd write down their answer on a piece of paper.

12:22.440 --> 12:27.240
And then when I was all done, when I'd called, you know, sufficiently many people, I'd

12:27.240 --> 12:28.520
tally up their answers.

12:28.520 --> 12:32.080
I'd find that, you know, 15% of people voted for Donald Trump.

12:32.080 --> 12:35.200
I'd do some statistics to attach error bars to that.

12:35.200 --> 12:38.960
And then I'd publish the, uh, publish the statistic.

12:38.960 --> 12:39.960
Okay.

12:39.960 --> 12:44.840
Now, note that like the thing that I wanted to find out was just this property of the

12:44.840 --> 12:48.280
distribution, what fraction of people voted for Donald Trump.

12:48.280 --> 12:56.240
But like incidentally along the way, I accumulated this large body of potentially sensitive

12:56.240 --> 13:01.200
information, what individual people voted for, who, who individual people voted for.

13:01.200 --> 13:02.200
Right.

13:02.200 --> 13:03.200
Okay.

13:03.200 --> 13:07.840
So think about the following alternative polling procedure, which turns out to be differentially

13:07.840 --> 13:14.800
private and will allow us to figure out the distributional statistic we care about,

13:14.800 --> 13:19.960
like fraction of people voted for Donald Trump without having to collect, you know, sensitive

13:19.960 --> 13:21.400
information about individuals.

13:21.400 --> 13:22.400
Okay.

13:22.400 --> 13:25.880
So I'm going to, again, call up some large collection of people.

13:25.880 --> 13:30.400
But now instead of telling them to, instead of instructing them to tell me whether they

13:30.400 --> 13:35.280
voted for Donald Trump, I'll tell them the following thing, I'll say flip a coin.

13:35.280 --> 13:40.080
If it comes up heads, tell me truthfully whether you voted for Donald Trump or not.

13:40.080 --> 13:44.640
If it comes up tails, though, tell me a random answer by which I mean, flip the coin

13:44.640 --> 13:50.600
again and tell me Trump if it came up heads and tell me not Trump if it came up tails.

13:50.600 --> 13:54.800
So and importantly, you're not going to tell me how the coin came out.

13:54.800 --> 13:55.800
Right.

13:55.800 --> 13:56.800
Okay.

13:56.800 --> 14:00.360
So I hear Trump or not Trump, but I don't know how your coins were realized.

14:00.360 --> 14:03.760
So I don't know whether you're telling me the truth or whether you're lying simply because

14:03.760 --> 14:05.200
of how the coins were flipped.

14:05.200 --> 14:06.200
Okay.

14:06.200 --> 14:13.080
So on the one hand, you now have a significant amount of plausible deniability.

14:13.080 --> 14:14.080
Okay.

14:14.080 --> 14:23.000
If all of a sudden the country collapses into a police state and my polling records are

14:23.000 --> 14:28.760
subpoenaed and you're called in front of the truth commission and that's suggested that

14:28.760 --> 14:36.400
you voted in one way or the other, you can reasonably say, no, I didn't.

14:36.400 --> 14:40.920
The answer that you're reading was simply the result of a coin flip because I was following

14:40.920 --> 14:42.080
this randomized protocol.

14:42.080 --> 14:43.080
Right.

14:43.080 --> 14:44.080
Okay.

14:44.080 --> 14:45.080
So you have plausible deniability.

14:45.080 --> 14:46.080
That's the guarantee of privacy.

14:46.080 --> 14:51.080
On the other hand, even though I cannot form strong beliefs about what any individual's

14:51.080 --> 14:57.480
data looks like, in aggregate, I can, I can, I can figure out to a high degree of accuracy

14:57.480 --> 14:59.680
what fraction of people voted for Donald Trump.

14:59.680 --> 15:05.280
And that's because I understand the process by which noise was injected into these measurements.

15:05.280 --> 15:06.280
Okay.

15:06.280 --> 15:14.360
In aggregate, the noise behaves in a very structured easy to understand way and I can subtract

15:14.360 --> 15:19.280
that noise out of the average, even though I cannot figure out for any individual who

15:19.280 --> 15:20.280
they voted for.

15:20.280 --> 15:25.480
That strikes me as somewhat counterintuitive in that, you know, thinking about the coin flip,

15:25.480 --> 15:31.040
you know, almost half of your data could be corrupted or quarter, maybe a quarter exactly.

15:31.040 --> 15:35.200
So if you think about it, right, it's a simple calculation.

15:35.200 --> 15:40.040
If I know that for every individual, three quarters of the time they're telling me the truth

15:40.040 --> 15:46.800
and a quarter of the time they're telling me the opposite of the truth, then say 15%

15:46.800 --> 15:51.120
of people in Philadelphia truly voted for Donald Trump, you know, I can write out on a piece

15:51.120 --> 15:57.720
of paper what percentage of people in expectation should report to me under this protocol voted

15:57.720 --> 15:59.040
for Donald Trump.

15:59.040 --> 16:00.040
Right.

16:00.040 --> 16:04.760
And because I've got a bunch of independent samples, the actual number of people who end up

16:04.760 --> 16:08.120
telling me this will be highly concentrated around its expectation.

16:08.120 --> 16:09.120
Right.

16:09.120 --> 16:13.400
And therefore, you know, what I need to do as the pollster is work backwards.

16:13.400 --> 16:18.840
What I know is the number of people who actually told me they voted for Donald Trump, but

16:18.840 --> 16:22.680
because I can compute this one-to-one mapping between the number of people who really did

16:22.680 --> 16:26.840
and the number of people I expect to tell me this, I can invert the mapping and figure

16:26.840 --> 16:33.320
out what the underlying population statistic was to a high degree of accuracy.

16:33.320 --> 16:34.320
Okay.

16:34.320 --> 16:41.320
Sounds like, you know, the method you're describing could be used both prior to data collection

16:41.320 --> 16:47.720
by instructing your respondents to follow this algorithm, or if you're an organization

16:47.720 --> 16:51.920
collecting data, you could collect the actual responses and then apply this algorithm

16:51.920 --> 16:53.720
before publishing the data.

16:53.720 --> 16:54.720
That's right.

16:54.720 --> 17:01.240
So the interaction that I described to you was what's called the local model of differential

17:01.240 --> 17:07.560
privacy and, you know, in this scenario, people's privacy was protected even from the pollster.

17:07.560 --> 17:08.560
Okay.

17:08.560 --> 17:10.880
He never wrote down the data.

17:10.880 --> 17:14.180
And of course, if that's what you want, then you have to apply these protections when the

17:14.180 --> 17:15.720
data is being gathered.

17:15.720 --> 17:16.720
Right.

17:16.720 --> 17:22.160
But as you say, if I'm an organization that's already got the data, I can apply privacy

17:22.160 --> 17:27.800
protections to the output of my computations to anything I release.

17:27.800 --> 17:33.200
So then obviously, privacy isn't protected from me, the organization that has the data.

17:33.200 --> 17:37.960
I've already got the data in the clear, but assuming there are no data breaches and

17:37.960 --> 17:45.200
no subpoenas, differential privacy can guarantee something about what any outside observer

17:45.200 --> 17:51.200
can learn about individuals in the data set by observing the outcomes of computations.

17:51.200 --> 17:53.320
And you're careful to say guarantee something.

17:53.320 --> 17:58.040
What exactly does differential privacy guarantee and what does it not guarantee?

17:58.040 --> 17:59.040
Yeah.

17:59.040 --> 18:03.840
So one thing that has been alighted in this discussion is that there's a quantitative

18:03.840 --> 18:09.880
parameter that comes with differential privacy called Epsilon.

18:09.880 --> 18:16.800
But what differential privacy guarantees formally is that no event becomes much more likely

18:16.800 --> 18:20.640
if your data is in the data set compared to if it's not.

18:20.640 --> 18:25.280
But what does much more likely mean, that means more likely by some multiplicative factor

18:25.280 --> 18:27.680
that depends on this parameter Epsilon.

18:27.680 --> 18:31.000
So suppose Epsilon is small.

18:31.000 --> 18:35.680
This is a pretty good guarantee because it says whatever it is that you're worried about

18:35.680 --> 18:41.800
from the perspective of privacy, whatever harm you're worried that the use of your data

18:41.800 --> 18:47.560
will cause to befall you, that harm is even though I don't know what you're worried

18:47.560 --> 18:54.760
about, I can promise you that the risk, the increased risk of that harm befalling you can

18:54.760 --> 18:58.080
be bounded as a function of this parameter Epsilon.

18:58.080 --> 19:03.240
But of course, if Epsilon is very big, then that's not a very strong guarantee.

19:03.240 --> 19:08.240
So it doesn't really mean anything if someone tells you that their algorithm is differentially

19:08.240 --> 19:11.400
private, unless they also tell you what this privacy parameter is.

19:11.400 --> 19:16.000
As the privacy parameter goes to infinity, differential privacy is no constraint at all,

19:16.000 --> 19:20.000
as it goes to zero, it becomes a very strong constraint.

19:20.000 --> 19:27.360
Going back to this fundamental constraint, it's that within the bounds of Epsilon, adding

19:27.360 --> 19:35.960
or removing an individual piece of data won't change the statistics of your overall distribution.

19:35.960 --> 19:36.960
Is that correct?

19:36.960 --> 19:37.960
That's right.

19:37.960 --> 19:40.960
It won't change the behavior of your algorithm.

19:40.960 --> 19:48.520
So adding or removing a single data point won't cause your algorithm to do something that

19:48.520 --> 19:52.080
is very different from the point of view of an observer.

19:52.080 --> 20:00.800
And so how does, how do we get from there to, again, the notion of privacy?

20:00.800 --> 20:06.520
And I guess you were setting that up in talking about the plausible deniability example.

20:06.520 --> 20:07.520
Yes.

20:07.520 --> 20:10.400
So there's a couple of interpretations of differential privacy.

20:10.400 --> 20:12.200
And I can walk through a few of them.

20:12.200 --> 20:16.160
So one is this plausible deniability guarantee.

20:16.160 --> 20:24.720
So if someone accuses you of having some particular data record, and the piece of evidence

20:24.720 --> 20:30.560
they have at their disposal is the output of a differentially private computation, you

20:30.560 --> 20:34.760
have plausible deniability in the sense that you can say that the piece of evidence they

20:34.760 --> 20:41.680
have in hand is essentially, as likely to have been observed, again, up to this factor

20:41.680 --> 20:46.200
relating to the privacy parameter, if your data had been different.

20:46.200 --> 20:47.200
Okay.

20:47.200 --> 20:51.640
Another way of saying this is, you know, suppose they've got some prior belief about what

20:51.640 --> 20:55.920
your data point looks like.

20:55.920 --> 20:59.080
And then they observe the output of a computation.

20:59.080 --> 21:04.720
And so they update their belief to some posterior belief using Bayes' rule, for example.

21:04.720 --> 21:09.240
And what differential privacy promises is that they would have performed nearly the

21:09.240 --> 21:14.560
same update, and therefore had nearly the same belief had your data been different if

21:14.560 --> 21:17.560
we hold the rest of the data set fixed.

21:17.560 --> 21:24.800
Another interpretation is this model of harm, and you can view this as a sort of utilitarian

21:24.800 --> 21:31.480
definition of privacy, you know, like how hard is it for me to convince you to contribute

21:31.480 --> 21:35.760
your data to my data set, if I'm going to use it for some statistical analysis?

21:35.760 --> 21:39.840
Well, why wouldn't you want to contribute your data?

21:39.840 --> 21:44.280
There's any number of reasons, and I might not know what they are, but, you know, presumably

21:44.280 --> 21:49.360
you're worried that as the result of the use of your data, something bad is going to happen

21:49.360 --> 21:50.360
to you.

21:50.360 --> 21:52.640
Maybe your health insurance premiums are going to rise, or maybe you're going to start

21:52.640 --> 21:55.480
getting drunk phone calls during dinner.

21:55.480 --> 21:59.560
And what differential privacy can promise is no matter what event that you're worried

21:59.560 --> 22:04.600
about, and I really mean no matter what event, so we can talk about the probability that

22:04.600 --> 22:09.920
your health insurance premiums rise for the probability that you get spam phone calls.

22:09.920 --> 22:17.920
This event will be, will have almost the same probability up to, again, this privacy parameter.

22:17.920 --> 22:23.400
In the following two hypothetical worlds, in the one world, you don't contribute your data

22:23.400 --> 22:27.840
to the computation in the other world you do, and everything else is held constant between

22:27.840 --> 22:28.840
these two worlds.

22:28.840 --> 22:32.240
That's the difference in differential privacy, right?

22:32.240 --> 22:37.640
So if I look at the two different worlds that are identical, except for this one fact,

22:37.640 --> 22:42.920
whether you contributed your data to my analysis or whether you did not, then the events that

22:42.920 --> 22:46.680
you're worried about, whatever they are, become almost no more likely when you contribute

22:46.680 --> 22:48.680
your data.

22:48.680 --> 22:53.960
And does that interpretation, it sounds like that assumes some anonymization?

22:53.960 --> 22:59.240
It doesn't assume anything, that follows sort of directly from the definition of differential

22:59.240 --> 23:02.360
privacy, that if you like, that is the definition of differential privacy.

23:02.360 --> 23:07.480
I guess I think I'm maybe I'm thinking of this in some perversely, but if I include

23:07.480 --> 23:15.400
my data and my data, includes my phone number, how does differential privacy address that?

23:15.400 --> 23:19.800
Oh, well, your data can include anything you like, including your phone number, but a

23:19.800 --> 23:23.280
differentially private algorithm, certainly can't look at your data record and publish your

23:23.280 --> 23:24.280
phone number.

23:24.280 --> 23:25.280
Right.

23:25.280 --> 23:31.040
And so is the idea that we're applying the coin flip, for example, to the publishing, you

23:31.040 --> 23:34.760
know, maybe it would randomly spit out phone numbers or something like that?

23:34.760 --> 23:40.720
Yeah, I mean, I think I'm getting stuck in a rat hole here, but so maybe one thing that's

23:40.720 --> 23:45.280
useful to keep in mind, you know, you can try to write down a differentially private algorithm

23:45.280 --> 23:50.480
for anything you like, but it's only for certain kinds of problems for which differentially

23:50.480 --> 23:53.480
private algorithms are going to be able to do anything useful.

23:53.480 --> 23:59.320
And those are our statistical problems, where the thing that you want to estimate is some

23:59.320 --> 24:04.360
property of the underlying distribution, so it's very good for sort of machine learning

24:04.360 --> 24:08.400
if I want to learn a classifier that on average makes correct predictions.

24:08.400 --> 24:12.840
But if I want to learn what your phone number is, you know, it's all well and good that

24:12.840 --> 24:17.840
I want to learn that, but by design, you know, there is no differentially private algorithm

24:17.840 --> 24:22.960
that will give me a non trivial advantage over essentially random guessing.

24:22.960 --> 24:27.600
Differential privacy isn't compatible with answering the kinds of questions that have

24:27.600 --> 24:30.600
to do with just a single individual, and that's by design.

24:30.600 --> 24:38.600
So that's a great segue to the applications of differential privacy in machine learning.

24:38.600 --> 24:45.280
Can you maybe start by talking about the specific machine learning, you know, problems or examples

24:45.280 --> 24:49.440
that differential privacy is, is trying to address in that application and maybe talk

24:49.440 --> 24:53.240
through some of the specifics of how that's done?

24:53.240 --> 24:54.240
Sure.

24:54.240 --> 24:58.040
So there's a couple of things that you might want to do subject to differential privacy

24:58.040 --> 25:00.120
when you're doing machine learning.

25:00.120 --> 25:04.280
So one is that just that you might want to solve a single machine learning problem subject

25:04.280 --> 25:05.280
to differential privacy.

25:05.280 --> 25:10.640
So maybe you've got some, for example, supervised classification task, you've got some label

25:10.640 --> 25:15.600
data you'd like to learn, you know, a support vector machine or a neural network of some

25:15.600 --> 25:22.480
sort that will minimize some loss function, maybe my classification error, when I apply

25:22.480 --> 25:24.240
it to new data.

25:24.240 --> 25:25.240
Okay.

25:25.240 --> 25:30.360
So that's the standard machine learning problem and differential privacy is extremely

25:30.360 --> 25:35.760
amenable to this kind of problem, essentially, because well, there's several reasons,

25:35.760 --> 25:41.080
but maybe the most fundamental reason is that this is inherently a statistical problem

25:41.080 --> 25:48.240
in the sense that the thing that I already for reasons of overfitting wanted to avoid

25:48.240 --> 25:53.120
when I'm solving a machine learning problem, depending too heavily on any single data

25:53.120 --> 25:54.120
point, right?

25:54.120 --> 25:59.040
So, so machine learning and privacy, they're sort of aligned in the sense that they're

25:59.040 --> 26:04.440
both trying to learn facts about the distribution without overfitting to the particular data

26:04.440 --> 26:09.760
set I have on hand overfitting is closely related to privacy violations and we can talk

26:09.760 --> 26:10.760
more about that.

26:10.760 --> 26:12.960
The connection turns out to go both ways.

26:12.960 --> 26:18.400
Another thing that you might want to do is more ambitious, you might want to construct

26:18.400 --> 26:23.280
a synthetic data set by which I mean, like rather than solving a single machine learning

26:23.280 --> 26:28.360
problem, maybe you want to construct a data set that looks like the real data set with

26:28.360 --> 26:33.840
respect to some large number of statistics or machine learning algorithms, but it's

26:33.840 --> 26:38.160
nevertheless differentially private, so I can construct this synthetic data set with

26:38.160 --> 26:42.120
a private algorithm and then release it to the world and then other people can go and

26:42.120 --> 26:46.360
try to apply their machine learning algorithms to this synthetic data set, the hope being

26:46.360 --> 26:51.120
that insights that they derive classifiers they train on the synthetic data set would

26:51.120 --> 26:53.880
also work well on the real data set.

26:53.880 --> 26:58.360
And then finally, and this relates back to the connections between differential privacy

26:58.360 --> 27:03.640
and overfitting, it might be that you don't care about privacy at all in your application,

27:03.640 --> 27:14.560
but you know, you want to, for example, repeatedly test hypotheses or fit different kinds

27:14.560 --> 27:19.480
of machine learning models while reusing the same holdout set over and over again, maybe

27:19.480 --> 27:22.240
because it's too expensive to get more data.

27:22.240 --> 27:28.040
Now normally this would be a really bad idea, sort of the standard test train methodology

27:28.040 --> 27:35.040
and machine learning entirely falls apart, basically, if you reuse the holdout set as part

27:35.040 --> 27:40.080
of an iterative procedure by which you're choosing your model.

27:40.080 --> 27:47.600
But as it turns out, when you perform your entire pipeline of statistical analyses subject

27:47.600 --> 27:53.680
to the guarantees of differential privacy, you can't overfit, so if you can be accurate.

27:53.680 --> 27:58.840
In sample, you can be guaranteed that you learn an accurate model out of sample, even

27:58.840 --> 28:01.320
if you've repeatedly reused the data.

28:01.320 --> 28:07.280
And when you say perform your entire pipeline subject to the guarantees of differential

28:07.280 --> 28:13.000
privacy, does that mean you are enforcing those constraints at every individual step

28:13.000 --> 28:18.200
or the, you know, relative to the inputs and outputs of the entire pipeline?

28:18.200 --> 28:24.560
It means that, you know, everything you shouldn't have touched the data in any way using a non-differentially

28:24.560 --> 28:25.560
private algorithm.

28:25.560 --> 28:29.280
So differential privacy has a very nice property that it composes.

28:29.280 --> 28:33.680
If I have two algorithms, you know, the first one is epsilon one, differentially private,

28:33.680 --> 28:36.400
and the second one is epsilon two, differentially private.

28:36.400 --> 28:41.480
Then if I run the first one and based on the outcome decide what I want to do at the second

28:41.480 --> 28:46.760
step and then I run the second algorithm, this whole computation is in aggregate, epsilon

28:46.760 --> 28:49.200
one plus epsilon two, differentially private.

28:49.200 --> 28:54.600
So, you know, if at every decision point I'm making my decision about what to do next

28:54.600 --> 28:59.320
as a function only of differentially private access to the data, then you've got these

28:59.320 --> 29:02.000
strong safety guarantees about overfitting.

29:02.000 --> 29:09.520
So maybe to make it a little bit more concrete, I've heard a few examples of scenarios that

29:09.520 --> 29:14.720
pop up in the machine learning world, and I'm vaguely recalling them, maybe you can provide

29:14.720 --> 29:23.360
a bit more detail, but one of them was an example of a, it's almost, it was like reverse

29:23.360 --> 29:29.280
engineering and object detector to determine whether an individual or a face detector to

29:29.280 --> 29:33.080
determine whether an individual face was in the training data set.

29:33.080 --> 29:38.960
Okay, so you're talking about maybe an attack on a classifier that wasn't trained in

29:38.960 --> 29:43.160
a differentially private way and the kind of thing that you might, the kind of reason

29:43.160 --> 29:49.400
why you might want to have privacy guarantees when you're training a learning algorithm

29:49.400 --> 29:51.800
that they don't come for free, if I'm.

29:51.800 --> 29:52.800
Exactly, exactly.

29:52.800 --> 29:58.080
Yes, so I think there's a couple of these kinds of attacks now, and I don't know the details

29:58.080 --> 30:04.640
of the specific one you're referring to, but you might have, you know, a priori before

30:04.640 --> 30:10.720
you started worrying about privacy, I think that, you know, okay, maybe if I'm, you know,

30:10.720 --> 30:15.880
releasing individual data records like in the Netflix example, I have to worry about privacy

30:15.880 --> 30:21.800
violations, but if I'm, if I'm just training a classifier, how could, how could releasing

30:21.800 --> 30:26.400
only the model parameters, you know, the weights in the neural network possibly violate

30:26.400 --> 30:27.400
anyone's privacy?

30:27.400 --> 30:28.400
Exactly.

30:28.400 --> 30:34.560
Yeah, and that intuition is wrong and you're describing the kind of attacks that, that

30:34.560 --> 30:41.360
show that it's wrong, but the basic, I think, premise underlying these attacks is that when

30:41.360 --> 30:48.440
you train a model without differential privacy, it'll tend to overfit a little bit, even

30:48.440 --> 30:51.440
if, even if this doesn't really affect the model performance.

30:51.440 --> 30:57.360
But what you find is that, you know, when you try to classify a face, for example, that

30:57.360 --> 31:04.360
was in the training data set, the model will tend to have higher confidence in its classification

31:04.360 --> 31:07.920
than when you try to classify an example that was not in the training data set.

31:07.920 --> 31:08.920
Okay.

31:08.920 --> 31:13.120
And it's sort of natural that you would expect that because the model got to update itself

31:13.120 --> 31:14.920
on the examples in the training set.

31:14.920 --> 31:15.920
Right.

31:15.920 --> 31:21.360
And by taking advantage of that, you can therefore figure out whether a particular person's

31:21.360 --> 31:26.560
picture was in the training data set or not by examining what is the confidence in

31:26.560 --> 31:28.560
the model's prediction.

31:28.560 --> 31:29.560
Okay.

31:29.560 --> 31:35.760
Are other examples that come to mind of, you know, where the notion of distributing

31:35.760 --> 31:41.840
models or, you know, more generally, I guess, statistical aggregates can fail to be privacy

31:41.840 --> 31:42.840
preserving?

31:42.840 --> 31:48.360
So, maybe the most obvious example is sort of naive training of a support vector machine.

31:48.360 --> 31:56.040
So the simplest way to, you know, the most concise way for me to describe to you the model

31:56.040 --> 32:01.680
of a trans support vector machine is by communicating to you the support vectors.

32:01.680 --> 32:06.640
But the support vectors are just examples from the training data set.

32:06.640 --> 32:13.440
So the most straightforward way to distribute a trained support vector machine is to transmit

32:13.440 --> 32:17.520
to you some number of examples from the training data set in the clear.

32:17.520 --> 32:21.720
So that's sort of obvious once you realize it, but, you know, it is one of the things that

32:21.720 --> 32:25.920
you might not have thought of initially when you, if you're coming from this position,

32:25.920 --> 32:29.920
but things that represent just aggregate statistics like trained models shouldn't be

32:29.920 --> 32:30.920
disclosed.

32:30.920 --> 32:31.920
Okay.

32:31.920 --> 32:32.920
Okay.

32:32.920 --> 32:43.320
What I'm hearing is, you know, I guess granted some in some classes of problem, maybe privacy

32:43.320 --> 32:47.200
isn't, you know, the greatest concern.

32:47.200 --> 32:52.680
But if differential privacy were free and easy to apply everywhere, you know, I might

32:52.680 --> 32:58.680
do that. What are some of the, you know, the issues or costs of applying differential

32:58.680 --> 33:02.680
privacy that come up when trying to apply it in the machine learning context?

33:02.680 --> 33:03.680
Yeah.

33:03.680 --> 33:05.680
So it definitely doesn't come for free.

33:05.680 --> 33:07.680
And I think there's costs of two sorts.

33:07.680 --> 33:14.200
So the first is sort of, maybe it's difficult to just acquire the expertise to implement

33:14.200 --> 33:19.000
all of these things at the moment, you know, a lot of the knowledge about differential

33:19.000 --> 33:22.840
privacy at the moment is contained in hard to read, you know, academic papers.

33:22.840 --> 33:26.760
There's not that many people who are trained to read these things.

33:26.760 --> 33:33.800
So if you're addressed some random company, it can be hard to even get started.

33:33.800 --> 33:39.640
But maybe the more fundamental thing is that although differential privacy is compatible

33:39.640 --> 33:44.920
with machine learning by which I mean, in principle, anything that you can, any statistical

33:44.920 --> 33:49.720
problem that is susceptible to machine learning absent differential privacy is, you know,

33:49.720 --> 33:54.200
can also be solved with machine learning with differential privacy guarantees.

33:54.200 --> 33:59.240
The cost is that if you want strong differential privacy guarantees, you'll need more data.

33:59.240 --> 34:04.240
And if you want the privacy parameter to be small, this thing that governs the strength

34:04.240 --> 34:08.760
of the privacy guarantee, you might need a lot more data to achieve the same accuracy

34:08.760 --> 34:10.000
guarantees.

34:10.000 --> 34:17.720
So as a result, it can be a tough sell to apply privacy technologies in a setting in which,

34:17.720 --> 34:22.840
you know, developers and researchers already have direct access to the data set because

34:22.840 --> 34:25.000
the data set's not getting any bigger.

34:25.000 --> 34:31.560
So if yesterday you could do your statistical analyses on your data set of 10,000 records

34:31.560 --> 34:36.720
and today I say, now you've got to do it subject to differential privacy.

34:36.720 --> 34:45.680
The accuracy of your analyses is going to degrade the place in which I've seen it successfully

34:45.680 --> 34:54.120
deployed overcoming sort of this kind of objection in industry has been in settings where because

34:54.120 --> 34:59.640
of privacy concerns, developers previously didn't have access to the data at all.

34:59.640 --> 35:05.720
And they're now, you know, once privacy, strong privacy guarantees are built in, are

35:05.720 --> 35:11.640
able to start collecting it, so it's a much easier sell if the privacy guarantees are

35:11.640 --> 35:16.060
going to give you access to new data sets that previously you couldn't touch because

35:16.060 --> 35:22.600
of privacy concerns, then it is to sort of add on X post when previously you were able

35:22.600 --> 35:29.560
to ignore privacy or the cost of privacy will tend to come in the form of less accuracy

35:29.560 --> 35:33.320
in terms of your, you know, costification error, for example.

35:33.320 --> 35:40.480
Okay, some of the known uses of differential privacy are places like Google, Apple, Microsoft,

35:40.480 --> 35:46.800
the US Census Bureau, are you familiar with those examples and what they're doing and

35:46.800 --> 35:49.400
can you talk about the ones that you are?

35:49.400 --> 35:58.000
Sure, so Google and Apple are both using differential privacy in the local model, this model

35:58.000 --> 36:05.160
of the polling agency trying to figure out how many people voted for Donald Trump in the

36:05.160 --> 36:06.360
example that I gave.

36:06.360 --> 36:12.800
Okay, so both of them are collecting statistics, you know, Google in your Chrome web browser

36:12.800 --> 36:19.480
and Apple on your iPhone in which the privacy protections are added on device.

36:19.480 --> 36:25.880
And what they're trying to do are collect simple statistics, population-wide averages.

36:25.880 --> 36:32.720
So if you look at the Apple paper, for example, they're collecting statistics on, you know,

36:32.720 --> 36:39.000
like what are the most frequently used emojis in different countries or for different websites?

36:39.000 --> 36:46.440
What fraction of people like it when videos automatically play as opposed to requiring

36:46.440 --> 36:47.440
some human intervention?

36:47.440 --> 36:52.920
So they're trying to collect population-wide statistics that allow them to improve user

36:52.920 --> 36:59.800
experience or improve things like predictive type and things like that.

36:59.800 --> 37:08.160
The US Census is doing something more ambitious and the US Census collects all the data in

37:08.160 --> 37:09.160
the clear.

37:09.160 --> 37:14.240
So they're not trying to protect the privacy of your data from the census, they're collecting

37:14.240 --> 37:15.240
it.

37:15.240 --> 37:20.320
Instead they're using differential privacy in the centralized model.

37:20.320 --> 37:24.880
But they release huge amounts of statistical information.

37:24.880 --> 37:34.120
So you can go on existing census websites and figure out the answers to questions like,

37:34.120 --> 37:38.760
how many people live in this collection of zip codes and work in this other collection

37:38.760 --> 37:39.920
of zip codes?

37:39.920 --> 37:40.920
Okay.

37:40.920 --> 37:48.120
And they're going to continue releasing these large amounts of statistical information

37:48.120 --> 37:52.520
about the US population, but for the 2020 census, they're going to release it subject

37:52.520 --> 37:54.800
to differential privacy protections.

37:54.800 --> 37:55.800
Interesting.

37:55.800 --> 38:03.400
And so they're releasing not individual data records, but more of these statistical aggregates

38:03.400 --> 38:05.040
subject to differential privacy.

38:05.040 --> 38:06.040
That's right.

38:06.040 --> 38:12.520
So in all of these applications, what's being released or statistics rather than actual

38:12.520 --> 38:13.840
synthetic data sets.

38:13.840 --> 38:14.840
Right.

38:14.840 --> 38:18.640
As far as I know, I don't know the details of what the census plans to do.

38:18.640 --> 38:22.680
And I'm not sure that's even been pinned down.

38:22.680 --> 38:29.120
In an academic setting, I have a former student, Stephen Wu, who's now a postdoc at Microsoft

38:29.120 --> 38:30.120
Research.

38:30.120 --> 38:34.560
But when he was here, he worked with colleagues in the medical school, a professor named

38:34.560 --> 38:41.680
Casey Green, to construct a differentially private synthetic medical data sets.

38:41.680 --> 38:49.280
So medicine is a field that's got a big problem in that there's a lot of data and it's starting

38:49.280 --> 38:56.000
to be susceptible to yielding all sorts of wonderful insights if we apply the latest machine

38:56.000 --> 38:58.000
learning technologies to it.

38:58.000 --> 39:04.000
But medicine is a domain where there are serious privacy concerns and legal regulations.

39:04.000 --> 39:06.240
And so it's very difficult to share data sets.

39:06.240 --> 39:11.080
Ideally, you'd like to share your data sets with other researchers, allow them to reproduce

39:11.080 --> 39:15.640
the kinds of garments you did on the data, combine data sets.

39:15.640 --> 39:20.000
And so what Stephen and his colleagues did was it gave sort of a proof of concept that

39:20.000 --> 39:28.120
you could use techniques for privately training neural networks and combine those with techniques

39:28.120 --> 39:34.320
for generating synthetic data for training gans that would let you create synthetic data

39:34.320 --> 39:39.600
sets that you could release to the public, but that would look like the original data

39:39.600 --> 39:43.120
set with respect to a very large class of machine learning algorithms.

39:43.120 --> 39:50.040
So you could train the algorithms on the synthetic data and then find that when you evaluated

39:50.040 --> 39:52.280
them on the real data, they did pretty well.

39:52.280 --> 39:53.280
Okay.

39:53.280 --> 39:54.280
Interesting.

39:54.280 --> 39:55.280
Interesting.

39:55.280 --> 40:00.440
This is sort of the more ambitious kind of technology that I think, as far as I know,

40:00.440 --> 40:06.640
has so far been the domain of only academic research, but maybe in the coming years

40:06.640 --> 40:11.160
will find industrial and government applications.

40:11.160 --> 40:17.000
Can you maybe share a brief word on the current research areas and around differential privacy

40:17.000 --> 40:18.360
and machine learning?

40:18.360 --> 40:24.560
So there are many in diverse and there are people focused on more practical problems

40:24.560 --> 40:26.800
and more theoretical problems.

40:26.800 --> 40:33.080
I myself, you know, just through my natural activities tend to focus on sort of the more

40:33.080 --> 40:34.800
theoretical problems.

40:34.800 --> 40:40.960
But I think that it remains, even though it's an old problem, it remains an important

40:40.960 --> 40:47.400
and unsolved problem to figure out sort of practical ways to generate useful synthetic

40:47.400 --> 40:49.880
data for large collections of tasks.

40:49.880 --> 40:55.320
We know, we've known for a while, you know, since my PhD thesis that these kinds of problems

40:55.320 --> 41:00.760
are possible in principle, there are inefficient information theoretic, you know, kinds of algorithms

41:00.760 --> 41:03.320
that accomplish them, but we don't get to have practical algorithms.

41:03.320 --> 41:06.080
I think that remains very important.

41:06.080 --> 41:11.320
You know, another important direction is that a lot of the academic literature to date

41:11.320 --> 41:15.560
has really focused on the central model of data privacy where there's a trusted database

41:15.560 --> 41:20.240
curator who gathers all the data in the clear in part because you can do more stuff in

41:20.240 --> 41:21.240
that model.

41:21.240 --> 41:23.720
So it's attractive to study it.

41:23.720 --> 41:28.440
But as we've seen differential privacy move from theory to practice, you know, two

41:28.440 --> 41:34.200
days, it's two largest scale deployments that Google and Apple are both in the local model.

41:34.200 --> 41:39.360
And there's a lot of things, I think, that we understand in the central model of differential

41:39.360 --> 41:42.760
privacy that we still don't understand in the local model.

41:42.760 --> 41:46.400
And that's too bad because the local model's turning out to be very important.

41:46.400 --> 41:52.960
So I think understanding basic tasks in the local model continues to be very important.

41:52.960 --> 41:58.120
And I mentioned briefly this sort of research agenda showing that you can use differential

41:58.120 --> 42:03.600
privacy to avoid false discovery and overfitting even when you don't care about privacy.

42:03.600 --> 42:11.160
I think this is one of the most general promising directions to, you know, attack the statistical

42:11.160 --> 42:13.400
crisis in science.

42:13.400 --> 42:18.840
But so far, we're just in early days, you know, we understand the basic sort of proof

42:18.840 --> 42:23.680
of concept kinds of results for why techniques from differential privacy might be useful.

42:23.680 --> 42:28.000
But we're a pretty far way off from having practical tools that, you know, working data

42:28.000 --> 42:33.960
scientists can use to prevent overfitting and with practically sized data sets.

42:33.960 --> 42:34.960
Okay.

42:34.960 --> 42:35.960
Great.

42:35.960 --> 42:41.960
We will share a link to your website so folks can take a look at some of your recent

42:41.960 --> 42:44.800
work and publications in this area.

42:44.800 --> 42:48.480
Before we close up, would you, is there anything else that you'd like to share with the

42:48.480 --> 42:49.480
audience?

42:49.480 --> 42:57.280
No, thanks for, thanks for listening and, you know, I'm always happy to hear about interesting

42:57.280 --> 42:59.320
new applications of differential privacy.

42:59.320 --> 43:05.680
So feel free to send me emails when I was just getting started and writing my PhD thesis.

43:05.680 --> 43:10.720
You know, all of this was a theoretical abstraction and it's been great fun, you know, hearing

43:10.720 --> 43:16.120
about and consulting with companies that are actually putting this into practice.

43:16.120 --> 43:20.520
So it's been a fun ride and I like to hear what's going on out there.

43:20.520 --> 43:21.520
Fantastic.

43:21.520 --> 43:22.520
Well, thanks so much, Aaron.

43:22.520 --> 43:23.520
Thank you.

43:23.520 --> 43:29.720
All right, everyone, that's our show for today.

43:29.720 --> 43:34.800
For more information on Aaron or any of the topics covered in this episode, head on over

43:34.800 --> 43:40.040
to twimla.com slash talk slash one thirty two.

43:40.040 --> 43:44.360
Thanks again to our friends at Georgian for sponsoring this series and be sure to visit

43:44.360 --> 43:52.400
their differential privacy resource center at gptrs.vc slash twimla for more information

43:52.400 --> 43:55.120
on the field and what they're up to.

43:55.120 --> 44:23.640
And of course, thank you so much for listening and catch you next time.

