1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,920
I'm your host Sam Charrington.

4
00:00:31,920 --> 00:00:36,760
You are invited to join us for the very first Twimblecon conference which will focus on the

5
00:00:36,760 --> 00:00:41,200
tools, technologies and practices necessary to scale the delivery of machine learning

6
00:00:41,200 --> 00:00:43,720
and AI in the enterprise.

7
00:00:43,720 --> 00:00:48,600
The event will be held October 1st and 2nd in San Francisco and early bird registration

8
00:00:48,600 --> 00:00:58,520
is open today at twimblecon.com, again that's twimblecon.com, I can't wait to see you there.

9
00:00:58,520 --> 00:01:03,520
All right everyone, I am on the line with Bill Felman, Bill is the Director of Data Science

10
00:01:03,520 --> 00:01:08,080
at USAAA, Bill, welcome to this week in machine learning and AI.

11
00:01:08,080 --> 00:01:10,080
Thank you Sam, thank you for inviting me.

12
00:01:10,080 --> 00:01:16,720
Absolutely, so we were originally planning to connect at the Stradda Data Conference

13
00:01:16,720 --> 00:01:24,920
in New York, but you unfortunately weren't able to make it due to hurricane Florence actually.

14
00:01:24,920 --> 00:01:28,280
Did everything work out okay for you with the hurricane?

15
00:01:28,280 --> 00:01:33,440
It actually did, I mean at the beginning of the week we had a zone of uncertainty that

16
00:01:33,440 --> 00:01:39,680
included where I live and then as a week progressed we shifted south so luckily it missed

17
00:01:39,680 --> 00:01:45,960
my area but still it created a lot of uncertainty and drama and preparation for it.

18
00:01:45,960 --> 00:01:50,880
I bet, I bet, so at the Stradda conference you were going to talk about topic modeling

19
00:01:50,880 --> 00:01:57,520
and we're going to dig into that in our conversation today, but before we do that why don't you

20
00:01:57,520 --> 00:02:00,680
tell us a little bit about your background?

21
00:02:00,680 --> 00:02:02,080
Sure, absolutely.

22
00:02:02,080 --> 00:02:07,280
So as you mentioned I'm the Director of Data Science team at USAAA which is the United

23
00:02:07,280 --> 00:02:14,560
Services Automobile Association and my team conducts research and applies advance analytical

24
00:02:14,560 --> 00:02:22,000
methods in support of our context center operations that serves 12.4 million members and how

25
00:02:22,000 --> 00:02:25,320
I got to that position.

26
00:02:25,320 --> 00:02:31,920
My background is in mathematics and I hold a bachelor's in mathematics, a master's in applied

27
00:02:31,920 --> 00:02:39,280
mathematics and a PhD in applied science and my applied science PhD dissertation was

28
00:02:39,280 --> 00:02:45,160
an area of computer vision and so what I did in my dissertation I developed a method

29
00:02:45,160 --> 00:02:52,880
to classify non-heat generating objects in thermal imagery and that work actually led

30
00:02:52,880 --> 00:02:59,560
me to an opportunity to work in an autonomy incubator at NASA Langley Research Center

31
00:02:59,560 --> 00:03:06,520
after I retired from the US Army and while in the autonomy incubator I worked on a multidisciplinary

32
00:03:06,520 --> 00:03:12,840
team I was doing computer vision and what we were doing was developing machine intelligence

33
00:03:12,840 --> 00:03:19,320
software for uncrewed systems and that actually that computer vision work is what led me to

34
00:03:19,320 --> 00:03:27,680
my efforts at USAAA involving natural language processing and so what I'm doing at USAAA

35
00:03:27,680 --> 00:03:35,880
is really looking in large corpus of documents and under trying to identify topics what I

36
00:03:35,880 --> 00:03:40,200
call unknown unknowns really the things we don't know what we don't know but if you go

37
00:03:40,200 --> 00:03:46,560
back to my computer vision really computer vision is looking at frames of imagery in each

38
00:03:46,560 --> 00:03:51,880
frame as a matrix where each value within the cells in the frame are either gray level

39
00:03:51,880 --> 00:03:57,720
values or RGB red, green, blue and when you look at natural language processing I'm really

40
00:03:57,720 --> 00:04:04,720
dealing with large volumes of documents and along with those documents the terms or vocabulary

41
00:04:04,720 --> 00:04:09,720
that are involved so I'm going to take those large corpus documents along with their vocabulary

42
00:04:09,720 --> 00:04:14,600
and I'm going to put them in a matrix and so I'm really either when I'm doing computer

43
00:04:14,600 --> 00:04:19,920
vision or if I'm doing natural language processing I'm applying the similar types of mathematics

44
00:04:19,920 --> 00:04:25,600
which involves the calculus and matrix algebra of probability and statistics to both types

45
00:04:25,600 --> 00:04:32,480
of application so it was a really good fit from what I was doing with computer vision and

46
00:04:32,480 --> 00:04:36,960
then transitioning over to natural language processing type work that I'm doing at USAAA.

47
00:04:36,960 --> 00:04:46,040
Oh nice nice and what is the what's the motivation for doing that kind of work in support

48
00:04:46,040 --> 00:04:51,240
of a context center what are the kinds of documents and data that you're dealing with and what

49
00:04:51,240 --> 00:04:55,360
business outcomes do they help drive for you.

50
00:04:55,360 --> 00:05:01,440
Good question so in our context center so we got a call channel we also had where agents

51
00:05:01,440 --> 00:05:07,000
are talking to our members on the phone we also have a chat channel where members are

52
00:05:07,000 --> 00:05:12,600
talking to our agents as well but also we have digital channels such as.com and mobile

53
00:05:12,600 --> 00:05:19,760
so these are very large volume of both unstructured and structured data and what we want to be

54
00:05:19,760 --> 00:05:25,400
able to do is gain insights in this data to better serve our members and this is where

55
00:05:25,400 --> 00:05:31,600
actually topic modeling comes into play and what you can think of topic modeling as a way

56
00:05:31,600 --> 00:05:36,960
to what I like to say uncover the unknown unknowns you know there's there's the things

57
00:05:36,960 --> 00:05:42,400
we don't know what we don't know and what topic modeling is is an unsupervised method

58
00:05:42,400 --> 00:05:46,080
that helps us discover those topics that are emerging.

59
00:05:46,080 --> 00:05:55,840
You mentioned a chat channel are you using AI as part of delivering that chat and I'm

60
00:05:55,840 --> 00:06:02,920
mostly asking because this concept of discovering the unknown unknowns seems like it would be

61
00:06:02,920 --> 00:06:10,080
really helpful in identifying you know for example topics that come up in chat conversations

62
00:06:10,080 --> 00:06:17,640
that you might need to create you know intense and support for in your chat system.

63
00:06:17,640 --> 00:06:23,960
Yes absolutely so what we want to be able to do and what we're doing is you know you

64
00:06:23,960 --> 00:06:30,040
have chat in the chat channel you have conversations between our agents and our members and

65
00:06:30,040 --> 00:06:35,800
same thing with the call channel and what we want to be able to do is identify topics

66
00:06:35,800 --> 00:06:43,120
that are emerging before we reach a peak call volume and that way we can say mitigate

67
00:06:43,120 --> 00:06:50,280
members concerns before we hit that peak call volume as an example if if there's some

68
00:06:50,280 --> 00:06:59,240
kind of force fire for example and in California or a hurricane you know as we start seeing

69
00:06:59,240 --> 00:07:04,160
conversations about that we can use topic modeling to get a better understanding of our

70
00:07:04,160 --> 00:07:09,320
the needs of our members and we can be able to be ready to provide the services before

71
00:07:09,320 --> 00:07:15,120
we even reach any kind of peak call volume it's really being more proactive than reactive

72
00:07:15,120 --> 00:07:24,760
to the after the fact okay okay so you've got this data set that comes from the is it

73
00:07:24,760 --> 00:07:34,680
coming primarily or solely from these chat and email interactions or are you also transcribing

74
00:07:34,680 --> 00:07:41,480
voice support calls what all how all does your or how do you generate your data set.

75
00:07:41,480 --> 00:07:46,160
So the the chat of course that's through the chat channel and that's that's tax that's

76
00:07:46,160 --> 00:07:51,600
written between you know the conversation between an agent and a member and then the actual

77
00:07:51,600 --> 00:07:58,360
call is a transcription so the call audio is transcribed to text and so we would be

78
00:07:58,360 --> 00:08:04,200
able to obtain that data which when we get it we it's redacted so it's all any kind

79
00:08:04,200 --> 00:08:08,080
of personal information or identifiable information or confidential information of our

80
00:08:08,080 --> 00:08:12,120
members is removed and then we'll be able to run topic modeling on that to be able

81
00:08:12,120 --> 00:08:14,240
to gain those insights.

82
00:08:14,240 --> 00:08:18,160
What approach is to topic modeling do you use.

83
00:08:18,160 --> 00:08:23,680
So the approach or the methodology that I use mainly is non-negative what's called

84
00:08:23,680 --> 00:08:30,880
non-negative matrix factorization so traditionally within the area of topic modeling there has

85
00:08:30,880 --> 00:08:36,560
been mainly like three methods which what the first method is is one of the original methods

86
00:08:36,560 --> 00:08:43,600
which is latent semantic indexing or lean semantics analysis and what that method does it takes

87
00:08:43,600 --> 00:08:48,120
a document term matrix so you can imagine these very large and by the way when I say

88
00:08:48,120 --> 00:08:55,840
document what I'm meaning is a any kind of unit of text under analysis it could be a it

89
00:08:55,840 --> 00:09:02,560
could be one chat it could be a chat or it could be a a call transcript it could even be

90
00:09:02,560 --> 00:09:09,040
a tweet or or some other kind of one structure data but so we're take you can imagine this

91
00:09:09,040 --> 00:09:15,040
document term matrix and along the say rows are all these documents along the columns

92
00:09:15,040 --> 00:09:21,200
are all these terms so there's different methods that you can use to decompose that matrix

93
00:09:21,200 --> 00:09:26,000
and to other matrices and be able to gain insights and like I mentioned the one of the original

94
00:09:26,000 --> 00:09:32,600
methods is latent semantic indexing and what that does it uses singular value decomposition

95
00:09:32,600 --> 00:09:39,680
to decompose that document term matrix into three other matrices and so but the criteria

96
00:09:39,680 --> 00:09:47,200
with that method is that it's creating or the matrices that are decomposed into our orthogonal

97
00:09:47,200 --> 00:09:55,600
vectors within each of those matrices and so that is a concern because a conversation is usually

98
00:09:55,600 --> 00:10:01,360
includes more than one topic you know we could be talking you know about hurricane Florence but

99
00:10:01,360 --> 00:10:05,760
we could also be talking about other topics involved as well and we want to be able to distinguish

100
00:10:05,760 --> 00:10:12,160
those type of topics and with latent semantic indexing it usually has an issue doing that because

101
00:10:12,160 --> 00:10:20,160
of the orthogonality in the in the matrices that it decomposes into another method is before we

102
00:10:20,160 --> 00:10:28,480
continue does that suggest that this latent semantic indexing might be better used for smaller

103
00:10:28,480 --> 00:10:36,400
documents like a tweet or an individual chat message as opposed to an entire transcript of a

104
00:10:36,400 --> 00:10:42,880
call or an entire chat interaction it could be a tweet it could but it's usually it's probably

105
00:10:42,880 --> 00:10:49,760
best applied when you don't have overlap within a given of different topics within a document

106
00:10:49,760 --> 00:10:53,680
which would be sort of it could be a tweet because you're restricted to the number character

107
00:10:53,680 --> 00:11:00,240
so you're usually talking about one topic is that kind of inherent to the the document or

108
00:11:00,960 --> 00:11:08,960
to the set of classes that you are looking to associate these documents with for example

109
00:11:08,960 --> 00:11:19,120
or a meaning if I've got a news site and I've got and I have like some you know strictly delineated

110
00:11:19,120 --> 00:11:26,320
like a set of like categories of news like you know health and finance and sports that are

111
00:11:27,120 --> 00:11:35,200
you know orthogonal does that mean that I can use LSIers it more have more to do with the content

112
00:11:35,200 --> 00:11:40,560
of the documents themselves it is usually it's probably more to do with the content of the document

113
00:11:40,560 --> 00:11:48,880
themselves themselves because you could you could be talking about if you had like a large corpus of

114
00:11:48,880 --> 00:11:56,240
tweets and let's suppose that they're talking about world cup and and the Olympics if you take all

115
00:11:56,240 --> 00:12:03,920
of those tweets and you just put them into one bucket then the latent semantic indexing might

116
00:12:03,920 --> 00:12:11,040
do okay with the the tweets since they may have be talking about different or the different types

117
00:12:11,040 --> 00:12:17,600
of topics and it may be able to distinguish them better than a say a call transcript that could

118
00:12:17,600 --> 00:12:24,080
include multiple different topics and so it makes sense. Just taking a step back these

119
00:12:25,200 --> 00:12:32,720
I'm thinking about the example that I gave where I'm trying to put documents into topics and that's

120
00:12:32,720 --> 00:12:37,760
a different kind of a problem that's more of a categorization type of a problem whereas what we're

121
00:12:37,760 --> 00:12:42,880
doing what you're we're talking about here with topic modeling is like an unsupervised learning

122
00:12:42,880 --> 00:12:50,000
like you just have the data and you're trying to identify what the topics are in the data itself

123
00:12:50,000 --> 00:12:58,240
is that is that right? Absolutely and you can think of it as a form of soft clustering okay so with

124
00:12:58,240 --> 00:13:05,120
the key means clustering for example that's a form of hard clustering where it's actually taking

125
00:13:05,120 --> 00:13:13,360
a specific document and categorizing it within a specific cluster but because we know that a document

126
00:13:13,360 --> 00:13:19,440
could include more than one type of conversation or topic or theme we want to we don't necessarily

127
00:13:19,440 --> 00:13:27,360
want a hard place that do a hard clustering to place that document within one specific cluster

128
00:13:27,360 --> 00:13:34,160
what we would rather do is see how well see how many topics are existing when that document

129
00:13:34,160 --> 00:13:40,480
and then say what is the probability that this document is associated with say topic number one

130
00:13:41,200 --> 00:13:46,640
what's the probability associated with topic number two etc so that's it's more of a soft clustering

131
00:13:46,640 --> 00:13:53,200
with is what topic modeling will do for you okay all right so latent semantic indexing

132
00:13:53,200 --> 00:13:58,400
it's kind of a traditional way that folks might approach this but it's got this limitation

133
00:13:58,400 --> 00:14:05,920
because it enforces that these terms be orthogonal to one another so it doesn't perform while when

134
00:14:05,920 --> 00:14:13,040
your documents have multiple topics in them yes absolutely and so another couple other methods

135
00:14:13,040 --> 00:14:17,440
that work pretty well is one of traditional method it's called latent dirtual allocation which is

136
00:14:17,440 --> 00:14:25,600
more of a probabilistic type method based on the dirtual distributions and it does pretty good

137
00:14:25,600 --> 00:14:34,240
but what we have found is that because it uses Gibbs sampling which is more of a random selection

138
00:14:34,240 --> 00:14:42,880
of the terms and the documents we see a tendency that the topics results vary from one run to another

139
00:14:42,880 --> 00:14:50,160
and so that's where I sort of lean more towards the third method which is non-negative matrix

140
00:14:50,160 --> 00:14:56,960
factorization and what that does it takes the original document term matrix and decomposes it

141
00:14:56,960 --> 00:15:03,840
into two other matrices one being a document topic matrix and a topic term matrix presumably

142
00:15:03,840 --> 00:15:11,760
because this isn't using Gibbs sampling your results given a particular corpus are more repeatable

143
00:15:11,760 --> 00:15:18,640
yes yes so with the non-negative matrix factorization what it's doing is it's more of an optimization

144
00:15:18,640 --> 00:15:25,120
process so what it's doing is decomposing that original document term matrix into the two other

145
00:15:25,120 --> 00:15:32,560
matrix matrices and the goal is to minimize the error between the original matrix and the product

146
00:15:32,560 --> 00:15:38,160
of the other two and it's pretty repeatable I mean it does a pretty good job at providing good

147
00:15:38,160 --> 00:15:47,920
results and also that you can run over continuously run and now when I hear matrix factorization I hear

148
00:15:47,920 --> 00:15:57,440
hard does that is that necessarily the case or you know maybe hard to scale is maybe more

149
00:15:57,440 --> 00:16:05,120
particular do you find that to be the case or are there methods for doing this matrix factorization

150
00:16:05,120 --> 00:16:13,600
that are you know computationally tenable at large scale yeah that so large scale it does pretty

151
00:16:13,600 --> 00:16:21,760
good is you know with using very large corpus sizes I guess the challenge can be when you start

152
00:16:21,760 --> 00:16:27,600
getting it running it on like near real-time data online data okay and and that can be a challenge

153
00:16:27,600 --> 00:16:33,680
but there are different methods that can use the original topics and then do some kind of

154
00:16:33,680 --> 00:16:38,080
resampling of the new topics or new documents that are coming in and integrating them in

155
00:16:39,360 --> 00:16:48,240
so scalability to large corpus isn't really actually a big deal and you know the time it

156
00:16:48,240 --> 00:16:54,640
takes to do this can be a big deal but there are some methods to overcome that yes yes

157
00:16:56,000 --> 00:17:02,080
okay so interesting so you know we've talked about this document term matrix as being the

158
00:17:02,080 --> 00:17:08,000
starting place for you know all three of these topic modeling methods but we haven't talked about

159
00:17:08,000 --> 00:17:15,760
how we even get to that how we represent the documents in this term matrix that's there are

160
00:17:15,760 --> 00:17:22,800
different ways to do that right sure so there's a there's a pipeline for this process and so what we

161
00:17:22,800 --> 00:17:29,840
do is we want to do some kind of pre-processing so we may want to of course tokenize the documents

162
00:17:29,840 --> 00:17:39,200
so each term is a single dimension but we also want to remove things like the stop words so the

163
00:17:39,200 --> 00:17:46,560
common terms like a the high frequency terms that may not be able to give us give us very good insights

164
00:17:46,560 --> 00:17:53,840
with the topics or the subjects so we remove those as well and then we may want to also apply some

165
00:17:53,840 --> 00:18:00,320
dimensionality reduction techniques so there's of course when you're working with a large corpus

166
00:18:00,320 --> 00:18:06,480
of documents the dimensions can be very large and that interferes with the processing time as well

167
00:18:06,480 --> 00:18:12,320
so there's dimensionality reduction techniques that we can apply what I have done in the past is

168
00:18:12,320 --> 00:18:20,560
you know do a single phase or stage where I'm running topic modeling to help reduce the

169
00:18:20,560 --> 00:18:27,920
dimensions across all the the documents as a pre-processing step as well and so what's an example

170
00:18:27,920 --> 00:18:38,400
of a dimensionality reduction technique that you might apply to this document term matrix so what

171
00:18:38,400 --> 00:18:45,360
I what I do usually as far as a dimensionality reduction technique is I will run

172
00:18:45,360 --> 00:18:53,520
non-negative matrix factorization on the entire corpus using two techniques one one is term

173
00:18:53,520 --> 00:19:01,360
frequency forming a term frequency matrix and the other frequent or method is using term frequency

174
00:19:01,360 --> 00:19:06,960
inverse document frequency and so term frequency what that's really doing is forming the original

175
00:19:06,960 --> 00:19:13,600
document term matrix that based on the frequency of terms whereas term frequency inverse document

176
00:19:13,600 --> 00:19:19,840
frequency is is looking at the frequency of terms within each document but also looking at the

177
00:19:19,840 --> 00:19:26,160
the frequency of a term across all the documents so it's sort of like offsetting so a very high

178
00:19:26,160 --> 00:19:33,520
frequency term that is existing across all the document may not receive as much weight as if

179
00:19:33,520 --> 00:19:39,440
it's high frequency but only occurring across a few documents and so what I'm going to do is I'm

180
00:19:39,440 --> 00:19:48,320
going to run the non-negative matrix factorization on the entire corpus at first and get and reduce my

181
00:19:48,320 --> 00:19:57,680
vocabulary size from say hundreds of thousands of terms to possibly down to a few hundred or a

182
00:19:57,680 --> 00:20:06,000
thousand terms and that is just the pre-processing and then I can go ahead and run get more refinement

183
00:20:06,000 --> 00:20:13,520
running topic modeling further on on those documents so how does doing your matrix your

184
00:20:13,520 --> 00:20:24,320
non-negative matrix factorization on the larger document term matrix gets you to reducing the

185
00:20:24,320 --> 00:20:32,320
number of terms so the it goes back to the original decomposition so the non-negative matrix

186
00:20:32,320 --> 00:20:40,400
factorization is decomposing the original document term matrix into the two other matrices which are

187
00:20:40,400 --> 00:20:47,520
the document topic matrix and the topic term matrix and so what it's going to do is it's you can

188
00:20:47,520 --> 00:20:56,080
reduce the number of terms based on the the number of topics so if you have so what it's going to do

189
00:20:56,080 --> 00:21:04,080
is if you look at the topic term matrix it's giving you the strength of association between

190
00:21:04,080 --> 00:21:11,200
the topics in the terms where so that if you have a term that has a higher rate it has a

191
00:21:11,200 --> 00:21:16,000
more of a higher strength of association with a given topic so if I say look at the

192
00:21:16,000 --> 00:21:26,320
top 100 terms within a given topic and just consider those terms as part of my vocabulary

193
00:21:26,320 --> 00:21:31,040
that I'm going to use then that will give me some kind of reduction dimensionality reduction

194
00:21:31,040 --> 00:21:38,240
as opposed to using all of the terms if you're looking at say these top 100 terms on a for a given

195
00:21:38,240 --> 00:21:46,560
topic are you then kind of zeroing out terms back in your document term matrix and kind of

196
00:21:46,560 --> 00:21:52,320
creating more of a sparse matrix or are you changing the shape of that matrix based on this new

197
00:21:52,320 --> 00:21:58,400
information that you have so it sort so it does change the shape so what it's doing is I'm going to

198
00:21:58,400 --> 00:22:07,200
be my initial run with the non-negative matrix factorization is going to create a new document term

199
00:22:07,200 --> 00:22:13,920
matrix but only include those terms that will have the highest association between that I had

200
00:22:13,920 --> 00:22:22,400
originally from the the preprocessing run with topics so it's what it's really doing is it's given

201
00:22:22,400 --> 00:22:32,400
me terms that are more relevant when I run subsequent topic modeling so just to kind of pull on this

202
00:22:32,400 --> 00:22:37,520
one you know one more step so you've got these terms you zero out you know say you've got 100,000

203
00:22:37,520 --> 00:22:44,160
terms and you've got well how many topics would be typical in your case so that's a good question

204
00:22:44,160 --> 00:22:50,320
actually that's a that's a sort of an open problem the number of topics but I do have we have a

205
00:22:50,320 --> 00:22:57,760
technique that we run it's to compute what is a coherent score interpretability of the topics

206
00:22:57,760 --> 00:23:04,240
and what that that running the coherence will do is allow us to identify sort of like the optimal

207
00:23:04,240 --> 00:23:10,720
number of topics that we need to run and so is this on the order of tens or hundreds or thousands

208
00:23:10,720 --> 00:23:18,720
for you know a you know a corpus of documents that well maybe give us kind of general numbers for

209
00:23:18,720 --> 00:23:24,320
each of these things how many documents do you tend to have tend to see you know rough order

210
00:23:24,320 --> 00:23:31,280
magnitude how many terms how many topics oh so it could be I mean we could easily be running

211
00:23:31,280 --> 00:23:40,400
say 50,000 documents through the through the topic modeling and in one run it could be more

212
00:23:40,400 --> 00:23:46,800
but usually around 50,000 the number of terms of course you know that's it's going to vary I mean

213
00:23:46,800 --> 00:23:54,080
if we're running creating a topic term matrix on a call conversations then you know the calls

214
00:23:54,080 --> 00:24:00,320
can vary from you know a couple minutes to many minutes so the size of the corpus is can vary

215
00:24:00,320 --> 00:24:06,800
as well or the documents can vary as well but it's all it's all I mean the only limitation of course

216
00:24:06,800 --> 00:24:14,240
is the the processing speed you know to that we're working with when you are identifying the terms

217
00:24:14,240 --> 00:24:24,720
that are most meaningful is that on a topic by topic basis or is that across the corpus specifically

218
00:24:24,720 --> 00:24:28,720
I'm asking about this this step where we're doing the dimensionality reduction that's so much

219
00:24:28,720 --> 00:24:34,880
like the meaning of the topic itself and the the coherence um specifically when we're looking

220
00:24:34,880 --> 00:24:40,240
doing that dimensionality reduction and trying to get from our 100,000 terms down to 100

221
00:24:40,240 --> 00:24:47,360
is that on a per topic basis or is that are you looking at the 100 terms that have the most

222
00:24:48,560 --> 00:24:55,840
weight across all topics does that question make sense yeah so the the final vocabulary is based

223
00:24:55,840 --> 00:25:03,520
on the the terms across all the topics that were used in the pre-processing phase okay so you've

224
00:25:03,520 --> 00:25:15,440
got these uh you identify these hundred say terms that uh most strongly correlate to these topics

225
00:25:15,440 --> 00:25:20,480
that you've identified and that kind of answered my question I guess that I've been trying to get to

226
00:25:20,480 --> 00:25:26,400
the long way around if you're doing it across the all of the topics then you can just kind of get

227
00:25:26,400 --> 00:25:31,600
rid of all the other terms in your document term matrix as opposed to if you were doing it on a

228
00:25:31,600 --> 00:25:39,040
topic by topic basis then you still have kind of this potentially large sparse matrix yes okay

229
00:25:40,480 --> 00:25:47,280
and and so this this resulting set of vocabulary in this pre-processing that we just ran based on

230
00:25:47,280 --> 00:25:55,440
this um um topic modeling to identify the the top terms across all topics across the entire

231
00:25:55,440 --> 00:26:01,440
corpus this vocabulary says what's going to be used in the subsequent topic modeling and that's the

232
00:26:01,440 --> 00:26:08,560
dimensionality reduction you mentioned tf idf and and term frequency how do those play into here again

233
00:26:09,600 --> 00:26:15,680
so in the in the subsequent steps I'm just going to be using ti tf idf um there's really

234
00:26:16,480 --> 00:26:21,600
so in the pre-processing when I do the initial run for non-negative matrix factorization I run

235
00:26:21,600 --> 00:26:26,080
usable techniques to identify the vocabulary the term frequency and the term frequency inverse

236
00:26:26,080 --> 00:26:32,800
document frequency um but once I identify the vocabulary in the subsequent steps I'll just be running

237
00:26:32,800 --> 00:26:39,280
the the tf idf which the difference between the two methods is that the term frequency it's looking

238
00:26:39,280 --> 00:26:48,080
at it's really assigning a equal weight to each term and so it um it's it's looking at the how

239
00:26:48,080 --> 00:26:54,560
for a given term how frequent that term occurs across the entire corpus but really there's no

240
00:26:54,560 --> 00:27:02,000
difference on how it's applying the the weight that weight value to term a as it is to term b but

241
00:27:02,000 --> 00:27:09,200
you may have a term that it's a high frequency not only in a given document but a very high

242
00:27:09,200 --> 00:27:16,800
frequency across all the documents so that specific term may not really give you much insights on

243
00:27:16,800 --> 00:27:23,440
the topic of the conversation so the other method which is term frequency inverse document frequency

244
00:27:23,440 --> 00:27:29,680
sort of offsets that it'll look at not only the frequency of the terms uh within a document but

245
00:27:29,680 --> 00:27:35,840
also how frequent that frequent that term occurs across other documents so it will give a high value

246
00:27:35,840 --> 00:27:42,080
when terms for example occurs many times within a small number of documents and that's what I

247
00:27:42,080 --> 00:27:48,320
they're using the subsequent topic modeling as well uh then you mentioned the coherence score

248
00:27:49,280 --> 00:27:58,720
what does that help you get to so you can think of coherence as a measure that captures the

249
00:27:58,720 --> 00:28:05,120
semantic interpretability of topics and it's it's based on the co-occurrence of topics or topic

250
00:28:05,120 --> 00:28:11,600
words and where that's important is that when you when the topic model or say non-negative matrix

251
00:28:11,600 --> 00:28:18,160
factorization is creating these categories consisting of these topic categories that are

252
00:28:18,160 --> 00:28:24,400
consisting of terms we want the analyst to be able to look at those topics and say oh I get it

253
00:28:24,400 --> 00:28:31,120
I understand what that topic is about and that starts getting into interpretability and you want

254
00:28:31,120 --> 00:28:39,680
to be able to generate a coherence score that is going to provide you a good semantic interpretability

255
00:28:39,680 --> 00:28:50,240
of the topics and that's it's that sort of like coherence score is a function of the number of topics

256
00:28:50,880 --> 00:28:58,400
and so what we have done is um identify the number of topics that will maximize the average coherence

257
00:28:58,400 --> 00:29:08,400
across the corpus and what's the intuition for kind of the the math you know the score itself what

258
00:29:08,400 --> 00:29:17,760
the coherent score is saying so it's based on co-occurrence of words so there's there's a few methods

259
00:29:17,760 --> 00:29:28,160
that are are used but what it's doing is it's it's looking at the counts of the number of documents

260
00:29:28,160 --> 00:29:36,960
containing like a set of terms words and it's using that and it's also taking consideration

261
00:29:36,960 --> 00:29:46,080
the the logarithm of that of that count and so it's it's you the closer for the specific coherent

262
00:29:46,080 --> 00:29:54,160
score um I thought I'm using the closer that value is to zero the more coherent your topic is

263
00:29:54,800 --> 00:30:01,440
and the more negative the value is the less coherent it is and so what we want to be able to do is

264
00:30:01,440 --> 00:30:09,600
identify the one number of the topics that will maximize coherence and then that's the the

265
00:30:09,600 --> 00:30:14,560
number of topics that we'll use and and be able to compute the the topic categories and also the

266
00:30:14,560 --> 00:30:21,760
coherent scores for each topic category okay and that number of topics that maximizes coherence

267
00:30:21,760 --> 00:30:30,000
is going to be ultimately based on the relationships between these terms in the given documents

268
00:30:30,000 --> 00:30:35,680
in the corpus yes the co-occurrence yes the relationship between terms okay do you have any kind

269
00:30:35,680 --> 00:30:42,560
of rules of thumb that you found for I guess it really depends a lot on the the types of documents

270
00:30:42,560 --> 00:30:49,280
but for Isaiah kind of a standard you know say for call transcripts do you have any rules of thumb

271
00:30:49,280 --> 00:30:55,040
where it you know it ends up being on the order of you know hundreds of hundreds of topics as

272
00:30:55,040 --> 00:31:01,120
opposed to you know tens or thousands so I've actually I mean this is probably surprising but I've

273
00:31:01,120 --> 00:31:09,360
actually a rule of thumb is between five and eight topics is a pretty good value to get

274
00:31:09,360 --> 00:31:15,360
into good interpretability if you start getting larger than that number of topics then you start

275
00:31:15,360 --> 00:31:21,840
losing the coherence how does it fail like are there patterns in the way it fails like does it pick

276
00:31:21,840 --> 00:31:30,640
up words that you know just aren't meaningful in the sense of you know not particularly additive

277
00:31:30,640 --> 00:31:39,680
to understanding what the topic is of a given document or is it trying to you know split topics

278
00:31:39,680 --> 00:31:44,960
to you know find you know find grain so there are words that are kind of similar that should be

279
00:31:44,960 --> 00:31:52,400
one topic like is there a GC patterns in the way it tends to fail if you have too many topics I

280
00:31:52,400 --> 00:32:00,080
think what it will try if you have too many topics what it will try to do is spread yes spread those

281
00:32:00,080 --> 00:32:10,320
terms across more more of those topic categories and therefore you start losing the interpretability

282
00:32:10,320 --> 00:32:16,400
but you know that when you when you do a topic modeling I mean it goes back to the the

283
00:32:16,400 --> 00:32:21,360
purpose of it you know it's doing soft clustering so it's it's definitely possible that you're

284
00:32:21,360 --> 00:32:31,440
going to see a term existing in more than one topic category you know for example if I if I

285
00:32:31,440 --> 00:32:37,840
have seven documents and you know four of the documents are talking about financial banks and

286
00:32:37,840 --> 00:32:44,800
three of the documents are talking about river banks more likely you're going to see the word bank

287
00:32:45,360 --> 00:32:52,800
across those all seven or all all the topics but you're probably going to be able to still be

288
00:32:52,800 --> 00:32:58,480
able to distinguish the topics because you're going to see things that are talking about river

289
00:32:58,480 --> 00:33:03,680
banks and certain topics and you're going to see things that are terms that are associated with

290
00:33:03,680 --> 00:33:10,320
um financial banks and other topics and the reason is is because it's based topic modeling is

291
00:33:10,320 --> 00:33:16,640
based on co-occurrence of terms so you're going to see a lot of um commonality of terms that are

292
00:33:16,640 --> 00:33:23,600
co-occurring together and those given documents placed in a specific topic and it and it does

293
00:33:23,600 --> 00:33:31,680
pretty good it does really good is there a way to identify what the primary topic of a given

294
00:33:31,680 --> 00:33:38,080
document is is that just kind of ranking the the topics based on some score probability

295
00:33:39,360 --> 00:33:43,760
so that that's a very good question and that's actually one of the purposes of doing the topic

296
00:33:43,760 --> 00:33:50,960
modeling as well so if you go back to the the original decomposition it's taken that document

297
00:33:50,960 --> 00:33:59,680
termatrix and decomposing it into a document topic matrix and also the topic term matrix so the

298
00:33:59,680 --> 00:34:05,600
the topic termatrix is giving you the strength of association between each of the terms and a given

299
00:34:05,600 --> 00:34:13,200
topic so that if I say I take the top 10 terms for a given topic which top 10 terms being the ones

300
00:34:13,200 --> 00:34:17,040
that have the highest weight of association with a given topic you know that gives me information

301
00:34:17,040 --> 00:34:22,960
about what that topic or what is the theme of that given topic but the other matrix the document

302
00:34:22,960 --> 00:34:30,000
topic matrix what that is doing is giving you the strength of association between each topic and

303
00:34:30,000 --> 00:34:36,880
a given document and where that's important is it goes back to a conversation so if you and I

304
00:34:36,880 --> 00:34:41,600
are having a conversation we're probably going to be talking about more than one topic but we

305
00:34:41,600 --> 00:34:47,040
may but want to identify well what's the primary topic and that's where that that document topic

306
00:34:47,040 --> 00:34:53,280
matrix is important because it will allow us to identify the topic that has the highest value or

307
00:34:53,280 --> 00:34:58,800
weight which is the highest strength of association between that topic and that that given document

308
00:34:58,800 --> 00:35:05,280
and I can look across a given corpus and run topic modeling on open to transcripts across a given

309
00:35:05,280 --> 00:35:12,640
corpus and I can be able to identify not only the topics of conversations across all the documents

310
00:35:12,640 --> 00:35:20,800
but also I can identify which documents had topic one as the primary topic of conversation.

311
00:35:21,520 --> 00:35:26,720
You've done this factorization you've got this document topic matrix and this topic term

312
00:35:26,720 --> 00:35:34,240
matrix can you then take this topic term matrix and kind of use it as a model that you can run

313
00:35:34,240 --> 00:35:42,240
inference against when you have a new document you kind of put that through your your term pipeline

314
00:35:42,240 --> 00:35:48,400
to get the terms in that document and then you kind of apply you multiply it by this topic term

315
00:35:48,400 --> 00:35:56,560
matrix to get the the topics in that document. So it's sort of like subsequent modeling like subsequent

316
00:35:56,560 --> 00:36:03,920
right right so trying to get the topics in a document that you didn't or maybe the the question

317
00:36:03,920 --> 00:36:10,080
that you know the question before this is does this topic term matrix tend to have strong predictive

318
00:36:10,080 --> 00:36:18,480
value for documents that don't exist within the corpus that it was created on is is not like a

319
00:36:18,480 --> 00:36:24,240
technique like a supervised learning where you're built into the process you're testing against

320
00:36:25,280 --> 00:36:32,560
data that you haven't built your model on. Yeah so I what you can do is so the non the topic

321
00:36:32,560 --> 00:36:38,880
modeling is definitely an unsupervised machine learning method and but you can think of it as being

322
00:36:38,880 --> 00:36:48,480
a precursor to a supervised method so you could really take the top the topic term matrix that is

323
00:36:48,480 --> 00:36:54,800
produced from this non-negative matrix factorization and look at each one of those topics that are

324
00:36:54,800 --> 00:37:02,160
as an output and and label them you can provide a label for each of the topic and then then create a

325
00:37:02,160 --> 00:37:13,120
supervised model say with a deep learning model that is able to classify a new call into one of

326
00:37:13,120 --> 00:37:21,520
those topic categories and then if you say go below a given threshold of probability perhaps

327
00:37:21,520 --> 00:37:27,280
maybe it's a new topic and you might want to put that new topic or that document associated with

328
00:37:27,280 --> 00:37:33,280
that one it fell below a threshold into a bucket and rerun topic modeling on it so it's sort you

329
00:37:33,280 --> 00:37:40,160
can almost think of this process as a feedback you're you're learning new topics and integrating them

330
00:37:40,160 --> 00:37:44,880
into a new model updating the model and then that model is making supervised models making predictions

331
00:37:44,880 --> 00:37:52,000
and then if it isn't able to predict a new document based on a given probability it puts it into

332
00:37:52,000 --> 00:37:59,440
a bucket and reruns topic modeling on it given that you have this limitation of five to eight

333
00:37:59,440 --> 00:38:09,120
topics in order to achieve interpretability or coherence is there is there a process through which

334
00:38:09,120 --> 00:38:15,360
you might you know if you've got a large corpus of say several hundred thousand documents

335
00:38:15,360 --> 00:38:23,440
or even a you know this fifty thousand document corpus cluster your documents or partition your

336
00:38:23,440 --> 00:38:31,680
documents and create separate topic models and so as to maximize the number of coherent topics that

337
00:38:31,680 --> 00:38:38,800
you end up with yeah I think you could you could definitely do as a one of the initial steps

338
00:38:38,800 --> 00:38:46,480
you could create rules that would separate the documents look based on the structure data the

339
00:38:46,480 --> 00:38:53,520
metadata and you know you you might be able to for for example calls you know you if the calls

340
00:38:53,520 --> 00:39:01,920
are going to the bank within the bank you know they might be going to deposits or or checking

341
00:39:01,920 --> 00:39:08,080
or something like that so as an initial step you might be able to categorize those calls and then

342
00:39:08,080 --> 00:39:15,600
runs separate topic modeling on each one of those categories you might be able to do really

343
00:39:15,600 --> 00:39:20,400
just it's a matter of looking at the structure data before you would separate them and then

344
00:39:20,400 --> 00:39:24,640
once you got those bucket of categories run topic modeling on them so you would have different

345
00:39:24,640 --> 00:39:29,360
topic models so what you're suggesting is as opposed to trying to do it in a machine learn kind

346
00:39:29,360 --> 00:39:36,560
of way take advantage of the the metadata if you will that's inherent in your problem to partition

347
00:39:36,560 --> 00:39:44,480
up your documents into you know I guess there's a sweet spot right a small a small of a

348
00:39:44,480 --> 00:39:51,760
corpus as possible but that still allows you to create a robust topic model absolutely you really

349
00:39:51,760 --> 00:39:56,640
what you're working with is semi structure data you know you're you're working with the structure

350
00:39:56,640 --> 00:40:02,080
data in the metadata and also of course the unstructured data which is the text and you're

351
00:40:02,080 --> 00:40:10,080
really combining the two to be able to do the topic modeling effort so you do this topic modeling

352
00:40:10,080 --> 00:40:19,440
you get these topics you know five to eight topics from a corpus of 50,000 transcripts or chat

353
00:40:19,440 --> 00:40:27,200
call or chat transcripts how do you then integrate these back into the call center operations to

354
00:40:27,200 --> 00:40:36,800
improve them so for it goes back to the example I used if we're identifying topics that are

355
00:40:36,800 --> 00:40:44,800
emerging before we reach a peak call volume what we're able to do is be more proactive with the

356
00:40:44,800 --> 00:40:52,320
topics we may be able to create better or inform our agents of these these certain topics that are

357
00:40:52,320 --> 00:40:59,520
coming in so they can be more prepared to mitigate any concerns or better serve our members

358
00:41:00,800 --> 00:41:07,840
we we may be able to even make adjustments if there's if there's confusion about something that's

359
00:41:09,200 --> 00:41:17,920
say on the mobile channels you know either dot com or or mobile we may able to make adjustments

360
00:41:17,920 --> 00:41:22,960
on those on those those different channels because we're starting to see topics and they're

361
00:41:22,960 --> 00:41:30,560
associated with those and then that also in turn could decrease any call volumes associated

362
00:41:30,560 --> 00:41:37,520
with those concerns but it's really being trying to be more proactive to better serve our members

363
00:41:37,520 --> 00:41:44,400
what rather than you know reacting after the fact and do you find it the initial steps of this

364
00:41:44,400 --> 00:41:51,360
process where you're doing dimensionality reduction end up being at odds a bit with trying to use

365
00:41:51,360 --> 00:41:57,600
this as kind of an early warning system because they get rid of emerging terms that might not have

366
00:41:57,600 --> 00:42:07,840
yet achieved some kind of critical mass no not really what I think the how we approach it it's

367
00:42:07,840 --> 00:42:17,680
able to identify the most significant terms and it's able to capture the most relevant topics

368
00:42:18,800 --> 00:42:23,840
it does a pretty good job it's able to surface the things that we didn't even know you know I

369
00:42:23,840 --> 00:42:28,160
go back to the unknown unknowns you know the things we don't know what we don't know you know it's

370
00:42:28,160 --> 00:42:34,480
able to surface those things so we're able to take action on them awesome awesome are there any open

371
00:42:34,480 --> 00:42:41,920
challenges or problems that you see recurring as you try to use these types of models you know I

372
00:42:41,920 --> 00:42:48,800
would say there's definitely open problems I think that choosing the number of topics is still

373
00:42:48,800 --> 00:42:54,640
an open problem I mean I mentioned that we you know use the coherence score to try to maximize

374
00:42:54,640 --> 00:42:59,360
coherence you what is the number of topics that maximize coherence I still think that's an open

375
00:42:59,360 --> 00:43:08,080
problem an appropriate corpus size I think that's still an open problem as well as the the document

376
00:43:08,080 --> 00:43:13,120
term matrix you know there's there's different weights that are used in a document term matrix such

377
00:43:13,120 --> 00:43:19,440
as the term frequency and term frequency inverse document frequency but I think there's more room

378
00:43:19,440 --> 00:43:26,080
for developing new weights and doing more research in that area and then of course you know I talked

379
00:43:26,080 --> 00:43:31,760
about coherence you know coherence is the interpretability of topics but I think also

380
00:43:32,720 --> 00:43:38,000
determining the relevancy of topics is really important because you know something could be

381
00:43:38,720 --> 00:43:45,040
interpretable but is it really relevant and we need to take action on it well Bill thanks so much

382
00:43:45,040 --> 00:43:50,080
for taking the time to chat with us about this I'm sorry that you didn't get a chance to present

383
00:43:50,080 --> 00:43:55,840
this as stratum sure folks would have enjoyed it but it's really interesting stuff well thank you

384
00:43:55,840 --> 00:44:05,280
Sam once again thank you for inviting me absolutely all right everyone that's our show for today

385
00:44:05,280 --> 00:44:11,920
for more information about today's show visit twimmel ai.com be sure to visit twimmelcon.com for

386
00:44:11,920 --> 00:44:18,160
information or to register for twimmelcon ai platforms thanks again to c3 for their sponsorship

387
00:44:18,160 --> 00:44:25,200
of today's episode to check out what they're up to visit c3.ai as always thanks so much for

388
00:44:25,200 --> 00:44:27,200
listening and catch you next time

