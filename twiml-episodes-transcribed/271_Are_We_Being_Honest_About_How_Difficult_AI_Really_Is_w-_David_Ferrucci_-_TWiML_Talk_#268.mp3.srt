1
00:00:00,000 --> 00:00:15,000
Hello and welcome to another episode of Twimble Talk, the podcast.

2
00:00:15,000 --> 00:00:25,000
We interrupt our program to bring you this important message.

3
00:00:25,000 --> 00:00:28,000
Happy birthday, Twimble.

4
00:00:28,000 --> 00:00:36,000
Today is a very special day and this is a very special show because we are celebrating the third.

5
00:00:36,000 --> 00:00:41,000
That's right, the third birthday of your favorite machine learning and AI podcast.

6
00:00:41,000 --> 00:00:48,000
I'll be honest, if you told me three years ago when I was struggling to get that very first episode of this week in machine learning and AI,

7
00:00:48,000 --> 00:00:57,000
recorded and posted, that three years later, I'd still be at it, that we'd be publishing two, three, four or sometimes five shows a week,

8
00:00:57,000 --> 00:01:03,000
that I'd have interviewed well over 300 incredible ML and AI researchers and practitioners.

9
00:01:03,000 --> 00:01:09,000
That we'd have an amazing community of fans that engage and encourage us daily.

10
00:01:09,000 --> 00:01:15,000
That we'd help hundreds of people learn machine learning and deep learning through our volunteer led study groups.

11
00:01:15,000 --> 00:01:19,000
That we'd hit five million downloads on our third birthday?

12
00:01:19,000 --> 00:01:22,000
Yes, you heard that right, that just happened?

13
00:01:22,000 --> 00:01:29,000
Well, let's just say that if you told me any of that, I'd have definitely thought you were crazy.

14
00:01:29,000 --> 00:01:35,000
It's been a truly amazing and humbling experience bringing you the podcast every week

15
00:01:35,000 --> 00:01:43,000
and I am more appreciative than ever of you, our listeners and the amazing community that we've built around this show.

16
00:01:43,000 --> 00:01:49,000
From the entire Twimble team, thank you so much and Happy Birthday.

17
00:01:53,000 --> 00:01:57,000
With that said, we'd love to hear your Twimble story.

18
00:01:57,000 --> 00:02:03,000
In our conversations with listeners over time, we've heard quite a few stories detailing what you've learned from the podcast

19
00:02:03,000 --> 00:02:08,000
and how you've applied it to your own work, research, philosophy and lives.

20
00:02:08,000 --> 00:02:16,000
And now we want to hear from all of you. Please take a moment to hop over to TwimbleAI.com slash 3B day

21
00:02:16,000 --> 00:02:27,000
or leave a voicemail at 1636-735-3658, letting us know your favorite tidbit that you've been able to take from the show

22
00:02:27,000 --> 00:02:30,000
and how you've applied it to what you do.

23
00:02:30,000 --> 00:02:35,000
Everyone who joins in will be sent a limited edition third birthday Twimble sticker

24
00:02:35,000 --> 00:02:40,000
and the best submissions have a chance to be featured in an upcoming episode of the show.

25
00:02:40,000 --> 00:02:51,000
Again, that's TwimbleAI.com slash 3B day for your written comment or 1636-735-3658 for your voicemail message.

26
00:02:51,000 --> 00:02:55,000
And whatever you do, stay tuned.

27
00:02:55,000 --> 00:02:59,000
We've got tons of exciting things in the works that we can't wait to bring you.

28
00:02:59,000 --> 00:03:05,000
For starters, next week we'll be releasing volume 2 of our AI platform series,

29
00:03:05,000 --> 00:03:10,000
a follow-up to one of your all-time favorites based on the feedback we've received.

30
00:03:10,000 --> 00:03:17,000
We've got much, much more in the pipeline for you including a huge update that we are super excited about

31
00:03:17,000 --> 00:03:20,000
but those are still top secret for now.

32
00:03:20,000 --> 00:03:24,000
Just be sure whatever you do to listen in next week.

33
00:03:24,000 --> 00:03:31,000
And now on to the show.

34
00:03:31,000 --> 00:03:34,000
Alright everyone, I am on the line with Dave Farouche.

35
00:03:34,000 --> 00:03:40,000
Dave is the founder, CEO and Chief Scientist at Elemental Cognition.

36
00:03:40,000 --> 00:03:43,000
Dave, welcome to this week of Machine Learning and AI.

37
00:03:43,000 --> 00:03:45,000
Hi, how are you? It's a pleasure to be here.

38
00:03:45,000 --> 00:03:48,000
Absolutely, absolutely great to have you on the show.

39
00:03:48,000 --> 00:03:56,000
So in some circles, you are perhaps best known as having built and led the IBM Watson team.

40
00:03:56,000 --> 00:04:01,000
And I'm curious, how did you arrive at that point in your career?

41
00:04:01,000 --> 00:04:05,000
Well, I was always interested in artificial intelligence.

42
00:04:05,000 --> 00:04:10,000
I mean, I have been since, actually since high school early college.

43
00:04:10,000 --> 00:04:15,000
It's been my fascination. I started programming, I guess, in high school.

44
00:04:15,000 --> 00:04:24,000
And right from the very beginning, I mean, after I think I wrote my first program was like in basic.

45
00:04:24,000 --> 00:04:29,000
I think it was on a PDP 11, I can't remember exactly, but it was really cheaper.

46
00:04:29,000 --> 00:04:39,000
And I think what blew me out of the water right away was, wow, if I can just describe my thought process,

47
00:04:39,000 --> 00:04:41,000
I can get a computer to do work for me.

48
00:04:41,000 --> 00:04:45,000
And that was just very exhilarating.

49
00:04:45,000 --> 00:04:52,000
And my mind directly went to this notion, I don't even think I personally had a word for it at the time.

50
00:04:52,000 --> 00:05:01,000
I learned later, it was called artificial intelligence, but this notion that if I can describe how I solve problems,

51
00:05:01,000 --> 00:05:08,000
if I can describe how I think, and I could put it in this language that the computer executes for me,

52
00:05:08,000 --> 00:05:14,000
I can get the computer to do all the hard work and allow me to kind of be creative.

53
00:05:14,000 --> 00:05:20,000
And I hated doing repetitive tasks, so this was just mind blowing to me.

54
00:05:20,000 --> 00:05:23,000
And I was on the path to become a medical doctor.

55
00:05:23,000 --> 00:05:25,000
My parents wanted me to be a medical doctor.

56
00:05:25,000 --> 00:05:27,000
That was like the thing to do in my neighborhood.

57
00:05:27,000 --> 00:05:30,000
If you were smart enough, I guess.

58
00:05:30,000 --> 00:05:32,000
But I just got this bug.

59
00:05:32,000 --> 00:05:39,000
And so by the time I was in college, I was doing more and more and more programming, programming everything I could possibly get my hands on,

60
00:05:39,000 --> 00:05:44,000
and then eventually switched to wanting to go to grad and set up going to medical school,

61
00:05:44,000 --> 00:05:47,000
wanting to go to graduate school and computer science.

62
00:05:47,000 --> 00:05:57,000
I eventually got my PhD and got a chance to work at IBM Research on an AI project.

63
00:05:57,000 --> 00:06:04,000
And interestingly enough, this was back in the, I think it was the late 80s.

64
00:06:04,000 --> 00:06:08,000
And artificial intelligence at the time became a bad word.

65
00:06:08,000 --> 00:06:10,000
And there was an AI winter.

66
00:06:10,000 --> 00:06:15,000
And it was all about the customers worrying that AI was going to take jobs.

67
00:06:15,000 --> 00:06:16,000
Can you imagine?

68
00:06:16,000 --> 00:06:19,000
And here we are in the same spot when we used later.

69
00:06:19,000 --> 00:06:20,000
It's fascinating.

70
00:06:20,000 --> 00:06:25,000
And they went around canceling AI projects.

71
00:06:25,000 --> 00:06:30,000
And my manager at the time said, you know, Dave, you've got to learn how to do.

72
00:06:30,000 --> 00:06:33,000
You could become a specialist in like a domain area.

73
00:06:33,000 --> 00:06:35,000
And you don't want to be a technologist, right?

74
00:06:35,000 --> 00:06:37,000
And we're not going to be doing AI anymore.

75
00:06:37,000 --> 00:06:38,000
And I quit.

76
00:06:38,000 --> 00:06:42,000
It's like because this is not that's clearly what I want to do.

77
00:06:42,000 --> 00:06:44,000
This is actually between my masters, my PhD.

78
00:06:44,000 --> 00:06:46,000
And so I went back and I finished my PhD.

79
00:06:46,000 --> 00:06:49,000
I actually ended up returning to IBM.

80
00:06:49,000 --> 00:06:56,000
And, you know, working on a number of different projects related to AI,

81
00:06:56,000 --> 00:07:02,000
to tech, analytics, speech, image analytics, building, software and infrastructure.

82
00:07:02,000 --> 00:07:07,000
I eventually got into open domain question answering had built a team.

83
00:07:07,000 --> 00:07:14,000
We competed in a lot of government-sponsored competitions in the open domain question answering track.

84
00:07:14,000 --> 00:07:19,000
So I got to a point where I was doing all the things I love to do.

85
00:07:19,000 --> 00:07:27,000
Software, software, architecture, software engineering, all around solving AI-type problems.

86
00:07:27,000 --> 00:07:29,000
And so I had built a team.

87
00:07:29,000 --> 00:07:36,000
So by about 2006, I had built a team of about 25 people,

88
00:07:36,000 --> 00:07:41,000
all, you know, specialists and a combination of software engineering and different aspects of AI,

89
00:07:41,000 --> 00:07:48,000
including natural language processing, machine learning, knowledge representation and reasoning.

90
00:07:48,000 --> 00:07:55,000
And this idea of doing this Jeopardy challenge came up.

91
00:07:55,000 --> 00:07:58,000
And actually, it came up two years before that.

92
00:07:58,000 --> 00:08:10,000
But the executive who wanted to do it kept on being turned down by the vast majority of scientists and researchers he would go to

93
00:08:10,000 --> 00:08:12,000
and say, hey, can we get this done?

94
00:08:12,000 --> 00:08:15,000
And they said, no, it's impossible. You can't do it. It's too hard.

95
00:08:15,000 --> 00:08:22,000
And I was interested in it when it came around in 2004 and 2005,

96
00:08:22,000 --> 00:08:24,000
but very busy with another project.

97
00:08:24,000 --> 00:08:29,000
And then at the end of 2006, I was coming off of the project.

98
00:08:29,000 --> 00:08:33,000
And I said, you know, I think this is possible.

99
00:08:33,000 --> 00:08:38,000
And I think we not only should we do it, we sort of have an obligation to do it.

100
00:08:38,000 --> 00:08:43,000
We've been working in open domain factoid question answering for some time.

101
00:08:43,000 --> 00:08:50,000
My team had participated in the track question answering track for a number of years.

102
00:08:50,000 --> 00:08:53,000
It was, I was doing a number of different things.

103
00:08:53,000 --> 00:08:57,000
It was maybe the team was about seven or so people dedicated to that.

104
00:08:57,000 --> 00:09:01,000
And I think we have to take this on.

105
00:09:01,000 --> 00:09:06,000
And if we had a chance of making the appropriate investment, we should do it.

106
00:09:06,000 --> 00:09:10,000
And even if it looks hard, we have to understand why it's hard.

107
00:09:10,000 --> 00:09:14,000
I mean, even if I fail, I'll be able to kind of tell the community, hey, look,

108
00:09:14,000 --> 00:09:17,000
we made a concerted effort at this. Here's how we did it.

109
00:09:17,000 --> 00:09:20,000
Here's what was hard about it. Here's what worked. Here's what didn't work.

110
00:09:20,000 --> 00:09:23,000
So it was a great opportunity to do real research.

111
00:09:23,000 --> 00:09:27,000
And to get that, you know, funded, you know, by IBM at the right levels,

112
00:09:27,000 --> 00:09:31,000
because they were excited about, you know, getting the the jeopardy challenge

113
00:09:31,000 --> 00:09:38,000
to think on television. And so I did, I did a feasibility study.

114
00:09:38,000 --> 00:09:41,000
We did a little bit. We played around some ideas.

115
00:09:41,000 --> 00:09:44,000
And I proposed it that we can do it.

116
00:09:44,000 --> 00:09:48,000
And they bought in and said, okay, you know, you're the guy.

117
00:09:48,000 --> 00:09:52,000
We're going to invest in this. And you're going to take on this challenge.

118
00:09:52,000 --> 00:09:56,000
And so I built up the team over the, that was the end of 2006.

119
00:09:56,000 --> 00:10:04,000
So 2007, 2008, 2009, 2010, completely focused on that building up,

120
00:10:04,000 --> 00:10:07,000
building up and rounding out the team.

121
00:10:07,000 --> 00:10:11,000
And basically going from scratch and building, building Watson,

122
00:10:11,000 --> 00:10:17,000
that ultimately won on jeopardy. And I guess we played it in 2011.

123
00:10:17,000 --> 00:10:23,000
So that's kind of the story from, from, from my passion all the way through,

124
00:10:23,000 --> 00:10:26,000
right through, right through the Watson stuff.

125
00:10:26,000 --> 00:10:33,000
And after Watson, did you jump right into elemental cognition or?

126
00:10:33,000 --> 00:10:35,000
So what's stopping points along the way?

127
00:10:35,000 --> 00:10:39,000
Yeah, so Watson was interesting because it was a huge,

128
00:10:39,000 --> 00:10:41,000
it was a tremendous success for us.

129
00:10:41,000 --> 00:10:46,000
It was a tremendous, the technical team was just so,

130
00:10:46,000 --> 00:10:49,000
had worked so hard and was so proud.

131
00:10:49,000 --> 00:10:54,000
We went into the final contest with about a 70, 75,

132
00:10:54,000 --> 00:10:59,000
between 70 and 75, there was 73% chance of winning based on all the stats that we did,

133
00:10:59,000 --> 00:11:02,000
all the simulations that we did.

134
00:11:02,000 --> 00:11:04,000
We hadn't worked our way up from when we started.

135
00:11:04,000 --> 00:11:07,000
There was about, you know, we were getting,

136
00:11:07,000 --> 00:11:09,000
we had zero percent chance of winning.

137
00:11:09,000 --> 00:11:13,000
You know, the, the system out of the box when we started was getting like 13%.

138
00:11:13,000 --> 00:11:14,000
All right.

139
00:11:14,000 --> 00:11:17,000
So it was a huge accomplishment.

140
00:11:17,000 --> 00:11:20,000
And it was in the middle of both sort of a combination of science,

141
00:11:20,000 --> 00:11:24,000
obviously, and we had the scientific results,

142
00:11:24,000 --> 00:11:27,000
business, you know, the business and the marketing at IBM.

143
00:11:27,000 --> 00:11:29,000
It was a big project for them and entertainment,

144
00:11:29,000 --> 00:11:33,000
because we had to work with the jeopardy of production folks and the television folks

145
00:11:33,000 --> 00:11:35,000
and pull the salt together.

146
00:11:35,000 --> 00:11:40,000
So having accomplished that, no one knew where it was going.

147
00:11:40,000 --> 00:11:42,000
It was such a focus on those things,

148
00:11:42,000 --> 00:11:44,000
and then no one knew where it was going.

149
00:11:44,000 --> 00:11:48,000
And interestingly, and I think this relates to kind of the perceptions

150
00:11:48,000 --> 00:11:50,000
of artificial intelligence.

151
00:11:50,000 --> 00:11:54,000
I think interestingly, and if you've saw, if you watched the games,

152
00:11:54,000 --> 00:11:57,000
but we presented this thing we called the answer panel,

153
00:11:57,000 --> 00:12:00,000
where it showed the top three answers and the chances we thought,

154
00:12:00,000 --> 00:12:03,000
the probability of that, those answers being right.

155
00:12:03,000 --> 00:12:07,000
And I think that the, putting that up on the television screen,

156
00:12:07,000 --> 00:12:13,000
got people to imagine that the machine was doing more than looking this stuff

157
00:12:13,000 --> 00:12:16,000
at a table, which of course it was.

158
00:12:16,000 --> 00:12:20,000
But I think when people just watch a computer give an answer,

159
00:12:20,000 --> 00:12:23,000
you know, the general thing is, well, computers just know everything.

160
00:12:23,000 --> 00:12:25,000
They just look it up and they know everything.

161
00:12:25,000 --> 00:12:27,000
But when they see this notion of wait a second,

162
00:12:27,000 --> 00:12:31,000
I must be doing more than that, because it's coming with a probability.

163
00:12:31,000 --> 00:12:34,000
So it must be kind of calculating why an answer might be right

164
00:12:34,000 --> 00:12:36,000
or why an answer might be wrong.

165
00:12:36,000 --> 00:12:40,000
And in fact, we were doing something certainly along those lines,

166
00:12:40,000 --> 00:12:45,000
including a confidence model for that based on hundreds of features

167
00:12:45,000 --> 00:12:49,000
and our underlying machine learning algorithms.

168
00:12:49,000 --> 00:12:53,000
And so this really captured the imagination, I think,

169
00:12:53,000 --> 00:12:55,000
of the consumer, of the customer.

170
00:12:55,000 --> 00:12:59,000
And before you knew it, IBM had a lot of people coming and saying,

171
00:12:59,000 --> 00:13:01,000
hey, we want this thing.

172
00:13:01,000 --> 00:13:03,000
Like, how do we get this thing?

173
00:13:03,000 --> 00:13:08,000
And at that point, the project took on a very different,

174
00:13:08,000 --> 00:13:12,000
you know, tone, very different nature was all about,

175
00:13:12,000 --> 00:13:14,000
how do we commercialize this?

176
00:13:14,000 --> 00:13:18,000
How do we bring AI and this technology to customers?

177
00:13:18,000 --> 00:13:22,000
And I personally wanted to kind of take a step back.

178
00:13:22,000 --> 00:13:27,000
And looking at how Watson worked and what my dreams were

179
00:13:27,000 --> 00:13:29,000
for artificial intelligence in general.

180
00:13:29,000 --> 00:13:31,000
So, you know, we're not really there yet.

181
00:13:31,000 --> 00:13:34,000
Well, there's a lot of technology here that could be exploited

182
00:13:34,000 --> 00:13:37,000
and leveraged in a variety of different applications.

183
00:13:37,000 --> 00:13:41,000
We're not at this point where the machine really understands language.

184
00:13:41,000 --> 00:13:44,000
And that's where I wanted to be.

185
00:13:44,000 --> 00:13:48,000
I wanted to be where I can fluently converse with the computer

186
00:13:48,000 --> 00:13:50,000
just like you can on Star Trek.

187
00:13:50,000 --> 00:13:53,000
And I can get it to understand what I'm saying.

188
00:13:53,000 --> 00:13:57,000
I can get it to deliver and summarize and read stuff,

189
00:13:57,000 --> 00:13:59,000
summarize information for me.

190
00:13:59,000 --> 00:14:02,000
I can get it to help me problem solve, you know,

191
00:14:02,000 --> 00:14:05,000
become like a thought partner through the fluency of language.

192
00:14:05,000 --> 00:14:07,000
Think the way I do only better.

193
00:14:07,000 --> 00:14:09,000
But in a way that I can understand it,

194
00:14:09,000 --> 00:14:11,000
it can be explained to me.

195
00:14:11,000 --> 00:14:13,000
I can tell it what I know.

196
00:14:13,000 --> 00:14:15,000
It can tell me what it knows.

197
00:14:15,000 --> 00:14:17,000
And this was the dream.

198
00:14:17,000 --> 00:14:19,000
And we were far from that.

199
00:14:19,000 --> 00:14:23,000
And I really wanted to step back and do that kind of thing.

200
00:14:23,000 --> 00:14:27,000
And IBM was on a very different road at that point

201
00:14:27,000 --> 00:14:31,000
because of the consumer interest, the customer interest.

202
00:14:31,000 --> 00:14:35,000
To be precise, not necessarily the consumer, but business interest

203
00:14:35,000 --> 00:14:37,000
and stuff like that.

204
00:14:37,000 --> 00:14:39,000
So I really wanted to take that step back.

205
00:14:39,000 --> 00:14:41,000
I got a chance.

206
00:14:41,000 --> 00:14:45,000
I was, you know, for a variety of reasons won't go into it.

207
00:14:45,000 --> 00:14:49,000
I, you know, some of my choices at the time were for limited.

208
00:14:49,000 --> 00:14:53,000
And I got really interested in this company not far from where I led,

209
00:14:53,000 --> 00:14:55,000
called Bridgewater.

210
00:14:55,000 --> 00:14:57,000
And I started working for Bridgewater.

211
00:14:57,000 --> 00:14:59,000
And part of the reason I was interested in Bridgewater

212
00:14:59,000 --> 00:15:01,000
also relates to my philosophy around AI,

213
00:15:01,000 --> 00:15:05,000
which is that Bridgewater is a hedge fund that was approaching markets

214
00:15:05,000 --> 00:15:07,000
in what they call the fundamental and systematic way.

215
00:15:07,000 --> 00:15:11,000
Meaning that any kind of prediction they were going to make,

216
00:15:11,000 --> 00:15:15,000
they were going to make sure they had an explicable model for that prediction.

217
00:15:15,000 --> 00:15:17,000
So this was the kind of AI I was interested in.

218
00:15:17,000 --> 00:15:19,000
Of course, you know, if you're in the market,

219
00:15:19,000 --> 00:15:21,000
you're a washing data.

220
00:15:21,000 --> 00:15:23,000
So it is about data and it's about data science.

221
00:15:23,000 --> 00:15:27,000
But it's not about sort of blind past predicts the future.

222
00:15:27,000 --> 00:15:30,000
It's really about building theoretical models and being able to explain yourself.

223
00:15:30,000 --> 00:15:33,000
So that was very aligned with my interest of completely different domain.

224
00:15:33,000 --> 00:15:37,000
But nonetheless, not expressly about language,

225
00:15:37,000 --> 00:15:39,000
but the same sort of approach toward AI.

226
00:15:39,000 --> 00:15:44,000
And I got involved with them and did some work for them,

227
00:15:44,000 --> 00:15:46,000
still doing some work for them.

228
00:15:46,000 --> 00:15:47,000
But during the course of that,

229
00:15:47,000 --> 00:15:52,000
got them really interested in sort of my vision for language understanding.

230
00:15:52,000 --> 00:15:56,000
And that ultimately the future of AI has to land in a place.

231
00:15:56,000 --> 00:16:00,000
And this is where we both really agreed that the future of AI has to land in a place

232
00:16:00,000 --> 00:16:03,000
that we build machines that are understandable, they're explicable.

233
00:16:03,000 --> 00:16:09,000
Their logic can be probed and can be challenged and can be explained.

234
00:16:09,000 --> 00:16:17,000
So I got, they gave me the opportunity to start my own company called Elemental Cognition.

235
00:16:17,000 --> 00:16:23,000
And that was in 2015 that I started that I started that.

236
00:16:23,000 --> 00:16:28,000
And that's been our mission is to focus on language understanding

237
00:16:28,000 --> 00:16:37,000
with the, to build a learning machine that can learn in a way that ultimately interprets language,

238
00:16:37,000 --> 00:16:42,000
builds on causal models based on its interpretation,

239
00:16:42,000 --> 00:16:47,000
can speak to people, acquire knowledge, reason about that knowledge,

240
00:16:47,000 --> 00:16:49,000
answer questions and provide explanations.

241
00:16:49,000 --> 00:16:53,000
I think this is the Holy Grail for language AI.

242
00:16:53,000 --> 00:16:57,000
And that's the mission for Elemental Cognition.

243
00:16:57,000 --> 00:17:02,000
Awesome. I hadn't realized the Bridgewater connection,

244
00:17:02,000 --> 00:17:10,000
Bridgewater for being perhaps popularized more recently with Ray Dalio's book.

245
00:17:10,000 --> 00:17:15,000
Yes, Ray wrote the book on principles

246
00:17:15,000 --> 00:17:20,000
and is popularizing that.

247
00:17:20,000 --> 00:17:24,000
Of course, it's the biggest hedge fund in the world.

248
00:17:24,000 --> 00:17:29,000
And it's sort of known for this approach toward markets,

249
00:17:29,000 --> 00:17:32,000
which is again fundamental and systematic where they build,

250
00:17:32,000 --> 00:17:36,000
they of course leverage the computer, but they build explicable systems.

251
00:17:36,000 --> 00:17:39,000
And that's kind of the link between Bridgewater and me

252
00:17:39,000 --> 00:17:42,000
is we have a very similar philosophy about AI.

253
00:17:42,000 --> 00:17:47,000
And one of the things that occurred to me as you were talking about the Watson experience

254
00:17:47,000 --> 00:17:52,000
and the way that you presented the results via this panel

255
00:17:52,000 --> 00:17:58,000
was that it was in some ways kind of an early view at AI explainability, right?

256
00:17:58,000 --> 00:18:01,000
We're not just going to show this one result and say it's the answer.

257
00:18:01,000 --> 00:18:05,000
We're going to show these results and explicitly acknowledge that, you know,

258
00:18:05,000 --> 00:18:07,000
there are probabilities involved.

259
00:18:07,000 --> 00:18:12,000
That's right, and there's a great story about that.

260
00:18:12,000 --> 00:18:15,000
I thought that was so important to do,

261
00:18:15,000 --> 00:18:18,000
because we had, you know, we had taped many games,

262
00:18:18,000 --> 00:18:25,000
we made practice games, and we would tape them and show them to different, you know, audiences.

263
00:18:25,000 --> 00:18:29,000
And at the time, I think my younger story was seven or eight.

264
00:18:29,000 --> 00:18:36,000
And we showed her one of these tapes of Watson playing against former Jeffrey players.

265
00:18:36,000 --> 00:18:41,000
And Watson, you know, didn't know an answer and decided not to answer.

266
00:18:41,000 --> 00:18:46,000
We didn't have that answer panel up that showed its top three choices.

267
00:18:46,000 --> 00:18:52,000
And when the computer didn't answer, she turned around and said, you know, did a crash.

268
00:18:52,000 --> 00:18:56,000
And I thought that was fascinating that, you know, at that young age, you know,

269
00:18:56,000 --> 00:19:02,000
she already had this notion that a computer should answer if it didn't answer, it's down.

270
00:19:02,000 --> 00:19:11,000
And I thought, no, I said, you know, Watson decided not to answer because it wasn't sure.

271
00:19:11,000 --> 00:19:13,000
And that's so completely different.

272
00:19:13,000 --> 00:19:16,000
And so, so then I just became convinced.

273
00:19:16,000 --> 00:19:18,000
And I, of course, was watching the game too.

274
00:19:18,000 --> 00:19:23,000
And I thought, you know, this is just not nearly as interesting and as exciting.

275
00:19:23,000 --> 00:19:27,000
Because, you know, when you're watching a human, you think the humans like you,

276
00:19:27,000 --> 00:19:30,000
when you're projecting what you would be doing answering a question,

277
00:19:30,000 --> 00:19:33,000
you'd be like wrestling with whether you do the answer or not,

278
00:19:33,000 --> 00:19:35,000
and whether or not you wanted to buzz in.

279
00:19:35,000 --> 00:19:40,000
But, you know, you don't have that same model for what a computer does, interestingly.

280
00:19:40,000 --> 00:19:45,000
And so, it was so obvious just not as interesting.

281
00:19:45,000 --> 00:19:49,000
So, I said, we really have to get that answer panel up on the screen.

282
00:19:49,000 --> 00:19:53,000
And at first, Jeffrey didn't want to do it.

283
00:19:53,000 --> 00:19:59,000
And eventually, long story short, we convinced Alex Trebek by showing him

284
00:19:59,000 --> 00:20:02,000
a couple of games with their answer panel up.

285
00:20:02,000 --> 00:20:04,000
And then, and he got so fascinated.

286
00:20:04,000 --> 00:20:09,000
But he said, you know, this is really jeopardy because your, your attention is distracted.

287
00:20:09,000 --> 00:20:11,000
That answer panel is so fascinating.

288
00:20:11,000 --> 00:20:16,000
It completely takes you away from the normal experience of what a jeopardy game is about.

289
00:20:16,000 --> 00:20:20,000
And so, we, um, so he then showed him a few, uh,

290
00:20:20,000 --> 00:20:23,000
a show him a few games with without the answer panel.

291
00:20:23,000 --> 00:20:25,000
And he was like, wait a second.

292
00:20:25,000 --> 00:20:31,000
Like this is more. Right. So he sort of, and it was so clearly the worst.

293
00:20:31,000 --> 00:20:34,000
And he was like, we got to get that up there.

294
00:20:34,000 --> 00:20:37,000
And I think that really changed the perception of AI.

295
00:20:37,000 --> 00:20:39,000
And you're absolutely right.

296
00:20:39,000 --> 00:20:41,000
It is the beginnings of why.

297
00:20:41,000 --> 00:20:45,000
And it's not enough because I think when machines are, are, are not doing

298
00:20:45,000 --> 00:20:50,000
factoid question answering, but they're making predictions, um, whether they're in,

299
00:20:50,000 --> 00:20:57,000
uh, law, um, you know, um, policymaking or law or, or, or, um, you know,

300
00:20:57,000 --> 00:21:02,000
health care or, or even finance anywhere where, you know, wow,

301
00:21:02,000 --> 00:21:05,000
this is going to affect your life in a serious way.

302
00:21:05,000 --> 00:21:11,000
Um, you, it's really a, a, um, it's a collaboration.

303
00:21:11,000 --> 00:21:15,000
I mean, I think the best way, it's not just an answer.

304
00:21:15,000 --> 00:21:17,000
It's really a back and forth collaboration.

305
00:21:17,000 --> 00:21:20,000
It's a dialogue that you have to go through.

306
00:21:20,000 --> 00:21:25,000
Because as you hear the answers, you want to know, why does that answer make sense?

307
00:21:25,000 --> 00:21:29,000
Am I missing something? Um, do I, now that I hear you tell me that answer?

308
00:21:29,000 --> 00:21:35,000
Maybe there are risks that I just didn't imagine or values that I weren't

309
00:21:35,000 --> 00:21:37,000
considering or weighing properly.

310
00:21:37,000 --> 00:21:42,000
Now that I see the answer and the reasons why that answer may be right or wrong.

311
00:21:42,000 --> 00:21:47,000
Um, so it, and, and, and you, and you can experience that just through the thought

312
00:21:47,000 --> 00:21:48,000
experiment.

313
00:21:48,000 --> 00:21:53,000
Just think of how you, um, interact with any human expert, whether you're consulting

314
00:21:53,000 --> 00:21:56,000
with the, um, with you're consulting with the lawyer or you're working with the

315
00:21:56,000 --> 00:21:58,000
teachers trying to help you understand something.

316
00:21:58,000 --> 00:21:59,000
Yeah.

317
00:21:59,000 --> 00:22:03,000
Um, I was going to say that sounds like my conversations with my wife about what we're

318
00:22:03,000 --> 00:22:04,000
going to have for dinner.

319
00:22:04,000 --> 00:22:06,000
There's that much back and forth.

320
00:22:06,000 --> 00:22:09,000
There you go. Right.

321
00:22:09,000 --> 00:22:12,000
Even when it's not that important, right?

322
00:22:12,000 --> 00:22:17,000
You know, you think you have the answer, but, um, or, you know, our doctor or a

323
00:22:17,000 --> 00:22:20,000
lawyer, you know, it really is, you know, you think it's simple, but it's not so

324
00:22:20,000 --> 00:22:23,000
simple when it's, when it's important enough.

325
00:22:23,000 --> 00:22:28,000
And I think at that point, you're really expecting the expert to start to bring

326
00:22:28,000 --> 00:22:33,000
you into the decision making process to give you the explanations that you need

327
00:22:33,000 --> 00:22:36,000
to kind of have that back and forth.

328
00:22:36,000 --> 00:22:40,000
Can you imagine, you know, making an important decision and, and asking an

329
00:22:40,000 --> 00:22:45,000
advisor and the advisor says, well, I think you should take this treatment or you,

330
00:22:45,000 --> 00:22:47,000
or you should invest your money here, whatever it is.

331
00:22:47,000 --> 00:22:49,000
And then you said, well, why do you think that?

332
00:22:49,000 --> 00:22:51,000
And the person just says, well, trust me.

333
00:22:51,000 --> 00:22:52,000
Right.

334
00:22:52,000 --> 00:22:53,000
It's my intuition.

335
00:22:53,000 --> 00:23:00,000
Well, this idea of it, uh, being not so simple is, is one that, uh,

336
00:23:00,000 --> 00:23:04,000
reminds me of something you said when we were speaking before, uh, the

337
00:23:04,000 --> 00:23:06,000
interview started.

338
00:23:06,000 --> 00:23:10,000
And that is that there's this big question for you around AI.

339
00:23:10,000 --> 00:23:15,000
And that is, are we really being honest about how difficult the problem is?

340
00:23:15,000 --> 00:23:18,000
Can you elaborate a little bit on that and what that, you know, what that statement,

341
00:23:18,000 --> 00:23:20,000
what that question means for you?

342
00:23:20,000 --> 00:23:21,000
Yeah.

343
00:23:21,000 --> 00:23:26,000
So, um, I think there's, I think what's going on in, you know, in the industry

344
00:23:26,000 --> 00:23:32,000
right now is we have a set of techniques that are good at a particular approach.

345
00:23:32,000 --> 00:23:37,000
Um, so we have this deep learning stuff, which by the way, I, I love,

346
00:23:37,000 --> 00:23:43,000
and I use any chance I get, but I'm also honest about what it's capable of and

347
00:23:43,000 --> 00:23:45,000
not capable of so far.

348
00:23:45,000 --> 00:23:50,000
And we have this technique and we build data sets and challenge problems that

349
00:23:50,000 --> 00:23:55,000
are, that are, you know, subject to that tech subject in one form or another

350
00:23:55,000 --> 00:23:59,000
are susceptible, vulnerable to that technique working.

351
00:23:59,000 --> 00:24:02,000
And, and I don't think they're ambitious enough.

352
00:24:02,000 --> 00:24:06,000
I think that understanding is actually a very hard problem.

353
00:24:06,000 --> 00:24:11,000
And I think if we step back and really think about how hard it is,

354
00:24:11,000 --> 00:24:14,000
independently of how a computer would work, and we say,

355
00:24:14,000 --> 00:24:17,000
what does it mean to understand stuff?

356
00:24:17,000 --> 00:24:21,000
To understand language, we just have to think about how much,

357
00:24:21,000 --> 00:24:26,000
how much energy and thought humans put into understanding each other.

358
00:24:26,000 --> 00:24:29,000
This is not a simple thing.

359
00:24:29,000 --> 00:24:31,000
We, we're, we're sort of language of machines.

360
00:24:31,000 --> 00:24:36,000
We're great at it and terrible at it at the same time.

361
00:24:36,000 --> 00:24:37,000
Right? We're great.

362
00:24:37,000 --> 00:24:40,000
We're, we're great at generating all kinds of language.

363
00:24:40,000 --> 00:24:41,000
We can write prolifically.

364
00:24:41,000 --> 00:24:42,000
We can talk prolifically.

365
00:24:42,000 --> 00:24:46,000
We can fill up, you know, news 24 or seven new shows.

366
00:24:46,000 --> 00:24:49,000
Um, do we reach and understand it?

367
00:24:49,000 --> 00:24:51,000
Like if you sit down, everybody's sort of nods,

368
00:24:51,000 --> 00:24:53,000
but what's your level of understanding?

369
00:24:53,000 --> 00:24:59,000
Even reading, um, whether, you know, whether it's an article or a book or whatever,

370
00:24:59,000 --> 00:25:03,000
people can debate endlessly for what's really meant,

371
00:25:03,000 --> 00:25:05,000
what you're really getting out of it.

372
00:25:05,000 --> 00:25:08,000
What, what, what is, what, what, what information is actually communicating

373
00:25:08,000 --> 00:25:10,000
and how precisely and how confidently,

374
00:25:10,000 --> 00:25:14,000
and why are we having different opinions about what's going on?

375
00:25:14,000 --> 00:25:18,000
In science, of course, we invented formal languages for this kind of stuff.

376
00:25:18,000 --> 00:25:20,000
Um, so in engineering, we don't rely on that.

377
00:25:20,000 --> 00:25:26,000
We rely on engineering specification diagrams and formal semantics and things like that.

378
00:25:26,000 --> 00:25:29,000
Um, and even there, uh, there are challenges in mathematics.

379
00:25:29,000 --> 00:25:31,000
We do mathematical proofs.

380
00:25:31,000 --> 00:25:35,000
And yet humans don't want to communicate that way clearly,

381
00:25:35,000 --> 00:25:37,000
but there has to be some in between.

382
00:25:37,000 --> 00:25:41,000
There has to be a recognition that understanding is actually pretty hard.

383
00:25:41,000 --> 00:25:44,000
Uh, humans invest an enormous amount of energy.

384
00:25:44,000 --> 00:25:50,000
Whether it be in teaching or journalism or, uh, writing or film,

385
00:25:50,000 --> 00:25:54,000
or that's an enormous amount of energy communicating and trying to understand each other.

386
00:25:54,000 --> 00:25:56,000
We struggle mildly with it.

387
00:25:56,000 --> 00:26:00,000
So, so do we expect, you know, computers to take a large corpora,

388
00:26:00,000 --> 00:26:03,000
digest them statistically and come out and do this?

389
00:26:03,000 --> 00:26:04,000
Right.

390
00:26:04,000 --> 00:26:06,000
There's just a lot more going on here.

391
00:26:06,000 --> 00:26:11,000
And I think we have to, we have to kind of open our eyes to what else is going on.

392
00:26:11,000 --> 00:26:14,000
Um, for you and I to get comfortable understanding each other,

393
00:26:14,000 --> 00:26:20,000
we probably have to spend a lot of time sort of synchronizing what our background knowledge is.

394
00:26:20,000 --> 00:26:24,000
Um, what are, what, how we, how we communicate about different things,

395
00:26:24,000 --> 00:26:27,000
how we use particular words, phrases, metaphors.

396
00:26:27,000 --> 00:26:30,000
If I know you're an expert at something, um,

397
00:26:30,000 --> 00:26:34,000
and if I know that too, I can use that as a foundation for doing metaphors.

398
00:26:34,000 --> 00:26:37,000
Um, so there's just a lot to do.

399
00:26:37,000 --> 00:26:41,000
And I think when we imagine elemental cognition,

400
00:26:41,000 --> 00:26:45,000
we imagine the computer kind of engaging with the human continuously.

401
00:26:45,000 --> 00:26:50,000
You know, it was becoming this thought partner that evolves within a community of humans,

402
00:26:50,000 --> 00:26:56,000
talking about a thing and reading about a thing and learning how to align its internal models

403
00:26:56,000 --> 00:27:01,000
and, and how to acquire the right background knowledge to speak and build understanding.

404
00:27:01,000 --> 00:27:07,000
And we start like with the kids, you know, the kid in first grade,

405
00:27:07,000 --> 00:27:11,000
reading to try to understand stuff and imagine collaborating with the teacher.

406
00:27:11,000 --> 00:27:14,000
Look, I don't know what this means. I don't know what that means.

407
00:27:14,000 --> 00:27:18,000
What, um, you know, why would, you know, why would you do that?

408
00:27:18,000 --> 00:27:22,000
And if you're talking about learning about plants and how they grow with light and water

409
00:27:22,000 --> 00:27:27,000
as the simplest sort of stuff, um, what do you have to know to understand that

410
00:27:27,000 --> 00:27:33,000
and put that in the right framework in your head so you can make me a useful predictions about what you've learned?

411
00:27:33,000 --> 00:27:36,000
It's a complex process that involves learning the language,

412
00:27:36,000 --> 00:27:40,000
learning how to map it on to models, learning how to reason over those models,

413
00:27:40,000 --> 00:27:43,000
learning what background knowledge applies and what doesn't apply,

414
00:27:43,000 --> 00:27:47,000
learning what the metaphors, the analogies, the phrases, the words,

415
00:27:47,000 --> 00:27:50,000
the word sense is mean in that context.

416
00:27:50,000 --> 00:27:54,000
And it always involves the kind of the going, the back and forth,

417
00:27:54,000 --> 00:27:58,000
doing the back and forth to kind of get your bearings in your context.

418
00:27:58,000 --> 00:28:01,000
So it's just a challenging, it's just a challenging problem.

419
00:28:01,000 --> 00:28:08,000
So one of the points you made is that we're not setting our aspirations high enough.

420
00:28:08,000 --> 00:28:12,000
And earlier in this conversation, you talked about the AI winter.

421
00:28:12,000 --> 00:28:17,000
Like, is there a relationship between, you know, where we set our aspirations

422
00:28:17,000 --> 00:28:22,000
and the kind of expectations that were set last time with the AI winter

423
00:28:22,000 --> 00:28:25,000
and what we need to do to manage them?

424
00:28:25,000 --> 00:28:30,000
Like, or rather, is there a risk in setting our aspirations too broadly

425
00:28:30,000 --> 00:28:35,000
that we're painting a picture beyond what the technology is capable of

426
00:28:35,000 --> 00:28:41,000
and we kind of set ourselves up for another deflation of those expectations?

427
00:28:41,000 --> 00:28:43,000
Yeah, so it's a good question.

428
00:28:43,000 --> 00:28:47,000
I mean, my perspective on that is a little bit too fold.

429
00:28:47,000 --> 00:28:50,000
I mean, I think we're not in the danger that we once were

430
00:28:50,000 --> 00:28:55,000
in terms of suffering another sort of full-blown AI winter.

431
00:28:55,000 --> 00:29:01,000
And the reason is because I think that there's so many,

432
00:29:01,000 --> 00:29:10,000
there's so much low-hanging fruit for deep learning to continue to provide value.

433
00:29:10,000 --> 00:29:16,000
And I think what deep learning has done for voice recognition,

434
00:29:16,000 --> 00:29:21,000
image recognition, how is it actually helped advance

435
00:29:21,000 --> 00:29:26,000
at least superficial and at least superficial NLP?

436
00:29:26,000 --> 00:29:32,000
I think these are for control systems as well.

437
00:29:32,000 --> 00:29:38,000
I think that this is enormous and will continue to deliver for some time.

438
00:29:38,000 --> 00:29:42,000
And I think we'll continue to deliver value and transformation

439
00:29:42,000 --> 00:29:47,000
actually for some time. The other prong of my answer though is I think

440
00:29:47,000 --> 00:29:52,000
that in some circles there are these expectations for this understanding thing.

441
00:29:52,000 --> 00:29:54,000
I think that's a harder problem.

442
00:29:54,000 --> 00:29:57,000
And I don't think we're yet making the right advancement.

443
00:29:57,000 --> 00:30:00,000
I think that problem is, I think that problem is tractable.

444
00:30:00,000 --> 00:30:04,000
But I don't think yet we're making the right investments.

445
00:30:04,000 --> 00:30:09,000
I mean, the elemental cognition, one of my goals is to demonstrate what I think

446
00:30:09,000 --> 00:30:13,000
the right type of investment is to tackle that problem.

447
00:30:13,000 --> 00:30:15,000
But I think we need more of that type of investment.

448
00:30:15,000 --> 00:30:20,000
And hopefully I can convince people that that's the case as we progress to get to that.

449
00:30:20,000 --> 00:30:23,000
But I think that's a longer road.

450
00:30:23,000 --> 00:30:30,000
I think that the other promising thing where these two paths come together

451
00:30:30,000 --> 00:30:36,000
is that as the AI continues to be impactful and transformative,

452
00:30:36,000 --> 00:30:40,000
it's engaging humans more and more.

453
00:30:40,000 --> 00:30:45,000
So what that means is that you're using, you know, you have your phone,

454
00:30:45,000 --> 00:30:49,000
your computer, you have all these different ways in which.

455
00:30:49,000 --> 00:30:56,000
You're interacting with applications today that engage humans in various forms of interactions.

456
00:30:56,000 --> 00:31:01,000
I think that's essential for the vision that if you ultimately want machines to understand

457
00:31:01,000 --> 00:31:05,000
you're going to need a deep levels of engagement from humans.

458
00:31:05,000 --> 00:31:07,000
And I think that's happening.

459
00:31:07,000 --> 00:31:09,000
That's transforming that's happening.

460
00:31:09,000 --> 00:31:10,000
We're seeing that.

461
00:31:10,000 --> 00:31:12,000
So you have huge opportunity to engage them.

462
00:31:12,000 --> 00:31:14,000
The chicken and the egg problem is that you kind of have to,

463
00:31:14,000 --> 00:31:20,000
you have to engage them well enough that they want to have the back and forth with you.

464
00:31:20,000 --> 00:31:25,000
I'm thinking of IVR systems and immediately pressing the zero button.

465
00:31:25,000 --> 00:31:27,000
Exactly.

466
00:31:27,000 --> 00:31:32,000
Or, you know, giving up on your favorite chatbot, you know,

467
00:31:32,000 --> 00:31:36,000
for anything, anything that's actually complicated or involved,

468
00:31:36,000 --> 00:31:39,000
and you just kind of give up.

469
00:31:39,000 --> 00:31:44,000
Or, let me talk to a human right away and you keep hitting the pound sign until you get a human, whatever it is.

470
00:31:44,000 --> 00:31:49,000
I mean, I think that, you know, so there's a little bit of a chicken and egg where they have to be good enough

471
00:31:49,000 --> 00:31:51,000
to engage you so they can learn from you.

472
00:31:51,000 --> 00:31:55,000
And so that's kind of like what you need that initial investment.

473
00:31:55,000 --> 00:31:59,000
So I'm optimistic that we're not going to suffer an AI winter.

474
00:31:59,000 --> 00:32:03,000
I worry a little bit of people go out there and expect these things to be extraordinary,

475
00:32:03,000 --> 00:32:06,000
at least from the understanding of the site right away.

476
00:32:06,000 --> 00:32:11,000
We have to kind of have a little bit of a cooperation, a collaboration,

477
00:32:11,000 --> 00:32:13,000
and you have to inspire that.

478
00:32:13,000 --> 00:32:20,000
So it sounds like you, you know, while you're respectful of deep learning and what it offers,

479
00:32:20,000 --> 00:32:24,000
you don't think it's the entire solution.

480
00:32:24,000 --> 00:32:28,000
How do you articulate what you think that solution looks like?

481
00:32:28,000 --> 00:32:34,000
So that's also an interesting question because I think the answer is a little bit more nuanced

482
00:32:34,000 --> 00:32:39,000
and my philosophy about that is a little bit more nuanced.

483
00:32:39,000 --> 00:32:50,000
So I'm sort of, with regard to the power of deep learning as a general approach to intelligence,

484
00:32:50,000 --> 00:32:58,000
I'm a little bit, I'm probably lean toward, you know, agnostic may be positive,

485
00:32:58,000 --> 00:33:03,000
a little bit passive as a believer that maybe it's enough to achieve general intelligence

486
00:33:03,000 --> 00:33:05,000
in some theoretical way.

487
00:33:05,000 --> 00:33:10,000
Meaning there exists a neural network of some arbitrary depth and breadth

488
00:33:10,000 --> 00:33:12,000
that can get us to AGI?

489
00:33:12,000 --> 00:33:13,000
Yes.

490
00:33:13,000 --> 00:33:16,000
Given some unspecified data and training method.

491
00:33:16,000 --> 00:33:21,000
Given some unspecified features, your features, right?

492
00:33:21,000 --> 00:33:22,000
Exactly.

493
00:33:22,000 --> 00:33:27,000
So, probably right, exactly, you got it, you got it.

494
00:33:27,000 --> 00:33:32,000
And just to put a fine point on that, you know, the brain is generating features.

495
00:33:32,000 --> 00:33:37,000
It's generating an enormous amount of internal features, particularly when it comes to

496
00:33:37,000 --> 00:33:42,000
socio-economic stuff, emotional stuff, things that relate a lot to how we understand

497
00:33:42,000 --> 00:33:46,000
and build internal models that we could then apply language to.

498
00:33:46,000 --> 00:33:48,000
It's generating a lot of its own stuff.

499
00:33:48,000 --> 00:33:52,000
It's not external data and some effect it's internal data.

500
00:33:52,000 --> 00:33:57,000
You can argue that with exposure to enough of exactly the same stimulus

501
00:33:57,000 --> 00:34:02,000
that, you know, some specified, you know, yet to be specified in the neural net

502
00:34:02,000 --> 00:34:04,000
would generate a lot of those internal features.

503
00:34:04,000 --> 00:34:06,000
I don't know, not necessarily.

504
00:34:06,000 --> 00:34:11,000
And that's the interesting thing about the technology is not necessarily.

505
00:34:11,000 --> 00:34:15,000
It may build a completely different conceptualization of the world around it

506
00:34:15,000 --> 00:34:20,000
in order to survive or do similar things that would be completely incompatible with yours.

507
00:34:20,000 --> 00:34:25,000
So, but anyway, like I said, I mean, sure, maybe, right?

508
00:34:25,000 --> 00:34:32,000
And now the other approach is, if I said my goal was to take all this content,

509
00:34:32,000 --> 00:34:38,000
all these symbols that is our language, and map it to representations

510
00:34:38,000 --> 00:34:42,000
that represent our understanding of that language,

511
00:34:42,000 --> 00:34:48,000
which is now something that more directly models the full internal representation

512
00:34:48,000 --> 00:34:51,000
that we may have or something that's isomorphic to it.

513
00:34:51,000 --> 00:34:54,000
Not necessarily the way we represented it in our brain,

514
00:34:54,000 --> 00:34:58,000
but represented in a way that we would end up with the same language for it.

515
00:34:58,000 --> 00:35:00,000
And I had enough of that data.

516
00:35:00,000 --> 00:35:05,000
And I trained in a deep learning system.

517
00:35:05,000 --> 00:35:12,000
Would it be able to now read and produce an understanding of a blind language?

518
00:35:12,000 --> 00:35:18,000
Maybe, maybe, but you need a hell of a lot of that kind of data.

519
00:35:18,000 --> 00:35:22,000
And we don't generate that type of data readily.

520
00:35:22,000 --> 00:35:29,000
We can't even agree often on what that common understanding of that thing is.

521
00:35:29,000 --> 00:35:32,000
As I said, this goes back to the back and forth.

522
00:35:32,000 --> 00:35:41,000
We sort of assemble and refine and align that understanding through our interaction and collaborations.

523
00:35:41,000 --> 00:35:44,000
So that's an interesting question.

524
00:35:44,000 --> 00:35:50,000
So where I end up with is, we need a hybrid system.

525
00:35:50,000 --> 00:35:53,000
And that hybrid system puts some stakes in the ground.

526
00:35:53,000 --> 00:35:57,000
It says, this is what I think an understanding is.

527
00:35:57,000 --> 00:36:02,000
And I demonstrate that that understanding is ambitious, but not in the lab.

528
00:36:02,000 --> 00:36:08,000
It's ambitious, but good and not so ambitious that it becomes impossible.

529
00:36:08,000 --> 00:36:16,000
For example, you can read a text and you can go on and on about all the depth of understanding

530
00:36:16,000 --> 00:36:21,000
and the layers and layers of meaning and the metaphorical implications and so forth and so on.

531
00:36:21,000 --> 00:36:27,000
Or you can get that text and you could say, I know all the agents.

532
00:36:27,000 --> 00:36:29,000
I know what they did. I know when they did it.

533
00:36:29,000 --> 00:36:33,000
I know the relative geospatial relationships.

534
00:36:33,000 --> 00:36:36,000
And I know how it lays out in time.

535
00:36:36,000 --> 00:36:40,000
And I can tell you what all the individual motivations were.

536
00:36:40,000 --> 00:36:43,000
And their incentives were to take the actions they took.

537
00:36:43,000 --> 00:36:46,000
And I can tell you what events cause what other events.

538
00:36:46,000 --> 00:36:48,000
That would be impressive.

539
00:36:48,000 --> 00:36:51,000
I would be very happy with that.

540
00:36:51,000 --> 00:36:55,000
If that was my fifth grader, I would say, good job.

541
00:36:55,000 --> 00:36:57,000
That's an impressive level of understanding.

542
00:36:57,000 --> 00:37:00,000
If I give you an arbitrary text and you can do that.

543
00:37:00,000 --> 00:37:06,000
But that's also, that's not everything, but it's damn impressive.

544
00:37:06,000 --> 00:37:10,000
So now to do that, what would I need to do?

545
00:37:10,000 --> 00:37:13,000
And the system would need to dialogue effectively.

546
00:37:13,000 --> 00:37:17,000
It would need to systematically be able to acquire knowledge.

547
00:37:17,000 --> 00:37:23,000
To do the full NLP stack and then some that we're also familiar with.

548
00:37:23,000 --> 00:37:28,000
It would need to be able to, if it's going to converse with you at all,

549
00:37:28,000 --> 00:37:33,000
it can't be completely absent of any background knowledge at all.

550
00:37:33,000 --> 00:37:39,000
So it needs to do kind of corpus analysis and knowledge graph building.

551
00:37:39,000 --> 00:37:42,000
It needs to be able to build an internal representation.

552
00:37:42,000 --> 00:37:47,000
And then reason over so it can make logical predictions and entailments.

553
00:37:47,000 --> 00:37:50,000
So you can imagine as I'm going through all this stuff,

554
00:37:50,000 --> 00:37:54,000
the system that we're building at EC has dialogue components,

555
00:37:54,000 --> 00:37:59,000
has deductive and inductive reasoning mechanisms.

556
00:37:59,000 --> 00:38:05,000
It does uses deep learning to do corpus analysis and knowledge graph building.

557
00:38:05,000 --> 00:38:08,000
It uses deep learning to do basic NLP.

558
00:38:08,000 --> 00:38:13,000
And of course, it has an architecture through which all these components are integrated.

559
00:38:13,000 --> 00:38:16,000
It's been, it's enormously challenging.

560
00:38:16,000 --> 00:38:21,000
We have a ways to go, but those are the ingredients that we're playing with in this system.

561
00:38:21,000 --> 00:38:23,000
How do you characterize?

562
00:38:23,000 --> 00:38:27,000
Can I wear you are with it relative to?

563
00:38:27,000 --> 00:38:31,000
Well, relative, relative to benchmarks that we need to define.

564
00:38:31,000 --> 00:38:35,000
The one one is like you, which is just outlined with regard to understanding.

565
00:38:35,000 --> 00:38:38,000
Well, that's exactly right. I mean, I think that look,

566
00:38:38,000 --> 00:38:45,000
we're about six months to a year away from I think defining a good,

567
00:38:45,000 --> 00:38:50,000
the way I like to phrase is a good, ambitious challenge problem.

568
00:38:50,000 --> 00:38:56,000
So in other words, one that has a data set, a clear metric.

569
00:38:56,000 --> 00:39:01,000
As I said, a very ambitious metric, a button on the less a clear metric.

570
00:39:01,000 --> 00:39:08,000
And in an evaluation process, that just sets the bar a lot higher than what we're seeing today.

571
00:39:08,000 --> 00:39:13,000
It's not limited by what deep learning stuff can do today.

572
00:39:13,000 --> 00:39:18,000
It sort of takes that barrier and says, hey, let's forget about what's possible and isn't possible today.

573
00:39:18,000 --> 00:39:22,000
What do we think on what we should be able to do?

574
00:39:22,000 --> 00:39:28,000
Would be a good definition of this understanding problem that is ambitious enough and challenging enough.

575
00:39:28,000 --> 00:39:33,000
But not impossible, or at least our perspective is not impossible.

576
00:39:33,000 --> 00:39:42,000
And also to demonstrate an approach that is viable against that challenge problem.

577
00:39:42,000 --> 00:39:45,000
And I'm super excited about that because I think this is what AI needs.

578
00:39:45,000 --> 00:39:49,000
I mean, if we really want to tackle understanding, we need that.

579
00:39:49,000 --> 00:39:54,000
When you think about this definition of understanding and the challenge problem,

580
00:39:54,000 --> 00:40:00,000
to what extent does it incorporate elements they get at nuance,

581
00:40:00,000 --> 00:40:04,000
like I'm thinking of things like Winagrad Schema challenge and things like that.

582
00:40:04,000 --> 00:40:08,000
Yeah, I mean, I think the Winagrad Schema challenge is interesting.

583
00:40:08,000 --> 00:40:13,000
And we should be able to tackle that type of stuff.

584
00:40:13,000 --> 00:40:18,000
So I think it absolutely has to get at the nuance that you suggest there.

585
00:40:18,000 --> 00:40:23,000
And again, because it is building an internal representation,

586
00:40:23,000 --> 00:40:27,000
it will get confused. It should know it's confused.

587
00:40:27,000 --> 00:40:31,000
It should be able to say, I don't understand this.

588
00:40:31,000 --> 00:40:35,000
I can't fit this into my prior models of how the world works.

589
00:40:35,000 --> 00:40:38,000
And here's how you can help me.

590
00:40:38,000 --> 00:40:41,000
I mean, that's how it should behave.

591
00:40:41,000 --> 00:40:47,000
And so you characterize this as a hybrid system.

592
00:40:47,000 --> 00:40:51,000
And I'm envisioning hybrid not just being a connection of two different things,

593
00:40:51,000 --> 00:40:54,000
but lots of different things.

594
00:40:54,000 --> 00:40:59,000
And it strikes me that a big part of how you glue all this stuff together

595
00:40:59,000 --> 00:41:06,000
is kind of formalizing the formal knowledge piece and representation.

596
00:41:06,000 --> 00:41:12,000
To what extent is that an important element of the overall functioning of a system like this?

597
00:41:12,000 --> 00:41:16,000
Yeah, so the formal representation is an important part.

598
00:41:16,000 --> 00:41:22,000
And so you could imagine, I mean, it's not, I don't think it's hard to imagine if you thought at all about,

599
00:41:22,000 --> 00:41:25,000
you know, architect these kinds of systems.

600
00:41:25,000 --> 00:41:27,000
You know, you have to, you know, language comes in.

601
00:41:27,000 --> 00:41:29,000
You have to go a little parsed.

602
00:41:29,000 --> 00:41:33,000
You have to be able to do an initial syntactic and semantic,

603
00:41:33,000 --> 00:41:36,000
at least shallow semantic interpretation.

604
00:41:36,000 --> 00:41:39,000
You're going to get all kinds of possibilities.

605
00:41:39,000 --> 00:41:41,000
So you're going to get different parses.

606
00:41:41,000 --> 00:41:44,000
You're going to get different word senses.

607
00:41:44,000 --> 00:41:50,000
You know, you're going to, you're going to get different semantic interpretations at different levels.

608
00:41:50,000 --> 00:41:52,000
And you have to start to make sense of it.

609
00:41:52,000 --> 00:41:56,000
And so when it gets to the making sense part,

610
00:41:56,000 --> 00:41:59,000
you have to be able to reason about it.

611
00:41:59,000 --> 00:42:05,000
So, you know, there's a formal representation that ultimately we map to.

612
00:42:05,000 --> 00:42:09,000
There are multiple reasoning engines that pour over this

613
00:42:09,000 --> 00:42:15,000
and start to evaluate different interpretations for the level of sense that they make.

614
00:42:15,000 --> 00:42:18,000
It's as smart as it's prior knowledge.

615
00:42:18,000 --> 00:42:24,000
So there's a Bayesian aspect to this as well as it uses prior knowledge to actually try to determine confidence

616
00:42:24,000 --> 00:42:27,000
in what is the right interpretation and how do I move forward?

617
00:42:27,000 --> 00:42:33,000
It has to be open ended as more information lands and gets acquired.

618
00:42:33,000 --> 00:42:38,000
It has to be able to kind of go back and say, okay, which direction do I go to continue to make sense out of this?

619
00:42:38,000 --> 00:42:43,000
It has to know what it's doing so that it can dialogue and ask questions

620
00:42:43,000 --> 00:42:45,000
and ask for help about its interpretations.

621
00:42:45,000 --> 00:42:50,000
And it has to do that in a way that isn't incredibly stupid

622
00:42:50,000 --> 00:42:53,000
because otherwise humans aren't going to engage in it.

623
00:42:53,000 --> 00:43:01,000
So it has to do everything it possibly can do in extracting back our knowledge from large corpora.

624
00:43:01,000 --> 00:43:04,000
Even if it doesn't completely understand the large corporate yet

625
00:43:04,000 --> 00:43:07,000
because it doesn't have the foundations to do that.

626
00:43:07,000 --> 00:43:12,000
It at least has to be able to use that to prune its search and shape its interaction with humans.

627
00:43:12,000 --> 00:43:16,000
So it doesn't get so stupid that nobody wants to talk to it.

628
00:43:16,000 --> 00:43:23,000
So these are all enormous challenges that bring in sort of every aspect of AI,

629
00:43:23,000 --> 00:43:27,000
save robotics type stuff, hardware type stuff,

630
00:43:27,000 --> 00:43:33,000
but in terms of knowledge representation, reasoning,

631
00:43:33,000 --> 00:43:38,000
natural language processing, learning, deep learning, reinforcement learning,

632
00:43:38,000 --> 00:43:41,000
dialogue management, you name it.

633
00:43:41,000 --> 00:43:48,000
You certainly make a good argument for the level of difficulty of the problem.

634
00:43:48,000 --> 00:43:53,000
The other part of the argument you're making is the type of investment

635
00:43:53,000 --> 00:43:57,000
and need to invest in solving it.

636
00:43:57,000 --> 00:44:00,000
How do you characterize that for folks?

637
00:44:00,000 --> 00:44:03,000
Yeah, so I mean, you know, the team, so first of it's interesting

638
00:44:03,000 --> 00:44:10,000
because I'm all, you know, EC while we're doing or approaching a hard research problem,

639
00:44:10,000 --> 00:44:14,000
you know, we're not strictly an academic research institution.

640
00:44:14,000 --> 00:44:19,000
We are engineering a system and we're continually refining it, building it, and evaluating it.

641
00:44:19,000 --> 00:44:23,000
So we have a mix of engineers and researchers,

642
00:44:23,000 --> 00:44:28,000
and you can imagine that there's quite a bit of diversity.

643
00:44:28,000 --> 00:44:33,000
So if people who specialize in machine learning, deep learning for folks,

644
00:44:33,000 --> 00:44:37,000
we have NLP folks, dialogue folks, we have knowledge representation,

645
00:44:37,000 --> 00:44:42,000
we have reasoning folks, we have linguists, so, you know,

646
00:44:42,000 --> 00:44:49,000
and having a underlying architecture that is laid out well enough

647
00:44:49,000 --> 00:44:52,000
that these individuals can work together.

648
00:44:52,000 --> 00:44:55,000
You know, and I did this on, even though this is,

649
00:44:55,000 --> 00:44:59,000
and towards a magnitude more complicated than the Watson architecture,

650
00:44:59,000 --> 00:45:06,000
the basic approach toward managing a team is like this is very similar,

651
00:45:06,000 --> 00:45:10,000
and how to conduct and assemble that team is very similar.

652
00:45:10,000 --> 00:45:15,000
So the first set of interests, very committed to the mission,

653
00:45:15,000 --> 00:45:18,000
you know, is incredibly important.

654
00:45:18,000 --> 00:45:22,000
And at the same time, coming at it from very different perspectives

655
00:45:22,000 --> 00:45:26,000
they all see themselves as contributing to a larger, more complex architecture

656
00:45:26,000 --> 00:45:31,000
that isn't just an architecture in theory.

657
00:45:31,000 --> 00:45:35,000
It's an engineered system and sort of you know where to plug in

658
00:45:35,000 --> 00:45:39,000
and how to contribute and sort of move the ball forward.

659
00:45:39,000 --> 00:45:44,000
So anyway, these are how I think about managing this type of project.

660
00:45:44,000 --> 00:45:48,000
In terms of the scale of the investment, also good question,

661
00:45:48,000 --> 00:45:54,000
always thinking about that myself, I'm always careful to invest incrementally.

662
00:45:54,000 --> 00:46:00,000
So as I see, as I see the pieces coming together and, you know,

663
00:46:00,000 --> 00:46:05,000
you start with a few people, you grow out, as you see pieces coming together,

664
00:46:05,000 --> 00:46:08,000
you look for where the bottlenecks are, you look where for the opportunities are

665
00:46:08,000 --> 00:46:11,000
and where the bottlenecks are, and you grow accordingly.

666
00:46:11,000 --> 00:46:15,000
And so I tend to do that very carefully and pick the right people to fit into

667
00:46:15,000 --> 00:46:19,000
the right places as we go. And so we've been doing that.

668
00:46:19,000 --> 00:46:25,000
We're up to about 24 people now or, and is that enough?

669
00:46:25,000 --> 00:46:28,000
No, I think that ultimately investment requires more than that.

670
00:46:28,000 --> 00:46:31,000
And it'll probably continue to grow. But as I said, we'll grow incrementally

671
00:46:31,000 --> 00:46:36,000
as we, as it's clear that that's where we're going to get the,

672
00:46:36,000 --> 00:46:39,000
you know, the incremental value.

673
00:46:39,000 --> 00:46:43,000
Earlier you mentioned that there's plenty of low hanging fruit

674
00:46:43,000 --> 00:46:48,000
in deep learning. What's the, the argument for investing in clearly a difficult

675
00:46:48,000 --> 00:46:52,000
problem relative to picking off some of that low hanging fruit?

676
00:46:52,000 --> 00:46:55,000
It's tough. It's a tough call.

677
00:46:55,000 --> 00:47:00,000
You know, it's difficult and it's difficult in terms of,

678
00:47:00,000 --> 00:47:05,000
I'm lucky, you know, that I have the investor that I have who's interested

679
00:47:05,000 --> 00:47:09,000
in the bigger picture and the longer term role that AI has to play.

680
00:47:09,000 --> 00:47:14,000
And how it, how it, how the human machine interaction gets shaped over time.

681
00:47:14,000 --> 00:47:19,000
And also has an appetite for a longer,

682
00:47:19,000 --> 00:47:25,000
longer term type investment and sort of bigger bang for the buck in terms of the impact.

683
00:47:25,000 --> 00:47:32,000
So that's good, hard to find than the 18 months or 24 month lower hanging fruit stuff.

684
00:47:32,000 --> 00:47:36,000
So that's hard to find, but I've got that.

685
00:47:36,000 --> 00:47:39,000
But it also hits you in the recruiting side because you get a lot of people

686
00:47:39,000 --> 00:47:41,000
who are like, you know, I want the quick win.

687
00:47:41,000 --> 00:47:45,000
And there are like quite a number of exciting applications.

688
00:47:45,000 --> 00:47:48,000
I think you could approach with, with just, you know,

689
00:47:48,000 --> 00:47:50,000
straight applications of deep learning.

690
00:47:50,000 --> 00:47:52,000
And they're cool and they're fun.

691
00:47:52,000 --> 00:47:59,000
So you end up really looking and finding people who are just really want,

692
00:47:59,000 --> 00:48:02,000
they want that machine that fluently talks to them.

693
00:48:02,000 --> 00:48:07,000
They want that, they want that, you know, that Star Trek computer that becomes their,

694
00:48:07,000 --> 00:48:08,000
you know, thought partner.

695
00:48:08,000 --> 00:48:12,000
And, you know, just some people really have that dream.

696
00:48:12,000 --> 00:48:16,000
And they understand that, that, that this is,

697
00:48:16,000 --> 00:48:18,000
this is one of the best shots for getting there.

698
00:48:18,000 --> 00:48:21,000
Maybe to kind of start to wrap things up.

699
00:48:21,000 --> 00:48:28,000
Can you talk a little bit about how you deliver something like that in phases?

700
00:48:28,000 --> 00:48:33,000
Or, or, is it, you know, definitionally kind of a big bang thing?

701
00:48:33,000 --> 00:48:37,000
Like we just get there because we have to have all these pieces just right

702
00:48:37,000 --> 00:48:43,000
in order to execute that ultimate vision.

703
00:48:43,000 --> 00:48:46,000
Yeah, so another, I, another really good question.

704
00:48:46,000 --> 00:48:51,000
And, and, and we, we work, we, you know, we struggle through that question.

705
00:48:51,000 --> 00:48:53,000
I mean, I think that there are phases.

706
00:48:53,000 --> 00:48:57,000
And I think that are both phases from a business perspective,

707
00:48:57,000 --> 00:49:01,000
from an investment perspective, and there are also phases from a,

708
00:49:01,000 --> 00:49:05,000
more of a scientific or research perspective.

709
00:49:05,000 --> 00:49:12,000
And we do spend time thinking about, you know, incremental applications

710
00:49:12,000 --> 00:49:15,000
that we can build with the technology that we're creating.

711
00:49:15,000 --> 00:49:17,000
It's part of the investment.

712
00:49:17,000 --> 00:49:20,000
It's not the main thrust right now.

713
00:49:20,000 --> 00:49:24,000
The main thrust is really staging the, the scientific work.

714
00:49:24,000 --> 00:49:28,000
And as I mentioned, you know, defining the problem really well,

715
00:49:28,000 --> 00:49:32,000
making sure that we can scientifically evaluate progress against it.

716
00:49:32,000 --> 00:49:38,000
We continually actually apply deep learning systems and see how they fail

717
00:49:38,000 --> 00:49:40,000
and do the air analysis.

718
00:49:40,000 --> 00:49:45,000
We continually apply our evolving system, look at where it's failing,

719
00:49:45,000 --> 00:49:47,000
and continue to try to improve it.

720
00:49:47,000 --> 00:49:50,000
And we, we create incremental milestones for ourselves.

721
00:49:50,000 --> 00:49:54,000
And that's important because to manage the project,

722
00:49:54,000 --> 00:49:57,000
you can't have this thing that's like five years away.

723
00:49:57,000 --> 00:50:00,000
You have to create those incremental milestones

724
00:50:00,000 --> 00:50:02,000
and even creating those themselves are challenged,

725
00:50:02,000 --> 00:50:04,000
particularly in the very beginning stages.

726
00:50:04,000 --> 00:50:07,000
We're getting to a point where, where we can,

727
00:50:07,000 --> 00:50:09,000
we can create those intermediate scientific milestones,

728
00:50:09,000 --> 00:50:12,000
I think more right, really, and more effectively,

729
00:50:12,000 --> 00:50:15,000
because we have enough of the architecture built out that we can do that.

730
00:50:15,000 --> 00:50:17,000
We can run this on content.

731
00:50:17,000 --> 00:50:21,000
We can do the air analysis. We can identify where the problems are.

732
00:50:21,000 --> 00:50:23,000
We can start to iterate on making the system smarter.

733
00:50:23,000 --> 00:50:26,000
So we're getting, we're at that point now, which is great.

734
00:50:26,000 --> 00:50:28,000
But that's a very important thing.

735
00:50:28,000 --> 00:50:32,000
And learning how to evolve those milestones well is important.

736
00:50:32,000 --> 00:50:36,000
On the, on the kind of incremental application side,

737
00:50:36,000 --> 00:50:40,000
that's also kind of getting some attention for us.

738
00:50:40,000 --> 00:50:44,000
And we're thinking about ways that we can use the,

739
00:50:44,000 --> 00:50:48,000
even though we don't have that deep general understanding yet,

740
00:50:48,000 --> 00:50:52,000
we've built a lot of impressive kind of NLP and,

741
00:50:52,000 --> 00:50:56,000
and representation and reasoning stuff that we think can help out

742
00:50:56,000 --> 00:50:59,000
in a number of different areas and replaying with that.

743
00:50:59,000 --> 00:51:02,000
We don't, we don't have to spin off though,

744
00:51:02,000 --> 00:51:04,000
because of our investment structure,

745
00:51:04,000 --> 00:51:06,000
we don't have to spin off those applications.

746
00:51:06,000 --> 00:51:10,000
But we want to keep ourselves honest and make sure that we can demonstrate

747
00:51:10,000 --> 00:51:15,000
to ourselves, our investors, that this technology will have impact

748
00:51:15,000 --> 00:51:18,000
and that it can, and it can do that incrementally.

749
00:51:18,000 --> 00:51:22,000
So that certainly is a part of what we, we think about and do.

750
00:51:22,000 --> 00:51:24,000
But it's, it's, it's good.

751
00:51:24,000 --> 00:51:27,000
You're asking like, you're, you're asking key questions

752
00:51:27,000 --> 00:51:29,000
to figuring out how to, how to manage a business like this.

753
00:51:29,000 --> 00:51:30,000
You're absolutely right.

754
00:51:30,000 --> 00:51:34,000
Well, Dave, thanks so much for taking the time to chat with me

755
00:51:34,000 --> 00:51:35,000
about what you're working on.

756
00:51:35,000 --> 00:51:37,000
It sounds like really interesting stuff.

757
00:51:37,000 --> 00:51:39,000
You bet. It's been a pleasure.

758
00:51:39,000 --> 00:51:41,000
Awesome, my pleasure. Thanks so much.

759
00:51:41,000 --> 00:51:42,000
Thank you.

760
00:51:46,000 --> 00:51:47,000
All right, everyone.

761
00:51:47,000 --> 00:51:49,000
That's our show for today.

762
00:51:49,000 --> 00:51:51,000
Make sure you leave us your birthday message

763
00:51:51,000 --> 00:51:54,000
over at twimmelai.com slash 3Bday

764
00:51:54,000 --> 00:52:00,000
or via voicemail at 1636-735-3658.

765
00:52:00,000 --> 00:52:02,000
We cannot wait to hear from you.

766
00:52:02,000 --> 00:52:05,000
We want to know your favorite gem from the podcast,

767
00:52:05,000 --> 00:52:07,000
what you've learned and how you've applied it

768
00:52:07,000 --> 00:52:09,000
to what you do.

769
00:52:09,000 --> 00:52:13,000
For information about today's guest, visit twimmelai.com.

770
00:52:13,000 --> 00:52:16,000
As always, and especially today,

771
00:52:16,000 --> 00:52:43,000
thanks so much for listening and catch you next time.

