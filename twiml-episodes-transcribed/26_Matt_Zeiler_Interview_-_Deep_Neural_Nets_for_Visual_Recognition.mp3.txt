Hello and welcome to another episode of TwimoTalk, the podcast where I interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
First and foremost, I'd like to start off by thanking you.
It's your dedication and support of this show that keeps us pumping week after week.
All your downloads, listens, reviews, comments, and general love for the show give us the
ability to grow and to continue to bring you great content.
As always, thank you to everyone who's sending quotes this past week.
And to all those who've received stickers in the mail, be sure to take pictures and show
them off by tweeting them to at TwimoAI.
For you new listeners to receive your TwimoSticker, just send us a tweet, Facebook comment, or comment
right on the show notes page with your favorite quote from today or any episode of the show.
The next couple weeks will be super busy and exciting for me.
Next week, I'm in Silicon Valley for the NVIDIA GTC conference.
If any Twimo listeners are planning to attend and would like to get together, I am totally
game to host a listener meetup, just let me know if you're interested and will make something happen.
And of course, the week after next will be the future of data summit that I'm hosting
in Las Vegas.
If you haven't already made plans to attend, it's not too late.
For more information about the summit, visit TwimoAI.com slash future of data.
And if you have any questions at all about the event, feel free to reach out to me via
the contact page at TwimoAI.com.
Now let's get to this week's show.
Today, we bring you our final interview from backstage at the NYU Future Labs AI Summit.
Our guest this week is Matt Zeeler.
Matt graduated from the University of Toronto, where he worked with deep learning researcher
Jeffrey Hinton, and he went on to earn his PhD in machine learning at NYU, home of
Jan LeCoon.
In 2013, Matt founded Clarify, a startup whose cloud-based visual recognition system gives
developers a way to integrate visual identification into their own products and whose initial image
classification algorithm achieved top five results in that year's ImageNet competition.
I caught up with Matt after his talk, which was titled, From Research to the Real World.
Our conversation focused on the birth and growth of Clarify, as well as the underlying
deep neural network architectures that enable it.
If you've been listening to the show for a while, you've heard me ask several guests
how they go about evolving the architectures of their deep neural networks to enhance performance.
Well, in this podcast, Matt gives the most satisfying answer I've received to date by
far.
Check it out.
I really think you'll enjoy this one.
And now on to the show.
All right, everyone.
I am here with Matt Zealer, CEO and founder of Clarify.
And we are live at the Future Labs AI Summit.
Matt, how are you doing?
Good.
Thanks for having me, Ellen.
Absolutely.
Thanks for being on.
We just left the stage where you were talking about, from research to the real world.
And we'll take a second to get into your talk in a moment.
But why don't you tell us a little bit about your background and about Clarify?
Sounds good.
So, I, way back, I grew up in Canada and in a small farm town, so I was never exposed
to big tech stuff.
And that always got me excited seeing it on TV and gave me excitement about starting
a company someday and for engineering at University of Toronto is what I did in my undergrad.
It was a pretty cool program called Engineering Science where you take every discipline of
engineering and then for the first two years and then the last two you specialize.
And when I was deciding which option to specialize in, I talked to one of Jeff Hinton's PhD students.
So Jeff is very famous in this field of AI and he was a U of T professor at the time.
And his student showed me this video of a flame flickering and he said it was completely
created by artificial intelligence.
And that's when I knew this was something completely different than anything else I've
seen before and anything else I could program, which I knew I had to do at the time.
And so I had to learn more and I took a computer option and Jeff Hinton's course in third
year and then did my undergrad thesis with Jeff.
So that was quite the opportunity.
Absolutely.
And I didn't know enough having just taken undergrad to start a company in this field.
So I decided to do my PhD and that's what brought me to New York to go to NYU.
And I focused over four years on neural networks applied to understanding images and that's
exactly what we do today at Clarify.
And the last year was particularly exciting where there was this event called ImageNet that
held every year and in 2012 Jeff Hinton again and two of his PhD students had some really
groundbreaking results applying neural networks to this competition compared to other algorithms
like edge detection, color detection, simple stuff.
And they blew out everybody and by a huge margin it was really a step function improvement
and accuracy of these things.
And so everybody who didn't believe in neural nets all of a sudden started believing in
them.
So what really triggered this explosion we're seeing in the AI world.
And so being at NYU where I was already working on this stuff I could get up and running
pretty quickly with that result and at the start of 2013 and continue to improve it throughout
the year.
And I realized that after interning at Google Brain, which is their neural net team, the
tech I had working back in New York was actually working better than what they had internally.
And so I saw that as a big opportunity to start a business because Google is well known
in the AI space and they were already ahead of many other companies out there.
And so that's what triggered it in the fall of 2013 incorporating Clarify.
And we ended up winning right off the start of this ImageNet competition.
So that was a great way to kick off the company and triggered a lot of inbound press and
invested from there.
And we've been offering this technology up as APIs and continuing to grow the business
through that.
That's pretty amazing.
We talk about ImageNet from time to time on the podcast.
Anything you can share about the experience building up to competing in ImageNet and successfully
winning it?
Yeah, it was a lot of fun.
It was back in the very early days of Clarify.
I really started programming our library and everything back in August of 2013.
And the submission deadline was in November.
And so I just had in my apartment one desktop with a couple of graphics cards in it because
they use GPUs for all this type of neural net stuff.
And it was just training away constantly.
And I submitted five results to the competition and they ended up winning the top five places.
So it was really exciting to have those early days of Clarify where it was just me and
the one machine.
And now we just surpassed 50 people in the company.
So it's also exciting now that it's been created to grow the company.
That's awesome.
Well, one of the questions that I ask frequently and haven't really, I think, to date got a
very satisfying response is, you know, when you're trying to, you know, starting from what
Hinton Steamed did in 2012 and you're trying to evolve a neural net architecture to take
on a problem, or if you're starting from scratch, like how do you approach the problem
of neural network architecture?
What's the thinking and the primitives that go into that that get you to winning image
net?
Yeah.
I had the very last paper on my PhD was actually focused on understanding these neural networks.
And people have understood what the very first layer of these networks do.
So neural nets are composed of many layers of processing that are learned from the data,
which is what makes them really powerful.
And the first layer, you can just visualize if you look at a neural net applied to images,
it ends up learning different edges and different colors very simply in the first layer.
But nobody really knew what was happening beyond that.
In the second layer, third layer, all the web to however many layers you have.
So the last paper of my PhD was focused on visualizing and understanding convolutional
neural networks.
And we saw that these edges form into things like corners and circles and T junctions and
parallel lines kind of mid-level things by the second layer.
The third layer starts extracting things like eyeballs and noses and ears.
And then the fourth and fifth layers might start learning faces and then the final layers
will output person.
So it actually builds up these levels of abstraction in a very sensible way.
And so in visualizing these things, we actually robbed Fergus and I at NYU, realized some deficiencies
of a neural net architecture that Jeff Hinton used with his team in ImageNet 2012.
And so we could actually improve the architecture kind of systematically by looking at these
visualizations.
And that gave a multiple percentage points improvement.
And then other things are more of an art than a science.
There's not a lot of theory around these neural networks.
It's more from experience when you see this kind of behavior change these types of parameters.
And so that's why there's very few experienced people in this field and that's why they're so
valuable.
It's that kind of experience that they bring to the table.
What's an example of if you see this change that?
So one example is if there's something called a learning rate, which tells you how big
of a step to update your model every time you see a new image in the case of our stuff.
And so if you have a really high learning rate, you'll see the accuracy just explode and
you'll see the parameters of the model just explode into really high values.
It becomes an unstable system.
And so you know you can actually decrease the learning rate to make that more stable.
And that's something that we see with non-neural network machine learning models as well.
Are there things that, is there an example specifically related to neural network architecture?
Like if you're seeing your neural network do X or not do X, maybe you need to add this
kind of layer or that kind of layer?
Yeah.
So the visualizations that we notice that looked incorrect in some sense were when we looked
at higher layers, there was a lot of blocking artifacts.
So it didn't look like a very sharp visualization.
And so that was a result of the first layer being really large filters and not being applied
at every pixel, one pixel stride, but every four pixels it was taking big jumps.
And so when you visualize it also plops down in big jumps.
And so that has to be throwing away information, it's skipping over too much.
And so by reducing the stride, reducing the filter size, it actually increased the accuracy.
So it sounds like the lesson then is or lessons are that as you said it's you know there's
art as well as science involves it's highly iterative.
But also I think what I learned in this discussion is that what really gave you the edge was being
able to kind of introspect if you will what was happening at the different layers of the
architecture and using the knowledge that you gain there to evolve the architecture.
Yeah, exactly.
Is the do you remember the name of the paper that you referred to?
It's called visualizing and understanding convolutional neural networks.
And is that paper related to the deep dream work?
So it's funny.
It's smiling here.
It's funny.
You mentioned that.
Yeah, I think it's a very similar technique.
So the purpose of my paper was to just go through a network, you input an image, it goes
feed forward through a network as if it was about to predict the categories.
And then you reconstruct back down to generate a pixel and deep dream basically did that
process in multiple loops so they would reconstruct a visualization, add it back to the original
image and then continue that in a cycle.
And that's where the system essentially gets locked into what it's dreaming about and
it starts visualizing kind of crisper objects that it can think about.
Okay.
So clarify is a bit of a success story in the NYU ecosystem, kind of where is the company
at now in terms of, you know, however you describe it size scale, you know, the problems
that you're going after, things like that.
Yeah, so it's been really exciting starting it in my apartment and then the very first
office was at NYU's incubator on Verix Street.
And then once we had about 10 people, we got our own office and that was around when we
raised our series B, which was a pretty big round, 10 million dollars.
And then just last fall in October of 2016, we raised our series B, which was a 30 million
dollar round, so lots of funding for growth in the future.
And we just surpassed 50 people, so growing really quick and we were only about 30 people
at the start of the year.
Wow.
Yeah.
That's amazing.
Yeah.
And so the, as I understand, the solution that you guys are offering is, and you can,
you know, correct and iterate on this, but it is kind of a visual understanding API if
you will, where you feed it images and it will tag those images for you and give you
probabilities of the likelihood of those tags.
And you know, that's something that we see, you know, there are numerous versions of this
available in a marketplace, you know, including by, you know, big players like Google and Microsoft
and Amazon.
Like what makes clarified different and how do you compete in a crowded field?
Yeah, so that's a good summary of what we offer and have been offering for over three years.
Now, so one big advantage for us is that we've been out at least three times longer than
most other people in this space, which means we've, and because of winning ImageNet, early
it means we've had a lot of conversations with customers.
We really understand lots of different industries and the problems that each of these customers
have in them.
And so we try and build those features into our platform.
And so some examples of that were just launched in the fall.
One is a search product and one is a training product.
So search very simply lets you throw in your images to our system and we take care of
the rest.
We understand them from the pixels up to recognize all these different concepts.
And then we let you search by the concepts that we can recognize, but also by image.
So if you take a picture out in the world, you can find visually similar pictures automatically
without even saying a word.
So that's really powerful as a discovery tool, and that applies across every industry.
Like retail, you can use it for shopping, gardening.
You can use it for gardening ideas or travel for planning your next vacation.
It's a universally useful feature in our platform.
And likewise, training is another universally useful feature.
We started off with this one size fits all model we call the general model.
We started building what we like to call domain models for food, travel, wedding, real estate,
nudity and so forth.
So there are more focus on different industries.
And then with custom training, we've gone one step further.
Now you can teach the platform to recognize whatever you care about.
And this is a huge step ahead of a lot of the other people out there.
And it's really important for our customers because now if you take a retail setting, they
have a way of categorizing their products, or e-commerce marketplaces.
People are used to manually tag in this content, so they already implicitly have a categorization.
Or for filtering out unwanted content, they already have a terms of use that includes nudity
and drugs and weapons, and even non-offensive stuff like, you know, they just don't want
advertisements on their site or stock photos on their site or money or people.
And so now with custom training, you can customize it to recognize exactly what you care about
in your business, which is very exciting.
What kind of accuracy lift do you see customers obtaining with custom training relative to
standard or domain models?
Yeah, it's a big lift because it's usually trained on their specific data.
So in terms of accuracy, numbers, I think it's not fair to compare because the general
model is meant for if you take your phone out of your pocket and take a picture and tell
you something meaningful.
We at Clarify have collected massive data sets to make that really accurate.
But when you focus on the 10 or 100 or 1000 categories that you care about only, then you
can end a strain on data that you have access to, then you can really drive the accuracy
because it's really solving your specific problem.
So as Clarify does Clarify offer both a consumer solution, take a, you know, an app that I take
a picture and it will recognize things as well as a business offering.
Yeah, we do have a consumer app called Forever, which lets you apply our general model and
our facial recognition to understand your personal photos.
And it's available on iOS right now.
That's spelled forever with a Y at the end, it's for every one of your photos.
And it even lets you teach things.
So this custom training that we just launched in our API, we actually launched it first
in forever.
So you can teach it like the name of your pet or your favorite sports car and all that
becomes searchable over your personal pictures as well as, you know, your mom's name and
your sister's name, it actually recognizes people.
So whenever you take a new picture of them, it'll automatically tag them and index them
for search.
Okay, and to what degree does having that is the main reason why or one of the main reasons
why you offer that because it allows you to expand the data set for which you're able
to train over.
Yeah.
How does that play into the model?
Yeah, exactly.
That was the initial, well, there was two initial incentives at the data collection and
it was always really a passion of mine.
Even before starting Clareify, before deciding on the API as building a platform, I was always
passionate about applying this technology to my own personal pictures and that's when
I knew it was actually ready to start a business around.
I built a very simple demo and just dragged and dropped my pictures into it and it gave
meaningful results.
And so it's always been a passion plus it's a great way for us to crowdsource data collection.
That's awesome.
What are some of the most popular business use cases that you're seeing?
Yeah, I like to bucket them into two big buckets.
One is organization and one is moderation and these apply across all different industries.
So organization, we have companies like Travago, for example, that are in the travel space.
They want to take all the user uploaded and hotel uploaded photos and tag them with our
travel model so that people may be the next time you're planning your vacation, you can
search for the best pool in Jamaica and it shows you the pictures to compare and make
your decision easier.
Same story applies in the wedding space with Stanley Pretty.
They have users upload their entire wedding album and they use our wedding model to categorize
them and that helps new users come to Stanley Pretty and find inspirations for their wedding.
The more new users and the more content, the more ad revenue that they can drive from
Stanley Pretty and on the moderation side, whenever there's an upload button on the internet
or in a mobile app, there's a chance that you're going to get offensive content and that
could be nudity, drugs, weapons, violence, lots of different stuff that you don't want
on your marketplace and advertisers don't want to be associated with this content either.
So now with our moderation models, you can build a brand safe section of your site and for
your community or to drive ad revenue and do that automatically.
And this is traditionally done with a manual work and it just doesn't scale both in terms
of how fast a human can respond to new uploads and to how much new content is being created
by cell phones all over the world. So we solve both of those by automating the process.
So I'm curious what your thoughts are on where you see opportunities in this space.
We're here at the Future Lab Summit. In addition to many of the New York AI community, there
are lots of NYU students. The next AI entrepreneur might be in the audience there.
Where do you think the opportunities lie for folks that are looking to do something?
Yeah, I think there's a huge amount of opportunity leveraging artificial intelligence and we're
starting to see that by the sheer number of startups that are being formed around AI.
I think one thing that excites me a lot is this personalization capability I mentioned
in talking about training your pets, training your mom's name and your photos. But that
could be anywhere. It could be in your Alexa learning about you. It could be in all your
devices. And the more we see that, the more AI is going to fit to each individual and I
think that's going to be the next wave of AI.
It's awesome. It's awesome. Any parting thoughts?
I would urge that any person interested in starting an AI company is not doing it to
ride the wave, I think. And not doing it just because you have cool technology, I think
you really want to have a passion for starting a company because as the company gets bigger
and bigger, you'll be doing, if you're the CEO, you'll be doing less and less of the
technology yourself. So just keep that in mind when you're thinking about building a business.
You mentioned that on stage. Does that sound like hard one, hard learn lessons?
Yeah, it's always itch that I want to scratch to get into the code and work on new stuff.
But there's a lot of things that only a CEO can do. And so that's my priority.
Right. Well, Matt, thank you so much for taking the time and being on the show is very
chatting with you.
Thanks for having me.
All right, everyone. That's our show for today. Once again, thanks so much for listening
and for your continued support. Please don't forget to share your favorite quote from this
show via the show notes page via Twitter or via our Facebook page. If you do, we'll be
happy to send you one of our laptop stickers. The notes for this show will be up on twomolei.com
slash talk slash 22 where you'll find links to Matt and clarify in the various resources
we've mentioned in the show. Thanks so much for listening and catch you next time.
