1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:30,960
I'm your host Sam Charrington.

4
00:00:30,960 --> 00:00:36,920
In this episode of our deep learning endoba series, we're joined by Oshonde Osaba, engineer

5
00:00:36,920 --> 00:00:41,720
at Rand Corporation and professor at the party Rand Graduate School.

6
00:00:41,720 --> 00:00:47,280
Oshonde and I spoke on the heels of the endoba where he presented on AI ethics and policy.

7
00:00:47,280 --> 00:00:52,480
We discuss framework-based approach for evaluating ethical issues such as applying the ethical

8
00:00:52,480 --> 00:00:57,360
principles laid out in the Velmont report and how to build an intuition for where ethical

9
00:00:57,360 --> 00:01:01,000
flashpoints may exist in these discussions.

10
00:01:01,000 --> 00:01:05,640
We then shift gears to Oshonde's own model development research and end up in a really

11
00:01:05,640 --> 00:01:10,040
interesting discussion about the application of machine learning to strategic decisions

12
00:01:10,040 --> 00:01:15,800
and game theory, including the use of fuzzy cognitive map models.

13
00:01:15,800 --> 00:01:20,040
Before we jump in, I'd like to send a big shout out to our friends at Google AI, which

14
00:01:20,040 --> 00:01:25,400
recently opened up applications for its 2019 residency program.

15
00:01:25,400 --> 00:01:29,880
The Google AI Residency is a one-year machine learning research training program with

16
00:01:29,880 --> 00:01:35,000
the goal of helping individuals from all over the world and with a diverse set of educational

17
00:01:35,000 --> 00:01:40,160
and professional backgrounds become successful machine learning researchers.

18
00:01:40,160 --> 00:01:44,960
Find out more about the program at g.co slash AI residency.

19
00:01:44,960 --> 00:01:47,200
And now on to the show.

20
00:01:47,200 --> 00:01:50,720
All right, everyone.

21
00:01:50,720 --> 00:01:53,280
I am on the line with Oshonde Osaba.

22
00:01:53,280 --> 00:01:59,360
Oshonde is an engineer at the RAN Corporation and a professor at the Party RAN Graduate

23
00:01:59,360 --> 00:02:00,360
School.

24
00:02:00,360 --> 00:02:03,000
Oshonde, welcome to this weekend machine learning and AI.

25
00:02:03,000 --> 00:02:04,600
Glad to be here.

26
00:02:04,600 --> 00:02:05,600
Awesome.

27
00:02:05,600 --> 00:02:12,360
So, you did your PhD in machine learning and now you spend quite a bit of your time working

28
00:02:12,360 --> 00:02:19,320
on the policy implications of machine learning and AI in particular with regard to ethics

29
00:02:19,320 --> 00:02:23,640
among actually rolling up your sleeves and doing some model development.

30
00:02:23,640 --> 00:02:25,840
Tell us a little bit about your background.

31
00:02:25,840 --> 00:02:26,840
Oh, yeah.

32
00:02:26,840 --> 00:02:32,320
So, I did my engineering degree at USC, the University of Southern California, across town

33
00:02:32,320 --> 00:02:33,320
from where I am.

34
00:02:33,320 --> 00:02:38,280
I mean, sunny Southern California, and Southern Monica by the beach.

35
00:02:38,280 --> 00:02:44,480
Yeah, so at USC, I was focused on more theoretical aspects of learning algorithms.

36
00:02:44,480 --> 00:02:49,680
So, I was trying to figure out how to improve machine learning algorithms, statistical learning

37
00:02:49,680 --> 00:02:53,880
algorithms using the care for the injection of noise.

38
00:02:53,880 --> 00:02:56,360
And it was a very theoretical type of thing.

39
00:02:56,360 --> 00:02:57,360
I was finished.

40
00:02:57,360 --> 00:03:00,000
I wasn't quite sure what to do with myself.

41
00:03:00,000 --> 00:03:03,880
So, you know, as far as this is where you could either go up to see the come valley and

42
00:03:03,880 --> 00:03:08,960
do some work for a commercial entity or figure out something to do in academia.

43
00:03:08,960 --> 00:03:11,160
I sort of found a halfway point.

44
00:03:11,160 --> 00:03:14,400
At Rand, Rand is a public policy think tank.

45
00:03:14,400 --> 00:03:16,320
Oh, they don't like the word think tank.

46
00:03:16,320 --> 00:03:20,160
We do a lot of thinking about policy problems.

47
00:03:20,160 --> 00:03:24,960
A mandate that Rand is to improve decision-making through objective analysis.

48
00:03:24,960 --> 00:03:30,160
And so, I had worked there over a summer while I was in graduate school looking, well,

49
00:03:30,160 --> 00:03:31,160
I need money.

50
00:03:31,160 --> 00:03:32,560
I worked there for the summer.

51
00:03:32,560 --> 00:03:35,240
And I figured it was a pretty good fit.

52
00:03:35,240 --> 00:03:42,200
So, I went back there and it was just around the time when lots of agencies, lots of people

53
00:03:42,200 --> 00:03:45,680
across the country are starting to think about machine learning and the implications.

54
00:03:45,680 --> 00:03:50,240
And so, I hit the ground running at Rand, just deploying models, deploying machine learning

55
00:03:50,240 --> 00:03:53,480
models to answer that response questions.

56
00:03:53,480 --> 00:04:00,360
And my growth at Rand has basically been me recognizing using my technical skills, what

57
00:04:00,360 --> 00:04:05,440
I would recognize in some of the normative implications, I'm trying to address them.

58
00:04:05,440 --> 00:04:10,200
The normative implications are using AI and machine learning in public policy spaces.

59
00:04:10,200 --> 00:04:12,600
So, I tried to balance both now.

60
00:04:12,600 --> 00:04:15,240
It's been an interesting ride, I guess.

61
00:04:15,240 --> 00:04:18,240
And you recently returned from the deep learning and daba.

62
00:04:18,240 --> 00:04:19,680
Tell us what did you present on?

63
00:04:19,680 --> 00:04:28,640
Yes, I also asked me because I had written some stuff on the issue of bias in algorithmic

64
00:04:28,640 --> 00:04:29,640
systems.

65
00:04:29,640 --> 00:04:37,240
I was asked to talk about more broadly, if you think about it, the question of bias or

66
00:04:37,240 --> 00:04:42,880
we call bias or equity in these algorithms, it's an ethical question.

67
00:04:42,880 --> 00:04:48,920
Because the ethical norms, they retry to say, okay, do our systems adhair or do it by

68
00:04:48,920 --> 00:04:50,640
a little ethical norms.

69
00:04:50,640 --> 00:04:56,160
And so, in the past year, I've been trying to broaden that discussion quite a bit.

70
00:04:56,160 --> 00:05:01,480
Not just on questions of ethics or bias, but also questions of ethics more broadly.

71
00:05:01,480 --> 00:05:07,160
And so, the question becomes, when a commercial entity, when a public entity decides to use

72
00:05:07,160 --> 00:05:13,200
machine learning for a particular decision problem, what are the ethical constraints, they

73
00:05:13,200 --> 00:05:18,840
should be looking into, what are the frameworks by which they can judge themselves to make

74
00:05:18,840 --> 00:05:22,200
sure that they're not violating x, y, z, ethical constraints.

75
00:05:22,200 --> 00:05:29,320
So, my talk at the end of I was trying to make that more formal, as opposed to just a touch

76
00:05:29,320 --> 00:05:34,600
of feeling discussion of ethics that often goes on, try to give formal, at least concrete

77
00:05:34,600 --> 00:05:37,520
frameworks by which we can think about these things.

78
00:05:37,520 --> 00:05:39,360
I like the sound of that.

79
00:05:39,360 --> 00:05:43,800
It sounds like there were a number of frameworks that you presented.

80
00:05:43,800 --> 00:05:46,520
Was there a specific number of them?

81
00:05:46,520 --> 00:05:49,960
Have you categorized the framework space, if you will?

82
00:05:49,960 --> 00:05:54,280
No, I have not, if I've done that, I think I would be very happy.

83
00:05:54,280 --> 00:05:58,920
But I mean, that's like, no, that's a huge, a huge step.

84
00:05:58,920 --> 00:06:04,720
But one of the things I relied on in discussing this was some of the work by Shannon Barlow-Altor

85
00:06:04,720 --> 00:06:07,640
of Santa Clara University.

86
00:06:07,640 --> 00:06:15,760
And the idea here is that if you try to impact in polls, normative, prescriptive ethics

87
00:06:15,760 --> 00:06:19,160
on the system, that this not always, it doesn't always work.

88
00:06:19,160 --> 00:06:24,760
Like, coming up with the universal ethical law for guidance, tech practice, tech development

89
00:06:24,760 --> 00:06:28,960
is not usually an easy or feasible task.

90
00:06:28,960 --> 00:06:32,680
So we went through this process where we went, okay, instead of trying to say this is

91
00:06:32,680 --> 00:06:39,680
how this should be, instead of imposing norms on just unilaterally, we tried to elicit

92
00:06:39,680 --> 00:06:41,520
the norms from case studies.

93
00:06:41,520 --> 00:06:47,960
So the first part of the conversation at the end of that was, okay, here are a couple

94
00:06:47,960 --> 00:06:55,640
of cases where people were thinking about using algorithms in a particular decision process

95
00:06:55,640 --> 00:07:01,880
how people build their intuition on where the ethical flashpoints might be.

96
00:07:01,880 --> 00:07:04,920
And then towards the end, we started coming up with frameworks.

97
00:07:04,920 --> 00:07:11,760
The one framework I liked that I sort of pushed a little bit was the development principle

98
00:07:11,760 --> 00:07:12,760
idea.

99
00:07:12,760 --> 00:07:17,920
So if you think about the development principles, it's really the set of principles.

100
00:07:17,920 --> 00:07:23,760
And I suppose that came out of a Belmont report, like I think in the 70s, trying to think

101
00:07:23,760 --> 00:07:29,800
through what are, how do you think through the implications of human experiments, of experiments

102
00:07:29,800 --> 00:07:33,760
that affect humans, so human subjects protection and stuff.

103
00:07:33,760 --> 00:07:37,800
And so part of the, no, no, yes, EEL, M-O-M-T.

104
00:07:37,800 --> 00:07:38,800
Okay.

105
00:07:38,800 --> 00:07:45,360
So part of the, on the center, I was trying to pull into the spaces every time you develop

106
00:07:45,360 --> 00:07:49,560
an algorithm to answer a problem that affects lots of people.

107
00:07:49,560 --> 00:07:54,000
At this sense, a social experiment, it's like, it's a social experiment that affects

108
00:07:54,000 --> 00:07:55,560
lots of human subjects.

109
00:07:55,560 --> 00:08:02,080
So it's not, it's not, it doesn't seem too far of a step to say that we should at least

110
00:08:02,080 --> 00:08:06,640
try to judge those new applications according to the development principles.

111
00:08:06,640 --> 00:08:11,040
And so we have things like, uh, download principles are, uh, it's basically your, your

112
00:08:11,040 --> 00:08:16,200
people critical in taking it a little bit further, so the basic one is do know harm, uh, there's

113
00:08:16,200 --> 00:08:21,080
the great, then the other one is, um, make sure your application is just, and then the

114
00:08:21,080 --> 00:08:23,240
third one is respect autonomy.

115
00:08:23,240 --> 00:08:29,000
Now, those sound very abstract and high-fifileting, and so one of the things, one of the tasks I

116
00:08:29,000 --> 00:08:35,040
gave myself was to try to, um, root those, make those more concrete in the case that

117
00:08:35,040 --> 00:08:37,880
is we're talking through, I thought the deep learning in Hidabah.

118
00:08:37,880 --> 00:08:43,240
And so the first one, you know how, actually interesting because, but the most part, we

119
00:08:43,240 --> 00:08:45,640
all think we are doing the best we can.

120
00:08:45,640 --> 00:08:51,760
So the, I think we decided that we, we limited not just to do know harm, but do know foreseeable

121
00:08:51,760 --> 00:08:57,240
harm, understand that we don't probably, we are always able to, to understand the impact

122
00:08:57,240 --> 00:09:01,960
and the piece of technology, any piece of model, any model has a particular space.

123
00:09:01,960 --> 00:09:08,760
So those are the types of conversations, case studies, to, to prime intuitions, um, building,

124
00:09:08,760 --> 00:09:12,880
building up, um, suggesting frameworks like the bell and press, at least the bell and

125
00:09:12,880 --> 00:09:19,400
press full frame, to help us, um, frame new, new app, thinking of new applications, and

126
00:09:19,400 --> 00:09:23,480
just trying to get general takeaways, general, understandable flashpoints, and I think about

127
00:09:23,480 --> 00:09:25,240
ethics and technology practice.

128
00:09:25,240 --> 00:09:28,040
Well, what were the case studies that you went into?

129
00:09:28,040 --> 00:09:36,400
So, yeah, this is like two or three weeks ago, the first one was, I guess, we're trying

130
00:09:36,400 --> 00:09:44,760
to do, so this is a currently famous, currently popular in famous is the right word, should

131
00:09:44,760 --> 00:09:51,120
people use facial recognition technologies for filing at boarders, now it's the first

132
00:09:51,120 --> 00:09:52,120
one.

133
00:09:52,120 --> 00:09:56,560
And we're, one of the things I was trying to do there, because it's an, I work mostly

134
00:09:56,560 --> 00:10:02,600
in the United States, the deep learning in Daba is, is a more, um, international community.

135
00:10:02,600 --> 00:10:09,800
I, I sent out a survey to try to get, to try to get people's, people's perspectives

136
00:10:09,800 --> 00:10:12,200
on these types of questions earlier on.

137
00:10:12,200 --> 00:10:13,200
Okay.

138
00:10:13,200 --> 00:10:18,160
And I, I made it, I made it such that the survey was, was tag based on where, what part

139
00:10:18,160 --> 00:10:23,480
of the world these people, um, the survey respondents identified with, um, so we had people

140
00:10:23,480 --> 00:10:24,480
like, yeah.

141
00:10:24,480 --> 00:10:28,800
And the, the idea is that we're increasingly, we are having this issue of, um, our value

142
00:10:28,800 --> 00:10:30,080
pluralism.

143
00:10:30,080 --> 00:10:33,000
So let's, let's walk through the, the chain there.

144
00:10:33,000 --> 00:10:37,920
The first part of the chain is, well, questions of equity, questions of biases are normative,

145
00:10:37,920 --> 00:10:39,360
ethical questions.

146
00:10:39,360 --> 00:10:43,400
The second part of the chain is that, well, different parts of the world have different

147
00:10:43,400 --> 00:10:47,560
ethical norms that I wish to leave, but it's a very cultural thing.

148
00:10:47,560 --> 00:10:53,440
And so part of the, the point of the survey was trying to elicit this, any disconnect,

149
00:10:53,440 --> 00:11:00,440
any discontinuities in how, say, somebody in Europe thinks about privacy versus say, somebody

150
00:11:00,440 --> 00:11:02,440
in Africa thinks about privacy.

151
00:11:02,440 --> 00:11:09,960
So I, I, there was, in the survey was, they didn't really elicit those sharp disconnects,

152
00:11:09,960 --> 00:11:14,720
but in the conversation at the end of that, there was this sense that some people thought

153
00:11:14,720 --> 00:11:19,920
it was perfectly fine and some people thought it wasn't, it was, there were such privacy

154
00:11:19,920 --> 00:11:26,080
constraints on these official recognition providing that the security benefits or any potential

155
00:11:26,080 --> 00:11:32,160
security, security benefits of official recognition technologies were, were overwhelmingly

156
00:11:32,160 --> 00:11:34,880
positive compared to the potential downfalls.

157
00:11:34,880 --> 00:11:41,080
And so there is not the conversation in that case that he illustrated that there was no,

158
00:11:41,080 --> 00:11:46,720
there was no unified perspective on the official recognition technology.

159
00:11:46,720 --> 00:11:52,320
And so when you, you have a situation where there's no unified perspective that seems

160
00:11:52,320 --> 00:11:58,840
like a perfect opportunity to try to apply some framework, how did you apply the framework,

161
00:11:58,840 --> 00:12:03,320
the Belmont frame to the, to this particular case study?

162
00:12:03,320 --> 00:12:11,480
So the, I think it's hard to take an issue, which there is polarization or significant

163
00:12:11,480 --> 00:12:18,560
diversity of opinion and try to marshal it into some preferred, whatever that preferred

164
00:12:18,560 --> 00:12:20,160
position is.

165
00:12:20,160 --> 00:12:29,400
So the, the, the procedure or the, the way forward was to highlight that, okay, you want,

166
00:12:29,400 --> 00:12:34,280
so one person brought up the idea that, well, governments, that technically more trustworthy

167
00:12:34,280 --> 00:12:39,320
than say commercial companies at the India use of official recognition technologies.

168
00:12:39,320 --> 00:12:45,200
And that is, that is a question that's terribly dependent on what your experience has been

169
00:12:45,200 --> 00:12:48,440
with government and what part of the world you're from.

170
00:12:48,440 --> 00:12:52,680
And so it's not like you cannot apply the framework and come up with a single answer that

171
00:12:52,680 --> 00:12:54,080
should control.

172
00:12:54,080 --> 00:13:01,160
So my, my job was essentially point out that, okay, if that's your perspective, recognize

173
00:13:01,160 --> 00:13:06,720
that the respectful personal autonomy, that, that factor, that, that's the third principle

174
00:13:06,720 --> 00:13:08,160
in the Belmont principle.

175
00:13:08,160 --> 00:13:13,600
That is not, you know, that's not uniformly guaranteed across the world with different

176
00:13:13,600 --> 00:13:14,600
governments.

177
00:13:14,600 --> 00:13:18,160
And so every time you're making decisions, it's not a, it's not a question, or you should

178
00:13:18,160 --> 00:13:20,400
always use facial recognition technology.

179
00:13:20,400 --> 00:13:24,480
It's more a question of when you're thinking about it, ask these questions, these types

180
00:13:24,480 --> 00:13:25,480
of questions.

181
00:13:25,480 --> 00:13:32,000
And that might help you make more contextual, context sensitive answers to those, to, to

182
00:13:32,000 --> 00:13:35,280
whether you should use the technology or not.

183
00:13:35,280 --> 00:13:44,200
Is that the same kind of process that you would walk one of your, a ran client through?

184
00:13:44,200 --> 00:13:51,680
Do you find that they, that they want more concrete answers or do they want, kind of these

185
00:13:51,680 --> 00:13:56,200
experiences that help them understand the, the scope of the issues?

186
00:13:56,200 --> 00:14:05,560
I think in general, when you're, when you're answering questions on, on behalf of somebody

187
00:14:05,560 --> 00:14:08,200
else, there is a balance that needs to be struck.

188
00:14:08,200 --> 00:14:15,760
You want to relay strong useful insights, strong useful frameworks for proceeding, when

189
00:14:15,760 --> 00:14:20,240
you leave the room, but at the same time, there is a, there is an expectation that you

190
00:14:20,240 --> 00:14:25,560
give them concrete answers, finally somewhat concrete answers to the issues that they are facing.

191
00:14:25,560 --> 00:14:28,640
So I try to strike a balance between the two.

192
00:14:28,640 --> 00:14:35,240
So I find somebody asks me, usually when I'm working on, so the, the presentation was a

193
00:14:35,240 --> 00:14:40,240
little bit, the deep learning in that position was, was at a higher level of abstraction than

194
00:14:40,240 --> 00:14:46,640
I would normally work at, because at, at that point, I'm trying to present as much insight,

195
00:14:46,640 --> 00:14:50,080
as much abstract insight as possible.

196
00:14:50,080 --> 00:14:56,480
And actually talking to clients, naturally dealing with governmental clients, I would, I would

197
00:14:56,480 --> 00:15:03,160
usually start with a concrete demonstration of, okay, this is, this is a facial recognition

198
00:15:03,160 --> 00:15:07,880
technology trying to, you, you're, you're suggesting that you might want to use, well,

199
00:15:07,880 --> 00:15:15,520
these technologies have X, Y and Z, X, Y and Z characteristics, the most important in

200
00:15:15,520 --> 00:15:20,400
the recent days, the recent months being that there is a differential error rate that has

201
00:15:20,400 --> 00:15:26,480
implications for, for justice, you can treat everybody the same, you can treat everybody

202
00:15:26,480 --> 00:15:29,840
equitably, whatever equitably needs life context.

203
00:15:29,840 --> 00:15:36,040
And how does that affect your procedures, your operations, and what do you intend to do

204
00:15:36,040 --> 00:15:41,560
to deal to address those, those are equity constraints, those equity constraints.

205
00:15:41,560 --> 00:15:46,200
So usually I'm working, at least I'm talking to clients at that level, at the more concrete

206
00:15:46,200 --> 00:15:52,640
level of, of detail, but the goal is not just to give them, if you can point out answers,

207
00:15:52,640 --> 00:15:58,200
the goal is to have useful abstractions that help make sense of the world, so that when

208
00:15:58,200 --> 00:16:02,960
somebody else comes with a similar type of problem, you can give them interesting answers,

209
00:16:02,960 --> 00:16:04,560
informed answers.

210
00:16:04,560 --> 00:16:08,560
What types of clients are these generally, and what types of questions are these generally,

211
00:16:08,560 --> 00:16:15,440
and who's generally the actual client, what is their role?

212
00:16:15,440 --> 00:16:21,080
People with experience at Rand Differ, it's one of those places where you make your own

213
00:16:21,080 --> 00:16:30,280
way, my projects, most of my projects tend to be focused on questions I think are important

214
00:16:30,280 --> 00:16:37,000
as opposed to client driven, but in general Rand as a whole has clients are biggest clients

215
00:16:37,000 --> 00:16:44,280
are in the house, about just under how our work comes from what we call the federally

216
00:16:44,280 --> 00:16:49,680
funded research and development centers, we have three of them, or I think we have four

217
00:16:49,680 --> 00:16:55,640
now, we have one for the US Army, one for the US Air Force, one for the Office of the Secretary

218
00:16:55,640 --> 00:17:04,920
of Defense, and one for DHS, and so we would generally take a series of portfolio of strategic

219
00:17:04,920 --> 00:17:11,920
and tactical research problems for those four, those four clients, or subsection, sub

220
00:17:11,920 --> 00:17:13,680
divisions of those four clients.

221
00:17:13,680 --> 00:17:19,280
On the other hand, we have a health division, we have a justative restructure and energy

222
00:17:19,280 --> 00:17:24,920
division, we have, we need to have a labor population division, but in general those

223
00:17:24,920 --> 00:17:32,520
will, those will house our non-defense questions, and there you'd have people getting grant

224
00:17:32,520 --> 00:17:38,680
money from NIH, NSF, or foundations like the Robert Wood Johnson Foundation, or the Gates

225
00:17:38,680 --> 00:17:42,880
Foundation, to answer some policy relevant questions.

226
00:17:42,880 --> 00:17:49,720
There isn't as much detail, I've hesitant to go into a lot more detail about the nature

227
00:17:49,720 --> 00:17:54,400
and of the clients and the types of questions I go into because that might become detestive.

228
00:17:54,400 --> 00:18:02,400
Sure, sure, no, that helps me understand the broad process by which folks are coming

229
00:18:02,400 --> 00:18:08,120
to you and the types of questions that they might be looking for.

230
00:18:08,120 --> 00:18:16,280
At the NW presented this one framework, are there other frameworks that you've relied

231
00:18:16,280 --> 00:18:21,480
on to address similar types of ethical issues?

232
00:18:21,480 --> 00:18:27,360
Also, there is one that I tend to, it's not so much a framework at all, I guess it's

233
00:18:27,360 --> 00:18:29,280
a framework in a sense.

234
00:18:29,280 --> 00:18:36,280
So oftentimes when people in the fairness, accountable, transparent, machine learning,

235
00:18:36,280 --> 00:18:41,600
generally the AI policy space, think about machine learning models, they generally think

236
00:18:41,600 --> 00:18:49,120
about it as single monolithic algorithm, single monolithic models that are solving single

237
00:18:49,120 --> 00:18:51,480
decision questions.

238
00:18:51,480 --> 00:18:58,160
Increasingly, there is this issue of systems level thinking, like even if every algorithm

239
00:18:58,160 --> 00:19:04,920
in a particular system is unbiased, is transparent and all that stuff.

240
00:19:04,920 --> 00:19:08,800
It doesn't mean that the outcome is on from the entire system and it's really going

241
00:19:08,800 --> 00:19:15,120
to be fair unbiased.

242
00:19:15,120 --> 00:19:18,720
Are there specific examples of that phenomenon that come to mind?

243
00:19:18,720 --> 00:19:23,360
It is easier to pay attention to say the criminal justice system where you're always trying

244
00:19:23,360 --> 00:19:29,840
to steal algorithms, but then we always try to steal algorithms at different parts of

245
00:19:29,840 --> 00:19:30,840
the system.

246
00:19:30,840 --> 00:19:35,920
For example, there's the risk criminal risk, risk racism estimation.

247
00:19:35,920 --> 00:19:42,880
There's a risk estimation algorithm that has been popular, has been part of the conversation

248
00:19:42,880 --> 00:19:48,840
for the past two years or so, based on public has worked, and the idea that has always

249
00:19:48,840 --> 00:19:50,760
been all over, we just need to fix that.

250
00:19:50,760 --> 00:19:55,480
Then there was an interesting conversation that has been happening over the past year,

251
00:19:55,480 --> 00:20:00,520
where even if you take this issue up, essentially, if falls on the umbrella,

252
00:20:00,520 --> 00:20:04,680
run away, feedback, run away, feedback loops in systems,

253
00:20:04,680 --> 00:20:10,680
where, because even if you have an algorithm that's performing safe and optimal way,

254
00:20:10,680 --> 00:20:20,200
historic inequities can cause the algorithm, can cause a system to diverge in outcomes

255
00:20:20,200 --> 00:20:25,800
for different groups. So, this is sort of like perturbations of the initial conditions

256
00:20:25,800 --> 00:20:31,480
for a system, for the criminal justice system, is causing this vast,

257
00:20:31,480 --> 00:20:36,440
outsized differences in outcomes. And this happens even if the algorithms

258
00:20:36,440 --> 00:20:40,440
themselves, without paying attention to the historical data, the algorithms themselves

259
00:20:40,440 --> 00:20:46,280
are trying to behave optimally. So, that's the one, of course, the example I have seen,

260
00:20:46,280 --> 00:20:52,360
but this behavior, same as one of the things I've been trying to do, is pay attention to,

261
00:20:52,360 --> 00:20:57,800
essentially, the systems level feedback effect of what's just focusing on individual algorithms

262
00:20:57,800 --> 00:21:04,600
and systems. So, we've been doing this for one of my projects' works on this equity concern,

263
00:21:04,600 --> 00:21:08,360
for the criminal justice system, for insurance systems, onto insurance systems,

264
00:21:08,360 --> 00:21:14,280
and for employment systems. And we emphasize the systemic aspect of what's

265
00:21:14,280 --> 00:21:19,800
to this in individual decision point aspect. And it's usually a lot more, I don't know,

266
00:21:19,800 --> 00:21:23,960
I find it a lot more enlightening than focusing on a single algorithm.

267
00:21:23,960 --> 00:21:32,520
Does looking at it from a system's perspective make these problems more or less actionable

268
00:21:32,520 --> 00:21:39,160
for the clients that are struggling to deal with them? I think it makes the analysis and the

269
00:21:39,160 --> 00:21:45,480
discussion a little bit more complex, but the world we live in is complex, social systems,

270
00:21:45,480 --> 00:21:50,760
which is algorithms that operate in the other complex. But one of the things that one of the

271
00:21:50,760 --> 00:21:57,400
extra levels, extra dimensions, degrees of freedom, that this type of perspective gives you is,

272
00:21:57,400 --> 00:22:02,840
well, okay, now algorithmism working out so well, you could do one of the two things,

273
00:22:02,840 --> 00:22:11,720
you could modify the model directly, or you could pay attention to the outcomes and do post-hoc

274
00:22:11,720 --> 00:22:20,040
modifications, post-hoc corrections, so the outcomes to try to correct any inequity in the model.

275
00:22:20,040 --> 00:22:26,040
And one of the, so an example of where this has gone wrong is, so in child welfare systems,

276
00:22:26,040 --> 00:22:31,960
I think there was a paper last year or the year before looking into the child welfare

277
00:22:31,960 --> 00:22:38,680
system, they tried to use an algorithm to better estimate the risk of adverse outcomes for

278
00:22:38,680 --> 00:22:45,240
children through that system. They had a situation where the algorithm was, was doing in terms of

279
00:22:45,240 --> 00:22:51,400
predictive powers doing pretty well, but in terms of implementation, so we had these social workers

280
00:22:51,400 --> 00:22:56,360
taking a look at the algorithm, but because either because of, they don't understand what the algorithm

281
00:22:56,360 --> 00:23:01,560
is doing or they don't trust that the algorithm is taking everything into account, they decide

282
00:23:01,560 --> 00:23:06,760
to circumvent whatever recommendations the algorithm gives them, so that's like a post-hoc

283
00:23:06,760 --> 00:23:11,880
modification to the behavior of an algorithm. If you're focused on the algorithm alone,

284
00:23:11,880 --> 00:23:15,960
then you're not going to, you're not going to pay attention to those types of,

285
00:23:15,960 --> 00:23:19,960
those types of secondary effects that actually quite important to the real world.

286
00:23:19,960 --> 00:23:24,520
And some of the work we're seeing in the criminal justice system is exactly this.

287
00:23:24,520 --> 00:23:32,600
The algorithm is doing an analysis for, does not matter what. So the algorithm was fine,

288
00:23:32,600 --> 00:23:39,880
but the post-hoc implementation was causing, was amplifying, very, very tiny difference in ways

289
00:23:39,880 --> 00:23:44,600
that were of well and sustainable. That is important going forward as we start thinking about

290
00:23:44,600 --> 00:23:50,280
where algorithms fit in systems. I guess it's a long way of saying yes, it's more complex,

291
00:23:50,280 --> 00:23:55,320
but it gives you more degrees of freedom to play, to try to intervene to correct mistakes.

292
00:23:56,040 --> 00:24:01,080
Right, right, right. So you're taking a step back and dealing with these kinds of problems,

293
00:24:01,720 --> 00:24:08,520
you've mentioned two tools that you use. One is applying frameworks like Belmont. Another is

294
00:24:08,520 --> 00:24:15,240
taking a systems thinking approach to looking at the problem. Are there other tools that you

295
00:24:15,240 --> 00:24:22,520
use in this way? I mean, I'm still developing my thinking. Could I say that there are the tools?

296
00:24:23,320 --> 00:24:31,880
Not so much. There is another space in which I think people that I've been trying to think of

297
00:24:31,880 --> 00:24:37,800
frameworks. And this is the part question of how do you regulate the free use of algorithms?

298
00:24:37,800 --> 00:24:42,120
Should you regulate the use of algorithms? And even if you wanted to, how would that work?

299
00:24:42,120 --> 00:24:48,600
And so the standard, the standard approach to regulation has always been in polls laws.

300
00:24:50,040 --> 00:24:54,280
If it was laws, either you can do this or you can't do that.

301
00:24:55,640 --> 00:25:01,960
Increasingly for spaces where compliance is hard to, to assure, there is the idea of

302
00:25:01,960 --> 00:25:07,400
where compliance is hard to assure, where norms are actually more important than hard regulations.

303
00:25:07,400 --> 00:25:12,040
The idea of soft governance approaches becomes more, is beginning to get more attention.

304
00:25:12,920 --> 00:25:18,600
I think the regulatory scheme, the frameworks for regulating the proper,

305
00:25:18,600 --> 00:25:26,200
or the beneficial use of algorithms, that's still there a lot there, but there's a lot of

306
00:25:26,200 --> 00:25:32,280
thinking going on there, but not much. There isn't as much hard and I guess there aren't as much

307
00:25:32,280 --> 00:25:38,920
proven frameworks that I can point to. If you kind of take that thought exercise around

308
00:25:38,920 --> 00:25:48,360
soft regulation and establishing norms and try to take on a broad issue like algorithmic

309
00:25:48,360 --> 00:25:55,000
transparency, you end up in likely a very different place than something like a GDPR.

310
00:25:55,960 --> 00:26:01,320
How would you compare and contrast the solution that you might end up with a

311
00:26:01,320 --> 00:26:07,240
soft regulatory approach and something like GDPR? That's an interesting question.

312
00:26:07,960 --> 00:26:16,920
I imagine that this is me giving my best educator guess and this is really the correct

313
00:26:16,920 --> 00:26:23,160
answer to this. I imagine that the level of soft regulations that you're trying to build norms

314
00:26:23,160 --> 00:26:29,320
and trying to build consensus that a broad coalition of people can agree to, a broad coalition

315
00:26:29,320 --> 00:26:36,040
of players can agree to. The general thing about broad coalition is decision-making that

316
00:26:36,040 --> 00:26:41,640
that record based on broad coalition building is it's going to be a weaker regulatory framework,

317
00:26:42,280 --> 00:26:47,880
but if you're working in a highly polarized, highly fractionated space, maybe that's the best

318
00:26:47,880 --> 00:26:53,480
you can do. GDPR is interesting because they come at it from a strong normative frame,

319
00:26:53,480 --> 00:27:02,360
the idea that the consumer is key, that privacy is not an absolute right, but it's a highly

320
00:27:02,360 --> 00:27:09,800
prioritized right, and that the people are central as opposed to the companies. That controls,

321
00:27:09,800 --> 00:27:14,920
and it's an interesting and important perspective. It's a perspective that's been missing from

322
00:27:14,920 --> 00:27:21,480
the history of American technology innovation, specifically when it comes to data driven,

323
00:27:21,480 --> 00:27:27,400
data driven innovation, but I don't think a sub-governance framework would have come up with that.

324
00:27:30,600 --> 00:27:41,000
For the yet word, here's the driving follow-up question. There are spaces where trying to assert

325
00:27:41,000 --> 00:27:48,600
a primary normative frame is the best solution to getting compliance. So one of the things about

326
00:27:48,600 --> 00:27:54,680
GDPR is there is a question of whether it's going to drive fragmentation and regulation.

327
00:27:54,680 --> 00:28:01,160
So you have United States, California is following GDPR's footsteps, but the United States has a

328
00:28:01,160 --> 00:28:06,840
federal system doesn't seem to be able to or want to or have the want to use a strong term to apply

329
00:28:06,840 --> 00:28:12,600
to a system. It doesn't seem to be moving in that general direction. China is certainly not moving

330
00:28:12,600 --> 00:28:24,040
in the same direction as GDPR. If you try to impose global laws concerning data privacy,

331
00:28:24,040 --> 00:28:29,240
and you come at it with that strong normative foundation like what the GDPR does,

332
00:28:29,960 --> 00:28:35,720
it's an interesting exercise to see if that was going to be a viable global

333
00:28:35,720 --> 00:28:43,400
up switch to regulation. Right. Particularly in light of the global nature of the technologies

334
00:28:43,400 --> 00:28:49,160
that they're aiming to govern. Interesting. So you spend roughly half your time on

335
00:28:49,160 --> 00:28:57,640
Kennedy's broad ethical and policy issues, and another half of your time at Rand on model

336
00:28:57,640 --> 00:29:05,240
development and kind of rolling up your sleeves and building systems or proofs of concepts for clients.

337
00:29:06,760 --> 00:29:15,160
Is that work also in this domain? Are you exploring explainability models and transparency?

338
00:29:15,160 --> 00:29:20,360
That kind of thing. I am a bit more exploratory tree in my technical work,

339
00:29:20,360 --> 00:29:28,280
largely because for me, the technical development keeps me honest. When I try to talk about

340
00:29:28,280 --> 00:29:33,320
the policy, regulatory and ethical framing, if I don't have a technical understanding,

341
00:29:33,320 --> 00:29:40,920
I think I am generally off base if I don't keep that understanding. But in terms of the actual

342
00:29:40,920 --> 00:29:49,960
types of technical models I tend to pursue. So there's stuff on the explanation. There isn't

343
00:29:49,960 --> 00:29:57,320
as much. So DARPA has a program on this, a program unexplainably. I talk to them a couple of

344
00:29:57,320 --> 00:30:06,200
times. Everybody talks to them to see what they're thinking. But I tend to, my perspective on it

345
00:30:06,200 --> 00:30:10,840
is a little bit, I think the models I tried to build so I addressed that a little bit more

346
00:30:11,640 --> 00:30:18,360
exploratory tree than what is probably the raining stuff. In general, I think where I've

347
00:30:18,360 --> 00:30:24,040
spent most of my thinking, most of my technical thinking, I've been on two things,

348
00:30:25,000 --> 00:30:30,760
older models. So this issue of explainability in machine learning models,

349
00:30:31,560 --> 00:30:40,600
it's sort of, I work with people who grew up in the heyday of the ATV AI, AI boom. It's sort of

350
00:30:40,600 --> 00:30:45,640
throwing back to that phase. So back then I had this system, like the systems that were super

351
00:30:45,640 --> 00:30:49,960
explainable, they could do forward inference, like most of our machine learning models can do.

352
00:30:49,960 --> 00:30:54,600
But they could also do backward inference in the sense that you give it a state of an output state

353
00:30:54,600 --> 00:30:59,800
and it can give you a set of likely input states that have caused, which is one form of explanation.

354
00:30:59,800 --> 00:31:04,680
So in the sense, those older systems were better at it than the current systems. And so one of

355
00:31:04,680 --> 00:31:11,960
the things I have tried to be explored in the space of explorability is this hybrid of older

356
00:31:11,960 --> 00:31:19,080
Xper system style models with current statistical machine learning models. This is not like

357
00:31:19,960 --> 00:31:25,880
heresy in the sense that you have people like Jerry, let's go back at Stanford doing this thing

358
00:31:25,880 --> 00:31:32,360
where he is using decision tree models to try to create a meta model summary of a black box model,

359
00:31:32,360 --> 00:31:40,120
however that black box is. And what my approach is to refer more to what they call further systems.

360
00:31:40,120 --> 00:31:46,200
So for the systems where these old experts, the expert systems based model, they were more,

361
00:31:46,200 --> 00:31:53,240
they were used often for linguistic mapping between even then parts of sets of systems.

362
00:31:53,720 --> 00:31:58,680
And so the idea there is to take a black box model, create a gray box model that is a bit more

363
00:31:58,680 --> 00:32:04,600
expletable that sort of summarizes that black box model. And sort of see if you can get better

364
00:32:04,600 --> 00:32:11,080
explanations, isn't that a gray box model. I think the couple of, so I mentioned Leskovich as

365
00:32:11,080 --> 00:32:17,320
I approached to that, there was also Lipton's work on born-again networks, I think trying to address

366
00:32:17,320 --> 00:32:23,960
it in a similar way. But these are all like a very tree type of model to see what's possible,

367
00:32:23,960 --> 00:32:29,720
what's feasible. But the other, the one I actually care about, well, care about is in my care about

368
00:32:29,720 --> 00:32:38,440
all these models. The one I actually spent a lot of time thinking about is the use of machine learning

369
00:32:39,400 --> 00:32:47,400
models or at least AI-stamp models to try to answer strategic questions in complex adaptive systems.

370
00:32:47,400 --> 00:32:54,920
So you have the situation where you have a stick, let me try to give it a useful example.

371
00:32:54,920 --> 00:33:01,400
So half, I went to paper a while ago, like a year or two ago, where you had a bunch of experts

372
00:33:01,400 --> 00:33:07,320
across the world who have thought very carefully about what does it take for local population to

373
00:33:07,320 --> 00:33:12,840
begin to support terrorism. So the common public support for industry and terrorism model.

374
00:33:12,840 --> 00:33:19,080
So you have all these experts pulling their insights to come up with interesting factors

375
00:33:19,080 --> 00:33:24,280
and interesting dynamics for how this might work. But that's all linguistic, that's all

376
00:33:24,280 --> 00:33:28,120
unstructured data. So one of the things I've been trying to do, and if you think about what

377
00:33:28,120 --> 00:33:33,800
RAND does, RAND is very focused on trying to improve decision-making, even in those types of

378
00:33:33,800 --> 00:33:39,320
complex domains where expertise is rare and is always debilistly defined, debilist in the sense

379
00:33:39,320 --> 00:33:45,080
that it's not, it's not crisply defined in terms of data. So I'm trying to come up with models,

380
00:33:45,080 --> 00:33:53,400
AI models that take those types of expertise and create positive models that allow you to explore

381
00:33:53,400 --> 00:33:59,640
and make that a strategy decision. And there's an old model, an old model from... Actually,

382
00:33:59,640 --> 00:34:05,080
it's not that old, it's coming back, it's being used in medical diagnosis. It's called a fuzzy

383
00:34:05,080 --> 00:34:13,320
cognitive maps. So it allows you to apply... Fuzzy cognitive maps? Okay. It allows you to formalize

384
00:34:13,320 --> 00:34:20,200
a complex decision space and allows you to explore what are the possible outcomes for different types of

385
00:34:20,200 --> 00:34:28,200
input. Actually, it's very interesting. It says that, yeah, I also did it for... So I'm

386
00:34:28,200 --> 00:34:34,200
interested in this because it allows me to take my quantitative skills and apply to all these

387
00:34:34,200 --> 00:34:41,560
areas of research that have formerly... That usually has a quantitative analysis. So I did it for

388
00:34:41,560 --> 00:34:49,160
terrorism stuff, like trying to understand the dynamics of the support for terrorism. I recently

389
00:34:49,160 --> 00:34:58,120
did it for a model of great amount of sense to see this trap. I don't know if I pronounced that

390
00:34:58,120 --> 00:35:02,760
properly because I always write it up. I never actually talk about it. It's that to keep it in this

391
00:35:02,760 --> 00:35:09,560
trap. So the idea that I only have two powers, one rising and one previously dominant,

392
00:35:09,560 --> 00:35:18,040
that there is a tendency towards that interaction engine in war. So that's how the

393
00:35:18,040 --> 00:35:23,800
historians discuss it. But it'd be interesting to see if there is data to support that. So you

394
00:35:23,800 --> 00:35:30,600
have to create a model that represents that dynamics and explorers using our cross-term based methods

395
00:35:30,600 --> 00:35:35,960
to see whether it is actually the case that if dynamics are such as the historians describe,

396
00:35:35,960 --> 00:35:40,280
you get an increased likelihood for war or not. And both of those examples that you

397
00:35:40,280 --> 00:35:47,320
decided, you know, particularly pre-having previously mentioned, you know, unstructured data,

398
00:35:47,320 --> 00:35:56,600
natural language. The spaces are so incredibly broad. How do you begin to constrain them such

399
00:35:56,600 --> 00:36:06,200
that you can, you know, start to model? So this is where it's really valuable to work in a

400
00:36:06,200 --> 00:36:14,760
building full of non-engineers, experts and PhDs who are non-engineers and anthropologists.

401
00:36:14,760 --> 00:36:22,600
Because what happens is I walk into the building, somebody walks up to me, either political

402
00:36:22,600 --> 00:36:26,840
scientists, usually political scientists or anthropologists, they walk up to me and say, oh,

403
00:36:26,840 --> 00:36:31,640
we have this. This problem will be exploring probably over lunch. And we talk through it

404
00:36:31,640 --> 00:36:37,880
and I start thinking because I'm an engineer, but I by temperament and by character. I start

405
00:36:37,880 --> 00:36:42,520
thinking, okay, what types of models, what types of quantitative models can I use to try to say

406
00:36:42,520 --> 00:36:50,760
something interesting about this problem they are dealing with? That's how that usually starts.

407
00:36:50,760 --> 00:36:56,600
And growing back and forth, taking my intuition, taking my modeling with their expertise,

408
00:36:56,600 --> 00:37:03,960
is usually some sort of solution, some sort of an approach begins to emerge. And because

409
00:37:03,960 --> 00:37:09,880
it's, because this type of interaction is actually quite novel, at least it's, it's,

410
00:37:09,880 --> 00:37:15,640
it's something people tend to avoid because it's interdisciplinary work is sort of annoying,

411
00:37:15,640 --> 00:37:20,600
the language is different. It really is annoying, I guess. I'm not joking on this.

412
00:37:20,600 --> 00:37:28,040
So it's much of what I end up doing is really low-hanging through it because if I claim to solve

413
00:37:28,040 --> 00:37:34,920
the really hard problems in those in the other spaces that require the level expertise,

414
00:37:34,920 --> 00:37:40,360
I can't rightly, I can't rightly claim to, even if I'm talking to a lot of experts.

415
00:37:40,360 --> 00:37:46,200
Okay, okay. Those are my, those are those those two explanation and positive

416
00:37:46,200 --> 00:37:51,000
explanations. Those are sort of my, my pet project. I spend a lot of time thinking and developing

417
00:37:51,000 --> 00:37:58,440
models on that. For actual work, we do things like recently we had some work on use of

418
00:37:59,560 --> 00:38:04,040
reinforcement learning and, again, and generative research networks to try to develop

419
00:38:05,160 --> 00:38:11,720
planning solutions in a, in an agent-based simulation model. So is that type of stuff that

420
00:38:11,720 --> 00:38:20,200
keeps me sharp and keeps me abreast, but it's not, I don't need to produce like a production

421
00:38:20,200 --> 00:38:31,400
ready code on that. The whole space of applying machine learning to the strategic decision-making

422
00:38:31,400 --> 00:38:39,720
is quite a fascinating one. Can you maybe take us into a little bit more detail for either

423
00:38:39,720 --> 00:38:45,240
the examples that you gave? You know, what does the data look like that you ended up using?

424
00:38:46,200 --> 00:38:53,880
You know, what's kind of the process? Okay, so, so the details, so what happens here is that

425
00:38:53,880 --> 00:39:00,600
you're, you're necessarily doing a high-grade type of model. Like you're trying to create a model

426
00:39:00,600 --> 00:39:08,120
that takes either equal parts, expert, expert, elicitated, elicitated structure and data

427
00:39:08,120 --> 00:39:13,480
in form structure and trying to merge it into one. The last people I wrote on this,

428
00:39:14,680 --> 00:39:23,000
we had, we had, for example, we demonstrated using, like, our explicitization to create a map,

429
00:39:23,000 --> 00:39:27,800
a quantitative map that we could then use for forecasting and prediction, but then we used

430
00:39:28,840 --> 00:39:35,960
Google Trans Data to estimate, to essentially time series data, to estimate this transfer

431
00:39:35,960 --> 00:39:41,400
of certain types of connections. So that would be some typo, Bob. The technical term is

432
00:39:41,400 --> 00:39:45,080
differential, helping in learning, but in reality, it's just some form of advanced,

433
00:39:46,040 --> 00:39:51,560
advanced, as it's not the right word. It's just an involved, also, device learning technique.

434
00:39:52,520 --> 00:39:57,560
In general, there's that we have, and half the time this provision has to, some of this provision

435
00:39:57,560 --> 00:40:03,400
has to confirm expert, expert, expert, elicitation. So it's sort of a semi-supervised in the sense that

436
00:40:03,400 --> 00:40:10,040
there is expert guidance for, say, the directionality of the edges, and so the maps I'm talking about,

437
00:40:10,040 --> 00:40:16,280
they're essentially director graphs. So there's expert guidance on the direction of the edges,

438
00:40:16,280 --> 00:40:23,240
in that director graph, and then there is data, lots of providers learning using data,

439
00:40:23,240 --> 00:40:31,960
to try to tune the strength of those connections. There are other ways to do it. This is the simplest

440
00:40:31,960 --> 00:40:42,120
way I've found. It begins to push into the renal of causal inference using Bayesian belief networks.

441
00:40:42,120 --> 00:40:47,000
So they are trying to, I guess it's causal knowledge to discover that's technically what you

442
00:40:47,000 --> 00:40:52,840
need to have to try to do. You're trying to take a bunch, if you had no expert whatsoever,

443
00:40:52,840 --> 00:40:57,960
you're trying to take a bunch of data, a bunch of time series data, and properly,

444
00:40:57,960 --> 00:41:06,600
properly, in fare using algorithms like on junction tree algorithms or expectation

445
00:41:06,600 --> 00:41:13,800
propagation. They're trying to correctly fare both, mostly the strength of the connections,

446
00:41:13,800 --> 00:41:22,520
or if you're doing an actual knowledge discovery, the actual graph, the fantasy relationships

447
00:41:22,520 --> 00:41:28,840
between nodes and between our variables. But because I'm working at a different dynamic,

448
00:41:28,840 --> 00:41:33,800
it's just the same algorithms to use the same algorithms to treat them differently.

449
00:41:33,800 --> 00:41:39,080
So I guess the best summary there is, it's a form of causal knowledge to discover

450
00:41:39,080 --> 00:41:43,480
causal inference using a different type of, or different type of model.

451
00:41:43,480 --> 00:41:47,800
I'm so curious, though, about the data, for example, in the case of this,

452
00:41:47,800 --> 00:41:53,720
trying to even forget the way you frame the problem with the terrorism.

453
00:41:53,720 --> 00:42:01,320
Oh, probably so. So that first paper, that I will not, that was entirely expert driven,

454
00:42:01,320 --> 00:42:06,760
expertise driven, so we had a lot of data driven. Oh, it's a different type of data,

455
00:42:06,760 --> 00:42:11,400
in this end. Meaning, or rather not statistical. Not statistical, exactly.

456
00:42:11,400 --> 00:42:18,200
So it's a form of, the form of, we had a corpus, a language corpus. The way technically you'd

457
00:42:18,200 --> 00:42:25,800
want to do this in a fully, in a fully, in a full project would be stick at natural language corpus

458
00:42:25,800 --> 00:42:31,960
that's relevant to the topic of interest in this terrorism, create, identify the relevant

459
00:42:31,960 --> 00:42:38,920
keywords. So in this case, there are things like keywords related to how the local population

460
00:42:38,920 --> 00:42:44,760
feels, the group is how legitimate the field of group is, how acceptable the risks are.

461
00:42:44,760 --> 00:42:52,280
So you create filters, language filters for the types of, for the types of identified factors.

462
00:42:52,280 --> 00:42:58,760
And then you just basically look for core currents of those, of those, of those filter terms

463
00:42:58,760 --> 00:43:06,440
with other terms. And core currents will agree, the more, the more to, to types of terms,

464
00:43:06,440 --> 00:43:10,840
core curve, the more you put, you increase the edge of the strength of the, of the connection

465
00:43:10,840 --> 00:43:19,320
between those two things. So in some sense, it's, it's using, it kind of almost a graphical

466
00:43:19,320 --> 00:43:27,320
approach to sentiment analysis across this corpus. Do you have any recommendations for folks that

467
00:43:27,320 --> 00:43:36,680
are interested or intrigued about this, you know, these fuzzy models or applications of machine

468
00:43:36,680 --> 00:43:42,520
learning to strategic decisions or game theoretical applications? Where should folks start looking?

469
00:43:44,280 --> 00:43:50,920
So for the fuzzy models, for the fuzzy cognitive maps, I think the best, the best discussion

470
00:43:50,920 --> 00:43:58,840
is still, yeah, there is a summary book, I think by El Penicchi, Papadro, Drew, in Greece,

471
00:43:59,560 --> 00:44:05,880
it's out there. I can, I can send a link later on. It's just a survey of fuzzy cognitive maps,

472
00:44:05,880 --> 00:44:13,560
theories and applications, recent applications. And on the, the issue of, on the issue of causal

473
00:44:13,560 --> 00:44:21,400
inference, I would, I would say the books like, I'm just name, corals, books on causality,

474
00:44:23,640 --> 00:44:29,160
in a different, in a different tradition, I would say maybe, Rubin, it's not Rubin's books,

475
00:44:29,160 --> 00:44:33,880
there are a couple of books on propensity score, score methods that are relevant.

476
00:44:34,600 --> 00:44:38,920
So I have a ton of books, but I don't remember, I don't always remember the name,

477
00:44:38,920 --> 00:44:45,720
I never tried, I didn't remember them in the spot. On the issue of strategic, strategic decision

478
00:44:45,720 --> 00:44:52,120
making, using machine learning, I think, so that's definitely an interesting area,

479
00:44:52,120 --> 00:45:00,600
largely because I feel like that underserved in the literature. So I refer to, I refer

480
00:45:00,600 --> 00:45:06,440
increasingly, at least while I was working on that particular project, to a really old book by

481
00:45:06,440 --> 00:45:12,760
Axe-Rod, I believe it's David Axe-Rod, the structure of decision making. And for that style of

482
00:45:12,760 --> 00:45:19,400
graphical, graphical, coalesce of expert knowledge, I haven't seen as much work, but they're

483
00:45:19,400 --> 00:45:25,560
paper, the fuzzy cognitive map papers do that. So we had the first paper that caused a cognitive

484
00:45:25,560 --> 00:45:32,440
map of how, how the economics and appetite in South Africa interrelated. So most of the

485
00:45:32,440 --> 00:45:38,920
interesting strategic uses of fuzzy cognitive maps are still in papers, I think. There is another

486
00:45:38,920 --> 00:45:48,040
area of, so this algorithmic game theory idea, the idea of mechanism design to try to solve

487
00:45:48,040 --> 00:45:54,680
some of the incentive problems you're seeing. I think Tim Rothgarten has a book called

488
00:45:54,680 --> 00:46:04,040
algorithmic interaction. I think that's going to be an increasingly important area when we're

489
00:46:04,040 --> 00:46:10,920
thinking about feedback systems and decision making. You're going to need some type of mechanism

490
00:46:10,920 --> 00:46:17,880
design process to try to incentivize people to act in specific ways, and there you get to,

491
00:46:17,880 --> 00:46:23,320
with the use of algorithmic game theory, you get to throw in all your quantitative skills to try

492
00:46:23,320 --> 00:46:29,320
to improve decision making of a game theory to set it. I think Rothgarten's book is the

493
00:46:29,320 --> 00:46:36,120
best in that space. Well, Oshonday, thank you so much for taking the time to chat with me about

494
00:46:36,120 --> 00:46:44,840
this stuff, really interesting stuff. Let's talk about it. All right, everyone, that's our show

495
00:46:44,840 --> 00:46:50,760
for today. For more information on Oshonday or any of the topics covered in this episode,

496
00:46:50,760 --> 00:46:58,120
head over to twimlai.com slash top slash 182. For more information on the entire deep learning

497
00:46:58,120 --> 00:47:05,640
and daba podcast series, visit twimlai.com slash endaba 2018. Thanks again to Google for their

498
00:47:05,640 --> 00:47:12,600
sponsorship of this series. Be sure to check out the 2019 AI residency program at g.co slash AI

499
00:47:12,600 --> 00:47:24,040
residency. As always, thanks so much for listening and catch you next time.

