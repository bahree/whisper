WEBVTT

00:00.000 --> 00:25.760
Hey everyone, hope you all had a wonderful holiday.

00:25.760 --> 00:30.520
For the next few weeks we'll be running back the clock with our second annual AI Rewind

00:30.520 --> 00:32.120
series.

00:32.120 --> 00:36.960
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,

00:36.960 --> 00:43.120
and other developments that made us splash in 2019 in key fields like machine learning,

00:43.120 --> 00:49.480
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.

00:49.480 --> 00:55.680
Be sure to follow along with the series at twomolai.com slash rewind 19.

00:55.680 --> 01:00.080
As always, we'd love to hear your thoughts on this series, including anything we might

01:00.080 --> 01:01.160
have missed.

01:01.160 --> 01:06.600
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via

01:06.600 --> 01:11.560
a comment on the show notes page you can find at twomolai.com.

01:11.560 --> 01:13.920
Happy New Year, let's get into the show.

01:13.920 --> 01:22.120
Alright everyone, we are here for AI Rewind, our second annual walk through the top trends

01:22.120 --> 01:28.480
and developments in machine learning and AI, and this time I am with Amir Zameer.

01:28.480 --> 01:33.760
Amir is an assistant professor of computer science at the Swiss Federal Institute of Technology

01:33.760 --> 01:35.760
or EPFL.

01:35.760 --> 01:42.960
In fact, Amir at this current moment for another three days is a postdoc at Affiliated

01:42.960 --> 01:49.800
with Stanford and UC Berkeley, where he was when we first spoke with him back in July

01:49.800 --> 01:55.000
of 2018, Amir, welcome back to the twomolai podcast.

01:55.000 --> 01:56.000
Thanks.

01:56.000 --> 01:57.000
Great to be here.

01:57.000 --> 02:02.680
Yeah, I'm really excited to dig into this conversation about what's new, your take

02:02.680 --> 02:07.040
on 2019 from a computer vision perspective.

02:07.040 --> 02:13.360
And so to kind of get us started with that, why don't we just take broad brushstrokes?

02:13.360 --> 02:15.080
What's your take on 2019?

02:15.080 --> 02:17.000
Thanks for having me.

02:17.000 --> 02:23.240
It was another exciting year, in my opinion, for fields with an AI and including computer

02:23.240 --> 02:26.680
vision, you know, complex conferences are expanding.

02:26.680 --> 02:28.920
So there's more talent coming in, more energy.

02:28.920 --> 02:38.120
I think CPR 2018 was 6,000 roughly, the number of attendees in 2019 was about 10,000.

02:38.120 --> 02:40.400
So we will see, and that was six months ago roughly.

02:40.400 --> 02:44.760
So we will see how it's going to be in 2020, but that expansion in size.

02:44.760 --> 02:49.960
How does that growth rate compare to NERPs, which is the one that gets a lot of headlines?

02:49.960 --> 02:50.960
Right.

02:50.960 --> 02:55.400
NERPs was, I came back from NERPs like roughly two weeks ago.

02:55.400 --> 03:00.120
I believe it was 13,000, but again, they are like six months apart.

03:00.120 --> 03:04.240
So, you know, we will see how CPR will be.

03:04.240 --> 03:10.640
But I think roughly the same size, maybe NERPs, somewhat bigger because it generally includes

03:10.640 --> 03:15.360
many different areas, not just vision, not just NLPMs, so on.

03:15.360 --> 03:19.920
Many vision people are there, myself included, but yeah, but they are a huge conference.

03:19.920 --> 03:22.840
It's big enough that you won't see your friends anymore.

03:22.840 --> 03:28.080
And are the paper submissions as CVPR growing as quickly as NERPs?

03:28.080 --> 03:29.080
Yes.

03:29.080 --> 03:30.080
Yes.

03:30.080 --> 03:31.080
Yeah.

03:31.080 --> 03:32.080
I don't know the exact same year.

03:32.080 --> 03:38.720
I'm an area chair, so I should know this number, but all in, all in what I'm sure about

03:38.720 --> 03:43.200
and is that there is a really sharp expansion.

03:43.200 --> 03:48.280
And, you know, to some extent that presents a problem for us academics because we have

03:48.280 --> 03:54.080
to find reviewers for this load of like papers and so on.

03:54.080 --> 03:55.080
But that's fine.

03:55.080 --> 03:57.480
That's a good problem to have.

03:57.480 --> 03:59.080
The overall outcome is positive.

03:59.080 --> 04:02.760
I'll be at some variances.

04:02.760 --> 04:08.520
And, you know, the growth was in a way, this was proportional because now, for the past

04:08.520 --> 04:12.160
like four or five years, there was a huge interest.

04:12.160 --> 04:15.240
And so we have a lot more like young talent in the field.

04:15.240 --> 04:20.600
For the same reason, they're young and new, so we don't have as many seasoned reviewers

04:20.600 --> 04:22.480
to value the papers for us.

04:22.480 --> 04:27.200
So to some extent, the review quality has a little bit of variance in it compared to

04:27.200 --> 04:28.360
few years ago.

04:28.360 --> 04:32.640
But again, like I said, that's a good problem to have because the field is just generating

04:32.640 --> 04:37.400
more results, so a little bit of variances, in my opinion, acceptable.

04:37.400 --> 04:42.920
So the field is growing dramatically as our other areas in ML and AI.

04:42.920 --> 04:44.520
What else is happening in vision?

04:44.520 --> 04:45.520
Right.

04:45.520 --> 04:51.800
In terms, more technically, I think, you know, we see a few trends that they are not too

04:51.800 --> 04:56.920
exclusive to 2019, but I think some of them are like maturing up in 2019.

04:56.920 --> 05:02.720
One metal trend that I see for sure is that we see a lot of mixes of areas like vision

05:02.720 --> 05:06.320
plus something else, like vision pros, graphics.

05:06.320 --> 05:11.520
I think a nominal example of that are all these like image synthesis pipelines, either

05:11.520 --> 05:14.560
GAN-based and whatnot.

05:14.560 --> 05:19.800
But the pipelines that essentially generate an image, that's, in a way, it's a graphics

05:19.800 --> 05:24.440
problem because graphic is about like generating something, good looking, that you put under

05:24.440 --> 05:26.480
your screen rendering things.

05:26.480 --> 05:28.400
And vision was the inverse of the problem.

05:28.400 --> 05:31.440
You already have an image and you want to understand it.

05:31.440 --> 05:38.760
But these two areas got blended together, and I think the first time in vision that I

05:38.760 --> 05:44.560
saw a reasonably working example was the paper picks the picks a few years ago.

05:44.560 --> 05:49.280
And after that, it became really popular and cycle GAN and so on.

05:49.280 --> 05:53.440
Those other papers actually came primarily from the vision community and of course the graphics

05:53.440 --> 05:55.720
community worked a lot on it too.

05:55.720 --> 06:01.400
So mixes of areas, vision pros, graphics, I guess we'll discuss it partially when we

06:01.400 --> 06:03.360
get into more details.

06:03.360 --> 06:05.440
Vision plus robotics is expanding.

06:05.440 --> 06:10.320
I think it's one of the areas to watch for sure, vision plus like adversarial robustness

06:10.320 --> 06:11.320
literature.

06:11.320 --> 06:17.760
I think that's something that we, not many of us, saw it coming, but in a way it actually

06:17.760 --> 06:24.960
makes sense that how the algorithms, basically there was a line of research going forward

06:24.960 --> 06:28.840
on making machine learning systems more robust and that there are examples where like

06:28.840 --> 06:37.480
poking concerns to people and turns out that if you have more robust algorithms for processing

06:37.480 --> 06:44.800
visual data, they are more useful sort, just processing non adversarial literature and

06:44.800 --> 06:47.240
non adversarial content as well.

06:47.240 --> 06:52.640
Like if you have an image synthesis pipeline, it works better if it was robustified even

06:52.640 --> 06:56.000
though they, even if you don't mess with the input anymore.

06:56.000 --> 06:57.760
We can discuss it in more details and go forward.

06:57.760 --> 07:04.320
But I think generally the trend of mixing different areas with vision is increasingly popular

07:04.320 --> 07:08.000
and I think there's actually a healthy reason to this.

07:08.000 --> 07:16.320
I think it's a realization of the fact that vision is a service to some downstream goal.

07:16.320 --> 07:21.520
It's a very powerful skill, but we don't usually observe the world for the purpose of just

07:21.520 --> 07:24.800
absorbing, like just understanding what's going on.

07:24.800 --> 07:29.440
We usually have an intent in mind, like we understand the world with, you know, when I,

07:29.440 --> 07:35.320
when I get up in the morning, when I open my eyes, I intend to get out of bed safely

07:35.320 --> 07:38.320
and navigate myself out of the bedroom.

07:38.320 --> 07:44.920
So the vision is a very practical skill and so we cannot really make that independent,

07:44.920 --> 07:48.280
the research that we do on vision of these downstream skills.

07:48.280 --> 07:54.760
So vision plus X is, to me, I see that as realization of that fact, especially in

07:54.760 --> 07:56.680
the context of robotics.

07:56.680 --> 08:01.840
The reason we mix vision and robotics together is that our robots need to have a complex

08:01.840 --> 08:05.640
understanding of the world acquired through the cameras.

08:05.640 --> 08:13.080
So whatever the vision pipeline outputs, it should be in a way curated to best support

08:13.080 --> 08:15.240
the downstream goal of a robot.

08:15.240 --> 08:19.360
I particularly don't care, for instance, if I have a robot in my home and it can detect

08:19.360 --> 08:23.480
all the objects and do all sort of like complex things.

08:23.480 --> 08:28.400
I don't really care how the vision pipeline works if the downstream goal of a robot's

08:28.400 --> 08:32.760
whatever it is, make the bed or do laundry or whatever it that is.

08:32.760 --> 08:37.760
If that works just fine, the vision can be as simple as it wants to be.

08:37.760 --> 08:44.840
And so it is really an end-to-end intertwined pipeline and I'm actually happy to see that

08:44.840 --> 08:54.000
these areas are mixing together because we can now do a more meditated design in our

08:54.000 --> 08:58.760
research and do vision in a way that it's more useful to our downstream goals.

08:58.760 --> 09:01.440
There are some caveats in this story, for instance, art.

09:01.440 --> 09:06.040
When I observe a painting, I'm watching it, I'm looking at it, but I'm appreciating it.

09:06.040 --> 09:12.160
I don't necessarily intend to do something with it, but generally speaking vision is a

09:12.160 --> 09:19.760
very practical skill and makes of areas in my opinion is a realization of that.

09:19.760 --> 09:26.040
Is there also an implication that vision has reached a level of maturity or meaning

09:26.040 --> 09:32.600
the core vision tasks have reached a level of maturity or performance that we can now

09:32.600 --> 09:38.040
even consider moving onto a real world, types of things and incorporating in these other

09:38.040 --> 09:45.360
areas, like we've solved enough of core vision to then mix it with these other fields?

09:45.360 --> 09:53.000
Yes and no, I would be really hesitant to say that we have solved enough core vision

09:53.000 --> 09:56.160
problems or we have solved them like fine.

09:56.160 --> 10:02.680
I actually think, let's say, the simplest example, probably the longest running problem

10:02.680 --> 10:08.400
in vision is, let's say, object detection. We are not a point that we can say that we

10:08.400 --> 10:13.960
can, with a high confidence, we can detect an object under varying lighting conditions

10:13.960 --> 10:17.880
and different contexts and so on and so forth.

10:17.880 --> 10:22.800
But at the same time, a huge amount of progress has been made, whether there's a way to make

10:22.800 --> 10:28.200
them useful, I think the answer is yes, and that's why many people that are not vision

10:28.200 --> 10:36.800
experts are actually using vision pipelines and we see APIs as well, and Microsoft or Google

10:36.800 --> 10:44.920
their APIs where you can do somewhat niche problems, but they're sufficiently reliable, something

10:44.920 --> 10:47.560
like face detection.

10:47.560 --> 10:53.520
So the fact that we have APIs that non-experts can use at the command line level, that basically

10:53.520 --> 11:01.440
means, yes, a certain level of maturity has been reached, but that does not mean that

11:01.440 --> 11:08.520
we can solve the problems now that seemingly are simple, because we have huge problems

11:08.520 --> 11:14.120
on that, like specifically 3D, like 3D perception, we have a lot of sensors for it and these

11:14.120 --> 11:18.200
sensors are expensive, such as lighter and so on and so forth.

11:18.200 --> 11:24.640
But understanding the content of an image in terms of 3D is far from like perfection,

11:24.640 --> 11:26.720
so there's a lot more work to do.

11:26.720 --> 11:34.360
We've got the field growing in size, we've got this mix of areas being explored, any other

11:34.360 --> 11:37.480
general trends that you're seeing in vision?

11:37.480 --> 11:42.400
I think in general, we see less and less fixed pattern recognition problems, like I said,

11:42.400 --> 11:46.160
object detection or segmentation and so on, that's going forward too and that's going

11:46.160 --> 11:47.560
forward to strong.

11:47.560 --> 11:53.040
We see progress, but the attention and energy from what I see is shifting towards these

11:53.040 --> 11:56.480
new horizons that are opening up right now.

11:56.480 --> 12:03.720
And I think there's generally also interest in unsupervised or self-supervised learning

12:03.720 --> 12:09.840
has been growing and it continued to grow and I think there were some good progress in

12:09.840 --> 12:16.440
the past year, but that's not something that I anticipate being solved anywhere in the

12:16.440 --> 12:17.440
near future.

12:17.440 --> 12:22.520
And we'll continue to see that as a significant area of research.

12:22.520 --> 12:29.680
Let's transition to some of the specific areas that you've identified to dig in deeper.

12:29.680 --> 12:37.760
We asked you to identify just a few papers that you thought represented the kind of progress

12:37.760 --> 12:42.480
that we've made in vision as a broad field in 2019.

12:42.480 --> 12:44.240
You found that particularly difficult to do.

12:44.240 --> 12:50.800
Can you talk a little bit about why and the areas that you've identified to discuss with

12:50.800 --> 12:51.800
us?

12:51.800 --> 12:56.640
There are a number of areas that I think it's worth discussion.

12:56.640 --> 13:00.680
Like I said, when you have a conference with 10,000 people and that's just CVPR.

13:00.680 --> 13:05.280
We have ICCV and ECCV as well, at least as top-tier vision conferences.

13:05.280 --> 13:10.520
I mean, you can imagine that there's a lot of research going on, thousands of papers

13:10.520 --> 13:11.520
coming out.

13:11.520 --> 13:20.160
It's hard to just pick out one or two or five papers because there's just more good work

13:20.160 --> 13:22.480
than those numbers and a handful of papers.

13:22.480 --> 13:26.480
But I think in terms of we can summarize the trends.

13:26.480 --> 13:34.840
One trend that I specifically see and I expect to grow is vision for robotics.

13:34.840 --> 13:40.760
So generally interesting robotics is growing, especially in what usually to go refer to

13:40.760 --> 13:46.880
as like robot learning these days, like reinforcement learning platforms or generally mix of learning

13:46.880 --> 13:50.480
based robotics with the classic robotics.

13:50.480 --> 13:53.040
And vision is part of this story.

13:53.040 --> 13:59.240
Like robot is a large intertwined framework and it involves multiple aspects like sensory

13:59.240 --> 14:05.120
hardware, perception, planning, control, and so on.

14:05.120 --> 14:10.720
And one common question is like what makes vision vision like robotics robotics then?

14:10.720 --> 14:14.560
These days, especially with the rise of robot learning where like one of the frequent claims

14:14.560 --> 14:17.480
is learning directly from raw pixels.

14:17.480 --> 14:21.760
So one question is like why we need like a specific vision algorithm at all?

14:21.760 --> 14:25.560
All we can do is like learning directly from pixels.

14:25.560 --> 14:34.600
So first like I draw the line between vision and let's say control, acknowledging that

14:34.600 --> 14:42.040
this sort of intertwined pipeline and clearly entwent, sensory observation comes in, goes through

14:42.040 --> 14:45.600
vision, goes through planning, goes through control, goes through hardware, and the robot

14:45.600 --> 14:47.360
in the end does something.

14:47.360 --> 14:49.800
So it's an end to end pipeline clearly.

14:49.800 --> 14:58.600
But to me as a vision researcher, what makes vision research is a robot has some sensory

14:58.600 --> 15:04.000
hardware like a camera and these sensors observe something and have some output.

15:04.000 --> 15:09.800
And the algorithm that just processes this output and extract some useful abstractions

15:09.800 --> 15:15.440
out of those high dimensional sensory data, that's the perception problem.

15:15.440 --> 15:21.680
So if the processing pipeline that black box sits closer to the sensors, it's more about

15:21.680 --> 15:25.480
understanding the world because those sensors are sensing the world.

15:25.480 --> 15:30.360
So the prior, the type of prior is that those black boxes user about like let's say the

15:30.360 --> 15:37.200
world is 3D or there is some dynamics in the world, there is like motion smoothness

15:37.200 --> 15:38.200
and so on.

15:38.200 --> 15:45.320
So perception uses these priors to to tame these high dimensional signal and then hands

15:45.320 --> 15:49.520
that over to the rest of the pipeline, let's say planning and so on.

15:49.520 --> 15:56.760
So the closer to the end we get, then comes the problem of control, like having these abstractions

15:56.760 --> 16:01.720
and having planned something, you want to issue an action that the robotic hardware executes

16:01.720 --> 16:03.120
for you.

16:03.120 --> 16:08.400
So to me, vision researchers are in the beginning of this end to end pipeline.

16:08.400 --> 16:12.800
They are more concerned about the world, they use the priors about the world and turn

16:12.800 --> 16:15.600
that into processing pipelines.

16:15.600 --> 16:22.720
And robotic researchers get these more abstractions and turn are closer to the agent, they understand

16:22.720 --> 16:27.080
the hardware, the robot better, so they control the robot so that they execute something

16:27.080 --> 16:32.360
that in a way that after doing multiple of these iterations, the outcome is achieved.

16:32.360 --> 16:39.080
So like I said, this is immediately brings these multiple fields together, vision plus

16:39.080 --> 16:46.600
robotics and it calls for us vision researchers to develop this vision box in a way that it

16:46.600 --> 16:49.800
can support the rest of the pipeline the best.

16:49.800 --> 16:56.040
It's not an open loop system, it's not like we receive a sensory observation like images

16:56.040 --> 17:01.240
and produce something, an object, frames and object detection and then we just say we don't

17:01.240 --> 17:05.560
care about what's going on after that, it is really important what comes after that.

17:05.560 --> 17:11.560
For instance, uncertainty estimation, it's really important for control theory people.

17:11.560 --> 17:16.520
So it makes sense that as vision researchers, we become aware of that and provide some form

17:16.520 --> 17:22.720
of uncertainty estimation that is tied with our detection and so on and so forth.

17:22.720 --> 17:30.760
So I think there are a number of clear developments in the field acknowledging this vision for

17:30.760 --> 17:33.040
robotics type of research.

17:33.040 --> 17:39.640
You drew a distinction early on in this between, and I want to get this distinction, I don't

17:39.640 --> 17:43.600
think I'm going to get this distinction correct, so please correct me, but it was something

17:43.600 --> 17:47.920
along the lines of vision versus learning from pixels.

17:47.920 --> 17:57.160
Right, well vision is learning from pixels, I see, so I understand, so yeah, so there's

17:57.160 --> 18:02.160
actually tabular also learning, it's a common word, it's not actually new, it's called

18:02.160 --> 18:07.400
like, I think it's Latin, it means like clean slate, so it's a system that let's say a robot

18:07.400 --> 18:13.680
that does tabular also learning in terms of vision, what it does is that, okay, I have the

18:13.680 --> 18:18.960
sensory output just raw pixels, and I'm going to learn directly from raw pixels, let's

18:18.960 --> 18:27.480
say, for instance, using a model free enforcement learning policy, and then you define your goal

18:27.480 --> 18:32.600
in terms of some reward function that rewards you when you do something right and penalizes

18:32.600 --> 18:38.120
you when you don't do that right, and then you hope that by direct interaction with the

18:38.120 --> 18:46.680
world, many, many, many data points, the system will learn how to do the job right, and that's

18:46.680 --> 18:53.280
generally what we see in the enforcement learning literature, especially like in model free

18:53.280 --> 18:59.200
enforcement learning, but there, the distinction is that learning directly from raw pixels,

18:59.200 --> 19:06.360
basically means the state of the world are these raw pixels, and by the way, I'm generalizing

19:06.360 --> 19:15.480
a lot here to get the general concept right, what vision is about when we are processing

19:15.480 --> 19:19.920
these raw pixels, instead of just using them raw, viewing them as like a 2D matrix that

19:19.920 --> 19:23.880
is coming from the camera, so that's what we know, vision is about like having some

19:23.880 --> 19:31.000
priors about the world, and instead of using this signal raw, we use those priors to extract

19:31.000 --> 19:35.440
some abstractions that are easier to understand, they're more interpretable, they're more efficient

19:35.440 --> 19:40.200
and so on, so an example, like I said, is the fact that the world is 3D, so when you look

19:40.200 --> 19:47.960
at an image, it's a projection 2D projection of a 3D world onto 2D plane, so it basically

19:47.960 --> 19:53.360
loses the 3D information, so if a robot is trying to make use of this 2D projection,

19:53.360 --> 19:59.600
it has to somehow recover that information, so either we hope that by these like millions

19:59.600 --> 20:05.720
and millions and millions of interaction, this robot actually understands 3D reconstruction

20:05.720 --> 20:09.320
to some extent at least to be able to solve the problem, because we know that some of

20:09.320 --> 20:13.360
these problems, especially in navigation, they need 3D perception, like you need to know

20:13.360 --> 20:18.680
how far the obstacle is to be able to avoid it, so there's no escape from that problem,

20:18.680 --> 20:23.760
at least at the course level, so either the hope is that by not providing these priors

20:23.760 --> 20:30.960
directly, and learning directly from pixels, the system by many millions of interactions

20:30.960 --> 20:37.040
it learns that, or some people like vision researchers, they use these priors about the

20:37.040 --> 20:43.000
world, they extract these abstractions out of their off pixels, and that's what the

20:43.000 --> 20:48.600
rest of the system brings them to their enforcement and learn policy uses, so you can learn

20:48.600 --> 20:54.360
them, you can view basic vision as identifying these like facts of the world, the fact that

20:54.360 --> 20:58.840
the world is 3D, the fact that there's like motion and smoothness, turn them into processing

20:58.840 --> 21:04.680
pipelines, so when the raw image comes in, it first goes through these abstractions, and

21:04.680 --> 21:09.320
then those abstractions are what the rest of the system, like the robotic system sees,

21:09.320 --> 21:13.720
so they're more interpretable, they're easier to tame, and you don't have to like redo

21:13.720 --> 21:22.880
this process every time that you turn a robot on during the learning phase, because these

21:22.880 --> 21:26.560
are just facts about the world that are generally true, it doesn't matter whether you're navigating

21:26.560 --> 21:31.520
or you're manipulating or you're finding an object or you're just going to GPS coordinate,

21:31.520 --> 21:38.520
you need some understanding of about 3D from the world, so it makes sense to turn that

21:38.520 --> 21:44.040
into more like the standalone problem where vision people solve and then plug that into

21:44.040 --> 21:45.040
the bigger pipeline.

21:45.040 --> 21:52.120
Now, when I talk to folks that are coming from the opposite direction or the other direction,

21:52.120 --> 21:59.280
the pixel-based learning and reinforcement learning, one of the things that they say

21:59.280 --> 22:05.520
is hot this year is model-based reinforcement learning, for example, where they're trying

22:05.520 --> 22:11.680
to learn a higher level model in the process of learning from the pixels to do some of

22:11.680 --> 22:16.880
the things that you're describing, and I guess the question that I'm asking is, does this

22:16.880 --> 22:24.960
mean that vision and the pixel-based approaches are converging or is it saying something else?

22:24.960 --> 22:30.280
So those models in the model-based RL, are they the same kind of abstractions that you're

22:30.280 --> 22:33.680
describing that are core to vision?

22:33.680 --> 22:40.560
Right, I mean, model-based RL or generally model-based everything, it's too general to the

22:40.560 --> 22:45.360
point that it can include anything in everything, so for instance, model-based, yeah, you can't

22:45.360 --> 22:49.920
have a model-based on the 3D of the world and that would become model-based something,

22:49.920 --> 22:57.520
so this doesn't necessarily mean there is a completely orthogonal approach to solving

22:57.520 --> 23:02.440
this problem that is going to replace vision or anything, generally when you have a model

23:02.440 --> 23:08.920
you're encoding some priors, either you're learning these priors or you're encoding them

23:08.920 --> 23:13.400
into the system, and like I said vision is more or less about that too, we define some abstractions

23:13.400 --> 23:18.200
that we believe are generally true about the world and we focus on solving those, up until

23:18.200 --> 23:23.560
a few years ago we were doing it completely independently of who uses the output like a robot,

23:23.560 --> 23:30.200
but now, like I said, with these areas being mixed, we are developing them in a more end-to-end

23:30.200 --> 23:36.040
manner, with the awareness that we need to develop them in a way that they're best useful to

23:36.040 --> 23:41.800
the rest of the pipeline, but yeah, in the end it's all the model, and there are people

23:41.800 --> 23:46.760
call different things in the model, there's like dynamics model, which is about prediction

23:47.560 --> 23:55.960
of the outcome of a certain action, so that helps with efficiency and so on, but for generally

23:55.960 --> 24:00.520
being able to solve this problem, you need to have a model and we are all in the business

24:00.520 --> 24:09.960
of developing that. So what are some of the specific papers that kind of exemplify this trend for

24:09.960 --> 24:17.400
you? Yeah, so for vision for robotics, like I said, the vision community primarily focused on

24:17.400 --> 24:24.600
the navigation, there are good reasons for that again, I think it's one of the most important

24:24.600 --> 24:28.680
things that are probably enlightening here is to look at the data, like where we get the data

24:28.680 --> 24:34.920
for for being able to solve, develop a vision model for an agent that is active in the world,

24:34.920 --> 24:40.120
like a robot that does navigation. So it's a big question, it's a very different from say

24:40.120 --> 24:45.080
an offline data set that sits in the hard drive of a computer, like I mentioned that, you take a

24:45.080 --> 24:51.480
picture once and that's it, you're annotated, you have no control over this pixel any longer,

24:51.480 --> 24:56.280
you cannot say like let me move a little bit in this image, like let me look at the same object

24:56.280 --> 25:01.640
from left or right and so on. So this pixel is like pre-recorded and is offline basically.

25:02.200 --> 25:07.800
Now by definition, active agent has some degree of freedom, so it can do something,

25:07.800 --> 25:14.040
it can move around for instance if it's an navigation agent. So there's a gap between

25:14.040 --> 25:21.560
like static and offline data sets and the type of data that's developing vision for robotics

25:21.560 --> 25:27.960
and balls. We basically need to have some online pipeline for generating data and that's usually

25:27.960 --> 25:34.840
why people use simulators. Again, till a couple of years ago, the simulators were primarily based on

25:36.120 --> 25:42.280
synthetic data. These are synthetic data meaning that a designer would sit down and model,

25:42.280 --> 25:47.880
let's say an apartment for you by putting together some cat models of chairs and tables and so on

25:47.880 --> 25:52.440
and they would be a computer graphics pipeline that renders that into pixels for you and we would

25:52.440 --> 25:57.480
use that as a source of data and that would solve the problem of being active because of course,

25:57.480 --> 26:04.520
you can just shift a little bit in this scene and re-render it. So it would give the possibility

26:04.520 --> 26:11.240
of a degree of freedom to the agent that is using the data but the main problem was that the data

26:11.240 --> 26:16.520
was not done from the real world. So there would be no guarantee that this would generalize and

26:16.520 --> 26:21.960
generally speaking since computer graphics is not completely solved yet. So the type of pixels

26:21.960 --> 26:27.640
that we get as a result of this are not fully photorealistic and even if they were photorealistic,

26:27.640 --> 26:32.920
the underlying semantics are coming from a designer and the designers have biases too.

26:33.960 --> 26:39.080
So if you look at the, let's say some of these models that existed in such data sets like Sun

26:39.080 --> 26:45.080
CG and so on, it's the moment you look at an image it's immediately clear that they are coming from

26:45.080 --> 26:50.360
a simulator they're not from the real world and what gives it out is not just the fact that the

26:50.360 --> 26:55.320
pixels are not like photorealistic is the fact that these models are usually too clean. No designer

26:55.320 --> 27:00.760
would sit down and design say a very messy bedroom and so on and so forth. So there was a

27:00.760 --> 27:07.480
there was a photorealism gap but there was a bigger actually semantic gap. So since a couple of

27:07.480 --> 27:12.200
years ago what changed and I think in my opinion was actually a big change in the community was

27:12.200 --> 27:17.800
that data sets came out that they're based on scans of real world buildings and then see if

27:17.800 --> 27:22.920
here are 16 we have one paper and called building parser does that I think I believe the first time

27:22.920 --> 27:31.640
that multiple large buildings were scanned using like commercial scanning pipelines in full in 3D.

27:31.640 --> 27:36.040
So you would have a mesh of one building I remember at that time you had I think six buildings

27:36.040 --> 27:41.320
of a Stanford scan so you could load the mesh in your computer and look at it and it's the entire

27:41.320 --> 27:49.560
building is at your disposal. So that later became a underlying model for many simulators and now

27:49.560 --> 27:55.880
the simulators instead of using the the designer to develop things or design things for you now

27:55.880 --> 28:03.720
there's actually a real building that is serving as the underlying data and so multiple datasets

28:03.720 --> 28:12.920
came out of that one of them is called like Stanford 2D 3DS Matterport 3D and Facebook replica

28:12.920 --> 28:20.440
and so on there was actually the last one in this time wise in Gibson in 2019 there was a

28:20.440 --> 28:25.720
2018 and so if you are we have the paper and Gibson that had brought that to really larger scale

28:25.720 --> 28:31.560
there was about 600 buildings that they were scanned. So your robot can virtually visit 600

28:31.560 --> 28:38.600
buildings interact with it in as far as navigation is involved of course and and learn from it and

28:38.600 --> 28:45.560
sings 100 is a lot if you go to a new building every week and visit all corners of it because

28:45.560 --> 28:51.720
these are scanners like scan everything if you go to building a new building every week it takes 10

28:51.720 --> 28:57.560
years to get to 600 buildings so there's a lot of visual data certainly more than what like humans

28:57.560 --> 29:06.360
probably observe by the end of age two that's where like vision is sufficiently developed so

29:06.360 --> 29:11.400
that wasn't excused anymore essentially lack of data. Given the the constraint that you mentioned

29:12.440 --> 29:18.120
on you know how long it takes to scan these buildings was the the data crowdsourced?

29:18.840 --> 29:24.920
Yes to to some extent we we spoke with we had like hammer man some of them we would send them

29:24.920 --> 29:30.040
to actually scans buildings for us some of them would have a scans already so it would acquire it

29:30.040 --> 29:36.680
from them but yeah we didn't actually scan them it was like people's that would scan them for

29:36.680 --> 29:43.240
different reasons then we would acquire them and there's actually since the scanning pipeline

29:43.240 --> 29:50.680
series scanning pipelines are now sufficiently mature there is actually a support chain for them

29:50.680 --> 29:57.320
primarily coming from real estate market you know most of the I'm not actually sure if it's

29:57.320 --> 30:01.400
most of the houses but a good percentage of the houses if you want to sell them well you have

30:01.400 --> 30:07.480
to scan them in 3D so you can put up a website for them so the buyer can navigate in it before

30:07.480 --> 30:14.120
they actually come see it and so on so that's actually a good resource of data for us and that

30:14.120 --> 30:20.040
actually created a supply chain of camera man that you can probably any city at least in the

30:20.040 --> 30:25.400
United States and in Canada you can hire a camera man that already has the camera and they can

30:25.400 --> 30:31.640
go with scan and within a few hours they can send them all this habitat competition this is a

30:31.640 --> 30:38.760
benchmark that Facebook is proposing for agents that are navigating spaces can we talk about what

30:38.760 --> 30:45.960
the kind of how do they quantify performance in this environment right yeah it's built

30:45.960 --> 30:52.600
Facebook has a team of software engineers and researchers that developed a habitat platform

30:53.080 --> 30:59.240
like you said to a large extent it was developed on top of Gibson and a few other works that

30:59.240 --> 31:06.920
exist in the community like it said in the past like Minos and so on so the way it works in general

31:06.920 --> 31:15.640
is that the tasks are specific like point math or point navigation the agent is dropped

31:15.640 --> 31:23.000
in a new building completely unseen and it's provided at a random location in a new building

31:23.000 --> 31:28.360
and it's provided with a coordinate to go to so the agent could be randomly spawn in a bedroom

31:28.360 --> 31:33.000
and the coordinate that it's provided as the goal location is somewhere in the living room

31:33.000 --> 31:42.520
and all it sees is a stream of RGB or RGB data and it has to now plan its way around the obstacles

31:42.520 --> 31:47.480
and safely navigate itself to that particular location which could be like tens of meters away

31:47.480 --> 31:52.600
so it's a hard problem to solve especially that this is a new building it's not like you could

31:52.600 --> 31:58.680
spend time scan this building something like a slam pipeline and then run ASDAR and something

31:58.680 --> 32:06.360
like that to plan like post plan trajectory is just like it like imagine as a human when you go to

32:06.360 --> 32:10.760
your friends house when they just bought a house they have never been there and once you enter

32:10.760 --> 32:16.600
the living room I want to use the bathroom you can probably think about okay the bathroom would be

32:16.600 --> 32:21.480
probably I need to be looking for some hallways or doors and maybe a little bit of search

32:22.120 --> 32:26.120
you would find your way to the bathroom but you would not be just randomly wandering around

32:26.120 --> 32:30.280
this building for hours and hours till maybe by chance you would find a bathroom

32:30.280 --> 32:37.960
right so so these agents actually the task is something similar to that completely unseen

32:37.960 --> 32:43.960
building a spawn in a random location and provided with a target coordinate just travel to it so

32:43.960 --> 32:51.240
that was the first competition of habitat in 2019 and there actually a lot of entries I don't know

32:51.240 --> 32:56.200
the exact number but they were enough to actually make a good competition there were two tracks

32:57.160 --> 33:02.840
and they were not about the task but they were about the type of data so if the agent all it sees

33:02.840 --> 33:10.520
is RGB that would be akin to having just an RGB camera on a real robot that would be RGB only

33:10.520 --> 33:15.560
track and there was another track with this RGBD that would be like having a 3D sensor to like connect

33:16.360 --> 33:21.240
now we are at a point that we have a challenge with like tens of teams enter and there is a good

33:21.240 --> 33:27.720
amount of energy in the community that is spent in this area so it's to me it's actually a very

33:27.720 --> 33:35.000
healthy progress towards vision plus robotics in this context and this like I said the robots are

33:35.000 --> 33:41.480
limited to navigation now and the reason for it is that our data platforms right now can support

33:41.480 --> 33:50.440
only navigation not manipulation technically speaking it's hard to scan a building and now go make

33:50.440 --> 33:56.120
a change in it and be able to render it these buildings are scanned aesthetically so we don't have

33:56.120 --> 34:03.880
a good like support pipeline yet as of now for capturing both dynamic content and interactive

34:03.880 --> 34:10.760
content there's work going in this direction the community too but I think it's I would summarize

34:10.760 --> 34:17.800
them to be still in a scouting stage and they're in fancy so we'll see how that plays out in a few

34:17.800 --> 34:23.160
years but navigation is something that you know it's a reasonably stable we have good data platform

34:23.160 --> 34:30.440
um for it and we can look at the output and and to be honest like I was personally impressed

34:30.440 --> 34:39.400
by the the performance of the the winning teams in both tracks it's it's it's actually a hard problem

34:39.400 --> 34:45.320
to drop an agent in an unseen building and give it a goal that is like tens of meters away

34:46.040 --> 34:50.200
navigating the path is hard because you need to identify a lot of things you need to know for

34:50.200 --> 34:54.200
instance where the doorway is to get yourself out of the room and the woman you're out of the room

34:54.200 --> 35:01.000
you don't want to hit like the walls or many obstacles in the way and then maybe the target is behind

35:01.000 --> 35:08.040
let's say a chair and so on so there's a lot of like a fine-grained planning that goes into successfully

35:08.040 --> 35:12.840
doing that but the success rates are actually higher than what I expected so hopefully in the next

35:12.840 --> 35:17.240
year we'll see this becoming more mature and the task of force need to be more realistic

35:17.240 --> 35:23.800
um point navigation is is probably the simplest one uh we need to be able to navigate towards

35:23.800 --> 35:28.760
uh like semantic network towards and that semantic navigation let's say if you task an agent with

35:28.760 --> 35:33.480
finding a key the key doesn't have a specific coordinate it can be anywhere but it's not in arbitrary

35:33.480 --> 35:39.240
locations too are there any other uh kind of highlights uh in the vision for robotics

35:40.520 --> 35:47.160
yeah I think I think I think this was habit that was uh and and Gibson and so on

35:47.160 --> 35:53.640
were actually a good uh representative of the progress uh in terms of the discussion that

35:53.640 --> 35:59.640
we had earlier is uh like if you can learn everything from raw pixels what we didn't need for like

35:59.640 --> 36:06.840
vision uh uh they were actually multiple papers that came out in this area that they had

36:06.840 --> 36:13.400
focused the studies on why actually it is critical to have vision pipelines when you're trying to

36:13.400 --> 36:22.040
do robotic tasks uh one of them was uh a paper that we did um um we published in um last year I believe

36:22.040 --> 36:27.960
in December 2018 on archive it's called mid-level vision representations improve generalization

36:27.960 --> 36:33.800
sample efficiency of vision water policies and an update that we published in in coral uh in

36:33.800 --> 36:41.240
November that's basically uh mid-level vision for uh for navigation and there's also another paper

36:41.240 --> 36:48.440
that had a very similar flavor concurrently came out that uh was actually published in um um

36:48.440 --> 36:53.240
I don't remember actually living in an an aerobotic strontal and that how it actually does

36:53.240 --> 36:59.800
combat a vision matter for action which is a very direct statement of of the question and the

36:59.800 --> 37:05.160
the conclusion to both of these is that yes it's really important and critical to have vision

37:05.160 --> 37:11.960
pipelines and the and the reason is again those priors if you don't supply your um your robotic

37:11.960 --> 37:16.040
pipeline with these priors about the world like the world is treaty there are objects and there's

37:16.040 --> 37:21.080
permanence and so on and so forth yes there's a way that they can learn it but it will take

37:21.080 --> 37:25.960
tremendous amount of data like millions and millions of interactions to recover those facts

37:26.760 --> 37:33.720
so either we we don't provide those priors and we directly use raw pixels and but the

37:33.720 --> 37:40.760
consequence of that is being inefficiency as we know let's say tabularasa or l is very inefficient

37:41.400 --> 37:45.560
that's why most of the time it's focused on simulators because you cannot do as much interaction

37:45.560 --> 37:51.640
with the real world or it will take years or we make ourselves prone to not generalization

37:52.360 --> 37:58.120
you could solve the problem for limited space for where you did learning that's when the algorithms

37:58.120 --> 38:03.880
find the shortcuts but then the moment you go to a new building because the same shortcuts that

38:03.880 --> 38:09.480
they used for learning there was based on bicep data so they wouldn't generalize so both of these

38:09.480 --> 38:16.120
papers and they strongly showed that supplying these priors which is the job of computer vision

38:17.640 --> 38:22.600
significantly improves efficiency so you can learn faster with less data and also it improves

38:22.600 --> 38:28.200
generalization so what you learn in one building it won't be specific that one building anymore

38:28.200 --> 38:36.840
so I think this was actually a very strong conclusion and in a way response to to to demand for a

38:36.840 --> 38:44.280
study that it is really important for robotic pipelines to to use like vision algorithms and priors

38:44.840 --> 38:50.600
so I think those that should basically summarize are at least in my opinion the important

38:50.600 --> 38:57.480
progress in vision for robotics in past year the next area you had in mind was 3d vision which

38:57.480 --> 39:05.080
we've actually talked quite a bit about in the robotics context yes exactly so I think that the

39:05.080 --> 39:12.200
motivation is clear the world is especially 3d but when we look at an image we are looking at

39:12.200 --> 39:22.040
actually a 2d projection of it so basically that calls for the problem of recovering the 3d

39:22.040 --> 39:28.440
structure as a human you understand the world in 3d just because the retina in our eyes receives

39:28.440 --> 39:34.200
a 2d projection that doesn't mean we are unable to recover that underlying 3d but we do that through

39:34.200 --> 39:42.600
multiple mechanisms such as a stereo we have eyes so a stereo is a solution to recovering 3d but

39:42.600 --> 39:47.000
even when you cover one of your eyes you can still see the world in 3d basically it means some

39:47.000 --> 39:53.320
brick ignition or learning based is in process so we need so recovering this 3d

39:53.320 --> 40:03.000
structure from a 2d projection such as an image is a strong problem in vision community especially

40:03.000 --> 40:10.920
in the past year I think I see a rise I think that was a demonstrated by the fact that the

40:12.280 --> 40:17.560
paper award nominations and both to see if you are an ICCV include actually papers and 3d

40:18.600 --> 40:25.240
and also I think the problem has actually another angle to and 3d vision now we have 3d sensors

40:25.240 --> 40:31.320
because 3d is important we need processing pipelines for this 3d data so this is different from

40:31.320 --> 40:37.480
the first problem they mentioned images 2d you want to recover 3d and then you will do something

40:37.480 --> 40:42.840
with that recovered 3d sometimes we have the 3d from a sensor like a lighter or connect and so on

40:43.560 --> 40:51.000
but the processing pipeline that views for images do not directly work on 3d data

40:52.120 --> 40:56.680
they either work or they are very inefficient or they just don't produce as much good results so

40:56.680 --> 41:03.880
that's actually the second category in 3d vision having pipelines for processing 3d data

41:05.400 --> 41:13.400
both of them I believe they expand it notably in 2019 and I see a rising trend over there so

41:13.400 --> 41:20.680
I expect to see more and more of it in the years to come I think they're like in the first

41:20.680 --> 41:27.160
like we can actually maybe talk about a specific example in each of these categories does that sound

41:27.160 --> 41:33.400
to okay yeah absolutely yeah so I think in terms of like generally when we talk about the first

41:33.400 --> 41:39.000
category of problems recovering 3d from a 2d image first question is that where we get the data

41:40.520 --> 41:45.240
you have to have an image and have the underlying 3d data if you want to do it in a

41:45.240 --> 41:50.040
so police supervise learning manner to all to learn let's say your network that receives an image

41:50.040 --> 41:58.680
and a spit out the 3d so where we get the data so 3d sensors are are there but they're less common

41:58.680 --> 42:08.680
than RGB so we have a lot more RGB content than RGB plus 3d out there it work around that is common

42:08.680 --> 42:20.680
and now is that people get RGB data let's say a YouTube video or multi-view cameras and then

42:20.680 --> 42:26.760
recover 3d using like classic methods like the structure from ocean slam and so on and then

42:26.760 --> 42:35.640
once that is recovered using classic methods they use the recovered 3d as a supervise supervision

42:35.640 --> 42:43.320
for doing RGB to 3d from like individual frames like monocular that has been working for a few

42:43.320 --> 42:49.720
years and that has been shown like effective to a reasonable extent one thing that makes that

42:49.720 --> 42:57.480
hard is dynamic content because like those classic 3d reconstruction methods usually work based

42:57.480 --> 43:03.240
on point correspondences and then you have moving objects into scene it becomes basically an

43:03.240 --> 43:07.480
ill-posed problem is that is it a treaty of the scene that is governing this motion or is

43:07.480 --> 43:12.120
actually there's like let's say a human that is moving in the scene and so on so and humans are

43:12.120 --> 43:17.080
actually specifically an important part because humans are a very important object in the world in

43:17.080 --> 43:24.120
general so it was one paper that I personally found like pretty cute and past year it was learning

43:24.120 --> 43:30.040
the depth of moving people by watching frozen people so it was it had a very interesting like

43:30.040 --> 43:36.760
workaround there was the mannequin challenge that became popular if you remember it was about like

43:36.760 --> 43:44.520
people just standing stationary for a period of time and somebody would film them so that is

43:44.520 --> 43:50.520
actually interesting because they did a lot of data collection for us implicitly so people

43:50.520 --> 43:56.520
just stood the stationary and somebody walked between them so we actually had they collected a

43:56.520 --> 44:02.360
snapshot of the world where it was frozen and people were specifically there because it was

44:02.360 --> 44:09.480
mannequin challenge so there was a moving camera but no dynamic content but whereas the rest of

44:09.480 --> 44:13.960
the times it's really hard if you think about it to find people when they are stationary and then

44:13.960 --> 44:18.760
they don't move unless they are sleeping when somebody is awake there's usually some at least

44:18.760 --> 44:25.640
micro motion so mannequin challenge was great that turned into a source of data that this paper used

44:25.640 --> 44:34.040
so now they they apply the methods that would not probably normally work for dynamic content

44:34.040 --> 44:40.920
in people and the mannequin challenge data and then use that as supervision for now they can do

44:40.920 --> 44:46.360
depth of moving people whereas they actually learned it on frozen people so they kind of tricked

44:46.360 --> 44:52.760
the system by learning from frozen people but when you when you do it framed by frame then it

44:52.760 --> 44:59.240
basically doesn't know that the person is moving so that that was an interesting so an interesting

44:59.240 --> 45:07.240
paper and I think it was a representation of the fact that still when we are doing 3D vision

45:08.520 --> 45:15.320
data is a big problem there and this paper actually addressed the data by but it's nice

45:15.320 --> 45:22.760
interesting challenge to happen past year yeah it's a fairly it's quite creative but it also

45:22.760 --> 45:29.160
breaks the question you know for you know where will we get the data sets required to generalize

45:29.160 --> 45:34.920
this kind of approach right yeah so like I said there are aspects of the world that are static

45:35.640 --> 45:43.640
like I said for instance like in Gibson data these are human like primarily these are like

45:43.640 --> 45:52.760
residential places and and people just go and scan them and by default there's little motion

45:52.760 --> 45:58.440
unless there's a human in the same so humans very often are one of the main sources of like

45:58.440 --> 46:02.920
motion and there's some other forms of like motion too like if there's a fan on the ceiling that

46:02.920 --> 46:08.840
is like rotating there's no human there but it's there's motion there so but they're less common

46:08.840 --> 46:14.760
so and also like I said humans are around the most important objects in the world so it makes

46:14.760 --> 46:21.480
sense to actually have processing pipelines that that solve them and then we'll take it from there

46:21.480 --> 46:27.320
yes I guess we need to be either more creative or we can hope for algorithms that come out and

46:27.320 --> 46:33.800
handle the dynamic create a content better to be able to generalize it to to the more general setting

46:33.800 --> 46:42.120
yeah and and specific to this model is the idea that the model that's created with this technique

46:42.120 --> 46:48.520
could be kind of pulled out and used in a transfer learning kind of fashion and other pipelines

46:48.520 --> 46:56.920
or that you would incorporate this process into other pipelines and or none of the above

46:56.920 --> 47:02.840
is it just more of a proof of concept well I think the transfer in a way that happened was

47:02.840 --> 47:09.640
transferred from frozen people to moving people so the data was purely from mannequin challenge

47:09.640 --> 47:15.000
in which nobody's moving right by by the definition of mannequin challenge but once you learn from

47:15.000 --> 47:21.400
there that doesn't mean that when you're using the learn the learn model it has to be applied on

47:21.400 --> 47:29.720
on frozen people too so think about it this way what they let's say if you learn a frame by frame

47:29.720 --> 47:35.400
processing pipeline out of the mannequin challenge so it receives one image of a human

47:36.520 --> 47:43.720
and it can recover the 3D now during the actual data in the in the data set the people are frozen

47:43.720 --> 47:50.200
so five five frames in a row the person is frozen so it's not moving but it's a frame by frame

47:50.200 --> 47:55.000
process processing the frame first frame is independent of the second frame and third frame

47:55.000 --> 48:01.160
so at the test time you can actually apply it on a video where people are moving and it can just

48:01.160 --> 48:07.400
perfectly find recovered the 3D because it basically bridges no connection between the first

48:07.400 --> 48:13.560
frame and second frame so you can you can now apply that on moving people actually they did

48:13.560 --> 48:20.680
show it in the paper that they learn on frozen people and they applied and moving people

48:20.680 --> 48:26.680
and that's the consequence of the fact that you know whenever you do say single frame processes

48:26.680 --> 48:32.680
you can the frames are independent that's actually an old trick you have we have seen it before

48:32.680 --> 48:40.360
in many papers and it goes from there so so that's the type of transfer I'm not aware of any sort of

48:40.360 --> 48:46.120
like other transfer learning being in play here but if we have a pipeline that it can recover

48:46.120 --> 48:52.440
the 3D of people reliably well I think that itself is a pretty important problem and worth

48:53.240 --> 49:01.240
worth attention so that was the on an example from recovering 3D from 2D images like I said the

49:01.240 --> 49:08.760
other batch of problems and 3D is processing the data that is 3D let's say an output of a light

49:08.760 --> 49:14.200
or sensor or connect let's say to extract semantics out of it you can do object detection given

49:14.200 --> 49:19.080
an image or you can do object detection given a lighter scan and you'd hope that if you have

49:19.080 --> 49:23.000
something like lighter you would have more information than what an image gives you so you

49:23.000 --> 49:30.280
should be able to do a better job so that has been progress in general on this front to in in

49:30.280 --> 49:38.360
terms of efficiency and accuracy of extracting semantics given 3D data whether it's lighter

49:38.360 --> 49:47.240
or or connect and whatnot there are multiple papers actually there's I believe a Stanford

49:47.240 --> 49:54.760
there's a pipeline it's called Minkowski engine that is focused on an efficient 3D processing

49:55.880 --> 50:01.160
there's point that too that came a couple of years ago but it has been like matured since then

50:01.160 --> 50:10.040
a few months ago in an ICCV and so there was this paper on deep half voting for 3D object

50:10.040 --> 50:14.920
detection in point cloud that's actually another representative I believe it was one of the

50:14.920 --> 50:21.400
award nominations too so 3D sensors typically give you something like a point cloud and then you

50:21.400 --> 50:27.000
want to do various type of semantic extractions on top of that an object detection is probably

50:27.000 --> 50:33.720
most nominal example you want to put a bounding box around the objects that you see and so this

50:33.720 --> 50:41.480
paper actually had some pretty good results on that and the idea in general was and voting so

50:41.480 --> 50:48.600
when you look at let's say the point cloud is as a name says the cloud of 3D points and each

50:48.600 --> 50:55.080
point can vote for what kind of object that it belongs to like all the points that belong to a

50:55.080 --> 51:01.160
chair that can vote for for being a chair and for like some parametrized like our president

51:01.160 --> 51:07.640
of points let's say a center of the chair and um this paper actually used this voting pipeline

51:07.640 --> 51:16.680
and a voting voting concept and report actually good results in terms of object detection something

51:16.680 --> 51:25.000
similar to that actually happened in 2D sector 2 I believe it was called center net

51:25.000 --> 51:29.880
on something or something similar to that that they're like pixels and an image vote for the center

51:29.880 --> 51:37.240
of the object that they belong to so that's in a way it's different from the the the preceding

51:40.120 --> 51:46.040
angle that was basically labeling each pixels and its own like such as in segmentation and so on

51:46.040 --> 51:51.880
or finding finding exact like it's it wouldn't be a voting pipeline that's that's a distinction

51:51.880 --> 52:00.120
between them so um I think that was also just just to have a nominal example of papers that do 3D

52:00.120 --> 52:05.480
processing for extracting semantics and I and I see that um to be again a rising movement

52:06.200 --> 52:13.320
to develop more efficient pipelines with an vision community for recovering 3D or processing 3D

52:13.320 --> 52:20.120
and you know doing more work rounds um for lack of data there let's move on to the next category

52:20.120 --> 52:26.600
now they're area that I I believe in 2019 continued to observe progress with self supervised learning

52:26.600 --> 52:32.680
in general we can I think summarize that things are working sufficiently where for fixed mapping

52:32.680 --> 52:38.120
problems if you have enough data meaning that if you have an image you want to like extract something

52:38.120 --> 52:43.240
out of it let's say objects and so on if you have enough data it's a matter of probably designing

52:43.240 --> 52:48.600
your architecture and struggling with with hybrid parameters a little bit and training it for long

52:48.600 --> 52:55.080
enough you'll get some sufficient results the question that have been concerning a lot of vision

52:55.080 --> 53:00.840
researchers several since like several years ago after deep learning probably immediately after

53:00.840 --> 53:04.680
deep learning way it came was that whatever you're going to do if you don't have enough data

53:05.320 --> 53:12.040
and that gave rise to multiple research directions um self-suppose learning being one of them

53:12.040 --> 53:19.160
and I think the general uh the general question and the the goal there is that you want to

53:19.160 --> 53:26.680
rely on less and less on human labeled data you have a lot of data raw data in hand

53:26.680 --> 53:33.000
but just rely less and less on human labeled data and use raw data to identify trends in there

53:33.000 --> 53:39.560
and then kick a start your um learning pipeline to the point that the the the reliance on

53:39.560 --> 53:47.160
and human labels is just only for the last mile and perhaps exceptions so self-suppose learning

53:47.160 --> 53:54.120
has been going forward for a number of years um that you can actually look at the workshops

53:54.120 --> 54:00.520
within vision and machine learning community um to and you'll see that every year there's actually

54:00.520 --> 54:08.280
a popular workshop in that um in ICML last year in 2019 the we also had this workshop on on

54:08.280 --> 54:16.120
self-suppose learning hopefully we'll have it again in ICML 2020 in CBPR 18 uh we had the beyond

54:16.120 --> 54:22.280
supervised so there's a lot of basically focus on this problem in the community and a lot of

54:22.280 --> 54:29.320
good researchers have focused on it um so it's not as specific to 2019 as a trend but in 2019 I think

54:30.200 --> 54:35.800
it was for the first time that we saw that uh the image net problem essentially being solved

54:35.800 --> 54:43.960
with less labels or uh it was to be more specific it was the first time that without using labels a

54:43.960 --> 54:51.400
self-supervised pipeline was able to achieve as uh good results that uh uh data as a

54:51.400 --> 54:57.960
fully supervised pipeline would receive and that was actually a good point um I believe there were

54:57.960 --> 55:06.280
like two or three papers that reported that uh success the one that I particularly liked was the

55:06.280 --> 55:12.360
there's a paper based on contrastive predicting coding um so this paper the contrastive

55:12.360 --> 55:17.000
predicting coding that the title of the paper is data-efficient image recognition with contrastive

55:17.000 --> 55:22.680
predictive coding um so the contrastive predicting coding is not a new idea it has existed

55:22.680 --> 55:29.960
existed before but I think they really rendered it into a mature pipeline to the point that

55:29.960 --> 55:36.280
it's stable now and it can at as far as this image net data set is concerned you can actually

55:36.280 --> 55:41.800
get good image net classification results without having to use the image net labels which is

55:41.800 --> 55:47.800
uh which is actually interesting and a good proxy for for progress here the contrastive

55:47.800 --> 55:53.720
predicting coding is one of the ideas that have worked towards this purpose is actually not a complex

55:53.720 --> 56:00.600
concept in general you can let's say you you you learn the regularities in the visual world to be

56:00.600 --> 56:06.520
able to learn good features out of it let's say if you start if I show you the like top half of an

56:06.520 --> 56:11.080
image you will have an idea of what the bottom half would be like if in the top half if you see

56:11.080 --> 56:15.640
like a head of a dog probably you would say in the bottom half there would be the body of a dog

56:15.640 --> 56:20.600
so now when would you be able to make that prediction he would be able to make that prediction when

56:20.600 --> 56:26.120
you know how the world looks like you know that dogs are not just these like heads that are like

56:26.120 --> 56:33.160
floating in the air right there's usually a body to support it so so the raw image if you have a

56:33.160 --> 56:40.520
lot of them is enough for for learning that so so that's basically uh like in a very simplified

56:40.520 --> 56:47.560
way the idea behind like predictive coding you show part of the data and you learn your system

56:47.560 --> 56:53.160
to predict the rest of the data and this requires no labeling because the data is like raw and

56:53.160 --> 56:59.000
available to you you just mask it during learning and you get the on your network to predict the

56:59.000 --> 57:05.560
mask part so that this turns out to be actually a good way of learning features and the paper that

57:05.560 --> 57:11.400
I mentioned actually uses this idea in a more advanced way to not rely on human defined labels

57:11.400 --> 57:17.480
that's in contrast to let's say something like AlexNet that basically just started from images

57:17.480 --> 57:26.280
from the iteration zero of learning it would map pixels into human defined labels human

57:27.000 --> 57:32.280
human defined labels of course those thousand classes but actually in like every a single

57:32.280 --> 57:40.120
instance of an image was labeled by playing annotator so um so that was actually there I think that

57:40.760 --> 57:47.560
what you need to be aware of for the year to come is that this was shown for image net and this

57:47.560 --> 57:55.240
does not mean that we can do this for all problems and vision and under all settings so the caveat

57:55.240 --> 57:59.160
essentially is that this is a specific setup there's a certain architecture there's a linear

57:59.160 --> 58:04.120
layer and so on this doesn't mean that this will continue to happen regardless of these design choices

58:05.000 --> 58:10.600
and uh but still we didn't have this even even this prior to this year so I think it's

58:10.600 --> 58:17.000
that we can there's something to celebrate here and the more importantly uh even if it was

58:17.000 --> 58:23.160
architecture agnostic this is about image net and image net is not the world it's very niche part

58:23.160 --> 58:28.520
of the world it's a very dog biased part of the world because there are like many dog classes

58:28.520 --> 58:32.680
in there and then for instance from other species and animals is like a less of them

58:32.680 --> 58:39.320
so of course this is about the bias that the design of any dataset involves um the fact that we

58:39.320 --> 58:44.200
showed this on as a community on image net that doesn't mean that we can show it for everything

58:44.200 --> 58:51.480
so we can we can call it a success when for any image recognition problem we would have a

58:51.480 --> 58:56.840
self-supervised learning pipeline in which we would not rely have to rely on human labels too much

58:56.840 --> 59:00.520
and be able to solve that problem let's say if somebody does that for a depth estimation

59:01.160 --> 59:07.400
or object segmentation and so on then it would be a lot more convincing but that was definitely

59:07.400 --> 59:12.760
a progress for 2019 and so's with us learning too now we've seen uh uh

59:13.800 --> 59:23.000
2019 and and 2018 as well has brought um self-supervised learning to the fore on the NLP

59:23.000 --> 59:31.240
uh within the realm of NLP in a big way with models like Bert and Elmo and others you know

59:31.240 --> 59:38.280
is there a relationship between the you know this being a focus area vision in the vision space

59:38.280 --> 59:43.640
do you think you think that we're kind of pulling from you know inspiration from you know one to

59:43.640 --> 59:49.880
the other no no it's actually a great point it's both ways um like predictive coding in a way

59:49.880 --> 59:57.480
actually has similarities to the models that that work well in NLP too so um like a language model

59:57.480 --> 01:00:03.720
in general is something similar to that like you give the beginning of of a sentence where

01:00:03.720 --> 01:00:08.040
there is character words but you give it the beginning and it turns to send to predict what comes

01:00:08.040 --> 01:00:13.400
next and it can push it really it's a not just the next word that comes but many words that

01:00:13.400 --> 01:00:18.840
that come after that or you can turn it into like fill in the blank problem so it's all about like

01:00:18.840 --> 01:00:23.560
masking part of the data and filling it in whether it's the next part of the data or some like

01:00:23.560 --> 01:00:32.200
proceeding part of data um but the the common just here is that uh we use raw data to be able to

01:00:32.760 --> 01:00:38.360
we define a problem that is primarily based on masking part of the data and get the system to learn

01:00:38.920 --> 01:00:45.640
to to fill that gap and that is just a good feature and then what comes out of this like

01:00:45.640 --> 01:00:49.800
pipeline is a feature whether it's learned on text yeah it becomes something like

01:00:49.800 --> 01:00:55.480
birthed if it's learned on images it becomes something like the um predictive coding pipeline

01:00:55.480 --> 01:01:01.400
that I mentioned earlier and they become good features that are just sufficiently at least

01:01:01.400 --> 01:01:06.040
as for us let's say for images uh image that was involved sufficiently universal so could you

01:01:06.040 --> 01:01:11.720
use those features to to read different things out of it such as like image not classification

01:01:11.720 --> 01:01:16.520
so yeah there are similarities actually and I see that going both ways there are many people

01:01:16.520 --> 01:01:24.360
that that that work on both um the self-survised like models for both image and and text

01:01:25.080 --> 01:01:32.760
another category I think is worth mentioning um and it it's uh it's interesting to me the connection

01:01:32.760 --> 01:01:40.680
between either cellular postmas um research and and vision so um it's basically an interesting

01:01:40.680 --> 01:01:46.200
finding so in general the robustness issue is that you know you would have an image recognition

01:01:46.200 --> 01:01:51.640
pipeline let's say object detection and turns out that if you can make very small changes in the

01:01:51.640 --> 01:01:57.720
input uh something that humans even won't see like adding a few pixels or making a change like

01:01:57.720 --> 01:02:03.960
very tiny tiny changes in the pixels there's a way to actually mess with the uh object detection

01:02:03.960 --> 01:02:11.640
pipeline uh whereas the humans wouldn't see that change at all so that kind of made uh um

01:02:11.640 --> 01:02:16.840
it raised a big question and it made a lot of people concerned also it's it's

01:02:16.840 --> 01:02:21.560
besides like the concerns and safety and so on uh it was actually an interesting question

01:02:21.560 --> 01:02:28.120
intellectually that why is it that uh image recognition pipeline that seemed to be working well

01:02:28.120 --> 01:02:33.640
there's a there's such a big loophole that we can mess with the pixels in a very imperceptible

01:02:33.640 --> 01:02:43.160
way um that that uh changes the output such drastically um so there has been progress in in the

01:02:44.120 --> 01:02:49.720
in the community of adversarial robustness community um towards like methods that just

01:02:49.720 --> 01:02:56.440
robustifies the neural networks we respect to these changes um a lot of work came out of Alexander

01:02:56.440 --> 01:03:04.760
Matthews group at MIT and they showed actually good progress towards like high frequency adversarial

01:03:04.760 --> 01:03:10.520
patterns and they basically train the augment the the the training of a system in a way that

01:03:11.720 --> 01:03:17.560
the the outcome is in vain and respect to that kind of knowns is and then therefore you would

01:03:17.560 --> 01:03:22.760
expect more robustness and indeed the system does become more robust at least with respect to that

01:03:22.760 --> 01:03:30.840
particular adversarial pattern now I think what was interesting uh in retrospect it's common sense

01:03:30.840 --> 01:03:39.000
but it it had to be shown that if you robustify every image recognition pipeline you would expect

01:03:39.000 --> 01:03:45.240
it to have a better understanding of the world and therefore it should actually work even better

01:03:45.240 --> 01:03:50.440
for applications that are not within the context of adversarial robustness so let's say if you

01:03:50.440 --> 01:03:55.880
um if you have a image recognition pipeline that with a little bit of changes in the pixels around

01:03:55.880 --> 01:04:01.640
the dog it would just misclassify a dog for an ostrich and it just basically means it didn't

01:04:01.640 --> 01:04:07.480
quite understand what a dog is um so it was prone to these kind of mistakes now if you have a robust

01:04:07.480 --> 01:04:12.280
system that it doesn't make that system anymore it means it better understood what a dog is so

01:04:12.280 --> 01:04:17.160
it better understood the manifold of real world images and what's possible in the real world and not

01:04:17.160 --> 01:04:25.240
so those this paper actually in Europe's just uh two weeks ago um from Alexander Matri's group that

01:04:25.240 --> 01:04:32.760
basically showed that if you have a robust image classifier image classifier that is like trained

01:04:32.760 --> 01:04:38.280
with this robustness mechanism you can use it to synthesize better looking images compared to

01:04:38.280 --> 01:04:45.000
the same exact classifier just without being trained with uh with adversarial robustness the title

01:04:45.000 --> 01:04:50.120
paper I believe was image synthesis with a single robust classifier and they basically

01:04:50.120 --> 01:04:58.520
show it that uh side by side you can look at uh synthesized image coming out of a neural network

01:04:58.520 --> 01:05:02.200
you know in image synthesis pipeline we always use some sort of pre-trained neural network

01:05:02.920 --> 01:05:11.080
an image network and what not uh there's a requirement um um in the in the system so for that

01:05:11.080 --> 01:05:18.360
uh pre-trained neural network if you use a robust classifier versus a non very same exact data same

01:05:18.360 --> 01:05:24.040
exact architecture the robust classifier just renders much better looking uh images for various

01:05:24.040 --> 01:05:29.560
kind of synthesis problems like in painting or transferring sketches to images or super

01:05:29.560 --> 01:05:35.240
resolution and so on so this was an interesting uh finding it like I said it makes sense because of

01:05:35.240 --> 01:05:41.400
force and more robustness I mean to understood you would hope that you'd understood the uh uh the

01:05:41.400 --> 01:05:46.200
manifold of real world images better so therefore you should be able to do better synthesis too

01:05:46.200 --> 01:05:51.240
but you finally have a paper that that actually showed that too which I personally find interesting

01:05:51.240 --> 01:05:56.040
and it's another example of the uh mix of different areas with the envision and this time

01:05:56.760 --> 01:06:04.680
adversarial about its next research the the generalization result makes me think a little bit of

01:06:04.680 --> 01:06:10.280
the parallel to multi-task learning where we've seen over the past you know relatively recently

01:06:10.280 --> 01:06:14.520
I think past year or two a lot of work's been going into multi-task learning that showed that

01:06:15.480 --> 01:06:21.960
just the addition of another task and the training process helps the networks generalize and

01:06:21.960 --> 01:06:31.480
perform better in a sense often in this uh in the uh adversarial uh robustness research

01:06:31.480 --> 01:06:38.600
the robustification of the network is another task and so these are kind of similar results

01:06:38.600 --> 01:06:44.760
or or related potentially results right to to some extent that's true like essentially the

01:06:44.760 --> 01:06:49.640
robustness is as a result of augmenting the loss with the additional terms that you wouldn't

01:06:49.640 --> 01:06:57.000
normally have and you can view as multi-task learning uh versus single-task learning as augmenting

01:06:57.000 --> 01:07:03.880
the loss of a single task framework with more losses like more tasks that it would normally have

01:07:03.880 --> 01:07:10.600
so there's a there's that like regularization effect that uh though I have to say that in multi-task

01:07:10.600 --> 01:07:17.240
learning that observation has been challenged quite few in quite few different settings it

01:07:17.240 --> 01:07:23.480
really depends for instance what task that you learn together to be able to uh to be able to

01:07:23.480 --> 01:07:29.560
actually see the benefits of multi-task learning um uh ironically actually the publish one paper

01:07:29.560 --> 01:07:35.080
on this area which is exactly called which tasks should be learned together in multi-task learning

01:07:35.800 --> 01:07:43.320
and that that the the title says at all that there's a question there to ask like it's just being

01:07:43.320 --> 01:07:49.720
multi-task is not necessarily better uh there's like multiple other parameters in play such as what

01:07:49.720 --> 01:07:55.880
tasks should be learned together and there's also like the uh an equal amount of research going into

01:07:55.880 --> 01:07:59.880
let's say even if you know what tasks that you want to learn together what the architecture should be

01:07:59.880 --> 01:08:04.360
how the sharing exactly should happen should it be like self-parameter sharing hard parameter sharing

01:08:04.360 --> 01:08:09.080
some sort of progressive sharing where you decide what parts of the parameters and layers should

01:08:09.080 --> 01:08:15.320
be shared and which parts should be dedicated to to networks to to different tasks so um to be able

01:08:15.320 --> 01:08:22.280
to get the benefits of multi-task learning um I personally believe that a lot more research has to

01:08:22.280 --> 01:08:28.680
be done and the examples that we see are probably when we got lucky and they worked out doesn't

01:08:28.680 --> 01:08:34.040
mean we completely understood the problem yet so you know there you have we've kind of gone through

01:08:34.040 --> 01:08:44.040
these four key areas you see us making some advances uh in the vision field in 2019 and those are

01:08:44.040 --> 01:08:49.960
primarily from kind of the academic perspective how you seeing these trends play out in the real world

01:08:50.600 --> 01:08:55.960
yeah I mean uh I think if by the real world you mean like who has used them or in terms of like

01:08:55.960 --> 01:09:04.520
commercials um I think probably I see two areas um image synthesis like I said it became uh

01:09:04.520 --> 01:09:10.600
it entered the new maturity level like we see apps that come out to actually use these kind of

01:09:10.600 --> 01:09:17.080
learning-based synthesis that was a thing a child of the marriage between vision community and graphics

01:09:17.080 --> 01:09:25.320
community and these apps that like transfer you from like young to old or like smiley to not

01:09:25.320 --> 01:09:32.040
smiling and so on so forth and then there's uh companies like Nvidia I know that they're active

01:09:32.040 --> 01:09:38.920
on that and it had the consequences of uh you know the photo fakery concerns like deep fakes and so on

01:09:38.920 --> 01:09:43.800
that if things become so well then how do you know when something is real and when it is not

01:09:43.800 --> 01:09:48.760
but it's safe to say that that technology has been proven effective and has been adopted

01:09:49.640 --> 01:09:57.560
so that's one area that I see um it it entered a maturity level that was uh uh commercialized

01:09:57.560 --> 01:10:03.400
and also used by ordinary people and their phones and so on um another interesting area

01:10:03.400 --> 01:10:11.640
continues to be autonomous driving um um again a car an autonomous car is actually a robot it's a

01:10:11.640 --> 01:10:18.200
it's a big robot it has sensors it has perception that's planning is control though it's a specific

01:10:18.200 --> 01:10:24.600
one it's one for navigation designed to move on the roads that you know marked up and so on so

01:10:24.600 --> 01:10:33.000
forth so in a way um um it's an example of like robotics vision plus robotics and and again I'm

01:10:33.000 --> 01:10:37.480
as commenting and the perception aspect because there's a lot of things that going to getting

01:10:37.480 --> 01:10:46.840
something like autonomous driving um car to work um uh we sell progress um now tesla continues

01:10:46.840 --> 01:10:52.200
to release new features like a smart summon that you can call the car to to come to you

01:10:52.200 --> 01:11:00.280
um in a parking lot and the waymo I heard um during nerfs that they just released

01:11:00.280 --> 01:11:07.000
a an app that works in Arizona then it can actually order caps that are completely autonomous

01:11:07.000 --> 01:11:12.440
there's no no person in the cap from what I know I haven't used it I haven't been to Arizona

01:11:12.440 --> 01:11:18.680
since I heard this but I'm looking forward to doing it um so waymo did that and it's actually

01:11:18.680 --> 01:11:23.160
find it impressive you'll see how it it plays out but it can actually have these autonomous caps now

01:11:24.200 --> 01:11:29.800
um and different companies have different approaches like you know we know the tesla is um

01:11:30.680 --> 01:11:37.640
you know Elon Musk and and he has been vocal against like using heavy 3d sensors like LiDAR

01:11:38.840 --> 01:11:44.440
waymo has the safety first approach even at the expense of like coming having more sensors on it

01:11:44.440 --> 01:11:49.160
so different companies have different approaches and like I said we have seen progress

01:11:49.160 --> 01:11:56.600
such as the waymo release or tesla features but one summary I have is that again the last mile

01:11:56.600 --> 01:12:02.920
has been proven to be more difficult than the optimistic anticipations like a few years ago and

01:12:02.920 --> 01:12:09.400
many people had like predicted this like rod brooks and so on um if you look at the predictions about

01:12:09.400 --> 01:12:13.880
like when autonomous driving would be here let's say five years ago or three years ago let's say

01:12:13.880 --> 01:12:20.920
if you would hear by 2020 it would be uh available but it was then then 2018 and 19 they would

01:12:20.920 --> 01:12:26.680
push it like yeah a little bit more into the future like five years and you know in 2019 we heard

01:12:26.680 --> 01:12:33.240
this question uh multiple time answered but we don't know exactly when it will be here which is

01:12:33.240 --> 01:12:38.440
representation of the fact that again the last mile has been like proven to be harder than the

01:12:38.440 --> 01:12:45.320
optimistic anticipations but um but I'm watching it actually carefully because it's an interesting

01:12:45.320 --> 01:12:51.640
area there's a lot of investment in it there's a lot of talents going in it um so if something

01:12:51.640 --> 01:12:57.720
wants to work I believe autonomous driving will be one of the first examples of as the application

01:12:57.720 --> 01:13:04.200
of say vision plus AI machine learning and so on and uh so we'll see how it goes do you think the

01:13:04.200 --> 01:13:12.200
last mile problem applies specifically or only to autonomous driving whereas uh do we see this

01:13:12.200 --> 01:13:19.880
generally across uh bringing research-based uh innovations in the vision domain into real world

01:13:19.880 --> 01:13:25.160
applicants? No I absolutely see it as uh as a latter it's not about autonomous driving anymore

01:13:25.160 --> 01:13:34.040
any any real world system has a lot of unanticipated issues that you probably won't even think

01:13:34.040 --> 01:13:42.200
of before you reach there and um many of these pipelines that we have right now they have caveats

01:13:42.200 --> 01:13:48.680
let's say if you have a sensor that it it works fine it might continue to work fine during summer

01:13:48.680 --> 01:13:56.040
but during winter when it is snowed how do you know um how it's going to react or like let's say

01:13:56.040 --> 01:14:00.280
something like autonomous driving or even like say indoors navigation if you have a robot home

01:14:00.280 --> 01:14:05.160
that like navigates safely from one point to another let's say when that happens for household

01:14:05.160 --> 01:14:12.760
robotics? I'm pretty sure that you would be surprised by um by the way the system works when

01:14:12.760 --> 01:14:17.800
you take it to a different country when the houses look different or things that are that are okay

01:14:17.800 --> 01:14:23.880
here they're not okay there and and so on so forth so now I definitely believe that any kind of

01:14:23.880 --> 01:14:29.560
real world like adoption of of this research that we're doing especially in terms of machine

01:14:29.560 --> 01:14:37.000
learning where there's a factor of uninterpretability as of now um the last mile will be proven

01:14:37.000 --> 01:14:45.320
proven harder so let's shift gears from looking backwards into 2019 to looking forwards to

01:14:45.320 --> 01:14:53.320
the future maybe give us your top predictions for computer vision for uh 2020 or or beyond I've

01:14:53.320 --> 01:14:58.120
been challenging the folks that I've been doing are AR rewind series this year since we're at the

01:14:58.120 --> 01:15:05.720
end of a decade to project uh a decade to the future and no one takes me up on it. The problem

01:15:05.720 --> 01:15:11.000
with making predictions is that there's a pretty good chance that um for turning out to be wrong

01:15:11.000 --> 01:15:17.080
like if if a decade ago you asked anybody about you turning I'm pretty sure the chance of that being

01:15:17.080 --> 01:15:23.320
the prediction um would be even though it just like it's in 2010 it just happened two years after

01:15:23.320 --> 01:15:29.320
that in in in in computer vision but yeah I don't think I'm not sure if anybody saw that coming

01:15:29.320 --> 01:15:34.520
but maybe some people say they saw it coming but they were probably very very small percentage

01:15:35.080 --> 01:15:41.880
but um I think you know I can extrapolate a little bit at least if I don't even make you know

01:15:42.840 --> 01:15:48.120
two controversial like predictions one thing that I um in terms of since we just managed

01:15:48.120 --> 01:15:52.440
the commercial discussion I think at least I can say that from the commercial development perspective

01:15:52.440 --> 01:15:57.640
I'll continue to watch autonomous driving progress I really view that as a good proxy for

01:15:57.640 --> 01:16:05.080
progress in a well-focused and well-invested area so lack of talent lack of data lack of money

01:16:05.080 --> 01:16:11.800
is not really an excuse so it we are really down to solving the problem the physical problem over there

01:16:12.440 --> 01:16:18.760
and and and one of the other reasons that I particularly watch autonomous driving uh

01:16:18.760 --> 01:16:25.160
progress is that perception wise ironically autonomous driving to some extent is is a simplified

01:16:25.160 --> 01:16:29.960
case compared to the general perception case there are there are complications but there's

01:16:29.960 --> 01:16:34.840
the reason it's simplified compared to general perception is that like you know autonomous driving

01:16:34.840 --> 01:16:40.120
they're like lane markers and there's signs and there's some code of conduct and there's a lot of

01:16:40.120 --> 01:16:46.680
data um so the lane markers are you know designed to tell you where you are there as like when you're

01:16:46.680 --> 01:16:52.120
just walking in in the woods there's no lane marker so it's the same navigation problem but you

01:16:52.120 --> 01:16:58.360
have to it's the lost kind of a lot less information there's a lot less sort of design ahead of time

01:16:58.360 --> 01:17:04.440
that's gone into making the problem perceptually simpler so if if we can solve let's say a lane

01:17:04.440 --> 01:17:09.720
detector for autonomous driving a big problem is solved like for a general say navigation problem

01:17:09.720 --> 01:17:15.160
in unstructured setting there's no lane to detect so you have to really do something more than that

01:17:15.160 --> 01:17:23.400
so in some senses in some senses autonomous driving's perception is simplified so I'm hoping to first

01:17:23.400 --> 01:17:28.920
see the simplified problem solved before we even like dream of solving the more uh complex and

01:17:28.920 --> 01:17:35.160
unstructured problems but of course there's like like other big issues over there like

01:17:35.160 --> 01:17:39.640
there's safety risk the speed that's autonomous driving is high that basically means you have very

01:17:39.640 --> 01:17:46.120
limited amount of time for making decisions you have to make make decisions like way ahead of time

01:17:46.120 --> 01:17:52.760
compared to the normal setting when the speed is like uh low and then sensitivity is high because

01:17:52.760 --> 01:17:59.400
there's like a risk of fatal accidents and varying conditions cars go everywhere and like during

01:17:59.400 --> 01:18:05.240
the night during the day during the snow foggy and so on so forth and sensors get constantly

01:18:05.240 --> 01:18:10.840
interfered by sun and whatnot then you're capped by the price um so there is like that's not to

01:18:10.840 --> 01:18:16.360
basically play down to complications but at least let's say purely focusing in detection aspect

01:18:16.360 --> 01:18:20.840
there are some things in autonomous driving that are simplified so I continue as a vision researcher

01:18:20.840 --> 01:18:26.440
to watch autonomous driving progress and to see where that goes and when it is that you're going

01:18:26.440 --> 01:18:33.640
to finally seal it and say that you safely have autonomous driving system and uh so that's a good

01:18:33.640 --> 01:18:41.160
proxie proxie proxie for me that I watch another aspect that I think it's outside this niche

01:18:41.960 --> 01:18:47.160
like more specific applications like autonomous driving if we like they're yeah they're like other

01:18:47.160 --> 01:18:55.080
applications that we could watch there's actually a lot of focus on like a startups and robotics

01:18:55.080 --> 01:18:59.080
that of course they need to solve vision to some of them are focused on warehouses some of them

01:18:59.080 --> 01:19:07.720
focus on indoor spaces but if you want to think about like more like general settings

01:19:07.720 --> 01:19:14.760
about vision commercially speaking I look forward to more like democratization of vision pipeline

01:19:14.760 --> 01:19:21.240
so there are APIs right now so it's a very democratic way of of using vision like if you're

01:19:21.240 --> 01:19:28.760
somebody that has no expertise in vision you can actually use Microsoft Azure or Google Cloud

01:19:28.760 --> 01:19:34.040
to do certain tasks like face detection or some object detection but compared to general

01:19:34.840 --> 01:19:42.920
what vision can do that's a very small part of the potential so another good proxy for progress

01:19:42.920 --> 01:19:50.280
is that how much these democratic tools uh vision how far they go how much you go beyond like face

01:19:50.280 --> 01:19:55.560
detection and and so on and like I said there's a good reason that these APIs have these now because

01:19:55.560 --> 01:20:00.200
they are the things that work good then many other things that we have they just don't work

01:20:00.200 --> 01:20:05.080
good enough to be at the API level and that's why I'm saying like the more I see

01:20:05.080 --> 01:20:11.480
brought to the API level that works reliably and sufficiently well that's a good progress

01:20:12.760 --> 01:20:17.080
proxy for our progress that we can see and examples are like 3D again

01:20:18.280 --> 01:20:23.480
there's no good API at least from what I know that would recover the underlying 3D structure

01:20:23.480 --> 01:20:30.120
from from given images for you there's like are you predicting that for 2020?

01:20:33.080 --> 01:20:40.680
Well maybe let's like up mystically yeah I don't know whether we are going to get to the

01:20:40.680 --> 01:20:47.320
API level probably not but I think we're the progress in the field points to the direction that

01:20:47.320 --> 01:20:51.720
people would have started like thinking about bringing that to the API level hopefully based

01:20:51.720 --> 01:21:00.360
on the progress they'll make in in 2020 so yeah we'll I'm also observing these and looking forward to

01:21:00.360 --> 01:21:05.320
more like democratization of the tools that we have envisioned because that's really when

01:21:05.320 --> 01:21:11.320
as vision researchers we will let the product of a work to be used by people that are not vision

01:21:11.320 --> 01:21:18.680
researchers yeah yeah so that's something from the from the commercial perspective from the

01:21:18.680 --> 01:21:24.280
technical perspective I think actually the trends that I anticipate would be more or less

01:21:24.280 --> 01:21:30.440
in extrapolations of the trends that we be discussed I do believe that vision plus robotics

01:21:30.440 --> 01:21:38.040
will continue to go forward as strong the way it will change is that we will we will actually

01:21:38.040 --> 01:21:44.360
start working on more specific sorry more general problems like right now let's say in this

01:21:44.360 --> 01:21:49.480
example of like habitat challenge and Gibson to discuss things very primarily about navigation

01:21:49.480 --> 01:21:53.240
even with the navigation it was about one case of navigation I go to this particular

01:21:53.240 --> 01:22:01.000
coordinate but looking at the more general setting the find objects for me or solve the tasks

01:22:01.000 --> 01:22:08.440
that or like a sequential game like find object a and then go to an exit like an example it says

01:22:08.440 --> 01:22:14.040
the rescue robot we send the robot to a building that is on fire this robot has not been in this

01:22:14.040 --> 01:22:20.520
particular building before but you task it with find the victim and go to the closest fire exit

01:22:20.520 --> 01:22:28.120
so it's a really useful application just that but we are not there yet and so in terms of vision

01:22:28.120 --> 01:22:35.640
plus robotics I expect the proportion of researchers interested in this topic to to expand

01:22:36.200 --> 01:22:43.640
and the technical part the problems to become more challenging more realistic and hopefully work

01:22:43.640 --> 01:22:48.920
on the more like more general setting not limited settings that we have been focusing on so far

01:22:51.160 --> 01:22:59.720
I multitask learning came up in our discussions and I think that's an area that I hope to see

01:22:59.720 --> 01:23:07.160
progress in it generally you know there's very little things about vision that is single task

01:23:07.160 --> 01:23:13.560
like your job with an image doesn't end the moment to detect objects in it or the moment to detect

01:23:13.560 --> 01:23:20.680
the depth of it the moment to detect the vanishing points for any practical use for most practical

01:23:20.680 --> 01:23:28.040
uses you usually need multiple of things such task at the same time so the problem is like

01:23:28.040 --> 01:23:34.920
essentially multitask to be useful so being able to bring multitask learning to

01:23:34.920 --> 01:23:44.600
a more reasonable state where we can solve multiple problems efficiently and reliably and

01:23:44.600 --> 01:23:53.160
consistently is an important problem we did see some progress in 2019 but we are far from

01:23:53.160 --> 01:23:59.560
reaching that level to the point that we can supply a multitask vision pipeline with one image

01:23:59.560 --> 01:24:06.600
and in the uploaded produces a breadth of different abstractions extracted in a way that

01:24:06.600 --> 01:24:12.680
that is done independently and sorry that is done efficiently of course you can't have like under

01:24:12.680 --> 01:24:18.840
the hood you can have like end different independent pipelines for end different tasks that's inefficient

01:24:18.840 --> 01:24:24.280
because there's a lot of redundancy between them so you don't want to do that being able to do

01:24:24.280 --> 01:24:31.320
that efficiently do not sacrifice the accuracy over there all these end outputs should have a

01:24:31.320 --> 01:24:36.760
reasonable quality and something that I'm personally excited about these days and hopefully

01:24:38.200 --> 01:24:43.640
soon we'll release this stuff on it is on consistency so the output of a multitask system should

01:24:43.640 --> 01:24:49.160
be consistent your objects and your computer cannot be inconsistent with each other so how we

01:24:49.160 --> 01:24:57.320
actually learn do the learning in a way that the outcome is has some guarantees for being consistent

01:24:58.120 --> 01:25:02.440
and one thing that also I think we have overlooked for too long especially again in the

01:25:02.440 --> 01:25:08.840
conflicts of multitask learning is uncertainty for each of these outputs sure we are providing

01:25:08.840 --> 01:25:14.520
some predictions but you can't do much with the prediction especially as a practitioner

01:25:14.520 --> 01:25:20.040
if it's not associated with some confidence metric this is a dog in an image but with what

01:25:20.040 --> 01:25:25.240
confidence if it's if you're 99% confident and versus you're 50% confident your decision

01:25:25.240 --> 01:25:30.360
probably changes if there's a tiger within like two meters of you with 99% confidence

01:25:30.360 --> 01:25:35.720
I'm gonna run if it's 1% confidence probably I'm not gonna run I'm gonna do something else

01:25:35.720 --> 01:25:44.600
so there's there's a real real value in an extra acting uncertainty from the visual signal

01:25:45.880 --> 01:25:51.720
surprisingly that's not actually a big part of the computer vision research right now

01:25:51.720 --> 01:25:56.680
and I think that's part of the part of the reason that's happening is that division plus x

01:25:57.800 --> 01:26:05.480
movement is recent so the moment you want to provide vision outcome towards some downstream

01:26:05.480 --> 01:26:10.040
goal whether that's a roboticist or some and so on so forth you'll realize that they need a

01:26:10.040 --> 01:26:13.880
little bit more than whether you're providing them and one of the things that they they definitely

01:26:13.880 --> 01:26:20.680
need is uncertainty estimation so you bring this up in the context of multitask is the idea that

01:26:21.720 --> 01:26:32.360
the the consistency desire and the uncertainty desire can be formulated as additional tasks or

01:26:32.360 --> 01:26:37.320
objectives as part of your training right the reason I brought up in the context of multitask learning

01:26:37.320 --> 01:26:44.360
is that multitask learning itself provides an opportunity for a new way of quantifying uncertainty

01:26:44.360 --> 01:26:49.640
but uncertainty is still useful and it can be done in a single task setting too and in the machine

01:26:49.640 --> 01:26:56.440
learning community there's actually work on it more than vision community that systems that

01:26:56.440 --> 01:27:02.680
supply a confidence score besides the prediction that they're making even that's a single task

01:27:02.680 --> 01:27:07.640
there's like research being done and that and so on but it has been again proven harder than

01:27:07.640 --> 01:27:16.120
anticipated because neural networks are found to be making confident mistakes so because of various

01:27:16.120 --> 01:27:24.840
different things that's most of them are actually theory they're papers in just past icml there was

01:27:24.840 --> 01:27:30.440
probably more like a track that why this is happening that's basically a question rather than

01:27:30.440 --> 01:27:34.760
something that you already know the answer to it but we have made this observation that neural

01:27:34.760 --> 01:27:40.680
networks make confident mistakes so that's a problem so basically means that even if you have a

01:27:40.680 --> 01:27:49.720
measure of uncertainty chances are that might not be reliable because if you extract that uncertainty

01:27:49.720 --> 01:27:54.280
out of a single task system there's no redundancy over there or there's no unzombling

01:27:54.280 --> 01:27:59.800
that uncertainty estimation might not be very may accurate too so multitasking there's a way

01:27:59.800 --> 01:28:06.120
to solve that because essentially under the hood there are multiple processing pipelines for

01:28:06.120 --> 01:28:12.680
different abstractions and the consistency across them is actually a good proxy for how accurate

01:28:12.680 --> 01:28:22.440
each of them are and there has been research on it and hopefully I think soon maybe within a few

01:28:22.440 --> 01:28:29.000
weeks we will also release new material on that but from what I see the method point is that we

01:28:29.000 --> 01:28:35.560
definitely need to to output uncertainty estimates out of the visual processing pipelines that we do

01:28:35.560 --> 01:28:41.880
and and more specifically on multitask learning and so on what I see is that there is a there's

01:28:41.880 --> 01:28:47.640
a good opportunity to solve that problem at least within the multitask learning framework and

01:28:47.640 --> 01:28:53.720
we'll see how 2020 turns out awesome awesome any additional predictions for us I think that's

01:28:53.720 --> 01:29:01.960
that that's enough to embarrass myself in the year from now you heard it here first autonomous

01:29:01.960 --> 01:29:10.200
driving in 2020 that's what you said right well I didn't for directors but I hope

01:29:10.200 --> 01:29:19.400
nice nice at the very least Amir and an autonomous vehicle cab in Arizona yes that's

01:29:19.400 --> 01:29:24.920
at least a thing there and I'm looking forward to trying it awesome well Amir thanks so much

01:29:24.920 --> 01:29:33.400
for taking the time to share with us your again your take on 2019 and predictions or

01:29:33.400 --> 01:29:43.160
anticipations for the the year ahead thanks for having me all right everyone that's our show

01:29:43.160 --> 01:29:49.240
for today for more information on today's guest or for links to any of the materials mentioned

01:29:49.240 --> 01:29:56.760
check out twimmelai.com slash rewind 19 be sure to leave us a five star rating and a glowing review

01:29:56.760 --> 01:30:02.600
after you hit that subscribe button on your favorite podcast catcher thanks so much for listening

01:30:02.600 --> 01:30:12.600
and catch you next time

