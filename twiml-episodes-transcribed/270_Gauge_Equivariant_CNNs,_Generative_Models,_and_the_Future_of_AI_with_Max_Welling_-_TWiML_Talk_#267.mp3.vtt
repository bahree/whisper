WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.040
I'm your host, Sam Charrington, before we dive in, a quick community update.

00:35.040 --> 00:40.240
The Twimble study group is added again for the next four weeks, starting May 25th, we'll

00:40.240 --> 00:45.240
be diving headfirst into Peter Abiel's full stack deep learning course.

00:45.240 --> 00:50.480
This course is a great complement to the fast.ai courses we've done so far and covers practical

00:50.480 --> 00:56.600
topics, like problem formulation, data acquisition and preparation, establishing the right frameworks,

00:56.600 --> 01:01.480
platforms and compute infrastructure, debugging and ensuring reproducibility, and deploying

01:01.480 --> 01:03.680
and scaling your models.

01:03.680 --> 01:08.720
For more information or to register for this study group, visit twimbleai.com slash full

01:08.720 --> 01:09.880
stack.

01:09.880 --> 01:14.800
I can't thank our volunteer study group hosts enough for all their hard work, a huge shout

01:14.800 --> 01:20.400
out to Michael, Christian, Kai, Sanyam, Joseph, Dinesh and everyone else that's been involved

01:20.400 --> 01:24.760
in making this group happen.

01:24.760 --> 01:29.000
A hearty thanks to our friends at Qualcomm for sponsoring today's show.

01:29.000 --> 01:33.000
As you'll hear in the conversation with Max, Qualcomm has been actively involved in

01:33.000 --> 01:38.880
AI research for well over a decade, leading to advances in power efficient on device AI,

01:38.880 --> 01:44.800
as well as an algorithm such as Bayesian deep learning, grass CNNs, gauge, echrovariant CNNs

01:44.800 --> 01:46.200
and more.

01:46.200 --> 01:50.680
Of course, we know Qualcomm powers some of the latest and greatest Android devices with

01:50.680 --> 01:53.400
their Snapdragon chipset family.

01:53.400 --> 01:57.640
From this strong foundation in the mobile chipset space, Qualcomm now has the goal of scaling

01:57.640 --> 02:01.600
AI across devices and making AI ubiquitous.

02:01.600 --> 02:06.520
In this vein, a product I'm particularly looking forward to is their Cloud AI 100 line

02:06.520 --> 02:11.480
of data center inference chips, which I learned about at the recent press launch.

02:11.480 --> 02:16.680
To learn more about what Qualcomm is up to, including their AI research platforms and developer

02:16.680 --> 02:24.920
tools, visit twimmelai.com slash Qualcomm.

02:24.920 --> 02:27.880
Alright everyone, I am on the line with Max Welling.

02:27.880 --> 02:33.360
Max is a research chair in machine learning at the University of Amsterdam, as well as

02:33.360 --> 02:39.280
vice president of technologies at Qualcomm and a fellow at the Canadian Institute for

02:39.280 --> 02:41.680
advanced research or C-FAR.

02:41.680 --> 02:44.520
Max, welcome to this week in machine learning and AI.

02:44.520 --> 02:46.680
Thank you very much Sam.

02:46.680 --> 02:50.000
It's great to get a chance to chat with you Max.

02:50.000 --> 02:56.560
You're quite accomplished in the field of machine learning and I'd love to understand what

02:56.560 --> 03:02.400
drew you to AI and how you kind of started your career in this area.

03:02.400 --> 03:08.280
I actually started not in AI, I started in physics, so I did my PhD thesis in theoretical

03:08.280 --> 03:15.240
physics and that was fun, but it was also somewhat abstract in the sense that I couldn't

03:15.240 --> 03:20.640
see how I could have a major impact with that and so I decided I wanted to change to

03:20.640 --> 03:26.040
some a field that was a bit more, you know, less static and more dynamic at that point

03:26.040 --> 03:27.040
in time.

03:27.040 --> 03:33.080
And so I wanted to do something like neuroscience, which I thought was a great choice.

03:33.080 --> 03:41.680
And so I applied to Caltech to Pietro Perona's lab, but I basically ended up doing computer

03:41.680 --> 03:47.840
vision there and then later when I went to London, I worked with Jeff Hinton, I was doing

03:47.840 --> 03:48.840
machine learning.

03:48.840 --> 03:56.280
So I sort of migrated my andered from physics through computer vision to machine learning.

03:56.280 --> 04:02.480
And there, I really found a home because I thought, okay, this is going to grow big in

04:02.480 --> 04:09.880
the future, which actually happened and there's so many applications with which this field

04:09.880 --> 04:14.640
can impact the world in a positive way that I sort of decided to stay there.

04:14.640 --> 04:20.400
That first jump into computer vision, was that computer vision applied to neuroscience

04:20.400 --> 04:21.400
in some way?

04:21.400 --> 04:28.800
No, it was just computer vision basically analyzing images on a computer screen, basically,

04:28.800 --> 04:29.800
right?

04:29.800 --> 04:35.120
So it didn't have anything to do with neuroscience, although I was, you know, it was fascinated

04:35.120 --> 04:41.840
by that question and I, you know, I was going to some meetings on neuroscience and, you

04:41.840 --> 04:44.720
know, integrated somewhat with neuroscience in Caltech.

04:44.720 --> 04:49.040
In the end, I was doing computer vision, which was great too.

04:49.040 --> 04:51.080
Okay, okay.

04:51.080 --> 04:58.680
And I get a chance to talk to a lot of folks that started their career in physics.

04:58.680 --> 05:05.360
A lot of folks more on the applied side, whether astronomy or, you know, dealing with things

05:05.360 --> 05:11.320
like, you know, some folks that work at using colliders and things like that, but you are

05:11.320 --> 05:14.720
more on the theoretical side.

05:14.720 --> 05:15.720
That's right.

05:15.720 --> 05:21.760
Yes, I was doing my thesis in 2 plus 1 dimensional quantum gravity, so it was a very abstract

05:21.760 --> 05:22.760
and theoretical.

05:22.760 --> 05:28.880
It gave me a good, you know, basis for mathematics, but it wasn't very applied, that's true.

05:28.880 --> 05:33.440
And so at some point along the line, you started a company, Cypher.

05:33.440 --> 05:37.560
When was that in your career and what was the inspiration for the company?

05:37.560 --> 05:40.840
Yeah, so that was actually when I came back to the Netherlands.

05:40.840 --> 05:47.520
So I had my career was in North America and a little bit of time in London until about

05:47.520 --> 05:52.400
six years ago when I decided to come back to the Netherlands.

05:52.400 --> 05:56.480
And fairly quickly after that, we founded this company.

05:56.480 --> 06:02.880
It was an interesting story because we were working with a big Dutch bank and they wanted

06:02.880 --> 06:09.080
to do a competition to predict, you know, what ads, you know, customers would want to

06:09.080 --> 06:11.760
click on when you offered them.

06:11.760 --> 06:16.400
And they were doing that with a bunch of big sort of consultancy companies.

06:16.400 --> 06:20.120
And we were basically, I was asked to basically arbitrage, right, to make sure that, you know,

06:20.120 --> 06:22.760
everything went right and the competition was fair.

06:22.760 --> 06:27.640
And in order to do that, I just asked one of my master's student, Taco Cohen, who was

06:27.640 --> 06:33.200
also working at Qualcomm at this point, to, you know, to also implement some models.

06:33.200 --> 06:38.480
And he did that on his laptop, laptop with a single GPU in it.

06:38.480 --> 06:43.120
And then we looked at the results, you know, we basically, we beat all the big companies

06:43.120 --> 06:47.800
and the big banks that, you know, if that's the case, then you get the job.

06:47.800 --> 06:53.520
And so that was the, that was the start of our company, right, I, I then, so then two

06:53.520 --> 06:58.040
other people joined, I'm a block of work and you're a Sunday, you're a son of was an

06:58.040 --> 07:01.120
old physics pal of mine.

07:01.120 --> 07:06.040
And that's what started the company and, and from there, you know, five, five good years

07:06.040 --> 07:12.040
running, we did lots of consultancy with companies and, you know, looked at their AI problems.

07:12.040 --> 07:15.640
And then Qualcomm came around and said, you know, you have a good sized group here, all

07:15.640 --> 07:19.360
very experienced people, nice practical attitude.

07:19.360 --> 07:24.080
And they wanted to acquire us and, you know, there was a couple of other companies also

07:24.080 --> 07:28.280
that were looking at us, but, you know, we went with Qualcomm.

07:28.280 --> 07:33.440
And at the time Qualcomm came around, were you looking to be acquired or were you ready

07:33.440 --> 07:39.400
to, to try something else or did it, you know, did the opportunity just present itself

07:39.400 --> 07:42.600
and you evaluated it on its terms?

07:42.600 --> 07:44.400
Yeah, we did the latter.

07:44.400 --> 07:48.120
So we weren't optimizing, you know, for being acquired or anything.

07:48.120 --> 07:51.720
It was just like, you know, we were having a lot of fun and some people actually thought,

07:51.720 --> 07:54.120
you know, that we should keep going and grow.

07:54.120 --> 07:57.320
Other people thought, you know, maybe it's, you know, maybe it's actually interesting

07:57.320 --> 08:02.200
to join a big company and, you know, have a lot more, um, like a lot more power.

08:02.200 --> 08:05.080
You can, you can make a bigger impact if you're part of a big company.

08:05.080 --> 08:10.240
Um, and so it was basically an opportunity that we evaluated at that point and we thought,

08:10.240 --> 08:11.240
oh, is this good?

08:11.240 --> 08:12.240
We should try this.

08:12.240 --> 08:20.600
Uh, but prior to the primary focus of the company was on, uh, consulting, there wasn't

08:20.600 --> 08:23.120
a product being built or, or something along those lines.

08:23.120 --> 08:25.600
Yeah, we, we did actually build a product as well.

08:25.600 --> 08:29.900
Um, so there was an active learning tool, which was, uh, actually implemented the

08:29.900 --> 08:32.840
Tata steel, um, and it works as follows.

08:32.840 --> 08:34.880
So, um, you have an expert.

08:34.880 --> 08:39.360
So there was like cameras looking at slabs of steel, which were being produced and there

08:39.360 --> 08:42.920
was sometimes small little, uh, sort of defects on that steel.

08:42.920 --> 08:47.520
Um, we had a battery of cameras above it and, um, we were detecting these defects.

08:47.520 --> 08:52.360
Um, but, you know, there were a couple of classes and some of these defect classes were very

08:52.360 --> 08:53.360
rare.

08:53.360 --> 08:55.120
We didn't have a lot of data on them.

08:55.120 --> 08:58.000
And so the algorithm didn't perform very well.

08:58.000 --> 09:01.480
And so then the algorithm assessed itself and said, okay, so for the, for these types

09:01.480 --> 09:04.440
of, you know, images, we need more labels.

09:04.440 --> 09:06.960
And so that was then shipped to an expert.

09:06.960 --> 09:08.240
The expert would label it.

09:08.240 --> 09:12.880
It would go back into the system and the system would learn again and become better, gradually

09:12.880 --> 09:14.120
over time.

09:14.120 --> 09:17.520
So that's active learning where there's a human in the loop and the algorithm interacts

09:17.520 --> 09:21.320
with the human, um, and that was basically our product.

09:21.320 --> 09:22.320
Okay.

09:22.320 --> 09:27.600
Uh, I don't, I don't, I can't speak necessarily to the approach, but it sounds, uh,

09:27.600 --> 09:35.800
very similar, uh, at least from a problem domain and general direction to, uh, well, lots

09:35.800 --> 09:41.680
of folks are going after this area, but landing AI, Andrew Ng's company, uh, comes to mind

09:41.680 --> 09:49.480
is kind of tackling that similar kind of applying, uh, AI to industrial problems, including

09:49.480 --> 09:52.360
like defect detection, things like that.

09:52.360 --> 09:57.560
Um, was that, uh, was that product kind of one of a portfolio of, uh, uh, uh, uh, uh,

09:57.560 --> 10:02.960
challenges that you were, uh, going after or was that a big focus?

10:02.960 --> 10:04.600
It wasn't a big focus, actually.

10:04.600 --> 10:07.520
So this company was more opportunistic in that sense.

10:07.520 --> 10:14.520
So we, um, we basically talked to a lot of companies in a very diverse, uh, set of sectors.

10:14.520 --> 10:21.160
So from finance to, you know, to, uh, manufacturing, you know, to retail.

10:21.160 --> 10:24.720
And we basically, you know, we were very good at going in and talking to these people

10:24.720 --> 10:28.160
and say, okay, what is your, you know, what is your, what is your problem look like?

10:28.160 --> 10:29.920
What is your, where is your opportunity?

10:29.920 --> 10:31.160
What data do you have?

10:31.160 --> 10:35.520
And then we did a quick assessment and we made a recommendation and, uh, perhaps it's

10:35.520 --> 10:37.120
very quick demo.

10:37.120 --> 10:41.920
And then, um, and if they were happy, then we went for slightly longer sort of trial

10:41.920 --> 10:46.680
where we would actually, you know, implement, you know, a system to just show them that,

10:46.680 --> 10:51.000
you know, we can, you know, you can actually get value out of, out of the data that they

10:51.000 --> 10:52.080
have.

10:52.080 --> 10:55.560
And if they, they were still, you know, happy after that, and this was like maybe six

10:55.560 --> 10:59.120
months later, then we would go into a, you know, an actual implementation of the whole

10:59.120 --> 11:00.120
thing.

11:00.120 --> 11:04.840
And so we repeated this many times as many companies which gave the team an enormous

11:04.840 --> 11:09.000
amount of, uh, sort of experience with a very broad spectrum of problems.

11:09.000 --> 11:12.160
And I think that's what made the team also very attractive.

11:12.160 --> 11:20.480
Uh, so that acquisition was in 2017, you're a couple of years in now at Qualcomm.

11:20.480 --> 11:26.040
You know, this thesis of kind of the umph of a larger company, uh, creating some opportunities

11:26.040 --> 11:27.040
for you.

11:27.040 --> 11:28.440
Did that thesis bear out?

11:28.440 --> 11:36.320
The interesting part of working for Qualcomm for me, um, was that I, um, have always taken

11:36.320 --> 11:41.200
compute for granted, um, basically if I needed to compute something, then, you know, there's

11:41.200 --> 11:45.000
this computer, which has some chips in it and it will do its job.

11:45.000 --> 11:52.040
Um, but with the advance of deep learning, um, we see that, you know, bigger models, um,

11:52.040 --> 11:53.040
just perform better.

11:53.040 --> 11:56.320
And so compute becomes a really important part of the equation.

11:56.320 --> 12:02.480
Um, and so, you know, a good way to make progress, um, is to make sure that your algorithm

12:02.480 --> 12:04.240
actually runs extremely efficiently.

12:04.240 --> 12:09.760
And so of course, uh, you know, the, um, the GPUs came and, you know, they made compute,

12:09.760 --> 12:12.960
deep learning compute a lot more efficient, which is part of why things are going so

12:12.960 --> 12:18.440
well with deep learning, um, but this is a very fundamental problem of question, right?

12:18.440 --> 12:24.760
It's like, okay, so, you know, a brain doesn't compute, you know, with, uh, 32 bits precision,

12:24.760 --> 12:26.480
um, it's very noisy.

12:26.480 --> 12:32.320
Um, so should we, you know, change our compute paradigm in, you know, in our computer as

12:32.320 --> 12:33.320
well?

12:33.320 --> 12:37.920
Should we, you know, should we maybe try to train with a lot less precision, maybe

12:37.920 --> 12:40.120
a couple of bits precision?

12:40.120 --> 12:44.400
Um, and, um, do these neural networks have to be so huge, right?

12:44.400 --> 12:48.680
These neural networks typically have like even hundreds of millions of parameters, sometimes

12:48.680 --> 12:52.680
billions of parameters, do they necessarily have to be that big because that can, can

12:52.680 --> 12:56.480
choose a lot of, uh, memory and compute as well.

12:56.480 --> 13:01.440
Um, and also, you know, if you go a little deeper, if you dig a little deeper in the problem,

13:01.440 --> 13:06.120
then what you find is all these, all these parameters of these big models, they are living

13:06.120 --> 13:11.360
in, uh, sort of off-shift memory called DDR, and you have to bring them to the registers

13:11.360 --> 13:15.440
where you do all the Mac computes, and multiply and accumulate computes.

13:15.440 --> 13:20.040
Um, and that movement of data costs a lot of energy.

13:20.040 --> 13:23.160
And so, but in the brain, actually, it isn't so separated.

13:23.160 --> 13:28.720
We call that separation as a normal architecture, where memory and computer are separated.

13:28.720 --> 13:33.880
Um, but in, uh, in a brain, actually, what you find is that the memory and the computer

13:33.880 --> 13:39.800
are very close, right, because, you know, things are stored in these, uh, synapses, um, between

13:39.800 --> 13:44.200
neurons and, and the neurons compute, um, and so much more distributed.

13:44.200 --> 13:49.080
And, um, so, so there is a lot of fundamental questions of, can we, can we just change the

13:49.080 --> 13:55.600
compute paradigm, um, so that we can do things far more energy efficient, um, and that's,

13:55.600 --> 14:01.880
that's basically what I find the biggest thrill of working in Qualcomm in actually trying

14:01.880 --> 14:09.440
to make that happen, um, and then scaling up, uh, sort of AI computations, a lot, which

14:09.440 --> 14:13.640
might actually be, you know, one of the big reasons why we're making a lot of progress.

14:13.640 --> 14:17.600
And then, uh, yeah, basically shipping, whenever you have such a thing, and you build, you

14:17.600 --> 14:22.880
know, a chip on a, on a new paradigm or a new principle, you can then put them in a

14:22.880 --> 14:26.880
chip, and you can then ship them to, you know, a billion customers in a billion phones,

14:26.880 --> 14:27.880
right?

14:27.880 --> 14:31.160
And that's the scaling you were asking about, which is extremely exciting.

14:31.160 --> 14:38.080
You've got joint appointments, both with, uh, in academia and Qualcomm, uh, that's becoming

14:38.080 --> 14:41.600
increasingly common, but everyone kind of manages it differently.

14:41.600 --> 14:44.880
Uh, what's the relationship for you?

14:44.880 --> 14:49.200
How is your, your work and research distributed across those appointments?

14:49.200 --> 14:50.200
Yeah.

14:50.200 --> 14:54.480
So I have like about half an appointment in academia and half an appointment in Qualcomm

14:54.480 --> 14:55.480
and at Qualcomm.

14:55.480 --> 14:56.480
Okay.

14:56.480 --> 15:03.000
Which I think is actually a very good distribution, um, so it typically, in the university, you

15:03.000 --> 15:09.600
work on more of fundamental questions, which are very far out, um, and don't have a, you

15:09.600 --> 15:15.280
know, sort of a, a horizon of being, you know, productized or, you know, finding applications

15:15.280 --> 15:16.600
of four years, maybe.

15:16.600 --> 15:23.280
So at, at Qualcomm, we work on, you know, on problems, which between, you know, uh, one year

15:23.280 --> 15:29.800
and four years or five years, um, will find an application within the company, um, in

15:29.800 --> 15:34.560
academia, you're completely free to do whatever you want and it could be, you know, 100 years

15:34.560 --> 15:35.800
out if you wanted to.

15:35.800 --> 15:36.800
Mm-hmm.

15:36.800 --> 15:41.200
So it's interesting to have one lag in sort of both of these, uh, ecosystems in both

15:41.200 --> 15:42.600
of these environments.

15:42.600 --> 15:45.520
It does strike me that some of the things that we're talking about here, coming up with

15:45.520 --> 15:52.320
fundamental new compute architectures, you know, perhaps that are more inspired by, uh,

15:52.320 --> 15:56.680
the brain and, and synaptic and neural architectures, things like that.

15:56.680 --> 16:01.160
That could be, uh, very far reaching, uh, research.

16:01.160 --> 16:07.760
It is, are you working on, uh, that, uh, that kind of work in the academic setting as

16:07.760 --> 16:08.760
well?

16:08.760 --> 16:16.120
Um, so the, the compute, um, sort of thinking about how compute interacts with, um, you

16:16.120 --> 16:20.880
know, uh, with AI and machine learning and intelligence, that's something that I exclusively

16:20.880 --> 16:22.320
do at Qualcomm.

16:22.320 --> 16:28.760
Although it did start, um, in academia, so I was doing, um, Bayesian deep learning.

16:28.760 --> 16:34.960
So Bayesian statistics is a particular paradigm, um, as statistical paradigms, you have frequent

16:34.960 --> 16:40.400
distant Bayesian sort of statistics and Bayesian statistics, you've put probability distributions

16:40.400 --> 16:41.400
over your model.

16:41.400 --> 16:44.680
You basically say, I don't know, you know, what my model really is.

16:44.680 --> 16:50.640
Um, I say I have some probability distribution, um, over my possible parameters of my model

16:50.640 --> 16:55.200
and then when I see data, I'm going to narrow down that distribution of parameters to the

16:55.200 --> 16:59.080
ones that are actually describing the data that I see.

16:59.080 --> 17:02.640
Um, so, so that's something that wasn't really applied at deep learning.

17:02.640 --> 17:07.240
And so we started to apply that statistical paradigm to deep learning, which was a challenge

17:07.240 --> 17:10.000
because there was, you know, millions and millions of parameters there.

17:10.000 --> 17:11.800
You have to sort of handle that way.

17:11.800 --> 17:18.840
Um, but what we found is, um, that we can use, we could use that paradigm to, uh, basically

17:18.840 --> 17:24.880
compress a neural network, um, you know, with a factor of 100, uh, without losing any

17:24.880 --> 17:28.600
of the accuracy, which was actually a big shock to me, which is like, okay, so we working

17:28.600 --> 17:32.920
with these models, which are, we'd have a million parameters, but you could train the

17:32.920 --> 17:33.920
same model.

17:33.920 --> 17:38.280
You could just keep one out of 100 parameters and throw away all the rest, um, and you

17:38.280 --> 17:41.960
would have a model of which would function exactly the same way, which would not be any

17:41.960 --> 17:44.160
worst than the one that you started with.

17:44.160 --> 17:48.200
Um, and so that's generally known as, as compression neural network compression.

17:48.200 --> 17:50.560
So they are heavily over parameterized.

17:50.560 --> 17:51.560
Mm-hmm.

17:51.560 --> 17:54.040
So and we did that within the paradigm of Bayesian deep learning.

17:54.040 --> 17:59.520
It was a very good fundamental tool, um, to, you know, to do that compression.

17:59.520 --> 18:08.960
Did you apply this compression to these Bayesian deep learning models or is there, uh, something

18:08.960 --> 18:16.560
fundamental about Bayesian deep learning that allows it to, um, you know, that synergistic

18:16.560 --> 18:18.280
with this kind of compression?

18:18.280 --> 18:19.280
Yeah.

18:19.280 --> 18:20.280
So it's more the latter.

18:20.280 --> 18:21.280
Okay.

18:21.280 --> 18:22.880
Um, so it's basically, so you have a neural network.

18:22.880 --> 18:27.200
Because we do this kind of compression with, uh, with traditional CNNs and, and the like

18:27.200 --> 18:28.200
as well, right?

18:28.200 --> 18:29.520
Yes, that's right.

18:29.520 --> 18:36.800
So, so basically, um, it's one way, one technique to do this compression on a neural network.

18:36.800 --> 18:42.560
It has some additional advantages, which is that you can also, um, sort of expression

18:42.560 --> 18:46.880
on certainty over a prediction, right? So you can actually say, you know, I think it's

18:46.880 --> 18:52.320
this class or, you know, uh, this is happening in an image, but, you know, I'm 80% certain

18:52.320 --> 18:54.800
that that's actually correct, right?

18:54.800 --> 18:58.280
And so the Bayesian paradigm also allows you to do that.

18:58.280 --> 19:03.640
But, um, you can also use it to prune large parts of your neural network away.

19:03.640 --> 19:06.400
So it's sort of a principle way of doing that.

19:06.400 --> 19:10.480
Um, and so we started doing that and that's, you know, coming back to academia for this

19:10.480 --> 19:15.080
talk, um, so we started doing that in academia because that was a very academic exercise at

19:15.080 --> 19:16.080
that point.

19:16.080 --> 19:19.160
But it became practical, um, because we could compress these neural nets.

19:19.160 --> 19:22.160
And then we, you know, Qualcomm, we, we took it to Qualcomm.

19:22.160 --> 19:28.760
And now there's a whole team, uh, led by Tamin, um, Blancavort, who is basically, um,

19:28.760 --> 19:33.560
trying all, you know, sorts of algorithms to try to compress these neural networks, you

19:33.560 --> 19:35.720
know, in the, in the most practical way.

19:35.720 --> 19:38.040
And often the Bayesian way is not the most practical way.

19:38.040 --> 19:43.480
You know, it could be perhaps like a very good way, but it's not yet a very hands-off,

19:43.480 --> 19:44.680
you know, way to do things.

19:44.680 --> 19:50.640
And so some of these other methods, which are much simpler to understand, um, can also

19:50.640 --> 19:53.320
compress these neural networks to basically the same degree.

19:53.320 --> 19:57.520
And those are the ones that actually make it into the toolkits that we use at Qualcomm.

19:57.520 --> 20:01.800
It sounds like there's kind of a dynamic relationship.

20:01.800 --> 20:05.720
I guess as one would expect between the kind of things you're working on from an academic

20:05.720 --> 20:12.680
perspective and what you're doing at Qualcomm, although, um, you know, they differ in their

20:12.680 --> 20:13.680
timeframes.

20:13.680 --> 20:18.360
Yeah, they, you know, and it is a little bit like that.

20:18.360 --> 20:23.800
So, um, so another maybe great example, which I'm very enthusiastic about, is, um, work

20:23.800 --> 20:32.160
that I do with Taco Cohen and Maurice Weiler, um, on, um, so one is, uh, was a PhD student,

20:32.160 --> 20:37.520
now, uh, full-term employee at Qualcomm, Taco Cohen, and then Maurice Weiler is now, uh,

20:37.520 --> 20:42.280
PhD student at Qvalab, so Qvalab is actually a lab funded by Qualcomm at the University

20:42.280 --> 20:43.280
of Amsterdam.

20:43.280 --> 20:49.280
Um, and so that, so that work is, um, to include symmetries, um, in deep learning.

20:49.280 --> 20:56.440
So in other words, if I, you know, turn my head, um, the objects that I see, you know, turn

20:56.440 --> 21:02.120
around, um, in, in my brain, um, however, it's still the same objects, right?

21:02.120 --> 21:03.840
And so that's what we call a symmetry.

21:03.840 --> 21:07.880
If I, you know, if I move something, then it's still the same object, even though it,

21:07.880 --> 21:13.840
it has moved, um, and so incorporating these types of symmetries into neural networks

21:13.840 --> 21:20.120
is actually a very powerful way to make them better, turns out, um, and, um, and so, you

21:20.120 --> 21:23.920
know, with Taco Cohen, we started that process, um, you know, almost, I don't know, six

21:23.920 --> 21:28.120
years ago, or something like that, um, and it became quite successful, and now very

21:28.120 --> 21:33.080
recently, um, we did something that actually is quite, you know, amazing, I would say.

21:33.080 --> 21:38.640
So we started to use the mathematics of, um, general relativity, which is, uh, you

21:38.640 --> 21:45.000
know, the fundamental theory of gravity, uh, made by Einstein, um, and the same mathematics

21:45.000 --> 21:48.680
is actually in quantum field theory, which is, you know, behind the standard model, which

21:48.680 --> 21:53.200
is the fundamental theory, but particles, um, and this, this, this, this, this theory is

21:53.200 --> 21:59.120
called gauge theory, um, and it was actually the topic for my PhD thesis, which is interesting,

21:59.120 --> 22:05.120
I would say, so that came back after 20 years, um, and we started to incorporate these mathematical

22:05.120 --> 22:11.400
ideas into deep learning, um, and now we can do deep learning on arbitrary manifold.

22:11.400 --> 22:15.960
So you can think of a sphere, like the, the earth, and you want to detect storms or other

22:15.960 --> 22:21.640
weather patterns, um, on the earth, um, then you can use this particular tool, or, you

22:21.640 --> 22:26.960
can deep learning tool to, to, to analyze, you know, these, these manifolds, um, you

22:26.960 --> 22:33.440
can also think about, um, you know, in VR, right, in, in virtual reality, um, you generate,

22:33.440 --> 22:36.760
you know, maybe a game or something like that, so you generate objects in your world,

22:36.760 --> 22:42.360
these are synthetic, um, and you can sort of, uh, put texture, high resolution texture

22:42.360 --> 22:45.040
on these objects using, you know, this kind of tool.

22:45.040 --> 22:51.040
And so it's a very, very fundamental, you know, exercise, academic exercise to take, you

22:51.040 --> 22:56.400
know, these mathematical theories or, or, that are used in theoretical physics and put

22:56.400 --> 23:00.600
them into a deep learning algorithm, um, but then actually when you're done, once you're

23:00.600 --> 23:04.440
done, and you, you look, you know, at it from a distance, you, you discover all these

23:04.440 --> 23:09.120
beautiful applications, actually, um, and that's, I think the perfect, you know, I think

23:09.120 --> 23:13.120
the perfect line of thinking, right, so you, you start with something very fundamentally,

23:13.120 --> 23:18.680
you make big progress, um, you make an impact on, you know, a lot of, you know, the whole

23:18.680 --> 23:23.040
field and then you find that there is all sorts of interesting applications popping, popping

23:23.040 --> 23:26.400
up, um, that you hadn't thought, thought about before.

23:26.400 --> 23:33.280
Uh, and so, uh, some questions on that, uh, you started out talking about, uh, the work

23:33.280 --> 23:39.600
that you were doing around symmetry, uh, and then transitioned into the, the, the gauge,

23:39.600 --> 23:46.040
uh, networks, are those, uh, I didn't catch the relationship between those or those, uh,

23:46.040 --> 23:48.920
is that work specifically related to the one lead to the other?

23:48.920 --> 23:49.920
Yeah.

23:49.920 --> 23:50.920
Yeah, yeah, yeah, absolutely.

23:50.920 --> 23:55.120
So, um, yeah, that's a good question because I went very fast over that, but in, in, in,

23:55.120 --> 24:00.800
so, and it follows precisely the same, uh, steps as we're done in physics, right?

24:00.800 --> 24:07.240
And I think in, uh, you know, 19, no, six or so, um, Einstein came with his special theory

24:07.240 --> 24:13.720
of relativity, which basically described how different observers who move at a constant

24:13.720 --> 24:17.160
speed relative to each other, see different phenomenon.

24:17.160 --> 24:24.600
And, and he and Maxwell, uh, found that, um, basically magnetism will turn into electricity

24:24.600 --> 24:29.920
if you, you change observer and, you know, and, and move it with a constant speed, um,

24:29.920 --> 24:35.080
versus, so if a, if a, if a static observer sees an electric, you know, electric field

24:35.080 --> 24:37.920
and, and moving observer will see a magnetic field.

24:37.920 --> 24:41.120
Um, and so, um, and so that's the symmetry, right?

24:41.120 --> 24:44.720
It's a constant symmetry, it's like, you rotate something or you, you know, you rotate

24:44.720 --> 24:51.760
the whole, you know, earth around or something like that, um, now, uh, after that, um, came,

24:51.760 --> 24:57.360
you know, Einstein generalized that into general relativity and he said, well, actually,

24:57.360 --> 25:02.680
it's, it's not just, you know, these global symmetries, but actually, you know, we should

25:02.680 --> 25:07.000
be able to have a larger set of symmetries, which is basically, I should be able to,

25:07.000 --> 25:13.600
you know, to change speed, you know, and, and acceleration at every, any point in time,

25:13.600 --> 25:19.080
and I can sort of change my frame of reference at, differently at every point in space

25:19.080 --> 25:20.080
time.

25:20.080 --> 25:24.040
Um, and that's a much more general theory, it's called a local, uh, symmetry.

25:24.040 --> 25:26.520
So it's, it's a, it's a much more general symmetry.

25:26.520 --> 25:30.640
It's called a local symmetry and a local symmetry is also referred to as a gate symmetry.

25:30.640 --> 25:35.720
And it turns out when you think about that, that kind of large, a set of symmetries, then

25:35.720 --> 25:41.080
he figured out, for instance, that acceleration and gravity are actually, you know, the same

25:41.080 --> 25:45.440
phenomenon, but observed by, you know, one person who is standing still and another person

25:45.440 --> 25:48.760
who is accelerating relative to the other person.

25:48.760 --> 25:52.640
And so now again, you know, you'll, you'll have symmetries incorporated in your theory

25:52.640 --> 25:54.960
and your theory becomes a lot richer.

25:54.960 --> 25:55.960
So that's happened.

25:55.960 --> 25:58.360
So exactly that progression has happened for us too.

25:58.360 --> 26:03.760
So we started with these global symmetries, um, and then we turned to local symmetries

26:03.760 --> 26:10.240
and, you know, we went from basically, you know, doing deep learning on flat images to

26:10.240 --> 26:12.600
do deep learning on curved manifolds.

26:12.600 --> 26:13.600
Interesting.

26:13.600 --> 26:14.600
Yeah.

26:14.600 --> 26:20.400
When you first started talking about symmetries, the, uh, the thing that came to mind was,

26:20.400 --> 26:25.240
you know, that maybe you were going after the same type of problem as Jeffrey Hinton,

26:25.240 --> 26:31.960
uh, who you've worked with and, uh, his capsule networks, uh, but it sounds like, uh,

26:31.960 --> 26:34.200
you ended up in very different places.

26:34.200 --> 26:37.040
But I would say that, um, you're actually quite right there.

26:37.040 --> 26:42.680
So it turns out that, um, this is, you know, this is a little hard to explain, but out

26:42.680 --> 26:48.520
of this theory of symmetries, um, capsules emerge quite naturally.

26:48.520 --> 26:52.080
Um, and it's a bit hard to explain how that precisely works.

26:52.080 --> 26:56.960
Um, but it's, it's true that what you'll have is you'll have these neurons.

26:56.960 --> 27:01.920
Um, so when you, when you apply translational invariance, these neurons will now be

27:01.920 --> 27:05.760
what we call feature maps, which is an entire sort of filter image.

27:05.760 --> 27:11.560
Um, and when you apply additional symmetries like rotation, you get sort of stacks of filter

27:11.560 --> 27:17.440
maps, um, that sort of, that sort of transform into each other if you rotate the underlying

27:17.440 --> 27:18.440
image.

27:18.440 --> 27:21.320
And that's, that stack of filter maps is what we call a capsule.

27:21.320 --> 27:26.120
And that's actually what Jeff Hinton also talks about when he, when he talks about capsules.

27:26.120 --> 27:27.120
Mm-hmm.

27:27.120 --> 27:30.360
Now he has this sort of dynamic routing algorithm, which is, which is something on top

27:30.360 --> 27:34.960
of these capsules, which, you know, we haven't implemented in our code yet.

27:34.960 --> 27:38.320
Um, but there, these two things are actually remarkably related.

27:38.320 --> 27:40.360
So you were actually quite right there.

27:40.360 --> 27:48.680
So within the, the gauge CNNs, you have this concept of a capsule, uh, as well.

27:48.680 --> 27:50.000
Is that correct?

27:50.000 --> 27:55.760
It's, I would say, uh, yes, but it's also already in the sort of normal, uh, sort of, uh,

27:55.760 --> 27:57.560
what we call group CNNs.

27:57.560 --> 28:04.760
So it's, it, so before we did gauge CNNs with the local symmetry, um, capsules also naturally

28:04.760 --> 28:12.480
appear in, you know, these, uh, these groups, uh, like a global rotation, uh, or a translation

28:12.480 --> 28:14.240
or something like that.

28:14.240 --> 28:24.360
And, uh, beyond the conceptual, uh, well, you've got this, this, uh, common conceptual

28:24.360 --> 28:33.560
foundation between, uh, Hinton's capsule networks and your gauge, echrovariant, CNNs are,

28:33.560 --> 28:39.200
are there other relationships between the two, um, that grot of these, or what are the

28:39.200 --> 28:45.120
relationships between the two that kind of grot of these, uh, kind of the shared feature

28:45.120 --> 28:48.120
maps and in variances within the networks?

28:48.120 --> 28:54.240
Yeah, I think it, it would be a fairly technical discussion to try to explain that, but I

28:54.240 --> 29:00.640
just, let me, let me limit myself to saying that, um, sort of in Jeff, in Jeff Hinton's

29:00.640 --> 29:07.600
theory, um, he, he, he basically says, okay, so I, I want my feature maps to be divided

29:07.600 --> 29:16.280
up in, in these capsules and then basically the particular configuration within a capsule,

29:16.280 --> 29:22.520
you know, is the pose of something, um, and then, you know, the, maybe the, you know,

29:22.520 --> 29:27.880
the strength of, you know, how much of that capsule is present, you know, is, is, is,

29:27.880 --> 29:33.520
it, that indicates how strongly a particular object is seen in the image.

29:33.520 --> 29:39.920
Um, so he, he goes in with, uh, sort of an intuition and he, he, he builds it in, um,

29:39.920 --> 29:44.200
where in our case, we start with the symmetry from the principle of symmetry and actually

29:44.200 --> 29:48.520
the math, you know, basically drives us to these capsules.

29:48.520 --> 29:52.760
But in the end, these, they were, they're actually completely the same thing.

29:52.760 --> 29:58.120
So, so I would say we laid a mathematical foundation for the capsules that, that, Hinton's

29:58.120 --> 30:01.720
intuition sort of, uh, brought, um, yeah.

30:01.720 --> 30:05.200
And so, you know, there's, so he, he has the Namooka routing, which is something that

30:05.200 --> 30:08.640
is sort of on top of it, which, which we don't do necessarily.

30:08.640 --> 30:13.480
We, you know, we, we went into the direction of, of course, Gage Equivareans, um, but it's,

30:13.480 --> 30:18.640
you know, we, we are basically in parallel sort of developing, uh, these ideas, I would

30:18.640 --> 30:19.640
say.

30:19.640 --> 30:20.640
Okay.

30:20.640 --> 30:30.040
And so is the idea with these Gage CNNs related to, uh, are they more compact or do

30:30.040 --> 30:35.840
they open up new, uh, applications or do they perform better?

30:35.840 --> 30:38.560
What, what does this approach buy us?

30:38.560 --> 30:45.120
Right. So, um, so it's, it's mostly, if you want to do a deep learning on a, on something

30:45.120 --> 30:46.720
that's not a plane, right?

30:46.720 --> 30:49.480
So the manifolds, yeah.

30:49.480 --> 30:54.640
So imagine, you know, you want to, you know, so of course, a simple example is, think

30:54.640 --> 31:00.080
of the earth and think of a signal on the earth, like maybe temperature or, you know, wind

31:00.080 --> 31:03.520
patterns or something like that, and you want to predict, you know, maybe the weather

31:03.520 --> 31:07.600
in 10 days or something like that, or you want to find where is the storm or where is

31:07.600 --> 31:11.920
the, you know, the weather pattern that you're interested in.

31:11.920 --> 31:12.920
Yeah.

31:12.920 --> 31:17.480
I also did a really interesting interview with a woman named Nina Mielan, who does, uh,

31:17.480 --> 31:24.080
or Mielane, who does a Python package, Jams, that's that it's focused on doing statistics

31:24.080 --> 31:25.080
on manifolds.

31:25.080 --> 31:30.200
And the example that she gave was pretty interesting, it's like if you want to do statistics

31:30.200 --> 31:37.000
or learning on something like a human heart in medicine, uh, it is, uh, you know, much

31:37.000 --> 31:44.000
more naturally amenable to dealing with it as a set of manifolds or curves than in a,

31:44.000 --> 31:47.000
you know, a typical rectilinear space.

31:47.000 --> 31:48.000
Yeah.

31:48.000 --> 31:49.000
So exactly.

31:49.000 --> 31:53.320
So that's on our to-do list, and we are actually collaborating with people who know much

31:53.320 --> 31:57.760
more about, you know, these medical applications than we do.

31:57.760 --> 32:00.320
But that's certainly one of the applications area.

32:00.320 --> 32:01.320
So application area.

32:01.320 --> 32:05.800
So you can think of a, of a beating heart or something like that, then you would put

32:05.800 --> 32:08.760
like a mesh on that beating heart.

32:08.760 --> 32:13.760
Um, and you could sort of occur to mesh as opposed to a straight mesh.

32:13.760 --> 32:17.120
Well, the mesh has to live on the manifold, right?

32:17.120 --> 32:21.320
So there's a lot of nodes you distribute over this mesh and you connect them by little

32:21.320 --> 32:23.040
lines, basically.

32:23.040 --> 32:29.280
Um, and so, and so then you can try to maybe, you know, predict whether this heart is,

32:29.280 --> 32:35.280
um, has, you know, strange behavior or something like that as abnormal behavior, um, or you

32:35.280 --> 32:39.960
could try to figure out, you know, where's, you know, where certain pieces of the heart

32:39.960 --> 32:40.960
are.

32:40.960 --> 32:42.200
Are you want to segment them out or something like that?

32:42.200 --> 32:45.200
Maybe you want to detect where the vessels are, the valves are or something like that,

32:45.200 --> 32:46.200
right?

32:46.200 --> 32:51.200
So certainly, you know, that's a, that's a prime example of where we have a, a manifold,

32:51.200 --> 32:54.600
um, and where we want to do deep learning on that manifold.

32:54.600 --> 32:55.600
Um, yeah.

32:55.600 --> 32:58.440
And so that's certainly something that we are planning to do.

32:58.440 --> 32:59.440
Okay.

32:59.440 --> 33:06.160
And talking about this mesh, um, you also bring up the idea or brings up for me, the idea

33:06.160 --> 33:19.080
of like graphs, um, is the gauge CNN kind of fundamentally a graph CNN or, um, is that

33:19.080 --> 33:21.280
something that you're working on as well?

33:21.280 --> 33:23.080
You're asking really good questions, I would say.

33:23.080 --> 33:32.040
So in fact, um, one of the reasons why, um, you know, we are exploring this kind of mashed

33:32.040 --> 33:39.160
version of gate CNNs, which is we put a mesh on the manifold and then we send messages

33:39.160 --> 33:45.760
between the nodes over the edges, um, is that it looks a lot like a graph convolution,

33:45.760 --> 33:51.320
which is, which is another object where you do deep learning on graphs.

33:51.320 --> 33:57.440
But, um, a manifold, even a mashed manifold is not a graph because, um, this has something

33:57.440 --> 34:00.640
to do with the fact that the neighbors are not sort of exchangeable.

34:00.640 --> 34:02.800
The neighbors are, are not a set.

34:02.800 --> 34:07.080
The neighbors are actually ordered, um, they, they, you know, something to their right

34:07.080 --> 34:09.680
has a different meaning than something to the left.

34:09.680 --> 34:15.840
And so people sometimes actually do graph convolutions on these meshes, um, but that's

34:15.840 --> 34:17.880
actually slightly suboptimal.

34:17.880 --> 34:24.600
And so the way we do it, so this, this gauge equity very neural networks, um, is precisely,

34:24.600 --> 34:28.720
um, designed to do it optimally in, in a way, to do it the right way.

34:28.720 --> 34:34.200
And we have a PhD student now at Qualcomm was Qualcomm and PhD student at University of Amsterdam,

34:34.200 --> 34:37.600
Pim de Hanh, was precisely working on this topic.

34:37.600 --> 34:42.720
So he's precisely trying to nail this topic of, you know, um, what is the difference between

34:42.720 --> 34:49.280
a graph convolution and this mashed gauge convolution and, you know, is there a real practical

34:49.280 --> 34:53.760
advantage above, you know, doing it correctly in a mathematical sense?

34:53.760 --> 34:57.760
You know, is there also a practical advantage of doing it in this particular way?

34:57.760 --> 35:04.920
Uh, so all of these, uh, we've talked about, uh, things from kind of very optimization-focused

35:04.920 --> 35:16.040
ideas like compression to Bayesian deep learning and gauge, uh, CNNs within, within Qualcomm,

35:16.040 --> 35:24.880
these are all more research oriented topics, as opposed to, um, kind of product oriented

35:24.880 --> 35:25.880
topics.

35:25.880 --> 35:27.080
Is that right?

35:27.080 --> 35:33.280
Well, um, I would say that the, um, it's true to some degree, but I would say that compression,

35:33.280 --> 35:39.440
neural network compression and quantization, um, has immediate, uh, sort of practical

35:39.440 --> 35:40.440
applications.

35:40.440 --> 35:45.800
And, in fact, you know, the team of time in blank, time in Blunkervort is already implementing

35:45.800 --> 35:49.600
these methods into a toolbox, which is going to be commercialized.

35:49.600 --> 35:54.280
So, and the reason why this is important is that, um, if I have my neural network that

35:54.280 --> 35:58.400
I'm very fond of, and I trained it in the cloud, maybe using TensorFlow or PyTorch or

35:58.400 --> 36:03.240
something like that, um, and now I want to run it on my phone, um, but it's too big to

36:03.240 --> 36:07.320
run on my phone, um, because it takes too much energy and then my phone doesn't have that

36:07.320 --> 36:08.320
much memory.

36:08.320 --> 36:09.320
Right?

36:09.320 --> 36:14.520
So, what I want to provide to the customer is a tool that automatically compiles this

36:14.520 --> 36:20.280
big neural network into a much smaller neural network that basically has the same performance,

36:20.280 --> 36:26.880
um, as the bigger one, um, but runs on our Snapdragon, uh, sort of AI chipsets, um, you

36:26.880 --> 36:28.960
know, very, very efficient to be.

36:28.960 --> 36:34.760
And, you know, doing that is actually, you know, you need to, you know, get the neural

36:34.760 --> 36:41.600
network into a much more lean sort of, um, framework, um, and then you also need to compile

36:41.600 --> 36:46.480
the operations that this neural network has to do, which is basically matrix multiplications,

36:46.480 --> 36:48.120
matrix vector multiplications.

36:48.120 --> 36:52.600
Um, you have to compile them in such a way that they run very quickly on the particular

36:52.600 --> 36:57.280
piece of hardware, um, and, and you can also optimize that a lot.

36:57.280 --> 37:01.480
Um, and so we are, for instance, also working on, um, on algorithms.

37:01.480 --> 37:07.280
So this is Chang-Yong-Oh and Stratus-Govus, where, you know, people at, uh, this Q-Valab, um,

37:07.280 --> 37:13.840
we are working on what we call Bayesian optimization algorithms, which is, um, algorithms which optimize

37:13.840 --> 37:17.240
over a very large discrete space, right, of choices.

37:17.240 --> 37:22.520
So the choices are, you know, should I first, you know, do this particular multiplication

37:22.520 --> 37:25.640
and then this addition or should I first do it the other way around?

37:25.640 --> 37:29.920
So there is an exponential number of possible choices that you have there in optimizing that

37:29.920 --> 37:30.920
code.

37:30.920 --> 37:37.920
Um, and so how can you quickly explore, um, all of these, all of these possibilities?

37:37.920 --> 37:44.400
And so Chang-Yong developed a beautiful algorithm that, that, with a minimal number of trials,

37:44.400 --> 37:51.040
um, quickly zooms in on the optimal sort of, uh, configuration of these discrete choices.

37:51.040 --> 38:03.120
In order to apply a Bayesian optimization to this kind of problem, you first need to expose

38:03.120 --> 38:09.280
the, I guess the features or the, uh, kind of the control levers, if you will, of the

38:09.280 --> 38:14.720
problem, which operation goes first versus second, those aren't necessarily, necessarily

38:14.720 --> 38:18.760
naturally, uh, expose from the models.

38:18.760 --> 38:22.280
Is that right? Do, do you have to kind of do something or build something in order to

38:22.280 --> 38:25.800
be able to apply Bayesian optimization to these kinds of problems?

38:25.800 --> 38:26.800
Yeah.

38:26.800 --> 38:29.240
So typically you can think of two levels.

38:29.240 --> 38:34.880
So, um, let's, let's say on a phone, um, so if I have a particular proposal of doing

38:34.880 --> 38:40.560
my computations, um, then I can, you know, compile that onto the phone, the actual physical

38:40.560 --> 38:43.240
phone, and run it and measure how well I was doing.

38:43.240 --> 38:47.080
Now, of course, that takes some time, right, we are talking about, you know, maybe seconds

38:47.080 --> 38:51.640
or something like that, I don't know, um, to actually measure, you know, that, that

38:51.640 --> 38:52.640
configuration.

38:52.640 --> 38:55.200
And so if you do it that way, you go very slow.

38:55.200 --> 39:00.320
Um, but you can also build a simulator, right, and the simulator would, um, basically

39:00.320 --> 39:05.360
figure out in approximation how good that particular configuration would be.

39:05.360 --> 39:07.240
But it's uncertain, right?

39:07.240 --> 39:11.880
And so you can imagine that, um, you sort of, you know, you first do a couple, you know,

39:11.880 --> 39:15.440
a couple of steps of optimization on the simulator, which is very fast.

39:15.440 --> 39:19.600
And then when you get too uncertain, you then go to the physical device and you try a

39:19.600 --> 39:24.920
few things, um, to see how, how well you're doing, you didn't update, update your simulator

39:24.920 --> 39:29.640
or the sort of surrogate model that, that estimates how well you're doing.

39:29.640 --> 39:33.080
Um, and then you sort of keep optimizing there again.

39:33.080 --> 39:38.360
So there is this basically game that you play, um, you can do, you can measure things

39:38.360 --> 39:42.920
very precisely, but quite expensively, or you can measure things very quickly, but sort

39:42.920 --> 39:47.120
of approximately, right? And, you know, this is the choices that you have and you have

39:47.120 --> 39:52.600
to navigate those choices as quickly as possible to get to the final best possible configuration.

39:52.600 --> 39:59.360
One of the characteristics of dealing with, uh, devices that need to be produced in

39:59.360 --> 40:04.800
silicon is relatively long lead times, as opposed to software products.

40:04.800 --> 40:10.160
Can you talk a little bit about productizing these types of ideas in, in that kind of

40:10.160 --> 40:11.160
environment?

40:11.160 --> 40:16.000
Yeah, so I would say, um, we mostly work on software, right?

40:16.000 --> 40:23.160
So in, so in, in, in many ways, what we do is, um, can, can be quite quickly productized,

40:23.160 --> 40:27.680
I would say, because it's basically a software tool or, you know, enhancement of some kind.

40:27.680 --> 40:28.680
Okay.

40:28.680 --> 40:31.280
Um, but we do work with hardware folks, right?

40:31.280 --> 40:37.000
So we do work on, you know, exciting new hardware developments, um, Princess Computing

40:37.000 --> 40:41.280
Memory is something, you know, it's, it's well known, so a bunch of startup companies

40:41.280 --> 40:46.840
are also working on that, um, that's basically where you would, instead of moving that data

40:46.840 --> 40:53.680
that I talked to you about before, from, uh, the DDR memory to the chip, you would basically

40:53.680 --> 40:56.800
do the computation directly in that memory cell.

40:56.800 --> 41:02.800
So you would directly do your computation in, you know, in, in memory, which is actually,

41:02.800 --> 41:08.200
you know, analog, so this is actually, you know, faults and currents and stuff like that.

41:08.200 --> 41:13.200
Um, and so now the game is, and that's a very interesting game, you know, the game is,

41:13.200 --> 41:19.480
you know, you try to develop a piece of hardware, um, that runs optimally for a particular

41:19.480 --> 41:22.800
algorithm, like a deep learning algorithm in this case.

41:22.800 --> 41:27.480
Um, and at the same time, you're trying to adapt the deep learning algorithm, uh, to work

41:27.480 --> 41:30.800
as well as possible on that particular piece of hardware.

41:30.800 --> 41:35.600
And so this is a trend, um, that you can see much more generally, which is that hardware

41:35.600 --> 41:40.720
design and software design are starting to become more and more integrated and entangled

41:40.720 --> 41:41.880
with each other.

41:41.880 --> 41:46.040
It's not just here, but it's in many other places where you can see that, you know, a piece

41:46.040 --> 41:51.760
of hardware being replaced by pieces of software run on, sort of, uh, a deep learning engine

41:51.760 --> 41:57.760
on a chip, um, you know, you have, you know, an, in a heterogeneous compute environment

41:57.760 --> 42:02.440
with many different types of compute, like DSP and CPU and GPU, et cetera.

42:02.440 --> 42:07.440
Um, if you're faced with a certain computation, you will have to distribute that computation

42:07.440 --> 42:12.320
across all of these, uh, you know, different compute engines, and you have to think smartly

42:12.320 --> 42:13.640
about how to do that, right?

42:13.640 --> 42:17.840
And again, there is a controller, sort of an intelligent agent that will have to learn

42:17.840 --> 42:19.080
how to do this efficiently.

42:19.080 --> 42:25.080
So you can see that software machine learning or, you know, learnable sort of software,

42:25.080 --> 42:29.160
um, and hardware are going to get tight, more and more tightly integrated, which I think

42:29.160 --> 42:31.000
is a very fascinating development.

42:31.000 --> 42:37.360
Let's maybe shift gears and talk a little bit about, uh, kind of forward looking, uh,

42:37.360 --> 42:38.360
ideas.

42:38.360 --> 42:46.160
You recently wrote, uh, posts about the, uh, kind of responding to a rich Sutton blog.

42:46.160 --> 42:49.400
I'll let you maybe talk a little bit about the background and then the posts, but it kind

42:49.400 --> 42:56.200
of ponders this idea of, you know, what's most important models versus data versus compute.

42:56.200 --> 43:02.240
Um, can you talk a little bit about that, that work and how you see that, um, kind of

43:02.240 --> 43:05.720
playing out, uh, in the, the future of the space?

43:05.720 --> 43:06.720
Yeah.

43:06.720 --> 43:11.480
So I think this was more like my, sort of rainy Sunday afternoon, sort of write up about

43:11.480 --> 43:16.440
something that, you know, I threw out there and it actually got a lot of attention.

43:16.440 --> 43:17.760
So it was interesting.

43:17.760 --> 43:22.000
Instead of hit the right, I guess the right nerve, people are very interested in that kind

43:22.000 --> 43:23.000
of thing.

43:23.000 --> 43:25.960
Um, and it, it's really about a super fundamental problem.

43:25.960 --> 43:30.400
And I think, you know, researchers, uh, we should talk about this more because it, it

43:30.400 --> 43:34.960
might determine from any young researchers in the field, you know, where they want to

43:34.960 --> 43:36.840
head with their particular research.

43:36.840 --> 43:42.120
Um, and so I was actually very grateful to rich that he posted that particular post.

43:42.120 --> 43:47.320
Um, he basically said something and I made, I, I may, you know, charge you a little bit,

43:47.320 --> 43:54.920
but it's like, um, you know, uh, we should really not, um, try to model all that much, um,

43:54.920 --> 44:01.240
because in the end, uh, if we focus on scaling our architectures or sort of more general

44:01.240 --> 44:07.600
purpose, um, machine learning architectures, um, if we wait long enough, then, you know,

44:07.600 --> 44:12.680
using Moore's law, you'll basically get to a point where you always get beaten by these

44:12.680 --> 44:18.520
kind of, uh, sort of scalable algorithms that are basically just, you know, eating data

44:18.520 --> 44:20.520
and, and turning them into predictions.

44:20.520 --> 44:24.800
Um, and, you know, there's been a lot of examples that actually, you know, where this was

44:24.800 --> 44:25.800
actually the case, right?

44:25.800 --> 44:31.640
So we had, um, you know, we had models of speech where, you know, people had built, you

44:31.640 --> 44:39.480
know, models of the human, you know, uh, voice tract, um, and they were sort of, you

44:39.480 --> 44:43.920
know, they had, they were, they were modeling how people produce speech and then they were,

44:43.920 --> 44:48.360
you know, matching that with the actual observations and then trying to figure out, you know, what,

44:48.360 --> 44:52.840
what the mouth, how the mouth actually moved and therefore what word was being spoken.

44:52.840 --> 44:58.360
Um, and later people found, well, you know, if you just collect enough data, then, um,

44:58.360 --> 45:03.400
you can just map, you know, basically take, take the, the audio signal that hits your

45:03.400 --> 45:09.480
microphone, right, and, and map it, learn to map it to the words that produced it.

45:09.480 --> 45:14.520
Um, and if you have enough of those pairs, like audio signal and word, um, then this,

45:14.520 --> 45:19.960
this sort of, in, in some sense, stupid if you want, uh, sort of a statistical tool, uh,

45:19.960 --> 45:23.640
vastly outperformed, um, these kind of more mechanical tools.

45:23.640 --> 45:31.000
Um, and this happened in speech and it happened in, in, in, also in, um, in computer vision,

45:31.000 --> 45:35.880
where, um, basically the best methods are now these deep learning methods where you basically

45:35.880 --> 45:40.600
collect a huge amount of data on images, you segment them, you tell me what's the objects

45:40.600 --> 45:45.720
in the image and then you train all that data and you get now with them that performs very,

45:45.720 --> 45:47.240
very well, better than anything else.

45:47.240 --> 45:55.640
I mean, this all goes back to the, I think the most, uh, quoted example of this, uh, and

45:55.640 --> 46:01.480
there are many are the Peter Norvig, uh, unreasonable effectiveness of data paper, where he talks

46:01.480 --> 46:06.040
about Google, you know, they're not better at this because they have better algorithms,

46:06.040 --> 46:08.280
it's because they just have more data.

46:08.280 --> 46:09.800
Absolutely, right?

46:09.800 --> 46:13.800
So a lot of these internet companies are in the business of getting data.

46:14.520 --> 46:18.840
Um, and by having all that data, they can do things that, you know, we didn't

46:18.840 --> 46:25.720
thought were possible before, um, and this was a very valuable lesson, right? So, you know, collecting more data and

46:25.720 --> 46:31.560
having the compute to do the computations, um, is basically, you know, one of the reasons why

46:31.560 --> 46:37.080
we're seeing all this progress, um, but the, but it begs the question, how far can we take it?

46:37.080 --> 46:38.680
Right? This particular idea.

46:38.680 --> 46:42.840
And there's other people like Josh Tenenbaum, who has been, who have been saying the opposite.

46:42.840 --> 46:48.760
So he's been saying, well, you know, um, the world actually operates in the other direction.

46:48.760 --> 46:55.480
Which means that, you know, physics, um, basically the dead, the data generating process

46:55.480 --> 47:00.920
is much more like we have objects in the world, these objects move under the laws of physics,

47:00.920 --> 47:05.000
or maybe the laws of psychology or sociology when we interact with each other.

47:05.880 --> 47:08.520
Um, there's causal relationships, right?

47:08.520 --> 47:10.760
You know, things cause other things to happen.

47:10.760 --> 47:15.160
And then finally, you know, signals hit our sensors, right?

47:15.160 --> 47:16.840
And that's what's being recorded.

47:16.840 --> 47:21.400
So that's the direction of, you know, the physics of the world into the sensors.

47:21.400 --> 47:23.480
That's what we call the data generative model.

47:24.040 --> 47:26.280
And then the, and then deep learning does the opposite.

47:26.280 --> 47:33.240
It goes from these signals on the sensors and directly shortcuts a path to predict,

47:33.240 --> 47:35.880
you know, what were the objects which were producing these signals.

47:36.840 --> 47:40.040
Um, and actually the brain interestingly does something similar, right?

47:40.040 --> 47:45.160
So we, we have in our brain the ability to, to simulate the world, right?

47:45.160 --> 47:48.920
I can, I can close my eyes and I can imagine, you know,

47:48.920 --> 47:54.440
what it means to ride on a horse or something like that or to fall off a building, right?

47:54.440 --> 47:57.160
And I can just see it happen under the laws of physics.

47:58.120 --> 48:02.760
At the same time, I can also instantaneously recognize objects in the world.

48:02.760 --> 48:09.400
So there's these pathways in our brain which just take in sensory data and immediately produce

48:09.400 --> 48:14.040
segment the world into objects and, and sort of tell me what's in the world without me thinking

48:14.040 --> 48:17.560
about it. So in, in our brain, we have these two modalities as well.

48:17.560 --> 48:21.160
And, and, and Kanamon calls this slow and fast thinking basically.

48:21.160 --> 48:23.800
So it is two quite different pathways to, to think.

48:24.680 --> 48:29.160
And so, um, so the question becomes, you know, how far can we take this good of data driven

48:29.160 --> 48:34.840
approach? And, and my, my, my post was about, well, humans are a lot better in certain things

48:35.400 --> 48:41.880
than current algorithms. Algorithms have trouble with generalizing away from the domain

48:41.880 --> 48:46.680
in which they are trained, right? If I train an algorithm to play go, and then I tell it,

48:46.680 --> 48:51.560
okay, now play chess, right? Or, or play go on a smaller board even or, you know,

48:51.560 --> 48:55.240
with different color stones or something, it gets confused because it wasn't trained for that.

48:55.240 --> 49:01.480
Right. For example, that, that always comes to mind for me was a video of, um,

49:02.360 --> 49:07.480
and Peter Abil's lab training a robotic arm to think untangle a rope.

49:07.480 --> 49:12.040
And it does great on a green, you know, when the rope is on a green table,

49:12.040 --> 49:15.640
but when it's on a red table, and I'm making these colors up, but the idea is the same.

49:15.640 --> 49:20.680
When it's on a red table, it totally fails. It's just, it's, the models are that dependent on the

49:20.680 --> 49:27.640
specifics of, uh, the, with the environment in which they're trained. Yeah. And so, um, and,

49:27.640 --> 49:32.040
and that's a very good example. And so, um, you know, maybe another example is, you know,

49:32.040 --> 49:35.800
I was driving on a road a couple of times, and then, you know, there was road works. And what

49:35.800 --> 49:41.240
they did was they kept all the white sort of lane dividers on the road, and they just put,

49:41.240 --> 49:46.600
you know, yellow ones, you know, on completely different positions. And everybody knew what to do,

49:46.600 --> 49:53.480
even though the, the white lane dividers were still there. Um, and I'm pretty sure an algorithm,

49:53.480 --> 49:57.240
you know, when it's not trained on that, will that get totally confused?

49:57.880 --> 50:03.640
Right. And what, so what's the underlying problem is that, um, so they don't generalize because

50:03.640 --> 50:10.440
they don't understand the world in this generative data generating way. So, and, and, and,

50:10.440 --> 50:14.600
you know, what Josh Tannenbaum has been saying about the others have been saying is that you

50:14.600 --> 50:21.240
cannot collect nearly enough data to, you know, to capture all of your corner cases, right? I mean,

50:21.240 --> 50:25.400
it's like, there's so many things that can happen in the world. It's like an exponential number

50:25.400 --> 50:31.240
of things that can happen in the world. And many things are very rare. Um, and so we need ways to

50:31.240 --> 50:38.280
generalize the lessons we learn in one context into sort of a completely different context. And,

50:38.280 --> 50:43.000
you know, one school of thought is that the only way to do that is to understand the world in a

50:43.000 --> 50:49.880
generative way. So, because, and the reason is that the generative direction is much more efficient

50:49.880 --> 50:55.240
because, you know, it's, it's modular. Yeah, we have things that, you know,

50:55.240 --> 51:01.880
relatively independently do things operate on these are the objects and the agents in the world,

51:02.600 --> 51:09.960
right? And they follow causal laws. They follow the laws of physics. And there's very few parameters

51:09.960 --> 51:14.280
in that, right? We know that, you know, the, the laws of physics eventually have very few parameters.

51:15.080 --> 51:21.480
Um, and so, so that generative direction is a lot simpler than the opposite direction.

51:21.480 --> 51:26.440
You said something really interesting in there, though, uh, the entire phrase was, was understand

51:26.440 --> 51:32.440
the world in a generative way. When I think of most of the generative models that we're talking

51:32.440 --> 51:38.200
about, there's not any understanding there. It's just spitting out probabilistically the, the best

51:38.200 --> 51:45.080
next thing. Yeah. So, so understand, you know, thinking about, uh, what it really means to understand

51:45.080 --> 51:50.040
something is a whole different area. But you could think about, you know, predicting the future,

51:50.040 --> 51:54.440
right? So let's say, understand, if you understand the world at some level, I can predict the future

51:54.440 --> 52:01.400
better, right? And now let's imagine, you know, um, I need to do some of a prediction. And I don't

52:01.400 --> 52:06.920
know the causal laws. If I know the laws of physics and I know the laws of psychology and I know

52:06.920 --> 52:12.120
what causes what it's going to be far easier for me to predict the future than if I don't have all

52:12.120 --> 52:18.120
of that. Um, in fact, I can train something in one context and if I understand how physics work

52:18.120 --> 52:23.000
and I get into a completely different context, I can still predict the future, right? Um,

52:23.000 --> 52:29.400
because I understand the causal mechanisms. And so, so the claim is that the world is a lot

52:29.400 --> 52:35.320
simpler in the causal direction or in the sort of physical sort of data generative direction.

52:35.320 --> 52:41.240
And it's a lot more complicated than the opposite direction. Yet, if I define my application

52:41.240 --> 52:47.720
area narrowly enough, I can collect a huge amount of data on that particular narrowly defined

52:47.720 --> 52:53.400
problem. And then the inverse, you know, going directly from the sensors back to the predicting

52:53.400 --> 52:58.600
the object is going to be more effective, right? Because that's the deep learning direction.

52:59.240 --> 53:04.360
So with sufficient data, you know, that is the direction in which you actually also want to predict,

53:04.360 --> 53:10.200
which is the inverse direction from the generative model. And, you know, that is the most effective

53:10.200 --> 53:14.760
way of doing things if you have a lot of data and you have to find your domain narrowly enough.

53:14.760 --> 53:19.240
And I think therefore, we need to find sort of a middle ground. We basically have to say if,

53:19.240 --> 53:23.640
if, you know, if we don't know much about a domain, if we get thrown into a new situation,

53:23.640 --> 53:28.600
we need to rely on our our generative models of the world. And we need to, you know,

53:28.600 --> 53:33.720
try to, you know, invert those in order to make predictions. Yet, if we collect enough data

53:33.720 --> 53:39.560
in a certain domain, then we can form this direct pathway from the sensors directly to making

53:39.560 --> 53:44.840
predictions. And then that's going to be the most effective and accurate way of, of making predictions.

53:44.840 --> 53:50.440
How do you take action on this idea or how do we as a community of practitioners and,

53:50.440 --> 53:55.240
and researchers kind of take action on this idea? Well, it depends a little bit on what you're

53:55.240 --> 54:00.440
interested in, right? So if you're interested in a narrow domain problem, like you want to predict

54:00.440 --> 54:05.160
speech or you want to do speech translation or something like that, then you should go with the

54:05.160 --> 54:09.560
deep learning approach, right? Because it just that, just the best thing because you can collect a lot of data

54:09.560 --> 54:15.960
and works best. If you're interested in solving general AI, so developing, sort of agents that

54:15.960 --> 54:22.280
are versatile and that can operate in many different circumstances, right? Then I think, you know,

54:22.280 --> 54:27.800
you may have to start thinking about integrating these two models. And so what we have been doing

54:27.800 --> 54:40.600
in the lab, in sort of my sort of M lab at the university, is we have been taking a sort of somewhat

54:40.600 --> 54:45.960
older paradigm, which we call graphical models. So graphical models were models, which were very

54:45.960 --> 54:52.040
popular, like maybe 10 years ago or so, where you would take sort of nodes in a graph. They seem

54:52.040 --> 54:57.960
to be coming back a little bit, don't they? Maybe. I hear a ton about graphical models nowadays.

54:57.960 --> 55:02.600
Like in recent NURPs, the past couple of years in NURPs, a lot of people seem to be doing work

55:02.600 --> 55:08.840
around graphical models. Okay, well, it seems to stand to reason that, you know, something that is

55:08.840 --> 55:17.080
good eventually will find its way back. The deep learning fire has burned out a little bit. And then,

55:17.080 --> 55:21.800
of course, you know, you then you then get a phase where you can start to synthesize things,

55:21.800 --> 55:25.240
where you can say, okay, we had this in the past. We have now, there's both are really good things.

55:25.880 --> 55:29.640
Let's see if we can sort of combine them a little bit. So this is some of the work that we are

55:29.640 --> 55:36.120
doing. So we have sort of a model where we will sort of generate, you know, we have a generated

55:36.120 --> 55:42.440
model, let's say a Kalman filter, which is a dynamical model of, you know, somebody moving around

55:42.440 --> 55:49.000
in the world and sort of observing sort of things about the world. And in these graphical models,

55:49.000 --> 55:54.920
every node means something, right? That's the agent's position at every point in time is one node

55:54.920 --> 56:00.600
in this graph. And if you want to figure out, let's say, okay, if I have these sort of,

56:00.600 --> 56:07.320
these partial observations about the world, like a bunch of images maybe taken from the agent,

56:07.320 --> 56:13.960
can I try, can I infer, you know, where the agent was? And that you can do with something that we

56:13.960 --> 56:18.920
call inference. So that's a different inference than we these days, we call inference on the deep

56:18.920 --> 56:24.440
neural net. This is a probabilistic inference where we basically say, what's the probability

56:24.440 --> 56:30.760
distribution of this person being at this position at this point in time given all the observations

56:30.760 --> 56:38.680
I have right now. And so that's actually a message passing scheme. So you send messages over the

56:38.680 --> 56:43.960
edges of this graph to figure this out. That was belief propagation. And it was a very popular

56:43.960 --> 56:49.560
research topic, you know, 10 years ago. Now we also have something. So the other option would

56:49.560 --> 56:56.440
be to collect a huge amount of data. Basically, you know, person is here and observes this,

56:56.440 --> 57:00.520
if person is here observes this, right? So if you collect a huge amount of data, you can also do it

57:00.520 --> 57:07.640
the opposite way. You can say, okay, map directly from my observations, you know, back to the positions

57:07.640 --> 57:12.440
of this particular person, because I happen to have that data available. And if I have enough of

57:12.440 --> 57:17.880
that data, then I can actually do a better job, typically, than my Kelvin filter, because my,

57:17.880 --> 57:24.760
you know, big, my Kelvin filter has like strong assumptions on linearity and Gaussianity,

57:24.760 --> 57:31.720
um, basically build in it. And if the world isn't, you know, linear, um, then you're in trouble,

57:31.720 --> 57:37.640
because, you know, you'll make, you'll make wrong predictions. Um, but if you just build this

57:37.640 --> 57:42.120
neural network, which goes in the opposite direction, takes the observations and directly maps onto

57:42.120 --> 57:46.600
the, onto the locations, then, you know, if you put enough parameters in it and you have enough

57:46.600 --> 57:52.600
data, then you can train it very, very accurately, right? So now the trick becomes, okay, so if I put

57:52.600 --> 57:58.920
into a new environment, I don't have data. So I will rely on this sort of, you know, clunky

57:58.920 --> 58:03.400
Kelvin filter, but it will give me a fairly good estimate. And then as I go and collect more and

58:03.400 --> 58:08.120
more data, as I mean, that is, as I live in that environment, I'm going to train up the model in

58:08.120 --> 58:14.280
the opposite direction. Um, and, um, that at some point will become more accurate than my Kelvin

58:14.280 --> 58:20.280
filter. Um, and then, you know, I just basically switch to the sort of, to the, to the deep learning

58:20.280 --> 58:26.200
sort of solution. And so this is one example where, you know, we have a message passing scheme on a

58:26.200 --> 58:32.360
graph that, you know, automatically switches between either the old-fashioned sort of, um, inference

58:32.360 --> 58:37.480
and graphical model to the sort of more modern sort of graph convolutional neural networks.

58:38.200 --> 58:47.960
Interesting. A lot of the folks that are pursuing deep learning as a path to AGI kind of

58:47.960 --> 58:53.720
ultimately feel like the, you know, this thing that we call AGI or maybe the step before AGI will be

58:55.160 --> 59:02.920
an ensemble of perhaps many, uh, deep models as opposed to, you know, one single Uber model.

59:03.720 --> 59:11.720
If you kind of apply that same thinking to what you're describing, you can envision an ensemble

59:11.720 --> 59:21.560
of many deep models and many generative models. And I'm almost thinking of like a hybrid car.

59:21.560 --> 59:25.400
If you've ever, if you've been in a Prius recently, you might have seen that little picture where

59:25.400 --> 59:31.720
they show you like whether it's the battery that's driving the motor or the, the motor that's kind

59:31.720 --> 59:36.680
of charging the battery, kind of this back and forth flow, depending on what's happening and what

59:36.680 --> 59:43.320
the model is being or the agent is being exposed to that's determining, you know, whether we're,

59:43.960 --> 59:49.400
um, you know, relying more on the deep model in any particular point in time or the generative

59:49.400 --> 59:54.200
model. Is that kind of the way you, you know, a way that you can see this evolving?

59:54.920 --> 01:00:01.160
Yeah. So that's certainly, um, the way I see this evolving. So, um, let me stay with the example

01:00:01.160 --> 01:00:07.240
of a car, maybe so you couldn't actually a modern car, even a self-driving car, would still have

01:00:07.240 --> 01:00:13.160
a whole bunch of rule sets to cover all the corner cases. Um, because you can't collect enough data

01:00:13.160 --> 01:00:19.320
on these corner cases, but maybe in the future, you know, there are enough, you know, for a particular

01:00:19.320 --> 01:00:24.360
set of corner cases, let's say, uh, you know, you're driving on freeways, you can do fine, you've

01:00:24.360 --> 01:00:30.120
collected a lot of data, but now you turn into Amsterdam, right? And there's so many exceptions

01:00:30.120 --> 01:00:35.320
and difficult situations where bicycles will crush you and pedestrians will do weird things and

01:00:35.320 --> 01:00:40.280
walk through red lights and all that, what's happening in Amsterdam. So you cannot rely on,

01:00:40.280 --> 01:00:45.160
on just a learned model because it will fail, right? And so you have to basically go back to,

01:00:45.160 --> 01:00:51.160
you know, if this, then that, if this, then that, um, and, um, and you have to know when to switch,

01:00:51.160 --> 01:00:57.480
right? And so, and then maybe I can look back to Bayesian statistics. So you have to understand

01:00:57.480 --> 01:01:02.440
when you don't understand, right? So it is deep learning model or this machine learning model.

01:01:02.440 --> 01:01:08.680
Well, basically have to figure out, okay, so I'm running out of my, you know, domain where I'm

01:01:08.680 --> 01:01:14.680
trained. Um, I'm starting to fail here. I'm going to switch over to rules or I'm going to switch

01:01:14.680 --> 01:01:20.680
over to the actual driver, um, to make sure I don't get into an accident. Um, and it's this

01:01:20.680 --> 01:01:26.520
interaction where, you know, and, and then you can imagine where a lot of cars are slowly,

01:01:26.520 --> 01:01:31.080
you know, getting into new situations and they're all learning distributively, you know,

01:01:31.080 --> 01:01:35.080
collectively, they're learning about some of these corner cases and they get embedded into the

01:01:35.080 --> 01:01:39.640
model and then we slowly switch, you know, to using that model. And what's interesting to me about

01:01:39.640 --> 01:01:48.280
that example is that the, the premise seems to be that we will get to a point where the complexity

01:01:48.280 --> 01:01:54.360
of the environment is too much for the learned model and then we need to switch to rules.

01:01:54.360 --> 01:02:01.800
Uh, but a lot of the, where learn models have proven themselves to be effective and powerful or,

01:02:01.800 --> 01:02:07.320
you know, these situations where we can't come up with the rules because the rules are too complex.

01:02:07.960 --> 01:02:14.360
Yeah. So, you know, clearly, um, in many situations where you have enough data, you should not

01:02:14.360 --> 01:02:20.520
use rules, um, because, you know, it's just not good enough, right? Um, in fact, you should just

01:02:20.520 --> 01:02:26.120
train that mapping directly. But the advantage of rules is that you can express them in human

01:02:26.120 --> 01:02:31.640
language, right? I mean, we know in some sense when they're driving a car, right? You know, we know,

01:02:31.640 --> 01:02:36.760
you know, when to stop for a red light, you know, when to, you know, when, when, when somebody is,

01:02:36.760 --> 01:02:41.480
you know, passing the street or something like that or complicated combinations of situations,

01:02:42.520 --> 01:02:49.240
we know how to, you know, express them in human language and turn them into a rule. I mean,

01:02:49.240 --> 01:02:52.920
there's going to be a gigantic number of rules. I think it's, I don't know, genre,

01:02:52.920 --> 01:02:58.440
million lines of codes in a car or something like that. So it's like huge. Um, but, you know,

01:02:58.440 --> 01:03:03.960
it's, it is in some sense, you know, a backup system for situations that you can't cover with

01:03:03.960 --> 01:03:09.800
your deep learning yet. And as I understand it, um, actually, I've, when I heard a talk about this,

01:03:09.800 --> 01:03:17.480
uh, by Raquel Earthasoon actually, uh, working for Uber, uh, she mentioned that, um, uh, a large number

01:03:17.480 --> 01:03:22.760
of, you know, the, the, you know, the large fraction of the intelligence of a car is actually

01:03:22.760 --> 01:03:28.600
still rule based. I'm sure they want to move more to deep learning, but it's still a pretty large

01:03:28.600 --> 01:03:35.400
fraction is still rule based. Awesome. Well, Max, this has been an amazing discussion. Uh, we could

01:03:35.400 --> 01:03:42.520
continue. I'm sure for, uh, another hour, but, um, I really appreciate you taking the time to,

01:03:42.520 --> 01:03:49.160
uh, uh, jump on with us and to share a bit about what you're working on. Thanks. It was a pleasure

01:03:49.160 --> 01:03:58.280
talking to you. Fantastic. Thank you very much. All right, everyone. That's our show for today.

01:03:58.280 --> 01:04:04.040
If you like what you've heard here, please do us a huge favor and tell your friends about the show.

01:04:04.040 --> 01:04:08.840
And if you haven't already hit that subscribe button yourself, make sure you do so you don't miss

01:04:08.840 --> 01:04:14.680
any of the great episodes we've gotten in store for you. As always, thanks so much for listening

01:04:14.680 --> 01:04:44.520
and catch you next time.

