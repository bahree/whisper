Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Two weeks ago we celebrated the show's third birthday and a major listenership milestone.
And last week we kicked off the second volume of our listener favorite AI platform series,
sharing more stories of teams working to scale and industrialize data science and machine
learning at their companies.
We've been teasing that there's more to come and today I am super excited to announce
the launch of our inaugural conference, Twimblecon AI platforms.
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices
necessary to scale the delivery of machine learning and AI in the enterprise.
Now you know Twimble for bringing you dynamic practical conversations via the podcast and
we're creating our Twimblecon events to build on that tradition.
The event will feature two full days of community oriented discussions, live podcast interviews
and practical presentations by great presenters sharing concrete examples from their own experiences.
By creating a space where data science, machine learning, platform engineering and ML ops
practitioners and leaders can share, learn and connect, the event aspires to help see
the development of an informed and sustainable community of technologists that is well equipped
to meet the current and future needs of their organizations.
Some of the topics that we plan to cover include overcoming the barriers to getting machine
learning and deep learning models into production, how to apply ML ops and DevOps to your machine
learning workflow, experiences and lessons learned in delivering platform and infrastructure
support for data management, experiment management and model deployment, the latest approaches
platforms and tools for accelerating and scaling the delivery of ML and DL and the enterprise.
Some deployment stories from leading companies like Google, Facebook, Airbnb, as well as
traditional enterprises like Comcast and Shell and organizational and cultural best practices
for success.
The two day event will be held on October 1st and 2nd in San Francisco and I would really
love to meet you there.
EarlyBurt Registration is open today at Twimblecon.com and we're offering the first ten listeners
who register the amazing opportunity to get their ticket for 75% off using the discount
code TwimbleFirst.
Again, the conference site is Twimblecon.com and the code is TwimbleFirst and now on to
the show.
All right, everyone.
I am on the line with Karen Levy.
Karen is an assistant professor in the Department of Information Science at Cornell University.
Karen, welcome to this week in machine learning and AI.
Thanks.
Nice to join you.
So, Karen, you have a background in both law and sociology and you're currently teaching
in an information science department.
How did that come about?
Yeah, so I started out.
I went to law school and then I worked in a federal court for a couple of years after
that and then kind of in the course of that, I got interested in sort of the social causes
of the legal problems that came through the court on a fairly regular basis.
So I went and got a PhD in sociology and when I was in graduate school, I got really
interested in technology as kind of a route towards social control, right?
Like a different way that people's life chances are impacted besides the law.
So I started to look more at kind of intersections between the law and technology and how people's
decisions are made and their social interactions are structured as a result of the technologies
around them.
It landed me in an information science department and yeah, that's where I am now.
A lot of your focus now is on the social aspects of surveillance and monitoring.
Maybe tell us a little bit about kind of your, the types of research that you do around
that area.
Sure.
So yeah, I'm really interested in surveillance.
So when I, my kind of framework for thinking about technology is that I'm really interested
in the way we use technologies to enforce rules and sometimes those rules are like state
or federal laws and sometimes they are organizational rules or even kind of like expectations and
norms of behavior within intimate relationships like families or friendships.
But I'm really interested in how we use technology to sort of enforce all of those expectations
and one of the ways that we often do that is through surveillance, right, is through
creating a record of the things that people do as it kind of means by which to tell whether
or not they're following the rules or not.
And so I look at that in a variety of different contexts, but the kind of the two big contexts
where I've spent the most time and energy and my research are workplaces.
So I've done a fair amount of work on surveillance in the workplace and intimate context.
So I do a lot of work on families and sexual relationships, intimate relationships, trying
to understand the role that technology plays in how people relate to one another there.
You know, I think one of the things that I'm realizing as we're starting to talk about
this topic is that I find it scary, like the thought of jumping into this conversation
because while I tend to be very optimistic about technology, particularly AI, the whole
surveillance thing kind of freaks me out sometimes.
You're laughing, are you familiar with that kind of take on it?
Well, I mean, I like the idea that within the first five minutes of us starting to talk
you get scared.
I don't know what that says about the research, but yeah, I mean, so there is kind of, certainly,
you could characterize my interest in technology as being sort of about the dark side, right?
About kind of maybe the nefarious ways people use technology or the unintended consequences
that the use of these technologies might have on particularly vulnerable groups of people.
I'm definitely interested in that, but I don't kind of approach it as like a technology
naysayer.
Like I think we ought to implement, I think, you know, I'm like a, I don't think we ought
to be let-ites, right?
Like I think there's a lot of positive roles that technologies can plan our lives, but
I think doing that in a way that's attentive to, you know, ethical and privacy concerns,
and particularly the role that technologies can have on marginalized groups that are marginalized
in all different sorts of ways, right?
Economically or socially, like it doesn't make any sense to deploy things without paying
attention to that.
Right, right.
So we've got this relationship that's developing between artificial intelligence and data
collection where AI has really been enabled by an array of data collection technologies
and really the increased digitization both in our lives and our work.
And it's also hungry for data and so it drives more data collection.
What are, you know, when you think about just the increase in the amount of data that's
being collected about us, like how do you, do you have like a taxonomy or a framework
for thinking about the different impacts that this has?
So my sense is that maybe, you know, to taxonomize different sorts of data, like I don't
do that in a formal way, but I do think like one of the promises and perils of, you know,
the scale and granularity of the data collection that we do now is how readily repurposed or combine
different data sources are, right?
Like that's part of the beauty of what we can do with data now.
But it also creates risks for people that are perhaps unforeseeable, right?
And so because of that, I think, you know, proceeding with caution makes a lot of sense.
And, you know, having kind of a good feel for how, you know, again, particularly marginalized
communities might be differentially impacted by data collection.
So like an example that I like to use in my teaching is about like census population data,
right?
Which feels like kind of not that interesting maybe, right?
Like censuses are just, they seem like pretty, you know, kind of general high level data collection.
But there are all these examples in the past of how that data gets, you know, reused perhaps
years later for all kinds of different purposes, including like human rights abuses, right?
This is something that my colleague Alvaro Badoya at Georgetown has written a lot about.
So things like that, right?
Like things like sort of the lives that data can have later on, right?
Or in the context of like, I've done a bunch of work with some other folks here at Cornell
about intimate partner violence, right?
Like thinking about how the data you generate on your phone or on your computer, you know,
might be very interesting to a partner, to an abusive partner, right?
Or me actually like reveal quite a bit more about your whereabouts than you might anticipate
or know, you know, those are really important considerations for people in the way people
experience, you know, technology and privacy and security.
And so I think paying attention to those kind of like really like my almost mundane like
day-to-day exchanges we have that involve our data, you know, it's not all about our relationship
with kind of like the big tech companies, right?
It's also about our relationships with one another and how those get mediated through
the data trails that we generate.
As we're thinking about this, do you, how do you structure that for, does your research
aim to like structure for that for us like how we should be thinking about this or are
you more kind of exploring different different stories and how folks are impacted?
Yeah, I mean, I would say a lot.
So certainly there are some design implications, right?
So I think paying attention to examples of things that have happened in the past can
help us to think more critically about how we design systems for the future.
But a lot of it I think is actually what you said is like kind of telling these stories,
right, of how people, how people's lives end up changing or end up, you know, impacted
in these different ways based on their interactions with data intensive systems.
And, and I think honestly a lot of the time like we tend to kind of use the most readily
available tool.
So one kind of theme that runs through a lot of my research is how, you know, oftentimes
we might use a data driven system to address a problem that might actually be better addressed
using some other tool, right?
So like, for example, I've done a bunch of work about truck drivers and trying to understand
how truck drivers are affected by the technologies that are used to track them.
And the reason why we use those technologies is because truck drivers are really tired,
right?
They're overworked and they become unsafe.
And so we use technologies to try and ensure that they're not breaking federal rules about
how much they can drive.
That's fine, right?
That's one approach.
Another approach that I argue would be actually much more effective is just to change like
some of the labor laws around trucking, right?
To change the way we incentivize different types of work, right?
Like structural changes to the organization of the industry, you know, that's not necessarily
a data intensive solution, but you know, it's not the one we've adopted, right?
And so oftentimes I think we tend to use technology as kind of this like band-aid solution
for problems that are inherently economic or social or cultural, but we tend to approach
them as technology problems.
And that often I think is the source of some of these unintended consequences for people.
In the case of the truck drivers, it strikes me that, you know, one thing that that illustrates
is that the technology can be more accessible to the individual groups, for example, trucking
companies or truck drivers, you know, as opposed to, you know, structural changes, some
of those structural regulatory changes that you're describing require, you know, broad
consensus across a large influence group of people and organizations.
Does that play a role in some of those choices?
Yeah.
I mean, I think the accessibility of technology, you know, as you mentioned, right, the
inquiry, like the cheapness with which we can start to gather data and analyze data definitely
lends itself to that being sort of an attractive solution for solving problems.
And I think that can often be like a really positive thing.
But as you say, right?
Like that often might keep us from actually addressing some of the structural problems
that might be more effective at kind of getting at the root cause of a problem.
So in trucking, right, like you can monitor a driver to find out if he's super tired,
right?
Like that's pretty easy to do.
But what's much harder is like making sure he doesn't, he isn't incentivized to get that
tired in the first place.
And you can see why that's politically more difficult, economically more difficult.
But if you don't change that structure, then you're going to end up, you know, with
almost this arms race, right, where people are still going to want to act the way they're
incentivized to act.
And you put technology in their place as sort of a roadblock, but, you know, you haven't
actually addressed the root of the problem.
Kind of that, you know, there's certainly merit in addressing the structural issues.
Is there something wrong with using technology as just another tool to help drive the kind
of behavior that we want?
Or in other words, you know, what are some of the implications on these drivers of the
surveillance technology that's been put in place to monitor their behavior?
Yeah.
So, I mean, technology can certainly be like a really useful tool in the, in the toolkit
for enforcing behavior, you know, for incentivizing behaviors that we think are desirable or safer
or better for society, like it absolutely can have a role.
If we depend on it a lot, I think what we end up doing is making responsible the parties
who often have like the least social and economic power, right?
So essentially, like in trucking, for example, the way that these technologies end up functioning
is to kind of punish drivers for doing like what they almost have no choice but to do, right?
Like essentially, you know, in order to make ends meet, they've kind of violated the law
for decades, like for generations and everybody kind of knows that, right?
That's like a known fact within the industry.
And now they get kind of hit from both sides because they're still incentivized to do all
those things, but now they're also punished for doing them, right?
And they kind of bear the brunt of that.
And so I think the consequence of sometimes using technology as kind of like the first course
of action is that oftentimes it can have those impacts on maybe the person with the least
structural power.
And that like merits a subnormative consideration, right?
We have to ask ourselves, is that the way we want to address social problems is by kind
of, you know, enforcing these rules kind of at the low end of the spectrum?
And sometimes the answer might be yes, right?
Or sometimes it may be that a problem is so pressing or, you know, so consequential
that what that we do want to make sure that people like have to follow the rules.
But we also want to ask like, well, why are they motivated not to follow the rules in
the first place?
And might we do something to actually improve their quality of life or, you know, change
kind of the circumstances such that they're not putting out in that position?
Right.
It sounds like there may be an element of the situation that these drivers are in that
we haven't fully explored.
They're both required by someone not to work too much.
There's some bounds placed on the number of hours, consecutive hours that they work
without rest, but it sounds like there are also incentives on them working more, you
know, beyond those limits, you've referred to incentives just to, you know, earn a living,
put food on the table, all of that kind of thing.
Are there also incentives on the part of their companies to do that?
And maybe, you know, let's explore this relationship between the technology and what the companies
are asking these drivers to do.
Because otherwise, it leaves open questions as to, you know, why the driver is not more
responsible for their individual behavior in this situation.
Yeah.
I mean, like so one parallel that you might, that maybe is, you know, an easy one to wrap
your head around is, is, you know, thinking about like drug laws or something, right?
We can say like, oh, you know, we really, like it's against the law to use these drugs,
right? It's against a lot to do, you know, a variety of things.
And we can enforce those rules, you know, and we often do, right?
Like using technology, but it doesn't change the fact that people use and sell drugs,
right?
And so better enforcement, more consistent enforcement, like may have a role in the way
we choose to fight that problem, but it doesn't actually change the underlying structure
of neighborhoods or the opportunities people have available to them, right?
Like it's a piece of the puzzle that it's easy to rely on, but it's not the whole puzzle.
And I think, you know, what you brought up about kind of like the network of interests,
right?
Like the companies that truckers work for, things like that.
Yeah, that does like definitely play a role in the way we, you know, think about technology
and rules.
So like, what ends up happening in tracking is that companies buy these systems that track
drivers, right?
Because the government actually now requires drivers to be, to have their time tracked
it early.
And then companies say like, well, we bought this system.
But actually now is like pretty cheap or free for us to like also run a bunch of other
analytics on how drivers are doing, meaning like how much fuel, what's their fuel economy,
right?
Like, you know, are they saving enough money for the company?
Are they driving efficiently?
Are they not breaking hard?
Things like that.
And so they end up actually tracking a much wider swath of driver's behavior.
And so it definitely changes the relationship to the company in that it means that drivers
end up actually getting managed in this more granular like real time way, in this way
that they weren't before.
And this is true across a lot of industries, like a lot of low wage workers end up being
supervised really closely.
It's interesting to me in trucking because this is a group of workers who have like sort
of historically gone out of the reason why their truck drivers is because they didn't want
this kind of oversight, right?
Like they wanted to have a little bit of freedom and autonomy.
And so it really becomes like a dignity issue for a lot of them, right?
To feel as though, you know, their trucks are almost like their homes.
They're in their trucks for weeks at a time, maybe.
And now to kind of like have this in like this oversight, real time oversight from the
government or from their employers, like it's quite, it's quite a thing for them.
It's very different from like me being surveilled at work or you being surveilled at work.
Like the nature of the workplace is just different.
And so how does this manifest itself in terms of maybe relationship between the drivers
and their companies?
What have you seen in that regard?
Yeah.
So you know, it's interesting, right?
Like a lot of the drivers that I've talked to have said things like, you know, like what
it shows if you watch me really closely is that like you don't trust me, right?
You don't trust me to do the right thing.
You don't trust me to be safe.
You don't trust me to know my body well enough, right?
Like many, many, many drivers compare this to like feeling like a criminal or feeling
like a child, right?
They see it as like this really kind of demeaning experience to be tracked in real time.
So that obviously, like it's pretty unpopular among most workers to feel like their work
and livelihoods are being supervised so closely, kind of a sort of ironic like Coda to that
is that, you know, they say like, well, you can't know my body.
You can't know how tired I am.
The next wave of technologies actually do sort of are able to infer those things more
closely because a lot of them involve like wearable technologies or cameras that are trained
on a driver's eyelids to monitor fatigue, things like that.
So they actually are quite a bit more invasive and can infer quite a bit more about whether
a driver is tired.
You know, that doesn't necessarily make truckers feel better.
They don't say like, oh, well, now you like are inside my body or have, have information
about what's going on inside my body that doesn't affect their privacy concerns, but you
know, they're kind of, they kind of can't win one way or the other.
How are the drivers reacting to this?
Have they started like organizing against the trucking companies or it, it sounds like
there's multiple issues, well, there's clearly multiple issues here, but you know, there
is the, you know, this technology being put in place to address a specific issue around,
you know, driver fatigue, but then there are kind of the downstream effects of this surveillance
and the data that's being collected about these drivers and how that's being used and
how that is, how that's being used specifically to manage these drivers.
Where is the industry at with regards to addressing this tension that's, that's starting to
be created?
Right, you're right that there's quite a bit of kind of resistance or, you know, workers
kind of viewing these technologies as, as unpopular as invasive and this is true across
lots of different workplace surveillance contexts, but I think you feel it really strongly
in trucking just kind of based on the culture of the occupation.
So, you know, as in most low wage workplaces, you know, workers don't often have the
ultimate say they might not have, you know, as much power as management to make decisions
about whether these things are in place, in trucking, you know, to some extent the stuff
is now federally mandated like as of just a few months ago, so to some extent there's
not much that employers can do, but there are some kind of movements where you see some
companies trying to be responsive to this saying like, yeah, we'll treat you like an adult
or there are certain types of data that we won't collect or that we won't give our
dispatchers access to things like that.
So, you see that a bit, but, you know, it's not a, it's not an industry with heavy unionization,
which you might, you know, unions might be a party you would expect to push back on things
like that.
So, there aren't a lot of like really clear avenues for workers to, you know, resist the
sort of data collection.
It's just kind of becoming a new normal in the industry, and so that's a lot of what
I'm interested in is kind of like, what does that transition end up looking like?
You know, how do you see truckers resist? So like, one of the things that I've studied
fairly extensively is trying to understand all the different ways that workers like try
to thwart these technologies like block data collection or falsify data collection.
You know, otherwise trying kind of maintain a little bit of independence where the technology
is sort of seen as taking that away from them.
And what kinds of things have you seen there?
A whole bunch of stuff.
I mean, actually like a whole bunch of really interesting stuff.
So they, you know, kind of on one end of the spectrum, workers will just like outright
break the thing, right, like we'll destroy it.
Sometimes, you know, and it's kind of purposefully visible way as like sort of a form of protest.
And on the other end of the spectrum, there are things that you can do that are much more
subtle.
Like you can, you know, kind of, I don't want to give all their secrets away, right?
But you can kind of do things to, you know, get extra time out of the systems if you know
what the limitations of the system are, right?
Like you can eek out a little bit of extra driving time and actually kind of interestingly,
you know, you mentioned their relationships with companies.
These actually sometimes kind of want them to do those things or will instruct them about
how to cheat because companies kind of want it both ways, right?
They want to monitor and enforce the rules, but they also like want their stuff moved
at the speed of business.
So sometimes actually truckers actually get kind of told that, you know, you've got to
violate the rules anyway.
And here's how you do it.
So there's a variety of different things they do, right?
That are designed.
And some of them, you know, like actually don't even get them any more driving time, but
they're kind of just about like preserving kind of their identity or their feeling
about autonomy.
So like my favorite example of this is a trucker on YouTube, actually, who shows other
truckers how to kind of get behind the system, the system runs on like a Windows XP backbone
and he shows like here's how you can actually play solitaire on your monitor, right?
Like, which is great.
Like that's not a thing that you're supposed to be able to do, but he like manages to figure
out how you do it.
And I'm sure like that's no longer viable.
Like I'm sure, you know, that's probably no longer a feasible thing to be able to do
with these systems.
But though he does it, you know, that's not making him any extra money, but that's like
a way for him to kind of assert himself.
And I actually find that really interesting, right, that there's a lot of resistance that
takes place that's much more about like maintaining identity and autonomy than it is necessarily
about kind some kind of instrumental like making more money, something like that.
Have these stories taught you anything about how the broader society, you know, will
react to increased surveillance?
Like are there, you know, again, I guess I'm, you know, maybe I'm being overly analytical
in this conversation, but I'm looking again for the taxonomy of, you know, resistance
to surveillance.
Yeah, I mean, a lot of the strategies that like truckers, truckers are the context that
I know the best, right?
Because I've spent the most time studying that industry, but you see some of these, you
know, same techniques across other contexts of work or other just relational contexts,
right, where people try and falsify data streams or, you know, find some way to make the technology
appear to, you know, show one account when in fact, you know, another account might be,
it might be different.
So I think, you know, to some extent, like the, that you see that, I wouldn't say it's
universal, but you see that across a lot of different contexts.
I'm curious where you fall on kind of the surveillance spectrum personally, like do you, do you
use ad blockers?
Do you put tape over your computer camera?
Like, yeah, it's, so it's a good question, right?
So like, I do use ad blockers.
Where this comes up, actually, the most for me is, you know, a lot of my work also looks
at families, looks at how people and families surveil one another or an intimate relationships.
And there, you know, one of the, one of the reasons I got like the most interested in
that context is because I would go to all these privacy and security conferences and
I like where people are, you know, working on, like, say national security issues or consumer
privacy.
Where, you know, these are like pretty staunch advocates of personal privacy.
And then I would talk to them about like, oh, do you do something to like track your
kids whereabouts or like, could you tell me where your spouse is right now?
And a lot of them were like, oh, yeah, like I definitely like read all my kids texts,
you know?
And that struck me as like a really interesting situation, right, that like a lot of,
a lot of the things that were really resistant to, you know, when they come, come at us from
the government or from big multinational corporations, we're quite willing actually
to kind of replicate in our own intimate lives.
So that's something I've gotten really interested in lately is trying to understand kind of how
people, how people become surveyors themselves, right, in their own homes.
So I have a colleague Luke Stark.
We just wrote a paper called the surveillance consumer that tries to look at how people
actually become data collectors themselves in their own homes using a variety of consumer
products, like, you know, nanny cams and things like that and trying to understand how
that too ends up having this disproportionate effect on marginalized people.
And when you say marginalized people, are you referring to specifically within the home
relationship?
Yeah, people, yeah, so I use marginalized fairly broadly, just to mean people who have,
you know, relatively less power in a situation.
So in families, certainly, like, I have a bunch of this work on domestic violence that,
you know, we're, you know, it's often, often, but not always, women and children who suffer
disproportionately from things like, you know, spyware, but in other contexts too.
So like I mentioned, like the nanny cams and nannies in homes often tend to be women of
color from, you know, with lower socioeconomic status, who end up being disproportionately
surveilled in their work, you know, as a result of kind of this consumer, like consumer
led surveillance.
So it ends up being kind of like just another way in which communities of color, women,
you know, people who historically have less power end up kind of suffering the brunt of
data collection.
It's interesting, actually, right?
So my colleague Jonas Lerman has this really lovely piece about how when we think about
surveillance, marginalized people are kind of simultaneously overrepresented and underrepresented.
Right?
So like we have all of these examples of how data sets are biased because, you know, they
only include white men, right?
Or they like don't include this kind of variety of people or images or of texts or whatever
it is and that that can lead to these really biased outcomes.
But then we also have these problems where like there's over representation in the data,
right?
Where like communities of color or poor people are historically like way over, over-surveiled,
right?
Much more data is collected about them for things like, you know, getting public benefits
or in the course of, you know, the education system or criminal justice system.
And so both of those things, I think are true at once, right?
You have like harms both from inclusion and exclusion.
So yeah, it's interesting to me to kind of try and untangle what that looks like in day
to day life.
And you've alluded to, you know, specific examples of unintended consequences in this,
in the application of surveillance, particularly within these, or as it impacts marginalized
communities, can you maybe talk through some specific examples there?
There are a bunch of people who've done a lot of really excellent work in the Serena.
So there's a bunch of work, for example, on risk assessment and criminal justice and
in predictive policing that many of my colleagues have done in which, you know, they demonstrate
that predictive policing systems which are trained on, you know, crime data that's already
over-represents, you know, communities of color, marginalized communities, tends to,
you know, just exacerbate the over-policing of those communities, right, for reasons
that, you know, you just create these feedback loops.
So that's, you know, one example that, you know, it's one example in this particular context.
But we see it in lots of different contexts, right, Virginia U-Banks is a scholar at
SUNY Albany who's written this really wonderful book called Automating Inequality that documents
how local governments implement various, like, allocation algorithms to try and do things
like apportion, housing to homeless people, or to direct public benefits programs, things
like that, and how often, you know, because of various biases in the data or, you know,
kind of the limitations that these systems are operating under, you know, it's not to
say that they necessarily make the problems worse necessarily, but they may have impacts
on communities that are not readily recognized, right, and, and, and, of course, they get
a little bit more, the effects on communities can be a little bit more obscured, right?
So, like, when, when communities adopt risk assessment algorithms or, you know, other
systems like the people who are in positions of power to make decisions may not, you
know, have a great sense for, for the right questions to be asking, or how these things
will end up impacting their communities in the long run, which is not their fault, right,
like, they're really difficult questions even for, you know, technical folks to untangle,
but it definitely is a cause for concern as these, as these tools kind of gain traction
across a bunch of different domains.
Yeah, it strikes me that part of what you're, maybe it's not something that you're saying.
It's a thread that I'm seeing in some of these stories is with regard to technology or
surveillance in particular, you know, there's an element of power corruption here, meaning,
you know, people will adopt surveillance to try to address specific point issues and surveillance
tends to be, you know, almost this beast that consumes other, you know, adjacent freedoms
or something like that.
I'm trying to articulate this without being overly dramatic, but it was easy to get
dystopian really fast, which I think is actually a problem, like, I think we should be
really critical actually of our own, you know, like the critical community is, I think,
a really important one to bring into these conversations, but we ought to also be critical
of, you know, our own techniques and our own assertions, right, that there is potentially
a lot of good to be, to come from some of these applications, but I think you're right,
right, like so this sometimes gets called like surveillance creep, right, but like, once
you started gathering data, you know, it gets easier to just add a little bit more data
to the pile or to use the data for some other purpose, because you've got it now, right,
this is, and there was a few years ago, a big debate in kind of the privacy community
about collection restrictions versus use restrictions, right, so the idea that you tell companies,
for example, like these, these are the types of data you can collect versus like, once
you have data, here's how you can use the data, and the industry kind of was in favor
generally of these use restrictions, right, because as opposed to collection restrictions,
because they, you know, allowed them to amass more data, but, you know, then there was
kind of some subsequent backlash where people say like, well, you know, like once you've
collected it, like the game is kind of, you've given the game away at that point, right,
like because things get used for all kinds of different purposes or rules change, right,
like we have, you know, you can put all these safeguards in place that you want, but,
you know, you don't always know who's going to be in power. I mean, this has come up actually
even recently with, like New York City had this municipal ID program, okay, they rolled
out a couple of years ago for undocumented people, right, where they said like, here's
how you can get a municipal ID card that will allow you to do things like use the library,
right, even though you don't have other documentation. And after doing, I mean, it was by,
you know, meant to be like a positive social program, and it's a program that actually,
you know, sort of accidentally created real risks for these groups of people, because
suddenly now there were concerns that, you know, actually those, those records will be,
would actually be used to target those people, right, for things like Homeland Security
enforcement, right, for ICE enforcement. So, like those are the types of risks, right,
like that's, that's an extreme one, but those are the types of things that, that we get concerned
about, right, is that, you know, terms of service change, privacy policies change, you know,
stuff gets hacked, like there are all kinds of different potential routes for data to be used
in ways that we don't anticipate. You made an interesting point about being critical in our
critiques has, how do we, how do we think about that? How do we, you know, how do we do that?
Yeah, so I think about this a lot, right, because now that I'm in information science,
so, and the information science program I'm in is really closely tied to computer science,
so I sit in the same building with a bunch of computer scientists all the time,
which I really enjoy, because I learn a lot from, you know, understanding their
perspectives on these issues. I think the best thing that that we can do is, like, honestly,
to try and amass more technical understanding. So I think, like, the worst examples of kind of
critique of these systems come from people who don't necessarily, who have a thin understanding
of how the systems actually work in practice, and that means both technically how they work and
what their social effects are, right, so you can't really understand one without the other.
And then the other thing is, is I think actually about engagement, it sounds kind of,
you know, touchy-feely, but I think engagement with practitioners is the other, like, best way
forward to kind of avoid painting like the other side as being the other side or as being like a
straw man, you know, but to understand, like, the actual, you know, motivations and values
behind these decisions, which really requires, like, actually talking to people.
So maybe this is just like a plug for social science, but I really believe that, like,
I really believe that, you know, having kind of more social science expertise at the table,
and having people, having social scientists then take seriously technical expertise,
and really try to understand it is, you know, the best we can do.
Maybe this should be posed as a confirmation or question, but you made it sound like you find
that people in computer science, in technology, are way less concerned about these issues than lay
people. And that's almost my, I almost have the opposite instinct. Oh yeah, I definitely not
what I tried to, that wasn't what I was hoping to portray. Okay, okay. Yeah, yeah, no, I definitely
don't feel that way. I think, like, kind of the easy, so the easy critique that sometimes made
of algorithmic systems from the outside, right, can be that algorithms have this kind of
depersonalizing effect, right, or that, you know, the people designing them, like, don't really
understand, you know, what biases might be embedded in them and don't really care, right? Like that,
I think is like, I'm obviously, I'm creating a straw man by describing that as this process of
straw manning that some other people do, but I think that's like, that's like the worst form of
criticism, right? Because it doesn't acknowledge what you just said, which is that, like, oftentimes,
I think people are like quite deeply concerned about the systems they create and the inequities
that those might exacerbate. So I do, I think it's exactly what you said. I think recognizing those
intentions and actually working with people to try and build the best systems we can, like, that
ought to be our goal, I think, is critical scholars, not necessarily to try and tear down, you know,
efforts to try and improve those things. So you're working on a book on surveillance,
particularly as it applies to the truckers that you've been working with. It sounds like that
book is in progress, but if you were to kind of step back and, you know, or maybe project forward
to the the conclusion of this book, you know, what are the key messages or takeaways for folks that
are, I guess in the case of listeners of this podcast, primarily technologists, primarily,
you know, working to create, refine, perfect some of these technologies. You know, what are the
takeaways for them in terms of, you know, understanding and dealing with the implications that they have
on different communities and particularly marginalized communities? Yeah, so I think, I mean,
I think they're probably two. Like, so the first is something that I kind of alluded to earlier,
which is, you know, technology is a really, is a useful tool for solving a lot of problems,
but not every problem is best solved through technology. And I can understand why particularly
technologists, right, would want to put their tools to use in a particular way. But I think
sometimes asking, like, well, you know, holistically, is this the best approach or is this an
approach that could be combined with other approaches, you know, or different forms of expertise,
you know, how will we evaluate what the real effects of these tools are on communities? Like,
that seems super essential to me. And then the other piece that I think is, you know, like, like,
in the case of these truckers, for example, right, like, there's concern recently about, like,
the potential for massive unemployment among truck drivers based on the development of autonomous
vehicles. Now, I think that that's an overstated concern. But, like, what, I mean, we would be telling
sort of a simple and boring story if we focused only on the numbers, right? If we focused only on
something like job loss, I think a more important and nuanced take on it would be to actually
understand kind of like the dignity and quality of life that results when we put some of these
systems in place, right? Which might not be something you can measure in numbers, right? Might
not be something that you can look at in terms of dollars or, you know, jobs loss or something
like that. But actually, like, that's, that ends up being like what it is to live with these
systems, right? To live with data being collected about you or to live with automated decisions being
rendered about you. And for that, I think it really takes like getting into the community,
talking to the people who are affected on the front lines. Like, I just can't see that there's
any replacement for trying to understand problems at that level. And just to be concrete about
this, in the case of the truck drivers that will undoubtedly be impacted by self-driving
trucks, how do you see that impacting them, their dignity, their, the way they work, and the way
they feel about their work? Yeah, so, I mean, yeah, so because of kind of the nature of,
so the nature of work, right, is that work is really complicated, right? And so to say like, oh,
well, now we have self-driving trucks, so we don't need human truckers anymore, is to like really
kind of oversimplify what really anybody's job looks like, right? So I think we will, and there are
a lot of things that self-driving trucks are like very, very far from being able to accomplish
technically, right? So what I think that means, and what a lot of people think that means is that,
you know, it's not that we have like robots that replace people, and then we're done, it's that
like very slowly robots change, you know, people end up working with automated systems, right? And
so it's like the working with that becomes interesting and important. And there are ways that we can
work with machines that, you know, augment both where everybody gets to kind of use their
strengths, right? And it becomes like a really kind of collaborative, you know, supportive,
synergistic relationship. And then there are ways where it doesn't appear that way, right? Where
like the machine becomes kind of like the supervisor of you, right? Or the machine kind of,
you know, in the case of truckers, right? Like actually like has a bunch of insight into your body,
right? Or is seen as sort of intrusive? So like I mentioned, you know, truckers having these
cameras trained on their eyelids, you know, or they're all these systems that will like jolt them
or flash lights in their eyes or, you know, text their partners, like all these different things
that they get tired. And then, you know, you can imagine why that like changes kind of the dignity
in the nature of the job, right? Like being kind of forced to sort of hybridize with these systems
is really different than kind of this kind of mutually supportive environment that we might otherwise
create. And so it all becomes about the details, right? It all becomes about kind of like how we
roll these things out, you know, who's incentives we acknowledge. And I think there's a lot of room
to do it well. And in ways that actually like make work better for people. But it takes like really
careful thought, I think when we when we try to do that. Again, maybe acknowledging my inclination to
kind of framework eyes and analyze this. Because you love it. You love it.
Like I'm thinking about like is there has anyone started working on like the, you know,
principles of these this hybridization hybrid job roles? What does that mean? And how can we think
about that consistently across different types of jobs? Or does that even do you think that even
makes sense? Like are there things that apply equally to the to the truckers and people working
in office environments and people working in industrial environments as these jobs are getting
increasingly hybridized? Or do you think that they're they're all separate? Yeah, it's a good question.
I don't think there are probably consistent answers, but I think there are probably consistent
questions to ask across those different contexts, right? So understanding things like an occupational
culture, the occupational culture of trucking is very different from the occupational culture of
say like office work, right? But that but asking questions about the culture and the values
and the traditions of an occupation seems like indispensable to me, right? Understanding like,
you don't just drop technology in and run away, but like understanding, you know, who are these
people? Why are they here? Like what is important to them? Which is both like, you know, monetary,
right? Like what what, you know, how do they make their money and like how is this going to affect
how they make their money? But also these kind of questions around identity and tradition and
culture, things like that. You know, questions about autonomy, I think are probably important
to ask across contexts. Like we pretty much know, right, that people like having autonomy in their
work, like almost, you know, you know, across lots of different occupations, right? Like people
don't like having things directed so much so that they feel like they're part of a machine.
And so when we build machine human systems, like making sure that people have, you know,
some of that decisional capability feels really important, right? And again, like that'll look
different, like if you're looking at doctors working with robots to do surgery versus a truck driver,
you know, with like kind of with an autonomous supervisor in the truck, but the values are the
same, right? Well, Karen, thank you so much for taking the time to chat with us about this. It's
been a really interesting conversation. Well, thanks. I enjoyed it. Thank you.
All right, everyone. That's our show for today. For more information about today's guest,
or to follow along with AI Platform Volume 2, visit twimmelaii.com slash AI Platforms 2.
Make sure you visit twimmelcon.com for more information or to register for Twimmelcon AI
Platforms. As always, thanks so much for listening and catch you next time.
