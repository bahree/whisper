WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:26.480
I'm your host Sam Charrington.

00:26.480 --> 00:34.720
All right everyone, still here in Vancouver at NERPs, continuing our coverage of this

00:34.720 --> 00:39.880
incredible conference and I've got the pleasure of being seated with Blaz Aguera Yarkas.

00:39.880 --> 00:42.720
Blaz is a distinguished scientist with Google AI.

00:42.720 --> 00:45.120
Blaz, welcome to the Twimble AI podcast.

00:45.120 --> 00:46.120
Thank you so much.

00:46.120 --> 00:47.120
Thanks for having me.

00:47.120 --> 00:48.120
Absolutely.

00:48.120 --> 00:53.880
So, you are doing an invited talk here at the conference tomorrow morning on social intelligence

00:53.880 --> 00:57.800
and we're going to dig into what exactly that means for you, but before we do, I'd love

00:57.800 --> 00:59.960
to get a bit of your background.

00:59.960 --> 01:00.960
Sure.

01:00.960 --> 01:03.480
So, it's a little motley.

01:03.480 --> 01:09.180
I started off in physics, it's an undergraduate at Princeton and I studied physics and applied

01:09.180 --> 01:10.180
math there.

01:10.180 --> 01:18.080
I took a year off between my third and fourth years because I was not a very good student.

01:18.080 --> 01:21.840
And I really started to get into bio physics pretty heavily.

01:21.840 --> 01:24.400
So you're all for after?

01:24.400 --> 01:28.560
During, or a little bit before and then during.

01:28.560 --> 01:32.320
So I worked for a little while in Stan Leibler's lab there.

01:32.320 --> 01:36.680
He was working on bacterial chemotaxis and that actually is going to figure a little bit

01:36.680 --> 01:39.280
into my talk tomorrow morning.

01:39.280 --> 01:45.080
So it's the intelligence behaviors of bacteria and how it is that they find food.

01:45.080 --> 01:50.360
They're obviously a really small, simple system but maybe not quite as simple as people think.

01:50.360 --> 01:59.600
And then from there, my next advisor, Bill Bialik, is somebody with a physics background

01:59.600 --> 02:04.480
as well, but also a computational neuroscientist and he ran this course in Woods Hole at the

02:04.480 --> 02:09.200
Marine Biological Lab called Methods and Computational Neuroscientists, Methods and Computational

02:09.200 --> 02:10.200
Neuroscience.

02:10.200 --> 02:11.200
Sorry.

02:11.200 --> 02:14.840
I don't know if you're familiar or how many of your listeners are with MBL with Marine

02:14.840 --> 02:18.840
Biological Laboratory, but it's this place where a lot of Princeton?

02:18.840 --> 02:23.320
No, no, it's on Cape Cod, so it's right on the elbow of Cape Cod across from Arthur's

02:23.320 --> 02:24.320
Vineyard.

02:24.320 --> 02:28.920
It's this little tiny town, it's very cute, and there's this kind of ramshackle lab that's

02:28.920 --> 02:36.880
been there since the 19th century that a lot of visiting neuroscientists and biologists

02:36.880 --> 02:39.520
have been going to for many, many years.

02:39.520 --> 02:43.840
A lot of really basic discoveries in neuroscience were made there.

02:43.840 --> 02:45.840
So it's kind of this cool place.

02:45.840 --> 02:51.960
And at this course, at Methods and Computational Neuroscience, I met my now wife, Adrian

02:51.960 --> 02:52.960
Fairhol.

02:52.960 --> 02:53.960
Oh, wow.

02:53.960 --> 03:00.440
So she also came up in physics and studied originally chaos and turbulence and fluid dynamics and

03:00.440 --> 03:04.200
things like this and was making the switch to computational neuroscience, so we met

03:04.200 --> 03:06.720
there.

03:06.720 --> 03:11.560
And then she ended up getting a faculty job at University of Washington, which is how

03:11.560 --> 03:14.640
we ended up moving to Seattle.

03:14.640 --> 03:21.280
And around that time, I started a company and it was no longer really part of academia

03:21.280 --> 03:23.560
at that point.

03:23.560 --> 03:26.320
And the company got acquired by Microsoft a couple of years later.

03:26.320 --> 03:29.600
And the company was doing computer vision type of work, or?

03:29.600 --> 03:31.080
Yeah, somewhat.

03:31.080 --> 03:32.080
somewhat.

03:32.080 --> 03:37.160
It was doing sort of multi-resolution representations of documents of various kinds.

03:37.160 --> 03:44.200
So it was a combination of wavelet-ish kind of tricks and UX.

03:44.200 --> 03:46.880
I think wavelet is like Kryptonite for me.

03:46.880 --> 03:50.320
That was the hardest thing that I studied in grad school for whatever reason.

03:50.320 --> 03:52.120
It was very difficult to rock.

03:52.120 --> 03:53.120
It was hard.

03:53.120 --> 04:00.280
Yeah, my advisor in grad school in Applied Math was in Grin Dobeschi, who was one of the inventors

04:00.280 --> 04:01.280
of wavelet.

04:01.280 --> 04:02.280
Oh, wow.

04:02.280 --> 04:03.280
And that might have not.

04:03.280 --> 04:04.280
Yeah, it helped.

04:04.280 --> 04:08.480
She was absolutely wonderful, very, very smart, very kind.

04:08.480 --> 04:13.760
And I think one of the greatest living mathematicians, if I, I don't know, maybe unbiased.

04:13.760 --> 04:18.200
But anyway, yeah, Microsoft acquired it.

04:18.200 --> 04:22.560
And I did immediately turn the team toward more kind of computer visionary things right

04:22.560 --> 04:23.560
after that.

04:23.560 --> 04:29.480
So Photosynth, which started off as the Phototourism project by a University of Washington professor

04:29.480 --> 04:35.800
and a Microsoft research scientist, together with their grad student, Noah Snavley, was doing

04:35.800 --> 04:38.040
3D reconstruction environments from 2D images.

04:38.040 --> 04:40.680
And that was really my introduction to computer vision.

04:40.680 --> 04:42.040
That was very classical.

04:42.040 --> 04:46.120
It wasn't like deep nets or anything like this, it was geometric computer vision.

04:46.120 --> 04:49.240
But I kind of fell in love with that field.

04:49.240 --> 04:53.880
And ended up at Microsoft, you know, sort of doing a lot of leading of teams doing that

04:53.880 --> 04:54.880
kind of work.

04:54.880 --> 05:00.280
So Microsoft's, you know, OCR team and their kind of photogrammetry type teams, the teams

05:00.280 --> 05:04.800
that ended up doing a lot of work for HoloLens, for tracking the head using outward-facing

05:04.800 --> 05:08.320
cameras, all that kind of stuff was part of my team at the time.

05:08.320 --> 05:09.320
Oh, wow.

05:09.320 --> 05:10.320
So I was at Microsoft for seven years.

05:10.320 --> 05:16.480
So it was the CTO of Bing Maps, which also had some kind of computer vision, VR, photogrammetry

05:16.480 --> 05:19.880
kind of stuff going on, and Bing Mobile.

05:19.880 --> 05:21.760
And then I went to Google.

05:21.760 --> 05:23.120
That was six years ago.

05:23.120 --> 05:24.120
Come, of course.

05:24.120 --> 05:28.360
So many people that are in this field that have some connection to Bing.

05:28.360 --> 05:29.360
Yeah.

05:29.360 --> 05:32.200
Yeah, I think I shouldn't bad mouth.

05:32.200 --> 05:37.440
I mean, it was creative and scrappy at the time, you know, whether Microsoft was really

05:37.440 --> 05:41.160
committed to running these things, I guess, you know, as anybody's guess.

05:41.160 --> 05:42.160
Right.

05:42.160 --> 05:43.160
Right.

05:43.160 --> 05:47.160
But yeah, I mean, one of the reasons that I ended up leaving Microsoft was because about

05:47.160 --> 05:52.160
six years ago, they had just kind of lost the phone war and it became clear that they

05:52.160 --> 05:55.360
were going to be moving away from being a consumer-focused company and they were going

05:55.360 --> 05:57.320
to start working on just, you know, enterprise stuff.

05:57.320 --> 05:59.280
And that wasn't that interesting to me.

05:59.280 --> 06:02.600
And that was around the same time also that the whole deep learning revolution was really

06:02.600 --> 06:04.320
getting into full swing.

06:04.320 --> 06:08.760
And I was very excited about sort of machine learning and computational neuroscience

06:08.760 --> 06:10.080
re-converging.

06:10.080 --> 06:14.120
And Google is the obvious place, you know, where the kind of hotbed of a lot of that.

06:14.120 --> 06:15.120
So nice.

06:15.120 --> 06:17.120
So what do you research at Google?

06:17.120 --> 06:22.840
Well, at Google, I started a team called Saribra, which is not a name that we've generally

06:22.840 --> 06:25.560
used in public, but that's not at all heady.

06:25.560 --> 06:28.920
Well, well, that's the flow.

06:28.920 --> 06:30.920
Thank you.

06:30.920 --> 06:32.160
It's the plural of brain.

06:32.160 --> 06:34.960
So there was a brain team already that, you know, Jeff, Jeff Dean started the brain team

06:34.960 --> 06:36.680
a few years before.

06:36.680 --> 06:42.040
And I went to Google to start a team that would take a much more decentralized approach.

06:42.040 --> 06:46.120
So rather than one brain, it would be many brains, everybody would have a little brain.

06:46.120 --> 06:51.240
And I had a kind of very augmentation-focused point of view, you know, that rather than

06:51.240 --> 06:57.200
having, you know, one giant AI running in a data center, these things would have to shrink,

06:57.200 --> 07:01.400
they would have to democratize, they would have to go into devices, run locally.

07:01.400 --> 07:06.520
I had a lot of reasons for really wanting to push in that direction, including privacy,

07:06.520 --> 07:09.800
which I will talk about a bit tomorrow.

07:09.800 --> 07:14.040
So mobile nets and a lot of these kind of efficient ways of running neural nets locally came

07:14.040 --> 07:16.040
from our team.

07:16.040 --> 07:20.960
I, again, am running the, you know, the groups at Google, the two things like OCR and

07:20.960 --> 07:27.120
face recognition and a bunch of other sort of image understanding primitives.

07:27.120 --> 07:32.920
But we also power a lot of AI or ML features or whatever you want to call them in Android.

07:32.920 --> 07:37.040
And also on other kinds of devices, including these little kind of coral boards, which

07:37.040 --> 07:41.080
are sort of an IoT kit for doing, for doing local.

07:41.080 --> 07:46.480
Yeah, those were, I think those were just, well, I guess it's maybe half a year ago at

07:46.480 --> 07:49.440
the TensorFlow Developer Conference, I think I have one.

07:49.440 --> 07:50.440
Yeah, that's right.

07:50.440 --> 07:51.440
That's right.

07:51.440 --> 07:53.200
So yeah, we're very excited about those.

07:53.200 --> 07:54.200
Cool.

07:54.200 --> 07:59.360
You mentioned OCR and of all the things that we've talked about, I think of that, or it's

07:59.360 --> 08:03.160
probably easy to think of that as a solved problem and all the problem.

08:03.160 --> 08:08.280
But there's probably a lot of, you know, I guess even just saying it, there's probably

08:08.280 --> 08:16.640
like this last mile problem where in order to get to usable or better levels of accuracy

08:16.640 --> 08:21.080
and performance, kind of that those last few percentage points are really hard to get

08:21.080 --> 08:22.080
to.

08:22.080 --> 08:25.960
So yeah, you say, I mean, it solves problem and yeah, I mean, it's good enough for practical

08:25.960 --> 08:26.960
use.

08:26.960 --> 08:29.240
There are a lot of engines that are good enough for practical use.

08:29.240 --> 08:33.720
But A, of course, extra percentage points are always useful, you know, so a little bit

08:33.720 --> 08:34.800
more is always better.

08:34.800 --> 08:38.840
But also, the OCR team that I ran at Microsoft was still using a lot of these classical

08:38.840 --> 08:43.280
techniques that would first, you know, they would have a whole pipeline of different stages,

08:43.280 --> 08:47.720
first segmenting out letters and then, you know, doing template matching and then using

08:47.720 --> 08:50.080
language model and all kinds of stuff like this.

08:50.080 --> 08:57.320
And the direction that I think and that the people in the OCR team believe are really the

08:57.320 --> 09:01.400
most fruitful now are much more end-to-end and much more neural.

09:01.400 --> 09:05.880
So imagine it's more like a scanner that scans the entire line maybe bidirectionally and

09:05.880 --> 09:10.040
emits a string of characters, kind of like a speech engine might.

09:10.040 --> 09:15.880
If you do it that way, then, you know, joined letters and ligatures don't matter, cursive

09:15.880 --> 09:21.400
doesn't matter, handwriting and print could be the same, Arabic and other languages that

09:21.400 --> 09:25.880
don't have good, you know, distinctions between letters, I shouldn't say good, but that

09:25.880 --> 09:29.120
don't distinguish clearly between letters and a more cursive sort of approach.

09:29.120 --> 09:30.480
All of those things work.

09:30.480 --> 09:34.320
And that sort of generalness and also just weird fonts, you know, there are a lot of things

09:34.320 --> 09:38.360
that are easy for us to read that a classical OCR engine can.

09:38.360 --> 09:41.920
So thinking about it more like a real vision problem, you know, with a brain behind it as

09:41.920 --> 09:45.880
opposed to just a classical kind of letter clustering problem with the language model

09:45.880 --> 09:46.880
tacked on.

09:46.880 --> 09:53.400
So is the focus of that work today achieving the level of accuracy that we previously

09:53.400 --> 09:58.480
achieve with traditional approaches, with deep approaches, or have we-

09:58.480 --> 10:01.360
Oh, that's not a far surpasser past, yeah, we've already far surpassed.

10:01.360 --> 10:02.360
Okay.

10:02.360 --> 10:03.360
Right.

10:03.360 --> 10:08.160
I mean, the goal now is to be able to do that in a way that is compact, real time runs

10:08.160 --> 10:12.800
on devices, doesn't- it doesn't have to be told what language something is in or what

10:12.800 --> 10:16.960
kind of script has a unified model for every imaginable kind of screen, you know, those

10:16.960 --> 10:17.960
kind of goals, right?

10:17.960 --> 10:20.720
So the kind of things that a person can do.

10:20.720 --> 10:24.120
But yeah, the neural methods have long surpassed the classical methods.

10:24.120 --> 10:25.120
Got it.

10:25.120 --> 10:32.280
Yeah, Jeff has been on the podcast previously and has mentioned that the transition from

10:32.280 --> 10:37.840
traditional machine translation to neural machine translation resulted among other

10:37.840 --> 10:42.480
things in increased performance and a reduction in the size of that code base from half a million

10:42.480 --> 10:47.600
lines of code to- I forget what the number was 50 or something like an astounding number.

10:47.600 --> 10:48.600
Is there a similar-

10:48.600 --> 10:49.600
It's exactly the same.

10:49.600 --> 10:50.600
Exactly the same.

10:50.600 --> 10:51.600
Exactly the same story.

10:51.600 --> 10:55.440
And learning, you know, the code is very- it suddenly becomes very small because all

10:55.440 --> 11:00.440
of the structure, all the statistical structure in the thing is being learned rather than

11:00.440 --> 11:01.440
coded.

11:01.440 --> 11:05.040
And it means that a lot of the assumptions that you might make in that code don't have

11:05.040 --> 11:10.320
to be made at a programming time like letters being distinct or being read from left to

11:10.320 --> 11:16.160
right or, you know, or not being slanted, being able to, you know, to be kind of characterized

11:16.160 --> 11:18.320
in terms of connected components or boxes and so-

11:18.320 --> 11:19.320
Right.

11:19.320 --> 11:22.040
And so you're invited to talk here at NURBS is about OCR.

11:22.040 --> 11:23.040
No.

11:23.040 --> 11:24.040
No.

11:24.040 --> 11:29.240
No, the team is pretty big and OCR is only three or four people I think.

11:29.240 --> 11:30.240
Yeah.

11:30.240 --> 11:31.240
Yeah.

11:31.240 --> 11:37.520
What about social intelligence, which is more related to the concept of cerebris and many

11:37.520 --> 11:38.520
brains.

11:38.520 --> 11:40.240
I imagine.

11:40.240 --> 11:43.720
What exactly are you discussing in the talk?

11:43.720 --> 11:44.720
Right.

11:44.720 --> 11:49.760
Well, it's a wide-ranging talk and, you know, I guess it has the shape of, you know,

11:49.760 --> 11:53.560
like some- a lot of very broad considerations at the beginning and then some specific technical

11:53.560 --> 11:57.240
work in the middle and then maybe broadening it back out a little bit at the end.

11:57.240 --> 12:04.600
The broad themes are that, you know, I guess we've gone to the point like with OCR that

12:04.600 --> 12:08.760
if you know what the goal is, if you're able to score or to make a loss function for

12:08.760 --> 12:13.320
what you're doing and we have lots of training data like we do with OCR or faces or whatever.

12:13.320 --> 12:15.440
A couple of my considerations.

12:15.440 --> 12:16.440
Minor considerations.

12:16.440 --> 12:17.440
Yeah.

12:17.440 --> 12:18.440
Obviously they're not here.

12:18.440 --> 12:19.440
Well, but this is the thing.

12:19.440 --> 12:22.440
I don't think that they used to be considered, you know, all that limiting.

12:22.440 --> 12:23.440
Right.

12:23.440 --> 12:28.160
In the days of, I don't know, the Dartmouth Summer Project or something, you know, people

12:28.160 --> 12:33.080
were like, we don't know how to solve AI, you know, think about it, you know, only brains

12:33.080 --> 12:37.040
can do things like understand language and writing and so on.

12:37.040 --> 12:42.760
Surely if we figure out how to do those kinds of things, we'll crack the secret of intelligence.

12:42.760 --> 12:46.400
And now we're like the dog that caught the car, you know, like we, you know, any of those

12:46.400 --> 12:51.160
things that can be characterized cleanly, we can achieve superhuman performance basically.

12:51.160 --> 12:54.140
I mean, I'm making it slightly for looking statement, not all of these things are superhuman

12:54.140 --> 12:58.160
yet, but it's, you know, like if you, if you say something isn't then like next year

12:58.160 --> 12:59.160
it will be.

12:59.160 --> 13:00.160
Right.

13:00.160 --> 13:05.560
You know, so, so like we've solved it and yet none of the systems that we've built are

13:05.560 --> 13:09.040
intelligent in a way that you or I would recognize as intelligence.

13:09.040 --> 13:10.040
Right.

13:10.040 --> 13:14.000
It's, they're just, it's just functions, you know, it's just progression really in

13:14.000 --> 13:15.000
the end.

13:15.000 --> 13:20.000
And that's why we say, you know, like AI or ML data, ML slash data science in a way, you

13:20.000 --> 13:24.340
know, the, the projects of AI and data science could not be more different, you know, data

13:24.340 --> 13:28.840
science is just about, you know, modeling data and AI is about making minds.

13:28.840 --> 13:32.200
I mean, on the face of it, like really are these the same thing.

13:32.200 --> 13:36.140
So, you know, I think if there's a single theme for my talk, it's like, well, what is that,

13:36.140 --> 13:39.960
what is that gap and why and why is it there?

13:39.960 --> 13:46.640
The reason that I called it social AI or social intelligence was that I've come to believe

13:46.640 --> 13:52.440
in the last few years that, that sociality, which is to say, our interactions with each

13:52.440 --> 13:58.240
other are not incidental to intelligence, but are actually fundamental to it.

13:58.240 --> 14:03.920
In the sense that life isn't a one player game, it's not like we evolved intelligence

14:03.920 --> 14:10.080
as a, as an adaptation in order to get by in a really hard video game environment where

14:10.080 --> 14:13.800
nature is trying to kill us and we have to outwit it.

14:13.800 --> 14:19.300
On the contrary, like an individual human is a lot worse at outwitting nature than our

14:19.300 --> 14:23.200
A-band sisters, like drop one of us naked in the jungle and like only a handful of us

14:23.200 --> 14:28.480
will make it, you know, maybe the piraha, you know, in Brazil or something, but it's

14:28.480 --> 14:29.800
very few people who can make it.

14:29.800 --> 14:31.000
We get to watch it on TV.

14:31.000 --> 14:33.000
Yeah, exactly, exactly.

14:33.000 --> 14:35.680
So, you know, our environment is each other.

14:35.680 --> 14:43.620
And there, there is a famous researcher, Robin Dunbar, who proposed the social

14:43.620 --> 14:47.400
cortex hypothesis way back in the 90s.

14:47.400 --> 14:53.000
And he's not the first France V underval and France devalze and many other researchers

14:53.000 --> 14:59.720
of the same kind of idea that we are essentially the role of intelligence is to model others.

14:59.720 --> 15:04.920
And since you and I are of the same species, it's useful for me to be able to model you

15:04.920 --> 15:08.360
and to understand what you're going to do.

15:08.360 --> 15:12.320
And so my brain will grow bigger in order to model yours, but we share genes.

15:12.320 --> 15:16.200
So in the process, your brain grows bigger as well, and you have a kind of, you know,

15:16.200 --> 15:20.120
arms race or a feedback loop, and that's how you get these kind of intelligence explosions

15:20.120 --> 15:26.040
that we've seen in the apes and in cetaceans and dolphins and whales and some other species.

15:26.040 --> 15:32.360
Echoes in a lot of ways, Yvahar Ari's from sapiens, idea that our kind of core achievement

15:32.360 --> 15:37.720
as a species is the ability to collaborate and communicate goals and stories to one another

15:37.720 --> 15:40.720
and kind of project forward.

15:40.720 --> 15:43.480
Yeah, I absolutely agree with that point of view.

15:43.480 --> 15:49.880
There are a bunch of recent books, Harari's are among them, but also Nicholas Kristakis,

15:49.880 --> 15:55.520
Blueprint, the human swarm, Moffitt's book, Pat Churchill's book, Conscience, a number

15:55.520 --> 15:59.560
of books that I think really, you know, sort of start to expound that point of view.

15:59.560 --> 16:04.960
But I don't think a lot of that has been heard in our community in the kind of AI or

16:04.960 --> 16:05.960
ML communities.

16:05.960 --> 16:14.440
So you're proposing this idea that intelligence is a collaborative idea, how does that manifest

16:14.440 --> 16:18.880
itself or what are some of the kind of next layer points that you're making in the talk?

16:18.880 --> 16:19.880
Sure.

16:19.880 --> 16:25.080
Well, so one of them is that when you expand, when you kind of zoom out to think about

16:25.080 --> 16:30.600
not just the intelligence of an individual, but the intelligence of a group of a society,

16:30.600 --> 16:33.760
which, you know, most of our achievements are, of course.

16:33.760 --> 16:34.760
Right.

16:34.760 --> 16:39.560
And then you are in a multi-agent kind of universe, if you want to think about it in kind

16:39.560 --> 16:41.080
of our terms.

16:41.080 --> 16:47.040
And in a multi-agent universe, even if every individual agent is doing optimization, in

16:47.040 --> 16:51.480
other words, has a clear loss function or objective and is doing, I don't know, gradient descent

16:51.480 --> 16:57.120
to optimize it, when you zoom back and look at the whole, that isn't a longer the case.

16:57.120 --> 17:00.240
And you can see that even in a really simple example, like, you know, the most minimal

17:00.240 --> 17:05.840
ecology you can make is an ecology of two things, and GANs are an example of an ecology of

17:05.840 --> 17:06.840
two things.

17:06.840 --> 17:07.840
Right.

17:07.840 --> 17:11.640
You have an actor and an artist and a critic, and their goals are different, and they're

17:11.640 --> 17:12.640
kind of misaligned.

17:12.640 --> 17:13.640
Right.

17:13.640 --> 17:14.640
The artist is trying to fool the critic.

17:14.640 --> 17:17.880
The critic is trying to winkle out, you know, to sauce out the artist.

17:17.880 --> 17:23.560
And when you look at what happens in the interaction between those two, they pursue each other, and

17:23.560 --> 17:26.640
there's a spiral, you know, so this is a dynamical system.

17:26.640 --> 17:31.360
And dynamical systems have chaos, you know, they have vorticity, they have limit cycles.

17:31.360 --> 17:33.560
And that does not look like gradient descent anymore.

17:33.560 --> 17:38.440
Like, if you look at the dynamics of gradient descent, it looks like a curl free field.

17:38.440 --> 17:39.440
Curl free?

17:39.440 --> 17:40.440
Yes.

17:40.440 --> 17:44.280
Meaning that there's no twist in trajectories.

17:44.280 --> 17:45.680
Everything goes downhill.

17:45.680 --> 17:48.880
So trajectories have no curve in them.

17:48.880 --> 17:53.520
Whereas these GANs, you know, that's why they're so quite hard to train, you know, because

17:53.520 --> 17:56.760
they have predator-prey kinds of dynamics, essentially.

17:56.760 --> 18:01.600
So what happens is a lot more complicated, you know, it so happens that GANs were invented

18:01.600 --> 18:07.280
to have a fixed point that coincides with the optimum of a function that can be written

18:07.280 --> 18:10.720
down, which reproduces a distribution of PFX.

18:10.720 --> 18:14.840
But that's just, you know, that's just an artifact of how we cooked that particular one

18:14.840 --> 18:15.840
up.

18:15.840 --> 18:20.120
And in fact, it's not as if GANs, when they converge or are in general, necessarily at

18:20.120 --> 18:22.680
such a global optimum either, you know.

18:22.680 --> 18:27.720
So that's like the minimal ecology, and when you consider that we're made out of cells,

18:27.720 --> 18:32.240
you know, neurons are cells that each have their own objectives and so on.

18:32.240 --> 18:36.240
It's not just ecologies when you look at, say, multiple people that have that property,

18:36.240 --> 18:40.720
but a single brain or a single thing is in turn composed of some things that have their

18:40.720 --> 18:42.640
own goals and agendas.

18:42.640 --> 18:45.480
So it's, you know, it turtles all the way down and all the way up.

18:45.480 --> 18:49.440
And when you start looking at it that way, then I guess I'm not saying that optimization

18:49.440 --> 18:54.560
is dumb or is a bad way to look at things, but it's more like that's just a local force.

18:54.560 --> 19:00.480
And when you look at it at the system, the behavior of that system cannot be determined

19:00.480 --> 19:03.640
by just looking at those local forces, you have to look at the entire picture.

19:03.640 --> 19:09.600
I mean, it's, it's especially fascinating when you think of it in the context of research

19:09.600 --> 19:15.600
like, you know, how much of our behavior is controlled by our microbiome or I was just

19:15.600 --> 19:21.880
watching some TV show, maybe it was like our Earth type of show that showed, you know,

19:21.880 --> 19:26.840
how some parasite would infect these ants and cause them to do the, like, these zombie-like

19:26.840 --> 19:34.320
behavior and control, which I think underscores your point that, you know, so much wall behavior

19:34.320 --> 19:38.520
and, you know, manifest behavior that we see is controlled by kind of the interactions

19:38.520 --> 19:42.520
of things as opposed to, you know, some optimization function.

19:42.520 --> 19:43.520
Exactly.

19:43.520 --> 19:51.680
Very between parasitism or predation or exploitation and symbiosis is, not only is it a fine line,

19:51.680 --> 19:53.280
I'm not even sure the line exists.

19:53.280 --> 19:54.280
Well, certainly.

19:54.280 --> 19:59.280
If you step back and look at a system, right, right, right, right, what is the system's

19:59.280 --> 20:02.280
optimization function may be very different from that of the ant.

20:02.280 --> 20:03.280
That's right.

20:03.280 --> 20:06.360
But even if you look at things, you know, even if you try to make firm boundaries around

20:06.360 --> 20:10.440
say an individual person, I mean, like there was, there was a recent discovery that I thought

20:10.440 --> 20:13.840
was really cool about the arc virus.

20:13.840 --> 20:17.600
So there are a lot of retroviruses that have been incorporated into our gene, into our

20:17.600 --> 20:20.720
germline, into our genome that are passed on.

20:20.720 --> 20:24.640
Some of them for many, many hundreds of millions of years.

20:24.640 --> 20:27.520
And the arc virus is one of those.

20:27.520 --> 20:32.640
So you know, we've known that it's there for a long time, but nobody knew what it was

20:32.640 --> 20:33.640
doing.

20:33.640 --> 20:34.640
It was finally caught.

20:34.640 --> 20:35.960
So I understand this.

20:35.960 --> 20:41.440
You say past or kind of incorporated into our geneful meaning, the body fundamentally

20:41.440 --> 20:44.040
creates this virus as part of existing.

20:44.040 --> 20:45.240
That's exactly right.

20:45.240 --> 20:47.600
Like the virus is no longer distinct from your genetics.

20:47.600 --> 20:51.600
It's not that it's, you know, endemic in a population and gets passed.

20:51.600 --> 20:52.960
It's literally part of your DNA.

20:52.960 --> 20:53.960
Okay.

20:53.960 --> 20:57.880
So these are retroviruses that inject themselves into the DNA and into the germline and propagate

20:57.880 --> 21:02.480
along with you, just like mitochondria, you know, those are bacteria, of course, right?

21:02.480 --> 21:06.920
We're full of some of these kind of symbiotic things and arc, it turns out, you know, it was

21:06.920 --> 21:11.640
caught in this electron microscope actually forming its viral coat and, you know, forming

21:11.640 --> 21:12.640
its capsid.

21:12.640 --> 21:13.640
Okay.

21:13.640 --> 21:15.960
And if you knock it out in mice, they can't remember anything longer than 24 hours.

21:15.960 --> 21:16.960
Wow.

21:16.960 --> 21:21.440
So like they're involved in some way that we don't fully understand in memory formation.

21:21.440 --> 21:22.440
Huh.

21:22.440 --> 21:23.960
You know, wow.

21:23.960 --> 21:24.960
Right.

21:24.960 --> 21:28.040
Some people think that AIDS, you know, it was kind of on the way.

21:28.040 --> 21:32.400
I mean, hopefully we've now, you know, we've now sort of controlled it.

21:32.400 --> 21:37.080
But we might have essentially been witnessing a kind of event that has happened many, many

21:37.080 --> 21:43.840
times in human history, wherein, you know, a virus begins in very virulent and devastating,

21:43.840 --> 21:49.160
but eventually kind of goes global as it were and becomes incorporated into our genes

21:49.160 --> 21:50.160
for good.

21:50.160 --> 21:51.480
And so to make this more concrete.

21:51.480 --> 21:52.480
Yes.

21:52.480 --> 21:53.480
Yes.

21:53.480 --> 21:54.480
Yes.

21:54.480 --> 21:59.800
Well, it may be before, you know, a pit stop before we go into more concrete and technical,

21:59.800 --> 22:01.600
you know, it calls to mind.

22:01.600 --> 22:06.400
Obviously, I guess kind of a lot of the research in, I think, maybe the 80s or 70s, like

22:06.400 --> 22:15.880
swarming behaviors and things like that, which imagining are very simplistic models relative

22:15.880 --> 22:17.600
to the things that you're thinking about.

22:17.600 --> 22:18.600
Well, yes, yes.

22:18.600 --> 22:19.600
Also, factable.

22:19.600 --> 22:20.600
Yeah.

22:20.600 --> 22:21.600
That's right.

22:21.600 --> 22:25.760
So that's, I was, if I have time tomorrow, I'm going to show two things, two sort of technical

22:25.760 --> 22:31.120
things, one of which is very simple and is very much along those kind of 80s swarming

22:31.120 --> 22:32.120
sorts of lines.

22:32.120 --> 22:34.560
It's a simulation of bacteria.

22:34.560 --> 22:41.160
So trivial system, the learning is very, very simple and so you can characterize it completely.

22:41.160 --> 22:47.080
And the other one is about neural nets and update rules for cells and synapses.

22:47.080 --> 22:50.880
So that one applies to, you know, to the kind of systems that we build.

22:50.880 --> 22:55.400
So the bacteria one, yeah, it's very much inspired by that kind of centiphanes to, you

22:55.400 --> 22:58.160
know, sort of work from the 80s and 90s.

22:58.160 --> 23:03.760
And the idea there is, you know, bacteria have E. coli in particular, can either run or

23:03.760 --> 23:04.760
tumble.

23:04.760 --> 23:07.160
So when they run, they're going in a straight line and when they tumble, they randomize

23:07.160 --> 23:08.160
their direction.

23:08.160 --> 23:11.680
It has to do with the direction the flagella are rotating.

23:11.680 --> 23:15.840
And you know, they're too small to have a sense of like the global environment, you know,

23:15.840 --> 23:19.680
they can just go straight or, or kind of reverse, kind of like those RC cars that have

23:19.680 --> 23:24.040
a like go straight or, you know, reverse and turn, kind of, so it's a one bit output.

23:24.040 --> 23:30.040
And so I did a little simulation of them and they have a food source that moves around

23:30.040 --> 23:32.840
and if they don't get enough food, they die.

23:32.840 --> 23:37.400
And if they, if their food gets, if their energy level gets high enough, they reproduce.

23:37.400 --> 23:43.080
They can also conjugate when they touch, they can swap a little bit of DNA and they, and

23:43.080 --> 23:44.600
they, they hand her go some mutations.

23:44.600 --> 23:46.480
So it's artificial evolution.

23:46.480 --> 23:52.520
The first experiment involves thinking about the genome as just being the Q table.

23:52.520 --> 23:56.000
In other words, you know, in RL language, right, just that the table, you know, stimulated

23:56.000 --> 23:57.400
to behavior.

23:57.400 --> 24:01.680
And you can see how, you know, with this kind of evolutionary pressure, they evolve to follow

24:01.680 --> 24:02.680
the food.

24:02.680 --> 24:05.720
There's a kind of, you know, their algorithms that let you do that, essentially they involve

24:05.720 --> 24:08.840
tumbling more when you're in the food and running more when you're far away from the

24:08.840 --> 24:09.840
food.

24:09.840 --> 24:13.840
And statistically that will, that will make the colony of bacteria follow the food around.

24:13.840 --> 24:20.040
I'm imagining the picture that comes to mind for me is the kind of the classic picture

24:20.040 --> 24:26.080
of an RL agent, I forget the game, but it's like a boat game where the RL agent kind of learns

24:26.080 --> 24:29.720
that it can rack up points by just spinning around in this particular place.

24:29.720 --> 24:30.720
Yeah.

24:30.720 --> 24:31.720
Yeah, that's right.

24:31.720 --> 24:36.120
Well, I mean, one of the fun things, of course, about RL is that any kind of machine learning

24:36.120 --> 24:39.120
really is that, you know, it can, it'll, if there's a way to cheat, it'll figure it

24:39.120 --> 24:40.120
out.

24:40.120 --> 24:41.120
Right.

24:41.120 --> 24:42.120
Like life.

24:42.120 --> 24:43.120
Right.

24:43.120 --> 24:44.120
That's right.

24:44.120 --> 24:46.640
So, yeah, the, it figures out how to follow the food around this way.

24:46.640 --> 24:49.720
Things get a little more interesting if you add an additional output and you let them

24:49.720 --> 24:52.120
emit an extra chemical that they can also sense.

24:52.120 --> 24:53.120
Okay.

24:53.120 --> 24:56.560
So, that, and that's done by bacteria, they have chemo, chemo signaling.

24:56.560 --> 25:00.160
So, you know, in the rules of the game that I set up, if they're signaling, they're losing

25:00.160 --> 25:01.160
health faster.

25:01.160 --> 25:02.160
So, it's costly.

25:02.160 --> 25:03.160
It's costly to signal.

25:03.160 --> 25:06.880
So, you, you would, you would first think that, well, the first thing to learn is to

25:06.880 --> 25:12.760
not signal because, you know, it's, it has no advantage for them, basically, and it,

25:12.760 --> 25:14.520
and it burns energy faster.

25:14.520 --> 25:15.520
But they don't.

25:15.520 --> 25:16.520
They keep signaling.

25:16.520 --> 25:20.480
So, in, you know, generation after generation, you know, restart after restart, they, you

25:20.480 --> 25:23.720
know, almost all of them retain the signaling capability.

25:23.720 --> 25:28.400
And what you realize, of course, is that they become a super organism when they, when they

25:28.400 --> 25:30.160
start to share genes.

25:30.160 --> 25:34.760
And, you know, so, thinking about them as individuals or as a, or as a single, as, as a

25:34.760 --> 25:38.520
single organism, as a tissue, you know, it's kind of like in the eye of the beholder.

25:38.520 --> 25:39.520
There's no, right?

25:39.520 --> 25:42.240
It's, it's, that's not determined by the rules of the game.

25:42.240 --> 25:49.200
And so, are we talking about the observed behavior in the game or in the thing that you

25:49.200 --> 25:50.200
model?

25:50.200 --> 25:51.200
Yes and yes.

25:51.200 --> 25:52.200
Yes, yes and yes.

25:52.200 --> 25:57.040
I mean, in the model, you can reduce everything to very, that's a very, very simple principle

25:57.040 --> 26:01.120
since, and see all of those behaviors emerge, which is, which is fun, because then you,

26:01.120 --> 26:03.240
you know, then it becomes clear that we're understanding something.

26:03.240 --> 26:05.240
So, yeah, they signal to each other.

26:05.240 --> 26:06.920
They obviously are behaving like a super organism.

26:06.920 --> 26:11.520
You can't say what is, you know, what is the agent, you know, is it one, is it many?

26:11.520 --> 26:16.320
And it's also kind of hard to say what they're optimizing, you know, like, if you take a

26:16.320 --> 26:20.040
very Darwinian red of tooth and claw kind of perspective, they're like, well, they're,

26:20.040 --> 26:23.640
they're trying to, they're maximizing their energy intake, well, but who is maximizing

26:23.640 --> 26:24.640
their energy intake?

26:24.640 --> 26:26.480
Is it an individual bacterium?

26:26.480 --> 26:28.640
Is it the whole colony?

26:28.640 --> 26:33.680
How do you think about, you know, the population or the size relative to what is being taken

26:33.680 --> 26:35.480
and what does optimization actually look like?

26:35.480 --> 26:42.400
Because in simulation, you observe behaviors that are sub-optimal, either individually

26:42.400 --> 26:46.840
or globally, but optimal otherwise, or well, even talking about optimal is hard.

26:46.840 --> 26:50.520
So I, you know, you just, you know, you're just going to the optimization that you propose,

26:50.520 --> 26:55.120
that it's all about energy intake and thus kind of speed of reproduction.

26:55.120 --> 26:59.000
Well, all I said was that they die if they go to zero and they reproduce if they get

26:59.000 --> 27:00.000
to one.

27:00.000 --> 27:01.000
I didn't say what was being optimized.

27:01.000 --> 27:02.000
Okay.

27:02.000 --> 27:05.360
So, you know, the thing with evolution is that like, what you see is what makes it, you

27:05.360 --> 27:09.320
know, what persists exists, you know, we're not actually writing down a laws function.

27:09.320 --> 27:13.320
So you can ask what is optimized and that's actually an inverse reinforcement learning

27:13.320 --> 27:14.320
problem.

27:14.320 --> 27:16.840
In other words, you have a queue table, now you back out policy and what's the right,

27:16.840 --> 27:18.440
what's the reward.

27:18.440 --> 27:22.480
And I kind of cheated in order to get, so IRL inverse reinforcement learning is actually

27:22.480 --> 27:27.920
a hard problem in general, but I cheated by switching things up so that rather than the

27:27.920 --> 27:31.880
genome being the queue table, now the genome is the reward map.

27:31.880 --> 27:35.920
So they essentially evolve a reward system or an emotional system if you want to think

27:35.920 --> 27:36.920
about it that way.

27:36.920 --> 27:42.280
And we see like, well, what's the reward system that actually results in survival.

27:42.280 --> 27:45.600
And what comes out is kind of what should expect, which is that in general, you know,

27:45.600 --> 27:51.000
you look at this many, many times, death is bad, signaling is bad, because that's, you

27:51.000 --> 27:57.600
know, that hurts and you lose health, right, mating is good, survival is good, food

27:57.600 --> 28:00.400
is good, you know, those are positive, these are negative, so it's, you know, it's kind

28:00.400 --> 28:03.680
of what you'd expect, but the surprise is the error bars.

28:03.680 --> 28:08.480
So when you look at specific winning solutions, that is solutions that, you know, converge

28:08.480 --> 28:12.640
and that, and that persist, they're all over the map.

28:12.640 --> 28:15.880
So those are averages that I just gave you, but the variance is huge.

28:15.880 --> 28:20.040
They're all these different reward maps that all work, you know, they, some of them result

28:20.040 --> 28:24.480
in, meaning dying off is a lot harder than continuing in a sense.

28:24.480 --> 28:28.880
Well, the, the set of, if you can find a, I mean, the set of, of, of emotional systems

28:28.880 --> 28:34.400
or rewards that work is small relative to the total imaginable space of reward maps,

28:34.400 --> 28:39.520
but it's also very varied, right, and there's not just one, there's not just one, okay,

28:39.520 --> 28:44.520
but there are many and some of them involve lots of exploration and therefore death is

28:44.520 --> 28:49.800
not bad for those, they're not afraid of death in some sense, some of them are very conservative

28:49.800 --> 28:53.400
and therefore have smaller populations, they hate death, they don't signal very much,

28:53.400 --> 28:56.480
they follow the food run really closely and all of those are viable.

28:56.480 --> 29:00.840
So, you know, this is kind of one data point, just so we don't get stuck in time on this

29:00.840 --> 29:03.640
thing, because I can ask you a bunch of questions about this particular thing.

29:03.640 --> 29:08.400
This is one data point that informs a lot, a larger perspective.

29:08.400 --> 29:10.920
What's the other, you mentioned a second example?

29:10.920 --> 29:11.920
Yes.

29:11.920 --> 29:16.680
So, yeah, all of this is really just a kind of extended example of, you know, of some

29:16.680 --> 29:21.560
of the problems that arise when you try and look at a, a real evolutionary system in

29:21.560 --> 29:25.880
terms of optimization and try and ask basic questions like what is it optimizing?

29:25.880 --> 29:31.520
So, I then take that to the home, I guess, to the thing that concerns all of us at this

29:31.520 --> 29:36.080
conference, which is, all right, so how do you train a model?

29:36.080 --> 29:43.200
There's been a bunch of work in recent years on meta-learning, which involves, you know,

29:43.200 --> 29:44.200
learning to learn.

29:44.200 --> 29:49.880
So, learning maybe what the update rule is at a synapse or learning how, you know, not

29:49.880 --> 29:52.480
assuming that the learning algorithm is fixed.

29:52.480 --> 29:56.880
And I really like that approach because, you know, our genome basically has in it the

29:56.880 --> 30:00.600
learning rule for our neurons and synapses and that evolved.

30:00.600 --> 30:06.240
So, I began playing around with systems that evolve, use evolution to determine the rules

30:06.240 --> 30:11.480
for neuron state and synapse state over time.

30:11.480 --> 30:13.360
And therefore, that have to learn to learn.

30:13.360 --> 30:16.920
They learn to learn on an evolutionary time scale and they learn on a behavioral time

30:16.920 --> 30:17.920
scale.

30:17.920 --> 30:18.920
That makes sense.

30:18.920 --> 30:24.800
So, the learning to learning includes instinct and imprinting and other kinds of things.

30:24.800 --> 30:32.520
And the learning to learn, you know, establishes the ability to go from stimuli to generalization.

30:32.520 --> 30:37.260
And if you do that, you basically have a little kind of LSTM at every neuron and every

30:37.260 --> 30:44.160
synapse that has local state and has its own time scales and so on that it learns evolutionarily.

30:44.160 --> 30:49.280
And you can get very, very fast learning of standard kind of machine learning models.

30:49.280 --> 30:51.960
I give, you know, some M-nist type examples.

30:51.960 --> 30:53.960
So learning from very, very few examples.

30:53.960 --> 30:57.520
And that's interesting if you just think about it in terms of optimization.

30:57.520 --> 31:02.600
You know, this is in the spirit of some other work that has been done, a Ravi and La Rochelle

31:02.600 --> 31:08.160
wrote a really nice paper in 2016 or 2017 that did something similar.

31:08.160 --> 31:13.360
My version of this is a little more general in that it's not designed necessarily to optimize.

31:13.360 --> 31:14.360
It's kind of broader.

31:14.360 --> 31:17.600
It's just like any old, you know, it's a very general kind of synapse rule in a very

31:17.600 --> 31:19.880
general cellular evolution rule that I don't say.

31:19.880 --> 31:21.200
So I understand this.

31:21.200 --> 31:28.200
You've got the picture that's forming in my head is kind of multiple agents or entities

31:28.200 --> 31:35.320
that have some kind of embedded, you know, memory, sequence, LSTM based thing.

31:35.320 --> 31:41.040
And in their interactions, they can essentially learn and solve M-nist type problems.

31:41.040 --> 31:45.560
So I do show an example like that, but I mean, if it's similar at all what you're saying.

31:45.560 --> 31:47.800
Well, the simpler version is as follows.

31:47.800 --> 31:48.800
Okay.

31:48.800 --> 31:53.160
Imagine that you just have, you know, in one neural net, a single kind of LSTM, a single

31:53.160 --> 31:57.240
set of weights for an LSTM that live at every cell and every synapse.

31:57.240 --> 31:58.640
It's like your genome, right?

31:58.640 --> 32:03.480
So that determines how the cell responds to inputs and how the weights change.

32:03.480 --> 32:09.720
So it's kind of a dynamic weight model based on the parameters of the LSTM's at each

32:09.720 --> 32:11.400
of the neurons.

32:11.400 --> 32:12.400
Essentially.

32:12.400 --> 32:13.400
Okay.

32:13.400 --> 32:14.400
And it's a common LSTM.

32:14.400 --> 32:17.040
So, you know, it's a small, it's like the difference with the genome, the connectome.

32:17.040 --> 32:19.280
The genome is very small, connectome is very large.

32:19.280 --> 32:22.680
The LSTM parameters are, you know, are the same everywhere, like a convolutional net kind

32:22.680 --> 32:23.680
of.

32:23.680 --> 32:24.680
Okay.

32:24.680 --> 32:29.200
And so they comprise the genome of the neural net and whatever weights it learns over

32:29.200 --> 32:30.200
time or the connectome.

32:30.200 --> 32:32.560
The same everywhere in structure, not in value.

32:32.560 --> 32:33.560
That's right.

32:33.560 --> 32:34.560
That's right.

32:34.560 --> 32:39.280
So different state at every neuron synapse, but the same weights, right, on the LSTM.

32:39.280 --> 32:40.280
Right.

32:40.280 --> 32:45.120
Which in turn determine the rule for updating the weights in the neural net, a little confusing

32:45.120 --> 32:46.120
I know a bit.

32:46.120 --> 32:47.120
Yeah.

32:47.120 --> 32:48.120
It's hard to do without the whiteboard.

32:48.120 --> 32:49.120
It's a little hard without the whiteboard.

32:49.120 --> 32:50.120
We run into that a lot.

32:50.120 --> 32:51.120
I'm sure.

32:51.120 --> 32:52.120
I'm sure.

32:52.120 --> 32:55.120
So, yeah, how do you train, how do you determine the genome?

32:55.120 --> 32:57.240
Well, now you do have to have a population.

32:57.240 --> 33:01.160
So you have a population of neural nets that in the beginning have random genomes and the

33:01.160 --> 33:06.400
attempt to learn, meaning you feed them in these digits, you feed them in error signal,

33:06.400 --> 33:11.040
and the ones that do a better job survive and reproduce, meaning they share their LSTM

33:11.040 --> 33:16.480
weights and yield into generation, and the ones that do poorly die off.

33:16.480 --> 33:21.600
So you can use evolution strategies to do this, I use CMAES, or you can use more classical

33:21.600 --> 33:22.600
kinds of evolution.

33:22.600 --> 33:23.600
CMAES.

33:23.600 --> 33:30.760
CMAES is a, so the ES is evolution strategies, and CMA is a particular variant of this

33:30.760 --> 33:37.400
that assumes that the set of parameters is a Gaussian blob, and essentially estimates

33:37.400 --> 33:42.920
the gradient of the fitness along that Gaussian and uses that to kind of make a new Gaussian

33:42.920 --> 33:43.920
in the next generation.

33:43.920 --> 33:44.920
Okay.

33:44.920 --> 33:51.800
So it's a toy model of evolution, and it's very robust optimizer for moderate dimensionality.

33:51.800 --> 33:57.800
So you do that with the genes, and you end up evolving a kind of net that learns really

33:57.800 --> 34:01.280
fast, if that's what you make your fitness function, like learn from the minimum number

34:01.280 --> 34:03.240
of examples, learn as fast as possible.

34:03.240 --> 34:08.240
So like in a normal, you know, MNIST kind of set up, you know, you measure how much training

34:08.240 --> 34:12.920
data in terms of how many passes over, you know, all 50,000 training examples or something.

34:12.920 --> 34:17.240
This one, you know, you measure its performance in terms of how many digits you show it.

34:17.240 --> 34:21.160
So like after you've shown it 10 or 20 digits, it's already doing pretty well.

34:21.160 --> 34:23.880
So it's, you know, really, really, really small in.

34:23.880 --> 34:29.240
And are you able to infrospect into it and understand what it's learning, what the hell

34:29.240 --> 34:30.240
are you exactly?

34:30.240 --> 34:31.240
Yeah.

34:31.240 --> 34:36.280
Well, I haven't thought enough of that yet, but I'm measuring like the texture and feature

34:36.280 --> 34:38.640
maps of a neural network.

34:38.640 --> 34:39.640
That's right.

34:39.640 --> 34:43.400
Well, I don't think that what it's, what it's learned, what the network is learning,

34:43.400 --> 34:48.440
that the weights are, yeah, weights is an ambiguous term here.

34:48.440 --> 34:53.240
I don't think that what the synapses get set to is particularly different from a normal

34:53.240 --> 34:54.240
neural net.

34:54.240 --> 34:55.240
Okay.

34:55.240 --> 34:56.240
What's different is the update rules.

34:56.240 --> 34:57.240
Yeah.

34:57.240 --> 34:59.080
And the update rules are governed by a little LSTM.

34:59.080 --> 35:02.840
So figuring out what that is doing is, you know, it's kind of like a biology problem.

35:02.840 --> 35:04.240
It's actually not that.

35:04.240 --> 35:06.400
It's not that trivial.

35:06.400 --> 35:10.480
But you can do things like, you know, normally when we do backprop, we assume symmetric

35:10.480 --> 35:14.920
weights, meaning, you know, that though you use the same weights when you're backpropagating

35:14.920 --> 35:18.360
is when you forward propagate, you have to, otherwise you can't take the derivative.

35:18.360 --> 35:21.760
But we know that in real brains, that's not how it works.

35:21.760 --> 35:25.600
Like, you know, signals don't propagate backpropagate through synapses in the top 10.

35:25.600 --> 35:26.600
Right.

35:26.600 --> 35:27.600
Right.

35:27.600 --> 35:31.240
And so usually we think of MNIST as this toy problem, but for someone that works on OCR,

35:31.240 --> 35:37.880
particularly relevant, are these things that you can, you have a path to actually using

35:37.880 --> 35:38.880
putting production?

35:38.880 --> 35:39.880
Oh, yeah.

35:39.880 --> 35:40.880
And for what?

35:40.880 --> 35:46.080
Well, I mean, in this case, the goals are to be able to learn from small data, which

35:46.080 --> 35:48.800
I care about from the point of view of privacy.

35:48.800 --> 35:52.760
And I care about because, you know, I think one of the Achilles' heels of deep learning

35:52.760 --> 35:55.520
as we do it today is its reliance on mass and amounts of data.

35:55.520 --> 36:00.200
And the reason we have that reliance is because we don't have very sophisticated learning

36:00.200 --> 36:04.240
rules, and they don't embed any priors that have been learned over, you know, evolutionary

36:04.240 --> 36:07.160
time the way ours have about the world, right?

36:07.160 --> 36:11.280
It's all of those priors and all of that intelligence that lives in our genome that lets us learn

36:11.280 --> 36:12.280
so fast.

36:12.280 --> 36:16.520
So, you know, this is an attempt to say like, well, can we reproduce what evolution did

36:16.520 --> 36:22.120
in order to learn statistical priors and learning rules that radically outperformed the feature

36:22.120 --> 36:27.600
engineered rules that we have today, you know, Adam and Ada Grad and SGD and all that?

36:27.600 --> 36:30.200
So that's interesting, just from the point of view of the table.

36:30.200 --> 36:31.200
I give what I want it.

36:31.200 --> 36:35.440
The question is, is there a catch, right?

36:35.440 --> 36:40.800
You know, do we just throw LSTMs in everything and, you know, now everything converges way

36:40.800 --> 36:41.800
quickly?

36:41.800 --> 36:42.800
Yeah.

36:42.800 --> 36:51.200
I mean, it's much more computationally expensive to do that, of course, but on the other

36:51.200 --> 36:55.280
hand, if you can learn from many, many fewer examples, then, you know, it's still a good

36:55.280 --> 37:00.240
thing, even computationally, and certainly from the point of view of data.

37:00.240 --> 37:01.240
Is there a trade-off?

37:01.240 --> 37:02.400
Yeah, absolutely.

37:02.400 --> 37:06.040
The trade-off is that, you know, when you're learning from very few examples, that means

37:06.040 --> 37:11.280
that you're bringing much heavier weight and sometimes rather opaque priors to bear on

37:11.280 --> 37:12.280
the problem.

37:12.280 --> 37:16.360
So, you know, you're subject to more cognitive fallacies and all kinds of, you know, all the

37:16.360 --> 37:17.960
things that humans are subject to.

37:17.960 --> 37:23.920
So all the issues that we talk about is bias that's being introduced in, you know, our

37:23.920 --> 37:28.800
data distributions are potentially magnified manyfold because we're training on much less

37:28.800 --> 37:29.800
data.

37:29.800 --> 37:30.800
Possibly.

37:30.800 --> 37:33.760
Although, I guess I would say a couple of things.

37:33.760 --> 37:37.600
I mean, one is that when you have very small, when you can learn from very small amounts

37:37.600 --> 37:43.520
of data, then you can perhaps be a bit more careful about how you curate that.

37:43.520 --> 37:47.120
But, and also, of course, the fact that the genome is very small means that you maybe

37:47.120 --> 37:51.880
have a little bit more hope of understanding, you know, what those biases and meta priors

37:51.880 --> 37:52.880
are.

37:52.880 --> 37:56.400
So I think it's still positive from the point of view of problems like ML fairness and

37:56.400 --> 37:57.400
so on.

37:57.400 --> 38:01.160
But that's definitely something that we have, you know, one has to look at very, very closely

38:01.160 --> 38:07.560
because, yeah, you know, newborn babies, you know, will react with fear to snake

38:07.560 --> 38:08.560
like objects.

38:08.560 --> 38:11.920
So like, even at a very high level, object like ignition kind of level, you know, there's

38:11.920 --> 38:15.600
something in the genome that, you know, that makes snake like things scary, and so you

38:15.600 --> 38:20.040
can imagine the problems that can arise from, you know, from having, you know, priors

38:20.040 --> 38:23.920
like that, hidden priors like that, you know, in the genome for learning that.

38:23.920 --> 38:26.440
So yes, that's definitely an issue.

38:26.440 --> 38:31.920
But at a more meta level also to connect this to the bacteria stuff, you know, what you

38:31.920 --> 38:38.200
can imagine is that this rule, it's not just a learning rule, it's actually a rule for

38:38.200 --> 38:40.720
how brains or how behavior works.

38:40.720 --> 38:42.680
And in that sense, it's like an emotional system.

38:42.680 --> 38:46.400
What has been learned by evolution is what is good or bad as well locally.

38:46.400 --> 38:53.000
And that's something I think that this is a route toward, toward real AI in ways that

38:53.000 --> 39:00.400
I don't think we can do with, I don't think we can get to that by hand writing either

39:00.400 --> 39:04.560
update rules or fitness functions or loss functions.

39:04.560 --> 39:10.920
And so in the, the model we were just talking about, the individual agents, actors, whatever

39:10.920 --> 39:13.880
you want to call organisms are these LSTMs.

39:13.880 --> 39:14.880
Right.

39:14.880 --> 39:18.840
So that's kind of looking at a, looking at a neural net as a society of neurons if you

39:18.840 --> 39:19.840
want to think about it.

39:19.840 --> 39:20.840
Right.

39:20.840 --> 39:25.440
You can also then take these brains and put them together and have them teach each other

39:25.440 --> 39:28.000
you have the kind of things that you were pointing out, right, have them interact socially

39:28.000 --> 39:30.000
and that's also super interesting.

39:30.000 --> 39:34.800
And I think that that's how you actually get emotional systems that, you know, that will

39:34.800 --> 39:35.800
work.

39:35.800 --> 39:41.880
So, so yeah, I have come to the belief that our route, I mean, I don't know what to call

39:41.880 --> 39:47.400
AI, I guess, right, but our route to like real brains has to go through this, like,

39:47.400 --> 39:48.400
Stalin intelligence.

39:48.400 --> 39:49.400
Yeah.

39:49.400 --> 39:53.560
It has to go through the social route and stopping hand defining the loss functions and

39:53.560 --> 39:54.560
the update rules.

39:54.560 --> 39:55.560
Interesting.

39:55.560 --> 40:01.160
One of the things that is common between this warm intelligence approach we were referring

40:01.160 --> 40:07.960
to earlier and what you just described is kind of a, you know, everything's the same.

40:07.960 --> 40:11.960
Like you're building these systems out of components that are the same and groups of components

40:11.960 --> 40:13.600
that are the same.

40:13.600 --> 40:17.720
Whereas, you know, maybe the counter example that we've talked about is GANS where you've

40:17.720 --> 40:20.240
got these two distinct things with different goals.

40:20.240 --> 40:21.240
Yes.

40:21.240 --> 40:26.160
Do you have you started looking at more heterogeneous systems?

40:26.160 --> 40:27.160
Yeah.

40:27.160 --> 40:28.160
I have.

40:28.160 --> 40:32.920
So, even within a single neural net, I've often played with having different genes for

40:32.920 --> 40:38.360
every layer, for example, or having different brain areas that have different, that have

40:38.360 --> 40:43.160
different genes in them that are interacting, you know, with, you know, that are sort of

40:43.160 --> 40:47.760
feeding, feeding into each other with asymmetric weights that are, you know, whose interactions

40:47.760 --> 40:51.040
are learned, or having multiple species together.

40:51.040 --> 40:54.680
And have entirely different kinds of inputs and outputs and that, you know, so this gets

40:54.680 --> 40:59.040
into very a life, a very artificial life kind of paradigm.

40:59.040 --> 41:00.880
So yeah, I think those are all really interesting roots.

41:00.880 --> 41:05.040
I mean, the challenge with a lot of this is, of course, that, you know, we've really relied

41:05.040 --> 41:09.640
on the fact that these things look like scores or in games for a long time in order to talk

41:09.640 --> 41:12.840
about, like, what is state of the art, what is better or worse, you know, it's research

41:12.840 --> 41:18.400
groups competing or collaborating with each other with a well-defined metric.

41:18.400 --> 41:23.960
And it's really hard to take these more social and organic kind of approaches and cope

41:23.960 --> 41:28.880
with the same sort of clear metrics, you know, for what constitutes progress.

41:28.880 --> 41:32.960
So that's one of the big challenges that I want to kind of leave the audience with, you

41:32.960 --> 41:39.760
know, how do we keep that sense of clarity of progress about, you know, how we're advancing

41:39.760 --> 41:44.400
in our understanding and our ability to solve these problems when we, you know, I believe

41:44.400 --> 41:47.880
inherently need to start looking at things that don't have such well-defined scores.

41:47.880 --> 41:54.720
And is that because the things that we should care about are internal metrics of these social

41:54.720 --> 42:02.400
entities or if we want to get kind of near-termish value out of them, we're applying them, the

42:02.400 --> 42:08.400
problems that have some type of score associated with them, you know, the things that we've

42:08.400 --> 42:15.080
been doing, like translation and OCR and, you know, playing games and things like that.

42:15.080 --> 42:18.760
Why aren't those metrics sufficient for these types of systems?

42:18.760 --> 42:19.760
Yeah, yeah.

42:19.760 --> 42:21.880
So both of the things you just said are correct.

42:21.880 --> 42:28.640
I mean, I'm not saying that metrics are bad thing for OCR, you know, or for face recognition

42:28.640 --> 42:32.620
where, like, you know, it's very clear what is good or bad, you know, so the problem with

42:32.620 --> 42:37.360
an alphairness for face recognition, for example, is now much discussed and it's actually

42:37.360 --> 42:38.360
really well posed.

42:38.360 --> 42:44.720
But if it doesn't work as well at distinguishing faces for some group of people that we define

42:44.720 --> 42:47.200
socially, then that's a problem.

42:47.200 --> 42:51.320
And the answer is clear, you know, like sample better in that space, you know, measure

42:51.320 --> 42:56.240
better in that space and, you know, and set a higher bar for, you know, for everything

42:56.240 --> 42:57.720
to performically well.

42:57.720 --> 43:02.880
The problem is that most real stuff doesn't actually fit into that paradigm.

43:02.880 --> 43:08.720
You know, what is the loss function for, you know, for a credit score for the correct

43:08.720 --> 43:14.600
assignment of a credit score or for couples counseling, you know, or for good or bad art,

43:14.600 --> 43:19.040
or for, you know, how to rank notifications on Android for that matter, you know, so

43:19.040 --> 43:22.400
like, really bread and butter stuff, you know what I'm saying, like, if you just optimize

43:22.400 --> 43:25.960
for India, I can tell you a thing or two about the loss function for ranking notifications

43:25.960 --> 43:26.960
on Android.

43:26.960 --> 43:29.440
Well, you know, and actually there are rules.

43:29.440 --> 43:33.000
Yeah, so at the moment, at the moment, it's unfortunately a lot of a lot of hard rules

43:33.000 --> 43:36.400
that, yeah, that themselves, I mean, that's where back to Q tables again, like if you

43:36.400 --> 43:39.920
try and back, back out using enforcement enforcement learning, like, okay, those rules

43:39.920 --> 43:43.720
are all, you know, kind of carefully considered, like, what are they optimizing?

43:43.720 --> 43:45.240
You cannot write down what they're optimizing.

43:45.240 --> 43:46.240
Right.

43:46.240 --> 43:47.240
Right.

43:47.240 --> 43:50.920
They're coming from a whole bunch of different socially constructed intuitions.

43:50.920 --> 43:59.840
So kind of your point is you're proposing this potentially much more powerful paradigm

43:59.840 --> 44:05.400
and with this much more powerful paradigm, we should be able to push into areas that

44:05.400 --> 44:12.200
we can't apply, you know, I always fault our saying traditional, you know, since, right,

44:12.200 --> 44:17.000
but she learning as we know it today, she learning as we know it today, which is very much

44:17.000 --> 44:21.880
based on kind of being able to write down these loss functions, but, you know, we're kind

44:21.880 --> 44:22.880
of in it.

44:22.880 --> 44:25.880
It's a little bit of a chicken and egg like we can't write down a loss function so we don't

44:25.880 --> 44:31.080
know how to apply these new things or even how to measure success with these new things.

44:31.080 --> 44:35.040
Well, and what I would argue is that, you know, it, again, it's social, right?

44:35.040 --> 44:39.240
So the point of ranking, I mean, if you were doing the ranking and, you know, you're

44:39.240 --> 44:42.480
like a little person living in somebody's phone doing the ranking for them and you were

44:42.480 --> 44:45.600
able to consult also with the other rankers, you know, and everybody else's phone using

44:45.600 --> 44:50.960
federated learning or something like that, then, you know, the fundamental tool that

44:50.960 --> 44:55.520
you'd want to be able to do that effectively is empathy.

44:55.520 --> 44:59.360
You know, it's not, it's not like maximized engagement, you know, or some kind of stupid,

44:59.360 --> 45:01.160
you know, quantitative measure like that.

45:01.160 --> 45:02.160
Right.

45:02.160 --> 45:04.360
It's more like, you know, be good for the humans.

45:04.360 --> 45:05.360
Right.

45:05.360 --> 45:06.360
Right.

45:06.360 --> 45:12.320
And that requires modeling the other and using that as a guide for how to behave collectively

45:12.320 --> 45:13.320
and individually.

45:13.320 --> 45:16.640
And there may be conflicts in that, like, you know, whether you model them at the individual

45:16.640 --> 45:20.760
level or the collective level, you know, just like in medical ethics, you have conflicts

45:20.760 --> 45:25.160
about, say, you know, I don't know, giving people antibiotics, right?

45:25.160 --> 45:29.520
And in the aggregate, it's not not necessarily a good thing if you're only optimizing for

45:29.520 --> 45:31.280
the individual, you'll do it more.

45:31.280 --> 45:32.280
And so on.

45:32.280 --> 45:33.880
The same issues come up here.

45:33.880 --> 45:39.240
So, you know, you've got to kind of create an entity that has clear allegiances and clear

45:39.240 --> 45:44.680
kind of, you know, empathic goals that don't quite look like lost functions, but look

45:44.680 --> 45:47.720
more like being able to relate socially to the relevant kinds of entities.

45:47.720 --> 45:52.720
And you're sounding at this from a biological perspective, I imagine that there are connections

45:52.720 --> 45:57.480
to many other, you know, economics and sociology and other things if you were to really get

45:57.480 --> 45:59.120
into the social aspect of this.

45:59.120 --> 46:00.120
That's right.

46:00.120 --> 46:02.400
And dynamical systems theory and various other fields.

46:02.400 --> 46:03.400
Right.

46:03.400 --> 46:05.800
There's a lot of other fields come into it, yeah, exactly.

46:05.800 --> 46:06.800
Awesome.

46:06.800 --> 46:07.800
Awesome.

46:07.800 --> 46:12.600
Well, guys, thanks so much for taking the time to, you know, share what you're doing and

46:12.600 --> 46:16.640
give us a preview of what you're speaking about tomorrow.

46:16.640 --> 46:20.120
Of course, folks that are listening to this can actually go and check out the recording

46:20.120 --> 46:21.120
of your talk.

46:21.120 --> 46:22.120
Absolutely.

46:22.120 --> 46:24.640
If they've listened to all of this, they've gotten a longer version than they've talked

46:24.640 --> 46:25.640
tomorrow morning.

46:25.640 --> 46:27.520
So, thank you for asking such great questions.

46:27.520 --> 46:28.520
Nice.

46:28.520 --> 46:36.600
All right, everyone, that's our show for today.

46:36.600 --> 46:42.880
For more information on today's guests, visit twimmaleye.com slash shows.

46:42.880 --> 47:01.240
Thanks so much for listening and catch you next time.

