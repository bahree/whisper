1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,880
I'm your host Sam Charrington.

4
00:00:31,880 --> 00:00:36,520
This week we're celebrating the second anniversary of this podcast.

5
00:00:36,520 --> 00:00:41,920
If the pod has made a difference for you, please show us some love by letting us know how

6
00:00:41,920 --> 00:00:48,960
over on our second anniversary page at twimlai.com slash 2av.

7
00:00:48,960 --> 00:00:53,280
You want to hear the ways the podcast has helped you or your business, how it's enabled

8
00:00:53,280 --> 00:00:58,200
you to find or connect to resources you found valuable, or how it's educated you about

9
00:00:58,200 --> 00:01:00,800
new and interesting topics.

10
00:01:00,800 --> 00:01:04,960
Submit your written or audio comments via the page we've set up, and we'll be sharing

11
00:01:04,960 --> 00:01:10,760
a few of your stories with your permission of course, in a special podcast episode celebrating

12
00:01:10,760 --> 00:01:13,480
the Twimble community.

13
00:01:13,480 --> 00:01:18,720
In today's show, I sit down with David Van Veilin, assistant professor of bioengineering

14
00:01:18,720 --> 00:01:21,040
and biology at Caltech.

15
00:01:21,040 --> 00:01:25,840
David joined me after his talk at the Figure 8 train AI conference to chat about his research

16
00:01:25,840 --> 00:01:31,280
using image recognition and segmentation techniques in biological settings.

17
00:01:31,280 --> 00:01:35,800
In particular, we discuss his use of deep learning to automate the analysis of individual

18
00:01:35,800 --> 00:01:39,360
cells and live cell imaging experiments.

19
00:01:39,360 --> 00:01:44,000
We had a really interesting discussion around the various practicalities he's learned

20
00:01:44,000 --> 00:01:48,640
about training deep neural networks for image analysis, and he shares some awesome insights

21
00:01:48,640 --> 00:01:52,360
into which of the techniques from the deep learning research have worked for him and

22
00:01:52,360 --> 00:01:53,840
which haven't.

23
00:01:53,840 --> 00:01:57,600
If you're a fan of our nerd alert shows, you'll really like this one.

24
00:01:57,600 --> 00:02:01,880
Before we jump in, I'd like to send a shout out to our friends over at Figure 8 for their

25
00:02:01,880 --> 00:02:07,280
continued support of the show and their sponsorship of this week's series which all took place

26
00:02:07,280 --> 00:02:09,320
at train AI.

27
00:02:09,320 --> 00:02:15,080
Figure 8 is the essential human and loop AI platform for data science and machine learning teams.

28
00:02:15,080 --> 00:02:19,920
The Figure 8 software platform trains, tests, and tunes machine learning models to make

29
00:02:19,920 --> 00:02:22,240
AI work in the real world.

30
00:02:22,240 --> 00:02:27,800
Learn more at www.figure-8.com

31
00:02:27,800 --> 00:02:32,600
Just a reminder, this episode was recorded live on site so there's some unavoidable

32
00:02:32,600 --> 00:02:34,200
background noise.

33
00:02:34,200 --> 00:02:39,240
And now on to the show.

34
00:02:39,240 --> 00:02:44,600
All right everyone, I am here with David Van Bellen at the train AI conference.

35
00:02:44,600 --> 00:02:49,440
David is a system professor in the biology and bioengineering department at Caltech.

36
00:02:49,440 --> 00:02:52,040
David, welcome to this week in machine learning and AI.

37
00:02:52,040 --> 00:02:53,520
Thank you for having me.

38
00:02:53,520 --> 00:02:54,520
Awesome.

39
00:02:54,520 --> 00:02:57,440
Why don't we get started by having you tell us a little bit about your background and

40
00:02:57,440 --> 00:03:00,720
how you came into AI as a biologist?

41
00:03:00,720 --> 00:03:01,720
Yeah.

42
00:03:01,720 --> 00:03:05,680
So I can start from the beginning.

43
00:03:05,680 --> 00:03:13,080
So I was born in Los Angeles, grew up on the East Coast, upstate New York for middle school

44
00:03:13,080 --> 00:03:16,400
and West Virginia for high school, where upstate.

45
00:03:16,400 --> 00:03:24,040
So we lived in Liverpool, which is, I want to say like within like an hour of Syracuse.

46
00:03:24,040 --> 00:03:30,800
So it was very, it was upstate enough where things like like effects know were a very,

47
00:03:30,800 --> 00:03:31,800
is a reality.

48
00:03:31,800 --> 00:03:36,680
Yeah, I asked because I grew up in New York City and went to school, upstate it.

49
00:03:36,680 --> 00:03:37,680
Okay.

50
00:03:37,680 --> 00:03:38,680
Awesome.

51
00:03:38,680 --> 00:03:39,680
And that's our New York.

52
00:03:39,680 --> 00:03:40,680
Yeah.

53
00:03:40,680 --> 00:03:41,680
I'm familiar with RPI.

54
00:03:41,680 --> 00:03:44,880
They have very, very good students.

55
00:03:44,880 --> 00:03:45,880
Yeah.

56
00:03:45,880 --> 00:03:46,880
Yeah.

57
00:03:46,880 --> 00:03:53,680
And so I went to high school in West Virginia and graduated, went to college.

58
00:03:53,680 --> 00:03:57,480
So I went to MIT and I was a double major in math and physics.

59
00:03:57,480 --> 00:04:05,000
At the same time, I had the luxury of having like fresh and biology taught by like Eric

60
00:04:05,000 --> 00:04:10,000
Lander, who's, you know, big shot scientists who like runs like the Broad Institute, between

61
00:04:10,000 --> 00:04:11,800
like MIT and Harvard.

62
00:04:11,800 --> 00:04:17,600
You know, he was like one of the big early pioneers in like the human genome research project.

63
00:04:17,600 --> 00:04:20,280
And he was also as a background as a mathematician.

64
00:04:20,280 --> 00:04:25,560
And so the way he explained biology really resonated with me and made me want to like sort

65
00:04:25,560 --> 00:04:29,160
of continue that thread further.

66
00:04:29,160 --> 00:04:37,560
And so after I finished at MIT, I went to the MD PhD program at UCLA and Caltech.

67
00:04:37,560 --> 00:04:42,400
And so the way that program works is you spend two years, first two years, doing medical

68
00:04:42,400 --> 00:04:46,040
school, then you do your PhD and then you return for your clinical training.

69
00:04:46,040 --> 00:04:53,040
And I did my PhD at Caltech in the employee phase of the department with Rob Phillips.

70
00:04:53,040 --> 00:04:59,080
And there, it was a really great environment because there's exposure to sort of, you

71
00:04:59,080 --> 00:05:03,760
know, thinking about problems in biology, using like ideas from physics.

72
00:05:03,760 --> 00:05:08,600
But then there was also some very quantitative experiments that we're doing as well.

73
00:05:08,600 --> 00:05:11,000
And so there was a lot of imaging.

74
00:05:11,000 --> 00:05:13,480
There was a lot of image analysis.

75
00:05:13,480 --> 00:05:17,440
And there was a lot of image segmentation and so image segmentation, you know, figuring

76
00:05:17,440 --> 00:05:21,400
out what parts of your images correspond to like which objects you care about.

77
00:05:21,400 --> 00:05:22,400
We have to do a lot of that.

78
00:05:22,400 --> 00:05:26,080
So that was sort of like my first exposure to those sorts of problems.

79
00:05:26,080 --> 00:05:30,320
And there was sort of this given where if you were sort of the more mathy type and you

80
00:05:30,320 --> 00:05:34,480
could do the physics calculations, then people just assume that you're going to be good

81
00:05:34,480 --> 00:05:38,480
at doing the image segmentation and that you'd figure out how to get those tools to work

82
00:05:38,480 --> 00:05:40,960
to solve like your problems of interest.

83
00:05:40,960 --> 00:05:45,280
And by and large, like, you know, they're right, but it's still like a separate skill set

84
00:05:45,280 --> 00:05:47,560
that you have to learn.

85
00:05:47,560 --> 00:05:53,520
And yeah, and so that was probably like my first introduction to computer vision, to image

86
00:05:53,520 --> 00:05:57,560
segmentation, image recognition was during my graduate training.

87
00:05:57,560 --> 00:06:03,440
Finished my PhD on 2011, finished my MD 2013.

88
00:06:03,440 --> 00:06:09,960
So I went back to clinical training from 2011 to 2013 and you know, got like more exposure

89
00:06:09,960 --> 00:06:14,240
to problems in clinical medicine.

90
00:06:14,240 --> 00:06:19,920
And then after that, I went up to Stanford to continue training as a postdoctoral fellow.

91
00:06:19,920 --> 00:06:24,760
And so I was up at Stanford for about four or five years with Marcus Cover in the bioengineering

92
00:06:24,760 --> 00:06:26,560
department.

93
00:06:26,560 --> 00:06:33,400
And there there's when I started getting into sort of like the more machine learning

94
00:06:33,400 --> 00:06:40,680
slash AI space and so your initial exposure to computer vision was more traditional tools

95
00:06:40,680 --> 00:06:47,400
that are close to machine learning, so if you're doing edge finding filtering, thresholding,

96
00:06:47,400 --> 00:06:56,240
watershed transforms, you know, wavelets, all of that, you know, that was my first exposure.

97
00:06:56,240 --> 00:06:59,760
And so what you would usually end up doing is you, you know, you have an idea of like what

98
00:06:59,760 --> 00:07:02,960
these tools did and how each one worked.

99
00:07:02,960 --> 00:07:10,000
And then you'd basically bash some collection of these tools together to work on like your

100
00:07:10,000 --> 00:07:12,040
problem like of interest.

101
00:07:12,040 --> 00:07:15,880
And so the experiments that we were doing at the time, and so I think you probably saw

102
00:07:15,880 --> 00:07:21,880
some of it at the talk that I gave yesterday is, you know, we have these images of cells

103
00:07:21,880 --> 00:07:26,600
and so at the, so let me just give it back story of the talk for like your listeners.

104
00:07:26,600 --> 00:07:30,320
So part of the talk I was sort of showing like what I was doing during my graduate work.

105
00:07:30,320 --> 00:07:34,120
We were very interested in like the life cycle of these viruses and in particular how these

106
00:07:34,120 --> 00:07:38,680
viruses got their DNA from inside the viral capsid into like the bacterial cell.

107
00:07:38,680 --> 00:07:42,440
We didn't really have an idea of how it worked and so we had this idea to create an experiment

108
00:07:42,440 --> 00:07:43,680
to like let us watch.

109
00:07:43,680 --> 00:07:49,080
And so the challenge was, can you figure out a way to look at one piece of DNA being transferred

110
00:07:49,080 --> 00:07:52,360
from like inside the virus to inside the cell?

111
00:07:52,360 --> 00:07:57,120
The answer is yes you can, you have to use these very particular kind of dye that will

112
00:07:57,120 --> 00:08:00,880
stain the DNA while it's inside the viral capsid and then you can watch the dye molecules

113
00:08:00,880 --> 00:08:05,040
get transferred between the virus into the bacterial cell.

114
00:08:05,040 --> 00:08:08,800
So you get these gorgeous movies but we then have to do is quantify these movies so you

115
00:08:08,800 --> 00:08:12,120
can, you know, have some sense of the dynamics of this process.

116
00:08:12,120 --> 00:08:18,120
And to get a sense of the type of movie that we're looking at, like what's the field of

117
00:08:18,120 --> 00:08:23,400
view and in terms of how zoomed in this is this, is are you looking at one transfer, one

118
00:08:23,400 --> 00:08:26,440
organism or is it many, many that you have to figure out?

119
00:08:26,440 --> 00:08:32,440
For one paper's worth of data, there's roughly about 50 or so events that you're looking

120
00:08:32,440 --> 00:08:33,440
at.

121
00:08:33,440 --> 00:08:37,240
And it's not that often.

122
00:08:37,240 --> 00:08:42,520
So you're generally looking at maybe like a few hundreds of you live you, there might

123
00:08:42,520 --> 00:08:48,320
be, you know, somewhere between 20 to 100 is those cells within that field of view.

124
00:08:48,320 --> 00:08:54,280
And you know, all of those cells maybe like a couple will have like an infection event

125
00:08:54,280 --> 00:08:58,240
within what time period within the time period of about 30 minutes.

126
00:08:58,240 --> 00:09:04,800
So these movies were taken in roughly like one snapshot every 30 seconds to every minute.

127
00:09:04,800 --> 00:09:09,480
And you know, that's sort of like how the, that data was collected, granted it's been

128
00:09:09,480 --> 00:09:14,000
about seven years since I collected that data set.

129
00:09:14,000 --> 00:09:17,880
But from what I remember, that's, that's what we, that's what we did.

130
00:09:17,880 --> 00:09:21,000
And so you get these like, you get these gorgeous movies.

131
00:09:21,000 --> 00:09:26,120
And so what you need to then do is, you know, identify, okay, which part of the movie

132
00:09:26,120 --> 00:09:30,320
are which pixels within each frame correspond to like the cells, which ones correspond to

133
00:09:30,320 --> 00:09:34,880
background, which ones correspond to the virus, which ones correspond to the viruses that

134
00:09:34,880 --> 00:09:36,600
we actually care about.

135
00:09:36,600 --> 00:09:39,680
And then once you've done that, then you can go and quantify your images, right?

136
00:09:39,680 --> 00:09:43,520
And so you can ask, you know, how bright were the viruses over time, how bright were the

137
00:09:43,520 --> 00:09:49,960
cells over time and use that to generate, you know, some like quantitative curves of,

138
00:09:49,960 --> 00:09:54,120
here's how much DNA was being transferred as a function of time.

139
00:09:54,120 --> 00:09:59,200
And then once you have like that, those curves, then you can go and do your, you know, your

140
00:09:59,200 --> 00:10:05,000
fancy modeling, your physics and, you know, determine, okay, well, from this set of hypotheses

141
00:10:05,000 --> 00:10:08,960
for how I think this process is happening, you know, which ones are actually like consistent

142
00:10:08,960 --> 00:10:10,880
with like the data that we've generated.

143
00:10:10,880 --> 00:10:15,880
And so when you're trying to determine the, the brightness, which tells you how much DNA

144
00:10:15,880 --> 00:10:21,200
was transferred, is this, how much in terms of the length of the sequences transferred

145
00:10:21,200 --> 00:10:22,200
or some of the other.

146
00:10:22,200 --> 00:10:27,080
It's ideally like what you want to, that's ideally like what you want is like how many

147
00:10:27,080 --> 00:10:33,280
base pairs or how many nanometers worth of DNA has been transferred like over time.

148
00:10:33,280 --> 00:10:35,920
And so you kind of use a calibration curve.

149
00:10:35,920 --> 00:10:40,360
And so you know at the beginning of the event, you know, the virus has about 48 kilobase

150
00:10:40,360 --> 00:10:45,960
pairs of DNA at the end of the event, like it should have zero and use some like linear

151
00:10:45,960 --> 00:10:51,200
interpolation, you know, assuming that, you know, you know, linearly related, you know,

152
00:10:51,200 --> 00:10:55,800
brightness versus like how much DNA, and then you can like infer from the movies, go from

153
00:10:55,800 --> 00:10:59,720
brightness to like the amount of DNA.

154
00:10:59,720 --> 00:11:02,520
So where does machine learning and AI fit into all that?

155
00:11:02,520 --> 00:11:03,520
Yeah.

156
00:11:03,520 --> 00:11:08,000
So machine learning and AI fits in, and how do you do like the object, how do you do like

157
00:11:08,000 --> 00:11:10,160
the object recognition, the image segmentation?

158
00:11:10,160 --> 00:11:15,440
And, you know, that experiment was, you know, it was sort of like the classical computer

159
00:11:15,440 --> 00:11:18,080
vision approaches that we used to analyze that data.

160
00:11:18,080 --> 00:11:22,240
And when I started my postdoctoral fellowship at Stanford, I had ideas for experiments that

161
00:11:22,240 --> 00:11:23,520
I'd like to do.

162
00:11:23,520 --> 00:11:29,720
And basically what I wanted to do was, you know, look at genome scale knockout libraries

163
00:11:29,720 --> 00:11:34,520
with image them and then look at them with single cell resolution.

164
00:11:34,520 --> 00:11:36,280
So what's a genome scale knockout library?

165
00:11:36,280 --> 00:11:42,760
Genome scale knockout library is a collection of strains where in each strain, you have one

166
00:11:42,760 --> 00:11:45,880
gene that's been removed or inactivated.

167
00:11:45,880 --> 00:11:51,720
And so if you're interested in, you know, say like a particular like biological process.

168
00:11:51,720 --> 00:11:55,680
So the ones that the one that I was interested in at the time and that my lab is currently

169
00:11:55,680 --> 00:11:58,040
interested in are host virus interactions.

170
00:11:58,040 --> 00:12:02,320
And so you want to understand how do viruses and their host talk to each other and what

171
00:12:02,320 --> 00:12:06,080
parts of the host are important for that communication.

172
00:12:06,080 --> 00:12:12,600
There are experiments that you can do where if you have a way to look at the communication

173
00:12:12,600 --> 00:12:19,840
using imaging, then you can basically go one by one, remove a gene from like the host.

174
00:12:19,840 --> 00:12:26,240
And if that gene was important, then that's going to alter what the communication was

175
00:12:26,240 --> 00:12:31,720
or what like the outcome of the violent infection ended up being.

176
00:12:31,720 --> 00:12:36,600
And you know, that's what you know, the host in this case is the host in this case is not

177
00:12:36,600 --> 00:12:38,560
the virus, but the cell that's infecting.

178
00:12:38,560 --> 00:12:39,560
Right.

179
00:12:39,560 --> 00:12:40,560
Yeah.

180
00:12:40,560 --> 00:12:47,760
So moving that particular sequence prior to the infection, prior to the infection.

181
00:12:47,760 --> 00:12:51,800
And so you've got a control experiment, you've got a control where nothing's been removed

182
00:12:51,800 --> 00:12:56,560
and then you have a collection of strains where in each strain, one gene has been removed.

183
00:12:56,560 --> 00:13:03,400
And so there are collections that like this that exists for certain strains of E. coli,

184
00:13:03,400 --> 00:13:06,920
there are collections that exist for East.

185
00:13:06,920 --> 00:13:13,080
People at the Chan's upper initiative are creating collections like this that exist for, you

186
00:13:13,080 --> 00:13:15,160
know, mammalian cells.

187
00:13:15,160 --> 00:13:19,680
And these are really powerful tools because, you know, because they exist in like this

188
00:13:19,680 --> 00:13:24,400
array format where one well of additional have one strain that has one gene removed, then

189
00:13:24,400 --> 00:13:32,160
you can systematically go one by one and ask, okay, does this removal, does that influence

190
00:13:32,160 --> 00:13:33,520
what I was interested in studying?

191
00:13:33,520 --> 00:13:37,760
So in this case, does it influence the outcome of particular viral infection?

192
00:13:37,760 --> 00:13:42,480
So what we want to do is to be able to read out a screen of this library using imaging

193
00:13:42,480 --> 00:13:47,280
and imaging with single star resolution, there you're going from the case of where you're

194
00:13:47,280 --> 00:13:53,320
doing, you know, 50 or 100 or 1000 cells to doing millions.

195
00:13:53,320 --> 00:13:59,320
And there you don't have the luxury of being able to like go back and like manually correct

196
00:13:59,320 --> 00:14:00,320
like segmentation errors.

197
00:14:00,320 --> 00:14:06,120
You really need something that's going to work 99% of the time if not, if not better.

198
00:14:06,120 --> 00:14:08,080
And that's where the machine learning comes in.

199
00:14:08,080 --> 00:14:13,200
And so I remember when I was a medical student, I was sort of like browsing Facebook and

200
00:14:13,200 --> 00:14:21,280
I realized like, hey, you know, Facebook actually knows like where the faces are in, you know,

201
00:14:21,280 --> 00:14:25,400
in like all the images I'm uploading, not only that, but it has like ideas of like who's

202
00:14:25,400 --> 00:14:27,320
in these, who's in these pictures?

203
00:14:27,320 --> 00:14:32,480
Like, is it you? Is it like your, you know, like you're one of your three like best friends?

204
00:14:32,480 --> 00:14:36,240
And you know, just, you know, I've done like a lot of microscopy in my, um, over my

205
00:14:36,240 --> 00:14:37,240
career.

206
00:14:37,240 --> 00:14:43,200
And it's like, I know that the images that I collect on microscope are a lot simpler than

207
00:14:43,200 --> 00:14:48,160
the images of like me doing Brazilian Jiu-Jitsu or hanging out with my friends that like

208
00:14:48,160 --> 00:14:49,160
a part of your whatnot.

209
00:14:49,160 --> 00:14:50,160
Right.

210
00:14:50,160 --> 00:14:55,240
You know, what are those guys doing and can I steal it, um, or borrow or co-op, but, you

211
00:14:55,240 --> 00:15:00,240
know, rallies like, you know, blatant theft, you know, can I take those tools and then repurpose

212
00:15:00,240 --> 00:15:02,760
them to like work on our microscopy data?

213
00:15:02,760 --> 00:15:07,680
And so the answer was, you know, the answer was yes, um, and on the time that I started

214
00:15:07,680 --> 00:15:11,200
my postdoc and started like, think about these issues again, was literally right as the

215
00:15:11,200 --> 00:15:13,840
deep learning and fletch and curve, um, started to take off.

216
00:15:13,840 --> 00:15:14,840
Okay.

217
00:15:14,840 --> 00:15:20,320
And so it turned out, you know, yes, you can repurpose these tools and it, you know, it

218
00:15:20,320 --> 00:15:23,520
ends up working like really, really, really well.

219
00:15:23,520 --> 00:15:28,440
And so we had a paper that we did, um, and plus computation biology, um, basically showing

220
00:15:28,440 --> 00:15:35,960
that for the single cell image segmentation problem, the existing tools, um, work as

221
00:15:35,960 --> 00:15:39,760
well as a state of the art, um, for a cross a variety of different cell types.

222
00:15:39,760 --> 00:15:45,920
So whether you're imaging bacteria, whether you're imaging mammalian cells in cell culture,

223
00:15:45,920 --> 00:15:51,040
if you're looking at phase images, if you're looking at fluorescence, microscopy images,

224
00:15:51,040 --> 00:15:55,160
that, you know, the tools work really well and it's at a point that, you know, people

225
00:15:55,160 --> 00:15:59,160
in the cell biology community, you know, really need to start taking like a really close

226
00:15:59,160 --> 00:16:04,000
look at these tools and, you know, start thinking about like, how can these, um, how can

227
00:16:04,000 --> 00:16:08,520
these change the way that I do my experiments and what experiments will they let me do?

228
00:16:08,520 --> 00:16:10,520
That would previously have been like impossible.

229
00:16:10,520 --> 00:16:11,520
Okay.

230
00:16:11,520 --> 00:16:12,520
Yeah.

231
00:16:12,520 --> 00:16:16,840
And so did you use off the shelf CNNs and all the kind of stuff, or did you have to start?

232
00:16:16,840 --> 00:16:21,000
And yeah, so like I started at the beginning.

233
00:16:21,000 --> 00:16:28,480
And so yeah, so at the time, um, when I first started, um, Andre Carpathi put together

234
00:16:28,480 --> 00:16:32,840
and faithfully put together like their first CS231 end course, which I think is now kind

235
00:16:32,840 --> 00:16:34,360
of like iconic in the field.

236
00:16:34,360 --> 00:16:38,760
I got to sit on that for a couple of weeks, um, to like sort of like get, um, familiarize

237
00:16:38,760 --> 00:16:42,280
myself with the field, enough of the point where I can go and like, you know, hack some

238
00:16:42,280 --> 00:16:47,640
piece of code together, Karrist didn't really, um, was didn't really exist at the time.

239
00:16:47,640 --> 00:16:49,840
Um, cafe was like a new thing.

240
00:16:49,840 --> 00:16:54,560
Um, TensorFlow didn't exist and it was basically, you know, do you want to learn kuda?

241
00:16:54,560 --> 00:16:57,120
Or were you like, okay, like learning like piano?

242
00:16:57,120 --> 00:17:01,160
And so when I first started, um, I was programming in like naked piano, had to figure

243
00:17:01,160 --> 00:17:05,880
out how to pipe data into like, you know, simple like CNN, um, architectures.

244
00:17:05,880 --> 00:17:07,200
How do you like save models?

245
00:17:07,200 --> 00:17:10,360
How do you like, you know, once you like save parameters, like how do you load it?

246
00:17:10,360 --> 00:17:15,360
Like the really nice frameworks that exist today, like those didn't, um, those didn't

247
00:17:15,360 --> 00:17:16,960
exist at the time.

248
00:17:16,960 --> 00:17:19,480
And so, you know, sort of like had to like figure like all those things out, get some like

249
00:17:19,480 --> 00:17:21,720
hacky pieces of code that would work.

250
00:17:21,720 --> 00:17:26,920
But the first, like, basically like the very simplest, you know, CNN architectures that

251
00:17:26,920 --> 00:17:31,520
you could write down ended up working, um, really well.

252
00:17:31,520 --> 00:17:35,520
And so actually like, as far as this stuff that my lab uses, um, in production for the

253
00:17:35,520 --> 00:17:41,120
data that we analyze, um, the data that we analyze with, um, well, with our collaborators,

254
00:17:41,120 --> 00:17:45,520
um, we, you know, still use those like very simple, um, neural network architectures.

255
00:17:45,520 --> 00:17:50,760
Um, I would say like there's, um, there's a lot of like really cool stuff that people

256
00:17:50,760 --> 00:17:51,760
have done.

257
00:17:51,760 --> 00:17:58,360
Um, but I found it very useful to pay attention to what's going to generalize on small datasets,

258
00:17:58,360 --> 00:18:01,640
as opposed to, you know, what's the latest and greatest that are out there.

259
00:18:01,640 --> 00:18:06,840
And, you know, just from like my own personal experience, you know, some of the things that

260
00:18:06,840 --> 00:18:11,040
will perform like really well on image net, like say like residual networks, um, they have

261
00:18:11,040 --> 00:18:12,040
issues with overfitting.

262
00:18:12,040 --> 00:18:16,720
And if you don't have like that large corpus of data where your network can learn all

263
00:18:16,720 --> 00:18:22,360
of the edge cases, you'll still, you'll get more like practical utility from like, you

264
00:18:22,360 --> 00:18:26,920
know, the simpler neural networks trained, um, in an approach, in a way where you like,

265
00:18:26,920 --> 00:18:32,600
you're doing like the regularization, um, and the, you know, normalization and like the

266
00:18:32,600 --> 00:18:34,360
post processing, like correctly.

267
00:18:34,360 --> 00:18:41,000
And so was your, your training data, the datasets that you had traditionally created using

268
00:18:41,000 --> 00:18:43,960
manual segmentation or traditional computer vision?

269
00:18:43,960 --> 00:18:44,960
Yeah.

270
00:18:44,960 --> 00:18:49,920
So those were, so I would say like this is true and academia is true in life is that if

271
00:18:49,920 --> 00:18:54,160
you want to get people excited about something, like first you have to prove that it's going

272
00:18:54,160 --> 00:18:55,160
to work.

273
00:18:55,160 --> 00:19:01,280
Um, and so the first generation of training data, um, like I created like myself.

274
00:19:01,280 --> 00:19:10,240
And so I annotated a couple images of bacterial cells in image J and, you know, image J is

275
00:19:10,240 --> 00:19:16,000
a, um, Java, a Java program that people have labeling box bounding box tools.

276
00:19:16,000 --> 00:19:21,760
It's like a general, it's a general purpose, um, image visualization and also image analysis,

277
00:19:21,760 --> 00:19:23,600
um, toolkit that the NIH supports.

278
00:19:23,600 --> 00:19:24,600
Okay.

279
00:19:24,600 --> 00:19:27,800
So they have tools for annotating images.

280
00:19:27,800 --> 00:19:33,120
It's relatively clunky, but it worked then and it works now, um, for some cases like

281
00:19:33,120 --> 00:19:34,880
we, um, we still use it.

282
00:19:34,880 --> 00:19:35,880
Mm-hmm.

283
00:19:35,880 --> 00:19:37,120
But yeah, I generated it.

284
00:19:37,120 --> 00:19:38,120
It worked.

285
00:19:38,120 --> 00:19:41,880
You threw out a few different numbers when you said a couple of images.

286
00:19:41,880 --> 00:19:42,880
Yeah.

287
00:19:42,880 --> 00:19:46,560
Is this a couple of images with 50 things that need to be annotated or a couple of images

288
00:19:46,560 --> 00:19:48,480
with a million things that need to be annotated?

289
00:19:48,480 --> 00:19:52,720
A couple of images with a couple with about a hundred-ish things that need to be annotated.

290
00:19:52,720 --> 00:19:53,720
Okay.

291
00:19:53,720 --> 00:19:57,840
And we paid, um, for the plus computation biology paper and even now, we paid very close

292
00:19:57,840 --> 00:20:01,080
attention to data augmentation.

293
00:20:01,080 --> 00:20:06,040
We paid a lot of attention to normalizing images before they go into like the neural network.

294
00:20:06,040 --> 00:20:09,320
So you can account for like variations in image acquisition.

295
00:20:09,320 --> 00:20:14,480
And we paid a lot of a tent, we paid like a good amount of attention to post-processing

296
00:20:14,480 --> 00:20:17,800
what happens to like the output of the neural networks afterwards.

297
00:20:17,800 --> 00:20:21,640
And can you still leverage some of the, some of the classical tools that you use in computer

298
00:20:21,640 --> 00:20:27,760
vision, like the watershed transform, like active contours or whatnot to sort of refine

299
00:20:27,760 --> 00:20:31,520
like what the neural network produces to produce something that you can like actually like

300
00:20:31,520 --> 00:20:34,360
use on like real data.

301
00:20:34,360 --> 00:20:38,040
And we found that by like paying attention like those things, you know, you don't have to

302
00:20:38,040 --> 00:20:41,880
use like the latest and greatest, you know, stuff in the neural networks in the deep learning

303
00:20:41,880 --> 00:20:48,240
space, relatively simple things with those things like paid attention to work like really

304
00:20:48,240 --> 00:20:49,240
well.

305
00:20:49,240 --> 00:20:52,480
And did you iterate to paying attention to all those things?

306
00:20:52,480 --> 00:20:55,640
Or did you just kind of start there and that's what work?

307
00:20:55,640 --> 00:21:02,880
We were always iterating people create like really, really great pieces of work all the

308
00:21:02,880 --> 00:21:03,880
time.

309
00:21:03,880 --> 00:21:07,880
And like there's just an innately curious part of me wondering like how well will that

310
00:21:07,880 --> 00:21:12,800
stuff work on the data sets that we've generated and the other people have generated sort

311
00:21:12,800 --> 00:21:14,400
of like in this space.

312
00:21:14,400 --> 00:21:20,160
And now that I'm running my own group, I have people to sort of like help explore like

313
00:21:20,160 --> 00:21:21,160
that space.

314
00:21:21,160 --> 00:21:27,880
I will say that we're primarily interested in like the scientific questions that we can

315
00:21:27,880 --> 00:21:30,440
answer with these tools.

316
00:21:30,440 --> 00:21:34,040
And that curiosity kind of like drives our interest in the deep learning space.

317
00:21:34,040 --> 00:21:38,080
We're well, I find it like interesting and mathematically elegant.

318
00:21:38,080 --> 00:21:40,880
We're not necessarily like deep learning for like deep learning sake.

319
00:21:40,880 --> 00:21:44,200
It's more like deep learning, you know, because like we have like real problems that

320
00:21:44,200 --> 00:21:46,600
we're trying to, um, that we're trying to address.

321
00:21:46,600 --> 00:21:47,600
That's the way it should be.

322
00:21:47,600 --> 00:21:48,600
I think so.

323
00:21:48,600 --> 00:21:49,600
Yeah.

324
00:21:49,600 --> 00:21:51,600
I think so.

325
00:21:51,600 --> 00:21:55,480
So you mentioned a few kind of tricks and techniques that you paid attention to, data

326
00:21:55,480 --> 00:21:56,480
augmentation.

327
00:21:56,480 --> 00:21:57,480
Yeah.

328
00:21:57,480 --> 00:22:00,320
How did you go about that or what were some of the highlights or anything stand out in

329
00:22:00,320 --> 00:22:04,440
terms of, you know, doing this really made it work for us.

330
00:22:04,440 --> 00:22:08,560
Was it standard kind of, you know, rotating things around and changing brightness levels

331
00:22:08,560 --> 00:22:09,840
and that kind of thing?

332
00:22:09,840 --> 00:22:13,600
I would say like the things that like, the things that like we found that worked, um,

333
00:22:13,600 --> 00:22:19,160
is just like being like trying to be a systematic, um, as we could with the things that we tried.

334
00:22:19,160 --> 00:22:23,080
And so like we have tried, um, so I can like just go through the list of things that we

335
00:22:23,080 --> 00:22:26,680
tried for our plus computational, um, biology paper.

336
00:22:26,680 --> 00:22:31,480
The usual like data augmentation things that we've tried, um, so image rotations, image

337
00:22:31,480 --> 00:22:37,480
flipping, those things like really matter, um, I would say there were some things that

338
00:22:37,480 --> 00:22:41,240
we tried that didn't really make so much of an impact.

339
00:22:41,240 --> 00:22:47,160
So image sharing and so sharing is like another like another operation that you can, um, that

340
00:22:47,160 --> 00:22:48,280
you can do to augment your data.

341
00:22:48,280 --> 00:22:51,720
We found that that didn't really like impact like performance of like the neural networks

342
00:22:51,720 --> 00:22:52,720
that we got.

343
00:22:52,720 --> 00:22:59,520
Um, I would say paying attention to how you're doing the training, um, made an impact.

344
00:22:59,520 --> 00:23:06,720
And so we found that for us, the simpler, you know, simpler stochastic gradient is sent

345
00:23:06,720 --> 00:23:14,760
with, with Nesterov momentum that ended up being, um, better, roughly speaking than, you

346
00:23:14,760 --> 00:23:17,440
know, RMS prop, Adam, et cetera, et cetera.

347
00:23:17,440 --> 00:23:21,320
But that was also conditioned on like what networks that we were using and like what features

348
00:23:21,320 --> 00:23:23,000
we had within those networks.

349
00:23:23,000 --> 00:23:29,280
And so bash normalization, um, sort of influences like which one is going to work better, um,

350
00:23:29,280 --> 00:23:33,160
whether using like dropout or using like bash normalization, like with dropout, we tend

351
00:23:33,160 --> 00:23:34,160
not to do that.

352
00:23:34,160 --> 00:23:39,320
To not to use dropout or to not to use bash normalization, then we generally speaking,

353
00:23:39,320 --> 00:23:40,800
don't use dropout.

354
00:23:40,800 --> 00:23:45,640
And I would say like I, we generally speaking, don't use dropout that much.

355
00:23:45,640 --> 00:23:49,360
Uh, I don't know how general this quote unquote finding is.

356
00:23:49,360 --> 00:23:56,120
But one thing that we noticed was if you look at the, let me just take like a brief second

357
00:23:56,120 --> 00:23:59,480
to sort of like stay like how we do like our neural network training.

358
00:23:59,480 --> 00:24:00,960
And so we sort of repose.

359
00:24:00,960 --> 00:24:01,960
Yeah.

360
00:24:01,960 --> 00:24:05,560
So basically like the task that we're trying to do is instant segmentation.

361
00:24:05,560 --> 00:24:09,720
And so for your listeners who are sort of like familiar with this space, it's the same

362
00:24:09,720 --> 00:24:13,200
task that networks like mask our CNN, um, solve.

363
00:24:13,200 --> 00:24:16,520
We started working on this before mask our CNN came out.

364
00:24:16,520 --> 00:24:19,280
And so there's sort of like, you know, different ways of like slicing that up.

365
00:24:19,280 --> 00:24:25,160
Well, the way that, uh, we did it is, is a framework that Dan Cirisson posed in a paper,

366
00:24:25,160 --> 00:24:29,120
um, many years ago, which is recasting the instant segmentation problem as an image

367
00:24:29,120 --> 00:24:30,920
classification problem.

368
00:24:30,920 --> 00:24:35,720
And trying to classify like every pixel as either being, um, in interior, being inside

369
00:24:35,720 --> 00:24:39,520
a object, in this case, inside a single cell, being at the boundary of a single cell,

370
00:24:39,520 --> 00:24:41,800
or being as part of the background.

371
00:24:41,800 --> 00:24:46,560
And you can looking at things in that way, then, you know, it's basically like an image

372
00:24:46,560 --> 00:24:51,360
classification problem, um, we train using like sample wise, um, training.

373
00:24:51,360 --> 00:24:56,400
And so we basically have these, we have like these maps of what every pixel like should

374
00:24:56,400 --> 00:25:01,240
be, we crop like a small area around like each pixel, feed that into a deep convolutional

375
00:25:01,240 --> 00:25:03,880
neural network to do the classification.

376
00:25:03,880 --> 00:25:08,200
And once it's trained, um, we basically use dilated convolutions and dilated pooling

377
00:25:08,200 --> 00:25:13,640
kernels to run that classification neural network on entire image, um, to generate like

378
00:25:13,640 --> 00:25:14,640
dance predictions.

379
00:25:14,640 --> 00:25:19,440
What are the, what are dilated convolutions and dilated, um, so dilated convolutions

380
00:25:19,440 --> 00:25:26,880
are basically, if you take your convolutional kernel, um, dilations are basically just like

381
00:25:26,880 --> 00:25:33,080
inserting zeros in between, um, the weights of your, of your convolutional kernel.

382
00:25:33,080 --> 00:25:36,480
And there's a similar, um, there's a similar thing, um, in the pooling kernel.

383
00:25:36,480 --> 00:25:37,480
Yeah.

384
00:25:37,480 --> 00:25:42,720
And so it turns out you, you kind of have to do that because, because you're training

385
00:25:42,720 --> 00:25:50,200
this neural network, uh, to do pixel wise classification, once it's trained to do dance prediction, if

386
00:25:50,200 --> 00:25:55,800
you try to feed it in patch by patch, if you have two neighboring pixels, then basically

387
00:25:55,800 --> 00:26:00,280
all of the patches around it are essentially redundant computations.

388
00:26:00,280 --> 00:26:04,680
And so you need, um, you need to use like the dilated, um, kernels to, you know, get like

389
00:26:04,680 --> 00:26:11,160
a fully convolutional, um, execution instead of your, your dilation factor end up being

390
00:26:11,160 --> 00:26:13,160
like a hyper parameter.

391
00:26:13,160 --> 00:26:15,800
No, it's like, it's like mathematically like exact.

392
00:26:15,800 --> 00:26:20,160
If you dilate things in the right way, then it ends up being like mathematically the same

393
00:26:20,160 --> 00:26:24,200
as like running everything like, um, doing like patch wise, um, patch wise predict show.

394
00:26:24,200 --> 00:26:25,200
Okay.

395
00:26:25,200 --> 00:26:28,720
So I got like sidetracked on, on, on, on that.

396
00:26:28,720 --> 00:26:32,240
So like there's a reason, I mean, this happens to me like all the time as I'm getting, as

397
00:26:32,240 --> 00:26:38,040
I'm getting older, um, we were talking about the kind of various ways you've tweaked

398
00:26:38,040 --> 00:26:43,800
the process, and you mentioned that, um, you know, while there are some, uh, newer approaches

399
00:26:43,800 --> 00:26:50,880
to segmentation, you, um, kind of rolled your own based on a paper, yeah, and, uh, ended

400
00:26:50,880 --> 00:26:54,000
up using this dilation as a way to do the pixel by pixel.

401
00:26:54,000 --> 00:26:55,000
Yeah.

402
00:26:55,000 --> 00:26:56,000
Okay.

403
00:26:56,000 --> 00:26:57,000
Yeah.

404
00:26:57,000 --> 00:26:58,840
So, and we're all, I think we're also talking about like what parts did we find, or what

405
00:26:58,840 --> 00:27:00,240
authentic, and what wasn't.

406
00:27:00,240 --> 00:27:01,240
Yeah.

407
00:27:01,240 --> 00:27:04,280
So like the output of this, um, the output of like this approach is you get like probably

408
00:27:04,280 --> 00:27:10,040
maps for what's like inside of it, so what's the cell boundary, and what's like a background.

409
00:27:10,040 --> 00:27:16,400
Um, if you use drop, and so for the post processing, it's actually helpful to have like some notion

410
00:27:16,400 --> 00:27:17,400
of uncertainty.

411
00:27:17,400 --> 00:27:23,720
And so if the neural network is like not sure, it's easier for the post processing for, let's

412
00:27:23,720 --> 00:27:26,520
say, the picture was not, pixel where it's not sure.

413
00:27:26,520 --> 00:27:32,640
It's more useful to have that pixel value be like 0.5 instead of, you know, making a very

414
00:27:32,640 --> 00:27:34,720
certain like false call.

415
00:27:34,720 --> 00:27:40,200
And so what we found, uh, with some of our experiments with dropout is that it was essentially

416
00:27:40,200 --> 00:27:44,440
removing some of the uncertainty that was encoded in these probability maps, and that

417
00:27:44,440 --> 00:27:48,880
made it a little harder to do like the downstream, um, processing.

418
00:27:48,880 --> 00:27:51,400
That's why I, I kind of like shy away from dropout.

419
00:27:51,400 --> 00:27:56,280
Um, I don't know like how general this is, and, but this is like that was like one of

420
00:27:56,280 --> 00:28:02,600
the, um, empirical observations on that, um, you know, I've noted during like my, um,

421
00:28:02,600 --> 00:28:07,440
time-experiment of these things, um, other things that like really help, um, more, like

422
00:28:07,440 --> 00:28:13,880
more training data, the more data that you have to capture more and more edge cases, then

423
00:28:13,880 --> 00:28:18,240
like the better, um, the better the neural networks are going to perform, uh, we found that

424
00:28:18,240 --> 00:28:23,920
roughly if you have like standardized like acquisition, um, conditions, and you collect

425
00:28:23,920 --> 00:28:28,120
your training data on like the same acquisition conditions, then you really only need, I'd

426
00:28:28,120 --> 00:28:32,000
say like on the order of like a few hundred, um, to a thousand cells to get like neural

427
00:28:32,000 --> 00:28:37,440
networks and it'd be like good enough to analyze like subsequent experiments, but the more,

428
00:28:37,440 --> 00:28:40,480
the more training data that you get, then the better that it's going to be.

429
00:28:40,480 --> 00:28:43,560
And I think now we're getting to the point, I mean, we're getting the point we're having

430
00:28:43,560 --> 00:28:46,240
like more and more like data sets that are like publicly available.

431
00:28:46,240 --> 00:28:52,000
So when we published our paper in plus computational biology, we released like all the data sets

432
00:28:52,000 --> 00:28:57,960
we annotated, I think it was on the order of like maybe like, you know, 10 to 15-ish

433
00:28:57,960 --> 00:29:04,000
images, it was quite a bit of work to do, um, but you know, I'm a big fan of if you're

434
00:29:04,000 --> 00:29:08,720
publishing something, you know, make everything they're doing like openly available, um, and

435
00:29:08,720 --> 00:29:13,080
I think like as we're seeing, there is more people are recognizing that the challenges

436
00:29:13,080 --> 00:29:16,360
in this space aren't necessarily like the algorithms anymore.

437
00:29:16,360 --> 00:29:20,080
It's like having access to like good like high quality data, right?

438
00:29:20,080 --> 00:29:25,040
If you give someone, you know, a good data set, you know, there's going to be like some

439
00:29:25,040 --> 00:29:29,120
nerd in some basement, you know, who's like lives on stack overflow is going to be able

440
00:29:29,120 --> 00:29:33,720
to put together like some kind of machine learning solution, uh, using that data set.

441
00:29:33,720 --> 00:29:36,880
It's just are there data sets that are out there.

442
00:29:36,880 --> 00:29:40,680
And I think now we're starting to see like more and more data sets, um, like in this space.

443
00:29:40,680 --> 00:29:42,920
And so the ones that come to mind.

444
00:29:42,920 --> 00:29:49,920
So I think there's, there was one that was released by, um, the Broad Institute, um, it was

445
00:29:49,920 --> 00:29:54,160
like part of like the 2018, um, Kaggle data science competition, uh, for doing nuclear

446
00:29:54,160 --> 00:29:55,160
segmentation.

447
00:29:55,160 --> 00:29:59,320
Um, that was one of the things that we tackled in our plus computation biology paper.

448
00:29:59,320 --> 00:30:05,400
But more data sets on more data types and you'll get better on better neural networks.

449
00:30:05,400 --> 00:30:12,240
And as we saw today, um, you know, figure eight is releasing annotated data on H&E pathology

450
00:30:12,240 --> 00:30:16,800
images, um, for doing nuclear segmentation on a variety of like different tissue types.

451
00:30:16,800 --> 00:30:22,800
And so I think like the more and more data sets that we have that cover more and more,

452
00:30:22,800 --> 00:30:26,840
um, use cases, then like the closer we're going to get to actually having like general tools

453
00:30:26,840 --> 00:30:31,880
that, you know, people be able to use like off the shelf, um, which I think is going to

454
00:30:31,880 --> 00:30:33,440
be like really, really awesome.

455
00:30:33,440 --> 00:30:34,440
So yeah.

456
00:30:34,440 --> 00:30:40,000
Uh, in my view, like I view this space as the challenges, I don't want to like downplay

457
00:30:40,000 --> 00:30:42,880
how hard it is to like it started like in machine learning and deep learning.

458
00:30:42,880 --> 00:30:46,200
It's like, you know, of course, there's like, there's like challenges and issues, but

459
00:30:46,200 --> 00:30:52,240
it's really, I'd say like data sets, um, are like one big challenge and deployment, um,

460
00:30:52,240 --> 00:30:56,200
challenges and other challenge, um, once you have like a neural network, um, that's trained

461
00:30:56,200 --> 00:30:57,600
to like do your task.

462
00:30:57,600 --> 00:31:02,040
How do you give it to like other people so they can use it for their problems and like

463
00:31:02,040 --> 00:31:03,040
their data?

464
00:31:03,040 --> 00:31:04,040
Yeah.

465
00:31:04,040 --> 00:31:05,040
Yeah.

466
00:31:05,040 --> 00:31:08,320
And have you, have you started to tackle that one at all or one thing that I talked about

467
00:31:08,320 --> 00:31:14,000
at my talk, um, yesterday was, you know, sort of like what are, what are we working on

468
00:31:14,000 --> 00:31:18,280
on the deep learning space, um, one is, yeah.

469
00:31:18,280 --> 00:31:21,840
So one is just deployment and experiments, experiments that we do in our lab.

470
00:31:21,840 --> 00:31:25,000
That requires, you know, a little bit more training data because like the experience that

471
00:31:25,000 --> 00:31:31,280
we like to do or sort of like, like large scale imaging and then also deploying, um, these

472
00:31:31,280 --> 00:31:36,080
neural networks so you can do like segmentation in, in tissues, right?

473
00:31:36,080 --> 00:31:41,840
And so there's sort of like the, you know, well controls, you know, cell culture type experiments,

474
00:31:41,840 --> 00:31:46,200
but then there's a messier world of, you know, imaging and, you know, real life tissues

475
00:31:46,200 --> 00:31:49,720
that, you know, pathologists, um, pathologists do.

476
00:31:49,720 --> 00:31:54,560
And so these require different scales of training data and the things that you'd really like

477
00:31:54,560 --> 00:32:00,560
to do in these spaces are either like very complicated tissues or having three dimensional

478
00:32:00,560 --> 00:32:01,560
data sets.

479
00:32:01,560 --> 00:32:05,800
So being able to do two dimensions in space plus time or being able to do like three

480
00:32:05,800 --> 00:32:10,800
dimensional, um, spatial, um, three dimensions in space, say like you're using like a confocal

481
00:32:10,800 --> 00:32:15,480
microscopy, um, setup or, um, another like more sophisticated instrument.

482
00:32:15,480 --> 00:32:18,600
And so that requires a different scale of training data because instead of annotating

483
00:32:18,600 --> 00:32:22,520
a couple of frames to be able to look at the objects, you have to be able to like annotate

484
00:32:22,520 --> 00:32:24,240
like entire movies.

485
00:32:24,240 --> 00:32:28,760
And that's what I've been working with, um, people that figure it with, um, is, how do

486
00:32:28,760 --> 00:32:32,040
you train people on the crowd to like do these, um, sorts of tasks?

487
00:32:32,040 --> 00:32:34,560
Oh, I think we're just, I think they ended up talking.

488
00:32:34,560 --> 00:32:38,160
I think we showed, you know, sort of like some of like the stuff that's off the press.

489
00:32:38,160 --> 00:32:43,000
I think we're just now getting people, um, in the crowd able to annotate the 3D data

490
00:32:43,000 --> 00:32:49,200
sets, um, Robert Monroe show being able to, you know, do like object tracking, um, in

491
00:32:49,200 --> 00:32:50,200
the crowd.

492
00:32:50,200 --> 00:32:54,280
I'm super, super excited about that because I think that's one of the, you know, one of

493
00:32:54,280 --> 00:32:59,240
the big, um, challenges, um, in lifestyle imaging and, you know, to do that, what you need

494
00:32:59,240 --> 00:33:03,040
are, you know, nice, um, annotated, um, annotated data sets.

495
00:33:03,040 --> 00:33:08,240
And so we've been working on annotating on issues with like crowdsourcing, like, um,

496
00:33:08,240 --> 00:33:13,280
data set annotation, we're also, um, thinking about deployment as well.

497
00:33:13,280 --> 00:33:17,080
And so there are a lot of things to think about there.

498
00:33:17,080 --> 00:33:23,520
Um, so yeah, I would say there's a difference between what I like to call like academic

499
00:33:23,520 --> 00:33:30,720
software, which is, you know, someone has like a cool idea they put together a nice set

500
00:33:30,720 --> 00:33:37,800
of scripts, um, in Python or MATLAB or R. And for, well, for MATLAB and R, they're relatively

501
00:33:37,800 --> 00:33:42,840
like well-contained environments and you can give someone a dot M file, um, or an R file

502
00:33:42,840 --> 00:33:45,400
and it'll kind of work on another MATLAB installation.

503
00:33:45,400 --> 00:33:47,360
But Python's a different beast.

504
00:33:47,360 --> 00:33:51,040
Everybody's got like their own version, their own limitation and for all of the different

505
00:33:51,040 --> 00:33:55,120
stacks on the library, does a different libraries.

506
00:33:55,120 --> 00:33:57,040
And so that's a, that's a problem, right?

507
00:33:57,040 --> 00:34:01,920
Is that I can't give somebody like a link to my GitHub and say like, good luck.

508
00:34:01,920 --> 00:34:07,040
Um, you have to, there's a lot more, um, there's a lot more work that has to be done.

509
00:34:07,040 --> 00:34:11,600
So, you know, it's basically, you know, doing your due diligence, looking at like the best

510
00:34:11,600 --> 00:34:15,920
tools that are out there for solving these problems, um, we have switched to doing like

511
00:34:15,920 --> 00:34:18,960
our development inside of NVIDIA Docker now.

512
00:34:18,960 --> 00:34:22,800
And we're looking as, um, to use that for, you know, deployment.

513
00:34:22,800 --> 00:34:26,200
So like we have collaborators who use some of those scripts that we generate, um, and

514
00:34:26,200 --> 00:34:31,400
now instead of, you know, giving them like, hey, here's like the Python file, um, you

515
00:34:31,400 --> 00:34:35,680
know, hey, here's like a Docker container, um, that has like all of the packages, um,

516
00:34:35,680 --> 00:34:36,680
installed.

517
00:34:36,680 --> 00:34:42,280
And, you know, there's layers of work to, uh, that has to be done like on top of that.

518
00:34:42,280 --> 00:34:46,440
Ideally, you know, you'd have like your Docker container, um, up and running, you'd have

519
00:34:46,440 --> 00:34:48,280
stuff like wrap within like a web framework.

520
00:34:48,280 --> 00:34:52,120
You have, you know, that backend talking to like a front end with some semblance of like

521
00:34:52,120 --> 00:34:56,640
user interface, um, and now that I have like a team of people thinking of these sorts

522
00:34:56,640 --> 00:35:00,840
of issues, um, you know, we're thinking of, um, we're thinking of like just implementing

523
00:35:00,840 --> 00:35:04,920
on some of these things to like have it easier for people like within the lab, um, people

524
00:35:04,920 --> 00:35:10,320
within the labs that we collaborate with and also like the people who, um, you know, download

525
00:35:10,320 --> 00:35:13,440
our stuff and use it, um, make it easier for them to use too.

526
00:35:13,440 --> 00:35:14,440
Awesome.

527
00:35:14,440 --> 00:35:15,440
Awesome.

528
00:35:15,440 --> 00:35:16,440
Yeah.

529
00:35:16,440 --> 00:35:17,440
Uh, well, David, this has been really, really interesting.

530
00:35:17,440 --> 00:35:19,760
Thanks so much for taking the time to chat with us.

531
00:35:19,760 --> 00:35:21,960
Um, it's been absolute pleasure.

532
00:35:21,960 --> 00:35:23,040
Um, thank you very much.

533
00:35:23,040 --> 00:35:27,440
Uh, I should like, I mean, I would like to think like a few people, um, if I'm like a

534
00:35:27,440 --> 00:35:28,440
lot, go right ahead.

535
00:35:28,440 --> 00:35:29,440
Yeah.

536
00:35:29,440 --> 00:35:33,680
Uh, like I think the graduate students, um, who have, um, been worth rotating with me,

537
00:35:33,680 --> 00:35:35,880
like the last, um, this quarter.

538
00:35:35,880 --> 00:35:41,360
So Dylan Bannon and Nadia Vilevich, uh, I've got to thank, um, people I've worked with

539
00:35:41,360 --> 00:35:42,360
at Stanford.

540
00:35:42,360 --> 00:35:45,160
And so I had the privilege of working with Nicholas Quatt.

541
00:35:45,160 --> 00:35:48,320
She's a very talented undergrad who did a lot of the experiments I talked about yesterday.

542
00:35:48,320 --> 00:35:52,160
Um, my postdoctoral advisor, um, Marcus covert, um, he was great.

543
00:35:52,160 --> 00:35:55,560
And it was his lab that, um, sort of gave me like the intellectual freedom to start exploring

544
00:35:55,560 --> 00:36:00,760
like these, um, sets of issues, um, you know, looking at like AI and the interface with

545
00:36:00,760 --> 00:36:07,160
uh, the biological sciences, um, I had a great, great ongoing collaboration with, uh, Michael

546
00:36:07,160 --> 00:36:09,600
Angelo, who's a professor at the college department at Stanford.

547
00:36:09,600 --> 00:36:15,040
Um, he's done amazing, um, amazing stuff, um, on digital pathology and developing like

548
00:36:15,040 --> 00:36:17,640
the next platform of digital pathology instruments.

549
00:36:17,640 --> 00:36:21,480
I will let you look at literally, you know, dozens of different biological markers within

550
00:36:21,480 --> 00:36:22,480
tissues.

551
00:36:22,480 --> 00:36:27,480
And he's had an amazing postdoctoral fellow, um, Leot Karen, um, who I worked with as

552
00:36:27,480 --> 00:36:31,080
well, uh, unfortunately, you didn't get to stick around for the talk yesterday, um, but

553
00:36:31,080 --> 00:36:33,400
I talked about some of the work I was doing with them.

554
00:36:33,400 --> 00:36:39,920
And also like to think, um, Casey Wong and long, hi, um, I do, I do work with their labs,

555
00:36:39,920 --> 00:36:40,920
um, as well.

556
00:36:40,920 --> 00:36:41,920
Awesome.

557
00:36:41,920 --> 00:36:42,920
So yeah.

558
00:36:42,920 --> 00:36:43,920
Awesome.

559
00:36:43,920 --> 00:36:44,920
And of course, the folks that figure eight for the support of your work and having us

560
00:36:44,920 --> 00:36:52,120
here at the podcast, folks that figure eight, um, Andy, Justin, Robert, um, everybody's

561
00:36:52,120 --> 00:36:54,800
been like, everybody's been amazing.

562
00:36:54,800 --> 00:37:02,560
Their openness has, I think is, I was like, it's a model that people in academia should

563
00:37:02,560 --> 00:37:07,840
pay attention to as far as like collaborations between like academia and industry, like the

564
00:37:07,840 --> 00:37:11,040
things that figure eight's allowing my lab to do.

565
00:37:11,040 --> 00:37:14,320
And the resources they've given us, like the work that, you know, we're doing and we're

566
00:37:14,320 --> 00:37:17,280
going to do would not be possible like without them.

567
00:37:17,280 --> 00:37:18,280
Fantastic.

568
00:37:18,280 --> 00:37:19,280
Yeah.

569
00:37:19,280 --> 00:37:25,360
Other funders, uh, NH, or welcome fund, uh, Paul Allen Family Foundation, um, they funded,

570
00:37:25,360 --> 00:37:30,760
um, a large, um, large amount of work and also the division of biology and engineering

571
00:37:30,760 --> 00:37:34,720
at Caltech for posting my laboratory and also, um, funding us as well.

572
00:37:34,720 --> 00:37:35,720
Awesome.

573
00:37:35,720 --> 00:37:36,720
Well, thanks so much, David.

574
00:37:36,720 --> 00:37:37,720
Thank you.

575
00:37:37,720 --> 00:37:40,960
All right, everyone.

576
00:37:40,960 --> 00:37:49,240
That's our show for today for more information on David or any of the topics covered

577
00:37:49,240 --> 00:37:56,320
in this episode, head on over to twimlai.com slash talk slash one forty one.

578
00:37:56,320 --> 00:37:59,520
Thanks again to figure eight for their sponsorship of this episode.

579
00:37:59,520 --> 00:38:06,920
To follow along with the train AI series, visit twimlai.com slash train AI 2018.

580
00:38:06,920 --> 00:38:12,720
And last, but not least, go ahead and show us some love for our second anniversary and

581
00:38:12,720 --> 00:38:19,240
share how the podcast has been helpful to you over at twimlai.com slash two A V, the

582
00:38:19,240 --> 00:38:21,440
number two A V.

583
00:38:21,440 --> 00:38:51,400
Thanks so much for listening and catch you next time.

