1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,440
I'm your host Sam Charrington.

4
00:00:32,440 --> 00:00:37,840
Two weeks ago we celebrated the show's third birthday and a major listenership milestone.

5
00:00:37,840 --> 00:00:42,840
And last week we kicked off the second volume of our listener favorite AI platform series,

6
00:00:42,840 --> 00:00:47,200
sharing more stories of teams working to scale and industrialize data science and machine

7
00:00:47,200 --> 00:00:49,800
learning at their companies.

8
00:00:49,800 --> 00:00:54,160
We've been teasing that there's more to come and today I am super excited to announce

9
00:00:54,160 --> 00:00:59,360
the launch of our inaugural conference, Twimblecon AI platforms.

10
00:00:59,360 --> 00:01:04,600
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices

11
00:01:04,600 --> 00:01:09,400
necessary to scale the delivery of machine learning and AI in the enterprise.

12
00:01:09,400 --> 00:01:14,840
Now you know Twimble for bringing you dynamic practical conversations via the podcast and

13
00:01:14,840 --> 00:01:18,640
we're creating our Twimblecon events to build on that tradition.

14
00:01:18,640 --> 00:01:24,240
The event will feature two full days of community oriented discussions, live podcast interviews

15
00:01:24,240 --> 00:01:30,440
and practical presentations by great presenters sharing concrete examples from their own experiences.

16
00:01:30,440 --> 00:01:34,640
By creating a space where data science, machine learning, platform engineering and ML ops

17
00:01:34,640 --> 00:01:39,640
practitioners and leaders can share, learn and connect, the event aspires to help see

18
00:01:39,640 --> 00:01:44,800
the development of an informed and sustainable community of technologists that is well equipped

19
00:01:44,800 --> 00:01:48,560
to meet the current and future needs of their organizations.

20
00:01:48,560 --> 00:01:52,880
Some of the topics that we plan to cover include overcoming the barriers to getting machine

21
00:01:52,880 --> 00:01:58,120
learning and deep learning models into production, how to apply ML ops and DevOps to your machine

22
00:01:58,120 --> 00:02:03,040
learning workflow, experiences and lessons learned in delivering platform and infrastructure

23
00:02:03,040 --> 00:02:08,440
support for data management, experiment management and model deployment, the latest approaches

24
00:02:08,440 --> 00:02:14,600
platforms and tools for accelerating and scaling the delivery of ML and DL and the enterprise.

25
00:02:14,600 --> 00:02:19,400
Some deployment stories from leading companies like Google, Facebook, Airbnb, as well as

26
00:02:19,400 --> 00:02:25,040
traditional enterprises like Comcast and Shell and organizational and cultural best practices

27
00:02:25,040 --> 00:02:27,000
for success.

28
00:02:27,000 --> 00:02:31,600
The two day event will be held on October 1st and 2nd in San Francisco and I would really

29
00:02:31,600 --> 00:02:33,600
love to meet you there.

30
00:02:33,600 --> 00:02:39,520
EarlyBurt Registration is open today at Twimblecon.com and we're offering the first ten listeners

31
00:02:39,520 --> 00:02:45,720
who register the amazing opportunity to get their ticket for 75% off using the discount

32
00:02:45,720 --> 00:02:47,320
code TwimbleFirst.

33
00:02:47,320 --> 00:02:55,320
Again, the conference site is Twimblecon.com and the code is TwimbleFirst and now on to

34
00:02:55,320 --> 00:02:56,320
the show.

35
00:02:56,320 --> 00:02:57,320
All right, everyone.

36
00:02:57,320 --> 00:02:59,320
I am on the line with Karen Levy.

37
00:02:59,320 --> 00:03:05,000
Karen is an assistant professor in the Department of Information Science at Cornell University.

38
00:03:05,000 --> 00:03:07,560
Karen, welcome to this week in machine learning and AI.

39
00:03:07,560 --> 00:03:08,560
Thanks.

40
00:03:08,560 --> 00:03:09,480
Nice to join you.

41
00:03:09,480 --> 00:03:18,120
So, Karen, you have a background in both law and sociology and you're currently teaching

42
00:03:18,120 --> 00:03:20,240
in an information science department.

43
00:03:20,240 --> 00:03:21,960
How did that come about?

44
00:03:21,960 --> 00:03:24,800
Yeah, so I started out.

45
00:03:24,800 --> 00:03:27,720
I went to law school and then I worked in a federal court for a couple of years after

46
00:03:27,720 --> 00:03:32,240
that and then kind of in the course of that, I got interested in sort of the social causes

47
00:03:32,240 --> 00:03:36,160
of the legal problems that came through the court on a fairly regular basis.

48
00:03:36,160 --> 00:03:41,280
So I went and got a PhD in sociology and when I was in graduate school, I got really

49
00:03:41,280 --> 00:03:46,240
interested in technology as kind of a route towards social control, right?

50
00:03:46,240 --> 00:03:52,760
Like a different way that people's life chances are impacted besides the law.

51
00:03:52,760 --> 00:03:58,040
So I started to look more at kind of intersections between the law and technology and how people's

52
00:03:58,040 --> 00:04:03,760
decisions are made and their social interactions are structured as a result of the technologies

53
00:04:03,760 --> 00:04:04,760
around them.

54
00:04:04,760 --> 00:04:09,720
It landed me in an information science department and yeah, that's where I am now.

55
00:04:09,720 --> 00:04:18,240
A lot of your focus now is on the social aspects of surveillance and monitoring.

56
00:04:18,240 --> 00:04:24,440
Maybe tell us a little bit about kind of your, the types of research that you do around

57
00:04:24,440 --> 00:04:25,440
that area.

58
00:04:25,440 --> 00:04:26,440
Sure.

59
00:04:26,440 --> 00:04:28,200
So yeah, I'm really interested in surveillance.

60
00:04:28,200 --> 00:04:32,560
So when I, my kind of framework for thinking about technology is that I'm really interested

61
00:04:32,560 --> 00:04:38,240
in the way we use technologies to enforce rules and sometimes those rules are like state

62
00:04:38,240 --> 00:04:43,760
or federal laws and sometimes they are organizational rules or even kind of like expectations and

63
00:04:43,760 --> 00:04:48,000
norms of behavior within intimate relationships like families or friendships.

64
00:04:48,000 --> 00:04:52,760
But I'm really interested in how we use technology to sort of enforce all of those expectations

65
00:04:52,760 --> 00:04:55,640
and one of the ways that we often do that is through surveillance, right, is through

66
00:04:55,640 --> 00:05:00,760
creating a record of the things that people do as it kind of means by which to tell whether

67
00:05:00,760 --> 00:05:02,880
or not they're following the rules or not.

68
00:05:02,880 --> 00:05:06,800
And so I look at that in a variety of different contexts, but the kind of the two big contexts

69
00:05:06,800 --> 00:05:11,360
where I've spent the most time and energy and my research are workplaces.

70
00:05:11,360 --> 00:05:17,040
So I've done a fair amount of work on surveillance in the workplace and intimate context.

71
00:05:17,040 --> 00:05:22,240
So I do a lot of work on families and sexual relationships, intimate relationships, trying

72
00:05:22,240 --> 00:05:26,960
to understand the role that technology plays in how people relate to one another there.

73
00:05:26,960 --> 00:05:31,920
You know, I think one of the things that I'm realizing as we're starting to talk about

74
00:05:31,920 --> 00:05:37,240
this topic is that I find it scary, like the thought of jumping into this conversation

75
00:05:37,240 --> 00:05:44,400
because while I tend to be very optimistic about technology, particularly AI, the whole

76
00:05:44,400 --> 00:05:49,080
surveillance thing kind of freaks me out sometimes.

77
00:05:49,080 --> 00:05:52,840
You're laughing, are you familiar with that kind of take on it?

78
00:05:52,840 --> 00:05:57,000
Well, I mean, I like the idea that within the first five minutes of us starting to talk

79
00:05:57,000 --> 00:05:59,000
you get scared.

80
00:05:59,000 --> 00:06:03,520
I don't know what that says about the research, but yeah, I mean, so there is kind of, certainly,

81
00:06:03,520 --> 00:06:07,400
you could characterize my interest in technology as being sort of about the dark side, right?

82
00:06:07,400 --> 00:06:12,640
About kind of maybe the nefarious ways people use technology or the unintended consequences

83
00:06:12,640 --> 00:06:20,120
that the use of these technologies might have on particularly vulnerable groups of people.

84
00:06:20,120 --> 00:06:24,520
I'm definitely interested in that, but I don't kind of approach it as like a technology

85
00:06:24,520 --> 00:06:26,480
naysayer.

86
00:06:26,480 --> 00:06:30,000
Like I think we ought to implement, I think, you know, I'm like a, I don't think we ought

87
00:06:30,000 --> 00:06:31,480
to be let-ites, right?

88
00:06:31,480 --> 00:06:34,960
Like I think there's a lot of positive roles that technologies can plan our lives, but

89
00:06:34,960 --> 00:06:40,160
I think doing that in a way that's attentive to, you know, ethical and privacy concerns,

90
00:06:40,160 --> 00:06:44,080
and particularly the role that technologies can have on marginalized groups that are marginalized

91
00:06:44,080 --> 00:06:45,600
in all different sorts of ways, right?

92
00:06:45,600 --> 00:06:50,760
Economically or socially, like it doesn't make any sense to deploy things without paying

93
00:06:50,760 --> 00:06:52,160
attention to that.

94
00:06:52,160 --> 00:06:53,160
Right, right.

95
00:06:53,160 --> 00:07:00,640
So we've got this relationship that's developing between artificial intelligence and data

96
00:07:00,640 --> 00:07:09,720
collection where AI has really been enabled by an array of data collection technologies

97
00:07:09,720 --> 00:07:15,720
and really the increased digitization both in our lives and our work.

98
00:07:15,720 --> 00:07:21,960
And it's also hungry for data and so it drives more data collection.

99
00:07:21,960 --> 00:07:31,960
What are, you know, when you think about just the increase in the amount of data that's

100
00:07:31,960 --> 00:07:37,600
being collected about us, like how do you, do you have like a taxonomy or a framework

101
00:07:37,600 --> 00:07:42,080
for thinking about the different impacts that this has?

102
00:07:42,080 --> 00:07:46,920
So my sense is that maybe, you know, to taxonomize different sorts of data, like I don't

103
00:07:46,920 --> 00:07:53,160
do that in a formal way, but I do think like one of the promises and perils of, you know,

104
00:07:53,160 --> 00:07:58,760
the scale and granularity of the data collection that we do now is how readily repurposed or combine

105
00:07:58,760 --> 00:08:00,320
different data sources are, right?

106
00:08:00,320 --> 00:08:05,040
Like that's part of the beauty of what we can do with data now.

107
00:08:05,040 --> 00:08:09,320
But it also creates risks for people that are perhaps unforeseeable, right?

108
00:08:09,320 --> 00:08:14,520
And so because of that, I think, you know, proceeding with caution makes a lot of sense.

109
00:08:14,520 --> 00:08:19,120
And, you know, having kind of a good feel for how, you know, again, particularly marginalized

110
00:08:19,120 --> 00:08:22,160
communities might be differentially impacted by data collection.

111
00:08:22,160 --> 00:08:27,480
So like an example that I like to use in my teaching is about like census population data,

112
00:08:27,480 --> 00:08:28,480
right?

113
00:08:28,480 --> 00:08:30,760
Which feels like kind of not that interesting maybe, right?

114
00:08:30,760 --> 00:08:36,000
Like censuses are just, they seem like pretty, you know, kind of general high level data collection.

115
00:08:36,000 --> 00:08:42,440
But there are all these examples in the past of how that data gets, you know, reused perhaps

116
00:08:42,440 --> 00:08:46,120
years later for all kinds of different purposes, including like human rights abuses, right?

117
00:08:46,120 --> 00:08:51,360
This is something that my colleague Alvaro Badoya at Georgetown has written a lot about.

118
00:08:51,360 --> 00:08:52,520
So things like that, right?

119
00:08:52,520 --> 00:08:56,920
Like things like sort of the lives that data can have later on, right?

120
00:08:56,920 --> 00:09:00,400
Or in the context of like, I've done a bunch of work with some other folks here at Cornell

121
00:09:00,400 --> 00:09:02,480
about intimate partner violence, right?

122
00:09:02,480 --> 00:09:08,000
Like thinking about how the data you generate on your phone or on your computer, you know,

123
00:09:08,000 --> 00:09:11,480
might be very interesting to a partner, to an abusive partner, right?

124
00:09:11,480 --> 00:09:15,840
Or me actually like reveal quite a bit more about your whereabouts than you might anticipate

125
00:09:15,840 --> 00:09:19,680
or know, you know, those are really important considerations for people in the way people

126
00:09:19,680 --> 00:09:23,840
experience, you know, technology and privacy and security.

127
00:09:23,840 --> 00:09:27,680
And so I think paying attention to those kind of like really like my almost mundane like

128
00:09:27,680 --> 00:09:34,160
day-to-day exchanges we have that involve our data, you know, it's not all about our relationship

129
00:09:34,160 --> 00:09:35,920
with kind of like the big tech companies, right?

130
00:09:35,920 --> 00:09:39,480
It's also about our relationships with one another and how those get mediated through

131
00:09:39,480 --> 00:09:42,560
the data trails that we generate.

132
00:09:42,560 --> 00:09:49,240
As we're thinking about this, do you, how do you structure that for, does your research

133
00:09:49,240 --> 00:09:54,720
aim to like structure for that for us like how we should be thinking about this or are

134
00:09:54,720 --> 00:10:02,160
you more kind of exploring different different stories and how folks are impacted?

135
00:10:02,160 --> 00:10:04,560
Yeah, I mean, I would say a lot.

136
00:10:04,560 --> 00:10:06,680
So certainly there are some design implications, right?

137
00:10:06,680 --> 00:10:09,720
So I think paying attention to examples of things that have happened in the past can

138
00:10:09,720 --> 00:10:14,520
help us to think more critically about how we design systems for the future.

139
00:10:14,520 --> 00:10:18,360
But a lot of it I think is actually what you said is like kind of telling these stories,

140
00:10:18,360 --> 00:10:23,200
right, of how people, how people's lives end up changing or end up, you know, impacted

141
00:10:23,200 --> 00:10:27,640
in these different ways based on their interactions with data intensive systems.

142
00:10:27,640 --> 00:10:32,000
And, and I think honestly a lot of the time like we tend to kind of use the most readily

143
00:10:32,000 --> 00:10:33,000
available tool.

144
00:10:33,000 --> 00:10:37,120
So one kind of theme that runs through a lot of my research is how, you know, oftentimes

145
00:10:37,120 --> 00:10:43,360
we might use a data driven system to address a problem that might actually be better addressed

146
00:10:43,360 --> 00:10:45,360
using some other tool, right?

147
00:10:45,360 --> 00:10:51,920
So like, for example, I've done a bunch of work about truck drivers and trying to understand

148
00:10:51,920 --> 00:10:56,720
how truck drivers are affected by the technologies that are used to track them.

149
00:10:56,720 --> 00:11:00,000
And the reason why we use those technologies is because truck drivers are really tired,

150
00:11:00,000 --> 00:11:01,000
right?

151
00:11:01,000 --> 00:11:02,000
They're overworked and they become unsafe.

152
00:11:02,000 --> 00:11:06,120
And so we use technologies to try and ensure that they're not breaking federal rules about

153
00:11:06,120 --> 00:11:07,960
how much they can drive.

154
00:11:07,960 --> 00:11:08,960
That's fine, right?

155
00:11:08,960 --> 00:11:09,960
That's one approach.

156
00:11:09,960 --> 00:11:13,600
Another approach that I argue would be actually much more effective is just to change like

157
00:11:13,600 --> 00:11:15,400
some of the labor laws around trucking, right?

158
00:11:15,400 --> 00:11:18,680
To change the way we incentivize different types of work, right?

159
00:11:18,680 --> 00:11:23,800
Like structural changes to the organization of the industry, you know, that's not necessarily

160
00:11:23,800 --> 00:11:29,160
a data intensive solution, but you know, it's not the one we've adopted, right?

161
00:11:29,160 --> 00:11:32,760
And so oftentimes I think we tend to use technology as kind of this like band-aid solution

162
00:11:32,760 --> 00:11:37,120
for problems that are inherently economic or social or cultural, but we tend to approach

163
00:11:37,120 --> 00:11:38,600
them as technology problems.

164
00:11:38,600 --> 00:11:42,840
And that often I think is the source of some of these unintended consequences for people.

165
00:11:42,840 --> 00:11:48,640
In the case of the truck drivers, it strikes me that, you know, one thing that that illustrates

166
00:11:48,640 --> 00:11:57,600
is that the technology can be more accessible to the individual groups, for example, trucking

167
00:11:57,600 --> 00:12:03,920
companies or truck drivers, you know, as opposed to, you know, structural changes, some

168
00:12:03,920 --> 00:12:09,440
of those structural regulatory changes that you're describing require, you know, broad

169
00:12:09,440 --> 00:12:15,480
consensus across a large influence group of people and organizations.

170
00:12:15,480 --> 00:12:18,840
Does that play a role in some of those choices?

171
00:12:18,840 --> 00:12:19,840
Yeah.

172
00:12:19,840 --> 00:12:23,680
I mean, I think the accessibility of technology, you know, as you mentioned, right, the

173
00:12:23,680 --> 00:12:28,000
inquiry, like the cheapness with which we can start to gather data and analyze data definitely

174
00:12:28,000 --> 00:12:31,440
lends itself to that being sort of an attractive solution for solving problems.

175
00:12:31,440 --> 00:12:34,800
And I think that can often be like a really positive thing.

176
00:12:34,800 --> 00:12:35,800
But as you say, right?

177
00:12:35,800 --> 00:12:39,600
Like that often might keep us from actually addressing some of the structural problems

178
00:12:39,600 --> 00:12:42,840
that might be more effective at kind of getting at the root cause of a problem.

179
00:12:42,840 --> 00:12:46,440
So in trucking, right, like you can monitor a driver to find out if he's super tired,

180
00:12:46,440 --> 00:12:47,440
right?

181
00:12:47,440 --> 00:12:48,440
Like that's pretty easy to do.

182
00:12:48,440 --> 00:12:51,920
But what's much harder is like making sure he doesn't, he isn't incentivized to get that

183
00:12:51,920 --> 00:12:52,920
tired in the first place.

184
00:12:52,920 --> 00:12:56,720
And you can see why that's politically more difficult, economically more difficult.

185
00:12:56,720 --> 00:12:59,520
But if you don't change that structure, then you're going to end up, you know, with

186
00:12:59,520 --> 00:13:03,360
almost this arms race, right, where people are still going to want to act the way they're

187
00:13:03,360 --> 00:13:04,880
incentivized to act.

188
00:13:04,880 --> 00:13:08,920
And you put technology in their place as sort of a roadblock, but, you know, you haven't

189
00:13:08,920 --> 00:13:11,720
actually addressed the root of the problem.

190
00:13:11,720 --> 00:13:17,200
Kind of that, you know, there's certainly merit in addressing the structural issues.

191
00:13:17,200 --> 00:13:22,960
Is there something wrong with using technology as just another tool to help drive the kind

192
00:13:22,960 --> 00:13:25,040
of behavior that we want?

193
00:13:25,040 --> 00:13:31,160
Or in other words, you know, what are some of the implications on these drivers of the

194
00:13:31,160 --> 00:13:35,800
surveillance technology that's been put in place to monitor their behavior?

195
00:13:35,800 --> 00:13:36,800
Yeah.

196
00:13:36,800 --> 00:13:41,240
So, I mean, technology can certainly be like a really useful tool in the, in the toolkit

197
00:13:41,240 --> 00:13:46,800
for enforcing behavior, you know, for incentivizing behaviors that we think are desirable or safer

198
00:13:46,800 --> 00:13:51,320
or better for society, like it absolutely can have a role.

199
00:13:51,320 --> 00:13:55,720
If we depend on it a lot, I think what we end up doing is making responsible the parties

200
00:13:55,720 --> 00:13:59,320
who often have like the least social and economic power, right?

201
00:13:59,320 --> 00:14:04,360
So essentially, like in trucking, for example, the way that these technologies end up functioning

202
00:14:04,360 --> 00:14:08,480
is to kind of punish drivers for doing like what they almost have no choice but to do, right?

203
00:14:08,480 --> 00:14:13,440
Like essentially, you know, in order to make ends meet, they've kind of violated the law

204
00:14:13,440 --> 00:14:16,880
for decades, like for generations and everybody kind of knows that, right?

205
00:14:16,880 --> 00:14:19,440
That's like a known fact within the industry.

206
00:14:19,440 --> 00:14:22,560
And now they get kind of hit from both sides because they're still incentivized to do all

207
00:14:22,560 --> 00:14:25,440
those things, but now they're also punished for doing them, right?

208
00:14:25,440 --> 00:14:28,240
And they kind of bear the brunt of that.

209
00:14:28,240 --> 00:14:32,640
And so I think the consequence of sometimes using technology as kind of like the first course

210
00:14:32,640 --> 00:14:37,440
of action is that oftentimes it can have those impacts on maybe the person with the least

211
00:14:37,440 --> 00:14:39,440
structural power.

212
00:14:39,440 --> 00:14:42,040
And that like merits a subnormative consideration, right?

213
00:14:42,040 --> 00:14:46,920
We have to ask ourselves, is that the way we want to address social problems is by kind

214
00:14:46,920 --> 00:14:51,880
of, you know, enforcing these rules kind of at the low end of the spectrum?

215
00:14:51,880 --> 00:14:53,400
And sometimes the answer might be yes, right?

216
00:14:53,400 --> 00:14:57,880
Or sometimes it may be that a problem is so pressing or, you know, so consequential

217
00:14:57,880 --> 00:15:01,760
that what that we do want to make sure that people like have to follow the rules.

218
00:15:01,760 --> 00:15:04,840
But we also want to ask like, well, why are they motivated not to follow the rules in

219
00:15:04,840 --> 00:15:05,840
the first place?

220
00:15:05,840 --> 00:15:09,680
And might we do something to actually improve their quality of life or, you know, change

221
00:15:09,680 --> 00:15:13,080
kind of the circumstances such that they're not putting out in that position?

222
00:15:13,080 --> 00:15:14,080
Right.

223
00:15:14,080 --> 00:15:18,720
It sounds like there may be an element of the situation that these drivers are in that

224
00:15:18,720 --> 00:15:21,120
we haven't fully explored.

225
00:15:21,120 --> 00:15:27,680
They're both required by someone not to work too much.

226
00:15:27,680 --> 00:15:32,240
There's some bounds placed on the number of hours, consecutive hours that they work

227
00:15:32,240 --> 00:15:39,720
without rest, but it sounds like there are also incentives on them working more, you

228
00:15:39,720 --> 00:15:47,800
know, beyond those limits, you've referred to incentives just to, you know, earn a living,

229
00:15:47,800 --> 00:15:49,840
put food on the table, all of that kind of thing.

230
00:15:49,840 --> 00:15:54,720
Are there also incentives on the part of their companies to do that?

231
00:15:54,720 --> 00:16:00,480
And maybe, you know, let's explore this relationship between the technology and what the companies

232
00:16:00,480 --> 00:16:02,600
are asking these drivers to do.

233
00:16:02,600 --> 00:16:06,200
Because otherwise, it leaves open questions as to, you know, why the driver is not more

234
00:16:06,200 --> 00:16:09,560
responsible for their individual behavior in this situation.

235
00:16:09,560 --> 00:16:10,560
Yeah.

236
00:16:10,560 --> 00:16:14,400
I mean, like so one parallel that you might, that maybe is, you know, an easy one to wrap

237
00:16:14,400 --> 00:16:18,160
your head around is, is, you know, thinking about like drug laws or something, right?

238
00:16:18,160 --> 00:16:22,200
We can say like, oh, you know, we really, like it's against the law to use these drugs,

239
00:16:22,200 --> 00:16:25,400
right? It's against a lot to do, you know, a variety of things.

240
00:16:25,400 --> 00:16:27,760
And we can enforce those rules, you know, and we often do, right?

241
00:16:27,760 --> 00:16:32,360
Like using technology, but it doesn't change the fact that people use and sell drugs,

242
00:16:32,360 --> 00:16:33,360
right?

243
00:16:33,360 --> 00:16:36,600
And so better enforcement, more consistent enforcement, like may have a role in the way

244
00:16:36,600 --> 00:16:41,200
we choose to fight that problem, but it doesn't actually change the underlying structure

245
00:16:41,200 --> 00:16:43,800
of neighborhoods or the opportunities people have available to them, right?

246
00:16:43,800 --> 00:16:48,560
Like it's a piece of the puzzle that it's easy to rely on, but it's not the whole puzzle.

247
00:16:48,560 --> 00:16:52,120
And I think, you know, what you brought up about kind of like the network of interests,

248
00:16:52,120 --> 00:16:53,120
right?

249
00:16:53,120 --> 00:16:54,960
Like the companies that truckers work for, things like that.

250
00:16:54,960 --> 00:17:01,240
Yeah, that does like definitely play a role in the way we, you know, think about technology

251
00:17:01,240 --> 00:17:02,240
and rules.

252
00:17:02,240 --> 00:17:06,320
So like, what ends up happening in tracking is that companies buy these systems that track

253
00:17:06,320 --> 00:17:07,320
drivers, right?

254
00:17:07,320 --> 00:17:11,600
Because the government actually now requires drivers to be, to have their time tracked

255
00:17:11,600 --> 00:17:12,800
it early.

256
00:17:12,800 --> 00:17:15,560
And then companies say like, well, we bought this system.

257
00:17:15,560 --> 00:17:19,560
But actually now is like pretty cheap or free for us to like also run a bunch of other

258
00:17:19,560 --> 00:17:24,760
analytics on how drivers are doing, meaning like how much fuel, what's their fuel economy,

259
00:17:24,760 --> 00:17:25,760
right?

260
00:17:25,760 --> 00:17:27,880
Like, you know, are they saving enough money for the company?

261
00:17:27,880 --> 00:17:29,160
Are they driving efficiently?

262
00:17:29,160 --> 00:17:30,480
Are they not breaking hard?

263
00:17:30,480 --> 00:17:31,480
Things like that.

264
00:17:31,480 --> 00:17:34,880
And so they end up actually tracking a much wider swath of driver's behavior.

265
00:17:34,880 --> 00:17:39,480
And so it definitely changes the relationship to the company in that it means that drivers

266
00:17:39,480 --> 00:17:44,520
end up actually getting managed in this more granular like real time way, in this way

267
00:17:44,520 --> 00:17:45,880
that they weren't before.

268
00:17:45,880 --> 00:17:49,560
And this is true across a lot of industries, like a lot of low wage workers end up being

269
00:17:49,560 --> 00:17:51,360
supervised really closely.

270
00:17:51,360 --> 00:17:55,680
It's interesting to me in trucking because this is a group of workers who have like sort

271
00:17:55,680 --> 00:17:59,240
of historically gone out of the reason why their truck drivers is because they didn't want

272
00:17:59,240 --> 00:18:00,920
this kind of oversight, right?

273
00:18:00,920 --> 00:18:03,640
Like they wanted to have a little bit of freedom and autonomy.

274
00:18:03,640 --> 00:18:06,600
And so it really becomes like a dignity issue for a lot of them, right?

275
00:18:06,600 --> 00:18:09,880
To feel as though, you know, their trucks are almost like their homes.

276
00:18:09,880 --> 00:18:13,360
They're in their trucks for weeks at a time, maybe.

277
00:18:13,360 --> 00:18:17,120
And now to kind of like have this in like this oversight, real time oversight from the

278
00:18:17,120 --> 00:18:20,800
government or from their employers, like it's quite, it's quite a thing for them.

279
00:18:20,800 --> 00:18:23,920
It's very different from like me being surveilled at work or you being surveilled at work.

280
00:18:23,920 --> 00:18:27,600
Like the nature of the workplace is just different.

281
00:18:27,600 --> 00:18:35,040
And so how does this manifest itself in terms of maybe relationship between the drivers

282
00:18:35,040 --> 00:18:36,040
and their companies?

283
00:18:36,040 --> 00:18:37,720
What have you seen in that regard?

284
00:18:37,720 --> 00:18:38,720
Yeah.

285
00:18:38,720 --> 00:18:39,720
So you know, it's interesting, right?

286
00:18:39,720 --> 00:18:45,640
Like a lot of the drivers that I've talked to have said things like, you know, like what

287
00:18:45,640 --> 00:18:49,560
it shows if you watch me really closely is that like you don't trust me, right?

288
00:18:49,560 --> 00:18:51,000
You don't trust me to do the right thing.

289
00:18:51,000 --> 00:18:52,560
You don't trust me to be safe.

290
00:18:52,560 --> 00:18:55,120
You don't trust me to know my body well enough, right?

291
00:18:55,120 --> 00:18:59,200
Like many, many, many drivers compare this to like feeling like a criminal or feeling

292
00:18:59,200 --> 00:19:00,200
like a child, right?

293
00:19:00,200 --> 00:19:07,560
They see it as like this really kind of demeaning experience to be tracked in real time.

294
00:19:07,560 --> 00:19:13,720
So that obviously, like it's pretty unpopular among most workers to feel like their work

295
00:19:13,720 --> 00:19:20,720
and livelihoods are being supervised so closely, kind of a sort of ironic like Coda to that

296
00:19:20,720 --> 00:19:23,120
is that, you know, they say like, well, you can't know my body.

297
00:19:23,120 --> 00:19:24,800
You can't know how tired I am.

298
00:19:24,800 --> 00:19:28,360
The next wave of technologies actually do sort of are able to infer those things more

299
00:19:28,360 --> 00:19:33,160
closely because a lot of them involve like wearable technologies or cameras that are trained

300
00:19:33,160 --> 00:19:35,480
on a driver's eyelids to monitor fatigue, things like that.

301
00:19:35,480 --> 00:19:39,440
So they actually are quite a bit more invasive and can infer quite a bit more about whether

302
00:19:39,440 --> 00:19:40,440
a driver is tired.

303
00:19:40,440 --> 00:19:42,760
You know, that doesn't necessarily make truckers feel better.

304
00:19:42,760 --> 00:19:47,360
They don't say like, oh, well, now you like are inside my body or have, have information

305
00:19:47,360 --> 00:19:52,400
about what's going on inside my body that doesn't affect their privacy concerns, but you

306
00:19:52,400 --> 00:19:55,000
know, they're kind of, they kind of can't win one way or the other.

307
00:19:55,000 --> 00:19:56,840
How are the drivers reacting to this?

308
00:19:56,840 --> 00:20:01,200
Have they started like organizing against the trucking companies or it, it sounds like

309
00:20:01,200 --> 00:20:06,080
there's multiple issues, well, there's clearly multiple issues here, but you know, there

310
00:20:06,080 --> 00:20:15,280
is the, you know, this technology being put in place to address a specific issue around,

311
00:20:15,280 --> 00:20:21,840
you know, driver fatigue, but then there are kind of the downstream effects of this surveillance

312
00:20:21,840 --> 00:20:27,280
and the data that's being collected about these drivers and how that's being used and

313
00:20:27,280 --> 00:20:32,520
how that is, how that's being used specifically to manage these drivers.

314
00:20:32,520 --> 00:20:37,720
Where is the industry at with regards to addressing this tension that's, that's starting to

315
00:20:37,720 --> 00:20:38,720
be created?

316
00:20:38,720 --> 00:20:43,760
Right, you're right that there's quite a bit of kind of resistance or, you know, workers

317
00:20:43,760 --> 00:20:47,960
kind of viewing these technologies as, as unpopular as invasive and this is true across

318
00:20:47,960 --> 00:20:51,480
lots of different workplace surveillance contexts, but I think you feel it really strongly

319
00:20:51,480 --> 00:20:55,920
in trucking just kind of based on the culture of the occupation.

320
00:20:55,920 --> 00:21:01,160
So, you know, as in most low wage workplaces, you know, workers don't often have the

321
00:21:01,160 --> 00:21:04,800
ultimate say they might not have, you know, as much power as management to make decisions

322
00:21:04,800 --> 00:21:09,400
about whether these things are in place, in trucking, you know, to some extent the stuff

323
00:21:09,400 --> 00:21:12,840
is now federally mandated like as of just a few months ago, so to some extent there's

324
00:21:12,840 --> 00:21:18,080
not much that employers can do, but there are some kind of movements where you see some

325
00:21:18,080 --> 00:21:21,480
companies trying to be responsive to this saying like, yeah, we'll treat you like an adult

326
00:21:21,480 --> 00:21:24,960
or there are certain types of data that we won't collect or that we won't give our

327
00:21:24,960 --> 00:21:27,000
dispatchers access to things like that.

328
00:21:27,000 --> 00:21:32,800
So, you see that a bit, but, you know, it's not a, it's not an industry with heavy unionization,

329
00:21:32,800 --> 00:21:36,680
which you might, you know, unions might be a party you would expect to push back on things

330
00:21:36,680 --> 00:21:37,680
like that.

331
00:21:37,680 --> 00:21:43,160
So, there aren't a lot of like really clear avenues for workers to, you know, resist the

332
00:21:43,160 --> 00:21:44,160
sort of data collection.

333
00:21:44,160 --> 00:21:47,840
It's just kind of becoming a new normal in the industry, and so that's a lot of what

334
00:21:47,840 --> 00:21:52,160
I'm interested in is kind of like, what does that transition end up looking like?

335
00:21:52,160 --> 00:21:55,200
You know, how do you see truckers resist? So like, one of the things that I've studied

336
00:21:55,200 --> 00:21:59,360
fairly extensively is trying to understand all the different ways that workers like try

337
00:21:59,360 --> 00:22:03,960
to thwart these technologies like block data collection or falsify data collection.

338
00:22:03,960 --> 00:22:07,520
You know, otherwise trying kind of maintain a little bit of independence where the technology

339
00:22:07,520 --> 00:22:10,000
is sort of seen as taking that away from them.

340
00:22:10,000 --> 00:22:12,360
And what kinds of things have you seen there?

341
00:22:12,360 --> 00:22:13,360
A whole bunch of stuff.

342
00:22:13,360 --> 00:22:15,640
I mean, actually like a whole bunch of really interesting stuff.

343
00:22:15,640 --> 00:22:21,160
So they, you know, kind of on one end of the spectrum, workers will just like outright

344
00:22:21,160 --> 00:22:24,920
break the thing, right, like we'll destroy it.

345
00:22:24,920 --> 00:22:30,800
Sometimes, you know, and it's kind of purposefully visible way as like sort of a form of protest.

346
00:22:30,800 --> 00:22:33,280
And on the other end of the spectrum, there are things that you can do that are much more

347
00:22:33,280 --> 00:22:34,280
subtle.

348
00:22:34,280 --> 00:22:37,920
Like you can, you know, kind of, I don't want to give all their secrets away, right?

349
00:22:37,920 --> 00:22:42,440
But you can kind of do things to, you know, get extra time out of the systems if you know

350
00:22:42,440 --> 00:22:44,680
what the limitations of the system are, right?

351
00:22:44,680 --> 00:22:48,360
Like you can eek out a little bit of extra driving time and actually kind of interestingly,

352
00:22:48,360 --> 00:22:50,920
you know, you mentioned their relationships with companies.

353
00:22:50,920 --> 00:22:54,000
These actually sometimes kind of want them to do those things or will instruct them about

354
00:22:54,000 --> 00:22:56,240
how to cheat because companies kind of want it both ways, right?

355
00:22:56,240 --> 00:23:00,400
They want to monitor and enforce the rules, but they also like want their stuff moved

356
00:23:00,400 --> 00:23:02,600
at the speed of business.

357
00:23:02,600 --> 00:23:06,680
So sometimes actually truckers actually get kind of told that, you know, you've got to

358
00:23:06,680 --> 00:23:08,440
violate the rules anyway.

359
00:23:08,440 --> 00:23:10,440
And here's how you do it.

360
00:23:10,440 --> 00:23:12,720
So there's a variety of different things they do, right?

361
00:23:12,720 --> 00:23:13,720
That are designed.

362
00:23:13,720 --> 00:23:16,960
And some of them, you know, like actually don't even get them any more driving time, but

363
00:23:16,960 --> 00:23:20,520
they're kind of just about like preserving kind of their identity or their feeling

364
00:23:20,520 --> 00:23:21,520
about autonomy.

365
00:23:21,520 --> 00:23:28,080
So like my favorite example of this is a trucker on YouTube, actually, who shows other

366
00:23:28,080 --> 00:23:35,200
truckers how to kind of get behind the system, the system runs on like a Windows XP backbone

367
00:23:35,200 --> 00:23:40,000
and he shows like here's how you can actually play solitaire on your monitor, right?

368
00:23:40,000 --> 00:23:41,000
Like, which is great.

369
00:23:41,000 --> 00:23:44,200
Like that's not a thing that you're supposed to be able to do, but he like manages to figure

370
00:23:44,200 --> 00:23:45,200
out how you do it.

371
00:23:45,200 --> 00:23:46,880
And I'm sure like that's no longer viable.

372
00:23:46,880 --> 00:23:51,040
Like I'm sure, you know, that's probably no longer a feasible thing to be able to do

373
00:23:51,040 --> 00:23:52,040
with these systems.

374
00:23:52,040 --> 00:23:55,160
But though he does it, you know, that's not making him any extra money, but that's like

375
00:23:55,160 --> 00:23:58,480
a way for him to kind of assert himself.

376
00:23:58,480 --> 00:24:01,400
And I actually find that really interesting, right, that there's a lot of resistance that

377
00:24:01,400 --> 00:24:06,400
takes place that's much more about like maintaining identity and autonomy than it is necessarily

378
00:24:06,400 --> 00:24:11,080
about kind some kind of instrumental like making more money, something like that.

379
00:24:11,080 --> 00:24:20,920
Have these stories taught you anything about how the broader society, you know, will

380
00:24:20,920 --> 00:24:23,920
react to increased surveillance?

381
00:24:23,920 --> 00:24:29,720
Like are there, you know, again, I guess I'm, you know, maybe I'm being overly analytical

382
00:24:29,720 --> 00:24:34,040
in this conversation, but I'm looking again for the taxonomy of, you know, resistance

383
00:24:34,040 --> 00:24:35,560
to surveillance.

384
00:24:35,560 --> 00:24:43,040
Yeah, I mean, a lot of the strategies that like truckers, truckers are the context that

385
00:24:43,040 --> 00:24:44,040
I know the best, right?

386
00:24:44,040 --> 00:24:47,200
Because I've spent the most time studying that industry, but you see some of these, you

387
00:24:47,200 --> 00:24:52,040
know, same techniques across other contexts of work or other just relational contexts,

388
00:24:52,040 --> 00:24:57,960
right, where people try and falsify data streams or, you know, find some way to make the technology

389
00:24:57,960 --> 00:25:03,040
appear to, you know, show one account when in fact, you know, another account might be,

390
00:25:03,040 --> 00:25:06,400
it might be different.

391
00:25:06,400 --> 00:25:09,880
So I think, you know, to some extent, like the, that you see that, I wouldn't say it's

392
00:25:09,880 --> 00:25:13,280
universal, but you see that across a lot of different contexts.

393
00:25:13,280 --> 00:25:18,720
I'm curious where you fall on kind of the surveillance spectrum personally, like do you, do you

394
00:25:18,720 --> 00:25:19,720
use ad blockers?

395
00:25:19,720 --> 00:25:22,520
Do you put tape over your computer camera?

396
00:25:22,520 --> 00:25:25,520
Like, yeah, it's, so it's a good question, right?

397
00:25:25,520 --> 00:25:28,160
So like, I do use ad blockers.

398
00:25:28,160 --> 00:25:31,640
Where this comes up, actually, the most for me is, you know, a lot of my work also looks

399
00:25:31,640 --> 00:25:37,480
at families, looks at how people and families surveil one another or an intimate relationships.

400
00:25:37,480 --> 00:25:41,000
And there, you know, one of the, one of the reasons I got like the most interested in

401
00:25:41,000 --> 00:25:44,480
that context is because I would go to all these privacy and security conferences and

402
00:25:44,480 --> 00:25:48,720
I like where people are, you know, working on, like, say national security issues or consumer

403
00:25:48,720 --> 00:25:49,720
privacy.

404
00:25:49,720 --> 00:25:54,160
Where, you know, these are like pretty staunch advocates of personal privacy.

405
00:25:54,160 --> 00:25:57,320
And then I would talk to them about like, oh, do you do something to like track your

406
00:25:57,320 --> 00:26:00,920
kids whereabouts or like, could you tell me where your spouse is right now?

407
00:26:00,920 --> 00:26:04,400
And a lot of them were like, oh, yeah, like I definitely like read all my kids texts,

408
00:26:04,400 --> 00:26:05,400
you know?

409
00:26:05,400 --> 00:26:09,240
And that struck me as like a really interesting situation, right, that like a lot of,

410
00:26:09,240 --> 00:26:13,560
a lot of the things that were really resistant to, you know, when they come, come at us from

411
00:26:13,560 --> 00:26:16,640
the government or from big multinational corporations, we're quite willing actually

412
00:26:16,640 --> 00:26:19,640
to kind of replicate in our own intimate lives.

413
00:26:19,640 --> 00:26:24,240
So that's something I've gotten really interested in lately is trying to understand kind of how

414
00:26:24,240 --> 00:26:29,560
people, how people become surveyors themselves, right, in their own homes.

415
00:26:29,560 --> 00:26:31,480
So I have a colleague Luke Stark.

416
00:26:31,480 --> 00:26:36,000
We just wrote a paper called the surveillance consumer that tries to look at how people

417
00:26:36,000 --> 00:26:40,320
actually become data collectors themselves in their own homes using a variety of consumer

418
00:26:40,320 --> 00:26:44,440
products, like, you know, nanny cams and things like that and trying to understand how

419
00:26:44,440 --> 00:26:50,560
that too ends up having this disproportionate effect on marginalized people.

420
00:26:50,560 --> 00:26:56,400
And when you say marginalized people, are you referring to specifically within the home

421
00:26:56,400 --> 00:26:57,400
relationship?

422
00:26:57,400 --> 00:27:01,080
Yeah, people, yeah, so I use marginalized fairly broadly, just to mean people who have,

423
00:27:01,080 --> 00:27:03,120
you know, relatively less power in a situation.

424
00:27:03,120 --> 00:27:07,360
So in families, certainly, like, I have a bunch of this work on domestic violence that,

425
00:27:07,360 --> 00:27:12,320
you know, we're, you know, it's often, often, but not always, women and children who suffer

426
00:27:12,320 --> 00:27:17,520
disproportionately from things like, you know, spyware, but in other contexts too.

427
00:27:17,520 --> 00:27:22,480
So like I mentioned, like the nanny cams and nannies in homes often tend to be women of

428
00:27:22,480 --> 00:27:27,400
color from, you know, with lower socioeconomic status, who end up being disproportionately

429
00:27:27,400 --> 00:27:33,120
surveilled in their work, you know, as a result of kind of this consumer, like consumer

430
00:27:33,120 --> 00:27:34,520
led surveillance.

431
00:27:34,520 --> 00:27:39,760
So it ends up being kind of like just another way in which communities of color, women,

432
00:27:39,760 --> 00:27:44,000
you know, people who historically have less power end up kind of suffering the brunt of

433
00:27:44,000 --> 00:27:45,000
data collection.

434
00:27:45,000 --> 00:27:46,000
It's interesting, actually, right?

435
00:27:46,000 --> 00:27:49,800
So my colleague Jonas Lerman has this really lovely piece about how when we think about

436
00:27:49,800 --> 00:27:55,000
surveillance, marginalized people are kind of simultaneously overrepresented and underrepresented.

437
00:27:55,000 --> 00:27:56,000
Right?

438
00:27:56,000 --> 00:27:59,800
So like we have all of these examples of how data sets are biased because, you know, they

439
00:27:59,800 --> 00:28:01,880
only include white men, right?

440
00:28:01,880 --> 00:28:06,720
Or they like don't include this kind of variety of people or images or of texts or whatever

441
00:28:06,720 --> 00:28:09,600
it is and that that can lead to these really biased outcomes.

442
00:28:09,600 --> 00:28:13,080
But then we also have these problems where like there's over representation in the data,

443
00:28:13,080 --> 00:28:14,080
right?

444
00:28:14,080 --> 00:28:18,840
Where like communities of color or poor people are historically like way over, over-surveiled,

445
00:28:18,840 --> 00:28:19,840
right?

446
00:28:19,840 --> 00:28:22,960
Much more data is collected about them for things like, you know, getting public benefits

447
00:28:22,960 --> 00:28:26,400
or in the course of, you know, the education system or criminal justice system.

448
00:28:26,400 --> 00:28:28,920
And so both of those things, I think are true at once, right?

449
00:28:28,920 --> 00:28:33,120
You have like harms both from inclusion and exclusion.

450
00:28:33,120 --> 00:28:37,320
So yeah, it's interesting to me to kind of try and untangle what that looks like in day

451
00:28:37,320 --> 00:28:39,280
to day life.

452
00:28:39,280 --> 00:28:52,000
And you've alluded to, you know, specific examples of unintended consequences in this,

453
00:28:52,000 --> 00:28:59,360
in the application of surveillance, particularly within these, or as it impacts marginalized

454
00:28:59,360 --> 00:29:04,360
communities, can you maybe talk through some specific examples there?

455
00:29:04,360 --> 00:29:09,080
There are a bunch of people who've done a lot of really excellent work in the Serena.

456
00:29:09,080 --> 00:29:13,840
So there's a bunch of work, for example, on risk assessment and criminal justice and

457
00:29:13,840 --> 00:29:20,040
in predictive policing that many of my colleagues have done in which, you know, they demonstrate

458
00:29:20,040 --> 00:29:26,040
that predictive policing systems which are trained on, you know, crime data that's already

459
00:29:26,040 --> 00:29:30,200
over-represents, you know, communities of color, marginalized communities, tends to,

460
00:29:30,200 --> 00:29:35,080
you know, just exacerbate the over-policing of those communities, right, for reasons

461
00:29:35,080 --> 00:29:38,160
that, you know, you just create these feedback loops.

462
00:29:38,160 --> 00:29:43,400
So that's, you know, one example that, you know, it's one example in this particular context.

463
00:29:43,400 --> 00:29:49,040
But we see it in lots of different contexts, right, Virginia U-Banks is a scholar at

464
00:29:49,040 --> 00:29:54,040
SUNY Albany who's written this really wonderful book called Automating Inequality that documents

465
00:29:54,040 --> 00:30:00,640
how local governments implement various, like, allocation algorithms to try and do things

466
00:30:00,640 --> 00:30:06,720
like apportion, housing to homeless people, or to direct public benefits programs, things

467
00:30:06,720 --> 00:30:12,160
like that, and how often, you know, because of various biases in the data or, you know,

468
00:30:12,160 --> 00:30:16,960
kind of the limitations that these systems are operating under, you know, it's not to

469
00:30:16,960 --> 00:30:20,720
say that they necessarily make the problems worse necessarily, but they may have impacts

470
00:30:20,720 --> 00:30:26,400
on communities that are not readily recognized, right, and, and, and, of course, they get

471
00:30:26,400 --> 00:30:31,840
a little bit more, the effects on communities can be a little bit more obscured, right?

472
00:30:31,840 --> 00:30:37,160
So, like, when, when communities adopt risk assessment algorithms or, you know, other

473
00:30:37,160 --> 00:30:41,560
systems like the people who are in positions of power to make decisions may not, you

474
00:30:41,560 --> 00:30:45,680
know, have a great sense for, for the right questions to be asking, or how these things

475
00:30:45,680 --> 00:30:50,320
will end up impacting their communities in the long run, which is not their fault, right,

476
00:30:50,320 --> 00:30:54,960
like, they're really difficult questions even for, you know, technical folks to untangle,

477
00:30:54,960 --> 00:30:58,920
but it definitely is a cause for concern as these, as these tools kind of gain traction

478
00:30:58,920 --> 00:31:00,400
across a bunch of different domains.

479
00:31:00,400 --> 00:31:06,520
Yeah, it strikes me that part of what you're, maybe it's not something that you're saying.

480
00:31:06,520 --> 00:31:14,240
It's a thread that I'm seeing in some of these stories is with regard to technology or

481
00:31:14,240 --> 00:31:21,720
surveillance in particular, you know, there's an element of power corruption here, meaning,

482
00:31:21,720 --> 00:31:31,560
you know, people will adopt surveillance to try to address specific point issues and surveillance

483
00:31:31,560 --> 00:31:39,000
tends to be, you know, almost this beast that consumes other, you know, adjacent freedoms

484
00:31:39,000 --> 00:31:40,000
or something like that.

485
00:31:40,000 --> 00:31:44,520
I'm trying to articulate this without being overly dramatic, but it was easy to get

486
00:31:44,520 --> 00:31:49,200
dystopian really fast, which I think is actually a problem, like, I think we should be

487
00:31:49,200 --> 00:31:54,160
really critical actually of our own, you know, like the critical community is, I think,

488
00:31:54,160 --> 00:31:57,400
a really important one to bring into these conversations, but we ought to also be critical

489
00:31:57,400 --> 00:32:02,080
of, you know, our own techniques and our own assertions, right, that there is potentially

490
00:32:02,080 --> 00:32:05,880
a lot of good to be, to come from some of these applications, but I think you're right,

491
00:32:05,880 --> 00:32:09,440
right, like so this sometimes gets called like surveillance creep, right, but like, once

492
00:32:09,440 --> 00:32:13,680
you started gathering data, you know, it gets easier to just add a little bit more data

493
00:32:13,680 --> 00:32:18,560
to the pile or to use the data for some other purpose, because you've got it now, right,

494
00:32:18,560 --> 00:32:23,520
this is, and there was a few years ago, a big debate in kind of the privacy community

495
00:32:23,520 --> 00:32:28,240
about collection restrictions versus use restrictions, right, so the idea that you tell companies,

496
00:32:28,240 --> 00:32:32,760
for example, like these, these are the types of data you can collect versus like, once

497
00:32:32,760 --> 00:32:37,440
you have data, here's how you can use the data, and the industry kind of was in favor

498
00:32:37,440 --> 00:32:41,640
generally of these use restrictions, right, because as opposed to collection restrictions,

499
00:32:41,640 --> 00:32:45,480
because they, you know, allowed them to amass more data, but, you know, then there was

500
00:32:45,480 --> 00:32:49,240
kind of some subsequent backlash where people say like, well, you know, like once you've

501
00:32:49,240 --> 00:32:53,960
collected it, like the game is kind of, you've given the game away at that point, right,

502
00:32:53,960 --> 00:32:58,040
like because things get used for all kinds of different purposes or rules change, right,

503
00:32:58,040 --> 00:33:01,240
like we have, you know, you can put all these safeguards in place that you want, but,

504
00:33:01,240 --> 00:33:04,520
you know, you don't always know who's going to be in power. I mean, this has come up actually

505
00:33:04,520 --> 00:33:09,880
even recently with, like New York City had this municipal ID program, okay, they rolled

506
00:33:09,880 --> 00:33:14,280
out a couple of years ago for undocumented people, right, where they said like, here's

507
00:33:14,280 --> 00:33:18,440
how you can get a municipal ID card that will allow you to do things like use the library,

508
00:33:18,440 --> 00:33:22,600
right, even though you don't have other documentation. And after doing, I mean, it was by,

509
00:33:22,600 --> 00:33:27,400
you know, meant to be like a positive social program, and it's a program that actually,

510
00:33:27,400 --> 00:33:31,880
you know, sort of accidentally created real risks for these groups of people, because

511
00:33:31,880 --> 00:33:35,880
suddenly now there were concerns that, you know, actually those, those records will be,

512
00:33:35,880 --> 00:33:40,440
would actually be used to target those people, right, for things like Homeland Security

513
00:33:40,440 --> 00:33:44,840
enforcement, right, for ICE enforcement. So, like those are the types of risks, right,

514
00:33:44,840 --> 00:33:49,000
like that's, that's an extreme one, but those are the types of things that, that we get concerned

515
00:33:49,000 --> 00:33:53,960
about, right, is that, you know, terms of service change, privacy policies change, you know,

516
00:33:53,960 --> 00:33:58,440
stuff gets hacked, like there are all kinds of different potential routes for data to be used

517
00:33:58,440 --> 00:34:06,200
in ways that we don't anticipate. You made an interesting point about being critical in our

518
00:34:06,200 --> 00:34:13,800
critiques has, how do we, how do we think about that? How do we, you know, how do we do that?

519
00:34:14,600 --> 00:34:18,040
Yeah, so I think about this a lot, right, because now that I'm in information science,

520
00:34:18,040 --> 00:34:21,720
so, and the information science program I'm in is really closely tied to computer science,

521
00:34:21,720 --> 00:34:24,360
so I sit in the same building with a bunch of computer scientists all the time,

522
00:34:25,240 --> 00:34:28,440
which I really enjoy, because I learn a lot from, you know, understanding their

523
00:34:28,440 --> 00:34:33,960
perspectives on these issues. I think the best thing that that we can do is, like, honestly,

524
00:34:33,960 --> 00:34:40,360
to try and amass more technical understanding. So I think, like, the worst examples of kind of

525
00:34:40,360 --> 00:34:44,280
critique of these systems come from people who don't necessarily, who have a thin understanding

526
00:34:44,280 --> 00:34:48,200
of how the systems actually work in practice, and that means both technically how they work and

527
00:34:48,200 --> 00:34:51,960
what their social effects are, right, so you can't really understand one without the other.

528
00:34:53,480 --> 00:34:57,320
And then the other thing is, is I think actually about engagement, it sounds kind of,

529
00:34:57,320 --> 00:35:01,480
you know, touchy-feely, but I think engagement with practitioners is the other, like, best way

530
00:35:01,480 --> 00:35:07,960
forward to kind of avoid painting like the other side as being the other side or as being like a

531
00:35:07,960 --> 00:35:13,240
straw man, you know, but to understand, like, the actual, you know, motivations and values

532
00:35:13,240 --> 00:35:16,440
behind these decisions, which really requires, like, actually talking to people.

533
00:35:17,560 --> 00:35:21,880
So maybe this is just like a plug for social science, but I really believe that, like,

534
00:35:21,880 --> 00:35:25,960
I really believe that, you know, having kind of more social science expertise at the table,

535
00:35:25,960 --> 00:35:29,320
and having people, having social scientists then take seriously technical expertise,

536
00:35:29,320 --> 00:35:32,840
and really try to understand it is, you know, the best we can do.

537
00:35:32,840 --> 00:35:39,000
Maybe this should be posed as a confirmation or question, but you made it sound like you find

538
00:35:39,000 --> 00:35:51,560
that people in computer science, in technology, are way less concerned about these issues than lay

539
00:35:51,560 --> 00:35:57,400
people. And that's almost my, I almost have the opposite instinct. Oh yeah, I definitely not

540
00:35:57,400 --> 00:36:02,600
what I tried to, that wasn't what I was hoping to portray. Okay, okay. Yeah, yeah, no, I definitely

541
00:36:02,600 --> 00:36:06,440
don't feel that way. I think, like, kind of the easy, so the easy critique that sometimes made

542
00:36:06,440 --> 00:36:12,520
of algorithmic systems from the outside, right, can be that algorithms have this kind of

543
00:36:12,520 --> 00:36:18,360
depersonalizing effect, right, or that, you know, the people designing them, like, don't really

544
00:36:18,360 --> 00:36:23,400
understand, you know, what biases might be embedded in them and don't really care, right? Like that,

545
00:36:23,400 --> 00:36:28,520
I think is like, I'm obviously, I'm creating a straw man by describing that as this process of

546
00:36:28,520 --> 00:36:32,520
straw manning that some other people do, but I think that's like, that's like the worst form of

547
00:36:32,520 --> 00:36:36,600
criticism, right? Because it doesn't acknowledge what you just said, which is that, like, oftentimes,

548
00:36:36,600 --> 00:36:41,640
I think people are like quite deeply concerned about the systems they create and the inequities

549
00:36:41,640 --> 00:36:47,000
that those might exacerbate. So I do, I think it's exactly what you said. I think recognizing those

550
00:36:47,000 --> 00:36:53,640
intentions and actually working with people to try and build the best systems we can, like, that

551
00:36:53,640 --> 00:36:59,080
ought to be our goal, I think, is critical scholars, not necessarily to try and tear down, you know,

552
00:36:59,080 --> 00:37:05,400
efforts to try and improve those things. So you're working on a book on surveillance,

553
00:37:05,400 --> 00:37:12,040
particularly as it applies to the truckers that you've been working with. It sounds like that

554
00:37:12,040 --> 00:37:18,200
book is in progress, but if you were to kind of step back and, you know, or maybe project forward

555
00:37:18,200 --> 00:37:26,280
to the the conclusion of this book, you know, what are the key messages or takeaways for folks that

556
00:37:26,280 --> 00:37:31,960
are, I guess in the case of listeners of this podcast, primarily technologists, primarily,

557
00:37:32,680 --> 00:37:39,480
you know, working to create, refine, perfect some of these technologies. You know, what are the

558
00:37:39,480 --> 00:37:48,920
takeaways for them in terms of, you know, understanding and dealing with the implications that they have

559
00:37:48,920 --> 00:37:56,600
on different communities and particularly marginalized communities? Yeah, so I think, I mean,

560
00:37:56,600 --> 00:38:00,680
I think they're probably two. Like, so the first is something that I kind of alluded to earlier,

561
00:38:00,680 --> 00:38:06,200
which is, you know, technology is a really, is a useful tool for solving a lot of problems,

562
00:38:06,200 --> 00:38:10,520
but not every problem is best solved through technology. And I can understand why particularly

563
00:38:10,520 --> 00:38:15,640
technologists, right, would want to put their tools to use in a particular way. But I think

564
00:38:15,640 --> 00:38:20,200
sometimes asking, like, well, you know, holistically, is this the best approach or is this an

565
00:38:20,200 --> 00:38:25,160
approach that could be combined with other approaches, you know, or different forms of expertise,

566
00:38:25,160 --> 00:38:29,640
you know, how will we evaluate what the real effects of these tools are on communities? Like,

567
00:38:29,640 --> 00:38:36,440
that seems super essential to me. And then the other piece that I think is, you know, like, like,

568
00:38:36,440 --> 00:38:41,320
in the case of these truckers, for example, right, like, there's concern recently about, like,

569
00:38:41,320 --> 00:38:47,080
the potential for massive unemployment among truck drivers based on the development of autonomous

570
00:38:47,080 --> 00:38:52,280
vehicles. Now, I think that that's an overstated concern. But, like, what, I mean, we would be telling

571
00:38:52,280 --> 00:38:56,440
sort of a simple and boring story if we focused only on the numbers, right? If we focused only on

572
00:38:56,440 --> 00:39:02,520
something like job loss, I think a more important and nuanced take on it would be to actually

573
00:39:03,320 --> 00:39:07,160
understand kind of like the dignity and quality of life that results when we put some of these

574
00:39:07,160 --> 00:39:11,960
systems in place, right? Which might not be something you can measure in numbers, right? Might

575
00:39:11,960 --> 00:39:15,560
not be something that you can look at in terms of dollars or, you know, jobs loss or something

576
00:39:15,560 --> 00:39:20,200
like that. But actually, like, that's, that ends up being like what it is to live with these

577
00:39:20,200 --> 00:39:25,240
systems, right? To live with data being collected about you or to live with automated decisions being

578
00:39:25,240 --> 00:39:29,720
rendered about you. And for that, I think it really takes like getting into the community,

579
00:39:29,720 --> 00:39:35,080
talking to the people who are affected on the front lines. Like, I just can't see that there's

580
00:39:35,080 --> 00:39:40,440
any replacement for trying to understand problems at that level. And just to be concrete about

581
00:39:41,960 --> 00:39:49,880
this, in the case of the truck drivers that will undoubtedly be impacted by self-driving

582
00:39:49,880 --> 00:39:58,360
trucks, how do you see that impacting them, their dignity, their, the way they work, and the way

583
00:39:58,360 --> 00:40:04,200
they feel about their work? Yeah, so, I mean, yeah, so because of kind of the nature of,

584
00:40:05,480 --> 00:40:09,640
so the nature of work, right, is that work is really complicated, right? And so to say like, oh,

585
00:40:09,640 --> 00:40:14,360
well, now we have self-driving trucks, so we don't need human truckers anymore, is to like really

586
00:40:14,360 --> 00:40:18,360
kind of oversimplify what really anybody's job looks like, right? So I think we will, and there are

587
00:40:18,360 --> 00:40:22,280
a lot of things that self-driving trucks are like very, very far from being able to accomplish

588
00:40:22,920 --> 00:40:27,960
technically, right? So what I think that means, and what a lot of people think that means is that,

589
00:40:27,960 --> 00:40:32,040
you know, it's not that we have like robots that replace people, and then we're done, it's that

590
00:40:32,040 --> 00:40:36,920
like very slowly robots change, you know, people end up working with automated systems, right? And

591
00:40:36,920 --> 00:40:42,200
so it's like the working with that becomes interesting and important. And there are ways that we can

592
00:40:42,200 --> 00:40:46,520
work with machines that, you know, augment both where everybody gets to kind of use their

593
00:40:46,520 --> 00:40:51,160
strengths, right? And it becomes like a really kind of collaborative, you know, supportive,

594
00:40:51,160 --> 00:40:55,320
synergistic relationship. And then there are ways where it doesn't appear that way, right? Where

595
00:40:55,320 --> 00:41:00,120
like the machine becomes kind of like the supervisor of you, right? Or the machine kind of,

596
00:41:00,920 --> 00:41:05,160
you know, in the case of truckers, right? Like actually like has a bunch of insight into your body,

597
00:41:05,160 --> 00:41:08,760
right? Or is seen as sort of intrusive? So like I mentioned, you know, truckers having these

598
00:41:08,760 --> 00:41:13,880
cameras trained on their eyelids, you know, or they're all these systems that will like jolt them

599
00:41:13,880 --> 00:41:19,400
or flash lights in their eyes or, you know, text their partners, like all these different things

600
00:41:19,400 --> 00:41:24,200
that they get tired. And then, you know, you can imagine why that like changes kind of the dignity

601
00:41:24,200 --> 00:41:29,880
in the nature of the job, right? Like being kind of forced to sort of hybridize with these systems

602
00:41:29,880 --> 00:41:33,880
is really different than kind of this kind of mutually supportive environment that we might otherwise

603
00:41:33,880 --> 00:41:37,320
create. And so it all becomes about the details, right? It all becomes about kind of like how we

604
00:41:37,320 --> 00:41:42,680
roll these things out, you know, who's incentives we acknowledge. And I think there's a lot of room

605
00:41:42,680 --> 00:41:47,800
to do it well. And in ways that actually like make work better for people. But it takes like really

606
00:41:47,800 --> 00:41:54,840
careful thought, I think when we when we try to do that. Again, maybe acknowledging my inclination to

607
00:41:55,560 --> 00:41:58,920
kind of framework eyes and analyze this. Because you love it. You love it.

608
00:42:00,120 --> 00:42:05,800
Like I'm thinking about like is there has anyone started working on like the, you know,

609
00:42:05,800 --> 00:42:12,600
principles of these this hybridization hybrid job roles? What does that mean? And how can we think

610
00:42:12,600 --> 00:42:17,240
about that consistently across different types of jobs? Or does that even do you think that even

611
00:42:17,240 --> 00:42:22,200
makes sense? Like are there things that apply equally to the to the truckers and people working

612
00:42:22,200 --> 00:42:26,840
in office environments and people working in industrial environments as these jobs are getting

613
00:42:26,840 --> 00:42:32,680
increasingly hybridized? Or do you think that they're they're all separate? Yeah, it's a good question.

614
00:42:32,680 --> 00:42:36,920
I don't think there are probably consistent answers, but I think there are probably consistent

615
00:42:36,920 --> 00:42:41,400
questions to ask across those different contexts, right? So understanding things like an occupational

616
00:42:41,400 --> 00:42:45,720
culture, the occupational culture of trucking is very different from the occupational culture of

617
00:42:45,720 --> 00:42:51,640
say like office work, right? But that but asking questions about the culture and the values

618
00:42:51,640 --> 00:42:57,000
and the traditions of an occupation seems like indispensable to me, right? Understanding like,

619
00:42:57,000 --> 00:43:01,720
you don't just drop technology in and run away, but like understanding, you know, who are these

620
00:43:01,720 --> 00:43:07,240
people? Why are they here? Like what is important to them? Which is both like, you know, monetary,

621
00:43:07,240 --> 00:43:11,720
right? Like what what, you know, how do they make their money and like how is this going to affect

622
00:43:11,720 --> 00:43:15,560
how they make their money? But also these kind of questions around identity and tradition and

623
00:43:15,560 --> 00:43:20,520
culture, things like that. You know, questions about autonomy, I think are probably important

624
00:43:20,520 --> 00:43:25,400
to ask across contexts. Like we pretty much know, right, that people like having autonomy in their

625
00:43:25,400 --> 00:43:29,800
work, like almost, you know, you know, across lots of different occupations, right? Like people

626
00:43:29,800 --> 00:43:34,760
don't like having things directed so much so that they feel like they're part of a machine.

627
00:43:34,760 --> 00:43:39,160
And so when we build machine human systems, like making sure that people have, you know,

628
00:43:39,720 --> 00:43:45,320
some of that decisional capability feels really important, right? And again, like that'll look

629
00:43:45,320 --> 00:43:50,360
different, like if you're looking at doctors working with robots to do surgery versus a truck driver,

630
00:43:50,360 --> 00:43:55,000
you know, with like kind of with an autonomous supervisor in the truck, but the values are the

631
00:43:55,000 --> 00:44:00,280
same, right? Well, Karen, thank you so much for taking the time to chat with us about this. It's

632
00:44:00,280 --> 00:44:04,520
been a really interesting conversation. Well, thanks. I enjoyed it. Thank you.

633
00:44:08,040 --> 00:44:13,160
All right, everyone. That's our show for today. For more information about today's guest,

634
00:44:13,160 --> 00:44:19,640
or to follow along with AI Platform Volume 2, visit twimmelaii.com slash AI Platforms 2.

635
00:44:19,640 --> 00:44:26,960
Make sure you visit twimmelcon.com for more information or to register for Twimmelcon AI

636
00:44:26,960 --> 00:44:56,800
Platforms. As always, thanks so much for listening and catch you next time.

