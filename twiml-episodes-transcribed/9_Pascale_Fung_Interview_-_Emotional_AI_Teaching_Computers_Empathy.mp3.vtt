WEBVTT

00:00.000 --> 00:16.360
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:16.360 --> 00:21.480
people doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:27.240
I'm your host Sam Charrington.

00:27.240 --> 00:30.600
Once again, the recording you're about to hear is part of a series of interviews

00:30.600 --> 00:36.440
I recorded live from the O'Reilly AI and Stratoconferences in New York City.

00:36.440 --> 00:41.720
My guess this time is Pascal Fung, professor of electrical and computer engineering at

00:41.720 --> 00:45.000
Hong Kong University of Science and Technology.

00:45.000 --> 00:50.800
Pascal gave a really interesting presentation at the AI conference, focused on how we teach

00:50.800 --> 00:57.200
computers and robots to understand human emotion and be empathetic.

00:57.200 --> 01:02.560
She also had some really interesting things to say about the theoretical foundations of

01:02.560 --> 01:06.160
the various modern approaches to speech understanding.

01:06.160 --> 01:10.040
And we dig into all of this in our conversation.

01:10.040 --> 01:14.640
As always, I'll be linking to Pascal and her research in the show notes, which you'll

01:14.640 --> 01:20.480
be able to find at twimlai.com slash talk slash nine.

01:20.480 --> 01:24.600
As is unfortunately the case with my other field recordings, there's a bit of unavoidable

01:24.600 --> 01:27.960
background noise, but it's not too bad.

01:27.960 --> 01:31.600
And now on to the show.

01:31.600 --> 01:44.760
Alright, hey everyone, I'm here at the O'Reilly AI conference and I'm with Pascal Fung,

01:44.760 --> 01:51.640
who is a professor of electrical engineering at Hong Kong University of Science and Technology.

01:51.640 --> 01:58.760
And I sat in on her talk earlier on emotions and AI and how do we enable computers to recognize

01:58.760 --> 01:59.760
emotions.

01:59.760 --> 02:04.320
And she graciously agreed to spend a few minutes with us to tell us a little bit about

02:04.320 --> 02:05.840
what she's working on.

02:05.840 --> 02:07.600
So welcome Pascal.

02:07.600 --> 02:08.600
Thank you.

02:08.600 --> 02:13.200
How about we start with talking a little bit about your background and the kinds of things

02:13.200 --> 02:14.200
you work on?

02:14.200 --> 02:15.200
Sure.

02:15.200 --> 02:20.920
So my background, I am an electrical engineer and computer scientist.

02:20.920 --> 02:27.360
And I've been working on speech recognition since 1988 and then move on to statistical

02:27.360 --> 02:30.320
machine translation in the 90s.

02:30.320 --> 02:36.920
And after I became a professor at the HKUST, I lead a team working on speech language

02:36.920 --> 02:44.040
and more recently on emotion and mood recognition or sort of using statistical modeling and machine

02:44.040 --> 02:45.040
learning methods.

02:45.040 --> 02:47.920
That's my technical background.

02:47.920 --> 02:51.760
I worked at different places before.

02:51.760 --> 03:00.200
I was a student and while working on my PhD thesis at Bell Labs, I was very lucky to work

03:00.200 --> 03:06.280
with some of the best people in the area and in the early 90s when I started my thesis,

03:06.280 --> 03:14.120
it was when the field of natural language processing was transforming from a heavily

03:14.120 --> 03:19.800
knowledge-based, linguistics-based field to statistical modeling.

03:19.800 --> 03:23.200
And at the time, statistical modeling for language was very controversial.

03:23.200 --> 03:24.200
Right.

03:24.200 --> 03:27.720
People actually didn't believe you could learn language or study language or understand

03:27.720 --> 03:30.760
language with just statistics.

03:30.760 --> 03:38.600
But 20 years later, now that is the mainstream approach or everything we do is with machine

03:38.600 --> 03:42.560
learning statistical modeling in natural language processing.

03:42.560 --> 03:48.040
One of the slides you put up had a quote from a professor who I think you mentioned you

03:48.040 --> 03:52.480
work with that said, for every language, like fire, or anything like that.

03:52.480 --> 03:57.640
He's sort of the father of statistical speech recognition.

03:57.640 --> 04:00.680
So the field holds a lot to him.

04:00.680 --> 04:03.440
He passed away a few years ago.

04:03.440 --> 04:09.480
And then this quote of his was controversial, but what was the quote?

04:09.480 --> 04:15.840
The quote was that every time I fire a linguist, the speech recognition accuracy goes up.

04:15.840 --> 04:26.200
So it came from at the time at IBM, he was leading the IBM group with a group of mathematicians

04:26.200 --> 04:32.040
and information theoreticians to work on the problems of speech recognition, which was

04:32.040 --> 04:37.960
previously worked on using knowledge-based approach in AI.

04:37.960 --> 04:41.480
And actually, his group wasn't allowed to use that approach.

04:41.480 --> 04:43.240
They somehow did it anyways.

04:43.240 --> 04:48.880
So there was always a little bit of conflict between the knowledge-based AI community and

04:48.880 --> 04:54.080
the at the time, the statistical-minded people.

04:54.080 --> 05:01.840
So there were papers written about the empiricist versus rationalists in the 90s.

05:01.840 --> 05:08.840
And I remember my first conference presentation on a statistical-based natural language processing

05:08.840 --> 05:09.840
paper.

05:09.840 --> 05:14.800
I was yelled at by some of the senior people in the field.

05:14.800 --> 05:17.120
Yeah, those were the days.

05:17.120 --> 05:23.280
But now it's totally non-controversial at all, because you can see machine learnings everywhere.

05:23.280 --> 05:31.520
And yeah, so he said that because the approach he proposed, his group proposed, was very

05:31.520 --> 05:37.800
radical at the time, which is not looking at how to imitate human at all, but just looking

05:37.800 --> 05:41.040
at the input and output of the task.

05:41.040 --> 05:48.760
So for example, for speech recognition, the input is speech waveforms and the output should

05:48.760 --> 05:49.760
be words.

05:49.760 --> 05:55.240
And for machine translation, the input could be French and output should be English.

05:55.240 --> 06:01.200
And they basically treated all these problems as the kind of information theoretic problems

06:01.200 --> 06:02.200
to solve.

06:02.200 --> 06:07.040
So a different mathematical approach from the traditional knowledge-based AI approach

06:07.040 --> 06:08.520
at the time.

06:08.520 --> 06:14.720
Can you maybe give a 30,000-foot background of information theory and how it plays into

06:14.720 --> 06:15.720
all this?

06:15.720 --> 06:16.720
Okay.

06:16.720 --> 06:17.720
I'll try.

06:17.720 --> 06:24.520
So information theory was invented by the entire field, came from a paper written by Claude

06:24.520 --> 06:31.280
Shannon in 1948 called the Mathematical Theory of Communication.

06:31.280 --> 06:36.040
So it basically looks at, you know, the information coding.

06:36.040 --> 06:42.520
For example, if you want to transmit telephone signal through a telephone cable, there's

06:42.520 --> 06:46.160
only that much information you can transmit.

06:46.160 --> 06:54.320
And how many simultaneous calls can you transmit in one cable is limited by physics, actually.

06:54.320 --> 06:55.320
Right.

06:55.320 --> 07:02.920
And so the information theory really is talking about, you know, how do you encode on transmit

07:02.920 --> 07:05.520
and decode information.

07:05.520 --> 07:11.000
So the earliest application was, of course, telephone systems.

07:11.000 --> 07:12.240
So no coincidence.

07:12.240 --> 07:17.120
So there was no accident that Claude Shannon was also at the labs.

07:17.120 --> 07:24.120
And then later on, information theory was then applied, actually, at the time when Claude

07:24.120 --> 07:30.600
Shannon came up with this information theory, he already had the paper on the information

07:30.600 --> 07:31.600
of language.

07:31.600 --> 07:32.600
Oh, right.

07:32.600 --> 07:33.600
Yes, yes.

07:33.600 --> 07:41.000
He actually wrote a paper very early about how language can be encoded and learned.

07:41.000 --> 07:46.400
And so he was one of the earlier pioneers of AI.

07:46.400 --> 07:50.520
Even though people don't think of him all in that way, he had no idea.

07:50.520 --> 07:51.520
Yes.

07:51.520 --> 07:57.520
And he actually also had a cent of American paper on the first chess playing game, chess

07:57.520 --> 08:00.160
playing algorithm in the 60s.

08:00.160 --> 08:01.160
So, yeah.

08:01.160 --> 08:06.040
So information theory became an entire field of research.

08:06.040 --> 08:14.720
And it's applied widely in many, many different areas, most importantly telecommunications communications.

08:14.720 --> 08:21.080
And then, of course, in all the statistical learning field, we also use information theory.

08:21.080 --> 08:34.000
For example, look at how we can learn to model a language from information theory at the

08:34.000 --> 08:35.000
point of view.

08:35.000 --> 08:42.520
So it's more like a, you can think of as message encoding kind of way, rather than linguistically

08:42.520 --> 08:43.840
motivated way.

08:43.840 --> 08:49.160
So it's a different way of looking at mathematical approach of looking at problems.

08:49.160 --> 08:50.160
Right.

08:50.160 --> 08:53.240
You mentioned that in your talk and I found that fascinating and it never occurred to

08:53.240 --> 08:54.240
me.

08:54.240 --> 09:00.320
I think you, you pose the idea of thinking of the machine translation problem as you've

09:00.320 --> 09:05.520
got this, you've got this message that is, you're trying to translate French to English.

09:05.520 --> 09:07.760
You've got this message that's in noisy English.

09:07.760 --> 09:08.760
Yeah.

09:08.760 --> 09:09.760
Exactly.

09:09.760 --> 09:12.040
You clean up the noise and get to the true English.

09:12.040 --> 09:14.920
So for example, it's exactly that.

09:14.920 --> 09:19.200
So for example, the word order are different in different languages.

09:19.200 --> 09:24.680
So it can be, you can think of that word reordering as a kind of a distortion.

09:24.680 --> 09:29.360
And then some of the words actually in French English, some of the words are the same.

09:29.360 --> 09:30.360
Like 40%.

09:30.360 --> 09:31.360
Yeah.

09:31.360 --> 09:32.360
Right.

09:32.360 --> 09:33.360
But other words are different.

09:33.360 --> 09:34.360
Right.

09:34.360 --> 09:38.960
So you can, again, think of a word that's different, a different language being a kind of noise.

09:38.960 --> 09:42.000
I mean, a kind of distortion, I'm sorry.

09:42.000 --> 09:46.240
But if you can learn that distortion, then you learn the translation.

09:46.240 --> 09:49.760
So and that is some information theoretic approach.

09:49.760 --> 09:52.720
That's what Google Translate is still based on.

09:52.720 --> 09:54.600
So interesting.

09:54.600 --> 10:04.120
So how, starting from what are some of the, you know, the algorithms or approaches that

10:04.120 --> 10:10.200
kind of come out of the information theory background, like how is it applied more concretely

10:10.200 --> 10:11.200
to that problem?

10:11.200 --> 10:12.200
Okay.

10:12.200 --> 10:18.640
So for example, all modern speech recognition software is based on the noisy channel model.

10:18.640 --> 10:25.240
So the whole idea of you can train a speech recognizer with lots of data.

10:25.240 --> 10:27.960
So let's say voice search and all that.

10:27.960 --> 10:35.240
It's all based on what other people have said and it's based on these days, millions

10:35.240 --> 10:40.920
of hours of speech data like Siri, for example, it's trained on this data.

10:40.920 --> 10:48.360
And then it uses a different kind of machine learning method.

10:48.360 --> 10:55.520
But all these speech recognition methodologies based on one big paradigm is still the

10:55.520 --> 11:01.480
noisy channel model, which is speech inwards out through this channel.

11:01.480 --> 11:10.040
Now the latest approaches has turned part of these methods into using deep learning to

11:10.040 --> 11:18.560
replace some modules, for example, replacing how phonemes can be modeled or replacing part

11:18.560 --> 11:23.520
of the predicting what will come after which word.

11:23.520 --> 11:30.880
So some of this is now enabled by deep learning, but the whole paradigm is still an information

11:30.880 --> 11:32.600
theory approach.

11:32.600 --> 11:38.080
Similarly for things like Google Translate, it's still based on the, what I just talked

11:38.080 --> 11:40.400
about the noisy channel model.

11:40.400 --> 11:48.760
And recently there's some research work on using Neuronat, but we haven't seen any commercial

11:48.760 --> 11:56.800
application that claims to be using Neuronat for, so end to end Neuronat for formation translation

11:56.800 --> 11:57.800
yet.

11:57.800 --> 11:58.800
Okay.

11:58.800 --> 12:00.800
Okay.

12:00.800 --> 12:05.640
So how did you, how did you kind of get it?

12:05.640 --> 12:10.320
It's all to mention all the encryption software, all that is based on information theory.

12:10.320 --> 12:11.320
Right.

12:11.320 --> 12:12.320
Yeah.

12:12.320 --> 12:13.320
Okay.

12:13.320 --> 12:16.960
That is not my field, but that's actually a main application for this kind of work.

12:16.960 --> 12:17.960
Yeah.

12:17.960 --> 12:18.960
Okay.

12:18.960 --> 12:19.960
Awesome.

12:19.960 --> 12:20.960
Awesome.

12:20.960 --> 12:24.040
How did you get into the study of emotion?

12:24.040 --> 12:26.560
How did I get into the study of emotion?

12:26.560 --> 12:33.920
Basically we noticed that for, so I've been working on spoken language understanding for

12:33.920 --> 12:34.920
a long time.

12:34.920 --> 12:42.720
And I've participated on different efforts from different generations of virtual agents,

12:42.720 --> 12:44.240
what we call virtual assistants today.

12:44.240 --> 12:45.240
Okay.

12:45.240 --> 12:52.680
So the earliest system in the 80s was funded by DARPA, which was a ticket booking system.

12:52.680 --> 12:59.760
Why you call and say I want to go from here to there and then it's trying to book a ticket

12:59.760 --> 13:00.920
for you.

13:00.920 --> 13:05.720
And from that time on, we have seen different generations of dialos systems up until today

13:05.720 --> 13:12.120
we have Siri and Cortana and working on these dialos systems, I've noticed that we've

13:12.120 --> 13:20.200
always sort of just look at literal meaning of a user query.

13:20.200 --> 13:26.560
So the machine just pays attention to what is the destination, the origin city, how

13:26.560 --> 13:31.280
I mean text you want, what kind of restaurant you're looking for.

13:31.280 --> 13:37.920
So very sort of literal interpretation of your query, which means that your query has

13:37.920 --> 13:39.400
to be very clear.

13:39.400 --> 13:44.240
These days people always complain, Siri doesn't work well and all that.

13:44.240 --> 13:50.360
To us, researchers, we can see why it doesn't work well because the system assumes users

13:50.360 --> 13:57.280
to be very clear and say explicitly what they really want to get.

13:57.280 --> 14:02.040
And unless the users are lawyers, I mean, very few people talk that way.

14:02.040 --> 14:06.280
We talk naturally, we expect you to understand what I mean.

14:06.280 --> 14:10.240
And for example, you just laughed because you know I was trying to be funny.

14:10.240 --> 14:16.040
And that kind of information is completely lost in our dialos systems of previous generations.

14:16.040 --> 14:19.800
But it is very, very important for true communication.

14:19.800 --> 14:30.400
If we want to go past the current sort of plateau of understanding, we have to also incorporate

14:30.400 --> 14:35.800
the understanding of emotion intent and all that in addition to understanding the meaning

14:35.800 --> 14:37.400
of the words.

14:37.400 --> 14:40.960
So that's when I started working on incorporating.

14:40.960 --> 14:46.960
So I do not recognize, I don't work on recognizing emotion for emotion sake.

14:46.960 --> 14:51.640
It's really recognizing emotion for communications.

14:51.640 --> 14:55.960
So that's what I call empathy module.

14:55.960 --> 15:02.480
And then when I look at the future applications, what we do, the sum of the most immediate

15:02.480 --> 15:07.360
applications that immediate in the sense that in the next 20 to 30 years we'll see why

15:07.360 --> 15:17.120
desperate application will be healthcare and elderly care, because by the year 2050 there

15:17.120 --> 15:22.080
will be more old people than young children in the world.

15:22.080 --> 15:28.360
And so elderly care is a big area and governments will be running out of resources, human resources

15:28.360 --> 15:30.960
to take care of these elderly.

15:30.960 --> 15:34.360
So we're going to need a lot of machine assistance.

15:34.360 --> 15:39.240
Now my mother spends a lot of time alone.

15:39.240 --> 15:43.120
She's very independent, she's in her ceremonies.

15:43.120 --> 15:48.080
She doesn't want to live with me and she wants to be left alone to do her own thing.

15:48.080 --> 15:54.720
And I'm always worried and in her independence, I worry about her health.

15:54.720 --> 16:01.880
She seems healthy when I see her, but at her age I want to make sure that she's fine

16:01.880 --> 16:02.880
right now for example.

16:02.880 --> 16:04.600
I want to know she's fine right.

16:04.600 --> 16:12.160
So this kind of, I want to know her health conditions but also her mental conditions.

16:12.160 --> 16:19.200
So if she doesn't want to live with me, now how about if I have, if we build a home

16:19.200 --> 16:27.880
robot who can be there at her, you know, at her home or be around her and converse with

16:27.880 --> 16:34.200
her, sometimes just to get a sense of how she's doing simple things, right.

16:34.200 --> 16:38.640
And then sends me a message so I know how she, I mean, all sends me a curve of her vital

16:38.640 --> 16:46.960
signs in addition to her emotional state, then that will help the guilty children, busy

16:46.960 --> 16:53.000
guilty children, but also help the elderly because in a lot of people have emergency problems

16:53.000 --> 16:58.080
like a heart attack or something that could have been, they could have been saved if someone

16:58.080 --> 16:59.240
knows.

16:59.240 --> 17:05.800
And then there are others who can get lonely and depressed and then they can also be helped

17:05.800 --> 17:10.760
by machine to some extent, not completely by machines, you know, living with just machines

17:10.760 --> 17:17.280
is also very sad, but but when there are not enough humans around people around then

17:17.280 --> 17:19.440
the machines can help.

17:19.440 --> 17:26.800
So this is why I want to work on empathetic robots, in the case, in the case of a crisis

17:26.800 --> 17:32.480
situation like a heart attack, where does empathy and what does that come in?

17:32.480 --> 17:40.360
I think heart attack is basically its emergency, then the robot has to basically alert, call

17:40.360 --> 17:41.440
the ambulance, right?

17:41.440 --> 17:47.880
That's the first thing, but what empathy comes into play is that so daily reminder to take

17:47.880 --> 17:48.880
medicine.

17:48.880 --> 17:56.680
In some of the aging studies, people have found that a lot of elderly, they don't want

17:56.680 --> 18:05.200
to take medicine unless somebody talks to them, like sometimes you have to sort of entice

18:05.200 --> 18:08.760
them, I mean some elderly select children.

18:08.760 --> 18:16.600
So in that case, the machine doesn't just say take your medicine and repeatedly insisting

18:16.600 --> 18:21.600
that you take your medicine, like with the same command, that will be extremely annoying

18:21.600 --> 18:23.960
and it will have the opposite effect, right?

18:23.960 --> 18:31.160
So the machine needs to know that the elderly is hesitant or resisting and how is the person's

18:31.160 --> 18:39.640
patient's emotional state and to know whether now the machine can insist or it's time to

18:39.640 --> 18:48.160
call a doctor or nurse or whether telling the patient a bedtime story will suit them.

18:48.160 --> 18:53.560
So that requires empathy, if you think about all the nurses in the hospital, a lot of

18:53.560 --> 19:00.640
their job, a lot of their work and their tasks are very repetitive and sometimes the better

19:00.640 --> 19:05.280
nurses are the ones who really have a very good bedside manner and what is the bedside

19:05.280 --> 19:10.160
manner other than being empathetic, just being empathetic, you know, for both doctors

19:10.160 --> 19:11.160
and nurses.

19:11.160 --> 19:15.920
So if you want doctors and nurses to be empathetic, obviously you want the healthcare robots

19:15.920 --> 19:16.920
to be empathetic.

19:16.920 --> 19:17.920
Right, right.

19:17.920 --> 19:24.640
You talk, I interpreted its focus on empathy recognition, but your description is also

19:24.640 --> 19:27.560
talking about what you might call generative empathy.

19:27.560 --> 19:28.560
Right, right.

19:28.560 --> 19:34.800
So empathy has two sides, it's the emotional recognition and then the appropriate emotional

19:34.800 --> 19:35.800
response.

19:35.800 --> 19:36.800
Okay.

19:36.800 --> 19:42.680
So empathy, I only, in my talk, I focus mostly on the emotion recognition part because that

19:42.680 --> 19:47.920
is hard until we can recognize the emotion, we don't know how to react, right?

19:47.920 --> 19:50.840
So I focus on that.

19:50.840 --> 19:56.640
But the response part, I talked a little bit at the end that we're trying to learn the

19:56.640 --> 20:04.280
appropriate response as well from data, so also using deep learning.

20:04.280 --> 20:11.560
You mentioned healthcare as a use case, I think there's often, for a while now, people

20:11.560 --> 20:13.760
have talked about a customer service use case.

20:13.760 --> 20:14.760
Right, right.

20:14.760 --> 20:15.760
Sure, sure.

20:15.760 --> 20:19.760
You know, the, the, the whole line will recognize when you're getting frustrated.

20:19.760 --> 20:20.760
Sure, sure.

20:20.760 --> 20:30.720
In fact, at AT&T Bell Labs in the 90s already, they, they, a group worked, came out with

20:30.720 --> 20:33.400
the system called How May I Help You?

20:33.400 --> 20:40.280
So when you call the AT&T line, it's a, it's a virtual system, virtual operator that

20:40.280 --> 20:43.360
talks to you first and says, How May I Help You?

20:43.360 --> 20:47.760
And you basically say whatever you want and then it goes to different categories.

20:47.760 --> 20:48.760
Yeah.

20:48.760 --> 20:50.920
And that is the intention classifier.

20:50.920 --> 20:51.920
Okay.

20:51.920 --> 20:57.640
And also at the time, AT&T, AT&T had internal programs to analyze all this call center

20:57.640 --> 21:01.360
data to see whether people upset, they're happy or not.

21:01.360 --> 21:04.400
So that is already the beginning of emotion recognition.

21:04.400 --> 21:08.240
That was my first contact with emotion recognition.

21:08.240 --> 21:12.880
And it was for, for customer service, indeed, it is a big area.

21:12.880 --> 21:18.600
But I don't get the sense that it's widely deployed or at least not in a way that I would

21:18.600 --> 21:23.200
see as an every person, maybe it's more at, yeah, it's at the back end.

21:23.200 --> 21:26.280
So this is the thing because it's not consumer facing.

21:26.280 --> 21:31.960
It's really for, it's really in the, in the area, the realm of data analytics.

21:31.960 --> 21:39.120
So it's more of a tool used by corporations to improve the efficiency of their call centers.

21:39.120 --> 21:40.640
And a performance management.

21:40.640 --> 21:42.880
Performance, yeah, they do do that.

21:42.880 --> 21:48.160
There are companies that provide technology to customer service.

21:48.160 --> 21:49.160
Okay.

21:49.160 --> 21:50.160
Yeah.

21:50.160 --> 21:51.160
Yeah.

21:51.160 --> 21:52.160
Yeah.

21:52.160 --> 22:07.320
So you also talked about the use of convolutional neural nets and recognizing emotion and kind

22:07.320 --> 22:15.080
of drew some interesting correlations across, you basically that the CNNs were able to functionally

22:15.080 --> 22:17.920
approximate the cochlear functionality.

22:17.920 --> 22:18.920
Yeah.

22:18.920 --> 22:23.640
And some perception system, indeed, that's what we found.

22:23.640 --> 22:29.880
So I think my talk, I started out by saying, okay, traditionally speech recognition, look

22:29.880 --> 22:35.920
at these human, human, some perception system and try to imitate that.

22:35.920 --> 22:42.280
But we sort of hit the bottleneck and we had to move away from that and go with the information

22:42.280 --> 22:47.640
theoretic approach where we actually try, we don't try to imitate human model at all.

22:47.640 --> 22:54.000
And what's interesting with CNN is that a lot of times CNN or other deep neural net are

22:54.000 --> 22:56.160
being used as a black box.

22:56.160 --> 23:02.920
So we know it works, we don't know why, and humans being humans are always interesting

23:02.920 --> 23:03.920
knowing why.

23:03.920 --> 23:04.920
Right.

23:04.920 --> 23:10.520
And in fact, there's a practical reason is that if you ever want to commercialize a technology

23:10.520 --> 23:16.040
like that, to provide to your customer, they want to know why, what it's learning.

23:16.040 --> 23:25.080
So we then took a look at the CNN, different layers of CNN and saw that it was actually,

23:25.080 --> 23:32.720
as I mentioned in my talk, that it's basically approximating the future bank in our cochlea

23:32.720 --> 23:35.240
that's connected to the auditory system.

23:35.240 --> 23:40.560
And then we also saw that it's picking up on the amplitude, the peaks in the amplitude

23:40.560 --> 23:42.880
that correspond to different emotions.

23:42.880 --> 23:47.440
Now we thought that was very interesting that we could actually see what's going on.

23:47.440 --> 23:54.160
For example, it was always, CNN was first applied to image recognition.

23:54.160 --> 24:01.240
They used like seven, eight, nine layers of CNN to achieve that purpose.

24:01.240 --> 24:05.800
And in image recognition, they were able to see that each layer, so for example, one

24:05.800 --> 24:12.880
layer is recognizing the edges of image and the other layer is looking nice, maybe something

24:12.880 --> 24:15.520
else, some features on your face and all that.

24:15.520 --> 24:17.720
So it's all very obvious and really nice.

24:17.720 --> 24:24.600
And we were never able to explain how the neural networks on speech and language.

24:24.600 --> 24:28.720
So it was interesting to see why works on emotion.

24:28.720 --> 24:32.600
We still are not able to figure out what it's doing on languages.

24:32.600 --> 24:35.600
So each layer is recognizing.

24:35.600 --> 24:41.880
We were hoping that each layer will correspond to some linguistic functions, such as syntax

24:41.880 --> 24:47.440
or we haven't seen anything that neat yet.

24:47.440 --> 24:48.440
So.

24:48.440 --> 24:49.440
Interesting.

24:49.440 --> 24:50.440
Yeah.

24:50.440 --> 24:52.840
So it's not blinding learning something.

24:52.840 --> 24:58.080
It's learning something which has a physical meaning.

24:58.080 --> 25:04.720
But you made the point that it's also an error to correlate it too tightly to brain function

25:04.720 --> 25:06.240
because that doesn't really work.

25:06.240 --> 25:07.240
It doesn't.

25:07.240 --> 25:08.240
It's not.

25:08.240 --> 25:09.240
Because no.

25:09.240 --> 25:13.640
So even though I said it approximates human perception system, it's really the hearing

25:13.640 --> 25:14.640
system.

25:14.640 --> 25:15.640
Right?

25:15.640 --> 25:16.640
It's not the understanding part.

25:16.640 --> 25:18.280
It's the hearing system.

25:18.280 --> 25:21.320
And we know exactly how our hearing system function.

25:21.320 --> 25:22.920
We know very, very well.

25:22.920 --> 25:23.920
We don't guess.

25:23.920 --> 25:30.440
But how our mind's functioning, understanding the meaning, we don't know.

25:30.440 --> 25:38.000
We're activating our, you know, I mentioned 100 billion neurons and 100 trillion synopsis

25:38.000 --> 25:42.520
to get that until we have neural network of their size.

25:42.520 --> 25:50.480
It's hard to come up with something similar to human cognitive ability.

25:50.480 --> 25:51.480
So it's not.

25:51.480 --> 25:52.480
We don't have that.

25:52.480 --> 25:59.280
So that's why I say it's no coincidence that we can use.

25:59.280 --> 26:03.240
We can't explain what CNN is doing for perception.

26:03.240 --> 26:08.640
So speech recognition and emotional recognition are both perception problems.

26:08.640 --> 26:16.520
Perception is actually easy because we understand human perception, how our skin feels the

26:16.520 --> 26:17.520
temperature.

26:17.520 --> 26:20.800
We actually understand the physics of that very, very well.

26:20.800 --> 26:25.720
But once you go into understanding, which is language understanding, which is the trans,

26:25.720 --> 26:31.160
you know, if I want to use a noisy channel model, it would be from words to meaning.

26:31.160 --> 26:36.680
Once we're getting to the realm of that, we are kind of clueless to how humans.

26:36.680 --> 26:41.960
So there are a lot of linguistic theories about how humans think, how humans understand.

26:41.960 --> 26:46.640
But you know, for every linguistic theory, there are other people who say no.

26:46.640 --> 26:55.400
So there's no, no, there's no scientific truth that we all share right now about how human

26:55.400 --> 26:59.760
mind understands language or understands anything else.

26:59.760 --> 27:03.240
How do you know a video of a cat is funny or not?

27:03.240 --> 27:05.840
How does our mind interpret humor?

27:05.840 --> 27:06.840
We actually don't know.

27:06.840 --> 27:09.160
A lot of marketers would like me to be able to.

27:09.160 --> 27:14.320
So yeah, how do you predict, for example, one thing we were asked since we could classify

27:14.320 --> 27:19.200
music, we were asked by a company and say, hey, can you predict whether a song is going

27:19.200 --> 27:21.200
to be a hit or not?

27:21.200 --> 27:26.200
Maybe we could predict that, wouldn't that be amazing?

27:26.200 --> 27:33.160
No, you know, because you know, even we look at big data of all the past hit songs, it

27:33.160 --> 27:38.640
can learn, it cannot predict what the next one, not yet, maybe.

27:38.640 --> 27:43.720
So all we can do right now is use engineering models to approximate input and output.

27:43.720 --> 27:48.800
You know what I mean, it's really a mimicry, like just looking at this input, can we come

27:48.800 --> 27:53.120
out with output that's similar to the truth?

27:53.120 --> 28:00.120
So we're not, we're not in any way near to imitating human minds and that will be, even

28:00.120 --> 28:05.080
though, so even the term deep learning is a new term for something that has been around

28:05.080 --> 28:06.080
for a while.

28:06.080 --> 28:08.880
So that's a particular kind of machine learning.

28:08.880 --> 28:09.880
Right.

28:09.880 --> 28:16.280
It is, it is no deeper, let's say that are the kind of machine learning methods, it's

28:16.280 --> 28:25.720
a terminology and also neural network, it's a very, very rudimentary kind of neural network,

28:25.720 --> 28:31.720
you know, for speech recognition that might be tens of thousands of neurons and for emotional

28:31.720 --> 28:35.160
recognition, much, much fewer.

28:35.160 --> 28:39.520
So that cannot compare to the human brain.

28:39.520 --> 28:45.840
One of the things that I noticed in your presentation is that when you're doing

28:45.840 --> 28:52.080
the emotional, emotional recognition, you're mapping it to kind of the, you know, these

28:52.080 --> 28:56.160
names, common names we have for emotion, angry sad, whatever.

28:56.160 --> 29:01.360
Is that model even too simple or is there an underlying more nuanced model for emotion?

29:01.360 --> 29:03.360
I'm not sure they're.

29:03.360 --> 29:09.160
Yeah, well, so there's a lot of research done on the underlying model for emotion, such

29:09.160 --> 29:16.120
as valence arousal, you know, and there are models that try to predict that first before

29:16.120 --> 29:18.320
they predict the final label.

29:18.320 --> 29:22.520
And to interrupt you, because I saw the slide, but these folks haven't, valence arousal is

29:22.520 --> 29:25.160
valence is like the strength of the emotion and-

29:25.160 --> 29:27.080
No arousal is the strength.

29:27.080 --> 29:29.760
Valence is the positive and negative of the emotion.

29:29.760 --> 29:36.200
And so the various, you know, angry sad, cool, it's a combination of, yeah, they are a

29:36.200 --> 29:41.520
combination of different values of valence and arousal, this is one emotion theory.

29:41.520 --> 29:47.640
So these are models that psychologists came up with to try to organize what we know

29:47.640 --> 29:49.680
about emotion.

29:49.680 --> 29:56.080
And so it's just human minds, we're symbolic animals, you know, we need to have names,

29:56.080 --> 29:58.160
we give names to everything.

29:58.160 --> 30:02.640
So it's just easier for us to give a name to emotion, so we know what we're talking

30:02.640 --> 30:11.200
about, rather than give this vague, you know, number, valence arousal, if I tell you, oh,

30:11.200 --> 30:15.080
this is valence arousal, this is this number, you wouldn't know what I'm talking about,

30:15.080 --> 30:20.440
if I say he's, you know, he's showing happiness on his face, you kind of, kind of, no.

30:20.440 --> 30:24.840
So it's kind of emotional recognition, it's kind of like speech recognition when it was

30:24.840 --> 30:28.960
only recognizing isolated words.

30:28.960 --> 30:31.640
It's oversimplified for sure.

30:31.640 --> 30:34.240
We're not good at all with emotion recognition.

30:34.240 --> 30:40.520
You saw my slides, the performance is nowhere near the ability, the performance of recognizing

30:40.520 --> 30:44.680
words, recognizing emotion is much harder right now.

30:44.680 --> 30:50.400
One reason you pointed out is that it's hard to define where emotion is, you know, for

30:50.400 --> 30:55.960
example, maybe it's easy to see if somebody's happy, but is he smiling because he's happy

30:55.960 --> 30:59.320
as he's smiling because he's trying to be polite.

30:59.320 --> 31:03.400
And also, what about emotions like frustration?

31:03.400 --> 31:05.200
How can you tell?

31:05.200 --> 31:11.840
Sure, some things are obvious, you know, when someone's frustrated, if they're rolling

31:11.840 --> 31:16.440
their eyes or something, but there are other times, you know, from the voice, from the tone

31:16.440 --> 31:24.680
of your voice, we can tell a lot of things, but we cannot tell everything all the time.

31:24.680 --> 31:28.720
So it is along with the goal, you saw the accuracy, the accuracy these days, even the

31:28.720 --> 31:36.640
best commercial systems, it's just like 60 percent, you know, and most 70 percent.

31:36.640 --> 31:39.880
And speech recognition, we're talking about above 90 percent, right?

31:39.880 --> 31:45.240
So there's a long way to go for emotional recognition, and especially some more complex

31:45.240 --> 31:57.040
emotions, like humor, sarcasm, yeah, sarcasm, humor and, and deceit, you know, is this

31:57.040 --> 31:59.440
person lying?

31:59.440 --> 32:04.040
And there are colleagues in the field who have come up with systems that, that has 70

32:04.040 --> 32:09.280
some percent accuracy in detecting deceit and actually performs better than humans, turns

32:09.280 --> 32:13.320
out we are now very good at detecting deceit, we're not good at all.

32:13.320 --> 32:16.720
And humans are not very good in recognizing emotions.

32:16.720 --> 32:17.720
Yeah.

32:17.720 --> 32:22.320
You know what we found with, when we did some human subject studies, is that we found

32:22.320 --> 32:31.400
that women are better, women are more empathetic than men, there was actually quantifiable.

32:31.400 --> 32:37.760
And then women can detect emotion across languages, languages, in languages they don't know.

32:37.760 --> 32:38.760
Interesting.

32:38.760 --> 32:42.560
Also better than men in the same language.

32:42.560 --> 32:46.680
So you can talk about why the reason, you know, we're programmed to be mothers, we must

32:46.680 --> 32:49.440
recognize the emotion of a baby early on.

32:49.440 --> 32:51.160
And I think there's some merit to that.

32:51.160 --> 32:59.520
So, although I'm not a anthropologist, I cannot prove that, but, yeah, so, interesting.

32:59.520 --> 33:04.360
And then the reasons, you know, you see there are a lot of women who are nurses, doesn't

33:04.360 --> 33:08.480
mean men cannot be, but just happen that way, right.

33:08.480 --> 33:19.520
A lot of the caregivers, women, you know, kindergarten teachers, you know, just nannies, you know.

33:19.520 --> 33:24.000
So if we want to build robots, we want them to be more like that, more empathetic.

33:24.000 --> 33:25.000
Right.

33:25.000 --> 33:26.000
Great.

33:26.000 --> 33:28.400
Well, thanks so much for taking the time to sit down with me.

33:28.400 --> 33:29.400
It was a great discussion.

33:29.400 --> 33:30.400
I really appreciate it.

33:30.400 --> 33:31.400
Thank you.

33:31.400 --> 33:32.400
Thank you.

33:32.400 --> 33:36.800
Would you like to share how folks can find your research, or are you on any of the social

33:36.800 --> 33:37.800
media networks?

33:37.800 --> 33:48.080
Well, it's easy to Google my name, which is Pascal Fung, P-A-S-E-A-L-E, Fung is F-U-N-G.

33:48.080 --> 33:54.600
If you Google my name, you come to my website, which lists all my our publications, our projects

33:54.600 --> 33:55.760
and all that.

33:55.760 --> 33:59.560
And they can, they can email me via that website as well.

33:59.560 --> 34:00.560
Great.

34:00.560 --> 34:01.560
Great.

34:01.560 --> 34:02.560
Well, thanks so much.

34:02.560 --> 34:03.560
Thank you.

34:03.560 --> 34:06.800
All right, everyone, that's it for today's show.

34:06.800 --> 34:11.240
If you enjoyed this show or have something to add to the discussion, please leave a comment

34:11.240 --> 34:17.960
on the show notes page at twimmaleigh.com slash talk slash nine.

34:17.960 --> 34:24.680
Or tweet to me at at Sam Charrington or at twimmaleigh to let me know what you think.

34:24.680 --> 34:44.640
Thanks so much for listening and catch you next time.

