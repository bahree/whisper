All right, everyone. Welcome to another episode of the Twimble AI podcast. I am your host, Sam
Charrington. And today I'm joined by Ali Rodel, Senior Director of Machine Learning Engineering
at Capital One. Before we get going, please take a moment to hit that subscribe button wherever
you're listening to today's show. Ali, welcome to the podcast. Thank you. Thank you.
Very happy to be here. I'm super excited to jump into our conversation. We'll be talking about
the platform that you are building there at Capital One to support machine learning at the company.
We had a great chat about old school tech just before we got started and we bring some of that
vibe through it would be a lot of fun. Before we get going, I'd love to have you share a little bit
about your background, your role, how you came to work in the field, all that good stuff. Sure.
Yeah, old school tech. Yeah, I think I am technically old for this industry. Yeah, I'm 49. So,
yeah, I've been working in software engineering or in tech in general for about 26 years. Most of that
in highly regulated industries of various sorts. I've actually technically done ground up data
center builds back way back in the day when we had that thing called data centers. But yeah,
I started as a software engineer. I've worked in a bunch of different areas, including
doing a lot of data movement, like actual migration of data around the world for these data center
builds and whatnot. And just recently, or five years ago, joined Capital One, which was my
actually entry into the machine learning space. Part of that, I was more in the software engineering
and infrastructure space. It was brought in here to help us mature our SDLC life cycle in the
machine learning space, which is now called the MDLC model development life cycle, I suppose,
which I've helped to do since I started, moved in a couple different places, all of which
has been in the machine learning, I guess we'll call it platform space or in the machine learning
infrastructure space in general. And yeah, now I own all the model development platforms here.
Awesome. So you own model development platforms there. Let's jump right into into that and talk
a little bit about your, really the platform journey. Were you at Capital One from the beginning
of the company's involvement in machine learning? How much of that have you seen evolve?
No, I was not. It started before I got here, but it has evolved a lot. And I think in general,
the industry, I mean, I don't need to tell you this industry is moving very quickly.
And evolving is even an understatement, just like gargantuan shifts. When I came in,
we certainly had machine learning in place. As I'm sure you've read, Capital One has made some
very large scale investments in getting off of antiquated infrastructure moving into the public
cloud, like really modernizing how we run everything. And because of that, we've been able to
leverage the ability to redo everything over and over again. We can really evolve quickly
because everything is elastic. And not to move away from the machine learning space, but
at the end of the day, all this stuff has to run somewhere. So when I came in, there was
certainly a fair amount of machine learning that has accelerated significantly. I actually came
from working in one of our lines of business, or I came into a center area, I moved into one of
our lines of business and was building up some capabilities there. We've now consolidated all
of that together and are kind of building a consolidated offering. Like I guess that you could say
we have matured probably faster than others because of that ability to be like elastic and
basically do terraform destroy and kill everything and then create it again, you know,
fairly quickly. So yeah, I was not here in the beginning, but it's been five years, so we've
evolved a whole lot since then. Yeah. Yeah. Yeah. It's interesting to hear you kind of ascribe
to cloud the ability to counteract what I think as two really strong kind of inertial forces,
one kind of large company with lots of existing legacy systems and two highly regulated industry.
And you're kind of in the nexus of both. And I guess they tend to maybe go hand in hand.
Yeah. I mean, I think you're describing why my job is hard.
But yeah, I think traditionally one thinks of a bank, one thinks of a large company and one thinks
of static, not changing. We're afraid to upgrade it. And one definitely thinks of mainframe-based
processes, even if you're using distributed compute, there's obviously a mainframe there somewhere,
right? But yeah, no, that's not the case. And when you look at what we're trying to build
and the speed with which we have to change things, it just wouldn't work.
You're just bolting, you would be building virtualization layers on a system that was not made
to move this quickly. And by that, I mean, let's take an example of, let's say you're training a
very large model. And by a large model, in our case, I would mean 1,000 node spark clusters.
So maybe I think that's large for me, maybe that's not large for Google. I don't know,
I've never worked at Google. But large ish, right? Like large enough that you will break things
when you run a large, like 1,000 node spark cluster for a feature generation run.
You're not going to run that on like a, I mean, I guess you could run it on a VMware cluster,
but it's not really going to scale. And you're not going to be able to leverage the fact that I
can turn it off and go home and not pay for it, right? So it's not uncommon for us to run
many, many, many workloads of that size at the same time. And there's a human there. I mean,
of course, we run these on a schedule matter, but there's a, there's a human there waiting for it
to finish. So like there's a, there's a, a way that you can leverage infrastructure now that
that's par for, for working now, right? The ability to run things at that scale.
In the past, I think that would have been a big deal. Now, that's just par. That's just getting
us through the door. You have to be able to do that. That's the basics. That's just the foundations,
right? Now we're going to be able to build on top of that. But if you don't have those foundations,
you can't, like you don't even know what you can build. So maybe talk us through what it is that
you are trying to build. Like what's the, the, the vision, not really kind of the far vision,
but like what is the product that you are trying to offer the data scientists and machine learning
engineers there at Capital One? I think our job is to, is to make it easy. Like data science is
not easy, but it shouldn't be hard to do your job, right? Like my job is, and my organization's
job is to make doing data science work easy. And that means I, you should be able to log on.
You know, if you're, let's say you're new to the company, you log on and you see how,
how do people do data science here? What, what am I supposed to do? I want to be able to show you
how we do work here, right? We're in a highly regulated industry. We can't just,
you don't get access to everything, right? You can't just be like, oh, I want to look at this
customer record. No, no, no. There's a, there's a, there's a whole way that this is, you know,
manage and dealt with. But like once you do have access to things, I want to make it easy. I want
you to be able to come in, sit down. Oh, this is how we run distributed compute. This is how we
run distributed training. This is how we run, do experiments. This is how we, like, spin up
Jupyter Notebooks. And, and this is what you can do in a notebook. This is what you should not do in
a notebook. Like, I want it to be very, very easy. I want it to be simple. Like, we, we, we try to
incentivize through ease of use versus, like, declaratory, thou must do these things.
Though we also, of course, have to have those two. But we try and just make data science work
easy. And that's not easy. Yeah, absolutely. Does your team scope span from
some of the low, low infrastructure that we've talked about all the way up to the data scientist,
developer experience, you know, managing experiments and launching notebooks and all that?
It does. Yeah. So, um, I, I often joke that you can't really undervalue this
a strong DevOps team. Like, it's just not, actually, I guess that's not a joke. That's just a statement
effect. You can't undervalue a strong DevOps team. Yeah. So we, we own design of the system,
provisioning of the system, day to day running of the system. And that includes user support. So
I've got teams that, you know, we'll go in and be like, have solid, like, infrastructure
experience, very solid AWS experience, very solid Kubernetes, like Docker based,
capability experience. And they start to verge into understanding those data science workloads
and those machine learning workloads, but they're not the folks who are going to go in and help
troubleshoot, like, why isn't my H2O predict function working? And then I've got the other side,
which is folks who are doing, like, hands-on implementation of reusable capabilities that
data scientists will work from and actually utilize. So it kind of runs the gambit. And those,
those teams support each other. I'm not asking a DevOps engineer to necessarily understand the
underpinnings of how machine learning training is executed. But I'm under, I'm asking him to understand
the profile of it and how it executes. So for example, that 1,000 nodes barc cluster I mentioned
in the past, you know, 10 minutes, you can kill a Kubernetes cluster very easily with large-scale
distributed compute. So those DevOps engineers, those platform teams need to know how to scale that.
And one thing that's interesting about the conversation we're having so far is the,
again, kind of the conversation is kind of grounded in infrastructure. And clearly you're coming
at this from like a software engineering perspective where others come at this from a data science
perspective and, you know, they're building the same tools, but worrying a lot less about the,
you know, the infrastructure that's running that. I'm really curious your take on, you know,
the impact that that's had on, you know, what you're building, the way you think about what you're
building, the user experience there. Yeah, it's interesting. It does seem like people usually come
at this from one angle or the other, right? They come into this from either I have an academic
background or I'm like this new machine learning engineering background, but focused on the actual
implementation of machine learning. Or, and I suppose it is less common, they come at it from a
systems background or from a software engineering background. Yeah, I, I have found that
when you boil all of the problems down that we run into on a regular basis,
they really generally fall down to software engineering fundamentals under the hood. And
really, when you really boil it down, things are still constrained by the same constraints
they were 30 years ago. Memory, CPU, and disk IO. I mean, we have many layers of virtualization
before we actually get down to some of these, but at the end of the day, the fundamentals
are still there. So, so now when you're going way up these layers, we're building these platforms,
and you know, I'm not the only person, I'm sure that encounters this, but like when you're building
these platforms, you're, you're trying to optimize a user experience in a very immature software stack,
the machine learning ecosystem, fairly immature. So, with the, like, immature stack that we're
working with, and, and what I mean by that is, like, there's a lot of, what I would refer to
as lazy, lazy engineering. You, you run into a lot of things that, that run it, like, have to run
as root. This has to run as root. Nothing has to run as root. You're just not trying to make it
not run as root. Meaning stuff that your team has created, or stuff that like vendors are providing,
or open source tools. Then vendors are providing open source tools. We use a lot of open source software
as, as do many places, who are, you know, we're here in the machine learning space. Many things need
to run as root. I'm taking an example, a Kotib, a hyper parameter optimization tool, has to run as root.
I'm sure it doesn't have to run as root. I'm sure there is a way to have it not run as root,
but, but out of the box, it does. So, that means, you know, we have to do something to make it. So,
it can't run as root, because I work in a very secure place. I don't let people run things as
root, right? That is a big, big no-no, right? And I programmatically prevent that from happening,
right? So, just as an example, like, coming at this from a systems background, you know,
we're solving these problems, same problems we solved 20 years ago, with the Java ecosystem,
when nothing would run and you couldn't install anything. Here we are today, in the machine learning
ecosystem, by the way, Java ecosystem solved it, right? Sudo app get install Tomcat, and you're good,
right? That didn't work 20 years ago. Nowadays, you can't do Sudo app get install ML platform,
right? So, you have to solve all these problems.
The Java solve that or Linux solve that? Linux solve that. Yeah, you're right. I guess maven to
some extent, but you're right. Linux, all that. I mean, package management has made working in
software engineering just so much more pleasant. It really used to suck.
I referenced us talking about crafty technologies and, like, old school experiences earlier,
and part of that conversation was, like, the JavaScript ecosystem and package management and
all that kind of stuff. I think to your earlier point, like, the same fundamental computing problems
occur in different domains. So, do you think we'll get to app get install, experiment management
system or ML ecosystem? I do. I do. I mean, when you look at the problems that are associated with
installing software, when that software is responsible for generating large amounts of money,
those problems tend to get solved. And the open source ecosystem
has really changed the game as far as the scale of people trying to solve problems.
So, it's kind of like a ratchet. I feel like it's like a ratchet. When things get solved in
the open source community, they don't tend to get unsolved again, right? So, it's like, click,
click forward. Oh, we've solved this package management problem. Okay, we don't have to worry
about package managers anymore. I mean, you do, but you don't really, right? You know, or click,
click, click forward. Oh, we've solved, I don't know, running large scale like app server type stuff,
right? Like Tomcat, whatever you want to insert there. Like, great. We don't have to solve that
anymore because the, you know, there's thousands of engineers that contribute little changes to these
things. So, I think we, I think it will get solved, but it'll just open up other problems. We'll have
to fix, you know, follow on problems, we'll have to fix that. And you also characterize that as
AppGet install ML ecosystem. Do you think that it, it is at that scope ML ecosystem or like individual
tools that people will mix and match into some, some broader thing? Like, how much,
another way to ask that is how customized to your environment and specific needs is the,
the platform that you're building, which you know, still we haven't really talked very
concretely about, we've talked about it pretty abstractly, but how's, you know, customized,
is it to your specific needs? So, that's the second one's a good question, the first one's a good
question. So, I think, you know, would it be at the ecosystem level or would it be at the individual
component level? I don't know. You know, I don't know. I think that vendor tools will allow you to do
that as a ecosystem level. I think eventually you will probably get to an open source ability to
do that. I mean, today, if you don't work in a highly regulated lockdown environment, you can
install Coopflow and it works, right? And it's, and it's pretty good. That being said, in a company
that controls things, you know, either a little bit or a lot, and us being, of course, on the
allot side, nothing will run without being modified. So, you know, I think you'll find in, really,
any super lockdown place, everything will have to be modified. So, from our perspective, you know,
we have to, we have to touch everything. It's one of the reasons why we like open source software
so much because you can pop open the hood and see what's really happening. And you also, you can
change things, right? And, and in some cases, we will actually modify, you know, even the like,
base image stuff in some of these images so that it'll run here. Now, we try to commit some of
those changes, of course, backup stream, but, you know, we have a bit of a build process there.
So, I would say that we modify a lot of the infrastructure side, but we try to enable
capabilities that are fairly standard on the user interface side. Like, if you're using open
source projects, we try not to change how they're used. Like, otherwise, you're just getting too
much into it. And then it makes it very hard to do upgrades. So, we try to keep things fairly vanilla
there. And when you look at the Kubernetes ecosystem with machine learning, you just get this
big wealth of open source capabilities you can use. And what we end up doing is we end up
really modifying how it'll run. But we try to give, we try to make it so that people can apply
their knowledge from elsewhere here, right? I don't know what any of this is, right? We want
them to come in and say, oh, I've used this before and have a, have a familiar interface.
Yeah. So, kind of taking a step back when you think about the platform that you're building,
what are the main components and how do they fit together to create the experience you're
looking to create for your users? Sure. So, when you think about our data science community,
they range wildly from stats, PhD, kind of sort of knows Python to super strong. What I would
even refer to as a full stack data scientist, like, they're very strong engineers. So, we have to
enable all of them to do each of the parts. And when you think about component-wise, maybe I can
focus on like the stages of the, of a model going through the system and all the pieces we have to
enable, you know, when, when, no matter if they're like the stats PhD or the full stack data science
is person, they need to come in and they need to be able to explore. They need to be able to pull in
data, right? So, we've got to provide Jupyter notebooks. We need to provide the ability to do
exploration locally within the constraints of being able to pull data. You're not going to run
pull five terabytes of data down to your laptop. So, we need to provide Jupyter notebooks. We need
to provide an entry point to the ability to run large-scale distributed compute from those
Jupyter notebooks, potentially from local desktop as well from their IDE. And then when they start
to need to really dig into the data, they're going to be running large-scale distributed compute
generally. I mean, maybe small-scale, but either way, not much is done in a single instance
these days that is going to have significant predictive value. So, we then need to enable
basically spark and task capabilities to be able to be run. I'll say easy in an easy way,
which is kind of a counter statement because nothing about distributed compute is easy,
but we need to enable basically spark and task workloads so that people can run those workloads.
We do provide, I call it t-shirt size clusters that people can use. Small, medium, large,
extra, large, extra, extra, extra large, that type of thing so that people can work with it.
And then once they're ready to start to settle on what they really need from a feature set
perspective, they need to be able to put that somewhere securely. They need to be able to put
that somewhere so it can be worked with and honed in on. So, we provide abstractions that we build
for streaming data to our data lake. And by mean abstractions, I want to make it easy,
I want to make it so that they can just dump data. Basically, I don't think that's the actual
method call, but it's similar and dump that data. So, they're working within our ecosystem,
they're working in an notebook. We provide reusable components that they can work
with in each of these. There's a workflow engine in there, of course, for running like a
DAG-based workflows. And then they've got their data. Same mechanisms or same entry points,
they're going to start training models. So, there's reusable components for that. It is not as
easy as just using the same configurations, of course, for every model. Everything is nuanced.
So, we provide customized ability, but we provide base capabilities that people can work from on
that. And then, of course, once they have their model trained, there's a thing along the way,
there's hyper parameter optimization, they're going to run. All this is infrastructure that needs
to be spun off and worked with, ideally, in a way that our data scientists aren't even aware
that it's happening other than spin up time, then they need to, of course, package and deploy
the model. And that's a whole other process that is kicked off and is very controlled. But
those are kind of like the building blocks, I guess. And there's a lot under the hood there. There's
there's a maybe 30, 50 different Kubernetes clusters that we're working with to enable a lot of
these users, very large-scale AWS infrastructure under the hood, all of which, ideally, nobody will
know about. Through those layers of abstraction. And to what degree does Qflow provide the various
functionality that you're exploring or have you had to plug things into Qflow for the various,
you know, user-facing elements. You mentioned like training models,
experiment management, pushing data, different places. These things that you're kind of using
the primitives that the Qflow is providing, or are you using more specialized packages in
that Qflow environment? So we use some capabilities in Qflow, like their notebook provisioner,
we use the capabilities on hyperparameter optimization, so that cotid embeddings.
We use a lot of the visualization capabilities for Qflow pipelines. And the Qflow pipelines
are go abstraction. We use our go. And yeah, Qflow pipelines is basically an abstraction built
on top of our go workflow. Now there is movement, of course, to potentially add other workflow
engines in there, but we're using our go. But then there are other aspects of what Qflow provides
that we do not enable. So there are some of the experiment tracking stuff, artifacts store,
some other capabilities there that we would rather control, and we would rather route data to our
backend systems. So we have other libraries and custom built capabilities there that are either
abstracted away, so you don't even need to know about it. So like I'm gathering metadata on you,
and you're on your model, like you shouldn't have to tell me what you're doing, or I should be
able to correct that, get that myself. And then there are other capabilities that come shipped with
Qflow, but we will swap them out. So like we run our own spark abstractions, or our own
dashed abstractions, that kind of thing. And technically you could use what Qflow comes with, but
we need, we want it to be a little different. We need it to be either a little easier to use,
or we want it to scale better, or something like that. So it's a combination.
And you've referenced scaling and distributed compute quite a bit thus far. I'd love to have
you talk a little bit more about that, particularly from the perspective of the challenges that you've
run into, you know, building and managing that kind of environment. Sure. And this is one of the
harder parts of what we are enabling. And one of the more fun parts probably, just to fix.
So, I mean, spark and dusk are fairly standard, just generally, I think, from a running
distributed compute perspective. They both can kill a Kubernetes cluster.
And I can even, and as can as can Argo, actually, by the way.
And what does that mean? Kill the Kubernetes cluster, right? Because you kill a node and it's
supposed to pop back up. Right. But if you kill the control plane, nothing pops back up.
So yeah, we've had situations with both. So I would say there's a couple of things that you just
have to consider when running any multi-tenant platform like this. One is you're running a multi-tenant
platform. There are a bunch of people working on this platform. And in this case, in a user-interactive
mode, in a Jupyter Notebook, potentially in a different UI, but they're working. This is their
job. This is where they work. So you have to be careful. You don't want to let people run
very, very large-scale stuff that could kill the cluster. I'll answer what I meant, what your
question about. What does it mean kill the cluster in just a second? So you just have to be careful.
What I mean by kill the cluster is Spark, for example, when you shut down a Spark cluster,
the actual Spark destroy hits the Kubernetes, can hit the Kubernetes API server all at once.
It doesn't cascade. So if you run a thousand node Spark cluster, it's a thousand concurrent
requests against the Kubernetes API server. If you don't, message floods blowing up the
controller. Your DDoSIC itself. Yeah. Yeah. Yeah. So there are ways to make that not happen.
There are ways to use different schedulers and whatnot. But at the end of the day,
if you DDoS your API server, there's a chance you're not going to recover the cluster.
There are aspects of the Kubernetes control plane that are a little fragile.
But on the other side, in some other cases, we've seen DASC, for example, basically CRUSH
at CD. The CD is the key value store that Kubernetes uses on the backend. DASC has CRUSHed.
I actually just discussed this the other day. I think at 800 nodes, if a certain configuration,
at CD fell over. And once at CD is down, you're done. You're very, very hard to recover.
So that's what I mean by kill a cluster. It can happen. So we segregate workloads.
We actually, we dubbed the Jet Train cluster because we let people run as fast as they
wanted. We have since killed that cluster. So we're coming up with a new name. But basically,
you segregate workloads. And you ensure that your users don't expect those things to stick around.
We want people to push the boundaries. We want people to run the level of scale they need to do
their job. So it's my job to make sure they don't notice when something falls over.
So we segregate and we make it so that if you have to try again, it's as quick as possible.
But yeah, it's challenging at scale. Like I said before, it all comes down to this fundamental
three things, CPU memory and disk. With so many levels of abstraction on top of those things,
you don't always know which ones falling over, but something falls over. And so it's just
understanding the nature of your workloads and routing things to the right place. I know
I won't say, of course, the names of the models, but I know certain models that require a very
large amount of compute here. And I know when they need to run very large workloads. So we route them
accordingly. So they don't crush their neighbors. Yeah. Yeah.
You talked about the control plane issues. Kubernetes is also, you know, people have been trying to
run stateful workloads on Kubernetes for a long time. A lot of people recommend against it
as well. Like, have you run into issues with the stateful nature of machine learning workloads
and the kind of inherently stateless nature of Kubernetes? Yes, we have. And some of it is,
I think, users being used to being users who are used to the like dumping everything to HDFS
on a persistent like EMR cluster in AWS, for example, versus not doing that. Yes, we have. So
what we've had to balance is speed of nodes spin up and the ability to scale massively with
the ease of dumping to essentially persistent scratch space, or the ease of being able to work
in a stateful manner. And our teams who are running very large distributed capabilities seem
willing to make that trade off. So dumping like snapshots to S3, which is a little slower,
but allows you to recover, you know, if they're failures, still it works. And yes, there are
capabilities you can deploy for intra cluster storage, MINIO, being one of them. I think there are
others out there as well. But we actually haven't, we haven't gone with any of them yet. We haven't
started using them. So, so yeah, I think, I think for us, it was just a trade off, but you're right,
it is, it is an interesting choice for the entire industry to settle on Kubernetes, which is an
inherently stateless execution engine when we are working with inherently stateful
workloads. It is an interesting quandary. You spoke previously about the range of user
personas, let's say that you need to support some folks, you know, their approach to that is
to, you know, for those folks that prefer working in Jupiter notebooks to like try to turn the
Jupiter notebook into this executable, deployable kind of artifact, other folks, you know,
kind of say, hey, we're going to teach all our data scientists how to use, you know, Git and
you know, QCTO and whatever those tools are, low level tools, like how have you managed,
you know, those kinds of decisions and what approach do you take there?
So, I'll just say first, first hand, you have to use Git, you have to use source control,
no matter where you fall on that, on that paradigm or on that range.
And to be clear, I think for some, it's less about, you know, using it or not, but about like
abstracting it so that, you know, every time a notebook is checkpointed, it's automatically
creating a, it's automatically committing and the user doesn't know anything about it.
Are you, are you specifically saying that your users have to use it and are manually committing
stuff? That's a, a non-negotiable in that environment? Yes, yes, I believe I am saying that.
So, and the reason being, you're making me think, actually. So, the reason being that
we need to be able to recreate everything we do, right? Look, we need to be able to explain
how we got from this idea to this model score. And in order to do that, we need to be able to
rebuild this model from scratch. Now, you can snapshot, you know, you can use these notebooks
as first class citizens in, in execution and let, and you can, you know, paper mill, there's
a couple other projects out there. Yeah. And they chose to basically say, well,
data scientists want to work in notebooks. Great. Let's treat this as a first class citizen,
as a really stable, operationally executable body of work. And, and we are, I would say,
we are currently thinking about that. Right now, we, we do not have anything like that in place.
We do allow people to, of course, work in juvenile books. Use these for experiments.
We will leave them running for a fairly long period of time. I don't really like the idea of
this thing being permanent. But, but we will let them, you know, keep them up and come back in,
start working the next day. But at the end of the day, as it is today, we, we do not think of
these things as operationally executable. That may change in the future. We do ask our data
scientists to, to use source control to do some level of local, you know, development work.
Not a, not a ton. You can absolutely work on a single instance compute node with the
Jupyter notebook and have a serialized model object at the end of your process and have a team
do something with that quote unquote. That, that, that is a, that is a way you can work. But,
we, we, we have found that people graduate from that and need more capabilities.
And when you only work in a Jupyter notebook, it is hard to enforce
rigor in code that goes into like building those models. So, we, we have, we're, it's a balance.
It's a balance, right? I don't want to turn my data scientists into software engineers.
Definitely don't want to have to do that, right? I want them to do data science work.
But, I also want to be able to recreate the model they're building. And I want to be able to
have another data scientist come in and work on that model and, and, and be able to, you know,
get clone the code and, and, and, and start working. So, it's a balance.
Early on, you refer, and in fact, a couple of times in this conversation, you referred back to kind of
the, the, these three foundational constraints that ultimately you're trying to deal with,
you know, compute memory disk. Can you, are there, are there kind of a, you know, set of guiding
principles that you apply? Like, when I think about those, you know, those are, you know,
longstanding constraints, you know, computational constraints, you're coming from a software
engineering, kind of a traditional software engineering approach. You know, we've talked a little bit
about how, how that perspective is different from folks that come at it from more of the model
perspective. I'm wondering, you may be abstracting a little bit, are, are there software engineering
principles that, kind of guide the way you approach these problems? It's, it's a little bit of a
joke, but I actually almost reverse, reverse the principle in this concept, in this situation.
A lot of people think that compute is unlimited, right? Because we, we work in the public cloud,
like everybody, everybody read, like compute is infinite in the cloud. So, one of the things we,
we try and do is make sure that people understand that it is not. It is very scalable, but it is not
unlimited. So, so, when you think about, like, software engineering fundamentals, and, and this
is not necessarily applied to data science work, but it's more applied to the productionization of
what you're building and, and, and some of the actual building of those platforms. Testing is a
really big part of the basics here, right? If even, even for data science work, like unit tests
are not a bad idea to write, for, for, you know, some of the, the logic that's being built.
So, testing is a really big thing that we always come back to, and building just solid
build processes and, like standards, implement, like standards enforcement on this stuff.
So, really down to, like, the basics of the SDLC life cycle. A lot of what I think of as my job
is actually to just be a very annoying person for my engineers. Like, I, I, you know, I ask about
how, like, okay, where are your tests? Okay, great, okay. How, how, how are we focusing on our
code quality? Like, like, like, never, these conversations never stop, right? And, and I, I really,
I really focus a lot on those basics on, on, you know, microservices based, like, abstractions,
right? Like, are we doing too much in one unit? Right? Like, just some of these, like,
fundamental principles, we don't really ever get away from them. So, so we focus a lot on that,
even in, like, developing components, for example, like, for, like, reusable components for
executing within, like, a machine learning, like a DAG for running machine learning model training.
Like, there's a discussion going on actually around how much do you put into a reusable component,
because there's overhead in, in splitting between the components. So, so, like, thinking about that
from a microservices based architecture perspective and, and focusing on just those building blocks.
That's, that's me being annoying, basically, with my engineers. Like, we're, we're not focusing on,
on the high-level stuff, we're focusing on, like, the basics of software engineering.
I'm a big, I'm, I'm a big proponent of a lot of testing, automated testing in particular.
Yeah, and you referenced, you referenced DevOps earlier, and some of the, I guess,
CICDs, not strictly a DevOps thing, but I figure, like, the, I'm imagining kind of the,
this idea of continuous delivery, continuous integration of that figure,
you're strongly into the way you say some of the, the discipline that you're trying to create there.
It does, it does, and I think there's two angles to it. There's the building of the platform,
and there's the using of the platform. So, within building the platform, it is, as inherent,
in the platform building, as it is in any software project, right? I have,
myriad conversations with my engineers, and with my, like, distinguished engineer folks,
who are kind of guiding things around, you know, how are the nightly builds going? How are we,
you know, like, are we release candidates in the downstream testing from other systems,
the usual stuff, right? And then there is an element of that in the using the platform,
but it shouldn't be as much, right? At the end of the day, for model training, there isn't,
like, I don't really want my data scientists to have to deal with, like, CICD capabilities,
but at the end of the day, they do, they do generate something at the end of their work that is
then an executable, right? So, that, the building of that needs to be
repeatable. You need to re-build that model in a way that is auditable. So, there is a CICD
component there, but we automate it. So, it happens, but, like, a data scientist shouldn't have
to deal with it, but it does happen. There is a auditable build process that is run on a,
a build server somewhere that generates the serialized model object that is actually run in production.
Okay. Awesome. Awesome. Where do you see this all going? Both at Capital One and, you know,
a little bit more broadly, you know, the industry we've, you know, hinted at it a little bit, but
what are the short, medium and long-term kind of directions for your team in platform,
or make Capital One? We can start there. Wow. Short, medium, and long-term. All right. So, I think,
I think that there are a couple areas that we'll be focusing on around building more and more
reusable capabilities. So, I don't really want each data science team dealing with,
I don't know, we'll choose a modeling framework. XGBoost, some running of, like, XGBoost model training,
right? Like, there are capabilities there that I think that can be reusable, and we have reusable
components today, but I think continuing to rebuild those out and expand those. So, for us, that is the case.
Long-term, I do expect all of this to be easier. I think as an industry, this will get easier. I don't
think we will be in this immaturity space forever. I think we will, of course, there's the next thing
that will be immature, but I, you know, if I knew what that was, I'd probably be very wealthy.
So, but, like, I think, as we'll get easier, I think that the ability to
integrate better explainability capabilities into these models, or into the creation of these
models will allow us to accelerate building capabilities faster, but also, like, it'll make us
more comfortable, like, using pre-packaged capabilities, like, pre-packaged software,
like, we shouldn't have to necessarily go in and hack. That's my terrible phrase for doing work,
right? But we shouldn't have to, like, go in there and, like, hack a lot of this stuff. It should
be able to be run in our environment, but it isn't. So, that's where we kind of get all those
very strong DevOps engineers in there, and kind of getting it all working. So, I do think this will
get easier. How is another conversation? Well, maybe if I do know, I'm not going to tell you,
I don't know, like, maybe that's my future career, but no, I don't know how, necessarily,
but I imagine it will get easier. Awesome. Well, Ali, thanks so much for sharing with us a bit
about what you've been up to and the platform you're building, and of course, you're coming from
your perspective as a software engineer. My pleasure. Thanks for having me.
