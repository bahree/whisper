Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast, we're featuring a series of conversations from the Nips conference
in Long Beach, California.
This was my first time at Nips and I had a great time there.
I attended a bunch of talks and of course learned a ton.
I organized an impromptu roundtable on building AI products and I met a bunch of wonderful
people, including some former Twimble Talk guests.
I'll be sharing a bit more about my experiences at Nips via my newsletter, which you should
take a second right now to subscribe to at twimblei.com slash newsletter.
This week through the end of the year, we're running a special listener appreciation contest
to celebrate hitting one million listens on the podcast and to thank you all for being
so awesome.
Tweet to us using the hashtag Twimble1Mill to enter.
Everyone who enters is a winner and we're giving away a bunch of cool Twimble swag and
other mystery prizes.
If you're not on Twitter or want more ways to enter, visit twimblei.com slash twimble1Mill
for the full rundown.
Before we dive in, I'd like to thank our friends over at Intel Nirvana for their sponsorship
of this podcast and our Nips series.
While Intel was very active at Nips with a bunch of workshops, demonstrations and poster
sessions, their big news this time was the first public viewing of the Intel Nirvana
neural network processor or NNP.
The goal of the NNP architecture is to provide the flexibility needed to support deep learning
primitives while making the core hardware components as efficient as possible, giving
neural network designers powerful tools for solving larger and more difficult problems
while minimizing data movement and maximizing data reuse.
To learn more about Intel's AI products group and the Intel Nirvana NNP, visit Intel
Nirvana.com and now on to the show.
All right, everyone.
I am here in Long Beach, California at the Nips conference and I am seated with Juan
Bruna, Assistant Professor at the current Institute at NYU and also at the Center of Data
Science at NYU and Michael Bronstein, who is currently on sabbatical at Harvard.
Juan and Michael, welcome to this week a machine learning and AI.
Thank you.
But why don't we get started by having each of you introduce yourselves and tell us a
little bit about how you got involved in machine learning and artificial intelligence?
Yes, so I did my masters and my PhD in France and actually at the beginning my training
was more on the single processing and applied math and I was not so much into machine learning
up until I started to mutate during my PhD and enter machine learning through the ideas
and tools from single processing.
So that was five years ago and ever since I have been getting more and more interested
in ways and problems in which we can maybe leverage some of the more techniques and ways
of mathematical tools from applied math into some of the current modern problems in
deep learning.
Yeah, that's kind of how I arrived here.
Awesome.
And Michael, so I did my studies in Israel and the Technion and my main background is in
geometry, so different types of geometry, metric geometry, spectral geometry, mostly applications
to computer vision and computer graphics problems and in recent years, well, probably with
like many other people, I wouldn't say everybody, but the following a little bit, the
hype of deep learning, we are trying to apply some of deep learning methods to geometric
data, which comes with many of its challenges that are, there are many similarities and
many dissimilarities from the more traditional Euclidean soul to say data, okay?
And I mentioned you're on sabbatical at Harvard, where are you otherwise affiliated?
So otherwise I explain, I spend most of my time between Switzerland and Israel, full
professor at the University of Lugano in Switzerland, Tel Aviv University in Israel, and I also have
a position as a principal engineer at Intel, perceptual computing.
You sound like a very busy guy, don't tell the way wife.
The two of you delivered a tutorial, was it today, right?
Yes.
You did a tutorial, tell us a little bit about the tutorial you did.
Okay, so the tutorial is called geometric deep learning on graphs and manifolds, okay?
So the, in one sentence, the goal of what the tutorial is about is how can we leverage
the successful techniques that the deep learning developed to process images, text and speech
into data and tasks where you might have, where your input might be a bit more exotic.
For example, proteins, or it could be data from a social network, or it could be data captured
from a, let's say, a Kinect, where you have a bunch of point clouds in, in, in 3D.
And so what the tutorial is trying to do is to say, basically give a picture of our current
understanding of how deep learning models can be used and developed in this regime and
also try to describe some of the future and current open directions.
And Michael, in your introduction, you drew one of the key distinctions here, which is
Euclidean space versus other geometric spaces.
Can you elaborate on that distinction?
Sure.
So basically, the difference between Euclidean and non-Euclidean data, one of the main
differences is that you don't have the possibility of vector space operations.
So in Euclidean space, you can take your sample, your point, and you can add or subtract
two points.
You cannot do the same thing on a graph, for example, you cannot add or subtract two vertices
on a graph.
So basically, you have more general structures, but also less operations that you, you can
do with these structures.
So you need to reinvent many of the building blocks that, basically, such as convolutions
or pudding, or that are commonly used in deep learning architectures when you need
to deal with these data.
So Zohan, why don't you walk us through the general structure of the tutorial?
How did you set the context?
How did you get started?
Well, I mean, historically, yeah, so there's a little clique of researchers that we started
to look into this problem a few years ago, maybe like three, four years ago.
So that was at the time where I was a postdoc in Yalekunz Lab in New York, like at the
time of the postdoc.
So there we, and together with Arthur Slam, who is another organizer of the tutorial,
we came up with a very simple, I would say, first model that was trying to set up the
techniques that later on we realized, and Michael and his group also helped us understand
that they were far too naive, right, that there were a lot of things that could be improved
upon this first idea.
So I guess that since the very beginning, we kind of saw that there was a very interesting
exchange of ideas, and also leveraging the fact that we came from slightly different backgrounds,
right?
Michael's group, they have a lot of expertise in geometry and understanding manifolds,
and we came up with an expertise maybe where we had been working hands-on with convolutional
neural networks and some of the ideas that I also worked under in my PhD that involve
more the harmonic analysis of this convolution neural networks.
And so the tutorial came quite naturally.
We have been also collaborating in a variety of different projects, yeah, I mean, that's
how it came out.
So for us, basically, the paper that Ron mentioned that he also, as a postdoc, was actually
an inspiration, and that's where we started looking into these kind of problems.
So we've been working, of course, on spectral geometry for a long time, and in computer
graphics, geometry, processing, community, these or similar ideas have been around for
a while, but basically combining it with learning and redefining operations, for example,
in the spectral domain, as they did in their influential paper, was, to some extent,
eye-opening, and basically we took it from there.
So we rolled together a review paper that appeared in a triple-esignal processing magazine
just this summer, that for us, it was also doing some homework.
We discovered many works in other communities that existed for quite some time, or maybe
we're almost forgotten, or not given sufficient attention.
And maybe even in different communities, people calling the same things with different
names, or to some extent even, I wouldn't dare say reinventing approaches just without
probably being aware of what is done in other communities.
So that never happens.
Well, of course, it always happens, but in this way, because it's new, it's a new field.
We can probably already call it a new field, or a new trend, or a new niche in machine
learning, and because it's new, then basically it's an effort of, there are several seeds
that exist in other domains, and I think now people start getting, maybe, a uniform
picture of what exists, and what's the relation between different methods.
So I think the tutorial is very timely.
So you mentioned spectral analysis.
When I think of that, I think of things like fast freer transforms, and the like, how
does that fit into graphs and manifolds, and non-uclidean spaces, and all these things?
One way you can think about it is that it's like your dictionary, right?
And we should probably even take a step back and explain what an FFT is for a folks who
might not be familiar with that.
Yes, so maybe, if we take a step sufficiently back, we can maybe start with physics, right?
So, so, so, yes, so there's a very fundamental equation in physics that governs how things
oscillate, right?
But like in a guitar, or you know, in a domain, like if you take, you know, like a drum,
and you hit it, there's some, some equation that is going to determine how things are going
to oscillate.
And so the modes of oscillation, as you, as you might suspect, they are related to the
Fourier analysis, right?
So, so the, and the equation that, and the operator that governs this behavior is called
Laplacian, right?
So, so the Laplacian is the, the mathematical, it's a differential operator that in the
occlusion domain might look very inoffensive, right?
It's just the, you take the partial derivatives with respect to all directions, and you just
sum everything.
But it turns out that, that from this operator, you can use this as a vehicle to generalize
things, right?
Because now, if you want to generalize convolutions from the occlusion domain to an
unequally in domain, you will have a hard time, right?
Because they are defined through something that doesn't exist in the nodule domain.
But if you, if you take one step back and they say, okay, maybe I cannot directly use
the convolution, but maybe I can step back and, and use the, this Laplacian operator as
a tool to maybe go from one world to the other.
And as it turns out, the Laplacian is a, is a operator that is intrinsically defined with
very weak assumptions.
In it, what it means is that even on, on a graph or on a manifold, it's an operator that,
that exists, and it's, well, it's, it has a very complete and rich structure and status.
And these, the application of the Laplacian to these different domains, this is all work
that, you know, was done with regard to that domain, independent of what we're trying
to do here with machine learning.
Yeah, absolutely.
I mean, I think that, I mean, I, I would not claim that I know exactly when these things,
but I'm sure that many famous physicists from even like the 90th century were aware of,
you know, the distance and the generality of this operator, right?
And this is way, I think I believe that when people refer to spectrograph theory, it's
most of it, or a big part of it, is in developing the properties of Laplacian and related operators
in graph.
As applied to graphs, yes, okay.
And so, so in that respect, the Fourier transform is now a concept that you can, of course,
if you analyze it and the occlusion domain, it has a very, again, a very rich structure
and it's useful beyond our imagination, right?
It's going to be used to do fast, mostly thanks to the fast Fourier transform.
But it's also an object that exists in general graphs, perhaps not with the only detail
that is like a single detail, but might be very important, is that it doesn't come with
a fast algorithm, right?
That's the, I would say that this is the main, or one of the main technical differences.
So, when I think of the Fourier transform or the FFT, I think of, you have some input
signal and you pass it through this Fourier transformation and it basically decomposes
it into its frequency parts.
How does that apply to a graph?
What does that even mean?
So, on the graph, basically, in the Euclidean case, you can, as you said, you can represent
a function or a signal as a superposition of signs or co-science, right?
Basically, some harmonic functions.
So, these functions turn to be eigenfunctions or eigenvectors of the Euclidean operator,
in the Euclidean case and the one-dimensional case, it's just the second order derivative.
So, basically, if you replace this Euclidean operation with a non-Euclidean operation, with
a graph operation, or in a manifold, that would be what is called differential geobotid,
a plus-biltrami operator, basically, you get exactly the same thing.
You get eigenfunctions of this operator that turn to be an orthogonal basis, basically,
it's a self-adjoint operator.
So, it has orthogonal eigen decomposition.
The eigenvalues play a role of frequencies that are exactly, as Ron mentioned, vibration
modes of, yeah, basically, the eigenvalues that you obtain in the heliports or the spatial
part of the wave equation that governs vibration of a domain.
Ron, you were saying, no, it's just that maybe one way where you can maybe picture this
thing in your head is, yeah, if you take like a spherical plate that would correspond
to a drum, as we understand it, right, just a drum, and then you could imagine like deforming
somehow the drum and just playing it, you would, intrinsically, you would still be playing
things that can be seen as a superposition of like fundamental waves that would look
a bit more funny, right, that would probably adapt, and they would be like the formation
of the original science and coscience, depending on how you have to form the original drums.
Ron, interesting.
This is interesting stuff.
It's bringing me back to my DSP class and I don't think eigenvalues and eigenvectors
has really come up much on the podcast either, but we're not going to go into the linear algebra.
We'll assume a bunch of that.
So basically just to kind of, to catch us up, this is all kind of background to the tutorial.
Maybe let's take a moment to talk about applications of this to make it a little bit more concrete
for everyone.
How was the, some of the stuff that you're doing applied?
So maybe, yeah, we can illustrate a couple of very different applications.
So one that I'm personally involved in is Article Physics.
So the sort of experiments that people do in a large collider and like in particle accelerators
where they're there, the goal of the game is to say very precise things about how the
standard model, like some very specific properties of the standard model.
So the way it works is that you do like a very, an experiment where you clash two particles
certain, you know, with very, very large speed and then they produce what they call a jet.
And a jet is like a shower of little events that can be detected through a very expensive
and very sophisticated detector, like looks like a cylinder.
So and the goal of the game is to say, well, given this observation that looks like this
point cloud in my detector, how can I infer what was the underlying physical theory, right?
And so it's really a machine learning and their machine learning is a very, very fundamental
step for this experimental physicist.
And so there you use this techniques that we described in the tutorial to essentially
learn a data driven model that looks as input a set of like a point cloud in model in the
color limiter and tries to predict if it was due to theory A or theory B.
Before we go to the other application, when I think about when I visualize this, I'm
visualizing, you know, something that, you know, is very well within the bounds of Euclidean
stuff, right?
It's a three-dimensional point cloud, like why do we need to apply the stuff that you've
done as opposed to the usual stuff?
Very good question.
So the answer is that you're not forced to apply what you do.
So if you decide to stay in the Euclidean route, you would have to somehow quantify your
measurements, such that they look like a regular grid.
And so we thought there are two potential risks.
The first one is that if the precision that you need, if the little blobs that the particles
create are very small, you might need to sample at, you know, you will need a sampling rate
that will make inputs incredibly large, right?
That you will basically be paying a huge price in order to transform your input into something
that is not regular grid.
Maybe another way to put that is in order to solve this in Euclidean space, you have
like a quantization noise that you have to sample more than otherwise if you were to use
a different.
So that's a trade-off, right, between, so once you choose a quantization step, there's
a trade-off between how large and how sparse your input is going to look like versus how
much resolution you are going to lose.
So in that respect, the models that we propose here, they are an alternative that don't have
this limitation, right?
That can stay at the native, let's say they don't lose any information because we don't
need to quantize, we don't need to go into this regular grid.
And I would say that there's another potential advantage is that when you look at the experiment
that comes from a detector, as if it was an image, right, in a regular grid, the underlying
assumption is that the statistics, you know, the statistics of the input, they are following
the same assumption as the statistics of natural images, namely that everything is stationary
and that there's a, I would say, a canonical way to compare, like, to measure distance between
points, right?
Whereas in physicists, they have a very good and sophisticated notions of how one should
be comparing particles.
Okay, so there's a, in other words, there's a more physics-aware version of a neural network
that, I mean, another way to say it is that for physicists, it's very important to have
a model where you can infuse and you can incorporate, like, prior information about how things
should be compared into the model.
And so the graph formulation that we are working on is allowing you to incorporate this thing
into the model.
Whereas if you use just the quantization and the standard convolution of your network,
it doesn't seem so easy to incorporate and to accommodate for this.
Okay.
So when I go from my picture of the three-dimensional Euclidean for this application to maybe
something that's a little bit less traditional, I think, of, like, maybe looking at your
points in some kind of spherical coordinate system.
Is that kind of, is that what you're doing or is it, because we keep coming back to graph
and I'm trying to wrap my head around where graph.
Yes.
So here, the graph enters the game as just like a data structure that you use to relate
particles.
Okay, so I think that your input is discreet.
It contains a number of particles that can be variable, depending on each event.
And what you learn is a network that learns how to relate, how to relate and propagate
information across the set of particles.
And so it doesn't see it into this extrinsic Euclidean space.
So of course, you could say, well, you know, it's just a, I mean, yeah, I can see my input
as being n particles.
I can also see it as, you know, as you say, just put everything in the sphere and then
I just see it as a function, like it's an image in that sphere, right?
And then the number, the notion that I have in particles completely disappears, right?
And then you can connect everything with everything.
I would say that it's not that there's one approach that is systematically better than
the other.
It's really, there's context in which you might want to use one formulation versus the
other.
So what I see here is that we are just providing another tool that now our practitioners
can use and maybe then they can combine with the other one.
And we also have reasons to believe that in some contexts, one formulation might scale
better than the other.
For example, if you're having a moment of four dimensions, I had a moment of, you know,
like eight dimensions, well, in our case, the architectural changes to the model model
would be minimal, right?
There's nothing in our scaling that depends on the, like it depends very quickly on the
dimension of the input.
Whereas if you had to do the quantization on the domain, you would pay a huge price for
this small change.
Interesting.
So you're going to give a second example.
I can give a second example or Michael can give an example as you prefer.
Michael, why don't you give one?
Yeah.
So I can give you as an example, a paper that actually will be presenting tomorrow here
at NIPS.
Okay.
And the example is recommended systems.
So, you know, probably the most famous example is the Netflix problem, you know, the
Netflix is a movie rental company and they have probably tens of millions of users and
probably hundreds of thousands of different movies.
So the users can give different scores to movies, whether they like the movie or not,
let's say, on a scale between zero and nine.
So basically yet, you can describe this information as a huge matrix that is very sparse
example because, of course, even if you watch continuously the movies throughout your entire
lifetime, you'll probably watch just a small percentage of what they have.
So they want to feel in the missing entries of these huge metrics, basically they try
to interpolate it.
And the standard approach is user algebraic techniques, basically try to fit a low-dimensional
model to these data, minimizing the matrix rank or more correctly, basically the convex
proxy of the rank, the so-called nuclear norm.
So this problem formation doesn't have any geometric structure, for example.
You can shuffle the columns and the rows of the matrix.
If you remove one of the columns, there are infinitely many ways you can fill it in.
So if you have some either side information that you can construct it from the data, some
notion of affinity between users or items or actually both, that you can represent as
a graph, for example.
So think of a social network and maybe a little bit naive model that friends have similar
tastes.
So this already allows you to think of the matrix as some kind of space with geometric
structure.
You can talk about notions like smoothness, so basically you can say that the values in
these metrics vary significantly or insignificantly when you go from one vertex on a graph to
another vertex on a graph, and you can actually learn optimal filters, spectral filters.
For example, on these graphs, actually it's a product of two graphs, so you can, the best
analogy from single processing will be a two-dimensional free-attent form, like free-attent form and
image.
Okay, two-dimensional free-attent form.
Yeah.
So think of, and the free-attent form of an image, right?
You apply free-attent form to the columns and rows of the image.
So here instead of Euclidean structured rows and columns have matrix that leaves on two
different graphs.
Right, so obviously the rows for example represent items or movies, and the columns represent
users.
So these are two different graphs, and you can apply filters in this frequency domain
that is now basically characterized by the eigenvalues of the two operations of the rows
and the column and the column graphs.
And basically this way you can have a better way of feeling in the missing elements of
the matrix that accounts for the similarities between different users and movies.
How specifically does the frequency domain, the Fourier transform, tie into the graph itself
and in this context?
So it's very similar to what João described before, basically the spectral definition
of convolution, right?
You can do a convolution in the spectral domain, right?
The classical convolution, this is what we call the conversion theorem in single processing
that you can implement filtering or convolution in the frequency domain just as a product,
point-wise product of free-attent forms, right?
And this is what we usually do in standard single processing because we can efficiently
compute the free-attent form using F50.
And you can do the same thing for images, right?
You can do two-dimensional filters in the frequency domain.
So basically extend this analogy to a matrix data.
Basically you can think of it as an image, but the domains where the rows and the columns
of these image leave, they have graph structure, sort of standard Euclidean structure.
If someone has a problem where they've got some set or sets of entities that have some
inherent structure, one relative to the other, as opposed to some of the more traditional
image, an image or other data types, that's an area where they could be looking at an
approach like this and it might give them some advantage.
Absolutely.
So you mentioned a setting that is now approached and defined through different names, so now
there's something that is becoming very popular recently called a meta-learning, right?
Where is this idea that maybe rather than using a traditional supervised learning, where
I have very few number of samples and then very few number of labels and then the environment
is non-stationary, so it changes all again and again.
Maybe what I can learn is a rule that by looking at how maybe the inputs relate to each other,
even if the individual distribution of the labels and the images change, maybe the underlying
relationship between labels and images that I need to learn is something that I can leverage
by exploring more and more data.
So once you start setting up problems using this formulation, you end up with a task where
you have to learn how to relate different things, very different objects.
And so this is a terrain where it's very natural to look at our models, so we have a recent
paper that is currently under review where we think about what's called a few-shot learning
problem using this model, using a graphical network.
And so what is interesting is that somehow it generalizes and it includes, as particular
cases, some of the models that people have been using and developing in the recent year
or so to attack this problem.
And so this is just to say that there are many tasks that you could imagine across AI
and across sciences, the underlying thing that you need to learn is how to relate objects
to each other.
And so once you have to do this relational task, then the natural data structure for that
is a graph, just a point cloud.
So I expect that they will see more and more applications of these technology in the
near future.
So one of the things that this conversation brings to mind is a recent conversation I
had.
In fact, here it nips with Sri Ram Natharajan, who studies statistical relational AI.
Are you familiar with that line of research and how that relates to this?
Not really.
Okay.
Just check it out.
Similar thinking he's also looking at graph-based approaches and applying them to the healthcare
domain and other domains.
So in the case of the, you know, any of the examples we've talked about, I can, you know,
we've already talked about some of the advantages of this approach over traditional approaches.
One of them is that in the case of the particle physics work, it's, you know, maybe more intuitive
for the physicists because it maps more closely to the tools and the way they're used to thinking
about the domain.
What are the other advantages of this approach?
Are there performance advantages in terms of either computational or model accuracy or
things like that?
Yeah.
So what I would say that the main advantage is definitely the fact that it's more general
so you can, there are problems in which it might be the only, your only choice, right?
That there's no, there's no, including alternative to do so.
I would say that whenever you're, I mean, you could ask the question, well, if I just forget
about the grid structure of an image and just treat it as a graph, right?
And I run my model, this model is it going to do better than the CNN?
Right.
I would say that the answer is no.
Yeah.
That's not really the point, right?
It's not really the point, right?
So I would say that the main strength of, of the model is really to, to respect the
invariance of the data, for example, if you are treating a, you know, if you are just
observing a, you know, a point cloud, you know, that the order in which I give you the
point cloud is completely relevant, right?
So therefore, you're, you're, if you can certify that your model is going to exactly, if
exactly the same output independent of how the, the input are permitted, then you are kind
of respecting that this kind of natural invariant of the data.
So I would say that, that relative to models that are, for example, maybe using sequential
networks, like, for example, Rekord neural networks, here, it could be that, that eventually
these models based on graphs and sets are going to be more sample efficient and maybe
they can get you better performance precisely because they are a bit more tailored to the
data format, like to the input data format.
Maybe a good example would be applications from the domain of graphics and computer vision
and in particular analysis of deformable 3D shapes.
So this is actually, I think it's a good illustration because you can treat such objects in two
different ways.
You can treat them as Euclidean objects, basically, they are things that live in three-dimensional
space, right?
You can apply standard, let's say, convolutional neural networks, maybe, on volumetric representations
of, of these objects, or you can think of them intrinsically from the perspective of differential
geometry, basically, model them as many folds.
And what you gain in this way by resorting to these kind of architectures is, you gain
a deformation invariance.
So basically, your model is by construction invariant to inelastic deformations of the
shape.
And if you're a task, for example, is deformation invariant correspondence or deformation
invariance similarity, it means that you can deal with way less training examples because
you don't need to show to the network all the possible deformations that the shapes can
undergo in order to learn these deformations from examples.
You basically have these invariance built into the model.
And the difference can be very dramatic.
The difference can be in orders of magnitude, less training samples.
So in other words, the approach you're taking to model, because it's more tailored to the
problem domain, it kind of restricts your search space and you don't have to provide examples
for things that wouldn't really exist in the domain itself, but do exist geometrically
in Euclidean space.
Exactly.
Or maybe a better way to say it is that you try to model axiomatically as much as possible
or as much as make sense in your specific problem and everything that cannot be modeled
axiomatically because, of course, there is a limitation to what you can model basically
in handcrafted way, everything that deviates from your model you learn.
Okay.
Other other topics that you covered in tutorial that we haven't touched on yet?
So just very briefly, so one other potential area of application that we are currently exploring
to what extent, if you now have a language to learn our graph-based structure, you can
use it for a combinatorial optimization problems that are naturally defined over graphs.
So this is a completely different domain of application, because there the goal is not
so much to solve a task that you don't know how to solve, is more to solve or to approximate
a task faster.
We're talking like traveling salesmen and these kinds of graphs.
Exactly.
Exactly.
So we briefly touched upon one of such problems in the tutorial, which is the quadratic
assignment problem.
But what?
The quadratic assignment, which contains the travel statement problem is that you can think
it as a particular case of that one.
So there the general setup is really an instant of this trend that you can always have this
analogy between an algorithm to solve a task and a neural network.
And this analogy works by looking at the algorithm and then unraveling typically the algorithm
involved a series of iterative steps, iterations, so you can just see these iterations as being
different layers of a network.
And then once you have this analogy, then you can try to study like a trade-offs between
a computation and accuracy, by just replacing the guarantees that the algorithm gives you
by just a data driven approach where you just feed the parameters of the network to a
dataset of solved problems.
So this is an interesting and potentially also useful because in many domains, especially
when it comes to combinatorial optimization, there are problems in which it's still an
open research area, how to come up with a vision approximation of intractable problems.
So here, one of the potential uses of what we presented is, well, now we are providing
a family of, let's say, trainable architectures that can be used to guide and to provide good
trade-offs between accuracy and complexity for problems such as the travel experience,
salesman, or other interesting things.
Interesting.
So the paraphrase is that what you've done is your research is kind of providing a way
to express graph-oriented problems in terms of neural networks, traveling salesmen, map
coloring, all these other combinatorial kind of graph problems.
They're typically very difficult to solve exactly and so there are all kinds of approximations
and heuristics, but for a certain level of complexity, those don't work very well.
So now, your research applied to them gives you a way to solve these using neural networks.
Well, yes.
So I would not say potentially.
Potentially.
Potentially, exactly.
It's a question mark and I think it's a question mark that it's worth exploring, right?
That's correct.
Sure.
Because in some applications, it could be useful that, for example, not just have a single
algorithm with a single aristic, but to have a toggle that you can select between how
many cycles do you want to spend versus how much accurate do you want the solution to
be, right?
And be able to learn adaptive trade-offs and all these things are interesting.
Then there's another declination of this area of research that is a bit more going into
the theoretical computer science, namely to what extent the models that we learn could
be interpreted as algorithms that we still don't know.
This might be, it could well be that it doesn't work because it relies on this fact that
can we interpret or can we uncover what the neural network is learning?
And we know that this is typically a hard thing to do, right?
Even for a convolutional neural network, we don't really know how the network figures
out how to solve the problem.
But what I'm saying is that in some context, it could be interesting to try to understand
and analyze kind of things that the network learned.
So we have these graph problems.
There are graph problems in computer science as well.
Your research allows us to express those as neural networks if we could then peer into
the neural network that might give us some insight into these computer science problems
that we're trying to model in a first place.
Yes.
Interesting.
Interesting.
How about implementation?
Like I'm imagining, like at this point in time, you can't just do, you know, TensorFlow
TF.
You know, graph solver and do this.
How does that work?
So to some extent, we are trying to leverage existing tools not to reinvent the wheel.
Basically, the underlying framework is a standard one.
We use TensorFlow, for example.
We just create some custom things that then boil down again to some standard operations
like matrix multiplication.
So basically the short answer is yes, it is that easy.
It is built on top of some standard frameworks.
Okay.
Some minor but potentially profound differences in the fact that the scaling up, like using
these models on large graphs or large domains involves, you know, matrix multiplication
with matrix that are large.
And so the structure that we have is that these matrices are sparse.
And so hopefully we see more and more integration of sparse linear algebra into PyTorge and
TensorFlow, etc. but there's a fundamental difference now that maybe the hardware, like
GPUs, they are excelling at a specific form of operation that is not very friendly with
sparse matrix multiplication.
Okay.
But again, I'm not an expert in this low-level implementation, but this, I would say,
is one of the main differences between, you know, running a complement or running a
graph convolution.
So you've released some code that works on their TensorFlow, but it isn't necessarily
amenable to scaling up just yet.
There's stuff that needs to be figured out.
Maybe a one way to get a sense for the complexity of this has anyone like beyond the, you know,
the two of you and folks that are like deep in this research use this.
Are you aware of any like external arms length applications?
So people, different people from different communities try to use.
I wouldn't say that it's extremely popular yet, but it probably, it starts to become.
So many different domains, many different applications can be, the problems in these domains
can be formulated using graphs, graphs at the end are very generic and very convenient
representation of any kind of relations or interactions you can think of.
So that's really, very generic framework of describing certain types of data.
So yeah, people that are even not experts in machine learning that come from different
domains that bring certain applications, they try to basically, they see that graphs allow
to formulate their problems in a natural way and they are curious to try out these approaches.
Okay.
Great.
Great.
This is really exciting stuff.
Where can folks go to learn more about it, download the TensorFlow code or read some
of the papers?
So we have a dedicated website that is easy to remember, it's geometricdiplearning.com.
geometricdiplearning.com also.
Where, yeah, where I think the idea is to have all the tutorial material.
We have the review paper that Michael mentioned.
We have also the recent literature by not just us, but other groups that use the tools.
And then we are also going to have in two months, we're going to have an IPAM workshop here
in Los Angeles, where I think I'm also looking forward to it because it's what you are
saying that people from different domains and people from different disciplines will
come together and essentially present their data problem or their information.
And what I'm expecting is that we are going to see more and more this realization that
actually the models and the tools can be used across more domains than maybe we are expecting.
Okay.
Awesome.
Great.
Well, Joanne, Michael, thank you so much for taking the time to chat.
I enjoy the conversation.
Thank you very much.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
To follow along with the NIP series, visit twimmelai.com slash NIPS 2017.
To enter our Twimmel 1 Mill contest, visit twimmelai.com slash Twimmel 1 Mill.
Of course, we'd be delighted to hear from you either via a comment on the show notes page
or via a tweet to add Twimmelai or add Sam Charrington.
Thanks once again to Intel Nirvana for their sponsorship of this series.
To learn more about the Intel Nirvana NNP and the other things Intel's been up to in
the AI arena, visit intelnervana.com.
As I mentioned a few weeks back, this will be our final series of shows for the year.
So take your time and take it all in and get caught up on any of the old pods you've been
saving up.
Happy holidays and happy new year.
See you in 2018.
And of course, thanks once again for listening and catch you next time.
