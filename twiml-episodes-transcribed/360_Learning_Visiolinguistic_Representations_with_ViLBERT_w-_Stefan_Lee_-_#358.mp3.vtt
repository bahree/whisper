WEBVTT

00:00.000 --> 00:13.400
Welcome to the Tumel AI Podcast.

00:13.400 --> 00:16.240
I'm your host Sam Charrington.

00:16.240 --> 00:24.480
Hey, what's up everyone?

00:24.480 --> 00:28.820
Unless you're Jared Lido or on the cast of Big Brother Germany, you've undoubtedly seen

00:28.820 --> 00:32.300
your world shift dramatically over the last few weeks.

00:32.300 --> 00:36.060
For most of us, this is simply uncharted territory.

00:36.060 --> 00:41.340
And unfortunately, we aren't quite able to see the finish line just yet.

00:41.340 --> 00:47.220
Now more than ever before is the time for us all to lean into our various communities.

00:47.220 --> 00:52.740
We encourage you to take this opportunity to connect with your fellow human, albeit virtually.

00:52.740 --> 00:57.820
And please know that the Tumel community, which is over 3,500 strong on Slack, is here

00:57.820 --> 01:00.580
for you, and we welcome you to join us.

01:00.580 --> 01:05.020
Take this chance to join a study group, work on a Kaggle competition, share your thoughts

01:05.020 --> 01:06.540
on a paper you're reading.

01:06.540 --> 01:10.700
We'd love to hear what you're working on and offer our support.

01:10.700 --> 01:13.900
For those who might just need a friendly hello, I'm here for that too.

01:13.900 --> 01:17.980
If you're so inclined, feel free to reach out via Twitter, our Slack channel, or just

01:17.980 --> 01:21.580
shoot me an email to Sam at Tumel AI dot com.

01:21.580 --> 01:25.260
Please be safe and take care of your health and that of those around you.

01:25.260 --> 01:28.260
Peace and love, and on to the show.

01:28.260 --> 01:30.060
All right, everyone.

01:30.060 --> 01:34.580
I am here at NURPS 2019, and I am with Steffen Lee.

01:34.580 --> 01:39.460
Steffen is an assistant professor at Oregon State in the School of Electrical Engineering

01:39.460 --> 01:40.780
and Computer Science.

01:40.780 --> 01:42.700
Steffen, welcome to the Tumel AI podcast.

01:42.700 --> 01:44.500
Yeah, thanks for inviting me.

01:44.500 --> 01:45.500
Absolutely.

01:45.500 --> 01:50.460
Let's talk a little bit about your background before we dive into some of the many things

01:50.460 --> 01:52.740
that you're presenting here at the conference.

01:52.740 --> 01:53.740
Sure, sure.

01:53.740 --> 01:59.100
So I did my PhD at Indiana University, and most of my work there was sort of on the core

01:59.100 --> 02:00.420
computer vision side.

02:00.420 --> 02:05.060
So how do I use computer vision to help scientists do various tasks?

02:05.060 --> 02:09.060
So a lot of it was replacing what would otherwise be human labeling tasks.

02:09.060 --> 02:10.060
Okay.

02:10.060 --> 02:14.700
I get bored somewhat quickly, so in my postdoc I extended out to vision and language,

02:14.700 --> 02:21.060
so I'm thinking about problems where an agent has to reason about visual input and linguistic

02:21.060 --> 02:22.060
input.

02:22.060 --> 02:26.860
And not only understand the visual content, but it has to express that understanding by

02:26.860 --> 02:30.260
creating language or responding to language, things like that.

02:30.260 --> 02:33.780
And is visual question answering one of the tasks that is interesting to you?

02:33.780 --> 02:34.780
Yep.

02:34.780 --> 02:37.780
So visual question answering, I've got a number of pieces of work on that topic.

02:37.780 --> 02:38.780
Okay.

02:38.780 --> 02:42.820
There's also things like captioning or doing visual dialogues from multi-round Q&A style

02:42.820 --> 02:43.820
dialogues.

02:43.820 --> 02:44.820
Okay.

02:44.820 --> 02:45.820
Cool.

02:45.820 --> 02:48.700
And then lately I've been extending out to tasks that not only have vision language,

02:48.700 --> 02:50.220
but also some form of action.

02:50.220 --> 02:55.620
So these are vision and language tasks situated in embodied context, where an agent tests

02:55.620 --> 03:00.060
to see and talk and move to accomplish some sort of task.

03:00.060 --> 03:04.060
And are the agents that we're referring to here, are they simulated agents?

03:04.060 --> 03:09.020
Uh, so mostly they're simulated agents, but some recent work has sort of extended out

03:09.020 --> 03:12.460
of the simulator into physical platforms with some surprising success.

03:12.460 --> 03:13.460
Okay.

03:13.460 --> 03:18.500
What's an example of a platform that you're using for simulation or for the robotic lab?

03:18.500 --> 03:22.860
For the robotics on the robotic side, is they, uh, what's the kind of dimensionality,

03:22.860 --> 03:24.060
how complex are they?

03:24.060 --> 03:25.060
Yeah.

03:25.060 --> 03:29.260
So most of the work we've been working on so far has been on this pie robot platform, which

03:29.260 --> 03:34.700
is something that Facebook has recently released, which is this wonderful low-cost, robotic

03:34.700 --> 03:37.260
platform, a low co-bot, I think they call it.

03:37.260 --> 03:38.260
Okay.

03:38.260 --> 03:40.500
That has a really nice interface for machine learning practitioners.

03:40.500 --> 03:41.500
Oh, really?

03:41.500 --> 03:42.660
So I have not come across that yet.

03:42.660 --> 03:43.660
It's worth looking at.

03:43.660 --> 03:47.820
So it's, you know, you can say from pie robot, import robot, and then robot, go forward

03:47.820 --> 03:48.820
one meter.

03:48.820 --> 03:52.500
And that's sort of the level of the interface, so it's a real machine learning persons robot

03:52.500 --> 03:54.500
for things like navigation and grasping.

03:54.500 --> 04:00.220
So here at the conference, you've got a number of presentations that you're, and posters

04:00.220 --> 04:07.740
you're involved in, uh, we'll talk about, uh, one of them in most detailed billboard, uh,

04:07.740 --> 04:10.740
but what are some of the others that you've been focusing on?

04:10.740 --> 04:11.740
Sure.

04:11.740 --> 04:15.220
So this year I'm presenting Ember things that you said, uh, one of them is one of these

04:15.220 --> 04:16.220
embodied tasks.

04:16.220 --> 04:17.220
Mm-hmm.

04:17.220 --> 04:22.580
We work on what's called vision and language navigation, where an agent is sort of spawn

04:22.580 --> 04:26.740
in a never-before-seen environment, and given a natural language and navigation instruction.

04:26.740 --> 04:31.660
So it was something like go down the hall, turn left at the wolf head, and stop on the

04:31.660 --> 04:35.340
third bedroom, the one with the yellow bed spread or something like this.

04:35.340 --> 04:40.940
So instructions are this mix of trajectory clues, like go forward and turn left, and, and

04:40.940 --> 04:45.060
visual grounding landmarks, and then the goal is to have an agent that can reason about

04:45.060 --> 04:48.380
all this while actually following that path in a simulated, uh, world.

04:48.380 --> 04:49.380
So I am.

04:49.380 --> 04:53.700
And what are the, uh, priors that the agent's bringing into this environment?

04:53.700 --> 04:58.500
Does it already know what a wolf head is, or is it need to figure that out from training?

04:58.500 --> 05:02.820
So it's a tricky question, um, a lot of vision and language, and we'll touch on this in

05:02.820 --> 05:07.940
Villebert, uh, sort of starts from a set of pre-trained image features, and a set of

05:07.940 --> 05:12.220
pre-trained language features, uh, what it doesn't have is a sense of grounding.

05:12.220 --> 05:15.900
So it may be able to represent a wolf head visually, and it may understand the word

05:15.900 --> 05:20.940
wolf head, but the connection between sort of the visual incarnation of a wolf head and

05:20.940 --> 05:22.700
then the term, um, isn't there.

05:22.700 --> 05:27.300
And that you expect to learn during the specific task, um, though in Villebert, the point

05:27.300 --> 05:30.500
is that we're trying to pre-trained grounding itself.

05:30.500 --> 05:33.060
And so you've got the agent's paper.

05:33.060 --> 05:34.060
Yep.

05:34.060 --> 05:37.780
And then there's Villebert, and then I'm also giving a talk at a workshop on emergent

05:37.780 --> 05:39.100
communication.

05:39.100 --> 05:45.540
So if you have agents that interact to perform some task, and you want them to share

05:45.540 --> 05:50.300
information between each other or communicate, uh, how could you make that communication

05:50.300 --> 05:51.900
protocol more interpretable to humans?

05:51.900 --> 05:52.900
Mm-hmm.

05:52.900 --> 05:55.460
And one way to do that is to make them actually use sort of discrete symbols.

05:55.460 --> 05:59.460
So something that looks like a word, uh, more or less, that doesn't necessarily have

05:59.460 --> 06:00.460
that meaning.

06:00.460 --> 06:05.100
And then the talk is about how do we make these, uh, these communication protocols more

06:05.100 --> 06:06.100
interpretable?

06:06.100 --> 06:09.540
You know, this whole field was kind of, I don't know, if popularized as the right word

06:09.540 --> 06:16.940
or, uh, villainized a few years ago with the Facebook, uh, those two agents that were

06:16.940 --> 06:20.420
said to have developed their own coded language.

06:20.420 --> 06:21.420
Yeah.

06:21.420 --> 06:24.860
Uh, similar kind of vein of, of research here.

06:24.860 --> 06:28.420
Um, yeah, except for the fact that we, we want to understand what the code is.

06:28.420 --> 06:29.420
Right.

06:29.420 --> 06:30.420
Um, yeah.

06:30.420 --> 06:35.100
Um, yeah, that was this, uh, dealer, no, dealer paper, um, right.

06:35.100 --> 06:37.100
I had some similar, similar goals.

06:37.100 --> 06:38.100
Yeah.

06:38.100 --> 06:39.100
Yeah.

06:39.100 --> 06:45.780
Um, and so it's, in, in a sense, kind of merging the field or the desire for explainability

06:45.780 --> 06:48.780
into this emergent communication work.

06:48.780 --> 06:49.780
Yeah.

06:49.780 --> 06:50.780
Yeah.

06:50.780 --> 06:54.060
And it, um, if you're thinking about building an agent that eventually will work with

06:54.060 --> 06:57.180
a human, uh, communication ends up being a big part of that.

06:57.180 --> 06:58.180
Right.

06:58.180 --> 06:59.180
That's how we organize ourselves.

06:59.180 --> 07:01.900
So making sure that we can understand what the agent is saying and that the agent can

07:01.900 --> 07:04.980
understand what we're saying is sort of one of the goals of my research.

07:04.980 --> 07:11.580
But not constrained necessarily by English allowing the agent to come up with its own stuff.

07:11.580 --> 07:16.460
If we can map it back to some space that has human-like structure, that would be fine.

07:16.460 --> 07:17.460
Right.

07:17.460 --> 07:22.620
Um, so rather than having a unique, unique word for the concept of red ball would prefer

07:22.620 --> 07:26.500
the agent to have a word for red that modifies a word for ball.

07:26.500 --> 07:30.260
So even if it's not English, it's something we can map back to a way that we understand,

07:30.260 --> 07:32.300
uh, how language works.

07:32.300 --> 07:35.380
And so, Wilbert, uh, what's Wilbert all about?

07:35.380 --> 07:36.380
Yeah.

07:36.380 --> 07:42.220
So I hinted on this a little bit ago, but Wilbert's about learning the associations between

07:42.220 --> 07:48.580
the visual incarnation of a concept and the linguistic incarnation of a concept.

07:48.580 --> 07:54.660
And this is sort of tricky because most people instantly hallucinate the visual part

07:54.660 --> 07:58.660
of something whenever they hear the other word when it's when I said wolf head, I assume

07:58.660 --> 08:01.100
a lot of people immediately thought of like Game of Thrones type of things.

08:01.100 --> 08:04.380
Um, and there was some visual concept that got brought to mind.

08:04.380 --> 08:07.580
But the machines don't by did it sort of automatically have this, right?

08:07.580 --> 08:11.140
You can, I mean, you can learn language in just in a linguistic context.

08:11.140 --> 08:14.140
And in fact, that's what most of national language processing does.

08:14.140 --> 08:15.140
Yeah.

08:15.140 --> 08:17.940
Is just learn it, it's an association with other words.

08:17.940 --> 08:18.940
Uh-huh.

08:18.940 --> 08:21.500
Likewise, on the visual side, you're just sort of learning to represent some sparse set

08:21.500 --> 08:27.900
of classes and those classes often relate to specific nouns, but they don't have a sense

08:27.900 --> 08:29.220
of closeness, right?

08:29.220 --> 08:33.380
So there's no idea that the feature for a cat should be close to the feature for a tiger

08:33.380 --> 08:37.460
because they're related linguistically or in certain taxonomies.

08:37.460 --> 08:42.020
So the point of Villebert is to try to learn these associations between vision and language

08:42.020 --> 08:43.020
directly.

08:43.020 --> 08:46.060
And this is something we'll usually call a visual grounding of a word.

08:46.060 --> 08:47.060
Okay.

08:47.060 --> 08:53.260
And so what's the, the general approach there, presumably it involves, uh, Burt, transformer

08:53.260 --> 08:54.260
models.

08:54.260 --> 08:55.260
Yeah.

08:55.260 --> 08:56.260
The name is not as well.

08:56.260 --> 09:01.220
Um, yeah, so if you, this, if you look at the Burt models and some of the big successes

09:01.220 --> 09:05.020
in an LP, it's these large self-supervised tasks.

09:05.020 --> 09:11.380
So they take a large language corpus and they learn certain little things to build supervision

09:11.380 --> 09:12.700
from unlabeled data.

09:12.700 --> 09:16.580
So they'll, you know, they'll mask out a few words and have them reproduct it based

09:16.580 --> 09:20.740
on the other linguistic context or their last of a sentence follows another sentence

09:20.740 --> 09:21.740
in text.

09:21.740 --> 09:25.820
And what we do is we, we find analogs for that in the vision and language space.

09:25.820 --> 09:26.820
Okay.

09:26.820 --> 09:31.740
So there's this data set called conceptual captions that came out recently, which is this

09:31.740 --> 09:32.740
massive data set.

09:32.740 --> 09:36.180
I think it's about in the order of about three million, uh, image text pairs.

09:36.180 --> 09:37.180
Oh, wow.

09:37.180 --> 09:41.140
Where they just found images online that had alt text.

09:41.140 --> 09:44.500
So some human had provided some alternative text for people, usually for people with visual

09:44.500 --> 09:45.500
impairment.

09:45.500 --> 09:47.540
You might interact with this by mousing over an image.

09:47.540 --> 09:48.540
Sure.

09:48.540 --> 09:49.620
And it produces some, some text tag.

09:49.620 --> 09:53.660
Uh, they did some processing on top of that, but it's this sort of Webly supervised data

09:53.660 --> 09:55.140
where they scraped all this down.

09:55.140 --> 10:03.540
You know, the images filtered for kind of simple, simple kind of image net types of depictions

10:03.540 --> 10:06.140
or they just raw whatever's out there on the web.

10:06.140 --> 10:09.940
They're across the board from everything from, you know, flicker style images to pictures

10:09.940 --> 10:12.860
of maps and things like this.

10:12.860 --> 10:16.820
So it's pretty, pretty broad, but that's sort of where we start as our data source and

10:16.820 --> 10:22.260
we perform similar self-supervised trainings where we're masking out certain image parts

10:22.260 --> 10:26.420
of the image, just at random, and then asking it to reconstruct those given the rest of

10:26.420 --> 10:28.700
the image and the language.

10:28.700 --> 10:33.220
And likewise, we're asking, does this sentence match with this image or not or masking out

10:33.220 --> 10:36.460
parts of the language and having it reconstruct from the image and the text?

10:36.460 --> 10:41.820
So we're designing this sort of self-supervised multi-modal task with this large, weekly supervised

10:41.820 --> 10:42.820
data source.

10:42.820 --> 10:48.740
And what we get at the end is a model that has built some representations that bridge between

10:48.740 --> 10:52.940
vision and language and then we fine tune that for a wide variety of other tasks.

10:52.940 --> 11:00.900
Now randomly masking out parts of the image sounds like a very blunt instrument to apply.

11:00.900 --> 11:02.940
Yeah, it is.

11:02.940 --> 11:09.100
So part of it is we don't know which parts of the image are being described by the associated

11:09.100 --> 11:10.660
alt text.

11:10.660 --> 11:15.580
So it's not clear that we should mask out certain regions versus certain other ones.

11:15.580 --> 11:20.620
So yeah, it is a blunt instrument, but I think if we were going to be more precise, we

11:20.620 --> 11:22.380
would already need to know the grounding.

11:22.380 --> 11:23.860
And that's exactly what we're trying to learn.

11:23.860 --> 11:28.060
Yeah, I think the picture that came to mind was doing some kind of object detection in

11:28.060 --> 11:32.740
the image and then masking out one or more of the objects.

11:32.740 --> 11:39.500
It kind of almost predicates the image net kind of image that I initially asked about

11:39.500 --> 11:41.660
as opposed to more natural scenes.

11:41.660 --> 11:45.260
Well, not only does it predicate that, it's also a good question.

11:45.260 --> 11:50.940
How many objects can we detect until recently it was something the biggest data set for

11:50.940 --> 11:53.540
detection was something like 80 or 100 classes.

11:53.540 --> 11:57.180
I think open images brought it up to one or 2,000.

11:57.180 --> 12:01.860
But if you actually look at the richness of text, we use all sorts of visual words all

12:01.860 --> 12:02.860
the time.

12:02.860 --> 12:05.460
Even if you're looking around where we're recording right now, there's lots of objects

12:05.460 --> 12:08.220
that you know a word for.

12:08.220 --> 12:13.340
You could describe, but they would never show up in a detection challenge.

12:13.340 --> 12:16.420
So the question is, how do we build groundings for those things?

12:16.420 --> 12:19.220
Not just for things we can already detect, because if we can already detect them, we sort

12:19.220 --> 12:21.180
of how do you have the grounding?

12:21.180 --> 12:24.980
Talk a little bit more about how you've kind of adapted the process for training a

12:24.980 --> 12:29.260
BERT-like model, transformer model, to incorporate this additional information.

12:29.260 --> 12:30.420
Yeah, absolutely.

12:30.420 --> 12:36.020
So one of the sort of core parts of the BERT model is that it treats this sequence of

12:36.020 --> 12:42.100
words, a sentence, sort of as a set of independent inputs and then does a bunch of self-attention

12:42.100 --> 12:44.100
in between to process this.

12:44.100 --> 12:49.460
And the actual position of a word in that sequence is just denoted by some positional embedding.

12:49.460 --> 12:55.260
So it's really just a set of words with some sequentiality added as a feature representation.

12:55.260 --> 12:56.900
And you could think about an image the same way.

12:56.900 --> 13:01.940
You could think about an image as a set of bounding boxes, for instance, of just various regions

13:01.940 --> 13:06.980
in the image that have been pulled out and some associated visual feature and a positional

13:06.980 --> 13:07.980
encoding.

13:07.980 --> 13:12.740
And this box out from over here, so it's just a set, it's an unordered set, in fact.

13:12.740 --> 13:18.260
So the actual input API for BERT looks the same in our model for the visual side and the

13:18.260 --> 13:19.260
linguistic side.

13:19.260 --> 13:24.540
As we're just assuming we have a set of features with some sort of encoding put in.

13:24.540 --> 13:28.820
And then we're sort of directly copying over a number of the self-supervised losses that

13:28.820 --> 13:30.220
BERT is actually designed for.

13:30.220 --> 13:31.220
So masking out parts.

13:31.220 --> 13:35.700
The image we're asking about alignment between the text and the image.

13:35.700 --> 13:39.660
In BERT, it's a sentence and the next sentence, and you're predicting whether one comes

13:39.660 --> 13:40.660
after the other.

13:40.660 --> 13:43.940
But in our case, it's an image and a sentence, and we're asking, does this align or not?

13:43.940 --> 13:48.140
Is this actually a pair from conceptual options?

13:48.140 --> 13:54.220
And then sort of if you're asking about more internal mechanisms, we rely on a co-attentional

13:54.220 --> 13:57.780
transformer, which is something we introduce here, but it's been introduced by a number

13:57.780 --> 14:02.780
of people simultaneously, because it's sort of intuitive idea, that rather than doing

14:02.780 --> 14:08.340
self-attention, you're attending one modality based on queries from the other modality.

14:08.340 --> 14:12.660
So you're using the language to attend over the image, and then you're taking that sort

14:12.660 --> 14:17.620
of pooled image feature back into the language to inform the rest of that computation.

14:17.620 --> 14:22.620
And we have those sort of co-attentional layers periodically through this structure.

14:22.620 --> 14:30.380
The result is kind of a jointly trained model that is kind of trained on the relationship.

14:30.380 --> 14:33.620
It's not you've kind of trained on some visual stuff, and then some language stuff, and

14:33.620 --> 14:36.700
you found a way to kind of merge them together.

14:36.700 --> 14:40.460
Yeah, I mean, I think that's a pretty fair assessment.

14:40.460 --> 14:43.620
We aren't training the vision from scratch, so we are sort of starting with some frozen

14:43.620 --> 14:48.500
representation, but we are allowing it to learn the association between the two together.

14:48.500 --> 14:52.820
But it's a very dynamic back and forth process, so it's not just like a direct assignment.

14:52.820 --> 14:55.380
So what's the frozen representation that you're starting from?

14:55.380 --> 14:59.860
We're starting from a faster RCNN trained on visual genome, so it's just sort of a standard

14:59.860 --> 15:01.780
practice in the vision and language space.

15:01.780 --> 15:06.140
Okay, and so how is that incorporated into the virtual model?

15:06.140 --> 15:09.620
So that's the thing that actually outputs these bounding boxes.

15:09.620 --> 15:10.620
Okay.

15:10.620 --> 15:13.940
So it's sort of like your object detection idea, except for that we are not stopping at

15:13.940 --> 15:15.700
the classes it actually predicts.

15:15.700 --> 15:18.820
We're looking for the associations more broadly.

15:18.820 --> 15:25.660
Okay, so the bounding boxes aren't random random there driven by this.

15:25.660 --> 15:27.660
They're driven by the faster RCN model.

15:27.660 --> 15:30.060
You can see arbitrary, more so than random perhaps.

15:30.060 --> 15:32.300
Or not even arbitrary, but there is picking out.

15:32.300 --> 15:33.620
You can also sample from them.

15:33.620 --> 15:37.060
So the faster RCN model can produce quite a lot of bounding boxes, and you can sample

15:37.060 --> 15:40.460
from it if you'd like to increase the randomness there.

15:40.460 --> 15:44.100
If you actually look at some of the bounding boxes that come out, many of them aren't really

15:44.100 --> 15:49.980
object aligned, because if you look at the visual genome data, it's actually fairly noisy.

15:49.980 --> 15:52.860
Some of it's very clean, some of it's very noisy.

15:52.860 --> 15:56.220
So sometimes you'll have, for instance, an object is like a road very often or something

15:56.220 --> 15:57.220
like this.

15:57.220 --> 16:01.500
It doesn't do a great job of honing in on specific objects.

16:01.500 --> 16:06.980
Where does Wilbert fit into kind of the broader scope of your research?

16:06.980 --> 16:08.580
What do you want it to take you?

16:08.580 --> 16:12.780
Is it a means or an end or a little bit of both?

16:12.780 --> 16:15.100
Yeah, I mean, so it's both.

16:15.100 --> 16:16.100
Right.

16:16.100 --> 16:21.500
So there's a fundamental question of how do we align language and vision?

16:21.500 --> 16:25.940
And that's the sort of interesting research question, and then why would we want to do

16:25.940 --> 16:28.860
that is perhaps a useful follow-up.

16:28.860 --> 16:31.780
And there's a number of things that are sort of concrete applications.

16:31.780 --> 16:36.420
If you could align vision and text really well, you can train models to do things like

16:36.420 --> 16:40.740
VQA or Visual Question Answering, which can help people to visual impairment ask about

16:40.740 --> 16:45.060
the world around them and get answers back, or you could do image captioning to provide

16:45.060 --> 16:50.220
descriptions of what someone would be seeing if they didn't have the same visual impairment.

16:50.220 --> 16:51.220
Yeah.

16:51.220 --> 16:59.620
So the ideas that someone could take this Wilbert model off the shelf and kind of VALA transfer

16:59.620 --> 17:05.420
learning, just drop it into their application, and what would the inputs be and what are

17:05.420 --> 17:06.420
the outputs?

17:06.420 --> 17:07.420
Yeah.

17:07.420 --> 17:08.420
Yeah.

17:08.420 --> 17:12.740
So in our case, the inputs are sort of image features that they're looking at, and then

17:12.740 --> 17:14.700
some sort of text query.

17:14.700 --> 17:20.500
So we transfer image features, meaning the bounding boxes extra from some model.

17:20.500 --> 17:26.940
So, since the Wilbert paper, we've applied it to something on the order of 12 different

17:26.940 --> 17:29.620
vision and language tasks.

17:29.620 --> 17:36.060
So you can pre-train this and then use it as a base to perform fairly well, fairly quickly

17:36.060 --> 17:40.140
on a wide range of these visual and language reasoning tasks.

17:40.140 --> 17:43.580
So the inputs, some bounding boxes, and some text.

17:43.580 --> 17:48.220
So let's say I was doing question answering, and I would take my image, and I would extract

17:48.220 --> 17:51.740
these bounding boxes and feed it in, and then I would feed my question as the text.

17:51.740 --> 17:56.860
And I can train output that just predicts some set of answers, and that trains quickly

17:56.860 --> 18:02.020
and trains well, rather than training it from scratch on this status set.

18:02.020 --> 18:03.780
And so you can train it.

18:03.780 --> 18:08.580
You can kind of fine tune for specific task types.

18:08.580 --> 18:14.260
Presumably you can also fine tune it with specific types of data or kind of data language

18:14.260 --> 18:15.260
relationships.

18:15.260 --> 18:16.260
Yeah.

18:16.260 --> 18:17.260
How might that work?

18:17.260 --> 18:21.300
So this is actually what comes up often in these tasks, right?

18:21.300 --> 18:25.820
So Wilbert's pre-trained on this huge set from conceptual captions that covers like we

18:25.820 --> 18:28.020
talked about earlier, quite a few things.

18:28.020 --> 18:33.140
But if you go to something like VQA, which is based off Coco, there's 80 major object categories

18:33.140 --> 18:37.940
and the images are all things people were proud enough to put on Flickr, so that they

18:37.940 --> 18:39.540
could eventually get downloaded.

18:39.540 --> 18:42.780
So that's a much more constrained image set.

18:42.780 --> 18:47.500
But when you're fine-tuning, you're fine-tuning the whole of Wilbert, and you are feeding in

18:47.500 --> 18:49.580
those images and they search for a question.

18:49.580 --> 18:50.580
So it can fine-tune.

18:50.580 --> 18:51.580
Got it.

18:51.580 --> 18:52.580
So you're kind of doing both at the same time.

18:52.580 --> 18:59.540
You're fine-tuning the visual data set, but also the, you know, for the specific task

18:59.540 --> 19:00.540
that you're asking it to do.

19:00.540 --> 19:01.540
Yep.

19:01.540 --> 19:02.540
That's exactly right.

19:02.540 --> 19:03.540
Okay.

19:03.540 --> 19:04.540
Interesting.

19:04.540 --> 19:05.540
Interesting.

19:05.540 --> 19:11.060
And so kind of going back to the previous question, you know, where does this lead do

19:11.060 --> 19:17.900
you think from the broader perspective of kind of visual, kind of this integration between

19:17.900 --> 19:20.780
visual and language tasks?

19:20.780 --> 19:21.780
Yeah.

19:21.780 --> 19:28.980
So I think the grounding problem itself could use sort of more attention.

19:28.980 --> 19:33.620
We've been very tasked-driven as a community, and we're learning sort of myopic groundings,

19:33.620 --> 19:34.620
right?

19:34.620 --> 19:39.300
So if I train a model for VQA, it understands what Coco images look like, and it understands

19:39.300 --> 19:43.820
what questions are, or likewise if I do a captioning model.

19:43.820 --> 19:45.860
Sometimes this is even more shocking.

19:45.860 --> 19:50.620
You've trained a model to caption Coco images, and Coco images have a really small set

19:50.620 --> 19:51.620
of classes.

19:51.620 --> 19:55.860
For instance, one example that I like to show from a recent paper is they don't have any

19:55.860 --> 19:57.660
guns.

19:57.660 --> 20:01.860
So if you've fed this caption, we're trained on Coco with an image of a man in a red

20:01.860 --> 20:05.300
hat with a red shirt holding a shotgun.

20:05.300 --> 20:08.860
And the caption is a man in a red hat and a red shirt holding a baseball bat.

20:08.860 --> 20:09.860
Okay.

20:09.860 --> 20:13.420
Because he's wearing what looks like a baseball uniform, and he's got something in his

20:13.420 --> 20:14.420
hands.

20:14.420 --> 20:16.020
Might as well be a baseball bat.

20:16.020 --> 20:20.980
And if we talk back to these potential applications of helping people with visual impairment, that

20:20.980 --> 20:23.220
kind of mistake doesn't seem justifiable.

20:23.220 --> 20:24.220
Yeah.

20:24.220 --> 20:30.100
There was a tweet going around just yesterday of the day before where someone passed a

20:30.100 --> 20:38.540
chart into a captioning system, and it was like a chart with like three, you know, three

20:38.540 --> 20:44.220
trend lines or three graph, three data sets that were kind of diverging.

20:44.220 --> 20:48.340
And it was like something with two scissors.

20:48.340 --> 20:49.340
Yeah.

20:49.340 --> 20:50.340
Yeah.

20:50.340 --> 20:52.860
So there's that whole side of things.

20:52.860 --> 20:53.860
Right.

20:53.860 --> 20:57.500
And there's this other paper that I recently did with a co-author Peter Anderson called

20:57.500 --> 21:02.500
No Caps, which is a novel object captioning at scale, and it addresses these sort of problems.

21:02.500 --> 21:07.620
Where how do I caption objects that maybe I've never heard talked about before?

21:07.620 --> 21:12.020
So I don't have linguistic data about shotguns, for instance, but I might have an object

21:12.020 --> 21:16.740
detector that can tell me that that object exists, and then how do I incorporate that into

21:16.740 --> 21:18.740
language descriptions?

21:18.740 --> 21:20.260
Tell me more about that process.

21:20.260 --> 21:21.260
Sure.

21:21.260 --> 21:22.260
How does that work?

21:22.260 --> 21:23.260
How does that work?

21:23.260 --> 21:25.620
So that's one of the open questions.

21:25.620 --> 21:30.460
So we presented a data set and ran some initial baselines from techniques that had sort

21:30.460 --> 21:33.180
of already been floating around on this.

21:33.180 --> 21:37.180
Did you also propose a model that performs well on the task?

21:37.180 --> 21:38.180
No.

21:38.180 --> 21:39.180
Not in that paper.

21:39.180 --> 21:40.180
Okay.

21:40.180 --> 21:41.180
So, not yet.

21:41.180 --> 21:42.180
Coming soon.

21:42.180 --> 21:43.180
Potentially.

21:43.180 --> 21:52.420
But the way it typically works is the model will consider regions in the image as potential

21:52.420 --> 21:55.020
words that it could use in its output.

21:55.020 --> 21:59.180
So even if it doesn't really know what the right language for that specific part of the

21:59.180 --> 22:03.660
image is, it might recognize that that's an important thing to talk about and want to

22:03.660 --> 22:08.020
try to put it into the caption wholesale, and then you rely on the object detector to

22:08.020 --> 22:09.020
tell you what that's...

22:09.020 --> 22:13.900
Maybe an action in a red hat with some object that I can't identify that seems important.

22:13.900 --> 22:19.540
A man with a red hat holding a that, where that is the image region that contains the

22:19.540 --> 22:20.860
gun in this case.

22:20.860 --> 22:24.620
And then you rely on object detectors to tell you, or image classifiers, to tell you what

22:24.620 --> 22:29.740
that image patch actually was, just some subset of already defined kind of things.

22:29.740 --> 22:34.100
So basically, it's saying, if I already know how to use language to describe the world,

22:34.100 --> 22:40.060
but I don't know specific objects, it seems silly that I should have to go collect new

22:40.060 --> 22:43.420
descriptions that contain that object.

22:43.420 --> 22:47.060
You know how to talk, but you may not know what a widget is, but if I point you to a widget,

22:47.060 --> 22:50.700
you can then talk about the widget, you just need that first grounding.

22:50.700 --> 22:53.540
So this line of work is really about given that grounding.

22:53.540 --> 22:56.820
How do you incorporate it into language models or how do you actually talk?

22:56.820 --> 23:02.140
And it kind of, it likes what, likewise, seems silly to pick some other random object.

23:02.140 --> 23:07.980
You know, it probably isn't this thing and stick it in this place.

23:07.980 --> 23:09.980
Yeah.

23:09.980 --> 23:13.580
Confidence estimation is an open problem in this space.

23:13.580 --> 23:21.260
I mean, does the model know it's not a baseball bat, or does it really think it is?

23:21.260 --> 23:25.580
It's something that needs to be explored, where we're going to actually deploy these things.

23:25.580 --> 23:32.020
His other thoughts on kind of interesting challenges in this visual language domain or

23:32.020 --> 23:33.820
the grounding problem in particular.

23:33.820 --> 23:38.180
I mean, I'll use the question as an opportunity to talk about some other recent work that I'm

23:38.180 --> 23:39.180
excited about.

23:39.180 --> 23:40.180
Yeah.

23:40.180 --> 23:45.900
So, for a long time, in the community, grounding has been on static images.

23:45.900 --> 23:51.460
But there's actually a lot of concepts that rely on motion, that rely on interaction

23:51.460 --> 23:52.460
to ground.

23:52.460 --> 23:56.020
I don't know if I could, I could give you a photo and you could tell me it looks like

23:56.020 --> 24:00.660
people are talking, but they could just be sitting there quietly as well, right?

24:00.660 --> 24:04.420
It's interaction that actually sort of tells you what's going on.

24:04.420 --> 24:11.460
So lately, I've been moving, are we primarily in this or definitionally talking about kind

24:11.460 --> 24:18.540
of verb concepts as opposed to noun concepts or is it independent of that particular distinction?

24:18.540 --> 24:24.100
I think that's a fine generality, I think that's a fine way to understand it in general.

24:24.100 --> 24:26.820
Though some verb concepts can be learned from static images.

24:26.820 --> 24:27.820
Sure.

24:27.820 --> 24:28.820
So it's like jumping, right?

24:28.820 --> 24:31.900
We sort of defies gravity, so we know what they're roughly doing.

24:31.900 --> 24:32.900
Yeah.

24:32.900 --> 24:34.780
They'll actually jumping versus falling.

24:34.780 --> 24:38.500
That nuance doesn't really get captured very easily for a lot of images.

24:38.500 --> 24:39.500
Yeah.

24:39.500 --> 24:44.060
So, I've been interested in settings where agents have to demonstrate their visual and

24:44.060 --> 24:49.420
linguistic understanding in simulated environments by acting, this was sort of like the VLN stuff

24:49.420 --> 24:52.300
I talked about earlier, the first work.

24:52.300 --> 24:55.700
But there's also things like if I drop an agent in an unseen environment and I ask them

24:55.700 --> 25:02.020
a question, what color is the car, how does an agent navigate, see the car, know that

25:02.020 --> 25:06.740
it now has enough information to answer, and then offer up an answer in return.

25:06.740 --> 25:10.100
So these sort of problems are things that I'm increasingly interested in.

25:10.100 --> 25:16.780
Is the problem there as you've framed it up, passing a model of sequence of images and

25:16.780 --> 25:25.500
having it identify either caption or identify an action or something else, or is it more

25:25.500 --> 25:30.980
the lot of things that you describe seem different, like the agent navigating the environment

25:30.980 --> 25:31.980
etc.

25:31.980 --> 25:36.260
What you described sounds a lot like what image, video captioning or video question answering.

25:36.260 --> 25:37.260
Yeah.

25:37.260 --> 25:43.860
But I'm much more interested in sort of situations with this active perception, so embodiment

25:43.860 --> 25:44.860
of some kind.

25:44.860 --> 25:49.300
So when I ask what color is the car, if the agent starts in a bedroom, it needs to understand

25:49.300 --> 25:53.460
the cars who are outside, it needs to get into the hallway because it knows hallways tend

25:53.460 --> 25:54.460
to lead outside doors, right?

25:54.460 --> 25:58.100
It shouldn't go to the bathroom attached to the bedroom, there's no outside doors there.

25:58.100 --> 25:59.100
Right.

25:59.100 --> 26:03.940
So it has to start reasoning about structural priors in the world and navigating according

26:03.940 --> 26:07.740
to those to actually decide and output an answer in the end.

26:07.740 --> 26:13.820
So it's more about controlling the agent towards a goal that's specified in natural language.

26:13.820 --> 26:19.580
And do you have existing data sets that will allow you to get there or is that part of the

26:19.580 --> 26:21.820
challenge is coming up with new data sets?

26:21.820 --> 26:22.820
Yeah.

26:22.820 --> 26:27.900
So I think data sets are obviously a big driver of progress in machine learning a couple

26:27.900 --> 26:31.940
about a year ago, the team in Georgia Tech, where I was working, output this embodied

26:31.940 --> 26:37.660
question answering project, which revolved around this, and then there's also the vision

26:37.660 --> 26:42.980
and language navigation task from Peter Anderson, which relies on sort of falling instructions

26:42.980 --> 26:45.460
in environments again by controlling the agent.

26:45.460 --> 26:49.100
So it's recent, but people are producing these sort of things.

26:49.100 --> 26:50.380
Anything else you want to cover?

26:50.380 --> 26:54.500
No, I think that's fine for right now, if you have more questions, I'm happy to answer

26:54.500 --> 26:55.500
more.

26:55.500 --> 27:02.540
So we're keeping our eyes peeled, we'll be keeping our eyes peeled for your upcoming publications.

27:02.540 --> 27:05.220
Thanks so much, Stefan, for taking the time to chat with us.

27:05.220 --> 27:06.220
Yeah, thanks for having me.

27:06.220 --> 27:07.220
All right.

27:07.220 --> 27:08.220
Thanks.

27:08.220 --> 27:13.420
All right, everyone, that's our show for today.

27:13.420 --> 27:17.660
To learn more about today's guest or any of the topics mentioned in this interview, visit

27:17.660 --> 27:19.860
twimmelai.com.

27:19.860 --> 27:24.500
Of course, if you like what you hear in the podcast, please, please, please subscribe,

27:24.500 --> 27:28.380
rate, and review the show on your favorite pod catcher.

27:28.380 --> 27:56.900
Thanks so much for listening, and catch you next time.

