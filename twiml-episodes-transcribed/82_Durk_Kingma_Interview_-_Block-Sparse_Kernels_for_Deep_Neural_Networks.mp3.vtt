WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people, doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:26.240
I'm your host Sam Charrington, we've got a special treat for you with this show.

00:26.240 --> 00:31.960
This episode is actually part of the OpenAI series of shows we ran last week, but because

00:31.960 --> 00:37.280
it features hot off the presses research, we weren't able to release it with those shows,

00:37.280 --> 00:39.720
but your weight is now over.

00:39.720 --> 00:45.280
This episode features an interview with Dirk Kingma, a research scientist at OpenAI.

00:45.280 --> 00:50.440
Although Dirk is probably best known for his pioneering work on variational auto encoders,

00:50.440 --> 00:55.280
he joined me this time to talk through his latest project on block sparse kernels, which

00:55.280 --> 00:58.480
OpenAI just published this week.

00:58.480 --> 01:03.640
Block sparsity is a property of certain neural network representations, and OpenAI's work

01:03.640 --> 01:08.600
on developing block sparse kernels helps make it more computationally efficient to take

01:08.600 --> 01:10.560
advantage of it.

01:10.560 --> 01:14.760
In addition to covering block sparse kernels themselves and the background required to understand

01:14.760 --> 01:18.560
them, we also discuss why they're important and walk through some examples of how they

01:18.560 --> 01:19.560
can be used.

01:19.560 --> 01:25.040
I'm happy to present another fine Nerd Alert show to close out this OpenAI series and

01:25.040 --> 01:27.560
I know you'll enjoy it.

01:27.560 --> 01:32.000
Support for our OpenAI series is brought to you by our friends at Nvidia, a company which

01:32.000 --> 01:35.840
is also a supporter of OpenAI itself.

01:35.840 --> 01:40.040
If you're listening to this podcast, you already know about Nvidia and all the great things

01:40.040 --> 01:43.680
they're doing to support advancements in AI research and practice.

01:43.680 --> 01:49.240
And as you'll hear in this show, Nvidia's DGX1 deep learning supercomputer was instrumental

01:49.240 --> 01:53.520
to Dirk and his team's work on the block sparse kernels project.

01:53.520 --> 01:58.320
If you happen to be at Nips this week, be sure to check out Nvidia's booth at the Expo,

01:58.320 --> 02:02.480
as well as the four accepted papers being presented by their team.

02:02.480 --> 02:09.040
To learn more about the Nvidia presence at Nips, head on over to twomlai.com slash Nvidia.

02:09.040 --> 02:19.240
And now on to the show.

02:19.240 --> 02:25.640
All right everyone, I am on the line with Dirk Kingma. Dirk is a research scientist with

02:25.640 --> 02:26.640
OpenAI.

02:26.640 --> 02:29.440
Dirk, welcome to this week in machine learning and AI.

02:29.440 --> 02:30.440
Thank you very much.

02:30.440 --> 02:31.440
It's my pleasure.

02:31.440 --> 02:32.440
It's great to have you on the show.

02:32.440 --> 02:36.600
I am looking forward to learning a bit about your background and what you're working

02:36.600 --> 02:37.600
on.

02:37.600 --> 02:41.280
Why don't we start with the first of those and have you tell us a little bit about how

02:41.280 --> 02:45.200
you got involved in machine learning and AI research?

02:45.200 --> 02:46.200
Sure.

02:46.200 --> 02:53.520
So, I started, I think, my first research position when I was doing my master's degree.

02:53.520 --> 02:57.600
I emailed Jan Lecun out of the blue with a couple of research ideas.

02:57.600 --> 02:59.400
I had no clue where he was.

02:59.400 --> 03:00.400
He responded.

03:00.400 --> 03:04.840
And, you know, it was welcoming me into his lab for six months.

03:04.840 --> 03:08.240
So I looked up where he was and turned out to be New York.

03:08.240 --> 03:10.240
So I was very happy with that, of course.

03:10.240 --> 03:11.240
I'm happy.

03:11.240 --> 03:12.240
Like 2009.

03:12.240 --> 03:16.840
Was that for, like, a postdoc or not a postdoc, but like a research position or?

03:16.840 --> 03:17.840
Yeah.

03:17.840 --> 03:21.040
So I was very interested in AI.

03:21.040 --> 03:27.160
I did a master's program in Utrecht in the Netherlands and a friend of mine was studying

03:27.160 --> 03:30.800
in Cornell and he got me interested in a couple of topics.

03:30.800 --> 03:37.480
So I spent about a year, you know, reading about the various ideas in the field and quickly

03:37.480 --> 03:42.200
discovered, you know, the Jan Lecun had a lot of interesting research going on.

03:42.200 --> 03:46.720
So I applied for a position in his lab and was accepted happily.

03:46.720 --> 03:53.160
So this was, you know, about three years before the learning resolution started.

03:53.160 --> 03:54.840
And I had a great time there.

03:54.840 --> 03:58.560
So that was, yeah, that was, you know, 2009.

03:58.560 --> 04:00.920
I published a paper with him.

04:00.920 --> 04:05.480
I decided to do a start-up afterwards with a friend of mine in the Netherlands.

04:05.480 --> 04:10.680
We did that for a couple of years and then I decided to go back to research and I spent

04:10.680 --> 04:19.720
again about half a year in Jan Lecun's lab and applied for a PhD position with Maxwelling.

04:19.720 --> 04:25.360
So he's a professor in, we just started and answered them and before he was a professor

04:25.360 --> 04:31.240
at Irvine and my girlfriend at the time was still studying in the Netherlands.

04:31.240 --> 04:34.600
So I, you know, preferred to have a position there.

04:34.600 --> 04:40.280
So I was very lucky to get a position with Maxwelling in 2013.

04:40.280 --> 04:42.360
And I was his first PhD student there.

04:42.360 --> 04:48.440
So basically, when you start as a professor, you get like a free student as your first,

04:48.440 --> 04:49.440
you know, student.

04:49.440 --> 04:51.840
So I was his free student.

04:51.840 --> 04:56.320
And because of that, I was also lucky to not be bound to a particular research direction

04:56.320 --> 04:57.320
or topic.

04:57.320 --> 05:02.440
So I had, you know, relative freedom to pursue my interests from the start.

05:02.440 --> 05:08.320
So what I was interested in was combining probabilistic models with deep learning.

05:08.320 --> 05:13.920
So I thought this was a field that was sort of underappreciated, you know, the combination.

05:13.920 --> 05:20.440
And the first paper I published with Maxwelling was on the variational autoencoder, which

05:20.440 --> 05:30.280
is a paper that sort of introduces a way to combine Bayesian inference in a natural way

05:30.280 --> 05:32.040
with deep learning.

05:32.040 --> 05:39.240
And this method allows you to train generative models in a principled way and a scalable

05:39.240 --> 05:40.240
way.

05:40.240 --> 05:42.760
So it skills you have to large data sets and to large models.

05:42.760 --> 05:45.800
So that's for the key advantages, okay?

05:45.800 --> 05:52.040
And we've talked about variational autoencoders on the podcast a few times, but I'm looking

05:52.040 --> 05:56.200
forward to digging into that topic with you.

05:56.200 --> 05:58.880
What else did you work on there?

05:58.880 --> 06:05.200
Right, so, you know, the variational autoencoder that sparked a wave of various application

06:05.200 --> 06:09.480
papers and extensions also in our lab.

06:09.480 --> 06:16.680
So we worked on trying to get uncertainty estimation work well in deep learning models.

06:16.680 --> 06:22.720
So as you know, most deep learning models all you do is basically learn a single value

06:22.720 --> 06:28.000
of the parameters that need to, you know, good predictions on your test set and your

06:28.000 --> 06:29.000
training set.

06:29.000 --> 06:30.000
Right.

06:30.000 --> 06:34.720
But it doesn't necessarily give you a good sort of estimate of how certain you should be

06:34.720 --> 06:38.600
about your predictions given your limited data.

06:38.600 --> 06:44.640
And the combination with variational inference, a potential way to achieve that.

06:44.640 --> 06:50.680
So deep behavioral variational autoencoders introduced a trick called reprimerization trick,

06:50.680 --> 06:55.760
which was also applicable to estimating a posterior distribution over the parameters,

06:55.760 --> 07:01.880
which is basically gives you a quantitative estimate of the uncertainty over your parameters

07:01.880 --> 07:03.720
given your training data.

07:03.720 --> 07:07.080
So we did some like a paper on that.

07:07.080 --> 07:10.320
And yeah, so that was one extension we did.

07:10.320 --> 07:18.280
And also I, because of the variational autoencoder paper, there was a similar research direction

07:18.280 --> 07:24.400
within the lab of deep mind that happened about at the same time.

07:24.400 --> 07:32.400
I happened to publish my paper a bit earlier, but so they developed independently a very

07:32.400 --> 07:33.400
similar trick.

07:33.400 --> 07:38.320
And because of this coincidence, we started also collaborating on this topic.

07:38.320 --> 07:47.200
So I went to deep mind in 2014 and we collaborated there on applying this method to the problem

07:47.200 --> 07:53.800
of semi-supervised learning, which means that if you want to train a classifier from

07:53.800 --> 08:00.240
training data, but not all your training images are labeled, then you would want to make

08:00.240 --> 08:08.080
use of the unlabeled data to get better predictions than what you would get if you would solely

08:08.080 --> 08:10.720
train on the labeled images, right?

08:10.720 --> 08:11.720
Okay.

08:11.720 --> 08:16.120
Yeah, so we basically proposed a method using variational autoencoder.

08:16.120 --> 08:21.440
And the results that we got at the time were, you know, state of the art, but of course

08:21.440 --> 08:28.440
we're, you know, quickly eclipsed by, you know, a wave of other papers and, you know,

08:28.440 --> 08:30.320
that's how it happens, right?

08:30.320 --> 08:32.160
Yes, that's how it happens.

08:32.160 --> 08:36.880
But I mean, the variational autoencoder effort that come up in so many different contexts,

08:36.880 --> 08:43.840
it seems like, I mean, you mentioned that even in your prior lab, you know, the initial

08:43.840 --> 08:48.800
paper was followed up by a bunch of applications, papers that you worked on, but a lot of folks

08:48.800 --> 08:54.000
have worked on papers using that papers and methods, I should say.

08:54.000 --> 09:00.600
I think, you know, one of the first places I saw it was in the context of, like, generative

09:00.600 --> 09:02.360
models for texts.

09:02.360 --> 09:04.000
Is that a popular, am I collecting that?

09:04.000 --> 09:08.320
Is that a popular place to use variational autoencoders, or maybe it was, there was another

09:08.320 --> 09:16.600
one where someone took a, I think they took a bunch of, like, movie videos or movie clips

09:16.600 --> 09:20.880
or something like that and ran them through a variational autoencoder and tried to, you

09:20.880 --> 09:25.000
know, generate a movie clip, which was kind of interesting as well.

09:25.000 --> 09:31.840
Before we dive into variational autoencoders, we haven't gotten yet to open AI, have we?

09:31.840 --> 09:32.840
Right.

09:32.840 --> 09:42.760
So, in the end of, or I think somewhere in mid 2015, I was approached by Rick Rubman, who

09:42.760 --> 09:48.520
was assembling sort of an initial team for open AI.

09:48.520 --> 09:54.800
And to me, it was very intriguing because, yeah, it matched very well with sort of my own

09:54.800 --> 10:03.080
philosophy that I think it's good to, you know, to publish and to, you know, to write open

10:03.080 --> 10:09.760
source code for your experiments, but also to, you know, put emphasis on, you know, maximizing

10:09.760 --> 10:13.080
the probability of a positive future with AI.

10:13.080 --> 10:14.080
Right.

10:14.080 --> 10:19.080
So, yeah, there is, of course, currently a lot of concentration of talent in the field

10:19.080 --> 10:21.640
at a small number of places.

10:21.640 --> 10:29.200
And I thought it would be good to have more of, you know, to perhaps, you know, safeguard

10:29.200 --> 10:32.800
the, you know, the openness of the field and the collaboration.

10:32.800 --> 10:33.800
Right.

10:33.800 --> 10:39.720
So, yeah, there is, because of the inflow of talent into, you know, commercial labs,

10:39.720 --> 10:46.720
there is a risk that eventually, you know, a lot of these labs will, you close up and

10:46.720 --> 10:52.040
that, you know, the benefits of the field might not be as evenly distributed as we would

10:52.040 --> 10:53.040
like.

10:53.040 --> 10:59.600
So, that is one of the goals of open AI, you know, to make sure that the benefits are distributed

10:59.600 --> 11:01.680
eventually in a fair way.

11:01.680 --> 11:02.680
Yeah.

11:02.680 --> 11:09.480
Also, it was, of course, located in California, which is not a lot of bad place to live.

11:09.480 --> 11:10.480
Not at all.

11:10.480 --> 11:11.480
Not at all.

11:11.480 --> 11:13.920
And you've been at Open AI for how long?

11:13.920 --> 11:19.040
Since the start, basically, so the first few months I was still working from Amsterdam.

11:19.040 --> 11:20.040
Okay.

11:20.040 --> 11:23.320
And then I moved here in 2016.

11:23.320 --> 11:24.320
Okay.

11:24.320 --> 11:25.320
Yep.

11:25.320 --> 11:26.320
Awesome.

11:26.320 --> 11:27.320
Awesome.

11:27.320 --> 11:30.960
Well, one of the things that I wanted to ask you, given that we're recording this the

11:30.960 --> 11:38.360
week before NEPs, and I am going to NEPs, actually for the first time, is I'm assuming

11:38.360 --> 11:42.480
NEPs is like your home conference, so you've been there many times.

11:42.480 --> 11:47.880
I wanted to, you know, get a sense from you both, you know, if there's anything in particular,

11:47.880 --> 11:53.760
you're looking forward to about, you know, this particular conference or, you know, any

11:53.760 --> 11:58.840
tips on approaching NEPs as it's become, you know, quite a large conference.

11:58.840 --> 11:59.840
Yeah.

11:59.840 --> 12:03.440
So, this has indeed become an enormous conference.

12:03.440 --> 12:08.360
As you probably know, the field is still more or less doubling every year.

12:08.360 --> 12:13.440
I think the, the attendance of NEPs, you know, follows a similar trend.

12:13.440 --> 12:17.640
So, you know, personally, for me, it's also a big, you know, social event.

12:17.640 --> 12:21.040
Of course, it's probably, you know, the biggest conference in our field now.

12:21.040 --> 12:25.360
So, all your friends and other colleagues come all over the world, you know, assemble

12:25.360 --> 12:32.920
at a single place, and this is a great, you know, moment to discuss collaborations

12:32.920 --> 12:37.280
or to discuss the newest results, you know, have a beer together, etc.

12:37.280 --> 12:43.600
In terms of preparation, I would recommend people to read just, you know, through the agenda

12:43.600 --> 12:48.480
and make sort of a list of, you know, papers they think are most interesting before they

12:48.480 --> 12:52.720
go to the conference, because, you know, like whenever you're there, you're, you know,

12:52.720 --> 12:53.720
it is overwhelming.

12:53.720 --> 12:59.240
So, there, like you will have, you know, no time to, you know, to orient.

12:59.240 --> 13:00.240
Yeah.

13:00.240 --> 13:03.040
And the agendas are overwhelming, to be honest.

13:03.040 --> 13:04.040
Yeah.

13:04.040 --> 13:05.840
There's a ton of stuff going on there.

13:05.840 --> 13:06.840
Yeah.

13:06.840 --> 13:07.840
Yeah, absolutely.

13:07.840 --> 13:08.840
Yeah.

13:08.840 --> 13:12.320
And of course, one, I think the interesting thing is that a lot of the work that you

13:12.320 --> 13:15.040
will see is already published at archive, right?

13:15.040 --> 13:16.040
Right.

13:16.040 --> 13:21.360
So, as soon as you see a paper, you know, you can actually already read it, you know,

13:21.360 --> 13:25.840
long before the paper has been published on the NEPs website.

13:25.840 --> 13:31.600
And often, you know, you follow up work has already, you know, followed the archive.

13:31.600 --> 13:37.080
So, some papers are already outdated, you know, at the moment you see them at NEPs, which

13:37.080 --> 13:38.320
is a bit tragic.

13:38.320 --> 13:39.320
That's funny.

13:39.320 --> 13:42.760
And you're publishing some work next week as well?

13:42.760 --> 13:43.760
That's right.

13:43.760 --> 13:44.760
Yeah.

13:44.760 --> 13:51.200
So, with two of my colleagues, we worked on, you know, publishing a set of your kernels

13:51.200 --> 13:59.040
to GPU kernels, which is a software for the GPU, which allow you to train and build

13:59.040 --> 14:02.400
models with block sparse layers.

14:02.400 --> 14:05.600
So, block sparse GPU kernels.

14:05.600 --> 14:09.560
Maybe you talked to me before we dig into the block sparse element of this.

14:09.560 --> 14:17.480
Tell me a little bit more about GPU kernels and where they fit in in the kind of the software

14:17.480 --> 14:18.480
stack.

14:18.480 --> 14:24.000
I think about, you know, at the lowest level, I think about, you know, frameworks like

14:24.000 --> 14:28.400
TensorFlow and things like that at a much higher level, where the kernels fit in.

14:28.400 --> 14:29.400
Right.

14:29.400 --> 14:33.440
So, GPU kernels are, you can view them as sort of middleware.

14:33.440 --> 14:34.440
Okay.

14:34.440 --> 14:43.680
So, they are libraries that require you to, you know, program on the fairly low level.

14:43.680 --> 14:51.120
And they allow you to sort of maximize, you know, usage of, you know, GPUs, which are,

14:51.120 --> 14:55.400
of course, you know, it is hardware that runs very parallel.

14:55.400 --> 14:59.160
So, it requires you to specialize software to make full use of that.

14:59.160 --> 15:07.320
Basically, they are a set of middleware that is, you know, typically already implemented

15:07.320 --> 15:08.840
in your framework.

15:08.840 --> 15:09.840
Okay.

15:09.840 --> 15:14.200
So, you basically call in various functions that make use of, you know, office kernels

15:14.200 --> 15:15.400
when you build your models.

15:15.400 --> 15:16.400
Right.

15:16.400 --> 15:21.760
So, for example, when you, you know, when you implement the convolutional network in

15:21.760 --> 15:29.280
TensorFlow, then you typically use, you know, various pre-built layers that themselves

15:29.280 --> 15:34.120
call, you know, GPU kernels to efficiently evaluate, you know, the forward path and the

15:34.120 --> 15:37.800
backward path and the gradient computation on the GPUs.

15:37.800 --> 15:42.360
Are you developing the kernels in CUDA or at a lower level than that?

15:42.360 --> 15:43.360
Right.

15:43.360 --> 15:45.760
So, I did not develop the kernels myself.

15:45.760 --> 15:46.760
Okay.

15:46.760 --> 15:52.200
So, this was all done by Scott Gray, who is a GPU kernel expert.

15:52.200 --> 15:53.200
Okay.

15:53.200 --> 15:54.200
Yeah.

15:54.200 --> 15:56.200
So, he basically wrote all the kernels.

15:56.200 --> 15:57.800
So, like, all the credits go to him.

15:57.800 --> 15:58.800
Right.

15:58.800 --> 16:04.880
I guess I was more asking like, well, I guess I should have asked, does one develop kernels

16:04.880 --> 16:10.880
in CUDA or is this at an even lower level than something like a CUDA?

16:10.880 --> 16:13.480
I'm just trying to get a sense for where they fit in.

16:13.480 --> 16:19.480
So, these kernels were developed both at the C level and at the assembly level.

16:19.480 --> 16:20.480
Okay.

16:20.480 --> 16:21.880
So, fairly low level.

16:21.880 --> 16:22.880
Yeah.

16:22.880 --> 16:23.880
Yeah.

16:23.880 --> 16:27.760
And then you mentioned, brought up convolutional neural nets as an example.

16:27.760 --> 16:32.120
Can you give some examples of the types of kernels that, you know, a framework might implement

16:32.120 --> 16:38.840
is this, like, for things like tensor operations, or they add a higher level or lower level

16:38.840 --> 16:39.840
than that?

16:39.840 --> 16:40.840
Okay.

16:40.840 --> 16:44.240
So, your question is, like, how you can use these kernels in your models?

16:44.240 --> 16:45.240
No.

16:45.240 --> 16:51.320
So, I guess I'm still talking about GPU kernels generally, and, you know, what operations

16:51.320 --> 16:57.000
they tend to represent in, for example, the case of a CNN in a framework that's, you

16:57.000 --> 16:59.840
know, it's calling down to a bunch of lower level kernels.

16:59.840 --> 17:03.120
I'm just looking for examples of what those kernels might be.

17:03.120 --> 17:04.120
Right.

17:04.120 --> 17:11.120
So, in a typical CNN, you know, you have a GPU kernel for, you know, the forward convolutional

17:11.120 --> 17:12.120
computation, right?

17:12.120 --> 17:17.600
So, you have a convolutional layer in a convolutional network, which is basically a linear layer

17:17.600 --> 17:20.640
that applies, like, a convolution or the inputs.

17:20.640 --> 17:21.640
Mm-hmm.

17:21.640 --> 17:23.840
And the result is the output of the layer.

17:23.840 --> 17:26.880
So, that's where you use, you know, GPU kernels.

17:26.880 --> 17:32.160
And then there are, for example, GPU kernels for elements-wise operations.

17:32.160 --> 17:33.160
Okay.

17:33.160 --> 17:34.160
Yeah.

17:34.160 --> 17:35.160
Got it.

17:35.160 --> 17:42.040
And so, the kernels that you are publishing are for block sparse operations.

17:42.040 --> 17:43.040
What are those?

17:43.040 --> 17:45.080
So, block sparse operations.

17:45.080 --> 17:55.360
So, what we release is it is a generalization of the usual matrix multiplication and convolutional

17:55.360 --> 17:57.760
kernels you typically use.

17:57.760 --> 17:58.760
Okay.

17:58.760 --> 18:06.040
So, typically, what you have is when you use a convolutional layer in your model is a

18:06.040 --> 18:14.080
weight matrix or, like, a weight tensor, which is dense, meaning that all the entries in

18:14.080 --> 18:18.840
the weight, in tensor, a weight matrix, like, have a non-zero value.

18:18.840 --> 18:19.840
Right.

18:19.840 --> 18:27.680
So, there is that in, you know, as you increase the width of the network, the number of weights

18:27.680 --> 18:28.680
increases quadratically.

18:28.680 --> 18:29.680
By width.

18:29.680 --> 18:32.680
You mean the number of features?

18:32.680 --> 18:34.200
The number of features.

18:34.200 --> 18:35.200
Right.

18:35.200 --> 18:36.200
Right.

18:36.200 --> 18:39.840
So, there is a quadratic relationship between the number of features and the number of weights.

18:39.840 --> 18:40.840
Mm-hmm.

18:40.840 --> 18:46.520
If you use a dense, your kernel, like, a dense weight matrix, an obvious solution to this

18:46.520 --> 18:53.680
is to, instead of using a dense weight matrix, you use a weight matrix where not necessarily

18:53.680 --> 19:01.520
all blocks, you know, or all your weights in the tensor, have a non-zero value.

19:01.520 --> 19:09.360
So, what the block sparse kernels allow you to do is to define which of, so you basically

19:09.360 --> 19:17.400
define your weight matrix or your weight tensor into blocks of either 8 by 8 or 16 by 16

19:17.400 --> 19:19.560
or 32 by 32.

19:19.560 --> 19:27.040
And then you can say beforehand for each block, whether that block has the value of zero

19:27.040 --> 19:32.280
for every entry in the block, or actually has an actual, you know, learns, you know,

19:32.280 --> 19:33.280
weights value.

19:33.280 --> 19:34.280
Right.

19:34.280 --> 19:39.880
Yeah. So, basically if the weight values are zero, which is equivalent to having a block

19:39.880 --> 19:43.880
is, you know, all zero, then you don't, you don't have to compute, you know, that block

19:43.880 --> 19:50.040
because the output of the layer of the operation is not a function of that block.

19:50.040 --> 19:52.360
So, you can, you know, skip that computation.

19:52.360 --> 19:57.680
And so, you would use this in a scenario where, or maybe I should just ask you to give

19:57.680 --> 19:58.680
us some examples.

19:58.680 --> 20:05.600
Imagine that, you know, what you're essentially saying by using a block sparse matrix is

20:05.600 --> 20:10.080
that you don't care about the relationship between some features and some layers.

20:10.080 --> 20:11.120
Is that the idea?

20:11.120 --> 20:19.440
It gives you more flexibility in choosing how your neurons are connected between layers.

20:19.440 --> 20:20.440
Mm-hmm.

20:20.440 --> 20:26.040
Basically in all the existing architectures, you've given a particular layer, all the input

20:26.040 --> 20:28.240
and output features are connected.

20:28.240 --> 20:29.240
Right.

20:29.240 --> 20:34.960
But this is not necessarily, you know, the optimal way to use your parameters, right?

20:34.960 --> 20:42.880
So, given a particular budget of parameters, you might want to use, for example, a wider

20:42.880 --> 20:49.800
network that where not all features are interconnected, but only like half of them are interconnected.

20:49.800 --> 20:50.800
Mm-hmm.

20:50.800 --> 20:52.560
Or, you know, 25% of them.

20:52.560 --> 20:57.560
Or you might want to say that, okay, I'm going to just use, you know, connecting a half

20:57.560 --> 21:01.480
of the neurons with each other and I'm going to increase, you know, the depth by a factor

21:01.480 --> 21:02.480
two.

21:02.480 --> 21:09.640
So basically by introducing sparsity, given a certain parameter budget, you can have either

21:09.640 --> 21:12.480
much wider or much deeper networks.

21:12.480 --> 21:16.120
So this is one particular application that we investigated.

21:16.120 --> 21:20.560
But there are, you know, much more applications possible that we haven't even touched upon.

21:20.560 --> 21:26.560
So there's a whole, like a whole series of papers that are coming out now that show that

21:26.560 --> 21:33.560
after training a neural network, it is typically possible to remove, you know, 99% of the weights

21:33.560 --> 21:36.480
without significantly affecting performance.

21:36.480 --> 21:37.480
Right.

21:37.480 --> 21:42.960
Which is, you know, it is a curious finding, like turns out that most of the weights are

21:42.960 --> 21:46.040
useless, you know, for prediction.

21:46.040 --> 21:51.360
These kernels, they will allow you to actually make use of this redundancy.

21:51.360 --> 21:56.680
So after training, you could potentially just move all the ways, you know, they're useless.

21:56.680 --> 22:00.520
That's how you have network that is much faster to evaluate.

22:00.520 --> 22:05.360
So it would give you a speed up and, you know, future work would also involve, you know,

22:05.360 --> 22:07.640
learning the connectivity, right?

22:07.640 --> 22:12.160
So this is something we have not done yet, but we hope that we or others will do a future

22:12.160 --> 22:13.160
work.

22:13.160 --> 22:19.520
It's basically a bit similar to the brain, you know, the neurons should be connected,

22:19.520 --> 22:21.760
which, you know, should not be connected.

22:21.760 --> 22:25.080
So basically this is a forum of learning the structure of the model, you know, beyond

22:25.080 --> 22:30.600
just the value of the weights, you also, you know, you also learn where, you know, where

22:30.600 --> 22:31.600
you have weights.

22:31.600 --> 22:32.600
Right.

22:32.600 --> 22:33.600
Right.

22:33.600 --> 22:38.600
And yeah, we also have some preliminary work that we did with Ristus, so we just, an

22:38.600 --> 22:40.000
internet was here this summer.

22:40.000 --> 22:41.000
Okay.

22:41.000 --> 22:48.640
So right now, you are saying before training, you're specifying like where the sparsity

22:48.640 --> 22:49.640
blocks are.

22:49.640 --> 22:50.640
Yeah.

22:50.640 --> 22:51.640
Is that correct?

22:51.640 --> 22:52.640
And then.

22:52.640 --> 22:53.640
Yeah.

22:53.640 --> 22:58.080
And then you're suggesting that there's, you know, work, you're hoping to see work where

22:58.080 --> 23:03.120
that's learned as part of the training process as opposed to being specified up front.

23:03.120 --> 23:04.120
Yeah.

23:04.120 --> 23:10.520
So this is something where we have some preliminary work, but I think this is a very exciting

23:10.520 --> 23:15.120
direction that we or others can pursue in, you know, future work.

23:15.120 --> 23:16.120
Yeah.

23:16.120 --> 23:24.680
So determine where the, where the spars blocks are currently or conversely where the connections

23:24.680 --> 23:25.680
are.

23:25.680 --> 23:31.280
Is it, you know, is it essentially another kind of hyperparameter or a meta hyperparameter

23:31.280 --> 23:38.000
or something that you are training and evaluating, you know, with regard to some optimization

23:38.000 --> 23:42.120
that you're trying to make or some, you know, or the performance of your, your network

23:42.120 --> 23:49.320
as a whole or is there, you know, some set of heuristics or intuition that tells you,

23:49.320 --> 23:52.120
you know, where you should have the connections.

23:52.120 --> 23:53.120
Right.

23:53.120 --> 23:59.680
So in choosing the sparsity pattern in our published work, we took inspiration from the field of

23:59.680 --> 24:02.120
the small world networks.

24:02.120 --> 24:03.120
Small world networks.

24:03.120 --> 24:04.120
What are those?

24:04.120 --> 24:05.120
Yeah.

24:05.120 --> 24:11.560
So small world networks, they are a type of graph basically that you find various systems,

24:11.560 --> 24:12.560
including the brain.

24:12.560 --> 24:13.560
Okay.

24:13.560 --> 24:14.560
Okay.

24:14.560 --> 24:16.600
So you also find it in social networks, right?

24:16.600 --> 24:23.560
So you or any person on earth is connected to any other person on earth in a small number

24:23.560 --> 24:24.560
of steps.

24:24.560 --> 24:25.560
Right.

24:25.560 --> 24:26.560
Six degrees of Kevin Bacon.

24:26.560 --> 24:27.560
Right.

24:27.560 --> 24:28.560
Right.

24:28.560 --> 24:29.560
Right.

24:29.560 --> 24:35.520
So this means that even though, you know, the number of connections you have and the number

24:35.520 --> 24:42.520
of connections that any person, you know, in the world has is relatively low, you are still

24:42.520 --> 24:45.560
connected to any other person in the small number of steps.

24:45.560 --> 24:51.560
And you find the same property in the human brain at a functional level at least.

24:51.560 --> 24:57.960
So, you know, the functional modules in the brain are often mostly locally connected,

24:57.960 --> 25:03.240
but there are some more or less random long range connections.

25:03.240 --> 25:07.720
And due to these more or less, you know, long range, long term connections, you know, the

25:07.720 --> 25:13.800
whole brain is connected in the small number of steps, which means that information is

25:13.800 --> 25:20.480
spread throughout the brain relatively quickly, even though you have a huge number of neurons.

25:20.480 --> 25:25.440
So this is something we took inspiration from when choosing, says, you know, a disparity

25:25.440 --> 25:27.440
mask in our networks.

25:27.440 --> 25:33.440
There are very simple algorithms for generating graphs that have this property.

25:33.440 --> 25:36.080
So these are called small world graphs.

25:36.080 --> 25:41.000
They basically, they just, you know, require you to have a certain percentage of your connections

25:41.000 --> 25:43.040
in your random.

25:43.040 --> 25:48.600
And so this is indeed a hyper parameter, but we do have the guarantee that information

25:48.600 --> 25:50.800
mixes in the relatively small number of steps.

25:50.800 --> 25:56.360
So even though you introduce varsity in your network, so for example, we train, we

25:56.360 --> 26:04.280
have trained a big LSTM that is, you know, 98% sparse, which means that, you know, 98%

26:04.280 --> 26:08.320
of the connections between type cents are not there, right?

26:08.320 --> 26:13.600
So any neuron is only connected to about, you know, 2% of the neurons in the previous

26:13.600 --> 26:15.400
time step.

26:15.400 --> 26:20.520
So even though it's only, you know, it's only 2% in a small number of steps every neuron

26:20.520 --> 26:26.240
is connected with every other neuron, which means that we basically do is we introduce

26:26.240 --> 26:32.040
a number of internal steps in the network between external time steps that allow information

26:32.040 --> 26:36.960
to spread through the whole network before you process a new input.

26:36.960 --> 26:38.760
Can you say that last part again?

26:38.760 --> 26:39.760
Right.

26:39.760 --> 26:43.160
So because of the small world property, right?

26:43.160 --> 26:50.800
We basically only need to introduce a small number of intermediate steps between inputs

26:50.800 --> 26:54.920
to basically have a network that is fully connected.

26:54.920 --> 27:03.000
So after we see an input at a certain time step, you want the, you know, your brain to

27:03.000 --> 27:09.320
basically, you know, integrate that information across, you know, all neurons.

27:09.320 --> 27:13.720
And because of the small world property, you only need about, you know, 5, you know,

27:13.720 --> 27:22.640
steps of the internal time steps in order to make sure that the information is spread

27:22.640 --> 27:25.160
through your, yeah, your whole network.

27:25.160 --> 27:30.280
The time steps are our property of LSTMs.

27:30.280 --> 27:35.120
Does this apply to other types of models as well, like CNNs?

27:35.120 --> 27:36.520
Yes, definitely.

27:36.520 --> 27:42.080
So we, we applied the same technique, you know, to convolutional kernels.

27:42.080 --> 27:48.640
So in this case, you, you know, we, it's your sparsity in the feature dimensions,

27:48.640 --> 27:54.520
convolutional kernels, and we also found there that indeed helps you get better performance.

27:54.520 --> 27:57.120
So yeah, we've done a couple of experiments.

27:57.120 --> 28:02.720
So one is on, so basically what we showed is that if you take an existing architecture

28:02.720 --> 28:10.120
with an existing convolutional kernel, so you take in a, like a res net or, you know,

28:10.120 --> 28:18.480
the pixels CNN and you replace the regular convolutional kernels with locks, spars kernels

28:18.480 --> 28:24.720
and then you eat or widen or deepen the network, you know, while keeping a number of parameters

28:24.720 --> 28:29.280
the same, you get better performance in many situations.

28:29.280 --> 28:36.320
What's the analog to introducing additional time steps in the LSTM and the convolutional

28:36.320 --> 28:37.320
network?

28:37.320 --> 28:38.320
Right.

28:38.320 --> 28:44.480
So in the convolutional network, you can, for example, in a res net, what worked well is

28:44.480 --> 28:50.480
that we, we simply doubled the, you know, the depth of the res net.

28:50.480 --> 28:58.040
So a res net, you know, consists of a couple of stages and between stages, you down sample.

28:58.040 --> 29:03.720
So in one experiment, we took a res net for, you know, C410 and we, we doubled the depth

29:03.720 --> 29:11.280
of the network and we introduced 50% sparsity, the 50% of the weights are not there anymore,

29:11.280 --> 29:16.000
which means that, you know, the total number of parameters is approximately the same as

29:16.000 --> 29:17.000
before.

29:17.000 --> 29:18.000
Right.

29:18.000 --> 29:23.040
And because of the additional depth, you still have a good mixing of information.

29:23.040 --> 29:28.000
And so the rest of, so we kept the rest of the architecture the same, so the same

29:28.000 --> 29:32.960
murder rates, the same, you know, the same hyper parameters, we kept, you know, everything

29:32.960 --> 29:38.600
fixed and we saw that it led to an improvement of the accuracy of the model.

29:38.600 --> 29:43.960
The smart world model that's giving you, that's giving you an algorithm that you can apply

29:43.960 --> 29:49.760
to determine which 50% of the data you're basically getting rid of and which you're keeping.

29:49.760 --> 29:56.160
Or is it giving you a, you know, some kind of bound or guarantee that if you get rid

29:56.160 --> 30:01.680
of, you know, X percent of information, you'll still have the number of, you know, a given

30:01.680 --> 30:04.920
degrees of connectivity or convergence or something.

30:04.920 --> 30:07.160
What exactly is that telling you?

30:07.160 --> 30:13.040
So we're, we're not actually getting rid of any data, right, because the, so the state

30:13.040 --> 30:16.840
is still, is still dense of the model.

30:16.840 --> 30:20.600
So all we're doing is introducing sparsity into weights.

30:20.600 --> 30:25.600
So we're not, we're not, you know, removing any, any information from, from the state.

30:25.600 --> 30:30.480
While you're not getting rid of data, strictly speaking, you're still kind of getting rid

30:30.480 --> 30:36.920
of data in motion in a sense, like you're making it harder for the network to, to learn

30:36.920 --> 30:40.720
or to get a piece of data in a given step, right?

30:40.720 --> 30:46.000
You mean that the number of weights per step is, is reduced by a factor two, so the number

30:46.000 --> 30:48.920
of revenues is reduced by a factor two?

30:48.920 --> 30:49.920
Right.

30:49.920 --> 30:55.240
I guess, I guess what I mean is that, you know, ultimately when you do this, the network

30:55.240 --> 31:02.040
is still operating on less information than it had in a fully connected sense.

31:02.040 --> 31:12.480
Maybe before you, you know, do things like add time steps and, you know, broaden, broaden

31:12.480 --> 31:19.120
or deepen your network, just if you were to have a fully connected network and then you

31:19.120 --> 31:23.360
use zero out some of the weights, like the network is then, is it fair to say that the

31:23.360 --> 31:30.200
network is operating on less, less information than in the fully connected sense?

31:30.200 --> 31:35.600
Yeah, so it is important to note that this is something we do before training, right?

31:35.600 --> 31:39.360
So we actually train with the sparsity, right?

31:39.360 --> 31:45.880
So yeah, so before you deepen or widen, you remove half weights, then indeed you do simply

31:45.880 --> 31:50.280
have half the capacity, you know, to store anything in your weights, obviously, right,

31:50.280 --> 31:51.280
right?

31:51.280 --> 31:56.280
So it is, it is important, I think, to do, to keep the number of, you know, parameters

31:56.280 --> 32:01.520
at least equal, it's just basically you are assigning your parameters in a different

32:01.520 --> 32:02.880
way to the model.

32:02.880 --> 32:03.880
Okay.

32:03.880 --> 32:08.720
So the model I still, in principle, the same capacity to store information, it's just

32:08.720 --> 32:13.480
that the, the way it uses the parameters is a little bit different.

32:13.480 --> 32:20.320
Just to summarize that, you, you are indeed reducing the model's capacity to store information

32:20.320 --> 32:27.280
when you remove half, let's say, of the weights, but you're compensating for that by either

32:27.280 --> 32:29.920
increasing your breadth or your depth.

32:29.920 --> 32:33.080
Yeah, that's what we are indeed doing.

32:33.080 --> 32:34.080
Okay.

32:34.080 --> 32:39.920
And then in the future work, I have, you know, believed that we can figure out how to,

32:39.920 --> 32:42.880
you know, learn the sparsity.

32:42.880 --> 32:48.960
And, you know, actually, you know, during training, be able to remove a large percentage

32:48.960 --> 32:53.680
of the weights, you know, without getting worse performance.

32:53.680 --> 32:54.920
So that's division.

32:54.920 --> 33:01.400
How sensitive are, well, in the case where I think with the LSTM, you said you got rid

33:01.400 --> 33:07.560
of 98% of the weights, I would imagine that which weights you get rid of is very important

33:07.560 --> 33:11.400
or, you know, conversely, which weights you keep are very important.

33:11.400 --> 33:18.480
But do you have some way of measuring the sensitivity of a given networks performance

33:18.480 --> 33:21.400
to which weights you remove?

33:21.400 --> 33:26.160
So we choose a particular sparsity, you know, pattern before training.

33:26.160 --> 33:27.160
Right.

33:27.160 --> 33:31.360
And then we use either, you know, a wider or a deeper network, right?

33:31.360 --> 33:32.360
Right.

33:32.360 --> 33:38.440
So then we train the weights, so the model has to sort of figure out what meaning to design,

33:38.440 --> 33:41.040
you know, to each neuron and to each weight.

33:41.040 --> 33:42.040
Right.

33:42.040 --> 33:47.200
So we leave it up to the model to use, you know, the given sparsity pattern.

33:47.200 --> 33:53.040
So what we found that worked well in case of LSTM is the so, you know, so called Borevelsi

33:53.040 --> 34:02.760
Albert Rav, which is a type of small world network where you have a small number of neurons

34:02.760 --> 34:06.480
that are connected to a very large number of auto neurons.

34:06.480 --> 34:07.480
Mm-hmm.

34:07.480 --> 34:13.680
And then you have like a long tail of neurons that are very sparsely connected.

34:13.680 --> 34:17.920
We found that it, you know, that this worked, you know, really well for text.

34:17.920 --> 34:24.760
So what we did in this case is that we increased, you know, the size of the state of the LSTM,

34:24.760 --> 34:25.760
right?

34:25.760 --> 34:30.840
So you have a much wider model, which also gives you a longer memory of the past, right?

34:30.840 --> 34:38.480
Because you get, you know, to fit more information into the state of the LSTM than otherwise.

34:38.480 --> 34:45.440
And what we found is that if we trained a sentiment classifier based on that state, so we first

34:45.440 --> 34:52.360
train an LSTM, completely unsupervised on text, and then we train a linear classifier

34:52.360 --> 34:58.840
on the state of the LSTM to predict the sentiment reviews.

34:58.840 --> 35:04.040
And then so what we found is that we get state of the art results in predicting sentiment

35:04.040 --> 35:05.960
based on that model.

35:05.960 --> 35:11.680
So the previous state of the art was published by Alec Redford, which is also co-author of

35:11.680 --> 35:12.680
this paper.

35:12.680 --> 35:13.680
Okay.

35:13.680 --> 35:14.680
A couple of months ago.

35:14.680 --> 35:15.680
Okay.

35:15.680 --> 35:19.320
But now we found that if you train a much, you know, wider network to this sparse, you

35:19.320 --> 35:21.120
get even better results.

35:21.120 --> 35:27.280
This basically, you gave us a state of the art results on like five benchmarks of just

35:27.280 --> 35:28.880
playing sentiment in text.

35:28.880 --> 35:29.880
Right.

35:29.880 --> 35:35.680
And so this is an example of how, you know, within the same parameter budget, reconfiguring

35:35.680 --> 35:40.480
the way you use that parameter budget can give you much better results.

35:40.480 --> 35:41.480
Exactly.

35:41.480 --> 35:42.480
Yeah.

35:42.480 --> 35:47.560
And is there any intuition as to where you're likely to see that effect or where you

35:47.560 --> 35:52.680
would likely want to apply block sparse kernels, or is it something that, you know, will

35:52.680 --> 35:54.760
just have come out of experimentation?

35:54.760 --> 35:55.760
Yeah.

35:55.760 --> 36:01.200
So, you know, the space of possible architectures that you can train with these kernels is so

36:01.200 --> 36:02.200
huge.

36:02.200 --> 36:06.000
And there's just no way that, you know, we can explore it all.

36:06.000 --> 36:07.000
So, yeah.

36:07.000 --> 36:12.680
Is it what we aim, you know, with this release is to basically, you know, give it to the world

36:12.680 --> 36:18.640
and let everyone experiment with it because, you know, you know, we're way too small to

36:18.640 --> 36:20.720
explore this useful space.

36:20.720 --> 36:24.680
So yeah, I have, you know, some limited intuition.

36:24.680 --> 36:25.680
Mm-hmm.

36:25.680 --> 36:26.680
And what is that?

36:26.680 --> 36:31.320
What's your intuition telling you that where my folks want to look first or where would

36:31.320 --> 36:34.000
you like to see folks looking to apply this?

36:34.000 --> 36:35.000
Right.

36:35.000 --> 36:42.840
So, Scott is building in the capability now of actually having like a dynamic sparsity.

36:42.840 --> 36:47.960
So hopefully this will be finished before the release, you can see, meaning that varies

36:47.960 --> 36:48.960
during training.

36:48.960 --> 36:49.960
Yes.

36:49.960 --> 36:53.160
So, you can actually learn the sparsity mouse during training.

36:53.160 --> 36:54.160
Okay.

36:54.160 --> 36:57.400
And you could then potentially optimize it, you know, to get more performance.

36:57.400 --> 36:58.400
Right.

36:58.400 --> 36:59.400
Right.

36:59.400 --> 37:04.760
So, in other humans, it's, you know, we know that the way our own neurons are connected

37:04.760 --> 37:05.760
is also learned.

37:05.760 --> 37:06.760
Right.

37:06.760 --> 37:07.760
It's based on data.

37:07.760 --> 37:08.760
Right.

37:08.760 --> 37:09.760
Yeah.

37:09.760 --> 37:12.240
So, so this is, this is a way of, you know, learning the architecture of your model.

37:12.240 --> 37:17.040
So, personally, I think this is, this will be a very interesting area of research.

37:17.040 --> 37:18.040
Yeah.

37:18.040 --> 37:21.240
So, you know, maybe I'm kind of beating a dead horse here.

37:21.240 --> 37:28.240
But it sounds like there's really two things that could be accomplished here and that they're

37:28.240 --> 37:29.240
kind of different.

37:29.240 --> 37:34.080
And I'm wondering if, you know, there's some way that you think about these one is, you

37:34.080 --> 37:43.680
know, given a parameter budget, you know, re-architection your network using block sparsity

37:43.680 --> 37:46.880
to improve your results.

37:46.880 --> 37:51.240
But separate from that, there's this, you know, issue of this whole, you know, finding

37:51.240 --> 37:56.880
the right 2% of parameters that actually matter and then using block sparsity as a way

37:56.880 --> 38:02.440
to implement the network that, you know, just has what matters and presumably the result

38:02.440 --> 38:05.880
is that you're able to compute those, you know, much more quickly.

38:05.880 --> 38:06.880
Right.

38:06.880 --> 38:10.400
And I'm thinking about that the right way and are those, you know, do you think of those

38:10.400 --> 38:16.480
as the same problem or are they, you know, kind of, you know, two different problems,

38:16.480 --> 38:21.320
you know, that are enabled by this block spars kernel approach?

38:21.320 --> 38:22.320
I think you have right.

38:22.320 --> 38:23.320
Yeah.

38:23.320 --> 38:28.600
And these are indeed two problems that you can, you know, now tackle.

38:28.600 --> 38:32.760
And indeed, yeah, so either, you know, static sparsity, which is what we have done in our

38:32.760 --> 38:38.160
experiments or dynamic sparsity, which is sort of where you allure the sparsity pattern.

38:38.160 --> 38:39.160
Yeah.

38:39.160 --> 38:42.280
That is also an interesting application of this.

38:42.280 --> 38:47.200
And so it strikes me that a big part of the reason why you care about any of this at

38:47.200 --> 38:50.800
all is because of computational limitations.

38:50.800 --> 38:53.120
Is that the main idea?

38:53.120 --> 38:56.400
That's the main idea, but that's, that's I think the main idea of the whole field of

38:56.400 --> 38:57.400
computer science.

38:57.400 --> 38:58.400
Right.

38:58.400 --> 39:00.920
Too sure.

39:00.920 --> 39:07.440
You know, that was a little bit of a segue into in spite of the fact that, you know, we're

39:07.440 --> 39:10.000
getting around computational limitations here.

39:10.000 --> 39:16.040
You actually had access to some, you know, pretty fancy hardware to kind of try this

39:16.040 --> 39:17.040
out on.

39:17.040 --> 39:21.600
Can you talk a little bit about, you know, kind of the specific problem that you, you

39:21.600 --> 39:26.880
know, the problems that you were looking at to kind of push the, you know, the, the

39:26.880 --> 39:33.560
limit and then, you know, how the experimental results you saw on the, you know, the hardware

39:33.560 --> 39:34.560
that you were using?

39:34.560 --> 39:35.560
Absolutely.

39:35.560 --> 39:36.560
Absolutely.

39:36.560 --> 39:45.400
So one of the problems that we wanted to solve is to train a huge LSTM on a very large,

39:45.400 --> 39:48.520
very large data type of text and then the Amazon reviews.

39:48.520 --> 39:49.520
Okay.

39:49.520 --> 39:57.520
And we were so lucky to have access to an NVIDIA DGX1, which is hardware from NVIDIA

39:57.520 --> 40:04.200
that allowed us to train much larger models than we could have trained otherwise.

40:04.200 --> 40:11.280
So yeah, this, this was something that enabled us to basically get state of your performance

40:11.280 --> 40:14.040
on the Amazon reviews data set.

40:14.040 --> 40:22.040
And this was also instrumental to get the results I talked about on access to flying sentiment.

40:22.040 --> 40:26.640
What was it about the LSTM that made it huge?

40:26.640 --> 40:33.640
So the Amazon reviews data set is just a very large data set of reviews on Amazon obviously.

40:33.640 --> 40:36.960
Do you remember how many reviews, how many reviews are in that data set?

40:36.960 --> 40:44.240
I don't know, like Adam has, but it's, it's like a large portion of all of them.

40:44.240 --> 40:49.880
So basically the model that you could fit on this data is like an order of magnitude

40:49.880 --> 40:52.520
larger than anything we tried earlier.

40:52.520 --> 40:56.440
So you also need, you know, hardware to be able to fit such a model.

40:56.440 --> 41:02.200
And if you weren't using the DGX1, what would you have used otherwise and how did the results

41:02.200 --> 41:04.560
that you saw compare?

41:04.560 --> 41:14.960
So if you wouldn't have had a DGX1, then we would have had to use a cluster of GPUs.

41:14.960 --> 41:21.640
So recently it has become clear that for some problems, it is possible to, to train with

41:21.640 --> 41:27.480
very large mini batches or, you know, to spread out and training a close, you know, cluster.

41:27.480 --> 41:35.200
But then you, you will still get into problems of your number of parameters so that is not

41:35.200 --> 41:36.200
ideal still.

41:36.200 --> 41:41.160
And typically, you know, still need to, you know, fit your parameters on the single GPU.

41:41.160 --> 41:48.160
So even if you can, you know, split your mini batch data across multiple machines, then

41:48.160 --> 41:51.760
you're, you know, still bottlenecked by the memory of all of the single machines.

41:51.760 --> 41:52.760
Okay.

41:52.760 --> 41:57.200
And do you have a sense for at the end of the day, you know, well, how long did it take

41:57.200 --> 42:01.040
you to train your models and like how, do you ever sense for how much faster it would,

42:01.040 --> 42:05.720
you know, it was relative to what you would have done otherwise?

42:05.720 --> 42:11.480
I think it allowed us to train the model about twice as fast as otherwise.

42:11.480 --> 42:15.720
So it's still true, I believe, about, you know, two weeks to train the model.

42:15.720 --> 42:16.720
Oh, wow.

42:16.720 --> 42:17.720
Wow.

42:17.720 --> 42:24.880
It sounds like a really interesting project with some potentially broad applications that,

42:24.880 --> 42:30.160
you know, I'll be keeping an eye out for clearly, I'll be publishing a paper around this.

42:30.160 --> 42:36.080
Are you also publishing code or is it more the, the research results that are the important

42:36.080 --> 42:38.400
takeaway for folks that want to build on it?

42:38.400 --> 42:39.400
Yeah.

42:39.400 --> 42:40.400
So it's a second.

42:40.400 --> 42:41.920
So we are actually publishing the code.

42:41.920 --> 42:42.920
Okay.

42:42.920 --> 42:44.640
So we are publishing the GPU kernels.

42:44.640 --> 42:50.800
I think this is by far the most interesting part of what we release because this actually

42:50.800 --> 42:57.160
allows practitioners and researchers to do, you know, completely new things very easily.

42:57.160 --> 43:05.360
So it's just basically a matter of importing a new library and, you know, replacing your

43:05.360 --> 43:11.720
existing conclusions or matrix applications with, you know, the block sparse ones and

43:11.720 --> 43:12.720
they're good to go.

43:12.720 --> 43:16.960
So yeah, it is actual software to use for others.

43:16.960 --> 43:17.960
Okay.

43:17.960 --> 43:23.120
Do you at this point have a place that you can point folks to or do you know where folks

43:23.120 --> 43:26.120
will be able to find this work once it's published?

43:26.120 --> 43:33.800
You can find a work on OpenEI.com and you go to the blog posts there.

43:33.800 --> 43:38.040
You will find a blog post on this topic and a link to get up.

43:38.040 --> 43:39.040
Okay.

43:39.040 --> 43:40.040
Fantastic.

43:40.040 --> 43:41.040
Great.

43:41.040 --> 43:42.040
Great.

43:42.040 --> 43:43.040
I really enjoy chatting with you.

43:43.040 --> 43:46.440
Is there anything else that you'd like to leave the audience with?

43:46.440 --> 43:48.680
Yeah, I think we had a very interesting conversation.

43:48.680 --> 43:51.120
Thank you for inviting me to the show.

43:51.120 --> 43:56.320
And yeah, I encourage everyone, you know, to keep sharing results, you know, to publishing

43:56.320 --> 44:03.360
source code, all of your experiments and keep an eye out on the research of OpenEI.

44:03.360 --> 44:04.360
So that's it.

44:04.360 --> 44:05.360
Great.

44:05.360 --> 44:06.360
All right.

44:06.360 --> 44:07.360
Well, thanks.

44:07.360 --> 44:08.360
Thanks very much.

44:08.360 --> 44:09.360
Okay.

44:09.360 --> 44:16.680
All right, everyone, that's our show for today.

44:16.680 --> 44:21.280
Thanks so much for listening and for your continued feedback and support.

44:21.280 --> 44:26.120
For more information on Dirk or any of the topics covered in this episode, head on over

44:26.120 --> 44:30.440
to twimlai.com slash talk slash 80.

44:30.440 --> 44:36.240
To catch up on our OpenEI series, visit twimlai.com slash open AI.

44:36.240 --> 44:41.440
Of course, you can send along your feedback or questions to me via Twitter to at Sam

44:41.440 --> 44:47.960
Charrington or at twimlai or leave a comment right on the show notes page.

44:47.960 --> 44:51.120
Thanks once again to Nvidia for their support of this series.

44:51.120 --> 44:56.480
To learn more about Nvidia and their presence at nips, remember to head on over to twimlai.com

44:56.480 --> 44:58.480
slash Nvidia.

44:58.480 --> 45:09.920
And thank you once again for listening and catch you next time.

