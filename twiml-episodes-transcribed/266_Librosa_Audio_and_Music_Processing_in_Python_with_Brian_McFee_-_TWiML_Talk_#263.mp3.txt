Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast we're featuring a series of shows that highlight just a few of
the great innovations and innovators at the intersection of three very important and
familiar topics, data science, the Python programming language and open source software.
To better understand our listeners' views on the importance of open source and the projects
and players in this space, I'm conducting a survey which I'd be very grateful if you took
a moment to complete.
To access the survey, visit Twimbleai.com slash Python survey.
Please hit pause now and we'll wait for you to get back.
That's twimbleai.com slash Python survey.
Before we dive into the show, I'd like to send a huge thanks to our sponsor for this
series IBM.
Speaking of open source, IBM has a long history of engaging in and supporting open source projects
that are important to enterprise data science, projects like Hadoop, Spark, Jupiter and
Cubeflow to name just a few.
IBM also hosts the IBM data science community, which is a place for enterprise data scientists
looking to learn, share and engage with their peers and industry renowned practitioners.
Here you'll find informative tutorials and case studies, Q&As with leaders in the field
and a lively forum covering a variety of topics of interest to both beginning and experience
data scientists.
Check out and join the IBM data science community by visiting IBM.com slash community slash
data science.
Alright everyone, I am on the line with Brian McFee.
Brian is an assistant professor of music technology and data science at NYU, as well as the
creator of LibRosa, Python based library for audio and music processing that we'll be
spending a bit of time chatting about in the show.
Brian, welcome to this week in machine learning and AI.
Thanks for having me.
Absolutely.
Absolutely.
So it's not very often that I speak to anyone on this podcast that was a postdoctoral
researcher in a center for jazz studies and ends up working in machine learning.
I'd love to hear a little bit about your background and how you arrived at working at
this intersection of music and machine learning.
Yeah, sure.
I mean, it's not too surprising because I think there's end of wanting that category.
But yeah, so I kind of fell into music backwards from being more of a general purpose machine
learning kind of person and then just kept going with it because I was having a good
time.
Oh, wow.
Okay.
So, yeah, so I was in grad school in UC San Diego doing my PhD and I was working on kind
of, I wanted to do general machine learning stuff and had been working on some kind of applied
computer vision type things and some real time embedded system type things and was really
having a great time with it.
And my neighbors in the office that I shared with, what, six, seven other PhD students.
So two of them had started a lab of their own doing basically search engines from music.
So they were building statistical models of different kinds of sounds and different kinds
of music and they wanted to build like what they called it Google for music, right?
So you go in, you search for high energy pop song with, I don't know, fast beat and
a tuba in the background or whatever.
Okay.
So it would index a huge collection by the terms that you described and then figure out
which tracks match your query or are likely to match your query.
And off you go, you have music and this was like, oh, 2007, 2008.
So that's after Pandora, right?
So Pandora had been on the scene for a while.
But it was well before Pandora started doing audio content analysis.
Okay.
So they were the way that their system worked and the way that these two other students
who I should call it by name are Doug Turnbull and Luke Barrington.
So they wanted to build like an automatic Pandora, right?
So Pandora had their music genome project analysts listening to music, filling out a huge
questionnaire, categorizing, filing and all that kind of stuff.
And their thinking was, hey, computers should be good at this.
Let's give that a shot.
So they started doing that and I would overhear their conversations and occasionally chip
in with some advice because I had some kind of web development background before going
to grad school, which was useful for them.
And eventually I just realized that they were having a way better time than I was and I
should switch tracks.
And somehow my department was cool with that and let me basically do over and change directions
completely.
Wow.
Nice.
And that's what I did.
And I didn't want to do what they were doing because they had plenty to do on their
own.
But I'm not going to do a search engine.
I'm going to do a recommender system and I'm going to do something that's kind of one
or zero touch.
So instead of describing what I want, I'll give you an example of what I want and then
the algorithm should produce a sequence of songs to listen to.
My motivation for this was spending a lot of time in the car driving up and down the coast
to California, going home to visit family every now and then.
It's a long drive by yourself and my iPod on shuffle was not doing it for me.
I thought there should be a smarter way to do it.
So that's what I wanted to do.
So I started building content-based similarity models and statistical models, playlists and
things of that nature.
And then it was fun.
So I just kept doing it.
Awesome.
Well, that certainly explains the title of your dissertation more like this, Machine Learning
Approaches to Music similarity.
Yep.
That's what that's all about.
As part of your dissertation, did you come up with a working system and what specific
parts of the many challenges in that general area, were you able to flesh out?
Yeah.
So I came up with a system that more or less worked.
The big challenge in any kind of music research is always data, especially if you're doing
things that are centered around commercial music and popular music and trying to model
the listening behavior of large populations of people, getting access to that kind of
data is, well, it's easier now, it was very difficult in 2009 to 2012 when I was doing
this stuff.
Yeah.
So access to data for supervision and also access to audio content because who's going
to go out and buy like 20,000, 50,000 songs, whatever it takes to actually evaluate one
of those systems meaningfully.
Right.
So those were the two big challenges, which are not at all technical challenges that are
entirely social challenges, but that's the way it is.
On the technical side, the things that I was particularly interested in were, so one
integrating different views of the data, so I didn't want to do something that was purely
audio based.
I wanted to take in whatever kind of information I could get.
So you could have the audio from the songs is one representation.
You could have tags from, say, last FM or, what else?
All music guide or music brains or whatever, you know, people tag music on the internet
and that data is out there and it tells you something, you could have lyrics, you could
have social network data, you could have artist biographies, you could have all sorts
of stuff.
And they all live in kind of different geometrical spaces, right?
Like I compare tags to tags, I compare lyrics to lyrics, but tags to lyrics is a little bit
weird and tags to audio is totally weird.
So I spent a lot of time developing models that could integrate across these different
modalities and put them into one similarity space that was kind of optimized to simulate
trends in listening behavior.
So if you have, like, I don't know, how familiar your listeners might be with collaborative
filtering and recommender systems and that sort of thing.
I think fairly, fairly familiar.
Okay.
So the idea there is, I mean, the cartoon idea is you represent items in terms of the
people that consume them.
So two items are similar if they have a large overlap in their populations.
And two items are dissimilar if they have very little overlap.
So you can induce a geometry out of that and then try to train a machine learning model
to transform features so that it produces not necessarily the same similarity, but a
similarity that gives you the same kinds of recommendations.
So it doesn't need to get the geometry exactly right.
It just needs to get, say, like the rank order of nearest neighbors.
That's what you want to preserve because that's what you're going to use for recommendation
anyway.
And you mentioned that tags to lyrics is weird and another example.
What did you mean by weird and what way?
So I mean, in one sense, they're both text.
So yeah, you can do that.
But the statistics are very different, right?
In lyrics, you'll have a lot of stop words and other things because it's natural language
more or less.
So just the really detailed mechanics of how you would compare lyrics to text or lyrics
to tags is not as straightforward as you might expect.
There's also difficulties around open versus close vocabulary or dealing with popularity
or missing data or backwards or endgrams or all those kinds of things.
So yeah, the example that popped up in my mind was, you know, the tag love song and love
song lyrics seems like there's, you know, strong correlations there, but I can imagine
they're much more challenging, much more challenging combinations.
Oh, yeah.
I mean, you can take a song like this is not a love song.
Compare that.
Depending on your endgram count, you get totally different results, right?
Uh-huh.
So yeah, those things are notoriously difficult to get right.
And so you were, so the lab that you were that you started out switching over to initially,
this was at in San Diego.
Yeah.
It was the computer edition lab at UC San Diego.
Okay.
And then you eventually affiliated with lab Rosa at Columbia and that's where the name for
a liberalza came from.
I'm assuming.
Yeah, that's right.
So I finished my PhD in 2012.
Uh-huh.
About the same time Dan Ellis, who is a professor at Columbia, now at Google, who was like
lab Rosa was his lab.
So he had just come into source of funding for a postdoc and I thought like it would
be really good to work with Dan because every time I have an idea, he's already done it
like five years ago, way better than I was going to do it.
And published papers and has data online and code and everything.
So I thought, yeah, it would just, it would save a lot of time if I just try to work with
him.
And that turned out to work out.
Nice.
And so what was your focus at that lab?
So that was a joint position between lab Rosa and the Center for Jazz Studies at Columbia.
And the project was to build a basically a content based jazz discography without really
defining what that was up front.
Uh-huh.
But the idea was that the Center for Jazz Studies had all these kind of archival recordings
and session recordings and all this back catalog of stuff that they wanted to index and
they wanted to do it in a way that made it more directly searchable.
So you could index all the metadata that tells you something, but be a lot more powerful
if you could say, you know, filter down to all the Coletrain recordings and just excerpt
the saxophone solos.
Right?
That's the sort of thing you would like to be able to do.
Yeah.
But nobody's going to go and mark up the start and stop points of every saxophone solo.
Right.
Right.
But that's the sort of thing that a computer might be good at, or at least could plausibly
be good at.
Pretty quickly we realized that all the algorithms that we had for doing this kind of stuff were
not developed on or evaluated on jazz and didn't really take into account the specifics
of the idiom, so to speak, which basically means that nothing worked.
So he spent a lot of time trying to generalize the methods so that they would adapt to basically
things that were modern western pop music or classical, which is what everything was
built on.
But meaning you were able to identify pre-existing algorithms that given, you know, modern pop
or classical could pick out certain instrument solos?
Solo detection wasn't so much of a problem back then, it's become a little bit more of
a problem now by problem on being like research area.
But certainly the were algorithms for doing things like beat tracking or structural segmentation
or maybe some source separation, although that's changed a lot in the last few years.
But even something as simple as beat tracking, you know, just like tracking where the quarter
notes are in a piece, mostly those are developed on classical or western pop music and just
fell over hard when you give them anything with swing in it.
It's kind of the application for beat tracking or some of the applications for beat tracking.
So it's often used as a pre-processing stage in other kind of more high level analyses.
So one thing that you might do is track the beats of a song and then use the estimated
beat positions as kind of a nonlinear time sampling grid so that you can take a bunch
of features that you extracted with the signal, re-sample them on this non-uniform grid.
And now you have something that's kind of invariant to tempo.
And that could be useful for doing things that are more high level structural analysis,
like first chorus first or maybe one track the down beats.
So went one bar changes to the next bar.
That's those are the sort of thing that you don't want to be too sensitive to small
variations in tempo, but they need to account for larger contextual ranges.
So you set out to build this search engine.
It sounds like you're a little bit of a head of a game relative to your dissertation
and that you have this huge source of data via the Center for Jazz Studies.
Yeah, and it's a huge source of data and it was nice and that it was kind of scoped
down in terms of style and the limited recording times from like the 30s up to 1970, I think.
It's basically to rule out fusion because then things get way more complicated.
But yeah, it was nice because it meant that like some things that we think of as really
hard in these analysis, like structure analysis, find the first chorus, verse, whatever.
Those things are really difficult when you have to account for all the different forms
that these can take.
But if you narrow down to a much more specific genre or style, sometimes you can add additional
constraints to your inference that make the problem a lot more easy.
Okay.
So like a typical recording in our collection would have just a standard jazz form, like play
the head, soloist take turns, play the head out, done.
And that made things a lot easier to analyze.
And so how did this challenge lead to the development of Librosa?
While we're there, explain what Librosa is.
Yeah, so I'll take sort of a detour of this, but I'll get back to the question.
So we have a music information retrieval, which is the name to this kind of area of interdisciplinary
research.
There's the annual conference called ASMR for the International Society for MIR.
And it happens in the fall every year.
And in 2012, it was happening like maybe a couple of weeks after I started at Columbia.
And at the conference, there was a breakout session about trying to transition away from
MATLAB and into Python as a research field.
And it might not feel like it now, but seven years ago, it was kind of a question mark,
like is Python really where we should go?
Why would we do this?
Because there were plenty of other frameworks that people could have adopted.
Sure.
And MATLAB was still very strong in academia.
It was.
And it still is in a lot of ways.
So in signal processing, right, which is where a lot of this research comes from.
And the way that research had been conducted in this field, since I don't know when,
like certainly before I started it, was you inherit a pile of MATLAB scripts that do some
kind of analysis.
And then you run those scripts and maybe tweak them a little bit, do your analysis, and
then you do another project, and you tweak the scripts a little bit more.
There wasn't any sort of versioning, there wasn't any packaging, if someone else joins
your lab, you just hand them whatever your scripts are in their current state at the time.
And maybe they're compatible with what they need from some other place, or maybe they're
not.
And it was basically a big old mess.
So I was kind of looking for new things to do, and being a computer scientist and not
a electrical engineer or a musician, but hey, that's a problem that I can help with.
So as soon as I got back from that conference, I just started writing code.
And the way that it really started was taking all the code that we had from Dan Ellis's
group at Columbia, that was just independent MATLAB scripts with different parameterizations
and different dependencies, and just trying to consolidate them and get them a unified interface
and port them over to Python.
So that was the start of Librosa, which was named after Labrosa.
And Labrosa actually stands for Laboratory for the Recognition and Organization of Speech
and Audio.
So it is actually a proper acronym.
But also it was just, I think Dan Lex-Pink and everything in the lab was pink, so that
nicely.
I'm not sure which came first, you'd have to ask him that.
So yeah, so Librosa really set out to be kind of fixing the core bottleneck in moving
people away from MATLAB and into Python, or doing MIR work.
Because at the time, Psychic Learn was already fairly mature.
The ANO was out there.
There were a few kind of deep learning frameworks that hadn't quite stabilized into what we
have these days, but they were enough to do some damage.
But the key part that was missing was kind of the front end, like ingesting audio and
transforming into these standard representations that people know how to work with.
So that's really where Librosa kind of aimed to sit and expanded a little bit from there
into kind of general tools for working with music and audio, including visualization and
some transformation, some effects processing.
But mostly it's meant to be kind of that interface layer between what we know how to do
algorithmically and what we want to feed into a statistical model if that makes any sense.
Okay.
Yeah.
Can you walk us through maybe some typical user workflows?
Yeah.
So for myself, a lot of what I'll do is have an idea, like maybe I want to try out some new
filtering or some new representation.
And I'll typically work in a Jupyter notebook.
So a lot of this is kind of developed to be easy to use in notebook environments.
And I'll do a bit of hacking in a notebook, work on some small examples, make some plots,
make some figures, maybe fit a small model.
And then once that's kind of settled into something that I like, then I'll try and pull
out whatever the core piece that I've been working on is, whether it's like a new representation
or it's usually a representation kind of thing.
It's a new way to do a execute transform or some different windowing function on a
short time Fourier transformer, you know, something fairly low level and grungy.
I'll take that out of a notebook, pull it into a script and then start building up experimentation
scripts around that that'll run on a cluster.
So the typical way that I use it for doing machine learning experiments is I'll have a collection
of audio on disk, I'll write a script that does some transformation and then map that
out over my audio collection and save the results as numpy file or an hdf file or whatever
is most convenient and then have a separate downstream process that we'll consume those
and run them through a TensorFlow model or a PyTorch model or something like that.
And then the analysis often involves like after the model is fit and I want to look at
the results, that'll typically involve loading into audio redoing some stuff on the fly.
That's usually the notebook as well, but it's always helpful to have some kind of plotting
and visualization routines built into the library that we can just rely on to do kind of
air analysis, model inspection, that kind of thing.
So that's where some feature creep comes from, but I think it's pretty long on the control.
Okay, okay.
So the types of the API or the types of functions that Liberoza is providing are some of the
low-level things that you describe like FFTs and spectrograms and the like.
Yeah, so it could be anything from spectrograms.
We don't provide our own FFT, but we give wrappers for doing short time Fourier transforms,
which are these days sci-fi can do that, but when we started, there was no short time
Fourier transform, which is just, you know, taking a long signal, chunking up in a small
pieces and then doing an FFT of each piece.
And we've talked about FFTs on this podcast at least once, but for anyone it doesn't,
you know, it's not familiar with that terminology.
It's basically a way that you can pull out the frequency components of an audio signal.
That's right.
I always explain it as like running your audio through a prism and separating out the
colors.
Oh, that's a great analogy.
Yeah.
Yeah, it works, it works especially well with Pink Floyd fans, other people your mileage
may vary.
So yeah, it's that kind of stuff, but also ways of post-processing Fourier transforms.
So there are different features that people have derived that try to bake in certain
inferences for the representation that are useful for other tasks.
So for example, if you want to do something like automatic chord recognition, so I want
to know what chords are playing at what point, what point in the song.
You don't actually care about octaves, you don't care if I'm in this register or another
register, you just care about which pitch classes are active.
So people design methods and representations that kind of normalize out the octave information
but retain the pitch class identities, which in principle are things that you can learn
and these days most people just learn that directly, but it is still useful to have kind
of a simple method for doing that transformation that you can just crack open and see what it's
doing exactly.
So those are called chroma features, so chroma for color, and then there are other things
like the male spectrum, which is kind of derived from human perceptual experiments, that's
like pseudo logarithmic, and a bunch of other kind of spectral representation features.
But then there's other stuff like doing sample rate conversion or separating harmonic and
progressive sources or who knows what, there's a lot of stuff in there.
B-tracking is in there, some basic stuff for building up structural segmentation algorithms,
working on a new feature for doing a bunch of inverse problems now, so taking those representations
back to the time domain, so hopefully that'll be in the next version.
Are those features used for generative types of use cases or something else?
Yeah, that's the thinking, because a lot of these days you're starting to see a lot
of generative models of audio that work on time series features, I just like produce
the raw waveform, which is great when that works, but doesn't always work.
And doing generative models on complex valued spectra is challenging because doing
gradient descent on complex numbers can be challenging.
So people often will build generative models of magnitude spectra, so they take the Fourier
transform and throw out the phase, and there are ways of trying to estimate phase and recover
the original signal, or something close to the original signal given on the magnitudes.
And we haven't had that in the brose of for a while, but it's always been on the to-do
list, so we're finally putting that in there, and it's nothing fancy or new, but it's
just there to help people kind of prototype their generative models or, you know, investigate
their systems otherwise.
So a lot of the features and functionality that we've talked about as far are things that
you would use for analysis or for feature extraction, feature expression, are there components
of librosa that are directly doing learning as well, or is it primarily for the, for
kind of audio data preparation or the data science phases?
Yeah, that's a great question.
So there are some things that are built in that include like some very light statistical
training.
So those would be things like detecting node on-sets or detecting beats.
Those have, properly, those are supervised learning problems, right?
You're trying to build some mapping that comes from some input representation to a set
of decisions, whether it's like, is this a B, or is this a new node, or whatever it
happens to be, and those are in there because those are useful pre-processing stages for
other more high-level algorithms, as I mentioned earlier.
But we try to shy away from having too many kind of pre-trained models in librosa, and
the reason for that is that data is fairly scarce still somehow, and we spent a lot of
time in this field analyzing and re-analysing with the same data sets.
So if we started building models in librosa that were pre-trained on some data set, it's
very easy for someone down the road to pick up the library, not realize what the statistical
dependencies of the model that are built in are, and then run it over the same data and
report some number that sounds great, but it's totally vacuous because they're actually
running on the training set.
Got it.
So we try and avoid that, but that would be a good thing to avoid.
Yeah, and there are other libraries out there that do include pre-trained models, and that's
a good thing, as far as I know, they're pretty careful about documenting where those models
come from and how they're fit.
It still makes me nervous to have something that's fairly low level, and likely to be
at a lot of upstream pipelines to be that sensitive to statistics.
But that said, there are some other things that are more unsupervised, so one of the problems
that I like to work on is structure analysis, where you take a song or a time series more
generally and break it into pieces that are connected and maybe recurrent later in the
piece, and that, if you look at it the right way, it's a graph partitioning problem, right?
So each time point becomes a vertex in a graph.
You add edges between vertices that are somehow similar, whether it's similar in time or
similar by repetition features, and then you partition that graph, and that's kind of
an unsupervised problem that's confined to a single recording at a time.
So we give you some tools to work with that sort of stuff because there isn't really a notion
of statistical contamination from other datasets or inductive bias.
So yeah, so we make that kind of stuff easy, but for the most part, we try to shy away
from anything to statistical, but it also makes you an attest to a lot easier.
Right.
Right.
I can see that.
The structure analysis, you mentioned it a couple of times that you went into a little
bit more detail just previously.
Can you talk a little bit more about that?
The graphical analysis element of that, and kind of how that's implemented.
What is the user doing?
Are they just saying go do this, or are they kind of building out a model and kind of
implementing it and you're providing support?
Where does the user's app and what the browser is providing start and end?
Yeah, absolutely.
So for that particular problem, we don't really provide an end-to-end solution in the library,
but we do have an example gallery in the documentation that shows a bunch of kind of not necessarily
small, but self-contained algorithms and projects that take a few different pieces of tools
in LaGrosa and combine them so that you can do some sort of analysis.
So structural segmentation is one of those examples, and that particular method is based
on a paper I wrote four years ago with Dan Ellis, five years ago now.
I don't know.
Some number of years ago.
Is the Laplacian segmentation method paper?
Yeah, that's right.
Okay.
We actually have a new paper coming out that's kind of revising that and replacing sort
of the under-the-hood similarity with something a little bit smarter, but the basic idea is
the same.
Was this paper the first time this graph-based approach to structural segmentation was proposed?
I think it was the first time that it was called out as such.
There were some prior work that had similar ideas and were for more of a signal processing
perspective and didn't really make full use of kind of the graphical underpinnings of
what was going on.
I was thinking specifically of a paper by Harold Grokens and my regular, where they come
up with almost the same kind of method, but in a very different way, so it was interesting
to see that connection play out.
There were a lot of works that kind of approached it from not necessarily a bold graph, but from
a chain graph perspective, so they would have like a mark-up chain that are just trying
to decide, am I in the same section or a new section?
And then you have a much simpler kind of linear chain conditional random fields or
a structure under the problem.
But yeah, I think no one had really done spectral clustering on that before.
It's kind of fiddly and finicky to get right, and we still didn't get it 100% right,
but we got it closer to right in that work.
I'm curious, and I don't know if this question is one that can be answered generally, but
I'm curious about the, you know, if you're taking a given time series sample and making
that a node in a graph and connecting it to other nodes based on this structural analysis,
that's the cardinality of a given node in terms of number of connections.
Is it like very dense or is it 2, 3, 4, or I'm trying to get a feel for kind of the complexity
of this graph?
Yeah, yeah, like what are the degree bounds on it?
So the way that we did it, at least the first time around, was we actually built two graphs
and then merged them.
And what were the two?
So the first graph is taking every point in time and connecting it just to its left and
right neighbors in time.
Okay, so if I'm time T, I have a like to T minus 1 and to T plus 1.
And then you can refine that a little bit by adding some sort of weight on those edges
that kind of measures how similar the features are between T and T plus 1.
So if the sound is basically the same, it has the same kind of spectral shape, then there
will be a strong edge.
But if it goes from like loud, bursty noise down to absolute silence, there will be a
very weak edge.
So you can imagine this makes it a little bit easier to partition the graph and recover
change points.
Okay.
So that's the first graph we call it the conveyor belt because you just move along in
time and that's all you can do.
The second graph was linking between repetitions of the same observations.
And I say same as really main similar.
So if you imagine I play some chord at time T and then I play that same chord at some
other time U, then I'll add an edge between T and U.
And if I can detect that those same sounds have recurred.
So what you're asking about the kind of degree complexity of this graph really depends
on how repetitive it's the recording.
Got it.
In practice, we do some adaptive bandwidth estimation based on the observer features and
do some quantile estimates to say like, well, we're going to build this Gaussian kernel
over the feature space and figure out some nice quantile thresholds such that we get
strong connections above that threshold and very weak connections below it.
Off you go.
It's all the usual kind of kernel tuning literature type stuff there.
So we're talking through this kind of method in general and how much you provide via the
library and how much the end user has to build.
And you made the point that it's not quite end-to-end, but you're providing some of the fundamental
pieces.
Yeah, the way I see that the users that I'm trying to, like my 80% of users are really
the people that are building machine learning systems around audio analysis.
It's not the people that want to do an audio analysis directly, if that's a distinction
that really makes sense.
Most people want to come set to it.
But yeah, so basically I try to build the tools that make it easy to build the actual system.
And so maybe as maybe to wrap things up, you kind of started out in this, started out
down this path, you know, with this very explicit focus on Python as a, you know, direction
and community, you know, versus MATLAB, which was the incumbent, if you will.
And now you're a few years down that path, like how did that all?
I think we know like generally how it worked out.
But for you and this project, I'm curious, are there any non-obvious kind of observations
that you might make around just Python as a tool or community or domain for doing this
kind of work?
So I think for me, timing was everything.
Like I've been developing this library on and off for the last six, seven years now.
And it's like really hitting the steepest point on the Python data science acceptance
curve, I think.
Like I don't think there's a whole lot more growth to be had in terms of number of users
for Python.
I think that's like, like Python is where it is and that's great.
There are other languages that might tip away at some of that, say Julia or mostly Julia.
But the thing that attracted me to Python in the first place was A, that's where a lot
of the data analysis software was being written.
So the machine learning packages were all going that direction.
At the time, it was psychic learning, Theano, now it's PyTorch, Keras, TensorFlow, all
that stuff.
But more than that, it was all the other kind of ecosystem of packages for doing everything
else, right?
Text analysis and web hosting and anything else that you wanted to do, it kind of took
over from Pearl in that respect, at least the way that I see it.
And thank goodness for that.
I know.
Right?
And I don't think that's going away.
And I think that makes a lot of things easy and nice.
But you know, Python does have some limitations, really in terms of scalability and speed.
And the, you know, projects like Numba or Rapids or, you know, all desk, you know, those
are all kind of chipping away at different aspects of this.
Make it easier to scale up and run over, you know, 100,000 recordings, if you want.
That's exciting to see.
It's, you can really see how difficult it is to make that stuff work really well, though.
Yeah.
As an early doctor of those things, I constantly find myself trying to figure out how can
I work around what I anticipate will be difficult a year from now, and that's often difficult.
Yeah, I mean, those are the main things.
I've recently started teaching a big data class here at NYU, which is kind of heavily focused
on how deep in Spark and MapReduce and all that kind of stuff.
And seeing how nice Scala is in a lot of ways and thinking like, man, there is a way to
combine these two things.
That would be great.
But I really don't see any way to do it right now.
Meaning Python and Scala or Spark and Net ecosystem and Python, I mean, there are folks
that do Python stuff on Spark.
It's certainly not as popular as Scala, I don't believe.
Right.
No, more in terms of not just Python, but so, for example, in Librosa, we have a lot of
stuff that calls out to packages written in C or Numba compiled functions or things that
don't necessarily integrate well with Java-based language.
Got it.
PySpark is great in that it hides a lot of that stuff, but it also is not so great in that
the data serialization issues can really bite you and you have to move in between the
JPM and the Python environment.
And maybe that's not a bad thing if the data you're moving is small, but if it gets large
then that's a problem.
And large is a point if you're going to go through the trouble of getting a Spark cluster
up and running.
I mean, maybe if you have your data living in HDFS and you can just like pull in some
identifier that tells you where to get the audio content and then you analyze it in Python,
that's fine.
But if you're pulling in blogs of decoded audio data, that's really expensive.
So, there are some subtleties to making these things really integrate nicely together.
I think we still haven't quite figured out as a community.
I don't know.
It seems like the, especially the column store stuff seems to be making a lot of those
transitions easier, so projects like Feather or Arrow, things like that.
It's nice to see those things kind of take it off.
Cool.
Well, Brian, thanks so much for taking the time to chat with us about what you're working
on.
Librosa sounds like a really interesting tool and having kind of not played with it,
but kind of poked around a little bit in the docs and as someone who does a lot of work
with audio, it looks like a really interesting environment and a way to play with kind of
get up to speed with doing some machine learning stuff on audio.
Yeah.
Oh, thanks for having me.
This is fun.
Absolutely.
Thank you.
All right, everyone, that's our show for today.
If you like what you've heard here, please do us a huge favor and tell your friends about
the show.
And if you haven't already hit that subscribe button yourself, make sure you do so you
don't miss any of the great episodes we've gotten in store for you.
As always, thanks so much for listening and catch you next time.
