WEBVTT

00:00.000 --> 00:09.920
All right, everyone. I am here with your Donnus,

00:09.920 --> 00:16.080
Katanidis. Your Donnus is research director at CNRS in Paris.

00:16.080 --> 00:19.440
That's the National Center for Scientific Research,

00:19.440 --> 00:24.240
as well as the head of quantum algorithms with QCWARE.

00:24.240 --> 00:27.680
Your Donnus, welcome to the Twelma AI podcast.

00:27.680 --> 00:32.320
Thanks for having me. I'm looking forward to this talk.

00:32.320 --> 00:35.520
It's been a while about a year, a little over a year,

00:35.520 --> 00:39.920
since we talked about quantum machine learning on the show here.

00:39.920 --> 00:44.720
And I imagine that the field has advanced quite a bit.

00:44.720 --> 00:49.120
So I'm looking forward to an update as well as your take.

00:49.120 --> 00:54.720
You recently delivered a main conference keynote at ICML on the top.

00:54.720 --> 00:59.360
So can we say that quantum machine learning is going big time now or blowing up?

01:01.680 --> 01:06.080
I always say that quantum machine learning is the most overhyped

01:06.080 --> 01:09.280
and underestimated area of quantum computing.

01:09.280 --> 01:11.680
And it can be both in superposition, right?

01:11.680 --> 01:16.160
So I think we have many exciting new results.

01:16.160 --> 01:22.240
This is true. And to me, it was very important to give this talk at ICML,

01:22.240 --> 01:27.040
because I really believe in the fact that we need to work together

01:27.040 --> 01:29.280
the quantum and the classical ML community,

01:29.280 --> 01:31.360
because you have the problems.

01:31.360 --> 01:36.320
You know what are the bottlenecks on the computational side?

01:36.320 --> 01:39.920
You've been looking at these problems and the specifics for many years.

01:39.920 --> 01:41.600
We are coming from quantum algorithms.

01:41.600 --> 01:45.760
We understand what quantum computing can offer.

01:45.760 --> 01:48.720
And if we put these things together, I think we can do great things together.

01:49.920 --> 01:51.680
So tell us a little bit about your background.

01:51.680 --> 01:54.400
How did you come to work on quantum machine learning?

01:55.680 --> 02:01.280
So I started working on quantum algorithms more general 20 years ago.

02:01.280 --> 02:05.440
So I started my PhD in 2000 in Berkeley.

02:05.440 --> 02:07.600
So I was in California for a few years.

02:08.720 --> 02:14.160
And there I worked on different things, mostly quantum algorithms for communication

02:14.160 --> 02:21.520
networks, and for cryptography, and some quantum algorithms, not so much on machine learning.

02:21.520 --> 02:25.840
And the only machine learning I did was a classical result on

02:26.960 --> 02:30.160
classical recommendation systems through an interest you that I did with

02:30.160 --> 02:33.920
Pravagkar Lagavan at some point in the beginning of 2000.

02:34.800 --> 02:37.600
And after that, I went to MIT.

02:37.600 --> 02:40.080
I worked with Peter Schor for a couple of years.

02:40.080 --> 02:45.360
And then I moved to Paris in 2006 as a research director for CNRS.

02:46.560 --> 02:51.040
Okay. And so you've been working on quantum algorithms for 20 years.

02:51.040 --> 02:53.680
How long is quantum algorithms even been a thing?

02:56.320 --> 02:57.440
Not much more than that.

02:57.440 --> 02:58.320
21 years.

03:01.360 --> 03:06.080
So I think the first breakthrough result that people

03:06.080 --> 03:11.360
cite on quantum algorithms is Peter Schor's algorithm for factoring large numbers.

03:11.360 --> 03:15.440
And this was in 93, so it's like 27 years ago.

03:16.080 --> 03:21.520
And this was kind of the result that made people very interested in looking at this new model of

03:21.520 --> 03:30.960
computation and try to figure out what does it have to offer when we look at trying to

03:30.960 --> 03:37.280
compute and code information using quantum mechanics and not classical physics.

03:38.960 --> 03:46.720
So to me, it's a very exciting thing because it kind of tries to understand in some sense also

03:46.720 --> 03:52.720
what are the computational limits of nature, because nature is a quantum mechanical system.

03:52.720 --> 03:55.200
So this is what we try to understand.

03:56.000 --> 04:00.800
And maybe it's important to say in the beginning that quantum computing is not

04:00.800 --> 04:06.800
a faster processor, right? So it is not that everything that you run nowadays on a classical

04:06.800 --> 04:10.960
computer, you can just put it on a quantum computer and it will run faster.

04:10.960 --> 04:13.840
This is really not how it works.

04:13.840 --> 04:17.920
It's a completely different paradigm.

04:17.920 --> 04:20.080
It's a different way of computing.

04:20.080 --> 04:26.720
And we need to design new algorithms and different algorithms in order to harness the power of

04:26.720 --> 04:33.280
quantum mechanics. And we can do it for certain tasks and we can provide algorithms that run

04:33.280 --> 04:37.760
much faster, even exponentially faster sometimes compared to classical computers.

04:37.760 --> 04:39.200
But this is not always the case.

04:39.200 --> 04:44.640
For many tasks, a classical computer will be as good as a quantum computer.

04:44.640 --> 04:49.520
So we need to figure out exactly for what applications a quantum computer can offer

04:49.520 --> 04:54.880
something more, whether it is about speed up, speed or speed or performance.

04:54.880 --> 04:59.200
And to do that, we need to look at the algorithms and design algorithms.

05:04.800 --> 05:11.440
In what ways has the way that we think about the computing itself,

05:11.440 --> 05:16.960
a quantum computing change over these 27, you know, both years?

05:16.960 --> 05:24.960
As it just been advancing, you know, getting more qubits or has the fundamental architecture

05:24.960 --> 05:27.440
or approaches of quantum computing change?

05:29.120 --> 05:34.000
Yeah. So I guess in the beginning, there were two quite disjoint communities that were working

05:34.000 --> 05:38.880
on quantum information. I come from the theoretical computer science part.

05:38.880 --> 05:44.720
And there, quantum computing was something which it was completely theoretical.

05:44.720 --> 05:50.560
It was part of like the theoretical computer science groups as in Berkeley or in other places.

05:51.200 --> 05:57.840
And it was just a model of computation that we were trying to understand what we can do with it.

05:57.840 --> 06:00.960
It was something completely mathematical and abstract.

06:00.960 --> 06:06.320
We had no idea if it would ever, you know, realize exactly.

06:06.880 --> 06:11.040
But in some sense, we were almost mathematicians so we didn't care about these things, right?

06:11.040 --> 06:15.840
It was a great thing to do math and we were proving theorems, right?

06:15.840 --> 06:23.200
At the same time, physicists were trying to actually implement and figure out ways to control

06:23.200 --> 06:29.840
these very, very small quantum systems that could be one electron or one photon or one atom, right?

06:29.840 --> 06:34.800
And in order to be able to control these type of systems, you need the very, very precise ways

06:34.800 --> 06:40.960
of doing it. So, and physicists in the beginning were doing this because they wanted to study physics,

06:40.960 --> 06:45.840
right? And understand the laws of physics. And little by little, you know, the two communities

06:45.840 --> 06:49.840
started coming together because on our side, we were saying, look, if we had the quantum

06:49.840 --> 06:54.640
computer, there are all these amazing things that you can do. And I think the physicists realized

06:54.640 --> 07:01.040
that maybe there is more to do than study physics. And they started getting into doing circuits

07:01.040 --> 07:09.200
and gates and cubits. And and little by little, we started seeing the first very small quantum

07:09.200 --> 07:15.760
machines. We are quite far still from having a universal fault tolerant quantum computer.

07:16.320 --> 07:20.720
But we have seen many strides when it comes to the hardware development.

07:21.360 --> 07:26.000
We've seen Google's announcement a few months ago about the supremacy experiment.

07:26.000 --> 07:32.480
Many other technologies are, you know, they are not only for superconducting cubits,

07:32.480 --> 07:40.000
but with trapped ions and other technologies. And we also, I think, have many advances when

07:40.000 --> 07:46.400
it comes to thinking algorithmically about this model of computing. There will be many new algorithms.

07:46.880 --> 07:52.080
And quantum machine learning is something that started quite later. So, I think that the result

07:52.080 --> 08:00.720
that kind of initiated this, the subfield was the quantum algorithm for solving linear systems.

08:00.720 --> 08:08.080
And that was sometime in 2009. So, I could say that quantum machine learning started maybe very,

08:08.080 --> 08:17.040
you know, sporadically from 2009. And then we had kind of the first end-to-end application or

08:17.040 --> 08:23.920
quantum recommendation system in a theoretical paper sometime in 2016. So, between five and ten

08:23.920 --> 08:31.760
years of quantum machine learning. And it was at the quantum recommendation algorithm that

08:32.640 --> 08:41.680
that Ewan Tang did that research about. We did a show with Ewan just about a year ago April

08:41.680 --> 08:48.960
of last year. Some of that research. Yeah. So, yes, it was exactly this quantum recommendation

08:48.960 --> 08:56.800
system. It's a very interesting story, the sense that, you know, I was getting to know this new

08:56.800 --> 09:01.760
quantum techniques that had to do with linear algebra. And because I had worked on classical

09:01.760 --> 09:08.000
recommendation systems back in 2001, I kind of figured out that maybe this is the right problem

09:08.000 --> 09:14.080
to actually look into and try to find the quantum algorithm. So, we started working with

09:14.080 --> 09:22.560
Michael Thor and Pamprakash. And we came up with this quantum application. And in the beginning,

09:22.560 --> 09:28.000
we, you know, we benchmarked our result compared to the best classical results that existed out

09:28.000 --> 09:34.240
there. And we could say that there was an exponential gap between our quantum algorithm and the

09:34.240 --> 09:42.320
classical algorithm. And then probably Ewan, you know, said more about this, she started working

09:42.320 --> 09:47.040
on this problem in the beginning in order to prove that classical algorithm is cannot do better

09:47.040 --> 09:52.240
than the ones we already had. But little by little, she realized that maybe there are some different

09:52.240 --> 09:57.920
techniques inspired by our quantum algorithm that could give you a new classical algorithm,

09:57.920 --> 10:05.600
right? And this is what happened. She came up with a very nice classical result that showed that

10:05.600 --> 10:12.000
at least in theory that the gap, the speed, the speed up that we can expect between the quantum

10:12.000 --> 10:18.320
and the classical algorithm is not any more exponential, but it's polynomial. And of course,

10:18.320 --> 10:24.480
the polynomial in the absolute is less than an exponential gap. But somehow this is not the end

10:24.480 --> 10:30.240
of the story in the sense that this polynomial is actually a very, very large polynomial,

10:30.240 --> 10:35.120
meaning that if you look at instances where you would want to do recommendation systems,

10:35.120 --> 10:41.040
like looking at Amazon or Netflix or, you know, online purchase systems like that,

10:41.040 --> 10:46.080
then actually this polynomial gap is even bigger than the one we had before, which was an exponential

10:46.080 --> 10:52.960
one. But somehow, from a theoretical point of view, it says that the gap that we have cannot be

10:52.960 --> 10:58.400
exponential is only polynomial. But from a practical point of view, it's to be more interesting

10:58.400 --> 11:03.280
when it comes to machine learning because we really need to solve real problems. The gap is even

11:03.280 --> 11:08.400
bigger than it was before. So, so we still have to see how to take this gap and actually

11:08.400 --> 11:14.800
implemented in a quantum computer that we don't have yet. And there will be, you know, slowdowns

11:14.800 --> 11:19.840
because the clock speed of a quantum computer is not as fast as a classical computer and things

11:19.840 --> 11:26.560
like that. But we still have a very decent theoretical gap of something like a million or a

11:26.560 --> 11:31.840
billion times faster, right? And hopefully out of this billion times faster, we can keep something

11:31.840 --> 11:37.120
that makes sense. Even if you say it's a thousand times faster in practice, this would be a really,

11:37.120 --> 11:45.920
really great thing to do. Also speaks to the idea that even if we don't ever get quantum computers

11:45.920 --> 11:51.120
that can run these algorithms, perhaps we learn, you know, from the approaches we're developing

11:51.120 --> 11:57.280
and exploring quantum algorithms and kind of bring some of what we learn back to classical algorithms.

11:57.280 --> 12:03.360
This is absolutely true. And this was a great example on the work of you and on the

12:03.360 --> 12:08.480
recommendation systems. In machine learning, there are more examples like that. For example,

12:08.480 --> 12:13.840
when it comes to neural networks, which is something that we know that it works very well,

12:13.840 --> 12:19.200
but we don't necessarily understand why and why this one doesn't, the other one doesn't somehow.

12:20.000 --> 12:24.480
Like we've been trying to figure out what is the right architectures for defining some quantum

12:24.480 --> 12:31.120
neural networks. And the difference there, for example, I have to do with things that it's not

12:31.120 --> 12:36.720
easy to apply an own linearity because it quantum is a reversible and linear evolution.

12:36.720 --> 12:43.280
So then you have these extra constraints that make you think of different ways of defining

12:43.840 --> 12:48.800
neural networks. And this can also go back to classical neural networks and define new ways

12:48.800 --> 12:54.160
of doing classical neural networks that could use the intuition of the quantum to provide

12:54.160 --> 12:59.440
classical neural networks that can be more accurate or more efficient. So there is a lot of

12:59.440 --> 13:03.680
back and forth between classical and quantum information. And this is why we need to work more

13:03.680 --> 13:10.080
closely with the classical ML community as well. Maybe we can take a step back and have you

13:12.160 --> 13:20.240
ground us on quantum computing and what are the fundamental ideas there? I'll be honest,

13:20.240 --> 13:28.640
I've heard it numerous times. It's still difficult to wrap my head around, supervisition and

13:28.640 --> 13:36.240
similar concepts. And so maybe you can start by talking about what you think are the key ideas

13:36.240 --> 13:45.680
for folks that are coming from classical approaches. So yeah, it's not an easy task to do to explain

13:45.680 --> 13:54.560
quantum in a live, but I will do my best. I guess the first comment is to say that one does not need

13:54.560 --> 13:59.120
to really understand quantum mechanics and all the postulates of physics and quantum mechanics

13:59.120 --> 14:03.600
in order to start playing around with quantum algorithms and quantum information.

14:03.600 --> 14:09.120
So for us, it's mostly a mathematical model that we need to understand, which is a little

14:09.120 --> 14:14.080
different than the one that we use for classical computing. And once you have this mathematical

14:14.080 --> 14:20.000
model in place, then in some sense the physics part stops and then the algorithmic and the

14:20.000 --> 14:25.360
computer science part starts. So the main difference between classical and quantum computing,

14:25.360 --> 14:30.800
people probably have heard this many times, is that the carrier of information is not a bit,

14:30.800 --> 14:36.320
which is a zero or a one, but it's a quantum bit, which can be a zero and one at the same time.

14:37.760 --> 14:41.840
It's not very different than saying that I have a random bit, that sometimes it's zero,

14:41.840 --> 14:50.640
sometimes it's one. So you can define the space of this random bit with two probabilities,

14:50.640 --> 14:57.360
the probability of getting zero and the probability of getting one. So you can associate in a random

14:57.360 --> 15:03.280
bit a two-dimensional vector. It's the same thing that quantum, you can associate in a quantum bit

15:03.280 --> 15:09.520
a two-dimensional vector, but now this vector doesn't have positive probabilities that sum up to one.

15:09.520 --> 15:16.320
It has complex numbers whose squares sum up to one. In other words, if you have a quantum

15:16.320 --> 15:22.480
system, you can think of a quantum system as a vector in some very high-dimensional

15:22.480 --> 15:29.200
Euclidean space. The same way that if you have a random variable, which is more than one bit,

15:29.200 --> 15:37.600
you can think of a distribution, which is in an exponential space and you have positive

15:37.600 --> 15:43.440
probabilities that sum up to one. Here, the quantum system is defined by, again, an exponential

15:43.440 --> 15:50.080
size vector with positive or negative numbers, actually even complex numbers whose squares sum up

15:50.080 --> 15:57.760
to one. So this is the basic difference, right, of how do you encode information and what is the

15:57.760 --> 16:03.840
carrier of information in quantum? And once you have this, then you want to understand how can

16:03.840 --> 16:08.080
they evolve the system? So I have a quantum system. What kind of operations can they apply to

16:08.080 --> 16:17.120
this quantum system to evolve it? And it's very simple in some sense because you want to evolve

16:17.120 --> 16:22.960
a quantum system in a way that the quantum system remains a quantum system. So since the quantum

16:22.960 --> 16:30.400
system was this vector in Hilbert space with some fixed norm, let's say norm one, then the only

16:30.400 --> 16:35.360
kind of operations that you can apply to it is some unitary operation because this is the

16:35.360 --> 16:41.120
operation that keeps the norm of the vector the same, right? The same way that we're saying,

16:41.120 --> 16:46.640
if I have a probability distribution, how can I evolve it? I can apply a stochastic matrix

16:46.640 --> 16:51.440
because it keeps probability distribution, it will give you probability distribution.

16:51.440 --> 16:56.320
Here you have a quantum state, you apply a unitary matrix, you're going to get a different quantum

16:56.320 --> 17:02.960
state, that's still a quantum state, right? So this is easily the way of applying operations on

17:02.960 --> 17:08.400
quantum systems. The quantum system is a vector, you apply a unitary matrix, you get a different vector,

17:09.200 --> 17:14.000
right? And the second thing, and this is also very important that you can do to this quantum

17:14.000 --> 17:20.560
system is that you can try to observe it, right? But kind of what we know from the postulate

17:20.560 --> 17:25.760
of quantum mechanics is that when you observe something, this is not just the passive observation

17:25.760 --> 17:30.880
that doesn't change anything, but the moment you try to observe the system, the state of the

17:30.880 --> 17:39.280
system changes itself, right? So the quantum measurement just specifies what will be the outcomes

17:39.280 --> 17:44.320
that you're going to get, the possible outcomes that you're going to get out of this observation,

17:44.320 --> 17:50.240
and with what probability each outcome will come up, right? So when you measure something,

17:50.240 --> 17:56.080
then you'll have a distribution over possible classical outcomes that come from this quantum

17:56.080 --> 18:02.720
state that you had before. So I know it's a mouthful, but basically if we understand this,

18:02.720 --> 18:08.320
this the notion of a tube bit, and the fact that I can apply a unitary operation to it,

18:08.320 --> 18:14.160
or I can measure it and observe some classical information out of it, then we basically have

18:15.280 --> 18:18.560
the basics that we need in order to start talking about quantum algorithms.

18:18.560 --> 18:27.280
And maybe the one simple quantum procedure that I can discuss and it's quite interesting for

18:27.280 --> 18:34.240
machine learning is the one that has to do with estimating the distance between quantum states,

18:34.240 --> 18:42.080
right? So you can think of two quantum states as encodings of two data points, right? And one

18:42.080 --> 18:46.560
of the things that we need to do many times in machine learning is figure out the similarity

18:46.560 --> 18:51.280
between these two points, right? So you want to estimate something like the inner product

18:51.280 --> 18:58.640
between these two points. As long as these data points are now encoded into this quantum state,

18:59.200 --> 19:07.200
it's fairly efficient to compute the distance or the inner product between these data points.

19:07.840 --> 19:12.880
And this is one of the things that we use in order to do, for example, classification or

19:12.880 --> 19:21.920
clustering based on similarity learning. I can talk maybe about one more, a little more elaborate

19:21.920 --> 19:27.440
thing that we can do without getting into many of the details, which is linear algebra procedures.

19:28.080 --> 19:33.200
And again, I'm talking about this because machine learning is a lot of, you know, linear algebra,

19:33.200 --> 19:40.160
whether you want to train neural networks, but also when you want to do more traditional machine

19:40.160 --> 19:46.320
learning like principal component analysis or support vector machines, they're always these

19:46.320 --> 19:51.200
matrices that you have to handle. You need to figure out, you know, top eigen spaces,

19:51.200 --> 19:59.600
eigenvectors and things like that. So quantum is particularly powerful in performing this type

19:59.600 --> 20:04.560
of computations that have to do with eigenvalues and eigenvectors in a more efficient way.

20:04.560 --> 20:10.720
These tools are very powerful, but they're also very subtle. And this is where a lot of the hype

20:11.520 --> 20:17.520
comes also when it comes to quantum machine learning. These procedures are not always faster,

20:18.080 --> 20:22.640
but they can be faster if we really pay attention and we apply them to the right

20:23.200 --> 20:29.120
applications and do the right use cases. And when we do that, this is how we can get very fast

20:29.120 --> 20:35.280
to accommodation systems faster both than the classical or the quantumly inspired classical

20:35.280 --> 20:40.640
still but much faster when we applied to things like spectral clustering or expectation

20:40.640 --> 20:49.680
maximization, things where this linear algebra part is really the bottleneck of the computation.

20:51.040 --> 20:56.800
So you alluded to this earlier and I think it's really coming out in this conversation around the

20:56.800 --> 21:04.080
algorithms, the speed up that we're talking about here doesn't come from just running it on

21:04.080 --> 21:09.520
a super fast hardware that actually doesn't really exist. It's because there's something fundamental

21:09.520 --> 21:18.400
about the way we can work with qubits that we can't do bits. And I'm thinking of it as a non-linearity

21:18.400 --> 21:22.160
that's probably not the right way to think about it, but you can do the different things.

21:22.160 --> 21:26.240
Can you help us get to the essence of what these different things are that we can do that make

21:26.240 --> 21:33.680
better? Sure. I will do my best. Again, you know, quantum mechanics is not the easiest thing

21:33.680 --> 21:37.200
to get into issue about and there are many things that we don't really understand.

21:39.600 --> 21:50.160
But I think the main idea of why you would expect a quantum algorithm to be faster, for example,

21:50.160 --> 21:56.560
than a classical algorithm, is because we can utilize this notion of superposition. When I talk

21:56.560 --> 22:03.120
about superposition, it means that for example, imagine you want to estimate the distance between

22:03.120 --> 22:10.400
one point and many different points. It could be the centroids that you have calculated from

22:10.400 --> 22:17.520
different classes or some other data points. What we can do is things like, instead of estimating

22:17.520 --> 22:23.120
the distance of the point with each one of the data points one after the other, so we have to

22:23.120 --> 22:29.520
spend a lot of time if the number of these points is great, right? We can kind of go into a super

22:29.520 --> 22:36.480
position of the data points and kind of start estimating fast things that have to do with the

22:36.480 --> 22:45.760
average distance or things like that. So I would maybe say that it's some sort of parallelism

22:45.760 --> 22:51.200
that is happening at some point, but we should not think of it as a parallel computer either.

22:52.160 --> 22:55.840
So it's not a nonlinear computer and it's not a parallel computer.

22:57.120 --> 23:03.040
And it's certainly not just a faster processor, right? So I usually get this question,

23:03.760 --> 23:08.320
shouldn't be just simpler if we just get a compiler that will take my classical algorithm,

23:08.320 --> 23:12.960
make it into a quantum algorithm and then I don't have to learn anything new and I will be done.

23:12.960 --> 23:21.280
It's not that easy. It's not that easy because you really have to understand mentally. It's just not

23:21.280 --> 23:30.240
that easy. It's fundamentally different. You can use this parallelism, the super position.

23:30.240 --> 23:34.880
At the same time, as we said, every time you try to extract information out of these things,

23:35.440 --> 23:41.840
you end up destroying your states and you get only a small part of the information. So there's

23:41.840 --> 23:49.120
this interplay between using this big Hilbert space to encode information, but figuring out very

23:49.120 --> 23:53.760
clever ways of extracting the information that you need and not care about the rest.

23:54.560 --> 24:00.080
So I guess this is the best way I could explain it in a couple of minutes.

24:00.720 --> 24:08.480
What I'm hearing is something, the picture that's forming, that's probably inaccurate, is

24:08.480 --> 24:17.920
something along the lines of classical computing. We do a lot of iterative types of computation

24:18.560 --> 24:26.000
and there is an element of these properties of quantum super position and observation that

24:26.720 --> 24:35.200
allows us to look at a quantum data structure and get a lot of what we might otherwise have to

24:35.200 --> 24:40.640
iterate in classical computing. Is there any of that that is true?

24:46.880 --> 25:00.880
Yes, I know. No, no, it's true that I think a different way of looking at it could be that

25:00.880 --> 25:08.160
what the quantum computer enables you to do is kind of search many different computational paths

25:08.160 --> 25:15.200
at the same time, but this is not enough because then if you just say, let me search all of them in

25:15.200 --> 25:20.400
super position and then let me measure, then what you get is just a random path, which you could

25:20.400 --> 25:26.560
have done it also by a randomized classical algorithm, right? So what you need to do is first of all

25:26.560 --> 25:31.920
go into a super position of these paths, but then figure out clever ways of just

25:32.800 --> 25:38.480
getting rid of the paths that you don't like so that at the end, only the good paths that you

25:38.480 --> 25:43.600
that will lead to a solution remain as possibilities of the quantum algorithm.

25:44.720 --> 25:52.000
So there is this inherent stochasticity in quantum, which is also in randomized algorithm.

25:52.000 --> 25:57.520
The next thing is that because as I said, we don't deal only with probabilities, but also with

25:57.520 --> 26:04.160
this positive, negative, complex amplitudes, there is a lot of interference between these paths,

26:04.160 --> 26:09.840
and if you are clever enough, you will interfere the back paths and you will make them disappear,

26:09.840 --> 26:14.800
and you will only get good paths that lead to solutions at the end of your algorithm.

26:14.800 --> 26:27.840
Okay. And so you've got these kind of principles like being able to do distances and linear algebra,

26:28.320 --> 26:32.640
how does that get us to quantum machine learning algorithms and kind of what's

26:32.640 --> 26:39.120
different, you know, state and landscape of quantum ML? Yeah, it's a very nice question.

26:39.120 --> 26:45.280
So I can start maybe from the simplest thing, so we can start with supervised learning and,

26:45.280 --> 26:50.640
you know, the first thing that one might want to do would be some sort of classification, right? So

26:52.160 --> 26:55.840
again, there are two different ways of, okay, there are many different ways of trying to do

26:55.840 --> 27:03.040
classification. One of them is based on similarity learning, right? So you somehow map your data

27:03.040 --> 27:08.160
points into some points in space, and then you try to figure out which points are close to each

27:08.160 --> 27:12.800
other so that you can give them one label and which other points are far and close so you can

27:12.800 --> 27:16.800
give them a different label, right? So this is things that kind of everyone knows in classical

27:16.800 --> 27:21.600
machine learning, we are, you know, getting to understand more and more what's happening.

27:21.600 --> 27:26.160
And again, there one thing, the simplest thing you can do is to say that, okay, every time you want

27:26.160 --> 27:32.000
to estimate the distance, why don't you use a quantum computer to estimate the distance, right?

27:32.000 --> 27:37.040
But you can go a little bit more in the more quantum versions of these things, where you say,

27:37.040 --> 27:43.520
okay, but maybe not only do I want to estimate each distance separately, but imagine that I

27:43.520 --> 27:50.240
want to, I have a point, I have a bunch of different centroids, I want to soft classify my point,

27:50.240 --> 27:58.800
depending on which centroid is closer to me, right? So different things you can do is not do it one

27:58.800 --> 28:04.720
by one sequentially, but again, go to the superposition of the centroid, and this will allow you to sample

28:04.720 --> 28:09.360
the right centroid with the correct probabilities, for example. So these are kind of the things that

28:09.360 --> 28:14.480
you can do in the simplest problems, like you already have your point and you want to classify them

28:14.480 --> 28:22.800
in the space that you are, right? If you want to go to a next level, we can say that many times

28:23.760 --> 28:29.360
before being able to classify your points, you need to pre-process your data and map them from

28:29.360 --> 28:35.280
one space to a different space, for example, through some dimensionality reduction techniques. We can

28:35.280 --> 28:40.960
think of principle component analysis or feature analysis, linear discriminant and all this type of

28:41.840 --> 28:47.360
very powerful classical techniques. And the reason they are very powerful is also because they

28:47.360 --> 28:52.720
are computationally hard to do, right? Because you need to figure out the eigenvectors and the

28:52.720 --> 28:59.760
eigenvalues of your of your data matrices. And there we can use more powerful and elaborate

28:59.760 --> 29:06.720
quantum procedures that have to do with inverted matrices, finding eigenvectors. So this mapping

29:06.720 --> 29:14.960
from a higher dimensional space of your data points to a smaller dimensional space where you

29:14.960 --> 29:20.320
believe that the classification will be good. For example, by looking at your top eigenspace,

29:20.320 --> 29:26.960
then this mapping can also be done in a quantum way. And this is usually most of the times the

29:26.960 --> 29:33.040
bottleneck of the classification, how to find the right space to put your points.

29:35.360 --> 29:40.480
We can also try to define quantum deep learning and quantum neural networks.

29:40.480 --> 29:48.240
Before we go. So you started talking about quantum with quantum supervised learning.

29:48.240 --> 29:57.520
You know, we've you've got this the way you described it was used quantum computing for

29:58.560 --> 30:05.840
the distance part of that task as opposed to the entire task. Is that typical of a quantum

30:05.840 --> 30:10.320
approach that you kind of cherry pick a particular part that's hard classically and apply

30:10.320 --> 30:18.880
quantum to it as opposed to end and quantum. Both both cases are possible, right? For example,

30:19.520 --> 30:26.000
if you have an algorithm like I give you a number and you need to factor the number into two prime

30:26.000 --> 30:31.200
factors, then the entire algorithm is equal to algorithm. And because the entire algorithm is

30:31.200 --> 30:35.760
a quantum algorithm, this is why we need like millions and millions of tubes to actually implement

30:35.760 --> 30:46.640
this algorithm, right? There are more hybrid algorithms where there is a classical outer algorithm

30:46.640 --> 30:54.720
and then at the specific points, one can do a specific part of the computation on on a quantum

30:54.720 --> 31:00.720
computer, get back a result and continue with the classical program, the classical algorithm,

31:00.720 --> 31:05.600
right? For example, you already get ready to say that when I was discussing about something like

31:05.600 --> 31:11.760
near a central classification, many parts will still happen classically because quantum cannot

31:11.760 --> 31:18.720
offer something to that, right? Figuring out the centroids of a labeled set of data, it's pretty

31:18.720 --> 31:24.160
simple, it's never the bottleneck classically, so let's do it classically, right? Then at some

31:24.160 --> 31:29.280
points, when you have to find distances or you have to start projecting data points to different

31:29.280 --> 31:35.040
spaces, then for the specific task, we can use a quantum computer and then we can, you know,

31:35.040 --> 31:40.400
continue within our classical algorithm. So for the way we do it, that you see where, for example,

31:40.400 --> 31:46.960
when we say we have a quantum nearest centroid algorithm, what it looks to someone who wants to

31:46.960 --> 31:53.200
use it is basically the same that using psychic learn to do classical nearest centroids,

31:53.200 --> 31:58.080
there is a Python environment, you have a notebook and you run it, so this is exactly what

31:58.080 --> 32:05.840
the quantum thing feels like, you just say instead of run your classical nearest centroid and

32:05.840 --> 32:12.080
fit your data and predict it, you say fit and predict your data with the quantum nearest

32:12.080 --> 32:16.880
centroid. And what happens at the back is that there is a classical program that at some point

32:16.880 --> 32:22.320
says send this data to your quantum computer, measure your quantum computer, get some classical

32:22.320 --> 32:29.440
data out and continue with your classical computation. Another question that this raises for me is,

32:29.440 --> 32:34.160
I think of, you know, when you've got these quantum algorithms operating in the

32:35.680 --> 32:42.880
context of classical approaches and classic problems and kind of regular data,

32:45.840 --> 32:51.200
I guess there was a part of the way I was thinking about this that wanted the classical algorithms

32:51.200 --> 32:57.760
to be operating on classical data, like really complex data with these, you know, weird states

32:57.760 --> 33:03.920
and, you know, crazy tensions and things like that. You know, what's the relationship between

33:03.920 --> 33:10.160
the data and the quantum algorithms and do you have to do something to data to make it

33:10.160 --> 33:13.440
operable in a quantum environment? It doesn't have to be more complex.

33:13.440 --> 33:21.120
It's a very good question. You're really pinpointing on one of the very important

33:21.120 --> 33:26.720
subtle points for quantum machine learning. And this is what do we do with classical data

33:26.720 --> 33:33.200
and how do we load classical data into a quantum format, which is the one that we need in order

33:33.200 --> 33:38.400
to run our quantum algorithms, right? So let me try to explain a little bit what this is.

33:38.400 --> 33:44.560
So you have some classical data, you have some classical description of a point in some

33:44.560 --> 33:52.320
end-dimensional space, right? What we would need in order to apply some fast, for example,

33:52.320 --> 33:56.960
quantum linear algebra techniques or estimating the distances and things like that,

33:56.960 --> 34:03.520
we need some quantum state that in some sense encodes this classical data point into this quantum

34:03.520 --> 34:11.920
state, okay? And this is not a priori trivial thing to do. We are asking to create a quantum state

34:11.920 --> 34:19.840
out of a classical description of classical data. So there are many different ways of approaching

34:22.000 --> 34:30.240
this task. One is to say that we will need to develop some special type hardware,

34:30.240 --> 34:36.720
like your classical RAM that you have on your computer, that you can say, okay, just load the

34:37.760 --> 34:45.040
data that exists in position 35, then your RAM goes to position 35 and brings back this data.

34:45.040 --> 34:49.680
You don't have, you know, as it used to be with tapes and this starts from the beginning and,

34:50.320 --> 34:55.440
go all the way to the point that you want. So you have this fast access to the data, right?

34:55.440 --> 35:01.840
What we would need for a quantum computer is also to have some sort of fast quantum access

35:01.840 --> 35:09.680
to this data, right? So there were a few proposals that had to do with some kind of more exotic

35:09.680 --> 35:17.120
technological things to do it on hardware, right? What we try to do with QCware and I've been working

35:17.120 --> 35:23.600
on this for quite some time now is to figure out can we efficiently load classical data with

35:23.600 --> 35:28.960
current quantum technology? Like I don't want to use anything that doesn't exist. I want to use

35:28.960 --> 35:36.240
machines that Google or IBM is coming up with, right? And can I use these types of machines to load

35:36.240 --> 35:41.840
classical data on quantum states? And what we found is kind of the optimal ways of doing it.

35:41.840 --> 35:49.600
So the optimal way says that the circuit that you need in order to load a data point that has

35:49.600 --> 35:56.160
and features must have size n otherwise you're missing your data, right? But you can have it

35:56.160 --> 36:00.880
in a very, very shallow way. So the depth of the circuit that kind of corresponds to the time

36:01.520 --> 36:08.000
that it will take for the quantum circuit to actually apply this operation is only logarithmic

36:08.000 --> 36:15.680
on the dimension of the data. Which means that I need these two bits which have optimal size

36:15.680 --> 36:22.480
and I can make them very, very shallow, which means very, very fast. Okay? And this is one of the

36:22.480 --> 36:30.160
ways, one of the bottlenecks that we managed to get over with. And this is why if someone told me

36:30.160 --> 36:36.400
maybe three years ago how far is real quantum machine learning applications? I would have predicted

36:36.400 --> 36:40.400
something which would have been further down the road than I could say now.

36:40.400 --> 36:56.560
It sounds like the the process of applying quantum algorithms then if it assumes that you are

36:56.560 --> 37:05.360
accessing quantum data and that quantum data was originally real world data points say some

37:05.360 --> 37:15.680
time series data points or something. Does the process of using quantum algorithms inject some

37:15.680 --> 37:22.320
noise, you know, noise in a sense of, you know, these kind of, you know, static data points are

37:22.320 --> 37:32.320
projected into some probabilistic quantum space. And, you know, is that a disadvantage that

37:32.320 --> 37:37.840
quantum has to overcome in order to be useful on? I don't know, what is not, is there a set of,

37:37.840 --> 37:43.760
I guess you could apply quantum to physics problems and you've got this inherently quantum

37:44.560 --> 37:55.360
source, but for everything else. Yeah, very good question. So we will have to deal with noise

37:55.360 --> 38:01.440
in the quantum setting. And we do have to deal with noise first because the computers that we have

38:01.440 --> 38:09.200
now that we will have in a few years will be noisy, meaning that when I have a qubit and I tell my

38:09.200 --> 38:14.880
qubit just go to the state, the qubit doesn't really go to this state but to a state close to the state.

38:17.440 --> 38:22.640
And this is because it's extremely difficult to really control quantum systems at that level

38:22.640 --> 38:29.680
of precision. So there will be noise and this is why we call the error that we are now and we

38:29.680 --> 38:35.360
will be in the next few years, the NISC error for noise intermediate scale quantum machines.

38:37.120 --> 38:43.520
And now it's so fresky who coined the term, it wasn't me. So

38:45.840 --> 38:52.800
and the question is, is this noise going to kill the quantum machine learning applications or not?

38:52.800 --> 39:00.400
Right. I'm quite optimistic and I will tell you why I'm quite optimistic because

39:02.880 --> 39:08.480
the data that we have even for classical machine learning are already very noisy.

39:08.960 --> 39:14.640
Right. The whole point of machine learning is try to extract the signals out of very noisy data.

39:15.040 --> 39:21.280
Right. So if I tell you that you have a bunch of data and then I perturb a little bit the data

39:21.280 --> 39:27.440
all of them. Right. If I have you know cats here and dogs there and I perturb all the cats and the dogs

39:28.080 --> 39:33.440
you expect your machine learning algorithm to still be able to discern the cat from a dog.

39:34.000 --> 39:39.680
Right. Because already the data that you learned on, for example, were very noisy and very

39:39.680 --> 39:46.240
fuzzy images of cats and dogs. Right. So somehow there is already noise on the data.

39:46.240 --> 39:52.240
Okay. So for the quantum case, there will be noise on the data in the sense that if you give me a quantum

39:52.240 --> 39:58.080
a classical data point, the quantum state that I will construct will be close to the correct point,

39:58.080 --> 40:04.560
but not exactly. And then quantum will add even more noise when I try to estimate, for example,

40:04.560 --> 40:11.200
the inner product or the distance between two points, that computation will also have a little bit

40:11.200 --> 40:16.640
of noise there. But again, this is something that we even do in classical machine learning.

40:17.200 --> 40:22.640
For example, when we are training neural networks, many times we inject artificial noise

40:22.640 --> 40:28.400
on the computation because we want a neural network to be robust. Right. We wanted to be robust.

40:28.400 --> 40:39.040
We want to to be robust against both adversarial adversaries, but also as a way to increase the

40:39.040 --> 40:45.440
privacy of the data. So this is also something very interesting that I'm quite interested recently,

40:45.440 --> 40:55.920
is that one, the main way maybe to deal with privacy in machine learning is again to make your

40:55.920 --> 41:00.480
data a little bit more noisy or the computation a little bit more noisy. In the sense that

41:01.040 --> 41:07.760
it's enough noise to hide specifics of the data. Right. But still you can extract the useful

41:07.760 --> 41:12.880
information that you need in order to solve your problem. So somehow this is how the quantum

41:12.880 --> 41:18.320
thing will do, not because you want it to do it, but because it will do it by itself. Right. So

41:19.200 --> 41:22.960
privacy or something like that. Yes, it's inherently private.

41:24.480 --> 41:33.280
And so is the idea of understanding the way that noise

41:33.280 --> 41:42.880
is injected from kind of classical round to quantum round. Is this

41:44.880 --> 41:50.400
kind of a pedestrian thing that is assumed and no one cares about? Or is this like a research

41:50.400 --> 41:57.440
topic that people are working towards an information theory of quantum or something like that?

41:57.440 --> 42:05.120
Yeah. No, it's very important. It's very important. For example, I will just give you a small

42:05.120 --> 42:13.680
example. Right. And we were, we started working on unsupervised learning and we started from,

42:13.680 --> 42:18.560
you know, clustering 101. So we wanted to find a quantum analog of k-means.

42:18.560 --> 42:28.080
Right. And we figured out what the algorithm should look like. But as I said, every time we will

42:28.080 --> 42:34.960
be doing a quantum procedure, we were adding noise to the computation. Right. So what we had to do

42:34.960 --> 42:40.960
is to go back to the classical algorithm and say, okay, even classically, if I start adding noise

42:40.960 --> 42:46.800
now and we know what type of noise we were adding, would the clustering still be good or not?

42:46.800 --> 42:55.200
Right. What we did is that we did extensive simulations on real data sets like M-list and

42:55.200 --> 43:00.560
Iris and many different, you know, canonical. Let's call them data set where people actually

43:00.560 --> 43:08.080
not for clustering. There were more, you know, synthetic data in other runs. But we looked at what

43:08.080 --> 43:15.280
happens when your classical k-means algorithm has noise in it. Right. We defined a new

43:15.280 --> 43:22.080
classical clustering algorithm. And what we found out is that, obviously, as long as your noise is

43:22.080 --> 43:29.600
not enormous where everything becomes noise. Right. Your clustering doesn't lose anything from

43:29.600 --> 43:36.240
from the accuracy for decent amounts of noise. Right. Even for decent amount of noise,

43:36.240 --> 43:42.400
you still have very good clustering. Okay. Of course, you can find data that will destroy your

43:42.400 --> 43:47.680
algorithm. Right. But when you test it on the data that you expect to run your clustering algorithm

43:47.680 --> 43:55.040
on, you can handle a lot of noise. And this is what also enabled us. And this is some results that

43:55.040 --> 44:02.480
we published last year at Newribs to figure out both what we expect from the quantum algorithm to

44:02.480 --> 44:10.320
give as accuracy. And also how much faster it will actually be because the runtime of the quantum

44:10.320 --> 44:15.920
algorithm depends on how precise or how noisy you can, you want your computation. Right. The more

44:15.920 --> 44:21.280
precise you need the computation the more time you have to spend. But the fact that even having

44:21.280 --> 44:28.480
quite big noise, the accuracy did not suffer. We could say that the running time of the quantum

44:28.480 --> 44:34.640
algorithm when you have a bigger computer would be much faster than the classical k-means algorithm.

44:34.640 --> 44:42.080
And so with those results that were only accessible to you via simulation in comparison or is there

44:42.080 --> 44:50.480
some theoretical framework that says under set of conditions, we know that the noise will be decent

44:50.480 --> 44:59.040
in converting this over to quantum. Yeah. So yeah, I come from a computer science background.

44:59.040 --> 45:06.080
So obviously we have to prove the theorems. Right. And we proved exactly the trade-off between

45:06.880 --> 45:14.320
how much error you can have in the computation. How much running time, how long you have to run.

45:15.600 --> 45:22.960
And then by simulations, we figured out the errors that you can handle and still have good

45:22.960 --> 45:27.920
accuracy. And for that error, we went back and we said, okay, so what is the running time

45:27.920 --> 45:35.440
and how fast will the algorithm be? And this we took from asymptotic theoretical analysis of how

45:35.440 --> 45:41.760
the k-means algorithm, the quantum k-means algorithm works. Okay. Yeah. Cool. So you're about to

45:43.680 --> 45:52.000
speak about quantum neural networks. Yes, quantum neural networks is a very intriguing to me

45:52.000 --> 45:57.520
because we are in a very bizarre situation. And I think maybe in classical machine learning,

45:57.520 --> 46:04.640
people were in a similar situation maybe 20 or 30 years ago where we kind of think we have ideas

46:04.640 --> 46:11.280
on what the architectures of quantum neural networks should be, right, to do things like classification.

46:12.480 --> 46:18.080
But we only have like 10 or 20 qubits to try things out and see how they work.

46:19.200 --> 46:24.160
So it's the same as telling you propose to me a neural network that you think can classify

46:24.160 --> 46:31.600
well, but you cannot simulate it. So then you're kind of stuck because you cannot prove many things

46:31.600 --> 46:36.640
for neural networks. The main thing that you do is that you run it and you see that it works.

46:36.640 --> 46:42.240
And if it doesn't work, then you see how to tweak it to make it work, right? So for us, it's very

46:42.240 --> 46:47.840
difficult because we don't have the ability to actually simulate this quantum neural networks

46:47.840 --> 46:53.280
because every time you try to simulate it on a classical computer, there is this exponential

46:53.280 --> 47:00.560
blow-up on the time. So if I have a neural network of 100 qubits, then I need two to the 100

47:00.560 --> 47:07.200
dimension for my classical computer to simulate it. And I don't have 100 qubits to run it on either.

47:08.000 --> 47:17.280
So it's very difficult to find ways of really giving evidence if not proofs of why we would expect

47:17.280 --> 47:23.840
this quantum neural networks to work. So what we did on our side, it's two things. The first

47:23.840 --> 47:30.560
thing we said, okay, let's not define quantum neural networks. Let's go back to classical neural

47:30.560 --> 47:37.760
networks. We know that they work very well. Can I speed up the training on them if I have a quantum

47:37.760 --> 47:43.280
computer on the side? So I'm not going to use a quantum circuit as a quantum neural network,

47:43.280 --> 47:48.160
but I'm going to use the algorithms on a quantum computer to train my classical neural network

47:48.160 --> 47:54.000
faster, right? And again, because there is a lot of linear algebra there, we can also prove that

47:54.000 --> 48:00.640
in some cases there are speed ups that you can get from from quantum training, okay? And the second

48:00.640 --> 48:09.600
case was that we are trying to use the intuition that we have from quantum algorithms

48:09.600 --> 48:15.040
to at least come up with quantum architecture is where we can prove something very simple

48:15.600 --> 48:21.360
that they're not going to be words that the classical ones. Even that is not very clear, right?

48:22.160 --> 48:28.000
So I'm trying to get kind of guarantees that say that if you run this quantum neural network,

48:28.720 --> 48:36.160
at least you have the guarantee that it will run at least as well as an equivalent classical one

48:36.160 --> 48:42.000
and hopefully it will run better, but you know, what can we do?

48:45.280 --> 48:52.240
And so is that second part? Have you developed the quantum neural networks or

48:54.240 --> 49:00.800
so we have some for now there's some internal work with QCWARE where we develop some new

49:00.800 --> 49:07.360
architectures based on the intuition that we got from from how to load quantum data because

49:07.360 --> 49:12.000
as I said, this is also something fairly new, but what are the optimal ways of

49:12.000 --> 49:16.640
floating data because for a neural network this is kind of, you know, half of the thing that you

49:16.640 --> 49:23.120
have to do is load the data and then pass it through the neural network. So the fact that we

49:23.120 --> 49:29.760
figure out the optimal ways of loading the data and doing inner products with different data,

49:29.760 --> 49:36.320
this is what kind of gave us the intuition of how we should be defining this quantum architectures

49:36.320 --> 49:43.200
and hopefully we will be able to get some more theoretical guarantees. We are working on it,

49:43.200 --> 49:50.800
we are not there yet, but we are quite hopeful that we will be able to at least propose some

49:50.800 --> 49:58.480
architecture with some provable guarantees now. Is there a way to characterize where we are with

49:58.480 --> 50:05.760
quantum neural networks and the kind of the language that we use for classical neural networks

50:05.760 --> 50:12.800
like, you know, we're at the single hidden layer feed for network stage or, you know,

50:14.560 --> 50:21.200
so I think we're orthogonal to the kind of complexity that we see in classical.

50:21.200 --> 50:30.240
So if you're asking about what type of experiments we can do, right, on a real quantum computer,

50:31.120 --> 50:40.640
I think some of the most impressive experiments is to say that let me look at the M-nist

50:41.600 --> 50:48.560
data set of 100 in digits. I will only pick two digits set of 10 because I cannot handle 10,

50:48.560 --> 50:55.760
I can handle two of them and instead of 700 pixels, we used four pixels and I do

50:57.280 --> 51:02.320
figure out if you have three versus one by looking at four pixels, you know,

51:02.320 --> 51:07.520
blurry images of three's and ones. So when it comes to real hardware, you know,

51:08.320 --> 51:13.520
real data, we're very far from figuring out whether quantum neural networks will work or not,

51:13.520 --> 51:18.640
right? And the implementation of the number of qubits that we have to work with kind of the

51:18.640 --> 51:25.440
the web bar compute bar. At the same time, we have many different proposals for architecture,

51:25.440 --> 51:32.800
so how this quantum neural networks should look like. It's very difficult to prove anything

51:32.800 --> 51:40.400
and you cannot test them. So that's why what we're trying to do is come up with an architecture where

51:40.400 --> 51:46.880
you can at least have some guarantees, some provable guarantees. And it's not easy, it's not easy,

51:46.880 --> 51:52.640
but we're making progress, everyone, many people are working on this, we're making progress,

51:52.640 --> 51:58.160
the progress I think will be much faster when we get our hands on better hardware because this

51:58.160 --> 52:02.960
is kind of a deep letting is empirical to some extent, right? You need to try it out.

52:02.960 --> 52:12.560
And so our people doing, you know, anything approximating kind of, you know,

52:12.560 --> 52:20.320
the, you know, very sophisticated networks, you know, quantum CNNs or quantum deep reinforcement

52:20.320 --> 52:29.760
learning or, you know, on paper at least. So we had, we had the paper that last year's ICLR,

52:29.760 --> 52:36.080
where we discussed quantum convolutional neural networks. Okay.

52:37.360 --> 52:42.960
On our side, that paper was very theoretical in the sense that we used classical CNNs

52:42.960 --> 52:51.680
and quantum ways of training the CNNs. Okay. They have been some proposals on quantum CNNs as well.

52:53.520 --> 52:58.320
You have to find quantum ways to do the different layers, like the pooling, the, you know,

52:58.320 --> 53:07.840
applying the nonlinearity and this type of things. Again, ideas are out there. We need to

53:08.560 --> 53:15.600
find better ideas. We'll be getting better ideas when we find ways of, of trying out things and

53:15.600 --> 53:21.600
figuring out why they work or why they not work. For example, a quantum circuit is a reversible

53:21.600 --> 53:27.600
computation. It's a linear of this unit that is a linear operations. So the first thing is how do

53:27.600 --> 53:35.600
you apply a nonlinearity if you, if you have a linear operator, right? So even that is not

53:35.600 --> 53:40.720
obvious what is the correct way of injecting nonlinearity in a quantum neural network.

53:41.280 --> 53:48.080
The many different things. Maybe I will measure and then I get a sample and this induces

53:48.080 --> 53:54.000
a nonlinear element. Maybe I need to get rid of some of the cubits and look at the subset of

53:54.000 --> 54:00.080
cubits. This also includes some nonlinear element. So even that is not trivial in the many different

54:00.080 --> 54:10.560
ways. And anything on the reinforcement learning side? Reinforcement learning. I think it's probably

54:10.560 --> 54:20.000
the least advanced area. It's also not the easiest one. That's fine. So we started with supervised

54:20.000 --> 54:27.360
learning a few years ago. I'm supervised learning last year. And there have been very few

54:27.360 --> 54:37.360
papers on quantum reinforcement learning. And what we did with the student of mine is to look at

54:38.960 --> 54:44.880
policy iteration before we go to deep reinforcement learning even through, you know,

54:44.880 --> 54:52.000
iterative methods as you said and solving linear systems one after the other to update your policy

54:52.000 --> 54:59.200
and improve your policy and things like that. There are things we can do. And I think reinforcement

54:59.200 --> 55:05.440
learning is quite interesting for quantum for different reasons. One of the reasons is that you

55:05.440 --> 55:10.720
don't have this problem that we discussed earlier of loading data because it's not like you have

55:10.720 --> 55:14.960
images which you have no idea about and you need to really look at all the data and load it.

55:15.680 --> 55:21.200
Your data and the reinforcement learning is basically what you have learned of your game by

55:21.200 --> 55:26.800
taking some moves and figuring out where you're going. So you can kind of produce the data

55:26.800 --> 55:36.320
as you are exploring your state space, right? And this is easier data in the sense that we can

55:36.320 --> 55:43.680
construct data and it's not just data that we have to store and load from memory, right?

55:43.680 --> 55:50.720
So this is one of the reasons why I do think that reinforcement learning is a very interesting

55:50.720 --> 55:59.600
thing for quantum algorithms. And then adding deep RL into the mixture. Yes, it's a great thing to do

55:59.600 --> 56:06.880
and not much has been done. But as I told you, we have a very young field. We have pretty much five

56:06.880 --> 56:14.000
good years of doing things and we have a very small community. So hopefully after the ICML talk

56:14.000 --> 56:19.360
and the interview with you, more people from the classical ML community will start getting

56:19.360 --> 56:26.480
interesting about what is this thing out there. And I'm very happy to talk to people and figure

56:26.480 --> 56:36.240
out how to work together. What are the limitations of, we do this kind of distinction

56:36.240 --> 56:42.160
early on between classical algorithms, I'm sorry, between quantum algorithms and quantum computing.

56:43.040 --> 56:51.680
And a lot of the things we're looking for, a lot of the interesting work is in the algorithms

56:51.680 --> 57:01.920
and independent of being able to run them. To what degree is simulation viable for quantum

57:01.920 --> 57:14.000
algorithms? Can we simulate quantum algorithms in a classical machine? So if you want to simulate a

57:14.000 --> 57:22.640
general quantum computation, a quantum algorithm on hundred qubits, then the equivalent classical

57:22.640 --> 57:29.840
problem that you need to solve has dimension two to the hundred. Precisely because there's a state

57:29.840 --> 57:36.880
of a quantum system of a hundred qubits is a two to the hundred dimensional vector.

57:36.880 --> 57:45.840
So this is why the maximum quantum system that we can simulate classically is something between

57:45.840 --> 57:52.960
30 and 40. I think for now there's something like 32, 34, 36 and this is the limit because you get

57:52.960 --> 57:58.560
to things like two to the 32 when you're touching the limits of what you can do.

57:58.560 --> 58:04.480
And we're depending on what kind of qubits or how you count or which vendor we're kind of at

58:04.480 --> 58:15.680
that point with quantum machines now. So we're beyond the point where it makes sense to simulate

58:15.680 --> 58:21.120
because we have access to the actual machines that are beyond the capacity of what we can simulate.

58:21.120 --> 58:29.120
You're right that for example both Google and IBM have a 53 qubit machine.

58:29.120 --> 58:38.480
So we will never be able to simulate a general computation on 53 qubits because you would have

58:38.480 --> 58:47.200
this two to the 53 object that you have to handle. At the same time this would be the case if you

58:47.200 --> 58:54.880
had perfect qubits somehow that you know what they do. Because if you have 50 qubits that are so

58:54.880 --> 59:00.080
noisy then it's very easy to simulate what will happen at the end it would be just garbage.

59:02.320 --> 59:08.400
And it was exactly the point where what Google managed to do with the supremacy experiment is to

59:08.400 --> 59:16.080
have these 53 qubits good enough that at the end you don't get total garbage but with very small

59:16.080 --> 59:20.880
probability you get something that has to do with what you were trying to compute.

59:20.880 --> 59:29.680
And this is exactly the point where where the reason we call this you know and I think it's a

59:29.680 --> 59:36.160
very important experiment right the experiment said that you have nowadays a machine that can do

59:36.160 --> 59:44.720
something completely useless but something that you cannot simulate classically right now the

59:44.720 --> 59:50.720
holy grail is to go from something completely useless to something very useful that also you cannot

59:50.720 --> 59:56.720
do with a classical computer. And this is what we are trying to reach this point with quantum

59:56.720 --> 01:00:02.800
machine learning with quantum chemistry with quantum optimization we are not there yet but we are

01:00:02.800 --> 01:00:10.560
doing good progress and I think you know the only thing we can do and I think this is my responsibility

01:00:10.560 --> 01:00:17.200
there's a scientist as well is to try to accelerate this process so that we can get to real world

01:00:17.200 --> 01:00:23.120
applications as fast as possible because at the end we do want to have an impact and to make

01:00:23.120 --> 01:00:29.600
this well the better place so we're trying our best. Awesome awesome well you're done is thanks so

01:00:29.600 --> 01:00:36.880
much for taking the time to share with us a bit about your recent keynote and your research

01:00:36.880 --> 01:00:49.280
fascinating topic and conversation and I appreciate it. Thank you very much for the invitation.

