WEBVTT

00:00.000 --> 00:08.720
If you have to spend time on designing and optimizing manually for every problem you have

00:08.720 --> 00:12.240
and deploy it onto the device, that's not going to scale, right?

00:12.240 --> 00:19.040
So our researchers focus not on manually figuring out how to optimize the neural network itself.

00:19.040 --> 00:23.200
They focus on innovative ways of doing AI.

00:23.200 --> 00:34.160
All right, everyone. Welcome to another episode of the Twomo AI podcast. I am, of course,

00:34.160 --> 00:39.920
your host, Sam Charrington. And today I'm joined by Marali Akula, Senior Director of Software

00:39.920 --> 00:45.920
Engineering at Qualcomm. Before we get going, be sure to take a moment to hit that subscribe button

00:45.920 --> 00:52.400
wherever you're listening to today's show. Marali, welcome to the podcast. You began your career

00:52.400 --> 00:57.440
with more of a traditional software engineering focused. Tell us a little bit about how you found

00:57.440 --> 01:02.080
your way into machine learning. Thanks Sam, excited to be on the show. Thanks for having me.

01:02.880 --> 01:11.120
Like you said, I'm a software guy and I started working on a wireless software stack. I worked in

01:11.120 --> 01:19.600
the beginning couple of commercial products and commercial teams. Then after that, I joined

01:19.600 --> 01:25.600
the Carpet R&D group, which is a research group. And I was part of the 4G research team.

01:26.640 --> 01:32.640
So at Qualcomm, when you were talking of 4G research, we build the entire system.

01:33.680 --> 01:38.720
We bring it up OTA, which is over there and test it out properly.

01:40.480 --> 01:47.600
And make sure it works in the real world constraints. So we have this mindset,

01:47.600 --> 01:53.600
unless you build it, it's not real. So we build things that can scale into the scalable and

01:53.600 --> 02:00.160
deployable, for example. You start with the radio and the hardware, the firmware, the software,

02:00.160 --> 02:08.800
all the layers, talked to bottom. And we do this before even you go to standardize or commercialize it.

02:08.800 --> 02:14.080
So this is an entirely a pre-commercial setting. We build the entire thing. So that's what I was

02:14.080 --> 02:24.640
doing for a while, part of the 4G research team. Then late 2000s, the first of the Android smartphones

02:24.640 --> 02:31.200
was coming out. My boss was Jeff Gellar at that time. He came on this podcast a couple of times.

02:31.920 --> 02:37.120
So he suggested that I look into a smartphone user experiences and he connected me to a team

02:38.160 --> 02:43.600
which was looking into latency in wireless protocol and how we can improve the smartphone user

02:43.600 --> 02:49.280
experience. I started looking into the smartphone. And beyond wireless, there were so many components

02:49.280 --> 02:55.600
which you built into it. And also during that time, new applications were showing up and

02:56.400 --> 03:03.760
the so much going on. And I was really amazed about so much technology we packed into this platform.

03:04.480 --> 03:10.000
And I really wanted to work on the entire smartphone user experience. So that's what happened.

03:10.000 --> 03:17.040
And it was a lot of fun. After that, I continued on machine learning projects. And we did that.

03:17.040 --> 03:26.080
It was before the deep learning took off. And over time, it's become more and more deep learning.

03:26.960 --> 03:30.720
So that's where I am. Doing software engineering and AI research today.

03:30.720 --> 03:36.080
Awesome. Awesome. And so tell us a little bit about your role today and the team that you

03:36.080 --> 03:43.280
that you work with. I have a broader organizational responsibility as the head of the Carpet R&D

03:43.280 --> 03:49.840
software engineering team that is looking into forward-looking efforts across the AI spectrum.

03:50.960 --> 03:59.120
My day-to-day where I spend most of my time, I lead a software team in a cross-discipline effort

03:59.120 --> 04:08.400
for AI research. So researchers applied ML, folk, software, hardware, testers. We all sort of work

04:08.400 --> 04:15.360
together in a cross-discipline effort. And we apply the same mentality which we I talked to

04:16.000 --> 04:22.640
talked about earlier in other research efforts. If you unless you build it, it's not real. So we

04:22.640 --> 04:29.760
build out the technology that comes out of our AI research and prove that it can be deployed

04:29.760 --> 04:38.400
onto devices. So that is broadly the role we have. And on one side to make that happen,

04:38.400 --> 04:43.840
we are working with researchers jointly. And on the other side, we are working with the commercial

04:43.840 --> 04:50.080
teams to be always ready. So by doing this, looking at the entire system, building it out,

04:50.080 --> 04:59.440
we are in a unique position to figure out how or when our innovations can go into our products.

04:59.440 --> 05:04.320
So we'll get the entire full stack for that. Can you elaborate a bit on the full stack?

05:04.320 --> 05:09.840
I know from our past conversations, that's a concept and a terminology that's important to you

05:09.840 --> 05:16.800
and the team. But it's also something that we throw around in the industry quite a bit.

05:16.800 --> 05:24.080
So when we talk about full stack, let me start about what happens when Qualcomm is looking into AI

05:24.080 --> 05:32.560
technology. We have a lot of research going on to advance fundamental AI for applications that

05:32.560 --> 05:38.880
are going to be running on Snapdragon platform, either now or in future in different products.

05:38.880 --> 05:44.560
When you are innovating new ways of doing AI, researchers run the experiments on

05:44.560 --> 05:50.960
servers, which run in racks in a data center. And it's all good because you can run lots of

05:50.960 --> 05:57.680
experiments and get good results. Now the innovations which come out of that from this big compute,

05:59.280 --> 06:05.840
how do you make them run on the resource constraint devices? The software which runs on the

06:05.840 --> 06:11.280
device, the hardware architecture, the software, the tools which you use to deploy them onto the

06:11.280 --> 06:18.080
device, all that is different from what you did the software which you used when you actually

06:19.680 --> 06:27.120
ran the experiments. So that's the one which I call the full stack that is needed to deploy

06:28.160 --> 06:35.600
your AI innovations onto the devices. So we will have to look into the entire full stack.

06:35.600 --> 06:42.560
So research into the full stack is extremely important for us. We need to continuously improve

06:42.560 --> 06:50.880
every layer of this full stack and build it out and also show that it is real and it can be

06:50.880 --> 06:55.760
deployable and this keeps us ready all the time so that any innovation we have, we can actually

06:55.760 --> 07:03.600
deploy into the real world devices. So that's what we do. We continuously look at this hardware

07:03.600 --> 07:09.120
software full stack and figure out how to deploy it onto this resource constraint that

07:09.120 --> 07:14.640
mobile devices. When I hear you talk about that lifestyle, I think a little bit about I guess

07:14.640 --> 07:19.920
Docker and where the container technologies came from, you know, developers running things on

07:19.920 --> 07:24.960
their desktops and you know, trying to get it in a prod and not working and say, hey, it worked on

07:24.960 --> 07:33.200
my machine. And you know, in a lot of places, those same kind of container technologies have been

07:34.800 --> 07:41.920
adopted for machine learning, you know, workloads. But there's this whole other set of complexities

07:41.920 --> 07:50.720
when you're talking about mobile and devices, you know, not just kind of the runtime, but the

07:50.720 --> 07:55.760
constraints that you alluded to earlier in terms of power and other things. Can you elaborate a

07:55.760 --> 08:05.280
bit on when you think about devices in particular for machine learning, you know, what are the

08:05.280 --> 08:09.840
constraints and maybe, you know, I think we're all kind of familiar with, you know, them as users,

08:09.840 --> 08:15.520
like, hey, not a lot of memory, not a lot of compute, bandwidth issues, but, you know, there's some

08:15.520 --> 08:19.440
non-obvious things or like, how do you think about that as an engineer who works on this stuff

08:19.440 --> 08:26.560
constantly? Yeah, it's interesting. You mentioned how people build applications and deploy them and

08:26.560 --> 08:33.600
Docker containers. There is no Docker container on the device. But there are applications,

08:33.600 --> 08:40.480
applications which are running in real time, right, very efficiently. And with the neural network,

08:41.040 --> 08:47.600
efficiently mapped to the hardware, there's many constraints on the device. It's not, first of all,

08:47.600 --> 08:54.160
all these mobile devices, they're on batteries. They're mobile, right. If you take a smartphone or a

08:54.160 --> 08:59.760
VR headset or an autonomous driving car, power efficiency is extremely important. That is,

08:59.760 --> 09:04.960
I would say almost number one, real time latency is important. When you're running experiments on

09:04.960 --> 09:11.840
the server or even applications, for example, you can run it in batch mode on data. But when you're

09:11.840 --> 09:19.120
running on the device, as a signal comes from cameras, you need to be making inferences in real time.

09:19.120 --> 09:22.720
Again, you need to know that this is not the only thing which is running on the device. There's

09:22.720 --> 09:26.160
a lot of other things which are running through. And they all have to run in real time.

09:26.960 --> 09:37.040
Latency is extremely important. Area is a big concern. The chip size, for example, the size of the

09:37.040 --> 09:43.200
block accelerator which you're using, the memory sizes, they're all important because in a way,

09:43.200 --> 09:51.040
they impact power efficiency and latency. You can add more area and get the better latency,

09:51.040 --> 09:57.040
but you'll be trading off against power, right. So area is a big concern. The

09:58.640 --> 10:05.120
number of sensors, the resolution, the number of applications which you're running, the tasks,

10:05.120 --> 10:10.480
everything is just increasing. The size of the neural networks, and we need to pack more and

10:10.480 --> 10:15.200
more compute and you can't just keep increasing the chip size, right. So area is very important.

10:16.000 --> 10:24.080
And how you run it on the machine learning accelerator, using the on-chip memory versus going off-chip.

10:24.080 --> 10:28.000
When you actually run the neural network, you have limited on-chip memory. You can increase,

10:28.000 --> 10:31.760
but it's going to cost and area and power, you know, all those things come into picture.

10:31.760 --> 10:37.280
So on-chip memory. So there's many constraints like that when you run and it becomes extremely

10:37.280 --> 10:45.360
important that how you optimize the neural network to deploy and properly map it to your hardware

10:45.360 --> 10:53.840
resources and the entire, you know, software hardware to look at, you know, we look at all that

10:53.840 --> 10:58.080
and develop techniques on an ongoing basis how to optimize your neural network for deployment.

10:58.080 --> 11:05.040
So this is the neural networks are going to continue to get complex and complex, right. And we need

11:05.040 --> 11:12.400
to make them smaller and efficient and deploy them. You know, no from prior conversations with,

11:13.600 --> 11:19.680
you know, folks like Jeff who you mentioned, they're Qualcomm as well as folks on the research side

11:19.680 --> 11:29.280
that techniques like quantization and compression and related techniques are, you know, often,

11:29.280 --> 11:37.280
often a subject of research there. Are those the main types of things that you are applying when

11:37.280 --> 11:42.400
you're talking about getting these complex models working on mobile devices or are there other

11:42.400 --> 11:47.520
things? How do you think about that landscape and process? Yeah, I think what you mentioned,

11:47.520 --> 11:54.320
they are very important, those and lot more. We are looking at, when I say,

11:54.320 --> 12:02.240
entire full stack, you know, it starts from the top. We work very closely with the researchers,

12:02.240 --> 12:06.880
so our researchers when they design the network itself, you know, they are looking at what are the

12:06.880 --> 12:15.200
parallelism in this design, which we can exploit and how do we design this so that it can run

12:15.200 --> 12:20.400
efficiently on a resource constraint device, right? It starts from there. And after that,

12:22.560 --> 12:27.200
you come up with a good design for your architecture and how do you train? How do you train so that

12:27.200 --> 12:35.520
you can deploy it onto the device? Normally, if you train freely, it becomes pretty complex.

12:37.040 --> 12:42.320
One example of how to train for deployment onto the device is neural architecture search,

12:42.320 --> 12:49.120
while you are designing your neural network, you could use neural architecture search to get

12:49.120 --> 12:56.160
the best optimal architecture for your design, right? So that is next. And once you have the

12:57.120 --> 13:02.320
neural network, then how do you deploy it? You get the best accuracy and how do you deploy? When

13:02.320 --> 13:08.960
you deploy, these are the couple of things which you mentioned already. We run on integer hardware.

13:08.960 --> 13:16.960
And when you train, you are training in floating point. So quantization techniques are important.

13:16.960 --> 13:23.920
And compilation, how do you optimally map the tensors to the memory? And how do you map the compute

13:23.920 --> 13:29.840
to the instructions of the hardware accelerators? So those are important. So that is the next step,

13:29.840 --> 13:36.000
right? How do you deploy? And then how do you run on the device? When you optimally compile this

13:36.000 --> 13:45.120
onto the device, the application stages the sensor data coming from the camera, how does it get

13:45.120 --> 13:50.640
moved into the accelerator? What are the preprocessing stages? Then you run the neural network,

13:50.640 --> 13:57.040
what are the postpassing stages? So that entire application architecture is next and then the hardware

13:57.040 --> 14:05.520
architecture, right? So all of it. So that's what we look at when we say, you know, looking into

14:05.520 --> 14:11.600
full stack. So I'll also mention one other thing which is extremely important, right?

14:13.120 --> 14:20.880
I might have alluded to this in, you know, previous efforts which I mentioned too. So it takes a

14:20.880 --> 14:29.360
continuous research effort not just to design the neural network, but also to design tools and

14:29.360 --> 14:37.040
techniques which can make deploying them onto the device important. So we do a continuous research

14:37.040 --> 14:47.600
into full stack with a mindset that our research in a way is running on real devices on an ongoing

14:47.600 --> 14:53.680
basis, okay? We sort of connect that being in the middle. We look at everything hardware software,

14:53.680 --> 15:00.000
algorithmic layers and all that and also the we collaborate, we collaborate with everybody because

15:00.000 --> 15:06.240
research is happening in universities, research is happening in our research teams and even in

15:06.240 --> 15:10.880
product teams, a lot of innovations are happening. So we sort of connect the entire dot.

15:10.880 --> 15:19.520
Can you maybe take us through an example of, you know, when you think of a complex model that's

15:19.520 --> 15:26.400
coming out of research, you know, what that, what that looks like and how you kind of walk this

15:26.400 --> 15:34.720
process that you identified for getting, you know, into the device? Yeah, sure. Monocular depth

15:34.720 --> 15:45.440
estimation is an application which we recently implemented and demonstrated. I can walk you through

15:45.440 --> 15:54.640
that. Monocular depth estimation is about creating a depth map of the scene in front of you. So

15:54.640 --> 16:01.920
this is very important for many applications. Thinking like autonomous vehicles or as one example.

16:01.920 --> 16:08.240
Autonomous vehicles is one example. Even mobile phones when you're, you know, pointing at something

16:08.240 --> 16:13.360
and you need to detect features of the face. For focusing for in the camera, for example.

16:13.360 --> 16:19.440
Yeah, a lot of applications like that and VR headsets drones, you know, drones are going at high

16:19.440 --> 16:25.120
speed. So you need to detect obstacles very fast. So in your autonomous driving example.

16:26.560 --> 16:33.280
How far is the car in front of you in the scene? And how far is a pedestrian or obstacle?

16:33.920 --> 16:40.480
So all those are very important and depth estimation is creating that depth map.

16:40.480 --> 16:46.800
We need to infer for every pixel in the scene a depth value. How far it is?

16:47.840 --> 16:56.000
And Monocular depth estimation is about applying a neural network and inferring this depth map

16:56.000 --> 17:02.320
from a single camera image. Monocular depth estimation is about singular single camera image.

17:02.320 --> 17:07.520
As opposed to like you've got some controlled environments with cameras all over the place

17:07.520 --> 17:15.280
that you can interpolate to figure out depth. Exactly. Yeah. So that is the application and we implemented

17:15.280 --> 17:21.280
it on the device and demonstrated, you know, we took it around the time for Newrips.

17:23.040 --> 17:31.040
So when we implemented that, then I can explain how we went through the process. So what happens

17:31.040 --> 17:40.560
is when accuracy is extremely important in this use cases, right? Researchers can get the

17:40.560 --> 17:46.000
best accuracy, but through those experiments you can end up creating very complex models.

17:46.000 --> 17:51.760
Then what ends up happening is it becomes so complex, it becomes almost invisible to

17:51.760 --> 17:57.440
deploy it on the device and there are a bunch of state of the art neural networks which can achieve

17:57.440 --> 18:06.000
that. So what our researchers did is they working closely with us and also working on an ongoing

18:06.000 --> 18:16.800
basis with this device mentality. Our researchers designed a neural network where at training time

18:16.800 --> 18:23.680
they increase the complexity, but that doesn't translate to inference time. So because of that,

18:23.680 --> 18:29.840
you can get best accuracy at training time, but you can still run it on the device. So that's

18:29.840 --> 18:34.480
where it starts, right? As I explained, you know, I think part of what you're saying is it starts

18:34.480 --> 18:45.440
of research like there are ways to do this kind of depth estimation, but the complexity of the

18:45.440 --> 18:51.200
resulting models is such that they're not feasible starting places for something that you eventually

18:51.200 --> 18:57.200
want to run on device. So you're kind of starting at the algorithm level with the thought of,

18:57.200 --> 19:03.760
hey, this eventually needs to run on the device. That is correct. Yeah. I mean, you could just say,

19:03.760 --> 19:09.120
oh, I can just increase add more layers, right? I'll increase the complexity and get, but we know

19:09.120 --> 19:15.600
what happens if you do that. So our researchers have that mindset. So it's a very important innovation

19:15.600 --> 19:21.600
to be able to increase the training time complexity, but not let that go to the device.

19:22.320 --> 19:29.920
So that's what our researchers did. And after that, we have a neural network and we take the design

19:29.920 --> 19:41.760
and the full stack team next took that even after that, right? Our inference per second speed was

19:41.760 --> 19:51.680
23 FPS. So that's not going to make it real time. So the next step was to make the model smaller

19:51.680 --> 19:59.680
and our full stack team used neural network architecture search NAS techniques to reduce the size

19:59.680 --> 20:05.440
we have NAS tools which we use. So that is the next step. Now you make the model smaller. The next

20:05.440 --> 20:14.960
step is to deploy it on the device. To do this, you need to quantize it. Some models quantize

20:14.960 --> 20:20.080
well, some models don't quantize well. In this particular case, we have to apply the our

20:20.080 --> 20:25.280
aim at techniques post training quantization techniques to make sure we don't lose the accuracy.

20:25.280 --> 20:33.920
So that is the next step. And after that, we use our compiler tools and compile it properly so

20:33.920 --> 20:40.880
that it can be mapped properly to the hardware which we have. And the next step is application design,

20:40.880 --> 20:48.400
right? We design the application properly and run it on the device, measure on device performance,

20:49.360 --> 20:54.800
and we have an integration team which touches it properly and makes sure everything is stable.

20:55.680 --> 21:00.080
The application, the architecture, the tools, any improvements which we made.

21:00.080 --> 21:07.840
And that is the entire process of how you take one of the use cases like monocular depth estimation

21:07.840 --> 21:15.920
all the way to the device. Let's dig into all that. You mentioned the first step is the algorithm.

21:15.920 --> 21:19.920
Can you share a little bit more detail about the algorithm that was developed?

21:19.920 --> 21:27.120
It's called, again, that the technology is monocular depth estimation. Our researchers came up with

21:27.120 --> 21:35.680
a method called x-digital. x-digital? Yeah, I'll summarize it. That's a lot of

21:36.400 --> 21:40.000
and we'll of course link to the full paper in the show notes.

21:41.680 --> 21:47.520
So what they did is they I earlier gave a hint that you know our researchers

21:48.560 --> 21:55.680
increased the complexity of the network while training, right? So the way they did it is they

21:55.680 --> 22:04.080
added semantic information. That increases the complexity of the network, but that semantic

22:04.080 --> 22:10.560
information is not needed on the device. So you don't need to transfer that network down and

22:11.440 --> 22:18.000
the training also happens in a self supervised manner. So this is a summary of what they ended up

22:18.000 --> 22:25.440
doing. We use extra information, increase the complexity of the network, but that extra information

22:25.440 --> 22:29.680
which is semantic information is not required to be transferred onto the device.

22:30.800 --> 22:36.320
So the method is called x-digital method. So that is the first step, you know, which our

22:36.320 --> 22:41.360
researchers were able to come up with this innovation. You mentioned the next step was

22:41.360 --> 22:48.160
applying neural architecture search. Even before you do that, you invest all this time and research

22:48.160 --> 22:54.240
like coming up with a more efficient algorithm. Like why do you need to go through the next step

22:54.240 --> 23:00.480
is it you know do you have predefined performance targets that you're trying to hit and you kind

23:00.480 --> 23:08.160
of go as far as you can with algorithm but you're not quite there or is there a you know I'm thinking

23:08.160 --> 23:13.920
of like some kind of efficient frontier of like you know where you get all your efficiencies from

23:13.920 --> 23:20.400
to overuse efficiency maybe. I think that's an excellent question. So think of it this way. This

23:20.400 --> 23:26.320
thing but we already have that in our aim at toolkit. So we were able to we didn't have to get into

23:27.440 --> 23:31.680
you know they were able to design the end-to-end system in a way where it can actually run on the

23:31.680 --> 23:36.320
device. They're looking at the complexity of the network while they're designing. When we are

23:36.320 --> 23:40.240
working jointly with them you know being in the same team software engineers working with the

23:40.240 --> 23:49.040
researchers, we looked at the end-to-end architecture and we found that this entropy encoding and

23:49.040 --> 23:54.160
entropy decoding could be a bottleneck. The reason it is is we can exploit the parallelism in

23:54.160 --> 23:58.880
the neural network and map it to our accelerator but when you do entropy decoding,

23:59.680 --> 24:04.400
when you need to decode a symbol you need to you need the previous symbol that means you need to

24:04.400 --> 24:08.800
decode the previous symbol it's sort of serialized the entire thing. So you have this wonderful neural

24:08.800 --> 24:14.800
network part which can be paralyzed but the resulting latent you end up serializing it. So we

24:14.800 --> 24:23.040
discussed with the researchers and then they looked at the latent and the mathematical properties

24:23.040 --> 24:29.440
of it and figured out how can we paralyze it and they figured out that latent are actually either

24:29.440 --> 24:34.720
independent or conditional independent and they were able to like figure out how to design a parallel

24:34.720 --> 24:41.520
entropy encoder and parallel entropy decoder. So once they did that we took that and implemented

24:41.520 --> 24:48.000
it on the CPU. SnapDragon platform is a heterogeneous platform we have the accelerator, we have the

24:48.000 --> 24:54.560
CPU, we have the GPU, we have DSPs. So we were able to implement the parallel entropy decoder

24:54.560 --> 25:00.320
on the CPU and match the parallelism which is happening on the accelerator and then we were

25:00.320 --> 25:05.200
able to get it to real time. A lot of our conversation thus far has been focused on

25:05.200 --> 25:14.000
trying to achieve like inference latency targets and other inference oriented

25:16.320 --> 25:24.240
objectives on devices. Is there anything that you're aware of looking at more of the

25:24.880 --> 25:32.160
like training on devices? Is that is that a thing yet? It is a thing. So are the past

25:32.160 --> 25:42.000
number of years the full stack on the device was more about running inference workloads

25:42.800 --> 25:49.680
while training was mostly centralized and on the servers. So the full stack accounts for

25:49.680 --> 25:57.280
optimizing the inference the forward pass on the device but lately past couple of years

25:57.280 --> 26:05.040
are even more we're seeing more and more training on the device using the local data.

26:05.760 --> 26:10.000
Primarily for personalization reasons you have some data which is very unique to you you can

26:10.000 --> 26:15.120
personalize the model for yourself. So those kind of use cases have already been deployed and

26:15.120 --> 26:23.680
they're showing up. So training is becoming an important part of the full stack and we are looking

26:23.680 --> 26:32.640
at it. So we at Neurip's 21 we actually implemented a federated learning system

26:33.600 --> 26:38.960
for distributed training completely on the devices. Nothing is happening on the server except for

26:39.680 --> 26:46.560
getting the incremental updates from various devices and combining them. So we implemented

26:46.560 --> 26:53.200
end-to-end federated learning training system. So that is also part of our full stack effort

26:55.360 --> 26:59.920
and it requires a lot of software engineering. It's a distributed system. So you need to make

26:59.920 --> 27:05.600
sure the entire architecture works well and it works for both PyTorch and TensorFlow users.

27:06.160 --> 27:11.280
We created libraries on the device. We created connections to a coordinated running in the cloud

27:11.280 --> 27:20.480
so that everything scales up to thousands of devices. And is the idea that like Amit and Dana and

27:20.480 --> 27:26.000
some of these other tools that it kind of just goes into the bag of tricks and the next time you

27:26.000 --> 27:33.360
need to build a model that could benefit from distributed training you can apply this or do you

27:33.360 --> 27:39.840
think for in the near future it will require that manual kind of hand crafting that we've talked

27:39.840 --> 27:45.600
a little bit about. Yeah so the goal is to avoid anything manual which is not going to scale.

27:45.600 --> 27:52.640
So you need to build these tools. They are going to become automatic more and more. And the federated

27:52.640 --> 27:59.760
learning framework we package it in a way that our teams who want to do federated learning can

27:59.760 --> 28:05.120
just take it and run the experiment but also be able to deploy without having to do a lot of work

28:05.120 --> 28:11.040
downstairs. Awesome, awesome. So thinking back to the examples that we've talked about

28:12.960 --> 28:20.000
the monocular depth estimation, the neural video codec, we've talked about kind of this process

28:20.000 --> 28:27.760
up from getting from research to real world kind of touch it, see it on a device. Does that mean

28:27.760 --> 28:35.760
that they're products and that they're productized or is that a further step that you take these

28:35.760 --> 28:41.840
products or these projects? There is a further step. So we are still part of the research team.

28:43.120 --> 28:51.200
Whatever we create is deployable. But our focus is on creating a solid research pipeline.

28:51.200 --> 28:59.200
A research pipeline of fundamental AI improvements and also how they can be optimally

29:00.080 --> 29:07.280
deployed onto the device. So across the full stack. So we innovate, implement and create

29:07.280 --> 29:13.440
this research pipeline. But while doing that, the product teams also have innovative advances

29:13.440 --> 29:20.240
in the architecture, both hardware and software. And we work very closely with the product teams.

29:20.240 --> 29:28.960
And by doing a full stack approach in the research team, we are always ready. When the need arises,

29:30.000 --> 29:37.840
the product teams can immediately decide to take some of the innovations. Some go very early,

29:37.840 --> 29:46.160
some go a little late. But immediately as a very soon, take it and put it into their commercial

29:46.160 --> 29:51.920
roadmap, product roadmap, then after that, it's about scaling. Scaling across Qualcomm business.

29:51.920 --> 29:59.280
That's a pretty big effort. So we focus on the research side, building a solid research pipeline

29:59.280 --> 30:06.480
and our product teams are focusing on the product roadmap. We work very closely and collaboratively.

30:06.480 --> 30:16.480
And the form that these products end up taking, like does that, say, monocular depth estimation,

30:16.480 --> 30:24.320
would that be built into a silicon or would that be software that's part of a toolkit or

30:24.320 --> 30:28.800
kind of all of the above, depending on what makes the most sense? Monocular depth estimation is one

30:28.800 --> 30:34.000
example. We have a lot of AI technology in our platform, in Snapdragon platform. We have the entire

30:34.000 --> 30:38.640
camera, everything which is needed to do camera, everything which is needed to do video processing.

30:39.360 --> 30:46.080
So the answer is all of the above. All these innovations, like applications,

30:46.880 --> 30:51.840
our product teams will take integrated into the products. Same thing with the

30:54.080 --> 30:59.520
full stack innovations, our software team, product software team takes it and puts it into the

30:59.520 --> 31:07.680
roadmap. But we also make sure that we figure out the best way we can put it out into the community,

31:07.680 --> 31:15.440
into the world. Like for example, a model efficiency toolkit. We felt, you know, it's important that

31:15.440 --> 31:23.200
that's an open source project. So anybody can use it to quantize to run an integer hardware.

31:23.200 --> 31:28.960
We also have a model zoo project. Some of the applications will just put it out in the model zoo.

31:30.320 --> 31:34.800
And with some of them, we just directly give it to the customer. So they can integrate. So it's

31:34.800 --> 31:43.440
sort of all of the above. Okay. Awesome. Awesome. Well, Marali, thanks so much for taking the time to

31:43.440 --> 31:51.360
share a bit about how you see kind of this full stack ecosystem and getting some of the research

31:51.360 --> 31:59.520
ideas into product. Thank you. Thanks, I'm just so really grateful. Thanks.

