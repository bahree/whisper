1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,200
I'm your host Sam Charrington.

4
00:00:31,200 --> 00:00:36,280
I'd like to start out by sending a huge thanks to Qualcomm for their support of the podcast

5
00:00:36,280 --> 00:00:39,520
and for sponsoring today's episode.

6
00:00:39,520 --> 00:00:44,160
As you're here in my conversation with Jeff, Qualcomm is taking a systems approach to helping

7
00:00:44,160 --> 00:00:50,720
the industry address the challenges associated with AI on mobile devices and at the edge.

8
00:00:50,720 --> 00:00:55,000
In support of their Snapdragon chipset family, which powers some of the latest and greatest

9
00:00:55,000 --> 00:01:00,360
Android devices, Qualcomm provides their own suite of software tools and is also actively

10
00:01:00,360 --> 00:01:05,720
supporting a variety of partner and industry projects, including the Android Neural Network

11
00:01:05,720 --> 00:01:12,720
APIs, TensorFlow Lite, the TinyML initiative, and the Open Neural Network Exchange or Onyx

12
00:01:12,720 --> 00:01:14,480
ecosystem.

13
00:01:14,480 --> 00:01:21,120
To learn more about Qualcomm's AI research, platforms, developer tools, and ecosystem support,

14
00:01:21,120 --> 00:01:26,400
visit twimbleai.com slash Qualcomm.

15
00:01:26,400 --> 00:01:29,600
A quick community update before we dive in.

16
00:01:29,600 --> 00:01:33,800
Many of you are aware that we've been hosting a couple of paper reading meetups in conjunction

17
00:01:33,800 --> 00:01:34,800
with the podcast.

18
00:01:34,800 --> 00:01:40,240
Well, I'm excited to share that Matt Kenny, Duke staff researcher and longtime listener

19
00:01:40,240 --> 00:01:44,880
and friend of the show, has stepped up to help take this group to the next level.

20
00:01:44,880 --> 00:01:50,160
The paper reading meetup will now be meeting every other Sunday at 1 p.m. U.S. Eastern

21
00:01:50,160 --> 00:01:56,760
time to dissect the latest and greatest academic research papers in machine learning and AI.

22
00:01:56,760 --> 00:02:00,760
If you want to take your understanding of the field to the next level, please join us

23
00:02:00,760 --> 00:02:09,640
this Sunday, July 4th, or check twimbleai.com slash meetup for more upcoming community events.

24
00:02:09,640 --> 00:02:13,840
We've also got a couple of study groups currently running, with one group working through

25
00:02:13,840 --> 00:02:18,920
the fast.ai deep learning from the foundations course, formerly known as deep learning for

26
00:02:18,920 --> 00:02:26,280
coders part two, and another working through the Stanford CS224N deep learning for natural

27
00:02:26,280 --> 00:02:28,800
language processing course.

28
00:02:28,800 --> 00:02:32,640
These study groups just started and will be working on these courses through October

29
00:02:32,640 --> 00:02:37,640
and November respectively, so it's not too late to join in.

30
00:02:37,640 --> 00:02:45,880
Sign up on the meetup page at twimbleai.com slash meetup.

31
00:02:45,880 --> 00:02:48,480
Hi everyone, I am on the line with Jeff Galhar.

32
00:02:48,480 --> 00:02:53,480
Jeff is VP of technology and head of AI software platforms at Qualcomm.

33
00:02:53,480 --> 00:02:56,240
Jeff, welcome to this week in machine learning and AI.

34
00:02:56,240 --> 00:02:57,240
Thank you very much.

35
00:02:57,240 --> 00:02:59,840
I'm pleased to be here and thank you for having me, Sam.

36
00:02:59,840 --> 00:03:01,640
It's great to have you on the show.

37
00:03:01,640 --> 00:03:05,600
Let's jump right in and chat a little bit about your background.

38
00:03:05,600 --> 00:03:10,760
You have spent 30 years at Qualcomm as far as that, right?

39
00:03:10,760 --> 00:03:14,240
That's right, not in a linear sort of fashion.

40
00:03:14,240 --> 00:03:19,840
I like to say I'm on my second tour of duty and we can talk about how I came and went.

41
00:03:19,840 --> 00:03:26,080
But yes, I've been here a long time since the long arc of Qualcomm's innovation capacity.

42
00:03:26,080 --> 00:03:31,800
So tell us a little bit about some of the things you've done at the company and how you've

43
00:03:31,800 --> 00:03:37,000
come to get involved in their AI software efforts.

44
00:03:37,000 --> 00:03:38,560
Yeah, perfect, perfect.

45
00:03:38,560 --> 00:03:44,760
So I started as a young engineer a few years after the company got started and so I've

46
00:03:44,760 --> 00:03:51,160
had the pleasure of seeing the company go through ups and downs and challenges and innovate

47
00:03:51,160 --> 00:03:58,640
in first and CDMA before that and other communication vehicles and then move on to 3G and

48
00:03:58,640 --> 00:04:01,760
4G and now 5G wireless standards.

49
00:04:01,760 --> 00:04:07,240
And I spent a lot of my career working in wireless, both in hardware and software.

50
00:04:07,240 --> 00:04:12,040
I worked on ASICs and semiconductors, but most of my career has been in systems and software

51
00:04:12,040 --> 00:04:13,160
work.

52
00:04:13,160 --> 00:04:19,720
And then as a part about Devastrature, the Qualcomm went through in the late 90s.

53
00:04:19,720 --> 00:04:26,680
I left Qualcomm, it was end of two or one and ended up landing at a small startup.

54
00:04:26,680 --> 00:04:30,560
Actually, I didn't know that they were sort of doing what we would today consider sort

55
00:04:30,560 --> 00:04:34,080
of machine learning with support vector machines and so on, but that's what they were doing

56
00:04:34,080 --> 00:04:41,160
and joined them in ran engineering for that company, did commercial work for the government

57
00:04:41,160 --> 00:04:43,920
and for Fortune 500 companies.

58
00:04:43,920 --> 00:04:47,680
And that was my first exposure to sort of AI and machine learning, let's say in the

59
00:04:47,680 --> 00:04:49,000
modern age.

60
00:04:49,000 --> 00:04:53,320
And so I learned a lot there and then came back to Qualcomm and with that company we sold

61
00:04:53,320 --> 00:04:58,160
that company and came back to Qualcomm and then again rejoined wireless and spent a good

62
00:04:58,160 --> 00:05:04,600
chunk of the last, you know, 15 years, 16 years I've been back in wireless.

63
00:05:04,600 --> 00:05:10,600
And then I got a chance to, I was asked to help co lead a project that had gotten started

64
00:05:10,600 --> 00:05:12,320
in spiking neural networks.

65
00:05:12,320 --> 00:05:15,440
So we had made an investment in a small company.

66
00:05:15,440 --> 00:05:17,960
We did a joint research program with this company.

67
00:05:17,960 --> 00:05:19,280
What was that company?

68
00:05:19,280 --> 00:05:24,320
The company has brain corporation, it's still going well here in San Diego.

69
00:05:24,320 --> 00:05:27,920
Qualcomm Ventures took an investment in them and they've since gone on to do sort of

70
00:05:27,920 --> 00:05:31,200
automated robotics and vision systems.

71
00:05:31,200 --> 00:05:36,120
But we did early work with them on the pretext of and in conjunction in some sense with

72
00:05:36,120 --> 00:05:40,760
the DARPA Synapse program, who was actually sort of at the end of the Synapse program,

73
00:05:40,760 --> 00:05:46,440
but we were taking a lot of those similar ideas like IBM was at the time as well.

74
00:05:46,440 --> 00:05:52,440
And really tried to see if we could build sort of biologically accurate systems, vision

75
00:05:52,440 --> 00:05:58,440
systems in this case, in the way that we think of deep learning today, but sort of predating

76
00:05:58,440 --> 00:06:02,880
sort of this Krochevsky kind of, you know, aha moment.

77
00:06:02,880 --> 00:06:09,640
And when that happened, when in 2011, 2012, you know, it kind of became obvious that these

78
00:06:09,640 --> 00:06:12,840
deep neural networks had, you know, hit upon something, right?

79
00:06:12,840 --> 00:06:16,920
It resurrected, you know, back prop and hit upon something.

80
00:06:16,920 --> 00:06:22,960
We moved the program away from spike in neural networks as all kinds of challenges.

81
00:06:22,960 --> 00:06:32,800
And it subsequently, subsequently, we evolved into a deep learning and kind of program.

82
00:06:32,800 --> 00:06:36,920
That was during my time in Qualcomm research.

83
00:06:36,920 --> 00:06:42,560
What has happened in the last few years is that the work that we did there, I left whenever

84
00:06:42,560 --> 00:06:46,240
our commercial group now run our commercial AI software.

85
00:06:46,240 --> 00:06:50,520
And what is, we think of now as Qualcomm AI research has grown and continued based on

86
00:06:50,520 --> 00:06:53,320
that sort of initial work.

87
00:06:53,320 --> 00:06:55,360
And now we're sort of partners.

88
00:06:55,360 --> 00:06:59,800
They work on long-term research and bring innovations that we then bring to products.

89
00:06:59,800 --> 00:07:06,440
And we work with our SOC channels and our customers that are using our SOCs to help them, you

90
00:07:06,440 --> 00:07:10,120
know, do AI on our chips.

91
00:07:10,120 --> 00:07:15,440
Okay. And for folks that are interested in hearing a bit more about what's happening

92
00:07:15,440 --> 00:07:22,160
from our research perspective at Qualcomm, they might be interested in checking out the

93
00:07:22,160 --> 00:07:30,960
recent interview with Max Welling that is Twomel Talk 267 from back in May.

94
00:07:30,960 --> 00:07:36,200
But that was a great conversation, as I'm sure this one will be.

95
00:07:36,200 --> 00:07:39,920
You mentioned the spiking neural networks work.

96
00:07:39,920 --> 00:07:44,400
I don't think Max and I got into any of that kind of stuff.

97
00:07:44,400 --> 00:07:49,320
And it's come up, you know, maybe superficially on the show once or twice.

98
00:07:49,320 --> 00:07:53,640
But I don't think I've ever got anyone to kind of share, you know, who's worked in it

99
00:07:53,640 --> 00:07:56,400
to share a little bit of background on that.

100
00:07:56,400 --> 00:07:58,680
Is that something you can maybe spend a few minutes on?

101
00:07:58,680 --> 00:07:59,680
Sure.

102
00:07:59,680 --> 00:08:00,680
Sure.

103
00:08:00,680 --> 00:08:02,880
I have to dust off some old memories, but I'm going to get to do.

104
00:08:02,880 --> 00:08:09,440
So okay, so the basic idea was, you know, in some sense, this basic intuition that

105
00:08:09,440 --> 00:08:12,760
neural networks are made of, you know, stacks of neurons and they're interconnected

106
00:08:12,760 --> 00:08:13,760
somehow.

107
00:08:13,760 --> 00:08:18,560
And they're the some learning mechanisms where the strength of the connections is, you

108
00:08:18,560 --> 00:08:23,680
know, somehow related to the experience, if you will, that the neurons get just like

109
00:08:23,680 --> 00:08:26,400
an artificial neural network in some sense.

110
00:08:26,400 --> 00:08:31,320
But the gentleman who started Braincore Eugenia Sikavic had written a book about how to mathematically

111
00:08:31,320 --> 00:08:39,400
model the basic 20 or so synaptic behaviors, as we understand them in mammals, I'd say.

112
00:08:39,400 --> 00:08:44,440
And the visual system is the most well sort of studied part of our cortex.

113
00:08:44,440 --> 00:08:51,360
And so that was the premise of, and what I would say is that was kind of our seed moment

114
00:08:51,360 --> 00:08:58,000
for Qualcomm to get started in what has now sort of become a clearly a revolution computation.

115
00:08:58,000 --> 00:09:03,600
And so it was a bat, it was a really early stage bat that the next computing evolution

116
00:09:03,600 --> 00:09:06,760
would be in this general direction.

117
00:09:06,760 --> 00:09:11,920
And we thought, hey, look, well, the brain does this amazing thing in 20 billion neurons

118
00:09:11,920 --> 00:09:16,840
and 20 watts, let's say roughly speaking, gee, could we build some kind of computing machine

119
00:09:16,840 --> 00:09:18,760
that kind of works like that.

120
00:09:18,760 --> 00:09:25,280
And so we did, and we had some early success, and we built a large scale like training and

121
00:09:25,280 --> 00:09:27,360
simulation kind of environment.

122
00:09:27,360 --> 00:09:32,600
And we did work in FPGA's to actually build hardware, made a design hardware for such

123
00:09:32,600 --> 00:09:34,120
a machine.

124
00:09:34,120 --> 00:09:38,120
And what it really, I think if you boil it down to where it didn't work, it worked in

125
00:09:38,120 --> 00:09:39,120
a lot of ways.

126
00:09:39,120 --> 00:09:41,960
But where it didn't work is back prop.

127
00:09:41,960 --> 00:09:49,280
There was no sort of fundamental principled way to train these networks in a way that produced

128
00:09:49,280 --> 00:09:50,280
stable output.

129
00:09:50,280 --> 00:09:53,600
And so you see that maybe a little bit with GANs, you see them maybe with the struggles

130
00:09:53,600 --> 00:09:55,880
of people having RNNs and related things.

131
00:09:55,880 --> 00:09:57,480
But this was on a massive scale.

132
00:09:57,480 --> 00:10:02,280
You could train small scale networks to do very, very simple things like be tricked into

133
00:10:02,280 --> 00:10:06,600
the optical illusions that human visual cortex systems can be tricked into.

134
00:10:06,600 --> 00:10:09,720
But it's an interesting use case.

135
00:10:09,720 --> 00:10:10,720
Yeah.

136
00:10:10,720 --> 00:10:13,840
Well, it was kind of a concept of like where we kind of on the right track.

137
00:10:13,840 --> 00:10:18,520
And so you could do a very simple like, think about an intention mechanism.

138
00:10:18,520 --> 00:10:24,320
If you stare at an object, your visual system will start, you know, looking for changes

139
00:10:24,320 --> 00:10:28,920
in lines and edges and changes in light, and you could get the system to kind of do that

140
00:10:28,920 --> 00:10:29,920
same sort of thing.

141
00:10:29,920 --> 00:10:33,480
If it's stared at an object long enough, the object would sort of vanish because you

142
00:10:33,480 --> 00:10:39,400
don't have any, you know, psychotic eye movement or anything to sort of stimulate the neurons.

143
00:10:39,400 --> 00:10:43,800
But as you tried to build like a real thing to say a visual system or an object recognition

144
00:10:43,800 --> 00:10:48,040
system at that kind of scale, they just were super hard to train.

145
00:10:48,040 --> 00:10:52,360
And so there were limits and they're basically, you know, fundamental limits that systems

146
00:10:52,360 --> 00:10:56,320
like ANNs with back prop basically solve for us today, right?

147
00:10:56,320 --> 00:11:00,000
And what is the spiking refer to and spiking neural networks?

148
00:11:00,000 --> 00:11:01,000
Yeah.

149
00:11:01,000 --> 00:11:07,640
So literally the sort of the idea that in a synapse, right, in a neuron, in a biological

150
00:11:07,640 --> 00:11:10,080
neuron, it is, you know, gets all this input.

151
00:11:10,080 --> 00:11:13,800
Think about it like a sort of sigma delta kind of time coded input.

152
00:11:13,800 --> 00:11:15,800
You get a little bit of stimulus, a little bit of stimulus.

153
00:11:15,800 --> 00:11:22,120
And when it reaches some kind of threshold, depending on the kind of neuron, it fires.

154
00:11:22,120 --> 00:11:27,600
It sends a signal down to the next neuron or next neurons, however it's connected.

155
00:11:27,600 --> 00:11:29,520
And it's got some kind of decay function, right?

156
00:11:29,520 --> 00:11:35,080
So you can think about these not being like a step function, most of them aren't.

157
00:11:35,080 --> 00:11:37,720
They've got some kind of exponential sort of decay function.

158
00:11:37,720 --> 00:11:41,840
So when we think of spiking, one of the intuitions was for low power.

159
00:11:41,840 --> 00:11:48,680
It was like only stimulate the neurons that are implicated in some transaction, right?

160
00:11:48,680 --> 00:11:53,800
And modeling them biologically in a biologically accurate way in terms of how real neurons

161
00:11:53,800 --> 00:11:59,960
collect input and then, you know, fire, right, discharge their output to the next downstream

162
00:11:59,960 --> 00:12:00,960
neuron.

163
00:12:00,960 --> 00:12:05,440
Because of course your brain is not, really neurons are not active all at the same time.

164
00:12:05,440 --> 00:12:08,600
So that's when we think of spiking, what we really mean is, you know, what is that impulse

165
00:12:08,600 --> 00:12:13,120
function when you reach the, you know, exit threshold, if you will, of the neuron, what

166
00:12:13,120 --> 00:12:14,120
is that impulse?

167
00:12:14,120 --> 00:12:15,520
You know, what is the output?

168
00:12:15,520 --> 00:12:18,240
And at what point does it reach that output, right?

169
00:12:18,240 --> 00:12:19,240
Yeah.

170
00:12:19,240 --> 00:12:20,240
Yeah.

171
00:12:20,240 --> 00:12:25,200
And have you followed the work in this field, it still comes up.

172
00:12:25,200 --> 00:12:29,720
So I imagine there's, you know, there's some progress that's been made.

173
00:12:29,720 --> 00:12:33,080
Do you have a sense for where we are today?

174
00:12:33,080 --> 00:12:37,720
Well, I do follow it a little bit when it comes up in various, you know, journals and

175
00:12:37,720 --> 00:12:39,320
blogs and so on.

176
00:12:39,320 --> 00:12:43,800
I still feel like we're at a place where in some sense we don't have a fundamentally

177
00:12:43,800 --> 00:12:50,800
principled way to train these kinds of systems, and I guess over time, do we have that for

178
00:12:50,800 --> 00:12:52,640
regular deep neural networks?

179
00:12:52,640 --> 00:12:53,640
Well, okay.

180
00:12:53,640 --> 00:12:58,680
So we could put some parameters around that, but we have backprop, so we have a structured

181
00:12:58,680 --> 00:13:02,680
way of propagating these errors of tuning, right?

182
00:13:02,680 --> 00:13:07,680
The strengths of biases, if you will, of all these connections.

183
00:13:07,680 --> 00:13:12,520
But I think that part of it is that what I think artificial neural networks have shown

184
00:13:12,520 --> 00:13:20,880
us so far, you don't need anything as complicated as actually modeling the biology of a neuron

185
00:13:20,880 --> 00:13:23,880
to achieve some pretty impressive results.

186
00:13:23,880 --> 00:13:27,400
In the same way that airplanes, this is probably an old analogy, but in the same way that airplanes

187
00:13:27,400 --> 00:13:33,080
don't flap their wings, you know, it's likely that we can build, they're still very complicated

188
00:13:33,080 --> 00:13:38,480
systems, but systems that don't work quite like the brain works and achieve maybe similar

189
00:13:38,480 --> 00:13:39,960
results, right?

190
00:13:39,960 --> 00:13:47,040
And so most of the work I think in spiking has moved in the direction of things like hardware

191
00:13:47,040 --> 00:13:52,240
architectures that are event driven, there are quite a number of efforts in that area,

192
00:13:52,240 --> 00:13:58,600
and using those kinds of architectures to basically map artificial neural networks onto substrates

193
00:13:58,600 --> 00:14:05,040
that are more energy efficient or more computationally efficient in some way by applying like an

194
00:14:05,040 --> 00:14:10,280
attention mechanism, right, by applying a different way of computing these things by using analog

195
00:14:10,280 --> 00:14:12,120
circuitry in a lot of cases.

196
00:14:12,120 --> 00:14:18,440
So different strategies where the angle is more about low power or more about, you know,

197
00:14:18,440 --> 00:14:23,720
activating only the parts of the network that need to be activated at a given point in time.

198
00:14:23,720 --> 00:14:27,600
So that was I think quite a digression in your bio.

199
00:14:27,600 --> 00:14:28,600
Yeah.

200
00:14:28,600 --> 00:14:34,640
There you go, you want the arc of how we end, how I ended up here and that's how I ended

201
00:14:34,640 --> 00:14:35,640
up here.

202
00:14:35,640 --> 00:14:36,640
Awesome.

203
00:14:36,640 --> 00:14:37,640
Awesome.

204
00:14:37,640 --> 00:14:43,920
And so today maybe dive a little bit deeper into your current role and some of the things

205
00:14:43,920 --> 00:14:47,680
that Qualcomm is working on from a software perspective.

206
00:14:47,680 --> 00:14:48,680
Sure.

207
00:14:48,680 --> 00:14:49,680
Sure.

208
00:14:49,680 --> 00:14:54,520
So as part of this going back a little in the bio as part of this early research we were

209
00:14:54,520 --> 00:14:59,320
doing, we started on things like face detection and object detection and this led us to buy

210
00:14:59,320 --> 00:15:03,640
some companies and set up some additional research offices and so on.

211
00:15:03,640 --> 00:15:10,880
And part of that effort is we realized in this, you know, predates a lot of other movers

212
00:15:10,880 --> 00:15:16,480
here, predates TensorFlow Lite and so on, we realized that in order for us even to experiment

213
00:15:16,480 --> 00:15:23,760
internally to do this on Qualcomm Snapdragon SOCs, we needed some kind of toolkit to, you

214
00:15:23,760 --> 00:15:29,200
know, run all these kernels on our GPU, for example, or on our DSP.

215
00:15:29,200 --> 00:15:35,400
And so we started and we built what now you can go to the developer network and download

216
00:15:35,400 --> 00:15:43,160
the Snapdragon neural processing SDK and but an early version and we targeted at internal

217
00:15:43,160 --> 00:15:48,520
use cases mostly and for evaluating our research and it started to become clear that our customers

218
00:15:48,520 --> 00:15:52,720
were coming to us and saying, Hey, you know, I've got an AI thing I want to try to run.

219
00:15:52,720 --> 00:15:56,880
And I saw your demo, I didn't realize you could do this and this and this on an edge device

220
00:15:56,880 --> 00:15:59,880
on a mobile device on a phone.

221
00:15:59,880 --> 00:16:03,760
Can we have that toolkit basically you have a toolkit and so that led us to commercialize

222
00:16:03,760 --> 00:16:10,920
a toolkit and so today my role is to lead a global organization that commercializes our

223
00:16:10,920 --> 00:16:17,040
AI software stacks not just that toolkit, but our overall AI software stacks in order

224
00:16:17,040 --> 00:16:25,000
to make it our hardware basically our high performance SOCs accessible to internal and external

225
00:16:25,000 --> 00:16:30,600
customers who want to run, you know, AI powered solutions on our SOCs.

226
00:16:30,600 --> 00:16:37,800
And so that what that really has taken a form of is that we are architecture and our chips

227
00:16:37,800 --> 00:16:44,320
is heterogeneous, not every core or every use case lends itself to, you know, every piece

228
00:16:44,320 --> 00:16:47,320
of hardware depending on what your use case is.

229
00:16:47,320 --> 00:16:54,040
And so we provide high performance solutions principally targeting our GPU, our hexagon

230
00:16:54,040 --> 00:17:01,040
DSP and in our flagship parts are HTA or tensor accelerator core.

231
00:17:01,040 --> 00:17:06,880
And then we provide access, high performance access to those hardware blocks either through

232
00:17:06,880 --> 00:17:15,680
our SDK or through Android NN API, which is, you know, Google Android's newest sort of

233
00:17:15,680 --> 00:17:19,040
neural network API and framework.

234
00:17:19,040 --> 00:17:25,080
And recently at Google IOW we announced direct work with the TensorFlow Lite team to power

235
00:17:25,080 --> 00:17:31,680
the backend of TensorFlow Lite with our hexagon neural network accelerator library.

236
00:17:31,680 --> 00:17:37,800
So those are examples of where we're building these high performance AI software blocks

237
00:17:37,800 --> 00:17:46,000
and then exposing them via various ecosystem strategies based on what the ecosystem needs

238
00:17:46,000 --> 00:17:48,160
for those use cases.

239
00:17:48,160 --> 00:17:54,560
You know, when you kind of take a step back and look at this landscape with similar functionality

240
00:17:54,560 --> 00:18:06,120
being exposed via these varying APIs, you mentioned a bunch of them, the Android NN APIs, TensorFlow

241
00:18:06,120 --> 00:18:12,920
Lite, you've got your own stack that a developer can interact directly with as well as, you

242
00:18:12,920 --> 00:18:17,760
know, that supports these other things like what's the best way to make sense of all that

243
00:18:17,760 --> 00:18:25,160
if you're a developer or machine learning engineer that needs to deploy stuff out to a device,

244
00:18:25,160 --> 00:18:26,920
how do you know what you should be using?

245
00:18:26,920 --> 00:18:33,360
Yeah, that's a great question and we face a lot of those kinds of questions.

246
00:18:33,360 --> 00:18:38,880
You know, it's hard to make a universal recommendation, but I could give a little bit of guidance.

247
00:18:38,880 --> 00:18:44,560
I would say that maybe 18 months ago the question, if you'd asked a question similar to it,

248
00:18:44,560 --> 00:18:51,160
which training framework do I use, because the question was really about this kind of explosion

249
00:18:51,160 --> 00:18:54,160
of training frameworks that had happened, right?

250
00:18:54,160 --> 00:18:55,160
We do still have questions.

251
00:18:55,160 --> 00:18:59,960
Now are we talking about kind of TensorFlow versus PyTorch versus something else?

252
00:18:59,960 --> 00:19:06,360
Sure, it was PyTorch, which didn't exist at the time, by the way, 18 months ago.

253
00:19:06,360 --> 00:19:17,960
So a cafe to TensorFlow, now PyTorch, MXNet, CNTK from Microsoft, and then if you think

254
00:19:17,960 --> 00:19:23,760
about it, you think even globally, then in Asia, there's a list, there's Chainer and

255
00:19:23,760 --> 00:19:33,160
there's Parrot and there's all kinds of proprietary mace from Xiaomi, proprietary and quasi-proprietary

256
00:19:33,160 --> 00:19:35,760
frameworks that are out there.

257
00:19:35,760 --> 00:19:41,560
And so one of the things that we tried to do early on was to pick a few, we couldn't support

258
00:19:41,560 --> 00:19:50,160
them all and provide converters from those frameworks to our SDK so that customers didn't

259
00:19:50,160 --> 00:19:52,560
have to, in some sense, make some of these choices.

260
00:19:52,560 --> 00:19:58,160
They could train in TensorFlow if they wanted, they could train in cafe, and then you

261
00:19:58,160 --> 00:20:04,760
can convert your network from those frameworks into something we understand and it can accelerate

262
00:20:04,760 --> 00:20:05,760
for you, right?

263
00:20:05,760 --> 00:20:08,760
And so part of it was just making it easier.

264
00:20:08,760 --> 00:20:14,560
Now in the process, Onyx came along, in part to address this issue of, look, I've got

265
00:20:14,560 --> 00:20:18,360
all these training frameworks and I don't have a way to move between them and I sort of

266
00:20:18,360 --> 00:20:23,200
get locked in with the operators or the techniques that that training system TensorFlow

267
00:20:23,200 --> 00:20:25,560
PyTorch cafe use.

268
00:20:25,560 --> 00:20:32,880
And then I have exactly this problem, oh, I want to run on this SOC, but I started in TensorFlow

269
00:20:32,880 --> 00:20:35,880
and they don't support that operator in TensorFlow.

270
00:20:35,880 --> 00:20:43,280
And so what would really, the advice I give people is, look, there are a lot of training

271
00:20:43,280 --> 00:20:48,840
frameworks, but there's maybe three or four Macs that are really robust and popular.

272
00:20:48,840 --> 00:20:56,440
A lot of the toolkits are going to find their way through one of those into either RSDK

273
00:20:56,440 --> 00:20:59,800
or let's say through TensorFlow Lite onto our SOC.

274
00:20:59,800 --> 00:21:06,240
And so, you know, the funnel is getting narrower, you will have a rich choice of training environments.

275
00:21:06,240 --> 00:21:10,040
A lot of them are doing these eager mode, very Python-like kinds of environments.

276
00:21:10,040 --> 00:21:14,840
And when you're ready to deploy to your edge device, your phone, your IoT device, your

277
00:21:14,840 --> 00:21:17,520
automobile, there'll be an on-ramp, right?

278
00:21:17,520 --> 00:21:25,280
And then our job is to make that network run as fast as possible, you know, power, constraint,

279
00:21:25,280 --> 00:21:28,360
kind of environment that a lot of these applications face.

280
00:21:28,360 --> 00:21:37,120
So in other words, the story today is more like, look, choose one of the popular frameworks

281
00:21:37,120 --> 00:21:46,240
that maps to the experience that you want to have as a developer, you know, ALA, the

282
00:21:46,240 --> 00:21:51,960
differences between TensorFlow versus PyTorch versus something else.

283
00:21:51,960 --> 00:21:58,920
And, you know, we'll build essentially middleware that ensures that, you know, that framework

284
00:21:58,920 --> 00:22:04,160
that you're using can take advantage of, transparently to you, you know, all the things that

285
00:22:04,160 --> 00:22:06,520
underlying chipset is capable of.

286
00:22:06,520 --> 00:22:08,240
Yeah, that's a great way to put it.

287
00:22:08,240 --> 00:22:15,200
That's exactly kind of our perspective is to reduce the friction for our users, customers,

288
00:22:15,200 --> 00:22:20,440
partners, to go from, you know, where they're comfortable onto high-performance solutions

289
00:22:20,440 --> 00:22:22,080
on our devices.

290
00:22:22,080 --> 00:22:26,480
Yeah, forget the timeframe you threw out there, but, you know, historically, it was really

291
00:22:26,480 --> 00:22:33,520
all about kind of this training experience is the follow-on to that that today it's

292
00:22:33,520 --> 00:22:36,240
more about inference?

293
00:22:36,240 --> 00:22:39,880
Well, okay, so I'll make a twist on this.

294
00:22:39,880 --> 00:22:45,440
Yes, it's about inference in the sense that there are, you know, billions of devices

295
00:22:45,440 --> 00:22:51,040
that people want to run some kind of AI-powered inference solution, you know, whether it's

296
00:22:51,040 --> 00:23:00,440
a camera, whether it's audio, like translation, like, you know, Google Home, like, you know,

297
00:23:00,440 --> 00:23:03,800
the explosion of audio use cases we're seeing.

298
00:23:03,800 --> 00:23:09,160
I would characterize that training on device and I will characterize this a little bit more

299
00:23:09,160 --> 00:23:14,560
subtly, what I'll call personalization of the experience is something that I think will

300
00:23:14,560 --> 00:23:17,720
become important and we're starting to see and we're starting to make investments in

301
00:23:17,720 --> 00:23:18,720
that direction.

302
00:23:18,720 --> 00:23:20,040
We have done it in the past.

303
00:23:20,040 --> 00:23:21,360
We know it's possible.

304
00:23:21,360 --> 00:23:26,600
I want to personalize like how my gallery is organized or something in my phone, okay,

305
00:23:26,600 --> 00:23:27,600
right?

306
00:23:27,600 --> 00:23:28,600
Pretty straightforward.

307
00:23:28,600 --> 00:23:35,760
I think the next evolution of personalization will be about things like really contextualizing

308
00:23:35,760 --> 00:23:41,880
the experience of my device to me, to my preferences, to how I use the device.

309
00:23:41,880 --> 00:23:47,240
But we're not there yet, but I would characterize, in my view, the training aspect to be less

310
00:23:47,240 --> 00:23:54,880
about, you know, big data and, you know, massive data sets and more about taking like, if

311
00:23:54,880 --> 00:24:00,960
you will, a vanilla experience and then personalizing that vanilla experience out of the box

312
00:24:00,960 --> 00:24:06,560
experience into something that feels a lot more customized, right, to your experience,

313
00:24:06,560 --> 00:24:07,560
right?

314
00:24:07,560 --> 00:24:11,520
On this personalization point, this is a really interesting point that I've been curious

315
00:24:11,520 --> 00:24:12,520
about for a while.

316
00:24:12,520 --> 00:24:15,840
Do you have a sense for kind of how this is done today?

317
00:24:15,840 --> 00:24:21,960
You know, for example, you know, think about an app like, you know, Gmail that is doing

318
00:24:21,960 --> 00:24:29,480
it the predictive replies or even now as you're typing, like predicting the end of your sentences.

319
00:24:29,480 --> 00:24:35,800
You know, presumably they built out some language model there and it's able to do, you

320
00:24:35,800 --> 00:24:42,200
know, it's able to predict based on, you know, tons and tons of data that they've got

321
00:24:42,200 --> 00:24:43,200
of people's emails.

322
00:24:43,200 --> 00:24:48,840
And that's a whole separate issue about like the privacy issues associated with that,

323
00:24:48,840 --> 00:24:49,840
et cetera.

324
00:24:49,840 --> 00:24:56,760
But, you know, but it also is kind of appears to be personalized to me in the sense that

325
00:24:56,760 --> 00:25:04,800
I think it kind of replies in the way that I tend to reply as opposed to like just some

326
00:25:04,800 --> 00:25:07,600
generalization of what everybody does.

327
00:25:07,600 --> 00:25:14,640
And so how does that, you know, how are folks achieving that today, you know, with machine

328
00:25:14,640 --> 00:25:16,800
learning models on devices?

329
00:25:16,800 --> 00:25:17,800
Well, okay.

330
00:25:17,800 --> 00:25:22,680
So I'd say that, you know, you've really hit on what I can still consider sort of a big

331
00:25:22,680 --> 00:25:26,200
data use case.

332
00:25:26,200 --> 00:25:30,560
And I hate to speculate exactly how Google is doing this, you know, on a behind the scenes

333
00:25:30,560 --> 00:25:32,040
in Gmail.

334
00:25:32,040 --> 00:25:36,760
I think the personalization on device is relatively nascent.

335
00:25:36,760 --> 00:25:40,560
I think this is one of these emerging, you know, things to watch and to come back in

336
00:25:40,560 --> 00:25:43,840
a year or 18 months and talk about it where it's moved.

337
00:25:43,840 --> 00:25:49,840
We're seeing some, I'll put them in the personalization category and maybe people don't think

338
00:25:49,840 --> 00:25:56,600
about it this way, but nascent things like, um, like the ability for you to have fingerprint

339
00:25:56,600 --> 00:25:58,080
or face unlock, right?

340
00:25:58,080 --> 00:26:02,920
That's a personalized thing, you have a generic feature face unlock, but it unlocks for

341
00:26:02,920 --> 00:26:06,040
your face or your fingerprint, right?

342
00:26:06,040 --> 00:26:07,520
Speaker identification, right?

343
00:26:07,520 --> 00:26:13,000
So the ability for a device to know that you're Sam and I'm Jeff, because I've, we've

344
00:26:13,000 --> 00:26:17,120
said 10 keywords or five keywords and it picks up builds a pattern, right?

345
00:26:17,120 --> 00:26:22,320
It's doing that with, let's say, building some kind of classifier by using the features

346
00:26:22,320 --> 00:26:28,040
of the built-in model and then running a classification pass over this, right?

347
00:26:28,040 --> 00:26:32,680
These kinds of things we know are possible, we've done them ourselves on device.

348
00:26:32,680 --> 00:26:36,840
What I think the next revolution will be much deeper integration.

349
00:26:36,840 --> 00:26:44,040
What is the, it's not just a single app, it's like what is the sort of experience of, um,

350
00:26:44,040 --> 00:26:47,960
of the device really looking at how you interact with a lot of different kinds of data and

351
00:26:47,960 --> 00:26:53,800
then drawing some personalization experiences out of that, right?

352
00:26:53,800 --> 00:26:58,880
So that ain't going back to the privacy thing so that in the long arc of it, these things

353
00:26:58,880 --> 00:27:03,440
don't require, you know, massive amounts of cloud data, for example, right, to, in order

354
00:27:03,440 --> 00:27:06,040
to get that sort of more intimate experience.

355
00:27:06,040 --> 00:27:12,920
Yeah, I feel that there's like, I'm struggling with the right language to describe this problem

356
00:27:12,920 --> 00:27:13,920
space.

357
00:27:13,920 --> 00:27:17,680
And maybe it's because it's so new we don't have it yet, but, you know, if you or if anyone

358
00:27:17,680 --> 00:27:21,800
else is listening to this, you know, does have it, you know, getting touched, but I guess

359
00:27:21,800 --> 00:27:28,760
it strikes me that there are different classes of use case here, like the face ID and voice

360
00:27:28,760 --> 00:27:36,760
ID strikes me as like, uh, it's a relatively simple use case, uh, you know, where you,

361
00:27:36,760 --> 00:27:42,160
you kind of train this classifier and it's, I don't know, the word monolithic is, it comes

362
00:27:42,160 --> 00:27:43,160
to mind.

363
00:27:43,160 --> 00:27:47,360
You know, but it seems like there are, there's, these other use cases, maybe more like

364
00:27:47,360 --> 00:27:53,640
Gmail, where what you want to do is kind of akin to some kind of hierarchical model where

365
00:27:53,640 --> 00:28:00,720
you've got like the, it's almost like a edge transfer learning kind of thing where you've

366
00:28:00,720 --> 00:28:05,280
got the met the master model, but then you want to fine tune that model based on, you

367
00:28:05,280 --> 00:28:08,880
know, a set of data that is available on the device.

368
00:28:08,880 --> 00:28:13,960
And maybe, maybe blurring a bunch of lines here because my Gmail data is all in the cloud.

369
00:28:13,960 --> 00:28:19,920
Um, uh, but the independent of where the, the data is, there's like a data set that's

370
00:28:19,920 --> 00:28:26,640
purse, that's very personal and then a model that's based on a bunch of people's data.

371
00:28:26,640 --> 00:28:30,320
So Google talked a bit about this kind of approach.

372
00:28:30,320 --> 00:28:34,800
It's not a new idea, but I think, you know, it may be closer to being put into practice.

373
00:28:34,800 --> 00:28:41,880
They discuss a little bit at Google IO about, for example, um, G board, um, and, uh, this

374
00:28:41,880 --> 00:28:44,880
I, you know, the federated learning and how do you do federated learning?

375
00:28:44,880 --> 00:28:49,120
And so what I think you just described in some senses a federated learning use case,

376
00:28:49,120 --> 00:28:50,120
right?

377
00:28:50,120 --> 00:28:53,720
And the example they gave, I thought was, was very, uh, very interesting, you know, you

378
00:28:53,720 --> 00:28:58,160
could think about it like crowdsourcing and it is that, uh, but then, you know, applying

379
00:28:58,160 --> 00:29:04,200
machine learning to it, which is if some new term gets hot and a lot of users start

380
00:29:04,200 --> 00:29:10,760
tweeting a hashtag or some new, you know, term gets coined because of news or politics

381
00:29:10,760 --> 00:29:12,040
or who knows what?

382
00:29:12,040 --> 00:29:16,160
Then, you know, in today, you might have to type that, I don't know, pick a number 50 times

383
00:29:16,160 --> 00:29:20,720
or something before your learning keyboard goes, you know, this guy types his word a lot

384
00:29:20,720 --> 00:29:21,720
over and over and over again.

385
00:29:21,720 --> 00:29:23,040
Maybe I should remember it, right?

386
00:29:23,040 --> 00:29:24,040
Right.

387
00:29:24,040 --> 00:29:26,200
And if you could crowdsource in some sense, right?

388
00:29:26,200 --> 00:29:33,120
If you could like update the predictive model by sourcing and we've done some research

389
00:29:33,120 --> 00:29:38,640
here in privacy preserving, uh, federated learning, um, in a privacy preserving kind

390
00:29:38,640 --> 00:29:45,800
of way, then you can, ideally, you know, climb that sort of hill in terms of the importance

391
00:29:45,800 --> 00:29:48,080
of new terms much more quickly, right?

392
00:29:48,080 --> 00:29:49,600
Maybe in hours or days.

393
00:29:49,600 --> 00:29:52,360
Oh, yeah, that's super interesting, right?

394
00:29:52,360 --> 00:29:57,480
And I think that that kind of application is going to be, and now this is, we're drifting

395
00:29:57,480 --> 00:30:02,640
away from personalization into privacy distributed learning and so on, but there are interesting

396
00:30:02,640 --> 00:30:07,760
applications, I think in like healthcare, for example, uh, similarly, um, you know,

397
00:30:07,760 --> 00:30:14,960
my individual experience, um, is like a tiny, tiny fraction of the collective experience

398
00:30:14,960 --> 00:30:21,120
of a lot of users using the same medicine or with the same symptoms or, you know, whatever

399
00:30:21,120 --> 00:30:22,120
it is, right?

400
00:30:22,120 --> 00:30:28,320
If I can aggregate that data in a privacy preserving, you know, assured kind of way, then

401
00:30:28,320 --> 00:30:33,680
I get a benefit if I can share my experience with others and I can get back personalized

402
00:30:33,680 --> 00:30:36,880
recommendations or I can be told, hey, look, your symptoms are a lot like this other

403
00:30:36,880 --> 00:30:41,800
person over here and we just diagnosed as other person with such and so, um, you know,

404
00:30:41,800 --> 00:30:48,000
you can think about other, you know, life-improving sorts of experiences that rely essentially

405
00:30:48,000 --> 00:30:53,120
on the same property, which is an aggregation of a lot of individual pieces of data and

406
00:30:53,120 --> 00:30:58,120
you could get a personalized in some sense, you know, experience out of that.

407
00:30:58,120 --> 00:31:03,240
So we're talking about inference before, yeah, I got a little feel sorry about that.

408
00:31:03,240 --> 00:31:06,040
No, no, it's awesome.

409
00:31:06,040 --> 00:31:12,480
But I think that led us to, led us to personalization.

410
00:31:12,480 --> 00:31:18,520
One of the questions that I had, I was at the launch for the Cloud AI100 back in April,

411
00:31:18,520 --> 00:31:25,520
which is for those that don't know or don't recall Qualcomm's kind of foray into data

412
00:31:25,520 --> 00:31:37,400
center inference chips or systems and I'm curious is, you know, to what degree, you know,

413
00:31:37,400 --> 00:31:44,480
your teams are starting to think about how all of this, you know, software stack applies,

414
00:31:44,480 --> 00:31:48,920
you know, similarly or differently to the data center devices.

415
00:31:48,920 --> 00:31:50,840
Sure, sure, yeah.

416
00:31:50,840 --> 00:31:58,120
So work with that team a little bit, I think the, that's a complicated, it's a complicated

417
00:31:58,120 --> 00:31:59,120
answer.

418
00:31:59,120 --> 00:32:05,720
So again, like the question we discussed earlier about what does it mean to, you know,

419
00:32:05,720 --> 00:32:10,080
what is a recommendation for when somebody wants to run inference on an edge device and

420
00:32:10,080 --> 00:32:12,680
do they start in TensorFlow or PyTorch?

421
00:32:12,680 --> 00:32:19,600
All those questions and then some come up in the data center because of course you have

422
00:32:19,600 --> 00:32:25,960
a really expensive capital, you know, investment in a data center and a lot of these kinds

423
00:32:25,960 --> 00:32:31,360
of customers are committed to PyTorch or they're committed to TensorFlow or they're committed

424
00:32:31,360 --> 00:32:34,760
to a proprietary framework.

425
00:32:34,760 --> 00:32:42,480
And so we have very analogous issues in order to enable that server chip in the data center

426
00:32:42,480 --> 00:32:43,880
and to provide our customers.

427
00:32:43,880 --> 00:32:45,840
Again, the goal is the same.

428
00:32:45,840 --> 00:32:51,760
You pick your training system, let's say TensorFlow and we want to provide a high performance,

429
00:32:51,760 --> 00:32:57,160
in this case, inference in the first generation product at, you know, high density, lower

430
00:32:57,160 --> 00:33:01,520
power, all the things that you heard about in the pitch and how do we do that kind of

431
00:33:01,520 --> 00:33:03,800
in a frictionless sort of way.

432
00:33:03,800 --> 00:33:06,920
And so let me make it maybe a few points here.

433
00:33:06,920 --> 00:33:13,120
One is the problems are analogous and so we can share our experiences and we do between

434
00:33:13,120 --> 00:33:20,040
how we've seen the edge inference dynamics evolve as the marketplaces become somewhat

435
00:33:20,040 --> 00:33:26,520
more mature and, you know, what kinds of questions and needs customers will have as they bring

436
00:33:26,520 --> 00:33:28,200
their problems to these devices.

437
00:33:28,200 --> 00:33:33,200
So that's one area where I think it's again, it's very analogous and our goal will be

438
00:33:33,200 --> 00:33:39,560
very similar which is provide the lowest friction path that we can for people to bring their

439
00:33:39,560 --> 00:33:43,280
cloud inference tasks to our devices.

440
00:33:43,280 --> 00:33:48,880
But linking a lot of stuff that Qualcomm says does does well and we're invested in and

441
00:33:48,880 --> 00:33:55,120
thinking about Qualcomm as a company that does really well with end end very high complexity

442
00:33:55,120 --> 00:33:56,600
system problems, right?

443
00:33:56,600 --> 00:34:02,840
When you think about a wireless system, it's not just the device, the chip in the handset,

444
00:34:02,840 --> 00:34:08,240
it's the standards, it's the radio, you know, protocols, it's what happens at the base

445
00:34:08,240 --> 00:34:11,920
station versus what happens at the edge device, the whole thing.

446
00:34:11,920 --> 00:34:17,080
When you kind of zoom out and you say, how is this going to fit with AI, then you have

447
00:34:17,080 --> 00:34:23,640
a really, really very complicated and very interesting situation that I think few companies

448
00:34:23,640 --> 00:34:28,800
in the world are really well positioned to sort of look at the whole thing and that's

449
00:34:28,800 --> 00:34:33,600
if I've got an inference chip in the cloud or at the, let's call it heavy edge of the

450
00:34:33,600 --> 00:34:38,560
radio access network and I've got a bunch of edge devices, whether it's their phones

451
00:34:38,560 --> 00:34:45,080
or their IoT devices or their cars and I connect it all with 5G or Wi-Fi or both, now

452
00:34:45,080 --> 00:34:46,320
what happens, right?

453
00:34:46,320 --> 00:34:49,560
And so these are the kinds of problems we're starting to think about, how do we enable

454
00:34:49,560 --> 00:34:55,320
customers to apportion their use case between their edge device, their, you know, maybe edge

455
00:34:55,320 --> 00:34:58,760
compute and the cloud, they're going to have different latencies.

456
00:34:58,760 --> 00:35:02,600
The closer you are to the edge of the 5G network, you have very, very low latencies, super

457
00:35:02,600 --> 00:35:03,600
high bandwidth.

458
00:35:03,600 --> 00:35:09,120
So maybe you can do things like AR and VR split rendering, very easily, right?

459
00:35:09,120 --> 00:35:14,920
So you have lighter weight, you know, headsets that take advantage of massive compute, not

460
00:35:14,920 --> 00:35:16,480
that far away.

461
00:35:16,480 --> 00:35:21,320
Maybe similarly you can do this kind of stuff with, you know, voice translation and other

462
00:35:21,320 --> 00:35:22,920
kind of hard use cases.

463
00:35:22,920 --> 00:35:28,680
You do a certain amount of it on device and then you send the rest of it over 5G to another

464
00:35:28,680 --> 00:35:32,520
inference solution that's not so far away in terms of latency.

465
00:35:32,520 --> 00:35:36,600
These are the kinds of problems we're starting to think about to kind of go beyond just,

466
00:35:36,600 --> 00:35:42,040
okay, how do we enable our customers to run on our devices in the cloud and in the edge,

467
00:35:42,040 --> 00:35:46,400
but then how do we help them start to assemble, you know, real systems that take advantage

468
00:35:46,400 --> 00:35:49,200
of all of these building blocks that we have to offer them?

469
00:35:49,200 --> 00:35:52,920
Yeah, as we've talked about on the software side, it's not just Qualcomm, like there's

470
00:35:52,920 --> 00:35:56,560
tons of initiatives.

471
00:35:56,560 --> 00:36:03,520
One of them that we talked a little bit about before we started rolling here was TinyML.

472
00:36:03,520 --> 00:36:04,960
What's going on with that?

473
00:36:04,960 --> 00:36:08,800
What is it and what are the goals there?

474
00:36:08,800 --> 00:36:14,480
We had a first ever, hopefully annual, I think it'll be annualized, a conference, code

475
00:36:14,480 --> 00:36:20,080
shared between Qualcomm and Google, a couple months back, the TinyML conference and you

476
00:36:20,080 --> 00:36:24,720
can look at for it and LinkedIn and there's some, you know, websites and so on that we

477
00:36:24,720 --> 00:36:26,540
can link you to.

478
00:36:26,540 --> 00:36:34,520
And the idea there is to think about really low power, really simple devices, right?

479
00:36:34,520 --> 00:36:40,200
And so for example, working with Pete Warden at Google, he's made sort of this recent push

480
00:36:40,200 --> 00:36:49,580
on sort of microcontroller-sized devices to run ML and again, at the TensorFlow summit

481
00:36:49,580 --> 00:36:54,160
a few months back, he showed a real simple, you know, microcontroller-powered board that

482
00:36:54,160 --> 00:36:58,780
could do a wake word detection and so on and very, very small footprint.

483
00:36:58,780 --> 00:37:04,280
So this is a like a logical extension of the work that we're already doing, you know,

484
00:37:04,280 --> 00:37:08,380
moving, you know, all the way from the server we were just talking about out to mobile phones

485
00:37:08,380 --> 00:37:14,640
and then out even farther to, you know, microcontroller-sized devices, whether it's a sensor, temperature

486
00:37:14,640 --> 00:37:20,160
sensor, you know, some of the kind of really simple device, you know, what does it take

487
00:37:20,160 --> 00:37:26,740
to do machine learning there, can you run TensorFlow ultra light there, can you do it in, you

488
00:37:26,740 --> 00:37:30,380
know, 30, 40, 50K of memory, you know, how many MIPS does it take?

489
00:37:30,380 --> 00:37:35,780
So this requires innovation in the software frameworks, you know, how do you actually,

490
00:37:35,780 --> 00:37:40,580
you know, build a library that's that small that can do something meaningful and Google

491
00:37:40,580 --> 00:37:43,820
and others, they're working with, are making progress there.

492
00:37:43,820 --> 00:37:45,460
What kind of hardware should you have?

493
00:37:45,460 --> 00:37:50,700
So you think about, again, these really simple low power, maybe you want to device, it'll

494
00:37:50,700 --> 00:37:56,740
run a year or two years or three years on a battery, right, can you do that?

495
00:37:56,740 --> 00:37:57,980
This is the direction that we're doing.

496
00:37:57,980 --> 00:38:05,020
We're partnering with, you know, academia and with people like Pete Warden and his team

497
00:38:05,020 --> 00:38:11,020
at Google and starting to explore this and over the course of time, this will lead us

498
00:38:11,020 --> 00:38:16,060
to hardware innovations and software innovations to solve, you know, machine learning problems

499
00:38:16,060 --> 00:38:19,380
on these really, really small devices.

500
00:38:19,380 --> 00:38:25,420
It also brings me to another point I wanted to mention that kind of going back to my roots

501
00:38:25,420 --> 00:38:32,180
on around Qualcomm AI research and the product side is a lot of our research focus is on

502
00:38:32,180 --> 00:38:33,500
these kinds of problems.

503
00:38:33,500 --> 00:38:38,860
How do you compress a network, quantize a network, you know, compactify it, if you will,

504
00:38:38,860 --> 00:38:44,380
in some way, and how do you make the software systems, the frameworks, what you call the

505
00:38:44,380 --> 00:38:48,420
middleware, I think it was a good term, how do you make those lean and small so they don't

506
00:38:48,420 --> 00:38:51,740
take a ball on a memory and they provide a lot of computation.

507
00:38:51,740 --> 00:38:56,300
And then how do you innovate on the hardware so that it all goes together as a system?

508
00:38:56,300 --> 00:39:01,660
So I've got a compression scheme that leverages something I'm doing on a hardware that takes

509
00:39:01,660 --> 00:39:06,500
advantage of, you know, how my frameworks are set up and then how do I go from TensorFlow

510
00:39:06,500 --> 00:39:12,140
or PyTorch through that whole chain of events to get on to a device to do a meaningful

511
00:39:12,140 --> 00:39:17,300
amount of AI in a power constrained kind of situation?

512
00:39:17,300 --> 00:39:23,820
I guess you said something that suggested the middleware being on device or taking up device

513
00:39:23,820 --> 00:39:31,300
resources versus being part of the development stack, you know, that's running on the, you

514
00:39:31,300 --> 00:39:36,460
know, the developer workstation is to what degree is that the case that, you know, as

515
00:39:36,460 --> 00:39:44,700
you're layering all of these elements, you know, they're all contributing to kind of

516
00:39:44,700 --> 00:39:47,780
the resource taking, you know, using resources on device.

517
00:39:47,780 --> 00:39:49,980
Yeah, that's a great question.

518
00:39:49,980 --> 00:39:52,900
So it's an area we focus on a lot.

519
00:39:52,900 --> 00:39:58,900
And again, I want to highlight for your listeners, I think a really interesting kind of

520
00:39:58,900 --> 00:40:03,380
area of innovation, something to keep an eye out on, is going back to this question

521
00:40:03,380 --> 00:40:09,100
of an inference, it has been mostly the case that the pipeline has looked something like

522
00:40:09,100 --> 00:40:14,300
you training your favorite framework, and this is true, not just of our frameworks, but

523
00:40:14,300 --> 00:40:18,980
the other SSEs in the marketplace have very similar strategies, you training your favorite

524
00:40:18,980 --> 00:40:23,140
framework, you do some amount of quantization, compression, optimization, call it what

525
00:40:23,140 --> 00:40:29,140
you want, and you deploy to your edge device and that middleware, that runtime, whether

526
00:40:29,140 --> 00:40:35,580
it's TensorFlow Lite or it's a proprietary one, has to take that graph, that model, and

527
00:40:35,580 --> 00:40:37,020
interpret it in some way.

528
00:40:37,020 --> 00:40:42,860
So whether it arrives in an open format like TensorFlow Lite's format or it's a proprietary

529
00:40:42,860 --> 00:40:47,220
format, you have to read it, you have to deploy it to your hardware or accelerated whatever

530
00:40:47,220 --> 00:40:51,220
run it over your blast library, whatever your acceleration mechanism is.

531
00:40:51,220 --> 00:40:55,780
And so basically you're interpreting this graph on the fly, which is really handy if

532
00:40:55,780 --> 00:41:01,260
you're an app developer, let's say, and you want to provide an in-store app that has

533
00:41:01,260 --> 00:41:06,860
machine learning, very, very hard to predict, you know, to build an app that's tailor-made

534
00:41:06,860 --> 00:41:12,580
for Snapdragon, this chip or whatever, you really want a pretty general purpose toolkit,

535
00:41:12,580 --> 00:41:16,940
and that's what something like Android and an API provides.

536
00:41:16,940 --> 00:41:20,820
But if I want to hit a microcontroller or I want to hit a purpose built device, like

537
00:41:20,820 --> 00:41:25,660
it's a home speaker, I know what it will do, it will listen to wake words, it'll do translation

538
00:41:25,660 --> 00:41:29,100
or whatever, and it'll talk to the internet for the things they can't do, talk to the

539
00:41:29,100 --> 00:41:30,100
cloud.

540
00:41:30,100 --> 00:41:37,020
I have a very, you know, price-conscious sort of customer, and the bomb is really important.

541
00:41:37,020 --> 00:41:38,540
I want high-performance.

542
00:41:38,540 --> 00:41:39,540
Bomb-gillematerials.

543
00:41:39,540 --> 00:41:40,540
Billematerials, right?

544
00:41:40,540 --> 00:41:41,540
Yeah.

545
00:41:41,540 --> 00:41:44,660
Yeah, how much it costs them to manufacture it, and how much a consumer is willing to spend

546
00:41:44,660 --> 00:41:49,220
to buy, you know, ten of them or five of them at their house, let's say.

547
00:41:49,220 --> 00:41:55,620
And there, what we're heavily involved in now, let's say, particularly for our hexagon

548
00:41:55,620 --> 00:42:03,740
on DSP, is compiler technology, making our middleware a lot more modular so that you can

549
00:42:03,740 --> 00:42:11,340
pull out of the accelerator library just the parts you need for your use case, and compilers,

550
00:42:11,340 --> 00:42:13,980
and making these things all work in conjunction with each other.

551
00:42:13,980 --> 00:42:21,940
So to long answer your question, we're, I think the future world will be a little bit separated.

552
00:42:21,940 --> 00:42:26,540
You'll have these cases where I won't know in advance what my model needs to do.

553
00:42:26,540 --> 00:42:31,900
This is the app store use case, and I need to provide middleware that's as lean as possible,

554
00:42:31,900 --> 00:42:35,100
but yet provides a very generic experience.

555
00:42:35,100 --> 00:42:40,540
So I can essentially run any network that a customer might deploy to my device, and another

556
00:42:40,540 --> 00:42:48,980
world of these from microcontroller up to some, you know, larger device where I have the

557
00:42:48,980 --> 00:42:53,300
network in advance, I know exactly what it needs to do, and so therefore I can spend a lot

558
00:42:53,300 --> 00:42:56,820
of time compiling and optimizing it like you would with a piece of software.

559
00:42:56,820 --> 00:43:03,500
I can distill the network in some way, both architecturally like compressing it or quantizing

560
00:43:03,500 --> 00:43:09,700
it, but then my middleware then is assembled, if you will, based only on the operators that

561
00:43:09,700 --> 00:43:12,300
that network requires for that use case, right?

562
00:43:12,300 --> 00:43:17,700
And so I end up with a very compact, but still high performance experience, right?

563
00:43:17,700 --> 00:43:22,700
So these are other areas in which we're making investments and working with the ecosystem

564
00:43:22,700 --> 00:43:27,700
to innovate, and you can expect, over the course of time, for us to provide tooling that

565
00:43:27,700 --> 00:43:33,620
will allow our customers to essentially compile their networks into libraries that are

566
00:43:33,620 --> 00:43:37,180
sort of custom built for their use case.

567
00:43:37,180 --> 00:43:43,860
Do you envision a future where maybe, you know, the answer is that this has been happening

568
00:43:43,860 --> 00:43:54,620
for years, but as opposed to the step beyond kind of compiling down to standard chipsets

569
00:43:54,620 --> 00:44:02,860
or SOCs might be compiling kind of the neural network down to some kind of HGL that can

570
00:44:02,860 --> 00:44:10,780
be implemented via FPGA or some other kind of hardware description that can then be produced

571
00:44:10,780 --> 00:44:17,780
so that the Silicon itself is optimized for these super cost-sensitive high-volume applications

572
00:44:17,780 --> 00:44:19,340
that you described earlier?

573
00:44:19,340 --> 00:44:20,340
Yeah, sure.

574
00:44:20,340 --> 00:44:29,060
I think, so I got a good answer for you, which is your listeners over to the TVM stack

575
00:44:29,060 --> 00:44:30,260
from University of Washington.

576
00:44:30,260 --> 00:44:33,540
It's now a patchy project, so you can find it there.

577
00:44:33,540 --> 00:44:38,140
The idea there is, in short, there's a lot of really good innovation happening there,

578
00:44:38,140 --> 00:44:43,900
but one of their outputs is, for example, a capacity to generate essentially custom hardware

579
00:44:43,900 --> 00:44:48,460
for the TVM program, if you will, that it's compiling.

580
00:44:48,460 --> 00:44:56,140
So that is already starting to happen, and people like Microsoft today already deploy FPGAs

581
00:44:56,140 --> 00:45:01,180
into their data centers, I don't know exactly what models they run, but targeting specific

582
00:45:01,180 --> 00:45:02,180
models.

583
00:45:02,180 --> 00:45:06,740
Again, so it's happening, and I think we can well expect for certain use cases for

584
00:45:06,740 --> 00:45:08,260
this to continue to happen.

585
00:45:08,260 --> 00:45:15,140
The trade-off, of course, is that you're making a strong commitment to essentially do a

586
00:45:15,140 --> 00:45:16,140
piece of hardware.

587
00:45:16,140 --> 00:45:21,820
Now, an FPGA gives you flexibility, but you're really committing yourself to a particular

588
00:45:21,820 --> 00:45:27,460
instantiation of a network in a market that is highly dynamic, right?

589
00:45:27,460 --> 00:45:28,460
Yeah.

590
00:45:28,460 --> 00:45:29,460
People are thinking that it works all the time.

591
00:45:29,460 --> 00:45:31,300
So that's the trade-off, right?

592
00:45:31,300 --> 00:45:35,220
And so that's the balance we're trying to strike is designing, for example, our tensor

593
00:45:35,220 --> 00:45:37,900
accelerator hardware.

594
00:45:37,900 --> 00:45:41,060
We've got a first generation of the marketplace now, and you can well expect it to continue

595
00:45:41,060 --> 00:45:46,580
to evolve, where we're trying to find that point where we provide very high performance

596
00:45:46,580 --> 00:45:49,820
experiences and the right amount of flexibility.

597
00:45:49,820 --> 00:45:52,820
So going back to what I was saying earlier, OK, I'm an app developer.

598
00:45:52,820 --> 00:45:55,660
I want maximum flexibility on my platform.

599
00:45:55,660 --> 00:46:01,500
I want to run any neural network that has hundreds of operators in it.

600
00:46:01,500 --> 00:46:02,900
OK, that's one use case.

601
00:46:02,900 --> 00:46:06,540
And I want to take my same basic design and workflow.

602
00:46:06,540 --> 00:46:11,980
And I want to design a home speaker, let's say, or a thermostat or whatever.

603
00:46:11,980 --> 00:46:14,620
And I got a much more limited set of use cases.

604
00:46:14,620 --> 00:46:18,580
But I also have more limited computer, more limited memory.

605
00:46:18,580 --> 00:46:19,580
Can we serve both?

606
00:46:19,580 --> 00:46:26,300
And so the compiler, the distiller, will serve the IoT, or embedded, or purpose-built

607
00:46:26,300 --> 00:46:28,140
device use case.

608
00:46:28,140 --> 00:46:32,740
And the same accelerator building blocks that we're building and the same accelerator hardware

609
00:46:32,740 --> 00:46:38,060
will also be available for your smartphone, for your automobile, whatever, where it's not

610
00:46:38,060 --> 00:46:41,580
so clear that the exact use case is kind of boiled down.

611
00:46:41,580 --> 00:46:46,940
And so instead of focusing on, say, FPGA's in a home speaker, I think the direction will

612
00:46:46,940 --> 00:46:51,660
be how do we use tiny ML techniques, how do we use compression, how do we innovate in

613
00:46:51,660 --> 00:46:58,700
the hardware so that we can provide this high performance experience, and a design sort

614
00:46:58,700 --> 00:47:05,020
of approach that meets the sort of memory, power, constraints, cost point for these use

615
00:47:05,020 --> 00:47:06,020
cases?

616
00:47:06,020 --> 00:47:10,940
Well, more thing that I wanted to be sure to cover, and it's related to everything that

617
00:47:10,940 --> 00:47:15,620
we've been talking about, Qualcomm's been pretty active in the Onyx community.

618
00:47:15,620 --> 00:47:22,300
How does that play with all of the other ecosystems that we've been discussing?

619
00:47:22,300 --> 00:47:27,340
And we can take a step back and describe for folks that don't know it Onyx, and maybe

620
00:47:27,340 --> 00:47:30,260
kind of an update on what's been going on with it.

621
00:47:30,260 --> 00:47:31,260
That's good.

622
00:47:31,260 --> 00:47:32,260
That's a good foundation.

623
00:47:32,260 --> 00:47:39,460
So Onyx, I don't know, started maybe roughly a year and a half ago, something like that,

624
00:47:39,460 --> 00:47:45,300
set forth by Microsoft, Facebook, and Amazon, and we were asked at the very early stages

625
00:47:45,300 --> 00:47:51,460
to join, and we were, I think the first mobile SSC vendor to join, and the first mobile

626
00:47:51,460 --> 00:47:59,300
SSC vendor to make Onyx converters into our accelerator framework shipping product.

627
00:47:59,300 --> 00:48:06,180
So, real deep sort of interaction with Onyx, but what is the sort of idea?

628
00:48:06,180 --> 00:48:10,260
The idea was to provide an open standard, if you will.

629
00:48:10,260 --> 00:48:14,980
It's an open source project, interchange format, so that going back to your question about

630
00:48:14,980 --> 00:48:20,140
what do we recommend to our customers in terms of how to bring inference to devices?

631
00:48:20,140 --> 00:48:27,060
Well, again, going back maybe 12 or 18 months, the issue was that all these training frameworks

632
00:48:27,060 --> 00:48:31,460
had different paradigms, different file formats, different ways of interchanging data.

633
00:48:31,460 --> 00:48:36,940
So Onyx's kind of first kind of principal idea was, okay, let's define an interchange format,

634
00:48:36,940 --> 00:48:43,700
so I can move easily from PyTorch to Cafe, or PyTorch to TensorFlow, and so on.

635
00:48:43,700 --> 00:48:48,300
And so by and large, I think it's achieved that translation between these frameworks that

636
00:48:48,300 --> 00:48:52,740
have different sets of operators and different assumptions is always a bit tricky.

637
00:48:52,740 --> 00:48:58,340
But we do have customers that come to us and want to run Onyx produced models.

638
00:48:58,340 --> 00:49:02,340
And the nice thing is that it's helped with this funnel I talked about earlier.

639
00:49:02,340 --> 00:49:07,140
A lot of frameworks, chain, or others, have adopted Onyx support.

640
00:49:07,140 --> 00:49:13,500
And so now the fact that we support Onyx as one of the on-ramps to our SOCs means that

641
00:49:13,500 --> 00:49:18,820
we've actually opened up the number of training frameworks our customers can use.

642
00:49:18,820 --> 00:49:22,940
So long as they can come through that Onyx kind of gateway.

643
00:49:22,940 --> 00:49:32,860
And so we're seeing quite a bit of interest in using that as an on-ramp to our products.

644
00:49:32,860 --> 00:49:35,860
And we're also chairing the Onyx Edge working group.

645
00:49:35,860 --> 00:49:42,620
And so the idea there is to define in broad strokes what this goes also back to the data

646
00:49:42,620 --> 00:49:43,620
center discussion.

647
00:49:43,620 --> 00:49:49,580
We had the set of operators you want in a data center generally a lot richer than the

648
00:49:49,580 --> 00:49:53,940
set of operators that you need in practice on an edge device, whether that's a mobile

649
00:49:53,940 --> 00:49:55,420
device or IoT device.

650
00:49:55,420 --> 00:50:02,820
And so part of the work of the edge working group is to define sort of a subset, a strict

651
00:50:02,820 --> 00:50:09,300
and defined subset of the full set of Onyx operators that we define as sort of a conformance

652
00:50:09,300 --> 00:50:10,540
set for edge.

653
00:50:10,540 --> 00:50:14,980
So if you said I'm Onyx Edge 1.0 although we don't really coin this term but think about

654
00:50:14,980 --> 00:50:21,060
it that way, compliant, then I support these 65 operators and they work in this way and

655
00:50:21,060 --> 00:50:23,860
we can write a sort of compliance test around it, right?

656
00:50:23,860 --> 00:50:29,620
And then interoperability becomes a lot more meaningful because today one of the issues

657
00:50:29,620 --> 00:50:34,940
when somebody says that their Onyx compliant is okay which of the 130 plus operators

658
00:50:34,940 --> 00:50:40,380
whatever the number is today, do you actually support and is that combination meaningful

659
00:50:40,380 --> 00:50:43,940
for use cases that you want to deploy to the edge?

660
00:50:43,940 --> 00:50:44,940
Right.

661
00:50:44,940 --> 00:50:45,940
So that's our involvement.

662
00:50:45,940 --> 00:50:53,740
Again, the big theme here is we want to be able to talk about what does compliance mean,

663
00:50:53,740 --> 00:50:58,260
what does it mean to support AI at the edge?

664
00:50:58,260 --> 00:51:02,900
How do you make it reduce the friction for our customers to do that, right?

665
00:51:02,900 --> 00:51:08,420
We've covered a lot of ground, a lot of very fun digressions, I would say.

666
00:51:08,420 --> 00:51:10,900
Anything else that you wanted to make sure we covered?

667
00:51:10,900 --> 00:51:16,780
Look, I just appreciate the opportunity to be on the show and to on your podcast and

668
00:51:16,780 --> 00:51:20,340
to reach out to your audience.

669
00:51:20,340 --> 00:51:28,340
I'm really excited about where things are going, a bit of a pitch I think maybe for the

670
00:51:28,340 --> 00:51:32,580
company and proud of the company and what we do.

671
00:51:32,580 --> 00:51:38,380
This is a big systems problem, AI, I view it as a real paradigm shift in computing

672
00:51:38,380 --> 00:51:43,500
and it's a big systems problem, but that I mean that I don't think you can just look

673
00:51:43,500 --> 00:51:51,020
at just the edge or just a piece of silicon or just a piece of middleware and deduce

674
00:51:51,020 --> 00:51:54,300
from that, what are the implications across the whole thing?

675
00:51:54,300 --> 00:52:00,260
This is a pervasive kind of technology shift we're undergoing and it's I think fundamentally

676
00:52:00,260 --> 00:52:05,420
going to change how we build computing systems and how we build all the systems that

677
00:52:05,420 --> 00:52:09,620
were in all automated devices that exist today and then all the ones that are going to

678
00:52:09,620 --> 00:52:11,220
be invented.

679
00:52:11,220 --> 00:52:17,140
When we think about this, when your listeners interact with your podcast, I'd encourage

680
00:52:17,140 --> 00:52:22,380
them to be thinking about you're going to have various experts and those experts rightfully

681
00:52:22,380 --> 00:52:25,660
so focus on a slice of this.

682
00:52:25,660 --> 00:52:30,420
When we think about in total an end to end system, I think it's really important if

683
00:52:30,420 --> 00:52:35,780
we're going to like our focus is on high performance, low power, let's say to put it in a sound

684
00:52:35,780 --> 00:52:41,620
bite, that requires huge amounts of innovation in the algorithms and architecture of networks

685
00:52:41,620 --> 00:52:46,900
in the hardware, in the middleware like we talked about, that's a big problem and that's

686
00:52:46,900 --> 00:52:51,740
basically the problem that we're trying to tackle that I'm deeply involved in and excited

687
00:52:51,740 --> 00:52:57,340
to see where this goes, takes a lot of people, we have to work with our ecosystem partners

688
00:52:57,340 --> 00:53:02,300
to make it happen, but really excited at the rate of progress we're seeing and just really

689
00:53:02,300 --> 00:53:06,780
excited about what's going to come as we enable more of these capabilities for people to

690
00:53:06,780 --> 00:53:07,780
innovate on.

691
00:53:07,780 --> 00:53:10,540
Well Jeff, thanks so much for being on the show.

692
00:53:10,540 --> 00:53:15,180
Oh, absolutely, thank you for the invitation, should do it again in some period of time

693
00:53:15,180 --> 00:53:20,500
and we can reflect back on what are these predictions came true and which ones fell flat?

694
00:53:20,500 --> 00:53:22,220
Absolutely, absolutely.

695
00:53:22,220 --> 00:53:23,220
Thanks so much.

696
00:53:23,220 --> 00:53:28,860
Okay, thank you, you have a good day.

697
00:53:28,860 --> 00:53:31,380
Alright everyone, that's our show for today.

698
00:53:31,380 --> 00:53:35,220
If you like what you've heard here, please do us a favor and tell your friends about the

699
00:53:35,220 --> 00:53:36,380
show.

700
00:53:36,380 --> 00:53:40,540
And if you haven't already hit that subscribe button yourself, make sure you do, so

701
00:53:40,540 --> 00:53:44,580
you don't miss any of the great episodes we've got in store for you.

702
00:53:44,580 --> 00:53:48,620
Thanks again to Qualcomm for their sponsorship of today's episode.

703
00:53:48,620 --> 00:53:53,200
Check them out at twomla.com slash Qualcomm.

704
00:53:53,200 --> 00:53:56,580
As always, thanks so much for listening and catch you next time.

