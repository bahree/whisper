1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,120
I'm your host Sam Charrington.

4
00:00:34,120 --> 00:00:39,920
As we approach Twimblecon AI platforms, I'd like to let you all in on our first major announcement

5
00:00:39,920 --> 00:00:41,760
from the conference.

6
00:00:41,760 --> 00:00:46,600
Now you all love this podcast for great guests and interviews and we're bringing that concept

7
00:00:46,600 --> 00:00:48,800
right to the Twimblecon stage.

8
00:00:48,800 --> 00:00:54,760
I am super excited to announce that Andrew Eng will be joining me on stage at Twimblecon

9
00:00:54,760 --> 00:00:57,720
for a live keynote interview.

10
00:00:57,720 --> 00:01:02,320
Many of you know Andrew from his work at Stanford, Coursera or his many other efforts in the

11
00:01:02,320 --> 00:01:06,640
industry including recently founding deeplearning.ai.

12
00:01:06,640 --> 00:01:11,520
Andrew and his work have been super impactful on my life and career and I know that's the

13
00:01:11,520 --> 00:01:13,920
case for many of you as well.

14
00:01:13,920 --> 00:01:18,880
In our conversation we'll be discussing the state of AI in the enterprise, the barriers

15
00:01:18,880 --> 00:01:24,000
to using deep learning and production and how to overcome them, his views on tooling and

16
00:01:24,000 --> 00:01:30,640
platforms for efficient AI delivery and other topics from his recently published AI Transformation

17
00:01:30,640 --> 00:01:32,440
Playbook.

18
00:01:32,440 --> 00:01:36,920
Be on the lookout for more great speaker announcements rolling out over the course of the next few

19
00:01:36,920 --> 00:01:37,920
weeks.

20
00:01:37,920 --> 00:01:39,920
You don't want to miss this event.

21
00:01:39,920 --> 00:01:47,440
Get your tickets now at twimblecon.com slash register.

22
00:01:47,440 --> 00:01:49,520
Alright everyone, I am on the line with Zach Lipton.

23
00:01:49,520 --> 00:01:54,680
Zach is an assistant professor in a temperate school of business and an affiliate faculty

24
00:01:54,680 --> 00:01:59,320
in the machine learning department and high school of public policy at CMU.

25
00:01:59,320 --> 00:02:01,760
Zach, welcome to this week a machine learning in AI.

26
00:02:01,760 --> 00:02:03,480
Thanks for having me.

27
00:02:03,480 --> 00:02:06,680
Let's get started by having you share a little bit about your background.

28
00:02:06,680 --> 00:02:15,400
How did you end up in this intersection of business, machine learning, public policy?

29
00:02:15,400 --> 00:02:16,800
Lots of different areas.

30
00:02:16,800 --> 00:02:19,640
That's a good question.

31
00:02:19,640 --> 00:02:26,480
I'm not sure completely, but I can't really be completely honest.

32
00:02:26,480 --> 00:02:28,240
I can't claim it was planned or anything.

33
00:02:28,240 --> 00:02:33,240
I think if anything maybe it came from sort of just doing what I want to do at every

34
00:02:33,240 --> 00:02:41,960
given point and not following a very specific path that was laid out and as a result wound

35
00:02:41,960 --> 00:02:46,280
up maybe some plays different than the standard thing.

36
00:02:46,280 --> 00:02:54,040
You recently transitioned into your professorship at CMU from your PhD.

37
00:02:54,040 --> 00:02:55,360
What was that in?

38
00:02:55,360 --> 00:02:57,320
So I did my PhD in computer science.

39
00:02:57,320 --> 00:03:06,720
I kind of had a circuitous path to PhD and out of PhD, but I did my undergraduate in

40
00:03:06,720 --> 00:03:08,800
math and economics at Columbia.

41
00:03:08,800 --> 00:03:14,960
That was a long time ago and I was a musician sort of before, during and after and that was

42
00:03:14,960 --> 00:03:15,960
my main thing.

43
00:03:15,960 --> 00:03:22,040
I was playing this saxophone and I wanted to be a hustling jazz musician which is a hard

44
00:03:22,040 --> 00:03:30,480
life even if you're the very best and luckiest, but certainly if you're not the luckiest.

45
00:03:30,480 --> 00:03:35,080
So I was doing that for a long time after I think I was happiest playing music when I was

46
00:03:35,080 --> 00:03:43,680
an undergrad and there was this kind of amazing balance about being at Columbia and having

47
00:03:43,680 --> 00:03:48,880
the academic side of my life be a little bit more on the technical side which felt more

48
00:03:48,880 --> 00:03:52,400
natural for me at least within academic environment.

49
00:03:52,400 --> 00:03:58,520
I enjoyed jazz music as sort of a folk music and there's something very organic about learning

50
00:03:58,520 --> 00:04:02,520
that music as an oral tradition and learning it by spending your nights out until I am

51
00:04:02,520 --> 00:04:08,160
playing it with people who speak that music or who play that music kind of natively.

52
00:04:08,160 --> 00:04:13,080
And there was something that I never quite loved about taking classes in jazz music

53
00:04:13,080 --> 00:04:17,040
like in a university or conservatory setting and it always felt a little bit artificial.

54
00:04:17,040 --> 00:04:22,480
I think maybe other other art forms might be more amenable to that.

55
00:04:22,480 --> 00:04:28,280
And so I guess after spending a certain amount of time playing music being outside I had

56
00:04:28,280 --> 00:04:33,000
some like personal setbacks so I was like hemorrhaging money and living in the lower

57
00:04:33,000 --> 00:04:37,680
east side and the only way I was able to live in the lower east side is I was in a rent

58
00:04:37,680 --> 00:04:43,840
control department and usually that means that unfortunately in New York was kind of neglected

59
00:04:43,840 --> 00:04:48,720
by the landlords and so it was like a moldy apartment as people vomiting on your sidewalk

60
00:04:48,720 --> 00:04:52,840
and all that and then I went out and visited a friend who was actually a musician who

61
00:04:52,840 --> 00:04:58,040
was doing a PhD in music composition so he was a jazz musician but he had he was an amazing

62
00:04:58,040 --> 00:05:05,160
pianist and has you know the kind of rounded chops that he could also make it in a composition

63
00:05:05,160 --> 00:05:06,160
program.

64
00:05:06,160 --> 00:05:10,320
He came out and started a graduate program at UC Santa Cruz so I came out and visited

65
00:05:10,320 --> 00:05:17,400
him and I didn't want to go to grad school for music but that experience of being out there

66
00:05:17,400 --> 00:05:25,000
and like after just kind of being in this sort of falling apart type state in New York

67
00:05:25,000 --> 00:05:32,480
city in this rundown apartment and feeling unhealthy and everything then I was out in Santa

68
00:05:32,480 --> 00:05:37,280
Cruz and Santa Cruz like the sun is shining it's the most beautiful place in the world.

69
00:05:37,280 --> 00:05:45,160
The 90 year olds, the 90 year olds look 30 years old and then they you know that has

70
00:05:45,160 --> 00:05:49,720
group of composers would get together every week and they had these listening sessions

71
00:05:49,720 --> 00:05:54,080
and it was almost like a reading group you know like they would like it was very different

72
00:05:54,080 --> 00:05:58,680
than maybe like my experience in music which is a being a social music was hard to have

73
00:05:58,680 --> 00:06:02,840
like a kind of critical intellectual discourse about it on the level because you sort of

74
00:06:02,840 --> 00:06:07,280
were it was very personal to people and you were trying to get gigs working with people

75
00:06:07,280 --> 00:06:13,840
and I thought there was this missing part of my life that you know it's like very academic

76
00:06:13,840 --> 00:06:19,360
slash New York thing and me that just wants to like have very like candid arguments with

77
00:06:19,360 --> 00:06:24,760
people about things and I was I was in Santa Cruz and it was just beautiful and I was amazing

78
00:06:24,760 --> 00:06:29,160
and these composers would get together once a week and someone would curate and they play

79
00:06:29,160 --> 00:06:32,560
a bunch of music and then people would just scream at each other about it and like really you

80
00:06:32,560 --> 00:06:37,880
know not not in an ad home in a way but just really you know express really really strong

81
00:06:37,880 --> 00:06:42,880
kind of critical opinions about it and there was something about this this environment that

82
00:06:42,880 --> 00:06:47,640
made me think you know like that oh that's it like it wasn't that I want to go to

83
00:06:47,640 --> 00:06:50,280
grad school for music but it was like oh I'm going to move to California I'm going to do

84
00:06:50,280 --> 00:06:55,000
a PhD I'm going to get into that kind of environment where I feel like I have that kind of intellectual

85
00:06:55,000 --> 00:07:02,480
life and so I went back home and I like got my landlord degree to let me break my lease

86
00:07:02,480 --> 00:07:10,160
early I asked my parents if I could steal our old like 2004 Toyota Corolla I took the sign

87
00:07:10,160 --> 00:07:14,560
up take the GREs and then I was like I'm going to go do a PhD and I haven't even decided

88
00:07:14,560 --> 00:07:19,080
like what the PhD was going to be in so then like the next step was like well what am I going

89
00:07:19,080 --> 00:07:24,120
to do and actually came together as whole like really just kind of like ridiculous you

90
00:07:24,120 --> 00:07:28,920
know like unqualified plan to do PhD came together extremely fast like two to three weeks

91
00:07:28,920 --> 00:07:34,840
it was just kind of all set up it just kind of I don't know why I didn't even know what

92
00:07:34,840 --> 00:07:39,560
machine learning really was but I knew I wanted to do PhD and I went I had a good friend who

93
00:07:39,560 --> 00:07:44,920
was sort of had been a you know a bit of a mentor for me who was a biophysics professor

94
00:07:44,920 --> 00:07:49,400
and I used to I built him I taught myself to program a little bit and I built him a website

95
00:07:49,400 --> 00:07:53,960
and so we would hang out and play chess sometimes and drink coffee and he's to invite me to

96
00:07:53,960 --> 00:07:57,400
the reading group so I had a little bit of a sense of what academic life was like even though

97
00:07:57,400 --> 00:08:02,600
I I don't really know biology or physics and he was just always for some reason he took

98
00:08:02,600 --> 00:08:06,440
an interest in me and we were we were close and so I talked to him I was like you know maybe

99
00:08:06,440 --> 00:08:12,200
I should do PhD in biology and he's like you know that's not fun you know it's like that's

100
00:08:12,200 --> 00:08:17,160
a top heavy basically it was like that's you know it's a great job and it's an amazing field

101
00:08:17,160 --> 00:08:22,840
if you're on top but he's like you know it's going to take you six years to figure out how to

102
00:08:22,840 --> 00:08:27,480
be a useful lab tech before you can even start doing anything creative and by that time you're

103
00:08:27,480 --> 00:08:31,400
going to be you know you know on your way to 40 years old like that's that's not the route

104
00:08:32,520 --> 00:08:37,080
and I thought about some more it's like what do I taught you know I've taken one only one or two

105
00:08:37,080 --> 00:08:42,520
undergraduate computer science classes but somehow that felt like the right thing like there was

106
00:08:42,520 --> 00:08:46,600
something just like really I think I think a lot of people had the first time you learn computer

107
00:08:46,600 --> 00:08:51,480
science you learn about algorithms and you suddenly start learning how to think about how to

108
00:08:51,480 --> 00:08:55,160
start formalizing things you see in the world where how would you model that computationally

109
00:08:55,160 --> 00:08:59,240
and then you start thinking about things like the structure in real world problems that

110
00:08:59,240 --> 00:09:02,760
makes them amenable to a efficient algorithm and something about that like kind of clicked with

111
00:09:02,760 --> 00:09:07,240
me when I was young so even if I hadn't sort of followed up on it just having these like two or

112
00:09:07,240 --> 00:09:11,240
three undergraduate computer science classes made this impression on me and I knew how to program

113
00:09:11,240 --> 00:09:17,720
just enough to cause trouble and so thought about it well you know that's that's the thing I

114
00:09:17,720 --> 00:09:22,840
could maybe sort of run with you know but that was you know it was a very thin thin basis to hang

115
00:09:22,840 --> 00:09:28,760
my hat on but fortunately fortunately when I applied to PhD there are some people who were

116
00:09:28,760 --> 00:09:34,760
willing to take a chance on me and and one of them was UC San Diego which is a absolutely fantastic

117
00:09:34,760 --> 00:09:39,960
school that you know everyone in the world should apply to if you're interested in not if you're

118
00:09:39,960 --> 00:09:46,680
interested in computer science or surfing or beautiful weather just building a new life

119
00:09:46,680 --> 00:09:51,640
sounds like you pattern match Santa Cruz pretty well except for maybe I don't know eight degrees

120
00:09:51,640 --> 00:09:57,640
warmer on average or something yeah it's a bit warmer it's you know they both I think have the year

121
00:09:57,640 --> 00:10:03,080
round moderate weather thing right kind of happened to both have the surfing I think Santa Cruz is

122
00:10:03,080 --> 00:10:10,200
a bit happier you know San Diego has multiple sides like you've got the La Jolla kind of stuffy

123
00:10:10,200 --> 00:10:17,000
Mitch Romney kind of side and then you've got the I got a lot Navy presence there so that that's a

124
00:10:17,000 --> 00:10:25,080
big player in San Diego but yeah you've got that that beautiful year round weather and that

125
00:10:25,080 --> 00:10:30,600
attracts something cool great food also and so did you just jump right in take some courses

126
00:10:30,600 --> 00:10:36,680
find your way to machine learning or what was that initial connection to the the world of ML

127
00:10:36,680 --> 00:10:43,560
well ML was just the thing from the start like that was my I didn't know any ML like literally

128
00:10:43,560 --> 00:10:48,680
if you told me to like write out you know explain to you on the whiteboard like a classic algorithm

129
00:10:48,680 --> 00:10:54,280
like logistic regression or something I would not have known I would not have been able to do it

130
00:10:54,280 --> 00:10:59,400
or you know explain gradient descent like I hadn't I hadn't ever implemented a machine learning

131
00:10:59,400 --> 00:11:07,160
algorithm but what I knew was that I spent one year in San Francisco 2012 so like when I made

132
00:11:07,160 --> 00:11:12,040
this plan you know that there's one problem with the whole grad school pipeline which is that

133
00:11:12,040 --> 00:11:17,400
it's a great pipeline or you know maybe maybe people have some fault with it or something but

134
00:11:17,400 --> 00:11:21,960
overall like it works well if you're already in the system so like if you're an undergrad and

135
00:11:21,960 --> 00:11:25,880
then you want you you know you want to go to masters you're like well you know I'm a junior now

136
00:11:25,880 --> 00:11:30,440
I got to start thinking about that and and and you do it and you're not like off the path

137
00:11:30,440 --> 00:11:36,280
while you're making that plan right but the the trick is if you're doing something completely

138
00:11:36,280 --> 00:11:43,160
different you know if you're like yo I'm I'm playing a saxophone at three in the morning for 40 bucks

139
00:11:43,160 --> 00:11:49,720
in some weird dive that's gonna close down it a couple months and I want to go do a PhD

140
00:11:49,720 --> 00:11:55,800
then it's like you're you're out of the system like what are you gonna so so so this was like

141
00:11:55,800 --> 00:12:03,400
spring two thousand like end of spring 2012 is what you need that's that's like your last moment

142
00:12:03,400 --> 00:12:09,400
to make a move like a serious like hard hard turn even if you play everything right to like

143
00:12:09,400 --> 00:12:14,760
get in the PhD for fall 2013 right so you have to kind of not just like have this thing come

144
00:12:14,760 --> 00:12:21,240
together but you then have to somehow stick with it despite not being like in that rhythm for

145
00:12:21,240 --> 00:12:25,880
some period of time so so so my move is basically I knew that if I stayed in New York I would just

146
00:12:25,880 --> 00:12:31,320
keep doing what I was doing so my move is to like break my lease sign up for the GREs get that part

147
00:12:31,320 --> 00:12:35,640
out of the way because that was the part that I knew about and then move to California and you

148
00:12:35,640 --> 00:12:42,120
know I was like well you know I'll I'll basically go to San Francisco maybe find a way to be uh

149
00:12:42,120 --> 00:12:49,000
be a lemming for a startup or something until um you know while while I'm putting that together

150
00:12:49,000 --> 00:12:52,680
at least I wouldn't be like in New York still hanging out till five in the morning like trying

151
00:12:52,680 --> 00:12:58,840
to hustle for gigs because I'd be I'd be out of you know and it just also felt natural like change

152
00:12:58,840 --> 00:13:03,320
change your life change your location like to wake up every morning in a new place so I moved to

153
00:13:03,320 --> 00:13:06,680
San Francisco actually I did everything where you know it was a really wild time like I moved I

154
00:13:06,680 --> 00:13:11,320
lived in one of these like totally could not possibly have been legal they call like hacker hostels

155
00:13:11,320 --> 00:13:16,440
or something oh yeah like I saw some I saw I have this like problem which is maybe part of why

156
00:13:16,440 --> 00:13:22,200
I have wound up in all kinds of weird situations but if you can cast like a situation as a choice

157
00:13:22,200 --> 00:13:26,920
to have an adventure or not have an adventure and that's like a valid lens on the situation

158
00:13:26,920 --> 00:13:32,440
then I'll choose the adventure and so and so I read and you know I was thinking like San Francisco

159
00:13:32,440 --> 00:13:36,920
like you know whatever Silicon Valley whatever whatever you know it didn't it was still a little

160
00:13:36,920 --> 00:13:41,720
more romantic at the time I think in 2012 it wasn't quite as like evil empires it is now but it

161
00:13:41,720 --> 00:13:46,680
was already pretty expensive and there was these articles in New York times about these weird

162
00:13:46,680 --> 00:13:50,440
hacker hostels we were like people would wrench them through Airbnb for like a month at a time

163
00:13:50,440 --> 00:13:55,480
and they were like packed in and so I went out and I lived like literally in a bunk bed with

164
00:13:56,680 --> 00:14:03,160
six people for a month until trying to make it big and Silicon Valley and yeah just trying

165
00:14:03,160 --> 00:14:07,560
uh yeah I don't know what it was just just found find out what it was I didn't know anyone out there

166
00:14:07,560 --> 00:14:13,480
so I really saw me some it was all like Germans met like a bunch of Germans who were hanging out

167
00:14:13,480 --> 00:14:20,200
for a minute yeah they love their hostels yeah so I moved to California bite the California coast

168
00:14:20,200 --> 00:14:25,080
and then set up in San Francisco and lived in this weird Airbnb for a minute and then I moved out

169
00:14:25,080 --> 00:14:29,880
to Oakland and that was great I actually ended up playing a lot of jazz again when I was in Oakland

170
00:14:29,880 --> 00:14:36,280
and getting to know that community and worked with the startup and then you know spent a significant

171
00:14:36,280 --> 00:14:40,600
portion of my time then I'm applying a PhD and I got lucky that someone took a chance on me

172
00:14:41,320 --> 00:14:49,960
nice nice and so you applied to ML programs you know not to be all CMU showblinist or something

173
00:14:49,960 --> 00:14:58,440
but I think CMU's unusual in having an ML program and was in the past was exceedingly unusual

174
00:14:58,440 --> 00:15:03,880
and having like a like an ML department like so I applied I applied to computer science and you

175
00:15:03,880 --> 00:15:08,200
know you check off maybe some interest areas or something but it's not like a a separate program

176
00:15:08,200 --> 00:15:12,600
you applied to computer science I think now things have gotten weird enough that you look at a

177
00:15:12,600 --> 00:15:18,040
a typical school has a has a typical university as a school of engineering within a department

178
00:15:18,040 --> 00:15:23,160
of computer science and within that some subgroup of people some working group that works on

179
00:15:23,160 --> 00:15:27,960
machine learning not as like a formal distinction although maybe there's some kind of committees or

180
00:15:27,960 --> 00:15:34,280
they band together for making hiring decisions or something but you know that's how CMU is very

181
00:15:34,280 --> 00:15:40,200
unusual a CMU has a school of computer science and within it a department of machine learning

182
00:15:40,200 --> 00:15:44,200
a department of robotics the department of a natural language you know it's called language

183
00:15:44,200 --> 00:15:49,480
technologies institute department of human computer action and stuff like that so you know at

184
00:15:49,480 --> 00:15:53,640
the time of applying most of the place you apply you just apply to CS and maybe lists or

185
00:15:53,640 --> 00:15:58,200
interest I think now things are getting weird because I think more places are copying the CMU model

186
00:15:58,200 --> 00:16:04,360
as places try to keep up with the band especially for like courses and in AI machine learning

187
00:16:05,800 --> 00:16:10,520
I think the other thing that's happening is just a lot of schools get I've read from colleagues

188
00:16:10,520 --> 00:16:16,360
who are professors elsewhere that you know at a lot of CS departments you'll get maybe CS will

189
00:16:16,360 --> 00:16:23,400
be like six faculty out of 50 or 40 or something but maybe 50 60 percent of the applications

190
00:16:23,400 --> 00:16:28,280
for grad school for PhD are people saying they want to do machine learning right so that creates

191
00:16:28,280 --> 00:16:33,080
a whole other dynamic where maybe they end up treating it even if it's not formally a separate

192
00:16:33,080 --> 00:16:36,520
application they have to throttle it a little bit because they're thinking well who are these

193
00:16:36,520 --> 00:16:40,200
people going to work with right right right and the alternatives you accept them all you know

194
00:16:40,200 --> 00:16:44,280
and then you know the half of them get the advice that they want and the other half end up doing

195
00:16:44,280 --> 00:16:51,160
compilers or something let's maybe talk a little bit about your your broad research interest nowadays

196
00:16:51,160 --> 00:16:57,640
what are you focusing on well you know I think the lens I mean okay there's a few a few kind of

197
00:16:57,640 --> 00:17:03,400
lenses that that I have in research right there's you know a lot of people are more applied a lot of

198
00:17:03,400 --> 00:17:07,560
people are more like theoretical or core algorithms and I kind of straddle that line a little bit

199
00:17:08,360 --> 00:17:15,560
and on the applied side my my biggest interest has been since before I started PhD and has

200
00:17:15,560 --> 00:17:21,240
continued to be throughout and as a young faculty member has been working a machine learning for

201
00:17:21,240 --> 00:17:29,080
healthcare so so that's kind of you know if I just step back and think about not you know papers

202
00:17:29,720 --> 00:17:34,280
in terms of like their aesthetic beauty or something but in terms of like

203
00:17:35,880 --> 00:17:40,040
if I could you know build something big like what would the grand vision be it would be I

204
00:17:40,040 --> 00:17:47,080
would like to have a positive impact in healthcare and I think there are opportunities to do it um

205
00:17:47,080 --> 00:17:53,240
but at the same time working on problems in healthcare I think it's also a great application

206
00:17:53,240 --> 00:17:59,400
not just because uh it's you know uh much better for yourself than working on advertisements

207
00:18:00,200 --> 00:18:06,280
but also because I think it just sort of puts you in touch with what's wrong right because you

208
00:18:06,280 --> 00:18:11,080
you basically you just can't afford like it's too important and the stakes are too high

209
00:18:11,720 --> 00:18:15,560
that if you're going to if you're gonna go out there and say this is how we should do decision-making

210
00:18:15,560 --> 00:18:20,440
or something or you know we could you people there's these sensational headlines your next doctor

211
00:18:20,440 --> 00:18:26,200
might be an AI and it's such absolute crap right but the reason why is because it really you know

212
00:18:26,200 --> 00:18:30,520
if you really think deeply about these things that puts you in touch with like the discrepancy

213
00:18:30,520 --> 00:18:36,920
between the tools that we uh have mastered and that we're building and in the actual real world

214
00:18:36,920 --> 00:18:44,040
problems we're claiming to to to make some impact on right so the the one hammer we have that

215
00:18:44,040 --> 00:18:49,000
everybody uh you know it is throwing all over the place wherever you can stick it in it's called

216
00:18:49,000 --> 00:18:53,000
supervised learning I'm sure you've talked about you know machine learning I guess you know

217
00:18:53,000 --> 00:18:57,880
you've run 9000 podcasts already and everyone who was talking about an actual working system

218
00:18:57,880 --> 00:19:01,880
probably was talking about supervised learning right they're they're limited exceptions or maybe

219
00:19:01,880 --> 00:19:07,720
someone just and with banded algorithms on advertisements but for the most part you know supervised

220
00:19:07,720 --> 00:19:13,000
learning is basically based on this idea that you're gonna get data that comes in right and it's

221
00:19:13,000 --> 00:19:18,600
gonna consist of inputs and corresponding outputs and you're gonna um try to predict the outputs

222
00:19:18,600 --> 00:19:25,320
based on the inputs and uh fortunately you know what makes it supervised is that you for the purposes

223
00:19:25,320 --> 00:19:30,280
of training are gonna have as large data set for which the outputs are known so it's like someone's

224
00:19:31,000 --> 00:19:37,000
standing over your shoulder and telling you what the right answer is and now the big big assumption

225
00:19:37,000 --> 00:19:43,720
is that the the data that you're then gonna see in uh in the real world you your training data

226
00:19:43,720 --> 00:19:49,480
was representative of it right so like basically the the historical data and the future data are

227
00:19:50,120 --> 00:19:54,440
assumed to be what called iid which means they're like independently sampled from the same exact

228
00:19:54,440 --> 00:20:00,600
distribution yeah and and that's just a oh oh wild assumption right when you then think like okay

229
00:20:00,600 --> 00:20:06,680
wait a minute so so what can we do it's a we can infer a likely output given an input

230
00:20:08,280 --> 00:20:15,880
um assuming that the future is in every like statistical way sort of you know the the the

231
00:20:15,880 --> 00:20:21,640
historical data is perfectly representative of the future in every you know important statistical

232
00:20:21,640 --> 00:20:25,960
way and that's just something that completely breaks down when you look at a lot of real world

233
00:20:25,960 --> 00:20:33,080
problems right so uh one thing that happens is that just well the the historical data is not

234
00:20:33,080 --> 00:20:38,680
representative so so this is a question that you know i think uh formally we talk about it in terms

235
00:20:38,680 --> 00:20:44,200
of uh we're called distribution shift and distribution shift could be kind of benign or not benign

236
00:20:44,200 --> 00:20:49,800
but um could it be kind of organic in a sense that it could be that hey you know one's day is different

237
00:20:49,800 --> 00:20:54,680
from Tuesday because it's because it's different from Tuesday right so uh if you're classifying

238
00:20:54,680 --> 00:20:59,720
news articles are somewhat people are you know different different stories are trending uh more

239
00:20:59,720 --> 00:21:03,960
you know if you're classifying by topic or something there's more sport stories you know today

240
00:21:04,920 --> 00:21:10,600
because Wimbledon is happening or something or you know the women's world cup and uh maybe there

241
00:21:10,600 --> 00:21:15,960
will be less uh one week from now who knows so that that's one way that things change but then

242
00:21:15,960 --> 00:21:23,240
there's more in serious ways that things change which is um the other key thing is that we often

243
00:21:23,240 --> 00:21:27,400
you know the the machine learning problem the formal statements all about making predictions

244
00:21:28,040 --> 00:21:33,560
but we're often not really concerned with making predictions we're concerned with um taking actions

245
00:21:33,560 --> 00:21:36,440
right it's all about driving decisions if you're accompanying your thumb out automation and machine

246
00:21:36,440 --> 00:21:41,640
learning is coming up as this multi billion dollar concern largely because of the hope that

247
00:21:41,640 --> 00:21:46,200
you know what makes technology that valuable it's it's something that you can do at scales not

248
00:21:46,200 --> 00:21:51,800
because it's just uh people are doing offline data analysis or you know trying to understand

249
00:21:51,800 --> 00:21:56,360
their customers qualitatively because they're trying to drive decisions and once you start making

250
00:21:56,360 --> 00:22:00,520
decisions now suddenly you're impacting the world and very often that very same environment that

251
00:22:00,520 --> 00:22:05,080
generates your future data and we just don't have great tools for understanding these kind of

252
00:22:05,080 --> 00:22:10,600
feedback loops right once we once you take the data extract information from it and then use it

253
00:22:10,600 --> 00:22:15,000
to change the way that you make decisions in a way that you know influences the world everything

254
00:22:15,000 --> 00:22:21,720
kind of falls apart and so I've done a lot of work recently um trying to look at well under what

255
00:22:21,720 --> 00:22:27,560
assumptions can you make models um one that are sort of guaranteed to be robust against certain

256
00:22:27,560 --> 00:22:35,000
kinds of distribution shift to short of that you know at least under what conditions uh what tools

257
00:22:35,000 --> 00:22:40,200
can you use to try to detect as efficiently as possible as quickly as possible when somehow your

258
00:22:40,200 --> 00:22:46,360
environment has changed um and beyond that to try to sort of gain some qualitative insight into

259
00:22:46,360 --> 00:22:51,560
is that is that shift pathological or not is this something that you expect to to break your model

260
00:22:51,560 --> 00:22:57,800
or or destroy the validity of your predictions anything about a medical setting right like you're

261
00:22:57,800 --> 00:23:01,960
trying to you're trying to you know you want to have the the doctor AI your next doctor is going

262
00:23:01,960 --> 00:23:06,040
to be an AI it's like presumably they have to be able to make treatment decisions not just uh

263
00:23:06,040 --> 00:23:11,480
uh you know uh predict what would have happened if a different doctor you know if if the doctor

264
00:23:11,480 --> 00:23:16,760
who would have treated you anyway uh had done their thing so that's that's a bit of a nuance

265
00:23:16,760 --> 00:23:22,200
how does that correspond to the distribution shift and the feedback loops that you were talking about

266
00:23:22,200 --> 00:23:28,200
because I think the fundamental premise of as to your point most everything we're doing here which

267
00:23:28,200 --> 00:23:34,040
is supervised learning is you know we're going to collect this data that represents the sage wisdom

268
00:23:34,040 --> 00:23:41,320
of all the best doctors and train our models on it and so then if you know our models making the

269
00:23:41,320 --> 00:23:46,920
the decisions you know that are close to what are you know the doctor that would have otherwise

270
00:23:46,920 --> 00:23:52,120
done them and does a good job at making those decisions and everything is good and rosy right

271
00:23:52,120 --> 00:23:58,920
the problem is that well there's a number of big problems but one is you know what one is that

272
00:23:58,920 --> 00:24:04,760
the act you know the world is changing naturally right and the actual doctor is has some understanding

273
00:24:04,760 --> 00:24:09,160
of of the biology of the disease and and is somewhat adaptable in this way right like when we

274
00:24:09,160 --> 00:24:13,560
look at the ways that machine learning models break because you you move the few pixels and an image

275
00:24:14,600 --> 00:24:19,800
that kind of stuff doesn't fool the humans so the humans are pretty robust and I think actually

276
00:24:19,800 --> 00:24:24,040
this is a and this is something that um my friend Jacob Steinhart and I talked about

277
00:24:24,040 --> 00:24:30,200
in in a paper about some kind of misleading trends or some problematic trends in scholarship

278
00:24:30,680 --> 00:24:35,480
is that there's this tendency in papers to to sort of make a kind of hyperbolic claim that is

279
00:24:35,480 --> 00:24:40,440
insubstantiated by by by the research and what one of the classic ways this happens is people

280
00:24:40,440 --> 00:24:45,160
talk about human level of performance right so the human level of performance it's actually not

281
00:24:45,160 --> 00:24:50,040
it's not quite the right compare if you're going to talk about sort of human capacity the human

282
00:24:50,040 --> 00:24:58,360
capacity isn't just for doing doing well in this very very constrained sort of like artificial

283
00:24:58,360 --> 00:25:05,320
environment that only exists when you truly have a randomized trained test split the human

284
00:25:05,320 --> 00:25:10,680
dermatologist is going to continue to be a good dermatologist even if the the light contrast

285
00:25:10,680 --> 00:25:15,240
slightly changes on the images that they're looking at right or even if the skin tone of the

286
00:25:15,240 --> 00:25:20,360
patients is different than it was in the training set problems where the the machine learning

287
00:25:20,360 --> 00:25:25,960
potentially is going to fall apart in a catastrophic way the other thing is right you know

288
00:25:27,000 --> 00:25:30,840
so there's there's an issue even with matching performance because like matching performance

289
00:25:30,840 --> 00:25:35,080
you know when we talk about performance which I'm about accuracy offense like well accuracy is

290
00:25:35,080 --> 00:25:40,360
itself a statistic right accuracy is only true it's only valid assuming a certain distribution

291
00:25:40,360 --> 00:25:45,400
of data and if that's something that could change like if you know the the the patients something

292
00:25:45,400 --> 00:25:49,000
something different happens you know the the patients start coming in with a different distribution

293
00:25:49,000 --> 00:25:54,680
of illnesses some disease starts becoming you know that there's an epidemic it's not quite you

294
00:25:54,680 --> 00:26:00,840
know pit not every patient you know you have to make adjustments to what what what illnesses or

295
00:26:00,840 --> 00:26:05,640
patients likely to have given their symptoms things like that the other side is that ultimately

296
00:26:05,640 --> 00:26:11,160
what you'd like to do is you'd like to not the dream of machine learning isn't in health care

297
00:26:11,160 --> 00:26:19,880
isn't that we're going to somehow replace a bunch of doctors and in the process keep health care

298
00:26:19,880 --> 00:26:24,520
like slightly worse but almost as good right like that's what you're talking about when you're like

299
00:26:24,520 --> 00:26:28,840
if you can predict what when you talk about this imitation type learning thing so so a bigger dream

300
00:26:28,840 --> 00:26:32,760
would be that you would actually be able to assist a decision-making process and so you'd be able

301
00:26:32,760 --> 00:26:37,400
to like like ultimately the thing that gets most people excited isn't just say hey we're going to

302
00:26:37,400 --> 00:26:43,480
automate doctors which you know already maybe misses the point about just how much of the work

303
00:26:43,480 --> 00:26:49,480
is involved is is not you know taking the the data that's already there and like trying to

304
00:26:49,480 --> 00:26:53,640
predict the doctor's decision but it was actually meeting with the patient and determining which

305
00:26:53,640 --> 00:26:57,720
tests to run in the first place you know which resulted in the data which actually was already

306
00:26:57,720 --> 00:27:02,600
all the work that the machine learning's not doing for you so you know that's another way that it's

307
00:27:02,600 --> 00:27:06,440
misleading you know if we we start the machine learning and we think we're doing what the doctor

308
00:27:06,440 --> 00:27:10,280
does but it's like well what about those test results where they can come from well because someone

309
00:27:10,280 --> 00:27:15,320
who ordered those tests so what you know so then what oh we also have to predict which test to

310
00:27:15,320 --> 00:27:18,920
order and then given the test we have to predict which disease given the disease we have to predict

311
00:27:18,920 --> 00:27:23,640
you know also which treatment they're going to recommend and so you know you start if you actually

312
00:27:23,640 --> 00:27:27,880
do a kind of fair analysis that puts together all the compounded errors that pop up and then

313
00:27:27,880 --> 00:27:32,200
account for a world that's constantly changing things get messy but then even on top of that

314
00:27:32,200 --> 00:27:36,920
it's like our goal isn't to just freeze medicine at this point in time get rid of all the doctors

315
00:27:36,920 --> 00:27:41,240
and then just like put into place machine learning algorithm that is based on what medicine looks

316
00:27:41,240 --> 00:27:47,880
like in 2018 what we'd like to do is be able to understand disease processes better and be able

317
00:27:47,880 --> 00:27:53,560
to understand be able to analyze data using say for example models that don't just make predictions

318
00:27:53,560 --> 00:27:58,840
but estimate treatment effects so that's something we call causal inference and ultimately right you

319
00:27:58,840 --> 00:28:03,400
like to be able to say hey if I intervene and do something different than what the doctor normally

320
00:28:03,400 --> 00:28:08,760
would do what do I expect the outcome would be or which patients should I assign which drugs can I

321
00:28:08,760 --> 00:28:12,920
can I you know can we make a dent in personalized medicine to do that we're not just trying to make

322
00:28:12,920 --> 00:28:16,680
predictions about what people would have otherwise done we're trying to figure out what are better

323
00:28:16,680 --> 00:28:21,080
things that we could do and that requires causal inference and causal inference actually now

324
00:28:21,080 --> 00:28:26,840
gives you gives you a whole other lens on the ways that humans and computers potentially need to

325
00:28:26,840 --> 00:28:31,080
combine what they're good at because causal inference is not something that you in general can just

326
00:28:31,080 --> 00:28:38,120
do given offline data and no prior information causal inference actually requires a certain

327
00:28:38,120 --> 00:28:43,400
inductive assumption certain assumptions about the causal graph or the mechanism that relates to data

328
00:28:43,400 --> 00:28:50,040
or which things listen to which things you know for example like the you know smoking causes cancer

329
00:28:50,040 --> 00:28:55,880
potentially but cancer probably doesn't cause smoking i'd be like a classic toy you know textbook

330
00:28:55,880 --> 00:29:02,440
example but building in certain certain kind of advanced knowledge together with offline data

331
00:29:02,440 --> 00:29:07,320
you know then we're able to to potentially identify a causal effect even without you know running

332
00:29:07,320 --> 00:29:13,720
experiments in the wild but we have to you know I think ultimately you know working on a task

333
00:29:13,720 --> 00:29:18,440
like medicine really exposes and thinking about what it would take to actually do something that

334
00:29:18,440 --> 00:29:22,200
you could actually run the wild exposes you you know makes you think about those things in a way that

335
00:29:22,200 --> 00:29:27,880
I think you know maybe you should think about it if you're doing recommender systems but the truth

336
00:29:27,880 --> 00:29:32,520
is that even if you don't think about it there's going to be a large company that's willing to

337
00:29:32,520 --> 00:29:37,560
throw it out in the world and see if they make more money or lose money and if they make money is

338
00:29:37,560 --> 00:29:42,360
kind of going to roll with it even if it's kind of doing the wrong thing right so we do that a lot

339
00:29:42,360 --> 00:29:46,040
with recommender systems where it's like what is the real task we're trying to solve I don't know

340
00:29:46,040 --> 00:29:52,920
curate interesting content but what do we actually do it's like we predict clicks because that's

341
00:29:52,920 --> 00:29:57,240
a data we capture so and that's another way we that the contract breaks right I've talked about

342
00:29:57,240 --> 00:30:02,520
the way contract breaks because the distribution changes because we actually interfere in the world

343
00:30:02,520 --> 00:30:07,800
and that kind of messes with all the future data we see is now from a different world the world

344
00:30:07,800 --> 00:30:13,240
in which you know customers are interacting with this this system that that changes everything

345
00:30:13,240 --> 00:30:17,320
but another way of the contract breaks is we're just predicting the wrong thing in the first place

346
00:30:17,320 --> 00:30:20,840
because the thing we really care about is something when we don't we don't capture a structure of

347
00:30:20,840 --> 00:30:26,120
data right if you think about that like with a lot of these issues potentially that that

348
00:30:26,120 --> 00:30:31,320
make people worried and concerned about problems uh regarding say for example racial or gender bias

349
00:30:31,320 --> 00:30:35,480
in in automated systems another area that's been a lot of time thinking about and working on

350
00:30:36,440 --> 00:30:41,000
you know one of the the clear failure mechanisms is the data that you capture that your model is

351
00:30:41,000 --> 00:30:45,800
the thing that's convenient not the not the the true the true data it's not the only way that things

352
00:30:45,800 --> 00:30:49,880
can fail but that's one of them and so you can imagine that you know you predict who's going

353
00:30:49,880 --> 00:30:55,880
who to hire based on who the people hired in the past but you know that that's what you measure

354
00:30:55,880 --> 00:31:00,520
is is who got hired in the past or who how are they rated by the interviewers in the past

355
00:31:00,520 --> 00:31:05,080
well you don't capture necessarily is the thing you actually care about which is you know how

356
00:31:05,080 --> 00:31:09,080
strong a candidate are they which is a more abstract concept so we end up relying on

357
00:31:09,080 --> 00:31:13,480
instinct that we have data for which is not the same thing and you know what what is what is

358
00:31:13,480 --> 00:31:18,520
that consequence of sort of optimizing the wrong thing and you know in the case of like YouTube

359
00:31:18,520 --> 00:31:23,800
recently they had a scandal where the consequence was sort of curating pedophilia or curating

360
00:31:23,800 --> 00:31:30,120
naked baby videos for people with this you know so yeah that's uh you know I think that that's a

361
00:31:30,120 --> 00:31:35,080
especially dark or scary uh consequence but you know this is something we we have to think about

362
00:31:35,080 --> 00:31:42,600
is is when the contract breaks um you know and I think by you know that sort of forces us to sort of

363
00:31:42,600 --> 00:31:48,200
go beyond just the narrow confines of doing and evaluating supervised learning models where

364
00:31:48,200 --> 00:31:54,040
mostly the technical content consists of people um you know just sort of the get squeezing out

365
00:31:54,040 --> 00:31:59,560
incremental predictive performance improvements assuming the task is the right task and sort of

366
00:31:59,560 --> 00:32:04,680
forces us to step back and think about either a more challenging or fundamental task like

367
00:32:04,680 --> 00:32:09,400
estimating causal effects or thinking about the consequences of some of these systems like

368
00:32:09,400 --> 00:32:13,160
you know using tools to say economic modeling and and that's one thing that's cool about sitting

369
00:32:13,160 --> 00:32:17,480
in the business school and having a social scientist and economists as colleagues.

370
00:32:18,360 --> 00:32:24,280
Well it talks me about some of the economic modeling uh tools and applications of those tools

371
00:32:24,280 --> 00:32:29,160
in the space. What are some examples of how that plays out in uh healthcare and other areas?

372
00:32:29,160 --> 00:32:36,280
I think the area where I personally am encountering economics the most like I came I got hired by

373
00:32:36,280 --> 00:32:44,040
you know kind of strangely early in PhD I got approached by the the Tepper school um was made

374
00:32:44,040 --> 00:32:49,560
looking to make um you know kind of uh move move in data science direct I think I was you know

375
00:32:49,560 --> 00:32:56,760
I was a little bit of an experiment um and uh for for a little while I was a bit separated I think

376
00:32:56,760 --> 00:33:02,360
um I wasn't like reading many economics papers or something I was wrapping up you know my kind of

377
00:33:02,360 --> 00:33:09,000
core like I see a milner of type work and and actually what two things that me recently have put

378
00:33:09,000 --> 00:33:14,120
me in touch with them is one you know uh starting to think a lot more about causality you take a

379
00:33:14,120 --> 00:33:21,000
step back and say well who's who's doing empirical causal effect modeling in in the world broadly

380
00:33:21,000 --> 00:33:26,920
and it's largely social scientists and you know like applied applied uh uh economists and

381
00:33:26,920 --> 00:33:31,640
econometrations right like this is our bread and butter is um there was some shock to this is

382
00:33:31,640 --> 00:33:37,480
dealing the is it true then increasing the minimum wage uh um decreases employment or is that

383
00:33:37,480 --> 00:33:43,320
not true right and then you have these various ways of trying to draw these inferences from from

384
00:33:43,880 --> 00:33:49,720
um you know shocks of the system in the natural world or you know various you know

385
00:33:49,720 --> 00:33:53,720
they're you know they're the you know the handful of tools that that they're kind of tried

386
00:33:53,720 --> 00:33:59,560
and true like instrumental variable analysis regression discontinuity um difference and

387
00:33:59,560 --> 00:34:02,920
difference some of these tools you know rely on some very strong assumptions and I think

388
00:34:02,920 --> 00:34:07,640
can sometimes drop you know if those assumptions are violated can lead to some wrong conclusions

389
00:34:07,640 --> 00:34:12,520
but that that's one way that I've kind of started crossing over and I think you know a lot of those

390
00:34:12,520 --> 00:34:17,080
tools are important because if you look at the work modeling you know when you look at the effect of

391
00:34:17,080 --> 00:34:22,440
this technology in the real world and you you want to start saying well what is the impact of um

392
00:34:23,960 --> 00:34:28,280
you know especially when you have these fuzzy systems right like uh risk scoring systems that

393
00:34:28,280 --> 00:34:33,320
have been driving policing decisions that are then influencing these outcomes and it's very hard

394
00:34:33,320 --> 00:34:37,640
you know from the machine learning standpoint if you there's big pile of papers that are just

395
00:34:37,640 --> 00:34:41,400
considering the classification aspect right I've got these people they belong to this group

396
00:34:41,400 --> 00:34:46,920
if you live belong to that group these are the these are the their inputs these are the um ground truth

397
00:34:46,920 --> 00:34:52,040
outputs these are the predictions let me let me do some uh kind of arithmetic on them but what

398
00:34:52,040 --> 00:34:56,840
you're not seeing is that like oh this system is generating a prediction this prediction is some kind

399
00:34:56,840 --> 00:35:01,800
of score the score is an input to a human who's making a decision and if you actually care about

400
00:35:01,800 --> 00:35:06,840
the downstream problem which is how is how is the introduction of this kind of system that is

401
00:35:06,840 --> 00:35:11,880
mining this information to make predictions driving decisions and influencing outcomes then you

402
00:35:11,880 --> 00:35:16,440
start crossing over into a land where actually the people who have the experience doing this are

403
00:35:16,440 --> 00:35:22,120
the social scientists of actually um looking at real world data and trying to figure out um you know

404
00:35:22,120 --> 00:35:26,920
what what what are what are what are these various effects um I think the other side actually

405
00:35:26,920 --> 00:35:32,120
if I could jump in there that's a really interesting point because I think we often hear from

406
00:35:32,120 --> 00:35:41,800
um the kind of the vendor community that's providing tools that are enabling uh these examples

407
00:35:41,800 --> 00:35:48,200
these use cases that you're describing that hey you know we're providing these tools to uh to feed

408
00:35:48,200 --> 00:35:54,040
into a human decision process as if you know that that somehow you know means that there's not

409
00:35:54,040 --> 00:35:58,680
going to be anything wrong with the system like the overall system and also that we don't need to

410
00:35:58,680 --> 00:36:04,760
further like study and understand that overall system with you know the impact of these new

411
00:36:04,760 --> 00:36:10,920
tools in it and I think you're pointing to that well actually studying you know the impact of things

412
00:36:10,920 --> 00:36:16,120
like this is you know something that we've been doing it for a while but just in other domains

413
00:36:16,920 --> 00:36:22,040
and these are techniques that we can apply to these systems yeah absolutely and I think

414
00:36:22,040 --> 00:36:29,480
I mean there's there's a lot to say about that right all in hand I feel like the easiest thing

415
00:36:29,480 --> 00:36:35,080
to say which um maybe is not it's like intellectually profound but like I think as like a public service

416
00:36:35,080 --> 00:36:40,520
announcement needs to be out there is like first of all like no um I think it's almost a bit

417
00:36:40,520 --> 00:36:44,760
or well it's like first of all no it's not okay just because there's an algorithm involved

418
00:36:44,760 --> 00:36:49,000
but at the same time it's also not just okay just it's not okay just because there's a human

419
00:36:49,000 --> 00:36:55,720
involved right right um and there's there's a lot of issues to sort through but both in terms

420
00:36:55,720 --> 00:37:00,120
of how we analyze these systems also in terms of you know I think one of the fundamental

421
00:37:00,120 --> 00:37:05,960
difficulties in the area is also figuring out you know what is what even what is our goal

422
00:37:05,960 --> 00:37:11,400
or what is what is the right thing to do here it's not always obvious um I think there are some cases

423
00:37:11,400 --> 00:37:18,120
that to us are like very obvious that something is wrong you know but it's not always obvious what

424
00:37:18,120 --> 00:37:21,720
is the right basis for making certain kinds of decisions especially when there's certain kinds

425
00:37:21,720 --> 00:37:28,280
of like intrinsic uh trade-offs okay so so so there's question about what's the right thing to do

426
00:37:28,280 --> 00:37:35,080
the other thing the other problem is it's also okay so the big meta point there I think also

427
00:37:35,080 --> 00:37:40,120
is that hey these aren't new problems that emerge just because we stuck an algorithm in there

428
00:37:40,840 --> 00:37:45,080
um but they sort of get seen through a new light and get a new kind of attention I think a lot of

429
00:37:45,080 --> 00:37:50,440
us in the machine learning community get stuck in this loop of sort of trying to re like you know

430
00:37:50,440 --> 00:37:55,800
you see a lot of people sort of talking about like AI ethics which is great it's great that people

431
00:37:55,800 --> 00:38:03,320
are trying to think about um what is right um and uh think about these sort of like social impacts

432
00:38:03,320 --> 00:38:08,600
of applying these automated systems but at the same time a lot of people are doing it because there's

433
00:38:08,600 --> 00:38:12,520
a little bit of a bandwagoning effect going on right where a lot of people are jumping into it

434
00:38:12,520 --> 00:38:18,120
kind of completely oblivious of the fact that people have been mulling over like what what are people

435
00:38:18,120 --> 00:38:24,520
you know what what are people's rights and what um what what is like an ethical way to to engage

436
00:38:24,520 --> 00:38:29,640
in a lot of these systems even before machine learning was introduced um and I was actually I

437
00:38:29,640 --> 00:38:38,200
the benefit of uh in early June Cynthia Dwork um who's uh an absolutely inspirational researcher

438
00:38:38,200 --> 00:38:43,000
um she's the inventor of differential privacy in one of the pioneers in the more algorithmic fairness

439
00:38:43,000 --> 00:38:51,400
world and Patricia Williams who's a professor at Columbia Law um and uh they they put together this

440
00:38:51,400 --> 00:38:58,040
fantastic um workshop at the Simon's Institute for theoretical computer science and so they've

441
00:38:58,040 --> 00:39:08,040
been branching out into some more interdisciplinary type workshops and this one was uh about sort of

442
00:39:08,040 --> 00:39:16,840
uh the you know race and data and and and and injustice and brought together people from I liked

443
00:39:16,840 --> 00:39:21,480
it because you know you normally have a spectrum that brings together people who are working in um

444
00:39:22,680 --> 00:39:29,160
computer science ML and then like computer science ML adjacent like you know the the I feel like

445
00:39:29,160 --> 00:39:33,960
there's a sweet spot that like the the spectrum for interdisciplinary interdisciplinary runs from

446
00:39:33,960 --> 00:39:41,160
um like uh information school type technical social scientist to theoretical computer scientist

447
00:39:41,160 --> 00:39:45,320
and that's the spectrum which is great that's still very broad and it's a wonderful set of people

448
00:39:45,320 --> 00:39:50,760
that I consider to be like a large part of my you know my home like the people who you will

449
00:39:50,760 --> 00:39:56,040
meet if you go to like a fat star right the fairness accountability transparency conference um

450
00:39:57,400 --> 00:40:01,720
but the cool thing about this workshop is a brought together people like uh Ruha Benjamin who

451
00:40:01,720 --> 00:40:09,240
is in uh Princeton and writes um you know teaches in like African-American studies and has studied

452
00:40:09,240 --> 00:40:15,480
very clearly um issues about race and technology in ways um that transcend not just machine learning

453
00:40:15,480 --> 00:40:23,320
or this moment in time but uh more broadly um kind of history of um ways of technology has been used

454
00:40:23,320 --> 00:40:29,160
as a as a tool that maybe exacerbates inequality you had a lot of people from public health and

455
00:40:29,160 --> 00:40:34,920
epidemiology who were there you had a lot of people who had um been involved on the ethical aspects

456
00:40:34,920 --> 00:40:40,600
from like the medical ethics community and uh when when genetics was like the big hot technology

457
00:40:40,600 --> 00:40:47,880
and there were all kinds of ways that genetics was being used in ways that um had kind of um maybe like

458
00:40:48,520 --> 00:40:55,960
ways that were expressed kind of dubiously in terms of um the you know ethical consequences

459
00:40:55,960 --> 00:41:02,040
like various things various sort of studies and population genetics that were sort of sort of

460
00:41:02,040 --> 00:41:10,600
sort of like neo eugenicist type uh work and um you know there there there's been a long history

461
00:41:10,600 --> 00:41:16,440
here that is outside machine learning I think is you know where we came from this and um being

462
00:41:16,440 --> 00:41:21,240
in touch with that and and you know sometimes it's actually a surprisingly you know when you come

463
00:41:21,240 --> 00:41:25,880
at it from the perspective of like us you know people a lot of people who are who are first thinking

464
00:41:25,880 --> 00:41:32,440
about these things and haven't like learned to um are only first thinking about it and in a relatively

465
00:41:32,440 --> 00:41:42,200
immature way and go to an area um people who've been looking at say criminal justice and um thinking

466
00:41:42,200 --> 00:41:48,840
about it and in a very mature way over a very long period of time and have a really deep understanding

467
00:41:48,840 --> 00:41:55,480
of the the kind of systemic problems in a way that goes beyond just um kind of some tried formalism

468
00:41:55,480 --> 00:42:01,480
that maybe fails to capture the the kind of like institutional level issues then um I think that

469
00:42:01,480 --> 00:42:06,280
you know there's a lot to learn there and I think you know this is also you know one of the big

470
00:42:06,280 --> 00:42:11,960
things that came out of it for me from that workshop that was I think um a really kind of profound

471
00:42:11,960 --> 00:42:18,280
insight was that we have a tendency in the technical community to try to reduce things to some kind

472
00:42:18,280 --> 00:42:24,840
of abstract problem but there's a big danger that we can actually I think a lot of us even a lot

473
00:42:24,840 --> 00:42:31,560
of us sort of purporting to work on problems of algorithmic fairness but through a technical lens

474
00:42:32,360 --> 00:42:36,280
by not understanding the broader context and not understanding the kind of systemic problems like

475
00:42:36,280 --> 00:42:40,120
it's not just the matter of making the false positive race equal between two groups or something

476
00:42:40,120 --> 00:42:45,560
like this but actually understanding um you know stepping back with like what does the data even

477
00:42:45,560 --> 00:42:52,360
mean right or where was this data collected or what you know um truly asking um the kind of

478
00:42:52,360 --> 00:42:59,160
foundational questions about the broader system in which the kind of this formalism is embedded

479
00:42:59,160 --> 00:43:04,840
we run the risk of of actually doing uh what some people are calling fairwashing

480
00:43:04,840 --> 00:43:10,600
which is that we could like take these fundamentally flawed systems where maybe the the precise way

481
00:43:10,600 --> 00:43:14,760
that machine learning is being used there there's something fundamentally wrong with it

482
00:43:14,760 --> 00:43:19,480
and we could go in and say oh you know I'm I'm into machine learning I'm into social good

483
00:43:19,480 --> 00:43:24,600
let me let me do some work at that area and come up with a little tweak on the you know there's

484
00:43:24,600 --> 00:43:31,640
a danger of coming up with just a small tweak on some existing algorithm right that sort of

485
00:43:31,640 --> 00:43:38,200
preserves precisely qualitatively what we're already doing but gives somehow the impression

486
00:43:38,200 --> 00:43:44,440
that you've made it like fair right I mean that's kind of reflective of a broader argument I

487
00:43:44,440 --> 00:43:51,800
guess that's happening in the ML research community at large that the extent to which you know

488
00:43:51,800 --> 00:43:58,280
the you know these kind of revolutionary advances versus you know incremental you know with

489
00:43:58,280 --> 00:44:04,440
regard to publishing papers like you know that we're you know a lot of the papers that we're seeing

490
00:44:04,440 --> 00:44:10,040
are kind of incremental application of some new training technique or something like that and

491
00:44:10,040 --> 00:44:17,800
there is some commentary that you know we're seeing kind of this huge exponential growth in

492
00:44:17,800 --> 00:44:23,160
the number of papers that are published but some are arguing that the field isn't advancing

493
00:44:23,160 --> 00:44:30,520
you know accordingly and so this is this kind of fairwashing argument you know in one hand

494
00:44:31,240 --> 00:44:37,480
you know the fairwashing element of it is different but it's also somewhat reflective of kind

495
00:44:37,480 --> 00:44:41,240
of the broader dynamic that's happening in the research community would you agree with that?

496
00:44:42,200 --> 00:44:46,600
Yeah I mean I think that the broader thing that it's kind of endemic is

497
00:44:50,040 --> 00:44:56,440
I think there's a lot of trends in the community largely due to the success of the field

498
00:44:56,440 --> 00:45:02,920
and the fact that it's grown in such a really I don't know if it's unprecedented in the

499
00:45:02,920 --> 00:45:08,840
scoop of all fields but it's unprecedented maybe in the scope of of of AI machine intelligence

500
00:45:09,560 --> 00:45:13,960
that has sort of resulted in this weird situation where the the review system is buckling a

501
00:45:13,960 --> 00:45:19,800
little bit the review system the pool of reviewers has had to grow in a way that maybe changes

502
00:45:19,800 --> 00:45:27,080
the standards of who's a qualified reviewer a lot of people are in a very volume driven I think

503
00:45:27,080 --> 00:45:32,280
what one thing that happens is as a community grows so fast not that many people are getting

504
00:45:32,280 --> 00:45:39,240
mentorship from you know true masters in the field but are really taking their cues from

505
00:45:39,240 --> 00:45:46,040
machine learning subreddit or something and you know are looking for looking for a splashy or

506
00:45:46,040 --> 00:45:52,840
just you know trying to get papers in or get citations so there's a lot of ways that the community

507
00:45:52,840 --> 00:45:59,480
may be struggling a little bit now in terms of quality control and I think that maybe is a natural

508
00:45:59,480 --> 00:46:04,440
thing that'll self correct is you know I think even this ICML was actually a step in the right

509
00:46:04,440 --> 00:46:11,240
direction then you know the other side is like the specific trend that you're talking about here

510
00:46:11,240 --> 00:46:19,880
which is maybe this particular thing that we talk about about maybe being a little bit abusive

511
00:46:19,880 --> 00:46:26,920
with language misusing language you know making one of the things about technical papers even

512
00:46:26,920 --> 00:46:31,240
when technical review works it has certain blind spots right like you you you submit a

513
00:46:31,240 --> 00:46:36,520
a nirips paper you submit an ICML paper you have an expectation there are certain things other

514
00:46:36,520 --> 00:46:44,120
reviewers are going to if the system works be very critical about and they are going to spot and

515
00:46:44,120 --> 00:46:52,200
among them you know you expect that they're going to spot if your your your notation makes no sense

516
00:46:52,200 --> 00:46:57,560
they're going to spot if your theorem is wrong hopefully they're going to spot if your experiments

517
00:46:57,560 --> 00:47:05,560
don't sort of support the kind of performance claim that you are making in a in a like absolute

518
00:47:05,560 --> 00:47:11,800
sense right but what they tend to miss and I think there's a blind spot in the review is the

519
00:47:11,800 --> 00:47:19,080
reviews are not generally critical about the this sort of you know they're not able to assess

520
00:47:19,080 --> 00:47:23,720
the critical arguments that are made and this is an area where you know the typical ml paper

521
00:47:23,720 --> 00:47:29,480
is a bit sloppy so typical ml paper will you know there's a caricature that you can make which is

522
00:47:29,480 --> 00:47:34,280
that your introduction is just kind of you know something is a problem and other people have worked

523
00:47:34,280 --> 00:47:40,360
on it or it's you know used to say some kind of generic mumbo jumbo there's more and more data

524
00:47:40,360 --> 00:47:44,600
and bigger and bigger computers and something something something and then you say something is

525
00:47:44,600 --> 00:47:49,720
a problem and right now people have to do the work and we can automate it with machine learning

526
00:47:49,720 --> 00:47:54,680
or something but it's basically in too many papers the introduction is not like an opportunity

527
00:47:55,320 --> 00:48:01,000
for for like some profound philosophical argument that that justifies the why you're solving

528
00:48:01,000 --> 00:48:06,280
a problem you are in the first place it's sort of a throw away piece that just is some kind of

529
00:48:06,280 --> 00:48:11,320
stage setting so you can move on to the meat of the paper which is here the equations and here

530
00:48:11,320 --> 00:48:17,240
are the quantitative results and that leaves you know is that the right bar for a paper a profound

531
00:48:17,240 --> 00:48:23,800
philosophical argument is that the case you know in other fields not for every paper but um you

532
00:48:23,800 --> 00:48:28,440
know well in other fields it is the contribution right now in other fields like the the argument is

533
00:48:28,440 --> 00:48:33,640
the substance of the paper I think a philosophy paper um depending on which area of you know say

534
00:48:33,640 --> 00:48:38,520
economics or something that that could be the essence of the paper can be the critical argument

535
00:48:38,520 --> 00:48:42,920
that you're making which is very different where where we sort of get to that I have a very

536
00:48:42,920 --> 00:48:47,320
well-formed machine learning problem and either I have an algorithm for it or I have a a smashing

537
00:48:47,320 --> 00:48:53,640
empirical result for it that is the that in the sweet spot I think for like a a NURPS or ICML

538
00:48:53,640 --> 00:48:58,280
paper like the sweet spot from a perspective of like most likely to get in is something like that

539
00:48:58,280 --> 00:49:02,200
I've got a very well-formed problem everyone's already decided it's important I don't have to argue

540
00:49:02,200 --> 00:49:09,240
for its validity and then you have the real sweet spot would be you had a just enough of a algorithmic

541
00:49:09,240 --> 00:49:13,800
contribution that there's a non-trivial theorem in there and then you also had some amount of

542
00:49:13,800 --> 00:49:17,400
experiments you know because there's one of these things like whoever it's whoever doesn't like

543
00:49:17,400 --> 00:49:22,360
your paper the most it probably is going to get it you know uh you know like there's some argument

544
00:49:22,360 --> 00:49:26,040
I don't know you know I don't know if this is truly true but you know there's maybe an

545
00:49:26,040 --> 00:49:29,800
ask a negative truth in it that like a paper doesn't get accepted it gets not rejected or

546
00:49:29,800 --> 00:49:35,800
something like that and so this way it's you know the person who would say ah you know if there's

547
00:49:35,800 --> 00:49:40,200
no real experiments I don't know it really works that that person is satisfied and the person who will

548
00:49:41,400 --> 00:49:47,320
their lazy way of accepting the paper as ads trivial they they get the they get the math that

549
00:49:47,320 --> 00:49:52,280
they're looking for than everyone is happy but but you know I think I think the language is a bit

550
00:49:52,280 --> 00:49:59,640
of a blind spot like for example it's extremely difficult to publish a position paper no matter

551
00:49:59,640 --> 00:50:05,320
how well argued it I think if you if you submitted a an eight-page essay that like really

552
00:50:06,120 --> 00:50:11,560
meticulously picks apart say the foundations of some kind of problem or some kind of application

553
00:50:11,560 --> 00:50:15,240
of machine learning in a way that it's extremely knowledgeable or well researched or whatever

554
00:50:15,240 --> 00:50:19,080
I think you'd have virtually no chance of getting it accepted at ICML or NURPS

555
00:50:20,440 --> 00:50:24,920
I think the NLP community is a bit better like some of their conferences will explicitly

556
00:50:24,920 --> 00:50:30,440
it if not you know say that's the number one priority and I don't think they should it's

557
00:50:30,440 --> 00:50:35,480
technical conference well these include position papers as like within the mandate of the conference

558
00:50:36,600 --> 00:50:39,960
where you do have a little bit more license for something like that is a lot of workshops

559
00:50:39,960 --> 00:50:45,160
are familiar to that kind of material but you know I think the danger is what you end up getting

560
00:50:45,160 --> 00:50:50,280
is what you get is a technical paper that maybe maybe it makes technical sense I mean even even

561
00:50:50,280 --> 00:50:55,560
there I think we have some weird issues but that's a whole other you know topic but you know even

562
00:50:55,560 --> 00:50:59,240
say the technical content of the paper makes sense what you have is a problem which is

563
00:50:59,240 --> 00:51:04,760
ostensibly valuable in part because of the virtue signaling that it is addressing some important

564
00:51:04,760 --> 00:51:13,800
real world problem right so so you start from that point but then the claim maybe you know the

565
00:51:13,800 --> 00:51:19,800
paper says like we make our algorithm fair by doing whatever you know this is the fair the

566
00:51:19,800 --> 00:51:26,280
fair DQN the fair GAN the fair whatever it is so the the claim like from the titles in the very

567
00:51:26,280 --> 00:51:30,840
title of the paper is the claim is somehow we have satisfied whatever it is this thing the

568
00:51:30,840 --> 00:51:35,000
thing called fairness fair and check you know we've made the ethical the ethical

569
00:51:36,280 --> 00:51:41,160
reinforcement learning agent or something like this so like the claim is sort of at the outset

570
00:51:41,160 --> 00:51:45,560
is not it's not like a firm technical mathematical claim is sort of like we solved ethics or something

571
00:51:45,560 --> 00:51:50,760
like this and then the introduction very often you know for for and I'm not saying every paper is

572
00:51:50,760 --> 00:51:55,640
like this but I think there's a pattern where the motivation might might either be you know on the

573
00:51:55,640 --> 00:51:59,800
worst side it could be you know there's plenty of words it's just like babbling but on the other

574
00:51:59,800 --> 00:52:07,320
side you could have work that is even intelligent but it's just that the the applicability to to

575
00:52:07,320 --> 00:52:12,680
that real world problem that you're claiming that you uh solved is just missing or it's not there

576
00:52:12,680 --> 00:52:18,840
or you know it's they're serious caveats and need to be stated um and then you know you have this

577
00:52:18,840 --> 00:52:23,240
these weird things that you like you have to argue that your your quantitative result is correct

578
00:52:23,240 --> 00:52:28,440
you have to argue you have to be meticulous about laying out like how you did your splits between

579
00:52:28,440 --> 00:52:36,520
train validation test data um how many runs did you do um you know make sure that your uh work is

580
00:52:36,520 --> 00:52:41,160
is reproducible by by communicating certain technical details you have to put the proofs for

581
00:52:41,160 --> 00:52:45,640
the theorems you can't just make a outlandish claim that some mathematical factors true you have

582
00:52:45,640 --> 00:52:50,120
to you have to prove it right but then there's things that you don't have to prove and I think these

583
00:52:50,120 --> 00:52:54,840
are subtle ways that that things go wrong like for example you don't have to argue for what you call

584
00:52:54,840 --> 00:52:59,960
something and some reviewers might get predicted they might be particular about language but I think

585
00:52:59,960 --> 00:53:07,000
a lot of people are willing to let that slide um and and this becomes I think especially a problem

586
00:53:07,000 --> 00:53:12,440
in this new dynamic where um the research isn't just among the research community right it's not

587
00:53:12,440 --> 00:53:19,240
just technical research is being read by PhD students there's this weird loop between research um

588
00:53:19,240 --> 00:53:26,840
industry uh governance and uh a huge part of that is the popular press um and so you wind up

589
00:53:26,840 --> 00:53:31,880
and assist more like people uh are maybe motivated I think I think you know people I don't think

590
00:53:31,880 --> 00:53:37,880
are malicious I think you wind up with someone maybe motivated by a genuine desire to uh do something

591
00:53:37,880 --> 00:53:45,400
that they feel has more um social you know sort of some kind of positive social impact or something

592
00:53:45,400 --> 00:53:51,560
like that but then they start working on and they think well um you know how am I get you know

593
00:53:51,560 --> 00:53:54,840
what would be a good title for this paper it's gonna get people to read it or something you know

594
00:53:54,840 --> 00:54:00,200
and and they're not necessarily thinking like hey I'm making a claim that this is this thing that

595
00:54:00,200 --> 00:54:04,760
solves this there's this real world problem and I'm making a claim that this algorithm that I'm

596
00:54:04,760 --> 00:54:12,440
putting which solves some very uh sort of like toy very reductive version of that problem um that

597
00:54:12,920 --> 00:54:18,200
uh you know that has some applicability to that that real world situation which it might not right

598
00:54:18,200 --> 00:54:24,200
and and then you know people you know like you know you put out this thing like um you know I'm gonna

599
00:54:24,200 --> 00:54:28,360
you know my thing will tell you if your algorithm is fair or whatever without even being clear about

600
00:54:28,360 --> 00:54:33,000
how we even address like the right ingredients or even speaking in the right language to be able

601
00:54:33,000 --> 00:54:38,680
to capture something like that and we even access the right data or any any of that and then it gets

602
00:54:38,680 --> 00:54:43,080
picked up in a story and say you know oh uh you know like for example like IBM has been a bit of a

603
00:54:44,760 --> 00:54:51,080
a bit of a like cavalier actor in this way right both with this both with medical and with fairness

604
00:54:51,080 --> 00:54:58,440
related things right so so IBM um did this thing where you know with Watson forever where they kind

605
00:54:58,440 --> 00:55:04,120
of made people think they were solving fundamental medical problems when they really didn't have

606
00:55:04,120 --> 00:55:07,640
they didn't have the goods to back it up and then now it's like oh we're getting ahead of the

607
00:55:07,640 --> 00:55:11,240
fairness thing which is you know it's great that on one hand you know the running is ad campaign

608
00:55:11,240 --> 00:55:14,920
they're raising awareness that's one thing but on the other hand they put out uh like an open

609
00:55:14,920 --> 00:55:19,240
source repo and they made it sound like hey you know here's a set of algorithms that'll make your

610
00:55:19,240 --> 00:55:24,360
stuff fair um and the big danger right is that someone would use that and think that like okay that

611
00:55:25,000 --> 00:55:29,800
that that'll clear us and and work it's even crazier and this may be ties into a paper that I

612
00:55:29,800 --> 00:55:34,600
wrote uh recently with Alex Cholde-Trova at the Heinz School of Public Policy and Julian McCauley

613
00:55:34,600 --> 00:55:41,880
from UCSD is that you know sometimes the algorithm actually could be super horrible from from any

614
00:55:41,880 --> 00:55:46,280
I think like reasonable ethical standpoint because people have miscast the problem right

615
00:55:46,280 --> 00:55:51,080
those simplifying decisions that you made that sort of seemed intuitive from setting up a toy

616
00:55:51,080 --> 00:55:55,800
problem when you weren't thinking about any real world data might actually result in something that's

617
00:55:55,800 --> 00:55:59,800
absolutely horrible um that you would never want to use right there's some examples of that

618
00:56:00,680 --> 00:56:05,880
so here's a problem in the in the United States our sort of perspective on like discrimination

619
00:56:05,880 --> 00:56:11,880
in a lot of context is is informed by um title seven of the Civil Rights Act of 1964

620
00:56:11,880 --> 00:56:16,040
now that gives I think our dominance in the research community means that maybe like the

621
00:56:16,040 --> 00:56:21,720
overall like fairness work is a little bit skewed by technical interpretations of United States law

622
00:56:21,720 --> 00:56:28,200
versus other countries which maybe have their own legal precedent um but that that you know that's

623
00:56:28,200 --> 00:56:35,320
that's where I think a lot of like thinking um comes from and and in that context you know there

624
00:56:35,320 --> 00:56:41,560
there's these basically the Civil Rights Act has these two um legal doctrines that have emerged

625
00:56:41,560 --> 00:56:46,040
from it and one is this notion of disparate treatment and the other is this notion of disparate

626
00:56:46,040 --> 00:56:53,080
impact and you know um these are motivated by discrimination and in housing and employment

627
00:56:53,080 --> 00:57:00,120
and a number of cases and you know disparate treatment is basically about intentional discrimination

628
00:57:00,920 --> 00:57:09,400
so it includes it subsumes discrimination based on like explicit consideration of something that

629
00:57:09,400 --> 00:57:15,720
is deemed uh what they call a protected um characteristic or like membership in a protected class

630
00:57:15,720 --> 00:57:21,640
right so this could be like um the what is someone's race what is someone's gender and in fact in

631
00:57:21,640 --> 00:57:27,240
the original Civil Rights Act of 1964 there's like some some some some listing of set of protected

632
00:57:27,240 --> 00:57:32,600
classes probably in 2019 there's some that we would want to include in things that we would think

633
00:57:32,600 --> 00:57:39,960
of as kind of like salient uh like wedges of discrimination in society that we would want to be

634
00:57:39,960 --> 00:57:45,000
especially productive of that were not included then like a sexual orientation or something but um

635
00:57:45,000 --> 00:57:49,080
I guess like 1964 America wasn't there yet so it's not part of the Civil Rights Act

636
00:57:50,200 --> 00:57:55,800
so disparate treatment is about intentional discrimination and um that includes but is not

637
00:57:55,800 --> 00:58:01,320
limited to like direct use of that characteristic so you know it's important to keep in in mind how

638
00:58:01,320 --> 00:58:06,120
the law works versus how math works with math we want to say I want to come up with a a crystal

639
00:58:06,120 --> 00:58:11,560
clear simple set of principles that's going to govern any kind of situation that I could possibly

640
00:58:11,560 --> 00:58:15,800
encounter right just it describes the world or you know physics or something like that we would

641
00:58:15,800 --> 00:58:24,360
kind of operate in this sort of way the law is more like a patchwork that is is plugging in laws

642
00:58:24,360 --> 00:58:28,520
in different places to try to deal with different things that are actually happening in the world

643
00:58:28,520 --> 00:58:32,920
right so they're there there's a set of drugs that are illegal there's a set of drugs that are not

644
00:58:32,920 --> 00:58:37,720
illegal but you know maybe you're qualitatively similar the reason why they're not illegal is

645
00:58:37,720 --> 00:58:42,600
because uh they haven't but they haven't been abused yet you know once they are then maybe

646
00:58:42,600 --> 00:58:46,840
they would become illegal right we would make a law we say oh there's a problem let's make a law

647
00:58:46,840 --> 00:58:51,480
and puts us in a weird situation for governing technology because technology might bring up new

648
00:58:51,480 --> 00:58:55,960
problems but our law is about dealing with uh certain when we think of discrimination when you're

649
00:58:55,960 --> 00:58:59,400
creating the law it's a career thinking of you're thinking of the racist person who has a sign

650
00:58:59,400 --> 00:59:05,320
out as we don't hire whoever that's one case right so let's so this retreatment I think and I

651
00:59:05,320 --> 00:59:11,080
and I want to be clear that I'm not a I'm not a legal scholar I'm just a uh a fake legal scholar or

652
00:59:11,080 --> 00:59:16,520
something um I'm a machine learning president who tries to be uh play one on TV I tries to be bilingual

653
00:59:17,160 --> 00:59:23,400
maybe more than is standard but um I don't you know I'm not like uh you know I'm sure that there's

654
00:59:23,400 --> 00:59:26,760
a number of people from from the other side that know this better but fortunately by by having

655
00:59:26,760 --> 00:59:31,880
dialogue with them you know we we've had a more refined we've been able to refine our you know

656
00:59:31,880 --> 00:59:36,440
um analysis on it right so that's this one thing this retreatment and the other side is disparate

657
00:59:36,440 --> 00:59:41,240
impact and disparate impact basically is trying to cover those situations whereby you can have

658
00:59:41,240 --> 00:59:47,160
what the law calls a facially neutral policy so facially neutral means like you know like on this

659
00:59:47,160 --> 00:59:55,080
it appears not to in any way like explicitly be um taking it you know using this kind of

660
00:59:55,080 --> 01:00:00,440
information and yet at the same time it can have an unjustified um disparate impact you know

661
01:00:00,440 --> 01:00:06,280
an unjustified disparity and so it's really important to think about like what's going on here

662
01:00:06,280 --> 01:00:09,880
and that on one hand we have disparate treatment disparate treatment is a doctrine that is

663
01:00:09,880 --> 01:00:16,040
concerned with intent right and then on the other side we have disparate uh impact uh a

664
01:00:16,040 --> 01:00:20,840
different legal doctrine which is concerned with um whether or not something is justified

665
01:00:21,560 --> 01:00:27,080
and then we're trying to uh people basically look to the law because they say hey well I want to

666
01:00:27,080 --> 01:00:32,760
make algorithms that you know don't do bad things so so let me look to the law because that that's

667
01:00:32,760 --> 01:00:37,960
like my existing body of work that I could draw from but the law talks about intention the law

668
01:00:37,960 --> 01:00:44,600
talks about justification and um you know I don't know what justification is in in the language of

669
01:00:44,600 --> 01:00:48,920
supervised learning but I'm pretty sure that basically it's not expressable in the like like

670
01:00:48,920 --> 01:00:52,680
basically we've got an insufficient language the the the way that we're talking about machine

671
01:00:52,680 --> 01:00:57,160
learning problems is too reductive and it doesn't doesn't possess the vocabulary to express

672
01:00:57,160 --> 01:01:00,680
this concept it's kind of like I know you've seen Judah Pearl if you read the book of lies kind of

673
01:01:00,680 --> 01:01:05,880
like that the way he's he's you know he's he's he's ringing that bell talking about um you know

674
01:01:05,880 --> 01:01:09,960
the all the long line of statisticians that didn't realize that causality was something that

675
01:01:09,960 --> 01:01:15,640
lied outside classical statistics and like no matter how hard you try there's not like just

676
01:01:15,640 --> 01:01:21,320
an expression in purely probability terms that you know tells you what causality is you have to

677
01:01:21,320 --> 01:01:26,440
introduce like external notation um I think it's a little bit like that you know we have the law is

678
01:01:26,440 --> 01:01:33,240
is coming from from this richer family of notions and and we don't know what it means to justify

679
01:01:33,240 --> 01:01:38,040
something because all we know how to do is basically say is something associated with something else

680
01:01:38,040 --> 01:01:42,360
we don't know why it's associated we don't know how to differentiate for example um

681
01:01:42,360 --> 01:01:51,320
you know uh why what is the difference between um so uh the difference between if you were to look

682
01:01:51,320 --> 01:01:55,400
at like educational outcomes right and say what what should what should institutes of higher learning

683
01:01:55,400 --> 01:02:01,720
do you know in their in their admissions processes vis-a-vis um black versus white Americans versus

684
01:02:01,720 --> 01:02:08,280
white you know versus Jewish Americans and those are two very different uh cases to think about and

685
01:02:08,280 --> 01:02:16,200
part of why they're different to think about is because um you know there's this whole um background

686
01:02:16,200 --> 01:02:21,640
of how that data was created it's not just is a group overrepresented versus another group

687
01:02:21,640 --> 01:02:26,440
but there's also you know what was the process you know in one case I think we have a very deep and

688
01:02:26,440 --> 01:02:33,560
well-documented knowledge of uh systematic you know like institutional um oppression of one group

689
01:02:33,560 --> 01:02:39,800
that you know created opportunities and um with held opportunities from the other and in the other

690
01:02:39,800 --> 01:02:45,400
case you sort of have a group that's maybe overrepresented despite being um discriminated against

691
01:02:46,040 --> 01:02:51,960
and and and you know these kind of point to different uh I think to most people's like ethical

692
01:02:51,960 --> 01:02:55,960
sensibilities different senses about you know how you should correct them but supervised learning

693
01:02:55,960 --> 01:02:59,400
doesn't give you those tools right supervised learning doesn't distinguish these them

694
01:02:59,400 --> 01:03:02,920
between these because it doesn't the the tools that we have in supervised learning for trying

695
01:03:02,920 --> 01:03:08,280
to address fairness don't don't look into where the data comes from so back back to the those

696
01:03:08,280 --> 01:03:12,680
two cases what people have done is they've tried to formalize you know disparate treatment and

697
01:03:12,680 --> 01:03:18,120
disparate impact as technical ideas that you can express just as simple statistical parodies

698
01:03:18,120 --> 01:03:21,640
and the reason why is because they want to build an algorithm right you want to we want to say hey

699
01:03:21,640 --> 01:03:27,960
here's a problem I can I can define it in technical terms I can fix it in technical terms

700
01:03:28,920 --> 01:03:33,960
so the way people interpret disparate treatment in technical terms is they say um uh disparate

701
01:03:33,960 --> 01:03:39,080
treatment says explicit use of the protected class or intentional use of the you know intentional

702
01:03:40,040 --> 01:03:45,560
kind of discrimination on that basis even if you know it's not you know used directly well we

703
01:03:45,560 --> 01:03:49,320
well let's throw out the intentional part because we don't really know how to model intent we

704
01:03:49,320 --> 01:03:52,680
don't know what that means it's not even clear if it means anything in the context of the

705
01:03:52,680 --> 01:03:57,160
supervised learning model you know who's intent are we talking about the human the model so you

706
01:03:57,160 --> 01:04:01,320
throw that part out and so then disparate treatment just becomes blindness so it means we just don't

707
01:04:01,320 --> 01:04:06,680
use that feature then disparate impact on the other hand we don't know um how to deal with the

708
01:04:06,680 --> 01:04:10,600
justification part because justification has we have to start thinking about where the data comes

709
01:04:10,600 --> 01:04:15,880
from we have to start thinking about what does it you know we you know that's two it's too philosophical

710
01:04:15,880 --> 01:04:20,760
so let's throw out the justification part let's just look at demographic parity right then

711
01:04:21,720 --> 01:04:28,200
among other things you know you say well um let's say that I wanted to minimize the demographic

712
01:04:28,200 --> 01:04:34,840
disparity between two groups while as much as possible maintaining the accuracy of the model right

713
01:04:35,400 --> 01:04:39,800
how would I do that and the answer is actually simple it's I would go in there and explicitly

714
01:04:41,880 --> 01:04:45,400
set the thresholds of the groups differently so that I would accept more people from one group

715
01:04:45,400 --> 01:04:49,560
is slightly less than the other than if I had just like run a supervised model instead of universal

716
01:04:49,560 --> 01:04:55,480
threshold and by doing that you would actually like optimally trade off you know the demographic

717
01:04:55,480 --> 01:05:01,160
parity versus the accuracy and then what people end up saying as well but we can't do that

718
01:05:01,160 --> 01:05:05,720
because we want to we don't want to have disparate treatment and the question is is it disparate

719
01:05:05,720 --> 01:05:11,800
treatment if it's in the service of diversity um and that actually becomes this actually you know

720
01:05:11,800 --> 01:05:17,320
it's so so the big problem like where we you know we actually we'll rent our own problem and got

721
01:05:17,320 --> 01:05:21,400
to work through this issue is that fortunately we thought instead of others we had friends who are

722
01:05:21,400 --> 01:05:25,400
in the legal community who work on these issues and are they passionate about them and are no

723
01:05:25,400 --> 01:05:29,160
hit when they're talked about the wrong way from technical community thinking back to us and said

724
01:05:29,160 --> 01:05:33,640
well you know you can't you can't have legal disparate treatment like what do you mean you're like

725
01:05:33,640 --> 01:05:38,040
well it's by definition if it's legal it's not disparate treatment right so there was this there was

726
01:05:38,040 --> 01:05:43,560
this kind of tension of so disparate treatment then doesn't actually mean blindness disparate treatment

727
01:05:43,560 --> 01:05:49,080
means it's unjustified right it's like disparate it's not like having having disparate treatment

728
01:05:49,080 --> 01:05:54,520
isn't just not being blind it's like having disparate treatment means not being blind in a scenario

729
01:05:54,520 --> 01:05:59,640
that's like illegal unless it's somehow over and considered to be legal so what we've done I think

730
01:05:59,640 --> 01:06:06,120
the big danger is we've we've overloaded these like reductive technical terms with the name of

731
01:06:06,120 --> 01:06:10,360
a legal doctrine so we've purported you know at the end of the day we say we've solved you know

732
01:06:10,360 --> 01:06:14,280
we've we've solved this for treatment we've solved this for an impact but we've solved this like

733
01:06:14,280 --> 01:06:19,000
these sort of toys statistical parodies we've characterized these trade offs it's useful work

734
01:06:19,000 --> 01:06:23,080
but the big danger I think is that we end up sort of misrepresenting the public that we've solved

735
01:06:23,080 --> 01:06:28,440
like a legal conundrum and we have it right so what we ended up looking at is so what then people do

736
01:06:28,440 --> 01:06:31,880
right is they they say well you can't have this for treatment because the loss is you can't even

737
01:06:31,880 --> 01:06:37,400
though actually you can potentially you know and this is I think people there's all kinds of

738
01:06:37,400 --> 01:06:41,560
other arguments like people are wary of doing anything that looks like affirmative action

739
01:06:41,560 --> 01:06:45,480
or calling it affirmative action whether or not they believe in affirmative action because

740
01:06:45,480 --> 01:06:50,840
there's a worry that you know even though affirmative action is legal that small changes in

741
01:06:51,560 --> 01:06:56,520
composition of the Supreme Court or something could or you know it could be in a precarious state

742
01:06:56,520 --> 01:07:00,040
so you want to come up with a solution that doesn't rely on that's a that's a separate issue for

743
01:07:00,040 --> 01:07:04,840
I think for more from like a strategic angle in terms of pitching policy then from a technical angle

744
01:07:06,200 --> 01:07:11,000
but what we end up doing is paper is basically look at this problem like one first how to how do

745
01:07:11,000 --> 01:07:17,960
these how do these technical notions relate to these legal ones but then even further when you

746
01:07:17,960 --> 01:07:24,360
try to trade off having you know this representational parity this demographic parity or or what

747
01:07:24,360 --> 01:07:30,520
we call you know what they call satisfying you know disparate impact or not having disparate impact but

748
01:07:30,520 --> 01:07:35,480
you know it's actually this like demographic parity in a statistical sense with accuracy and you

749
01:07:35,480 --> 01:07:39,560
try to do it without explicitly looking at the group membership you come up with these algorithms

750
01:07:39,560 --> 01:07:43,880
that have very very strange behavior and so we start analyzing you know what's the behavior of these

751
01:07:43,880 --> 01:07:50,120
algorithms and two really weird things happen one is that basically well you have all these features

752
01:07:50,120 --> 01:07:55,320
that are sort of predictive of whatever is the protected attribute right so correlated to yeah

753
01:07:55,320 --> 01:07:58,760
right which is sort of why you have a problem in the first place right well why disparate impact

754
01:07:58,760 --> 01:08:01,880
is the problem in the first place is that well it's not enough to just not look at that feature

755
01:08:01,880 --> 01:08:06,120
because it's associated with with a label it's associated with the other covariates like

756
01:08:06,120 --> 01:08:10,840
everything's all entangled if it's perfectly entangled like in the way that like you could just

757
01:08:10,840 --> 01:08:15,720
with a hundred percent accuracy recover that feature right then it turns out well because the

758
01:08:15,720 --> 01:08:21,160
optimal thing is to just set a class dependent threshold like and just move the threshold to sort

759
01:08:21,160 --> 01:08:26,040
of in a diversity promoting way that's what your model is going to do anyway right if your model's

760
01:08:26,040 --> 01:08:29,720
expressive enough that's explicitly what it's going to do so you know you've done all you've

761
01:08:29,720 --> 01:08:33,080
done through all these hoops to pretend that you're doing something else but when the feature is

762
01:08:33,080 --> 01:08:37,640
fully recoverable you're going to end up doing that exactly anyway so you're only kind of pretending

763
01:08:37,640 --> 01:08:44,200
to do something qualitatively different in which case why bother but then the the more troubling case

764
01:08:44,200 --> 01:08:49,800
is what happens when you know everything's correlated but not perfectly like you can only

765
01:08:50,440 --> 01:08:57,800
imperfectly infer someone's gender say from their non-protected traits and then the weird

766
01:08:57,800 --> 01:09:01,400
thing that happens is that if you take a lot of these algorithms off the shelf and you apply them

767
01:09:01,400 --> 01:09:06,040
on data so we looked at CS admissions data and we said what if we wanted to increase the number

768
01:09:06,040 --> 01:09:11,880
of a women admitted relative to men while without looking at the gender right so that's that's

769
01:09:11,880 --> 01:09:16,600
the that's the kind of problems that was adopted by a lot of problems is as we want to

770
01:09:17,400 --> 01:09:21,960
not have disparate impact but also not have disparate treatment you know keep in mind I'm like

771
01:09:21,960 --> 01:09:25,640
using a scare quotes because that's not you know they're not really talking about the legal doctrine

772
01:09:25,640 --> 01:09:31,720
but about the so we we tried to not use those terms so we called it impact disparity and treatment

773
01:09:31,720 --> 01:09:35,560
disparity to be like just you know it explicitly say in the introduction like you know that we

774
01:09:35,560 --> 01:09:39,880
use this to just be clear we're not talking about the the legal doctrine so if you say I don't want

775
01:09:39,880 --> 01:09:44,120
to have either of those and then subject to that I want to maximize accuracy what happens

776
01:09:44,680 --> 01:09:50,760
and what ends up happening is the model implicitly is trying to infer what is the what is the gender say

777
01:09:50,760 --> 01:09:56,200
so in our case it was gender was a sensitive trait and then it's trying to use the inferred implicit

778
01:09:56,200 --> 01:10:00,280
gender to flip decisions so basically you know you look at what was the threshold that you would

779
01:10:00,280 --> 01:10:03,720
have had anyway and you have some people that are a little bit above the threshold some people that

780
01:10:03,720 --> 01:10:08,200
are a little bit below the threshold um both among you know the men and above the women and what

781
01:10:08,200 --> 01:10:11,640
basically what the model is doing is it doesn't know which of the men are the women it knows which

782
01:10:11,640 --> 01:10:15,960
ones the things are more likely to be men or more likely to be women so end up being if you look

783
01:10:15,960 --> 01:10:20,760
at as compared to the unconstrained model whose decisions get flipped and it's well one because

784
01:10:20,760 --> 01:10:28,040
there there's an in-norded amount of men in the cs admissions data um it's it's largely just men

785
01:10:28,040 --> 01:10:32,360
having their decisions flipped from men who the model thinks are women having their models flip

786
01:10:32,360 --> 01:10:37,320
to positive and men who the model thinks are men having their very confidently having their decisions

787
01:10:37,320 --> 01:10:43,720
flipped to negative and then there's also some flips of women who and women who the model thinks are

788
01:10:43,720 --> 01:10:49,640
men might get hurt by the algorithm and women who the model thinks are most likely to be women but

789
01:10:49,640 --> 01:10:56,120
are a little bit below the threshold are benefited by it um and then you think well you know what you

790
01:10:56,120 --> 01:10:59,400
know if you step back if you think in terms of supervised learning and you just think hey what I

791
01:10:59,400 --> 01:11:04,600
care about is that there's some group level justice then you know in that through that lens you

792
01:11:04,600 --> 01:11:09,640
think you might still say this is you know this is worth it this is okay right even though it sounds

793
01:11:09,640 --> 01:11:13,320
acid on more it yeah it sounds a little bit weird especially when it's like what we could have

794
01:11:13,320 --> 01:11:17,480
just gone in there and just set the thresholds a little bit differently and just you know like our

795
01:11:17,480 --> 01:11:22,440
goal was we wanted to increase the representations in group like why aren't we just doing that but

796
01:11:22,440 --> 01:11:27,240
the weirder thing though I think that then this is the part where I think it kind of brings

797
01:11:27,240 --> 01:11:32,040
things full circle to like thinking a little bit more like say an economist or like a social scientist

798
01:11:32,040 --> 01:11:38,040
is you think not just about a prediction but a system of incentives right not just I'm making

799
01:11:38,040 --> 01:11:42,600
classifications but I'm making decisions and this is telling people what they should do to get

800
01:11:42,600 --> 01:11:47,160
into grad school right and it's like so if you start breaking it down you look into it and you say

801
01:11:47,160 --> 01:11:51,560
who are the people that the model thinks are most likely to be women I like say among the women

802
01:11:51,560 --> 01:11:56,520
like what what what are the the features that make the model think they're most likely to be women

803
01:11:56,520 --> 01:12:01,720
versus men and it turns out that there's discrepancy is based on who your requested advisor is which

804
01:12:01,720 --> 01:12:08,840
area you say you want to work in things like this right and so like to me that's like the scariest

805
01:12:08,840 --> 01:12:12,600
part about it it's not just that it's like a little bit like if it was just your injectors all

806
01:12:12,600 --> 01:12:18,600
these layers of unintended consequences and potentials for gaming and all kinds of stuff

807
01:12:18,600 --> 01:12:23,080
and even more than that right because it's not just like unintended consequences like who are the

808
01:12:23,080 --> 01:12:29,960
women like how do you how do you benefit from this policy as a woman you have to be in the field in

809
01:12:29,960 --> 01:12:36,200
which women are already well represented so that it's in the subfield so that the algorithm knows

810
01:12:36,200 --> 01:12:41,000
that you're a woman but the women who are actually hurt by it are the ones who are like in the field

811
01:12:41,000 --> 01:12:47,240
that is underrepresented those ones who are hurt because the model thinks they're men right so

812
01:12:48,040 --> 01:12:52,040
or things are more likely to be men and therefore things that's more likely to get more credit

813
01:12:52,040 --> 01:12:59,240
towards it's like you know equality constraint by rejecting them and so the danger here and so like

814
01:12:59,240 --> 01:13:04,520
I don't mean like I want to be very clear that one I think we need to do something to address these

815
01:13:04,520 --> 01:13:11,880
social issues and I think it's like a pressing concern of our time and I'm all for it I also

816
01:13:11,880 --> 01:13:18,680
think we need technical people working on it and thinking about these problems and even investigating

817
01:13:18,680 --> 01:13:24,440
the potential of technical solutions but I think you know the the danger is we need to one we need

818
01:13:24,440 --> 01:13:29,240
to be very clear about what is the problem we're solving and is this is this a solution anyone should

819
01:13:29,240 --> 01:13:33,640
even think about using off the shelf right because like these policy makers are saying we need an

820
01:13:33,640 --> 01:13:40,040
interpretability or explainability or or you know some kind of fairness whatever and when technologists

821
01:13:40,040 --> 01:13:44,280
are showing up because there's a bit of a career cookie to say hey I'm working on that and I'm

822
01:13:44,280 --> 01:13:49,160
I've made progress what about my research the big danger is well what what if what you're doing

823
01:13:49,160 --> 01:13:54,280
actually is really bad like would be really really bad for anyone to deploy that um compared to

824
01:13:54,280 --> 01:13:59,000
even doing something kind of naive right and and we have to be I think just really careful that we

825
01:13:59,000 --> 01:14:05,880
don't like sort of create this uh sense that like we've we've solved this societal problem in a way

826
01:14:05,880 --> 01:14:10,040
where like oh you know you could outsource the judgment to us we don't need we don't need the

827
01:14:10,840 --> 01:14:15,400
the kind of think about it in a much way it's like we we've actually turned this into a technical

828
01:14:15,400 --> 01:14:22,920
problem and solved it so so you know like you know outsource outsource the the like really really

829
01:14:24,360 --> 01:14:30,040
pertinent like debate that needs to happen to society to the technocrats or something

830
01:14:30,920 --> 01:14:36,040
so so right you know I think it's absolutely important work in an important area but the danger

831
01:14:36,040 --> 01:14:41,640
that I think we kind of highlight there is there is there is a huge risk of a certain kind of

832
01:14:41,640 --> 01:14:49,640
um folly of solutionism uh was that it was great uh getting a chance to finally catch up with you

833
01:14:49,640 --> 01:14:58,280
and definitely lots of important issues here and things to figure out but not via solutionism

834
01:14:58,280 --> 01:15:04,680
yeah yeah we gotta stay humble all right well thanks so much thanks for having me Sam

835
01:15:04,680 --> 01:15:13,800
all right everyone that's our show for today for more information on today's show

836
01:15:13,800 --> 01:15:21,960
visit twomolai.com slash shows make sure you head over to twomolcan.com to learn more about the

837
01:15:21,960 --> 01:15:37,880
twomolcan ai platforms conference as always thanks so much for listening and catch you next time

