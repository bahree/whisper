1
00:00:00,000 --> 00:00:15,720
Hello and welcome to another episode of Twomble Talk, the podcast where I interview interesting

2
00:00:15,720 --> 00:00:20,640
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:20,640 --> 00:00:25,240
I'm your host Sam Charrington, a few announcements before we get to the show.

4
00:00:25,240 --> 00:00:29,640
First, I'd like to take a second to give a virtual high five to everyone who entered

5
00:00:29,640 --> 00:00:34,280
our latest giveaway, in which two lucky listeners will get a chance to attend the recently

6
00:00:34,280 --> 00:00:39,640
rebranded AI conference in San Francisco, compliments of this week in machine learning

7
00:00:39,640 --> 00:00:42,920
in AI, and our friends over at O'Reilly.

8
00:00:42,920 --> 00:00:48,760
This has been our most active giveaway to date, with over 100 of you submitting entries,

9
00:00:48,760 --> 00:00:50,160
just wow.

10
00:00:50,160 --> 00:00:54,440
We haven't picked the winners yet, as of the time of this recording, but by the time you

11
00:00:54,440 --> 00:01:01,560
hear this, winners will be posted over at Twombleai.com slash AISF, and we'll give them a proper

12
00:01:01,560 --> 00:01:04,360
announcement on next week's show.

13
00:01:04,360 --> 00:01:09,880
Next up, about a month ago during my conversation with Chelsea Finn, I thought out loud about

14
00:01:09,880 --> 00:01:12,720
starting a virtual paper reading group.

15
00:01:12,720 --> 00:01:17,360
After receiving lots of positive support for the idea, we finally have a meetup to call

16
00:01:17,360 --> 00:01:18,600
our own.

17
00:01:18,600 --> 00:01:24,920
On August 16, we'll kick off the inaugural Twomble online meetup, where our first presenter,

18
00:01:24,920 --> 00:01:30,120
community member Joshua Manella, will lead a discussion on Apple's GANS paper, learning

19
00:01:30,120 --> 00:01:36,200
from simulated and unsupervised images through adversarial training, which is one of the

20
00:01:36,200 --> 00:01:40,360
best paper award winners from this year's CVPR conference.

21
00:01:40,360 --> 00:01:44,240
If you've already signed up, great, we look forward to seeing you there.

22
00:01:44,240 --> 00:01:49,200
If not, head over to twombleai.com slash meetup to get registered.

23
00:01:49,200 --> 00:01:54,360
Finally, if you've been paying attention, you know that after almost a year of procrastinating,

24
00:01:54,360 --> 00:01:57,160
I've finally launched my email newsletter.

25
00:01:57,160 --> 00:02:01,480
I've been having a blast with it, and you definitely want to subscribe.

26
00:02:01,480 --> 00:02:06,360
We've got some fun stuff in store exclusively for newsletter subscribers, so make sure

27
00:02:06,360 --> 00:02:11,200
you bounce on over to twombleai.com slash newsletter to sign up.

28
00:02:11,200 --> 00:02:16,720
Alright, as you all might know, a few weeks ago, I was in San Francisco for Wrangle, a great

29
00:02:16,720 --> 00:02:20,800
conference brought to you by our friends over at Claudeira.

30
00:02:20,800 --> 00:02:22,520
Wrangle is a super fun event.

31
00:02:22,520 --> 00:02:27,640
Each year, it brings an interesting and diverse community of data scientists to an intimate

32
00:02:27,640 --> 00:02:33,520
and informal setting, this year a music venue in SF's Mission District, for great talks

33
00:02:33,520 --> 00:02:39,280
on real data science projects and issues, not to mention cowboy hats and barbecue.

34
00:02:39,280 --> 00:02:44,120
While I was there, I had a chance to sit down with a few of the event's great speakers,

35
00:02:44,120 --> 00:02:50,640
including Drew Conway, founder and CEO of Aluvium, and a former data scientist with the CIA,

36
00:02:50,640 --> 00:02:57,000
Sherrath Rao, a listener and fan of this podcast and an engineering manager over at Instacart,

37
00:02:57,000 --> 00:03:02,080
and Aaron Schelman, a statistician and data science manager with Zimergen, a company

38
00:03:02,080 --> 00:03:06,280
using robots and machine learning to engineer better microbes.

39
00:03:06,280 --> 00:03:11,160
This show features my interview with Drew, whose Wrangle keynote could have been called

40
00:03:11,160 --> 00:03:14,960
Confessions of a CIA Data Scientist.

41
00:03:14,960 --> 00:03:19,880
The focus of our interview and the focus of Drew's presentation is an interesting set

42
00:03:19,880 --> 00:03:25,120
of observations he makes about the role of cognitive biases in data science.

43
00:03:25,120 --> 00:03:29,520
If your work involves making decisions or influencing behavior based on data driven

44
00:03:29,520 --> 00:03:35,280
analyses, and it probably does or will, you'll want to hear what he has to say.

45
00:03:35,280 --> 00:03:40,040
A quick note before we dive in, as is the case with my other field recordings, there

46
00:03:40,040 --> 00:03:44,880
is a bit of unavoidable background noise in this interview, sorry about that.

47
00:03:44,880 --> 00:03:47,480
And now on to the show.

48
00:03:47,480 --> 00:04:00,600
Alright everyone, I am here at the Wrangle conference, the guest of Claude Ara, who's sponsoring

49
00:04:00,600 --> 00:04:06,680
our series here, and I am with Drew Conway, who is the founder and CEO of Aluvium.

50
00:04:06,680 --> 00:04:08,480
It's going to be with you, Sam.

51
00:04:08,480 --> 00:04:10,600
It's great to have you on the show.

52
00:04:10,600 --> 00:04:14,880
So you just did a really interesting presentation that I tweeted a little bit about.

53
00:04:14,880 --> 00:04:18,040
Why don't we take a minute to have you introduce yourself to the audience?

54
00:04:18,040 --> 00:04:24,800
Sure, so I am the founder and CEO of Aluvium, we're a New York based company that builds

55
00:04:24,800 --> 00:04:30,160
what we call human-centered AI for the industrial industry, and what that means for us is we

56
00:04:30,160 --> 00:04:36,480
build software products that exist at the intersection of complex machine data, so

57
00:04:36,480 --> 00:04:43,520
think streaming data from an olive refinery, data from automated robotic systems, and human

58
00:04:43,520 --> 00:04:44,520
knowledge.

59
00:04:44,520 --> 00:04:47,960
One of the things that I can even talk a little bit about in the context of what I just

60
00:04:47,960 --> 00:04:53,800
presented here at Wrangle is me, my career has really been one that has kind of moved

61
00:04:53,800 --> 00:04:57,600
me through a path of working alongside and building software tools for people who need

62
00:04:57,600 --> 00:05:03,080
to make decisions from data, and Aluvium in many ways is a kind of culmination of a lot

63
00:05:03,080 --> 00:05:09,520
of thinking I've had over my career as to how best to extract maximum value from these

64
00:05:09,520 --> 00:05:15,040
complex streams of data and present a human being, you know, a man or woman who's working

65
00:05:15,040 --> 00:05:18,800
inside an industrial operation with the right information at the right time to make the

66
00:05:18,800 --> 00:05:19,800
best decision.

67
00:05:19,800 --> 00:05:22,400
So the company is just about two years old.

68
00:05:22,400 --> 00:05:27,240
We work primarily in what we call process manufacturing, so sort of distinct from the

69
00:05:27,240 --> 00:05:31,880
screen manufacturing and the way that I just grab is like screen manufacturing is screw

70
00:05:31,880 --> 00:05:37,360
in bolts and rivets and process manufacturing is typically, you know, pipes and boilers

71
00:05:37,360 --> 00:05:38,360
and things like that.

72
00:05:38,360 --> 00:05:44,080
And the reason that we focus on that second half is our approach to learning is really

73
00:05:44,080 --> 00:05:51,320
one where this continuous nature of information is more well suited for what we're doing.

74
00:05:51,320 --> 00:05:55,640
Interesting, particularly interesting because I've been spending a lot of time researching

75
00:05:55,640 --> 00:06:02,440
industrial AI or industrial applications of AI, and we're in the midst of a series of

76
00:06:02,440 --> 00:06:08,480
podcasts on industrial AI, although we're connecting here in a totally different context.

77
00:06:08,480 --> 00:06:11,200
And so why don't we jump into that?

78
00:06:11,200 --> 00:06:13,920
Tell us about your presentation here at Wrangle.

79
00:06:13,920 --> 00:06:14,920
Sure.

80
00:06:14,920 --> 00:06:15,920
So, yes.

81
00:06:15,920 --> 00:06:21,960
So the early part of my career when I first started a career was actually as a what was

82
00:06:21,960 --> 00:06:26,640
called then a computational social scientist inside the US intelligence community.

83
00:06:26,640 --> 00:06:32,880
And so this was a few years before data science was a profession, a sort of well-known profession

84
00:06:32,880 --> 00:06:37,000
and I think probably the people who were doing the work that I was doing then now are probably

85
00:06:37,000 --> 00:06:38,600
all data scientists.

86
00:06:38,600 --> 00:06:45,760
But what my job primarily was to think about how to build statistical programming and

87
00:06:45,760 --> 00:06:52,280
statistical software tools to support decision-making inside the intelligence loop, and basically

88
00:06:52,280 --> 00:06:56,000
that meant my work was split primarily into two big halves.

89
00:06:56,000 --> 00:07:00,720
One half was what I would call principally basic research.

90
00:07:00,720 --> 00:07:05,760
So things that are more academic in nature, I spent a lot of time thinking about graph

91
00:07:05,760 --> 00:07:11,880
theoretic models of network change and network moving over time.

92
00:07:11,880 --> 00:07:17,520
The other half of my work was sort of much more tactical in nature, so it was really building

93
00:07:17,520 --> 00:07:25,360
custom software and even kind of documentation systems for taking in a very wide breath

94
00:07:25,360 --> 00:07:26,600
of different kinds of data.

95
00:07:26,600 --> 00:07:32,080
So all the way from space-based assets and satellite imagery to signals intelligence,

96
00:07:32,080 --> 00:07:38,120
to ground sensing, to an unstructured written report from some PFC in the field and being

97
00:07:38,120 --> 00:07:45,600
able to distill all that information down in a reasonably timely way to help a sergeant

98
00:07:45,600 --> 00:07:51,200
major who is in the field and needs to know whether they go knock on the store as the

99
00:07:51,200 --> 00:07:55,560
person they're looking for going to be there, if they go inspect this shack, will they

100
00:07:55,560 --> 00:07:57,600
find the weapons that they're looking for.

101
00:07:57,600 --> 00:08:03,400
So it was a fascinating place to start my career and when I spoke about this morning

102
00:08:03,400 --> 00:08:10,280
was really part of the lessons that I learned from that with respect to how data science

103
00:08:10,280 --> 00:08:16,160
as a sort of profession and as industry can kind of get wrapped around the axle on bias

104
00:08:16,160 --> 00:08:20,320
and how to overcome that and how to help yourself as a professional data science as it really

105
00:08:20,320 --> 00:08:25,360
had to help the folks around you better leverage and use the tools that you have because everybody

106
00:08:25,360 --> 00:08:30,840
brings bias to their work and I think my experience in the intel community was one where I was

107
00:08:30,840 --> 00:08:35,440
sort of acutely aware of that because the stakes were relatively high.

108
00:08:35,440 --> 00:08:36,440
Right, interesting.

109
00:08:36,440 --> 00:08:42,920
So one of the silly questions that I had for you was, did you have to get your slides approved

110
00:08:42,920 --> 00:08:43,920
by me?

111
00:08:43,920 --> 00:08:51,760
No, no, no, so yeah, all things in my slides were sufficiently generalized that there's

112
00:08:51,760 --> 00:08:56,640
no need to do that but one thing that I did mention in the talk which is sort of interesting

113
00:08:56,640 --> 00:09:00,960
context for being a data science inside the intel community is sort of your access to

114
00:09:00,960 --> 00:09:01,960
tools.

115
00:09:01,960 --> 00:09:06,720
You know, so we, you know, we, the sort of collective community of people doing this work,

116
00:09:06,720 --> 00:09:10,960
we think about just this ready access to the latest and greatest open source tools and

117
00:09:10,960 --> 00:09:16,560
that as soon as Google or Facebook or whomever kind of open source this great new thing,

118
00:09:16,560 --> 00:09:20,160
well, let's figure out a way to play with it and we're going to get it into our workflow.

119
00:09:20,160 --> 00:09:24,360
That is absolutely not the case when the equipment that you're working, you know, literally

120
00:09:24,360 --> 00:09:28,440
the computers that you're doing your work on are classified pieces of equipment.

121
00:09:28,440 --> 00:09:36,160
And so one of the stories that I told in my talk was one where I was trying to overcome

122
00:09:36,160 --> 00:09:44,680
this motivated reasoning as an example of how that problem-motivated reasoning can actually

123
00:09:44,680 --> 00:09:49,920
be addressed through a kind of deliberate technical approach to analyzing data.

124
00:09:49,920 --> 00:09:54,520
And in this case, we were, we were looking at satellite imagery, the context here was the

125
00:09:54,520 --> 00:09:58,880
sort of never-ending search for weapons of mass destruction in Iraq, you know, the timing

126
00:09:58,880 --> 00:10:02,760
here is sort of mid-2000s, 2000s, 2006.

127
00:10:02,760 --> 00:10:07,520
And as I mentioned in my talk, there, I had the opportunity to work with these two, we

128
00:10:07,520 --> 00:10:12,800
would call image intelligence analysts who had been working for, I mean, by the time

129
00:10:12,800 --> 00:10:15,680
I got to know them, I think they've been passed their 30-year mark.

130
00:10:15,680 --> 00:10:20,280
And so these were, these were people with a tremendous amount of expertise and they were

131
00:10:20,280 --> 00:10:29,520
tasked with analyzing images, satellite images taken of Iraq and try to identify any places

132
00:10:29,520 --> 00:10:32,880
where you might see weapons of mass destruction.

133
00:10:32,880 --> 00:10:39,720
And one of the projects that we worked on, which they themselves asked for, was their

134
00:10:39,720 --> 00:10:43,640
intuition was that there were no, they were never going to find so.

135
00:10:43,640 --> 00:10:48,680
And unfortunately, because of this issue of motivated reasoning, they were continuing

136
00:10:48,680 --> 00:10:52,560
to be asked to just look and look and look at no end.

137
00:10:52,560 --> 00:10:56,080
And so we basically came up with this novel idea is like, well, why don't we try to automate

138
00:10:56,080 --> 00:11:03,440
this process so that you can, in aggregate show my automatically analyzing images and

139
00:11:03,440 --> 00:11:09,280
using very simple classification to try to identify where, oh, if there are any indications

140
00:11:09,280 --> 00:11:16,320
of suspicious activity, if that actually exists, because, you know, two men doing this on

141
00:11:16,320 --> 00:11:19,600
their own, they could be at this for the rest of their careers.

142
00:11:19,600 --> 00:11:24,320
And so the reason I bring this up, because it ultimately became a really interesting exercise

143
00:11:24,320 --> 00:11:26,480
in trying to get new tools in the building.

144
00:11:26,480 --> 00:11:32,280
So, you know, one of my greatest achievements, I think, as in this job was actually not technical

145
00:11:32,280 --> 00:11:38,360
but bureaucratic in that I was able to actually get early versions of scientific Python and

146
00:11:38,360 --> 00:11:43,080
OpenCV installed on a classified machine so that we could write a simple classifier to

147
00:11:43,080 --> 00:11:44,880
try to help these guys out.

148
00:11:44,880 --> 00:11:49,240
And ultimately, the success was that A, we were able to do that, but B, we were able

149
00:11:49,240 --> 00:11:52,280
to show an aggregate that there was really nothing there.

150
00:11:52,280 --> 00:11:56,440
And ultimately, we were able to use that as a way to dislodge these guys from having

151
00:11:56,440 --> 00:12:00,960
to continue to pursue something that ultimately they knew they were not going to find.

152
00:12:00,960 --> 00:12:05,680
And do you think that's changed at all the ability to get new technologies open source

153
00:12:05,680 --> 00:12:08,920
into these environments of a, you know, since you left that out?

154
00:12:08,920 --> 00:12:12,680
I think it's improved quite a bit, you know, and it's funny to think, I mean, 10 years

155
00:12:12,680 --> 00:12:19,080
is a long time in this sort of tech timeline, and they're, you know, large bureaucratic

156
00:12:19,080 --> 00:12:23,480
institutions like the military and the Department of Cancer are always trailing indicators,

157
00:12:23,480 --> 00:12:27,680
but one of the things that I've been very impressed by with the folks that I know that

158
00:12:27,680 --> 00:12:34,160
continue to work in this space is their sort of progression to being able to bring new

159
00:12:34,160 --> 00:12:35,160
tools in.

160
00:12:35,160 --> 00:12:40,160
But there, there's part of this, which is there I think there are better tools available

161
00:12:40,160 --> 00:12:44,800
commercially, and so that's always been an easy way to acquire new stuff, but having

162
00:12:44,800 --> 00:12:50,600
it not only in appetite, but a sort of avenue for bringing in open source tools, I mean,

163
00:12:50,600 --> 00:12:58,360
I know even, even AWS works with the Intel community now and creates, you know, distributed

164
00:12:58,360 --> 00:13:00,800
compute for them to actually use some of these tools.

165
00:13:00,800 --> 00:13:05,480
I mean, I think it's been obviously a real boon for their work, but I think more importantly

166
00:13:05,480 --> 00:13:09,400
for recruiting and retention for, you know, very talented people.

167
00:13:09,400 --> 00:13:10,400
Yeah.

168
00:13:10,400 --> 00:13:15,200
So you mentioned motivated reasoning in the context of this story of pulling technology

169
00:13:15,200 --> 00:13:22,080
into one of the agencies, tell us, you know, what that means, and spend some time talking

170
00:13:22,080 --> 00:13:26,560
about the other biases that you talked about in the year, because that was the bulk

171
00:13:26,560 --> 00:13:27,560
year.

172
00:13:27,560 --> 00:13:28,560
Yeah.

173
00:13:28,560 --> 00:13:29,560
Yeah.

174
00:13:29,560 --> 00:13:30,560
Yeah.

175
00:13:30,560 --> 00:13:31,560
So we see it all the time.

176
00:13:31,560 --> 00:13:36,880
It's basically, I ask you a question because I want you to give me the answer that I

177
00:13:36,880 --> 00:13:37,880
already know.

178
00:13:37,880 --> 00:13:38,880
Right.

179
00:13:38,880 --> 00:13:41,840
And motivated reasoning is a particularly sticky wicket when it comes to intelligence

180
00:13:41,840 --> 00:13:48,000
gathering, because policymakers will oftentimes have free-determined policy biases.

181
00:13:48,000 --> 00:13:52,200
And so your job, and as I mentioned in my talk, you know, particularly if you're working

182
00:13:52,200 --> 00:13:57,120
in civilian intelligence, you know, principally the CIA, you have one customer.

183
00:13:57,120 --> 00:14:01,000
Your customer is the president and by extension the White House.

184
00:14:01,000 --> 00:14:06,680
And so, you know, motivated reasoning can be very problematic if you have a customer

185
00:14:06,680 --> 00:14:08,960
with high levels of motivated reasoning.

186
00:14:08,960 --> 00:14:12,960
And there are a number of examples of that throughout history, you know, I spoke about

187
00:14:12,960 --> 00:14:18,880
the sort of search for weapons and mass destruction in the early part of the 2000s, but there

188
00:14:18,880 --> 00:14:20,200
are lots of other biases.

189
00:14:20,200 --> 00:14:21,200
Right.

190
00:14:21,200 --> 00:14:25,560
And I think the next stop and the one that I mentioned during the talk is this idea of

191
00:14:25,560 --> 00:14:26,560
confirmation bias.

192
00:14:26,560 --> 00:14:32,120
Which in many ways I think is sort of the, you know, the cousin or the direct relative

193
00:14:32,120 --> 00:14:37,720
of motivated reasoning, which is maybe I don't already have a predetermined policy outcome

194
00:14:37,720 --> 00:14:39,080
that I'd like to see.

195
00:14:39,080 --> 00:14:45,040
But I sure have a lot of bias in accepting, you know, analysis that confirms the thing

196
00:14:45,040 --> 00:14:46,040
that I already think is true.

197
00:14:46,040 --> 00:14:47,040
Right.

198
00:14:47,040 --> 00:14:52,800
And that is, again, this incredibly problematic lens through which to observe information

199
00:14:52,800 --> 00:14:55,880
that in and of itself is very hard to collect.

200
00:14:55,880 --> 00:15:00,640
You know, I think in the context of data science, people often talk about bias in the data

201
00:15:00,640 --> 00:15:02,120
generating process.

202
00:15:02,120 --> 00:15:03,120
And it's true.

203
00:15:03,120 --> 00:15:04,120
It's everywhere.

204
00:15:04,120 --> 00:15:08,560
You know, if you work at a big social media company, you have access to a tremendous amount

205
00:15:08,560 --> 00:15:09,560
of data.

206
00:15:09,560 --> 00:15:14,160
But some engineer along the way shows to collect that click stream.

207
00:15:14,160 --> 00:15:19,000
You know, that wasn't, that was done by a product manager who decided that they wanted

208
00:15:19,000 --> 00:15:21,680
to track a specific kind of action.

209
00:15:21,680 --> 00:15:22,680
Right.

210
00:15:22,680 --> 00:15:23,680
And that's bias.

211
00:15:23,680 --> 00:15:24,680
Right.

212
00:15:24,680 --> 00:15:29,200
So kind of intel context where there's policy decisions that need to be made off of it.

213
00:15:29,200 --> 00:15:33,680
You have an extremely limited set of kinds of information that you can get.

214
00:15:33,680 --> 00:15:38,400
And oftentimes that information is brought to you in sort of opportunistic way.

215
00:15:38,400 --> 00:15:44,240
You know, one of the, this is well after my time in the intel community, but of course,

216
00:15:44,240 --> 00:15:49,920
after, you know, after the raid, had a Salomon Bin Laden's base camp, a lot of the most valuable

217
00:15:49,920 --> 00:15:53,320
stuff that came out of that were all the laptop computers and information that they

218
00:15:53,320 --> 00:15:57,160
were like, lean there, you know, obviously that data is highly biased.

219
00:15:57,160 --> 00:15:59,480
And that was a, that was a collection of opportunity.

220
00:15:59,480 --> 00:16:03,720
There was no, there was no, like, a B test and no experiment to try to identify which

221
00:16:03,720 --> 00:16:06,040
would be the best pathway to do that.

222
00:16:06,040 --> 00:16:14,280
And so confirmation bias becomes extremely dangerous if you have sudden access to a new

223
00:16:14,280 --> 00:16:16,720
data set that you didn't already have.

224
00:16:16,720 --> 00:16:22,680
And without taking a delivered approach to analyzing it may reinforce that bias in a very

225
00:16:22,680 --> 00:16:23,680
dangerous way.

226
00:16:23,680 --> 00:16:24,680
Yeah.

227
00:16:24,680 --> 00:16:27,400
The last one that I mentioned during the talk and this is sort of where I left the talk

228
00:16:27,400 --> 00:16:32,400
and I think is something that we as a community really need to think about in the context

229
00:16:32,400 --> 00:16:41,880
of our work today now is sort of flat out denial that we, we as data scientists and we really

230
00:16:41,880 --> 00:16:46,880
as sort of technicians because I wouldn't bucket this only for people who, you know, writes

231
00:16:46,880 --> 00:16:55,600
just code, right, all folks who build tools with software have this attachment to both

232
00:16:55,600 --> 00:17:02,040
information that is biased and imperfect and tools that are, you know, sort of approximations

233
00:17:02,040 --> 00:17:06,120
of good ways of estimating what we think is occurring in the real world, the real world

234
00:17:06,120 --> 00:17:07,560
is really hard to measure.

235
00:17:07,560 --> 00:17:14,240
And so this kind of trifecta of imperfection means that everything that we do should be,

236
00:17:14,240 --> 00:17:19,320
you know, fraught with caveats and considerations for all of that stuff.

237
00:17:19,320 --> 00:17:24,640
But what that means is that we are, we accept the fact that we open the door to people who

238
00:17:24,640 --> 00:17:27,360
will try to poke holes and deny that that's true.

239
00:17:27,360 --> 00:17:32,280
Even if we can make a very confident assessment of something and we can show it to be true,

240
00:17:32,280 --> 00:17:37,680
but because of good hygiene around doing data science, it's very easy to poke holes in

241
00:17:37,680 --> 00:17:38,680
that.

242
00:17:38,680 --> 00:17:43,360
And I think there's, I think part of what we need to think about as a community is, well,

243
00:17:43,360 --> 00:17:45,400
how do we prepare ourselves for that?

244
00:17:45,400 --> 00:17:51,040
How do we get better at communicating this kind of, you know, these foibles in our work

245
00:17:51,040 --> 00:17:53,280
that we cannot pull away?

246
00:17:53,280 --> 00:18:00,640
But also, how do we get the consumers out of that information to be more willing to

247
00:18:00,640 --> 00:18:04,440
accept and more educated to a certain extent about that reality?

248
00:18:04,440 --> 00:18:10,080
I mean, one of the things that I mentioned during my talk was, you know, this, the election

249
00:18:10,080 --> 00:18:13,760
in 2016, the US election in 2016, I think it was a great example where people just had

250
00:18:13,760 --> 00:18:19,160
this expectation that, you know, you get, whoever has the higher percentage of winning

251
00:18:19,160 --> 00:18:20,160
is the winner, right?

252
00:18:20,160 --> 00:18:22,720
Because we have this kind of horse race mentality.

253
00:18:22,720 --> 00:18:27,040
But anybody who's ever been to a casino would know that if you, if there was a game at

254
00:18:27,040 --> 00:18:31,360
the casino that gave you, you know, 25% out of winning, you would never leave that game.

255
00:18:31,360 --> 00:18:32,360
Yeah.

256
00:18:32,360 --> 00:18:35,360
And that's essentially the game that we ended up playing and we just got the one in four

257
00:18:35,360 --> 00:18:41,200
chance that we didn't expect to see and I think part of that is this, you know, people

258
00:18:41,200 --> 00:18:47,120
just need to get a better understanding of, I mean, it's part of it, I think, is sort

259
00:18:47,120 --> 00:18:52,680
of becoming more numerate, but I think part of it is just we, we're responsible for that.

260
00:18:52,680 --> 00:18:56,200
The data community is responsible for kind of conditioning folks to understand that,

261
00:18:56,200 --> 00:18:58,920
but I think folks like Nate Silver do a great job and they're saying that I think there's

262
00:18:58,920 --> 00:19:02,120
a lot of success in the world and that, but we need more of it.

263
00:19:02,120 --> 00:19:03,120
Yeah.

264
00:19:03,120 --> 00:19:09,320
And it's, it's struck me that it's a bit of a fine line between, you know, denial and

265
00:19:09,320 --> 00:19:13,960
maybe it's not a fine, maybe it's a golf, but, you know, there's denial on one hand and

266
00:19:13,960 --> 00:19:16,760
then there's, you know, challenging the results.

267
00:19:16,760 --> 00:19:22,960
And I think, you know, we need to be, as a community, be open to having our results being

268
00:19:22,960 --> 00:19:27,440
challenged because the flip side of that is this idea that, you know, data and quote unquote

269
00:19:27,440 --> 00:19:28,440
AI is magic.

270
00:19:28,440 --> 00:19:29,440
Right.

271
00:19:29,440 --> 00:19:31,440
And whatever it says, that's what we got to do, right?

272
00:19:31,440 --> 00:19:35,640
And that's a whole different, just, you know, that poses a whole different set of challenges.

273
00:19:35,640 --> 00:19:36,640
I totally agree.

274
00:19:36,640 --> 00:19:39,640
And I think, you know, we are, we are right at the beginning of what I think will be a

275
00:19:39,640 --> 00:19:46,040
very interesting, you know, future, immediate future for, you know, sort of general consumer

276
00:19:46,040 --> 00:19:52,160
because it seems like the technology trend is to move in the direction of effectively

277
00:19:52,160 --> 00:19:55,000
using black boxes or solutions.

278
00:19:55,000 --> 00:19:59,840
And that means that we accept the fact that it will be very difficult to understand why

279
00:19:59,840 --> 00:20:01,960
a system makes a choice.

280
00:20:01,960 --> 00:20:07,480
And I think we are opening ourselves up to a really difficult set of circumstances that

281
00:20:07,480 --> 00:20:14,320
will very likely come sooner or later where, you know, intelligent software systems, whether

282
00:20:14,320 --> 00:20:18,960
you want to call it AI or sort of a general class of just not decision support systems

283
00:20:18,960 --> 00:20:23,600
but decision making systems that we don't understand.

284
00:20:23,600 --> 00:20:29,320
And, you know, I think the danger now is that in some sense, we've moved so quickly in

285
00:20:29,320 --> 00:20:33,280
the direction of having access to tools like this that the education component is just

286
00:20:33,280 --> 00:20:34,800
not kept up.

287
00:20:34,800 --> 00:20:38,800
And I think the flip side of this, which is in many ways a compliment to it, is I also

288
00:20:38,800 --> 00:20:41,240
think that there's an unnecessary amount of fear, right?

289
00:20:41,240 --> 00:20:46,720
I think now we have this pendulum swinging back the other direction where you have folks

290
00:20:46,720 --> 00:20:54,560
creating a kind of anxiousness around the arrival of tools like this and systems like

291
00:20:54,560 --> 00:20:58,240
this that doesn't really meet with the reality of how that technology trend is doing.

292
00:20:58,240 --> 00:21:05,440
So consumers are being conditioned now to not trust ATM machines and to not trust subway

293
00:21:05,440 --> 00:21:07,000
doors that open and close on their own.

294
00:21:07,000 --> 00:21:12,280
And I think this friction now is really, I mean, literally starting to heat up in a sense

295
00:21:12,280 --> 00:21:16,080
that, again, we're the folks that build these systems.

296
00:21:16,080 --> 00:21:19,600
And I think because of that, it's partly our responsibility to be able to go out and

297
00:21:19,600 --> 00:21:24,560
try to talk to folks and say, you know, this is how this works, you know, fine.

298
00:21:24,560 --> 00:21:28,400
We don't, I think we lack in sometimes good champions for this stuff, right?

299
00:21:28,400 --> 00:21:33,800
We have lots of really good entrepreneurs and technologists who can build this stuff.

300
00:21:33,800 --> 00:21:37,960
We need to find our champions who can actually kind of go out there and try to help folks

301
00:21:37,960 --> 00:21:42,680
understand how this is going to change their lives, you know, for good and potentially

302
00:21:42,680 --> 00:21:44,480
in ways that we don't even understand yet.

303
00:21:44,480 --> 00:21:45,480
Yeah.

304
00:21:45,480 --> 00:21:46,480
Yeah.

305
00:21:46,480 --> 00:21:49,680
I've got to ask the ATM and subway doors, a specific exam.

306
00:21:49,680 --> 00:21:53,880
No, no, I'm just thinking that you know, you hear, you hear all these, you know, a lot

307
00:21:53,880 --> 00:21:56,680
of kind of fear and certainty and doubt or stuff.

308
00:21:56,680 --> 00:22:00,000
And so, you know, actually, these would be more examples for my personal life where I'm,

309
00:22:00,000 --> 00:22:05,720
you know, asked by family members, you know, when am I going to show up to the ATM and

310
00:22:05,720 --> 00:22:10,560
it's going to be hacked or when am I going to show up, you know, autonomous vehicles going

311
00:22:10,560 --> 00:22:14,720
to turn on their own or use, you know, everybody, you know, people have seen, you know, fast and

312
00:22:14,720 --> 00:22:18,360
furious movies and they're like, is this how things are going to work?

313
00:22:18,360 --> 00:22:20,120
And I don't know.

314
00:22:20,120 --> 00:22:25,640
But I also know that that future is a little bit further away than Hollywood is presenting

315
00:22:25,640 --> 00:22:26,640
right.

316
00:22:26,640 --> 00:22:29,600
And there's a long way to go before we actually have to start thinking about that.

317
00:22:29,600 --> 00:22:35,360
I think because there is time, part of our job as a community is to try to fill that information

318
00:22:35,360 --> 00:22:36,880
gap in a little bit.

319
00:22:36,880 --> 00:22:44,560
The speaker after he said that in kind of running through similar sets of issues, more

320
00:22:44,560 --> 00:22:50,600
applied, societally, gave the audience the advice that, you know, as you're thinking about

321
00:22:50,600 --> 00:22:54,000
these systems you're building, think about the degree to which it resembles an episode

322
00:22:54,000 --> 00:22:55,000
of Black Mirror.

323
00:22:55,000 --> 00:22:56,000
Right.

324
00:22:56,000 --> 00:22:57,000
Yeah.

325
00:22:57,000 --> 00:23:01,880
I think that's, you know, if there's a useful rubric, that's probably a really good one.

326
00:23:01,880 --> 00:23:06,640
Like if you start to bleed into Black Mirror territory, and then you probably have made

327
00:23:06,640 --> 00:23:11,320
some choice that you might want to weakless enter.

328
00:23:11,320 --> 00:23:12,320
Yeah.

329
00:23:12,320 --> 00:23:14,560
And for those who don't know Black Mirror, probably the best analogy is like a British

330
00:23:14,560 --> 00:23:15,560
version of the Twilight Zone.

331
00:23:15,560 --> 00:23:16,560
Yeah.

332
00:23:16,560 --> 00:23:17,560
Yeah.

333
00:23:17,560 --> 00:23:19,040
It's a group heard of the Twilight Zone, but is, you know, the Twilight Zone.

334
00:23:19,040 --> 00:23:20,040
Very modern.

335
00:23:20,040 --> 00:23:22,400
Yeah, dealt with lots of different kinds of societal issues.

336
00:23:22,400 --> 00:23:28,080
Black Mirror tends to focus on technology, has a kind of specific or consistent thread.

337
00:23:28,080 --> 00:23:29,080
And well, I'm a big fan.

338
00:23:29,080 --> 00:23:30,080
Yeah.

339
00:23:30,080 --> 00:23:31,080
Same here.

340
00:23:31,080 --> 00:23:32,080
Same here.

341
00:23:32,080 --> 00:23:37,560
So what are the specific, you know, the top three things that you want folks to take

342
00:23:37,560 --> 00:23:41,000
away from your presentation and not just take away, but like go do.

343
00:23:41,000 --> 00:23:42,000
Right.

344
00:23:42,000 --> 00:23:46,560
I mean, I think the first one, you walk out of my talk, and you go back to your, you

345
00:23:46,560 --> 00:23:50,480
know, maybe tomorrow or this afternoon, you go back to your desk at the office.

346
00:23:50,480 --> 00:23:58,400
And sit down and think about how confirmation bias, motivated reasoning, and opportunities

347
00:23:58,400 --> 00:24:05,320
for denial exist in the current decision-making workflow in your company, your organization,

348
00:24:05,320 --> 00:24:10,280
and how your own work may be contributing to that, or helping to save it.

349
00:24:10,280 --> 00:24:14,680
And really, not to be overly negative, but, you know, I think that there is, everybody

350
00:24:14,680 --> 00:24:16,240
can think of examples of this.

351
00:24:16,240 --> 00:24:18,240
And I run a company, I can think of examples.

352
00:24:18,240 --> 00:24:25,200
I mean, I'm heavily motivated to sell my products, and that will impact my decision-making.

353
00:24:25,200 --> 00:24:32,200
And I have told my team, I have high expectation of them to step in and say, well, hold on, Drew,

354
00:24:32,200 --> 00:24:36,120
like, why are you, why do you think that that's a good choice to make?

355
00:24:36,120 --> 00:24:41,840
And this would be specifically in product building, even model selection for some of the,

356
00:24:41,840 --> 00:24:45,760
you know, algorithms that we may choose, you know, this stuff is, this stuff is not,

357
00:24:45,760 --> 00:24:46,760
you know, abstract, right?

358
00:24:46,760 --> 00:24:47,760
There's a real examples.

359
00:24:47,760 --> 00:24:48,760
Right.

360
00:24:48,760 --> 00:24:54,280
And I think the second one, you know, which is, I think, maybe more fun is, you know,

361
00:24:54,280 --> 00:25:01,440
go out and seek the first hand, you know, go try to, try to meet folks and see systems

362
00:25:01,440 --> 00:25:09,120
where this kind of intersection of bias data and decision-making exists in your community,

363
00:25:09,120 --> 00:25:15,680
you know, local government in, you know, in your local community organizing, you know,

364
00:25:15,680 --> 00:25:17,200
this stuff is everywhere.

365
00:25:17,200 --> 00:25:22,680
And one of the things that's nice about kind of professionally applying statistical methods

366
00:25:22,680 --> 00:25:29,720
and computing to this, in this context is that you can bring your own expertise to potentially

367
00:25:29,720 --> 00:25:34,120
a much smaller scale problem that impacts many, many more people's lives and allows people

368
00:25:34,120 --> 00:25:35,680
that you are your neighbors.

369
00:25:35,680 --> 00:25:41,320
And, you know, what, I've had a great opportunity in my life to work with, you know, the government

370
00:25:41,320 --> 00:25:44,480
in New York City and work with local communities there.

371
00:25:44,480 --> 00:25:47,760
And, you know, I was, I was, I was a co-founder of an organization called Data Kind, which

372
00:25:47,760 --> 00:25:54,280
really kind of grew out of this, really this question of how do you, how do you bridge

373
00:25:54,280 --> 00:26:00,200
the gap between talented data scientists and engineers and product managers and a social

374
00:26:00,200 --> 00:26:05,120
sector that has great data, really interesting problems, but doesn't have access to those

375
00:26:05,120 --> 00:26:07,240
talented data scientists and engineers.

376
00:26:07,240 --> 00:26:09,080
And Data Kind exists to do that.

377
00:26:09,080 --> 00:26:13,040
And, you know, you can go sign up on Data Kind's website, that's one way to do it.

378
00:26:13,040 --> 00:26:17,040
And the easier way to do it is just, you know, jump to a community meeting, see what,

379
00:26:17,040 --> 00:26:20,720
see what the Board of Ed is talking about, you know, they're making decisions about what

380
00:26:20,720 --> 00:26:25,120
data to collect on students, you can help, you know, help them make better choices.

381
00:26:25,120 --> 00:26:28,280
And the final one I'll say, which was the final, the sign I'll kind of call to action

382
00:26:28,280 --> 00:26:33,000
of my talk, which is, you know, if you're in a position of hiring people, which I'm

383
00:26:33,000 --> 00:26:36,520
sure many listeners are, you ought to really think hard about hiring veterans.

384
00:26:36,520 --> 00:26:44,040
I had an opportunity, as I said, to work alongside active service folks in the military.

385
00:26:44,040 --> 00:26:51,520
And there's some of the most brilliant, dedicated, you know, technically competent folks that

386
00:26:51,520 --> 00:26:54,000
even now I've ever had a chance to work with.

387
00:26:54,000 --> 00:26:58,560
And I think there are many underrepresented groups in technology and I think veterans

388
00:26:58,560 --> 00:27:01,840
is a group that people don't talk a lot about.

389
00:27:01,840 --> 00:27:10,160
And I think the transition from a career as a signal, you know, a signal analyst on a

390
00:27:10,160 --> 00:27:16,320
submarine to working as a data scientist is actually a lot narrower than certainly folks

391
00:27:16,320 --> 00:27:19,480
in that higher technologists think.

392
00:27:19,480 --> 00:27:24,560
And part of the issue is that a lot of people who come out of the service and then try

393
00:27:24,560 --> 00:27:28,680
to, you know, transition into a professional job, they just didn't even know that these

394
00:27:28,680 --> 00:27:29,680
jobs exist.

395
00:27:29,680 --> 00:27:34,680
You know, and I've worked with an organization in New York called the Iraq and Afghanistan

396
00:27:34,680 --> 00:27:39,880
Veterans of America to try to build, you know, access, you know, just to kind of make

397
00:27:39,880 --> 00:27:40,880
bring these two communities together.

398
00:27:40,880 --> 00:27:47,000
And again, you know, I think this is something that I do in New York, here in San Francisco,

399
00:27:47,000 --> 00:27:50,320
these, all these organizations exist and I would encourage folks in all their communities

400
00:27:50,320 --> 00:27:53,440
to try to work and do that.

401
00:27:53,440 --> 00:28:00,440
How do the biases that you talked about express themselves and your customers when they're

402
00:28:00,440 --> 00:28:07,120
trying to apply industrial AI and data driven decision making, what, how do you see them

403
00:28:07,120 --> 00:28:08,120
show up?

404
00:28:08,120 --> 00:28:16,320
That's a great question and, you know, we see a lot of the bias coming from customers

405
00:28:16,320 --> 00:28:22,280
at a point now where the buzz around AI and machine learning in the context of their

406
00:28:22,280 --> 00:28:23,280
work.

407
00:28:23,280 --> 00:28:29,080
So, you know, I know you're in, you're doing this series, I mean, the industry 4.0,

408
00:28:29,080 --> 00:28:34,280
you know, digital transformation, all these kind of terms, terms of art, basically what

409
00:28:34,280 --> 00:28:40,360
that means is, okay, we're not a software company, but we believe that we need to transform

410
00:28:40,360 --> 00:28:45,080
part of our business to rely more heavily on data and software, because that's what

411
00:28:45,080 --> 00:28:46,760
we think we need to do.

412
00:28:46,760 --> 00:28:50,520
But those are just words and so when you get to actual practical implications, you start

413
00:28:50,520 --> 00:28:56,760
talking to folks who, you know, their job is to make sure that the oil refinery has

414
00:28:56,760 --> 00:28:59,920
no safety issues, ever again, how do we do that?

415
00:28:59,920 --> 00:29:06,360
And because there's a potentially very large white space around what AI and machine learning

416
00:29:06,360 --> 00:29:12,280
can actually do to solve that problem, it's very difficult for that, you know, site

417
00:29:12,280 --> 00:29:17,920
coordinator or plant manager to think creatively about how to solve those problems.

418
00:29:17,920 --> 00:29:23,240
And so, the bias is that we see is, you know, particularly in the beginning, the first

419
00:29:23,240 --> 00:29:29,640
year of Ovaluvium, the bias that we saw is that when you, when you try to rely on customers

420
00:29:29,640 --> 00:29:36,280
creativity, they come back and say, I don't understand this.

421
00:29:36,280 --> 00:29:42,200
I don't, I don't see how it could possibly help my work, because there's no real tangible

422
00:29:42,200 --> 00:29:47,800
connection to all of this fancy math and compute that you want to throw at it and how this

423
00:29:47,800 --> 00:29:55,600
translates to my workforce getting home safely every day and my facility producing at

424
00:29:55,600 --> 00:29:56,600
maximum yield.

425
00:29:56,600 --> 00:29:59,760
And so I think for us, the journey that we've gone through as a company is actually trying

426
00:29:59,760 --> 00:30:05,200
to figure out how to more narrowly go after a set of, really a set of metrics and kind

427
00:30:05,200 --> 00:30:12,360
of value, values that can apply to answering that question because the denial bias, that's

428
00:30:12,360 --> 00:30:17,440
like, I'm going to poke holes in this because it's easier for me to say no than it is to try

429
00:30:17,440 --> 00:30:23,160
to do the work to understand this, you know, it's hard to, it's hard to fault the customer

430
00:30:23,160 --> 00:30:24,160
for that.

431
00:30:24,160 --> 00:30:29,240
Yeah, there, you know, our customers are folks who have worked 15, 20, 25, even 30 years,

432
00:30:29,240 --> 00:30:34,240
you know, in an oil rig, on a, you know, in a manufacturing floor and a power station,

433
00:30:34,240 --> 00:30:38,080
like software is not their job, software is my job.

434
00:30:38,080 --> 00:30:40,320
So I should be able to help them with that.

435
00:30:40,320 --> 00:30:45,600
And so how I can kind of minimize the gap in that is, is a way of minimizing that bias

436
00:30:45,600 --> 00:30:51,080
to try to help them understand and connect the dots between the work they do every day,

437
00:30:51,080 --> 00:30:54,920
the service that we can provide and then how that makes their work better.

438
00:30:54,920 --> 00:31:00,320
Interesting comments around kind of the role, the role of software, I think we go around

439
00:31:00,320 --> 00:31:04,840
ideas like software is eating the world and then, you know, you have big industrial companies

440
00:31:04,840 --> 00:31:10,840
like GE and Ford saying that they kind of committed themselves to become software companies,

441
00:31:10,840 --> 00:31:11,840
right?

442
00:31:11,840 --> 00:31:16,680
So, you know, on the one hand, I think those, you know, I guess maybe I'm a cheerleader

443
00:31:16,680 --> 00:31:20,720
for the industry and say, hey, you know, these companies, you know, every one of these

444
00:31:20,720 --> 00:31:25,160
companies need to start thinking a little bit more, at least like a software person in

445
00:31:25,160 --> 00:31:26,680
some way.

446
00:31:26,680 --> 00:31:32,080
But, you know, you're right, one of the things that has come up repeatedly in talking

447
00:31:32,080 --> 00:31:38,200
about the industrial AI in particular is how, you know, you are, you know, fundamentally

448
00:31:38,200 --> 00:31:43,880
trying to pair, you know, something is like the cutting edge of, you know, software and

449
00:31:43,880 --> 00:31:48,400
technology with, you know, someone who's, you know, been working in a particular, you

450
00:31:48,400 --> 00:31:54,240
know, working with a machine, you know, a CNC machine, you know, or a, you know, a boiler

451
00:31:54,240 --> 00:31:55,400
or something like that.

452
00:31:55,400 --> 00:32:00,120
And they know it's like they're the CNC whisperer, right, that thing makes a certain noise,

453
00:32:00,120 --> 00:32:04,000
they know that it's going to need some care and feeding and they know how to do it.

454
00:32:04,000 --> 00:32:09,320
And I think it's an interesting responsibility for us as technologists to try to figure out

455
00:32:09,320 --> 00:32:11,440
how to bridge these worlds.

456
00:32:11,440 --> 00:32:12,440
Yep.

457
00:32:12,440 --> 00:32:17,280
You know, at Aluvium, we, we, we really think about this as a first class part of what

458
00:32:17,280 --> 00:32:19,280
we're trying to do.

459
00:32:19,280 --> 00:32:24,680
No, we, we have, you know, set of company values and one that we, we really hold dear

460
00:32:24,680 --> 00:32:25,680
as an idea.

461
00:32:25,680 --> 00:32:26,680
We want to put people first.

462
00:32:26,680 --> 00:32:30,960
And I think, you know, that's, that's sort of easy to say and in the context of what

463
00:32:30,960 --> 00:32:38,120
our work, what that means is I have, I have no idea what a CNC whisperer knows and I'll

464
00:32:38,120 --> 00:32:39,120
never know.

465
00:32:39,120 --> 00:32:43,200
I will, I could, I could start today and work through a main in my career and never be

466
00:32:43,200 --> 00:32:44,200
as good as that.

467
00:32:44,200 --> 00:32:49,880
I mean, we've met people who are those, those, those folks, I mean, single, single individuals

468
00:32:49,880 --> 00:32:55,280
working in massive multinational companies that themselves have institutional knowledge

469
00:32:55,280 --> 00:32:59,040
that is probably invaluable to those organizations.

470
00:32:59,040 --> 00:33:03,160
And so to build software to support that, I think the, the admission that you have to

471
00:33:03,160 --> 00:33:07,320
make as a technologist is that you'll never know what they know.

472
00:33:07,320 --> 00:33:13,400
And so how can you build tools that shift the cognitive responsibility of that individual

473
00:33:13,400 --> 00:33:20,240
away from having to constantly be, you know, checking that CNC machine and pulling data

474
00:33:20,240 --> 00:33:26,200
off of it so that it knows exactly that series of vibrations or heat or spin that may indicate

475
00:33:26,200 --> 00:33:34,160
imminent failure to providing them with information in a timely manner that draws them to that,

476
00:33:34,160 --> 00:33:38,080
you know, opportunity to make a choice, to make a decision.

477
00:33:38,080 --> 00:33:40,120
And then they can get back to the work that they're really good at.

478
00:33:40,120 --> 00:33:44,000
And I think in some sense, it's just a matter of increasing cognitive margin.

479
00:33:44,000 --> 00:33:48,560
You know, there's, there's this, you know, this, this upward slope of data, right, constantly

480
00:33:48,560 --> 00:33:49,560
everywhere.

481
00:33:49,560 --> 00:33:52,560
And I think in the industrial space, which is, is largely not talked about outside of

482
00:33:52,560 --> 00:33:59,120
those kind of professional conferences is just as steep or, or more, you know, but the

483
00:33:59,120 --> 00:34:04,160
labor dynamics in those, in those industries are, you know, fixed or shrinking.

484
00:34:04,160 --> 00:34:05,160
Yeah.

485
00:34:05,160 --> 00:34:11,480
And so now you have this, this very problematic asymmetry between humans having to understand

486
00:34:11,480 --> 00:34:16,400
and, and deal with data and make decisions and systems that are just drowning them.

487
00:34:16,400 --> 00:34:17,400
Yeah.

488
00:34:17,400 --> 00:34:18,400
And information.

489
00:34:18,400 --> 00:34:23,200
And for us, he's the simplest way to bridge that gap as well, let's leverage that expertise

490
00:34:23,200 --> 00:34:29,480
that they have so that that, that, like, shifting of cognitive responsibility for the routine,

491
00:34:29,480 --> 00:34:37,080
boring, observational stuff can go to the computer and the, you know, imminent high value need

492
00:34:37,080 --> 00:34:41,640
to know now decisions can go to those experts and leverage that and really stitch that stuff

493
00:34:41,640 --> 00:34:42,640
together.

494
00:34:42,640 --> 00:34:43,640
Right.

495
00:34:43,640 --> 00:34:44,640
Right.

496
00:34:44,640 --> 00:34:45,640
Awesome.

497
00:34:45,640 --> 00:34:49,200
Sure, folks, to check out what you're doing, try to get down and engage with you.

498
00:34:49,200 --> 00:34:50,200
Yeah.

499
00:34:50,200 --> 00:34:56,720
So, you know, websites alluvium.io, that's A-L-L-U-V-I-U-M.io, alluvium.

500
00:34:56,720 --> 00:35:01,960
The easiest way to engage with me, you know, I'm, I'm on Twitter at Drew Conway, C-O-N-W-A-Y.

501
00:35:01,960 --> 00:35:04,400
I'm, I'm pretty easy, I'm a pretty easy Google.

502
00:35:04,400 --> 00:35:09,640
So, you know, I, I, I, I, I answer, I answer my emails, I like to, I like to engage with

503
00:35:09,640 --> 00:35:10,640
folks.

504
00:35:10,640 --> 00:35:15,360
So, if you have any questions about the company, we are, we are hiring as I think everybody

505
00:35:15,360 --> 00:35:16,360
is.

506
00:35:16,360 --> 00:35:20,520
So, you know, for us, the, the nucleus of, of our company is really around data science

507
00:35:20,520 --> 00:35:26,920
and engineering, but I think with a specific bent on streaming data and, you know, semi-supervised

508
00:35:26,920 --> 00:35:28,000
and unsupervised learning.

509
00:35:28,000 --> 00:35:31,880
So, if that's the kind of stuff with, you know, high volumes of streaming data that get

510
00:35:31,880 --> 00:35:33,720
you excited, we'd love to hear from you.

511
00:35:33,720 --> 00:35:34,720
Great.

512
00:35:34,720 --> 00:35:35,720
All right.

513
00:35:35,720 --> 00:35:36,720
Well, thanks so much, Drew.

514
00:35:36,720 --> 00:35:37,720
Thank you, Sam.

515
00:35:37,720 --> 00:35:38,720
It was a lot of fun.

516
00:35:38,720 --> 00:35:40,720
All right.

517
00:35:40,720 --> 00:35:45,840
All right, everyone, that's our show for today, thanks so much for listening and for your

518
00:35:45,840 --> 00:35:48,680
continued support of this podcast.

519
00:35:48,680 --> 00:35:53,400
For the notes for this episode, to ask any questions, or to let us know how you like

520
00:35:53,400 --> 00:36:00,880
the show, leave a comment on the show notes page at twomolei.com slash talk slash 39.

521
00:36:00,880 --> 00:36:05,280
Thanks again to our sponsor for the Rangle Conference Series Cloud Era.

522
00:36:05,280 --> 00:36:10,600
To learn more about Cloud Era and the company's data science workbench family of products,

523
00:36:10,600 --> 00:36:18,240
visit them at cloudera.com and be sure to tweet to them at at Cloud Era C-L-O-U-D-E-R-A

524
00:36:18,240 --> 00:36:21,400
to thank them for their support of this podcast.

525
00:36:21,400 --> 00:36:26,360
If you're interested in joining our meetup, you can register for that at twomolei.com slash

526
00:36:26,360 --> 00:36:33,040
meetup and don't forget to sign up for the newsletter at twomolei.com slash newsletter.

527
00:36:33,040 --> 00:36:43,040
Thanks again for listening and catch you next time.

