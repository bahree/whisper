Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington, a bit about the show you're about to hear.
This show is part of a series that I'm really excited about in part because I've been
working to bring it to you for quite a while now. The focus of this series is a sampling
of the really interesting work being done over at OpenAI, the independent AI research lab
founded by Elon Musk, Sam Altman, and others. A few quick announcements before we dive into the show.
In a few weeks we'll be holding our last Twimble Online Meetup of the year. On Wednesday,
December 13th, please join us and bring your thoughts on the top machine learning and AI
stories of 2017 for our discussion segment. For our main presentation, formal Twimble Talk guest,
Bruno Gensalves, we'll be discussing the paper, understanding deep learning requires rethinking
generalization, by Shi Wan Zhang from MIT and Google Brain and others. You can find more details
and register at twimlai.com slash meetup. Also, we need to build out our 2018 presentation
schedule for the meetup. So if you'd like to present your own work or your favorite third-party
paper, please reach out to us via email at teamattwimlai.com or ping us on social media and let us know.
If you receive my newsletter, you already know this, but Twimble is growing and we're looking
for an energetic and passionate community manager to help managing grow programs like the podcast
and meetup and some other exciting things we've got in store for 2018. This is a full-time role
that can be done remotely. If you're interested in learning more, reach out to me for additional
details. I should mention that if you don't already get my newsletter, you are really missing out
and you should visit twimlai.com slash newsletter to sign up. In this episode, I'm joined by Dario
Amadeh, team lead for safety research at OpenAI. While in San Francisco a few months ago, I spent
some time at the OpenAI office during which I sat down with Dario to chat about the work happening
at OpenAI around AI safety. In our conversation, Dario and I dive into the two areas of AI
safety that he and his team are focused on, robustness and alignment. We also touch on his research
with the Google DeepMind team, the OpenAI Universe tool, and how human interactions can be
incorporated into reinforcement learning models. This is a great conversation, and along with the
others in this series, this is a Nerd Alert Worthy show. A quick note before we jump in,
support for this OpenAI series is brought to you by our friends at NVIDIA, a company which is
also a supporter of OpenAI itself. If you're listening to this podcast, you already know about NVIDIA
and all the great things they're doing to support advancements in AI research and practice.
What you may not know is that the company has a significant presence at the NIPS conference going
on next week in Long Beach, California, including four accepted papers. To learn more about the NVIDIA
presence at NIPS, head on over to twimlai.com slash NVIDIA and be sure to visit them at the conference.
Of course, I'll be at NIPS as well, and I'd love to meet you if you'll be there, so please
reach out if you will. And now on to the show.
All right, everyone. I am here at the OpenAI offices, and I am with Dario Amade. Dario is a team
lead for the Safety Research team here at OpenAI. Dario, welcome to the show.
Thanks for inviting me. Absolutely. It's great to have you on. So why don't we get started by
having you tell us a little bit about your background and how you got interested in AI
and AI safety in particular? Yeah, so actually, my background was in computational neuroscience.
I did a PhD in kind of biophysics and computational neuroscience. I've always been interested in AI
and how intelligence works, but I felt like a lot of our AI systems 10 years ago weren't working
that well, and so I decided I wanted to study the brain, and then of course the deep learning
revolution came round, and I looked at it, and I said, oh, these systems are actually starting
to work. I want to be part of this. This now seems like the most interesting thing, so I ended
up working in Andrewings Group at Baidu for about a year, and then I worked at Google for about a
year, did a variety of stuff, speech recognition, natural language processing, and then
then I came here, and the way I got into AI safety was one of the things I noticed about
neural nets is there's this mixture of, they're very powerful, but they can be very opaque and
unreliable. When I was developing speech recognition systems, you train a system with an American
accent, and then it hasn't been trained in a British accent in speech, you give it some British
accent in speech, and it gets totally confused. This mixture of power, opacity, and very unpredictable
failures and weakness is kind of what made me think that we need to be careful to make sure
that these technologies do what we want them to do and don't do something unpredictable or even
dangerous. Nice. So maybe we can get started by having you give us a little overview of
kind of the research that you've been working on, like the broad brushstroke of the research that
you've been taking on. Yeah, sure. So I would say there's kind of two general areas that I think
about when I think about safety. One of them is what you might call robustness, which is the problem
that you have when you train a machine learning system, like a neural network on some problem,
like speech recognition or self-driving cars, and then you put it to work in some actual context,
and the distribution of inputs that you're facing changes in some way, for instance, the change in
the change in the speech accents, the change in conditions of a self-driving car, or if you have
a reinforcement learning agent exploring the environment, what it sees changes. So dealing with
that, and we've done a little bit on that, we've done kind of more on sort of the second thing,
which is what we call kind of like alignment with human goals. So I think a big concern, particularly
as AI systems get more and more powerful, is making sure that the AI systems have a sophisticated
high-level understanding of what humans want them to do, and that we can already see examples today
where AI systems, it's much easier to have a simple goal than it is to have a complex goal,
and I can get into a little more detail of why that's true for today's AI systems,
and that kind of really sets the stage for systems to kind of go off and kind of
maniacally pursue very kind of pathological simple goals. And I think we should instead have AI
systems that are as good at understanding what humans want them to do as they are accomplishing
what, you know, whatever tasks they've decided, it's important to do.
I think one of the best examples of that was from, I think it was some of the initial work that you
did with Google on this, where the example was you've got a robotic housekeeper, and you do it
to clean the room. Like, you don't want the robot sweeping all the dust under the rug.
Yeah. Yeah. Yeah. Yeah. Yeah. I mean, it's kind of an example of what we call or this term
from economics, actually, good-hard slot, which is that once a metric becomes a target, it ceases
to be a good metric. What that means is if you have a way of kind of passively measuring something,
it seems good, but then if you optimize it really hard, you might get something that, you know,
that you don't expect. So actually, you know what?
It isn't that all we're doing in AI is like optimizing it around a metric really hard.
Yeah. That's the problem. Yeah. I'm not going to argue with that. So we actually have,
since I came to open AI, we got kind of an even more vivid example of this that we got to
occur in an actual AI system. So, you know, we were kind of playing with training simple video games,
and you know, we had this boat race game where, you know, you're trying to complete a course
with a boat. And the only kind of easy way we have of measuring progress, again, the only simple
way we have of measuring progress is it gets these points when it knocks down these targets along
the course. You know, so kind of naively looking at it, you know, it's something like, well,
you know, it has to knock down these targets to complete the course. So okay, we're, you know,
we're going to give it, give it train reinforcement learning system, give it points for knocking
down the targets. And so great, it'll complete the course. And then I just, I just, you know,
I just set it in motion, didn't do anything for, you know, 24 hours. And then when I came back,
and I looked at it, the thing was going around in circles because it found this lagoon where it could
just get the maximum possible density of points. And, you know, of saying that example. Of course,
you can say, well, I don't know, you get what you ask for. The game was just broken. But I think
the difficulty is that the mapping between what we think we're likely to get when we set the training
process in motion and what we actually get, it's very discontinuous. It's not, you know,
mysterious or magical anything. You do, indeed, get what you ask for. But what worries me is this
kind of, you know, very, very unpredictable mapping between here's what we think we're trying to
get a system to do. And here is what the system actually actually ends up doing. And in retrospect,
of course, it made sense that it did that. But perspective predictability is a property that I'd
like our our systems to have. And that I believe they currently don't have.
So one of the first things that you did around this research was to publish a paper again. It's
the same paper I'm referring to with, I think it was in partnership with Google and another
organization. I forget the name of you essentially outline a set of rules like or do you think of them
as rules or goals or kind of general general research areas. So just to make sure there's kind of
two kind of major papers that we did in the last about year and a quarter. The first one was I did
while I was still at Google and it was done in collaboration with Google OpenAI Stanford Berkeley.
And this was kind of this agenda paper that kind of outlined various directions for for AI
safety. And then more recently about I think was what three three months ago now we published this
paper called Learning from Human Preferences, which was kind of our you know research kind of
attacking some some subset of those those problems. So it's kind of like that that paper from a
year ago was kind of the agenda doc that that laid out. And that's the one I'm referring to. Yeah,
you dig into the next one. Okay. Okay. But maybe you can take a second to kind of if you remember,
like working through that those points in the agenda because they're almost like, you know,
in some way is it fair to think of them as like azamaz new laws for the the neural network age or
something like that. A little bit. Yeah, I mean, they're in a sense trying to solve the same problem,
although it's less things a specific machine should should do as kind of areas of research where,
you know, to address particular types of problematic behavior that that could arise.
There were kind of five research areas that that we talked about and they were kind of divided
into into two topics similar to this kind of, you know, robustness versus value alignment things.
So, you know, kind of the schema we used was, you know, you have you have a machine learning system,
you want it to do something and something goes wrong and it does something other than what what
you intended it to do. Where do things go wrong? Things to go wrong because you had the wrong
objective function and you optimized it really hard. Or things to go wrong because you had the
right objective function, but something about the way you trained it went wrong. So that would be
like, you know, the self-driving car that's put in a new environment or, you know, the kind of robot
robot helicopter that, you know, is trying out behaviors and then destroys itself and, you know,
they kind of wasn't in the algorithm. So on the first one, the problem we talked about the
first one was reward hacking. So this is kind of the thing we demonstrated in the boat race where,
you know, you have you have this this measure, you optimize it really hard and you just you get
the wrong thing. The second one in that area that we talked about was what we call negative side
effects. So the idea is, you know, the world's the world's really big and basically it's a bit
similar to reward hacking, basically any simple objective that I can come up with probably refers to,
you know, very small set of things, right? So if I ask you to move this chair, I'm implicitly
not referring to just every other thing under the sun that that could be in the world and so,
right without breaking that window. Yeah, but by default, I'm kind of like not specifying all this
common sense stuff. So, so you might say side effects are really bad by default in AI systems.
And so, yeah, that was that was kind of the second the second problem in that in that category. So
those two are kind of like broad reasons why, you know, I picked out an objective function,
seemed innocent, seemed good and something went went very wrong with it. The third problem,
which is kind of between the categories is a thing we called scalable supervision, which,
you know, we dealt dealt with that some in the later paper, which is, you know, even even if I know,
you know, if I, if I were there for everything in AI system did, even if I could coach it and
tell it to do the right thing, if I could supervise every decision that it made, you know, make,
make sure it never got out of my sight and never did anything, I don't really have the bandwidth
to do that. How do I handle situations where I kind of know what I want the AI system to do,
but I have very, very limited, it's, it's not feasible for me to have more than limited
interaction with it. How do I handle that? That situation. So those were the three on the kind of,
like, you know, how do I get the right objective function? And then I have the right objective
function. How do I, how do I make sure something bad doesn't happen? The two problems were safe
exploration. So safe exploration is the idea that, you know, I have some objective function like
fly a helicopter. And even if that's right, even if I've set my system up in a way so that if I
give it enough time, learns how to fly the helicopter right, in the real world, if I have a robot
helicopter and it crashes, maybe it breaks and it is never able to fly again. So I have this
kind of expensive at the very least. Yeah, you have what's called an asymptotic guarantee that
you eventually get the right behavior, but it doesn't help you if you die before, before you get,
you get to the asymptotic limit. And, you know, so that's an area that's, it's gotten some attention
in machine learning, machine learning already, but I think as with many things, you know, kind of
neural nets and kind of very powerful policies and new tasks have just come along in the last three
or four years. And so the safe exploration literature is, you know, just kind of catching up to this.
And so one of the things we were saying in the paper was, you know, there's, there's an urgency
to making sure that, you know, the safety, the work on safety issues catches up with the work
in other areas that's kind of kind of hurtling ahead. Right. And we kind of, we kind of already
seen it. I mean, the one example I give is, you know, at OpenAI, we had this tool called universe
we were using for a while that lets you basically, you know, have a reinforcement learning system
that connects to the web and has as its kind of actions and abilities seen the screen moving the cursor
and clicking on it and anything. Reinforcement learning systems when they when you initialize them
tend to explore randomly. So the first time I ever trained a reinforcement learning system on
universe, it immediately opened up Chrome, right clicked, opened the Chrome developer tools kit,
changed the code in there, closed it, crashed Chrome and caused some kind of segmentation fault on my
computer. The first time, you know, just, just random behavior. So if your environment is complicated
enough, you really, really have to think about these issues. Yeah. You know, I could say the same
thing about, you know, Google's, Google's done some work on, I think it's, it's well known now
optimizing data centers and, you know, using reinforcement learning to, to optimize data center
energy usage and, you know, of course, there, there's some knobs you could turn there that would
break the data centers. And so that is an issue that I think they've had to think about as well.
Right. And then kind of the last issue was this distributional shift thing, which I, you know,
kind of alluded to a number of times and, you know, has to do with, you know, the environment
in your self-driving car changes or, you know, the, you train your, your speech system on one accent
and you change it to another accent. One situation which it came up was the infamous Google
guerrilla incident, you know, Google photos tagged some, some African-American individuals as guerrillas
and it was kind of a problem with the training set that, you know, the, the training set had been
awaited towards Caucasian individuals and so, you know, it got confused. It had no idea what,
what racism was about or, or any of these things. So here are failures of kind of already happened
and how, how can we better about, be better about machine learning systems kind of knowing,
knowing what they don't know. So that's kind of the overview of the, the problems. And so the,
the, the second paper is one that's kind of diving into kind of the first few of these problems.
Yeah, yeah, a couple, a couple of the first problems. So, you know, as with most things, we wrote this
kind of grand agenda and then we were like, oh, you know, we have so many ideas for how we could
work on any of these, which, which should we start on? So the thing we eventually settled on was,
you know, the kind of the reward hacking and scalable supervision stuff, having, having the wrong,
the wrong objective function and, you know, with, with that, what we wanted to think about was,
well, if you're trying to learn an objective function that's in line with what humans would want,
then you should probably learn that objective function straight from humans, right? Because,
you know, if you have some kind of hard to explain the static thing that, you know,
that it's hard to encode into a system, instead of trying to encode it, you kind of let the
human be the teacher, right? And in some sense, whenever we do supervised learning, we're doing
that a little bit, you know, a human has to label all the images to tell us, well, you know,
this is, this is what a duck is, this is what an ostrich is. But it hasn't been done very much in
the setting of reinforcement learning and kind of that's the setting that, you know, I think,
has both the most promise and that we should worry, worry the most about, right? Are you saying letting
the human be the teacher necessarily by observation or by the human communicating a set of rules?
By the human communicating. So, I'll kind of explain a little bit how it works and how we set it
up in our paper. So, for those who don't know, the usual setup with reinforcement learning is,
you know, you have kind of an agent that's interacting in an intertwined way with environment. And,
you know, you have some notion of rewards. So, you know, in the deep mind system that played,
played go, it's, you know, did you win or did you lose in the tarry? It's the score. And because
you're training the system, you know, by trial and error for, you know, many millions of iterations
through, it's actually very important in the way we currently train the system to have a kind of
programmatically evaluable reward function, right? So, you know, with go, if I had to have a human
look at the end of every game and say, did you win or did you lose, it would be very hard, right? It's
it's very important that I can just write a little script that says, yep, I have more territory
than you, therefore I won, you have more territory than me, therefore you won. And so, in general,
these systems are trained without much human, human intervention because there isn't time for,
and what that means is that the goals have to be something you can write in a simple program.
And so, the way we changed this was, you know, what if you have a human every once in a while
give some feedback on what the right goals are and whether the system is behaving in the way
that it should behave? So, the idea is, you know, with my reinforcement learning system, I take out
the reward function and I replace it with a model of what the right thing to do is that is trained
from a human. So, concretely, the way it works is, you know, I have my reinforcement learning system,
it starts out by acting randomly as all reinforcement learning systems do. And then every once in a
while, I take two clips of its behavior, just two randomly selected clips, give them to a human,
and the human takes a look at them and says, okay, this is a little more like what I want than that.
And, you know, the human has in mind some behavior that it wants the AI system to engage in. At the
beginning, neither of the two video clips is going to be particularly good. But, you know, the
humans like, yeah, this is a little more like what I want. Then, you know, the AI system has kind of
a reward predictor that it trains that that models the human choices. It tries to come up with a
reward that's consistent with what the human chooses. It goes off and optimizes that for a while.
Then it presents the human with two more video clips and the human says, this is better than that.
Updates the systems model and then the system goes off and, you know, plays around in the environment
and tries to achieve that goal better. So, instead of kind of careening off, you know,
optimizing a particular goal really hard, you have this interplay where the system optimizes a goal,
comes to the human and says, am I going in the right direction? The human, you know, reinforces it
or corrects it and then it optimizes some more. And so via this interplay, you can make sure that
you're kind of gradually training the system that stays in the direction the human wants it to go.
Well, at the same time, you know, every time the human interacts, they're kind of imparting,
you know, a little piece of what they have in mind to the AI system.
So can I jump in with a couple of questions here? So if the only source of, you know, data for
the system to optimize around is the input it's getting from the human. Yeah.
Like, what's it doing between the times that it's getting input from the human? And like,
does it work if you just compress that and take it out? Yeah. No, it actually doesn't because
if you kind of think about, you know, so maybe good to have an example. So let's say of a video game
where you're kind of trying to shoot spaceships and keep them from shooting you. So there's two
different parts to it, right? There's understanding that the goal of the game is to shoot the spaceships
and not be shot by enemy spaceships. And there's, there's the actual mechanical dexterity to find
the spaceships, shoot them and avoid them. So we can start to learn how to do that stuff. Yeah. So,
so basically what the system needs to do is it needs to figure out from you what its goal should be,
namely shooting the spaceships. And as it figures out that goal, then it needs to on its own,
you know, practice in the environment in order to learn how to achieve that goal. In practice,
it's a little bit more of an interplay, you know, it first, it understands that, you know,
it should be avoiding these shots that are coming down. And then it kind of goes off and
optimizes that. And then it gives you some some trajectories where it avoids the shots, but
doesn't actually score any points. And you're like, oh, this isn't any good. And then every once in
a while, it accidentally scores a point and you're like, yeah, yeah, you should do more of that.
Then it kind of figures out, oh, I need to not just avoid the shots, I need to actually shoot,
shoot the enemies. And then it takes some additional time to figure out how to do that. So it's
an interplay between, you know, the system learning what you want and the system learning how to
achieve what you want. And kind of that's why there's that interplay. So, you know, you only ever
see about, you know, in our paper, it was about 0.1% of what the system actually does. We trained it
on, you know, tasks like Atari, you know, Atari is a common benchmark, Atari games or a common
benchmark for reinforcement learning. So, you know, we trained it on, we trained it on some Atari
games and the human only had to see about 0.1% of what the actual agent saw, which is good because
the agent sees, you know, days of experience or so and we don't have time to, you know, to have
human sees, see all of that. Okay. What is the human evaluating? And, you know, in a simple game,
like the spaceship game you described, I'm imagining, what is the human evaluating the two frames on?
Like, is it just looking at the score? Is it a proxy for, you know, some way to detect the score?
So we link out the score because it's a little bit of a confounder. But, you know, we just,
we just say to a human, we're kind of in some ways trying to develop a pipeline such that
you could take a task and actually farm it out to humans who understand the task. So,
we blank out the score and, you know, we just, we gave it to some contractors and we said,
this is a game where, you know, you're trying to shoot enemy spaceships and avoid getting shot
by enemy spaceships. You know, you're going to see two video clips and, you know, click on the
video clip that you think does a better job of that, that particular one. So their video,
then that stills that out. Yeah, there, there are video clips that are a couple seconds long.
And so, you know, for many tasks, we're able to get, get a good sense of that. We were also able
to do tasks where, you know, there's no, no, so for Atari, you know, there is a score that you
could learn from and our point was that, you don't need it, you could learn without it.
There are also tasks, particularly kind of, you know, tasks, you know, like, that simulated robot
animals do where, you know, you want them to perform some trick that it's really hard to describe
mathematically, but that a human can recognize. So we, we taught kind of like little, little simulated
robot walkers to do kind of back flips in front flips. You know, we taught like walkers with two
legs to kind of like balance on their hind legs or, you know, do ballerina moves or, or things
like that. And so, you know, the, the exciting thing is usually if you want to train something with
reinforcement learning, you have to say, okay, what's the behavior I want? How do I write a mathematical
function that assesses whether that behavior was right or not? Whereas here, it's just, okay,
what's the behavior I want? Let me look at some video clips and, and try and reinforce that
behavior, which I think, you know, requires more human labor, but particularly if we farm it out,
it allows you to do a much wider variety of tasks. Yeah. Have you looked at,
it sounds like you're pulling two random clips. Have you explored like, selecting the clips based
on maximal distance or information or something? Yep. Yep. No, that's, that's a really great question.
So we actually did include that in the paper and it helped a little. So we actually,
instead of having kind of a single predictor of the human, the human preferences from the data,
one of the things we tried was having ensemble of three predictors that are trained on
subsets of the data. So this is kind of, you know, common statistical validation technique. And
then you look at cases where different predictors disagree with each other about which clip,
which clip they think the human would think was better. And that disagreement is a good proxy for,
oh, this is a hard case. This is a hard case to figure out. So then we kind of mine for hard
cases and preferentially present the human with hard cases instead of easy cases, right? So in
the spaceship example would be like, you know, you've really, you know, let's say, you know,
let's say the agent has really figured out that, you know, if you get shot by the ship, it's,
it's really bad, right? But then there are some intermediate cases where, you know,
the ship's laser is shooting at you and it almost hits you versus the one where it actually does
hit you. And like, you know, maybe your predictor hasn't quite, hasn't quite figured out the difference
between those then. And so you really want to show those to the humans. The human can, can
disambiguate. And we found that that indeed did speed speed things up, you know, on more tasks
than it, than it slowed things down. It wasn't a huge improvement and we're actually looking for
kind of more, more intelligent ways of doing this because, you know, I mean, that, that's really
getting into having systems be active and ask us questions about the things they're confused about
instead of receiving passive information about what we think they're, they're confused about.
So, you know, I think, I think in the future, that's going to be a big part of it that, you know,
we want such teaching to be a dialogue between, you know, the machine and us just like it would be
a dialogue between the teacher and the student or a parent and the child or, or, or something like that.
Now, the other interesting thing that jumped out at me in describing and in you describing
that last example is like the ambiguity from the humans perspective. Like, if the laser just
misses the ship, is that like, oh, bad because the ship was too close to the laser? Oh, that's
awesome. The ship was able to like, avoid the laser, you know, that it was really close. Like,
have you characterized that part of the problem at all? Yeah, I mean, I think there were some kind of
technical issues we ran into, which is, you know, how do you define goal of the task, right? So,
another example, like the example you gave is, you know, what if, what if your spaceship just
in a particular few second clip doesn't, doesn't encounter any enemy spaceships at all? So in other
words, it doesn't see anything. There's nothing, there's nothing it has to do. Is that good behavior
because it did what it was supposed to do, which is nothing? Right. Or is it bad behavior because
it's worse than a clip where it actually shot, shot the spaceship? In the formal reward formalism,
you're basically supposed to say that that's worse behavior because, you know, it didn't, it didn't
get the reward as opposed to the behavior where there was, it was a ship and, and did get the reward.
But of course, if I'm a human thinking about it, it's kind of, it's kind of ambiguous. And so,
we kind of tried to give the instructions in, you know, in such a way as to kind of, you know,
resolve, resolve those ambiguities. You know, we wanted to focus on, you know, can we incentivize
this behavior with clear one or two sentence instructions? But I think, you know, and the technical
description of it is, you know, there's a difference between the reward, which is the immediate reward,
you get the return, the advantage, which is how good a particular action is. So, you know, which
of these technical concepts and reinforcement learning do we actually want humans to reinforce?
And there's a question of how easy is it for humans to understand these different settings versus
how much does it, does it help the algorithm? So, these are all kind of things that like, you know,
have been explored a little bit in the literature, but again, you know, never, never with the level
of difficulty of tasks. So, this is kind of one of the areas that that we want to explore more.
And we consider really unexplored, right? This was really just a first paper. And, you know,
all these are like really great questions that we've, you know, we kind of ran into and thought
about some, but you definitely haven't fully answered. And so is the plan to, like, you kind of peel
off two of those, is the plan to go even deeper on those two or to peel off another one? So, you
know, I think this direction we're pretty excited about. And so I think we're going to do a lot of
things, you know, that kind of follow up on this human preference learning, you know, we see
applications to robotics, tasks like, you know, time, particular types of, of knots in rope. You
know, it's another task where the reward function is very hard to describe and people try and
specify it, but I think that that may be the weak link. So, you know, maybe, maybe we can do things
like that, things that physical robots do in the world, things like dialogue system, you know,
Microsoft made this dialogue system called TA that ended up kind of spewing racist nonsense,
you know, whether, whether a comment is offensive or not is precisely the kind of high-level
aesthetic concept that you can't learn really well with a simple objective function, you know,
so could you train a restricted dialogue system to, you know, to understand what, you know, racist
or sexist or offensive comments are and never make them. You know, some of that is simple,
it's just not using certain words, but, you know, but the concept of something being offensive
is also more subtle, right? I can say something that, you know, that doesn't use any offensive words,
but the content of what I'm saying might, might, might still be offensive. So, you know, can we,
can we, can we train a system to understand those, those distinctions? Actually, even things like
safe exploration, what it means for exploration to be safe, maybe that's something that we could
learn through human, human preference learning. There's been a lot, so there's, there's been a lot
of work on kind of learning, learning to learn and so you could imagine kind of learning to learn
safely. So, if I'm a human and I am learning to play an Atari game, right? You could think of
copying what the human does, but you could also think of copying the human's process of exploring
the Atari game, right? And there are different ways to explore, right? I can explore recklessly,
doing a lot of things with trial and error, or I can explore very carefully, making sure that nothing
bad ever happens. For instance, if I'm a human, you know, remote controlling a helicopter,
I'm not going to just try random things. I'm going to, you know, kind of gingerly try a few
things at first that you don't make the helicopter crash, particularly if I think that it'll break,
if it crashes, you know, and then, then I'll kind of gradually get more bold. And so, can we,
can we train machine learning systems to learn in the same safe way that humans learn? And could we,
could we do that by giving them feedback or examples or demonstrations of how, of how humans learn?
And, you know, then, then could we solve that, that problem? That way, we've also thought about
kind of richer, richer forms of human feedback. So, right now, it's just like is left better or
right is better? Sometimes, what I'm training these, you know, one of those systems, I'm just like,
I wanted to, I want to tell you that that thing you did is really, really good. And all I can do is
just just give that one, let's click, but like really, really, this was the great thing you did.
When you're giving the example of the models doing back flips and things like that, I was thinking
like, you need the Olympic panel with the 10s and the 9s and 8.5s and all that. So,
scalar, having a scalar dial is one thing. Ultimately, I just like to be able to give linguistic
feedback. I'd like, you know, I'd like to be say on this one, nice job. Do that again or nice job,
but go a little to the right. You know, nice job. That's only the first part of the move after you
do this. You need to do a flip. Right. And, you know, currently, our natural language processing
isn't really up to that task, but that's kind of a kind of long-term goal we'd work towards.
And, you know, the real long-term vision would be you have an AI system that is persistent in the
world is doing things on your behalf. And then you make sure that it always does the right things
instead of the wrong thing by this continuous dialogue between you and it where you teach it,
what you want it to do. And, you know, it gives you information and executes the things that
it understands that, you know, that you want it to do. And we've been talking mostly about
explicit feedback. Are you also thinking about like the explicit versus implicit feedback might
play into this? What do you mean by implicit feedback? So, for example, you know, maybe the
the TAY example. And all of the examples we've been talking about, you know, you are telling the
system, you know, this is good. This is bad. And we started with kind of this binary, you know,
good bad. And we talked about, you know, scalar, continuous, you know, degrees of goodness and
badness. But it's still like you're telling it explicitly. And I guess I'm wondering if there are
ways to either, you know, maybe I'm thinking also of like some of the emotional intelligence,
like pick up from your reaction. Like someone reacts in a way where they don't like what I'm doing,
but you know, they're not going to explicitly say, you know, that was a bad thing. And I guess at
some point, you know, it's all, it's all numbers, you know, feeding in some models. So, yeah. So,
I mean, I think the natural language feedback is a little bit getting it. It's kind of starting
to get at that, right? Where, you know, I mean, if, you know, people have kind of, you know,
people have ways of, you know, like, politely disagreeing or politely giving, giving negative
feedback. And those are, you know, patterns that that a machine learning system could
in principle pick up on at the same time. You know, I think in the end, if we ever want something
like that to happen, we're going to have to go kind of beyond just receiving feedback from
humans to actually having models of humans. So a human says something. And the AI system says,
okay, here's my picture of the human. And here are my hypotheses about why the human would say that
particular thing. And, you know, one of them is that it doesn't, doesn't want me to do this
thing. It wants me to do something else. And so, you know, that, that gets to kind of, you know,
modeling of other minds, right? Theory of mind is something we talk about a lot, a lot of neuroscience.
So, you know, this is, this is one of these things that I think AI systems will be able to get AI
systems to do that once they have some kind of rudimentary theory of mind. You know, I mean,
even you're saying that kind of makes you think of all the complexities like the model trend on
the Brit could not be used to interact with the American, right? Yep. Yep. But on the other hand,
you know, simple, simple animals have, you know, theory of mind, right? If you have, if you have pets,
you know, dogs can do this, even mice can do this. They have very complicated pictures of,
you know, how, how the humans around them work. They, they need to in order to, you know,
in order to survive. They need to know when humans are going to feed them. They need to know when,
you know, humans, humans are going to be, going to be unhappy with them. So, you know, I think
there's some hope of, you know, building systems that have some understanding of that short of full
human level intelligence. Yeah. Oh, cool, cool. Anything else that you guys are working on that you
wanted to talk about? Yeah. You know, I think we're going in several directions with kind of the
human human feedback stuff. You know, we have a kind of small team so far, but we're growing. So,
there's a lot of kind of project starting that are either, either offshoots of this or kind of
different, different directions on safety, but they're all pretty early. So, there's not not so much
to say, to say yet, but, you know, I'm hoping that a, that, you know, in a few months will kind of
have another batch of results that are kind of follow up some of the human feedback stuff and also
kind of new, new direction. Nice. Nice. There's a website that's like 10,000 hours or something like
that that like looks at, you know, these, I think looks pretty broadly at what they think are like
career opportunities. Oh, this is 80,000 hours. I actually did a podcast with them under, underestimated
it. I think that it is 8,000 hours is the length of a career, I think that's why I think that's
why they chose that number. So yeah, like their top career or their like top opportunity is AI
safety. Yeah. So, I mean, yeah, I don't have any opinions on like, I would never claim that,
you know, what I'm doing is like the most impactful thing you could, you could possibly do.
You know, I, so did a podcast with them. You know, I would, I would say instead that, you know,
the world has a lot of problems and, you know, and, and, and a lot of issues and, you know,
unfortunately, like multiple, incredibly serious issues. But, you know, one thing that I think
is, is true about about AI safety is that, you know, if I, if I look ahead to the next, you know,
two decades or so, I believe it could be a really serious issue. And I believe that, you know, not,
not that many people are thinking about it seriously in the sense of doing actual technical work on it.
You know, I think for a long time, it was, it was, you know, dismissed maybe with some,
some justification as kind of a kind of a crack potty thing. You know, I think that, that is,
that is starting to change. And so, I actually see opportunity there and that there's this important
problem that hasn't really been thought about in a careful enough way yet. And so, you know,
to me, that creates an opportunity to work on a problem that's very important to the world that,
you know, there, there aren't a giant number of smart people already kind of descending on it. And,
you know, because there's been kind of less thought about it, you know, maybe, maybe by,
by acting early, we can actually come up with, you know, we can actually come up with real solutions.
So, so to me, that's, that's kind of my rationale for feeling that it's, you know, a high-impact thing
for me to work on, that it, it's, it seems important and it seems like there aren't already 100,000
people working on it. Awesome. Awesome. Well, Daria, thank you very much.
Yeah, thanks. Thanks again for enlightening. All right, everyone. That's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on Daria or any of the topics covered in this episode,
head on over to twimlai.com slash talk slash 75. To follow along with our OpenAI series,
visit twimlai.com slash open AI. Of course, you can send along your feedback or questions via
Twitter to add Twimlai or at Sam Charrington or leave a comment right on the show notes page.
Thanks once again to Nvidia for their support of this series. To learn more about what they're
doing at Nips, visit twimlai.com slash Nvidia. And of course, thanks once again to you for listening
and catch you next time.
