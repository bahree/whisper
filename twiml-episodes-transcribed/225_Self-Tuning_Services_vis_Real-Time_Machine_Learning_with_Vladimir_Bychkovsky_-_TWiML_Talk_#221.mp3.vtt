WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.720
I'm your host Sam Charrington.

00:31.720 --> 00:37.740
Today we're joined by Vladimir Bichovsky, engineering manager at Facebook to discuss Spiral,

00:37.740 --> 00:43.440
a system they've developed for self-tuning high-performance infrastructure services at scale,

00:43.440 --> 00:46.160
using real-time machine learning.

00:46.160 --> 00:51.520
In our conversation, we explore the ins and outs of Spiral, including how the system works,

00:51.520 --> 00:55.840
how it was developed, and how infrastructure teams at Facebook can use it to replace

00:55.840 --> 01:02.200
hand-tuned parameters set using heuristics with services that automatically optimize themselves

01:02.200 --> 01:04.960
in minutes rather than weeks.

01:04.960 --> 01:09.440
We also discussed the challenges of implementing these kinds of systems, how to overcome

01:09.440 --> 01:15.120
user skepticism, and how to achieve an appropriate level of explainability.

01:15.120 --> 01:17.360
Now on to the show.

01:17.360 --> 01:18.360
All right, everyone.

01:18.360 --> 01:21.400
I am on the line with Vlad Bichovsky.

01:21.400 --> 01:26.360
Vlad is an engineering manager with Facebook based out of the Boston office.

01:26.360 --> 01:28.960
Vlad, welcome to this week in machine learning and AI.

01:28.960 --> 01:29.960
Thank you.

01:29.960 --> 01:30.960
Thank you very much.

01:30.960 --> 01:32.360
It's very exciting to be here.

01:32.360 --> 01:37.520
So today we're going to be talking about a project that you have worked on called Spiral,

01:37.520 --> 01:42.160
but before we do that, I'd like to explore a little bit of your background.

01:42.160 --> 01:46.000
You did your graduate work on Image Enhancement.

01:46.000 --> 01:47.880
Tell us a little bit about that.

01:47.880 --> 01:48.880
That's correct.

01:48.880 --> 01:56.080
Actually, before that, I did a little bit of systems work as well, so I worked in wireless

01:56.080 --> 02:02.280
networking and then transitioned into computational photography primarily because I really enjoyed

02:02.280 --> 02:05.840
photography, and I wanted to learn more about it.

02:05.840 --> 02:12.840
So for my thesis, I worked on automatic image enhancement, which is the magic button that

02:12.840 --> 02:18.680
people have on their phones, or actually, the algorithm I developed is now in Adobe Photoshop.

02:18.680 --> 02:23.960
If you dig deep into the menus, you could automatically set curves, and that's the algorithm

02:23.960 --> 02:27.200
that I built during my thesis.

02:27.200 --> 02:30.960
So yeah, and then after I finished my PhD, I joined Facebook.

02:30.960 --> 02:31.960
Nice.

02:31.960 --> 02:34.240
And what group are you in in Facebook?

02:34.240 --> 02:40.480
The group is called Machine Learning Experience, and it's the goal of our group is to basically

02:40.480 --> 02:44.320
deliver the benefits of machine learning for everybody.

02:44.320 --> 02:49.320
Generally, machine learning is kind of thought of as kind of this elite field for only people

02:49.320 --> 02:54.920
with incredibly strong backgrounds can contribute or even use, whereas what we're trying to do

02:54.920 --> 02:59.840
is kind of democratize machine learning so that every engineer can Facebook can benefit

02:59.840 --> 03:01.160
from it.

03:01.160 --> 03:10.240
Okay, and the specific project that you're working on is one called spiral that your group

03:10.240 --> 03:12.080
disclosed recently.

03:12.080 --> 03:13.920
Tell us a little bit about that project.

03:13.920 --> 03:14.920
Sure.

03:14.920 --> 03:16.960
Yeah, this is actually, I think it's a very exciting project.

03:16.960 --> 03:19.600
I'm glad to be part of it.

03:19.600 --> 03:24.040
So I think I thought it for a while about how to describe what we do, just the people who

03:24.040 --> 03:28.800
are not necessarily deep in computer science, and I think the best analogy is the following.

03:28.800 --> 03:35.880
So if you think about, for example, like coffee drinking habits, so we all want to sleep

03:35.880 --> 03:39.800
at night at the same time people do enjoy their coffee.

03:39.800 --> 03:45.400
And if you have to make a decision, do I drink coffee at 3 p.m. or not, right?

03:45.400 --> 03:49.880
You can do a bunch of experience where you drink coffee at 3 and then see if you can

03:49.880 --> 03:52.560
fall asleep at night, right?

03:52.560 --> 03:56.520
And based on the result, you kind of adjust what was the right call, like for example,

03:56.520 --> 04:01.600
I drink my coffee, then I can't fall asleep, okay, I record the outcome.

04:01.600 --> 04:07.560
And then the same thing next day, maybe I don't drink coffee after 3 p.m. and I fall asleep

04:07.560 --> 04:08.560
fine, right?

04:08.560 --> 04:13.440
And so if you kind of build up the data set and you kind of define a policy for yourself,

04:13.440 --> 04:18.080
you learn a policy which is I shouldn't be drinking coffee after 3, right?

04:18.080 --> 04:23.720
So the same kind of thing can be done for machines where you can literally in sort of

04:23.720 --> 04:27.760
in this environment where everything around you is changing all the time, you could kind

04:27.760 --> 04:30.520
of see the results of your actions.

04:30.520 --> 04:34.120
And you could see in retrospect whether or not the actions the machine has taken were

04:34.120 --> 04:39.200
good or bad and informed future decisions, does that make sense?

04:39.200 --> 04:44.720
So that is a really, really good explanation and example.

04:44.720 --> 04:48.600
And I think you may have just derailed the interview because the thing you just described,

04:48.600 --> 04:50.640
I really think it needs to exist.

04:50.640 --> 04:55.040
I've wanted the quantified self movement, I don't know if people even talk about this

04:55.040 --> 05:02.560
anymore, quantified self, but I've wanted this thing that just like tracks your, you

05:02.560 --> 05:08.040
can set up some metrics and track data points and then it applies machine learning to figure

05:08.040 --> 05:16.960
out the correlation between your actions and these other experiences you have, whether

05:16.960 --> 05:22.240
it's health or happiness or what have you.

05:22.240 --> 05:27.920
It automatically uses this data that you collect to make predictions as opposed to what

05:27.920 --> 05:34.600
quantified self really ever amounted to is providing like pretty pictures of the past,

05:34.600 --> 05:39.920
like the rear view mirror as opposed to the dashboard.

05:39.920 --> 05:44.760
And I can tell you more about why it's so important for Facebook or generally I think

05:44.760 --> 05:49.760
any company of that scale or any company wants to scale if you're interested.

05:49.760 --> 05:54.760
Well, so it sounds like you're not going to let me derail the interview by talking about

05:54.760 --> 06:03.320
applying this to quantified self, which is good, which is good, but it sounds like a set

06:03.320 --> 06:12.440
of problems that you can maybe loosely or roughly refer to as, I don't think there

06:12.440 --> 06:20.800
is low level as infrastructure management, but you're kind of using them to manage the

06:20.800 --> 06:24.360
way you configure.

06:24.360 --> 06:30.160
You have not low level infrastructure like high level software and kind of the deployment

06:30.160 --> 06:36.640
architecture of actually, I don't think deployment architecture is the right term either,

06:36.640 --> 06:43.160
but the configuration of software components that are driving various Facebook applications.

06:43.160 --> 06:45.200
Yeah, yeah, that's right.

06:45.200 --> 06:51.360
So another way, with the way I call it internally, it's more formally, it's automatic, adaptive

06:51.360 --> 06:52.680
policy, right?

06:52.680 --> 06:59.200
So it's automatic policy learning where instead of usually people define policies like coffee

06:59.200 --> 07:05.480
drinking policy or cash admission policy in terms of some sort of heuristic, right?

07:05.480 --> 07:09.840
They sort of look at like, oh, well, let's look at arrival times, let's look at something

07:09.840 --> 07:15.200
else and let's have a bunch of if else statements where we define the behavior.

07:15.200 --> 07:17.400
And that's generally, it generally works pretty well.

07:17.400 --> 07:22.040
There's nothing wrong with that approach in normal circumstances.

07:22.040 --> 07:27.320
I think the biggest challenge for the list comes up when you actually start going sort

07:27.320 --> 07:33.120
of at a much higher speed, specifically think about Facebook and this was published on

07:33.120 --> 07:42.520
their engineering blog, while back, Facebook is releasing a new version of web source code

07:42.520 --> 07:43.520
every hour.

07:43.520 --> 07:48.840
So literally, the version of Facebook you see is different from an hour to hour.

07:48.840 --> 07:54.480
So the code that runs it is changing and any system that depends on the code and makes

07:54.480 --> 07:58.920
assumptions about this code can potentially be outdated within hours.

07:58.920 --> 08:05.480
So if you think about sort of dub, dub, dub, or the web of Facebook being a dependency,

08:05.480 --> 08:10.520
it's something you kind of think it should behave a certain way if you if your service depends

08:10.520 --> 08:11.680
on it.

08:11.680 --> 08:13.880
And then every hour, it's actually changing.

08:13.880 --> 08:18.480
So whatever assumptions you made about how behaves may be completely invalid and not

08:18.480 --> 08:24.080
just invalid at sort of some sort of slow rate, literally every hour, it could be different.

08:24.080 --> 08:30.240
And so in that environment, it's very, very hard to sort of keep pace and update your code,

08:30.240 --> 08:33.440
update your logic of your service to stay optimal.

08:33.440 --> 08:37.760
So it's literally, this is not like some mechanism that we're just implemented because

08:37.760 --> 08:39.920
it's cool and we wanted to do it.

08:39.920 --> 08:45.280
It's literally a necessity to be able to run something at this scale and run it efficiently.

08:45.280 --> 08:48.720
So I'll give you an example of over cash, for example.

08:48.720 --> 08:54.160
If you have cash that's caching images, it could be movies, it could be anything else.

08:54.160 --> 09:00.440
And let's say that in the past, people have uploaded mostly PNG files and we cashed them

09:00.440 --> 09:03.240
and for PNG files, we may choose a certain limit.

09:03.240 --> 09:07.960
If the file is PNG file and is less than certain size, let's cash it.

09:07.960 --> 09:11.840
If it's too big, let's not cash it, vice versa, et cetera, they could sort of find a policy

09:11.840 --> 09:14.040
by hand with their statements.

09:14.040 --> 09:18.960
And then a little bit later, users are not uploading PGs anymore, they're mostly uploading

09:18.960 --> 09:19.960
JPEGs.

09:19.960 --> 09:20.960
Right?

09:20.960 --> 09:25.280
The policy you have written is completely useless because you're not caching any JPEG files

09:25.280 --> 09:28.400
and your system performs very poorly, right?

09:28.400 --> 09:32.240
So somebody has to go in and manually recode this.

09:32.240 --> 09:36.640
Somebody has to notice that metrics are out of auto-wack and has to recode it by hand,

09:36.640 --> 09:37.880
which is not sustainable.

09:37.880 --> 09:38.880
Right?

09:38.880 --> 09:40.920
Giving her things, how fast things are changing.

09:40.920 --> 09:44.400
And so what we do instead, something that spiraling can actually learn the optimal policy

09:44.400 --> 09:49.520
on the fly as the changes are occurring, as the changes in load are happening.

09:49.520 --> 09:58.200
So you're talking about learning the policy on the fly and that makes me think about techniques

09:58.200 --> 09:59.960
like active learning.

09:59.960 --> 10:07.600
Is that formally something that you've employed here or is it an adjacent area?

10:07.600 --> 10:08.600
It's related.

10:08.600 --> 10:12.840
If you say active learning is something we can do, it's something that we do in some

10:12.840 --> 10:18.600
other contexts, currently we don't do active learning inside spiral, that's not really

10:18.600 --> 10:19.600
necessary.

10:19.600 --> 10:23.480
So active learning is more about choosing examples to train on, right?

10:23.480 --> 10:27.640
So to be more specific, imagine if you're doing medical experiments, right?

10:27.640 --> 10:31.400
And you're doing some sort of experiments using new drugs or something like that.

10:31.400 --> 10:36.520
If you, and you're trying to figure out which combination of drugs works better, right?

10:36.520 --> 10:41.200
And each experiment costs you a lot of money to run, like it's literally not free, right?

10:41.200 --> 10:44.640
In this case, you can just say, okay, let me take a bunch of combinations and see which

10:44.640 --> 10:49.760
one works out, what you want to do is you want to take all previous experiments into account

10:49.760 --> 10:55.120
and figure out which combination to try next that gives you the most information, right?

10:55.120 --> 10:57.600
To kind of gains you the most information.

10:57.600 --> 11:03.200
In our case, it's, that's not the setup we have, but this applies in some other situations

11:03.200 --> 11:06.560
such as automatic configuration of services.

11:06.560 --> 11:10.800
So this is kind of looking a little bit forward, but something we're looking into sort of in

11:10.800 --> 11:18.440
the same vein of self maintaining services or self optimizing services is self tuning fleets.

11:18.440 --> 11:23.400
So if you think about a number of parameters that a given service has, it's actually very

11:23.400 --> 11:28.000
large and it's actually often not clear how to set them, especially if the environment

11:28.000 --> 11:29.000
is changing.

11:29.000 --> 11:34.800
It could be numbers such as number threads or sizes of queues, et cetera, right?

11:34.800 --> 11:39.480
And what is the optimal setting today, maybe different from the optimal setting tomorrow?

11:39.480 --> 11:45.120
And so what you want to do is you could use some of the machines in your fleet for experimentation,

11:45.120 --> 11:50.800
figure out which of them results in a better QPS, like the number of requests per second,

11:50.800 --> 11:51.800
right?

11:51.800 --> 11:53.880
And pick those parameters.

11:53.880 --> 11:57.840
And ideally you do this continuously and so this is exactly where active learning is

11:57.840 --> 12:03.120
very useful because you don't want to use lots of machines, sort of waste resources.

12:03.120 --> 12:07.800
You want to focus on the next combination of parameters that's more likely to be better

12:07.800 --> 12:09.040
than the current one.

12:09.040 --> 12:10.040
Does that make sense?

12:10.040 --> 12:15.520
Now, that does make sense and there's definitely this kind of sense or definition of active

12:15.520 --> 12:24.400
learning that's focused on trying to identify the training data, for example, that increase

12:24.400 --> 12:29.520
information gain so that you can reduce your overall costs of training.

12:29.520 --> 12:36.400
But there's also this sense, or I could be just, you know, I could be confusing ideas,

12:36.400 --> 12:40.920
but I also get the sense of active learning or think of it in the context of like incremental

12:40.920 --> 12:45.360
learning, meaning you get a piece of, you've got a trained model.

12:45.360 --> 12:50.880
And then you get a piece of data, you know, labeled data incrementally without going

12:50.880 --> 12:54.000
through an entire retraining, you're using that to enhance your model.

12:54.000 --> 12:56.480
Was there a better name for that than active learning?

12:56.480 --> 12:59.400
Yeah, the better name for that is online learning.

12:59.400 --> 13:00.400
Okay.

13:00.400 --> 13:01.400
So it's kind of like streaming.

13:01.400 --> 13:06.400
So if you think of streaming databases, so online learning is the form of kind of learning

13:06.400 --> 13:08.240
that's streaming, right?

13:08.240 --> 13:11.800
And that's actually exactly what we use for our system.

13:11.800 --> 13:15.240
And the main benefit of it in practice is feedback loops.

13:15.240 --> 13:21.160
So it's something actually machine learning researchers rarely think about, but as an engineer

13:21.160 --> 13:27.520
trying to just let's say enable adaptive policy in my service, I don't really want to spend

13:27.520 --> 13:32.160
time setting up a pipeline and finding out to more than I made some sort of mistake,

13:32.160 --> 13:33.160
right?

13:33.160 --> 13:37.760
Because it takes a while to do the training and then to shift the data and to get me back

13:37.760 --> 13:38.760
the model.

13:38.760 --> 13:41.160
It's, it's a very difficult feedback loop.

13:41.160 --> 13:45.520
So if I'm debugging something, it's, it's just not really acceptable, right?

13:45.520 --> 13:50.280
So if I'm training a huge neural network somewhere on the back end, it's, it's just difficult.

13:50.280 --> 13:54.960
So literally, imagine if you were trying to write a program and every time you compile,

13:54.960 --> 13:56.640
you have to wait a day, right?

13:56.640 --> 13:58.160
Realistically, we would not get anywhere.

13:58.160 --> 13:59.160
So, right.

13:59.160 --> 14:03.600
So the online model is actually really helping this case because with, with our system,

14:03.600 --> 14:08.160
one of the modes of operation is that when it's fully embedded and online, which means

14:08.160 --> 14:13.920
you literally plug it in, there's, it's actually two call sites and you plug in the data

14:13.920 --> 14:16.720
and you literally run your service.

14:16.720 --> 14:20.240
And as soon as your service starts running and providing feedback, you can start running

14:20.240 --> 14:21.240
and start seeing the results.

14:21.240 --> 14:24.800
You can start seeing the results improve, but you could see, oh, maybe I'm missing an

14:24.800 --> 14:25.800
important feature.

14:25.800 --> 14:29.760
Maybe it's, I'm misclassifying something, immediately change it, run it again and you're

14:29.760 --> 14:30.760
good to go.

14:30.760 --> 14:32.360
You can see the results immediately.

14:32.360 --> 14:38.000
So that's, that is exactly where online learning is very, very beneficial in practice,

14:38.000 --> 14:39.000
right?

14:39.000 --> 14:42.600
So in theory, people often talk about, oh, well, maybe you can't do quite as well online

14:42.600 --> 14:43.600
as you can do offline.

14:43.600 --> 14:47.960
Well, when you have all the data, something we're doing is we're kind of combining the

14:47.960 --> 14:53.880
best of both worlds, online learning is used to enable really, really tight feedback loops

14:53.880 --> 14:57.680
because it's, to me, this is probably the most important thing in engineering.

14:57.680 --> 15:01.800
If you're building something, you want to know how well it's doing, right?

15:01.800 --> 15:05.560
You want to know how well it's doing right now, not, not three days from now because

15:05.560 --> 15:09.280
you may lose context, you'll forget what you're doing, what you're trying to do, et cetera,

15:09.280 --> 15:10.280
et cetera.

15:10.280 --> 15:12.760
And this is something our system enables.

15:12.760 --> 15:16.360
When you're in the steady state, when you really figured out your features, you debunked

15:16.360 --> 15:20.320
your system, everything is good, now it's more about, okay, and then get a better, higher

15:20.320 --> 15:22.680
accuracy for my predictions.

15:22.680 --> 15:26.400
Then you can switch the offline learning mode and sort of, because the system was already

15:26.400 --> 15:27.400
debunked.

15:27.400 --> 15:31.640
And then you just improve your accuracy by using potentially larger model and more models

15:31.640 --> 15:33.200
and more complicated models.

15:33.200 --> 15:35.040
Does that make sense?

15:35.040 --> 15:42.680
The curse of me that I tend to think of the, that transition is being the opposite direction,

15:42.680 --> 15:51.160
meaning that you start with an offline train model and you use online learning to keep

15:51.160 --> 15:53.040
that model updated.

15:53.040 --> 15:57.240
But what you've done is you've kind of flipped that and you're using offline or rather

15:57.240 --> 16:07.320
online learning as a way to bootstrap very quickly and without requiring the model creators

16:07.320 --> 16:15.640
to understand the environment as well, collect the data, do all the feature engineering

16:15.640 --> 16:21.040
that they can do when they have that data and understand the environment.

16:21.040 --> 16:31.800
And then use the information from the online learning models that you've created to create

16:31.800 --> 16:36.000
incrementally better models by doing batch training.

16:36.000 --> 16:38.920
Yeah, that's exactly right, yeah.

16:38.920 --> 16:44.840
And so the challenge is always, so unlike traditional data sets, when you actually know what

16:44.840 --> 16:51.080
your features are or have complete data set, when we're talking about system generalists

16:51.080 --> 16:57.200
trying to optimize the system using machine learning, they don't necessarily know the features

16:57.200 --> 16:58.360
ahead of time, right?

16:58.360 --> 16:59.360
There's no data set.

16:59.360 --> 17:04.760
They're still trying to figure out, okay, is file size important for my caching policy?

17:04.760 --> 17:09.400
Maybe not, is the name of the data center from which this data comes in from?

17:09.400 --> 17:10.400
Is that important?

17:10.400 --> 17:11.400
Should I add that?

17:11.400 --> 17:12.400
Should I not add that?

17:12.400 --> 17:17.320
And in some respect, there is no, there's no, there's no data set before you start collecting

17:17.320 --> 17:18.320
it, right?

17:18.320 --> 17:22.320
It sort of assumes you say, oh, I should probably add file size, I should probably add

17:22.320 --> 17:23.320
something.

17:23.320 --> 17:27.360
Then then you create the data set, but it's really up to the engineer to plug it in, right?

17:27.360 --> 17:31.920
It doesn't come prepackaged as a set of benchmark of like, okay, here's a bunch of images

17:31.920 --> 17:35.880
and here's a bunch of labels, here's a cat and here's a dog, right?

17:35.880 --> 17:40.120
It's sort of part of the problem is that people are trying to understand their problem and

17:40.120 --> 17:44.400
trying to define it as they're solving it, right?

17:44.400 --> 17:49.600
And this is actually one of the probably biggest challenges for us in sort of something I find

17:49.600 --> 17:56.120
really, really exciting about this field is that we get to change how people think, literally.

17:56.120 --> 18:00.880
So when engineers, traditional engineers come to us with problems, they sort of, they

18:00.880 --> 18:05.560
always sort of think in terms of those fixed policies or sort of heuristics and picking

18:05.560 --> 18:10.880
some sort of thresholds for things and when we start working with them, their mind opens

18:10.880 --> 18:15.200
up and they realize that they can apply this whole slew of statistical methods to a problem

18:15.200 --> 18:18.560
which they considered very, very rigid.

18:18.560 --> 18:23.400
And so to me, that's just like one of those seeing those aha moments is really exciting.

18:23.400 --> 18:27.720
And oftentimes what happens next is that people realize that there's a whole bunch of new

18:27.720 --> 18:32.960
features that they could enable by switching to using a statistical approach from sort of

18:32.960 --> 18:33.960
a rigid heuristic.

18:33.960 --> 18:39.040
So there's kind of a bunch of really cool side benefits that come from learning more about

18:39.040 --> 18:41.800
how to apply systems like that.

18:41.800 --> 18:48.800
And so practically speaking, if I'm developing a system, say a caching system, one of the

18:48.800 --> 18:54.440
first things I might do in my system is pull a bunch of configuration information from,

18:54.440 --> 19:02.960
you know, environment variables or configuration files or what have you instead using spiral

19:02.960 --> 19:10.640
I'm calling an API to give me the kind of current best version of those configuration

19:10.640 --> 19:11.640
parameters.

19:11.640 --> 19:12.640
Is that correct?

19:12.640 --> 19:14.640
Yeah, almost.

19:14.640 --> 19:17.080
So in some respect, there's two ways to think about it.

19:17.080 --> 19:22.000
So if you think of a complete clean slate where your system puts up for the first time

19:22.000 --> 19:27.480
and we've never, spiral never has seen any data, so something that, and then you ask

19:27.480 --> 19:31.960
spiral for predictions for a given, for a given item that comes in, like do I cache

19:31.960 --> 19:36.200
this, do I not cache this, the spiral will just return the default value that you assigned

19:36.200 --> 19:39.280
because it has no information at this point.

19:39.280 --> 19:42.880
But as soon as your system starts providing feedback, as soon as you say, oh, I should

19:42.880 --> 19:46.720
have cache this item, oh, I should not have cache this item, and as soon as you start

19:46.720 --> 19:51.880
putting this information in, the next prediction that you ask will be better.

19:51.880 --> 19:58.880
And so all you need to do is just pull the information at the time when you know what

19:58.880 --> 20:00.840
decision should have been made, right?

20:00.840 --> 20:04.520
So when you're sort of on, for cache, it may be at eviction time when you're evicting

20:04.520 --> 20:10.280
an item and nobody has ever requested their item, you could say, well, I probably shouldn't

20:10.280 --> 20:11.840
have cache this, right?

20:11.840 --> 20:15.960
If it's like it wasn't necessary, like nobody asked for it, and vice versa, if you see

20:15.960 --> 20:19.920
an item that's been hit many, many times, you're like, okay, definitely next time I see something

20:19.920 --> 20:21.840
like this, I should cache it.

20:21.840 --> 20:22.840
Yeah.

20:22.840 --> 20:28.960
I think you just pointed out an important distinction, and that is spiral is not predicting

20:28.960 --> 20:36.960
for me the underlying configuration parameters that I might use to make a decision.

20:36.960 --> 20:43.000
It's predicting the decision, meaning it's not predicting, I'm not using it to tell

20:43.000 --> 20:48.760
me, you know, what queue length or, you know, how much JVM memory I might need or something

20:48.760 --> 20:56.440
like that, it's operating at a higher level and it is managing all those parameters under

20:56.440 --> 20:57.440
the covers.

20:57.440 --> 20:58.440
Is that correct?

20:58.440 --> 21:02.720
Indirectly, yes, it's managing the parameters of your policy, right?

21:02.720 --> 21:07.920
So for example, if you had a threshold before, if I should not cache images that are larger

21:07.920 --> 21:09.600
than 100 kilobytes, right?

21:09.600 --> 21:13.560
So you kind of had this 100 kilobyte parameter, and then you're like, well, maybe it should

21:13.560 --> 21:16.400
change it to 70 or 120, right?

21:16.400 --> 21:20.880
So and then you look at the quality of decisions, say, oh, yeah, 120 is way better, looks

21:20.880 --> 21:25.080
like I get a better cache hit rate or something, right?

21:25.080 --> 21:27.000
So that's taken away.

21:27.000 --> 21:31.080
So if you were trying to configure a policy, you no longer have to do that.

21:31.080 --> 21:37.800
You just provide examples of correct decisions, and spiral, spiral learns the rest of it.

21:37.800 --> 21:41.840
The tuning of parameters of a service is sort of a different project we're doing, which

21:41.840 --> 21:46.680
is kind of moving, looking forward at the things we're interested in.

21:46.680 --> 21:51.960
So we could tune the number of threads, but the setup is a little bit different than spiral.

21:51.960 --> 21:57.920
So in that setup, what you would do is you would provide statistics of how well the services

21:57.920 --> 22:02.920
operating as a whole with a set of parameters, and we would look at those statistics and

22:02.920 --> 22:08.440
let's say the number of requests per second that the service is providing with a given

22:08.440 --> 22:09.960
configuration.

22:09.960 --> 22:14.120
And we would compare it to other experience we have ran, and then using active learning

22:14.120 --> 22:16.040
would pick the next experiment.

22:16.040 --> 22:22.320
Okay, sounds like the queue size of 100 looks better than 110, let's try 90, right?

22:22.320 --> 22:26.960
And that all happens behind the covers, so under the covers and the engineer never has

22:26.960 --> 22:27.960
to see it.

22:27.960 --> 22:32.280
In the end, what they get is, oh, it looks like currently you get the highest number of

22:32.280 --> 22:38.960
requests per second if you use the queue length of 35 and the number of threads 15, right?

22:38.960 --> 22:43.160
And now you just set the two service and all of a sudden your utilization drops.

22:43.160 --> 22:46.520
You consume much less energy and you're serving many more requests.

22:46.520 --> 22:47.520
Got it.

22:47.520 --> 22:48.520
Got it.

22:48.520 --> 22:50.440
But that's a separate future project.

22:50.440 --> 22:51.440
That's correct.

22:51.440 --> 22:52.440
Yes.

22:52.440 --> 23:01.560
So then with spiral, imagining that, and you alluded to this, that engineers might have

23:01.560 --> 23:09.920
previously envisioned very simple policies for different aspects of their systems, for

23:09.920 --> 23:15.880
example, you know, cash based on file size or cash based on file type.

23:15.880 --> 23:25.920
And now by being able to specify these policies more declaratively, they, there's also a tendency

23:25.920 --> 23:28.200
to make the policies richer.

23:28.200 --> 23:29.200
Is that the case?

23:29.200 --> 23:33.280
Yes, it's sort of a very easy way.

23:33.280 --> 23:35.040
It's formalizing the problem, right?

23:35.040 --> 23:36.760
So you use exactly the right language.

23:36.760 --> 23:41.480
So it's, you kind of do more declarative programming than imperative programming.

23:41.480 --> 23:43.960
You declare what it is that you want.

23:43.960 --> 23:47.760
One interesting thing that happens when you start working with different engineer teams

23:47.760 --> 23:53.240
that want to try sort of using automatic policies is that they realize that their problem

23:53.240 --> 23:54.920
is not well defined, right?

23:54.920 --> 23:57.600
They say, oh, we really want to just optimize this metric.

23:57.600 --> 24:00.680
So this is our, that's what we want.

24:00.680 --> 24:05.080
And then we, when we try to formalize the problem because they have to code up that declarative

24:05.080 --> 24:09.120
solution to, okay, this is the right answer, this is what the right answer looks like,

24:09.120 --> 24:13.280
they realize, oh, that, that conflicts with, with this other objective, no, that's not

24:13.280 --> 24:14.640
what we want to do.

24:14.640 --> 24:17.520
And so that's change in mindset that happens.

24:17.520 --> 24:21.680
It's also beneficial in sense that people figure out problems that they've always had,

24:21.680 --> 24:26.080
but previously just had some sort of hard-quoted solution that made some sort of implicit

24:26.080 --> 24:27.080
trade-off, right?

24:27.080 --> 24:32.440
That's sort of the trade-off was hidden and fixed and they just lived with it.

24:32.440 --> 24:38.360
And switching to this higher level programming model effectively forces them to be more explicit

24:38.360 --> 24:42.520
about the trade-offs they're making, understand the trade-offs they're making the system.

24:42.520 --> 24:46.800
So again, I think this is one of the super exciting aspects of working in this field.

24:46.800 --> 24:52.760
Can you walk through an example that illustrates that both the specific changes that they

24:52.760 --> 25:00.360
need to make to implement these policies and to integrate with Spiral, but also where

25:00.360 --> 25:04.120
these types of trade-offs have come into play?

25:04.120 --> 25:09.360
So, I mean, it's difficult to do sort of an abstract sort of an example that would be

25:09.360 --> 25:13.960
easy to discuss on sort of an already without a Blackboard.

25:13.960 --> 25:19.080
But if you think about, again, something like cash admission policy, right?

25:19.080 --> 25:24.560
You sort of want, as soon as you previously could just say, oh, well, I think we should

25:24.560 --> 25:28.200
cash this type of files and if they're not bigger than this, right?

25:28.200 --> 25:30.240
And just roll with it.

25:30.240 --> 25:34.520
And then you look at some metric and see, oh, well, the cash rate, the cash hit rate looks

25:34.520 --> 25:35.520
good.

25:35.520 --> 25:38.960
All right, let's find, let's keep it as this, right?

25:38.960 --> 25:42.800
So you don't know what the opportunity cost is, right?

25:42.800 --> 25:49.760
So maybe you could have had a much better cash hit rate or maybe you're burning through

25:49.760 --> 25:50.760
flash, right?

25:50.760 --> 25:55.920
Let's say it's a bigger cash, not only use memory, but those use flash, right?

25:55.920 --> 26:03.120
And so now you start looking at the rates of how much flash you're running through.

26:03.120 --> 26:07.920
And you're like, oh, no, well, it looks like we're writing over flash a lot.

26:07.920 --> 26:13.160
Let me go back and maybe tune my policy or do something about it, right?

26:13.160 --> 26:16.600
And then that has an effect on your cash hit rate, right?

26:16.600 --> 26:21.360
So if you're storing your, maybe saving few items, you have lower cash hit rate.

26:21.360 --> 26:24.480
So and then maybe different engineers are working the two problems, right?

26:24.480 --> 26:29.040
So you have this issue where one team is working to improve the cash hit rate and the

26:29.040 --> 26:32.760
other team is trying to improve the rate of burnout of flash, right?

26:32.760 --> 26:37.240
And sort of you have people who are adjusting different parts of the system, which indirectly

26:37.240 --> 26:42.200
impact the other team without sort of realizing that explicit dependency.

26:42.200 --> 26:46.440
I mean, this is a really simple example and most people would probably figure out that

26:46.440 --> 26:50.960
like, okay, if I were changing this, we're going to affect the cash hit rate.

26:50.960 --> 26:54.280
But this is, this should give you a flavor of the types of trade-offs.

26:54.280 --> 26:59.720
Now if you have a single policy controlling the two, then, then you can set recall on

26:59.720 --> 27:03.720
your classifier and sort of make the trade-off very explicit, okay, I'll get this much cash

27:03.720 --> 27:08.280
rate, cash hit rate at this burnout rate, right?

27:08.280 --> 27:11.800
And sort of then you can say, okay, well, this is the classifier I have.

27:11.800 --> 27:14.000
And it looks like I can't have both, right?

27:14.000 --> 27:21.080
I can't have really high cash hit rate with the current policy and low burn rate.

27:21.080 --> 27:25.760
So then what you look at is, okay, maybe we can add additional features and sort of improve

27:25.760 --> 27:27.480
both at the same time.

27:27.480 --> 27:29.200
Does that make sense?

27:29.200 --> 27:34.560
Is it sort of using adaptive policy tools like Spiral allows you to make this trade-offs

27:34.560 --> 27:37.920
explicit and then sort of see them all in one place?

27:37.920 --> 27:47.440
And so you started talking about starting with these policies and then looking at optimization

27:47.440 --> 27:55.000
metrics and performance and features and feature engineering, that kind of thing.

27:55.000 --> 28:00.640
Can you talk a little bit about the data science element of this or the modeling element

28:00.640 --> 28:06.960
of this and specifically, is that abstracted away from the presumably, it's abstracted

28:06.960 --> 28:12.080
away to some extent from the engineers that are using Spiral or are they involved in developing

28:12.080 --> 28:17.200
statistical models for the specific policies that they want to implement?

28:17.200 --> 28:21.520
So that's a great question in a sense that this is something we're still trying to figure

28:21.520 --> 28:23.760
out kind of the best way to do.

28:23.760 --> 28:30.200
So initially when we started building Spiral, our view was that we have a lot of domain

28:30.200 --> 28:35.840
expertise, meaning that if people who are building caches and have built in caches for many

28:35.840 --> 28:42.120
years decide to use adaptive policy, they actually already have all the domain expertise

28:42.120 --> 28:43.120
we need, right?

28:43.120 --> 28:46.240
So in some respect, they really know what matters or what doesn't.

28:46.240 --> 28:51.560
In other words, if I have written a heuristic manually by hand that checks file sizes and

28:51.560 --> 28:57.080
file types and compression types as something that's important to my policy, that is my

28:57.080 --> 28:58.080
feature set, right?

28:58.080 --> 29:03.840
And sort of I already have experience and I already know what features matter.

29:03.840 --> 29:09.800
And that's partially what enables us to do what we do because the methods, as I said,

29:09.800 --> 29:14.760
we use our online methods and sort of their meant to be high performance, right?

29:14.760 --> 29:20.800
Because the cache is making lots and lots of decisions per second and you can't really

29:20.800 --> 29:23.920
have very heavy models to make those decisions.

29:23.920 --> 29:29.880
And so currently we partner with teams and help them do the data science.

29:29.880 --> 29:34.640
But moving forward, we're trying to template those solutions and sort of every team can

29:34.640 --> 29:40.840
do their own data science fairly easily with the tools that we built for them.

29:40.840 --> 29:45.120
And then they would know how well their system would perform if they were to plug Spiral

29:45.120 --> 29:46.120
in.

29:46.120 --> 29:47.600
Does that make sense?

29:47.600 --> 29:51.880
So in other words, we're transitioning from doing a lot of the data science ourselves

29:51.880 --> 29:55.920
and sort of jointly with the team with a sort of high-touch environment into more of a

29:55.920 --> 30:02.560
self-service environment where teams have an easy way to sort of plug in their data and

30:02.560 --> 30:08.120
get the insights and make a decision how to use that data.

30:08.120 --> 30:12.880
And this is sort of done through education, through documentation and through potentially

30:12.880 --> 30:17.520
shifting some of the work we do to data scientists on those teams or affiliated data

30:17.520 --> 30:18.520
scientists.

30:18.520 --> 30:22.160
So that's our path forward to sort of scaling this, right?

30:22.160 --> 30:24.040
Because we're not a very large team.

30:24.040 --> 30:26.120
We don't have hundreds of people working on this, right?

30:26.120 --> 30:30.080
So it's been noted to have impact on Facebook scale.

30:30.080 --> 30:34.160
It's if we just were to meet with every potential customer, we would burn out of hours

30:34.160 --> 30:35.160
in the day.

30:35.160 --> 30:36.160
Sure.

30:36.160 --> 30:37.160
Sure.

30:37.160 --> 30:44.080
Can you talk through the data science process as it exists today and in particular elements

30:44.080 --> 30:53.000
of the modeling that are unique to the way you've formulated this problem in system?

30:53.000 --> 30:54.000
Sure.

30:54.000 --> 30:59.080
So the most difficult data science, in this case, ends up being the part where we are

30:59.080 --> 31:01.160
formally defining the problem, right?

31:01.160 --> 31:04.440
So effectively, what we do is it's structured learning, right?

31:04.440 --> 31:09.120
So it's really you need features and you need labels.

31:09.120 --> 31:14.280
And part of the process to work with what teams to figure out, what are the features,

31:14.280 --> 31:17.360
which is something they usually know, and then what are the labels?

31:17.360 --> 31:22.000
And this is something I referred to earlier, which is people don't necessarily agree

31:22.000 --> 31:24.720
on what the labels should be and which labels are right.

31:24.720 --> 31:29.480
And that's sort of where we bring up some sort of contradictions in the objectives

31:29.480 --> 31:31.160
they're trying to achieve.

31:31.160 --> 31:36.440
So a lot of it is just talking through the problem and taking the main specific problems

31:36.440 --> 31:42.520
sort of a systems problem and bring it into machine learning terms and sort of to sort

31:42.520 --> 31:47.720
of figure out what is our data set, what are the right answers, what are the right labels?

31:47.720 --> 31:52.840
Can one of the big requirements of spiralist ability to generate the labels automatically,

31:52.840 --> 31:53.840
right?

31:53.840 --> 31:57.760
Because in order for system to stay adaptive, you need to be able to continuously

31:57.760 --> 32:00.720
feedback the new labels, right?

32:00.720 --> 32:03.920
To sort of continue to adopt the environment, you need to fit in the new data set, the

32:03.920 --> 32:07.000
fresh, the fresh correct answers.

32:07.000 --> 32:10.680
So a lot of it is number one, figure out if that can be done.

32:10.680 --> 32:14.320
And if it can be done, then the spiralist is not the right system for a particular use

32:14.320 --> 32:15.320
case.

32:15.320 --> 32:20.960
And if it can be done, then figure out what are those labels taking them and then just

32:20.960 --> 32:26.160
applying the basic machine learning methods to see if we can achieve desired accuracy,

32:26.160 --> 32:27.160
desired quality.

32:27.160 --> 32:34.840
And are there specific methods or techniques that are used particular to the online learning

32:34.840 --> 32:35.840
aspect of this?

32:35.840 --> 32:43.120
So one of the very old and very kind of battle-tested methods is something we also use, it's

32:43.120 --> 32:45.360
called multinomial-nave-base.

32:45.360 --> 32:50.680
It comes from a family of nave-base methods and it's sort of very simple that effectively

32:50.680 --> 32:52.160
is counting.

32:52.160 --> 32:59.240
So if you have, let's say one place where this is also used is spam detection, right?

32:59.240 --> 33:03.560
So that's actually what makes spam detection practical on desktop when you get an email

33:03.560 --> 33:08.920
and it says it has words like Viagra and something else, something else, okay, that's spam,

33:08.920 --> 33:09.920
right?

33:09.920 --> 33:14.680
Because your other email, your good email probably doesn't have words like Viagra in

33:14.680 --> 33:15.680
it, right?

33:15.680 --> 33:16.880
That's probably not something you can verse about.

33:16.880 --> 33:22.680
So if you count the number of times Viagra occurs in your email, it's very low in sort

33:22.680 --> 33:27.520
of a good email and if you count the number of times Viagra occurs in bad email, that's

33:27.520 --> 33:28.520
very high.

33:28.520 --> 33:34.400
So that's effectively what multinomial-nave-base and generate nave-base methods do.

33:34.400 --> 33:38.440
And the big benefit of those methods is that they're incredibly fast, right?

33:38.440 --> 33:40.680
Because you literally just do counting.

33:40.680 --> 33:45.240
There is no grading descent, there's none of that needs to be done and as soon as you

33:45.240 --> 33:50.920
updated your counters, your model has updated and you're now ready to make predictions with

33:50.920 --> 33:53.680
higher certainty or not a larger data set.

33:53.680 --> 33:59.880
You say as soon as you've updated your counters, what are those counters represent?

33:59.880 --> 34:05.840
Are those counters features from the input data or are they kind of internal state that's

34:05.840 --> 34:09.240
keeping track of thresholds or things like that?

34:09.240 --> 34:11.760
It's the internal state that keeps track of thresholds.

34:11.760 --> 34:18.760
Literally, like one counter is sort of the prior counter, empirical prior, which is how

34:18.760 --> 34:21.680
many good emails have you gotten versus bad emails, right?

34:21.680 --> 34:28.640
So if you just count that and if 99% of your emails are good and if you ask me to predict

34:28.640 --> 34:32.120
that for a new email, is that likely to be good or bad?

34:32.120 --> 34:35.720
Based on that single counter, I'll say, well, it's most like the good because really most

34:35.720 --> 34:40.400
of the emails you get are good without even looking at the content, right?

34:40.400 --> 34:46.400
And on top of that, I can look at word occurrences in the email and which words occur in that.

34:46.400 --> 34:51.920
And if that's, if that email says hello Sam and it says sort of how are you, let's get

34:51.920 --> 34:53.720
together this weekend.

34:53.720 --> 34:57.280
That's one set of words that are being hit right and like weekend and hello and everything

34:57.280 --> 34:58.280
else.

34:58.280 --> 35:02.360
Those are, all those counts are usually high for good emails.

35:02.360 --> 35:07.200
And then if it says, you know, Pills, Viagra, something, something, something else, those

35:07.200 --> 35:10.720
counts are very low when you're regular email, right?

35:10.720 --> 35:12.840
So does that, does that make sense?

35:12.840 --> 35:16.040
You're basically conditionals in your feature space.

35:16.040 --> 35:17.040
That's exactly right.

35:17.040 --> 35:21.760
So you're keeping kind of class conditional probabilities, you're keeping those scores

35:21.760 --> 35:26.120
and you're applying them to as soon as soon as you updated the counters, you have new conditional

35:26.120 --> 35:29.480
probabilities and you could use them to classify the next item.

35:29.480 --> 35:34.360
We've talked about the caching example multiple times.

35:34.360 --> 35:37.080
You've mentioned spam a couple of times.

35:37.080 --> 35:42.920
Is this used for content filtering types of applications at Facebook or is that more

35:42.920 --> 35:43.920
an example?

35:43.920 --> 35:45.440
That's definitely an example.

35:45.440 --> 35:50.360
Just anytime you see multi-nevenly based, like if you go into Wikipedia, they'll literally

35:50.360 --> 35:54.160
give you a spam as an example of how this method works.

35:54.160 --> 36:00.440
Are there other areas beyond the cache example that this is being used at?

36:00.440 --> 36:01.440
Yeah.

36:01.440 --> 36:02.440
So one other area.

36:02.440 --> 36:07.600
I mean, this literally can be used in any area where the examples can be generated automatically.

36:07.600 --> 36:11.840
Caches just one of them in a sense, like you, you know and retrospect which items you

36:11.840 --> 36:13.880
should or should not have cashed.

36:13.880 --> 36:20.360
Anytime you can retrospectively check a decision, the system applies and one other specific

36:20.360 --> 36:23.760
example is something like retribility.

36:23.760 --> 36:29.200
So imagine that you were many, so many companies run batch jobs, right?

36:29.200 --> 36:33.240
So Facebook has a lot of large system that runs batch jobs.

36:33.240 --> 36:37.240
And batch jobs often can fail for various reasons, right?

36:37.240 --> 36:42.680
Sometimes it could be there's a syntax error that I checked in bad code because I'm a

36:42.680 --> 36:44.920
irresponsible developer, right?

36:44.920 --> 36:49.680
And there's syntax error in it and after four hours of computing something, it hits my

36:49.680 --> 36:52.640
incorrect statement and breaks, right?

36:52.640 --> 36:54.240
Literally the job fails.

36:54.240 --> 36:57.240
The results have not been delivered.

36:57.240 --> 36:59.480
Or there may be instructions failure.

36:59.480 --> 37:02.680
Let's say there's network connectivity issues with a particular data center and the

37:02.680 --> 37:06.920
job is trying to get the data and it fails because it couldn't get the data, right?

37:06.920 --> 37:09.240
And maybe those does this be very quickly.

37:09.240 --> 37:13.440
So in the first case, you actually don't want to retry the job, right?

37:13.440 --> 37:16.640
But particularly because it could be very expensive to rerun it, right?

37:16.640 --> 37:20.880
So you sort of you run for a bunch of phases, you run it for hours, spend on hundreds of

37:20.880 --> 37:24.320
computers and then you fail because there is a syntax error.

37:24.320 --> 37:26.800
Well, if I and you're going to run it again the next time.

37:26.800 --> 37:27.800
Exactly.

37:27.800 --> 37:28.800
Exactly.

37:28.800 --> 37:29.800
So I'm going to fail again next time.

37:29.800 --> 37:31.600
I can retry it three times or four times or five times.

37:31.600 --> 37:34.480
I will still fail and I will waste resources.

37:34.480 --> 37:39.280
On the other hand, if the job is, if the failure was intermittent, you actually do want to

37:39.280 --> 37:41.360
retry, right?

37:41.360 --> 37:48.280
And this is exactly the same setup as with the cache because retroactively, you know what

37:48.280 --> 37:51.000
the right answer answer should have been, right?

37:51.000 --> 37:54.520
And certain types of jobs, no matter how many times you retry them, they fail.

37:54.520 --> 37:57.160
Other jobs succeed on retries.

37:57.160 --> 38:03.320
So learning to classify which, what to retry and what not to retry is again a type of scenario

38:03.320 --> 38:04.800
where we use spiral.

38:04.800 --> 38:11.360
So features here might be job, return codes and infrastructure metrics and things like

38:11.360 --> 38:12.360
that.

38:12.360 --> 38:17.600
Yeah, this feature features in this example that are literally logs, right?

38:17.600 --> 38:21.080
So to return codes, press the log that this job produced.

38:21.080 --> 38:26.320
So it's kind of just literally the text that was out, the last, at last, say, 100 lines

38:26.320 --> 38:32.320
of text that were produced and that's fed into the classifier, along with the kind of

38:32.320 --> 38:37.920
number of retries that happened, meaning, okay, this message was produced and we retried

38:37.920 --> 38:40.360
it three times and nothing good happened, right?

38:40.360 --> 38:41.920
We've never succeeded.

38:41.920 --> 38:46.160
So next time we get the similar message, well, that means we probably shouldn't retry it.

38:46.160 --> 38:55.960
So you've got these, these logs, I'm imagining a typical log is, you know, has some significant

38:55.960 --> 38:56.960
depth to it.

38:56.960 --> 39:02.400
It's not kind of abstracted down to a particular, you know, word or error code or something,

39:02.400 --> 39:03.760
but it's in there.

39:03.760 --> 39:10.600
How are you narrowing in on the, you know, the specific signal within a big log file?

39:10.600 --> 39:15.080
Well, then it's down through the magic of machine learning.

39:15.080 --> 39:21.600
I guess in the context of, of this, this online learning where you're, you're basically

39:21.600 --> 39:27.200
doing counting, right, you've, you've got to tell it what to count, right?

39:27.200 --> 39:31.120
You're not, yeah, I'm not, I'm not hearing that you're doing, you know, deep learning

39:31.120 --> 39:36.920
or something like that and you're training a model to figure out what to count.

39:36.920 --> 39:41.960
Well, we are, we are trying to, so there's, I mean, it's fundamental to deep learning

39:41.960 --> 39:45.240
or any kind of other learning, they're all similar, right?

39:45.240 --> 39:49.920
They're all, there is a, there is a theorem called no free lunch theorem.

39:49.920 --> 39:54.680
You can look it up on Wikipedia, but it, it's fairly, it's fairly complicated, but in

39:54.680 --> 39:59.000
a nutshell, it says that no classifier is superior to another classifier.

39:59.000 --> 40:04.360
In other words, if, if I can, with the right set of transformations on the input data,

40:04.360 --> 40:10.720
I can get results that are as good with a simple classifier as I would on raw data with

40:10.720 --> 40:13.800
a more sophisticated classifier, does that make sense?

40:13.800 --> 40:14.800
Sure.

40:14.800 --> 40:15.800
Yeah.

40:15.800 --> 40:20.600
Effectively, if I pre-process the logs using some sort of vectorization, using something

40:20.600 --> 40:25.320
else and then plug them into a very simple classifier, I'll get the results that are almost

40:25.320 --> 40:32.600
as good or, or as good as sort of a sophisticated neural, natural language processing method

40:32.600 --> 40:35.640
with, you know, recurring neural networks or something like that.

40:35.640 --> 40:36.640
Right.

40:36.640 --> 40:37.640
Well, that's what I was asking about.

40:37.640 --> 40:41.440
Even processing the logs and vectorizing them or creating an embedding or something like

40:41.440 --> 40:42.440
that.

40:42.440 --> 40:43.440
That's correct.

40:43.440 --> 40:44.440
Exactly.

40:44.440 --> 40:45.440
Yeah, that's what I was getting at.

40:45.440 --> 40:46.440
Okay.

40:46.440 --> 40:47.440
Cool.

40:47.440 --> 40:48.440
Interesting.

40:48.440 --> 40:49.440
Interesting.

40:49.440 --> 40:55.160
And so, you know, one of my initial thoughts and I alluded to this earlier in the discussion

40:55.160 --> 41:03.200
was, you know, kind of broad implications of this kind of technique in infrastructure

41:03.200 --> 41:08.240
management, for example, Google has kind of famously talked about some of their results

41:08.240 --> 41:13.760
in applying machine learning to managing, you know, HVAC systems within the data center.

41:13.760 --> 41:17.440
It seems like this kind of approach could have similar applicability.

41:17.440 --> 41:18.440
Correct.

41:18.440 --> 41:19.440
Yes.

41:19.440 --> 41:21.640
And that's what we're trying to do as well.

41:21.640 --> 41:27.880
I mean, if you weren't going to a large company, so a company that with as much infrastructure

41:27.880 --> 41:32.680
as Facebook or Google, at some point you want to take people out of the loop, right?

41:32.680 --> 41:35.200
And that's exactly what we're doing with our methods.

41:35.200 --> 41:40.160
We're trying to make sure people aren't doing kind of routine maintenance or mundane

41:40.160 --> 41:46.160
work and are actually focused on creative tasks by automating the routine work and sort

41:46.160 --> 41:47.960
of maintenance work.

41:47.960 --> 41:52.480
And so, have you done anything to try to apply this to kind of management of physical

41:52.480 --> 41:53.480
infrastructure?

41:53.480 --> 41:57.880
Our team in particular hasn't, but I don't know if other teams have.

41:57.880 --> 42:00.800
There's many other teams that are doing very interesting things.

42:00.800 --> 42:04.440
So we haven't, but I can't speak for all of Facebook.

42:04.440 --> 42:10.480
What were some of the main things that your team has learned in building the system and

42:10.480 --> 42:13.560
getting it in the hands of some users?

42:13.560 --> 42:18.240
I think the main thing that we've learned is that it's, you do have to change the mindset

42:18.240 --> 42:22.760
of people before they're comfortable using our system because initially when they start

42:22.760 --> 42:26.840
using it, the sort of just see it as magic, it's like, oh, this magical thing just makes

42:26.840 --> 42:27.840
decisions.

42:27.840 --> 42:30.840
It's like, why did it make that decision?

42:30.840 --> 42:34.520
And sort of, there's a price for an activity.

42:34.520 --> 42:36.200
There's a price for optimality, right?

42:36.200 --> 42:39.560
In this case, it's maybe somewhat reduced transparency.

42:39.560 --> 42:45.040
So if you think about the example I gave earlier, which is a sort of an EFELS tree of statements

42:45.040 --> 42:50.160
for you, where your policies encoded, you could look at any decision and you can trace

42:50.160 --> 42:55.200
it back through that tree and say, well, that image was not cashed because it was just

42:55.200 --> 42:57.960
over the file size limit, right?

42:57.960 --> 43:04.200
As soon as you start using statistical methods, that observability goes away, right?

43:04.200 --> 43:05.560
It's not, there were changes.

43:05.560 --> 43:07.560
It requires you to think in different terms, right?

43:07.560 --> 43:11.600
It would, it requires you to think in terms of larger data set and sort of statistical

43:11.600 --> 43:15.040
averages, not EFELS statements.

43:15.040 --> 43:16.680
I think that's the biggest challenge for us.

43:16.680 --> 43:23.120
It's sort of getting people to let go of control and trusting a machine learning system.

43:23.120 --> 43:27.760
And it's very much on us to prove to them that it's actually better than what they had

43:27.760 --> 43:28.760
before.

43:28.760 --> 43:34.200
We've been lucky in a sense that we've enabled our customers to do things they didn't

43:34.200 --> 43:35.680
think were possible.

43:35.680 --> 43:39.000
And so they, they're kind of embraced us and they said, this is great.

43:39.000 --> 43:41.160
This really saves me many hours in a day.

43:41.160 --> 43:44.360
And now we can do this thing that I didn't think we could.

43:44.360 --> 43:45.680
So that helps.

43:45.680 --> 43:50.000
But generally, when you start talking to people, they're a little bit skeptical like, well,

43:50.000 --> 43:51.360
is this going to do the right thing?

43:51.360 --> 43:52.360
How am I going to debug it?

43:52.360 --> 43:53.360
Right.

43:53.360 --> 43:59.280
So that's kind of our next challenge in terms of scaling this to sort of literally every

43:59.280 --> 44:03.800
engineer at Facebook and potentially after that, you know, every engineer in the world

44:03.800 --> 44:05.640
who wants to benefit from this.

44:05.640 --> 44:14.160
Have you started exploring approaches to creating some degree of observability, explainability,

44:14.160 --> 44:15.160
that kind of thing?

44:15.160 --> 44:16.160
Yes.

44:16.160 --> 44:19.200
And so that's a kind of an ongoing process.

44:19.200 --> 44:24.480
Traditionally, so this is something I actually learned during my, during the time of work

44:24.480 --> 44:30.880
to my dissertation, methods such as nearest neighbor methods tend to be easier to explain

44:30.880 --> 44:36.360
because whatever person says, why was this decision made, you could provide examples and

44:36.360 --> 44:42.480
say, well, because this example looks very similar to those two examples, which you said

44:42.480 --> 44:43.920
were positive.

44:43.920 --> 44:46.600
That's why this example is positive.

44:46.600 --> 44:47.600
Does that make sense?

44:47.600 --> 44:54.720
It's because nearest neighbors, family methods, they work by literally through examples.

44:54.720 --> 44:56.840
Like, here's a prototype examples.

44:56.840 --> 45:00.520
Now, tell me what this new, never seen before example.

45:00.520 --> 45:02.360
What is the label for this example?

45:02.360 --> 45:06.920
We can literally use that as an interface and say, well, it's because you provided us

45:06.920 --> 45:08.480
with this data.

45:08.480 --> 45:13.480
And so that's the direction we're kind of thinking about and sort of that's those methods we

45:13.480 --> 45:18.200
want to try to see if we can exploit and sort of run effectively online.

45:18.200 --> 45:19.200
Awesome.

45:19.200 --> 45:22.480
Well, Vlad, thanks so much for taking the time to chat with us about this.

45:22.480 --> 45:24.640
This is a really interesting project.

45:24.640 --> 45:25.640
Thank you very much.

45:25.640 --> 45:27.880
I'm, thank you for having me on the show.

45:27.880 --> 45:28.880
Awesome.

45:28.880 --> 45:29.880
Take care.

45:29.880 --> 45:30.880
Bye-bye.

45:30.880 --> 45:34.840
All right, everyone.

45:34.840 --> 45:39.920
That's our show for today for more information on Vlad or any of the topics covered in the

45:39.920 --> 45:45.800
show, visit twomla.com slash talk slash 221.

45:45.800 --> 46:12.560
As always, thanks so much for listening and catch you next time.

