1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:33,200
I'm your host Sam Charrington. A couple of weeks ago I spent some time at the Pegaworld

4
00:00:33,200 --> 00:00:39,040
conference in Las Vegas. The theme of the conference was automation, particularly in service

5
00:00:39,040 --> 00:00:43,720
of the customer experience, and I had a great time seeing all the advancements coming

6
00:00:43,720 --> 00:00:48,280
into this field by way of machine learning and AI.

7
00:00:48,280 --> 00:00:53,760
In this the final episode of our Pegaworld series, I'm joined by Vince Jeffs, senior director

8
00:00:53,760 --> 00:00:59,880
of product strategy for AI and decisioning at Pegasystems. Vince and I had a great talk

9
00:00:59,880 --> 00:01:06,440
about the role AI and advance analytics will play in defining future customer experiences.

10
00:01:06,440 --> 00:01:10,480
We do this in the context provided by one of his presentations from the conference, which

11
00:01:10,480 --> 00:01:16,400
explores four technology scenarios from Pegasystems innovation lives. These look at a connected

12
00:01:16,400 --> 00:01:22,420
car experience, the use of deep learning for diagnostics, dynamic notifications and

13
00:01:22,420 --> 00:01:27,480
continuously optimized marketing. We also get into an interesting discussion about how

14
00:01:27,480 --> 00:01:33,080
much is too much when it comes to hyper personalized experiences and how businesses can manage

15
00:01:33,080 --> 00:01:35,880
this challenge.

16
00:01:35,880 --> 00:01:41,160
Before we jump into the interview, a huge thanks one more time to Pegasystems for sponsoring

17
00:01:41,160 --> 00:01:46,360
this series. Amidst the discussion of automation and enhanced customer experience

18
00:01:46,360 --> 00:01:51,920
as a Pegaworld, the company announced several new AI powered capabilities as part of its

19
00:01:51,920 --> 00:01:55,960
new Pegat Infinity process automation platform.

20
00:01:55,960 --> 00:02:02,360
For more info on Pegat Infinity, head on over to pegat.com slash infinity.

21
00:02:02,360 --> 00:02:06,800
Alright, let's go.

22
00:02:06,800 --> 00:02:12,240
Alright everyone, I am here at Pegaworld in Las Vegas and I have the pleasure of being

23
00:02:12,240 --> 00:02:19,080
seated with Vince Jeffs. Vince is the senior director of product strategy for AI and

24
00:02:19,080 --> 00:02:22,960
decisioning. Vince, welcome to this week in machine learning and AI.

25
00:02:22,960 --> 00:02:24,600
Thanks Sam, great to be here.

26
00:02:24,600 --> 00:02:29,400
It's great to have you here. Let's get started by talking a little bit about your journey

27
00:02:29,400 --> 00:02:33,800
to AI. How did you get into this space? What's your background?

28
00:02:33,800 --> 00:02:39,440
Yeah, I've got a surturicus background to coming to where I am, but it's been a fun

29
00:02:39,440 --> 00:02:47,280
journey. I actually studied applied statistics at Georgetown in the University of Maryland

30
00:02:47,280 --> 00:02:54,440
long time ago and I think that's kind of what AI has become. We called it applied statistics

31
00:02:54,440 --> 00:03:00,280
back then. Now it's fancy term AI, but it's all good. From there I worked on the client

32
00:03:00,280 --> 00:03:07,480
side with UPS, working, building, decisioning and marketing technology systems. I worked

33
00:03:07,480 --> 00:03:13,040
with an agency, Rapp Collins. I worked with SAS, the business intelligence and analytics

34
00:03:13,040 --> 00:03:20,920
vendor and did that for about four years. Then I joined a company called Unica. Unica

35
00:03:20,920 --> 00:03:28,000
was actually doing predictive modeling with their first solution. Then they got into the

36
00:03:28,000 --> 00:03:32,440
marketing technology space and I spent ten years with Unica before they were bought

37
00:03:32,440 --> 00:03:41,320
by IBM. I spent a few years with IBM. I like to say, sort of, kiddingly, that my career

38
00:03:41,320 --> 00:03:48,560
took me from Big Brown to Big Blue. Then I ended up with PEGA. I spent the last four years

39
00:03:48,560 --> 00:03:56,240
with PEGA in that product strategy role. So did you know John Hogan? Yes, absolutely.

40
00:03:56,240 --> 00:04:04,480
We worked together in a past life. John's a great guy. He headed up engineering at Unica

41
00:04:04,480 --> 00:04:13,120
for years. Awesome. Tell us a little bit about your role at PEGA. What's your focus at

42
00:04:13,120 --> 00:04:17,680
PEGA systems? It's a lot of fun at this point in my career because I do get to think

43
00:04:17,680 --> 00:04:24,680
about the strategic factors that a software vendor needs to consider as they build product

44
00:04:24,680 --> 00:04:30,840
as they decide whether or not they need to maybe buy something or partner. That's really

45
00:04:30,840 --> 00:04:35,080
the kind of, when I tell people, there's three things I do. I worry about building buy-in

46
00:04:35,080 --> 00:04:42,000
and partnering and the right mix of that and when to do that. That's a fairly broad set

47
00:04:42,000 --> 00:04:48,680
of items there. I really do work closely with our product team. We have a product team

48
00:04:48,680 --> 00:04:57,240
in Amsterdam and a product team in Cambridge. We think a lot about facets of the way

49
00:04:57,240 --> 00:05:02,720
we would prioritize what we would build into our product. We want to be market driven.

50
00:05:02,720 --> 00:05:06,880
We want to do things that are innovative. We want to balance that with what our customers

51
00:05:06,880 --> 00:05:13,400
need and what they demand to make the software successful. Then we also have a lot of people

52
00:05:13,400 --> 00:05:19,800
call it technical debt that you have to always be paying back and fixing the underlying

53
00:05:19,800 --> 00:05:24,640
technology, being able to do things faster and more efficiently. We try to balance those

54
00:05:24,640 --> 00:05:28,440
three things and we've come up with a way of doing that. That's a big part of my job

55
00:05:28,440 --> 00:05:35,200
is working with everyone that's involved in that process. Then also I do work very much

56
00:05:35,200 --> 00:05:41,960
with our partner team. You're here at PEGA World with us. We announced this morning

57
00:05:41,960 --> 00:05:48,480
our launch of an ISV partner program. I've had a big hand in that and some of the partners

58
00:05:48,480 --> 00:05:52,320
that were mentioned that are going to be in that program, such as movable link and

59
00:05:52,320 --> 00:05:58,040
celebrous. In Prasado, I've worked very closely and helped bring those partners along and

60
00:05:58,040 --> 00:06:03,640
nurture them in their journey with us. Fantastic. Fantastic. You gave a couple of presentations

61
00:06:03,640 --> 00:06:09,600
here at the conference yesterday. What were those on? I did. I did. It was a fast, fun,

62
00:06:09,600 --> 00:06:16,680
furious day for me. I did a couple of presentations yesterday. One was a really cool thing that I'm

63
00:06:16,680 --> 00:06:22,520
very passionate about and that is driving new innovations into our product by having

64
00:06:22,520 --> 00:06:27,280
the idea of an innovation lab. It's not so much research. I like to think as research

65
00:06:27,280 --> 00:06:31,760
is more academic. It's more thinking about really applied ways that we can do cool new

66
00:06:31,760 --> 00:06:36,600
things in ways that we can actually affect customer journeys and customer experience

67
00:06:36,600 --> 00:06:43,760
with AI and machine learning. We did what we called for sneak peeks where we wove that

68
00:06:43,760 --> 00:06:49,280
into a story about a gal named Danielle who has a connected car and we're trying to

69
00:06:49,280 --> 00:06:53,760
predict things that might go wrong with her car. We're trying to help her experience

70
00:06:53,760 --> 00:06:59,400
by diagnosing things quicker. We're also trying to be more personalized when we communicate

71
00:06:59,400 --> 00:07:05,120
with her. Then ultimately we're trying to manage any of the communications we're doing

72
00:07:05,120 --> 00:07:10,840
to make sure we're always doing something intelligent, relevant and smart. What was the

73
00:07:10,840 --> 00:07:16,120
other presentation? The other presentation was a panel discussion. I was very thrilled

74
00:07:16,120 --> 00:07:25,440
to be able to host three of our great customers, British gas, GM and HSBC. As I kidded when

75
00:07:25,440 --> 00:07:31,920
I opened it up, we had gas, we had a car and we had financing. We had a nice mixture

76
00:07:31,920 --> 00:07:36,520
of use cases so you could go somewhere. We could go somewhere with that. That's exactly

77
00:07:36,520 --> 00:07:43,920
right. It was fun. They have great stories. I let them do most of the talking. I think

78
00:07:43,920 --> 00:07:49,600
we ended up with a nice blend of where they are today with AI and their use of it in a

79
00:07:49,600 --> 00:07:55,240
practical way and where they're on their journey to what we like to call Omni Channel

80
00:07:55,240 --> 00:07:59,560
experiences with customers and then where they're taking that, how they're expanding

81
00:07:59,560 --> 00:08:07,000
that out in their organizations to try to continue to drive more customer-centric value.

82
00:08:07,000 --> 00:08:13,000
Awesome. Maybe let's go back to that first presentation and talk through these four scenarios

83
00:08:13,000 --> 00:08:19,200
with an emphasis, of course, on the AI and machine learning bits that you were showing.

84
00:08:19,200 --> 00:08:25,800
Yeah, absolutely. As I said, we had four sneak peeks and the first one was really about

85
00:08:25,800 --> 00:08:33,040
Danielle having some car problems. There's connected cars today. GM tells a great story

86
00:08:33,040 --> 00:08:37,800
and they were on my panel and telling that story. Travis Bradburn tells that he's been

87
00:08:37,800 --> 00:08:43,040
10 years working with Pega and the connected car with GM. They really pioneered some of

88
00:08:43,040 --> 00:08:47,520
this. I based on Star in particular. On Star, exactly. Customers of GM that have the

89
00:08:47,520 --> 00:08:52,920
on-star package are familiar with the whole connected car experience. We based that

90
00:08:52,920 --> 00:08:58,480
on that sort of use case a little bit, but where we were taking it was into the deep learning

91
00:08:58,480 --> 00:09:04,720
area. We've been very big about applying what we call practical AI. There's plenty of

92
00:09:04,720 --> 00:09:12,600
models today that are very useful, supervised machine learning models such as logistic regression

93
00:09:12,600 --> 00:09:16,960
and Bayesian and lots of things that we use today, but what we are really experimenting with

94
00:09:16,960 --> 00:09:21,920
is the ability for deep learning to listen to a lot of different data sources and there's

95
00:09:21,920 --> 00:09:27,240
a lot of different data sources coming off both the connected car and Danielle herself

96
00:09:27,240 --> 00:09:33,320
using her mobile app and going on the website and engaging with the brand. We were learning

97
00:09:33,320 --> 00:09:40,400
sort of what might be going wrong with both Danielle's experiences and her car and basically

98
00:09:40,400 --> 00:09:46,880
that was about using deep learning to trigger off a early warning that there may be something

99
00:09:46,880 --> 00:09:53,640
trending bad with her car before we have to actually wait, which is the typical car

100
00:09:53,640 --> 00:09:58,120
users experience today. Suddenly, they get a check engine light and usually when they

101
00:09:58,120 --> 00:10:02,600
get that, there's already something wrong with their car. Sometimes it's something simple

102
00:10:02,600 --> 00:10:06,880
like maybe a gas caps loose, but more often than not, it's something where there's actually

103
00:10:06,880 --> 00:10:11,640
a part that's already failing. What we were trying to use is deep learning to preempt

104
00:10:11,640 --> 00:10:16,920
that to actually listen to all those different sensors and be able to give an early warning

105
00:10:16,920 --> 00:10:21,840
and maybe get that car into the shop to have a look at it before something actually breaks

106
00:10:21,840 --> 00:10:22,840
down.

107
00:10:22,840 --> 00:10:29,840
Can you take us inside the process of building out this demo in the lab? Where did the

108
00:10:29,840 --> 00:10:33,360
data come from, for example, and what were some of the specific data sources that you

109
00:10:33,360 --> 00:10:34,360
used?

110
00:10:34,360 --> 00:10:38,720
Right. The data sources were pretty much the device sensors that are on the connected

111
00:10:38,720 --> 00:10:51,640
car. We were listening to things like breaking gas mileage, oil pressure, and then any

112
00:10:51,640 --> 00:10:58,360
sensors themselves, like sensors that might be able to tell us that the oxygen levels

113
00:10:58,360 --> 00:11:04,040
in the system are trending in a bad direction. That could actually be an indication that

114
00:11:04,040 --> 00:11:10,680
some underlying part is about to fail in the near future. As we're in remember, this

115
00:11:10,680 --> 00:11:16,040
is a connected car. We're getting this data constantly. Every time they call it key

116
00:11:16,040 --> 00:11:24,360
turns, every time there's a key turn, the decision hub is being fed new data. We're able to

117
00:11:24,360 --> 00:11:30,360
learn, again, every time there's a key turn and she takes that car out on her commute,

118
00:11:30,360 --> 00:11:33,600
we're able to learn more about the performance of that vehicle.

119
00:11:33,600 --> 00:11:38,120
What we were doing is we were using deep learning to actually look into the future and say,

120
00:11:38,120 --> 00:11:42,320
we predict that the performance, not necessarily there's going to be a breakdown. It wasn't

121
00:11:42,320 --> 00:11:48,280
so much about actually a bad breakdown, but it was more about gas mileage performance

122
00:11:48,280 --> 00:11:55,640
going down. We want to be able to prevent her from just losing money on paying for unnecessary

123
00:11:55,640 --> 00:12:02,440
gas. That's the alert that we gave her. We said, nothing urgent. Your performance is degrading.

124
00:12:02,440 --> 00:12:08,320
If you come in, she was still under warranty. We'll have a look at this and then that led

125
00:12:08,320 --> 00:12:10,560
to the second sneak peak.

126
00:12:10,560 --> 00:12:16,400
You mentioned that the scenarios incorporated both data from the vehicle as well as data

127
00:12:16,400 --> 00:12:21,920
from Danielle herself did. Any of this Danielle originated data come into play in this

128
00:12:21,920 --> 00:12:29,680
first scenario? It did not in the first sneak peak. It did later on. We'll make sure

129
00:12:29,680 --> 00:12:34,000
to hit on that. You're leading us into the second sneak peak.

130
00:12:34,000 --> 00:12:40,760
What we had done is we had provided her with, again, a friendly alert when it's convenient

131
00:12:40,760 --> 00:12:45,760
and we could also automate the process of helping her schedule an appointment and using

132
00:12:45,760 --> 00:12:50,760
some AI technologies. Those aren't real fancy AI technologies, but they're good customer

133
00:12:50,760 --> 00:12:56,520
experiences to get her into a dealership maybe that she's used in the past and she's comfortable

134
00:12:56,520 --> 00:13:04,840
with and at a time that's comfortable for her. We can maybe not so much predict but give

135
00:13:04,840 --> 00:13:10,400
her convenience associated with her past behaviors.

136
00:13:10,400 --> 00:13:16,640
Then once we got her into the shop, really what we had, the second sneak peak was all about

137
00:13:16,640 --> 00:13:24,000
helping that technician through the diagnostic process. We're using some rules which Pega

138
00:13:24,000 --> 00:13:32,680
has a great rules engine along with some models to help that technician get quicker to

139
00:13:32,680 --> 00:13:40,200
a solution. As the technician was already given a next best set of service actions that

140
00:13:40,200 --> 00:13:46,560
they might look at and they were inspecting the car and saying, well, no, this is probably

141
00:13:46,560 --> 00:13:53,800
not the problem even though you're saying it's a 60% model saying it's a 60% propensity

142
00:13:53,800 --> 00:13:59,560
as soon as the agent gives the feedback to the model that that particular item wasn't

143
00:13:59,560 --> 00:14:05,520
the problem, then the model can recalculate and hone in on what is the likely problem. We

144
00:14:05,520 --> 00:14:11,240
had six or seven different things that were teed up as potential problems in this situation

145
00:14:11,240 --> 00:14:16,280
and we were able to help that technician quickly narrow down to the problem probably faster

146
00:14:16,280 --> 00:14:21,640
than they would have just buy somewhat trial and error which happens in the shop sometimes.

147
00:14:21,640 --> 00:14:29,720
It's interesting that example where we are with machine learning and AI today is kind

148
00:14:29,720 --> 00:14:36,600
of easy to think of that as passe, what's the right way to come at this. What's interesting

149
00:14:36,600 --> 00:14:42,760
to me about that specific example that you give as well, it doesn't strike us as kind

150
00:14:42,760 --> 00:14:49,640
of the way we think of machine learning and AI today. AI in the 70s and 80s was all about

151
00:14:49,640 --> 00:14:56,760
these expert systems that were trying to guide maintenance people through processes to help

152
00:14:56,760 --> 00:15:05,240
them move, help them resolve issues more quickly. It's interesting that we're kind of combining

153
00:15:05,240 --> 00:15:11,400
kind of in this rule system you're describing traditionally or older approaches or thinking

154
00:15:11,400 --> 00:15:16,280
around AI with more modern deep learning and other types of technologies.

155
00:15:16,280 --> 00:15:21,480
Absolutely and I think that's you've hit on what we feel is like you know sometimes innovation

156
00:15:21,480 --> 00:15:28,840
and I'll try to pull out a quote is rediscovering things from the past and then and then putting

157
00:15:28,840 --> 00:15:35,400
a new twist on them right or combining multiple innovations from the past and I think expert systems

158
00:15:35,400 --> 00:15:40,920
still have you know a very good role to serve in the right use cases it's just like when I'm

159
00:15:40,920 --> 00:15:45,880
sure you would agree with me with machine learning it's not about one algorithm it's not about

160
00:15:45,880 --> 00:15:51,400
this new deep learning which is you know really cool new ways to do neural networks which have

161
00:15:51,400 --> 00:15:57,240
been around for you know 20 years or more right but and there's some great advances there don't get

162
00:15:57,240 --> 00:16:01,960
me wrong but it's about picking the right algorithm for the right problems and then sometimes

163
00:16:01,960 --> 00:16:08,600
combining those together into a real solution for somebody like a technician or a car owner to

164
00:16:08,600 --> 00:16:15,400
benefit from the application of multiple sets of analytics together so yes there there wasn't one

165
00:16:15,400 --> 00:16:21,960
piece of analytics I could point to in that second sneak peak that's terribly new or innovative

166
00:16:21,960 --> 00:16:28,520
but it is the combination of these things in practical ways of application that really do drive

167
00:16:28,520 --> 00:16:34,680
value for consumers and organizations and so what was the third sneak peak then so the third

168
00:16:34,680 --> 00:16:41,720
sneak peak is a little cooler okay so so that's good because we don't want to get you know on an AI

169
00:16:41,720 --> 00:16:46,680
podcast and not have a few things that are cool to talk about so deep learning is cool and also

170
00:16:46,680 --> 00:16:52,520
this one is which is this one was about what we really called dynamic email notifications but

171
00:16:52,520 --> 00:16:57,240
we really believe this is going to any kind of dynamic treatments the customers might get and

172
00:16:57,240 --> 00:17:01,960
when I say treatments I mean you know whether you get something that is presented to you on a

173
00:17:01,960 --> 00:17:06,520
website or whether you get it in your mobile app or whether you get it in an email I would think

174
00:17:06,520 --> 00:17:12,680
of all those is sort of consumer treatments it's a brand having a conversation with you and this

175
00:17:12,680 --> 00:17:18,280
sneak peak was about making that email much more dynamic and when you think about it emails kind

176
00:17:18,280 --> 00:17:22,840
of an older channel now almost getting like direct mail right where you think yeah you know that's

177
00:17:22,840 --> 00:17:29,480
a lot of people have have you know predicted the death of email like they did cobalt right but

178
00:17:29,480 --> 00:17:36,120
as we heard on stage it at at Peggy yesterday from Affleck I think there's still cobalt running

179
00:17:37,080 --> 00:17:42,360
so so there's going to be email I believe for a while and it's still a very viable channel

180
00:17:42,360 --> 00:17:48,440
but what we're doing is making it dynamic so that it is essentially like a web page and so really

181
00:17:48,440 --> 00:17:54,200
a lot of people think about emails outbound treatment outbound marketing we really like to think

182
00:17:54,200 --> 00:18:00,040
of it now with this new sneak peak as inbound and that might sound crazy but what I mean by that

183
00:18:00,040 --> 00:18:05,720
is when that email goes out it's nothing more than a template there's containers in it that will

184
00:18:05,720 --> 00:18:11,400
be pre-populated that aren't pre-populated excuse me with content that's the way most emails

185
00:18:11,400 --> 00:18:17,160
are done today organizations have to back up weeks and get approval and decide exactly what's

186
00:18:17,160 --> 00:18:21,720
going to go into that email and then they send it out and then maybe you don't open it for another

187
00:18:21,720 --> 00:18:26,200
two or three days because you're behind on your email right or that's a personal email account

188
00:18:26,200 --> 00:18:30,520
you don't check it that often but maybe you go in there because you do a search for a brand

189
00:18:30,520 --> 00:18:35,800
you're interested in something and pop up that email now suddenly that content is three or four

190
00:18:35,800 --> 00:18:41,720
weeks stale that's not going to make for a great experience very often in the contextual world

191
00:18:41,720 --> 00:18:47,560
we live in so what we did in this sneak peak is we showed how working with a natural language

192
00:18:47,560 --> 00:18:55,560
generation partner that we're now partnered with Pursado and a dynamic email content provider

193
00:18:55,560 --> 00:19:02,360
movable link and Pega we could dynamically populate that email with content at open time

194
00:19:02,360 --> 00:19:08,120
so at open time when you open that email now it calls back to Pega it gets the next best

195
00:19:08,120 --> 00:19:13,880
content to put into that email in this case it was thanking Dan Yale for coming into the

196
00:19:13,880 --> 00:19:21,160
you know get her car service quickly and actually encouraging her to sign up to a new loyalty

197
00:19:21,160 --> 00:19:26,520
program that we had and even encouraging her with some you know points that were specific to her

198
00:19:26,520 --> 00:19:32,760
value and then the language that was used in there which is the Pursado piece was generated

199
00:19:33,640 --> 00:19:40,440
for Dan Yale so knowing what emotional language she responds to and you can do that if you have

200
00:19:40,440 --> 00:19:46,040
enough interactions you need a lot you need maybe 75 interactions with Dan Yale but if you do that

201
00:19:46,040 --> 00:19:52,040
and you test and learn you can learn that she might respond to appreciative language or

202
00:19:52,040 --> 00:19:56,920
urgent language and there's a set of emotions that Pursado sort of keeps track of and they tune

203
00:19:56,920 --> 00:20:02,840
the language and to you know what resonates with her and when you think about it that's great

204
00:20:02,840 --> 00:20:08,680
that's more like a brand having a conversation with a customer rather than having some one-size-fits-all

205
00:20:08,680 --> 00:20:15,800
treatment so there's AI happening at the Pursado level around this natural language generation

206
00:20:15,800 --> 00:20:22,680
and customization is there a lot of what I've seen at at the conference here and in the key notes

207
00:20:22,680 --> 00:20:29,160
is around optimizing the presentation of all offers kind of referring back to the the podcast I

208
00:20:29,160 --> 00:20:34,840
did with Rob Walker and the next best action and next best offer is there talk a little bit about

209
00:20:34,840 --> 00:20:42,680
the AI that's you know that is involved in that piece in this third sneak peak right right so

210
00:20:43,400 --> 00:20:52,200
when when Dan Yale opens that email Pursado has already sort of figured out probably a number of

211
00:20:52,200 --> 00:20:58,120
variations that they're going to test with Dan Yale they've maybe been again if you you know

212
00:20:58,120 --> 00:21:04,520
walk with my scenario and assume that yes we have interacted with Dan Yale before we've had a

213
00:21:04,520 --> 00:21:08,920
number of chances to test and learn with that with that language that we're going to use so they've

214
00:21:08,920 --> 00:21:14,520
got some other variations maybe call it 16 or 20 variations sitting out there that now they're

215
00:21:14,520 --> 00:21:18,760
continuing to test so that loop is going on each time we have an interaction with Dan Yale

216
00:21:19,400 --> 00:21:26,840
what we're doing is we're actually providing the the the actual piece of content that we want to

217
00:21:26,840 --> 00:21:31,240
so the image that we want to put in there maybe we want to put a you know an image of Dan Yale

218
00:21:31,240 --> 00:21:37,400
in her you know not her Dan Yale herself but somebody that you know Dan Yale might resonate with

219
00:21:37,400 --> 00:21:44,120
and the type of car she has and and then we're also putting in very specific copy that we might

220
00:21:44,120 --> 00:21:50,440
be controlling like the fact that she had just been in to the shop yesterday we can actually

221
00:21:51,320 --> 00:21:57,400
tune that to the fact that we put that language in that says thanks for coming in yesterday

222
00:21:57,400 --> 00:22:02,840
as opposed to you know just thanks for coming in so just little things like that that might sound

223
00:22:02,840 --> 00:22:08,920
a little trivial but they add up what we've what we've been able to prove is that as that email

224
00:22:08,920 --> 00:22:14,760
becomes more and more relevant and then we're populating in some very specific content again like

225
00:22:15,400 --> 00:22:22,200
and please sign up for our new rewards program that we just launched you know two days ago again we

226
00:22:22,200 --> 00:22:29,160
can be very specific about the context of that we would love to reward you with 100 value points

227
00:22:29,160 --> 00:22:36,680
for signing up and those value points again we can at real time recalculate and decide how many

228
00:22:36,680 --> 00:22:42,840
points we want to give to Dan Yale based on a model we have about her lifetime value and her

229
00:22:42,840 --> 00:22:48,280
loyalty and this sort of thing so we can be discriminative about how much points we give her versus

230
00:22:48,280 --> 00:22:55,240
somebody else based on her loyalty and we can do that again at open time if we can maybe

231
00:22:55,880 --> 00:23:01,960
digress from the sneak peeks for a second sure one question that is jumping out at me and

232
00:23:01,960 --> 00:23:07,960
this relates to a conversation I had with someone here at the event yesterday I think you know we've

233
00:23:07,960 --> 00:23:15,960
established that consumers appreciate personalized experiences we all know that you know we've

234
00:23:15,960 --> 00:23:20,360
all had the experience the the opposite of that where you you call into a call center to get

235
00:23:20,360 --> 00:23:24,760
something done you get bounced around four times and each time you have to tell them who you are

236
00:23:24,760 --> 00:23:28,920
and what the problem is right that's you know the antithesis of a personalized experience and now

237
00:23:28,920 --> 00:23:35,320
what we're talking about are these hyper personalized experiences where you're drawing on this vast

238
00:23:36,280 --> 00:23:43,000
you know database of knowledge you know based on interactions and I guess that the observation

239
00:23:43,000 --> 00:23:47,880
is that you know sometimes it comes across as creepy yeah right even for me I'm like very

240
00:23:47,880 --> 00:23:53,320
technology for I thought that's where you were going right very technology for like I love the

241
00:23:53,320 --> 00:23:58,920
technology that's enabling all this but you know I hear it and sometimes like and I'm wondering

242
00:23:59,560 --> 00:24:05,240
yeah you can talk about your general perspective on this but I'm wondering specifically

243
00:24:05,240 --> 00:24:10,840
if you've done research into you know from the perspective of innovation labs or if you've

244
00:24:10,840 --> 00:24:20,920
you know seen research elsewhere that tries to you know explore this personalization versus you

245
00:24:20,920 --> 00:24:27,160
know creepiness and and you know the way consumers you know do we have data that says that consumers

246
00:24:27,160 --> 00:24:32,440
really want that beyond the fact that they do actually click more yeah yeah it's it's it's a

247
00:24:32,440 --> 00:24:37,880
really interesting area and I'd say we don't have a lot of data on that and it is a good

248
00:24:37,880 --> 00:24:44,120
area for us as an industry to really dig into more especially with you know more of these you

249
00:24:44,120 --> 00:24:51,400
know regulations that are becoming more prominent right with like everybody knows about GDPR

250
00:24:51,400 --> 00:24:57,400
which just launched May 25th but I think that's just a that's really a swell that's going on

251
00:24:57,400 --> 00:25:02,840
it's not just in Europe I think it's happening everywhere quite frankly where consumers are waking

252
00:25:02,840 --> 00:25:07,480
up a little bit to the fact that there's been a lot of data collected about them and it's being

253
00:25:07,480 --> 00:25:13,000
collected and they need to sort of seize a little bit control of that you know let's face it

254
00:25:13,000 --> 00:25:20,520
consumers are empowered but they're also sometimes naive about what's going on in the background

255
00:25:20,520 --> 00:25:26,280
but then once they become less naive about that then they become more empowered and I think brands

256
00:25:26,280 --> 00:25:34,680
want to you know buy in large most trusted big name brands smart companies want to be responsible

257
00:25:34,680 --> 00:25:41,560
about that but they don't know where that line is exactly and they need to they need to be very

258
00:25:41,560 --> 00:25:47,240
careful about how they test that they don't want to test that in actual practice if they don't

259
00:25:47,240 --> 00:25:52,520
know what they're doing so that makes them a lot more conservative and that's not always good so

260
00:25:52,520 --> 00:25:58,280
what we believe is that they can test some of that using simulations and using other means to

261
00:25:58,280 --> 00:26:06,280
push the envelope a little bit in a lab if you will and then sort of slowly roll out and test

262
00:26:06,280 --> 00:26:15,080
that you know that line how do you test consumer attitudes and consumer reactions to you know

263
00:26:15,080 --> 00:26:21,240
a campaign and a set of interactions and via simulation yeah well I think first of all you need

264
00:26:21,240 --> 00:26:27,160
to do some of those tests with voice of the customer you know with panels and actually you know

265
00:26:27,160 --> 00:26:32,200
ask your customers right you don't have to ask all of them you can use statistics to figure out

266
00:26:32,200 --> 00:26:36,920
you know approximately whether your customer base is happy or not with this kind of a thing that

267
00:26:36,920 --> 00:26:43,560
you're doing but then what you can do is you can use simulations to kind of like run the scenarios

268
00:26:43,560 --> 00:26:51,160
that might play out in terms of things like say opt out it's not always a catastrophic event that

269
00:26:51,160 --> 00:26:57,400
happens when you cross the creepy line but it can be a bad marketing event right like if somebody

270
00:26:57,400 --> 00:27:04,120
opts out and now there's better ways to be a little bit more granular as a customer as a company

271
00:27:04,120 --> 00:27:10,680
about how you set permissions and allow consumers to set those permissions and so they you know

272
00:27:10,680 --> 00:27:16,120
you can get smart about them being able opt out of certain things but then you know entice them

273
00:27:16,120 --> 00:27:20,680
still to want to engage with your brand and other areas even if they're going to opt out of these

274
00:27:20,680 --> 00:27:26,280
these channels or these communications so I think you need to you know be smart about how you test

275
00:27:26,280 --> 00:27:34,760
that I think where you're going which is kind of interesting and it goes back to the the scenario the

276
00:27:34,760 --> 00:27:40,840
the sneak peak of you know using the data that you have and kind of additudinal

277
00:27:42,920 --> 00:27:48,120
learnings about a particular customer maybe there's a way to determine customers that will

278
00:27:48,120 --> 00:27:55,080
be creeped out and then send them more generic if you will emails or less kind of edgy

279
00:27:55,720 --> 00:28:00,760
types of emails I don't it's not clear to me like how you would learn that but it's a really

280
00:28:00,760 --> 00:28:07,000
interesting area and I guess to your to your other point you know maybe just ask right yeah yeah

281
00:28:07,000 --> 00:28:14,120
maybe ask in sample and use you know some traditional methods of statistics to extrapolate right

282
00:28:14,120 --> 00:28:20,440
but I think you know you're on to what you know we're kind of digging into a little bit which is

283
00:28:20,440 --> 00:28:25,880
you know you really have to you know you have to kind of blend that together and

284
00:28:25,880 --> 00:28:35,800
and and then you know be careful about how you roll it out and and and monitor it very closely

285
00:28:35,800 --> 00:28:40,920
right monitor what's going on is you're testing because you don't have to roll these things out

286
00:28:40,920 --> 00:28:47,400
in mass right you roll them out in very small increments when you put it into production and then

287
00:28:47,400 --> 00:28:52,520
you watch what's happening and you compare that with what you thought was going to happen in the

288
00:28:52,520 --> 00:29:00,040
simulations you did and and you adjust right that's one of the hallmarks we think of what Pega

289
00:29:00,040 --> 00:29:06,440
helps do is it really helps customers be able to be more agile about once they do do things in

290
00:29:06,440 --> 00:29:11,960
production about being able to quickly rotate and change you know directions if they need to

291
00:29:11,960 --> 00:29:17,720
or pull something back out if it's not working commonwealth bank of australia and the keynote

292
00:29:17,720 --> 00:29:23,160
this morning talked about how they were able to do those things you know in five hours now to make

293
00:29:23,160 --> 00:29:29,320
changes as opposed to something that took eight weeks to do so that's I think a key you know part of

294
00:29:29,320 --> 00:29:34,680
it is hey you know when you're using AI and when you when you're affecting customer experience

295
00:29:34,680 --> 00:29:40,280
it's going to be a test and learn process you're not going to your lab is not going to have it

296
00:29:40,280 --> 00:29:46,680
completely right right but if you have the ability to at least make an educated guess going in

297
00:29:47,320 --> 00:29:54,520
and then and ask your customers right so we touched on that and then you know pivot fairly

298
00:29:54,520 --> 00:30:00,680
rapidly when you see that it's not exactly going as we drew it up on the drawing board that's key

299
00:30:00,680 --> 00:30:05,720
and I think you can you know get smart then about how you treat those customers in which ones like

300
00:30:05,720 --> 00:30:14,200
you said which ones your modeling which ones you feel are you know going to be more sensitive versus

301
00:30:14,200 --> 00:30:18,840
not and and then your models can learn and get smarter about that is you get more and more

302
00:30:18,840 --> 00:30:23,960
customers that you're learning you know you're getting more granular about the sensitivity almost

303
00:30:23,960 --> 00:30:29,000
like prosados getting more and more granular about the language that you respond to you get more

304
00:30:29,000 --> 00:30:33,880
granular about your ability to decide whether Sam's going to be creeped out or Vince is going to

305
00:30:33,880 --> 00:30:41,320
be creeped out interesting interesting so popping back over to this fourth sneak peak yep yep so

306
00:30:41,320 --> 00:30:49,640
the fourth sneak peak the the story kind of culminated with with Danielle being happy being

307
00:30:49,640 --> 00:30:55,800
in a back in a happy place right she had her car fix she got this you know offered a sign up for

308
00:30:55,800 --> 00:31:01,960
this loyalty program and get points so she accepted that everything was looking great until the

309
00:31:01,960 --> 00:31:08,280
next morning she gets in an accident okay so we had to add a little drama to the to the scenario

310
00:31:08,280 --> 00:31:14,680
and that and of course she has a connected car so we have some we have some intelligence about

311
00:31:14,680 --> 00:31:21,400
maybe how bad that accident was now we don't want to cross the creepy line you know so we don't

312
00:31:21,400 --> 00:31:28,680
like reach out to her or anything but what we do do is and and the scenario continued with her

313
00:31:28,680 --> 00:31:34,280
then get it wasn't a major accident so the airbag didn't go off but she got on right after that

314
00:31:34,280 --> 00:31:40,600
happened she got on her mobile and she was looking on our service pages right maybe she was looking

315
00:31:40,600 --> 00:31:46,200
up to see if you know it's cosmetic damage you know sort of whatever whatever she needed to kind

316
00:31:46,200 --> 00:31:52,200
of you know do a little investigation on and that were those were cues that we were picking up

317
00:31:52,200 --> 00:31:57,400
so we're picking up the fact that suddenly Danielle's like looking at service pages and looking

318
00:31:57,400 --> 00:32:04,200
at these pages that nothing to do with maybe buying a new car or you know how to use her mobile app

319
00:32:04,200 --> 00:32:10,760
so we decided that and we call it next best moment that we had some marketing we had a marketing

320
00:32:10,760 --> 00:32:16,680
treatment teed up to go out to her in two days after sneak peak number three had concluded and

321
00:32:16,680 --> 00:32:21,160
that was going to be we were going to make her this big great offer on a new car because she had

322
00:32:21,160 --> 00:32:28,680
a older connected car well we decided that the next best action and the next best moment for that

323
00:32:28,680 --> 00:32:33,960
was going to be you know that night because she checks emails that night and we kind of learn

324
00:32:33,960 --> 00:32:39,960
that that's the time she's most likely to engage with emails what we did is we actually took that

325
00:32:39,960 --> 00:32:46,200
communication out of the queue because we recognize that there's something going on we don't know exactly

326
00:32:46,200 --> 00:32:53,400
what it is but we predict that she's probably going to be calling us you know for some more service

327
00:32:53,400 --> 00:32:59,640
and so therefore it's not time to be marketing to her so we actually pause that you know maybe

328
00:32:59,640 --> 00:33:05,240
we'll resume it later on again when you know when the time's right but the time wasn't right so we

329
00:33:05,240 --> 00:33:12,680
pause that and we really manage that next touch point proactively interesting interesting and

330
00:33:12,680 --> 00:33:21,400
so across the maybe talk a little bit about what what goes into that from is there MLAI

331
00:33:22,840 --> 00:33:29,160
involved in that fourth scenario and and how is it expressed yeah yeah good question we like

332
00:33:29,160 --> 00:33:34,680
to categorize decision management and the arbitration of decisions as AI we think it's fair

333
00:33:34,680 --> 00:33:39,880
actually there's there's been you know other third parties like Forster that have agreed with

334
00:33:39,880 --> 00:33:45,800
that that decision management is a form of AI I guess those things can be argued by the AI

335
00:33:45,800 --> 00:33:51,480
purists right and and you know to some extent I understand their arguments that you know let's say

336
00:33:51,480 --> 00:33:57,240
basic rules or even some of the basic robotics that's just pure automation right there isn't anything

337
00:33:57,800 --> 00:34:04,200
you know terribly sophisticated algorithmically going on let's just for the sake of this

338
00:34:04,200 --> 00:34:10,680
conversation say that that's part of AI decision management so we can we can decide the you know

339
00:34:10,680 --> 00:34:16,600
the the decisions that we're going to make either today when Danielle comes in channel in the

340
00:34:16,600 --> 00:34:21,720
mobile app we decide what we want to actually present in a container there or on the website

341
00:34:22,440 --> 00:34:28,120
but we also can look at the decisions that we've teed up right that our marketing organization

342
00:34:28,120 --> 00:34:34,680
has teed up and they're in a queue ready to go out and and we can arbitrate all that we can capture

343
00:34:35,000 --> 00:34:40,040
you know things we can sort of you know capture and kill things if we need to or capture and pause

344
00:34:40,040 --> 00:34:46,680
things so that was really more about again not fancy AI technology so to speak but the orchestration

345
00:34:46,680 --> 00:34:52,040
of decisions across channels which again what we've seen is that that that just has tremendous

346
00:34:52,040 --> 00:35:00,040
benefits for organizations yeah and that's been a core theme across the keynotes that I've seen here

347
00:35:00,040 --> 00:35:06,440
and and one of the things that I've observed is that a lot of emphasis has been placed on

348
00:35:07,080 --> 00:35:15,320
the distinction between this traditional approach to marketing that is characterized by

349
00:35:15,320 --> 00:35:23,960
set segments that are determined say on the on the period of you know weeks you know versus a

350
00:35:23,960 --> 00:35:30,440
continually optimized marketing loop that's in a real time putting the other offers for

351
00:35:31,800 --> 00:35:37,720
for customers and you know this model extends beyond marketing but marketing is an example that's

352
00:35:37,720 --> 00:35:42,520
been frequently used can you allow rate on that distinction and how you see that playing out

353
00:35:42,520 --> 00:35:48,600
absolutely I think that's a really important distinction and it actually is a you know we sometimes

354
00:35:48,600 --> 00:35:53,720
call it like a hundred and eighty degrees shift in in the way you think about how you approach

355
00:35:54,760 --> 00:35:59,400
understanding a customer and then taking action on that customer so when you use the more

356
00:35:59,400 --> 00:36:04,760
traditional approach as you called it which by the way is has you know still has applications

357
00:36:04,760 --> 00:36:10,040
today in some cases where you just simply don't don't have any customer data right if you have

358
00:36:10,040 --> 00:36:14,600
to go out and do advertising and you really don't have customers that are aware of you you might

359
00:36:14,600 --> 00:36:19,640
have to use segmented approach to reach those customers but once you have some customer data

360
00:36:19,640 --> 00:36:24,600
and that doesn't take very long as an organization right once you or on board a customer

361
00:36:24,600 --> 00:36:30,120
and they've transacted with you a little bit you've got a wealth of information about that individual

362
00:36:30,120 --> 00:36:37,240
customer why not use that in the moment for any conversation or treatment that you're going to

363
00:36:37,240 --> 00:36:44,920
have with that customer as opposed to putting them into buckets where they're going to be you know

364
00:36:44,920 --> 00:36:52,760
batch treated um sometimes uh in in a way that lots of other customers will be treated and maybe

365
00:36:52,760 --> 00:36:57,800
a few of those it's very relevant too but maybe a lot of them it doesn't resonate very well

366
00:36:57,800 --> 00:37:05,320
so it really is a more always on transactional approach to say that what we're going to do is we're

367
00:37:05,320 --> 00:37:12,040
really going to let this engine wake up and get the data that it's got about this customer

368
00:37:12,040 --> 00:37:19,480
as of right now and make a decision of what to put into something and do that populate it

369
00:37:19,480 --> 00:37:24,600
in real time literally in hundreds of milliseconds doesn't matter if it's a web container

370
00:37:24,600 --> 00:37:29,480
doesn't matter if it's a mobile app that's calling to get some next best action doesn't matter

371
00:37:29,480 --> 00:37:36,440
if it's a you know call center rep that that needs more information or wants something recalculated

372
00:37:36,440 --> 00:37:42,680
but you do that in real time as opposed to as you said sort of pre-calculating it sometimes weeks

373
00:37:43,480 --> 00:37:48,840
you know ago which things can lose their relevance very rapidly maybe that customer you know

374
00:37:48,840 --> 00:37:54,120
bought something that you're now you know you're you're marketing to them something they already

375
00:37:54,120 --> 00:37:59,960
you know purchased or or they're now interested in something else and you're not factoring that

376
00:37:59,960 --> 00:38:05,560
in to your conversation you need to sell this to Amazon because they do that all the time they do

377
00:38:06,920 --> 00:38:11,560
you were just shopping for shoes yes I bought this shoes why are you still telling me about this

378
00:38:11,560 --> 00:38:16,520
shoes and you know that's because that's a you probably know that's an algorithm that you know

379
00:38:16,520 --> 00:38:21,960
collaborative filtering and wisdom of the crowd algorithm again it has its applications

380
00:38:21,960 --> 00:38:27,960
especially when you've got tight product affinities it can matter you know this tie goes with this

381
00:38:27,960 --> 00:38:33,160
shirt sort of thing but then there's other times when that algorithm isn't working so well

382
00:38:33,160 --> 00:38:38,360
right and where you know something about that customer you need to use that intelligence rather

383
00:38:38,360 --> 00:38:45,080
than just the fact that other people buy these things together yeah yeah um and so maybe to to wrap

384
00:38:45,080 --> 00:38:54,120
up the session that you did with HSBC it was GM GM British gas British gas what were the highlights

385
00:38:54,120 --> 00:39:01,480
of that session yeah the highlights were really we spent about half the time talking about

386
00:39:01,480 --> 00:39:07,000
how they got to where they are and the fact that they're not there yet right they're getting

387
00:39:07,000 --> 00:39:12,520
great value but it's it's a job in these big these are all big companies right huge companies

388
00:39:12,520 --> 00:39:21,160
and so it's a long journey to to get a big company to really change its mindset about how it

389
00:39:21,160 --> 00:39:26,680
approaches these things and and to do that across all the channels that they're interacting with

390
00:39:26,680 --> 00:39:33,000
uh in in case of GM they started with on star that's great on star customers are getting a more

391
00:39:33,000 --> 00:39:39,400
dynamic and you know personalized and you know they're using more data to help improve the experience

392
00:39:39,400 --> 00:39:45,320
of the on star customer but what about the all the other GM customers that don't use on star right

393
00:39:45,320 --> 00:39:50,840
and so Travis talked about how they're starting to roll that out into more of their you know

394
00:39:50,840 --> 00:39:58,360
programs beyond just the on star program British gas uh Joe Allen told a similar story about where

395
00:39:58,360 --> 00:40:03,320
they really did tackle a lot in their first couple of years of putting peg in they really felt like

396
00:40:03,320 --> 00:40:08,280
you know if you're a gas or electricity customer you don't interact with those brands too often

397
00:40:08,280 --> 00:40:13,400
right but when you do it's really important right it might just be before your contract

398
00:40:13,400 --> 00:40:17,720
is up or so there's these really important moments of truth where you've got to get

399
00:40:17,720 --> 00:40:22,600
things well with synchronized so she talked about how they they couldn't start in just one

400
00:40:22,600 --> 00:40:29,000
channel they really went in you know in and she calls it we did inbound outbound and we ended up

401
00:40:29,000 --> 00:40:35,320
with unbound um which was I I love the tagline because the idea there is it's more channelless

402
00:40:35,320 --> 00:40:41,880
it's really not about channels it's about making decisions for the customer and being coordinated

403
00:40:41,880 --> 00:40:48,280
across omnichandals um but then she talked about how they're taking that to paid media um and

404
00:40:48,280 --> 00:40:53,640
you know outside of their own media which is really interesting and then she also talked about

405
00:40:53,640 --> 00:40:57,800
how they're using it now in their loyalty program and they're expanding the use of it in their

406
00:40:57,800 --> 00:41:05,080
loyalty program so um and then HSBC also talked along similar lines Fabian uh was great

407
00:41:05,080 --> 00:41:11,480
about explaining how the bank is really interested in getting those experiences right when customers

408
00:41:11,480 --> 00:41:17,240
are doing things on the website you know when they're when they're using a mortgage loan calculator

409
00:41:17,240 --> 00:41:23,720
that's incredible information you know if they stop using it suddenly you know there's opportunities

410
00:41:23,720 --> 00:41:28,680
to again reach out and try to help the customer maybe they were struggling with something you know

411
00:41:28,680 --> 00:41:33,720
maybe there's questions that you've got to answer for them and so they he told you know great

412
00:41:33,720 --> 00:41:40,200
examples of where brands need to continue to think about AI and the ability for it to drive

413
00:41:40,200 --> 00:41:45,800
you know self-service but also how the human comes into the loop in those it's really important

414
00:41:45,800 --> 00:41:52,920
that the agents be well equipped and they can get AI help to be better equipped to have great

415
00:41:52,920 --> 00:41:59,480
conversations with customers and be more you know be more convenient and more helpful with them right

416
00:41:59,480 --> 00:42:04,040
so we don't have those experiences that we started out with that you talked about with somebody

417
00:42:04,040 --> 00:42:09,480
bouncing around an IVR you know getting a rep that transfers you to another rep that asks you

418
00:42:09,480 --> 00:42:14,760
the same questions over and over and you're just about to pull your hair out um so it is really

419
00:42:14,760 --> 00:42:21,480
important to get the human well you know integrated into that loop with AI and you know um Fabian told

420
00:42:21,480 --> 00:42:27,480
great examples of how they're doing that fantastic Vince it's been great catching up on what you're

421
00:42:27,480 --> 00:42:33,240
up to anything you'd like to add to close us up no it's been great to talk here with you Sam it's

422
00:42:33,240 --> 00:42:40,680
it's an exciting topic I love your program and um I'm passionate about it as you can tell I know

423
00:42:40,680 --> 00:42:49,880
you are too so thanks for having me on absolutely thank you all right everyone that's our show for

424
00:42:49,880 --> 00:42:56,520
today for more information on Vince or any of the topics covered in this episode head over to

425
00:42:56,520 --> 00:43:06,120
twimmelai.com slash talk slash 154 to follow along with the pegaworld series visit twimmelai.com slash

426
00:43:06,120 --> 00:43:15,960
pegaworld 2018 for more information on pegassystems for their new pegat infinity offering visit pegat.com

427
00:43:15,960 --> 00:43:35,400
slash infinity as always thanks so much for listening and catch you next time

