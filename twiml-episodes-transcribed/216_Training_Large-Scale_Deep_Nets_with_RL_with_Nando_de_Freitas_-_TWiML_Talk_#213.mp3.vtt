WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.440
people, doing interesting things in machine learning and artificial intelligence.

00:21.440 --> 00:34.520
I'm your host Sam Charrington. Today we close out our NURP series and our 2018 conference

00:34.520 --> 00:40.000
coverage with this interview with Nando DeFratus, team lead and principal scientist at Deep

00:40.000 --> 00:46.240
Mind and Fellow at the Canadian Institute for Advanced Research. In our conversation,

00:46.240 --> 00:50.600
we explore Nando's interest in understanding the brain and working towards artificial

00:50.600 --> 00:56.000
general intelligence through techniques like meta learning, fuchsot learning and imitation

00:56.000 --> 01:01.840
learning. In particular, we dig into a couple of his team's NURP's papers, playing hard

01:01.840 --> 01:07.480
exploration games by watching YouTube and one shot high fidelity imitation, training

01:07.480 --> 01:11.880
large-scale deep nets with reinforcement learning. Enjoy.

01:11.880 --> 01:17.920
Alright everyone, I am here with Nando DeFratus. Nando is a team lead and principal scientist

01:17.920 --> 01:25.640
at Deep Mind as well as a Fellow of the Canadian Institute for Advanced Research or CIFAR

01:25.640 --> 01:29.840
as you might know it. Nando, welcome to this week in machine learning and AI.

01:29.840 --> 01:32.880
Thank you. Thank you very much for having me here. It's such a pleasure.

01:32.880 --> 01:37.680
Absolutely. Absolutely. It's great to finally get a chance to speak with you. We've been

01:37.680 --> 01:43.720
trying to coordinate something for a while now, but fortunately NURP brings us all together.

01:43.720 --> 01:49.400
I'd love to hear a little bit about your background and your path into ML&AI.

01:49.400 --> 01:57.440
Yes, so I got into ML&AI a very long time ago. We're going back to prehistoric times here.

01:57.440 --> 02:03.280
So actually it was around, it was in 1994, when I was an underground at the University of

02:03.280 --> 02:10.440
it, Father's Rund in Johannesburg, South Africa. And Professor, my control professor, introduced

02:10.440 --> 02:18.040
me to this thing called Neural Networks. I was fascinated by neural networks and tried

02:18.040 --> 02:24.920
to learn as much about them as possible, 100 maps by propagation, and ended up implementing

02:24.920 --> 02:31.680
something that eventually went into hardware and it became a controller for pneumatic

02:31.680 --> 02:39.600
control valves. With that work, I eventually even made it to an international conference.

02:39.600 --> 02:49.960
That was, I still remember 26-hour trip from my home to my first conference in Washington.

02:49.960 --> 02:57.600
It was extremely intimidating because I was the skit from Africa. And there I was meeting

02:57.600 --> 03:04.720
all these big shots and people that I thought I would never be able to ever hang out with.

03:04.720 --> 03:12.960
They were like heroes to me. Some folks like David Hackerman, Underbarco and a few others,

03:12.960 --> 03:20.680
like very prominent people in the field. It was a great experience. It was intimidating.

03:20.680 --> 03:25.720
And when I went back to South Africa, that made me wanted to do a masters. And so I continued

03:25.720 --> 03:33.440
my work with neural networks. And eventually through that, I was able to apply to universities

03:33.440 --> 03:39.200
and I got lucky, got a scholarship to Cambridge, and I went there, and I continued working

03:39.200 --> 03:47.600
on based neural networks. And that was my PhD. And it became very statistical, ended up going

03:47.600 --> 03:53.840
to Berkeley. And by just fluke, my supervisor there, still at Russell, was in computer science.

03:53.840 --> 03:59.680
So I somehow became a computer scientist. And eventually after two years there, I moved

03:59.680 --> 04:04.680
to UPC where I was a professor of machine learning, introducing some of the undergrad courses

04:04.680 --> 04:11.480
that you probably have seen on YouTube. And it was there that I first started participating

04:11.480 --> 04:20.000
in the SIFER program. And this was organized chiefly by Professor Jeff Hinton. And the

04:20.000 --> 04:28.880
goal back then was to understand the brain. And this was so amazing because it was suicide

04:28.880 --> 04:36.640
in those days to put a grant with that title. You would not get tenure. This was like a crazy

04:36.640 --> 04:44.840
thing to do. But fortunately, some of the key leaders in Canada believed in that ambitious

04:44.840 --> 04:53.360
vision. They're still there and they continue inspiring us. And that's what we did. We embraced

04:53.360 --> 05:00.160
that vision. Our research sort of steered toward that at the time, more different, more

05:00.160 --> 05:07.200
difficult path. And now it seems it's very easy and retrospect. What we do. Yeah, and

05:07.200 --> 05:10.880
that's kind of how I got into deep learning and how I got here.

05:10.880 --> 05:15.920
Have you since returned to applications in control systems?

05:15.920 --> 05:21.240
Yeah, I continue doing a lot of that. So a lot of the knowledge that I gained from my

05:21.240 --> 05:29.600
undergrad in my masters in control, things about certainly the automatic thinking about

05:29.600 --> 05:34.960
our differential equations, knowing how to sort of map things of legacy domain and so on.

05:34.960 --> 05:41.800
But as well as being able to do things like PID controllers, those are still the things

05:41.800 --> 05:48.240
that we use today. Like when we try to control robots and we might use neural networks

05:48.240 --> 05:53.440
these days to sort of set up parameters, automatic, of different PID controllers. But you

05:53.440 --> 05:55.440
know, all that background has been very useful.

05:55.440 --> 06:01.680
Yeah, those are some of my favorite courses from grad school, PID controllers and hysteresis

06:01.680 --> 06:12.840
and all these different things. But I've had the impression that that stuff is, you know,

06:12.840 --> 06:18.800
works so well and there's such a need for more deterministic response for a lot of

06:18.800 --> 06:25.400
the applications of this that there hasn't been much in terms of neural network applications

06:25.400 --> 06:28.000
for that. But it sounds like you've been doing that for a while.

06:28.000 --> 06:36.760
Yes, no, there's quite a few people who work with this. So control and the sort of combination

06:36.760 --> 06:43.680
of classical control techniques with the new, sort of the reinforcement learning techniques.

06:43.680 --> 06:47.040
Yeah, it's actually a very fruitful area of research. And I think a lot of the work that

06:47.040 --> 06:52.880
folks like Peter Abel at Berkeley do, like Michael Thunderpan at UBC, Emma Chauderof at

06:52.880 --> 06:58.480
the University of Washington, it all sort of involves a sort of a bit of a marriage between

06:58.480 --> 07:03.560
the classical techniques and the new techniques. And I think there's still a lot we can learn

07:03.560 --> 07:09.480
from classical control as well in terms of robustness and constraints. Because as we build

07:09.480 --> 07:14.720
controllers, we want to make sure that they're safe, that they we can verify them and so

07:14.720 --> 07:20.440
on. So these, these continue being challenges that we have to address in order to deploy

07:20.440 --> 07:21.440
the technology.

07:21.440 --> 07:27.080
So tell me a little bit about your research interests. What are, it sounds like you kind

07:27.080 --> 07:32.120
of definitely a lot of different things, but how do you kind of characterize the general

07:32.120 --> 07:35.080
scope and direction of your research?

07:35.080 --> 07:43.120
I think fundamentally it's still the same that it's always been. It's trying to understand

07:43.120 --> 07:50.040
the brain, trying to understand what it is to be me, what it is for you to be you and

07:50.040 --> 07:59.040
for us to be engaged in this discussion. That's the real goal. And of course we have to

07:59.040 --> 08:04.880
sort of break this problem in order to be able to attack it. We have to think of like what

08:04.880 --> 08:09.280
question, how to formulate the question. And often that's what leads to things like

08:09.280 --> 08:14.760
RL, you know, you can specify tasks and cost functions. It's a language to talk about

08:14.760 --> 08:20.360
the problem. We can sort of think of it in terms of cognitive abilities that we have

08:20.360 --> 08:26.720
like memory or imagination, curiosity and so on.

08:26.720 --> 08:32.040
So there's many ways to attack the problem of understanding the brain. And I think this

08:32.040 --> 08:38.800
is a huge problem. And there's so much yet to be done. And I've picked a few directions.

08:38.800 --> 08:44.960
I see my colleagues have picked out the directions. And I think it's going to involve many

08:44.960 --> 08:53.000
perspectives for us to eventually come up with an understanding at many levels of abstraction

08:53.000 --> 09:02.400
of what it is in the sense to be human. Do you think that our understanding of the brain

09:02.400 --> 09:09.520
has benefited our understanding of machine learning or the practice of machine learning

09:09.520 --> 09:18.840
more or less than machine learning has benefited our understanding of the brain? That's an

09:18.840 --> 09:23.200
interesting question. So it is very clear that the understanding of the brain has helped

09:23.200 --> 09:31.640
us build a get here. It's a couple years, but actually four years ago, I went to Japan

09:31.640 --> 09:38.120
and I had the honor of meeting Professor Fukushima, who was the author of the New Cognitron,

09:38.120 --> 09:43.320
which is essentially the convolutional neural network architecture that we all use these

09:43.320 --> 09:50.880
days. Janakun added backpropagation to that architecture and that kind of is what brought

09:50.880 --> 09:59.960
us here. That was a success story and vision and so on. When I spoke to Professor Kitagawa,

09:59.960 --> 10:06.920
he mentioned that we're inspired him to do that work was the work of Hubel and Vizel

10:06.920 --> 10:14.400
and Neuroscience. When he worked at his institute, I forget the name of the institute, but they

10:14.400 --> 10:21.800
put neuroscientists and computer scientists together. It was through this combination

10:21.800 --> 10:27.320
of different disciplines working together that he found out about these results in Neuroscience

10:27.320 --> 10:32.280
and decided to code this model. So I think the neuroscientists had a much greater

10:32.280 --> 10:42.480
potential. I'm told a set of writers was also influenced by cortical models of cells and

10:42.480 --> 10:50.400
so on in order to propose LSTM, which also one of the key components of machine learning.

10:50.400 --> 10:56.600
Has it helped the other way around? That's a much harder question. I mean, clearly with

10:56.600 --> 11:02.880
the success of backpropagation and so on, a few researchers at MIT, for example, they

11:02.880 --> 11:07.520
went on and they were starting to use that new knowledge to explain how the visual cortex

11:07.520 --> 11:16.320
might be working and so on. I also think more recently, folks are starting to contract

11:16.320 --> 11:22.440
coding and so on. Trying to look at the research that has happened, not just in machine learning,

11:22.440 --> 11:28.760
but the combination of statistics and econometrics and machine learning and biostatistics, looking

11:28.760 --> 11:36.880
at things like causal modeling in order to go back and try to understand, to try to formulate

11:36.880 --> 11:42.080
the theory of how it is that we should approach neuroscience and what sort of findings we're

11:42.080 --> 11:47.440
looking for. So I think to some extent, it also has penned out the other way.

11:47.440 --> 11:54.080
Yesterday, at the poster sessions, I was checking out one of the posters that was making

11:54.080 --> 11:59.240
kind of a strong correlation between what's happening in the brain and image processing

11:59.240 --> 12:05.840
and CNNs and I always thought of neuro networks in general and CNNs as kind of quote-unquote

12:05.840 --> 12:11.920
inspired by the brain, but not really exhibiting a very strong correlation. But they were talking

12:11.920 --> 12:20.040
about layers as like V1, V2, V4, whatever, and IT, which presumably are related to a model

12:20.040 --> 12:25.240
of how we think the visual information is processed in the brain. I didn't realize that there

12:25.240 --> 12:28.680
was that strong correlation between these two fields.

12:28.680 --> 12:35.720
Yeah, there appears to be. Certainly, for example, it's SIFAR. So every year, before

12:35.720 --> 12:42.360
New Europe, there's another workshop that takes place. It's a two-day workshop called the SIFAR meeting.

12:42.360 --> 12:48.840
And actually, I just came from it. There, we always have a combination of invite people

12:48.840 --> 12:53.680
working in neuroscience as well as people working in computer science and engineering physics

12:53.680 --> 13:02.480
and some people from different areas, from causality, et cetera. And we try to sort of use

13:02.480 --> 13:10.000
everyone's insights to sort of advance what we're mainly to choose what to research areas

13:10.000 --> 13:17.160
to explore next. So I think neuroscience has always informed what we do in machine learning

13:17.160 --> 13:21.600
and machine learning, I think, also informed neuroscience. So certainly, at least at that

13:21.600 --> 13:23.600
meeting, that's the case.

13:23.600 --> 13:31.040
So the driving motivation is to try to understand the brain, so that kind of reverse part of

13:31.040 --> 13:38.560
the feedback loop. And one of the recent areas that you've been exploring to help get there

13:38.560 --> 13:46.240
is been imitation learning. And in particular, you contributed to a paper here that's focused

13:46.240 --> 13:51.080
on third-party imitation learning. Let's just dive in. Tell us about that paper. What's

13:51.080 --> 13:52.080
the motivation there?

13:52.080 --> 14:00.040
Okay, so I'm excited about imitation because imitation is something that we all do. A lot

14:00.040 --> 14:07.640
of the things we learn are through a process of imitation. And in particular, I'm interested

14:07.640 --> 14:15.760
in a form of imitation called fused shot imitation, which is sort of follows from, you can sort

14:15.760 --> 14:22.560
of think of it in terms of meta learning. So the idea of meta learning is evolution is

14:22.560 --> 14:28.680
a learning process that happens at a very slow scale and it produces biological machines,

14:28.680 --> 14:36.120
you know, us. Machines are capable of learning very rapidly with very few data. We don't

14:36.120 --> 14:43.880
experience that much data throughout our lifetimes. And so we kind of want to do the same thing.

14:43.880 --> 14:51.960
We want to sort of learn machines that can imitate very rapidly. So we trained them for

14:51.960 --> 14:58.720
a very expensive process, but then we can deploy this machine and someone can show, demonstrate

14:58.720 --> 15:05.560
something to this machine and then the machine is able to do that. Now then comes another

15:05.560 --> 15:13.080
question of when you demonstrate, your demonstration might be with different objects, so different

15:13.080 --> 15:20.600
hands. And so you need to now deal with the third person here. What we mean by that is

15:20.600 --> 15:26.240
that you might have learned to do the task using, I don't know, a robot hands. And now you

15:26.240 --> 15:33.320
have to watch human hands doing the task and then you have to, the robot has to imitate.

15:33.320 --> 15:39.520
So very much like when children can imitate a dog by using their foot to scratch behind

15:39.520 --> 15:46.240
their ear. Right. And they're able to remap from one type of body to another body quite

15:46.240 --> 15:56.200
easily. And the contrast is, I guess first person imitation learning, which is, for example,

15:56.200 --> 16:02.120
you're looking at the actions that someone is playing in playing a game and learning

16:02.120 --> 16:07.080
based on those actions as opposed to just watching them, quote unquote, from a distance.

16:07.080 --> 16:14.040
Correct. So you precisely said in first person in Atari, for example, if you're playing

16:14.040 --> 16:18.080
Atari, first person, you would collect data from playing the game in Atari. We tried

16:18.080 --> 16:25.440
to fit the model to it. In third person, what we do is we actually go to YouTube and we

16:25.440 --> 16:30.280
look at people playing the games. And it's third person because there's a bit of, there's

16:30.280 --> 16:35.520
a domain gap. You know, in YouTube, sometimes this video is, well, the video will definitely

16:35.520 --> 16:42.320
be of a different resolution, different color. It will have ads inserted. It has all sorts

16:42.320 --> 16:48.720
of other artifacts. And so we need to use, be able to remap from that thing in YouTube

16:48.720 --> 16:54.920
to our domain so that we can play a play the game. And the inspiration for this for me

16:54.920 --> 17:06.240
was like watching a child, my nephew. Spending a lot of time playing, what's that thing

17:06.240 --> 17:14.960
that gets played? Minecraft. So he spent a lot of time watching Minecraft. And I remember

17:14.960 --> 17:18.520
telling my brother, you're not worried about this and my brother's like, oh no, he's

17:18.520 --> 17:22.080
just actually very good. He does really well in school when he does this sort of thing.

17:22.080 --> 17:29.800
And what he does is he watches these videos and then he goes and plays. And so I realized,

17:29.800 --> 17:36.400
well, that makes sense. This is what we should do. And so then I, you know, I work with these

17:36.400 --> 17:41.920
very gifted scientists and engineers who took the idea and went and implemented this

17:41.920 --> 17:48.000
for in the context of Atari. I think this sort of an initial step, which is more than anything

17:48.000 --> 17:55.120
just a demo of what could be done. I think there's a lot of footage in YouTube for all sorts

17:55.120 --> 18:01.000
of things. And so the hope would be that we eventually could train, say, Robert Manipulate

18:01.000 --> 18:09.800
is or in simulation or with real robots to be able to watch YouTube and be able to even

18:09.800 --> 18:14.200
though there's a domain gap because it's, you know, it's, they're watching different

18:14.200 --> 18:21.960
human hands say tying knots. And then they would have to then tie knots themselves. That's

18:21.960 --> 18:26.800
not actually easier. Like, tying shoelaces, for example, is something that even six-year-old

18:26.800 --> 18:35.560
human struggle. Interesting. I think I've come across videos that or examples of research

18:35.560 --> 18:43.000
that is trying to do some of that kind of thing, but not with actual instantiated robots,

18:43.000 --> 18:49.880
but like animations. So like, watch a video of dancing and try to get the stick figure

18:49.880 --> 18:55.160
to dance or things like that. It seems like there are a lot of parallel activities kind

18:55.160 --> 19:00.000
of going after the same type of problem. Correct. A lot of this can be done in simulation

19:00.000 --> 19:07.680
in a sense. It's easier to do it in simulation. Robots are basically machines that complicate

19:07.680 --> 19:15.360
to this. And they're not produced by a typically a man. They're like cars, except that with

19:15.360 --> 19:21.240
cars, we have these big companies that produce these nicely engineered machines. And robots

19:21.240 --> 19:27.440
are more like cars when they started appearing in the world, you know, they sort of make shift

19:27.440 --> 19:33.400
Frankenstein type things put together. And so they break down a lot of the time. It's

19:33.400 --> 19:39.640
very hard to do robotics. The software is somewhat lagging and so on. But nonetheless,

19:39.640 --> 19:44.920
it's interesting. Manufacturing keeps being one of the automation and manufacturing

19:44.920 --> 19:50.680
keeps being one of the driving forces behind, you know, control theory. In fact, and control

19:50.680 --> 19:58.520
and machine learning. And so has your work in this area with playing the Atari games?

19:58.520 --> 20:04.520
Have you is that also still in simulation or do you have some robotic thing playing the

20:04.520 --> 20:11.320
game? So that's in simulation. But we have been doing other works also doing imitation.

20:11.320 --> 20:19.400
And in particular, doing this few short imitation, where we do, we use simulation. So physics

20:19.400 --> 20:27.400
accurate simulation through an engine called Majoko. It's kind of what the OpenAI GM uses.

20:27.400 --> 20:35.200
And then we also use real robots to do these sort of things. And so we've been very interested

20:35.200 --> 20:42.600
in this sort of few short imitation, which is basically imitation with a few data. The

20:42.600 --> 20:48.600
challenge here is and the challenge being the challenge of what I call few short metal

20:48.600 --> 20:57.520
learning is to get a machine and being able to sort of demonstrate something new to the

20:57.520 --> 21:03.920
machine with novel objects with novel motion and have the machine be able to do the same

21:03.920 --> 21:12.920
thing. So if you think so, I want to go beyond building machines like classifiers, like

21:12.920 --> 21:21.360
a CNN or and instead, they're learning machines. They're machines that I can give to someone

21:21.360 --> 21:27.920
at a factory. And that person can then modify the machine via demonstration or via some

21:27.920 --> 21:34.520
formal language or natural language and get the machine to do something new or do it slightly

21:34.520 --> 21:42.920
different. So I always think of the next generation of AI machines as adaptable machines or tools

21:42.920 --> 21:49.920
that learn. So it's very much like when we give tools to animators. I think to people in

21:49.920 --> 21:54.640
factories and so on, what we need to do is give them tools that they can adjust and adapt

21:54.640 --> 22:01.640
and become much more productive. So they don't have to do all this sort of low level coding

22:01.640 --> 22:08.200
in order to or change hardware, etc. In order to deal with the fact that maybe the

22:08.200 --> 22:16.160
lids of the water bottles have changed in size. So when we're learning as humans, one

22:16.160 --> 22:23.160
of the things that we get to take advantage of is this relatively vast amount of quote unquote

22:23.160 --> 22:29.920
common sense that I guess you can kind of think of as like transfer learning in a sense.

22:29.920 --> 22:36.000
In the context of a few shot learning is there in your research in particular, is there

22:36.000 --> 22:44.920
a starting place? Is there some base that we've taught the agent via you tell me or we

22:44.920 --> 22:48.840
just starting from scratch with a few examples and saying go figure this out like what's

22:48.840 --> 22:54.360
the base? Yeah, so you're absolutely right. Transition is very important. So we probably

22:54.360 --> 22:59.600
need to unpack what I mean by few shot learning. So few short I mean it has to learn from

22:59.600 --> 23:08.400
few data. And then meta learning is there's this assumption that the learning is going

23:08.400 --> 23:13.520
to happen at least in two phases. There's going to be a very expensive phase which involves

23:13.520 --> 23:18.840
many tasks. We want to build machines that can do many things or one thing. So we train

23:18.840 --> 23:24.680
it to do many things and there's different ways to do this. But we should be thinking

23:24.680 --> 23:32.320
about this as evolution and evolution leads to what some people call priors. It's an initial

23:32.320 --> 23:37.760
state in your brain when you're born, you're born with a faculty of language. You start

23:37.760 --> 23:42.640
with some sort of prior. There's still a lot of discussion on how to best do meta learning

23:42.640 --> 23:50.320
in practice. And so there is this long process and then there is also another process which

23:50.320 --> 23:56.240
is the fast learning which just with a few data you should be able to learn to do something

23:56.240 --> 24:02.280
new with new objects or with new behaviors. So this is very important for robotics because

24:02.280 --> 24:10.240
unlike simulation robots cannot afford to do or repeat the task a trillion times until

24:10.240 --> 24:15.600
they get it right. And so it's important that they learn rapidly. So the name of the game

24:15.600 --> 24:25.920
then is maximized generalization while minimizing the amount of time needed to learn. Which

24:25.920 --> 24:32.320
of the two phases or both? Actually in both phases. In particular in the second phase you

24:32.320 --> 24:37.280
sort of assume that you will be given very few data and you will have very little compute

24:37.280 --> 24:43.840
power and you will have to come up with a solution very quickly. And of course if you haven't

24:43.840 --> 24:50.000
in the first phase constructed a representation that allows for abstraction so that you can

24:50.000 --> 24:56.960
go to so you can sort of easily move to new tasks. Then this is not going to work. So it's

24:56.960 --> 25:01.680
important that whatever representation you come up with that there be sort of these

25:01.680 --> 25:08.240
causality people call this like Burner-Scholkhoff and a recently Professor Yoshio Benji here

25:08.240 --> 25:14.960
in Montreal they call this sort of the independent mechanisms. We also often refer to this as concepts

25:16.000 --> 25:25.040
abstractions. And so by having these concepts abstractions or programs I did some work before

25:25.040 --> 25:29.040
on something called neuro program interpreters that I'm so very excited about which is trying to

25:29.040 --> 25:37.520
understand these modular components of things in the world like behaviors, programs, objects,

25:37.520 --> 25:45.200
and so on. So if we have that and at the same time we don't want to hard code this we want to

25:45.200 --> 25:51.760
learn this so that I think that's important. Then we have some much more hope that we will be

25:51.760 --> 25:56.640
able to generalize to novel objects and so on. So we'll be able to reuse some of the knowledge.

25:56.640 --> 26:03.040
So we'll know if we had knowledge for example of physics then whether there's new task

26:03.760 --> 26:09.360
involves dropping something then it doesn't matter whether it's an apple or a watermelon or

26:10.160 --> 26:15.520
or a piece of chalk. If you open your hand it will drop. You can predict it because you have an

26:15.520 --> 26:20.880
understanding of the laws of at least at an intuitive level you understand the laws of physics.

26:20.880 --> 26:27.440
So we need to we definitely need abstraction. So it's a few shots with the learning

26:27.440 --> 26:33.360
but in order to get generalization we need abstraction. We need to have a causal understanding

26:33.360 --> 26:41.280
of the mechanisms in the world. Kind of taking a step back we've got few shot meta learning,

26:41.280 --> 26:47.520
imitation, all of these are part of you know they're brought together to try to solve this problem

26:47.520 --> 26:52.960
is it is meta learning kind of like decore challenge and if you figure that out then the other two

26:52.960 --> 26:59.120
are easy or do they each kind of contribute their own unique challenges to the overall picture.

26:59.600 --> 27:04.080
I think they're all different perspectives of looking at the same problem which is like

27:04.080 --> 27:09.200
oh god that's the brain do it. So the meta learning perspective is very useful but of course

27:10.320 --> 27:13.840
it still leaves open the question of how do we choose the representation.

27:13.840 --> 27:20.960
Like in likewise we do some reinforcement learning when we do imitation sometimes sometimes

27:20.960 --> 27:27.200
we do supervised learning but whether you do reinforcement learning supervised learning or whether

27:27.200 --> 27:31.680
you choose to think about the problems in terms of reinforcement learning and off-policy learning

27:31.680 --> 27:36.240
or you or you instead choose to think about them in terms of causality and contractuals.

27:38.240 --> 27:42.720
It's kind of a it's a matter of taste. It still leaves the question of what other

27:42.720 --> 27:47.680
representations. So we still have to figure out what are these sort of independent representations

27:47.680 --> 27:53.600
and how to combine them in order to solve tasks. So I think all these different perspectives on the

27:53.600 --> 28:00.560
problem are very useful. The multi-agent perspective is also very useful because the most interesting

28:00.560 --> 28:09.840
thing at Neurip's is not like the objects around it's the people. So people are the most

28:09.840 --> 28:14.320
interesting in our environment. A lot of these abstractions that I'm talking about that are

28:14.320 --> 28:21.280
only possible with people. We can talk about a podcast because even though podcasts are things that

28:22.320 --> 28:27.600
or perhaps a better example is we can think about today Wednesday. There's no such a thing as

28:27.600 --> 28:34.320
Wednesday in the world Wednesday is a construct of our minds. It's an abstraction that we humans find

28:34.320 --> 28:40.480
useful to communicate and so it's eventually we do need to the multi-agent perspective.

28:41.360 --> 28:45.920
And another sort of thing that sort of comes into this sort of important in third person

28:45.920 --> 28:52.000
imitation is I'm able to say observe you and you have a slightly different body than mine

28:53.280 --> 28:59.200
but and I have the sort of third person view of you. So I'm looking at you and I can see your

28:59.200 --> 29:04.720
whole body and parts of your body that you can't see. I also have my own sort of perspective

29:04.720 --> 29:10.400
and I know what I'm feeling for example in my fingers and I know how cold this room is and I

29:10.400 --> 29:19.360
apologies and always. So the interesting thing is that I see if you think of I have a third person

29:19.360 --> 29:27.680
of view of you and I have a first person view of me and through that I can have a first person

29:27.680 --> 29:33.760
view of you. So I can more or less predict how you're finding the temperature in the room. I can

29:33.760 --> 29:40.160
probably predict what you're feeling in your fingers right now and at the same time I can also do

29:40.160 --> 29:45.840
the third person view of me I can sort of step out of my head and imagine me sitting here in this

29:45.840 --> 29:55.680
chair or renting a tube and so we then end up with this two by two matrix and eventually I think

29:55.680 --> 30:02.640
this being able to sort of step outside of my mind and to know that it's me here talking to you

30:03.440 --> 30:11.520
to be aware of that I think it's very profound and as part of understanding what is intelligence

30:11.520 --> 30:20.480
and how how do our brains work because this is about knowing how I relate to the world. How do I

30:20.480 --> 30:26.560
know what I know it's to know what I don't know to know what I don't know it's we're getting to

30:26.560 --> 30:33.440
this question of awareness to be aware of your knowledge it's something none of our machines

30:34.480 --> 30:44.560
have at present kind of done and and I mean in fact if we go in this direction we are not too far

30:44.560 --> 30:55.360
from there I say having a stab at understanding consciousness because that's to to some extent

30:55.360 --> 31:02.240
in non-metaphysical sense but to a computational we're moving toward a computational definition

31:02.240 --> 31:09.280
of consciousness here and there's a few scientists in working say on attention scheme of theories

31:09.280 --> 31:14.800
and so on that are starting to actually believe that this it might be possible for us to start

31:14.800 --> 31:22.800
talking about we already talk about like like for example a neural network policy with R&N

31:22.800 --> 31:27.840
the internal state is a subjective state and some of the work that I've already done with

31:27.840 --> 31:33.360
Misha Deniel one of the researchers I collaborate with shows that the models have an internal

31:33.360 --> 31:38.800
representation of the world even after they've no longer in the presence of that world you know

31:38.800 --> 31:44.880
sort of they've they've touched an object and the hand lifts even a while later they're still a

31:44.880 --> 31:51.040
memory of the shape of that object in the in the internal representation of the recurrent neural

31:51.040 --> 31:59.280
network and so if we go on step further to there being awareness in that neural network of what it

31:59.280 --> 32:05.600
knows that it that it knows that there was this object and it had a representation when we start

32:05.600 --> 32:12.240
combining this with you know with abstractions with counter five thought thinking and so on I

32:12.240 --> 32:18.240
think we are getting very close to intelligence the problem is we still don't know how to get

32:19.120 --> 32:26.000
these abstractions in order to be able to do third person imitation and in order to be able to

32:26.000 --> 32:32.720
get good awareness models of the world so when you're pursuing a line of research like the third

32:32.720 --> 32:40.400
person imitation learning can you walk us through you know the various phases of a project like that

32:40.400 --> 32:46.560
how does it evolve so this one in particular is it's fun you go collect videos of that's our

32:46.560 --> 32:53.360
in YouTube and because the first thing you need is data yeah so one initial state is coming up

32:53.360 --> 32:59.600
with representations that whether it's the video in YouTube or whether it's the the video that you

32:59.600 --> 33:06.560
can generate by playing the game in your own emulator in the lab the representations in both the

33:06.560 --> 33:12.800
main to such that they that they are equivalent so we call this sort of self super so one of

33:13.440 --> 33:18.400
micro operators which is the two that are equivalent the representation so it's just you the

33:18.400 --> 33:25.840
ideas of build neural networks that will whether whether they look at a YouTube video or the actual

33:25.840 --> 33:31.600
video in the emulator that I have they will produce similar representations okay so the questions

33:31.600 --> 33:35.920
how do we train this because we won't have super we don't have supervision and it's not an RL

33:35.920 --> 33:42.880
problem it's something that we're now referring to a self-supervision okay and so my one of my

33:42.880 --> 33:52.080
collaborators you survived are went to the YouTube videos and he came up with a technique contrast

33:52.080 --> 33:59.040
of learning now that allows us to be able to learn these representations so see for example

33:59.040 --> 34:09.840
tries to predict whether a video frame appears before or after a recent video frame or he tries to

34:09.840 --> 34:18.080
classify how many steps ahead is one segment of video from another segment okay or whether the audio

34:18.080 --> 34:25.040
video just an important part of Atari that we never get to see much of whether the audio signal

34:25.040 --> 34:32.320
car happens at the same time or not as the video signal and through and through just these very

34:32.320 --> 34:39.200
basic checks you can sort of formulate a you know training labels essentially you can train a

34:39.200 --> 34:46.240
neural network the typical techniques that we use there the sort of contrasting what is right versus

34:46.240 --> 34:52.880
what is wrong is basically what we call contrast of learning or and there's been many techniques for

34:52.880 --> 35:00.480
doing this max margin maximum likelihood in fact it's a form of thing this and in fact this is

35:00.480 --> 35:07.200
what kind of led to also Gantt it was that this kind of research of being able to so try to

35:08.000 --> 35:13.440
have a classifier that is telling you what's real what's not real and so you collect the video

35:13.440 --> 35:17.440
and I'm still not very clear on that okay so we're representations right so one representation is

35:17.440 --> 35:21.920
the one you've built up from the YouTube videos what's the other so we because we're using videos

35:21.920 --> 35:28.320
of that are very different for different players with different I know the difference of all

35:28.320 --> 35:34.000
the videos will all have different appearance but we through learning the representations like

35:34.000 --> 35:40.160
this we're able to get an in to embed the videos into vectors okay that are the same for all

35:40.160 --> 35:45.760
different variations of the video and once you have such an embedding embedding on a frame-by-frame

35:45.760 --> 35:52.880
basis or segment or entire video or something else typically on a frame-by-frame basis but you

35:52.880 --> 35:59.200
could sort of do this also conceivably you could try to do this of a sequences also okay

36:00.960 --> 36:07.120
and with this now you essentially now if you have a trajectory in YouTube now you can map this

36:07.120 --> 36:14.960
to a trajectory in latent space in this sort of embedding space and now this is essentially

36:14.960 --> 36:20.960
the trajectory that you need to follow when you play Atari so then we use this trajectory

36:20.960 --> 36:29.040
as a reward signal and then we just do reinforcement learning and here Toby Favme the other

36:29.040 --> 36:35.280
collaborator just went and tried a couple reinforcement learning agents and then it sort of learns to

36:35.280 --> 36:43.360
it's using the trajectory as the reward signal and it tries to follow it by taking actions in the

36:43.360 --> 36:50.000
game so essentially all we've done is we've used the data that we've served in the world solving

36:50.000 --> 36:57.840
the task in a slightly different setting we call it with a domain gap and and we've

36:59.360 --> 37:02.960
we first learn the features and then once we have those features we can use those features to

37:02.960 --> 37:08.240
construct the reward function and then we just try and just maximize the discounted sum of

37:09.280 --> 37:17.280
returns in order to solve the game okay you mentioned that the videos that you come across have

37:17.280 --> 37:22.240
you know different levels of quality and I thought you said something like different you know

37:22.240 --> 37:31.040
colors or levels of noise or whatever do you do any pre-processing like domain adaptation or

37:31.040 --> 37:38.880
out as anything to augment the the data in some way yes so in fact the self-supervised learning

37:38.880 --> 37:45.040
is a form of domain adaptation that's what we're trying to find features are sort of common for

37:45.040 --> 37:51.520
all these domains in Atari the gap the domain gap is not that big so it's somewhat

37:51.520 --> 37:58.960
easy for us to do some processing of a video scale and so on okay what we will need to do is

37:58.960 --> 38:04.560
really address and there's been a few papers but this is still I think largely an unsoft problem

38:04.560 --> 38:12.000
is be able to just watch a few videos of someone in YouTube doing something like pouring liquids

38:13.360 --> 38:20.080
and then be able to get a robot whether a simulation or real robot to also pour liquids

38:21.840 --> 38:28.800
that is very hard to do and in terms of mapping human hands to robot hands

38:28.800 --> 38:36.080
think the task there's a few nice efforts recently Chelsea Finn has done some good work in this we

38:36.080 --> 38:43.440
we've sort of been looking at this as well open AI has also some works but I think we still have

38:43.440 --> 38:51.760
a long way to go before solving the problem and so what's your sense for what the next step is the

38:51.760 --> 38:59.600
next piece of that puzzle so I think it's one of the things that I'm going to be betting on will

38:59.600 --> 39:08.480
be through coming up with better representations and currently there's two sort of hypothesis

39:08.480 --> 39:16.640
here one is just make the networks bigger and so the big networks is something we explored recently

39:16.640 --> 39:24.160
on a paper we actually just put on archive recently with an algorithm called metamimic

39:25.040 --> 39:35.040
the idea of metamimic is it closely imitates the demonstration like we in fact call this

39:35.040 --> 39:39.680
high fidelity limitations so you're trying to do something precisely like a human we do

39:39.680 --> 39:48.800
okay so and the idea is we're going to learn to do what humans do precisely for many many tasks

39:50.480 --> 39:55.840
so far we have only done it for one task with variation in the task but the intention is to do

39:55.840 --> 40:04.560
this for many tasks if you can learn to do one short imitation so if imitate many tasks then

40:04.560 --> 40:09.360
a test time we show a new task with new objects and a new check whether we can still imitate

40:09.360 --> 40:16.640
so we've in effect test generalization now what we found when doing when training metamimic

40:16.640 --> 40:21.360
is we had to keep the net making the networks bigger in order to improve generalization on the

40:21.360 --> 40:29.680
test set typically our real researchers haven't focused much on generalization and that this has

40:29.680 --> 40:36.320
changed recently over the last few years and I'm actually glad to see our colleagues all sort of

40:36.320 --> 40:42.480
embracing these generalization tests and what we're finding is here it becomes really important

40:42.480 --> 40:49.200
to make the networks big however we didn't know whether we could train the massive network so

40:49.200 --> 40:55.680
we trained for perception for things like vision and so on to also do control but in in this work

40:55.680 --> 41:00.640
we found out that it is indeed possible if you have enough sort of these demonstrations it's

41:00.640 --> 41:07.120
possible to train like in our work is the largest ever-trained neural networks to do RL

41:09.040 --> 41:15.040
and by orders of magnitude larger than any pre-existing network most of what we did before with

41:15.040 --> 41:21.600
Atari and so usually was with two layers neural networks and in fact professor Imot Todorov even

41:21.600 --> 41:28.800
complained about this at one stage I think if we are going to be doing control from pixels

41:28.800 --> 41:40.080
or from pixels and ego emotion and so on it's very important to use large network architectures

41:40.080 --> 41:49.120
and to learn how to model them properly and how to train them so the reason why we chose to go

41:49.120 --> 41:56.080
this way with metamemic is because if you observe children when humans are solving a task

41:56.080 --> 42:03.280
if they introduce irrelevant steps in between children will solve the task but they will also

42:03.280 --> 42:11.280
do the irrelevant task whereas a few experiments have shown that I think with chimpanzees and

42:11.280 --> 42:16.480
volunteers that they will go and solve the task immediately they will not do the relevant steps

42:16.480 --> 42:22.800
okay so humans have this propensity to over what psychologists call over imitate

42:22.800 --> 42:30.480
okay and essentially this is the strategy we're following at metamemic so build a very big network

42:30.480 --> 42:39.440
that will imitate precisely millions of things okay and that will allow it to sort of build into

42:39.440 --> 42:46.240
this model the capacity to when shown a new thing that I had not seen before it can just from one

42:46.240 --> 42:54.480
single demonstration repeat what has been done okay we also found that these models can also

42:55.120 --> 43:01.600
be further trained with RL to solve tasks more efficiently so that's one way to go okay that I'm

43:01.600 --> 43:06.000
gonna explore just very big networks and of course there's a lot of engineering because it's

43:06.000 --> 43:11.840
in the big networks we have to put a lot of thinking into normalization and so on so it's not

43:11.840 --> 43:17.440
when we say big networks it should not be understood that this is brute force engineering

43:17.440 --> 43:22.800
on the other hand is one requires very precise engineering in order to train these big networks okay

43:24.480 --> 43:29.120
this certainly has been my experience with some of our work on liberating as well as some of

43:29.120 --> 43:35.280
the brilliant work that your mind has put out on big gants and using gants as generative models

43:35.280 --> 43:41.440
recently on the other hand I also want to sort of come up with the representations that are

43:41.440 --> 43:46.160
more compositional so that's like the neural programming interpreters work that I did with

43:46.160 --> 43:52.960
Scott read a few years ago and that some folks like in Stanford like folks in Stanford have used

43:52.960 --> 43:59.120
to build things like neural task programmers that are able to actually do very sophisticated

43:59.120 --> 44:07.760
control just by exploiting modularity sort of program modularity as well as to you know

44:07.760 --> 44:12.960
sort of being able to break tasks into some tasks and so on yeah that's going to be a continued

44:12.960 --> 44:20.960
effort and I think I'm also thinking that in terms of building many causal models many small

44:20.960 --> 44:26.640
modules that you can then combine and if the small modules are doing the right thing and they have

44:26.640 --> 44:33.280
a good understanding of I mean a good understanding in the sense that the causal representations

44:33.280 --> 44:39.920
are world and so by combining them and they're in ways which we still have to devise

44:41.920 --> 44:46.800
and we should be able to get much larger models that can still represent the world and

44:48.320 --> 44:53.040
and you know and be able to generalize much better because these models will be compositional

44:53.040 --> 45:02.880
they will be able to have this combinatorial reuse of the components when you describe the first

45:02.880 --> 45:09.280
of those two directions building out these much bigger networks one of you may reference to

45:10.320 --> 45:17.120
kind of staying in the pixel domain do you see efforts to are there what are the alternative

45:17.120 --> 45:24.240
approaches that are being explored if any there's two different philosophies there is the learning

45:24.240 --> 45:32.480
from scratch philosophy that tries to learn just from pixels yeah and I often find too much from

45:32.480 --> 45:37.520
just pixels okay in fact if you're doing grasping and so on it's important that you don't just

45:37.520 --> 45:44.800
have a monocular image but you have stereo okay so death is a very important cue in order to

45:44.800 --> 45:50.720
grab things and you just have to try to grab things from a single video monocular video to see

45:50.720 --> 45:58.640
how hard it is even for humans to do it so there's that there's of course touch half-text and

45:58.640 --> 46:02.960
there's the you know the fact that we have little hairs in our ear that allow us to know the

46:02.960 --> 46:07.760
accelerations of our head and so on so we have a lot of internal gyroscopic information

46:07.760 --> 46:14.880
there's some cares less about kind of multi-sensory and more like is there a direction to you know

46:14.880 --> 46:21.120
I think the weights and activations in these networks kind of form concepts but we don't necessarily

46:21.120 --> 46:29.760
force them to form concepts that are meaningful to us or abstract in any way and I'm curious if

46:29.760 --> 46:34.880
that is a direction that people are pursuing that makes sense indeed so it's indeed there's two

46:34.880 --> 46:41.760
orthogonal things here one is so sort of indeed go for multi-sensory perception and then learn

46:41.760 --> 46:49.360
these networks to conduct actions and this is the big open question like for example in robotics

46:49.360 --> 46:56.000
with like these competitions with flying drones in a way that's an easy problem because the drone

46:57.120 --> 47:03.760
doesn't have to touch anything so there's no contact force says this should be an easy control

47:03.760 --> 47:09.120
it's an easy control problem if you know the state of the world what makes it hard is that they have

47:09.120 --> 47:14.160
to do this if they if they have to do this from pixels navigate from pixels and if there's people

47:14.160 --> 47:18.320
in the background that are moving and lighting changes and so on there's then becomes incredibly

47:18.320 --> 47:26.320
hard and it's still by far and an open problem so perception has not been solved right and that's

47:26.320 --> 47:32.320
essentially what this tells us or perhaps we're not using the right sensors we should and so

47:32.320 --> 47:40.960
mm-hmm now the the other side of this of the argument is also what other folks are doing is

47:41.600 --> 47:46.400
they're sort of saying we shouldn't be learning from scratch all the time you should fight in

47:46.400 --> 47:52.320
fact use that we have already learned some modules some perceptual modules or some action modules

47:52.320 --> 47:59.440
some controllers and sort of learn to reuse the components and that indeed I think is a very

47:59.440 --> 48:06.560
promising area if you certainly if you've learned controllers to achieve tasks it's possible

48:06.560 --> 48:11.200
to reuse them and here controllers you can think of for them as also like sub-programs and we can

48:11.200 --> 48:17.680
combine them what we haven't done well yet is being able to learn a representation of something

48:17.680 --> 48:24.880
like an object and then being able to exploit it that representation keep it in new

48:24.880 --> 48:32.240
you'll be able to harness it for future to build future representations or to keep learning

48:32.240 --> 48:38.480
continually more and more complex representations and be able to manipulate to be able to manipulate

48:38.480 --> 48:44.720
those objects that is still an open problem we don't quite know how to build and this type of

48:44.720 --> 48:51.520
model architectures and there's two philosophies as people who try to model this by learning everything

48:51.520 --> 48:57.120
from scratch and then there's people who of course try to inject much more domain knowledge

48:57.120 --> 49:03.440
and they argue with each other as to who is right but we've seen a lot of these debates

49:03.440 --> 49:09.440
in Twitter especially but I think the verdict is still open it's good that there's different

49:09.440 --> 49:16.000
perspectives on this well Nando any advice or words of wisdom or pointers for folks that are

49:16.000 --> 49:22.640
interested in kind of digging in a little bit deeper into this area losing thoughts I guess come

49:22.640 --> 49:28.640
to come to new reps and I clear and then it's sort of get get to know watch this podcast

49:31.280 --> 49:38.080
and there's also brilliant courses online like Andruing and so on so you where you can sort of

49:38.080 --> 49:43.680
get into this sort of material and get a much better understanding of the material we also

49:43.680 --> 49:50.160
organize summer schools we as in many researchers volunteer throughout the year to teach summer

49:50.160 --> 49:56.640
schools all over the world so here in Canada through the SIFA program lately I've been involved

49:56.640 --> 50:02.640
with teaching a summer school in Africa called the Deep Learning in Dapa which is a great initiative

50:02.640 --> 50:09.600
because and I think that is a parting thought that I want to bring into this AI is such a very

50:09.600 --> 50:17.520
powerful techniques AI will be very influential in our world not just the tech world but AI

50:17.520 --> 50:25.280
will shape politics AI will shape our economies and so on it is essential it is of paramount

50:25.280 --> 50:33.360
importance that everyone has access to AI if currently there is very strong biases

50:33.360 --> 50:42.320
women are underrepresented in AI there's a huge under representation of certain races in AI

50:42.320 --> 50:49.840
of certain continents in AI and we need to address these unbalances we need to address them for

50:49.840 --> 50:56.320
two reasons one then we want to build tools that are fair we want to make sure that AI is for all

50:56.320 --> 51:04.160
of us not just for the few and the other reason is because some of the people that have been

51:04.160 --> 51:11.920
marginalized from this actually have a lot to contribute we often look at Africa as there's

51:11.920 --> 51:17.520
this wall as there hasn't participated in the machine learning conferences this year at NIPS we

51:17.520 --> 51:23.440
have at least two papers that I know of that came from Africa so by helping a bit going and

51:23.440 --> 51:32.400
volunteering there we can sort of start reaping the benefits of their contributions and there's

51:32.400 --> 51:38.240
lots of things that came from Africa Gaussian processes were invented at my university by Professor

51:38.240 --> 51:48.160
Krig and he used to be called Krigging EC2 if you actually look at the history of AWS Africa played

51:48.160 --> 51:55.760
a big role in it you know this is where people in South Africa working on this that have given us

51:55.760 --> 52:02.880
the infrastructure that not pretty much holds all the data from all banks and nation states and so on

52:04.000 --> 52:11.520
it's important to also address the problems in Africa that we haven't even thought about

52:11.520 --> 52:16.240
because we haven't gone there and things like translation in South Africa there's 11

52:16.240 --> 52:23.760
official languages in other countries in Africa there's like 50 languages and when people tweet they

52:23.760 --> 52:33.200
usually use three languages mixed so translation is a huge problem and so when you go there you learn

52:33.200 --> 52:38.240
this and then you realize oh it's important to sort of start working more on unsupervised machine

52:38.240 --> 52:46.960
translation and so on there's also other problems one and I'm really going off track here there's

52:46.960 --> 52:52.640
a very long apologize for having gone this long another problem that I thought was very

52:52.640 --> 52:58.240
interesting in going to Africa two years ago was when I learned about Mum Connect so with Mum Connect

52:58.240 --> 53:09.280
Mum's essentially get enrolled when they go to hospitals into this messaging service and throughout

53:09.280 --> 53:17.120
Africa people use these very cheap old phones these two have here and they can still text and

53:17.120 --> 53:22.400
communicate quite efficiently in fact that the service plans are not as expensive as here in Canada

53:22.400 --> 53:31.120
so in many ways they have leapfrogged out inefficiencies and what they can do is the doctors can send

53:31.120 --> 53:37.600
the messages reminders of what to do after the child is born and so on and they can also if they

53:37.600 --> 53:43.840
have questions they can text the doctors so for example they could text the doctor and this is

53:43.840 --> 53:50.720
like an example of what happens like I've given water to my child all day and all night but the

53:50.720 --> 53:58.880
child keeps throwing up the water at that state the doctor can simply say boil the water

53:59.920 --> 54:06.080
provide that sort of basic advice that will save a life and in fact most if we want to have a

54:06.080 --> 54:11.200
huge contribution to the health of people in our planet those little things other things that

54:11.200 --> 54:18.400
sort of matter the most and you can learn about them when you go and include people from other

54:18.400 --> 54:24.720
communities and then you see the wonderful things that have already done to address this problem

54:24.720 --> 54:34.080
so they actually can contribute a lot of insights into the development of a machine learning tools

54:34.080 --> 54:38.800
well Nando thanks so much for taking the time the child it's great to get to speak with you

54:38.800 --> 54:40.880
thank you very much thank you for listening

54:40.880 --> 54:51.280
all right everyone that's our show for today for more information on Nando or any of the topics

54:51.280 --> 54:58.000
covered in this show visit twimmalei.com slash talk slash two thirteen you can also follow along

54:58.000 --> 55:06.000
with our nirip series at twimmalei.com slash nirips 2018 as always thanks so much for listening

55:06.000 --> 55:13.920
and catch you next time

