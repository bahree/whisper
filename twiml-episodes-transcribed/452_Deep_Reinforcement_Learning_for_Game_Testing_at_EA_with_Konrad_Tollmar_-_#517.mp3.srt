1
00:00:00,000 --> 00:00:17,120
All right, everyone. I am here with Conrad Tolmar. Conrad is a research director at Electronic

2
00:00:17,120 --> 00:00:24,080
Arts or EA, as you may know it, as well as an associate professor at KTH. Conrad, welcome

3
00:00:24,080 --> 00:00:31,920
to the Twimal AI podcast. Thanks Sam. Thanks for inviting us. It's lovely to be here. I'm really

4
00:00:31,920 --> 00:00:37,600
looking forward to digging into our conversation. We'll be talking about as the audience might imagine,

5
00:00:37,600 --> 00:00:44,240
the intersection of AI and games. Before we do, I'd love to have you share a little bit about your

6
00:00:44,240 --> 00:00:52,160
background. I mentioned KTH. What is KTH? KTH is Royal Institute of Technology in Stockholm. It's

7
00:00:52,160 --> 00:01:03,040
a technical university where I did my undergraduate as well as my PhD. I think my interest for AI started

8
00:01:03,040 --> 00:01:11,280
quite a long time ago, starting with computer vision. I've always been passionate about photography,

9
00:01:12,160 --> 00:01:19,600
and I saw them there was an opportunity to combine my kind of interest for photography with my

10
00:01:19,600 --> 00:01:33,360
academic. That's my starting point here. Nice. Tell us a little bit about the kind of research

11
00:01:33,360 --> 00:01:43,840
that interests you in your professorship and in your graduate studies. My PhD was about media

12
00:01:43,840 --> 00:01:52,800
spaces and we built different kind of interactive environments to connect places with video streams,

13
00:01:52,800 --> 00:02:00,000
but also being able to use sensors to convey other kinds of information if you're close or if you're

14
00:02:00,000 --> 00:02:06,320
in the proximity of the space and so forth. That led me eventually to explore that further

15
00:02:06,320 --> 00:02:13,520
after my I graduated and I spent some time working in smart and interactive environments.

16
00:02:14,560 --> 00:02:22,080
Some of these were for play and some were for more like everyday use and I think some of us could

17
00:02:22,080 --> 00:02:31,520
remember recall the kind of demos you saw out of MIT's Media Lab on the late 90s. A lot of

18
00:02:31,520 --> 00:02:38,880
this was kind of cool demos, but we were not really able to build any kind of more intelligent

19
00:02:38,880 --> 00:02:45,840
environments in this way. I then got the opportunity to do my postdoc at MIT AI Lab with Trevor

20
00:02:45,840 --> 00:02:53,200
Dorrell where I saw an opportunity to dwell into more the underlying technologies and start to

21
00:02:53,200 --> 00:03:00,320
really use computer vision and start to understand how you can track movements in the room, for example,

22
00:03:00,320 --> 00:03:08,800
and then see how this could kind of be come as an input in these kind of interactive spaces.

23
00:03:10,080 --> 00:03:17,280
Nice. That reminds me a little bit of as an undergrad, I went to RPI, Rensselaer, in New York,

24
00:03:17,280 --> 00:03:25,440
and they had interactive design program and they would put these installations up in the library.

25
00:03:25,440 --> 00:03:32,240
This was before a lot of modern things, like connect and other things, but they were always

26
00:03:32,240 --> 00:03:41,760
really interesting. And imagining now as you're describing this intersection of those kinds

27
00:03:41,760 --> 00:03:48,560
of installations and intelligence and machine learning like we've got now,

28
00:03:48,560 --> 00:03:53,680
is that what kind of connected you to games?

29
00:03:54,960 --> 00:04:01,760
Somewhat. I think we need to take a few more steps here, because I guess one of the

30
00:04:01,760 --> 00:04:08,560
really great opportunities, even if we didn't have AI as today. I mean, this was 20 years ago,

31
00:04:08,560 --> 00:04:14,800
almost. So we were just in the beginning of like starting to apply. I mean, the first versions

32
00:04:14,800 --> 00:04:21,680
of OpenCV came out, and we started to play around with this. But we were also able to get

33
00:04:21,680 --> 00:04:29,600
some kind of new cold hardware. So one of the products where we got box from Nokia, which was

34
00:04:30,240 --> 00:04:36,400
the very first camera phones that came out. We also need an integrated web browser.

35
00:04:36,400 --> 00:04:42,000
And this box was basically labeled feel free to play. So what do you do with the camera phone

36
00:04:42,000 --> 00:04:47,120
with off? And then we scratch our head for a bit. And we came up with this idea of, well,

37
00:04:47,120 --> 00:04:52,160
maybe you can search with images rather than keywords. And you take a picture of something.

38
00:04:53,040 --> 00:05:00,240
And that become your sort of research here. And these kind of things. And we also had a very early

39
00:05:00,240 --> 00:05:07,440
version of something similar to the connect camera, like a 3D camera, where we can track body

40
00:05:07,440 --> 00:05:13,040
movements. And we looked upon, okay, how can you use these body movements to interact with this

41
00:05:13,040 --> 00:05:21,040
3D world? And I think that was one of the first kind of things where I saw a bit of a connection

42
00:05:21,040 --> 00:05:27,920
to gaming, because we were trying to set up experiments around this and started to study sort of

43
00:05:27,920 --> 00:05:36,080
how people felt about navigating in a 3D world with your body gestures instead. And we played

44
00:05:36,080 --> 00:05:41,840
with different kind of tools that were available back then. But sort of in the end of the day,

45
00:05:41,840 --> 00:05:48,400
the one that we picked was actually a half-life. So we hacked half-life. And that become like the

46
00:05:48,400 --> 00:05:56,720
engine that we used for to create this more like 3D experiments. And this is something that I've

47
00:05:56,720 --> 00:06:04,400
been carrying with me ever since. And even fairly recently, we did studies on simulators for

48
00:06:04,400 --> 00:06:10,960
self-driving cars. We have been using game engines rather than these kind of physics simulators

49
00:06:10,960 --> 00:06:18,400
that is quite typical in automotive industry. Because somehow, if you're no more looking into

50
00:06:18,400 --> 00:06:27,280
sort of the user experience of these environments, they are the best. And maybe this has kind of

51
00:06:27,280 --> 00:06:33,280
brought me here at the EA to start to then look into more like what could we do with AI into the

52
00:06:33,280 --> 00:06:40,720
games as it is now. Got it, got it. And so tell us a little bit about your role at EA and what

53
00:06:40,720 --> 00:06:48,800
your focus is there. Well, we look into, I'm part of an applied research team called SID.

54
00:06:50,080 --> 00:06:55,680
And we look into all kinds of different emerging technologies that eventually could be used

55
00:06:55,680 --> 00:07:04,560
in the games. So we follow state of the art. We most of well several laws has PhD background.

56
00:07:05,680 --> 00:07:11,280
We research and we also contribute them. We publish papers and so forth. Maybe not the same

57
00:07:11,280 --> 00:07:17,840
extent as you would if you would be in there like an academic research lab. But so we stake

58
00:07:17,840 --> 00:07:23,120
connection with the community. We look upon these techniques and see how we can eventually use

59
00:07:23,120 --> 00:07:30,640
them into the games. And there are lots of different ways that AI can be used in games. What

60
00:07:31,680 --> 00:07:38,480
area of focus do you, what's the kind of more specific area of focus for you and your group?

61
00:07:39,440 --> 00:07:47,200
Well, as you say, there is plenty to do or there is plenty of opportunities nowadays. I mean,

62
00:07:47,200 --> 00:07:55,440
with this kind of formal explosion of AI over the last decade, it's rather a bit hard to pick

63
00:07:55,440 --> 00:08:04,080
what to do and what to not to do because you can do so many things. But I guess, I mean, people

64
00:08:04,080 --> 00:08:13,360
maybe think about AI more into the games. And this is something that we also look into. But for us,

65
00:08:13,360 --> 00:08:23,360
AI has also a fundamental role when it comes to creating the games. I mean, the kind of games that

66
00:08:23,360 --> 00:08:32,080
we do, they're massive. And the production of them are also very long and very costly. And

67
00:08:32,080 --> 00:08:41,600
there is a lot of things that could be done with AI into this. So we can use AI to create assets

68
00:08:41,600 --> 00:08:48,720
of all kinds. We can create, I mean, different kind of media speech. You can create animations

69
00:08:50,240 --> 00:08:56,400
and so forth. But you can also then use AI to test these assets. If they look okay,

70
00:08:56,400 --> 00:09:01,520
if they behave okay. And then you put them into the game and then you can start

71
00:09:01,520 --> 00:09:07,840
continue to look into, okay, how do they behave? How do they affect the gameplay experience?

72
00:09:07,840 --> 00:09:17,440
And then you can march on. And when you eventually have a game, I mean, collecting all kinds of

73
00:09:17,440 --> 00:09:22,320
information around the game, how people experience the game, what they like about the game, what they

74
00:09:22,320 --> 00:09:29,200
don't like about the game, how the game works out. Doing that data analytics is also, of course,

75
00:09:29,200 --> 00:09:39,760
a big part of how we apply AI at EA. When I think about games, this is perhaps true for many folks

76
00:09:39,760 --> 00:09:47,440
in audience, I think about deep reinforcement learning. To what degree are the, is the work

77
00:09:47,440 --> 00:09:55,680
that you're doing centered on the application of reinforcement learning? Yeah, thanks for asking.

78
00:09:55,680 --> 00:10:02,160
And yeah, it's definitely reinforcement learning is on the top of our agenda as well.

79
00:10:02,160 --> 00:10:06,880
However, it's not that easy to use. I mean, we play around with it and you know,

80
00:10:08,720 --> 00:10:15,520
the work that has been going on the past, you can show where you can demonstrate the feasibility.

81
00:10:16,560 --> 00:10:22,400
But then turn it into something that actually works in the game. So that's a bit of a different

82
00:10:22,400 --> 00:10:35,200
matters here. So you mentioned that testing is one of the areas that you focus on and you've

83
00:10:35,200 --> 00:10:42,880
published several papers on the application of machine learning to testing games. Can you give us

84
00:10:42,880 --> 00:10:54,480
an overview of that general area game testing and how AI fits in? Yeah, of course. So game testing

85
00:10:54,480 --> 00:11:02,240
came out actually out of some experiments we did a couple of years back when we looked into how to use

86
00:11:04,000 --> 00:11:11,600
reinforcement learning into the games. And we were successful in actually creating some NPCs,

87
00:11:11,600 --> 00:11:18,000
non-playable characters and put them into the game. And they were trained with reinforcement

88
00:11:18,000 --> 00:11:25,920
learning. But we also learned that it's quite hard actually to fine tune their behavior. So

89
00:11:26,800 --> 00:11:36,640
they actually behave in a way that is believable and that gamers like. And I think here you have

90
00:11:36,640 --> 00:11:43,200
a bit of a different twist as well. I mean, typically when you work with AI, you would like to

91
00:11:43,200 --> 00:11:50,560
make AI that if you have it in a self-driving car, for example, the car should behave like we drive

92
00:11:50,560 --> 00:11:58,960
them or preferably even better. AI into the game have somewhat different constraints. They all

93
00:11:58,960 --> 00:12:05,200
don't always have to work perfectly like that, but they need to be believable. And that is kind of

94
00:12:05,200 --> 00:12:13,280
one thing that we learned and that we can create agents that behave well, but still there is

95
00:12:13,280 --> 00:12:21,360
the subtle qualities or lack of qualities in how they behave that still gamers reacted towards.

96
00:12:22,240 --> 00:12:29,760
So we decided then to take a bit of a step back and order and look upon, okay, how could we use

97
00:12:29,760 --> 00:12:37,440
this agent maybe to start at least to explore testing the games with the agents. So this is

98
00:12:37,440 --> 00:12:43,920
something that we've been doing then for like two years or something like that now with in different

99
00:12:43,920 --> 00:12:54,400
flavors. What does it mean for the characters to be believable? Is it simply a matter of them not

100
00:12:54,400 --> 00:13:04,080
being too good and kind of having their own, you know, foibles or are there nuances to that aspect

101
00:13:04,080 --> 00:13:13,760
of believability? Yeah, so I think there's two parts of that. One is how they behave individually.

102
00:13:13,760 --> 00:13:25,200
And they need to walk as you expect them to do. So I mean, of course, I think we all tend to

103
00:13:25,200 --> 00:13:31,520
behave somewhat different in a game, but I mean, if you walk out in the real world, I mean,

104
00:13:31,520 --> 00:13:39,040
you follow a path and you typically don't walk over a piece of grass. And even in games, that behavior

105
00:13:39,040 --> 00:13:47,600
is happens again. So if this agent just walks straight and doesn't kind of have any

106
00:13:48,320 --> 00:13:56,400
tendency of variations so that we expect humans to have, then they start to appear a bit oddly.

107
00:13:57,360 --> 00:14:02,800
So that is one part, but then it's also, and this is even harder, how they act in the group.

108
00:14:02,800 --> 00:14:12,320
So if you have a group of agents, they need to kind of somehow understand each other. So we

109
00:14:13,280 --> 00:14:19,920
think that they have some kind of shared context or shared goal. If they're just running around on

110
00:14:19,920 --> 00:14:27,280
their own, they will behave also quite odd. At least the behavior of them will look like a bit odd.

111
00:14:27,280 --> 00:14:34,640
And when you think of them in groups, I'm imagining you were talking about both groups of these

112
00:14:35,680 --> 00:14:39,120
NPCs as well as when they're collaborating with humans.

113
00:14:40,560 --> 00:14:48,880
True. Collaborating with humans is also quite difficult because to read the gamers' intent

114
00:14:48,880 --> 00:14:55,600
and to participate in that gameplay, I think that it's still a bit further down the road.

115
00:14:55,600 --> 00:15:02,560
You can have them as companions in the games. And I think you see more and more games now

116
00:15:02,560 --> 00:15:12,800
that start to adapt that. So some of the players are AI agents and some of real humans.

117
00:15:12,800 --> 00:15:22,160
But again, to make this believable, it's depending on the game and depending on what task they have.

118
00:15:22,160 --> 00:15:28,560
If you have a crowd that follows you is one thing. If you should sort out some collaborative task

119
00:15:28,560 --> 00:15:36,560
that is very hard to do today. But on the other hand, if you twist this around, again, if you look

120
00:15:36,560 --> 00:15:44,640
into game testing, some of these constraints you can relax tremendously. Because even you can say

121
00:15:44,640 --> 00:15:51,600
that, well, it's sometimes good that these agents doesn't behave like humans or like everyone would

122
00:15:51,600 --> 00:15:58,480
do. Because you know, the way we test games is too folded. Firstly, I mean, and the most part

123
00:15:58,480 --> 00:16:05,840
of the testing is done by having a notch amount of human testers sit and testing through the games

124
00:16:05,840 --> 00:16:14,000
and through different scenarios or different parts of the game. But then we also work with

125
00:16:14,000 --> 00:16:20,560
some kind of automated tools. So we can create agents, script the agents that could solve some

126
00:16:20,560 --> 00:16:28,000
tasks. And then you can sort of drop them into the game and they you can see that the navigation

127
00:16:28,000 --> 00:16:33,520
mesh, for example, in the game works. So they can take they can go from one position to another

128
00:16:33,520 --> 00:16:41,040
position, but then when you work with reinforcement learned agents, sometimes it's good that they

129
00:16:41,040 --> 00:16:46,880
don't behave as humans. Because then they will find bugs that typically humans will not find.

130
00:16:46,880 --> 00:16:53,120
So they won't walk through a wall. That human testers might not do. You see a wall

131
00:16:54,320 --> 00:16:59,840
and then you'd walk around it, but the agents doesn't care. So they just walks straight

132
00:16:59,840 --> 00:17:05,520
through the wall instead. And that's how you sort of find this exploits in the game.

133
00:17:06,320 --> 00:17:14,320
Interesting, interesting. So when we're talking about testing in the context of,

134
00:17:14,320 --> 00:17:21,840
well, specifics of some of the articles that you share that we'll be talking about,

135
00:17:21,840 --> 00:17:28,080
the improving play testing coverage paper and the augmenting automated game testing paper,

136
00:17:28,080 --> 00:17:37,040
are we talking about using deep learning agents, for example, to test the games broadly,

137
00:17:37,040 --> 00:17:44,240
or are we talking about testing the NPCs in the games using other AI?

138
00:17:46,240 --> 00:17:52,320
I think what we need to realize there, there is a couple of challenges that we need to work on.

139
00:17:53,280 --> 00:17:59,600
And one of them is like a lot of the academic research about reinforcement learning agents

140
00:17:59,600 --> 00:18:09,120
and testing or playing games are still fairly simple games. And if you go into and what

141
00:18:09,120 --> 00:18:18,320
would like to apply that in like a full 3D game with a lot of other things, a lot of other

142
00:18:19,200 --> 00:18:29,120
agents and a lot of dynamics, that is way harder. And then you also need to make this work

143
00:18:29,120 --> 00:18:35,520
on to the game engines that we are using. So it eventually could be used in runtime.

144
00:18:38,720 --> 00:18:45,440
Got it. So I think you're suggesting that the Atari games that we associated with

145
00:18:46,240 --> 00:18:54,480
reinforcement learning are far simpler than the typical game that EA is producing today.

146
00:18:54,480 --> 00:19:03,040
And I get that. I think you mentioned I'm trying to confirm what I thought I heard,

147
00:19:03,040 --> 00:19:10,560
which was a couple of different use cases. One is that you want to introduce these NPCs,

148
00:19:10,560 --> 00:19:14,880
these automated characters into the games, and you need some way to test them.

149
00:19:16,480 --> 00:19:23,360
And then I think I heard separately that you want to be able to test the game

150
00:19:23,360 --> 00:19:28,160
to generally make sure that the walls don't let you walk through them. And that's potentially

151
00:19:28,160 --> 00:19:35,040
another area for the use of machine learning or reinforcement learning. And I'm curious which of

152
00:19:35,040 --> 00:19:44,240
those are you working on? Are you working on both of those? And what that, you know, so I'm curious

153
00:19:44,240 --> 00:19:52,080
to explore those areas. Yeah. So we have published a couple of papers recently and one was on

154
00:19:52,080 --> 00:19:58,720
called last year, where we basically just introduced this notion of using reinforcement learning

155
00:19:58,720 --> 00:20:07,520
agents into 3D games and looked upon what kind of test you can do with them. And then we saw

156
00:20:07,520 --> 00:20:13,680
there was a couple of things you can do. I mean, you can see what kind of coverage they have.

157
00:20:13,680 --> 00:20:23,360
They could find exploits in the games. You can find spaces, for example, where players might get stuck.

158
00:20:24,960 --> 00:20:31,280
But also other things like that. So that was the first stepping stone. And then from that,

159
00:20:31,280 --> 00:20:39,360
we have more looked into how you can generalize this in a couple of different ways. One is like,

160
00:20:39,360 --> 00:20:50,000
like we talked a bit about like an agent. If you train an agent, a typical behavior is then that

161
00:20:50,000 --> 00:20:58,720
it would maximize the sort of the how it goes from A to B. Not on the straight line maybe,

162
00:20:58,720 --> 00:21:07,040
but like as quick as possible. And this is typically not the way we play games. So a more

163
00:21:07,040 --> 00:21:13,440
human-like way to play games is to explore the game somewhat. And maybe you would like to check

164
00:21:13,440 --> 00:21:19,840
whether a place to go is safe enough and you kind of sneak in and sneak out and go around it

165
00:21:19,840 --> 00:21:28,960
in different ways. So that has been one of the explorations to look upon a model that is driven

166
00:21:28,960 --> 00:21:40,560
by curiosity instead of an optimized path as it is. And so tell us a little bit about that work

167
00:21:40,560 --> 00:21:51,200
and the idea of these models that are curiosity driven. How does that differ from kind of this

168
00:21:51,200 --> 00:21:58,400
classical idea of tuning your explorers, type of parameters in reinforcement learning?

169
00:22:00,480 --> 00:22:07,600
It goes along those lines of course, but what we needed to try out here is first kind of to what

170
00:22:08,480 --> 00:22:17,040
sort of what coverage these agents could have. But then as another problem that the rise here is like

171
00:22:17,040 --> 00:22:23,600
when you train these agents and you test the games with these agents, the agents themselves will

172
00:22:23,600 --> 00:22:31,440
not tell you really what they have done. So if an agent goes through a wall for example or if an

173
00:22:31,440 --> 00:22:39,200
agent doesn't find a spot, they will not tell you this. So a second part on this work was really

174
00:22:39,200 --> 00:22:49,120
about also how could you visualize the behavior of these agents. So a game designer eventually could

175
00:22:49,120 --> 00:22:55,680
look upon these visualizations and check whether their map or the world that they have created

176
00:22:56,240 --> 00:23:04,560
works as they wanted it to be. And so that is also become a very important part of that work.

177
00:23:04,560 --> 00:23:13,680
So collect all this data, creating different kind of visualization representations. So you can

178
00:23:13,680 --> 00:23:22,080
find trajectories through the maps, you can find like heat maps where people where agents are

179
00:23:22,080 --> 00:23:35,040
or haven't been and so forth. When you're creating an agent to explore a video game, are you and in

180
00:23:35,040 --> 00:23:49,760
your paper in particular, are you substituting some notion of score based reward system for one

181
00:23:49,760 --> 00:23:55,040
that's purely curiosity based? Do you like do you not care about the score if the agent you know it

182
00:23:55,040 --> 00:24:01,280
gets great coverage of the the game or is there also do you also need to build in some notion of

183
00:24:01,280 --> 00:24:07,280
score so that you're you know testing the things that humans would more likely do.

184
00:24:10,560 --> 00:24:17,840
Yes and no. I think still we are in the face of that this exploration that these agents are doing.

185
00:24:17,840 --> 00:24:26,720
It's fairly early on and make them actually to play the game in full. So they collect

186
00:24:27,280 --> 00:24:34,240
like levels and scores as you would normally do in a game. That is still research to come.

187
00:24:34,960 --> 00:24:41,920
So at this point we more looking into how we can make them navigate around and explore the

188
00:24:41,920 --> 00:24:50,720
space in the best way and also being able to I would say to if you look upon games it's not

189
00:24:50,720 --> 00:24:57,440
just running around in the games anymore. I mean typically in the game you have other kinds of

190
00:24:57,440 --> 00:25:07,040
ways to navigate. You have different kind of instruments. You might have elevators that could

191
00:25:07,040 --> 00:25:14,560
take you from one level to another. You might need to climb. Some of the new games also enable you

192
00:25:14,560 --> 00:25:22,960
to fly through the game so you have a jetpack or something like that and this more advanced

193
00:25:22,960 --> 00:25:30,880
form of navigation is still I would say very much work in progress on our side.

194
00:25:30,880 --> 00:25:38,640
I think I hear you trying to temper my enthusiasm or my sense of where things are with the

195
00:25:38,640 --> 00:25:47,280
class of games that you're working with. Maybe take us back and walk us through where you are

196
00:25:47,280 --> 00:25:56,400
with the agents and the types of games that you are focused on at EA. How far along are you and

197
00:25:56,400 --> 00:26:04,880
what are the key challenges that you run into when you kind of scale from Montezuma's

198
00:26:04,880 --> 00:26:16,400
Revenge to a modern video game? Right. Like we talked a bit about the way you navigate around in

199
00:26:16,400 --> 00:26:26,720
the space kind of comes down to the person now and what kind of game archetype you are if you're

200
00:26:27,360 --> 00:26:35,120
an aggressive player or if you're a cautious player. So this is very things that we are very

201
00:26:35,120 --> 00:26:42,960
much interested in where we look into now. Whether you with some form of imitation based learning

202
00:26:42,960 --> 00:26:51,280
demonstration by game testers or even the game designers could pick up some of these

203
00:26:51,920 --> 00:26:59,440
archetypes and then use this data when we train the agents. So the agents actually tend to

204
00:27:00,480 --> 00:27:07,200
kind of get this kind of more multitude of character when they test or play the game.

205
00:27:07,200 --> 00:27:18,080
So that's that's one thing but another thing that we also look into is one of the problems with

206
00:27:18,080 --> 00:27:22,960
reinforcement learning and training of reinforcement learning agents. I think a lot of us

207
00:27:24,000 --> 00:27:29,360
have discovered is that you do play, you train it on a fairly static environment.

208
00:27:29,360 --> 00:27:40,800
So and if you change something in the environment, you need to retrain the old sort of model again.

209
00:27:40,800 --> 00:27:49,680
And this is kind of very unpractical if you're in a game development cycle where you continuously

210
00:27:49,680 --> 00:27:58,000
change the games and the maps and you make updates and fix bugs and so forth. So this is something

211
00:27:58,000 --> 00:28:07,440
that we also are very much interested in how we can get models that is more adaptive to dynamic

212
00:28:07,440 --> 00:28:18,000
environments. Yeah. And how do you go about doing that? Well, I mean, there is a couple of

213
00:28:18,000 --> 00:28:28,240
different ways you can do that. I mean, one of the known techniques is called poet and that is

214
00:28:28,240 --> 00:28:38,480
kind of used to kind of dynamically change the complexity in the environment. So the agents encounter

215
00:28:38,480 --> 00:28:46,240
more and more complex environment to train on. Our approach is somewhat has drawn quite a lot

216
00:28:46,240 --> 00:28:53,040
of inspiration from poet, but has a bit of a different take in that we take two reinforcement

217
00:28:53,680 --> 00:29:02,560
agents. One is we call the generator and another one is called we as solver. So the generator

218
00:29:02,560 --> 00:29:12,160
basically creates games. So using procedural generated algorithm to create game elements.

219
00:29:12,160 --> 00:29:22,160
And the other agents try to solve this. And then they could work together as a pair in different

220
00:29:22,160 --> 00:29:32,240
ways. So one is you can create models that handle more dynamic environment. But you can also use

221
00:29:32,240 --> 00:29:40,800
this to kind of create levels of various kinds. So think about core ways, for example, where I

222
00:29:40,800 --> 00:29:47,520
have one of the agent is kind of creating the track for you. And the other agent is try to drive

223
00:29:47,520 --> 00:29:56,000
this track. And then they could work in a pair. And you can in that also try out how difficult this

224
00:29:56,000 --> 00:30:04,160
track is by having different agents test the track. And then over manipulation, you can then

225
00:30:04,160 --> 00:30:11,600
determine, well, this is a track level 10 or this is a track level five or something like that.

226
00:30:11,600 --> 00:30:20,000
So this is how you can eventually also then implement it in the game to not only test the game,

227
00:30:20,000 --> 00:30:29,600
but you can also hear tests sort of the complexity in the game. And then appropriately

228
00:30:29,600 --> 00:30:37,760
set different scores on how difficult the tracks are. Got it, got it. So it sounds like that last

229
00:30:37,760 --> 00:30:47,760
example ties into it's got this element of you training up an agent that can allow you to

230
00:30:47,760 --> 00:30:54,880
assess the complexity of a given environment. But you could also start to leverage it for

231
00:30:54,880 --> 00:31:04,880
creating new environments either in the studio or from the game itself. Am I hearing that?

232
00:31:04,880 --> 00:31:12,000
From the game itself, yeah. And I think this is for us a tool that could be used in different ways.

233
00:31:12,000 --> 00:31:19,680
I mean, you can imagine that this could eventually be a stepping stone into real game AI where you

234
00:31:19,680 --> 00:31:30,000
start to sort of adapt the environment towards the actual player and how the player plays the games.

235
00:31:30,000 --> 00:31:38,480
So you get a better gaming experience out of that. But you can also eventually think that this

236
00:31:38,480 --> 00:31:48,240
could be put into the tools when you create games. So if you put this agents into the editor,

237
00:31:48,240 --> 00:31:59,200
you can test sort of the map for or the environment in real time. So if you put two things to

238
00:31:59,200 --> 00:32:05,120
far apart, the agent will tell, well, I will never be able to jump from this to that. But then you

239
00:32:05,120 --> 00:32:10,480
can just in the editor, put them a bit together. And then you see, well, then you have a map that

240
00:32:10,480 --> 00:32:19,360
works. So you don't even have to test it eventually. Besides one of the things that we talked about

241
00:32:19,360 --> 00:32:30,960
this far, there are other aspects of using RL for game testing that you've kind of learned along

242
00:32:30,960 --> 00:32:40,320
the way. I'm curious about the practicalities of trying to apply RL in the ways that you

243
00:32:40,320 --> 00:32:47,520
are and are there, you know, their particular kind of, you know, lessons earned or tricks that

244
00:32:47,520 --> 00:32:54,400
you've come across or anything that will be worth sharing there. I think, and I heard that on

245
00:32:54,400 --> 00:33:01,760
the podcast here before, I mean, if you relate to self-driving cars a bit, there is one thing to

246
00:33:01,760 --> 00:33:09,360
test things in the lab or in an experiment. But then, of course, how you deploy it in the real world

247
00:33:09,360 --> 00:33:16,880
is very different. And of course, a game doesn't need to be bulletproof as a car or a self-driving

248
00:33:16,880 --> 00:33:23,120
car, but this should still work. I mean, the games are used by millions and millions of users,

249
00:33:23,120 --> 00:33:30,720
and they will find all kinds of bugs and problems in there, for sure. So the games need to be very

250
00:33:30,720 --> 00:33:39,520
robust and very functional. And also here, more to that, that some of these techniques are still a

251
00:33:39,520 --> 00:33:46,480
bit controversial, I would almost say, among game designers, because you don't really know what

252
00:33:46,480 --> 00:33:53,600
you get. I mean, AI is still much a black box. So if you train a model of some kind,

253
00:33:53,600 --> 00:34:04,800
you need to make sure that there is a way for a game designer or a tech artist that work with

254
00:34:04,800 --> 00:34:11,840
his assets or content that they could sort of add their flavor to it, and they could feel that

255
00:34:11,840 --> 00:34:24,960
they could control this, because getting to the game experience, it's a delicate piece of art,

256
00:34:24,960 --> 00:34:36,160
actually, to make games that feels good and are playable and are fun. And I think if you introduce

257
00:34:36,160 --> 00:34:43,680
a lot of AI things that kind of work, but still doesn't really behave as you think they should,

258
00:34:44,320 --> 00:34:50,960
I think that could rather ruin the game experience than add to it. So this is something that

259
00:34:50,960 --> 00:34:59,920
we need to work a lot more on how to kind of be able to control this AI agent or do its models

260
00:34:59,920 --> 00:35:10,720
and adapt them to different sort of context and usage. Beyond the work in reinforcement learning,

261
00:35:10,720 --> 00:35:20,400
you've also done some work in the application of CNNs to detecting glitches in games.

262
00:35:20,400 --> 00:35:26,400
Yeah, a little bit about that work. What is a glitch in the context? A glitch is,

263
00:35:26,400 --> 00:35:32,800
well, a glitch could be many different things. I mean, low level glitches are problems

264
00:35:33,600 --> 00:35:42,560
in the hardware or in the drivers. So suddenly you get the white space or a white screen or

265
00:35:42,560 --> 00:35:49,200
something like that. But then you have a lot of other kinds of glitches. Typically, it's more

266
00:35:49,200 --> 00:35:58,720
that when you create these games, there's tons of content and assets. And someone just forget

267
00:35:58,720 --> 00:36:06,800
to put the texture on something somewhere. So we started to then look into, okay, how could we use

268
00:36:06,800 --> 00:36:14,960
in reinforcement learning agent to kind of more walk around in the space to, yeah, look if everything

269
00:36:14,960 --> 00:36:23,680
looks okay or not. And then on top of that, that's kind of then fairly straightforward to use

270
00:36:23,680 --> 00:36:32,160
like a competition detector of some kind to detect when you get these glitches like a missing

271
00:36:32,160 --> 00:36:40,160
texture or something like that. And this is also something that is taking a considerable

272
00:36:40,160 --> 00:36:48,080
amount of time when we test the games. Because the games are huge and it takes a lot of time and

273
00:36:48,080 --> 00:36:56,560
a lot of man hours to sit and look through this kind of walkthroughs that we create over the games.

274
00:36:58,640 --> 00:37:05,840
So to be a little bit more concrete, it sounds like maybe an application of the work that we've

275
00:37:05,840 --> 00:37:13,920
talked about previously where maybe you've got this curiosity driven agent. But now the agent

276
00:37:13,920 --> 00:37:22,000
ends up in a position and a particular orientation. And you are able to capture an image from what it

277
00:37:22,000 --> 00:37:28,000
sees and then feed that into a CNN that's maybe, is it a classifier like glitch no glitch?

278
00:37:28,000 --> 00:37:34,800
Well, depending on the game and depending on how we trained, of course. But it's basically a

279
00:37:34,800 --> 00:37:43,040
classical classifier. And you also get a confidence score. So you kind of know how like

280
00:37:43,040 --> 00:37:52,560
accurate the detection is. And then you flag this and then you kind of walk around and find

281
00:37:52,560 --> 00:37:57,200
these glitches in to get the game. But then again, I mean, it depends on the game. I mean,

282
00:37:57,200 --> 00:38:05,120
the games are different. I mean, if you take some of the more open world games, there is a lot of like

283
00:38:05,120 --> 00:38:16,160
assets like houses, trees, I mean, all of that cars. But if you think about the sport games,

284
00:38:17,280 --> 00:38:23,200
there is other kinds of textures that you really need would like to have there. I mean,

285
00:38:23,200 --> 00:38:31,760
if you have longer types into the games, the closing and so on. And then you train the model

286
00:38:31,760 --> 00:38:39,360
for that kind of game. And you mentioned all these different classes of

287
00:38:41,040 --> 00:38:43,920
textures and things that you didn't insert into a game. Are you

288
00:38:43,920 --> 00:38:53,360
to what is your, I'm trying to think about if your model is looking for those things specifically,

289
00:38:53,360 --> 00:39:00,400
is it some kind of hierarchical model? Or is it just kind of looking broadly at an entire image?

290
00:39:01,600 --> 00:39:07,360
And just based on the training data looking for, for these glitches.

291
00:39:07,360 --> 00:39:14,800
Now, it's more looking broadly for glitches in general into the scene. So you have a specific

292
00:39:14,800 --> 00:39:21,200
camera position. And you change this camera position throughout the object. And you rotate around

293
00:39:21,200 --> 00:39:29,120
the object. You walk around taking different scenes. And that is then how you kind of detect this

294
00:39:29,120 --> 00:39:39,680
missing the textures or the glitches in the pictures. And in terms of your training data for that,

295
00:39:39,680 --> 00:39:47,760
do you have a historical database of glitches? Or I imagine there's a huge opportunity here to

296
00:39:47,760 --> 00:39:53,520
do synthetic data creation, creating artificial glitches or something like that.

297
00:39:53,520 --> 00:40:00,880
Where does the training data come from? I guess that's one of the upside of working at EA

298
00:40:00,880 --> 00:40:06,640
and work within the company. Because we have tons of data. We have tons of data from the game

299
00:40:06,640 --> 00:40:15,760
teams. We have tons of data from game tests. So there is a rich set of data that we can train for.

300
00:40:15,760 --> 00:40:24,640
But of course, we need to label them. And so forth. So in that case, we also annotate and synthetically

301
00:40:24,640 --> 00:40:32,480
generate glitches of various kinds. So we can train a better model, simply that perform better.

302
00:40:32,480 --> 00:40:39,120
So it's a bit of a mix there, actually. So we start with like what kind of glitches are

303
00:40:39,120 --> 00:40:48,240
typically encountered into the games. And then we augment them and sort of build up our data.

304
00:40:48,240 --> 00:40:53,360
That's because in the end, when you train this model, if there should be reliable,

305
00:40:54,560 --> 00:40:56,400
you need quite a lot of data now.

306
00:40:56,400 --> 00:41:10,160
Where do you see AI going in games? I think AI will go into the games in a multitude of ways.

307
00:41:11,840 --> 00:41:20,000
Like we talked quite a lot about here today, in how we create the games, how we create the assets,

308
00:41:20,000 --> 00:41:28,560
how we create the worlds, how we create the behaviors, how we create, if you look into games,

309
00:41:30,480 --> 00:41:39,840
that there is so many things, animations, speech, et cetera, that you can create with AI.

310
00:41:39,840 --> 00:41:48,320
But then eventually, we will more and more look into how AI could will be used in the games.

311
00:41:50,320 --> 00:41:56,720
And how it also could drive the game experience in different ways.

312
00:41:57,680 --> 00:42:07,440
You can imagine, for example, you introduce some AI physics. So you can create a bit more

313
00:42:07,440 --> 00:42:15,360
believable worlds by the parts of this that is driven by an AI model instead.

314
00:42:17,200 --> 00:42:23,920
And there is other things like that you can do into the games. You can, like we talked a bit

315
00:42:23,920 --> 00:42:32,320
earlier about, you can test assets as they are used in the games. So if you have

316
00:42:32,320 --> 00:42:39,600
user-generated content of some kind, that's of course not possible to test beforehand.

317
00:42:39,600 --> 00:42:45,520
But then you can check whether this asset or this kind of thing that someone has created

318
00:42:45,520 --> 00:42:52,960
works in the game. And you can help the user then to create a piece of content that actually

319
00:42:52,960 --> 00:42:59,680
do work in the game. I think that is also a very interesting kind of future challenge to look into.

320
00:42:59,680 --> 00:43:08,160
Awesome. Well, Conrad, thanks so much for joining us. It's great hearing a little bit about what

321
00:43:08,160 --> 00:43:18,400
you're working on there at EA and KTH and looking forward to kind of learning more as you continue

322
00:43:18,400 --> 00:43:31,840
to push forward in this area. Thanks for having me, Sam.

