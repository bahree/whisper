WEBVTT

00:00.000 --> 00:03.920
Hello everyone and welcome to the podcast.

00:03.920 --> 00:09.040
If you're looking for this week in machine learning and AI, you are in the right place.

00:09.040 --> 00:12.760
But if you're a regular listener, you may be wondering what happened to the teaser and

00:12.760 --> 00:14.640
the awesome intro music.

00:14.640 --> 00:19.120
Well, this is going to be a different kind of show, so I've skipped the usual intro to

00:19.120 --> 00:21.640
avoid confusion.

00:21.640 --> 00:25.200
This week we're going to take a break from the news format, and I've got a really interesting

00:25.200 --> 00:28.120
interview to share with you in its place.

00:28.120 --> 00:32.760
As you may recall, I spent Thursday at the Rangle conference in San Francisco, which

00:32.760 --> 00:37.120
was organized by Claude Ara, who was kind enough to sponsor the past couple of episodes

00:37.120 --> 00:38.720
of the podcast.

00:38.720 --> 00:40.360
I'm really glad I went to that event.

00:40.360 --> 00:44.880
The program was super solid and I met a bunch of great people.

00:44.880 --> 00:49.480
One of those people was Claire Cortell, whose work was discussed in one of the very first

00:49.480 --> 00:55.080
episodes of the podcast, and she was kind enough to agree to be interviewed for the show.

00:55.080 --> 00:59.040
We had a really fun discussion and touched on a bunch of interesting topics, including

00:59.040 --> 01:04.320
her background and what she's been up to, the open source data science master's project

01:04.320 --> 01:11.120
that she created, getting beyond the beginner's plateau in machine learning and data science,

01:11.120 --> 01:15.240
hybrid AI, which is of course the topic of the article of hers that we talked about

01:15.240 --> 01:20.920
on the podcast, and a recurring topic both here on this week in machine learning and

01:20.920 --> 01:26.880
AI, but also at the Rangle conference, and that is algorithmic ethics.

01:26.880 --> 01:31.040
Before we jump into the interview, a few quick logistic notes.

01:31.040 --> 01:32.880
First, what about the news?

01:32.880 --> 01:37.160
Well, if you've already signed up for the email newsletter that I've been talking about

01:37.160 --> 01:41.520
on the past few podcast episodes, you'll be receiving a summary of the week's news

01:41.520 --> 01:44.800
right in your inbox on Monday morning.

01:44.800 --> 01:49.960
If not, it's not too late to sign up at twomlai.com slash newsletter.

01:49.960 --> 01:54.960
Second, if you're excited about machine learning and AI and you've got research or writing

01:54.960 --> 02:02.280
skills, I'm looking for correspondence to contribute to the podcast and or the twomlai.com website.

02:02.280 --> 02:07.080
Shoot me a note at sam at twomlai.com if you're interested.

02:07.080 --> 02:11.720
Finally, I had a blast doing this interview and I want to know what you think about it and

02:11.720 --> 02:13.840
the interview format in general.

02:13.840 --> 02:22.120
As always, you can reach out to me at at Sam Charrington on Twitter, S-A-M-C-H-A-R-R-I-N-G-T-O-N

02:22.120 --> 02:24.920
with your comments, questions, or suggestions.

02:24.920 --> 02:30.880
All right, let's get to it onto the interview.

02:30.880 --> 02:41.360
All right, so I'm here with Claire Corthel at the Rangle conference in San Francisco.

02:41.360 --> 02:43.960
Hey, Claire, right to finally meet you in person.

02:43.960 --> 02:45.960
Hi, great to meet you in person, too, Sam.

02:45.960 --> 02:53.560
Yeah, so what's particularly exciting about getting to talk to you is I talked about

02:53.560 --> 03:00.400
your post a few, I guess it was one like the second podcast I did.

03:00.400 --> 03:05.720
You wrote that post around the same time on the hybrid AI and I thought that that was

03:05.720 --> 03:08.960
a really interesting post and it was one of the things that I talked about on the podcast.

03:08.960 --> 03:14.360
So I'd be looking forward to catching up with you on that as well as kind of getting an

03:14.360 --> 03:18.200
update on what you're up to and what you've been digging into.

03:18.200 --> 03:23.360
So maybe we can talk a little bit about your background and kind of how you got into

03:23.360 --> 03:25.240
data science and machine learning.

03:25.240 --> 03:26.640
Yeah, not a problem.

03:26.640 --> 03:32.840
So I'm a little bit weird because I'm not a theoretical physicist or some of us in

03:32.840 --> 03:37.360
data science are applied physicists, too, but I'm not in that camp.

03:37.360 --> 03:42.880
I actually started with product design and came into it from that perspective.

03:42.880 --> 03:50.440
So I was actually working on a very small product, kind of a startup within a startup when

03:50.440 --> 03:54.400
I decided that I wanted to understand more about our users and what they were doing.

03:54.400 --> 03:59.280
So I went to the parent company and I said, hey, I don't know much about this, but I think

03:59.280 --> 04:04.160
I should look into our user logs and try and understand more about how people are accessing

04:04.160 --> 04:09.080
the product and what might be happening and do some basic analytics and the head of

04:09.080 --> 04:12.880
engineering kind of looked at me and goes, hey, what logs?

04:12.880 --> 04:16.760
And I thought, well, this is problematic.

04:16.760 --> 04:20.400
How am I going to learn about my users if I don't have that?

04:20.400 --> 04:26.080
And it kind of started me on this long rabbit hole, which is now turned into my career.

04:26.080 --> 04:29.400
And I actually very much overshot that I did not end up in analytics.

04:29.400 --> 04:36.560
I work on machine learning applications and that problem space now, but I came at it from

04:36.560 --> 04:37.560
that perspective.

04:37.560 --> 04:44.600
And at that point, I was really looking for a new direction and decided to invest the

04:44.600 --> 04:51.240
next seven months in learning everything I could to prepare myself for a career and data

04:51.240 --> 04:52.520
side at data science.

04:52.520 --> 04:58.000
So I built a curriculum because at the time, I think insight had just started.

04:58.000 --> 05:04.680
So this was early 2013 and there weren't any academies that focused on this.

05:04.680 --> 05:10.720
So I built a curriculum around that and published it on GitHub and that's become a popular

05:10.720 --> 05:15.040
resource for people who want to get into data science and understand what it's all about

05:15.040 --> 05:17.520
because there wasn't really a road map at that point.

05:17.520 --> 05:21.800
But after the open data science masters, is that what that's all about?

05:21.800 --> 05:28.160
Open source data science masters, yes, the very descriptive, unimaginative name that

05:28.160 --> 05:29.160
I gave it.

05:29.160 --> 05:31.880
So it's exactly what it is though.

05:31.880 --> 05:38.760
So that was a really challenging project to work on.

05:38.760 --> 05:43.120
And I'm really happy that I was able to put that up in the open source world and to give

05:43.120 --> 05:44.120
you a preview.

05:44.120 --> 05:45.760
I'm working on a second version.

05:45.760 --> 05:49.720
There were breaking changes that floated up from Coursera who took a bunch of their

05:49.720 --> 05:50.720
courses offline.

05:50.720 --> 06:00.000
And so, yeah, I'm working on bubbling up those dependency problems and fixing them.

06:00.000 --> 06:02.360
So after that, I went to a company called Madermark.

06:02.360 --> 06:05.040
They're actually around the corner from where we are now.

06:05.040 --> 06:08.120
And they try to measure private company growth.

06:08.120 --> 06:14.000
So very similar to what Bloomberg does for public market companies and at the time when

06:14.000 --> 06:18.440
I joined that company, the CTO was ready to divest in machine learning.

06:18.440 --> 06:22.520
He was convinced it was not going to solve the problems that they were facing.

06:22.520 --> 06:30.040
It wasn't going to pose a reasonable set of solutions for their near term and midterm

06:30.040 --> 06:31.240
goals as a company.

06:31.240 --> 06:39.520
And I, on my first day, designed the key components of how we would move that strategy forward.

06:39.520 --> 06:45.800
That company now has a, I think it's a 10 person team working on data analysis and machine

06:45.800 --> 06:53.280
learning and they're going strong and I, I moved on from that to consulting about a year

06:53.280 --> 06:58.280
and a half ago and have been working with companies of various stages and sizes on getting

06:58.280 --> 07:02.320
started with data science or getting started with new functions within data science that

07:02.320 --> 07:03.680
they want to spin up on.

07:03.680 --> 07:09.320
So helping them understand how to get from A to B and what it's going to cost them for

07:09.320 --> 07:11.880
a solution space that they're investigating.

07:11.880 --> 07:17.960
There's a lot there to dig into on the open source data science masters.

07:17.960 --> 07:22.520
Now, was that a little bit of kind of building the, building the parachute as you're jumping

07:22.520 --> 07:25.960
out of the plane or building the airplane as you're taking off that kind of thing where

07:25.960 --> 07:27.480
you were learning as you went to want?

07:27.480 --> 07:28.480
Right.

07:28.480 --> 07:31.720
Or collecting laundry as I was falling out of the sky, I guess.

07:31.720 --> 07:32.720
Something like that.

07:32.720 --> 07:33.720
There's it.

07:33.720 --> 07:34.720
There's an appropriate analogy.

07:34.720 --> 07:35.720
Right.

07:35.720 --> 07:36.720
I like that.

07:36.720 --> 07:37.720
Absolutely.

07:37.720 --> 07:42.800
It was perhaps most challenging because I had to rewrite it as it was going.

07:42.800 --> 07:48.800
So I would continually check in with people I knew in industry and try and navigate to figure

07:48.800 --> 07:54.600
out what, what skills were actually applicable, what kind of depth I needed to go in on particular

07:54.600 --> 07:55.600
topics.

07:55.600 --> 08:00.320
What was actually key to understand and to the state, this is something that I, I hear a

08:00.320 --> 08:05.720
lot about from people who are hiring managers that when they try to hire people who are very

08:05.720 --> 08:12.440
fresh to the fields, sometimes they don't have the wealth of intuition distributed into

08:12.440 --> 08:13.520
the right places.

08:13.520 --> 08:17.800
So they may know how to build a model, but they don't know how to validate it and they

08:17.800 --> 08:26.400
don't know perhaps how to test data or work with data sets that are very messy.

08:26.400 --> 08:35.920
There are various kind of drawbacks to having a self-guided education and having to retarget

08:35.920 --> 08:38.800
that as you go is certainly challenging.

08:38.800 --> 08:39.800
So.

08:39.800 --> 08:44.480
And were you, did you learn yourself through a self-guided kind of approach?

08:44.480 --> 08:48.640
Did you collect all this by, you know, in the process of learning it or what was, I

08:48.640 --> 08:52.320
guess, what was the background that you brought to getting into data science?

08:52.320 --> 08:53.320
Where did you start?

08:53.320 --> 08:54.320
Yeah.

08:54.320 --> 08:55.320
I will be clear.

08:55.320 --> 09:00.320
I have a degree from Stanford in product design, but it's through a department called Science

09:00.320 --> 09:05.360
Technology and Society, and it's actually a hybrid engineering program.

09:05.360 --> 09:09.360
So you take two engineering tracks at once, and then it ties together with this ethics

09:09.360 --> 09:14.760
component, which is part of the reason that I talk a lot about ethics publicly, which

09:14.760 --> 09:16.800
we had an STS at RPI also.

09:16.800 --> 09:17.800
Oh, that's great.

09:17.800 --> 09:18.800
That's great.

09:18.800 --> 09:21.000
They snuck it into various places.

09:21.000 --> 09:27.480
It's a sister program to Simpsis, which became a more known program recently, because

09:27.480 --> 09:32.720
one of the Instagram founders came from that program, in any case, a very small program

09:32.720 --> 09:33.720
at that point in time.

09:33.720 --> 09:41.280
Now it's one of the biggest, and I focused in computer science and product design through

09:41.280 --> 09:42.280
mechanical engineering.

09:42.280 --> 09:46.520
So it was very product focused, but through those two lenses of engineering.

09:46.520 --> 09:53.120
So I came into this with a background in web stack engineering and UX and full digital

09:53.120 --> 09:58.800
product design, but I wasn't coming at it from having no programming experience.

09:58.800 --> 10:05.720
So moving into a Python workflow and using tools and technologies like SQL that I'd seen

10:05.720 --> 10:08.800
before was not the primary challenge.

10:08.800 --> 10:12.600
So I definitely wasn't starting from zero, like some people do.

10:12.600 --> 10:13.600
Yeah.

10:13.600 --> 10:16.640
What about the STATS component, where did that come from?

10:16.640 --> 10:21.920
I had taken some STATS classes in college, but there was actually one that I loved, and

10:21.920 --> 10:25.960
the professor thought I was the weirdest person in his course, I'm sure, because it was

10:25.960 --> 10:28.680
a bunch of people who wanted to go into management consulting.

10:28.680 --> 10:35.920
It was a kind of operational statistics class, like how do you understand how cars pass

10:35.920 --> 10:41.080
through four toll booths when you have, you know, two of them open at any given time

10:41.080 --> 10:45.640
block, you know, it's kind of convex optimization problems.

10:45.640 --> 10:51.160
And I thought it was just the most interesting stuff, and I couldn't come up with an application

10:51.160 --> 10:54.200
that was anywhere close to a career that I thought I might have.

10:54.200 --> 10:57.200
I just had no idea how this stuff would be applicable.

10:57.200 --> 10:58.640
Same thing with linguistics.

10:58.640 --> 11:04.920
I, for a long time, would read linguistics textbooks and read a lot of known Trump ski

11:04.920 --> 11:09.920
when I was in high school and more theory behind it, love that stuff.

11:09.920 --> 11:13.000
My parents thought I was going to major in it, and I said, it's not applicable, I can't

11:13.000 --> 11:14.000
use it.

11:14.000 --> 11:15.000
Yeah.

11:15.000 --> 11:19.480
Of course, flash forward to several years later, and I'm actually working quite a lot

11:19.480 --> 11:25.560
with unstructured text, and that's actually the biggest request that I hear in the market

11:25.560 --> 11:26.920
as a consultant.

11:26.920 --> 11:33.800
How do we work with text and understand it through a lens that works for us and isn't

11:33.800 --> 11:38.720
just a word cloud or a count of various themes coming up?

11:38.720 --> 11:46.640
How do we understand it from the perspective of the customer service industry, or we talked

11:46.640 --> 11:51.640
here earlier about data science and HR and understanding feedback, those types of applications

11:51.640 --> 11:55.880
are becoming very popular and widely requested.

11:55.880 --> 11:58.520
So things always come back, right?

11:58.520 --> 12:06.200
One of my favorite designers has this, well, the thing that he paints on billboards.

12:06.200 --> 12:12.480
These guys, Steven Seagmeister, he says, everything I do always comes back to me, and I think

12:12.480 --> 12:18.000
about those all the time, because there are always these little things, these little vignettes

12:18.000 --> 12:22.160
that you take, and you never quite know when you're going to come back to that, and it's

12:22.160 --> 12:24.680
going to be relevant to what you're doing.

12:24.680 --> 12:25.680
Yeah.

12:25.680 --> 12:26.680
Yeah.

12:26.680 --> 12:31.040
So you've built the open source data science masters for folks that are starting at

12:31.040 --> 12:35.440
zero and trying to work their way to, or starting someplace, and trying to work their

12:35.440 --> 12:37.880
way forward.

12:37.880 --> 12:43.080
What are the things that you find folks struggle with the most?

12:43.080 --> 12:47.360
The biggest challenge for the curriculum and for people going through it right now, I

12:47.360 --> 12:53.280
will say this is the two-sided problem, is that people can't find problems that are

12:53.280 --> 12:58.000
appropriately scoped to showcase their talent, and the curriculum can't necessarily provide

12:58.000 --> 12:59.160
that right now.

12:59.160 --> 13:07.000
I have investigated how I might go about providing sample data sets and questions alongside

13:07.000 --> 13:13.200
them that would give you a take-home package of something that would showcase your skills,

13:13.200 --> 13:19.320
but it's actually a lot more work than you do expect, and it's very difficult because

13:19.320 --> 13:21.440
it's a scoping problem at its heart.

13:21.440 --> 13:25.600
You have to have something that has enough depth, but isn't overwhelming, and can showcase

13:25.600 --> 13:33.120
a bunch of different skills, so it's a big challenge for people to sell themselves through

13:33.120 --> 13:37.880
providing that type of portfolio piece.

13:37.880 --> 13:43.560
And at this point, I think Kaggle does a really good job of curating data sets and providing

13:43.560 --> 13:51.480
conversations around analysis and modeling and predictive algorithms and ways to approach

13:51.480 --> 13:54.920
problems, and I usually direct people.

13:54.920 --> 13:58.600
I was going to ask you if Kaggle was one of the places that you point people.

13:58.600 --> 13:59.600
Yeah.

13:59.600 --> 14:04.400
I think they do a really good job with that, so they, I mean, their entire model is built

14:04.400 --> 14:13.840
around that type of work, appropriately, scoping questions around a set of data, and a lot

14:13.840 --> 14:17.640
of people to work on it, and sometimes rewarding them for that.

14:17.640 --> 14:18.640
Yeah.

14:18.640 --> 14:24.320
It's interesting that the podcast has a lot of folks that are somewhere on that curve.

14:24.320 --> 14:31.920
I hear from folks every once in a while asking about how they might apply machine learning

14:31.920 --> 14:38.080
AI to health care, or some problem that they have an interest in.

14:38.080 --> 14:42.640
And it's difficult to manage that scope as a beginner in part because you don't know

14:42.640 --> 14:44.640
what you don't know, right?

14:44.640 --> 14:45.640
Absolutely.

14:45.640 --> 14:50.680
But at the same time, a lot of times when you go to some of the public forums where people

14:50.680 --> 14:54.840
are asking, how do I learn this stuff?

14:54.840 --> 14:58.880
People will say, well, go take this course or course, that course or course, and then

14:58.880 --> 15:00.400
go work on a project.

15:00.400 --> 15:05.640
And the gap between take this course or course, and then go work on a project is actually

15:05.640 --> 15:06.640
pretty huge.

15:06.640 --> 15:07.640
Yes.

15:07.640 --> 15:08.640
Yes.

15:08.640 --> 15:13.760
And it's a gap that you have to fill with building analytical intuition, which is something

15:13.760 --> 15:16.720
that is very hard to teach, but is very learnable.

15:16.720 --> 15:24.600
So there's that counter intuition there that it's something that you can learn, and it's

15:24.600 --> 15:29.320
best learned from other people, but it's very hard to learn it from a book.

15:29.320 --> 15:36.560
So I do encourage people to use that practice, and for example, looking at a Kaggle competition

15:36.560 --> 15:40.960
around health care data and taking a stab at it without seeing what other people are

15:40.960 --> 15:45.320
working on, given a question, and then coming back to see how other people address that

15:45.320 --> 15:51.920
question, very useful workflow, and does provide you some of the asynchronous communication

15:51.920 --> 15:56.440
that you would otherwise have in person in a company.

15:56.440 --> 16:01.960
The other key component there that I think is really helpful is to have questions that

16:01.960 --> 16:05.760
are actually appropriate for the data, and to be very strict about your own workflow

16:05.760 --> 16:10.400
when you're answering that question, because you can get lost in the weeds everywhere.

16:10.400 --> 16:17.160
And in fact, I'd say most data science teams, their biggest struggle is not necessarily

16:17.160 --> 16:23.000
with structure, but with the rigor of having questions that they can actually test and

16:23.000 --> 16:27.120
hypotheses that they can actually test against.

16:27.120 --> 16:34.000
And I certainly do know teams that have a more R&D approach, and that can lead you to interesting

16:34.000 --> 16:41.600
places, but it doesn't necessarily help you answer a question, because you're not necessarily

16:41.600 --> 16:43.640
restricting yourself to that path.

16:43.640 --> 16:44.640
Yeah.

16:44.640 --> 16:45.640
Yeah.

16:45.640 --> 16:46.640
So you've got a teaching vent.

16:46.640 --> 16:50.240
You put together this set of resources, and the natural step consulting, right?

16:50.240 --> 16:53.640
We're here teaching clients that are actually trying to build these teams.

16:53.640 --> 16:54.640
Yeah, exactly.

16:54.640 --> 16:55.640
Exactly.

16:55.640 --> 16:56.640
It is very natural.

16:56.640 --> 17:03.120
And I think the biggest reward that I get in consulting is when I work with someone

17:03.120 --> 17:09.880
who's a little less technical or more distant from the data science, and they start to understand

17:09.880 --> 17:16.480
and into it as people who would be new to data science, they start to into it about what's

17:16.480 --> 17:21.000
going on with the data and how you can answer the question with the data and why it's appropriate

17:21.000 --> 17:26.800
or not appropriate and what manipulations they need to make and what type of data they

17:26.800 --> 17:33.200
need to make in intuitions about what they can do with it.

17:33.200 --> 17:35.160
So that's really rewarding.

17:35.160 --> 17:39.760
I've had the pleasure of working with a couple of product teams, and product teams are great

17:39.760 --> 17:44.680
because they have a vision for what they want as an outcome, and that outcome is really

17:44.680 --> 17:53.160
helpful, much like a driving question or hypothesis to guide you through a set of possible solutions

17:53.160 --> 17:55.600
with a lot of rigor and direction.

17:55.600 --> 18:01.520
So that's been really rewarding to see product managers saying, hey, I think this will work

18:01.520 --> 18:06.680
because I know that it worked in this other case, and we learned about that a couple of

18:06.680 --> 18:09.200
weeks ago, and it is very much like teaching.

18:09.200 --> 18:14.480
Now knowing a little bit about your background now, I can almost imagine the context out of

18:14.480 --> 18:20.680
which the hybrid AI blog post came, you know, product teams telling you, oh, can we just

18:20.680 --> 18:24.160
throw AI at this and not have any humans in the loop?

18:24.160 --> 18:26.160
Yeah.

18:26.160 --> 18:27.160
Did you hear a lot of that?

18:27.160 --> 18:34.040
I've definitely heard that like, well, we know that we have to have some mostly human

18:34.040 --> 18:38.640
approach, and we can use some predictive technology alongside them for a while, but we're

18:38.640 --> 18:41.040
really shooting for 100% at the end.

18:41.040 --> 18:49.280
And that ultimate vision is very problematic because as I explain in the post, and I can

18:49.280 --> 18:53.800
summarize that briefly, but hybrid AI in cases where you need to make sure that you

18:53.800 --> 19:01.120
need people to look at data where you're not certain how to predict an outcome or classify

19:01.120 --> 19:10.240
or whatever your objective is, have them look at that poorly or less confidently predicted

19:10.240 --> 19:18.240
data and make their own judgment about what should happen, allowing that to happen incorporates

19:18.240 --> 19:25.000
the possibility of future outcomes and future inputs.

19:25.000 --> 19:29.040
In cases where you haven't seen everything that you could possibly see because in the

19:29.040 --> 19:34.320
future there will be new and different options, it's only necessary that you would always

19:34.320 --> 19:43.640
involve people because you have to incorporate those new opportunities and those new possibilities.

19:43.640 --> 19:50.200
So I think tempering our expectations about how much work computers will do and what type

19:50.200 --> 19:56.560
of work they will do is really key to building the right solutions because otherwise we don't

19:56.560 --> 20:04.680
have a good Pareto 8020 approach to our problems where we can say, hey, let's set aside this part

20:04.680 --> 20:08.240
of the problem because we know it's always going to be too hard for the computer.

20:08.240 --> 20:14.560
It will cost us 80% of our time to solve 20% of the problem and it doesn't actually make

20:14.560 --> 20:15.560
sense.

20:15.560 --> 20:17.000
It's just route that to people.

20:17.000 --> 20:22.680
We might learn more about that problem space in the future, but we also know that there's

20:22.680 --> 20:27.040
kind of slop that we always need to account for and that's important.

20:27.040 --> 20:33.600
And it sounds like you don't think we're anywhere near, you know, closing that gap, getting

20:33.600 --> 20:37.240
to the humans out of the loop.

20:37.240 --> 20:40.840
It depends on the application you're looking at, absolutely.

20:40.840 --> 20:46.600
We used a human and the loop system at Mattermark for various tasks on the machine learning

20:46.600 --> 20:52.080
team and we actually had people in-house in addition to some systems where we had outside

20:52.080 --> 20:54.720
labeling done.

20:54.720 --> 20:57.520
And we had used vendors for that type of thing.

20:57.520 --> 21:07.040
There are a couple good options there, but I think there's a lot of work to do.

21:07.040 --> 21:13.920
I think the pragmatism on the shape of the solution, the solution space that's possible

21:13.920 --> 21:20.760
to achieve in a reasonable amount of time and any sort of reasonable cost for a solution

21:20.760 --> 21:25.840
all drive us toward this hybrid case.

21:25.840 --> 21:34.760
And you also discover pretty interesting things when you use people or services that have

21:34.760 --> 21:40.080
people labeling data or providing feedback because they will give you more information

21:40.080 --> 21:42.000
than you asked for in some cases.

21:42.000 --> 21:48.080
And we actually have had people in the past come back to us, find our email addresses.

21:48.080 --> 21:52.400
I don't think they were given them, so they actually went out and did research, found

21:52.400 --> 21:55.640
out how to email us and said, hey, you asked me this question.

21:55.640 --> 21:59.200
I actually think there's kind of an issue with how you phrased it.

21:59.200 --> 22:01.760
It doesn't fully address this other issue.

22:01.760 --> 22:05.320
Have you thought about that? I'm worried that I answered the question wrongly.

22:05.320 --> 22:10.400
I mean, these are people that were on your labeling team that felt so compelled to.

22:10.400 --> 22:13.920
These are actually people that weren't even on the team.

22:13.920 --> 22:19.400
They were a part of an outsource group of people that were paid to work on that data.

22:19.400 --> 22:26.120
So you learn a lot because you get more perspectives and more eyes on the data, which is always

22:26.120 --> 22:30.440
a good thing, especially when you're thinking about blind spots that you might have.

22:30.440 --> 22:35.120
Yeah, I thought even one of the simple things that I thought was pretty interesting about

22:35.120 --> 22:40.680
that post was you presented some kind of broad brush stats, and I don't remember the specific

22:40.680 --> 22:51.080
stats about something along the lines of AI by itself right now, a machine learning solution

22:51.080 --> 22:57.200
can get to 90% accuracy for generalized speech interpretation.

22:57.200 --> 23:01.440
But in order to really be usable, it needs to be 98 or something like that.

23:01.440 --> 23:07.320
I forget the numbers, but I think, you know, I don't think people think about that enough.

23:07.320 --> 23:12.640
They don't. They don't. So it's funny.

23:12.640 --> 23:17.040
When humans look at data, they have a very different perception of it than when they look

23:17.040 --> 23:24.480
at the metrics about the data. So for example, if you have a classifier for five classes,

23:24.480 --> 23:31.560
and you look at a 75% accurate classifier over all of those classes, it will look like garbage

23:31.560 --> 23:38.120
to you as a human. Even though that's pretty high, relatively speaking, and you probably

23:38.120 --> 23:45.760
did some work to get it to that point, you would probably still call it an unacceptable

23:45.760 --> 23:52.080
option or a non-prefable classifier because that's, it looks like garbage to you.

23:52.080 --> 23:59.600
I think the cases where that garbage matters is where we have to worry about the way that

23:59.600 --> 24:07.960
we build hybrid into solving that last component and getting to 99% or 95% or whatever we need

24:07.960 --> 24:14.800
to feel good about the application. For example, if we're predicting the health outcomes

24:14.800 --> 24:21.560
for a person, that's a very high stakes prediction, we would probably want to skew much further

24:21.560 --> 24:29.400
in the hybrid direction or in the human augmented direction than otherwise because the stakes

24:29.400 --> 24:35.920
are actually very high. So I think when we start to discriminate between types of applications,

24:35.920 --> 24:42.360
that's where we see this coming in. But even for consumer applications like Google

24:42.360 --> 24:47.920
Knowledge Cards, things like that, people still curate a lot of that information. It's

24:47.920 --> 24:53.760
not necessarily summaries that are generated by a computer. Sometimes there are people

24:53.760 --> 25:02.560
that are taught to create that data in a particular way. And I think we saw a great example

25:02.560 --> 25:09.840
of this a couple of weeks ago when news about how Facebook curates news articles came out.

25:09.840 --> 25:20.240
And that's a very good example of how your definitions of taxonomies, your acceptance of

25:20.240 --> 25:28.360
how things are classified and your incorporation of new information all impact your end user.

25:28.360 --> 25:37.120
And sometimes in very critical ways, it might sway how someone votes. It might give someone

25:37.120 --> 25:43.800
a perception of the world that they otherwise might not have in that case. So I think we're

25:43.800 --> 25:49.560
starting to see the impacts as well from consumer applications that we thought were not so

25:49.560 --> 26:00.040
high in terms of risk. And I look forward to seeing what they invest in at Facebook because

26:00.040 --> 26:06.520
I think I would wager that they have people working on this that have an eye on how to

26:06.520 --> 26:12.560
make this better. But at the end of the day, you do end up in a semi-political discussion

26:12.560 --> 26:19.480
about what fair and balanced means in journalism. And it becomes very domain specific. So I think

26:19.480 --> 26:26.600
it's healthy for society to grapple with that and for us to think very critically about

26:26.600 --> 26:30.200
how these things are actually working and sort of just engineering them away and having

26:30.200 --> 26:39.480
a 100% machine solution. Are you aware of anyone, any groups working on the problem of

26:39.480 --> 26:47.160
hybrid, either from an academic, other academic research topic areas in there somewhere or tools,

26:47.160 --> 26:52.280
platforms, or is it, you know, everyone kind of figuring this out on their own building

26:52.280 --> 26:56.920
their own custom thing? And that's just the state of the art right now.

26:56.920 --> 27:04.480
So the short story is that a lot of companies do build their own custom platforms for

27:04.480 --> 27:13.160
doing this. They usually leverage some sort of marketplace for data entry, data annotation,

27:13.160 --> 27:21.200
a question answering, and broader products like Amazon Turk is a very broad product. You

27:21.200 --> 27:28.200
can arbitrarily give people tasks and you place a bid on how much you would pay people for

27:28.200 --> 27:34.680
those tasks and they can choose to accept it. So a lot of companies will use that platform

27:34.680 --> 27:41.880
and build on top of it and do a lot of integration of that type of system on the back end. So in

27:41.880 --> 27:49.880
some ways, you know, they call this artificial artificial intelligence. In some ways, the

27:49.880 --> 27:55.080
component is actually a technology interface itself, which is very interesting to think

27:55.080 --> 28:01.200
about because there are people on the other side of the other side of the technology.

28:01.200 --> 28:08.680
But there are a couple other vendors that do things to support hybrid. Crowdflower is one

28:08.680 --> 28:17.760
in San Francisco that does some of that. To my knowledge, they do interobserver validation

28:17.760 --> 28:25.440
basically to give you multiple, multiple sets of eyes on a given answer to a question

28:25.440 --> 28:33.080
to ensure that it's correct so you don't have bigger sets of errors or unmeasurable error

28:33.080 --> 28:38.280
and you know where things are going to be more ambiguous. That in itself can be very

28:38.280 --> 28:45.080
valuable to because you can, you can basically say, here's this big set of data or this big

28:45.080 --> 28:49.760
set of questions, let's say, how would you answer these questions and give it to multiple

28:49.760 --> 28:55.120
people and you'll find out where people disagree and that tells you more about the ambiguity

28:55.120 --> 28:59.960
of the problem space and where you're going to have to make stronger decisions about what

28:59.960 --> 29:05.400
you think is right. So that's been a really helpful thing for clients to understand in

29:05.400 --> 29:13.520
the past and I think they have a pretty good understanding of how that works and we'll

29:13.520 --> 29:20.440
see if they build more products around that. Do you have a, you know, top three takeaways

29:20.440 --> 29:28.800
that, you know, you found that clients, you know, as you look across a set of clients,

29:28.800 --> 29:32.600
you know, these are the top three things that, you know, they all, you know, either learned

29:32.600 --> 29:38.600
or need to learn in order to be successful at this stuff? I can tell you the first one

29:38.600 --> 29:45.640
is always know what your question is. Be very precise and know exactly what the answer

29:45.640 --> 29:51.840
would look like if you saw it. So if you see the answer, you'll know that the right thing

29:51.840 --> 29:58.000
is happening. I certainly worked with companies that say, hey, we have all this data, we want

29:58.000 --> 30:03.160
to learn from it and I say, great, what do you want to learn? And they say anything. And

30:03.160 --> 30:08.920
so that's, that's a perfectly healthy and normal place to start, but at that point, you don't

30:08.920 --> 30:12.960
have a question where you can build anything. So you have to formulate questions and decide

30:12.960 --> 30:17.800
what's actually valuable for your business, which is more of a business and product space

30:17.800 --> 30:25.560
question formulation task. So that strategic involvement has necessarily become part of

30:25.560 --> 30:31.800
the business. Coming from a product background, I can appreciate that. I think there are

30:31.800 --> 30:38.480
a lot of other independent consultants and people I know who work solely on questions after

30:38.480 --> 30:42.440
they've been fully formed. And they say, you know, once you have the specs ready, happy

30:42.440 --> 30:50.720
to work on it. But otherwise, it's, it's not, it's not what we do. And that initial step

30:50.720 --> 30:55.760
of defining your question and knowing that it's an appropriate question for the data, it

30:55.760 --> 31:03.000
really is the space where we, we thrive and help our clients succeed. So if they can come

31:03.000 --> 31:08.640
in with a strong understanding of what they have and what they want, that's always better.

31:08.640 --> 31:15.520
I think that's true broadly in business. But we think what else? So I was just having

31:15.520 --> 31:22.160
a really good conversation over lunch with a couple people about how one of the things

31:22.160 --> 31:30.320
that we don't see as often in data science, machine learning, land is a strong leadership

31:30.320 --> 31:37.640
that knows how to market really well. So a lot of what I've seen data science team struggle

31:37.640 --> 31:42.520
with is marketing themselves internally or marketing themselves up and managing up

31:42.520 --> 31:50.720
to see sweet or the VP of engineering, whoever it is. And it's, it's really important

31:50.720 --> 31:57.080
to develop those soft skills and understand what your value is relative to the company.

31:57.080 --> 32:02.160
Sure. And I can say that. But at the end of the day, it's actually extremely difficult

32:02.160 --> 32:10.520
to define that value because your systems may be giving some feedback to a business team

32:10.520 --> 32:15.680
that allows them to make better decisions, but really they're making their own decisions

32:15.680 --> 32:20.640
and they're supporting them with data in some cases. But you don't know what the investments

32:20.640 --> 32:25.640
would have looked like otherwise. And so comparing the alternate universe that you might have

32:25.640 --> 32:31.280
been in had you not had the technology that your team is building can be extremely difficult

32:31.280 --> 32:36.880
to quantify. But that is part of the work of leadership. Right.

32:36.880 --> 32:45.800
Clearly. So I look forward to seeing more breakout leaders that are really good at that.

32:45.800 --> 32:51.960
And I think it'll necessarily be something that we see in the next few years. I wouldn't

32:51.960 --> 32:55.840
call myself a pessimist, but I would say we're kind of high on the hype cycle right now.

32:55.840 --> 33:01.600
And I'm not optimistic that we're going, the market will continue going up. So I'm

33:01.600 --> 33:08.160
speaking. It goes in a cycle of company saying, hey, we're going to make this big investment

33:08.160 --> 33:15.080
data science. We think that data science is a very valuable investment for us for these

33:15.080 --> 33:21.520
reasons. And then a couple of years later, they come back to the team and they say, so

33:21.520 --> 33:27.000
how have we done? And at that point, the team really needs to sell what they've done.

33:27.000 --> 33:31.640
Ideally, they'd be selling that along the way as well. And I think we're coming to the

33:31.640 --> 33:37.320
end of one of those periods where companies expect to see those big wins and teams really

33:37.320 --> 33:43.880
need to justify their existence and be able to move the needle and describe how they're

33:43.880 --> 33:50.360
moving the needle. Yeah. Yeah. Soft skills. Yes. Soft skills. Soft skills. Yeah. Take that

33:50.360 --> 33:55.200
vent diagram of all the things you were supposed to be as an Amazon. I just add like four more

33:55.200 --> 34:05.000
things to it. No problem. Yep. Nice. Nice. So that's two, but they're big. So. Yeah.

34:05.000 --> 34:09.240
Well, on the third, the third is probably just manage expectations, right?

34:09.240 --> 34:15.000
Oh, it's always. And any other, you know, any other of a number of sets of things in terms

34:15.000 --> 34:22.360
of the expectations always. You said you said it exactly right. Yeah. Managing the expectations

34:22.360 --> 34:28.200
is probably the biggest thing I do with clients. The first thing I say is I can't do anything

34:28.200 --> 34:34.360
to pull a big win out of hat. I won't be pulling a big win out of hat for you. If you still

34:34.360 --> 34:38.680
want to talk about this and you want to find out what this technology can do for you and

34:38.680 --> 34:44.120
how it can incrementally improve your business and create new opportunities for products. Let's

34:44.120 --> 34:47.640
talk about that. But it's not going to surface anything that you don't know about your own

34:47.640 --> 34:52.920
business because frankly, you know about your business. Your business is an existence. So you've

34:52.920 --> 35:01.720
must have some deeper understanding of what you're doing. And when I look at your deal flow,

35:01.720 --> 35:05.400
your best customers are your best customers. I'm not going to tell you that there's,

35:05.400 --> 35:11.720
there's a, there's a sleeper whale somewhere deep inside Salesforce. And that's okay.

35:12.520 --> 35:17.800
I think give you better confidence to make decisions and understand the differential value

35:17.800 --> 35:23.720
between things. But no promises. You really can't make promises. Yeah, absolutely.

35:23.720 --> 35:35.560
So, you know, going back to this conversation around hybrid AI, we started to talk about, you know,

35:35.560 --> 35:44.920
the role that humans in a loop play relative to, you know, their biases and, and, you know,

35:44.920 --> 35:50.520
quote unquote algorithmic bias and things like that, which actually that was the kickoff panel

35:50.520 --> 35:56.920
here at the Rangle conference. Is that that's something that you're spending some time looking

35:56.920 --> 36:05.960
at now as well, right? Yes. So these things are all interwoven in some way. The active learning

36:05.960 --> 36:15.960
and human and the loop patterns of hybrid are certainly ways to combat actively reinforcing

36:15.960 --> 36:24.360
pre-existing bias. If you construct a system to do expensive or amplify or both, you can do either

36:24.360 --> 36:31.880
end both. It depends on what you know about what you're doing, right? So if the example I give is

36:32.440 --> 36:39.160
a model that was built at one of my previous employers where we wanted to predict who would start

36:39.880 --> 36:44.760
a startup and leave their job and start a startup within the next six months. And the company had

36:44.760 --> 36:49.320
an intent to build this model, create a list of people that were going to start companies soon,

36:50.040 --> 36:56.840
and sell that list to investors as a type of pre-crime, basically algorithmic pre-crime for

36:56.840 --> 37:01.400
seed stage funds. And they could get in early, before people even knew that they were going to start

37:01.400 --> 37:10.120
companies, which is a like fascinating concept. So they used a number of factors to make this

37:10.120 --> 37:16.280
prediction like where you had gone to college, what kind of degree you had, what your job title was,

37:17.880 --> 37:24.200
what your previous employers were, there was a bucketing for the prestige of your college.

37:24.200 --> 37:29.480
So you know the IVs were at the top and kind of cascaded down through bigger institutions,

37:30.120 --> 37:38.440
and that included age. So what we ultimately saw when we predicted who

37:38.440 --> 37:43.800
would be a founder in the next six months was pretty interesting because all of those factors

37:43.800 --> 37:51.800
seemed to be directly relevant to how a person's career would develop them to be a founder in the

37:51.800 --> 37:59.720
future. And interestingly, a lot of the people in the list were 30-year-olds management, ex-management

37:59.720 --> 38:09.160
consulting, or ex-I bankers who were white males. And it didn't deviate too far from that.

38:09.160 --> 38:15.720
And at the time, I thought, you know, this is this is pretty uncomfortable, but I don't really

38:15.720 --> 38:25.080
know why. And it took me, it took me about a year to examine that emotional little more deeply.

38:25.080 --> 38:30.760
And underneath is actually a very good reason to be concerned because though you are making

38:30.760 --> 38:37.480
a prediction on characteristics that you think are fundamentally predictive of an outcome,

38:37.480 --> 38:47.960
they have bias from the world rolled up into those factors. So all of the decisions that were made

38:47.960 --> 38:52.760
to allow people to get to where they were and become founders in the previous

38:52.760 --> 39:00.680
state of the world and the training data is your prior for your prediction of who will be

39:01.160 --> 39:09.080
a future founder. And if you don't explicitly observe that, you know, I think I clicked through

39:09.080 --> 39:15.800
on LinkedIn to maybe that top 120 people on this list. And that's the only way that I knew

39:15.800 --> 39:23.880
that there was a certain split of like where people came from in terms of

39:23.880 --> 39:32.520
home country or home state, where people came from in terms of in in terms of

39:35.400 --> 39:42.040
age, all of these other characteristics, but even name can be ambiguous for what gender you are.

39:42.040 --> 39:47.800
So I didn't get a sense of who was actually in this list until I went and looked at it and we

39:47.800 --> 39:52.280
didn't have columns that said male female. We didn't test against that. We didn't predict on that,

39:52.280 --> 39:59.640
but there were only 13 women at the top of the list or near the top of the list. And I thought,

39:59.640 --> 40:08.840
well, that's that's somehow unfair, right? And I think looking back on that at the time,

40:08.840 --> 40:18.680
it was we were not talking about that specific type of diversity in the market for founders.

40:19.320 --> 40:25.560
Now that that conversation is happening more, it's become more unambiguous case where you can say

40:27.320 --> 40:34.760
all of the priors, all of the pattern matching, so to speak, literally coming from VCs is being

40:34.760 --> 40:39.480
encoded into the algorithm that's making this ultimate prediction. And that's not okay. So

40:40.280 --> 40:45.000
the question then becomes what do we do? And there are a couple of people doing really great work

40:45.000 --> 40:51.640
on this. I think there's one department at Carnegie Mellon where someone's coming up with validation

40:51.640 --> 40:59.080
metrics that will help you test against the the characteristics you know you might have

40:59.080 --> 41:07.480
bias outcomes on. So in this case, you would say how many men and women are there in our outcome?

41:08.040 --> 41:16.440
Does it fit our expectation for what we would want to happen? And I think the the real key insight

41:16.440 --> 41:25.800
here is that we want to build algorithms that will construct a world that we want to live in rather

41:25.800 --> 41:34.200
than a world that existed in the past. We know the flaws of our current society to a large extent

41:34.200 --> 41:39.800
and some people more than others, but as long as we can be vulnerable to one another

41:41.800 --> 41:52.760
and try and validate that we are not reinforcing unjust actions from the past and just perpetuating

41:52.760 --> 41:58.360
them with algorithms in the future, that is actually key to our work. And that's really important for

41:58.360 --> 42:07.800
us to carry as a torch going forward. So I'm very excited that we had actually one talk so far

42:07.800 --> 42:15.640
and we will have another talk today about algorithmic bias and harm and how these systems affect

42:15.640 --> 42:24.040
users and I think it's a conversation that needs to gain more traction in the practitioner space

42:24.040 --> 42:30.280
and we need to examine our own practices which more closely and know what we're doing.

42:32.360 --> 42:36.760
Perhaps the most egregious example of this in the press lately was a

42:36.760 --> 42:45.640
algorithm that police stations were using across the country to predict recidivism which

42:47.160 --> 42:53.560
is the one that was exposed in the pro-publica article. Yes, exactly. And they did a bunch of work

42:53.560 --> 42:59.320
that they put up on GitHub along with the data set to explain what was happening

42:59.320 --> 43:07.000
and how they had analyzed the outcomes and as far as they could see what was happening in that

43:07.000 --> 43:13.560
technology and I believe the company is still holding it as private IP so even the police

43:13.560 --> 43:19.640
departments don't understand how this model works. The journalist did a really good job of saying,

43:19.640 --> 43:25.800
this is a big problem, here are the metrics and here is the full explanation with the data of

43:25.800 --> 43:33.320
what's so wrong with this beyond the anecdotal evidence of this is predicting that one person who

43:33.320 --> 43:41.240
has committed one petty crime is more dangerous than someone who's a repeat criminal and has been

43:41.240 --> 43:51.240
violent. I think it's a really egregious case but I don't want to say that it's good these

43:51.240 --> 43:57.640
things happen but I think a few high profile cases will push the regulatory system to become

43:57.640 --> 44:05.480
more serious about this so insofar as like it has to get worse if it gets better. I'm hoping that

44:05.480 --> 44:11.240
that we can get out ahead of that as practitioners but regulation will certainly be getting there as

44:11.240 --> 44:17.240
more and more of these cases are uncovered. Do you have a vision for how regulation can play here

44:17.240 --> 44:22.440
without overly suppressing innovation which is a big concern that you hear on the other side?

44:23.000 --> 44:32.200
It is and a good example of where you see that is in loan assessment and the finance space where

44:32.200 --> 44:39.400
there are very strong regulations about how how you make decisions about what credit lines people

44:39.400 --> 44:47.080
will get so given again like things that got worse before they got better redlining in the past

44:47.080 --> 44:53.800
and other actions that have been taken that were deemed not legal after that. That environment has

44:53.800 --> 45:05.880
responded extremely strongly to that. There's a pretty good understanding of what is important

45:05.880 --> 45:11.080
in making that work transparent so that you can actually give someone feedback on why they were

45:11.080 --> 45:18.120
rejected for a loan or why they were given a certain loan amount or a certain credit line

45:18.120 --> 45:26.760
and while that's important I think there are improvements further to be made so I would

45:26.760 --> 45:32.600
expect that if the regulation comes down really hard in the way that it has on that industry

45:32.600 --> 45:38.280
if it comes down similarly on others as I think we're seeing that you just becoming interested in

45:38.280 --> 45:48.360
then it can stifle innovation and probably grind it to a very slow pace but we're resilient,

45:48.360 --> 45:56.760
we'll figure out ways to justify our existence and how we do our work and I think that's very healthy

45:56.760 --> 46:04.840
in the ecosystem and the the oven flow of these factors and regulation and innovation are always

46:04.840 --> 46:11.640
battling it out and the faster we can get out ahead of it and say no no no we actually know

46:11.640 --> 46:15.720
what we're doing and we actually know how this works and we are justifying these things and we are

46:16.440 --> 46:23.480
taking the appropriate precautions and trying to be as self-critical as possible and doing so

46:23.480 --> 46:31.080
honestly if we can do that then the regulation will be the regulatory environment will be very

46:31.080 --> 46:39.240
different when it finally comes to bear in these other areas that aren't just credit-worthyness.

46:39.240 --> 46:45.960
Yeah great great well we've got additional talks here to go check out anything you want to

46:45.960 --> 46:54.360
leave folks with point folks to I would say keep watching the algorithmic harm and ethics

46:54.360 --> 47:03.080
arena there's a lot of work being done there and there are people that are finding great solutions

47:03.080 --> 47:07.640
and people that are also of course always coming out with more critique and

47:09.080 --> 47:14.120
interesting philosophical perspectives to consider so stay involved in that conversation

47:14.120 --> 47:19.960
because it's an active one and everyone can be part of it. Yeah that's that's great and I think

47:19.960 --> 47:28.280
the you know your comment about you know AI encoding the future that we want as opposed to the

47:28.280 --> 47:36.200
past that we you know that we know I think it's a great one very very optimistic. Yes yes if you

47:36.200 --> 47:42.840
care about that stay involved in the conversation and and yeah be a part of it. Great all right thanks

47:42.840 --> 47:53.960
so much Claire. Thanks Sam. All right everyone that's our show for today I really hope you enjoyed

47:53.960 --> 47:59.480
the interview and thanks so much for listening. Of course you can find the notes for this and every

47:59.480 --> 48:07.800
show at the Twimalei.com website twimlai.com the notes for this particular show can be found at

48:07.800 --> 48:16.520
twimalei.com slash 11 the number 11 as always I really appreciate getting your tweets and emails

48:16.520 --> 48:23.640
and newsletter subscriptions and iTunes reviews so by all means keep them coming. Of course we'd

48:23.640 --> 48:29.800
love to have you join the conversation you can tweet me at Sam Charrington. Claire is Claire

48:29.800 --> 48:38.120
Corfell and I'm also increasingly using the Twimalei Twitter handle. T-W-I-M-L-A-I. Looking forward

48:38.120 --> 49:02.280
to hearing from you and catch you next time.

