All right, everyone. Welcome to another episode of the Twimmel AI podcast. I am your host,
Sam Charrington, and today I am joined by Jeff Kloon. Jeff is an associate professor at the
University of British Columbia and a faculty member at the Vector Institute. Before we get into
today's conversation, please be sure to take a moment to head over to Apple Podcasts or your
listening platform of choice. And if you enjoyed the show, please leave us a five-star rating and
review. Jeff, welcome to the podcast. Thank you. It's a pleasure to be here. I'm looking forward
to our conversation. We will be talking about some of your work relating to the path towards AGI.
In particular, this idea of AI generating algorithms. But before we dive into that topic,
I'd love to have you share a little bit about your background and what brought you to the field.
Yeah. Well, thanks again for inviting me. So my background actually going way back is philosophy.
So when I got through, you know, as an undergrad at University of Michigan,
I was trying to figure out kind of how does thinking work, you know, and how do we think about
thinking and how maybe could we be recreated in a machine. And I thought, you know, who's got
the market cornered on thinking about thinking, and I thought that was philosophy. So it was a
fascinating, you know, a few years. But ultimately, I started to get frustrated because I couldn't
test my hypotheses and try to learn by building. And so over time, I came to realize that the best
place for me would be in machine learning and trying to build intelligence to learn more about it.
And another quest that I've been on my whole life, which I was also fascinated about then,
still to this day, is kind of the explosion of complexity we see in the natural world.
You know, how does such an unintelligent algorithm like Darwinian evolution produce jaguars at hawks
and human brain and three-toed sloths? This amazing explosion of marvels in the natural world.
And I am super excited to think about could we create computer algorithms that can
replicate that creativity and endless innovation. So I went back to graduate school and started
studying, you know, machine learning, got a PhD in computer science, actually got a master's
degree in philosophy. That's a little bit of a different story. I went, that one did a postdoc
at Cornell and became a professor at the University of Wyoming. And then my friends and I had a
startup called geometric intelligence. Uber acquired that to create, you know, Uber artificial
intelligence lab, Uber AI labs. And so they asked us to move out to San Francisco to stand that up.
So we built up Uber AI labs and it was a wonderful place, a great intellectual
environment. And then I went over to open the eye and let a research team there.
And now I am a professor at the University of British Columbia.
Awesome. Awesome. Now geometric intelligence, was that with Ken Stanley?
It was. Ken Stanley is a long-standing friend and collaborator of mine. We still talk every week.
And he is, you know, part and parcel all the ideas that I've been working on throughout my career.
So yeah, he is the person who brought me, one of the people that brought me geometric and
we get to work alongside each other standing up Uber AI labs.
Awesome. Awesome. You mentioned that you went to philosophy to kind of learn about thinking.
You know, what's your top list of, hey, if you want to think about thinking, these are the things
you need to go read from philosophy. I think one of the most fascinating questions in all of science
is this idea that shows up in philosophy and we're starting to wrestle with it and think about
it and machine learning as well. And that is, when do you go from rocks to feeling? You know,
originally this planet was rocks and weather. And now we have beings that feel pain and fall in love
and love the taste of chocolate and they below the taste of something else. And we, you know,
philosophers call this qualia. And it's kind of this extra thing that gets layered onto the world
at some point. And we have very little understanding of how that might happen physically,
despite, you know, talking about it for millennia. And a central question that we're starting to
grapple with in machine learning is when might such feelings show up in AI systems? When might we
start to have to recognize the pain and suffering of AI agents? And when might we have to start
treating them as beings that have ethical worth that we can't say enslave or torture or make suffer
for our own ends? And this is the kind of stuff that I used to be afraid to even say out loud,
on a public podcast like this, because I thought I was career suicide to let people know that I was
thinking about these things, even though I deeply think that they're true. But increasingly,
you see more and more mainstream figures in the field worried about this. You know, I actually
recently saw a tweet from Richard Sutton that brought this idea up just like last week.
And there are many other people in the field that are becoming concerned. I think nobody really
thinks we're there yet. But also, I think nobody knows when we're going to cross that threshold.
And so we probably want to get ready so we don't accidentally cause tremendous amount of
suffering before we realize it and then have to feel really bad about it. So that's one example
of thinking by thinking. I think that, you know, I think I could probably make up some other
examples. But for the most part, it's probably why I went over to machine learning is that I wasn't
finding a lot of, I mean, there's a lot of fascinating topics for sure, but I wasn't finding a lot
of the answers to how do we really build the thinking machine and how does thinking really happen
in our head that I couldn't, that I wasn't seeing better address in the sciences than in philosophy.
Coming back to machine learning, you've talked about this idea of AI generating algorithms as,
you know, one of the possible paths to getting us to to generalize artificial intelligence.
Why don't you talk about that and introduce that idea in the context of the other paths and how
you think about the role each of them will play? Sure. So I think that if you look out into
machine learning, it's pretty clear that there is kind of a dominant paradigm. And this was
certainly true a couple of years back. It's increasing a little less tree, but for the most part,
look at any Neurops ICML-ICLEAR conference and what are most of the papers doing? They're saying,
hey, I think that there's a building block that is important, you know, and I'm going to try to
either introduce a new building block that we may not have had in our system before or create an
improved version of an existing building block. You know, different types of neural activation
functions are ways to normalize weights or layers, you know, better optimizers, maybe better
recurrent cells of memory and writable memory and, you know, all these different things.
And the thing that I find interesting about that is what we don't usually do is take a step back
and think about the really grand emissions of the field and think about how are we going to get
all the way to our grandest ambitions, you know, which is maybe making AGI. There's kind of this
assumption of one that in this first phase, we'll be able to manually identify all of these building
blocks and the right version of them. And then two, at some point, there's got to be some phase two
where we put all these pieces together. And that is just a herculean challenge if you really
think about it. And I think we should be clearied about how hard that might be to take all of the
different things that all of the community is building and stick them all together and tune all
their hyper parameters and make them interoperate perfectly. I mean, just think about debugging that thing,
let alone building it in the first place. It would require an Apollo scale or Manhattan project
scale effort, in my opinion, to pull that off. However, I think that if you look in the history of
the last 10, 20 years of machine learning, there is a undeniable trend. And that is that hand
design pipelines give way to learn pipelines as you have more compute and data. So let's take some
examples. The classic and first one, the shot herred on the world, with features inside of pipelines,
like vision, language, speech detects, et cetera. You know, we used to hand design all of our
features and then learn a little bit on top of those features. People like Jan LeCune and Yasha
Benjio and Jeff Hinton were saying, let's just learn the whole thing. They were right and everybody
knows that now. But that same exact trend has applied over and over again on other topics. So
look at architectures. We used to design them by hand, we still mostly do. But increasingly,
the best architectures are learned or at least they're competitive. RL algorithms themselves,
like learning to reinforcement learn, hyper parameter tuning, even data augmentation pipelines,
et cetera, et cetera, et cetera. The writing is on the wall. Once we get a lot of compute and
data, we let machine learning do the heavy lifting, we should just learn the whole thing. So what I
argue is that we should apply that thinking to the process of our grandest ambitions of creating
AGI itself and say, instead of trying to manually design all of the pieces of some
Rube Goldbergian thinking machine, let's just try to get out of the way, set up the system such
that compute and data and machine learning can do the heavy lifting for us. And let's try to
learn the whole thing. And so if you want to make progress on that, that would be this thing
that I call an AI generating algorithm. It's basically AI that makes better versions of itself.
It starts simple and it bootstraps itself up from simple origins all the way potentially to
AGI. And if we want to make progress, I think we have to push on three pillars. One, we have to
meta-learn the architectures. Two, we have to meta-learn the learning algorithms themselves.
And three, we have to automatically generate the learning environments so that the system can
learn forever and it's not trapped within one specific domain. And so that's the idea of an AI
generating algorithm. And I think it might be the fastest path to produce AGI. And even if it's
not, it's still scientifically interesting because it teaches us how can a system like Darwinian
evolution have produced us. It strikes me that the way AI generating algorithm, the way you kind of
pose that problem, it is maybe there's a pushback a little bit. It's kind of an accelerator to building
the building blocks, but to kind of switch rails from kind of this broader, more holistic approach,
we need to, we kind of have to figure out what's the objective function on general intelligence.
I'm like, we have no idea how to do that. Completely agree. I'm not saying it's going to be easy.
But and I think that you hit upon one of the central grand challenges in an effort like this.
Like, if we want to make such a system work, we have to figure out, yeah, what is the reward
function or the loss function for the overall system? What is it trying to optimize? And how,
how can we create what's called an open-ended algorithm that will keep going forever
and not stagnate? But I don't think that we lack ideas on this front. I think this is a
place where we can do a lot of research. And I think there's probably a touring award out there
for somebody who can figure out the right answer to that question. So I think it's a fascinating
kind of grand challenge of science to figure out what kind of a loss function could you put in
such a system such that it would bootstrap itself all the way up from, you know, almost being
completely unintelligent all the way past human intelligence. Yeah, and, you know, I kind of pose
that as a way to kind of fine tune my understanding of what you're trying to accomplish with your
work around AI-generating algorithms. Do you see that work as being applied to kind of evolving,
you know, that reward function in such a way that, you know, you get us beyond building the
individual components or, you know, are you focused on kind of evolving those individual components
more quickly without really understanding how they get us to, you know, this broader way of thinking
about, you know, getting to AGI. Yeah, so that very interesting question inspires me to make
three different comments because there's three things that are related there. So one thing is that
I think that one of the hardest things is trying to figure out how do you take all these different
pieces that like the community might be building and put them all together in the way that really
works with the kind of elegance and beauty and effectiveness of, say, the human mind.
And so I think rather than trying to separately create components even with a system that was
learning the components and then later like figure out how to put them together, the algorithm should
do all of that work for us, you know, it should say, you know, you've got system one type thinking
fast and slow, you know, system one, system two, fast and so you've got hierarchical RL with
different levels of abstraction, you know, that you have long term planning and short term planning
and continual learning and all these things have to work well together and we should just basically let
the an AIGA figure out how to create those pieces and make them work well together. So this path
would not be trying to separately create these components onto the reward function issue. There's
actually two fascinating questions there. One is kind of what would the reward function be for an
individual, individual agent in the system? And one thing that I think is fascinating is the system
itself would probably end up creating at least all of the intrinsic rewards for the agent. So evolution,
there's debate about exactly what it's what it's reward function is, but let's assume itself
replication, for example, with that simple kind of higher level reward function, look what it did
in your brain, you know, if you're like me, you love ice cream and you love guy, you know, seeing
beautiful views and you love running up mountains and you might be interested in romantic partners
and you probably really don't like putting your hand on a hot stove. You're probably also curious
and you like to play and learn and you probably care about what your peers think of you. You've got
all of these different rewards kind of baked into that motivates you to do the things that you do
every day, every week, every year, right? Where did those come from? Well, basically evolution figured
out that if it makes beings that have those kind of internal rewards, they are more effective at
this other thing which is self replication. And so I also don't think we should be trying to hand
specify like we do often in machine learning, oh, it should be curious, it should try to go to new
states, it should probably try to maximize its reward, it should probably try to interact with other
people and maybe it should try to communicate with other people, we'd like manually design all
these like hack down rewards that we inject in the system in the hopes to get to do what we want.
I think instead we probably need to be looking for that outer loop meta reward, it's really kind of
general and probably simple. And with enough compute, the internal stuff all kind of shows up in
the system and it kind of knows, it kind of creates a ton of intrinsic rewards like paying in
pleasure and curiosity, et cetera. So then the question is what is that outer loop reward function,
which is this is the third thing I wanted to talk about. We don't know exactly what that is,
but I will just give you a sketch and I'm sure this is wrong, it's not the right answer,
but it feels like it's pointing in the right direction and suggests the research we could do.
What if you had an agent that was motivated to learn somehow, it's like wants to continually
learn new things and the things that it wants to learn, we find interesting or useful and by
we, I mean humans. And so maybe somehow some way it's grounded to our world, maybe it has to
get better and better at solving real world problems or making money on earth or just making humans
think that what it's doing is worthwhile or imitating YouTube and things that we do.
That basically allows broadly writ a system that can learn forever and not learn to memorize
white noise patterns, but instead learn things that we find useful. Now that's overly simplistic,
it won't work in practice, but that is I think where a lot of the research should be focused,
which is can we figure out those kind of general reward functions plug it into an AIGA
and good things continue to happen forever. And I think that's a really interesting thing to
wait a frame it. If you look out into the natural world, we have seen two at least open-ended
processes that innovate forever. And one of them is a natural evolution that's been going
about 3.5 billion years, continues to surprise us with things like COVID. And human culture is the
other one, right? We continuously, we solve problems and in the process we create new opportunities,
new problems. You create one technology and suddenly it has a cascading effect that will
open up new opportunities and doors. We solve those problems and it generates et cetera, et cetera.
And so the question that I think is fascinating is one of my colleagues put it is,
could we create the computer algorithm that was worth running for a billion years?
That continues to innovate and delight, surprise, and be creative for a billion years. Right now,
when I started my career, the best algorithms weren't worth running for more than a few hours.
I'd say we're now at a point where we have some algorithms that are worth running for about
a month to three, and not much beyond that. But could we create something that would truly innovate
forever? And if we can, then we've made a lot of progress, I think, towards some really
fascinating scientific questions. So you've referenced a couple of times, you've got
a reward function, AIX or scenarioX, you stick it into AIGA and AI Generating Algorithm.
Let's talk about the state of AI Generating Algorithms. Let's make that a little bit more
concrete. What AI Generating Algorithms are out there, how far along are we? How do you think
about the state of that line of work? Yeah, great question. So as I mentioned, there are three main
pillars to an AIGA. You have to learn the architectures, you have to metal learn the learning algorithms
themselves, you have to automatically generate the environments. So we could separately say,
what kind of major work has been done in each of those pillars? And then we could talk about what's
even more fun as like has anything been trying to put those pieces together? Yeah, right.
So first in the automatically learning the architectures has been an explosion of work in
neural architecture search. It's kind of a thriving field and it's doing really well. I don't think
I need to mention the work that's done there. Your readers are probably familiar with it,
but suffice to say that many of the best architectures now are being learned, not hand design.
And I expect that soon will be most. In the second pillar, there is a lot of work that's
been happening over the last couple of years in this area called, often it just goes by metal
learning, but it's basically learning to learn. I was kicked off by these two amazing papers,
RL squared, and learning to reinforcement learning by Jane Wang. You've got the work by Chelsea
Finn and Sergey Levin on mammal. And you have some of my favorite work is the work I'd
open the eye, which happened before I was there on Rubik's Cube, but they basically just take
a big neural net. They asked it to solve Rubik's Cube in a variety of different situations with
different like friction and weight of the cube and the size of the hand, et cetera. And what they
showed, which is just mind blowing, is that it inside of a giant recurrent neural network
figures out a learning algorithm such that when you want to use it at the end of the day,
you've turned off your own SGD. You're not doing a hand design in the algorithm anymore.
You're completely handing it off to the recurrent neural net, which is invented its own way
to conduct experiments in the world, figure out kind of what type of world I'm in,
take the information it has learned from the world, and use it to then get very efficient
in solving the task. And we don't know how it did that. And that's an example of where it figured
out how to like do the thing we wanted to do without us having to hand design it and probably
a very complicated way that we wouldn't have been able to hand engineer. And so that is fascinating
work. And then the final pillar is automatically generating environments. So one of the first
big works in this area was Poet. This worked that I did with Ken Stanley and Joel Layman
and many others at Uber. AI Labs and Ray Rang was the lead on this. And the here the idea is
typically a machine learning. We pick the problem and then we try to like solve it for a
long time. Then we move on to the next problem. So we work on chess for a while, then we go to
go and then we go to starcraft and then we go to dodo. The problem there is that no matter how much
time you run that algorithm on go or starcraft, all you're going to get is a go playing agent
for a starcrafted playing agent. That's it. It's not going to do anything else more interesting
than that. And so the idea behind code is we don't want that. We want the system to produce
the learning challenges forever. And so the way the Poet works is it basically starts out
with an environment. And then once the agent is pretty good at solving that environment,
it creates an entirely new environment that it thinks is close enough that the skills learned
in the first environment will help in the second environment. And it basically keeps going forever
and kind of adding more and more learning challenges to the agent. And as the agent masters them,
it basically has it switched to harder and harder stuff. But not in a linear chain. It's not
headed in any direction, trying to solve one particular final target challenge. It's more like
what you see on earth or in human culture. It's just fanning out and getting better and better
and better and more not more knowledgeable at a variety of different things.
Do you think of that as kind of deriving from some of the work that's been done around
curriculum learning where you're kind of staging out a set of learning objectives to
accelerate an agent's ability to zero in on the specific things and then build on those things?
I do. Yeah, of course, there has been, obviously, decades of work in curriculum learning.
I would say that in my opinion, the history of most curriculum learning work however is,
I have this thing that I want to solve. What is the right curriculum that will get me there?
Yeah, there's a pre-determined target. Whereas Poet is saying, I want to learn everything.
How do I learn forever? There's a wonderful body of work and principles and kind of almost
like a philosophical paradigm that dates back to Ken Stanley and Joel Lehman. I'm this idea that
oftentimes when we pick an objective ahead of time and we try to get there, if it's a simple
objective, we can do it. If it's a really hard objective, we'll probably fail. Usually,
the best way to solve a really, really hard challenge is not to try to get anywhere in particular,
but to try to just learn and go everywhere. Go in any direction, follow any stepping stones,
kind of have serendipity inside of your algorithm. Then, eventually, you will learn the skills and
the knowledge to solve this task that maybe you originally did want to solve, but you shouldn't
try to solve it. There are a lot of examples from the history of technology on this. For example,
if you went back to Melania and you said, I have cooking over a fire and you said, all right,
I'm the king of the universe and I'm the king of earth and I'm only going to found scientists
in my kingdom that will give me better and better cooking technology that will cook things
faster and with no smoke. Well, you will never invent the microwave because to invent the microwave,
you had to have been working on radar technology and notice that a chocolate bar melted in your pocket.
Similarly, if you wanted to invent, you know, the modern computer, go back to the abacus.
Really good device. It's pretty good at computing at the time and you're like, I will fund anybody
in my kingdom and only people who will give me more computing and all of your grant proposals
would be like longer rods, more beads, you know, maybe a 3D abacus, but you would never invent
the modern computer because to do that, you had to have been working on electricity and vacuum
tubes and those are technologies that were not invented because they help with computation.
And so, we're trying to capture that kind of serendipity inside of these AI-generating algorithms
and open-ended algorithms. This is building on a subfield that's now thriving called
quality-diversity algorithms of which poet is one. And the idea is that you basically want
as many high-quality, yet diverse things in a growing archive or library of skill sets or
innovations, whatever it is you're trying to do. And so, poet is like that. It basically says,
I want to create creating environments, totally different environments, and agents that know how to
solve those environments. Overall, the system is getting smarter, the skill level is going up,
the amount of things we know how to do is going up, and that will continuously unlock
new stepping stones and new things that we can do. And eventually, you bubble out,
and in theory, if you did that in the right way, you might get all the way to human-level intelligence.
Now, if you don't mind, I want to go back to finishing my thought on the question of putting
pillars together. I think probably the most exciting work I know of that is put two of the pillars
together, not three, is X-land by Max Yeterbury out of DeepMine. They have this wonderful paper
where they build on the ideas of open-endedness and AI-generating algorithms. And even Max said
on Twitter in exchange, I had that this is an AI-generating algorithm, or I think that's in the paper as
well. And I think it's one of the best first examples of somebody going really big toward this
idea. And what they do there is they basically describe a whole, they say, sample from this huge
space of possible tasks that agents have to play against each other. And so, they sample a task,
they train an agent to solve the task, as the agent levels up, they give it new tasks and some kind
of fanning out curriculum, and they basically try to make it as good as possible as in as many as
possible. And in the end, the system itself trains agents that go from initially not knowing how to
do anything to ultimately being able to zero-shot solve tasks like hide and seek and capture the flag,
and all other kind of games that you might think would be good for agents to just know how to
to solve if they were generally intelligent. So, I think that was great work in that direction.
And in that particular case, how are they demonstrating that generalizability? Are they doing
something analogous to pull it where they're creating new environments, or are they
more hand-picking new environments, and showing that this agent that they've created is generalizable?
Yeah, good question. So, what they did is, like Poet, they basically said, we're going to be
able to describe an environment in a parameter vector. So, normally, we're used to think
of a parameter vector describing the weights of a neural net. Now, the environment is being
described by some descriptor, and they have a search space. So, basically, the huge space of
possible parameter vectors is specify all these environments, and they can sample an environment
and train an agent on it. One thing that they did, which I really like, is they also show that
vector to the agent. So, the agent knows what task it's trying to solve, which is a really good
idea, because it can then learn to generalize and even zero-shot new tasks. So, for example,
if it has learned over time the language of how the worlds are being specified. So, in there,
what it's like, your job is to get the blue ball and take it to the green area,
and not let your enemy get the green ball and take it to the red area. Well, that basically
describes capture the flag. I have to go get the thing, and I have to take it to my area,
and you know, prevent you from doing likewise. And so, they train on enough of these tasks, where
maybe some of the simpler tasks are get the green ball, and then some of the harder tasks are get
the green ball and move it to, you know, move it anywhere. And eventually, it's like this more
complicated thing. And so, in the end, they can basically sample a new task from a hell.task set,
like hide and seek or capture the flag. Give it to the agent. It has learned how to read the
description of the world, and immediately go and solve that relatively complicated game that's
never seen before. What you're describing here, and to some extent, what you're describing with
Poet reminds me of the work that's been done around NetHack, which is kind of specifying,
you know, trying to, and it's been a while since I had that conversation, but I'm thinking about
like, you know, creating this one vector that kind of specifies an environment. Are you familiar
with that work? And if so, how do you think about them relative to, how do you think about NetHack
relative to X-Land and Poet? Yeah, so I did hear about the competition, and I remember that
it's like a symbolic hybrid B.D. learning, which caused quite a splash. That's about the extent of
what I know about it. I will, I'll be able to just say just a few things that one is that I think,
in general, having agents be told the task that trying to solve is very, very helpful. It
shaves off probably orders of magnitude in terms of exploration difficulty. The example I love to
give is like, my dog is very capable, athletic, right? And if I wanted my dog to go around my
apartment and like, pick up all the green things and put them in a bin, my dog could do it if they
knew that was the task that would give them the treat, but I can't just tell that to my dog,
because we don't have the ability to communicate that kind of high level task descriptor. So,
in our old agent, it's in the same boat. If you just put it in a room and say, go, it's never
going to figure out to pick up all the green things and put them in a bin. But if you could tell
the agent that and knew what you meant, and it had some basic athletic skills, that's probably
no longer that hard of a task. So, I like that thing. And also, I think, you know, just the fact
that NutHack is being done in text is a great accelerant, because, you know, as we've seen with
so many things in the last few years, operating in text is cheap and quite powerful because you can
bring, you know, things like GPT to bear on. In the case of X on this vector,
is it specifying the task in specifying the environment or? I think the right, the easiest way
to think about it is that they do create a world, and then they create the description of what you're
supposed to do in that world. And so, the agent is basically told, oh, in this world, you're supposed
to get the green things and take them over to this bin. And it learns over time that, like, basically,
what those words mean, and that learns the skills to do that. Now, one thing you might be
thinking, and I think a lot of people ask me about in, when we talk about the agias, is, you know,
this is what Josh said in one one asked, once asked me, it's just like, this sounds super great,
but how are you going to do this without a planet-sized computer? Because that's what Earth had.
Fair point, Josh. I think that what we have to look at is, where are we going to get abstractions
or innovations that shave many orders of magnitude off of what was required to make Earth work?
Darwinian evolution is very, very unintelligent, and I'm not advocating we even necessarily need
evolutionary algorithms, just to be clear. I'm inspired by evolution, but not advocating that
that technology into the hood. But I do think there's probably many places where we could look
to things that could, you know, save orders of magnitude. So, if you have the right abstraction,
if you have the right domain, where you don't have to simulate chemistry and physical,
like low-level physics, you can simulate things higher level, or even be in an environment,
maybe like net hack, a text only environment. Those things can really help. And another thing I
think that it can help is to, you know, steal a quote from Newton and riff on it, that I think AI
will go farther if it stands on the shoulders of giant human data sets. And so we've seen that
with GPT and all the unsupervised pre-training right now is that we can basically take a huge step
forward just by, you know, starting from where humans are at. Now that we know how to learn from
human data. And so my team at OpenAI recently had a paper that was accepted at NURPS this year
called Video Pre-Training, which is a nod to GPT. And the idea there is that we have a very
simple way that you can go on to the internet and have AI learn by watching YouTube,
and learn how to act by watching YouTube. So mostly in games like using your computer or,
you know, maybe like playing Minecraft or whatever, there's all sorts of video tutorials online
trying to do that. The trick was that we didn't know how to get the labels. So if you're trying to
generate text or generate pixels or images or generate music, all the labels are out there, right?
From the previous text, you just have to predict the next word. The next word is right there on
whatever web page you're scraping. The problem in videos of people acting, whether they're humans
or robots or video game characters, is you see the agent doing its thing, but you don't know what
buttons it's pressing, like how it's moving its mouse or what keyboard, what keys it's pressing
as keyboard. So the simple idea behind VPT is that we just train a simple model that basically learns
to predict, oh, in Minecraft, if all of a sudden the character like, you know, jumped, you must have
hit the jump button. Or if it placed a block, you hit the place block button. That's a pretty simple
task. We train a little model to look at, to like learn how to do that. Then we run it across,
you know, years and years of video of people playing Minecraft on YouTube. We then get label data
for all of these videos on YouTube of how to play the game. We then pre-trained a model just like
GPT to go from past to what's the next action. And now we can zero shot have the agent out of
pre-training do very complicated behaviors. And with that baseline, we can then fine tune the
agent to solve any task we want it to do in Minecraft or, you know, very hard tasks. And what we
show in the paper is that, you know, zero shot, the agent's able to do really complicated things
to take humans like two to five minutes. But once you give it a challenge, if you don't pre-trained,
you have no hope of solving the challenge. But with this pre-training, you can end up learning
things like getting diamond tools in the game that take humans more than 20 minutes. And I think
it's like 24,000 sequential actions to achieve. And you can learn it relatively easily with
reinforcement because you started off by standing on a shoulders of giant human datasets. So to
me, that is kind of like how the era of pre-training fits into AAGAs is that human datasets can be
this huge catalyst or this huge level up that allows us to skip all the bootstrapping. Maybe the
first couple billion years of evolution get right to the good stuff where we have an agent that
understands language, understands our world, maybe knows how to act in that world. And then now we're
challenging it to go off and like learn new types of science and math and learn new skills and
like solve cancer and do things that we aren't even able to do yet. And so I think that's kind of
one of these exciting accelerants that might mean we don't need a platform as computer to pull
us off. In what ways do you see the VBT work as being generally applicable, meaning
are is it a set of techniques for teaching an agent to get really good at playing Minecraft or
is it something that we can generally apply to video understanding or video, you know, inferring
you know actions from videos? Yeah, I think it's very general. I think that you could use this
technique to learn how to do whatever it is that you see people on YouTube doing. Now there's
some major caveats. First of all, it's much easier, you know, if you're talking about a computer
usage. One of the things that we did in the Minecraft work is very intentionally we said we're
not going to go for a handcrafted Minecraft action, you know, a space where you have like a macro
where I hit a button and it like does this big complicated thing like chop sound a tree or places
of lock or crafts a crafting table or whatever. We basically within the game of Minecraft most humans
play it by using a mouse and keyboard. And so we said our agent has to learn the same thing, it has
to learn how to use a mouse and use a keyboard. And it has to just remind craft figure out how to
not only play the game but also within the game there's all these little tiny graphic user interfaces
where you have to drag and drop little icons and click on buttons and browse through like recipe
books and all sorts of stuff. It looks a lot like just learning how to use a computer like changing
your preferences and like dealing with menus and Microsoft Word and things like that. So I think
this technique is very general and basically could for example have an agent learn how to generally
use a computer if we got enough tutorial videos of how to use a computer. There's some unsolved
issues like different keyboard shortcuts and different programs do different things. So the
challenge is a little harder for that thing that has to predict what action must have been taken
given to video I've seen. But I think that's like a research problem that is solvable. You could even go more
crazy to something like robotics and say I can maybe learn by watching robotics videos but now you
have to solve a couple of extra problems like how do I infer what action must have been taken for whatever
rub but I end up wanting to use it with other actions in that rub that are a little bit different.
So there are some things to solve but I think it's um directionally right that you know this is
probably an easy path to extract a lot of the knowledge of learning how to act by watching
video demonstrations. And at a minimum anything that's in the computer states mouse and keyboard
that covers that covers so much. Think of all the things you can do on a computer with you know
just a mouse and keyboard and this system you know in theory should be able to learn how to do all of
that as well. And so how do you say a tying into the idea of AI generating algorithms?
Yeah well I you know so imagine for example that we like right when we did poet or when they did
X land the agent basically starts out knowing almost nothing and it has to learn how to pick up
the green ball and go you know get you to take it to the to the blue area or in poet it has to
learn how to walk or if it was Minecraft it has to learn from you know if we didn't pre-trained
to learn from scratch how to do all this stuff that's very computational inefficient.
So if you had this pre-trained model that you can use to seed the agents you know give the
agent some kind of common sense understanding we might refer to it as about its environment and
how actions correspond to outcomes then it could stand on the shoulders of giants as you put it
paraphrasing Newton. Exactly so imagine if it comes with GPTA skills so it knows all of human
language it knows how to talk to you it knows how to get instructions on what it should do it
also knows how to move around the world it knows how to do things on a computer or walk around
a 3D video game it knows how to play Starcraft, Dota, Doom, Chess go it knows all that stuff and
then you kick off this process. Now you're basically you're so far down the road that you probably
don't need that many more pieces to create a system that's truly kind of can auto-catalycally
learn tremendously interesting things and so you're skipping over that really inefficient thing
where it's learning to see and it's learning to understand language and it's learning how to
move its body so just like GPT went from you know think about the world like the pre-NLP world
before GPT you know we had hacked together all this manual stuff and it kind of worked but not
really and all of a sudden the GPT paradigm just like wow AI is exciting and it can do amazing
things that it's metal-learning and it can almost be human level but now apply that to learning to
like act in three dimensional worlds on computers you know I think that that is such a huge
place you know such a huge leapfrog forward as a place to start from when you're trying to do
something that create an open-ended system in an AI generator. Going back to Poet, a key element
of what you're trying to do there is you're creating new environments that serve the purpose of
teaching the agent new things and I want to try to to kind of dive into how direct it is
is that the you know the n plus 1th environment is that you know designed again some you know
reward function that is optimizing for learning or tries to predict the amount the agent will
learn or is it more of our random we're going to create a bunch of environments and hope the
agent learns something like tell me more about the algorithmic aspect of that. Yeah great question
so in Poet we were in trying to produce an initial proof of concept with a lot of these ideas
a lot of the individual pieces are pretty simple so in that work it was a simple evolutionary
based system you take a description of an environment you mutate it which means you just change
some of the numbers in the parameter vector that describes the environment that produces a
slightly different environment and then you basically keep it if it's not too hard and not too
easy for the agent which is basically a proxy for will it learn on it or not. Now you definitely
could get more intelligent and I think this is a fantastic direction to go where you train a
neural network model whose job it is to look at the history of what this agent has learned so far
and intelligently give it the next learning challenge that it thinks is right at the cusp of
I think you could like you're ready for this you know and this is where you're going to have
maximal learning progress but also that the task itself is worth learning is interesting.
Well even even in the case of Poet are you are you predicting whether it's too hard or too
easy or are you letting the agent try and kind of measuring progress and you know doing things
like early stopping or does the agent actually have to do it and that's when you know if it's too
hard or too easy. Yeah so in that work we took a pretty easy approach which is we created the
environment we tested the current agents that we had so far and we had hand to find for this
particular type of environment if you're basically have a score that's not too low and not too high
we'll say that's like that's in the middle and then after a while if you haven't been learning on it
I think we can get out but that's you know that's kind of in in retrospect since then there have
been people that have been working on versions that don't have these kind of hand defined limits
they're automatically basically trying to estimate learning progress and saying if I'm seeing
learning progress then keep going. For example so work we had out of OpenAI before VPT
it came out of Ingmar was the Lolita author came out of our team at OpenAI basically was
doing a different technique that was trying to measure learning progress on a task so this was
also in Minecraft we give you a challenge and we say you know initially we'll give you a challenge
and we'll search your records statistics on it and if it looks like after a few trials
that you are in fact in the sweet spot of learning then we'll keep it and if not we'll basically
stop sampling that as much so you're trying to basically it's almost like a bandit problem you're
pulling all these arms and if you start to get learning you keep pulling that until you run
you stop getting learning and then you go on to find something else and there's lots of different
ways that you could approach this probably this problem but I want to point out what I think is
probably the most interesting challenge which is I mean learning progress is hard to get right no
question but then the other question is is it interesting is the challenge worthwhile because
almost all of the learning progress based systems have pathologies that when you start to optimize
for learning progress you start to get environmental challenges the agent shows huge learning progress
on that are totally worthless imagine if I told you that your job was to I don't know like you
let's say you're you're running an obstacle course your job is to get I invented a whole obstacle course
and it had like you know balance beam and like a wall at a rope you know ladder you had to climb and
you did really well okay so you're pretty good on that task and then I just tack on at the end
like wiggle your arms in exactly this particular way but it's just a little bit of that at the end
well okay so you first did the obstacle course maybe we considered that interesting and at the end
you can get some learning if you happen to luckily figure out how to do wiggle your arm in
exactly the right way okay and now you're no longer after you learned that you no longer
learning anything because you figured out the right wiggling I just tack on a little bit more
different random wiggling this is kind of the equivalent of of memorizing white noise but in
motor space and I could probably endlessly add just a little bit more stuff for you to memorize
that is not going to generalize to our world or to even other environments it's just totally
uninteresting now that's super that's super interesting how do you characterize that interestingness
so that you can optimize for it yes welcome like decades of thinking in like open-endedness trying
to figure out the answer to this problem the short answer is we don't know yet but I have some
ideas we're currently working on in my lab now so stay tuned but I'll give you a flavor of some ideas
that are not exactly our current best ideas but you know in the spirit of them imagine for example
if you had to learn some skills in one environment that give you learning progress and they also
help you generalize to other environments that's already a little bit better because you're not
going to memorize random motor twitches another idea you could do is you could grind into the real world
so you have say a held up set of tasks from real games or the need to make money in the world or
to imitate you know animals on YouTube or something and you have to like practice if they the
the environment has obstacle courses maybe you have to learn to break your client if you never see
a human wiggling their arms in in the wild then it's probably not useful is that kind of the idea
or if learning that skill practicing on that wiggling task doesn't make you better at like being
American ninja or imitating you know American like humans in the American ninja obstacle course game
or imitating orangutan breaky eating or something like that if you don't get better at doing
something we care about by training on these tasks then maybe we consider those tasks to be
uninteresting so the system that's generating those tasks then has an extra challenge it's not
just trying to get learning progress because that's not sufficient it's necessary but not sufficient
it also has to generate tasks that are learning useful skills that transfer and help us solve
all the problems that we do care about and that all of a sudden starts to feel like okay maybe I
can see how that wouldn't have massive pathologies it might actually get us to worry why yeah it it
uh to your point it strikes me that this is a super interesting problem because
there's a big risk in like a big part of what we're trying to do is use algorithms and the
computer you know compute and data to allow the system to you know create and learn from you know
discontinuous patterns right that you know that we're not going to produce um but part of what
you're trying to do with this interesting the interestingness you know regularization let's call
it is like um it is to kind of dampen just kind of random things so it's like you you you want to
capture and you know amplify randomness that's useful but uh but kind of suppress you know
randomness it's not useful it sounds really hard without some kind of signal or ground truth or
something that's going to tell us and part of the point is like you don't if you think about it
from an evolutionary perspective you don't know what kind of step evolutions are going to
ultimately produce the success right yeah I it's super hard I think that's why it's fascinating
I consider this to be one of many grand challenges it's kind of laying over here in this general
area of research and AI generating algorithms um but it also doesn't feel hopeless um I feel like
maybe take your net hack domain or a text domain if you could if the system could generate the right
kind of problems basically like a teacher generating like lesson plans like work on this math problem
work on write this essay etc and periodically you were testing how they're doing out there in
the world at like helping us with our text-based problems or is it a better thing for an API of
people who want to use a language model um maybe there's some maybe maybe there's enough learning
signal there that could figure out the right sort of challenges to generate especially that again
if it's pre-training on what human teachers provide to students right and that's where you get
that big catalyst where you're not just trying to like do it from some from like from just randomly
generating problems and hoping that one of them happens to help you but you can take advantage
what we know about teaching maybe just get better at it yeah maybe switching gears a little bit um
but continuing in the broader theme of AGI how do you think about the the safety challenges of AGI
and the context of AI generating algorithms great question so I am in general very
concerned with the safety challenges regarding AGI so putting aside the specific concerns that
might come up with AI generating algorithms I think we as a community are playing with fire and
we're doing we're creating a very powerful technology and we have to be very careful that we do
it in the right way um and I'm not even honestly sure that you know if you gave me a button and
said could you pause a AGI development so we can figure out the right way to make it benefit
all humanity would you hit that button I probably would but I think that humanity doesn't have a
good track record I would because I'm so concerned about like the potential negative impacts that we
need time to get this right however I think that a humanity basically doesn't have a very good
track record of not inventing things when they can and like you know it's going to be the case
as somebody makes progress so condition on that I think that we need to make sure that we do the
best job that's possible maximize the potential upside and minimize the downside that's just my
comments on AGI in general now there are specific ethical concerns that come up with AI GA's that I
think are interesting one of them we get to that if I can okay I'm sure okay is in what ways is
is your working on AI not the exist you know proof that you wouldn't actually hit that button
well it's because of that very important caveat that I don't think that humanity will stop working
on it and so therefore I think that we mean it doesn't really matter if you individually hit the
button unless everybody hit the button hits the button so it's like a game theory kind of problem
in a sense yeah yeah yeah I don't know that I like the words it doesn't matter because I think
another way to say it is it's inevitable and so we might as well I might as well do my best to
try to make sure that it goes as positively as possible that's kind of how I think about
that's why if you gave me this button that I could stop everybody from working on it so we could
take 20 years and discuss the right way to do it I'd probably hit that button so if you're
I could tell you about what I think are some AI GA specific ethical concerns but also this is
an important topic so I'm happy just just to talk about AI safety as well and now we can you know
we'll we'll continue to talk about it but we can jump into the AIGA part okay so you might say
all right well is something there's something more risky about AIGAs than the manual path to AI
for example I think there are some things that might be more risky one of them is that it's almost
by its nature trying to create kind of a auto catalytic process that's getting bootstrapped thing
itself up and so and any given instance of the algorithm you might finally stumble upon the right
ingredients and boom you create this kind of lightweight process so you're kind of looking in
some sets for something that is like a fast takeoff or at least a medium takeoff in a way that
maybe the manual path won't have I think increasingly that's less less obvious that the manual path
also might not have that you know maybe GPT-6 just suddenly has the power to do amazing you know
like this AGI and can control the world's digital systems so who knows but at least in AIGA
as you're looking for that I think there's another one going back to this issue of qualia
and philosophy and that is you know what if I what if we could create a system that had like a
whole bunch of agents learning a whole bunch of different tasks interacting with each other
and we're basically simulating an entire human civilization or at least maybe sorry it's
simulating an earth up to the dawn of female civilization and then AGI is like once you get
humans in this simulation well how much untold suffering what you have caused in all of these
digital beings you know like like who is responsible for all the pain and suffering of dinosaurs
and hyenas and lions and their ancestors and is that a price we're willing to pay I think it's
interesting if we knew that these creatures were actually suffering is it worth it to get to AGI
that's something we should consider and a final thing that I think is really interestingly unique
to AGI is more so than many other attempts is that if and this is a big if but if you tried to create
your outer loop meta reward function that we talked about to be similar to the one on earth which
might be just like kill and don't be killed survival then maybe you recreate the red and tooth and
claw situation that happened on earth and so the values and the instincts of the entities that
come out of it are also potentially like us maybe they're selfish and they're violent and they
are deceptive etc you I think it'd be really awesome and interesting if we could create
versions of AGI's where the outer loop thing is very cooperative and you don't end up with those
kind of vices and you get more virtues and I also don't know to what extent the manual path
is going to create vices instead of virtues but thinking about kind of the values of the the
AGI that we create is absolutely essential like we need its values to be aligned with ours and
to the extent that it doesn't have our vices that dramatically increases the chances that things
go well once we make AGI. Yeah I think there's maybe an argument that of all of the ways the folks
are trying to get to AGI the AIGA is probably the most dangerous and that you are specifically
trying to enable the machine to you know create its own environments and as we discussed
earlier you know that may lead to creating its own reward functions right because that's going
to be the big challenge and that seems specifically like a point where you lose control.
Totally agree and I make that exact point in the paper on AGI generating algorithms
however if that is a con an important con I think we should need to do research to try to
like minimize the possible downsides of that con but let me also express what the pro is that
comes along with that and that is that if we believe that the manual path is kind of designing
AI it's probably we're probably going to be designed in our image and it's similar to us and
it's probably at least in the the the current path is going to be consuming human data so it's
going to look a lot like us. AIGA is because they could go off in so many different directions
is all gives us the amazing possibility of effectively doing alien cultural travel we get to see
all these totally different types of intelligences and societies and cultures that might pop out
of the system because it's so like possibly going in all these different directions that don't
have a lot to do with the biases that were making either the system and so yes some of them might
be unsavory and we need to make sure we have like the safety in place to like not like that be a
problem or try to minimize that for being a problem but at the same time like coming to understand
intelligence in general the space of possible culture and the space of intelligence like what is
the music and the math of an alien culture look like well here is a system that might actually show
it to us with greater probability than anything we're doing in the manual path AI and you know I
don't think we're going to invent interstellar travel for quite some time so this is kind of our
best shot to you to explore the space of possible intelligences in the universe and that
sounds like the science it's the stuff of science fiction but it's probably not as far off as
many people would imagine yeah it it does seem that even even kind of optimizing or or kind of
pursuing this AIGA oriented path it's still in the context of you know all of the data that we have
you know which has its buy I guess I'm I'm pushing back in that I'm not sure the AIGA to the
extent that it's built on you know access to all of our data and and that's going to be an
accelerant that it necessarily divorces us from the biases that the manual path will have totally agree
so the first AIGA is probably consume human data and look a lot like us and in that sense I
totally agree you don't get this this cultural travel you know seeing this massive diversity of
possible cultures but I think once we figured out how to make that system we probably then can go
back and try to create the system without consuming human data without that big step forward
um standing on the shoulders of human data sets giant human data sets and also maybe not as
much stuff built in our image and that's when you can start to kind of explore the space of
possibilities much more and so there's kind of a spectrum even within AIGA between like to what
extent they're going to look at us and it's not just consuming human data if we use the tricks that
I described for grounding what counts as interesting as solving problems in our world especially on
earth you know making money in our economy or whatever then that also is grounding it too
basically it would look like a lot like something that is quite human but maybe we figure out a way
to figure out tasks that are intrinsically interesting that are just internal to that world
like I said you have to learn tasks that we're here that generalize to other tasks within that
system within that kind of bubble maybe that still gives you intelligence but now it's totally
unhinged from earth so we've we've covered a lot of ground I think if we it's easy for me
you know relatively easy for me to wrap my head around kind of the you know the near term at
least the the kind of work that you're doing now and generating environments and training agents
on those environments and the the kind of the impulse that that you're using and creating
these environments to maximize learning is something that's going to you know accelerate
the evolution of intelligence let's say and then the long term is you know if you know if that
continues and we throw lots more compute at it lots more data at it you can see how there's an
argument that that's going to going to create an intelligence that is far beyond what the manual
path might do or there's a lot there in that statement that I don't like but at least that you
know there's some there's the opportunity to kind of to serendipitously stumble onto some
discontinuous innovation that gets us there that the manual thing might not at least what's in
that middle what do you see in that kind of middle term you know beyond the stuff that we're doing
now you know not quiet at the you know then the miracle occurs like what what what what what's the I
don't know three to five year kind of path for AIGA yeah good question so I think one thing that
we haven't seen people do a lot of some people have but not a lot of is putting all the pillows
together so for example we haven't seen a lot of architecture search kind of the learning
in conjunction with the learning to learning algorithm stuff in conjunction with environment
generation so I think we're going to see I think we're going to see more of that I also think
you're probably going to see more stuff like poet and like excellent you have these agents
who are just increasingly training on increasingly diverse challenges and becoming very general
intelligent and so what you're starting going to start to see I think is more and more sample
efficient reinforcement learning for two reasons one is that the algorithms the architectures
etc are better optimized to learn quickly but also that you're not always starting from scratch
you know people complain about the sample efficiency on like Dota or Atari or chess or whatever
compared to a human but like a human doesn't learn to play chess waking up in the middle of a
chess plane universe they start learning planning learning to play chess when they're six or seven
they've had years of data coming in so I think increasingly what you're going to see are
internal to these systems agents that basically already know a lot as they approach the next problem
and maybe you via pre-training maybe even the system itself being pre-trained and maybe you
can also models that are come from somewhere else that are dropped into these systems that are
trained and also models that are exported from these systems to become pre-training that are used
for other things so increasingly I think the era of starting from random weight initializations
will go away we're starting to see that now almost all many many many AI papers now are starting
with pre-trained components right and so this is kind of a quiet revolution that is basically AI
is no longer being as sample inefficient because we are starting not from scratch starting from
from a good foundation so I think that's probably what the next you know three to five years look
like increasingly capable sample efficient learning agents that can generalize and another
one that I would mention I think text in a conditioning on the task knowing what I'm supposed to do
is another way that they get much more sample efficient so I just think you'll see more and more
things like excellent and Rubik's Cube where these systems can do quite impressive things in
generalizing big distributions of environments instead of narrow agents trained from scratch for one
environment awesome well Jeff it was wonderful wonderful speaking with you and getting to learn a
little bit about your research and looking forward to keeping in touch likewise thank you very
much for having me I really enjoyed the conversation and it's a great fantastic podcast so it's
an honor to be on thanks so much Jeff
