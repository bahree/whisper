1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:34,400
I'm your host, Sam Charrington, hey what's up everyone, I'm on the road so I hope

4
00:00:34,400 --> 00:00:39,640
you'll forgive that this intro may not be up to our usual audio standards.

5
00:00:39,640 --> 00:00:44,640
Last week we dropped a huge Twimble Con content update announcing that we double the number

6
00:00:44,640 --> 00:00:48,960
of sessions and sharing a bunch of the amazing speakers and sessions that we've got in store

7
00:00:48,960 --> 00:00:50,640
for you.

8
00:00:50,640 --> 00:00:56,720
One content, another important focus for us in creating this event is community.

9
00:00:56,720 --> 00:01:01,320
We see this conference as an opportunity to help create and support a community of professionals

10
00:01:01,320 --> 00:01:06,600
dedicated to helping their organizations be more successful with machine learning and AI

11
00:01:06,600 --> 00:01:09,320
by sharing and learning from one another.

12
00:01:09,320 --> 00:01:13,360
As a part of this commitment to community we want to make sure that you're aware of our

13
00:01:13,360 --> 00:01:17,080
volunteer program and diversity scholarships.

14
00:01:17,080 --> 00:01:22,200
To learn more about both of these great initiatives visit twimblecon.com slash volunteer and if

15
00:01:22,200 --> 00:01:25,920
you're interested fill out the appropriate application.

16
00:01:25,920 --> 00:01:30,840
Submissions will be accepted through August 30th so get them in as soon as possible.

17
00:01:30,840 --> 00:01:33,120
And now on to the show.

18
00:01:33,120 --> 00:01:41,000
Alright everyone, I am on the line with Cynthia Rudin, Cynthia is a professor of computer

19
00:01:41,000 --> 00:01:47,040
science, electrical and computer engineering and statistical science at Duke University.

20
00:01:47,040 --> 00:01:49,840
Cynthia, welcome to this week in machine learning and AI.

21
00:01:49,840 --> 00:01:51,520
Thanks for having me.

22
00:01:51,520 --> 00:01:56,600
Let's jump right in and get started with a little bit of your background and how you came

23
00:01:56,600 --> 00:01:58,840
to work in machine learning.

24
00:01:58,840 --> 00:02:04,080
Yeah, I think I ended up working in machine learning because I like the name support vector

25
00:02:04,080 --> 00:02:05,080
machines.

26
00:02:05,080 --> 00:02:07,480
No, I'm just kidding.

27
00:02:07,480 --> 00:02:11,680
Well, I'm actually sort of only partially kidding.

28
00:02:11,680 --> 00:02:18,360
I happen to be walking around in Princeton at any sea labs and at the time there are

29
00:02:18,360 --> 00:02:23,800
a lot of researchers there who are really, really good at support vector machines and I walked

30
00:02:23,800 --> 00:02:29,040
into Gary Flakes office and he gave me Vapnik's book and I read it and it was, it was very

31
00:02:29,040 --> 00:02:34,640
difficult to read for me at that time, but after that I was sold, you know, it was, it

32
00:02:34,640 --> 00:02:38,120
was going to be machine learning all the way so that's it.

33
00:02:38,120 --> 00:02:47,040
Yeah, nice and were you, this was during grad school or, okay, awesome, I had, I ended up

34
00:02:47,040 --> 00:02:52,480
having two PhD advisors, so I'm Anchored Dubsheet was one of them, the first one and she

35
00:02:52,480 --> 00:02:57,960
said to me, you know, I haven't, you know, I don't work in machine learning is it okay,

36
00:02:57,960 --> 00:03:02,160
you know, after I asked her to be my advisor, you know, I said, Anchored, will you be my

37
00:03:02,160 --> 00:03:08,440
advisor after Gary left and she said, is it okay that there's no, that I, I don't work

38
00:03:08,440 --> 00:03:11,280
in machine learning and I've never advised a student in machine learning and I said,

39
00:03:11,280 --> 00:03:13,920
yeah, don't worry about it, everything will be fine.

40
00:03:13,920 --> 00:03:14,920
Nice, nice.

41
00:03:14,920 --> 00:03:15,920
Yeah.

42
00:03:15,920 --> 00:03:22,080
I also noticed that you're a three-time winner of the Informed Innovative Applications

43
00:03:22,080 --> 00:03:24,600
in Analytics Award.

44
00:03:24,600 --> 00:03:29,840
I, Informed was one of the conferences that I went to in grad school, but I've always

45
00:03:29,840 --> 00:03:40,520
associated with like industrial engineering more so than computer science or machine learning.

46
00:03:40,520 --> 00:03:45,240
You're a chair of some element of that conference as well, is that right?

47
00:03:45,240 --> 00:03:53,360
Well, okay, so Informed is, is the, the organization that, like the main organization in the

48
00:03:53,360 --> 00:04:00,280
United States that is, represents operations research and management science and the

49
00:04:00,280 --> 00:04:07,880
work that I do is very closely related to aspects of decision making and management.

50
00:04:07,880 --> 00:04:12,080
And so that's why I became involved with this organization.

51
00:04:12,080 --> 00:04:18,080
Informed does have a very active data mining section and, and I was a past, I'm a past

52
00:04:18,080 --> 00:04:23,840
chair of that data mining section and I'm also an editor for one of the journals, one

53
00:04:23,840 --> 00:04:29,600
of the top journals in that, sponsored by the organization called Management Science.

54
00:04:29,600 --> 00:04:34,000
Tell us a little bit about your, your research into, into that area.

55
00:04:34,000 --> 00:04:38,040
It sounds like it's fairly interdisciplinary.

56
00:04:38,040 --> 00:04:45,400
Yeah, I started off as a theorist working on kind of theoretical machine learning problems,

57
00:04:45,400 --> 00:04:48,040
convergence of adabuse specifically.

58
00:04:48,040 --> 00:04:54,680
But then after I, you know, graduated and after I finished my postdoc, I went to work on

59
00:04:54,680 --> 00:04:59,360
a project, an interdisciplinary project with the power company in New York City.

60
00:04:59,360 --> 00:05:05,120
The power company is called Con Edison and they, their job is to maintain the, the oldest

61
00:05:05,120 --> 00:05:10,600
and largest underground power network in the world, which is the New York City power

62
00:05:10,600 --> 00:05:11,600
grid.

63
00:05:11,600 --> 00:05:15,920
And at the time we were doing something that was, you know, totally crazy, which is, can

64
00:05:15,920 --> 00:05:18,320
you maintain the power grid with machine learning?

65
00:05:18,320 --> 00:05:23,600
Like can you use machine learning to help prioritize inspections and repairs on the grid?

66
00:05:23,600 --> 00:05:25,160
One was this?

67
00:05:25,160 --> 00:05:33,440
This was between 2007 and I think I worked on it all the way up through 2012 and beyond.

68
00:05:33,440 --> 00:05:34,440
Okay.

69
00:05:34,440 --> 00:05:35,440
Yeah.

70
00:05:35,440 --> 00:05:42,680
And, you know, no one, no one had done this before and we were using data that was from

71
00:05:42,680 --> 00:05:43,680
the 1890s.

72
00:05:43,680 --> 00:05:47,680
I mean, all these, all these huge, right?

73
00:05:47,680 --> 00:05:53,840
This was a huge amount of very complicated data that involved free text documents written

74
00:05:53,840 --> 00:05:57,240
by people who were trying to maintain the power network.

75
00:05:57,240 --> 00:06:02,440
There were engineers going into the, the manholes and doing repairs and they were giving

76
00:06:02,440 --> 00:06:06,360
that information to the dispatchers who were typing it in.

77
00:06:06,360 --> 00:06:11,520
And we also had accounting records dating back from all the way since the power grid essentially

78
00:06:11,520 --> 00:06:15,760
started, you know, since the days of, of Edison, Thomas Edison.

79
00:06:15,760 --> 00:06:20,480
So it was a really, really interesting, very, very challenging data set.

80
00:06:20,480 --> 00:06:22,480
A lot of the data we didn't know what it was.

81
00:06:22,480 --> 00:06:26,760
It was just sort of a pile of text and a pile of, just like garbage to us.

82
00:06:26,760 --> 00:06:31,040
We had no idea what it was, but it, you know, and data science as a field hadn't really

83
00:06:31,040 --> 00:06:34,520
been invented yet when we were working on this project.

84
00:06:34,520 --> 00:06:40,400
So the job of my team was to try to put all of these data together and figure out which

85
00:06:40,400 --> 00:06:44,240
manholes were the, were the ones that were the most likely to explode.

86
00:06:44,240 --> 00:06:51,200
So that, and do the repair work before, you know, that bad event actually happened.

87
00:06:51,200 --> 00:06:52,200
Wow.

88
00:06:52,200 --> 00:06:58,040
So this was a kind of big change from doing the theoretical work that I had done in the

89
00:06:58,040 --> 00:06:59,040
past.

90
00:06:59,040 --> 00:07:05,600
And while I, while I was working on this power grid reliability project, we made some errors

91
00:07:05,600 --> 00:07:10,760
that, you know, people make when you're working, when you're doing data science, we made

92
00:07:10,760 --> 00:07:15,200
some of the classic errors that, that you'd make.

93
00:07:15,200 --> 00:07:19,000
If you, if you didn't, you know, if you didn't have that experience, and I learned from

94
00:07:19,000 --> 00:07:26,560
that very quickly that black box models were not the way to go, because, you know, because

95
00:07:26,560 --> 00:07:31,280
the mistakes we made were things like not really understanding what the important variables

96
00:07:31,280 --> 00:07:34,440
were that, that we were using to make predictions.

97
00:07:34,440 --> 00:07:40,120
And if you don't really understand that stuff, you can't really make very good decisions.

98
00:07:40,120 --> 00:07:46,520
So I switched to kind of working on projects that were more along the lines of interpretable

99
00:07:46,520 --> 00:07:47,520
machine learning.

100
00:07:47,520 --> 00:07:52,200
So still keep everything machine learning, keep it data driven, but be able to understand

101
00:07:52,200 --> 00:07:57,760
exactly what the important variables were, how the variables were being combined, and

102
00:07:57,760 --> 00:08:00,320
then be able to make a decision that was informed.

103
00:08:00,320 --> 00:08:04,440
And not just a blind, oh, the model said this kind of decision.

104
00:08:04,440 --> 00:08:11,320
When I talk to folks about interpretability, I'll often ask them if they make a distinction

105
00:08:11,320 --> 00:08:15,080
between interpretability and explainability.

106
00:08:15,080 --> 00:08:20,760
And that's something that you, is, is very significant distinction to you.

107
00:08:20,760 --> 00:08:25,440
In fact, one of your papers is stop explaining black box machine learning models for high

108
00:08:25,440 --> 00:08:30,200
stakes decision and use interpretable models instead.

109
00:08:30,200 --> 00:08:38,040
Can you talk a little bit about that distinction and, and kind of lead us into a discussion

110
00:08:38,040 --> 00:08:40,880
of the main problem you're taking on with that paper?

111
00:08:40,880 --> 00:08:43,160
Well, let me give you a little bit of history.

112
00:08:43,160 --> 00:08:47,800
The field of data science sort of is evolving so quickly that things go off in different

113
00:08:47,800 --> 00:08:52,240
directions without really carefully thinking about them.

114
00:08:52,240 --> 00:08:54,800
And explainability was one of those things.

115
00:08:54,800 --> 00:08:57,080
So, yeah.

116
00:08:57,080 --> 00:09:03,080
So interpretability is a really old concept that people have been working on since the,

117
00:09:03,080 --> 00:09:10,120
you know, the 80s and 90s, you know, Leo Breiman and Cart and Quinlan, you know, these people

118
00:09:10,120 --> 00:09:14,560
were working on this in the, in literally the early 90s because they realized how important

119
00:09:14,560 --> 00:09:18,480
it was to have models that were interpretable.

120
00:09:18,480 --> 00:09:24,320
And that field has existed since that time, but then recently in the last few years,

121
00:09:24,320 --> 00:09:28,640
people have been working kind of only with black box models.

122
00:09:28,640 --> 00:09:35,160
And I think this is partly an illusion due to the fact that neural networks were performing

123
00:09:35,160 --> 00:09:39,520
really well for computer vision, starting in around 2012.

124
00:09:39,520 --> 00:09:43,440
And so people decided that, you know, we should only be teaching neural networks and that

125
00:09:43,440 --> 00:09:47,160
was the only type of machine learning that should be going forward.

126
00:09:47,160 --> 00:09:51,560
And so there was this whole group of people that could really, that really only knew how

127
00:09:51,560 --> 00:09:56,120
to do neural networks and black box models.

128
00:09:56,120 --> 00:10:03,560
There's also kind of the industry is propelled by these complex models because if these very

129
00:10:03,560 --> 00:10:07,120
simple models, then they can't license them and they can't make a profit off of them.

130
00:10:07,120 --> 00:10:11,480
So they really like to keep their models complicated.

131
00:10:11,480 --> 00:10:16,400
So because of that, people then tried to explain the black box models because they either

132
00:10:16,400 --> 00:10:20,560
couldn't produce interpretable models or they wanted to use black box models, but produced

133
00:10:20,560 --> 00:10:23,720
some sort of explanation for what they were doing.

134
00:10:23,720 --> 00:10:31,480
And you know, that, that evolved sort of very quickly kind of around 2015, 2016.

135
00:10:31,480 --> 00:10:35,360
And then sort of people forgot that, wait, you know, we might be able to produce interpretable

136
00:10:35,360 --> 00:10:38,800
models that are just as accurate as these black box models that, that wasn't sort of

137
00:10:38,800 --> 00:10:44,120
a thought that, that, that people were thinking in the last few years.

138
00:10:44,120 --> 00:10:49,040
And I wrote this paper to kind of remind people that, hey, you know, you don't always need

139
00:10:49,040 --> 00:10:50,040
a black box.

140
00:10:50,040 --> 00:10:54,360
You, and in fact, if you try to use black box models for high stakes decisions, really bad

141
00:10:54,360 --> 00:10:55,680
things can happen.

142
00:10:55,680 --> 00:10:59,480
And I gave a bunch of examples in that paper where bad things have happened because people

143
00:10:59,480 --> 00:11:04,200
tried to use overly complicated models when they didn't need them.

144
00:11:04,200 --> 00:11:07,120
I can go through some of the examples if you think it would help.

145
00:11:07,120 --> 00:11:09,000
But please, okay.

146
00:11:09,000 --> 00:11:13,760
So well, one of the examples is in criminal recidivism prediction.

147
00:11:13,760 --> 00:11:19,080
So there's a model that's used sort of throughout the US justice system, which is a very, it's

148
00:11:19,080 --> 00:11:23,160
a very unusual model for the justice system in that it's a very complicated model.

149
00:11:23,160 --> 00:11:25,800
It involves 137 factors.

150
00:11:25,800 --> 00:11:28,840
And nobody knows exactly how those factors are combined.

151
00:11:28,840 --> 00:11:30,320
Is this the compass model?

152
00:11:30,320 --> 00:11:31,800
Yes, this is compass.

153
00:11:31,800 --> 00:11:32,800
Okay.

154
00:11:32,800 --> 00:11:37,720
It's used by a private company that licenses access to software.

155
00:11:37,720 --> 00:11:42,960
And compass is used in, it's used regularly in parole decisions.

156
00:11:42,960 --> 00:11:48,720
And there was at least one parole decision that was very famous where it led to like an

157
00:11:48,720 --> 00:11:52,120
article in the New York Times, where the person was denied parole.

158
00:11:52,120 --> 00:11:57,760
And they figured out afterward that it was because of a typographical error on their

159
00:11:57,760 --> 00:11:59,360
compass score sheet.

160
00:11:59,360 --> 00:12:03,880
So in other words, the factors that went into that black box model, one of them had a typographical

161
00:12:03,880 --> 00:12:04,960
error in it.

162
00:12:04,960 --> 00:12:07,760
And nobody spotted it until after the parole decision was made.

163
00:12:07,760 --> 00:12:10,400
And by that point, you couldn't reverse the parole decision.

164
00:12:10,400 --> 00:12:14,720
So here you have a typographical error making a high stakes decision about someone's, you

165
00:12:14,720 --> 00:12:16,720
know, future.

166
00:12:16,720 --> 00:12:22,360
And I, you know, I don't, I think that's a kind of procedural unfairness that really kind

167
00:12:22,360 --> 00:12:23,960
of shouldn't exist, right?

168
00:12:23,960 --> 00:12:28,800
We shouldn't be making high stakes decisions that deeply affect people's lives based on

169
00:12:28,800 --> 00:12:33,120
typographical errors and those types of kind of clerical issues.

170
00:12:33,120 --> 00:12:34,120
Right.

171
00:12:34,120 --> 00:12:35,120
Right.

172
00:12:35,120 --> 00:12:40,120
And the underlying implication is that if this wasn't a black box model and we understood

173
00:12:40,120 --> 00:12:45,680
the basis upon which it was making this decision, we would have easily seen that in the process.

174
00:12:45,680 --> 00:12:49,280
Or the people that were responsible for this process.

175
00:12:49,280 --> 00:12:51,120
Yeah, that's the implication.

176
00:12:51,120 --> 00:12:54,920
I mean, there have also been typographical errors in very simple models.

177
00:12:54,920 --> 00:13:00,320
But if you think about, you know, the number of typographical errors in the simple models

178
00:13:00,320 --> 00:13:05,880
versus the more complicated models, you know, even if you do a very simple calculation,

179
00:13:05,880 --> 00:13:13,040
you know, if you have kind of a 1% error rate in your typographical errors, then it means

180
00:13:13,040 --> 00:13:17,920
that on almost every compass score sheet, there should be one typographical error.

181
00:13:17,920 --> 00:13:22,560
I'm not sure if that if that calculation kind of holds water, but the fact is that the

182
00:13:22,560 --> 00:13:28,040
more complicated the model, the much, it's much, much, much more easier to make an error

183
00:13:28,040 --> 00:13:30,000
than if it's a much simpler model.

184
00:13:30,000 --> 00:13:34,600
And also the simpler models can be double checked and triple checked, whereas the more complicated

185
00:13:34,600 --> 00:13:37,240
models is very, very difficult to check.

186
00:13:37,240 --> 00:13:43,600
There were other errors too that kind of propagated throughout society because of black box

187
00:13:43,600 --> 00:13:44,600
models.

188
00:13:44,600 --> 00:13:50,120
So for instance, another example was given to me by someone at the EPA at the Environmental

189
00:13:50,120 --> 00:13:51,600
Protection Agency.

190
00:13:51,600 --> 00:13:57,960
They had a very understandable model, they have a very understandable model for air quality

191
00:13:57,960 --> 00:14:02,840
that's used to assess whether it's safe to go outside.

192
00:14:02,840 --> 00:14:09,760
And at some point during the California wildfires, Google replaced the EPA's air quality

193
00:14:09,760 --> 00:14:15,000
measure with one from a, that was a proprietary model from a company.

194
00:14:15,000 --> 00:14:20,160
And that model told everyone that it was safe to go outside on a day when people saw

195
00:14:20,160 --> 00:14:23,600
a layer of ash on their cars.

196
00:14:23,600 --> 00:14:27,360
So yeah, so what went wrong with that model?

197
00:14:27,360 --> 00:14:29,080
I'm not sure.

198
00:14:29,080 --> 00:14:35,400
And it's pretty clear that the company who released that model wasn't, you know, there's

199
00:14:35,400 --> 00:14:36,720
something that went wrong.

200
00:14:36,720 --> 00:14:38,280
And nobody will know what it is.

201
00:14:38,280 --> 00:14:42,840
But clearly they didn't troubleshoot as carefully as they could have.

202
00:14:42,840 --> 00:14:46,720
And I'm guessing if they had used a model that they actually understood, then this kind

203
00:14:46,720 --> 00:14:50,080
of blatant mistake wouldn't have happened.

204
00:14:50,080 --> 00:14:57,960
Is there a framework or taxonomy for these kinds of errors or thinking about the different

205
00:14:57,960 --> 00:15:05,200
failure modes of black box systems or is it just, you know, black box systems, we don't

206
00:15:05,200 --> 00:15:08,480
understand them and that's a problem.

207
00:15:08,480 --> 00:15:12,680
Well, black box models can come in a couple of different varieties.

208
00:15:12,680 --> 00:15:16,640
Usually when I say a black box model, I mean, either a model that's too complicated

209
00:15:16,640 --> 00:15:21,480
for any human to understand or it's, you know, it's a function that's too complicated

210
00:15:21,480 --> 00:15:25,520
for humans to understand, like it has a million logical conditions in it or a bunch of things

211
00:15:25,520 --> 00:15:30,400
added together and then transformed and then added together again and then transformed.

212
00:15:30,400 --> 00:15:35,440
Or another type of way a model can be black boxes if it's proprietary, like if it's owned

213
00:15:35,440 --> 00:15:39,560
by a company that doesn't want to release the secret formula.

214
00:15:39,560 --> 00:15:43,760
And very often black boxes are both.

215
00:15:43,760 --> 00:15:51,840
So I think what you're asking me is to kind of talk a little bit more about kind of a way

216
00:15:51,840 --> 00:15:56,840
to go from black box to interpretable on sort of a gray scale, if you will, is that what

217
00:15:56,840 --> 00:15:57,840
you're asking about?

218
00:15:57,840 --> 00:16:05,600
I guess the step before that in my mind is, so you have, you know, this kind of class

219
00:16:05,600 --> 00:16:15,520
of models that are black box or, you know, otherwise opaque and you gave some examples of kind

220
00:16:15,520 --> 00:16:17,240
of the ways in which they fail.

221
00:16:17,240 --> 00:16:23,680
And I'm curious whether, you know, there are categories of, you know, failure modes,

222
00:16:23,680 --> 00:16:31,400
like, you know, you know, these fail because, and I don't even know what that might be,

223
00:16:31,400 --> 00:16:32,400
right?

224
00:16:32,400 --> 00:16:34,040
Like, you know, problems with the inputs.

225
00:16:34,040 --> 00:16:39,440
Like, you mentioned the, the, the, the typographic errors, like you, you know, they fit, you

226
00:16:39,440 --> 00:16:43,920
know, they could fail because, you know, you give them inputs and inputs are malformed

227
00:16:43,920 --> 00:16:50,720
and you kind of don't understand that, you know, it could be, you know, the underlying

228
00:16:50,720 --> 00:16:57,880
function doesn't well represent, you know, there's some kind of data drift and the, the

229
00:16:57,880 --> 00:17:03,440
model is trained on, you know, oh, yeah, there's, there's all kinds of stuff like that.

230
00:17:03,440 --> 00:17:07,520
I mean, it can go wrong anywhere, you know, when you trust a model, you, if you're going

231
00:17:07,520 --> 00:17:11,520
to trust a black box model, you don't just trust the formula, you actually have to trust

232
00:17:11,520 --> 00:17:14,200
the whole database that the model was trained from.

233
00:17:14,200 --> 00:17:19,120
And there's a lot of issues with trusting data, you know, data, I, I don't know if I've

234
00:17:19,120 --> 00:17:26,280
ever seen a clean database in my life, you know, if data can be very complicated and

235
00:17:26,280 --> 00:17:32,080
have all kinds of things wrong with it, it can be, you know, a lot of data can be missing

236
00:17:32,080 --> 00:17:37,000
or data can not represent the cases that you actually care about, or it can simply

237
00:17:37,000 --> 00:17:43,720
not represent the full range of cases that exist.

238
00:17:43,720 --> 00:17:48,040
Models can go wrong because they can overfit the data, so they can sort of memorize the

239
00:17:48,040 --> 00:17:56,680
data without actually generalizing to new cases, yeah, you can, you can have just about

240
00:17:56,680 --> 00:18:04,840
anything go wrong with a model or a data set, also, you know, a lot of these decisions,

241
00:18:04,840 --> 00:18:07,840
there's a cost involved to making a wrong decision.

242
00:18:07,840 --> 00:18:14,440
And machine learning is a field developed in kind of a low cost environment, you know,

243
00:18:14,440 --> 00:18:22,320
like predicting which advertisements some online user is going to click on.

244
00:18:22,320 --> 00:18:25,560
Now if you give the user the wrong advertisement, it's not a big deal.

245
00:18:25,560 --> 00:18:30,200
But if you deny someone's parole, that's a high cost decision.

246
00:18:30,200 --> 00:18:33,840
And then you also have the problem that the people who create the models are not the

247
00:18:33,840 --> 00:18:36,520
people who suffer those costs.

248
00:18:36,520 --> 00:18:40,920
So the people who created the compass model, they're not subject to it.

249
00:18:40,920 --> 00:18:47,840
So if they make a, you know, a bad prediction on someone because of a typographical error,

250
00:18:47,840 --> 00:18:50,680
the person who suffers is not the person who designs the model.

251
00:18:50,680 --> 00:18:55,760
And there's that, that sort of misaligned incentives that's a serious problem for machine

252
00:18:55,760 --> 00:18:58,960
learning deployment right now.

253
00:18:58,960 --> 00:19:03,240
One more question before we jump into kind of the path to addressing these issues.

254
00:19:03,240 --> 00:19:10,640
Do you, I think early I use opaque and black box kind of interchangeably are those interchangeable

255
00:19:10,640 --> 00:19:16,600
to you or do you, are there nuances there that are important to you?

256
00:19:16,600 --> 00:19:21,680
I think yeah, opaque and black box to me mean the same thing, but interpretable and explainable

257
00:19:21,680 --> 00:19:26,760
mean different things to me, because, you know, interpretable means that you can fully understand

258
00:19:26,760 --> 00:19:32,200
or you can understand the path of computations leading to the prediction, whereas explainable

259
00:19:32,200 --> 00:19:41,480
to me is like a post hoc, can you, can you somehow justify what the model did?

260
00:19:41,480 --> 00:19:46,920
And there are a wide variety of, well, there's a ton of research happening now to your earlier

261
00:19:46,920 --> 00:19:58,120
point on explaining these black box, black box models, but in the end, they're not interpreting

262
00:19:58,120 --> 00:19:59,760
what the model is doing.

263
00:19:59,760 --> 00:20:07,520
They are coming up with an explanation that hopefully correlates to that in some way.

264
00:20:07,520 --> 00:20:14,280
Yeah, but some of those explanations are such poor explanations that it's, you know,

265
00:20:14,280 --> 00:20:18,240
the explanations have almost nothing to do with the original model other than the fact

266
00:20:18,240 --> 00:20:22,840
that they produce predictions that are fairly similar.

267
00:20:22,840 --> 00:20:25,240
So they may not use the same important variables.

268
00:20:25,240 --> 00:20:32,320
They may claim the black box is depend on specific variables that they don't depend on.

269
00:20:32,320 --> 00:20:37,200
And yeah, they're sometimes their explanations are so incomplete that you actually don't

270
00:20:37,200 --> 00:20:41,440
really understand what the black box did at all, like, you know, they might give the same

271
00:20:41,440 --> 00:20:44,480
explanation for all of the different classes.

272
00:20:44,480 --> 00:20:48,800
So you really don't, you really don't actually know what the black box is doing.

273
00:20:48,800 --> 00:20:49,800
What's an example of that?

274
00:20:49,800 --> 00:20:56,120
It sounds like you're describing a system in which the explainability algorithm is like

275
00:20:56,120 --> 00:21:00,000
literally just given explanation that has nothing to do with anything.

276
00:21:00,000 --> 00:21:04,160
Well, you know, it's, it's funny.

277
00:21:04,160 --> 00:21:08,680
It's funny because people think that the explanations are meaningful, but they're not.

278
00:21:08,680 --> 00:21:13,320
So we give an example in the paper of saliency maps.

279
00:21:13,320 --> 00:21:15,560
And this is from my interpretable neural networks group.

280
00:21:15,560 --> 00:21:22,160
This is Oscar Lee and Chef Anchen and several other students where we're, you know, we're

281
00:21:22,160 --> 00:21:25,200
trying to understand what the saliency maps do.

282
00:21:25,200 --> 00:21:27,200
And what are saliency maps?

283
00:21:27,200 --> 00:21:28,200
Okay.

284
00:21:28,200 --> 00:21:31,480
Yeah, it's supposed to highlight the part of an image that the neural network is using

285
00:21:31,480 --> 00:21:32,640
to make its prediction.

286
00:21:32,640 --> 00:21:33,640
Okay.

287
00:21:33,640 --> 00:21:41,040
I like this attention heat maps that we've many of us have seen exactly, exactly.

288
00:21:41,040 --> 00:21:48,000
And so you get this neural network that says, okay, this is a picture of a golden retriever.

289
00:21:48,000 --> 00:21:50,720
And then it highlights the golden retriever's face.

290
00:21:50,720 --> 00:21:54,040
And you say, oh, yes, the network must be right because it's highlighting the golden retriever

291
00:21:54,040 --> 00:21:55,040
space.

292
00:21:55,040 --> 00:21:56,720
That's why it thinks it's a golden retriever.

293
00:21:56,720 --> 00:22:00,880
But then you ask the network, why do you think this image and you give the same image,

294
00:22:00,880 --> 00:22:01,880
okay?

295
00:22:01,880 --> 00:22:04,880
The same image of the golden retriever and you say, why do you think this image is a tennis

296
00:22:04,880 --> 00:22:05,880
ball?

297
00:22:05,880 --> 00:22:09,080
And it highlights exactly the same thing, the golden retriever's face.

298
00:22:09,080 --> 00:22:12,640
And it says, this is why I think it's a tennis ball.

299
00:22:12,640 --> 00:22:18,480
So it's giving you, it's just giving you this meaningless explanation for, you know, you

300
00:22:18,480 --> 00:22:22,680
think it's useful because it's telling you the correct pixels for the correct class.

301
00:22:22,680 --> 00:22:29,720
But it's just edges, you know, it's just telling you, I'm kind of looking over here.

302
00:22:29,720 --> 00:22:35,000
I guess my, my reaction to that is that there's, there's some, maybe there's some nuance

303
00:22:35,000 --> 00:22:36,000
missing there.

304
00:22:36,000 --> 00:22:43,280
Like it's more like conditioned on if this were a tennis ball, this would be why I think

305
00:22:43,280 --> 00:22:45,200
it's a tennis ball.

306
00:22:45,200 --> 00:22:48,080
But that's not what it told you, it told you it thought it was a golden retriever.

307
00:22:48,080 --> 00:22:50,560
So does that make that explanation wrong?

308
00:22:50,560 --> 00:22:55,880
Well, I mean, the, what happens when the neural network predicts, I mean, if it predicts

309
00:22:55,880 --> 00:22:59,480
correctly, fine, it's looking at the pixels that you think it should look at.

310
00:22:59,480 --> 00:23:04,680
But what if it predicts incorrectly, then it's just highlighting the edges as it did before.

311
00:23:04,680 --> 00:23:09,320
You know, there's no way to troubleshoot it because you, you're saying, well, it's looking

312
00:23:09,320 --> 00:23:11,840
at this, you know, there's an image of a golden retriever here.

313
00:23:11,840 --> 00:23:15,560
It says it's a conquer spaniel, but it's highlighting the face of the golden retriever

314
00:23:15,560 --> 00:23:19,080
and saying, this is why I think it's a conquer spaniel, like, right, right.

315
00:23:19,080 --> 00:23:21,440
You know, there's just no way to troubleshoot it.

316
00:23:21,440 --> 00:23:22,440
Yeah, yeah.

317
00:23:22,440 --> 00:23:27,920
Like it's the same thing maps are certainly helpful if you can find out that the network

318
00:23:27,920 --> 00:23:32,960
is looking in the wrong part of an image to make its prediction, but if it's looking

319
00:23:32,960 --> 00:23:37,240
at the correct pixels, you still have no idea what it's doing with those pixels.

320
00:23:37,240 --> 00:23:42,880
Going back to the, your title for this paper, stop explaining black box models.

321
00:23:42,880 --> 00:23:50,520
It sounds like your general contention is that none of these explainability approaches

322
00:23:50,520 --> 00:23:58,160
are adequate, and therefore we should just not be using black box models for high-stakes

323
00:23:58,160 --> 00:23:59,160
decisions.

324
00:23:59,160 --> 00:24:03,360
Well, if I want, if my credit is going to get denied when I go to the bank, I want to

325
00:24:03,360 --> 00:24:04,360
know why it got denied.

326
00:24:04,360 --> 00:24:09,840
I don't want somebody to say, oh, there were a million factors that went into it, but,

327
00:24:09,840 --> 00:24:13,800
you know, the primary one is, is that your credit history is not long enough.

328
00:24:13,800 --> 00:24:15,200
No, that's not good enough for me.

329
00:24:15,200 --> 00:24:20,480
I want to know exactly what part of my credit history made me get this loan, you know,

330
00:24:20,480 --> 00:24:23,080
made my loan be denied.

331
00:24:23,080 --> 00:24:28,400
And if I'm going to be denied parole, I don't want some 137 factors making that decision

332
00:24:28,400 --> 00:24:33,040
for me when the truth is that only three factors are important and I could get an equally

333
00:24:33,040 --> 00:24:35,920
accurate decision with only three factors, right?

334
00:24:35,920 --> 00:24:38,280
What if it was a typo that made the decision for me?

335
00:24:38,280 --> 00:24:41,920
What if my credit history wasn't entered in the computer correctly?

336
00:24:41,920 --> 00:24:47,480
There's a lot of reasons, really good reasons why for high-stakes decisions we should be

337
00:24:47,480 --> 00:24:53,000
using interpretable models rather than black boxes with post-doc explanations.

338
00:24:53,000 --> 00:24:58,040
Given those kinds of examples, one would think you wouldn't get a lot of resistance to

339
00:24:58,040 --> 00:25:05,680
that, but, you know, yet models like Compass and others exist, suggesting that, you know,

340
00:25:05,680 --> 00:25:08,280
maybe those folks aren't reading your paper.

341
00:25:08,280 --> 00:25:14,880
What needs to happen and, you know, what do you, do you propose something in the paper

342
00:25:14,880 --> 00:25:21,240
for, you know, getting us from, you know, where we are today to where we need to be?

343
00:25:21,240 --> 00:25:28,600
Well, so, yeah, I have gotten some resistance and I think part of the problem is that people

344
00:25:28,600 --> 00:25:30,600
really love their black boxes.

345
00:25:30,600 --> 00:25:36,560
They love the idea that a black box can uncover secret hidden patterns that they don't think

346
00:25:36,560 --> 00:25:40,360
an interpretable model could, you know, they say it's too simple, it couldn't possibly

347
00:25:40,360 --> 00:25:45,760
be that accurate, which for many problems is not true.

348
00:25:45,760 --> 00:25:49,800
In particular, for a criminal recidivism prediction, we've been able to produce interpretable

349
00:25:49,800 --> 00:25:54,720
models that are just as accurate as the best black box machine learning models.

350
00:25:54,720 --> 00:26:00,400
For credit risk assessment, we've worked on some data from FICO for an interpretable

351
00:26:00,400 --> 00:26:04,320
or an explainable machine learning challenge, but even in that challenge, you didn't need

352
00:26:04,320 --> 00:26:07,840
a black box model, you could, you could do it with an interpretable model.

353
00:26:07,840 --> 00:26:11,960
So, I think, you know, a lot of people, first of all, they love their black box models

354
00:26:11,960 --> 00:26:15,800
because they don't believe interpretable models exist, but also they want to make money

355
00:26:15,800 --> 00:26:17,880
from their black boxes.

356
00:26:17,880 --> 00:26:23,960
And they think the black boxes, you know, they uncover secret hidden patterns.

357
00:26:23,960 --> 00:26:28,000
And also black boxes are much easier to train than interpretable models, so that's a major

358
00:26:28,000 --> 00:26:29,000
issue.

359
00:26:29,000 --> 00:26:30,000
Easier to train?

360
00:26:30,000 --> 00:26:34,320
Yeah, it's much easier to train a black box than to train something that's interpretable,

361
00:26:34,320 --> 00:26:38,480
because if you're training the black box, you don't have to have constraints, you know,

362
00:26:38,480 --> 00:26:41,440
technical constraints on the model that force it to be interpretable.

363
00:26:41,440 --> 00:26:46,320
Whereas if you're creating interpretable models, you have to, you have to minimize loss,

364
00:26:46,320 --> 00:26:50,680
you know, make it accurate, subject to some kind of interpretability constraint.

365
00:26:50,680 --> 00:26:54,600
And those constraints make the problems, the optimization problems harder.

366
00:26:54,600 --> 00:26:59,640
So it's actually much harder to design an interpretable model than it is to design a

367
00:26:59,640 --> 00:27:01,160
black box model.

368
00:27:01,160 --> 00:27:05,280
But the software for designing interpretable models has advanced considerably over the

369
00:27:05,280 --> 00:27:07,000
last however many years.

370
00:27:07,000 --> 00:27:13,240
And so, you know, it's kind of sad that people are not even trying to construct interpretable

371
00:27:13,240 --> 00:27:21,200
models that they want to instead just run their black box algorithms and then try to explain

372
00:27:21,200 --> 00:27:24,880
what they're doing afterward rather than actually go to the trouble of correcting the underlying

373
00:27:24,880 --> 00:27:28,440
problem and designing the model to be interpretable in the first place.

374
00:27:28,440 --> 00:27:35,760
Let's dig into this a little bit more. When I hear interpretable models, I think the,

375
00:27:35,760 --> 00:27:41,440
I tend to interpret that, or let's not overuse interpret.

376
00:27:41,440 --> 00:27:50,560
The picture that forms for me is one of using kind of simpler models that have some

377
00:27:50,560 --> 00:27:56,560
inherent trait of interpretability, like a decision tree.

378
00:27:56,560 --> 00:28:06,400
For example, as opposed to what I thought I heard you suggesting was a model that may

379
00:28:06,400 --> 00:28:12,360
be more complicated than that, but has, you know, it's trained against some interpretability

380
00:28:12,360 --> 00:28:13,360
constraint.

381
00:28:13,360 --> 00:28:18,040
Well, decision trees you could write down as an optimization problem.

382
00:28:18,040 --> 00:28:19,040
No.

383
00:28:19,040 --> 00:28:26,920
So, you know, maximize accuracy, subject to a constraint on the size of the decision tree.

384
00:28:26,920 --> 00:28:32,760
Now, you know, the most popular algorithms for decision trees are Carton C4.5 and those

385
00:28:32,760 --> 00:28:36,320
algorithms stay back from the 90s and so they're not that good.

386
00:28:36,320 --> 00:28:43,680
I mean, they work, you know, they work surprisingly well for an algorithm being from the 90s, but

387
00:28:43,680 --> 00:28:49,640
they aren't as good as, you know, the modern machine learning methods because Carton C4.5

388
00:28:49,640 --> 00:28:53,200
don't actually optimize anything, you know.

389
00:28:53,200 --> 00:28:55,240
They're not optimization techniques.

390
00:28:55,240 --> 00:29:01,480
So, you know, over the last several years, there have been several teams working on optimal

391
00:29:01,480 --> 00:29:03,600
decision trees.

392
00:29:03,600 --> 00:29:08,520
And I gave an example in the please stop explaining paper of the work that we've done in the

393
00:29:08,520 --> 00:29:13,120
corals project, which is an optimal decision list paper.

394
00:29:13,120 --> 00:29:20,200
So the goal is to globally optimize over all possible decision lists, the, you know, minimize

395
00:29:20,200 --> 00:29:26,680
the loss, subject to the model being sparse.

396
00:29:26,680 --> 00:29:30,720
So, you know, that code is available and people could download and use it.

397
00:29:30,720 --> 00:29:32,640
Can you elaborate on that a little bit more?

398
00:29:32,640 --> 00:29:38,440
I have a decision lists relative to decision trees is something that I'm not too familiar

399
00:29:38,440 --> 00:29:39,440
with.

400
00:29:39,440 --> 00:29:42,320
Oh, a decision list is a one-sided decision tree.

401
00:29:42,320 --> 00:29:43,920
It's a series of if-then rules.

402
00:29:43,920 --> 00:29:44,920
Got it.

403
00:29:44,920 --> 00:29:45,920
Okay.

404
00:29:45,920 --> 00:29:46,920
Yeah.

405
00:29:46,920 --> 00:29:51,200
Decision lists are exponentially easier to create than decision than full blown decision

406
00:29:51,200 --> 00:29:52,560
trees.

407
00:29:52,560 --> 00:29:59,400
And both decision trees and decision lists to create them optimally is computationally hard.

408
00:29:59,400 --> 00:30:02,600
It's NP hard with no polynomial time approximation.

409
00:30:02,600 --> 00:30:08,280
So these are actually very, very hard optimization problems, but, you know, computers have increased

410
00:30:08,280 --> 00:30:14,320
their speed, you know, a million times in the last, oh, 20 years or something like that.

411
00:30:14,320 --> 00:30:20,680
So we actually can solve optimal decision list problems in reasonable amounts of time

412
00:30:20,680 --> 00:30:23,760
for reasonably large data sets.

413
00:30:23,760 --> 00:30:27,240
And that's what the corals project is about.

414
00:30:27,240 --> 00:30:33,840
With corals, have you applied it to some of the same types of problems, you know, for

415
00:30:33,840 --> 00:30:39,200
which you've kind of given these examples of, you know, the failures of black box models?

416
00:30:39,200 --> 00:30:40,200
Yeah.

417
00:30:40,200 --> 00:30:46,360
So we actually applied corals to the criminal recidivism problem.

418
00:30:46,360 --> 00:30:47,360
Okay.

419
00:30:47,360 --> 00:30:55,520
And what we found was that we found a very, very tiny model that I can probably tell you

420
00:30:55,520 --> 00:30:59,640
what it is if I can just grab it real quick, but we found a really small model that was

421
00:30:59,640 --> 00:31:04,600
just as accurate as, as compass or any of the other machine learning methods.

422
00:31:04,600 --> 00:31:09,200
And the model is so simple, it says, like, if you're between 18 to 20 years old in your

423
00:31:09,200 --> 00:31:14,640
mail, predict, you'll be arrested within two years, Elsa, if you're between 21 and 23

424
00:31:14,640 --> 00:31:18,720
and you have two to three prior offenses, predict, arrest, Elsa, if you have more than three

425
00:31:18,720 --> 00:31:21,440
priors, predict, arrest, otherwise predict, no arrest.

426
00:31:21,440 --> 00:31:27,240
So it's basically if you're younger and you have more priors, more prior crimes, then

427
00:31:27,240 --> 00:31:30,360
the algorithm predicts that you're more likely to be arrested.

428
00:31:30,360 --> 00:31:37,640
And this is, this is the algorithm making a prediction kind of independently based on

429
00:31:37,640 --> 00:31:38,640
the data.

430
00:31:38,640 --> 00:31:43,320
This is not, you're not trying to fit against what compass might do.

431
00:31:43,320 --> 00:31:49,040
No, this is just, this is just trying to predict the outcome, which is whether the person

432
00:31:49,040 --> 00:31:54,080
will be arrested within two years, this is data from Broward County, Florida.

433
00:31:54,080 --> 00:31:57,960
And we compared it directly against the compass scores.

434
00:31:57,960 --> 00:32:02,760
And what we found was that the corals model and the compass model were equally accurate

435
00:32:02,760 --> 00:32:08,960
in predicting whether someone will be arrested within two years of their release.

436
00:32:08,960 --> 00:32:13,440
So that tiny little model that I just told you, compared to a complicated black box

437
00:32:13,440 --> 00:32:20,080
model with over 130 factors, both are about equally accurate to predicting arrest.

438
00:32:20,080 --> 00:32:25,360
And also, if you try any other machine learning method, boosted decision trees right

439
00:32:25,360 --> 00:32:29,960
and for us, anything you want, you try it on the same data from Florida and it'll be

440
00:32:29,960 --> 00:32:33,200
almost equally accurate to both compass and corals.

441
00:32:33,200 --> 00:32:38,000
So as far as we can tell, for predicting arrest, there doesn't seem to be any benefit

442
00:32:38,000 --> 00:32:43,720
of using a very complicated black box model, you can use a very small decision tree to get

443
00:32:43,720 --> 00:32:44,720
it.

444
00:32:44,720 --> 00:32:47,520
It's a lot of computational work to produce that decision tree.

445
00:32:47,520 --> 00:32:55,040
But once you have the decision tree, there's no clear reason to use anything more complicated.

446
00:32:55,040 --> 00:33:00,280
When I think of black box models and you referenced this earlier, I think a lot of neural networks

447
00:33:00,280 --> 00:33:10,520
and deep learning and they are also notoriously computationally expensive and also data,

448
00:33:10,520 --> 00:33:19,880
you know, sample intensive, do the models that you're the interpretable models like corals,

449
00:33:19,880 --> 00:33:22,760
which you're also describing as computationally expensive?

450
00:33:22,760 --> 00:33:25,680
Do they, are they applicable for similar problems?

451
00:33:25,680 --> 00:33:29,920
And if so, do you have a sense for their relative computational expensiveness?

452
00:33:29,920 --> 00:33:34,320
Well, okay, so that gets us into computer vision.

453
00:33:34,320 --> 00:33:37,680
So there are certain problems that are just different from the other problems.

454
00:33:37,680 --> 00:33:44,400
So computer vision, speech, there are certain natural language processing problems.

455
00:33:44,400 --> 00:33:48,120
Some of these particular problems are the ones that machine learning algorithms have

456
00:33:48,120 --> 00:33:53,280
been super successful at starting from around 2012.

457
00:33:53,280 --> 00:33:59,400
And also for computer vision, the notion of interpretability changes, right?

458
00:33:59,400 --> 00:34:09,280
Even before we get to computer vision, there is work that's starting to happen and people

459
00:34:09,280 --> 00:34:16,800
starting to look at applying deep learning to tabular data.

460
00:34:16,800 --> 00:34:23,960
This has been done, you know, for a couple of, well, a couple of the examples that come

461
00:34:23,960 --> 00:34:32,240
to mind for me are some of the Kaggle competitions, and this is discussed in the Fast.ai course,

462
00:34:32,240 --> 00:34:33,240
quite a bit.

463
00:34:33,240 --> 00:34:40,080
The idea being that, hey, we can throw a deep learning network against this, and we

464
00:34:40,080 --> 00:34:44,880
don't have to have data scientists with lots of domain experience, people that know how

465
00:34:44,880 --> 00:34:51,520
to create deep learning models, you know, but don't know anything about the domain can

466
00:34:51,520 --> 00:34:57,560
perform all on these products, you know, just by throwing a deep learning model against

467
00:34:57,560 --> 00:35:05,240
these kind of tabular problems, the one that, one example is the Rothman stores, I think

468
00:35:05,240 --> 00:35:11,960
was the name Kaggle competition, where you're trying to predict, I forget store sales

469
00:35:11,960 --> 00:35:17,520
or something like that based on, you know, some set of tabular data, meaning not image

470
00:35:17,520 --> 00:35:24,600
or audio or any kind of media, you know, just regular data, like you might apply to, you

471
00:35:24,600 --> 00:35:27,160
know, a more simple machine learning model.

472
00:35:27,160 --> 00:35:32,000
Yeah, I mean, there's definitely a lot of domains in which, well, not a lot, but there's

473
00:35:32,000 --> 00:35:37,120
at least several domains in which neural networks are just the best, and there's no question

474
00:35:37,120 --> 00:35:38,120
about that.

475
00:35:38,120 --> 00:35:42,840
But there's no reason you can't also have neural networks that are interpretable.

476
00:35:42,840 --> 00:35:46,960
So we've been trying to design these neural networks for computer vision that are, that

477
00:35:46,960 --> 00:35:51,920
they do a form of case-based reasoning, so that they don't just produce a prediction,

478
00:35:51,920 --> 00:35:56,680
like, yes, this is a Siberian husky.

479
00:35:56,680 --> 00:36:02,200
The networks that we're trying to produce give you the reasoning process behind why it

480
00:36:02,200 --> 00:36:04,320
thinks this is a Siberian husky.

481
00:36:04,320 --> 00:36:11,520
It says, the network says, I think that this part of the image looks like this prototypical

482
00:36:11,520 --> 00:36:17,280
ear of a Siberian husky that I've seen before, and this prototypical, and this part of

483
00:36:17,280 --> 00:36:21,880
the paw of the Siberian husky looks like this prototypical paw of a Siberian husky that

484
00:36:21,880 --> 00:36:23,040
I've seen before.

485
00:36:23,040 --> 00:36:28,920
And so, you know, even with neural networks, you can still add interpretability constraints

486
00:36:28,920 --> 00:36:35,040
and, you know, not lose accuracy when you make predictions.

487
00:36:35,040 --> 00:36:38,800
So even a deep neural network doesn't have to be a complete black box.

488
00:36:38,800 --> 00:36:41,640
And one of the things that I've come across is like this, you know, turtles all the way

489
00:36:41,640 --> 00:36:42,640
down problem.

490
00:36:42,640 --> 00:36:52,160
Like, is it, is the, the interpretable model some kind of hybrid of, you know, explainable

491
00:36:52,160 --> 00:36:58,640
and, you know, the model, or is it like, what makes it inherently interpretable versus

492
00:36:58,640 --> 00:37:04,240
some explainable feature kind of bolted deep inside the architecture of the, the black

493
00:37:04,240 --> 00:37:05,240
box model?

494
00:37:05,240 --> 00:37:06,240
Does that make sense?

495
00:37:06,240 --> 00:37:11,760
Yeah, so, you know, what we're trying to do is design a deep neural network that will

496
00:37:11,760 --> 00:37:17,960
explain its predictions in a way that's similar to how a human would describe its reasoning

497
00:37:17,960 --> 00:37:20,680
process behind the predictions.

498
00:37:20,680 --> 00:37:26,400
We've been doing a lot of work on the CUB data sets, the bird identification data sets

499
00:37:26,400 --> 00:37:33,280
because bird identification is kind of difficult for humans and we can get reasonable explanations

500
00:37:33,280 --> 00:37:34,280
out of them.

501
00:37:34,280 --> 00:37:40,320
But the goal is to actually embed the network with the explanations, you know, with constraints

502
00:37:40,320 --> 00:37:45,320
as part of the explanation so that when the network says, I'm using, I'm comparing this

503
00:37:45,320 --> 00:37:49,600
image to these other images and that's how I'm making my decision.

504
00:37:49,600 --> 00:37:52,200
The network is, it's, this is not a post-hawk thing.

505
00:37:52,200 --> 00:37:53,760
This is actually part of the network.

506
00:37:53,760 --> 00:37:56,640
It's saying, it's saying, here is my decision process.

507
00:37:56,640 --> 00:37:57,640
I'm taking the image.

508
00:37:57,640 --> 00:38:01,320
I'm comparing it to this, this, this and that image.

509
00:38:01,320 --> 00:38:04,840
And because I think this looks like that, this looks like that, this looks like that,

510
00:38:04,840 --> 00:38:10,680
and this looks like that, and that's why I think this is a clay-colored sparrow as opposed

511
00:38:10,680 --> 00:38:13,120
to a robin or something.

512
00:38:13,120 --> 00:38:14,120
Okay.

513
00:38:14,120 --> 00:38:15,920
So it's not a post-hawk.

514
00:38:15,920 --> 00:38:19,280
It's, it's not, I trained the network and then afterward I tried to figure out what

515
00:38:19,280 --> 00:38:20,760
the network was doing.

516
00:38:20,760 --> 00:38:26,360
This is instead the network saying, this is the computation I'm doing and the, the, the

517
00:38:26,360 --> 00:38:31,600
ex, you know, this is the reasoning process behind why I made that prediction.

518
00:38:31,600 --> 00:38:33,720
Do you, do you understand the difference?

519
00:38:33,720 --> 00:38:41,400
I do understand the, I do understand the difference between those two, trying to think if there's

520
00:38:41,400 --> 00:38:47,160
a better way to describe the distinction that I was trying to make.

521
00:38:47,160 --> 00:38:50,720
The way I like to think about it is kind of like a real estate agent pricing houses.

522
00:38:50,720 --> 00:38:51,720
Mm-hmm.

523
00:38:51,720 --> 00:38:55,280
You know, the, the way real estate agents price houses that they look for comps, right?

524
00:38:55,280 --> 00:39:00,840
They look for comparable houses in the same, you know, maybe in the same neighborhood.

525
00:39:00,840 --> 00:39:05,320
And they say, well, that house has a, it's about the same square footage as yours.

526
00:39:05,320 --> 00:39:09,640
And it has a big backyard and yours has a big backyard.

527
00:39:09,640 --> 00:39:11,800
And this other house is right down the block.

528
00:39:11,800 --> 00:39:17,240
So the, you know, I'm getting a price from the location.

529
00:39:17,240 --> 00:39:20,840
And so the real estate agent combines all of that information from the comparable houses

530
00:39:20,840 --> 00:39:26,240
to try to create a price that is the real estate agent's prediction.

531
00:39:26,240 --> 00:39:28,720
And they're explaining to you how they came up with that price.

532
00:39:28,720 --> 00:39:33,920
It's like, I'm comparing, oh, they're backyard to your backyard and so on.

533
00:39:33,920 --> 00:39:42,560
And so the models that these interpretable computer vision models are they, at least

534
00:39:42,560 --> 00:39:48,960
the ones that you're working with, are they explicitly referencing some database of,

535
00:39:48,960 --> 00:39:54,800
you know, they, when they're making their decisions, are they referencing, is it like an

536
00:39:54,800 --> 00:40:00,160
information retrieval thing where they're referencing some database or some set of images

537
00:40:00,160 --> 00:40:05,800
from the training data, or is this, yes, the referencing images from the training data.

538
00:40:05,800 --> 00:40:09,200
They're saying, you know, I've seen a prototypical Robin before.

539
00:40:09,200 --> 00:40:16,760
I have a whole bunch of them in my database, you know, they, they, the throat of a Robin

540
00:40:16,760 --> 00:40:21,320
looks like this prototypical throat of a Robin that I've seen before.

541
00:40:21,320 --> 00:40:24,920
And it doesn't know what a throat of a Robin is, it's just highlighting part of the image

542
00:40:24,920 --> 00:40:29,180
and saying, I think this part of your image is similar to this part of this prototypical

543
00:40:29,180 --> 00:40:30,600
Robin image.

544
00:40:30,600 --> 00:40:34,440
And that similarity is what I'm using to help make my prediction.

545
00:40:34,440 --> 00:40:42,600
And so you, you've alluded to the difficulty that humans have in explaining some of their

546
00:40:42,600 --> 00:40:51,360
decisions in bird watching and in other areas, is the, you know, does that mean in a sense

547
00:40:51,360 --> 00:41:00,120
that the goal is to, to have kind of superhuman performance, not, not in a sensationalistic

548
00:41:00,120 --> 00:41:07,920
way, but to create the models that are better than humans at explaining what they're doing.

549
00:41:07,920 --> 00:41:12,480
I mean, there's a lot of critique of kind of the whole explainability thing that there's

550
00:41:12,480 --> 00:41:17,240
research that says that we make up things when we're asked to explain things kind of like

551
00:41:17,240 --> 00:41:19,240
how you're saying models do.

552
00:41:19,240 --> 00:41:23,480
Yeah, that, that, that's part of the reason why we're working on the recidivism project

553
00:41:23,480 --> 00:41:28,880
is because, you know, judges are humans and humans are biased black boxes, right?

554
00:41:28,880 --> 00:41:29,880
Humans, right.

555
00:41:29,880 --> 00:41:35,240
They, they claim they're making a decision because of acts, but the real reason that they're

556
00:41:35,240 --> 00:41:41,080
making the decision is unknown to them and to everyone else.

557
00:41:41,080 --> 00:41:46,400
And so you have different judges making different decisions for different reasons.

558
00:41:46,400 --> 00:41:51,080
And so the whole point was to have these very simple models that would kind of get everybody

559
00:41:51,080 --> 00:41:57,040
in the same page and, you know, be more consistent and be more accurate.

560
00:41:57,040 --> 00:42:01,600
Because also, you know, humans, humans can't process whole databases in their head and

561
00:42:01,600 --> 00:42:05,440
come up with accurate predictions, we're just not good at that.

562
00:42:05,440 --> 00:42:08,560
And that's why we rely on machine learning algorithms.

563
00:42:08,560 --> 00:42:15,520
So you know, we're hoping that by, by sort of leveraging large databases to make accurate

564
00:42:15,520 --> 00:42:21,440
predictions and having explanations that are, you know, understandable to human experts

565
00:42:21,440 --> 00:42:26,480
that will hopefully be able to help get everyone on the same page.

566
00:42:26,480 --> 00:42:30,920
And then, you know, if the human decision maker has extra factors that are not in the database,

567
00:42:30,920 --> 00:42:37,800
they can calibrate those extra factors into the model that they started with, right.

568
00:42:37,800 --> 00:42:42,920
So if the recidivism prediction model says there's a 76% chance you'll recidivate, but

569
00:42:42,920 --> 00:42:48,120
let's say there's some additional factor that the judge knows about that's not in the database.

570
00:42:48,120 --> 00:42:52,280
The judge can use that as a mitigating factor to change their decision.

571
00:42:52,280 --> 00:42:59,120
So can you tell a little bit about your future directions in this area?

572
00:42:59,120 --> 00:43:02,240
Well, we've been working on optimal decision trees for a really long time.

573
00:43:02,240 --> 00:43:07,840
I didn't even know how many years I've been working on optimal decision trees.

574
00:43:07,840 --> 00:43:13,480
I've also been working on optimal sparse linear models with integer coefficients for a long

575
00:43:13,480 --> 00:43:14,480
time.

576
00:43:14,480 --> 00:43:16,720
So these are like medical scoring systems.

577
00:43:16,720 --> 00:43:22,840
These are models that give you like one point if your age is above 50 and then two points

578
00:43:22,840 --> 00:43:26,040
if you've had a prior heart condition and so on.

579
00:43:26,040 --> 00:43:32,960
So those point scores, we're trying to fully optimize them based on data which substitutes

580
00:43:32,960 --> 00:43:38,200
for having a team of doctors in a room trying to decide the point scores.

581
00:43:38,200 --> 00:43:42,040
We're using, we're instead using large databases and machine learning algorithms to design

582
00:43:42,040 --> 00:43:47,680
the point scores, but the final models are very similar to what the doctors in the room

583
00:43:47,680 --> 00:43:52,880
might have constructed themselves.

584
00:43:52,880 --> 00:43:56,000
So we've been working on that for quite a while.

585
00:43:56,000 --> 00:44:01,960
So these are, this is optimal scoring systems and then the optimal neural networks project

586
00:44:01,960 --> 00:44:07,840
we've had going for a few years, sorry, not optimal neural networks, interpretable neural

587
00:44:07,840 --> 00:44:08,840
networks.

588
00:44:08,840 --> 00:44:12,520
We've had the interpretable neural networks project going for a few years.

589
00:44:12,520 --> 00:44:19,240
And then I also have several projects on causal inference where we're trying to do interpretable

590
00:44:19,240 --> 00:44:21,080
matching for causal inference.

591
00:44:21,080 --> 00:44:25,080
So if you have a treatment group and a control group, so people who've taken the drug and

592
00:44:25,080 --> 00:44:30,440
who haven't taken the drug and you want to determine what the effect of the drug is, then

593
00:44:30,440 --> 00:44:36,080
you would normally try to match, you normally try to match people with their identical twin.

594
00:44:36,080 --> 00:44:39,840
People who've had the drug match them with an identical twin who didn't have the drug

595
00:44:39,840 --> 00:44:44,120
and then you could figure out what the effect of the drug is.

596
00:44:44,120 --> 00:44:50,480
So we have a project on doing that in a more accurate way.

597
00:44:50,480 --> 00:44:52,840
I'm also still working on criminal recidivism.

598
00:44:52,840 --> 00:44:56,960
We've been working on that for several years now.

599
00:44:56,960 --> 00:45:02,520
We had a paper published in 2015 called interpretable classification models for recidivism prediction

600
00:45:02,520 --> 00:45:08,240
where we showed that the interpretable machine learning models were just as accurate for

601
00:45:08,240 --> 00:45:14,760
predicting all different kinds of recidivism than complicated machine learning models.

602
00:45:14,760 --> 00:45:19,360
So they're just as accurate as the complicated machine learning models.

603
00:45:19,360 --> 00:45:23,160
So we were still working on recidivism and then I have a lot of projects on health care,

604
00:45:23,160 --> 00:45:28,720
which is another high stakes domain where you care about interpretability.

605
00:45:28,720 --> 00:45:32,240
You want to make very careful decisions in health care.

606
00:45:32,240 --> 00:45:40,360
If you had to call out an area that you think needs more work or attention but is not

607
00:45:40,360 --> 00:45:45,800
one that you're working on, is there any particular thing that comes to mind?

608
00:45:45,800 --> 00:45:50,640
I mean, there's a lot of societal good applications that are almost impossible to work on because

609
00:45:50,640 --> 00:45:53,240
gathering the data is very, very difficult.

610
00:45:53,240 --> 00:45:57,680
I wish I could work on more problems like that but the data is just so hard to get that.

611
00:45:57,680 --> 00:46:00,000
Any particular example comes to mind?

612
00:46:00,000 --> 00:46:05,800
Well, I think income inequality in the world is a huge problem for instance.

613
00:46:05,800 --> 00:46:10,640
Like any major world problem, we should be throwing data and algorithms at it because

614
00:46:10,640 --> 00:46:11,640
we can.

615
00:46:11,640 --> 00:46:13,920
We have people, we have expertise.

616
00:46:13,920 --> 00:46:15,320
People want to work on those problems.

617
00:46:15,320 --> 00:46:16,960
It's just that it's hard to do it.

618
00:46:16,960 --> 00:46:17,960
Yeah.

619
00:46:17,960 --> 00:46:21,600
Those are more data problems than algorithmic problems.

620
00:46:21,600 --> 00:46:26,680
I think health care is a huge frontier.

621
00:46:26,680 --> 00:46:28,440
There's a lot of people working on health care.

622
00:46:28,440 --> 00:46:35,560
I think a lot of people realize how important it is but the data is really messy, is confounded

623
00:46:35,560 --> 00:46:39,400
in every possible way you could imagine.

624
00:46:39,400 --> 00:46:46,520
But it's also really important because if we understood what the right cancer treatments

625
00:46:46,520 --> 00:46:53,960
are and what to do about the opioid epidemic, that would be amazing if we could solve some

626
00:46:53,960 --> 00:46:58,080
of these problems with algorithms and so on.

627
00:46:58,080 --> 00:47:05,520
In terms of algorithmic challenges, one problem that we're just starting to try to tackle

628
00:47:05,520 --> 00:47:14,080
is whether you can know if an interpretable model exists before going to the trouble of

629
00:47:14,080 --> 00:47:19,480
finding one because finding them is computationally very demanding.

630
00:47:19,480 --> 00:47:20,480
Right.

631
00:47:20,480 --> 00:47:27,480
Before you go and do all that extra work, it'd be nice to know in advance whether you're

632
00:47:27,480 --> 00:47:31,280
likely to find an interpretable model.

633
00:47:31,280 --> 00:47:34,400
So we're just starting to think about that question now.

634
00:47:34,400 --> 00:47:39,520
Well Cynthia, thanks so much for taking the time to chat with me really, definitely really

635
00:47:39,520 --> 00:47:46,560
interesting work and a lot of thought-provoking issues that are involved in it.

636
00:47:46,560 --> 00:47:48,960
Okay, my pleasure.

637
00:47:48,960 --> 00:47:50,440
Thank you.

638
00:47:50,440 --> 00:47:56,280
Alright everyone, that's our show for today.

639
00:47:56,280 --> 00:48:02,320
For more information on today's show, visit twomolai.com slash shows.

640
00:48:02,320 --> 00:48:08,160
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platforms

641
00:48:08,160 --> 00:48:09,640
Conference.

642
00:48:09,640 --> 00:48:38,160
As always, thanks so much for listening and catch you next time.

