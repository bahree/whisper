WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:30.620
I'm your host Sam Charrington.

00:30.620 --> 00:35.640
In this episode we're joined by Hambiel Zhu, a PhD student in the Robotics Institute

00:35.640 --> 00:38.040
at Carnegie Mellon University.

00:38.040 --> 00:42.560
Han, who's on track to complete his thesis at the end of the year, is working on what is

00:42.560 --> 00:48.680
called the Panoptic Studio, a multi-dimension motion capture studio with over 500 camera

00:48.680 --> 00:53.720
sensors that are used to capture human body behavior and body language.

00:53.720 --> 00:58.240
While robotic and other artificially intelligent systems can interact with humans, Han's

00:58.240 --> 01:03.160
work focuses on understanding how humans interact and behave with one another so that we can

01:03.160 --> 01:07.320
teach AI-based systems to react to humans more naturally.

01:07.320 --> 01:13.400
In our conversation, we discuss his CVPR best student paper award winner, Total Capture,

01:13.400 --> 01:19.080
3D deformation model for tracking faces, hands, and bodies.

01:19.080 --> 01:23.720
Han also shares a complete overview of the Panoptic Studio and we dig into the creation and

01:23.720 --> 01:26.200
performance of the models and much more.

01:26.200 --> 01:28.240
Okay, enjoy the show.

01:28.240 --> 01:31.480
Alright everyone, I am on the line with Hambiel Zhu.

01:31.480 --> 01:36.960
Hambiel is a PhD student in Robotics at Carnegie Mellon University.

01:36.960 --> 01:40.120
Han, welcome to this week in machine learning and AI.

01:40.120 --> 01:42.280
Thank you for having me today.

01:42.280 --> 01:51.440
So you've been a PhD student at CMU since 2012 and a major focus of your work is in and

01:51.440 --> 01:56.120
around an environment that's been built there called the Panoptic Studio.

01:56.120 --> 01:57.720
What is the Panoptic Studio?

01:57.720 --> 02:04.800
Yeah, the Panoptic Studio is basically a multi-b system with more than 500 camera sensors

02:04.800 --> 02:08.520
and the system is also controlled by more than 50 machines.

02:08.520 --> 02:14.920
So this is indeed a giant system using computer vision technique to measure or sense humans

02:14.920 --> 02:16.840
by the behaviors.

02:16.840 --> 02:22.880
So basically we are very interested in the way we are using our bodies for social communication.

02:22.880 --> 02:28.160
For example, all these subtle facial expressions and body gestures and so on.

02:28.160 --> 02:33.160
And we really wanted to make a machine who can understand the body languages we are using

02:33.160 --> 02:38.240
and in the end we want to make a machine which can use these body languages to communicate

02:38.240 --> 02:39.480
with us.

02:39.480 --> 02:44.400
And for the purpose, the first thing we wanted to do is we wanted to collect this kind

02:44.400 --> 02:50.000
of motion capture data of naturally interacting people so that we can use some machine learning

02:50.000 --> 02:53.640
technique on top of that so that we can in the end machine can understand these kind of

02:53.640 --> 02:55.440
signals.

02:55.440 --> 02:59.760
And that was the main motivation of this Panoptic Studio and since we are interested

02:59.760 --> 03:06.160
in multiple people's interaction to avoid all this complicated occlusion kind of problem,

03:06.160 --> 03:11.680
we wanted to make a system with many, many sensors with many viewpoint so that we can

03:11.680 --> 03:16.200
really release these particular occlusion problems.

03:16.200 --> 03:20.840
And this Panoptic Studio goes back quite a ways it sounds like.

03:20.840 --> 03:27.680
Yeah, so CNU has really amazing history on this multi-b system because Professor Dakeo

03:27.680 --> 03:33.880
already started this multi-view system kind of research about, I think, 30 years ago.

03:33.880 --> 03:40.680
And at the moment, the major goal is to reconstruct the 3D world, especially 3D object and

03:40.680 --> 03:42.680
3D human body behavior.

03:42.680 --> 03:47.720
And to do that, obviously, this multi-view system is a really good kind of key to solve all

03:47.720 --> 03:49.880
this depth kind of ambiguity.

03:49.880 --> 03:55.680
At the moment, he already started this project with 15 machines and it was extremely challenging

03:55.680 --> 03:56.880
problem at the moment.

03:56.880 --> 04:03.160
And our Panoptic Studio is kind of third generation and our basic idea is maybe 40 or 50 cameras

04:03.160 --> 04:04.160
I will let it boring.

04:04.160 --> 04:11.520
Let's try to make a system with 1000 cameras, for example, and see what we can solve.

04:11.520 --> 04:16.040
And that was kind of the beginning of our studio and we particularly chose this social

04:16.040 --> 04:21.120
interaction analysis problem because that is indeed a challenging problem and very important

04:21.120 --> 04:26.160
future problems so that the machine can actually have a way to interact with us.

04:26.160 --> 04:35.400
And so how do you frame out this social interaction problem and what are the different research

04:35.400 --> 04:39.800
problems that come out of this broader problem?

04:39.800 --> 04:45.440
So basically, you can easily consider natural language or verbal language problems.

04:45.440 --> 04:53.360
So for example, nowadays, there is a very popular commercial AI system which can understand

04:53.360 --> 04:58.880
human's voice and human's language and use their languages to communicate with humans.

04:58.880 --> 05:04.800
But in that case, the main channel is verbal language, which has sentence and grammar

05:04.800 --> 05:06.280
or just rules.

05:06.280 --> 05:10.960
Basically, we wanted to do the similar thing with non-verbal languages.

05:10.960 --> 05:15.200
And for example, when humans are communicating each other, we use obviously our verbal

05:15.200 --> 05:19.960
languages, but we also use our facial expressions and our body languages.

05:19.960 --> 05:25.240
And all this subtle movement have some specific meaning, which is actually really hard to

05:25.240 --> 05:29.960
define, but humans are really easily understanding.

05:29.960 --> 05:37.400
And so basically, so maybe social signal understanding is something like expanding the dimension

05:37.400 --> 05:40.400
from this verbal channel to non-verbal channel as well.

05:40.400 --> 05:47.080
For example, the input can be, in this case, a verbal voice and also images or motion-catcher

05:47.080 --> 05:51.960
data and machine understand the meaning of each sudden movement and maybe machine can

05:51.960 --> 05:58.000
react to the input signals by using the similar outputs, for example, verbal language or non-verbal

05:58.000 --> 05:59.000
languages.

05:59.000 --> 06:05.360
And so one of the papers that we wanted to talk about is a paper that recently won the

06:05.360 --> 06:08.200
CVPR Best Student Paper Award.

06:08.200 --> 06:10.200
And that was your total capture paper.

06:10.200 --> 06:15.880
And the full headline or the full title is Total Capture, a 3D deformation model for

06:15.880 --> 06:21.960
tracking faces, hands, and bodies, which is right in line with what you've been describing.

06:21.960 --> 06:26.120
Do you want to start kind of jump into that paper or do you want to tell us a little bit

06:26.120 --> 06:32.880
about where that paper fits into kind of the broader scope of research that you're doing

06:32.880 --> 06:34.920
for your PhD in at the studio?

06:34.920 --> 06:41.000
Well, I think by explaining the paper, maybe the concept would be a little bit more clear.

06:41.000 --> 06:48.480
So, yeah, so basically, so let's say we are given a large scale data set and now we have

06:48.480 --> 06:53.160
some consensus that we can solve some problem using machine learning or this fancy deep learning

06:53.160 --> 06:54.440
techniques, right?

06:54.440 --> 07:00.160
So we have seen such kind of success in language part and computer vision or image processing

07:00.160 --> 07:03.280
part or object recognition part.

07:03.280 --> 07:07.040
And let's say we wanted to do the similar thing for body languages.

07:07.040 --> 07:11.800
The problem is the amount of data is extremely rare in this body language.

07:11.800 --> 07:16.160
And maybe the first problem is what kind of data we can use for this purpose, for example.

07:16.160 --> 07:23.080
So machine sees the world maybe using cameras and how can understand the human's behavior.

07:23.080 --> 07:28.000
Maybe machine can use the images and videos directly for the purpose similar to human because

07:28.000 --> 07:33.840
actually we are using our two eyes which captured the scene and we can do some all these visual

07:33.840 --> 07:38.560
kind of understanding part and indeed actually we know the meaning of each other movement

07:38.560 --> 07:39.960
of humans.

07:39.960 --> 07:46.880
But using directly images or videos are extremely challenging because it actually they are actually

07:46.880 --> 07:50.720
nothing to do with humans, but actually they are some pixels and machine need to understand

07:50.720 --> 07:55.160
the meaning of all these pixels so that they can finally infer oh, there is this human

07:55.160 --> 08:00.200
here, there is this human's arm here, arm is moving in such a way, I mean solving all

08:00.200 --> 08:04.040
these problems from the image or video is extremely challenging problem.

08:04.040 --> 08:08.840
So although this is still very popular area in computer vision, using such a data set

08:08.840 --> 08:14.600
for finer output meaning understanding the social interaction is extremely challenging.

08:14.600 --> 08:19.640
So maybe another way to do that is just by collecting a lot of motion capture data because

08:19.640 --> 08:25.200
motion capture is in a 3D space and maybe there can be camera independent, illumination

08:25.200 --> 08:30.480
independent and that each motion capture data like for example 3D skeleton, this already

08:30.480 --> 08:34.880
has some semantic meaning, we know where is the arm, where is the hand, where is, I mean

08:34.880 --> 08:37.120
how they are moving in 3D space.

08:37.120 --> 08:43.680
And this is usually done with the humans are wearing specialized suits with like I've seen

08:43.680 --> 08:48.280
pictures of these things, I don't know if they're balls or lights or sensors or something

08:48.280 --> 08:49.280
like that.

08:49.280 --> 08:50.280
Yeah, exactly.

08:50.280 --> 08:55.840
The area is extremely popular in movie industry and game industry and these are main way to

08:55.840 --> 09:01.120
capture the real world human body signal so that we can put that to the virtual world.

09:01.120 --> 09:07.280
But our idea is to use such data for understanding human behavior using machine learning techniques.

09:07.280 --> 09:13.880
So to do that, we wanted to collect such kind of motion capture data a lot from natural

09:13.880 --> 09:15.240
interacting people.

09:15.240 --> 09:20.480
So the key here is as you mentioned, we really don't want to use any artificial markers

09:20.480 --> 09:25.520
or suit because they may affect the natural motion of humans.

09:25.520 --> 09:30.160
For example, if you are wearing this kind of black suit and say you have all this sensor

09:30.160 --> 09:35.880
or all this kind of markers on your fingers, maybe your motion will be very kind of non-natural.

09:35.880 --> 09:40.320
We really wanted to avoid such kind of cases and we just wanted to capture that in natural

09:40.320 --> 09:43.160
human behavior when they are very naturally interacting.

09:43.160 --> 09:46.560
That means that the method itself should be in necklace.

09:46.560 --> 09:50.360
And this necklace motion capture area is also very popular in computer vision and computer

09:50.360 --> 09:55.640
graphics because if you can do the similar motion capture with a single camera or just multiple

09:55.640 --> 10:00.640
cameras, there can be really great to provide a really reconstruction output or it can

10:00.640 --> 10:07.160
be also used for many main purpose like some action recognition, for example, or all this

10:07.160 --> 10:10.240
movie industry or game industry.

10:10.240 --> 10:15.120
And this itself is a challenging problem and this is especially challenging if there

10:15.120 --> 10:19.120
are multiple people because some part is extremely occluded.

10:19.120 --> 10:23.160
And our idea is, all right, we wanted to collect such data set first so that we can somehow

10:23.160 --> 10:26.320
tackle the final problem we are very interested in.

10:26.320 --> 10:32.480
But solving this problem itself is challenging then why not using a very kind of a good system

10:32.480 --> 10:35.880
which can reduce the challenge of this problem.

10:35.880 --> 10:39.960
So that we can start this project and then if this is really meaningful, then actually

10:39.960 --> 10:44.600
we can expand the project to more challenging environments such as in the wild or maybe

10:44.600 --> 10:47.600
you can try to use some YouTube video for the similar purpose.

10:47.600 --> 10:52.840
So that was kind of the major motivation of our kind of the studio, all right.

10:52.840 --> 10:57.560
This problem itself is too challenging and even obtaining the data is challenging.

10:57.560 --> 11:02.480
So let's solve this problem, the initial data generation problem or human body measurement

11:02.480 --> 11:06.480
problem first so that we can actually tackle the later problem.

11:06.480 --> 11:12.080
And our total capture paper is one of the output of our measurement step.

11:12.080 --> 11:16.840
So here we are interested in interacting multiple people and we are also interested

11:16.840 --> 11:21.480
in measuring all these body signals at the same time, for example, facial expressions,

11:21.480 --> 11:26.560
finger motions and body gestures because when we are using our bodies, these parts are

11:26.560 --> 11:28.240
extremely correlated.

11:28.240 --> 11:32.160
Some specific facial expressions should be very, very correlated to hand signals and we

11:32.160 --> 11:36.800
wanted to make find, we want to find some rules humans are using when they are interacting.

11:36.800 --> 11:41.880
So catching subtle details of entire body part is extremely important, but doing that

11:41.880 --> 11:46.920
is very hard although we are using markers because of all these occlusion, self occlusion

11:46.920 --> 11:53.240
for example, human hands, human have when they're moving their hands, for example.

11:53.240 --> 11:58.080
So yeah, so our total capture paper is person in that direction and it actually showed

11:58.080 --> 12:04.920
some meaningful kind of output in measuring all these parts at the same time.

12:04.920 --> 12:11.200
One question I've got for you about this is are the humans that are in this environment?

12:11.200 --> 12:17.120
Are they just kind of naturally interacting, doing what they're doing and you're capturing

12:17.120 --> 12:23.200
that and then kind of going back and labeling it maybe from video or audio capture or is

12:23.200 --> 12:29.560
there a script that they're kind of working against and well, I'll let you answer then

12:29.560 --> 12:31.760
I may have a follow up question.

12:31.760 --> 12:38.240
Yeah, that's actually a very, very good question because although we have, we collect people

12:38.240 --> 12:44.240
who have no idea about our project, it's very hard to ask them to do some natural motion

12:44.240 --> 12:45.240
right because.

12:45.240 --> 12:46.240
Right.

12:46.240 --> 12:47.240
Because then it's not natural.

12:47.240 --> 12:48.240
Exactly.

12:48.240 --> 12:54.480
So for the purpose, actually we are very closely collaborating with psychologists and they

12:54.480 --> 12:57.680
are actually very careful about that.

12:57.680 --> 13:06.000
What we are doing currently is we build some specific social situation and we actually define

13:06.000 --> 13:13.000
some social game, which is a kind of negotiation game and this negotiation scenario where three

13:13.000 --> 13:19.440
people are negotiating each other and in this game, which we call haggling, there are two

13:19.440 --> 13:26.000
sellers and there is one buyer and the goal of this game is two sellers need to sell some

13:26.000 --> 13:32.440
competitive items to this buyer and if the seller actually successfully sells the product

13:32.440 --> 13:38.800
within a minute of time, we actually gave some bonus for them and because of this bonus,

13:38.800 --> 13:44.080
this monetary bonus, actually they can be really involved in this project because the game

13:44.080 --> 13:48.240
itself is pretty fun and actually they can additionally get this money because of the

13:48.240 --> 13:53.600
region actually we found that the motion became very natural for sure because of the

13:53.600 --> 13:58.800
special system at the very beginning when they first entered the studio, they actually

13:58.800 --> 14:03.960
see the cameras and they can, they usually see the round but we actually spend some time

14:03.960 --> 14:09.040
inside the studio so that they can be fully familiar with the system and because we don't

14:09.040 --> 14:13.240
put any kind of camera in front of their face because cameras are all this cameras are

14:13.240 --> 14:15.160
attached on the surface of the dome.

14:15.160 --> 14:19.560
So it just seems like some kind of special room and after some time, actually the room

14:19.560 --> 14:24.680
is just somehow similar to other kind of room with some special wallpaper.

14:24.680 --> 14:28.320
So we found that their motions are pretty natural and in the end, we actually did some

14:28.320 --> 14:29.320
questionnaire.

14:29.320 --> 14:33.400
It turns out the majority people completely forgot that they are inside the dome and they

14:33.400 --> 14:37.920
can fully kind of involved in this kind of social game.

14:37.920 --> 14:45.520
And so for labeling, have you perhaps in conjunction with your partners on the psychology

14:45.520 --> 14:51.360
side, like have you developed a taxonomy of gestures or some kind of labeling system

14:51.360 --> 14:59.600
or do you have a free, free description of the gestures, which I would imagine would

14:59.600 --> 15:01.440
be pretty difficult to deal with?

15:01.440 --> 15:06.720
Exactly, so that's actually a really big question about this project because let's say we

15:06.720 --> 15:13.360
can obtain all this body motion capture data from this, let's say three people's interaction.

15:13.360 --> 15:17.960
Let's say we have all this facial expression movement and this finger movement and body

15:17.960 --> 15:18.960
movement.

15:18.960 --> 15:25.840
Then what type of problem can we actually tackle given this somehow restrict social situation?

15:25.840 --> 15:31.400
Maybe we can do some type of annotation but actually annotation itself is some big question

15:31.400 --> 15:37.520
in this problem because what kind of label can you annotate in this behavior?

15:37.520 --> 15:42.800
In computer vision, for example, there is just area named action recognition, but different

15:42.800 --> 15:48.200
from object recognition, action is really hard to define because there's starting time

15:48.200 --> 15:51.520
and the time is usually really hard to define, right?

15:51.520 --> 15:56.400
And making the label, the name of each motion is also very challenging because making the

15:56.400 --> 16:00.400
name means we wanted to describe the motion using some language.

16:00.400 --> 16:08.000
The language is in a very discreet space and it's in a very low dimensional space while

16:08.000 --> 16:11.680
the expression, the signals we are using in our social communication is actually really

16:11.680 --> 16:13.200
high dimensional space.

16:13.200 --> 16:21.240
So in that sense, actually, we don't try to do any so-called kind of annotation label.

16:21.240 --> 16:25.280
What we are labeling is kind of more objective thing, for example, who is the winner of this

16:25.280 --> 16:31.600
game, who is the loser of this game, and for example, who is speaking at this moment

16:31.600 --> 16:35.840
so that we can actually be very objective about the label.

16:35.840 --> 16:37.680
And our scenario is something like this.

16:37.680 --> 16:41.840
So how can you maybe make sure that robot detection interesting is a social behavior?

16:41.840 --> 16:44.520
That is another good question we should ask.

16:44.520 --> 16:49.600
And maybe our solution for that problem is something like the following.

16:49.600 --> 16:57.320
Well, if a machine sees the world, sees the human's behavior, and if machine can predict

16:57.320 --> 17:04.760
the future motion of these people, then that can be maybe a way to define that robot has

17:04.760 --> 17:06.560
some understanding about our social behavior.

17:06.560 --> 17:12.160
More statistically, let's say we have all this data and we somehow delete some human's

17:12.160 --> 17:13.320
motion data.

17:13.320 --> 17:19.720
And can we actually predict, can robot predict the person's motion, I mean, this hidden

17:19.720 --> 17:23.800
person's motion by observing the other people's motion?

17:23.800 --> 17:26.120
This is exactly the way we are interacting, for example.

17:26.120 --> 17:30.560
When we are interacting, we observe other people's signal, and we're making some signal

17:30.560 --> 17:31.560
from our body.

17:31.560 --> 17:36.160
And the signal is sent to other people, and the other people are reacting to our signal.

17:36.160 --> 17:39.040
And actually, this is somehow the way we are doing communication.

17:39.040 --> 17:44.600
We just generate some signals using our body, send that, the signal is understood by people,

17:44.600 --> 17:49.080
and that is also, I mean, the reaction is also sent to us, and we are just exchanging

17:49.080 --> 17:51.160
some type of signals.

17:51.160 --> 17:54.360
And we wanted to do the similar thing for the machine, so machine is in the loop and

17:54.360 --> 17:59.040
machine see the world, machine decoded the social behavior with other people, understand

17:59.040 --> 18:01.720
the meaning, and somehow predict the future motion.

18:01.720 --> 18:05.800
So this is a way we define the problem at this moment, and this is exactly what we are

18:05.800 --> 18:07.280
currently working on.

18:07.280 --> 18:15.800
I'm trying to reconcile the idea that you're not doing any kind of annotation with the idea

18:15.800 --> 18:21.160
that you're able to predict motion, but I'm imagining you're not predicting the label

18:21.160 --> 18:22.160
of some motion.

18:22.160 --> 18:29.440
You're trying to predict motion itself, so where is the hand going to be at some future

18:29.440 --> 18:35.560
time based on some set of interactions as opposed to what's the name of the gesture

18:35.560 --> 18:38.400
that the person is going to do?

18:38.400 --> 18:39.400
That's exactly true.

18:39.400 --> 18:44.600
So we just use the motion capture data as the input signal for this machine learning

18:44.600 --> 18:48.920
tool, and output is also the similar types of signals.

18:48.920 --> 18:54.720
But based on the definition of the problem, we can consider different scenarios, but one

18:54.720 --> 19:01.560
kind of scenario we are considering is the input is the other people's body motion,

19:01.560 --> 19:07.520
which is the motion capture itself, the signal itself, and the output is the target person's

19:07.520 --> 19:08.920
future motion.

19:08.920 --> 19:15.200
For example, if somebody is speaking something, maybe our target person can be nodding, which

19:15.200 --> 19:21.360
is synchronized to other people's behavior, or our target person would be maybe laughing,

19:21.360 --> 19:26.400
or moving in a specific way, and actually can we predict such kind of future motion of

19:26.400 --> 19:32.040
the target person given other people's body behavior?

19:32.040 --> 19:33.040
And so does it work?

19:33.040 --> 19:34.040
How?

19:34.040 --> 19:40.160
It sounds like a really interesting problem formulation, and tell us about the system and

19:40.160 --> 19:44.640
how well it performs, and then walk us through kind of how you build it.

19:44.640 --> 19:51.760
Well, so that is something we are currently working on, and for sure to this problem,

19:51.760 --> 19:56.020
we really need a lot of data, because human's behavior would be very different, given the

19:56.020 --> 19:57.020
same situation, right?

19:57.020 --> 20:02.680
That would be maybe very related to our culture or personality, so given the same signal

20:02.680 --> 20:06.640
actually human's behavior would be very different, so basically it's multi-model, very good

20:06.640 --> 20:11.880
is this multi-model issue, and our data set is never sufficient for the purpose.

20:11.880 --> 20:15.800
So we really need to narrow down the scope at this moment, so that we can tackle the initial

20:15.800 --> 20:20.040
problem, and you can consider this simple problem first, for example, unless that we are

20:20.040 --> 20:25.000
always considering three people's behavior interactions, and the input is two sellers'

20:25.000 --> 20:26.000
body behavior.

20:26.000 --> 20:31.640
So we have this skeleton, 3D skeleton kind of motion capture data of two people, two sellers,

20:31.640 --> 20:35.440
and those are the input of our, for example, neural network architecture.

20:35.440 --> 20:39.960
And can we actually guess the gase direction of our buyer?

20:39.960 --> 20:44.520
So you can simply imagine that if somebody is speaking, if all that we don't have any

20:44.520 --> 20:49.960
verbal kind of signals, if somebody is speaking there, if it is some speaking specific kind

20:49.960 --> 20:56.400
of body behavior, and probably the buyer's gaze will be on this speaker, right?

20:56.400 --> 21:02.080
This is some very simple kind of scenario we can imagine.

21:02.080 --> 21:07.000
So this is just one channel, just estimating the gase direction of the target person.

21:07.000 --> 21:11.640
Can we increase the dimension, for example, what about the facial expression?

21:11.640 --> 21:16.400
Facial expression is indeed some 3D face key point, or we can just simply imagine the

21:16.400 --> 21:20.800
mesh itself, so we have all these vertexes moving 3D meshes.

21:20.800 --> 21:24.520
And this itself is the output of the network.

21:24.520 --> 21:29.600
And can you actually predict the target person who is the seller, a buyer in this case,

21:29.600 --> 21:33.640
of the buyer's facial expression based on the other people's kind of behavior?

21:33.640 --> 21:40.200
Or another case is maybe location, where is the location of our target person?

21:40.200 --> 21:44.640
Because basically humans try to have some distance among each other.

21:44.640 --> 21:48.120
And this distance is also somehow trained throughout our life.

21:48.120 --> 21:51.480
And we actually maintain some specific distance when you are communicating.

21:51.480 --> 21:56.440
So actually, machine can predict the distance of the target person and orientation of the

21:56.440 --> 21:57.440
person.

21:57.440 --> 22:01.320
So this is something we can imagine as a very low dimension signal.

22:01.320 --> 22:04.080
And we can actually consider higher, higher dimension signal, for example, can you actually

22:04.080 --> 22:10.000
predict the target person's skeletal movement, or long-term movement, or hand gestures.

22:10.000 --> 22:14.840
And at this moment, all this part is still remains in a very challenging problem, because

22:14.840 --> 22:17.800
we don't have any specific output in this case.

22:17.800 --> 22:21.800
But this is magic focused on our purpose, our motivation.

22:21.800 --> 22:22.800
Got it.

22:22.800 --> 22:34.320
So the big picture is understanding how to predict from these 3D motion capture models,

22:34.320 --> 22:39.680
human behavior in interactions, so that ultimately machines could better understand

22:39.680 --> 22:44.400
human behavior and make predictions based on it.

22:44.400 --> 22:52.040
The longer-term goal to get you there is being able to predict on a, say, pixel-by-pixel

22:52.040 --> 22:59.680
basis what the future state of one of these participants' body position is.

22:59.680 --> 23:07.440
But the intermediate steps or the near-term challenges, and that's really, I think you

23:07.440 --> 23:11.680
sense that that was my, you know, what prompted me to ask, is it working yet?

23:11.680 --> 23:14.000
Yeah, that sounds like a huge problem, and it is.

23:14.000 --> 23:19.480
So the way you're tackling that is these intermediate problems of, can we track the gaze

23:19.480 --> 23:20.480
direction?

23:20.480 --> 23:23.520
Can we track the facial expression?

23:23.520 --> 23:25.880
Can we track hand position, things like that?

23:25.880 --> 23:26.880
Exactly.

23:26.880 --> 23:33.160
So basically, our current kind of problem definition is, we want to make a machine to understand

23:33.160 --> 23:38.040
the human's behavior, and the way to do that is making some system which can predict

23:38.040 --> 23:40.480
the future motion of the target person.

23:40.480 --> 23:45.440
And there, we can imagine some simple good examples and application for this, for example,

23:45.440 --> 23:55.360
let's say we have this AI, which kind of Amazon Alexa or Google Home kind of AI system,

23:55.360 --> 24:02.080
which is nowadays just using verbal language, for example, audio or speaker.

24:02.080 --> 24:05.440
But let's say this small machine has video as well.

24:05.440 --> 24:10.760
So it can capture the scene, the human's behavior using camera, and actually it can display

24:10.760 --> 24:13.840
some behavior using their own display.

24:13.840 --> 24:18.120
I think this can be really interesting because now somehow they can understand the meaning

24:18.120 --> 24:21.120
of our BID behavior, and they can react with that.

24:21.120 --> 24:27.800
You can also consider many interesting AI systems or robot exist, and for example, if there

24:27.800 --> 24:33.960
is a toy robot, which is actually interacting with a child, and if the machine can understand

24:33.960 --> 24:40.120
the meaning of the child's body movement, basically let's say the baby cannot speak at

24:40.120 --> 24:43.400
all, but it can actually make some facial expression and so on and so forth.

24:43.400 --> 24:47.920
But still, the machine can understand this because human can do that, mother and father,

24:47.920 --> 24:51.600
they actually knows the meaning of some specific movement of this child.

24:51.600 --> 24:54.280
So this can be also interesting application.

24:54.280 --> 25:02.240
Also you can imagine some application in medical area, for example, we have all these surveillance

25:02.240 --> 25:08.440
systems, and we need to maybe monitor the elderly people's body behavior so that we can somehow

25:08.440 --> 25:13.960
identify some unusual kind of behavior.

25:13.960 --> 25:19.760
For example, in this case, all this measurement kind of skills and techniques and this understanding

25:19.760 --> 25:26.440
that their behavior would be extremely important kind of techniques for these applications.

25:26.440 --> 25:32.520
And so in the total capture paper you're presenting is a deformation model for tracking

25:32.520 --> 25:34.520
these face-hand and body positions.

25:34.520 --> 25:37.480
What do you mean by deformation model?

25:37.480 --> 25:45.800
So basically we can do some 3D reconstruction without multiple cameras because connect

25:45.800 --> 25:50.520
can do some type of thing or that camera can do some type of 3D reconstruction.

25:50.520 --> 25:57.120
Here deformation model basically means the human's behavior is parameterized by some

25:57.120 --> 26:02.960
limited amount of parameters and this parameter represent body motions.

26:02.960 --> 26:07.800
So body motion means each joint angle is one parameter for each joint.

26:07.800 --> 26:13.320
So we can consider some number of joint and each joint let's say have three-dimensional

26:13.320 --> 26:14.320
rotation vector.

26:14.320 --> 26:17.840
So that is some parameterization for body model.

26:17.840 --> 26:22.200
And also we can do some parameterization for shape deformation.

26:22.200 --> 26:28.520
So we have let's say 10 parameters and by just changing these parameters we can represent

26:28.520 --> 26:31.760
the different size and shape of the people.

26:31.760 --> 26:38.680
So without this parameterization actually reconstructing 3D humans are human is more challenging

26:38.680 --> 26:44.560
because maybe we need to reconstruct all these vertex, all these 3D point of all this

26:44.560 --> 26:49.040
entire body and that dimension is extremely large.

26:49.040 --> 26:54.320
For example let's say we want to reconstruct some mesh model for each individual and

26:54.320 --> 26:58.840
let's say this mesh model has let's say 10,000 vertexes.

26:58.840 --> 27:04.040
The basically we need to estimate the location of these 10,000 vertexes for each time instance.

27:04.040 --> 27:09.360
But the interesting thing is our surface is extremely correlated each other.

27:09.360 --> 27:12.480
So our vertex is not moving arbitrarily.

27:12.480 --> 27:17.440
If we know some location of one part we can easily guess at the location because they

27:17.440 --> 27:24.480
are somehow very, you know, in a low dimensional kind of space in the end.

27:24.480 --> 27:31.080
So this parameterization is basically to parameterize all these variations of human body motions

27:31.080 --> 27:35.600
and body shape in a low dimensional space but still we wanted to have some sufficient

27:35.600 --> 27:41.920
expressive power to express all these body behavior and the shape deformation.

27:41.920 --> 27:47.920
And our total model capture is basically so this deformation model is actually really

27:47.920 --> 27:55.000
popular in computer vision and computer graphics and there it is really popular as deformation

27:55.000 --> 27:57.320
model for body and faces.

27:57.320 --> 28:02.360
Our major contribution of our paper is we build a model which can actually include all

28:02.360 --> 28:07.880
this part together including body deformation, body motion, facial deformation, facial expression

28:07.880 --> 28:11.080
and finger motion and finger deformation and so on.

28:11.080 --> 28:18.680
And when you say you have built a unified model in what way is it unified?

28:18.680 --> 28:24.000
Is it, you know, have you built some ensemble that does each of the things well or have

28:24.000 --> 28:30.720
you kind of abstracted away from the individual parts of the body that you're modeling and

28:30.720 --> 28:38.040
really trained a model that can given any of those inputs, producer and output?

28:38.040 --> 28:41.960
So in this paper actually we defined two different models.

28:41.960 --> 28:47.400
The first model is more related to some independent model.

28:47.400 --> 28:53.200
For example, we have independent facial model and body model and hand model and we somehow

28:53.200 --> 28:56.960
consolidate together by attaching each other explicitly.

28:56.960 --> 29:02.200
So this model is called Frankenstein or Frank because the way we built this system is

29:02.200 --> 29:04.920
similar to this Frankenstein's model.

29:04.920 --> 29:12.120
But yeah, the problem is all part have individual parameterization so sometimes they are not

29:12.120 --> 29:13.880
consistent each other.

29:13.880 --> 29:18.520
The final model we built which we call Adam is actually has single parameter space for

29:18.520 --> 29:21.360
entire shape space.

29:21.360 --> 29:27.480
So if we change one parameter of this space, a phase by the, a phase by the fingers are

29:27.480 --> 29:32.680
changing together and basically defining this low dimensional deformation space means

29:32.680 --> 29:37.920
we want to find out some correlation of different part so that we can reduce the dimension

29:37.920 --> 29:38.920
of this space.

29:38.920 --> 29:40.440
That's the key.

29:40.440 --> 29:46.640
And since we built this model by collecting all this 3D reconstruction of body phase and

29:46.640 --> 29:51.400
hand together, actually when we built this model in a low dimensional space, the model

29:51.400 --> 29:54.800
somehow knows the correlation across bodies.

29:54.800 --> 29:59.600
For example, if somebody has really big maybe face, maybe their hands would be also big

29:59.600 --> 30:00.600
as well.

30:00.600 --> 30:06.280
And by somehow merging this deformation space together, someone we can reduce the space.

30:06.280 --> 30:09.160
So the final model actually has such kind of ability.

30:09.160 --> 30:14.360
So it has some amount of deformation space which can control all this body face hands

30:14.360 --> 30:15.360
together.

30:15.360 --> 30:21.360
And also all this motion on body motion space is in a single skeleton space.

30:21.360 --> 30:29.040
And that means so hand cannot be apart from the body and we know they are somehow linked

30:29.040 --> 30:34.160
together by a skeleton, which is quite important for a computer graphics area, for example,

30:34.160 --> 30:35.800
retargeting zone.

30:35.800 --> 30:40.040
So yeah, that's the meaning of all this total body motion catcher and deformation model

30:40.040 --> 30:41.200
space.

30:41.200 --> 30:48.960
And so is the idea that you were expressing it with the dimensionality reduction with

30:48.960 --> 30:57.800
the atom model that with the Frankenstein model, these parts are, the body parts are more

30:57.800 --> 30:59.280
closely related.

30:59.280 --> 31:07.280
We understand that if as you were describing earlier, if we create a mesh model of the

31:07.280 --> 31:14.560
face, if the vertex and the cheek moves, then the vertex and the eyes are probably going

31:14.560 --> 31:15.560
to move.

31:15.560 --> 31:19.720
Like with these relationships are relatively easy to understand.

31:19.720 --> 31:22.600
But on this atom model, you've got the single parameter space.

31:22.600 --> 31:27.440
And so moving a parameter may have implications in multiple places.

31:27.440 --> 31:35.120
And so it's more challenging and requires a different approach than you might take in

31:35.120 --> 31:40.960
reducing the parameter space on the Frankenstein side or on the ensemble side.

31:40.960 --> 31:43.480
Did I capture that correctly?

31:43.480 --> 31:45.440
Yeah, I am mostly.

31:45.440 --> 31:51.080
So basically the vision why we built this Frankenstein model is actually to build this

31:51.080 --> 31:52.080
atom model.

31:52.080 --> 31:57.800
So for example, for dimension reduction, we simply use, we can simply use PCA kind of method.

31:57.800 --> 32:02.640
But the input of this PCA model is basically the 3D mesh model of many people.

32:02.640 --> 32:05.640
So different shape and different body parts.

32:05.640 --> 32:11.880
So they actually, this PCA can learn the variation space of these humans of deformation

32:11.880 --> 32:13.560
and motion changes.

32:13.560 --> 32:19.600
But generating such input that the mesh data, which include all these details of face by

32:19.600 --> 32:25.120
the enhance, I mean, making such reconstruction is surface challenging, although we use many

32:25.120 --> 32:32.000
many cameras because we are interested in subtle movement of facial expression.

32:32.000 --> 32:35.200
And we are also interested in this subtle movement of fingers.

32:35.200 --> 32:38.560
And at the same time, we are interested in this large movement of bodies.

32:38.560 --> 32:43.400
So how to place the camera is actually a big problem because if we want to capture the

32:43.400 --> 32:45.880
face, the camera should be very close to the face.

32:45.880 --> 32:50.440
If we want to capture the detailed hand, maybe people wanted to use the camera, which is

32:50.440 --> 32:52.360
very close to the finger.

32:52.360 --> 32:57.720
So doing this reconstruction to build the AIDA model, the reconstruction is surface challenging.

32:57.720 --> 33:02.440
So we were only to make some intermediate status by using some already available 3D mesh

33:02.440 --> 33:08.120
model, a deformation model, and that's the main kind of motivation of this Frank model,

33:08.120 --> 33:11.840
which is easier to build without doing any PCA kind of learning.

33:11.840 --> 33:16.880
And after we have this model, actually, we use this model to reconstruct all these details

33:16.880 --> 33:18.040
together.

33:18.040 --> 33:23.840
And then finally, we reconstruct like 100 people's shape deformations, shapes and body motions.

33:23.840 --> 33:28.400
And then we just put all this to this PCA kind of tool so that we can actually learn the

33:28.400 --> 33:31.400
deformation space.

33:31.400 --> 33:34.520
And there are just actually two big regions of doing that.

33:34.520 --> 33:41.000
For example, if we build this deformation model for entire body part, actually we can use

33:41.000 --> 33:46.080
that in the more challenging situation, for example, now let's say we don't have this

33:46.080 --> 33:47.520
motif system.

33:47.520 --> 33:53.240
And now we have this single image or videos from maybe YouTube or Internet.

33:53.240 --> 33:59.320
And let's say our goal is similarly to reconstruct this 3D body behavior by the motion of the

33:59.320 --> 34:00.320
person.

34:00.320 --> 34:03.880
This is obviously extremely challenging problem because we now have single view and we

34:03.880 --> 34:07.480
wanted to get this 3D motion capture data.

34:07.480 --> 34:13.720
But since we now we have some model, which restricts the surface to this low dimensional

34:13.720 --> 34:18.240
space than the original high dimensional space, actually, the problem can be much easier

34:18.240 --> 34:22.960
because now what we need to do is just estimating the parameter of this model, which can express

34:22.960 --> 34:25.360
our measurements, which is the image.

34:25.360 --> 34:31.000
So the major kind of, I mean, the main motivation of research at this total capture is after

34:31.000 --> 34:36.600
we build this model, now we can use this model to do this motion capture face by the enhanced

34:36.600 --> 34:41.480
motion capture in this in the wild situation, so that we can collect more and more data

34:41.480 --> 34:43.160
in this challenge situation.

34:43.160 --> 34:48.040
So that is the major kind of motivation of building this a parametry model.

34:48.040 --> 34:52.800
And another big reason is once we have this parametry model, actually, the data

34:52.800 --> 34:54.280
structure can be very simple.

34:54.280 --> 35:00.160
So instead of having the same mesh structure, which has some arbitrary number of vertices,

35:00.160 --> 35:06.000
now we have some fixed amount of parameter to express the particular motion of the person.

35:06.000 --> 35:11.000
So now we can consider this parameters as the input of our machine learning tools, those

35:11.000 --> 35:12.600
that can be the input.

35:12.600 --> 35:16.560
So other people's motion is abstracted as a parameter.

35:16.560 --> 35:21.240
And then the machine learning tool produces some parameters as the output, which is actually

35:21.240 --> 35:24.280
presenting some motion of the target person.

35:24.280 --> 35:31.680
Can you comment at all about the size of the parameter space relative to the size of

35:31.680 --> 35:37.920
your typical 3D vectorized or mesh type of model?

35:37.920 --> 35:48.280
Well, I don't remember the exact number, but the rough number is we have 21 joint for

35:48.280 --> 35:54.880
the body, and each hand has 21 joint, and each joint has three dimension space.

35:54.880 --> 36:01.040
So 21, 21, and 21, and that is 63, and each joint has three dimensions.

36:01.040 --> 36:07.080
So that is for the motion to represent the body motion.

36:07.080 --> 36:10.520
And we can also have facial expression.

36:10.520 --> 36:17.400
And facial expression is not explained by the sculptor movement that is explained by another

36:17.400 --> 36:22.000
parameterization, which is expressed in the movement of this mesh structure.

36:22.000 --> 36:23.760
And that has 100 dimensions.

36:23.760 --> 36:32.480
So 100 dimension for facial expression, and about 200 dimension for body motion.

36:32.480 --> 36:35.280
And we have about 40 dimension to represent this shape.

36:35.280 --> 36:40.640
So by changing this 40 dimension, the shape, the size, and the shape of the body, bodies

36:40.640 --> 36:41.640
are changing.

36:41.640 --> 36:49.040
But it's more or less I cannot sum them up now, but about 400 to 500 dimension to represent

36:49.040 --> 36:52.440
the human's behavior, including face and body and hands.

36:52.440 --> 36:57.040
But the original dimension can be probably, if we don't have such kind of model, let's

36:57.040 --> 37:00.560
say we wanted to do the similar thing by a mesh structure.

37:00.560 --> 37:06.000
And then the mesh we are using is around maybe 10k of vertexes.

37:06.000 --> 37:11.200
So each vertex has three dimension, and that is more than about 30k dimension.

37:11.200 --> 37:16.920
So we reduced the 30k to this 500 dimension to represent a similar thing.

37:16.920 --> 37:29.360
When you look at the skeletal model, plus the face, you have several 100 or so, 120,

37:29.360 --> 37:35.800
130, then you said you can reduce that down to 40, and then you said, then you started

37:35.800 --> 37:41.640
saying 400, 500, and I didn't catch where the 400, 500 came from.

37:41.640 --> 37:44.040
Okay, let me try to explain this again.

37:44.040 --> 37:51.080
So if we just consider skeletons, which is kind of a stick figure, then we forget about

37:51.080 --> 37:53.400
all this surface movement.

37:53.400 --> 37:58.680
So we can consider the joint location for the body, joint location for the finger, and

37:58.680 --> 38:01.960
some surface key point movement of the face.

38:01.960 --> 38:06.680
So each point is a three dimension space, and we can maybe consider all this number of

38:06.680 --> 38:08.480
3D key point.

38:08.480 --> 38:12.960
So that is our way to represent human body, but let's say now we are more interested

38:12.960 --> 38:16.200
in the subtle details of the surface.

38:16.200 --> 38:22.400
That means we wanted to get the mesh as the input and output of our machine line tool.

38:22.400 --> 38:28.960
And in this case, now instead of just a handful number of stick figure joint location,

38:28.960 --> 38:32.760
now we consider all these vertex locations of the surface.

38:32.760 --> 38:41.280
So that is around, let's say, 40K dimension, because to represent some details, we need

38:41.280 --> 38:43.160
many many vertices.

38:43.160 --> 38:50.920
So the way to reduce this is instead of having independent vertex separately, now we already

38:50.920 --> 38:57.080
learned the correlation of all this vertex movement, and we can reduce the space by using

38:57.080 --> 39:01.280
the motion space and shape deformation space.

39:01.280 --> 39:07.440
So motion space means we just keep the similar motion parameter for each joint, which is

39:07.440 --> 39:13.920
similar to stick figure, and shape deformation space is actually catching the variation of

39:13.920 --> 39:18.680
the surface of the given fixed body motion.

39:18.680 --> 39:24.920
So by changing this deformation space, actually a surface shape can be changing, and by changing

39:24.920 --> 39:29.440
this motion space, human can behave some body motion.

39:29.440 --> 39:37.280
So this deformation, shape deformation space is about 40K, sorry, about 40 dimension,

39:37.280 --> 39:41.600
which is really extremely low dimension, because humans are these are very, very correlated

39:41.600 --> 39:42.920
in this case.

39:42.920 --> 39:46.520
And for motion space, we have three degree of freedom for each joint.

39:46.520 --> 39:54.640
And we have about 60 or 70 joint, so that is about 200 dimension to represent body motion.

39:54.640 --> 39:59.880
And also we have about about 100 kind of dimension to expect facial expressions.

39:59.880 --> 40:04.200
What you're able to represent with this lowest dimension, the lowest dimension of vector

40:04.200 --> 40:12.640
is 40, is not just the stick figure, the body positioning, but also the surface as well.

40:12.640 --> 40:13.640
Is that correct?

40:13.640 --> 40:14.640
Exactly.

40:14.640 --> 40:19.520
So we have a function to convert this parameters to the original mesh space.

40:19.520 --> 40:22.320
The function is somehow fixed function.

40:22.320 --> 40:27.280
So given this parameter space, parameters we obtain, we can convert that to this mesh

40:27.280 --> 40:28.280
space.

40:28.280 --> 40:31.720
So instead of keeping the mesh space, we can just keep the parameters.

40:31.720 --> 40:38.600
The problem obviously here is the expressive power of this model is limited.

40:38.600 --> 40:47.040
So for example, usually in nowadays, this model can express some kind of some body shape

40:47.040 --> 40:52.200
with minimum clothing, because clothing modeling is clothing is extremely challenging,

40:52.200 --> 40:54.480
because of all these variations we have.

40:54.480 --> 41:01.160
So usually when we build this model, the input data is limited to somehow subject with minimum

41:01.160 --> 41:02.480
clothing.

41:02.480 --> 41:07.680
Because of the vision, this model can express only the mesh with minimum clothing.

41:07.680 --> 41:12.360
For example, if you generate any arbitrary body shape and motion from this parameter,

41:12.360 --> 41:18.320
the output is almost always this kind of naked body with minimum clothing.

41:18.320 --> 41:26.080
You've reduced the dimension down to this 40 and then you're able to use this 40 to produce.

41:26.080 --> 41:28.480
You said you refer to it as a fixed function.

41:28.480 --> 41:33.640
Is this a function that's learned through machine learning model or what does that function

41:33.640 --> 41:36.360
come from?

41:36.360 --> 41:43.560
This function is somehow actually a very common function in graphics area.

41:43.560 --> 41:49.280
So actually from the motion capture, if in graphics area, from the motion capture data,

41:49.280 --> 41:55.200
which is basically the three degree of freedom for each joint, we can manipulate, we can

41:55.200 --> 41:59.840
animate the 3D character and actually the 3D character output is mesh, although the

41:59.840 --> 42:03.520
parameter input is this motion parameter.

42:03.520 --> 42:08.280
Similarly, we just use the similar function, which is called linear blending skinning.

42:08.280 --> 42:13.800
So we have some mapping between the vertex movement and the skeleton movement and we are

42:13.800 --> 42:15.920
just manipulating the skeleton.

42:15.920 --> 42:20.160
And then as an output, we can get the location of each vertex of the mesh.

42:20.160 --> 42:26.840
And so you've developed this representation of the body.

42:26.840 --> 42:29.840
How does this tie into the larger goal?

42:29.840 --> 42:31.120
Yeah, exactly.

42:31.120 --> 42:39.160
So first, we wanted to parameterize all this face and body and hands movement in a common

42:39.160 --> 42:42.200
format, a common motion capture format.

42:42.200 --> 42:46.760
That was one of the original building this model.

42:46.760 --> 42:52.840
And as I mentioned, the final goal is actually to use this model to get a similar motion capture

42:52.840 --> 42:54.320
output in the YouTube video.

42:54.320 --> 42:59.280
So actually, we are working on this project and now the goal is instead of having multiple

42:59.280 --> 43:04.200
view, we will just have a single view, single YouTube video, and we wanted to get the motion

43:04.200 --> 43:05.960
capture of the target person.

43:05.960 --> 43:11.400
And motion capture output has all this component facing body and fingers together.

43:11.400 --> 43:16.720
This is ongoing research and we actually get some meaningful result at this moment and

43:16.720 --> 43:21.200
which we plan to submit that to a conference in the end.

43:21.200 --> 43:26.600
So yeah, so basically, we wanted to convert all of our collected data to these types so

43:26.600 --> 43:30.440
that we can actually put this to our understanding part.

43:30.440 --> 43:32.680
That is the current one at this moment.

43:32.680 --> 43:34.080
We are working on.

43:34.080 --> 43:43.000
And is PCA the primary place that machine learning is used in this project or are there

43:43.000 --> 43:46.720
other places where you're building models as well?

43:46.720 --> 43:52.600
Well, actually, this project is a little bit far from the popular machine learning area.

43:52.600 --> 43:57.640
This is more like a three construction.

43:57.640 --> 44:05.720
In this paper, what we show is usually this finger reconstruction and facial of face reconstruction

44:05.720 --> 44:09.720
battery constructions are considered as separate problems.

44:09.720 --> 44:16.080
But we actually somehow demonstrate that we can capture all these components together

44:16.080 --> 44:18.520
without using any markers.

44:18.520 --> 44:26.360
And to do that, maybe one of the key kind of contribution is we use this to the key point

44:26.360 --> 44:29.360
estimation method for each individual camera.

44:29.360 --> 44:33.600
And indeed, we combine them together in this 3D space using a multi-view system.

44:33.600 --> 44:40.080
And in the end, this 3D key point we constructed by using this 2D key point detector.

44:40.080 --> 44:44.640
It turns out they are working extremely well in this challenging problem and we just showed

44:44.640 --> 44:48.840
the result as a total battery reconstruction result.

44:48.840 --> 44:53.920
So in that sense, probably, it would be good to share the 2D detection, post detection

44:53.920 --> 44:56.800
problem our lab is working on.

44:56.800 --> 44:59.360
And that is the open pose work?

44:59.360 --> 45:00.360
Exactly.

45:00.360 --> 45:01.360
Yeah, exactly.

45:01.360 --> 45:05.120
So this 2D body pose detection area is a very popular area.

45:05.120 --> 45:11.080
And one of the most successful areas in computer vision in recent years.

45:11.080 --> 45:18.000
So now we have really great algorithms which can detect this human's body key point, multiple

45:18.000 --> 45:22.680
people's body key point in YouTube video and single image.

45:22.680 --> 45:24.800
And open pose is a version of that.

45:24.800 --> 45:31.520
And one good thing about open pose is this is providing all these facial key point and

45:31.520 --> 45:37.480
body key point and finger to the key point in a single framework in real time.

45:37.480 --> 45:45.160
And multiple papers our lab has been presented is included in this work.

45:45.160 --> 45:51.200
And actually, this is closely related to our panel text studio project because we are

45:51.200 --> 45:56.520
using this detector to reconstruct all the total motion in the end.

45:56.520 --> 46:04.320
And at the same time, we use our system to generate new annotations set to train this

46:04.320 --> 46:06.080
2D detector.

46:06.080 --> 46:15.160
So this is a very interesting idea because usually this multivit system has been used to reconstruct

46:15.160 --> 46:16.160
3D.

46:16.160 --> 46:24.600
But now we believe this multivit system can be a way to generate some 2D annotations for

46:24.600 --> 46:29.320
2D detector because sometimes this generating 2D detector itself can be challenging.

46:29.320 --> 46:35.520
For example, let's say this is one paper we published in last CVPR and the goal is to

46:35.520 --> 46:40.360
make this 2D hand key point detector which is now the part of the open pose.

46:40.360 --> 46:41.560
And the main idea is very simple.

46:41.560 --> 46:46.720
We just wanted to make some data set for 2D hand key point detector.

46:46.720 --> 46:51.440
But just annotating this 2D hand key point is a challenging problem because of all these

46:51.440 --> 46:53.040
side focal illusions.

46:53.040 --> 46:59.400
So at the beginning, we hired some annotators and we asked them to annotate this 2D key point

46:59.400 --> 47:01.560
given some to the image.

47:01.560 --> 47:07.480
But it turns out usually these fingers are glued to each other and just clicking this

47:07.480 --> 47:09.560
finger key point is a challenging problem.

47:09.560 --> 47:15.520
And our idea is if we can capture some data in this multivit system, since we have 500

47:15.520 --> 47:21.160
views, let's say we wanted to capture some finger movement, we wanted to annotate some

47:21.160 --> 47:25.920
key point of some finger movement, then since we have this many views, we can easily

47:25.920 --> 47:31.880
find some good view point to easily annotate the target finger part.

47:31.880 --> 47:38.880
More importantly, if we can have some 3D reconstruction of the target hand, if we can

47:38.880 --> 47:45.880
project this 3D hand to 500 views, each individual view point can have a new annotation by

47:45.880 --> 47:48.640
projecting this 3D hand to this single view.

47:48.640 --> 47:56.680
And we just used this annotation for a neural network architecture to train the 2D detector.

47:56.680 --> 48:01.600
In terms of this, this is extremely successful in making this 2D key point detector and

48:01.600 --> 48:03.600
the key point detector.

48:03.600 --> 48:10.360
So in a way that is, it's kind of a specialized mechanism of data augmentation.

48:10.360 --> 48:12.360
Exactly, exactly.

48:12.360 --> 48:20.560
But in this case, we can use this multi-view supervision so that we can filter out some

48:20.560 --> 48:21.560
noise outputs.

48:21.560 --> 48:27.800
For example, since the original target hand should be in the 3D space, and our image

48:27.800 --> 48:35.120
is just capture of the 3D space in this 2D space, so ideally, if we can generate some

48:35.120 --> 48:39.520
rays from each individual camera, they should intersect in this 3D space, which is the

48:39.520 --> 48:42.000
original 3D joint location.

48:42.000 --> 48:46.040
If they are not, then we can simply say that maybe the detection or 2D measurement is

48:46.040 --> 48:47.040
wrong.

48:47.040 --> 48:51.640
So we can easily filter out all these noisy cases using this multi-view supervision.

48:51.640 --> 48:55.240
And that is a key to make this data augmentation.

48:55.240 --> 48:56.240
Yeah.

48:56.240 --> 48:57.240
Awesome.

48:57.240 --> 49:01.280
Well, Han, thanks so much for taking the time to share this with us.

49:01.280 --> 49:07.560
Are there any final thoughts that you'd like to share to kind of close us out?

49:07.560 --> 49:16.320
Yeah, so currently, we are trying to release entire of our dataset, regenerated from our

49:16.320 --> 49:17.480
panoptic studio.

49:17.480 --> 49:24.120
So people can easily download 500 videos for the same input.

49:24.120 --> 49:31.440
And also, they can get all these 3D annotations regenerated, for example, 3D key point and also

49:31.440 --> 49:38.360
depth information or 3D point cloud for the same target scene, so that we believe people

49:38.360 --> 49:42.600
can use this kind of data to many interesting computer vision and machine learning problem

49:42.600 --> 49:46.520
because people now have some 2D image input.

49:46.520 --> 49:49.400
And they can also have corresponding 3D annotations.

49:49.400 --> 49:53.920
And that can be maybe interestingly trained in many interesting machine learning techniques.

49:53.920 --> 49:58.360
For example, given the single image, can we generate 3D skeleton, given the single

49:58.360 --> 50:03.840
image, can we generate depth data or 3D point cloud because we all captured this all different

50:03.840 --> 50:09.720
sensors output using hardware synchronization and the calibration.

50:09.720 --> 50:14.720
So this can be easily kind of maybe not easy, but this can be in the end used for many interesting

50:14.720 --> 50:15.720
machine learning problems.

50:15.720 --> 50:16.720
Oh, wow.

50:16.720 --> 50:19.920
How many images or videos are you including?

50:19.920 --> 50:20.920
Wow.

50:20.920 --> 50:26.800
Not how many unique kind of scenarios or captures are you including?

50:26.800 --> 50:31.920
So the original data is about 10 hours of many different situations.

50:31.920 --> 50:39.400
So we have captured some musical instruments like playing piano, cello, and so on.

50:39.400 --> 50:46.080
And we also have many social motion capture results, for example, 3D point tracking.

50:46.080 --> 50:51.880
Also we have captured some very simple, example range of motion, which can be easily maybe

50:51.880 --> 50:55.400
useful, much easier kind of problems.

50:55.400 --> 50:59.520
So yeah, and we should collect more and more data and we try to release all of them in

50:59.520 --> 51:00.520
the end.

51:00.520 --> 51:01.520
Awesome.

51:01.520 --> 51:02.520
Awesome.

51:02.520 --> 51:06.040
Well, once again, thank you so much, Tom, for joining us.

51:06.040 --> 51:07.120
Thank you for having me.

51:07.120 --> 51:08.120
Thank you.

51:08.120 --> 51:14.800
All right, everyone, that's our show for today.

51:14.800 --> 51:21.120
For more information on Hambio or any of the topics covered in this episode, visit twimolei.com

51:21.120 --> 51:23.040
slash talk slash 180.

51:23.040 --> 51:27.280
If you're a fan of the podcast, we'd like to encourage you to head over to your Apple

51:27.280 --> 51:31.840
or Google podcast app and leave us a five-star rating and review.

51:31.840 --> 51:36.320
Your reviews help inspire us to create more and better content and they help new listeners

51:36.320 --> 51:38.120
find the show.

51:38.120 --> 51:55.280
As always, thanks so much for listening and catch you next time.

