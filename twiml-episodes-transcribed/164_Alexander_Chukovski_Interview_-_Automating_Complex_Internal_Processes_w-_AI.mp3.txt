Hey, everybody. Sam here. We've got some great news to share and also a favorite
to ask. We're in the running for this year's People's Choice podcast awards in both the
People's Choice and Technology categories and we would really appreciate your support.
To nominate us, you'll just head over to twimlai.com slash nominate where we've linked to and
embedded the nomination form from the award site. There, you'll need to input your
information and create a listener nomination account. Once you get to the ballot, just find
and select this week in machine learning and AI on the nomination list for both the
Adam Curry People's Choice Award and the this week in tech technology category. As you
know, we really, really appreciate each listener and would love to share in this accomplishment
with you. Remember, that url is twimlai.com slash nominate. Feel free to hit pause and take a
moment to nominate us now.
Hello and welcome to another episode of Twimletalk, the podcast why interview interesting people, doing
interesting things in machine learning and artificial intelligence. I'm your host, Sam
Charrington.
A quick update about the upcoming Twimla Online Meetup. On Tuesday, July 17th at 5pm Pacific
time, Nick Teague will lead a discussion on the paper Quantum Machine Learning by Jacob
Diamante at all, which explores how to devise and implement concrete quantum software for
machine learning tasks. If you haven't joined our meetup yet, visit twimlai.com slash meetup
to do so. In this episode, I'm joined by Alexander
Chikovsky, director of data services at Munich Germany based career platform, Expertier.
In this podcast, we explore Alex's journey to implement machine learning at Expertier.
In particular, Alex and I discussed the Expertier NLP pipeline and how it's evolved over time
to address the company's need for greater automation and the way it processes jobs on
its platform. We also discuss Alex's work with deep learning based NLP models, including
models like VDCNN and Facebook's fast text offering, which he's particularly excited
about.
Finally, we briefly touch on some recent papers that look at transfer learning for NLP,
how Alex keeps up with academic papers in general, and a few tips for people looking
to inject ML and DL into their products or projects. And now on to the show.
All right, everyone. I am on the line with Alex Chikovsky. Alex is director of data services
at Expertier. Alex, welcome to this weekend machine learning in AI.
Hi, hi Sam. Thank you for having me. Great to be on the show.
So why don't we get started by having you tell us a little bit about your background
and how you got into machine learning and data science. You come at it from the business
angle, which is kind of an interesting starting place, huh?
Yeah, exactly. So I studied economics major team finances statistics in the beautiful
city of Munich. And then till 2014, I used to manage a data processing team at Expertier,
which I'll tell you what the company does. And the data processing team was like very
large entity of the companies, like the core team. It had about 100 people doing all kinds
of stuff related to processing job data in 12 different markets, seven different languages,
so quite a lot of complexity. And my role was to kind of manage the team, put a direction
on it, make sure that everything runs more than efficient, that the quality is good to
develop some new web based tool because our whole data processing stack was Ruby on
the Rails application developed internally. And I kind of got into machine learning by
chance. So back in 2014, beginning of 2014, we had like a local optimum of efficiency,
so we had very high cost per job, which was like our internal KPI. And we were looking
of options of how to drive this cost down. So pretty much all the operational projects
that we could do to improve this were done, the technologically speaking, the platform
was very streamlined. So somehow someone came with the idea to use support vector machines
to kind of assist the classification process of the jobs, which is like the part that takes
a lot of time. And we had a very first implementation that did not really work out very well,
it was not taken very well by team. And then still kind of interesting results. So the
management and I kind of said, okay, let's look at what this machine learning thing can
do. So we did not have any capacity internally. So no one was actually familiar with machine
learning in the company. So we talked to a few external agencies to kind of get competency
from outside and help us to build one of the first prototype that we used in the beginning.
So more or less, I really come from the business side and I got into machine learning by chance
so to say it may help folks contextualize some of what you described by digging into
expertise and what the company does and what some of the main problems that you've been
tasked to solve there are. Yeah, all right. So, um, expert here is a closed career platform
for high paid professionals, which means that the jobs that are offered on this platform
have a very high salary. So they start at above 60,000 euros or about 100,000 US dollars
in the US. And it's a closed network where recruiters can contact candidates. And the
reason why expert here has been so good at solving this problem of offering jobs to candidates
is because we have a very extensive job classification, right? So if you go on a typical job board
today and you would search for a job, so let's say I would go to one of the large job boards
and I would search for a CEO position and everyone can do this experiment. I would most likely
get all kinds of jobs that have direct text match that has something to do with reporting
to the CEO or assistant to the CEO and maybe somewhere in the fifth or in the sixth result
page, I would get like a job that really matches. So that's kind of the problem of traditional
job search, the food tech search. And expert here solved this by adding a few additional
layers on the search site. So for example, we would have something called a career level,
right, where we would put jobs in different categories like a job for someone who is out
of university or a job that is for someone who leads projects or for someone who manages
teams or manages managers that manages team and so on. So you would be able to use these
filters and kind of dig really deep into finding a job that is really interesting for you
on the one hand. But on the other hand, if you set up your profile correctly, you would
get suggestions that are very relevant for you. So expert here has a very detailed job
toxonomy, classification of jobs. So for example, in industries, we have about 630 industries.
So if you're looking for a job in consulting or in risk and restructuring consulting,
we have it. It's there and we can offer it to you, right? So this is kind of the problem
that the team was trying to solve. So putting jobs in these extensive categories, right?
You know, the jobs, the listings sourced from public postings or they sourced privately,
proprietary listings that can be, and the question is really, how much noise is in those
listings or if you're sourcing them privately, are they cleaner than you might expect from
just a crawled set of listings? Yeah, that's a good question. So back when we started
in 2014, everything was very clean because we had kind of automated this whole crawling
process. It was still not completely automatic. So people still had to trigger them, but the
content of the job, so the job text was very clean. A little bit later, once we kind of
started crawling on large scale, another problem came where we would have a text that it's
not related to the job. So back when we were building this first iteration of the solution,
we had fairly clean job descriptions, I would say.
You kind of had this point in the business where you, you know, things were going okay,
but you needed to figure out a way to kind of make the next leap in productivity and
you came across machine learning as a possible opportunity. And you said it didn't necessarily
work, but it was promising enough that you continued to invest in it. What did you
do then? What was the next step?
Yeah, so the next step was, first of all, of course, convinced management that there
is an opportunity there. So we came up with a very high, a very motivational goal for the team,
so we would like to like cut our cost by 50% and at the same time increase the output by a factor
of two, the team, the team output. And as I mentioned, in the beginning, because we didn't have
any competency, we kind of talked to a few external agencies, we gave them data sets and they were,
so to say in a competition of showing us who can do better. And one of the agencies got
quite alright results. So we said, okay, let's give it a go. I mean, the cost perspective was okay,
so there was nothing to lose. And we decided to start working with them.
And what even gave you the confidence to put that kind of an aggressive goal on the table
after an experiment that didn't necessarily prove out that you can do it? It sounds like it
wasn't a foregone conclusion that you'd be successful when you threw that out on the table.
So this goal came mostly from our CEO, so I guess that from his experience of being a business
leader, he had to set very high goals to motivate people, like me and the other people in the team.
So yeah, I mean, the initial results were not really satisfying, but they were still kind of
interesting. So we saw that the support lecture machines, even though they only took the job title
and a part of the text kind of got a few of the functions of the jobs correctly. So we said,
okay, we need a large goal here. So it's either a goal, a bigger goal home. And I think from a
purely psychological perspective, if you have such a huge goal, you're not chasing the next 10%,
it's kind of more motivating. That's for me, and this is how I would do it if I were to implement
some kind of a new machine learning process in a company that's going to change completely
the partiums. I would really set high goals. How did the process kind of evolve over time
with this? What were the steps, I guess, to achieve in that goal? Did you achieve that goal?
What were the steps to getting there? Yeah, so in order to keep people interested,
yeah, we actually, we did achieve the goal. So we got our cost curve down to 30% from what the
baseline was when we started. So this is like 70% saving. But the most interesting part was that
we tripled the amount of jobs that we had on the platform back then. And for a lot of people,
this probably doesn't sound like a lot to triple the content of jobs. But as I imagine,
the beginning expert here focuses on these high-level jobs, right? So jobs above 100,000
US dollars. And if you look at an average page of a company that has 1000 jobs, maybe 5% of them
would be relevant for this type of market. So getting to tripling the amount of jobs that we had
on the platform, we actually had to increase our processing power. So the amount of jobs that we
pushed through the platform, we call it assembly line. So if the team used to be able to do like
manually, about 50, maybe 60,000 jobs a month, today we process close to 3 million every day.
So you can like imagine the gains that we had. That's huge difference.
Yeah, that's huge.
Yeah. And so there was a significant part of the workflow previously that was based on manual
categorization. And that's all or some portion of some large portion of that. It sounds like it's
been replaced with machine categorization. Yeah, this is correct. So this was, so we kind of built,
we kind of drew a picture of our complete value chain like from how a job comes to the platform.
Every step that it needs to go through before it's like finished and processed to be served to a
customer. And we identified which are the areas that took most of the time. And I mean, it was
not surprising, but the classification was like the biggest one. So that's where we started. So
if I look at the way the team has evolved today, it's mostly the work is now kind of quality
related to work. So there's very little classification going on. Maybe for jobs where the model is
still not very sure about the classification that it provides. And so how many iterations of your
pipeline have you gone through at this point? Did you kind of settle in on the, whatever the
first thing you did with the the winning consulting company after you set up on this path or have
you iterated your pipeline a few times? I would say that up until, I mean, you can see also
including today, we are constantly iterating. So we started with, if you think about our data
pipe, it had in the beginning four services, the four classification services. So today we have
close to 30 different services that are related to these data processing steps. So not all of them
are machine learning based, but I would say probably half of them are. So the way we started was
that the agency was like very optimistic. They were great guys based in Munich. So I think the very
first thing that we started was like very classical support vector machine type of classification,
you know, back of words. And it kind of worked for, I think we started with English first.
That's actually a good moment to mention that expert year supports seven different languages.
And if we look at English, it actually, we have to look at it twice because you have US English
and British English. And the way that those two countries write their jobs description is
completely completely different. So it took us about a year and a half and hundreds of different
iterations, which I will explain exactly what this means. To come to a moment where we said,
okay, we're happy with what we have now. So let's focus on quality. We focused on quality. We saw
that there are some areas to improve. We saw that there are some opportunities to get more data
and then we said, okay, you know, so many options. So let's build a real data science department
in the company. That's kind of how the whole thing evolved. But literations looked a little bit
like this. So the agency built the first models. And as I said, we looked at the results. They were
not very impressive. I would say, I would say something in the area. So if we take, for example,
function, so expert here has 19 different functions for a job. So stuff like sales, marketing, IT,
production, manufacturing. I think in the beginning, we had like an F1 score of maybe 45,
so maybe 50%, which was not in no way production ready. And we would look at the mistakes.
And there were, of course, the traditional mistakes where you would see that the classifiers were
not able to abstract well enough. But there were also other types of mistakes where actually
the classifier did the correct job. And then once we looked at the validation, the validated job,
we saw that the job was initially classified wrong. And we would then talk to the person that
created this job. And we would like try to understand why this mistake happened. And this was like
one of the first eye openers where we found out that people have humans and stuff, feelings and
stuff. So it would something like this would happen like a person would say, hey, you know,
that day I remember my girlfriend broke up with me. So I was like in such bad mood and I didn't
really, I wasn't really careful in what I was doing. Right. So you'd have these type of cases.
We had these quality managers and they would control the quality of the work like random samples
of people that were a little bit too junior. And there were a few people that were more,
you know, they were more careful. So if you were a researcher, that's how we call the people that
were classifying the jobs. So if you're a researcher, you know, okay, so today I have person A,
who is very careful at what he or she does. And tomorrow, I have person B. So this kind of
impacts the way I also work. So this is of course a challenge because it means that the data
said that we had to work with was in no way perfect. And we actually had initially the assumption
that we have these perfect data sets, which was not the case. So this is kind of a extreme cold shower
to begin with. And so did you go back and kind of clean up all your labels or did you do something
else? Yeah, this was, yeah, this was the initial plan to kind of go through the data set, take out
the bad stuff. We had this internal hierarchy where we knew if a person was junior or more senior,
so we kind of sorted stuff out there. But it still wasn't good enough. So what we did back then
was this is kind of the moment where we said, okay, a pure machine learning solution is not going
to work out. So maybe let's think if there is a hybrid solution that we can use. So a combination
of a classifier, additional features that we would extract from the text, so like pure text
analytics. And we would try to enforce the business logic in certain cases where the model fails
because the abstraction is too complicated. And in order for people to really understand this,
we have to come back at how people like really, really read jobs. So if you have a traditional
job description, right? I'm trying to guess the career level of this job. So a human would normally
start scanning the job description for specific keywords. So if I'm trying to find out if the
job is a senior one, I would look for keywords like four years of work experience, right? Or five
years of work experience. But because you have different companies and different languages and
different company size and different cultures in the companies, the way they write their
descriptions very heterogeneous, it's very different. So it's really hard to reach an abstraction
there. And if one person writes four years of work experience and our company would write
two to five years of work experience or they would write five years of relevant work experience.
Or yeah, you get the picture. So it's kind of an extreme amount of complexity there when you try
to classify this text. And at this point, was that okay? Let's do bag of words, extract. No,
sorry, not bag of words. Let's do five grams, see what are the most common combinations of words
related to the career level. Career level was very complicated. This was like the most
complicated one to classify. And we would then identify these, so to say, key text pieces and then
we would expand them. So that in the end of the day, when you had your you had like an assemble
model, but I don't know if there is an effort, honestly. So you have a classifier that gives
some kind of prediction, then you'd try to additionally see if you have matched some specific
semantic structures in the sentences, you would also try to see if you have matched some
parts of the text that you extracted in a text analysis part. And you would also
compare a classifier on the job title and description separately. And all of these information,
it will like go into one big model that will then use the product or machines to do like the final
decision of the classification of the job. And these took a lot of iterations. I would say
we used to probably do per language per per meter more than 100 iterations,
which means like eight languages, three main parameters, 24. Yeah, that's quite a lot. I think
you get the picture. And you mean in terms of training cycles or in terms of just the
versions of the model over time. So we would clean up the data set where we find inconsistencies,
we would retrain the models, then we would extract some new pieces from the text that are somehow
relevant for the text analytic part of the model. We would do some more iterations. We would
find out, for example, that specific job categories are not abstracted that well. And there we
would do like a very very hard rule that it's enforced and has a very high score that goes into
the final model. Interesting. And how long did this process take calendar wise? Was this,
you know, over the course of a couple of years or, you know, much shorter than that?
So English, we kind of covered in three months. So we got, I mean, we said that just to get
a understanding of all our KPIs. So we said that obviously we cannot get a score of 100%.
So we would be very happy if we get something like 55 or 60% of the jobs
to be classified at least like 95% correct. So like 60% Rico and 90% precision. And in order to get
there, it took us about three months for English, US and UK, another maybe four or five months for
German because there was differences in the Swiss German and the German German. And
somewhere in the beginning of 2016, we were done with French, Spanish, Italian and Dutch.
So I would say a year and a half, maybe close to two years with fine-tuning afterwards.
And when you look at the kind of the direction that this is all going, do you see, have you
started looking at some of the deep learning-based NLP models and beddings and things like that? And do
you see that changing your general approach, like taking away the need to do this hierarchical
type of model with both the, you know, traditional NLP text analytics stuff and the
ML classifiers or do you expect that you'll always have some degree of that hanging around?
Yeah, that's a very good question. So my hope is that at some point of time we will be able to
completely drop this whole text analytics and business rules part. I mean, it doesn't take that much
time to manage them nowadays. We do a lot more retraining. But we started a very, so deep learning
was such a huge fuss in 2017. So I think in June or July 2017, my team and I decided to kind
explore deep learning for text classification because like historically speaking, and if you do
this whole academic reviews, deep learning was always praised, you know, for text classification.
So we tried quite a lot of stuff. So we tried very deep convolutional neural networks,
VDCNN. We tried HDO text. So it's like a hierarchical deep learning. They got two pretty good
numbers. I would say if we look at career level, right? So out of the box, very little fine tuning.
We got to 80% quite quickly. But what really struck me, it was somehow by chance, one of the
people from our team, Viet, he found out about fast text from Facebook. I don't know if you're
familiar with this one. That's a text classification framework from Facebook, which you can actually train
on a prem normal computer on a desktop. And fast text really blew everything away. We were
literally not building our eyes. So on average, it outperformed all deep learning frameworks,
we tried by two, three, maybe four percent. But the difference in the training time is, you know,
for fast text training on 600,000 jobs, so 6000 data points, 600,000 data points,
trying to guess the four classes for career level. Fast text takes 2.5 minutes to train on a desktop
and VDCNN takes 16 hours on a 12 gigabyte GPU. So yeah.
And what about the sample efficiency? Were there any mark differences in the amount of data you
needed to provide the different models? Well, when it comes to data, we kind of decided to go
all-in because we have millions of hand classified samples. So the really, really, the point was,
okay, just try to throw in everything in that we know that it's good and see what comes out.
Okay. So with regards to this, there were not a lot of problems. There is one specific problem
where we are kind of experimenting right now with a little something from your show, the zero shot
machine learning. So we have a specific case where we have very few examples present as data set.
So we are kind of playing around with this one. I think there is a very interesting paper from 2017.
But yeah, deep learning was not the cure for us, to be honest. So we decided to stick to fast text.
So the fast text classifier, are you currently using that in production and how much of
all of the other stuff that you had, did it replace or do you see it replacing?
So yeah, we do use it in production. So as I said, we have this one model that kind of takes the
input from all of the other predictors. And fast text is also one that gives an additional
an additional score there for jobs where we are not sure of how the general classification
would perform. And I would really like to like use it completely. But I think that from a risk
perspective, I would still like to stick to this kind of ensemble solution that we have for a while
because as I said, we invested a lot of time in creating this whole business logic and textuality
crews. I wouldn't throw them out immediately. But looking at the future, if I'm going to do
retraining, I would definitely focus the energy on the fast text part because I think that this
really works very well for jobs. I don't know what is exactly. Maybe it's the length of the text.
Chops kind of have, on average, 2,000 characters. So maybe that's why fast text works very well.
Maybe because of the number of features that it can work with. But I think we'll keep the original
model for a while still. But if I can give a tip to anyone who starts into text classification,
it would be too looking to fast text. It's really amazing. It's really amazing.
So there was recently a post by Jeremy Howard and Seb Gruber talking about some work that they
were doing in applying transfer learning to NLP. I forget what models they use. But do you have
you come across that by any chance? Yeah. I'm just looking for one of the last papers that I
decided to use recently. It's called Structural, something in the area of structural
sentence similarities, the machine for short text. I think this is kind of related to transfer
learning. Then there was a fast AI. I think they also have.
Yeah, we're probably thinking about the same one. Jeremy Howard and Sebastien Ruder.
Yeah. Yeah. Yeah. Oh my god. Yeah. So once I wrote it, I was like, wow, wow. We got to try it
out. It's in our, it's in our Gira system. It's prioritized. So the moment the team gets a little
bit of air, they're trying it. They're going to try it out because there is only this graph. It's
amazing. It's well. Yeah. So it sounds like a big part of your, I don't know, with workflows the
right word, but a big part of the way you approach this problem and kind of staying ahead of the
curve is tracking the academic research and trying stuff out as you come across stuff that looks
interesting. Yeah. That's, this is absolutely correct. We cannot try to see what comes out in
the space. It's really hard to really monitor everything. So I'm just trying to get bits and
pieces from everywhere and then try occasionally stuff out that looks promising. I mean, it's really
important to kind of see what kind of data sets are used in the papers because like from my experience,
from what I saw in the deep learning part that we did all these tests with. If someone is interested,
there is a very long presentation on my LinkedIn profile with all the tests that we did.
So these papers, right, they use the academic papers, they use like very nicely balanced data sets.
And they don't necessarily have a touch to reality, like, right? So the way we work is the data
sets that we have are like very unbalanced and they need a lot of fine tuning. So when I see
these benchmarks of the deep learning models to other models in the papers, it looks very promising
but once you try it on your own data, the picture is completely different. So I still try to be like
innovative, try stuff out whenever we have time, but we also try to stick to the kind of established
things, like fast text, for example. Is the way you've addressed the imbalance nature of your
data set? Is there anything different that you do there beyond the general stuff that we've talked
about, the, you know, applying the business rules and things like that? So for the deep learning
part, we tried, of course, oversimpling. So we did like this with and without oversimpling.
It's especially relevant for specific classifications where, you know, you don't have a lot of jobs
in the sea level position, right? So you probably have like 1000 data points instead of 1 million,
which would be in the lower career levels. So the way what we tried out and it worked out quite well
was to do, to combine classes. Probably there is some term for this in the academic world,
but we would kind of keep lower classes where we had a lot of data points in one classifier,
right? So you'll have class one, class two, class three, and then class four would be a combined
of the classes where we had very little data. So this classifier would do like the first,
the first decision, you know, is it one, two, three, or four? And four could be then
class four could be then additional three more classes. And there, for example, we find out that
FASTX really captured very well this differentiation on the high classes that the deep learning model
did not abstract, was not able to abstract. So this is kind of my experience on the in-balance
data sets. Very cool. Any other, any other experiences that you'd like to share? Oh yeah.
So for people that kind of rolling out machine learning services in the organization,
I have a few tips. So first of all, as I said, you know, do some big and bold goals,
like double something or triple something so that you can kind of try to challenge the
status quo that you have in your organization. But at the same time, you have to be, of course,
able to balance by not setting the project scope to large. Because if you set the project scope
to large, then you have a lot of unknown factors, and you have a two optimistic roadmap,
and especially if you learn working a large company, you know, politics can be kind of a bummer
for data scientists because it tends to over promise. Then once you are kind of ready to roll out
your solution, my suggestion would be to kind of identify the people in your team that are
the very, you know, motivated curious people that are interested in trying new stuff out,
and first play it out with them a little bit, and after that roll it out to the whole organization.
So I would use this kind of small core team to improve the solution as much as I can,
and I would then roll it out into the whole organization because people are generally very
skeptic by nature. We had this kind of experience. So you give this amazing solution, right? And
the person says, sees that it kind of automates his or her job, so it helps him or her a lot.
And the first thing that they try to find out is why it cannot work or how it is wrong, right?
So they don't focus on kind of improving the solution, but they kind of focus on how to convince
you that this is never going to work out. And this was a big fight in the organization. So
this was a very kind of an interesting experience. And be careful with this like first implementations
because if you, if you kind of fail there, then you're going to have the general team who
have some kind of skepticism for the whole project, and it's hard to get away from it. So my
suggestion would be to rather release something once you're like very sure that it's working well
instead of kind of releasing too early and not being good enough. And with regards to, we should
like starting with machine learning, the cloud providers from my perspective are great. So
these are industry leaders. And we see this kind of trend of having off the shelf solutions
for machine learning. So from my perspective, this is going to be the future. And I already, I mean,
I'll talk a lot about it when conferences and people tend to not believe me, but I think that
in a few years, we are going to have a lot more solutions like drag and drop your data. And
we'll figure it out for you. And here's an API, you know, and just, you know, send us everything
that you want on this API. And we'll give you back the result. This is the way I think data science
had always developing. Well, Alex, thanks so much for taking the time to chat with me.
Yeah, thank you, Sam.
All right, everyone, that's our show for today. For more information on Alexander or any of the
topics covered in this episode, head over to twomlai.com slash talk slash 161. If you didn't hit
pause and nominate us for the podcast People's Choice Awards at the beginning of the show,
I'd like to encourage you to jump over to twomlai.com slash nominate right now and send us your love
and your vote. As always, thanks so much for listening and catch you next time.
