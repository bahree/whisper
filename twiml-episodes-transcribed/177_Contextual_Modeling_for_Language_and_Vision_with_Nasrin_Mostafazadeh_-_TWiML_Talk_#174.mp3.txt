Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. Today we're joined by Nasreen Mastifazade, senior AI research
scientist at New York-based Elemental Cognition. Our conversation focuses on Nasreen's work
in event-centric contextual modeling and language and vision, which she sees as a means of giving
AI systems a bit of common sense. We discuss Nasreen's work on the story closed tests, which
is a reasoning framework for evaluating story understanding and generation. We explore the details
of this task, including what constitutes a story, and some of the challenges it presents and
approaches for solving it. We also discuss how you model what a computer understands, building
semantic representation algorithms, different ways to approach explainability, and multimodal
extensions to her contextual modeling work. Enjoy.
Alright everyone, I am on the line with Nasreen Mastifazade. Nasreen is a senior AI research
scientist at Elemental Cognition in New York City. Nasreen, welcome to this week in machine
learning and AI. Thanks Sam, thanks for having me.
So I hear that you started working on AI in high school. Tell us a little bit about that
story. Sure, yeah. So I was into computer science and computer engineering in general.
Like, when I was a kid, you know, I basically, that was the toy that I had. And I always
loved the idea of doing something meaningful, but in a sense that, okay, I'm spending
a lot of time in front of computer doing different stuff. What is the best thing that you
can accomplish? And somehow I was introduced to the notion of programming and the fact
that you can build like pieces of software to do things for you. And to me, the idea of
building a piece of software that kind of automates what you do was really intriguing,
which is how I got into robotics initially. So, you know, I started working on robotics
for the couple of my really great friends back in high school that, you know, we then
ended up competing in Robocop competitions, which is this, you know, annual worldwide competitions
among different roboticists to, you know, accomplish different tasks, basically, with robots.
And yeah, so the story, you know, we'll go forward with like how I did that for like a
year and a half, and it was fantastic and we accomplished a lot. And I was always under
impression that it's actually a very complex task, right? You build these multi-agent systems
that are capable of integrating their world model with like different communications
that then the agents and like control aspects, like mechanics, imagined electronics, and
all those sorts of things are really complex. And then somehow through some things that
I'm not going to tell the story of today, I got introduced to the idea that National Language
Understanding and National Language Processing as a field in general is much more complex
than robotics. And that kind of blew my mind like, as I said, I thought I'm working on something
that is, you know, super high tech, which it was, but turned out that at the time, this
is like 12 years ago, there were no systems that could do National Language Understanding
at the level of a four-year-old kit. So, this was an interesting challenge for me to take,
which is how I kind of switched gears and started working on National Language Understanding
and National Language Processing since then. And so, the focus of your work since then has been
largely centered around this idea of event-centric contextual modeling in language and vision.
What exactly does that mean? Yeah, so I think we can start with the events first, right? So,
I've always been interested in looking at the world, basically, through the lens of events,
because events are such central entities of the world through which we, you know,
go about our daily lives, we see things that happen as a result of some stuff that happen,
which is an event. So, an event can cause something else. And then our understanding of the
dynamics of such events and how they shape our world on a day-to-day basis is really, you know,
a crucial part of any kind of cognitive ability. And that makes, you know, me and a lot of other AI
scientists very interested in modeling events. So, that's the event-centric part. And then the
context modeling is about, okay, I have an AI system, I want to build an AI system that can get
an input and then produce some sort of an output, right? That input could be something super complex,
imagine an entire world being perceived by a situated robot, or it could be a piece of text,
right? Or it could be a piece of, you know, an input, which is multimodal, so both text and an
image. So, that's where what context means in my work, basically. So, contextual modeling means
how would the AI system deal with, representing and understanding the input that is provided with.
And then the language on vision part is basically where I've applied this set of, you know,
you know, different pieces of work on event-centric contextual modeling.
My am in nature, I'm in natural language processing researcher, but I've worked on applying
different, you know, AI, you know, methodologies, basically, in the world of vision and language,
which has become, you know, more hot, basically, in the past three years or so.
So, what are some of the types of problems that you come up with and try to solve in this
context of event-centric contextual modeling? So, there are different aspects of the world that
we can basically see through the lens of events, right? One major piece of work that I've done
is on understanding stories, right? As you can imagine, narratives or stories are these
sequence of really eventful sentences that we, you know, basically have to go through on a daily
basis because as humans, we tell stories all the time, right? When we communicate stories,
our major part of how we communicate with each other. And so, one of the, you know, lines of work
that I've invested on in the past couple of years has been on narrative understanding or story
understanding where you want to build the AI systems. They can read, you know, a coherent
sequence of sentences which are event-centric in, you know, in nature. And then it should understand
it in a way that it can answer questions about it. And more so that it could also be able to
generate meaningful stories. So, this work, you know, basically involves not only understanding
a piece of text which happens to be narrative, but also be able to generate, you know, a sequence
of sentences that go together coherently as a meaningful story. And so, how do you tend to approach
that type of problem? Yeah, so, you know, there are different things that you should take care of,
right? If you want to build an AI system that can, say, understand stories. First and foremost is,
okay, what is an event in the story, right? Are we talking about, like, you know, novel by Shakespeare
or are we talking about, like, a sequence of, I don't know, four words? There are people who call
sequence of five words also stories. So, in my work, I particularly, you know, focused on
understanding a sequence of five sentences stories. So, these are, we call it common-sense stories
in a way that we are basically interested in understanding the most, like, daily things that
happen to anyone, you know, in general. That's why we call it common-sense. And we go about
understanding them. So, that's the first thing you have to address, right? What is an event in
the story? After you address that, then you should think about, okay, now I want to build a system
that can understand this, just kind of an input. And then it goes about different, you know, steps
that you should take. So, one is, okay, how do I even represent this piece of, you know, input,
which is a narrative? So, it goes, you know, hand in hand with notions of knowledge representation.
So, in the AI community, from, like, you know, back decades ago, people invested a lot of time
and effort on knowledge representation. And in the NLP community, in particular, we have a lot
of literature on semantic representation and meaning representation, right? So, in my work,
I'm mainly interested in extracting events, right? So, what is, in particular, important about
this story, let's call those events and let's extract those and call it the kind of representation
that I'm interested in. So, that will be basically the second thing you do. And then the third
thing is, okay, now I want to, you know, basically connect the dots. How do I know what are the
stereotypical relations that exist between these events in a story? And then that will give you
this so-called narrative structure of the story that you can basically get in as an input to any
other, you know, system that wants to, say, answer questions about this story. So, that's more or
less, you know, in big picture, how you would go about a story modeling?
So, you started out with talking about how the first thing to understand is, what is a story?
But it strikes me that there's also this question, you know, what do we even mean by understand,
when we're talking about a computer understanding a story? And there's more to it necessarily than,
you know, answering questions. How do you model or assess a computer's ability to understand
a story? Or is that even, you know, part of what you're trying to get at, or is it more performance
on individual tasks? Yeah, that's a very good question, right? And I would say a fundamental question
for the entire AI community. So, the way we go about defining what understanding even is and
the field these days is through these benchmarks that we define, right? So, actually, I personally,
you know, contributed to a benchmark for story understanding, which is a story closed test.
So, the kind of benchmark we defined in particular was as follows. So, the AI system is supposed
to read a sequence of four sentences, which is the context of the story, basically. And then the
task is to predict the ending to that story. So, in the five sentences, the stories that I told you,
you basically imagine that you dropped the last sentence, and the task for the AI system is to
predict that. So, I personally believe that is a good kind of a proxy for modeling, whether or not
a system is understanding the story that it has read. That's one way of putting it, but as it goes
with many benchmarks in the AI world, whenever you have a task and you make it into a benchmark and
then you collect a very particular narrow, in a sense, a test set for it could be hacked, right?
And it goes to say that maybe we shouldn't just define understanding in terms of beating a
particular benchmark, but more so deploying systems in the wild. So, I would say that there are
different ways that you can define what true understanding even is. Having benchmarks is a good
way of making, you know, having proxies, right? And having basically evaluating ourselves as we move
forward, but they're not the ultimate. This should not be the ultimate end goal. I would say of
what we call true understanding. So, I will go to, you know, to add that. So, understanding
could be through the lens of answering questions, right? So, I ask the system, what is the ending to
this story? And if it answers it correctly, that means it's understood, right? But as it goes,
it could be hacked, meaning that without true understanding, maybe a system, a black box system
can actually specify the right ending. If you build systems that can explain themselves,
that could be a win, I would say. So, which is a focus that I have at Elemental Cognition now,
where we are building AI systems, in particular, a story understanding systems that can explain
the decisions they make, which is a better way of kind of knowing what's behind the scenes,
what's, you know, under the hood for the system that is making a decision.
Before we dive into that, I want to make sure I understand the story closed test. You
are training your system, presumably, on some corpus of five sentence stories. And then you're giving it
four sentences that form some new unseen story and asking it to complete the sentence.
And it's to complete the story, yeah. To complete the story, right? So, to provide the
exact three. The final sentence. And in doing so, you demonstrate that it can draw out entities
and contexts from the story and present them in some way that makes sense. Is it, you know,
does, is a human grading the, the responses? I guess the, the origin of that question
is strikes me that generally speaking for this kind of task, you could have multiple correct
output answers. So how does that work? Exactly. So that's actually what we did. We ended up collecting
alternative endings and then the AI, you know, system is posed with two alternatives. One is a
wrong ending, one is the right ending, and then the task is to choose the right one. So that's
actually the kind of a classification task that we ended up doing. But as you can imagine,
the task could be generation, right? The agent could be just posed with the four sentences and then
the task is to generate the ending, basically open ended, right? As opposed to multi-choice
question. But yeah, for the actual story closed, as we ended up collecting the right and wrong
endings by crowdsourcing and then at the end of the day, it becomes a classification task.
And sourcing these answers, were you targeting specific mechanisms of correctness, meaning were
you trying to test specific aspects of the algorithm? So for example, I forget whose work this is,
maybe I've seen it in the context of Josh Tenenbaum's work, but this, the idea that, you know, within
this concept of context, there's so much that's unsaid. You know, the cup is on the table,
you know, there are physical forces that, you know, keep the cup on the table, it's not going to
fall unless some other force pushes it off. That kind of thing, like, are you, I imagine you could
create sentences that kind of test that common sense context, or there are other things that you can
target to test with your sentences. Right. Yeah. So there are, you know, you can, I can imagine having
stories that will, you know, kind of evaluate such cognitive abilities of a system. But the point
about the story closed test is that it's about generic daily life events that happen to anyone,
which is why they're called common sense. So I can give you an example. So this is an example,
you know, a story closed test. So the context is as follows. Karen was assigned a roommate, her
first year of college, and then the story continues that her roommate asked her to go to a nearby
city for a concert, and then Karen agreed happily. The show was absolutely exhilarating,
and then there are two alternative endings to the story. One is Karen became good friends with
her roommate, and the wrong ending, the second ending, which I gave it away is the wrong ending,
is Karen hated her roommate. Right. So this is the level of complexity and generality, actually,
the VR tackling for the story closed test. So it's really about a daily story that has happened,
can happen to anyone. And in particular, we have crowd sources, as I said, by very generic prompts
to the workers. So it's more about predicting the next event, which happens to be the,
you know, ending to the story, as opposed to the kind of phenomena that, yeah, you describe.
Was the algorithm trained only on similar stories, or was it trained on other
information that wasn't in that same story format? Yeah. So he kind of set up this
challenge being the story closed test, so that people can do whatever they can to accomplish the
task. So we did provide a corpus of 100,000 stories called rockestories. These are full five
sentences stories that actually people can use kind of as a positive examples. They can
mine narrative structures from them. They can build like models that predict what happens next
in the story and so on, but it's not directly labeled data for the task of story closed test.
And given that, we've left people, you know, free to use whatever corporate out there,
whatever resources, common sense knowledge bases, or modeling techniques that they have to tackle
the task. So that's basically on our end. We wanted people to, you know, the research community
to basically improvise and be creative and bring in their own resources. So yeah, that's about it.
What kind of engagement or uptake have you seen on this test? Have you seen any interesting
approaches to building out algorithms to? Yeah. Yeah. So it happens that then, so the reason I made
story closed test in the first place is that there wasn't any, you know, systematic way of
evaluating story understanding in the field. So, you know, to get rid of my colleagues, we made
this benchmark so that we have a way of evaluating our progressies moving forward in the field.
And because, as I said, there wasn't any such benchmarking place, what we made actually got a lot
of attention, specifically because we showed that human does 100% on this task. So we actually made
sure that the test set that we put out there is like, you know, doubly human verify so that there
are no boundary cases before choosing the right versus the wrong ending. And also the best,
you know, state-of-the-art model that we trade that we tried on the benchmark was getting some
very around like 59%. And as you can imagine, random baseline would do 50% human was doing 100%,
so there was like more than 40% gap between the best system results and the human performance.
So, you know, these two characteristics, I would say, really contributed to the task getting a lot
of attention. And, you know, since then, this was released about two years ago, a lot of teams,
different teams, you know, from academia and industry, however, have, you know, tackled the task,
there has been so many results. But I go back to the point that I made about hacking, right? So
after we released this data set for many months, maybe, you know, eight, nine months, ten months,
there wasn't any significant improvement. And then we made it into a share task, meaning that, you
know, we had a challenge and this workshop and there was a prize for the winner up. And then,
when, whenever you do that, there are different approaches that will come at you, which is actually
very healthy for a benchmark so that you know exactly what works and what doesn't. And in that,
you know, challenge that we ran basically, that challenge day, there was a team submission from
UDUB that had found out that actually without reading the context, which is the whole point about
this national language understanding framework, without reading the context, you can leverage some,
you know, stylistic features isolated in the endings, just alone that. Yeah, to find the right
ending. And the interesting fact about this group is that so that, you know, the guy that actually
contributes to this model used to work on detecting a fake eout previews and like detecting
notions such as like age or gender from, from a piece of text. And it happened that turns out
our wrong endings actually without our knowledge. Our wrong endings had some, had similar features
as with fake reviews. So they had made this, you know, kind of a very interesting, you know,
observation and they had leveraged that and turned out that you could do, you know, really much
better if you just just train a classifier that just detects the such features. So that was a very
interesting outcome of that challenge that we ran. And it's still, you know, there was a still a
huge gap between human performance and their performance, which was around 70, like two, I think,
or 76%. But it still was such a good example of what can go wrong when you collect data, right?
For a narrow test, as I said, in AI. And this isn't something that has happened only to story
close test, like other benchmarks in the field such as natural language inference, NLI or VQA
visual question answering have basically showcased similar patterns that when we go about collecting
data sets for testing purposes, they could be really biased. And often those biases are not even
revealed to us. If you're lucky, we'll catch some of them. But often they're not revealed. And then,
you know, we can go forward without knowing them and then patting yourselves on a shoulder that
we are doing, you know, deep language understanding and like, you know, just, you know,
claim victory that we are surpassing, I don't know, things like human performance, like
but the truth is that a narrow benchmarks are narrow in the sense that they don't really represent
the world. And we should be really careful with the way that we collect our data.
Right. And I'm sorry I wanted this that we were very careful actually with the way we collected
the story close this and still there were biases that there was no way we would have known ahead
of time. So again, goes about saying how much more care as a community, as a research community,
we should give to exploring, you know, different ways of collecting data and vetting them. And then,
as I said, more so, I believe in the in the power of testing your systems on multiple frameworks
and making sure that they scale beyond the particular training data that they're trained on
or overfitted on basically. You created this benchmark. Have you also
participated on the the other side of it building out models to try to improve on the state of the
art? Yeah. So actually, so the one, you know, I worked on this. We had many different systems,
as I said, but they weren't none of them were doing fantastically. They were basically kept at
60-some percent. And then actually the most recent thing that we did after realizing that with the,
you know, new results that are particularly shared in the challenge day was to come up with
another dataset so that these hidden biases are kind of, you know, taking care of. And so now, you
know, I research the research team that I'm working on right now at Elemental Cognition.
We're working on different kinds of stories that are not exactly rocket stories, but technically,
you know, you can test and, you know, evaluate our system on rocket stories as well. But it's just a
more challenging task that we are tackling right now than the story closed test.
Tell me about the types of approaches that you would take for, you know, either of these types of
tasks. I guess when I hear semantic representation, the first thing that I think of from a deep learning
perspective is like embeddings. Are there, does that come in the play or are there other, what are
the kinds of techniques that come in the play? Yeah, I will tell you about the best model that we had,
which was doing, you know, 59% when we released the dataset, basically. So that was a model that was
basically, as imagine, a sentence embedding model. All it did was that you want to build a model
that embeds the context, which is four sentences, in the shared semantic space with the right ending.
So basically, we trained these two parallel neural nets, one of which will embed the context,
one of which will embed the ending. And at the end of the day, you wanted the, you know, encoding
of the right ending to be closer to the context than the wrong ending. So this was called DSSM,
a deep semantic structure model that did like the best within the, you know, multiple models that
we had initially tried for this story closed test. And then actually, I will go about saying that
the core anti-state of the art, as it is about like, you know, released data points about a
month ago, was from OpenAI where they had, they have this nice piece of work that is, you know,
transformers that are also pre-trained on like a large language model that are doing actually
really good job, 86% on the story closed test. So that's, that goes about telling you that
there's, there are a lot of regularities that you can leverage by just, you know,
reading a lot of corporate out there and building a language model. So that's sort of not exactly
what I would have predicted in terms of what would be the best model that can tackle story
understanding. But the recent results show that actually a very strong language model can do a
good job in predicting the ending. With the language model that they trained, was it trained
specifically for this task or was it trained generally like training on embedding space generally?
Yeah. So the interesting thing about that work actually was that which kind of makes it,
you know, sets it apart from the other systems that have submitted on our story closed test
benchmark was that it was actually a generic language model, was this very big language model that is pre-trained
and then tuned for our particular story closed test. So story closed test comes with a validation
and a test set. So validation is what people usually use for for tuning purposes. But the nice,
as I said, the nice characteristic of OpenAI system was that it was a, you know, pre-trained
generic language model. That is actually, I will add this that I told you that we have now,
we are working on releasing a new story closed test version, the new version of the data set
that kind of bypasses and helps with, you know, improving the kind of biases that people had
found out about the data set. And we tested OpenAI system and it was the only system that was
still having a very high performance on the new data set, which goes about saying that
turns out the other, you know, submit assistance for actually leveraging the biases as opposed to
doing true language understanding. Is there a way to characterize whether the OpenAI system is
just better at leveraging biases, you know, as opposed to kind of true understanding? Yeah, that's
a very good point. And I actually, I do think that there's no way for us that there are not
there some hidden biases in any of the data sets that we are benchmarking our progress on.
And until we get to a point that we can really deploy a system in the wild and see that they can
basically model different kind of complex inputs and then generate different kinds of complex
outposts, there's no way of guaranteeing that, right? I'm sure that our new data set that we're
about to release also has new biases that we are not aware of, right? And there's a very good
chance that the OpenAI system, as an example, is just doing a better job at, you know, again,
like memorizing different sorts of irregularities that are hidden. So I think there's just really
no way of knowing that. But I guess as long as at least we have better practices in place for,
you know, testing and evaluating your systems on a variety of benchmarks, VR in a better shape.
One of the points you raised earlier was the idea of systems that can explain themselves. Now,
that can mean a lot of different things. But one of the things that it can mean is that a system
like this OpenAI system or some other system with this capability can describe, you know, why it is
producing the sentence that it's producing or choosing the sentence that it's choosing.
Is that when you say explain, is that the focus of your work?
Yeah, exactly. So as I said, the outcome of the challenge that we ran on the story close test
was kind of an eye-opening for me personally to think beyond classification tasks. And,
you know, with a few of my colleagues, we had a lot of back and forth. And the consensus was that
if we build AI systems that not only choose the right, whatever their, you know, space they
provided this, choose the right ending in the context of story close test, but also explain why
they did that so that we can basically probe them. We can see whether or not their decision
makes sense. Because as I said, for instance, in the case of the team, the UW team that was
leveraging the features, the, you know, stylistic features, the system can't explain that I chose
this because it had more number of adjectives versus adverbs or it had like, I don't know, like
eight words as opposed to five, right? It should say something that is logically sound to a human
reader. So that's, yeah, back to your point, that's exactly what I mean by explanation. And I do
think that explanation is this cognitive ability that us humans we have and working on the next
generation of AI systems that can explain themselves is not really only for the sake of say
evaluation or for the sake of, I don't know, like they're now these regulations and say EU that
push for AI systems not to be black boxes, but it's really about modeling this human capability
and this cognitive ability that we have as humans because explaining is a way that we showcase
our intelligence often and we have to have AI systems that can just portray that.
And so there are different ways of approaching that. Some take the explanations from an intrinsic
perspective and try to introspect on the actual model that's doing the deciding and put what
it sees there into words and others take more of an external perspective and try to apply some
other model to the primary model to generate explanations. How does your work in the space
approach that distinction? Yeah, so I personally, this is my personal view. I personally,
personally, personally, foremost, I believe that the explanation should be in national language
form, right? So there are different pieces of work off a lot also in the vision and language
community that they basically count your attention features as the way of explaining why a system,
you know, say chooses a particular, you know, outcome versus another, but I count national
language explanations as a reasonable way of interfacing with human. So back to your question
about there being two disjoint modules that one of which will do prediction. Let's say one of
which will do the generation. I would say that I'm not against either of those. It could be a
disjoint model. It could be a joint model, but as long as the system can actually somehow provide
the explanation that makes sense to the human that is judging whether or not makes sense,
it could be still useful. So there are different ways of looking at explanation, right? Is it for
basically trying to diagnose, diagnose like why a system makes a particular decision? Is it
for improving the system? Is it what is the purpose, right? And I would say that at the end of the day
as long as the system can improve, for instance, through the interaction it has with human for
explaining themselves itself, why do we care if there are two different modules that one of
which is generating the explanation and one of which is doing the prediction. But at the end
of the day, as I said, I think we can take explanation as a way of knowing whether or not the system
can tell us what's going on there to hood. And if a system chooses to have two different modules
that's just basically an implementation decision, but I can't imagine a successful system that
doesn't have those two modules communicated with each other. So again, but I'm not opposed to
the idea. So thus far we've talked primarily about natural language understanding aspects of
contextual modeling, but you recently presented at CVPR on some multimodal work incorporating both
language and vision. Can you talk a little bit about that? Sure, absolutely. So as I said, the work
on event-centric contextual modeling has different applications. So we talked about story understanding
another application of it for my work has been on language and vision. So the language and vision
community has seen a lot of interest, has received a lot of interest in the past like a couple of
years, basically after deep learning growth, which is because we can just basically have better
ways of encoding images, which is very nicely something that you can nicely feed into a
recurrent model with which you can generate language. So that's more so what my work has been
around as well. So image captioning has been the most popular application in vision and language
since the beginning, which is about how would you build an AI system that will caption it a given
image very literally. So how would you literally describe what you see in a photo? So my work has
been mainly focused on going beyond literal description and getting more so towards kind of
vision language tasks that require some degree of common sense reasoning. So to give you an example,
the very first work I started on vision language was about this static image. Imagine I'm just
describing it basically now. Imagine this aesthetic photo you see of two policemen with a fallen
motorcycle on the ground. And as human beings, when we see this, right, the tree status of
aesthetic objects, right, two policemen and a fallen motorcycle. From these three static objects,
we go beyond, we connect the dots and we infer that, oh, there should be a notion of injury,
or oh, there should have been a motorcyclist, right, or oh, like there should have been an accident
here. So these kind of connections that we make are really interesting and it wasn't something that
was already explored in the research community. So we basically defined this task called visual
question generation that what it does is it focuses on building an AI system that given a static
image that happens to be event-centric, meaning that something's happening in the image.
What is the most natural first question that pops to your mind, given that image? So that's basically
the VQG work that I did that started off a series of other vision and language projects that I
worked on. And, you know, I will go about asking you. So what would be the most natural question
that you would ask, given the image that I just described to you? At the highest levels,
like what happened? Yeah, what happened exactly? So we understand that something should have
gone wrong or should have occurred that caused that scene. Yeah, what happened is the
motorcycle is still alive. How like serious was the injury, stuff like that are the most common
things that people ask and it was the kind of a, you know, task that we wanted to push for to go
beyond literal description. And so how do you go about tackling that? Yeah, so we basically built
it for first and foremost, you need data, right? So we collected our own data set called VQG,
which was on more, more so on event-centric images, we queried like being search engine to get
images that have something happening in them, save fire, earthquake, injury, stuff like that
are an accident. And then using that data, we built this model that gets as an input, the feature
vector of an image, which, you know, could be a convolutional neural net. And then given that,
just train a language model, basically a conditional language model that will just generate the text
description, which just happens to be a question here. So we actually leveraged existing image
captioning models and just basically tuned them and retrained them on our own data. And it was,
you know, semi-successful in asking relevant questions given an eventful image.
And so with the data set you curated, you had these images of events, things that happened,
and then you had a single caption for each image. Our data actually comes with five questions per
given image, but you know, it could have been anything, right? It just happens that ours was five,
just for the training purposes. We had, we collected 15,000 of such images, each paired with five
questions. And is the task structured in such a way that it's a classification or a generation
of a new sentence based on it? Yeah, that's a very good question. It is generation here. And as it
goes with any generation task, you have the problem of, okay, how would you evaluate, which is a
major, you know, problem in the community. So here, we just, you know, try different metrics that
are classically used in the division of the language community. They're different methods,
you know, you can use blue, you can use media or we just use Delta Blue, which happened to correlate
the best with the human judgment. So can you elaborate on that? What is Delta Blue? What are the
inputs to that? Of course. So these are metrics, right? There's no like, it's not no AI, like
nothing is being trained. It's just a metric for evaluating a generated output versus some gold
standards, which is an inherent problem because for so many taskers, no limited set of possible,
you know, gold answers as it goes for this, right? There's so many different questions you can
ask given an image. But at the end of the day, you, you know, as in research community, at least,
what you have to resort to is to define a set of predefined, you know, limited set of predefined
questions here. Even there, there are so many ways to ask the same question. Let alone
different types of questions that you could ask for about a situation. Exactly, which goes to
saying that to what I just said, that it's just inherently problematic. I would say that we
yet don't know how to evaluate language generation, which is not machine translation. So in machine
translation, even although even a machine translation, blue, which is the metric of how would you
evaluate how good of a job you've done at translation, there even people use blue, which,
which, you know, there are different pieces of work that show that even that doesn't correlate
with human judgment that is strongly, but even in machine translation, the task is much more
defined, right? The semantic content of the source language is really close to the semantic content of
the, you know, the language that you're going to, whereas in dialogue or in story generation,
or in this vision of language test that I just talked about, it's not, nothing is, you know,
set predefined. So we have this major problem, which goes, again, goes back to the story I was
telling you for the story close test that we decided to go with the classification task because
generation inherently is hard to evaluate and classification gives us this ability to systematically
evaluate. So yeah, anyways, we've, you know, the language and vision work that I told you about,
we ended up using Delta Blue, which is this metric for basically counting the number of words that
occur in the generated output versus the gold few human authored, you know, questions that you
have in hand. So you generate a question based on the image and you're evaluating the performance
of, or you're evaluating that question based on Delta Blue, and presumably you're crowdsourcing
the gold standard answers. Exactly. Yes, it just comes from the data set, right? So the 15,000
data, you know, points to be collected, we set aside a portion of that for test, where that
that's a blue, you know, human authored gold questions come from. And so for your implementation
of a system to do this, what was the general approach you took? To general approach, as I said,
was this model that encodes the image using a convolutional neural net and then generates the,
yeah, and then generates a sequence of words using a recurrent neural net. And these are,
you know, there's this recurrent neural net language models are really strong in generating
grammatical outputs, meaning local coherency, but they're not really good at capturing
intricacies and generating basically contentful sentences. As you may have seen, you know,
if you look at the kind of language, the chat box generator, a lot of such original language,
work pieces of work, often the generations are bland, meaning that they're, you know, they're
usually safer on the safer side. You don't have a lot of contentful words or events and stuff
like that, but they do a really good job in generating coherent sentences. And did your work try to
address that? Yeah, so there are different, so we did, you know, we did try different things,
just the, you know, the question is, okay, I have this system that generates different kinds of
output. People usually have this end best list that they rerang. So basically, when you are
generating at the end of the day, you can have a, you know, search, right? And then the question
is how to do that search better, so that you hit the ones that are more contentful. So they're
like different little tricks in the paper we had at the, for the VQG work, we use your question
generation that I just told you about. We had this very simple heuristic that if, if in your
end best list, you have a sentence that has verbs in it, give it a higher rank, or if you have a
longer sentence, is most possibly a better sentence. And then you put all these different,
yeah, features together, and then you tune your model, you use merch, but they're different,
kind of, you know, rankers you can use out there. But, you know, it's not a solve, it's a,
it's a serious problem we have for the, you know, see, basically these kind of recurrent
neural net generations. And when you describe this earlier, you, I think you described it as a
conditional language model. Yeah, I mean, yeah, so you have to have a way of
conditioning on the input right here, the input as the image, the input could have been anything,
right? In the case of story generation, the input could be the previous sentence. Here, we want to
generate using the image, so we condition the generation of the language on the feature vector
of the image. I'm trying to, to visualize what that looks like, or how that is implemented.
You know, when I think about RNNs and time steps and all that kind of stuff, where does the,
the feature vector of the image come into play, or is that the input at these time steps?
Yeah, so the, the very last model we tried for the VQG task, actually, what it got as the input
was the FC7 feature of the, you know, looking the convolutional neural net, one of the, you know,
slices that you can get is like FC7. So this fully connected layer is what we fed into the RNN,
as the, just for the very first step. You can actually feed that in for all the steps. That's
something we tried. We just got worse results. We just conditioned the very first time step on
that. But as I said, that's a, that's a decision that you can make by just, just trial and error.
Awesome. We have covered a ton of ground. Are there any other things that you might want to
mention about your current research areas? Yeah, I think it we covered a lot. I think that
what I would conclude this, this, you know, today's time path is to just mention that I,
I've chosen to work on the topics in AI that I found to be really challenging in terms of
the amount of work that still is needed to be done to even, you know, scratch the surface.
So commonsense reasoning happens to be one of them. There's a consensus in the field these days
that we do yet don't have an AI system that can even have the commonsense understanding of a,
you know, four or five-year-old kid, let alone an actual human like adult. And I think that
the kinds of work on like story understanding, story generation, or division language tasks that
are event-centric, go about, you know, at least going one step beyond the existing
efforts for doing something that is a little bit more challenging. And I think it's important
to be mindful of how far we've come, which is to tackle a lot of, you know, previously challenging,
I would say, you know, kind of perception tasks, but we really have a long way going forward
doing more of your reasoning and cognitive tasks, which is my, my personal research interest,
and I think a lot more into, you know, people in the community should pay attention to it.
Well, Neswin, thank you so much for taking the time to chat with us.
Thank you. Thank you so much for having me.
All right, everyone. That's our show for today. For more information on Nesreen,
or any of the topics covered in this episode, visit twimmelai.com slash talk slash 174.
If you're a fan of the podcast, please pop open your Apple or Google podcast app,
and leave us a five-star rating and review. Your ratings are a great way to help new listeners
find the show. As always, thanks so much for listening and catch you next time.
