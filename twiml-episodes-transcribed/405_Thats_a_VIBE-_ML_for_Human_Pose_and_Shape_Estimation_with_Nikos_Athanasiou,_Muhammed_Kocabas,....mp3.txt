Welcome to the Tumel AI Podcast, I'm your host Sam Charrington.
Hey, what's up everyone?
I hope you all had a wonderful weekend.
As I mentioned last week, the popular causal modeling and machine learning course, hosted
by Tumel, featuring instructor Robert O'Sezua Ness, is back.
We've now held two sessions of this course and student feedback has been amazing.
Students love the in-depth course content, assignments and capstone projects, of course,
but the number one observation has steadily been about Robert's weekly study group sessions
and the one-on-one office hours that students have with him.
You can use this course as an intro to causal modeling by just spending a few hours a week
watching the course videos and attending the study group session, or really dig into
the materials and assignments and develop cutting-edge skills in the field.
Now, no promises, but a couple of former students have gone on to write academic research
papers with Robert based on the materials they learned in this course.
This morning, Robert and I held a great session about causal modeling and the course.
If you're interested in the topic of causal modeling or considering the course, I really
encourage you to check it out by visiting our course page at twimmelai.com slash causal.
As you all know, we are super excited about Twimmelfest, our upcoming AI festival that
is jam-packed with engaging community-oriented and community-driven content.
We are on the lookout for Twimmelfest hosts who will help us create the event.
As a host, you'll get to share your ideas, experiences, expertise, and passions with
the Twimmel community through a session that you propose and organize.
So far, we've had a bunch of wonderful submissions, including an exploration into the impact
that AI will have on the medical field, a quote-unquote rookie playbook for ML beginners,
a code names challenge where AI-powered bots compete, and many more.
There's still time to submit your session ideas by visiting twimmelfest.com, but time
is running out, so put together those great ideas and hit submit.
And even if you're not ready to host, be sure to register for Twimmelfest today.
The event takes place October 13th through 30th is totally free and your registration ensures
that you'll be among the very first to hear about important updates, giveaways, and announcements.
And now on to the show.
All right, everyone.
I am on the line with Mohammed Kojabash, Nikos Athanasu, and Michael Black.
They are with the Max Planck Institute for Intelligent Systems, and we're here to talk about their
most recent paper, a paper called Vib that we will get into in more detail.
But Mohammed, Nikos, and Michael, welcome to the Twimmel AI podcast.
Thanks.
Thanks for having us.
Awesome.
Let's start by having each of you share a little bit about your background and your role
at the Max Planck Institute and what got you, what in your background led you to this work
and what your general research interests are.
Mohammed, why don't we start with you?
Sure.
So I am Mohammed Kojabash.
I'm currently a PhD student in Max Planck Institute for Intelligent Systems in Perseving
Systems Department, doing computer vision and graphics research.
So I did my bachelor's in computer engineering in Istanbul Technical University in Turkey,
and my master's in Middle East Technical University in Turkey again.
So during my bachelor's, so I tried to explore different areas that is interesting for me,
ranging from robotics and LP to computer vision and machine learning.
I continued for a master's degree in computer vision and machine learning, and I started working
on human process estimation, which has broader implications and application areas in the community.
And yeah, and then I started doing a PhD again in the same topic.
Cool.
Cool.
Thank you.
Nikos.
So hi.
I'm Nikos Athanasio.
So I'm also currently a PhD student at Max Planck Institute, I mean my first year.
So a little bit about my background, I completed my bachelor's and master's degree in computer
engineering in Greece in National Technical University.
During the first year of my research, I started doing research basically on natural language
processing, but then I wanted to combine natural language processing, machine learning with
computer vision, so I started my PhD at Max Planck Institute.
I'm currently working on motion, understanding the coding and how we can combine motion
and actions with language semantics.
I would like to combine machine learning and mathematical modeling with human interactions.
I think that's the most interesting part of the machine learning research.
And that's why I followed the PhD degree and started last September.
Awesome.
Awesome.
Thank you.
Michael.
Well, I'm Michael Black.
I'm a director at the Max Planck Institute for Intelligent Systems.
And I've been working on problems in human motion estimation since the early 1990s.
And I became interested in human motion because computers can't really understand us and
be full partners with us until they understand our facial expressions, how we move, how we
interact with the world.
And I'm motivated by Aristotle who said to be ignorant of motion is to be ignorant of
nature.
And I think almost all my research focuses on how things move and trying to get computers
to understand that motion.
A little bit about my background.
I've lived all over the place, educated in the U.S. and Canada.
And before I came to Germany about nine years ago, I was a professor at Brown University.
And I've been in Germany since 2011 and came here to co-found this new institute for intelligent
systems that brings together scientists studying artificial intelligence and robotics and
using techniques all the way from machine learning, like we're talking about here, to physical
systems where they make new materials for soft robotics and so on.
It's a very interdisciplinary and exciting field.
So Mohammed, you mentioned that your area of interest is in pose estimation and affect
the paper that we'll be talking about vibe, which is short for video inference for human
body pose and shape estimation is kind of in the direction of advancing pose estimation.
You share a little bit about kind of the broader landscape of pose estimation and where your
paper is hoping to contribute.
I think we've, you know, folks in the audience have probably seen various images that come
from pose estimation papers and tools that show kind of like a stick figure superimposed
on a picture of a body.
What you're doing is a little different than that and that you're doing kind of 3D pose
estimation.
Talk a little bit about the broad landscape that your paper fits into.
Sure.
So basically the pose estimation is the task to estimate human pose from images, videos
or any kind of sensory data.
But in computer vision specifically, we tackle with the problem of estimating human pose
from images and videos specifically.
And we can define human pose in different kind of ways.
Like as you mentioned, one of them would be to estimate some key points like these are
some parts set of key points only that can be the joint locations or some lean locations
of the human bodies or we can also try to estimate the whole body like in a more structured
and misstructured manner like a human body mesh.
So this kind of representations have some kind of a hierarchy.
So a sparse set of key points are really lacking to explain the real human body.
So and also we can estimate the 2D or 3D sparse set of key points.
But when we proceed to more higher richer representations like human body meshes which
can define the whole body like the whole pose and shape, which has some bleacher information
that explains the human body.
So in this paper specifically, we try to estimate the whole body mesh, which is more explanatory
than on the sparse set of key points.
And also we tackle the problem in 3D space, which makes the problem more difficult to estimate
from the images or videos only.
Right.
And Nika, you mentioned that one of your interests is in applying vision to motion.
Can you talk a little bit about the motion aspect of this and where that comes in?
So we are aiming directly to capture motion.
So we are starting from a plane video, which are multiple images, the sequence of images.
And from those images that we have, we detect the person in the video and we estimate his
or her 3D pose and shape.
So many methods have looked at this problem, but the vast majority focus on single images.
So estimating the human pose in one image and a video is just a sequence of single images.
So you could just apply those techniques one frame at a time.
If you do that, you tend to get a jerky reconstruction that's not moving smoothly and naturally
like a human would.
And so one of the observations here is there's more information in the video sequence in
the continuity of the motion.
And if we can train the computer to exploit that by teaching it what it is to move like
a human, then we can exploit information across a long video sequence and get better results
and you would get by doing it frame by frame.
And as an example of the better nature of the results, you've got a great picture at
the beginning of the paper that shows some areas where the current state of the art method
in a challenging video doesn't seem to track very well, whereas your method performs
better.
What is that kind of reference method that you looked at and what's the data set that
you are building your model with?
Well, the competing method is from one of our very good friends and colleagues, Anju Kanazawa,
who actually did an internship in our institute.
So it's a friendly competition for who's going to have the state of the art.
Anju's amazing and she's a professor at Berkeley.
She also extended one of these single frame methods over time, but in a different way.
And one of the things that distinguishes our approach was actually inspired by something
she did earlier, which is to use a discriminator that knows something about, it's trained
to distinguish between fake human motions and real human motions.
And to enable that, we exploited a data set that we made public back in the fall called
a mass.
And a mass is a data set of about 45 hours worth of human motion capture data that captures
a wide range of how people move.
And then we used that to teach the computer basically what is a good human motion and
what isn't.
And when you say human motion capture, is this kind of regular video or is this the kind
of thing that we will often see with humans in like white suits with like balls around
them?
Yeah, the latter.
There are many, this is commercial motion capture data with markers and infrared reflective
lights and so on.
So very much a lab setup.
And many groups around the world have made small amounts of motion capture available.
Every data set though is in a different format and researchers were struggling to get enough
data to train deep neural networks.
As we all know, deep networks are great and they could do a wonderful job, but they need
a lot of data, they're data hungry.
And so what we did was pool together a dozen different data sets and put them all into
a single format, this representation of the human body we use called simple SMPL.
And by unifying them all, it provides enough training data to really do something bigger.
So Mohammed, you tell us a little bit about your kind of first steps in tackling this
problem.
Sure.
So the first step in general is to create a baseline for your method.
So what we did in the first step is to create a model that can only predict some pause
and shape from a video without using any intricate or complicated architecture.
So at that time, the first thing we tried, so there is a method called human mesh recovery,
which estimates a pause and shape from only a single image.
And we tried to extend this human mesh recovery to estimate a sequence of pauses given a
video using a recurrent neural network architecture.
So a recurrent neural network is basically a type of neural network that lets you integrate
some information from past frames or past data points.
So we take each output of CNNs from human mesh recovery method.
And we try to estimate the sequence of pauses from this recurrent neural network architecture.
This was the first thing that we tried to see how the model performs with the available
data.
And in the next step, we tried to integrate Amaz using a genitive error cell network approach.
So imagine we have this baseline method that produces some noisy or inaccurate estimations
of the motion happening.
And if we integrate a discriminator to the training stage, which tells if the predicted
motion is plausible or similar to real life motion, then we can supervise this baseline
method to produce better looking motions.
And this is what we did in the final stages, where we introduced Amaz data to set and
the discriminator, which supervises this baseline generator we had.
So let's go back to the initial stage.
You've got a CNN whose output you're feeding into an RNN.
What part of the CNN are you feeding into the RNN?
Is it kind of the output of a final classifier stage or is it some earlier layer in the
CNN that you're using as input to your RNN?
So let me first summarize the human mesh recovery architecture.
So we have a CNN that extracts the image features and then a regressor, which is a couple
of NLP layers that estimates simple body pose and shape parameters.
So the CNN part is fully convolution, a fully convolutional model.
And so we get rid of the regressor part to train the wipe and we only use the CNN part
to extract the image features of the paper frame.
And we feed these image features to recurrent neural network we have.
And you talked about the discriminator is the discriminator trained separately or kind
of end to end as part of training the CNN.
Yeah, so actually the generator generates some predictions and we train the whole thing
end to end.
So the discriminator takes as inputs the generators predictions of the pose and shape and it takes
also samples from a mass.
So a mass is exactly what Michael previously explained as the unified motion dataset.
So we use the discriminator to validate if the sequence of poses that we have is plausible.
It can actually be a real human motion.
So the discriminator's job is by taking those two different pose sequences to compare
them and refine the generators output sequence.
So we train this whole thing end to end and the discriminators takes the real fake samples
from a mass and that was generator.
When I think about the in this picture that you created for me Michael with the folks
and suits with the balls that process and those images, I think of those as kind of fairly
sparse points on the body that are being captured and yet these models are, you know, they
look to be like fairly high, you know, resolution, fairly fine grained.
How does the model go from kind of this sparse motion capture to more robust looking mesh?
Yeah, great question.
That's a whole nother paper.
So indeed, when you capture a motion capture lab, you've got a bunch of sparse markers.
And then the traditional method solves for a skeleton that explains those markers.
Now we replace that traditional solve with a different kind of solve.
We actually take our 3D body model.
This is the simple body model and fit it to the marker data in a using a process called
Mosh for motion and shape capture.
And this was a paper that appeared at Cigraf or Cigraf Asia a few years ago and does a really
good job of taking sparse motion capture markers and fitting the most likely body surface
that could explain them.
It turns out that from 20 to 60 sparse markers on the body, you can really figure out what
a person looks like.
You could think about it as a very, very sparse 3D scan of the person.
And with a powerful statistical model of the body, it's about how body shape varies and
how pose varies, you can fit a sequence of these very sparse markers and get out this
kind of realistic detail that we can then use to train other methods.
Maybe, Mohammed, you can help me get to the like the core element of this paper that
helps it perform beyond the previous method.
What's kind of that kernel of contribution here, innovation here that helps get to that
incremental performance and how did you arrive at that?
Was that obvious setting out at this project and you just kind of put the pieces in place
and it all worked or was there a evolutionary process here that helped you arrive at the
final architecture?
So the major contribution in this paper is this motion discriminator, which tells if the
predicted sequences are fake or real.
So this is the major contribution we have and this is the one that we get the most improvement
in the results in the results.
So actually, it is quite, so from the human mesh recovery paper from Anuju Kanazawa, we
already know that using this kind of a discriminator for single image pose estimation helps a lot
to have plausible bodies.
So and we thought that intuitively this should help for a sequence of poses.
And we already know that Amaz is a very large scale data set, which has different kind
of motions, which happens in real time and performed by the real human actors.
And this intuitive reasoning led us to create this architecture.
So yeah, during the development phase, we had a lot of technical difficulties and we just
get through them by doing like fine-grained experiments on which part improves, which
is led us to architecture we have.
What were some of those technical difficulties and experiments that helped you overcome them?
Yeah, for example, tuning the, we have this discriminator and generator and we have this
discriminator losses that tells if this sequence is real or fake and also some other losses
that tells the 2D joint locations and 3D joint locations of a given input video.
So we had to tune these 2 kind of losses to every on an optimal solution.
This was one of the things that we really take.
Maybe it's worth telling the audience also that one of the problems here is we want to
get 3D human poses in detail and yet there's not much ground truth training data where
you have video sequences with the true 3D human pose in correspondence.
And there are large data sets of labeled 2D human poses, but not 3D.
And so this is one way to combine sort of weak 2D information with very strong 3D information.
They're not paired.
So a mass doesn't have any images.
It's not associated with any real videos.
But the discriminator learns what it is to be a motion and then the generator learns
how to go from images to 3D.
Yeah, exactly.
Actually, that's what the high overview of the paper is we get the 2D data.
We output some pose sequences and then we use that huge data set a mass that has various
motions to refine those predictions to be able to not, between the two sequences, one
from a mass and the hour sequence, our generated sequence, the discriminator ideally should
not distinguish between those two.
So the discriminator should be 50% sure about which one is the real and which one is the
fake.
And that's a great point.
So the discriminator is operating purely in the motion domain.
It doesn't know anything about images or the frames of the video.
It's just answering the question, does this seem like the kind of motion that the human
body does based on the mass data set and the CNN and the other parts of the architecture
are more focused on extracting motion from the video frames?
Yeah, exactly.
Cool.
And so I think one of the points that you make early on in the paper is that the motion
that, and even the examples that you give, it's not like normal motion.
It's kind of complex motion that is not something that you see every day is that was the
mass data set kind of curated to identify and include lots of examples of complex motion
in it or is there something in the process that is kind of extrapolating to that kind
of complex motion?
Okay.
It's a good question.
There's a mixture in there.
There's a lot of mundane things like walking.
It seems like in motion capture labs, people capture a lot of walking.
So, but there are some data sets that are a little bit more extreme, have more interesting
poses.
And in fact, we captured something we call extreme poses, which we hired some gymnasts
to come in and do basically the wildest things that the human body can do, just to really
flesh out the space of what's possible.
And there's one other thing that happens in motion capture a lot for video games, people
often capture something like kicking a ball and turning left or turning right in very discreet
movements.
And so we have something called a transition's data set in there as well that captures
people doing one movement and then transitioning to another movement.
So this is again trying to expand these transitions are things that you might not think to capture
on your own, but are really part of the natural human behavior.
So there's a mixture in there and it's rich enough for this task.
Are there specific things that Mohamed Arniko having worked with this data set for this
particular task jump out at you as, you know, I wish the data set had more of, you know,
kind of category X because it would help you build more robust models?
Yeah, basically that a mass data set has almost all of the available mocap data by different
labs captured and in different conditions, but it doesn't have a lot of in the wild data.
That's the thing that it's missing the extreme poses and the in the wild kind of data.
So someone doing extreme gymnastic exercises or having like motion without a constant pace
and having different poses and like opening the hands or the feet extremely quickly or
slowly or I mean, we could always benefit from more variation of extreme and quick or fast
motions.
Yeah.
Now that we are starting to have, you know, I'm thinking like sporting events that are
captured with many cameras from lots of different angles, does that ever able to take the
place of motion capture types of information or are we not kind of sophisticated enough
to correlate all of that and and produce the kind of data sets that kind of traditional
motion capture is able to provide.
Have you looked at that Michael at all?
Yeah, so it's a let's come back to this idea that, you know, traditional motion captures
a bunch of sparse points on the body, not very detailed.
With the images, we've got thousands, hundreds of thousands of points on the body.
It just happened to be in 2D, but they are much richer in some sense.
We have all this facial expression information, details of the hands, subtle, you know,
subtle motions of the body breathing and so on.
You can see all that in a video.
So our hope is, you know, one day that motion capture from video will be more accurate
and more detailed than motion capture from a traditional mocap system today.
We're not there yet.
This is a step in that direction.
I think your intuition about combining multiple views is a good one and it's something
that we're we've looked at in the past and continue to look at.
Can we combine multiple 2D views of a person and get, you know, really high quality output?
I think the answer is yes.
I haven't seen a system yet that competes with the most accurate mocap systems.
But then the most accurate mocap systems, you know, cost hundreds of thousands of dollars
and have dozens of cameras.
I think if you built a video based mocap system of sort of that sophistication, I think
we might be getting close.
And does light are types of systems or like what we've seen with the connect, does that
play into this as well?
Yeah.
Absolutely.
We also work with connect.
The new Microsoft connect for Azure is allows you to put multiple connects together.
So you can actually have several of them looking at a person.
That's a really promising direction for a lightweight, easy to set up system.
We don't have anything to show yet, but yeah, cool, cool.
So Muhammad again, kind of going back to this opening picture in the paper, you know, it's
easy to see where your model is outperforming the traditional model and kind of eyeball the
delta.
How do you evaluate that performance more concretely and mathematically in the paper?
So we have several datasets that has ground through 3D joint positions.
And one of them is a dataset called 3D pauses in the wild again, that is developed in our
group in person systems group.
So this dataset is captured in the wild.
And there are some subjects varying some I am I am you sensors on their body.
And so this I am you sensors can be placed under the cloth.
So it doesn't affect the image quality.
And so you can get the 3D joint positions from those I am you sensors.
And we the major that the most prominent dataset we try is this 3D PW 3D people in the wild
dataset.
And so basically what we do is we take the sequences from the dataset, produces our results,
and also the other methods results and compares what is the distance between the ground through
the joint positions and the predicted ground to the joint positions.
And also there are some other indoor 3D datasets we also use.
One of them is called human 3.6M and the other one is called MPI informatics 3D human
position data set.
The 3D poses in the wild dataset is that you mentioned it uses a particular kind of
sensor is that sensor and the kind of sparsity of that sensor similar to what you see in
a mocap system or is it you know somehow more more dense I'm curious about the relationship
between the primary way your comparing performance is based on kind of these sparse representations
I wonder if that leaves something to be desired or an opportunity in terms of the
ability to kind of capture what's happening in complex motions with these sparse data
points on the body.
It's a good question.
So we use their commercial mocap suit called it from accents uses these inertial measurement
units.
I think there's 10 or 12 on the body, but we don't just use that.
We also use a camera tracking the people around and we use 2D measurements about the joint
locations in those images to precisely align the estimated 3D body pose with the images.
And so we have a technique that fits again our simple SMPL body model to these multiple
kinds of measurements the IMU measurements and the 2D measurements.
We also have to solve for the camera translation and rotation and things like that.
But putting this all together gives reasonable we don't call it ground truth we call it reference
data.
We're a little careful about that you know it's not the same level as a motion capture
lab.
But you get the benefit of being outside in in the wild with all the complex lighting
and occlusion and things that go on in you know in natural scenarios.
Okay.
It's a bit of a trade off it's the holy grail is to have perfect ground truth where you
know every single bit of the motion with no crazy sensors to get in the way outdoors in
the wild but it you know it doesn't exist.
So mom and mentioned earlier you know this process of kind of experimenting and identifying
you know things that didn't work with particular emphasis on the discriminator Nika someone
ring you know from your perspective you know if you can share another example of something
that maybe thought was gonna work and didn't and how you had to adjust the approach to accommodate
it.
Yeah the experimentation procedure was was tough but yeah there are some a few points
so we tried one minor point was that we tried the difficult CNN feature extractors and
that was a difficult part to experiment with but we quickly converts to an optimal one
and the other one is that we decided to use a self attention mechanism which is like
a major kind of a major contribution because actually the self attention mechanism is not
so commonly used in 3D modeling or modeling of human motion so we use the self attention
mechanism to get even better results on the discriminator part which is basically imagine
that we are processing the human motion sequence and we are extracting some features using
2G or U layers which is a type of very current neural network as Mohamed said before and
instead of hard pulling or combining these features statically we are using a weighted
schema to combine those two those features so the features that we have for every pose
in the sequence we combine them as a weighted sum in order to amplify the contribution
of most important frames of the sequence more for the discriminator's performance to improve
so that's what was too key technical difficulties that we have.
Were you able to develop any kind of intuition over why that worked and what the attention
mechanism ended up attending over?
We have made a population experiments that we tested the self attention mechanism so
we tried two different things so instead of using self attention we trying pulling the
features in a static way so concatenating the average and the max of the features and
using that for the discriminator to decide whether the motion is fake or real and we
compared with that case with our case and the performance seemed to improve.
It didn't improve in such a margin that the results were extremely visible but for sure
attention pointed out the frames that they should be corrected in the pose sequence and
that they were the more plausible ones.
I think about the various pieces of the model that you've built here, you've got CNN,
you've got Narin and you've got this attention mechanism, you mentioned VAE in the paper.
It seems like a lot of moving pieces and I'm wondering how that impacted the process
and also from a computational perspective if the training runs were very expensive.
You're also working with video which is a lot of data.
How was the process of working on this from a just computational perspective?
Did that have a lot of impact on the way you approach things, Mohammed?
So surprisingly the model we have both in training time and testing time is quite lightweight
even though we have lots of components.
So the heaviest part in our model is the CNN future extraction part.
And what we do during training, we pre-compure the CNN features and we don't update this
CNN module and we only train the recurrent neural network and since the number of parameters
is quite low with the simple body representation, our neural network is quite lightweight.
And we don't use very deep layers, only a couple of a stack of GRU layers is enough for us
and it wasn't that difficult for us to train the model.
So actually during training time for example, like in half an hour or in one hour we can
converge to a good solution since we use this already pre-computed CNN features.
And during training time we also get rid of the discriminator and we only use the generator
part and it makes the testing much faster.
Okay and then what about from an inference perspective, how expensive is that?
Again, it is not quite expensive since we only have the CNN which is resident 50 in our
case, which is again lightweight and it can run in real time in GPUs.
And the GRU layers we add, it is not again that expensive.
So in a commercial GPU, we can get almost real time speeds during the experience.
Very cool.
Maybe start to wrap things up, I love for each of you to kind of put this paper and
maybe talk about the things that you're either your top lessons learned or the thing
that you're most excited about with this paper and its contributions and kind of how
you see it moving forward in your own research.
Nikos, do you want to start us off with that?
Yeah, I can start.
So I joined the MBI in September and I didn't know a lot of things for 3D human modeling.
So and this paper is a very good first experience and I think one of the most important
features of the paper is that it is clear.
I mean the contributions are clear, we use a lot of unpaired 3D data to refine 2D predictions
actually to the predictions, actually 3D meshes that were predicted mostly from 2D key
points.
And our implementation is converges fast, it can create high quality and state of the
art, the predictions are high quality and state of the art.
And also one important thing is that we've made all the code and data publicly available
so we have detailed instructions for everyone so it's pretty easy for anyone if you want
or see wants to run and use our model and methods, we have clear instructions to do
it.
Great.
Mohammed?
So it seems that why we're working quite well on the restricted data sets for evaluation
we have, but in real life or in the wild videos there are a lot of difficult situations
and a human motion is quite difficult to model because humans are capable of doing lots
of different kind of movements and also in real life situations we have like objects
and scenes around us, we have occlusions caused by again objects or other people.
These kind of problems are still waiting to be tackled with and also we show with
wipe, we cannot perform well in these kind of settings and in the future there are lots
of things that we have to do to model human motion and also track and capture human motion
from videos and this is what I learned in the process even though we have the state of
the art model we cannot solve even some basic scenarios still.
Great.
Thank you.
Michael?
Well, I'm really excited about this paper and we're taking this but I'm always thinking
about the next thing and humans don't move for no reason, they don't move in a vacuum,
they move in the 3D world to solve tasks and to change that world to have an influence
on it.
You cross the room to open the door, you open the door to move through it, you move
through it to get to the next room, you're solving tasks all the time and in solving those
tasks you're interacting with the environment.
Even if you're interacting with another person you're moving your face and you're gesturing
in ways that try to have an influence on the other person, maybe without any contact
but you're still using your body to influence people or the world and this work at the
moment just treats the body in isolation.
The body isn't embedded in the environment, the CNN doesn't really know anything about
the environment, it's a single person not interacting with other people.
I think there's a tremendous amount we still have to do to get computers to really understand
us and those next steps will be about human-human interaction and about human-object-human-seeing
interaction.
Great.
Well, Mohamed Niko's, Michael, thanks so much for taking the time to share with us your
paper and what you're up to.
Thanks a lot.
Thanks.
All right, everyone, that's our show for today.
To learn more about today's guest or the topics mentioned in this interview, visit twimmolai.com.
Of course, if you like what you hear on the podcast, please subscribe, rate, and review
the show on your favorite pod catcher.
Thanks so much for listening and catch you next time.
