1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:21,760
I'm your host Sam Charrington.

3
00:00:21,760 --> 00:00:24,680
Hey what's up everyone, this is Sam.

4
00:00:24,680 --> 00:00:29,080
It's been nearly a month since we closed the books on our very first conference, Twimal

5
00:00:29,080 --> 00:00:31,160
Con AI Platforms.

6
00:00:31,160 --> 00:00:35,000
I really hope you've enjoyed the content we've shared from the event.

7
00:00:35,000 --> 00:00:39,920
We went into the conference with three main goals, first to provide attendees with great

8
00:00:39,920 --> 00:00:45,440
content focused on the many ways, organizations of all types are automating, accelerating

9
00:00:45,440 --> 00:00:48,400
and scaling machine learning and AI.

10
00:00:48,400 --> 00:00:53,560
Thanks to an incredible roster of speakers, we were able to deliver just that.

11
00:00:53,560 --> 00:00:58,520
Next, we wanted to bring the extended Twimal community together and the results here

12
00:00:58,520 --> 00:01:00,480
were truly amazing.

13
00:01:00,480 --> 00:01:04,680
I had the great fortune of meeting and conversing with a ton of wonderful people, including

14
00:01:04,680 --> 00:01:11,440
longtime Twimal listeners, former podcast guests and attendees new to the Twimalverse.

15
00:01:11,440 --> 00:01:16,000
One thing that I heard universally was how open and engaged everyone was and how much

16
00:01:16,000 --> 00:01:20,160
folks were enjoying the substantive meaningful conversations they were having with other

17
00:01:20,160 --> 00:01:21,480
attendees.

18
00:01:21,480 --> 00:01:24,840
To me, that sounds like a mission accomplished.

19
00:01:24,840 --> 00:01:30,280
Finally, TwimalCon was also an opportunity to celebrate all that we've been able to

20
00:01:30,280 --> 00:01:32,760
accomplish together as a community.

21
00:01:32,760 --> 00:01:36,680
This includes the many innovations that our speakers are presenting on in conference

22
00:01:36,680 --> 00:01:43,400
breakouts, as well as Twimal community achievements, like reaching our third year, 300th episode

23
00:01:43,400 --> 00:01:47,160
and 5 million download in the last few months.

24
00:01:47,160 --> 00:01:52,000
To be able to share these milestones with so many awesome friends, new and old was an

25
00:01:52,000 --> 00:01:54,680
amazing experience for all of us.

26
00:01:54,680 --> 00:01:58,880
I wanted to take this opportunity to thank each of you in our community for all of the

27
00:01:58,880 --> 00:02:01,960
many ways that you support and engage with us.

28
00:02:01,960 --> 00:02:03,600
Thank you so much.

29
00:02:03,600 --> 00:02:06,400
And now on to the show.

30
00:02:06,400 --> 00:02:10,240
All right, everyone.

31
00:02:10,240 --> 00:02:13,800
I am on the line with Archena Venkatraman.

32
00:02:13,800 --> 00:02:19,280
Archena is the John C Malone Assistant Professor of Electrical and Computer Engineering at

33
00:02:19,280 --> 00:02:21,440
John Hopkins University.

34
00:02:21,440 --> 00:02:24,360
Archena, welcome to the Twimal AI podcast.

35
00:02:24,360 --> 00:02:27,880
Thank you, Sam, and thank you for inviting me to be here today.

36
00:02:27,880 --> 00:02:34,640
I'm really excited about our conversation, and we'll jump right in by having you share

37
00:02:34,640 --> 00:02:36,360
a little bit of your background.

38
00:02:36,360 --> 00:02:41,400
It sounds like you had maybe more clarity than most starting from as early as the fourth

39
00:02:41,400 --> 00:02:42,400
grade.

40
00:02:42,400 --> 00:02:48,000
Yes, so both my parents are engineers.

41
00:02:48,000 --> 00:02:53,440
My mom is a professor of electrical engineering, and my dad is a professor of mechanical engineering.

42
00:02:53,440 --> 00:02:59,600
So as you can imagine, we had a very strong educational focus growing up.

43
00:02:59,600 --> 00:03:03,160
And I decided fairly early on, engineering was very cool.

44
00:03:03,160 --> 00:03:07,560
It was a way of solving problems, and I was very good in math and science, and I enjoyed

45
00:03:07,560 --> 00:03:08,960
them.

46
00:03:08,960 --> 00:03:16,640
And so around fourth grade is when I asked my mom about colleges, and where to go to

47
00:03:16,640 --> 00:03:22,880
college, and what were good schools, and she said that the best school was MIT.

48
00:03:22,880 --> 00:03:26,720
And I checked, and it was very close to home, which in the fourth grade was important to

49
00:03:26,720 --> 00:03:27,720
me.

50
00:03:27,720 --> 00:03:35,760
So I decided I would go to MIT, and so I worked through middle school, high school, learning

51
00:03:35,760 --> 00:03:42,640
lots of science, and doing many extracurricular activities, and finally did end up going to

52
00:03:42,640 --> 00:03:44,680
MIT for undergrad.

53
00:03:44,680 --> 00:03:45,680
And I liked it so much.

54
00:03:45,680 --> 00:03:49,080
I stayed there for masters and PhD.

55
00:03:49,080 --> 00:03:55,360
And through that educational pathway at MIT, I started in electrical engineering.

56
00:03:55,360 --> 00:04:02,960
I explored different areas, so I did research experiences in devices, in nanofabrication.

57
00:04:02,960 --> 00:04:09,520
And through those research experiences and coursework, I realized I loved signal processing.

58
00:04:09,520 --> 00:04:13,960
So that was the area that really gravitated towards me.

59
00:04:13,960 --> 00:04:18,680
I loved the idea of transforming signals into various domains to understand different

60
00:04:18,680 --> 00:04:24,400
properties of being able to estimate properties and characterize uncertainty.

61
00:04:24,400 --> 00:04:31,080
And so that was my concentration in undergraduate, and I did a master's with Alan Oppenheim,

62
00:04:31,080 --> 00:04:35,560
who is one of the fathers of modern day signal processing.

63
00:04:35,560 --> 00:04:38,280
And we looked at different signal representations.

64
00:04:38,280 --> 00:04:44,080
And I found that a very worthwhile experience and definitely helped me build fundamentals.

65
00:04:44,080 --> 00:04:49,320
And at the same time, I always was missing an application to it.

66
00:04:49,320 --> 00:04:55,680
And that's how I stumbled upon my PhD direction, which was loosely medical imaging.

67
00:04:55,680 --> 00:05:02,600
So sort of translating these properties of informatics and data science into understanding

68
00:05:02,600 --> 00:05:07,880
brain functionality through functional neuroimaging data, FMRI.

69
00:05:07,880 --> 00:05:16,240
And at the same time, still exploring and learning more and applying ideas from machine learning

70
00:05:16,240 --> 00:05:20,880
to these data streams and two different clinical populations.

71
00:05:20,880 --> 00:05:24,320
And that's kind of how my career has evolved.

72
00:05:24,320 --> 00:05:27,680
And I'm really enjoying the ride so far.

73
00:05:27,680 --> 00:05:35,120
And do you have a more pointed focus currently at Johns Hopkins, or are you looking out broadly

74
00:05:35,120 --> 00:05:38,080
around the intersection of these areas?

75
00:05:38,080 --> 00:05:41,680
So I think my research right now is very broad.

76
00:05:41,680 --> 00:05:48,560
So the most general characterization is that we are developing new machine learning tools

77
00:05:48,560 --> 00:05:57,600
and frameworks and algorithms to better understand and potentially treat neurological and psychiatric

78
00:05:57,600 --> 00:05:58,600
disorders.

79
00:05:58,600 --> 00:06:02,920
But in the space, in the neuro space, this is a very broad spectrum.

80
00:06:02,920 --> 00:06:11,160
So it ranges from basic science type questions where the goal is biomarker discovery and exploration

81
00:06:11,160 --> 00:06:14,520
through the ideas of clinical translation.

82
00:06:14,520 --> 00:06:21,800
So how do we provide information that might be helpful or actionable to even very far

83
00:06:21,800 --> 00:06:29,480
out their explorations of trying to understand perception and being able to alter perception,

84
00:06:29,480 --> 00:06:32,760
again, using different data streams and machine learning tools.

85
00:06:32,760 --> 00:06:36,600
And I would say throughout the spectrum, everything is very different.

86
00:06:36,600 --> 00:06:38,600
The disorders that we work on are different.

87
00:06:38,600 --> 00:06:43,280
The data sets that we use are different and then our modeling approaches are different.

88
00:06:43,280 --> 00:06:47,080
But they still have that flavor of kind of machine learning.

89
00:06:47,080 --> 00:06:48,880
So how much can you mind from the data?

90
00:06:48,880 --> 00:06:53,680
What can you understand and what can you formulate and pass on as information?

91
00:06:53,680 --> 00:07:00,360
Let's maybe dig into one or two of those areas to get a sense for the way you're able

92
00:07:00,360 --> 00:07:02,720
to apply machine learning.

93
00:07:02,720 --> 00:07:08,200
You mentioned basic science is one of the foundational areas and you're looking at things like

94
00:07:08,200 --> 00:07:13,920
biomarker discovery, how do MLM AI play into that?

95
00:07:13,920 --> 00:07:23,680
So one of the kind of the fields or disciplines that I've been heavily involved with is the

96
00:07:23,680 --> 00:07:29,600
branch of, I guess, computational neuroscience known as connectivity.

97
00:07:29,600 --> 00:07:36,800
So at a high level, it's treating the brain as a network of interconnected parts and

98
00:07:36,800 --> 00:07:42,960
not only looking at specific functionality of a region, but also considering that regions

99
00:07:42,960 --> 00:07:47,240
communicate with each other in different ways and trying to build models based on those

100
00:07:47,240 --> 00:07:49,200
communication patterns.

101
00:07:49,200 --> 00:07:54,840
So the data that we use is called resting state fMRI data.

102
00:07:54,840 --> 00:08:02,480
So unlike a conventional MRI acquisition where you would have the subject perform a certain

103
00:08:02,480 --> 00:08:08,280
task in the scanner, resting state differs in that it's a passive acquisition.

104
00:08:08,280 --> 00:08:14,920
So the subject is just instructed to lie there quietly, passively, and oftentimes just

105
00:08:14,920 --> 00:08:17,120
fixate on a crosshair.

106
00:08:17,120 --> 00:08:24,120
And what people have shown over the last decade or so is that the correlation patterns

107
00:08:24,120 --> 00:08:29,400
in this kind of steady state signal, they actually reflect different functional systems

108
00:08:29,400 --> 00:08:30,400
in the brain.

109
00:08:30,400 --> 00:08:36,640
So they tell us about communication patterns that are relevant in terms of cognitive functionality

110
00:08:36,640 --> 00:08:38,720
and biological functionality.

111
00:08:38,720 --> 00:08:47,040
And so based on this data, there have been a lot of sort of work of trying to formulate

112
00:08:47,040 --> 00:08:51,000
machine learning questions and they typically involve predictions.

113
00:08:51,000 --> 00:08:59,920
So can we predict based on this functional connectivity data, which subjects have a neurological

114
00:08:59,920 --> 00:09:02,120
disorder in which don't?

115
00:09:02,120 --> 00:09:08,840
And as sort of in conjunction with that prediction, can we identify different co-activation patterns

116
00:09:08,840 --> 00:09:16,120
or connectivity patterns in the brain that are sort of predictive of eventual diagnosis?

117
00:09:16,120 --> 00:09:19,560
And so that is kind of the general area.

118
00:09:19,560 --> 00:09:25,480
Now the work that we're doing right now is going one step beyond just a simple binary

119
00:09:25,480 --> 00:09:29,560
classification, so prediction of case versus controls.

120
00:09:29,560 --> 00:09:36,800
And we're trying to say, can we from this FMI data predict clinical severity of a particular

121
00:09:36,800 --> 00:09:37,800
patient?

122
00:09:37,800 --> 00:09:43,600
And we've the data that we've been using and our focus thus far has been on autism spectrum

123
00:09:43,600 --> 00:09:45,600
disorder.

124
00:09:45,600 --> 00:09:51,280
And the reason that sort of this continuous prediction task is relevant is because in

125
00:09:51,280 --> 00:09:57,400
any neuropsychiatric disorder, you just have a range of different behavioral characteristics,

126
00:09:57,400 --> 00:10:04,240
different symptoms, severity, different ability to respond to various interventions.

127
00:10:04,240 --> 00:10:09,760
And so building these predictive models, first of all, it allows you just in a black

128
00:10:09,760 --> 00:10:15,880
box sense to figure out what are strengths and weaknesses of different patients in your cohort.

129
00:10:15,880 --> 00:10:22,080
And also incorporating these ideas of biomarker discovery, if you can emphasize or if you

130
00:10:22,080 --> 00:10:27,720
can pick out or extract certain patterns in the brain that are relevant for understanding

131
00:10:27,720 --> 00:10:33,480
clinical severity or different manifestations, it might give other researchers a clue as

132
00:10:33,480 --> 00:10:39,920
to how do we develop better behavioral therapies, how do we evaluate behavioral therapies in

133
00:10:39,920 --> 00:10:44,840
terms of observing these patterns and watching as they grow in fade.

134
00:10:44,840 --> 00:10:52,240
And potentially, how do we develop new therapies based on other imaging techniques or other

135
00:10:52,240 --> 00:10:59,120
modalities such as drug discovery or electrostimulation, et cetera?

136
00:10:59,120 --> 00:11:03,080
How far along are you in this line of research?

137
00:11:03,080 --> 00:11:08,000
So this, again, it's very much at the level of basic exploration.

138
00:11:08,000 --> 00:11:11,000
So right now, we have developed frameworks.

139
00:11:11,000 --> 00:11:16,840
So they're, again, new machine learning frameworks that couple this discovery component, which

140
00:11:16,840 --> 00:11:21,920
is interpretable, which we're using a dictionary learning type framework with the predictive

141
00:11:21,920 --> 00:11:22,920
modeling.

142
00:11:22,920 --> 00:11:26,280
So just at a high level, a regression type framework.

143
00:11:26,280 --> 00:11:31,320
And so we've been able to, unlike other methods that we've seen and other methods that

144
00:11:31,320 --> 00:11:36,880
we've tried, we're able to predict severity to some extent, and we're even able to predict

145
00:11:36,880 --> 00:11:38,120
multi-score severity.

146
00:11:38,120 --> 00:11:46,560
So if you are looking at different quantifications of the patient, so being able to simultaneously

147
00:11:46,560 --> 00:11:53,040
understand those, with that said, I mean, there's certainly a prediction error, which is

148
00:11:53,040 --> 00:11:56,240
one direction we're moving in.

149
00:11:56,240 --> 00:12:01,160
And I think the way to tackle this is another aspect that machine learning is very good at,

150
00:12:01,160 --> 00:12:03,920
which is integrating data across different data sets.

151
00:12:03,920 --> 00:12:09,000
And so now we're trying to bring in other imaging types to get a more comprehensive picture

152
00:12:09,000 --> 00:12:14,880
of brain functionality or brain communication or connectivity, hopefully to improve that

153
00:12:14,880 --> 00:12:16,320
moving forward.

154
00:12:16,320 --> 00:12:21,160
But at the same time, our predictive models right now, they're performing at what is currently

155
00:12:21,160 --> 00:12:23,160
state of the art in the field.

156
00:12:23,160 --> 00:12:26,000
And at the same time, we're able to preserve that interpretability.

157
00:12:26,000 --> 00:12:31,720
And so we're finding patterns and interactions that are sort of interpretable from the autism

158
00:12:31,720 --> 00:12:38,840
standpoint and could eventually provide some sort of biomarker that might be meaningful.

159
00:12:38,840 --> 00:12:45,400
You mentioned that the models that you're using are state of the art, are there well-established

160
00:12:45,400 --> 00:12:51,200
benchmarks for these types of problems?

161
00:12:51,200 --> 00:12:56,360
In this space, so in the neuro space and in the functional connectomics space, I think

162
00:12:56,360 --> 00:13:03,440
one of the challenges is that we don't have very good benchmarks, both in terms of methodology

163
00:13:03,440 --> 00:13:06,960
and in terms of data.

164
00:13:06,960 --> 00:13:13,040
And so this is one of the reasons why the kind of neuro space and especially functional

165
00:13:13,040 --> 00:13:19,840
neuro imaging is unlike other areas where AI or machine learning has made very rapid

166
00:13:19,840 --> 00:13:20,840
advancement.

167
00:13:20,840 --> 00:13:27,120
So if you think about computer vision and image recognition and activity recognition, there

168
00:13:27,120 --> 00:13:33,960
are vast open source data sets, some millions and millions of examples, and here we are developing

169
00:13:33,960 --> 00:13:38,880
machine learning frameworks that essentially have to learn from tens of examples.

170
00:13:38,880 --> 00:13:45,120
And tens of examples to learn very complex functions means that sort of out of the box

171
00:13:45,120 --> 00:13:46,640
algorithms tend to fail.

172
00:13:46,640 --> 00:13:51,840
And so it's a lot more about building structured assumptions into the model in part to reduce

173
00:13:51,840 --> 00:13:57,080
the parameter space and in part to try and guide what you think is reasonable around all

174
00:13:57,080 --> 00:13:59,560
of the noise in the data itself.

175
00:13:59,560 --> 00:14:05,200
I'd love to dig a little bit deeper into the models and this framework.

176
00:14:05,200 --> 00:14:08,320
And when you say models in framework, are you using those interchangeably more or less

177
00:14:08,320 --> 00:14:11,560
or did they have very distinct meanings for you?

178
00:14:11,560 --> 00:14:13,320
No, I use them interchangeably.

179
00:14:13,320 --> 00:14:20,800
Okay, and so you mentioned that the framework is, you said dictionary layered, and I

180
00:14:20,800 --> 00:14:26,320
missed, I think I missed a word there, dictionary learning, dictionary learning.

181
00:14:26,320 --> 00:14:33,880
And so can you elaborate on how you're using the elements of the framework to incorporate

182
00:14:33,880 --> 00:14:40,560
in the operating knowledge about the relationships between the system level knowledge that you

183
00:14:40,560 --> 00:14:41,560
mentioned?

184
00:14:41,560 --> 00:14:50,360
So the way we are incorporating or structuring the model, it's in essence, we, so the dictionary

185
00:14:50,360 --> 00:14:55,440
learning component, or you can think of it as a basis expansion component, essentially

186
00:14:55,440 --> 00:15:00,760
what it assumes is that the kind of global connectivity pattern that we observe, which

187
00:15:00,760 --> 00:15:08,000
in our case is an input positive semi-definite correlation matrix, it is explained or it can

188
00:15:08,000 --> 00:15:14,080
be represented by a sparse collection of what we call elementary subnetworks.

189
00:15:14,080 --> 00:15:18,680
So these are canonical patterns of co-activation across the brain.

190
00:15:18,680 --> 00:15:22,160
You mentioned positive semi-definite connection matrix.

191
00:15:22,160 --> 00:15:27,760
This is your sparse matrix of connections between these different, and what level are

192
00:15:27,760 --> 00:15:28,760
you looking at?

193
00:15:28,760 --> 00:15:33,440
Are these connections between neurons or larger structures in the brain?

194
00:15:33,440 --> 00:15:38,800
So the correlation matrix, it's a Pearson correlation matrix.

195
00:15:38,800 --> 00:15:44,560
The dimensionality is region, so region in the brain.

196
00:15:44,560 --> 00:15:51,480
So if you think of the brain, you can parsulate it into different regions, and there are standard

197
00:15:51,480 --> 00:16:00,720
anatomical atlases that capture specific structures, and so they try to parsulate in a biologically

198
00:16:00,720 --> 00:16:02,400
meaningful fashion.

199
00:16:02,400 --> 00:16:08,480
So the benefit of region parsulation is that you reduce the amount of noise that's there

200
00:16:08,480 --> 00:16:14,120
if you take individual voxels, which are kind of the smallest unit of volume, very analogous

201
00:16:14,120 --> 00:16:16,280
to pixels and an image.

202
00:16:16,280 --> 00:16:21,280
So at the voxel level, this data is very, very noisy and very variable.

203
00:16:21,280 --> 00:16:27,240
So by kind of averaging across a slightly larger region, you tend to reduce some of that

204
00:16:27,240 --> 00:16:29,080
random noise.

205
00:16:29,080 --> 00:16:36,760
So once you've done that parsulation, you essentially have in this FMRI data, a time course or a

206
00:16:36,760 --> 00:16:39,080
signal from every region.

207
00:16:39,080 --> 00:16:43,280
And so you can compute a correlation matrix where every element of the correlation matrix

208
00:16:43,280 --> 00:16:47,120
is just the correlation between the time course at one region and the time course at another

209
00:16:47,120 --> 00:16:48,120
region.

210
00:16:48,120 --> 00:16:53,240
And so in this sort of functional connectivity, brain connectivity space, that's a standard

211
00:16:53,240 --> 00:16:56,520
input that people use into modeling frameworks.

212
00:16:56,520 --> 00:17:02,120
And it's kind of the the elementary unit of information that we tend to extract from

213
00:17:02,120 --> 00:17:05,240
these resting state FMRI data.

214
00:17:05,240 --> 00:17:08,440
Are these connectivity matrices?

215
00:17:08,440 --> 00:17:15,080
Do you get them as a essentially a time series of these connectivity matrices?

216
00:17:15,080 --> 00:17:20,520
So there has been a little work in looking at dynamic evolution.

217
00:17:20,520 --> 00:17:26,440
The most common approach is to kind of compute a single correlation matrix across the entire

218
00:17:26,440 --> 00:17:27,440
acquisition.

219
00:17:27,440 --> 00:17:31,560
So the acquisition is about six minutes long.

220
00:17:31,560 --> 00:17:38,000
So if you think of the sort of N by N region by region as the dimensionality of this correlation

221
00:17:38,000 --> 00:17:41,840
matrix, that's the input to the model.

222
00:17:41,840 --> 00:17:47,840
And so we assume that input can be represented by sort of a low-rankty composition of elementary

223
00:17:47,840 --> 00:17:48,840
subnetwork.

224
00:17:48,840 --> 00:17:53,720
So a subnetwork you can think of as a pattern of co-activation across the brain.

225
00:17:53,720 --> 00:17:58,200
So almost like a heat map of which regions are co-activating or highly correlated with

226
00:17:58,200 --> 00:18:01,800
each other in which regions are anticorrelated with each other.

227
00:18:01,800 --> 00:18:04,680
And so that is kind of the structured assumption.

228
00:18:04,680 --> 00:18:07,600
One of the structured assumptions on the model.

229
00:18:07,600 --> 00:18:16,200
The second assumption is that what is differing on an individual level is the contribution

230
00:18:16,200 --> 00:18:24,600
of those subnetworks to kind of create an entire brain connectivity or entire brain functionality.

231
00:18:24,600 --> 00:18:32,840
And so because those contributions, so not only are they salient for the data representation,

232
00:18:32,840 --> 00:18:36,480
so those are the features that we are using in the predictive modeling.

233
00:18:36,480 --> 00:18:40,480
And so there's a coupling between the data representation and the predictive modeling.

234
00:18:40,480 --> 00:18:45,480
And so in the predictive modeling, what we're trying to do is actually predict some measure

235
00:18:45,480 --> 00:18:47,200
of clinical severity.

236
00:18:47,200 --> 00:18:53,400
So in autism, there are kind of different batteries that you can use to quantify clinical

237
00:18:53,400 --> 00:18:54,400
severity.

238
00:18:54,400 --> 00:18:58,200
The most common is called the Autism Diagnostic Observation Schedule.

239
00:18:58,200 --> 00:19:01,480
So it's typically administered on children.

240
00:19:01,480 --> 00:19:09,160
And it's like a clinician evaluation where they kind of create settings for the child

241
00:19:09,160 --> 00:19:16,160
to play and the child to describe sort of different stories and to observe different

242
00:19:16,160 --> 00:19:19,480
sort of movie clips and explain what they think is happening.

243
00:19:19,480 --> 00:19:25,040
And so there's kind of a standard battery that is used to come up with a level of severity

244
00:19:25,040 --> 00:19:27,640
or level of deficit for autism.

245
00:19:27,640 --> 00:19:35,920
And so your model is making predictions into the space of this diagnostic system.

246
00:19:35,920 --> 00:19:41,200
And what does that look like, is that a kind of a one-to-ten numeric scale, or is it multi-dimensional

247
00:19:41,200 --> 00:19:46,200
across different types of behaviors or expressions of the disorder?

248
00:19:46,200 --> 00:19:54,400
The ADOS exam, so typically you would use the kind of the total measure that the clinician

249
00:19:54,400 --> 00:19:59,160
quantifies across this behavioral paradigm and get a single number.

250
00:19:59,160 --> 00:20:04,120
And it ranges from approximately zero to 30, just based on how it's designed.

251
00:20:04,120 --> 00:20:09,760
There are other diagnostic criteria and they were also using and also, and they provide

252
00:20:09,760 --> 00:20:12,400
a little bit different perspective of the patient.

253
00:20:12,400 --> 00:20:16,680
So the second one is called the Social Responsiveness Scale.

254
00:20:16,680 --> 00:20:19,280
So this is actually a parent report.

255
00:20:19,280 --> 00:20:23,560
So instead of observing the child, you give a questionnaire to a parent or a caregiver

256
00:20:23,560 --> 00:20:26,680
or a teacher about the behavior of the child.

257
00:20:26,680 --> 00:20:31,720
And that's a different way of scoring in terms of clinical manifestation.

258
00:20:31,720 --> 00:20:38,480
And then we're also using another type of behavioral paradigm that was developed by our, is commonly

259
00:20:38,480 --> 00:20:43,600
used by our collaborators at the Kennedy Krieger Institute.

260
00:20:43,600 --> 00:20:50,440
And so this is a, it's a behavioral paradigm that essentially measures the ability of autistic

261
00:20:50,440 --> 00:20:56,000
children to perform gestures on command and also to imitate gestures.

262
00:20:56,000 --> 00:21:03,520
And so one of the interesting observations of children with autism is that in addition

263
00:21:03,520 --> 00:21:10,080
to the well-known social issues, there are also issues with visual and motor systems.

264
00:21:10,080 --> 00:21:12,040
So visual and motor integration.

265
00:21:12,040 --> 00:21:17,320
So loosely hand-eye coordination is one of them ability to imitate gestures is another.

266
00:21:17,320 --> 00:21:21,800
And they, that type of deficit almost parallels the social dysfunction.

267
00:21:21,800 --> 00:21:26,120
So that's a different scale that we're also trying to predict as well.

268
00:21:26,120 --> 00:21:33,240
You mentioned earlier that the frameworks that you've developed achieve state-of-the-art

269
00:21:33,240 --> 00:21:39,440
performance, is that relative to other machine learning approaches?

270
00:21:39,440 --> 00:21:47,840
Or is this a scenario where you're comparing the result of your system to a clinician's

271
00:21:47,840 --> 00:21:49,800
ability to make predictions?

272
00:21:49,800 --> 00:21:54,600
I don't imagine clinicians are looking at FMRI data and trying to predict autism or

273
00:21:54,600 --> 00:21:57,120
am I wrong there?

274
00:21:57,120 --> 00:21:58,120
You are correct.

275
00:21:58,120 --> 00:22:01,360
Clinicians do not use FMRI data to predict autism.

276
00:22:01,360 --> 00:22:08,720
So in most neuropsychiatric disorders, the prediction is, well, the diagnosis is based

277
00:22:08,720 --> 00:22:12,960
on behavioral information and behavioral testing.

278
00:22:12,960 --> 00:22:21,160
So when I say state-of-the-art, what I'm trying to convey is just in relation to other machine

279
00:22:21,160 --> 00:22:22,160
learning algorithms.

280
00:22:22,160 --> 00:22:28,640
So on this data set, we have implemented a variety of algorithms from kind of basic regression

281
00:22:28,640 --> 00:22:34,600
models, which are very, very early iterations of machine learning through other kind of

282
00:22:34,600 --> 00:22:39,520
regression, well, sort of, so-port vector regression, random forests, which are a little

283
00:22:39,520 --> 00:22:45,200
bit more current, and we've even tried end-to-end deep learning approaches.

284
00:22:45,200 --> 00:22:50,920
And so in comparison to that spectrum, at least on our data set, we have found this kind

285
00:22:50,920 --> 00:22:55,760
of hybrid approach where we're looking, we're sort of coupling a data representation,

286
00:22:55,760 --> 00:23:00,080
as well as the predictive modeling to essentially perform the best.

287
00:23:00,080 --> 00:23:01,080
Okay.

288
00:23:01,080 --> 00:23:05,120
And with the advantage that you retain some degree of interpretability, which you may

289
00:23:05,120 --> 00:23:08,320
sacrifice in the end-to-end deep learning space.

290
00:23:08,320 --> 00:23:09,320
Yes.

291
00:23:09,320 --> 00:23:17,280
And so the work that you're doing, looking at predicting autism here, this is just one

292
00:23:17,280 --> 00:23:24,640
of the many disorders that you have looked at in your research, and another one is epilepsy.

293
00:23:24,640 --> 00:23:27,280
Can you talk a little bit about that work?

294
00:23:27,280 --> 00:23:28,280
Sure.

295
00:23:28,280 --> 00:23:34,880
So the epilepsy project, it's a little distinct from what we're trying to do with autism,

296
00:23:34,880 --> 00:23:40,640
and that the work with epilepsy is closer to clinical translation.

297
00:23:40,640 --> 00:23:46,920
So here, our goal is to actually provide information that would be relevant to clinicians

298
00:23:46,920 --> 00:23:49,520
as they're treating these patients.

299
00:23:49,520 --> 00:23:55,480
And so just to kind of give you a background on the application itself, so epilepsy is

300
00:23:55,480 --> 00:23:58,760
one of the most common neurological disorders.

301
00:23:58,760 --> 00:24:03,720
And so in general, the first line of defense is medication, and there are a variety of

302
00:24:03,720 --> 00:24:06,320
anti-aplectic drugs that have been developed.

303
00:24:06,320 --> 00:24:11,400
However, it's estimated that 20 to 40% of patients don't respond to medication in the

304
00:24:11,400 --> 00:24:13,800
sense that they'll continue to have seizures.

305
00:24:13,800 --> 00:24:17,840
And this is really our target cohort.

306
00:24:17,840 --> 00:24:23,360
And so for these patients that are called medically refractory, so they don't respond to these

307
00:24:23,360 --> 00:24:28,240
epileptic drugs, there's very limited alternatives.

308
00:24:28,240 --> 00:24:34,840
And it turns out the best alternative that we have right now is if we can identify, and

309
00:24:34,840 --> 00:24:41,040
if we can trace the seizures to a specific region in the brain that's triggering them,

310
00:24:41,040 --> 00:24:45,640
then clinicians can go in and surgically remove that part of the brain.

311
00:24:45,640 --> 00:24:51,520
And currently, that's kind of state of the art in terms of care, and that will probably

312
00:24:51,520 --> 00:24:58,220
have the best likelihood of the patient recovering in terms of their seizures being alleviated.

313
00:24:58,220 --> 00:25:04,360
And so our focus is on this realm of seizure detection and localization.

314
00:25:04,360 --> 00:25:08,960
So detecting when a seizure occurs through sort of time series measurements and also being

315
00:25:08,960 --> 00:25:13,880
able to localize what is the general area of the brain and what is a specific area of

316
00:25:13,880 --> 00:25:20,080
the brain that the seizures might be coming from, so that again, once we have that target,

317
00:25:20,080 --> 00:25:21,880
it can be acted upon.

318
00:25:21,880 --> 00:25:29,600
And so we're using a variety of machine learning tools and developing new frameworks to be

319
00:25:29,600 --> 00:25:36,480
able to look at non-invasive data, so data collected from EEG and from MRI in order

320
00:25:36,480 --> 00:25:40,800
to provide that localization information to clinicians.

321
00:25:40,800 --> 00:25:44,640
Can you talk a little bit about some of the models that you've developed in some more

322
00:25:44,640 --> 00:25:45,640
detail?

323
00:25:45,640 --> 00:25:46,640
Sure.

324
00:25:46,640 --> 00:25:53,120
So much of the work, especially in the last few years, has focused on EEG data.

325
00:25:53,120 --> 00:26:02,000
So EEG and Zophography is probably the first type of data that's collected for patients

326
00:26:02,000 --> 00:26:04,000
when they go into a hospital.

327
00:26:04,000 --> 00:26:10,400
And here they will actually be admitted into an epilepsy monitoring unit and these EEGs,

328
00:26:10,400 --> 00:26:14,920
so they're kind of sensors that are adhered to the scalp externally.

329
00:26:14,920 --> 00:26:19,400
They're placed on the patient and then the patient is monitored over several days.

330
00:26:19,400 --> 00:26:25,120
So that when they have seizures, clinicians can record that activity happening.

331
00:26:25,120 --> 00:26:32,800
And so currently the state of the sort of standard of care is actually to identify seizures

332
00:26:32,800 --> 00:26:35,600
both detect and localize by eye.

333
00:26:35,600 --> 00:26:42,880
So essentially a clinician will view all of these signals kind of on a computer screen

334
00:26:42,880 --> 00:26:48,720
and scroll through them and then try and visually identify markers of a seizure.

335
00:26:48,720 --> 00:26:53,920
So when it starts and also kind of based on when it starts trying to trace where they

336
00:26:53,920 --> 00:26:57,040
think it's happening, it's originating in the brain.

337
00:26:57,040 --> 00:27:03,480
From the EEG meaning they're looking at which of the leads, is it as simple as which

338
00:27:03,480 --> 00:27:11,000
of the leads is closest to a region that is likely to be functioning in this way?

339
00:27:11,000 --> 00:27:15,880
It's very similar, it's kind of which of the leads are manifesting these particular

340
00:27:15,880 --> 00:27:21,120
signatures that they've been trained to recognize.

341
00:27:21,120 --> 00:27:26,720
And so as you can imagine this is very time consuming, it's prone to human error.

342
00:27:26,720 --> 00:27:33,320
And so just one very kind of on the surface simple task is just developing machine learning

343
00:27:33,320 --> 00:27:36,920
algorithms that can do this automated detection.

344
00:27:36,920 --> 00:27:43,000
So essentially augmenting or replacing kind of what they're doing currently.

345
00:27:43,000 --> 00:27:46,720
And it turns out this is a very challenging problem.

346
00:27:46,720 --> 00:27:50,600
And it's challenging because EEG data is extremely noisy.

347
00:27:50,600 --> 00:27:56,160
And in fact the noise in EEG data tends to overwhelm the signal in terms of the particular

348
00:27:56,160 --> 00:28:02,480
seizure signatures and the particular seizure signatures look a lot like baseline EEG.

349
00:28:02,480 --> 00:28:08,240
So if you or I were to look at these EEG recordings, we would not be able to tell at all what

350
00:28:08,240 --> 00:28:10,080
the origin of the seizure is.

351
00:28:10,080 --> 00:28:14,680
It's kind of years and years of training and sort of pattern matching and building up

352
00:28:14,680 --> 00:28:21,600
models in their heads over kind of what particular features are we looking for and these features

353
00:28:21,600 --> 00:28:25,080
might change from patient to patient, etc.

354
00:28:25,080 --> 00:28:31,040
And so the way we're approaching it is to recognize that just at instantaneous points

355
00:28:31,040 --> 00:28:35,920
in time this data is extremely noisy and we might not be able to get a good detection.

356
00:28:35,920 --> 00:28:42,360
So kind of treat it as a temporal process and essentially to recognize that perhaps

357
00:28:42,360 --> 00:28:48,200
the spreading of these seizure activity or these abnormal activities are just as meaningful

358
00:28:48,200 --> 00:28:51,360
for identifying where and when the seizure is starting.

359
00:28:51,360 --> 00:28:55,920
So if a seizure is starting in a particular area of the brain, so that'll correspond to

360
00:28:55,920 --> 00:29:00,480
a certain area of EEG electrodes.

361
00:29:00,480 --> 00:29:05,400
We expect that that activity will spread locally before it just jumps randomly to another

362
00:29:05,400 --> 00:29:06,400
area.

363
00:29:06,400 --> 00:29:12,920
And so by modeling this kind of local spreading pattern and then inferring kind of where

364
00:29:12,920 --> 00:29:17,960
and when it starts from the data, we can potentially do a better job of both detection

365
00:29:17,960 --> 00:29:19,480
and localization.

366
00:29:19,480 --> 00:29:24,080
So the frameworks we're using here are based on probabilistic graphical models.

367
00:29:24,080 --> 00:29:30,240
And so roughly these probabilistic graphical models allow you to specify sort of latent

368
00:29:30,240 --> 00:29:35,400
or hidden random variables and they capture unoperable phenomenon in our case, the spreading

369
00:29:35,400 --> 00:29:37,000
of the seizure activity.

370
00:29:37,000 --> 00:29:42,080
And then there's observed variables which are related to the statistics of your data

371
00:29:42,080 --> 00:29:45,240
or the features of your data that you're interested in.

372
00:29:45,240 --> 00:29:50,600
And so sort of at a high level, these are, this is the framework that we're looking at.

373
00:29:50,600 --> 00:29:55,600
And we're actually embedding some deep learning into this because the latent variables, they

374
00:29:55,600 --> 00:30:00,000
give us interpretability because what we really care about is the progression of a seizure

375
00:30:00,000 --> 00:30:04,080
and backtracking the onset of seizure activity.

376
00:30:04,080 --> 00:30:09,320
And at the same time, we have a lot of EEG data and so we're training deep neural networks

377
00:30:09,320 --> 00:30:14,480
or artificial neural networks as a complex likelihood function.

378
00:30:14,480 --> 00:30:18,880
So being able to mine different patterns from the data and kind of feed that into the more

379
00:30:18,880 --> 00:30:21,280
interpretable element of the model.

380
00:30:21,280 --> 00:30:22,280
Interesting.

381
00:30:22,280 --> 00:30:24,840
It sounds like a super challenging problem.

382
00:30:24,840 --> 00:30:31,400
And one of the things that jumps out at me is we're often benchmarking or training on

383
00:30:31,400 --> 00:30:38,600
ground truth data and I wonder, how do we know how accurate the physicians are, right?

384
00:30:38,600 --> 00:30:45,840
And they're in the sense of, you know, they are making decisions based on these very

385
00:30:45,840 --> 00:30:49,320
noisy signals, the same noisy signals that you have to deal with.

386
00:30:49,320 --> 00:30:58,120
And they make a decision to take some action, maybe remove part of someone's brain.

387
00:30:58,120 --> 00:31:09,520
But how well do we know if at all how right they were in that it strikes me as just very

388
00:31:09,520 --> 00:31:16,840
difficult to kind of localize from EEG data to a specific part of a brain that's not

389
00:31:16,840 --> 00:31:18,920
acting correctly?

390
00:31:18,920 --> 00:31:21,400
So that's a fantastic question.

391
00:31:21,400 --> 00:31:24,240
So I think there are many layers to that question.

392
00:31:24,240 --> 00:31:30,120
So in terms of clinician accuracy, it's actually unclear how accurate they are.

393
00:31:30,120 --> 00:31:33,480
So they do have a lot more information than just the EEG.

394
00:31:33,480 --> 00:31:39,920
They have the patient history, patient behavior during a seizure is actually supposed to

395
00:31:39,920 --> 00:31:45,760
be fairly relevant in terms of likely areas that the seizure might start.

396
00:31:45,760 --> 00:31:52,080
Oftentimes they have MRI data, so a neuro radiologist can go through it and look for just

397
00:31:52,080 --> 00:31:54,280
structural abnormalities.

398
00:31:54,280 --> 00:32:02,160
I don't know and I haven't come across a systematic study that kind of quantifies asking a variety

399
00:32:02,160 --> 00:32:06,760
of experts across a variety of institutions and then try and understand kind of coordinates

400
00:32:06,760 --> 00:32:08,880
between them.

401
00:32:08,880 --> 00:32:16,640
One interesting statistic is that if you look at meta reviews, long-term seizure freedom,

402
00:32:16,640 --> 00:32:21,680
so postoperative seizure freedom, so if they actually go in and remove kind of a part

403
00:32:21,680 --> 00:32:29,400
of the brain, it's not dramatically high, so after, so if you're looking at five years

404
00:32:29,400 --> 00:32:32,600
seizure freedom rates, it's only about 50%.

405
00:32:32,600 --> 00:32:33,600
Okay.

406
00:32:33,600 --> 00:32:34,600
Right.

407
00:32:34,600 --> 00:32:41,400
So it's, again, there's many factors that could influence the 50%, so it could be that

408
00:32:41,400 --> 00:32:46,880
that particular patient develops some other type of lesion or problem and that took over,

409
00:32:46,880 --> 00:32:50,320
or it could be that there was an error in the care pathway.

410
00:32:50,320 --> 00:32:54,000
So maybe that patient should never have gone for surgery because it turns out there are

411
00:32:54,000 --> 00:32:59,760
multiple areas of the brain that were triggering seizures and that was missed during the review,

412
00:32:59,760 --> 00:33:05,520
or maybe the initial localization was incorrect and so the incorrect portion of the brain was

413
00:33:05,520 --> 00:33:06,520
removed.

414
00:33:06,520 --> 00:33:11,520
And so I think it's unclear which of those factors are involved and hopefully by inserting

415
00:33:11,520 --> 00:33:17,320
some machine learning into that process, it's not that we would replace clinicians because

416
00:33:17,320 --> 00:33:22,160
I don't think our algorithms are nearly at that point yet, but we might be able to provide

417
00:33:22,160 --> 00:33:28,240
information and if we, if the algorithm identifies kind of contradictory information to what

418
00:33:28,240 --> 00:33:33,200
the initial evaluation was, it'll allow the clinician to go back and really focus on

419
00:33:33,200 --> 00:33:37,280
that other area and say, do I really think there's a problem there or is there something

420
00:33:37,280 --> 00:33:39,200
I might have missed?

421
00:33:39,200 --> 00:33:43,680
Maybe we should check it out with another type of data modality.

422
00:33:43,680 --> 00:33:51,480
And are there, are you or are there folks doing multimodal models here?

423
00:33:51,480 --> 00:33:59,040
It strikes me that, you know, in your description of the resources that the physicians have

424
00:33:59,040 --> 00:34:04,880
access to to try to make a prediction, it's much more than just, you know, the time series

425
00:34:04,880 --> 00:34:08,760
date off of an EEG leaf, there's a lot more that they're looking at.

426
00:34:08,760 --> 00:34:15,280
And so it would ultimately we'd want models to be able to incorporate more of that data

427
00:34:15,280 --> 00:34:18,240
as well as that happening in the research.

428
00:34:18,240 --> 00:34:24,640
So that is exactly where my research is headed and so the work that we're doing in the lab.

429
00:34:24,640 --> 00:34:30,120
So we've been focusing on EEG, but just because it's more readily available.

430
00:34:30,120 --> 00:34:37,000
And we recently received some internal funding, a competitive internal funding award to acquire

431
00:34:37,000 --> 00:34:40,240
a multimodal MRI data for some of these patients.

432
00:34:40,240 --> 00:34:48,080
And so some of the work that I had done in my postdoc a while back was, well, showed that

433
00:34:48,080 --> 00:34:53,080
the resting state of MRI that I had mentioned earlier for the autism project that might actually

434
00:34:53,080 --> 00:34:59,720
be useful as another biomarker of seizure origin, especially in cases where there is no

435
00:34:59,720 --> 00:35:03,720
obvious physical lesion that you can see in the brain.

436
00:35:03,720 --> 00:35:09,160
And so by incorporating this resting state of MRI information, structural MR, we're hoping

437
00:35:09,160 --> 00:35:12,120
we can get a finer grain picture.

438
00:35:12,120 --> 00:35:16,640
And then again, once you have models in different modalities, you can look at concordance between

439
00:35:16,640 --> 00:35:23,080
them to first identify is this a reasonable surgical candidate and also what is the potential

440
00:35:23,080 --> 00:35:29,360
area where is most likely the seizure local localization area.

441
00:35:29,360 --> 00:35:35,040
So we've talked about some of the basic science work you're doing, the epilepsy work as

442
00:35:35,040 --> 00:35:39,720
an example of how you're trying to translate to treatment.

443
00:35:39,720 --> 00:35:44,520
You also mentioned work around perception, what do you mean there?

444
00:35:44,520 --> 00:35:47,440
So this is an interesting project.

445
00:35:47,440 --> 00:35:54,600
It started as kind of a pet project idea of mine, but I guess we've tried to pursue it.

446
00:35:54,600 --> 00:36:01,520
So the sort of at a high level, what the project is trying to do is manipulate emotional

447
00:36:01,520 --> 00:36:03,960
cues in human speech.

448
00:36:03,960 --> 00:36:10,560
The way this project idea came about is actually a lot of the work that I had done on autism.

449
00:36:10,560 --> 00:36:15,480
So my postdoc at Yale focused almost exclusively on autism.

450
00:36:15,480 --> 00:36:21,920
Again, it was more through imaging, but at the same time, I was thinking that one of

451
00:36:21,920 --> 00:36:28,520
the one of the hallmark features of autism is that the patients have difficulty perceiving

452
00:36:28,520 --> 00:36:33,360
social and emotional cues and that's particularly true in verbal.

453
00:36:33,360 --> 00:36:40,320
So in language language domain, and so what if we could take speech and amplify emotional

454
00:36:40,320 --> 00:36:45,720
cues to the point where an individual with autism could understand them readily, right?

455
00:36:45,720 --> 00:36:50,040
So if it's very exaggerated, it's at least high functioning individuals don't tend to have

456
00:36:50,040 --> 00:36:51,200
a problem.

457
00:36:51,200 --> 00:36:55,760
It's when it's more subtle that they tend to differ in terms of their perception relative

458
00:36:55,760 --> 00:36:58,240
to their peers who don't have autism.

459
00:36:58,240 --> 00:37:02,760
So if we could do that amplification, and if we could do it computationally, maybe we

460
00:37:02,760 --> 00:37:09,520
can use it as either way to study autism or as an assistive technology, right?

461
00:37:09,520 --> 00:37:14,360
And by doing it computationally, you also have the benefit that you can start slowly undoing

462
00:37:14,360 --> 00:37:15,360
it, right?

463
00:37:15,360 --> 00:37:21,680
So you can think about doing a really well, so a really exaggerated emotion amplification

464
00:37:21,680 --> 00:37:26,880
and then over time reducing it or over the course of an experiment reducing it and trying

465
00:37:26,880 --> 00:37:31,880
sort of quantify at the level an individual can perceive it.

466
00:37:31,880 --> 00:37:39,120
It turns out that no one knows how to amplify emotional cues and speech, it's an unsolved

467
00:37:39,120 --> 00:37:40,120
problem.

468
00:37:40,120 --> 00:37:47,800
I was going to ask that in order to be able to do this amplification based on, you know,

469
00:37:47,800 --> 00:37:53,760
with a dial that adjusts to the level of requirement, we need to be able to do it at all

470
00:37:53,760 --> 00:37:56,240
and I have not seen that.

471
00:37:56,240 --> 00:38:02,280
I think we're making a lot of progress on the, you know, detecting emotions via, you

472
00:38:02,280 --> 00:38:09,160
know, facial image data and speech data, but I've not seen much in terms of kind of

473
00:38:09,160 --> 00:38:14,640
modulating steady state speech data to kind of add that kind of inflection.

474
00:38:14,640 --> 00:38:21,840
Yes, it's an unsolved problem and it's actually not a commonly studied problem.

475
00:38:21,840 --> 00:38:27,400
I would say emotion from speech is hard in general, so even emotion recognition from speech

476
00:38:27,400 --> 00:38:31,120
is quite difficult without, without the video data.

477
00:38:31,120 --> 00:38:37,280
I mean, accuracy is tend to be fairly low and then synthesis or this amplification process

478
00:38:37,280 --> 00:38:39,640
is even more challenging.

479
00:38:39,640 --> 00:38:46,160
So essentially, well, in order to actually implement the project idea, we have to figure

480
00:38:46,160 --> 00:38:51,520
out how to manipulate emotions in speech and so that's one area that we're working

481
00:38:51,520 --> 00:38:57,600
on and in order to manipulate emotions in speech, we need a data and it turns out there

482
00:38:57,600 --> 00:39:02,800
are not a lot of parallel data sets, especially for English, for other languages, I think

483
00:39:02,800 --> 00:39:07,360
there exists a few, but for English and by parallel, I mean to say that if you want to

484
00:39:07,360 --> 00:39:14,080
learn a modulation function, you do want examples of consistent sentences and consistent

485
00:39:14,080 --> 00:39:15,080
actors, right?

486
00:39:15,080 --> 00:39:18,200
So you want the same person saying the same thing in different emotions to be able to

487
00:39:18,200 --> 00:39:20,480
learn a modulation function.

488
00:39:20,480 --> 00:39:27,400
And so with a very, very talented undergraduate student and a very talented graduate student,

489
00:39:27,400 --> 00:39:33,000
we collected this data, so we hired actors from the Baltimore area to come in and read different

490
00:39:33,000 --> 00:39:35,640
things with different emotions.

491
00:39:35,640 --> 00:39:42,920
And we have been developing AI frameworks to try and do this emotion morphing process,

492
00:39:42,920 --> 00:39:47,360
which is what we call it, this modulation process.

493
00:39:47,360 --> 00:39:52,920
And I think we have some preliminary success and we're hoping to build off of it, again,

494
00:39:52,920 --> 00:39:58,440
with integrating these model based frameworks and deep learning strategies and it's been

495
00:39:58,440 --> 00:40:03,360
very interesting learning about this other field and this other type of data which I've

496
00:40:03,360 --> 00:40:08,360
never worked with before and what the sort of tricks and subtleties are.

497
00:40:08,360 --> 00:40:11,000
Have you published the data set?

498
00:40:11,000 --> 00:40:20,280
So yes, so we wrote an initial paper on the data set itself and that appeared in your

499
00:40:20,280 --> 00:40:26,080
speech a few months or last month, I'm sorry, and the data is available.

500
00:40:26,080 --> 00:40:32,040
So it's on my lab website, there's a link where you essentially have to fill out a Google

501
00:40:32,040 --> 00:40:37,960
form or will attach to Google form and then once that's done, we'll send you a download

502
00:40:37,960 --> 00:40:41,760
link to actually download the data and play around with it.

503
00:40:41,760 --> 00:40:42,760
Got it.

504
00:40:42,760 --> 00:40:44,760
And what's the scope of the data set?

505
00:40:44,760 --> 00:40:51,720
How many samples across, how many kind of neutral utterances?

506
00:40:51,720 --> 00:40:56,560
So this initial data collection, it was a little small scale.

507
00:40:56,560 --> 00:40:59,640
So what we focused on were very short utterances.

508
00:40:59,640 --> 00:41:05,440
So single words, multi words phrases and then just a simple noun for predicate sentence

509
00:41:05,440 --> 00:41:11,200
structure because of course linguistics play a role in emotion perception and I think

510
00:41:11,200 --> 00:41:16,200
once you get very complicated, the number of linguistic configurations gets to be very

511
00:41:16,200 --> 00:41:17,200
high.

512
00:41:17,200 --> 00:41:19,040
So it's short utterances.

513
00:41:19,040 --> 00:41:27,960
So there are 10 actors I believe and sort of 250 utterances in five different emotions.

514
00:41:27,960 --> 00:41:33,960
So it's a total about six hours if you just sum up the audio clips.

515
00:41:33,960 --> 00:41:37,240
And what's the approach to modulation?

516
00:41:37,240 --> 00:41:44,040
Are you using something that you might use to synthesize speech like a wave net or something

517
00:41:44,040 --> 00:41:50,520
along those lines or are you doing more traditional signal processing?

518
00:41:50,520 --> 00:41:52,720
So it's a blend between the two.

519
00:41:52,720 --> 00:42:00,200
So wave net is its outstanding tool, but the domain it's looking at is text to speech synthesis.

520
00:42:00,200 --> 00:42:03,920
So essentially given text, it'll output speech.

521
00:42:03,920 --> 00:42:08,280
And it'll output speech in this wave net voice essentially.

522
00:42:08,280 --> 00:42:13,400
The problem that we're focusing on is kind of inputting speech and outputting speech.

523
00:42:13,400 --> 00:42:17,120
And so we've tried a couple of different techniques.

524
00:42:17,120 --> 00:42:23,400
So essentially the input speech you can decompose into kind of a pitch contour, a spectrogram

525
00:42:23,400 --> 00:42:25,480
and a periodicity signal.

526
00:42:25,480 --> 00:42:28,840
So it's a standard analysis pipeline.

527
00:42:28,840 --> 00:42:32,920
And so what controls emotion tends to be intonation.

528
00:42:32,920 --> 00:42:40,520
So in terms of perception and the signal that has a greatest role in intonation is the

529
00:42:40,520 --> 00:42:41,520
pitch contour.

530
00:42:41,520 --> 00:42:46,480
So how pitch varies as someone is speaking a word or a phrase.

531
00:42:46,480 --> 00:42:52,080
And so we've tried, so right now we're targeting this pitch contour as kind of this low-dimensional

532
00:42:52,080 --> 00:42:58,360
feature representation that can help us do this emotion manipulation.

533
00:42:58,360 --> 00:43:00,160
And so we've tried a couple of different things.

534
00:43:00,160 --> 00:43:02,640
We've just tried end-to-end pitch prediction.

535
00:43:02,640 --> 00:43:08,000
So inputting a pitch and then for a given emotion, just outputting a pitch value for that

536
00:43:08,000 --> 00:43:10,000
particular frame.

537
00:43:10,000 --> 00:43:15,360
And then we're right now trying kind of a combination of a model-based approach that's

538
00:43:15,360 --> 00:43:22,040
based on sort of registration or aligning signals using differential geometry or exponential

539
00:43:22,040 --> 00:43:23,120
mapping.

540
00:43:23,120 --> 00:43:27,800
And then that framework itself, we need to be able to predict the parameters of that

541
00:43:27,800 --> 00:43:32,960
warping or that registration so that we're using some deep learning approaches to do the

542
00:43:32,960 --> 00:43:34,480
prediction.

543
00:43:34,480 --> 00:43:39,800
And so the combination of the two is quite interesting and we're sort of, again, trying

544
00:43:39,800 --> 00:43:46,200
to go further in terms of the analysis to get a more consistent emotion warping process.

545
00:43:46,200 --> 00:43:53,200
This is a great introduction to some of the work you're doing there across what sounds

546
00:43:53,200 --> 00:43:59,480
like a very broad array of projects, so hats off to you.

547
00:43:59,480 --> 00:44:00,480
Thank you.

548
00:44:00,480 --> 00:44:07,320
I guess I'm curious, your thoughts on kind of where you see this all going.

549
00:44:07,320 --> 00:44:09,840
So I think it depends on the project.

550
00:44:09,840 --> 00:44:15,960
I mean, at a high level, we want to make everything work better than it currently does.

551
00:44:15,960 --> 00:44:21,880
So again, in terms of the basic science, to be able to do better, more targeted predictions

552
00:44:21,880 --> 00:44:25,600
with multimodal data.

553
00:44:25,600 --> 00:44:31,680
So in that same vein, I have some projects, or I have another project on sort of understanding

554
00:44:31,680 --> 00:44:34,040
imaging and genetic interactions.

555
00:44:34,040 --> 00:44:39,360
So and this is for schizophrenia, so being able to identify genetic markers that relate

556
00:44:39,360 --> 00:44:42,320
to imaging and also are predictive of diagnosis.

557
00:44:42,320 --> 00:44:47,760
And so kind of in that basic science realm to again, make the models more robust, more

558
00:44:47,760 --> 00:44:55,840
generalizable across data sets to extract biomarkers and potentially with these collaborations.

559
00:44:55,840 --> 00:45:00,520
As additional data comes in validated on sort of validate the models and algorithms on

560
00:45:00,520 --> 00:45:04,600
those so that they can be broadly disseminated as tools.

561
00:45:04,600 --> 00:45:09,760
I think on the translational side, we certainly want to do a better job of localization.

562
00:45:09,760 --> 00:45:17,680
So we just published probably the first method that kind of can take scalp EEG and then in

563
00:45:17,680 --> 00:45:21,840
many cases just simultaneously detect and localize the seizures.

564
00:45:21,840 --> 00:45:27,600
But again, our accuracy is not as high as it needs to be for clinical translation.

565
00:45:27,600 --> 00:45:32,960
And so to understand what sorts of other information can we put in there, can we use more data

566
00:45:32,960 --> 00:45:35,600
to get a better predictive model.

567
00:45:35,600 --> 00:45:40,520
And also to again see how this performs in a prospective fashion.

568
00:45:40,520 --> 00:45:46,880
And at the same time, look at multimodal MR as another non-invasive conglomeration of

569
00:45:46,880 --> 00:45:52,280
modalities and improve the prediction and be able to track that and validate that the

570
00:45:52,280 --> 00:45:55,040
outcome actually is reasonable.

571
00:45:55,040 --> 00:46:00,280
And then on the kind of the speech front, I think we have some preliminary success in

572
00:46:00,280 --> 00:46:02,120
terms of being able to do this motion.

573
00:46:02,120 --> 00:46:09,120
So emotion warping, so both quantitatively sort of how well do we predict what that emotional

574
00:46:09,120 --> 00:46:11,480
pitch contour looks like.

575
00:46:11,480 --> 00:46:16,360
We've done some qualitative experiments where we've reconstructed the speech and used

576
00:46:16,360 --> 00:46:21,160
Amazon Mechanical Turk to see to have people rate the speech utterances.

577
00:46:21,160 --> 00:46:23,080
But we'd like to go farther than that.

578
00:46:23,080 --> 00:46:28,960
So definitely improve our ability to create emotions and to manipulate emotions also to

579
00:46:28,960 --> 00:46:34,320
be able to generalize to longer phrases and do this in a more real time fashion.

580
00:46:34,320 --> 00:46:39,800
So I think that's the short term and then the long term is in epilepsy, being able

581
00:46:39,800 --> 00:46:47,680
to create an automated pipeline for patients to come in and be diagnosed, sort of plan out

582
00:46:47,680 --> 00:46:52,320
best therapeutic strategy and essentially improve outcomes.

583
00:46:52,320 --> 00:46:57,240
And then for the speech to actually use it to study different neuropsychiatric conditions

584
00:46:57,240 --> 00:47:02,480
and potentially combine them with other assistive technologies.

585
00:47:02,480 --> 00:47:03,480
Great.

586
00:47:03,480 --> 00:47:07,200
Well, thanks so much for taking the time to share all of that with us.

587
00:47:07,200 --> 00:47:12,080
No, thank you again for inviting me and having me speak here.

588
00:47:12,080 --> 00:47:16,280
All right, everyone.

589
00:47:16,280 --> 00:47:18,200
That's our show for today.

590
00:47:18,200 --> 00:47:23,880
To learn more about today's show, including our guests, visit twomelai.com.

591
00:47:23,880 --> 00:47:28,440
If you missed twomelcon or want to share what you learned with your team, be sure to visit

592
00:47:28,440 --> 00:47:34,480
twomelcon.com slash videos for more information about twomelcon video packages.

593
00:47:34,480 --> 00:48:03,000
Thanks so much for listening and catch you next time.

