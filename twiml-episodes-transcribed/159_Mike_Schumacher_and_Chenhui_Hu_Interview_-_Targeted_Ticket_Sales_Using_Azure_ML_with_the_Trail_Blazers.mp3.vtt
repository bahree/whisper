WEBVTT

00:00.000 --> 00:16.200
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.200 --> 00:21.320
people, doing interesting things in machine learning and artificial intelligence.

00:21.320 --> 00:31.280
I'm your host Sam Charrington.

00:31.280 --> 00:36.080
In today's episode of our AI and sport series, I'm joined by Mike Schumacher, director

00:36.080 --> 00:43.280
of business analytics for the Portland Trailblazers and Chen Hui Hu, a data scientist at Microsoft.

00:43.280 --> 00:48.200
In our conversation, Mike Chen Hui and I discuss how the Blazers are using machine learning

00:48.200 --> 00:54.160
to produce better targeted sales campaigns for both single game and season ticket buyers.

00:54.160 --> 00:58.360
Mike describes some of the early use cases the Trailblazers explored in their drive to

00:58.360 --> 01:03.280
apply analytics and machine learning to the process of increasing ticket sales.

01:03.280 --> 01:08.240
And Chen Hui elaborates on some of the unique challenges of the Trailblazers data set

01:08.240 --> 01:12.000
and the modeling techniques the team apply to solve their problem.

01:12.000 --> 01:13.760
All right, let's do it.

01:13.760 --> 01:19.040
All right, everyone. I am on the line with a couple of guests that I've been looking

01:19.040 --> 01:20.720
forward to speaking with.

01:20.720 --> 01:26.840
I've got Mike Schumacher, who is director of business analytics with Portland Trailblazers,

01:26.840 --> 01:32.480
basketball team and Chen Hui Hu, who is a data scientist at Microsoft.

01:32.480 --> 01:35.920
Mike and Chen Hui, welcome to this weekend machine learning and AI.

01:35.920 --> 01:37.560
Hi, Sam. Thanks for having us.

01:37.560 --> 01:39.080
Yeah, thanks for having me.

01:39.080 --> 01:40.080
Absolutely.

01:40.080 --> 01:45.520
Why don't we, in the tradition of the podcast, get started by having the two of you introduce

01:45.520 --> 01:46.520
yourselves.

01:46.520 --> 01:51.000
Mike, how did you get involved in analytics and machine learning?

01:51.000 --> 01:54.000
Sure. So my career started in software development.

01:54.000 --> 01:58.640
I got my masters in enterprise software architecture and was an application developer

01:58.640 --> 02:01.640
for 12 years before coming to the Blazers.

02:01.640 --> 02:07.080
In those first few years before I made it to the Blazers, a lot of my work shifted from

02:07.080 --> 02:12.200
peer development over to data warehousing, business intelligence, data analysis, data

02:12.200 --> 02:13.560
vis, those types of things.

02:13.560 --> 02:17.760
And my first few years with the Blazers was heavily focused on data management, getting

02:17.760 --> 02:22.760
our data warehouse set up, marketing, automation, CRM.

02:22.760 --> 02:26.880
As our systems and processes became more sophisticated, we started looking into machine

02:26.880 --> 02:30.880
learning and more predictive analytics and that's kind of where we're at today.

02:30.880 --> 02:31.880
Okay, great.

02:31.880 --> 02:32.880
And Chen Hui.

02:32.880 --> 02:37.000
Hi, I'm a data scientist at Microsoft AI and the research in Boston.

02:37.000 --> 02:42.320
I joined Microsoft last year and my current interest is to solve prediction and optimization

02:42.320 --> 02:46.160
problems in retail, marketing, and manufacturing.

02:46.160 --> 02:51.520
Before that, I finished my PhD study, focusing on medical image data mining.

02:51.520 --> 02:53.360
That's pretty much my respect.

02:53.360 --> 02:54.360
Awesome.

02:54.360 --> 02:55.360
Awesome.

02:55.360 --> 03:01.680
So Mike, my producer who stays on top of these things tells me that a good place to start

03:01.680 --> 03:07.640
is to congratulate you for putting quite the smack down on the few suns on an opening night.

03:07.640 --> 03:12.960
Yeah, I did not see that coming, especially with one of our best players sitting in

03:12.960 --> 03:13.960
the game out.

03:13.960 --> 03:18.000
So that was unexpected, but a very pleasant surprise for sure.

03:18.000 --> 03:19.000
Absolutely.

03:19.000 --> 03:20.000
Absolutely.

03:20.000 --> 03:23.360
Why don't we get started by having you tell us a little bit about some of the applications

03:23.360 --> 03:32.280
for analytics at the Blazers and how that has grown to involve machine learning?

03:32.280 --> 03:33.280
Sure.

03:33.280 --> 03:38.680
Yeah, we spend a lot of time working with various departments across the organization.

03:38.680 --> 03:41.800
We're almost like the data police at times.

03:41.800 --> 03:45.120
There's a lot of data warehousing that we do.

03:45.120 --> 03:48.000
We have integrations between different systems.

03:48.000 --> 03:51.400
We help with any type of data analytics project.

03:51.400 --> 03:56.680
The department might need help them help them use data to make better business decisions.

03:56.680 --> 04:00.440
So that's kind of the basis of our group.

04:00.440 --> 04:06.560
We do a lot with CRM, marketing automation, a lot of one-off projects where we're presented

04:06.560 --> 04:11.800
with a problem and we go out and do the analysis to try to help make a decision on something.

04:11.800 --> 04:14.560
And so that's kind of where we started.

04:14.560 --> 04:19.560
And as our foundation has grown and we've been able to build out that foundation with

04:19.560 --> 04:24.120
the data warehouse, we've started to reach out into these other areas like the predictive

04:24.120 --> 04:25.760
analytics and the machine learning.

04:25.760 --> 04:30.000
And that's where we decided to partner with Microsoft who went over and did some use

04:30.000 --> 04:35.880
cases with them and they've turned out pretty successfully and so we've been moving in

04:35.880 --> 04:38.120
that direction.

04:38.120 --> 04:41.320
And how many people on the analytics team there?

04:41.320 --> 04:43.640
We have six on our analytics team right now.

04:43.640 --> 04:44.640
Okay.

04:44.640 --> 04:49.920
We get involved in both front office types of use cases as well as back office player

04:49.920 --> 04:51.800
analytics and that kind of thing.

04:51.800 --> 04:54.360
We have two sides of the business.

04:54.360 --> 04:57.000
Our team in particular focuses on the business side.

04:57.000 --> 05:02.160
There is also some basketball analytics, but our team isn't as involved with those.

05:02.160 --> 05:03.160
Okay.

05:03.160 --> 05:04.160
All right.

05:04.160 --> 05:05.160
Great.

05:05.160 --> 05:10.240
So maybe we can start out by having you walk through one of the early use cases for machine

05:10.240 --> 05:12.120
learning with the team.

05:12.120 --> 05:13.120
Sure.

05:13.120 --> 05:14.120
Yeah.

05:14.120 --> 05:21.080
So when we first talked to Microsoft, we looked at how things came from in the past.

05:21.080 --> 05:26.440
And so we used data in the past to run some of our sales campaigns.

05:26.440 --> 05:31.360
But a lot of times we didn't have all the data we would need to set the campaign up the

05:31.360 --> 05:32.720
way we would like to.

05:32.720 --> 05:38.440
And so what we wanted to do was we wanted to come to Microsoft, give them access to some

05:38.440 --> 05:44.240
of our ticketing data, some of our sales data, and ask them to help us identify patterns

05:44.240 --> 05:48.800
that would allow us to target people in the right way.

05:48.800 --> 05:53.880
So we have over 200,000 people that have purchased tickets in the past.

05:53.880 --> 05:58.200
And we can't reach out to every single one of them and give them a phone call and talk

05:58.200 --> 06:01.120
to them about if they'd be interested in coming out for another game.

06:01.120 --> 06:08.000
And so the idea was as we wanted to use the data to determine who would be best to talk

06:08.000 --> 06:12.760
to, who else might be best just to be reached out to be an email or some sort of marketing

06:12.760 --> 06:14.240
and automation campaign.

06:14.240 --> 06:17.160
And so that's kind of the use case that we went to them with.

06:17.160 --> 06:25.520
So you do have folks that are actively outbound dialing out to pass ticket purchasers to try

06:25.520 --> 06:29.840
to get them to buy tickets for an upcoming game via the phone.

06:29.840 --> 06:30.840
Yes, absolutely.

06:30.840 --> 06:34.320
We have a sales team that's that's working with new customers.

06:34.320 --> 06:39.960
We also have a service team that that nurtures the season ticket holders and helps them maintain

06:39.960 --> 06:42.880
relationship with those pongoing buyers.

06:42.880 --> 06:43.880
Okay.

06:43.880 --> 06:49.080
And so what were some of the first steps in tackling this challenge, which is ultimately,

06:49.080 --> 06:51.960
ultimately it sounds like trying to sell more tickets?

06:51.960 --> 06:52.960
Yeah.

06:52.960 --> 06:58.600
The tactic that we went with was we wanted to make the data points that we had available

06:58.600 --> 07:03.800
that we thought were predictive in understanding who would be interested in buying tickets

07:03.800 --> 07:09.560
again, make that available to Microsoft, try to kind of stay hands off with in terms

07:09.560 --> 07:15.600
of our bias, not necessarily bias, but our history of what we thought was the type of

07:15.600 --> 07:20.120
person we should be targeting and give them that data, allow the machine learning to look

07:20.120 --> 07:25.880
at all the different data points and how they interact and help us essentially score

07:25.880 --> 07:31.120
the customers and identify who would be a potential customer again.

07:31.120 --> 07:38.680
Okay. So Chen, when you, when this project kind of landed on your desk, how did you approach

07:38.680 --> 07:40.840
it from the Microsoft perspective?

07:40.840 --> 07:44.640
So I joined this project, I think in the second phase.

07:44.640 --> 07:50.960
So initially, my colleague has been working with shareholders on two different use cases.

07:50.960 --> 07:55.200
One targeting at predicting the seasonal ticket sales.

07:55.200 --> 08:00.160
The other is targeting the Singletic game ticket sales prediction.

08:00.160 --> 08:06.040
And I imported the first use case to a new environment called Azure Mission Learning

08:06.040 --> 08:11.960
Workbench, which allows data scientists to do a lot of data preparation and divide

08:11.960 --> 08:17.680
up models, experiments and then deploying them at cloud scale very conveniently.

08:17.680 --> 08:22.960
So we kind of like improved our original work in this new environment and then made a

08:22.960 --> 08:27.800
demo in the ignite conference this September.

08:27.800 --> 08:33.320
And so was that primarily for the first use case or the second or did you have the

08:33.320 --> 08:35.760
same couple of steps with both use cases?

08:35.760 --> 08:39.880
For now, we only imported the first use case, but yeah, if time permitted, we can also

08:39.880 --> 08:42.000
do the same thing for the second one.

08:42.000 --> 08:43.000
Oh, okay.

08:43.000 --> 08:48.520
And so what were some of the early findings with that first use case?

08:48.520 --> 08:56.040
Did you find that there are any unique challenges associated with either the problem or the

08:56.040 --> 08:59.200
data set that the blazers brought you?

08:59.200 --> 09:00.200
Of course.

09:00.200 --> 09:05.760
Yeah, I think there are two at least two unique challenges in this project.

09:05.760 --> 09:11.760
First of all, we need to combine different data sources to profile each customer.

09:11.760 --> 09:19.920
For example, we have data from charbracers describing the demographic information, customer

09:19.920 --> 09:27.440
status, customer interest, purchase and patterns, attendance information and even team preferences.

09:27.440 --> 09:33.560
And then on the other side, we have the third party data from axel, which pretty much

09:33.560 --> 09:38.960
tells about the lifestyle interest, broader interest of each customer.

09:38.960 --> 09:45.320
And then we need to also process those data in a Azure data SQL database.

09:45.320 --> 09:51.720
And then we need to combine the data processing step with the model development step in the

09:51.720 --> 09:57.600
new environment and so the so called adrup email workbench environment.

09:57.600 --> 10:04.240
And then the second challenge is the process of develop the machine learning model.

10:04.240 --> 10:09.880
And then to come up the best machine learning model for prediction, essentially, I think

10:09.880 --> 10:15.480
the simplest way is just to share a bunch of models and then tune the hyperparameters,

10:15.480 --> 10:21.800
but it turns out pretty challenging to to log the history of all the experiments.

10:21.800 --> 10:28.000
But thanks to the new environment we have in the Azure Azure machine and workbench, we

10:28.000 --> 10:35.640
are able to log all the hyperparameters performance metrics very easily and then visualize them

10:35.640 --> 10:38.120
easily pick the best model.

10:38.120 --> 10:43.440
Okay, now that's a topic that we've covered on the podcast, quite a bit of late, various

10:43.440 --> 10:46.120
approaches to hyperparameter optimization.

10:46.120 --> 10:53.000
It sounds like what you're describing is some kind of inherent features within the

10:53.000 --> 10:59.240
Azure ML platform that allow you to track kind of different runs with different sets of

10:59.240 --> 11:03.120
hyperparameters, like what exactly are you doing there?

11:03.120 --> 11:11.600
So essentially in the Azure ML workbench, you can write like a Python script, for example,

11:11.600 --> 11:16.520
and then chain the machine learning model with different hyperparameters.

11:16.520 --> 11:25.360
And it allows you to also log the parameters that you are using and the performance metrics

11:25.360 --> 11:29.640
that you end up to have in this model.

11:29.640 --> 11:35.400
And then if you later change another set of hyperparameters, you can easily see how the

11:35.400 --> 11:38.960
hyperparameter is impacting the performance metrics.

11:38.960 --> 11:44.960
It will automatically generate a set of graphs telling you the impact of hyperparameters

11:44.960 --> 11:48.080
on each of your performance metrics.

11:48.080 --> 11:54.680
So does it end up being like a sensitivity analysis or is it more maybe more manual than

11:54.680 --> 11:56.240
that?

11:56.240 --> 12:03.440
So it's the original goal is like to provide you a purely automatic way to select the model.

12:03.440 --> 12:06.840
But of course, this is a pretty challenging.

12:06.840 --> 12:13.080
So now we have, I think it's kind of semi-automatic way because you can write another script

12:13.080 --> 12:18.600
just to do a parameter suite, meaning that you try different combinations of parameters

12:18.600 --> 12:21.160
and then just run a for loop.

12:21.160 --> 12:27.320
And after running this for loop, you end up with the curve that I just described, describe

12:27.320 --> 12:31.320
your like the impact of the hyperparameters.

12:31.320 --> 12:37.720
And then, ideally, you can also write another script to pick the optimal model based on

12:37.720 --> 12:39.200
certain criteria.

12:39.200 --> 12:44.600
OK, so it sounds like it's giving you some kind of facility to implement your own grid

12:44.600 --> 12:47.520
search of the hyperparameter space.

12:47.520 --> 12:53.120
What types of models, like what did you learn about the models themselves for this type

12:53.120 --> 12:55.360
of problem?

12:55.360 --> 12:58.600
And you know, were there any surprising findings there?

12:58.600 --> 13:03.880
And yeah, so we basically tried many standard classification models.

13:03.880 --> 13:09.960
For example, support vector machine logistic regression and boost decision tree random

13:09.960 --> 13:11.440
forest model.

13:11.440 --> 13:20.680
So first of all, almost all the models perform very almost the same in terms of the accuracy.

13:20.680 --> 13:24.880
But the boost decision tree model is slightly better.

13:24.880 --> 13:31.040
And in terms of the accuracy and precision recall, we end up with very high number, something

13:31.040 --> 13:36.440
around like 0.98 in our testing experiments.

13:36.440 --> 13:40.360
So that's pretty amazing to me at the first time.

13:40.360 --> 13:45.760
And then some other findings also are pretty interesting.

13:45.760 --> 13:53.520
For example, if we are targeting the prediction of seasonal ticket purchase, the information

13:53.520 --> 14:00.080
about historical ticket purchase in patent, like whether a customer is a single game ticket

14:00.080 --> 14:03.800
purchaser, it's very important for the prediction.

14:03.800 --> 14:09.000
And I think also the amount of money that the customer has spent in the last season is

14:09.000 --> 14:18.000
also a strong indicator, apart from that, the income or the occupation of the customer

14:18.000 --> 14:24.560
can also tell us a lot because the seasonal ticket is sort of expensive.

14:24.560 --> 14:31.200
Then people who has high income may tend to keep purchasing this seasonal ticket.

14:31.200 --> 14:36.560
Those all strike me as fairly intuitive findings.

14:36.560 --> 14:42.000
Like was there any surprises in the results that you got back from Microsoft?

14:42.000 --> 14:47.160
Yeah, I'd say that there were certainly ones that were intuitive.

14:47.160 --> 14:54.040
There were ones that jumped out to us like specific games or specific sets of numbers of

14:54.040 --> 14:56.640
games that a person has attended in the past.

14:56.640 --> 15:01.160
It was, you know, it's different combinations of those variables that we hadn't looked

15:01.160 --> 15:02.160
at before.

15:02.160 --> 15:06.520
A lot of times we're looking at something very specific like this person purchased,

15:06.520 --> 15:08.440
you know, five games in the past.

15:08.440 --> 15:13.640
So there's someone that's very likely to want to come back to a game the following year.

15:13.640 --> 15:16.640
There was just this combination of all these different data points.

15:16.640 --> 15:21.600
And then in one of the other use cases we looked at the Axiom data, so like the demographic

15:21.600 --> 15:28.200
psychographic lifestyle data that Axiom provided, data points that we'd never had before.

15:28.200 --> 15:32.480
Some of them that are obvious like watching particular sports channels or are those

15:32.480 --> 15:37.640
types of things actually were also predictive in the likelihood of someone purchasing.

15:37.640 --> 15:41.520
And so it was, it was pretty cool to see, you know, all these different data points

15:41.520 --> 15:43.560
at how they interacted.

15:43.560 --> 15:48.720
Can you give an example of the, you know, counterintuitive combination that you saw?

15:48.720 --> 15:55.160
Yeah, for example, there was one like there was a, and I'm not sure why, but anyone that

15:55.160 --> 16:02.480
had been to a Minnesota Timberwolves game and had a credit card, or sorry, a loan that

16:02.480 --> 16:07.560
wasn't a credit card was also, there was a significantly higher likelihood that they

16:07.560 --> 16:09.560
would purchase tickets again.

16:09.560 --> 16:13.200
Something that belonged that wasn't a credit, so there was something totally out of left

16:13.200 --> 16:18.640
field that yeah, I'm not have expected all really interesting, yeah, definitely.

16:18.640 --> 16:23.920
And so how, how have you used the, you know, the results of this work is this kind of

16:23.920 --> 16:30.280
running in production, or how have you integrated into your selling and marketing workflows?

16:30.280 --> 16:34.400
Yeah, for us, it's been kind of, we've been trying to prove out the software, I guess

16:34.400 --> 16:35.400
you could say.

16:35.400 --> 16:39.160
So we've just been getting into the machine learning and predictive analytics.

16:39.160 --> 16:43.080
And so what we wanted to do is run through a couple of use cases that we could actually

16:43.080 --> 16:47.080
measure the results to see if this is the tool we wanted to go with.

16:47.080 --> 16:52.320
And for like the first use case we went through, normally on our sales campaigns will

16:52.320 --> 16:56.400
convert it about 5% of the leads that we contact.

16:56.400 --> 17:01.640
In this particular case, we converted it to 25% and they were purchasing season tickets

17:01.640 --> 17:02.640
in general.

17:02.640 --> 17:05.680
So, you know, very substantial.

17:05.680 --> 17:10.880
We went through and did the second use case and looked at like what's the likelihood

17:10.880 --> 17:14.520
of a single game buyer coming back the following season.

17:14.520 --> 17:19.760
And in general, we get about 16% of our single game buyers back each year for various different

17:19.760 --> 17:24.040
reasons with the models that Microsoft helped us develop.

17:24.040 --> 17:31.600
We were able to identify them at a 30% rate versus a 10% rate of those that weren't coming

17:31.600 --> 17:32.600
back.

17:32.600 --> 17:34.840
And so they were definitely useful.

17:34.840 --> 17:39.320
Can you explain that last point again, like you were taking the model and then using

17:39.320 --> 17:46.600
that to target your outbound calls and you were able to increase the rate, the return

17:46.600 --> 17:51.160
rate for those people that you wouldn't have otherwise or for that group of people?

17:51.160 --> 17:57.320
Well, we weren't able to, we didn't change the rate at which we were converting them.

17:57.320 --> 18:02.080
With our single game buyers, we'll have about 15 to 20,000 people each year that are individual

18:02.080 --> 18:03.600
game buyers.

18:03.600 --> 18:08.920
With the models we wanted to use, we wanted to look at and see who was likely to come

18:08.920 --> 18:09.920
back.

18:09.920 --> 18:13.600
And so Microsoft went through and identified, you know, we identified the top 10%.

18:13.600 --> 18:17.920
We said, these are the people that are going to come back and of that 30% actually came

18:17.920 --> 18:18.920
back.

18:18.920 --> 18:24.240
Whereas the rest of the group that was identified as unlikely to come back, only 10% came

18:24.240 --> 18:25.240
back.

18:25.240 --> 18:30.160
So it was essentially was showing us that the model was able to identify these people, the

18:30.160 --> 18:35.240
people that we were looking for a lot better than our other, the other ways that we were

18:35.240 --> 18:36.240
doing this.

18:36.240 --> 18:40.960
And so in terms of like using that for the future, we, our, our eventual goal is to get

18:40.960 --> 18:45.680
into like a fully automated system where a customer or a fan comes to us, they opt into

18:45.680 --> 18:51.680
us either through a previous purchase, they sign up for some sort of sweepstakes, they

18:51.680 --> 18:54.240
join an email mailing list or something.

18:54.240 --> 18:58.440
And essentially, we'd like to be able to pipe them through the models, have them scored

18:58.440 --> 19:01.200
and then determine, you know, what's the best way to work with them?

19:01.200 --> 19:04.640
Do we want to get them into some sort of email drip campaign where we, you know, send

19:04.640 --> 19:07.480
them newsletters and we near term along that way?

19:07.480 --> 19:11.960
Should we be sending them sales marketing, should we be putting them in a call campaign and

19:11.960 --> 19:13.360
having a sales rep talk to them?

19:13.360 --> 19:18.200
And so essentially, we'd like to have it to a fully automated system, but we're still

19:18.200 --> 19:23.720
in the early stages just trying to, to make sure that the tool works way we wanted to.

19:23.720 --> 19:29.680
And now Azure, Azure ML is kind of a developer tool.

19:29.680 --> 19:35.520
How do you envision using that in the context of your various business processes?

19:35.520 --> 19:40.680
Would you be developing custom applications around it or using some interface where you

19:40.680 --> 19:42.320
can run reports against it?

19:42.320 --> 19:44.160
Have you thought that far about it?

19:44.160 --> 19:45.160
Yep.

19:45.160 --> 19:46.160
Yeah.

19:46.160 --> 19:47.160
So on my team, we have an application developer.

19:47.160 --> 19:49.040
We have a couple of analysts.

19:49.040 --> 19:55.560
We also have people that are focused on CRM and on the marketing and automation side.

19:55.560 --> 19:59.280
And the nice thing is is they play really well with Microsoft.

19:59.280 --> 20:05.400
And so essentially, we have the in house capabilities to be able to load the data into

20:05.400 --> 20:06.400
the models.

20:06.400 --> 20:12.800
And with Azure ML, you essentially get like a web service endpoint with the results.

20:12.800 --> 20:17.720
And so the idea there would be we would hook up some more ETL and then pipe that data

20:17.720 --> 20:21.480
over into our marketing automation system or over into CRM.

20:21.480 --> 20:26.960
And then from there, use the automations within those systems to move forward with the

20:26.960 --> 20:28.520
records.

20:28.520 --> 20:30.480
And then where do you see this going?

20:30.480 --> 20:35.320
What are some of the other use cases that you've got in mind for this?

20:35.320 --> 20:36.320
Yeah.

20:36.320 --> 20:40.960
Aside from like the fully automated scoring system, we're looking at other things.

20:40.960 --> 20:44.640
There's some built-in text analysis that we want to get into.

20:44.640 --> 20:45.960
We get survey data.

20:45.960 --> 20:49.800
We'd like to be able to make that survey data more actionable.

20:49.800 --> 20:56.200
Be able to look at the sentiment of the customer and be able to track that over time.

20:56.200 --> 20:59.880
We'd also like to look at some things like help, help on other projects that we look

20:59.880 --> 21:00.880
at each year.

21:00.880 --> 21:04.320
So one in particular is what we call our drop count model.

21:04.320 --> 21:10.720
And that's essentially how many people come to any given game for various reasons, whether

21:10.720 --> 21:17.320
it's for staffing or for just understanding what the sales might be for that game.

21:17.320 --> 21:22.520
Being able to model that out with this system rather than what we have been using in the

21:22.520 --> 21:23.520
past.

21:23.520 --> 21:26.520
And so there's a variety of different things that we want to get into.

21:26.520 --> 21:31.640
Some automated and some just one-off situations where we want to use the algorithms to be able

21:31.640 --> 21:33.680
to get us more efficient.

21:33.680 --> 21:35.880
And so you ran a couple of use cases.

21:35.880 --> 21:38.280
You got some promising early results.

21:38.280 --> 21:44.880
You're still kind of in advance of having this fully integrated into the way you do business.

21:44.880 --> 21:52.160
What are some of the barriers or impediments that keep you from kind of being all-in already?

21:52.160 --> 21:57.160
Time and workload, I guess, which I'm sure you'll hear from a lot of people.

21:57.160 --> 21:58.160
Uh-huh.

21:58.160 --> 22:03.360
No, we got a lot of great resources, but there's a lot of other responsibilities going

22:03.360 --> 22:04.360
on.

22:04.360 --> 22:07.560
So it's just getting that time to be able to work with it.

22:07.560 --> 22:11.840
System integrations have come a long way over time, but there's still development that

22:11.840 --> 22:14.960
needs to be set up to integrate between systems.

22:14.960 --> 22:18.400
So we're working with a couple of Microsoft products.

22:18.400 --> 22:23.280
We're using Tableau for our data visualization, our marketing automation tools, and other

22:23.280 --> 22:24.280
system as well.

22:24.280 --> 22:28.440
So there's a lot of integrations that need to be set up.

22:28.440 --> 22:33.960
And then, again, just identifying where we can get the most bang for our buck and the

22:33.960 --> 22:38.520
time that we do set up for the development because there's a lot of projects that this

22:38.520 --> 22:45.080
could be worked on or could be used on, but finding the one that would be most useful is

22:45.080 --> 22:47.240
also another challenge.

22:47.240 --> 22:48.240
Right.

22:48.240 --> 22:57.240
For some of the future use cases, do you envision having Microsoft like similarly do some

22:57.240 --> 23:04.920
of the data science pieces of it, the model development, or are you building capacity within

23:04.920 --> 23:07.320
the trouble lasers to do that kind of work?

23:07.320 --> 23:09.600
And where are you on that maturity curve?

23:09.600 --> 23:10.600
Yeah.

23:10.600 --> 23:11.600
Well, both.

23:11.600 --> 23:14.960
We want to continue our partnership with Microsoft because they've been great to work

23:14.960 --> 23:18.240
with, but we are building the capacity internally.

23:18.240 --> 23:20.080
I have some great people on my team.

23:20.080 --> 23:24.160
A couple people in particular are ramping up on this.

23:24.160 --> 23:29.920
We have an application developer that is very well versed in the integrations with different

23:29.920 --> 23:32.200
Microsoft systems.

23:32.200 --> 23:34.760
He manages our CRM system, which is also Microsoft.

23:34.760 --> 23:36.960
And so we have a good set up there.

23:36.960 --> 23:41.680
I have an analyst on my team that she's been spending countless hours just going through

23:41.680 --> 23:44.200
all the training and all the tutorials we've had.

23:44.200 --> 23:47.560
People come out and so just building up that internally.

23:47.560 --> 23:51.200
I'd say we're still in the early phases and that's probably probably why it's taken

23:51.200 --> 23:54.360
us a little bit longer to get there because we don't have a full-blown data scientist on

23:54.360 --> 23:55.360
our team.

23:55.360 --> 23:58.920
But we're certainly working on building that out internally while we continue to work

23:58.920 --> 24:00.440
with Microsoft.

24:00.440 --> 24:06.200
Is there a particular skill set that you can identify that's still lacking?

24:06.200 --> 24:08.000
No, not so much.

24:08.000 --> 24:14.160
I mean, it's essentially just getting used to the tool, getting used to how it works, understanding

24:14.160 --> 24:15.920
the different algorithms.

24:15.920 --> 24:19.320
There's a lot of different ways that things can be done and just gaining that maturity

24:19.320 --> 24:23.080
and experience in the data science type areas.

24:23.080 --> 24:24.080
Right.

24:24.080 --> 24:25.080
All right.

24:25.080 --> 24:26.080
Now, Chen, we have a question for you.

24:26.080 --> 24:35.800
I've looked at Azure ML Studio in the past and it can be, I think of the various cloud-based

24:35.800 --> 24:37.320
approaches to data science.

24:37.320 --> 24:44.000
I think it has an advantage in being pretty approachable because you can do a lot of gooey,

24:44.000 --> 24:45.200
almost whizzy wig.

24:45.200 --> 24:47.840
The type of thing that I think a lot of people expect from Microsoft.

24:47.840 --> 24:53.080
But it's also often developers and data scientists don't really like that kind of thing.

24:53.080 --> 24:59.840
It gets in their way and I'm wondering for you as a data scientist that has deep knowledge

24:59.840 --> 25:04.200
of this tool, how do you approach problems?

25:04.200 --> 25:11.120
Do you approach them within that gooey construct that is the default for Azure ML or are there

25:11.120 --> 25:14.600
other ways that you use the tools?

25:14.600 --> 25:15.880
We're a great question.

25:15.880 --> 25:21.200
So actually I have been using Azure Mission Learning Studio for about a year.

25:21.200 --> 25:25.440
So indeed, we have both pros and cons for this product.

25:25.440 --> 25:30.200
I think it's a great tool for entry-level data scientists because as you mentioned, they

25:30.200 --> 25:37.600
are like gooey and you can jack and jaw on the plan and then you can construct a mission

25:37.600 --> 25:40.200
learning experiment very quickly.

25:40.200 --> 25:44.280
And then also you can deploy it as a web service.

25:44.280 --> 25:50.160
But in the meantime, for senior data scientists, they may want to have more flexibility to

25:50.160 --> 25:57.360
control the machine learning model and also maybe do more complex data preparation steps.

25:57.360 --> 26:02.440
So that's why recently Microsoft launched a new tool called Azure Mission Learning

26:02.440 --> 26:06.680
Workbench, which is primarily for senior data scientists.

26:06.680 --> 26:13.760
Again, to end data science and advanced analytics solution for data scientists to like

26:13.760 --> 26:18.720
prepare data, develop experiments and deploy models at the cloud scale.

26:18.720 --> 26:24.640
But it also allows you to do a lot of things like without using the gooey.

26:24.640 --> 26:32.720
You can write everything basically using a Python script or develop the model, compare

26:32.720 --> 26:38.720
the model within the Azure ML Workbench and then you can also deploy it on different

26:38.720 --> 26:39.720
sources.

26:39.720 --> 26:43.800
And so is the user experience there?

26:43.800 --> 26:48.240
Does it offer kind of the notebook paradigm that's becoming pretty popular for this kind

26:48.240 --> 26:49.240
of thing?

26:49.240 --> 26:54.520
Or is it more at the command line and code level?

26:54.520 --> 26:56.520
Yeah, we have both actually.

26:56.520 --> 27:01.880
So first of all, if you are familiar with I-Python notebook, you can directly jump in to

27:01.880 --> 27:05.280
use that in the Workbench environment.

27:05.280 --> 27:13.040
If you are more familiar with Python script or command line environment, you can develop

27:13.040 --> 27:17.280
a Python script and that script through the command line environment.

27:17.280 --> 27:22.960
And more importantly, you can also deploy a machine learning model either on a local machine

27:22.960 --> 27:29.400
or on a cloud cluster through the command line interface.

27:29.400 --> 27:36.400
And one of the things that the ML Studio does for you is that it's kind of operating

27:36.400 --> 27:39.880
at this higher level of abstraction where you've got these building blocks for different

27:39.880 --> 27:48.280
types of models like you can drag and drop a random forest block model, for example.

27:48.280 --> 27:50.320
And then kind of wire it up with your data.

27:50.320 --> 27:57.120
Are you, do you have access to the same kinds of things in the Workbench?

27:57.120 --> 28:06.000
Or are you rather shifting to kind of your full native Python and scikit-learn and

28:06.000 --> 28:10.000
the kind of traditional Python ecosystem tools?

28:10.000 --> 28:17.960
So at this moment, we are kind of shifting towards the Python, IPython notebook, scikit-learn

28:17.960 --> 28:22.760
or kinds of tools instead of like again providing the GUI.

28:22.760 --> 28:27.280
So we talked about some of the kind of pros and cons of that tool set in general.

28:27.280 --> 28:36.080
Were there any challenges or places where you ran into kind of hit a wall or had to change

28:36.080 --> 28:43.480
your strategy with regard to which tools you used in solving the specific use cases

28:43.480 --> 28:46.640
that you worked on for the trouble users?

28:46.640 --> 28:53.120
So for this use case, I would say because we are not trying to analyze a huge worry

28:53.120 --> 28:58.200
about data and then also deploy it like, for example, Spark as a machine learning model

28:58.200 --> 29:06.400
on Spark, the data size is pretty, so far it's not, it's pretty small.

29:06.400 --> 29:14.760
So we can do the model development work both on EdgeML studio and EdgeML bench.

29:14.760 --> 29:23.760
So the primarily goal of improving our work in the new environment is to try to demonstrate

29:23.760 --> 29:29.320
the new capabilities of this EdgeML work bench.

29:29.320 --> 29:30.320
Okay.

29:30.320 --> 29:36.400
Okay, so you were able to do the things you needed to do with studio work bench is something

29:36.400 --> 29:41.800
that you'd go to if you wanted to scale it up or in terms of the data or wanted to

29:41.800 --> 29:49.120
deploy it out to Spark and you may deploy some use cases there just to kind of show them

29:49.120 --> 29:51.920
this new platform and how it compares to the original.

29:51.920 --> 29:57.360
Yeah, I would say the primary goal is to showcase, but also we want to work with chair

29:57.360 --> 30:05.120
brazers in case they want to move towards the new environment so that we can help them

30:05.120 --> 30:10.000
understand how to land on the new EdgeML work bench.

30:10.000 --> 30:15.160
And this question might be a little bit inside basketball, I guess.

30:15.160 --> 30:21.080
This is for Chen Wei, are you with the data scientist title, are you in a consulting organization

30:21.080 --> 30:27.480
with Microsoft or are you aligned with a product organization and just working with this

30:27.480 --> 30:33.880
particular customer because of their profile or how did you get involved in working on

30:33.880 --> 30:34.880
this project?

30:34.880 --> 30:35.880
Okay.

30:35.880 --> 30:42.560
So for my organization, originally, I think we are more tied to the product team, which

30:42.560 --> 30:46.240
is called Azure Mission Learning as you may know already.

30:46.240 --> 30:53.880
So we are actually the Azure data science team under the Microsoft AI and the research.

30:53.880 --> 31:01.440
So right now, this I think intersection between both the product team and the research team

31:01.440 --> 31:02.440
in Microsoft.

31:02.440 --> 31:09.200
So essentially, we want to bring the advanced research results to the product team so that

31:09.200 --> 31:13.040
we can develop like machine learning products based on those algorithms.

31:13.040 --> 31:16.120
So that's where we came.

31:16.120 --> 31:21.360
So we will try to stand in the middle and then try to understand both the research results

31:21.360 --> 31:25.840
and also the need for the customer about the product.

31:25.840 --> 31:31.520
And are there any research results that you can see playing a role in the types of problems

31:31.520 --> 31:34.560
that the trailblazers are dealing with?

31:34.560 --> 31:42.800
So for me, I'm pretty interested in like reinforcement learning and deep learning as well.

31:42.800 --> 31:47.600
So I was thinking about two different use cases.

31:47.600 --> 31:54.720
For example, if we have like a really large amount data, say like millions of billions

31:54.720 --> 31:59.720
of records, then we may try to build a model with machine with deep learning.

31:59.720 --> 32:05.400
So now because of the the goal original was trying to prove a concept.

32:05.400 --> 32:09.840
So we did not like use, I think a large volume of data.

32:09.840 --> 32:13.800
But later, yeah, if that's the case, we can try deep learning.

32:13.800 --> 32:21.000
And then another use case I was thinking about is to use reinforcement learning to learn

32:21.000 --> 32:28.720
the behavior of the customers and then try to send them offers which may sound more

32:28.720 --> 32:30.160
attractive to them.

32:30.160 --> 32:32.520
Can you walk us through how that might work?

32:32.520 --> 32:38.040
So this is again, so that's based on my intuition.

32:38.040 --> 32:42.560
It's not like a very mature idea being discussed with trailblazers.

32:42.560 --> 32:48.600
But for me, I think if we want to like a sale ticket to our customer, the first step

32:48.600 --> 32:53.160
is to predict whether the customer is willing to buy it or not.

32:53.160 --> 32:57.640
And then the second step maybe is to determine the best price, right?

32:57.640 --> 33:05.040
So in that sense, maybe we need to dynamic change the price according to certain criteria.

33:05.040 --> 33:14.000
For example, if there are other competitors like MBA teams offering the same discount

33:14.000 --> 33:19.680
to your customer, and then you may offer like at least the same discount, right?

33:19.680 --> 33:28.120
So a lot of factors may play a lure in this kind of price optimization use case.

33:28.120 --> 33:35.720
So I think again, machine learning such as reinforcement learning can be a good tool

33:35.720 --> 33:38.360
for solving such complex problem.

33:38.360 --> 33:44.880
So you're thinking that your reinforcement learner would be kind of exploring this

33:44.880 --> 33:51.880
state space of like different types of buyers and different prices and things like that

33:51.880 --> 33:54.800
and trying to find an optimal path through that.

33:54.800 --> 33:55.800
Yeah, yeah.

33:55.800 --> 34:01.560
So essentially what you can control is from I think it's mainly the price or the discount

34:01.560 --> 34:03.920
you offered to your customer, right?

34:03.920 --> 34:08.480
And then the environment that you are sitting in is very complicated.

34:08.480 --> 34:15.600
You have competitors, your customers may have different background.

34:15.600 --> 34:17.600
You cannot control all of this.

34:17.600 --> 34:23.560
So that's why we need to use reinforcement learning to dynamically update your price

34:23.560 --> 34:26.760
and discount to check your customer.

34:26.760 --> 34:31.960
And I'm guessing that Azure ML Studio doesn't do reinforcement learning yet.

34:31.960 --> 34:33.760
Yeah, indeed.

34:33.760 --> 34:41.600
We don't have reinforcement learning as maybe a toolbox, but internally we have another

34:41.600 --> 34:43.800
tool called CNTK.

34:43.800 --> 34:48.960
In CNTK, actually, it has a comprehensive set of functionalities.

34:48.960 --> 34:53.240
So it also offers you the flexibility to develop reinforcement learning.

34:53.240 --> 34:59.000
Actually, I think in the newly released version, it has some tutorial about that.

34:59.000 --> 35:05.880
Right before we close things down, are there, if there were three things that you could

35:05.880 --> 35:11.360
get from Microsoft in there in terms of the way they supported you or their tools, do

35:11.360 --> 35:13.440
you have a wish list for them?

35:13.440 --> 35:15.880
Offhand, I can't think of anything.

35:15.880 --> 35:20.120
We've been very excited about the results.

35:20.120 --> 35:27.440
I guess just being able to ramp up our internal knowledge, which they've been helpful with,

35:27.440 --> 35:31.960
and just getting the ability to spend some more time on these types of projects would

35:31.960 --> 35:34.000
be the two things that we're looking for.

35:34.000 --> 35:36.840
In terms of the tool itself, it's been great.

35:36.840 --> 35:40.720
There is anything at the moment that I can think of that we would add.

35:40.720 --> 35:41.720
Okay.

35:41.720 --> 35:42.720
Awesome.

35:42.720 --> 35:45.280
And can we anything else that you would add or would like to share?

35:45.280 --> 35:51.400
I would say that it's a really great experience to work with shape razors and then to come

35:51.400 --> 35:57.120
up with the solution and trying to improve the existing solution that we have built.

35:57.120 --> 36:02.040
So I really look forward like continuing working with you in the future.

36:02.040 --> 36:03.040
Cool.

36:03.040 --> 36:04.040
Awesome.

36:04.040 --> 36:05.040
Awesome.

36:05.040 --> 36:11.280
Well, I really enjoyed chatting with both of you and appreciate you taking the time.

36:11.280 --> 36:15.880
Mike, I wish you and the Portland Trailblazers a great season.

36:15.880 --> 36:16.880
Thank you.

36:16.880 --> 36:17.880
Appreciate that.

36:17.880 --> 36:18.880
All right.

36:18.880 --> 36:19.880
Thanks, guys.

36:19.880 --> 36:20.880
Thank you.

36:20.880 --> 36:21.880
Thank you.

36:21.880 --> 36:23.880
All right, everyone.

36:23.880 --> 36:30.080
That's our show for today for more information on Mike, Chenhui, or any of the topics covered

36:30.080 --> 36:36.640
in this episode, head on over to twimlai.com slash talk slash 156.

36:36.640 --> 36:43.680
To follow along with the AI and sports series, visit twimlai.com slash AI and sports.

36:43.680 --> 36:47.960
If you're a fan of the pod, we'd like to encourage you to head to iTunes or wherever

36:47.960 --> 36:52.800
you listen to podcasts and leave us your five star rating and review.

36:52.800 --> 36:56.920
We're super helpful as we push to grow the show and the community.

36:56.920 --> 37:25.440
As always, thanks so much for listening and catch you next time.

