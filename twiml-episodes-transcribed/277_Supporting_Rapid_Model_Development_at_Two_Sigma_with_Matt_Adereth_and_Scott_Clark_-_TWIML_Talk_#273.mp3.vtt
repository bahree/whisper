WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:35.320
I'm your host Sam Charrington. Two weeks ago we celebrated the show's third birthday

00:35.320 --> 00:40.080
and a major listenership milestone. And last week we kicked off the second volume of our

00:40.080 --> 00:45.200
listener favorite AI platform series, sharing more stories of teams working to scale and

00:45.200 --> 00:49.760
industrialize data science and machine learning at their companies.

00:49.760 --> 00:54.120
We've been teasing that there's more to come and today I am super excited to announce

00:54.120 --> 00:59.360
the launch of our inaugural conference, Twimblecon AI Platforms.

00:59.360 --> 01:04.600
Twimblecon AI platforms will focus on the platforms, tools, technologies and practices

01:04.600 --> 01:09.440
necessary to scale the delivery of machine learning and AI in the enterprise.

01:09.440 --> 01:14.880
Now you know Twimble for bringing you dynamic practical conversations via the podcast and

01:14.880 --> 01:18.640
we're creating our Twimblecon events to build on that tradition.

01:18.640 --> 01:24.240
The event will feature two full days of community oriented discussions, live podcast interviews

01:24.240 --> 01:30.440
and practical presentations by great presenters sharing concrete examples from their own experiences.

01:30.440 --> 01:34.640
By creating a space where data science, machine learning, platform engineering and MLOPS

01:34.640 --> 01:39.640
practitioners and leaders can share, learn and connect, the event aspires to help see

01:39.640 --> 01:44.800
the development of an informed and sustainable community of technologists that is well equipped

01:44.800 --> 01:48.560
to meet the current and future needs of their organizations.

01:48.560 --> 01:52.880
Some of the topics that we plan to cover include overcoming the barriers to getting machine

01:52.880 --> 01:58.120
learning and deep learning models into production, how to apply MLOPS and DevOps to your machine

01:58.120 --> 02:03.040
learning workflow, experiences and lessons learned in delivering platform and infrastructure

02:03.040 --> 02:08.400
support for data management, experiment management and model deployment, the latest approaches

02:08.400 --> 02:14.560
platforms and tools for accelerating and scaling the delivery of ML and DL and the enterprise,

02:14.560 --> 02:19.200
platform deployment stories from leading companies like Google, Facebook, Airbnb as well

02:19.200 --> 02:24.440
as traditional enterprises like Comcast and Shell and organizational and cultural best

02:24.440 --> 02:27.000
practices for success.

02:27.000 --> 02:31.600
The two day event will be held on October 1st and 2nd in San Francisco and I would really

02:31.600 --> 02:33.600
love to meet you there.

02:33.600 --> 02:39.520
EarlyBurt Registration is open today at Twimblecon.com and we're offering the first 10 listeners

02:39.520 --> 02:45.720
who register the amazing opportunity to get their ticket for 75% off using the discount

02:45.720 --> 02:48.120
code TwimbleFirst.

02:48.120 --> 02:54.920
Again, the conference site is Twimblecon.com and the code is TwimbleFirst.

02:54.920 --> 03:00.080
I am really grateful to our friends over at Sigopt who stepped up to support this project

03:00.080 --> 03:01.920
in a big way.

03:01.920 --> 03:07.000
In addition to supporting our AI platform's podcast series and next ebook, they've made

03:07.000 --> 03:12.960
a huge commitment to this community by signing on as the first founding sponsor for the event.

03:12.960 --> 03:17.760
Sigopt Software is used by enterprise teams to standardize and scale machine learning experimentation

03:17.760 --> 03:23.200
and optimization across any combination of modeling frameworks, libraries, computing

03:23.200 --> 03:25.800
infrastructure and environment.

03:25.800 --> 03:31.480
Teams like Two Sigma rely on Sigopt software to realize better modeling results much faster

03:31.480 --> 03:33.800
than previously possible.

03:33.800 --> 03:38.320
Of course, to fully grasp its potential, it's best to try it yourself.

03:38.320 --> 03:43.360
And this is why Sigopt is offering you an exclusive opportunity to try their product on some

03:43.360 --> 03:46.880
of your toughest modeling problems for free.

03:46.880 --> 03:53.040
To learn about and take advantage of this offer, visit twimblei.com slash Sigopt.

03:53.040 --> 04:01.240
And now on to the show.

04:01.240 --> 04:06.840
Right everyone, I am here in San Francisco and I am with Matt Adarath, who is a managing

04:06.840 --> 04:10.160
director at Two Sigma Investments and Scott Clark.

04:10.160 --> 04:16.320
Scott is the founder and CEO at Sigopt and has been on this show previously.

04:16.320 --> 04:19.280
Matt and Scott, welcome to this week in machine learning and AI.

04:19.280 --> 04:20.280
Thanks for having us.

04:20.280 --> 04:21.280
Thank you.

04:21.280 --> 04:22.280
Awesome.

04:22.280 --> 04:25.400
So, I'm really excited about diving into this conversation.

04:25.400 --> 04:33.560
We'll be talking about Two Sigma's journey with regard to building out its modeling and

04:33.560 --> 04:36.000
machine learning platform.

04:36.000 --> 04:38.360
Matt, you've not been on the show before.

04:38.360 --> 04:42.600
We'd love to hear a little bit about your background, how you got started working in machine

04:42.600 --> 04:46.880
learning and at this intersection of engineering and modeling.

04:46.880 --> 04:53.280
Yeah, it's always been an area that I've been interested in, the intersection of computer

04:53.280 --> 04:57.800
science and math, that's what I studied at Carnegie Mellon.

04:57.800 --> 05:03.080
And I always wanted to find some place where I could apply both of those disciplines and

05:03.080 --> 05:05.240
learn and grow in those.

05:05.240 --> 05:09.640
So after school, I went to Microsoft where I was there for about four years, working

05:09.640 --> 05:14.920
on office, which there wasn't so much math there, unfortunately, but it was very interesting.

05:14.920 --> 05:19.280
And then I left Microsoft to join a startup that was spun out of Microsoft Research where

05:19.280 --> 05:22.760
I was doing a lot of analytics on social networks.

05:22.760 --> 05:29.600
And I realized that I wanted to be at a place where I'd be able to do the math that I had

05:29.600 --> 05:31.520
learned and loved.

05:31.520 --> 05:36.600
And the thing that occurred to me was that finance would be a great place to do that because

05:36.600 --> 05:41.080
they exist at that intersection of math and computer science.

05:41.080 --> 05:45.640
And then the whole world of finance was also very interesting to me.

05:45.640 --> 05:51.840
So about 12 years ago, I joined Two Sigma, it was much smaller than.

05:51.840 --> 05:59.000
And I started working on tools and infrastructure there for analyzing data and really helped

05:59.000 --> 06:02.280
build out the platform that we have today.

06:02.280 --> 06:03.280
Awesome.

06:03.280 --> 06:05.320
We'll definitely be coming back to that.

06:05.320 --> 06:11.160
But before we do, Scott, give us a refresher about your background and what Sigopt is

06:11.160 --> 06:12.160
up to.

06:12.160 --> 06:13.160
Yeah, definitely.

06:13.160 --> 06:17.280
So Sigopt provides an experimentation and optimization platform that can bolt on top

06:17.280 --> 06:20.120
of platforms like we're talking about today.

06:20.120 --> 06:25.000
And really help amplify and accelerate the way that you get to impact from these models.

06:25.000 --> 06:30.600
I came to this via grad school where basically when I was doing my PhD at Cornell, I saw that

06:30.600 --> 06:35.200
experimentation and optimization was a core part of every project anyone in my department

06:35.200 --> 06:36.200
was doing.

06:36.200 --> 06:39.960
That was reinforced when I went to Yelp and worked on the advertising system.

06:39.960 --> 06:45.200
And so decided to build Sigopt to help solve this problem in an enterprise way for universities,

06:45.200 --> 06:49.040
government agencies and firms like Two Sigma around the world today.

06:49.040 --> 06:54.320
So Matt, you started Two Sigma 12 years ago, you said.

06:54.320 --> 07:00.960
Where was Two Sigma kind of in the journey of a platform?

07:00.960 --> 07:06.680
You started there working on tools and infrastructure as opposed to coming over from data science

07:06.680 --> 07:11.080
where you, you know, one of the first people working on tools in emperor was there already

07:11.080 --> 07:14.280
an established group focused on that.

07:14.280 --> 07:21.840
So there were teams working on it, but it was really more spread across the teams like

07:21.840 --> 07:28.680
our data, we had a data engineering team and there was a modeling engineering team.

07:28.680 --> 07:35.320
But they would each build out their own infrastructure of varying qualities and it was a little

07:35.320 --> 07:41.880
later that we started really trying to find the solution that would work for everyone.

07:41.880 --> 07:48.000
But since the beginning, the company was founded in 2001 and it was started as a quantitative

07:48.000 --> 07:52.920
investment manager with a focus on building out platforms for the things that we were trying

07:52.920 --> 07:57.280
to do with the knowledge that we would try to expand into different businesses and want

07:57.280 --> 08:00.440
to be able to leverage the solutions that we had already built.

08:00.440 --> 08:06.600
So it was already in play when I got there 12 years ago and it was more just like picking

08:06.600 --> 08:10.880
up the torch on certain areas that needed more focus.

08:10.880 --> 08:11.880
Okay.

08:11.880 --> 08:12.880
Okay.

08:12.880 --> 08:23.280
And so maybe talk a little bit about the various constituents you serve or you, do you

08:23.280 --> 08:28.720
have a single type of data scientist or a single type of skill set there?

08:28.720 --> 08:34.040
Do you have engineers as well as data scientists as well as other types of quantitative folks

08:34.040 --> 08:35.960
that are using the tools that you're building?

08:35.960 --> 08:42.080
Yeah. So it really is a wide variety of skill sets that we're trying to serve.

08:42.080 --> 08:48.800
Even within our modeling discipline, there are some folks who are much more technical and

08:48.800 --> 08:55.240
are looking at things from a like a technique perspective and then there are other folks

08:55.240 --> 09:01.680
who are much more focused on understanding their data sets and trying to figure out what

09:01.680 --> 09:07.000
the predictive value is based off of a deep understanding of where the data comes from

09:07.000 --> 09:08.520
and what it means.

09:08.520 --> 09:12.840
And roughly how many users does your group support?

09:12.840 --> 09:16.560
So the company is roughly a third modeling.

09:16.560 --> 09:19.720
So talking around 500.

09:19.720 --> 09:20.720
Wow.

09:20.720 --> 09:21.720
Okay.

09:21.720 --> 09:29.000
And so I guess what I'm curious most about is how the thinking about building out tools

09:29.000 --> 09:34.720
and platforms to support modeling has evolved over the time that you've been there.

09:34.720 --> 09:40.200
So one of the things that we try to do is not be prescriptive about the tools that our

09:40.200 --> 09:41.200
modelers use.

09:41.200 --> 09:48.760
We want to hire the best and we want them to be able to apply the tools and techniques

09:48.760 --> 09:55.120
that they are familiar with and that they are able to leverage the most.

09:55.120 --> 09:58.560
So that's a challenge when we're trying to build up platforms because if we're not being

09:58.560 --> 10:01.000
prescriptive, we can't really limit them.

10:01.000 --> 10:06.760
But what we try to do is identify the common threads that will benefit the most number of

10:06.760 --> 10:07.760
people.

10:07.760 --> 10:11.960
We also try to identify what we think are going to be the winners in terms of technologies

10:11.960 --> 10:15.800
where we can give them a little bit more of a push or support them a little bit more

10:15.800 --> 10:21.040
to make those extra easy to drive people to them without being prescriptive.

10:21.040 --> 10:23.080
How did Sigop come into play?

10:23.080 --> 10:25.160
How did you find one another?

10:25.160 --> 10:30.560
You know, was there a search around sigmas and what companies can we find?

10:30.560 --> 10:33.800
We definitely ran into each other at a lot of the similar academic conferences that

10:33.800 --> 10:37.360
we go to in Publish, whether it's ICML, NERIPS, that kind of thing.

10:37.360 --> 10:41.800
But I think a lot of it, after seeing each other, what drove us together was this shared

10:41.800 --> 10:47.600
desire to be very modeling driven and really help augment and amplify these experts.

10:47.600 --> 10:51.520
So it's not about, again, being prescriptive, like Matt said, it's more about giving them

10:51.520 --> 10:55.400
the tools to run as fast as they can once they've picked the direction.

10:55.400 --> 11:01.240
And so you want to be able to unify and standardize on things that make sense to standardize

11:01.240 --> 11:04.040
on, but then don't do it in a constraining way.

11:04.040 --> 11:08.380
So you want to provide optionality so that they can use their expertise, that context

11:08.380 --> 11:10.760
awareness, that domain expertise.

11:10.760 --> 11:13.160
But at the end of the day, really run as fast as possible.

11:13.160 --> 11:17.720
And I think modeling platforms help with that and agnostic optimization helps with that

11:17.720 --> 11:21.960
and good infrastructure helps with that, but it's really about really empowering these people,

11:21.960 --> 11:24.200
which is that shared vision I think we both have.

11:24.200 --> 11:32.320
Matt, when you think about the end-to-end modeling platform that your team is offering,

11:32.320 --> 11:35.560
how do you articulate the various components of that?

11:35.560 --> 11:40.440
When I think about these kinds of platforms, I tend to think of them in terms of data management

11:40.440 --> 11:46.320
and transformation in those kinds of things, experiment management and production or model

11:46.320 --> 11:47.320
deployment.

11:47.320 --> 11:54.280
Do you have a similar view of the landscape or do you organize things differently?

11:54.280 --> 11:57.200
At a high level, that's how we look at things.

11:57.200 --> 11:59.160
I would break it down a little bit more.

11:59.160 --> 12:07.720
Even at the data management stage, we have a finer breakdown where there are things around

12:07.720 --> 12:13.240
like data ingestion is a big thing for us because we have so many data sets that we take

12:13.240 --> 12:19.640
in that's what we do is we try to bring in as many data sets as possible and find the

12:19.640 --> 12:21.880
value in all of them.

12:21.880 --> 12:30.280
So bring ingesting at scale is a challenge, cleaning at scale and making sure that that

12:30.280 --> 12:35.920
data is not just available for doing data science, but making sure that it's available for

12:35.920 --> 12:41.360
our real-time trading systems, which is an additional challenge because there is this

12:41.360 --> 12:45.680
timeliness aspect to how soon we process the data.

12:45.680 --> 12:53.840
Then on the research side, we break things down even further around not just accessing

12:53.840 --> 13:02.120
the data, but transforming it, sharing transformations is a big concern for us and then modeling.

13:02.120 --> 13:03.120
Okay.

13:03.120 --> 13:08.200
Then on the modeling side, do you have further distinctions within that set of capabilities?

13:08.200 --> 13:14.360
Yeah. So the first part of the modeling capabilities is just doing the preliminary analysis of

13:14.360 --> 13:20.240
the data, like explore to our data analysis that everybody does.

13:20.240 --> 13:23.600
We have a lot of focus on the time seriousness of everything.

13:23.600 --> 13:25.360
We treat everything as a time series.

13:25.360 --> 13:28.800
We kind of have this belief that everything is a time series and if you have something

13:28.800 --> 13:34.520
that you think isn't a time series, you're probably wrong and you're going to regret

13:34.520 --> 13:39.420
not trading it as a time series because it changes and we need to be able to do point

13:39.420 --> 13:43.200
and time simulations and back testing for everything.

13:43.200 --> 13:49.520
So there's this whole phase of analysis and modeling and then once you have something

13:49.520 --> 13:56.280
that you think has predictive value, there's this second stage of seeing how it trades and

13:56.280 --> 14:04.440
that is really a different style of analysis than typical data science outside of

14:04.440 --> 14:05.440
finance.

14:05.440 --> 14:11.960
You mentioned previously that some of the things that you're doing are particularly challenging

14:11.960 --> 14:13.520
at the scale that you're doing.

14:13.520 --> 14:16.040
Can you give us a sense for the scale?

14:16.040 --> 14:21.820
So I mentioned that we have hundreds of researchers and they're each doing lots of different

14:21.820 --> 14:25.760
kinds of analysis at scale, whether they're doing the exploratory data analysis on large

14:25.760 --> 14:32.080
data sets or they're testing out their models and seeing how different parameters perform.

14:32.080 --> 14:37.680
In some of those cases, they may be running thousands or tens of thousands of very expensive

14:37.680 --> 14:39.200
simulations.

14:39.200 --> 14:48.760
So we have a huge demand for compute and a lot of challenges around scaling up to those

14:48.760 --> 14:52.080
managing that amount of compute.

14:52.080 --> 15:01.880
Scott, when you think about the challenges that Matt's talked about from a modeling perspective

15:01.880 --> 15:12.960
and curious from your perspective, having seen this play out at a number of customers, what

15:12.960 --> 15:18.160
of what he described would you say is unique to Sigma and what do you see broadly in the

15:18.160 --> 15:19.160
industry?

15:19.160 --> 15:20.400
Great question.

15:20.400 --> 15:27.320
So I think broadly, we definitely see people doing unique modeling, they're doing differentiated

15:27.320 --> 15:28.320
modeling.

15:28.320 --> 15:32.400
I think special about what they bring to the table, whether it's on the data side or whether

15:32.400 --> 15:37.400
it's the end application, and that's really, again, where domain expertise and contextual

15:37.400 --> 15:39.360
awareness plays a huge role.

15:39.360 --> 15:42.680
And whether that's in the asset management space or whether it's the work that we're doing

15:42.680 --> 15:46.800
with tech companies or the US intelligence community, everybody has something different

15:46.800 --> 15:48.560
they're trying to solve.

15:48.560 --> 15:53.440
And what we've found is more and more companies are starting to invest in platforms.

15:53.440 --> 15:58.560
Because of course, you've seen with the series that you're running and are continuing to run.

15:58.560 --> 16:02.440
And a lot of that comes down to people who are doing this already, they just might not

16:02.440 --> 16:05.960
have been doing it the right way, or they might not have been doing it in a way that

16:05.960 --> 16:09.600
could amplify across the entire organization.

16:09.600 --> 16:11.560
We see the same thing with experimentation.

16:11.560 --> 16:16.760
So there's a lot of parallels in terms of just the core concept of we have data, we're

16:16.760 --> 16:18.360
trying to solve a problem.

16:18.360 --> 16:23.640
It might be making trades in a market, it might be trying to make a recommendation for a

16:23.640 --> 16:24.920
streaming service.

16:24.920 --> 16:29.200
But at the end of the day, it's somebody trying to solve this very specific problem.

16:29.200 --> 16:34.000
And then teams like Matt and teams like Sigopt are trying to really just give them the tools

16:34.000 --> 16:39.320
to do those jobs better, without forcing them into a sandbox or without trying to give

16:39.320 --> 16:41.800
them a one size fits all solution.

16:41.800 --> 16:47.840
When I talk to folks that are trying to provide these types of tools, the goals that they

16:47.840 --> 16:50.400
have are all over the map.

16:50.400 --> 16:56.680
Some folks have these broad, I kind of asked about the different types of users that you're

16:56.680 --> 16:57.680
trying to support.

16:57.680 --> 17:04.640
Some folks, their primary goal is to make machine learning more accessible so that more teams

17:04.640 --> 17:08.040
can make models, can build models, that kind of thing.

17:08.040 --> 17:14.080
Other folks are driving towards their whole existence is around achieving some level of

17:14.080 --> 17:19.440
scale or kind of compressing the innovation cycle, things like that.

17:19.440 --> 17:26.280
How do you think about your prime directives, if you will, what is really driving to

17:26.280 --> 17:30.800
Sigma to continue to invest in this, if you had to stack rank them?

17:30.800 --> 17:35.000
Yeah, so the two things are we want to get better answers and we want to get them faster.

17:35.000 --> 17:43.040
We have lots of modelers, like I mentioned before, so any multiplier on their speed really

17:43.040 --> 17:44.040
adds up.

17:44.040 --> 17:46.800
Not just because there are so many of them, but because of what each of them is doing

17:46.800 --> 17:49.360
is so high value.

17:49.360 --> 17:54.360
The two things that we're looking at are getting better answers and getting them faster.

17:54.360 --> 18:00.360
The better answers can come from applying better techniques that may discover things that

18:00.360 --> 18:03.200
couldn't be discovered otherwise.

18:03.200 --> 18:08.080
The speed comes not just from the speed of the algorithms that are maybe doing these

18:08.080 --> 18:14.000
optimizations, but also the usability of the tool is really critical for us.

18:14.000 --> 18:19.880
It's very easy to lose a day to trying to figuring out some error message or some weird

18:19.880 --> 18:23.880
API that wasn't well designed, so that's something that we're also really focused on.

18:23.880 --> 18:31.280
A big part of that is around these kind of different elements of the platform that we've

18:31.280 --> 18:36.840
talked about, making sure the data is available to folks, making sure they can iterate on experiments

18:36.840 --> 18:37.840
very quickly.

18:37.840 --> 18:42.920
Do you have challenges on the production-alizing side as well, or your users mostly focused

18:42.920 --> 18:48.160
on analytical results in inquiry and not so much deploy models out?

18:48.160 --> 18:49.200
They do deploy models.

18:49.200 --> 18:53.040
We do have our modelers own their models end-to-end.

18:53.040 --> 18:57.960
That's something that we think there's a lot of value in doing, and there are challenges

18:57.960 --> 19:02.760
in writing a production model versus doing something in research that is another focus

19:02.760 --> 19:07.520
for my team, making sure that that transition is as seamless as possible.

19:07.520 --> 19:13.480
A lot of the benefits that you described in terms of increasing the pace of innovation

19:13.480 --> 19:19.400
and getting better results is focused on those researchers and their ability to turn through

19:19.400 --> 19:26.000
the possible solution space, if you will, for these problems that they're trying to solve.

19:26.000 --> 19:33.040
When you think about the things that you've done to address or attack the experimentation

19:33.040 --> 19:40.120
challenge, can you give us a spectrum of the types of things you've done from a platforming

19:40.120 --> 19:44.840
perspective to narrow in on those goals?

19:44.840 --> 19:46.840
Sure.

19:46.840 --> 19:48.840
I can give a couple examples.

19:48.840 --> 19:54.680
One is we focus a lot on the languages that people use, making sure that we have domain-specific

19:54.680 --> 20:02.800
languages that allow our modelers to express their problems really naturally, while still

20:02.800 --> 20:09.400
having the flexibility to cover all of the possible ideas that they might have.

20:09.400 --> 20:12.240
What's the native modeling environment for your folks?

20:12.240 --> 20:16.320
Is it primarily notebooks or something else?

20:16.320 --> 20:19.440
Over the last few years, there's been a big push towards notebooks.

20:19.440 --> 20:23.240
We've open sourced some of the stuff that we've built.

20:23.240 --> 20:30.240
Beaker X is one plugin that runs on top of Jupiter that allows people to work in a really

20:30.240 --> 20:36.880
seamless polyglot environment and has some internal things that are bespoke to our environment

20:36.880 --> 20:40.600
that make it just really easy for people to do the things that they need to do.

20:40.600 --> 20:46.040
Another example of the kinds of things that we focus on to make it easy for them is

20:46.040 --> 20:54.600
around running lots and lots of jobs, specifically back test simulations is a big focus for us.

20:54.600 --> 21:00.120
That's where a significant chunk of our compute is dedicated.

21:00.120 --> 21:08.760
Making that easy for modelers to interact with, to launch things at scale, to monitor and

21:08.760 --> 21:17.120
manage those things and to easily deploy to different cloud providers or reusing our

21:17.120 --> 21:21.920
in-house data centers, we just want to make that all as easy as possible so that they

21:21.920 --> 21:24.920
can get their jobs done quickly.

21:24.920 --> 21:33.520
What's the interface between the researchers and the Sigopt product and its capability?

21:33.520 --> 21:36.200
That's a great question.

21:36.200 --> 21:38.200
We have a few different solutions.

21:38.200 --> 21:44.800
Many folks who use the Sigopt API directly, it solves a problem that lots of people

21:44.800 --> 21:48.800
have and where they know that they have this problem, I have parameters I want to tune

21:48.800 --> 21:49.800
them.

21:49.800 --> 21:55.600
We also have a number of tools that are for solving specific problems that modelers

21:55.600 --> 21:58.560
have that use Sigopt under the hood.

21:58.560 --> 22:04.080
We really like that it works well as just a component in other tools where the model

22:04.080 --> 22:06.560
may not even know that they're using Sigopt.

22:06.560 --> 22:11.040
Scott, is that a pretty common experience among the folks that use Sigopt?

22:11.040 --> 22:17.240
Yeah, I would say there's different tools for different jobs and it's about sometimes

22:17.240 --> 22:23.800
to end platform that allows some flexibility for the modeler and sometimes it's something

22:23.800 --> 22:27.960
where it's a very point solution for a very specific problem.

22:27.960 --> 22:30.880
I guess I have one quick question if that's okay for me.

22:30.880 --> 22:35.240
As you start to build up these platforms, obviously you've been there for 12 years.

22:35.240 --> 22:39.160
You've seen a lot of different iterations of this over time.

22:39.160 --> 22:42.320
How do you like measure the efficacy of some of your efforts?

22:42.320 --> 22:46.000
You guys have been on the bleeding edge for a long time but I imagine there's been several

22:46.000 --> 22:49.120
iterations of infrastructure, of experimentation.

22:49.120 --> 22:55.320
How do you make sure you're making progress or how do you make decisions of what's worth

22:55.320 --> 22:59.080
continuing to invest in, what's worth building versus buying?

22:59.080 --> 23:01.560
We do have a lot of modelers.

23:01.560 --> 23:07.200
It is also few enough that we're able to talk to them and have conversations about what

23:07.200 --> 23:08.600
they find useful.

23:08.600 --> 23:13.640
That's something that we're always doing is just interfacing directly with our users and

23:13.640 --> 23:16.560
understanding what they like and what they don't like.

23:16.560 --> 23:18.440
We do have tons of metrics.

23:18.440 --> 23:22.480
We're a quantitative investment manager so we like to look at the numbers.

23:22.480 --> 23:29.200
We always are looking at things like usage, seeing how that's tracking to help identify

23:29.200 --> 23:31.400
where we should focus.

23:31.400 --> 23:34.640
In some cases, it's not driven by numbers.

23:34.640 --> 23:39.480
It's driven by just keeping an eye out on what's going on in the industry, in the broader

23:39.480 --> 23:44.800
ecosystem of data science and seeing what's applicable to us, and then we evaluate those

23:44.800 --> 23:46.920
things through experiments.

23:46.920 --> 23:53.880
When we were looking at Sigopt, we compared it to not just the in-house solutions that

23:53.880 --> 24:00.800
we were already using but also some other open source things like we looked heavily at

24:00.800 --> 24:07.840
GPIOP as one of the big alternatives and saw how it worked on a variety of optimization

24:07.840 --> 24:11.080
problems that we had identified.

24:11.080 --> 24:17.600
When you were looking at it relative to those other things, just curious, was there any

24:17.600 --> 24:18.600
particular?

24:18.600 --> 24:26.080
Was it a performance motivation that led you towards Sigopt or was it more of a user

24:26.080 --> 24:31.640
experience thing and how do you weight those things?

24:31.640 --> 24:32.640
Yeah.

24:32.640 --> 24:37.640
It's performance, both the quality of the solutions that it finds and how quickly it

24:37.640 --> 24:47.160
finds it, but also the usability and not just is the API sane, but things like how much

24:47.160 --> 24:50.720
tuning do you need to do of your hyperparameter tuning?

24:50.720 --> 24:57.080
We found that for a lot of other solutions, GPIOP in particular, it was very sensitive

24:57.080 --> 25:01.720
to its parameters that you would set it with.

25:01.720 --> 25:05.960
The fact that it wasn't something that we could just run and reliably get answers and

25:05.960 --> 25:09.560
that it was something that you had to fuss with a lot was another qualitative measure

25:09.560 --> 25:14.440
that kind of turned us off from pursuing those and made us go with Sigopt.

25:14.440 --> 25:18.160
You need a hyperparameter optimizer for your hyperparameter optimizer?

25:18.160 --> 25:19.160
Yes.

25:19.160 --> 25:20.760
It sounds like a lot of turtles.

25:20.760 --> 25:25.200
Yeah, the number one piece of feedback we got after I opened source Mo at Yelp was this

25:25.200 --> 25:29.000
is great, but you've taken my optimization problem and turned it into another optimization

25:29.000 --> 25:30.000
problem.

25:30.000 --> 25:35.840
But I guess, I mean, so you have 500 incredibly intelligent modelers, some of which have

25:35.840 --> 25:38.400
very deep expertise in mathematics.

25:38.400 --> 25:40.920
How do you decide for something like this?

25:40.920 --> 25:44.760
Obviously, you tried a bunch of in-house solutions, you looked at a bunch of different things.

25:44.760 --> 25:49.960
How do you make that trade off of, this is worth us spending up a team of 12 people and

25:49.960 --> 25:55.040
attacking it for years versus taking kind of a best in class solution?

25:55.040 --> 26:00.120
How do you make that trade off internally of, this is worth us solving internally versus

26:00.120 --> 26:01.720
this is worth us partnering?

26:01.720 --> 26:05.480
Yeah, well, we looked at the opportunity cost.

26:05.480 --> 26:11.360
We have these folks who we've hired who are very talented, but we want them to be working

26:11.360 --> 26:16.080
on specific problems with the best tools that are available.

26:16.080 --> 26:23.080
So if there's something that already exists and it's a lot cheaper than us paying 12

26:23.080 --> 26:28.120
folks to a year to build out, of course, we're going to go with it.

26:28.120 --> 26:37.200
Yeah, I'm curious about, certainly on the modeling side, a lot of Barton Parcel to experimentation

26:37.200 --> 26:43.720
is, you know, faux, you go down a path, it doesn't work out, you go down another path,

26:43.720 --> 26:49.640
and that's kind of inherent to the modeling process.

26:49.640 --> 26:54.640
I'm curious if there are any things that you can share from a platform perspective, things

26:54.640 --> 26:58.080
that you tried, you were very excited about, they didn't really work out.

26:58.080 --> 27:03.960
They didn't give you the kind of performance or the acceleration that you were looking

27:03.960 --> 27:04.960
for.

27:04.960 --> 27:09.160
One thing that I can cite that's relevant to this is we've been looking at black box

27:09.160 --> 27:12.720
optimization for a very long time.

27:12.720 --> 27:14.200
It's always been an interest of mine.

27:14.200 --> 27:17.320
I knew that it would be applicable to us.

27:17.320 --> 27:25.480
So it was about seven years ago, I started an initiative to make some more black box

27:25.480 --> 27:30.440
optimization algorithms broadly available at the company that, in ways that leveraged

27:30.440 --> 27:34.360
the rest of our platform and were just as easy as possible.

27:34.360 --> 27:42.160
And one of the issues that we ran into very quickly was when we started running things

27:42.160 --> 27:47.200
at a totally new scale that was required for doing these black box searches, a lot of

27:47.200 --> 27:54.800
the small pain points around having like long running simulations fail, all sorts of

27:54.800 --> 27:57.720
things that just go wrong when you have distributed systems.

27:57.720 --> 28:01.640
A lot of those things just broke the user experience completely.

28:01.640 --> 28:08.240
And we had to like table a lot of those initiatives around doing black box optimization to revisit

28:08.240 --> 28:13.560
our core platform for how we run all of these jobs.

28:13.560 --> 28:18.320
And what the interface was, how we deal with things like failures and just how do we make

28:18.320 --> 28:19.840
it work at scale.

28:19.840 --> 28:26.280
So it was kind of a pivot, we took this thing that we knew was a good idea but really kind

28:26.280 --> 28:32.720
of failed because the environment wasn't right and then focused on the environment instead.

28:32.720 --> 28:41.600
And then kind of tabled the black box optimization as a service because it was such a hard problem

28:41.600 --> 28:47.480
to fit your at the platform, which took us years to build out and get right.

28:47.480 --> 28:52.360
So I kind of view that as something that didn't work out, it had these additional benefits

28:52.360 --> 28:58.560
but it also set us up for success later on when something likes it comes along that we

28:58.560 --> 29:03.920
couldn't have leveraged if we didn't already experience our own failures trying to solve

29:03.920 --> 29:05.240
the same problem.

29:05.240 --> 29:06.240
Okay.

29:06.240 --> 29:12.880
And at the infrastructure level, are you now having gone through that process?

29:12.880 --> 29:18.200
Are you using something like a Kubernetes or some other open source or is it a proprietary

29:18.200 --> 29:20.160
distributed computing solution?

29:20.160 --> 29:21.160
Yeah.

29:21.160 --> 29:25.880
So when we started this, it was before Kubernetes was really even a thing.

29:25.880 --> 29:26.880
Okay.

29:26.880 --> 29:30.640
For a lot of the things that we're talking about, we were doing it before it really was

29:30.640 --> 29:31.640
a thing.

29:31.640 --> 29:37.840
Even the term data science wasn't really happening 12 years ago when I started and big

29:37.840 --> 29:42.920
data wasn't a hot phrase that everybody knew.

29:42.920 --> 29:51.680
So our solution was built on top of Apache Mace OS, which we built a framework on top

29:51.680 --> 29:52.680
of it.

29:52.680 --> 29:55.960
It has this nice, pluggable, like framework interface.

29:55.960 --> 30:00.480
So we were able to build something that handled our bespoke scheduling needs.

30:00.480 --> 30:05.160
We open sourced it, our scheduler that we use because we do know that there are other

30:05.160 --> 30:09.480
folks who have kind of similar problems and there are benefits to us to open sourcing

30:09.480 --> 30:10.480
it.

30:10.480 --> 30:11.480
Yeah.

30:11.480 --> 30:17.480
So you started before something like Kubernetes, you were using Mace OS, you know, in that

30:17.480 --> 30:23.360
kind of 12 years ago, time frame, Mace OS was the Kubernetes of distributed compute

30:23.360 --> 30:29.880
in the sense that it was very popular for these kinds of applications and we are investing

30:29.880 --> 30:32.680
heavily in Kubernetes now over the last few years.

30:32.680 --> 30:33.680
Okay.

30:33.680 --> 30:34.680
Yeah.

30:34.680 --> 30:35.680
Okay.

30:35.680 --> 30:36.680
Interesting.

30:36.680 --> 30:42.600
And so you went down this path to build out some distributed optimization stuff.

30:42.600 --> 30:49.520
You ran into limitations of the infrastructure and it sounds like you spent a few years building

30:49.520 --> 30:55.360
out the infrastructure to support kind of where you wanted the platform to go or the kinds

30:55.360 --> 30:59.560
of things that you knew you needed to be able to do.

30:59.560 --> 31:05.880
Can you characterize how much of your efforts are spent kind of at the low level infrastructure

31:05.880 --> 31:13.000
versus the higher level tools and services that are more researcher facing?

31:13.000 --> 31:14.160
Yeah.

31:14.160 --> 31:18.640
So we do have our engineering work is also roughly about a third of the company and it's

31:18.640 --> 31:21.640
broken up into several teams.

31:21.640 --> 31:26.240
I work on modeling engineering where we focus on building up the tools and infrastructure

31:26.240 --> 31:28.960
that are targeting our modelers.

31:28.960 --> 31:33.720
There is a separate team called platform engineering which is providing the platforms

31:33.720 --> 31:38.000
for all engineering and all of the company.

31:38.000 --> 31:40.440
So they're about the same size.

31:40.440 --> 31:42.960
I think modeling engineering might be a little larger.

31:42.960 --> 31:49.640
So what we find is that sometimes we will build out platform solutions to solve a specific

31:49.640 --> 31:54.800
modeling problem and then realize, oh, actually this is more broadly applicable and do a

31:54.800 --> 32:00.720
handoff to another team to the platform engineering folks which has happened with a lot

32:00.720 --> 32:03.880
of our compute specifically.

32:03.880 --> 32:10.280
Did your team kind of identify this problem or Kubernetes in particular and kind of stand

32:10.280 --> 32:15.080
that up and then hand that off to the platform team later or were they already kind of ahead

32:15.080 --> 32:20.040
of that curve or you know, you got the same point in the curve?

32:20.040 --> 32:25.680
So the initiative to adopt MESOS came out of modeling engineering specifically when

32:25.680 --> 32:31.680
we were trying to solve this modeling problem of doing black box optimization at scale

32:31.680 --> 32:35.160
on like simulations.

32:35.160 --> 32:41.200
And then it turned out, yes, this is a workable solution and it is more broadly applicable

32:41.200 --> 32:46.000
so we should transition it over to another work.

32:46.000 --> 32:54.040
Okay, got it and Scott, SIGUP has been doing some things with Kubernetes as well.

32:54.040 --> 33:00.320
Can you talk about that and the relationship between the work you're doing on the optimization

33:00.320 --> 33:07.320
side and the infrastructure do you see in particular do you see folks, other folks kind

33:07.320 --> 33:11.240
of express that same challenge where there's one, there are things they want to do to

33:11.240 --> 33:17.200
empower their modelers and their researchers, but they run into infrastructure challenges?

33:17.200 --> 33:18.200
Definitely.

33:18.200 --> 33:22.600
We see this all the time where especially if you're transitioning from a world where

33:22.600 --> 33:29.320
you're maybe doing manual tuning of a model where it's an expert trying to do it sequentially

33:29.320 --> 33:33.320
in a notebook environment or something like that doing 10-dimensional optimization in

33:33.320 --> 33:37.320
your head is hard enough but trying to do that maybe across a hundred different workers

33:37.320 --> 33:40.400
is maybe impossible.

33:40.400 --> 33:45.120
But in addition to that being a hard optimization problem, it becomes just a hard like resource

33:45.120 --> 33:49.760
management problem of SSHing into a hundred different machines and making sure that they

33:49.760 --> 33:52.240
don't fail like Matt said and everything like that.

33:52.240 --> 33:55.480
So we're seeing more folks really doing it like that.

33:55.480 --> 34:00.400
We've definitely had users where they want to take advantage of our ability to do high

34:00.400 --> 34:06.640
parallelism as part of our optimization suite when we support up to a hundred individual

34:06.640 --> 34:07.640
workers.

34:07.640 --> 34:12.440
I've seen users, yeah, literally SSHing doing a handful of different machines and trying

34:12.440 --> 34:17.000
to like, they're using screen to keep the sessions alive and they're trying to do everything

34:17.000 --> 34:19.040
like that.

34:19.040 --> 34:23.080
And at the end of the day you could be an expert in deep learning and expert modeling,

34:23.080 --> 34:27.400
but then all of a sudden there's this massive barrier of DevOps and doing this right.

34:27.400 --> 34:33.760
So we try to be active members in this community, we're contributors to Kubeflow, we've developed

34:33.760 --> 34:38.360
our own solution called orchestrate that handles all of this for you as well.

34:38.360 --> 34:42.040
Take something that you might have written in a notebook and then very easily you can

34:42.040 --> 34:47.000
containerize it up, pass it off to a Kubernetes cluster, and then sigopt acts as this distributed

34:47.000 --> 34:50.760
scheduler for all of your training and tuning jobs.

34:50.760 --> 34:55.720
And we see this as again helping just amplify what these modelers are already doing.

34:55.720 --> 35:00.080
Like again, you could be an expert in defining the model, understanding the data and things

35:00.080 --> 35:01.080
like that.

35:01.080 --> 35:06.040
But you don't want that barrier of parallelism, that barrier of distribution to be what

35:06.040 --> 35:09.520
prevents you from really getting to that best possible answer.

35:09.520 --> 35:13.280
So we see this as something that some people are building into their platforms.

35:13.280 --> 35:18.840
We see this as some individual researchers just want to be able to have this superpower,

35:18.840 --> 35:22.520
but we're continuing to invest in that, both in actively contributing to the open source

35:22.520 --> 35:27.800
community and building specific tools tailored to the enterprise that we can serve our customers

35:27.800 --> 35:29.800
with.

35:29.800 --> 35:40.360
And across the folks you talk to, how do folks know when they need to transition from this,

35:40.360 --> 35:46.200
I'm going to do optimization, man, I guess there's one, there's the step zero I'm doing

35:46.200 --> 35:51.920
in my head and I need to do it programmatically or I need to do it manually.

35:51.920 --> 35:56.120
And then I need to do it in a more automated fashion.

35:56.120 --> 36:01.080
I'm trying to get a how do folks come to terms with, folks that aren't heavily investing

36:01.080 --> 36:07.120
in platforms or optimization or some type of automation, what are the things that you

36:07.120 --> 36:13.000
see like clicking for them that motivate them to start investing?

36:13.000 --> 36:14.000
Yeah.

36:14.000 --> 36:18.440
So I think it goes back to what Matt was saying around opportunity cost and you can define

36:18.440 --> 36:20.680
opportunity costs in a variety of different ways.

36:20.680 --> 36:24.600
So for some firms we work with, it's about expert productivity.

36:24.600 --> 36:28.000
So we're working with a global technology consulting firm where when they rolled us out

36:28.000 --> 36:32.800
globally, every single team that was using SIGOP was able to complete their client engagements

36:32.800 --> 36:34.600
30% faster.

36:34.600 --> 36:40.160
We're also working with a small startup that has five PhDs doing really cutting edge AI

36:40.160 --> 36:44.280
and their founder says they think of SIGOP is just another member of the team because

36:44.280 --> 36:47.680
it's taken this burden of doing this manual tuning off of them.

36:47.680 --> 36:50.960
And so that expert time can be a massive opportunity cost.

36:50.960 --> 36:55.800
But also, just if the performance of your models matters, that's more opportunity cost

36:55.800 --> 36:56.800
as well.

36:56.800 --> 37:01.560
Because usually optimization of architectures, feature transformations, hyper parameters,

37:01.560 --> 37:06.400
it takes time, but it's usually also orthogonal to any of the feature engineering you're doing.

37:06.400 --> 37:09.680
So it's added benefit that's otherwise being left on the table.

37:09.680 --> 37:12.320
It's not about replacing what your data science was doing.

37:12.320 --> 37:14.760
It's about just adding to what they're doing.

37:14.760 --> 37:17.000
So that's another big opportunity cost.

37:17.000 --> 37:20.160
And then finally, it's time to market its compute costs, it's things like that.

37:20.160 --> 37:25.480
If you're spinning up GPUs and waiting around for sometimes weeks to get the results of

37:25.480 --> 37:31.000
something being tuned, cutting that down to days or hours can really change the way you

37:31.000 --> 37:32.640
do iterative modeling.

37:32.640 --> 37:37.120
And so I think it's really about seeing if you run into one of these pain points or if

37:37.120 --> 37:41.840
there's just a lot of opportunity being left on the table, expert time, compute time to

37:41.840 --> 37:47.800
market, or just the value of these models that you're investing so heavily in.

37:47.800 --> 37:54.720
And are there particular types of models that are prevalent among your researchers?

37:54.720 --> 38:00.920
And from a platform perspective, are you building out specific platform capabilities

38:00.920 --> 38:03.000
to support specific kind of models?

38:03.000 --> 38:09.040
Or do you think about the world more model agnostic?

38:09.040 --> 38:14.600
So like I said before, we try not to be prescriptive about what our modelers do and we really

38:14.600 --> 38:16.600
do everything.

38:16.600 --> 38:23.840
And obviously, over the last few years, there's been a lot of buzz around deep learning and

38:23.840 --> 38:28.760
it is something that we have been looking at really deeply.

38:28.760 --> 38:32.680
No, put on bombs.

38:32.680 --> 38:39.920
But that is one technique where it really did turn out that we need to build some things

38:39.920 --> 38:46.400
that are specific to supporting that technique.

38:46.400 --> 38:53.960
So it is a pretty broad class of models or a broadly applicable technique.

38:53.960 --> 38:54.960
Right.

38:54.960 --> 39:00.680
So we have had to invest into things that are targeting deep learning scenarios.

39:00.680 --> 39:06.080
Just to even test out whether it works on our types of problems.

39:06.080 --> 39:12.360
And as we all know, hyper parameter tuning there is also a big challenge.

39:12.360 --> 39:16.280
So that's another place where we are playing Sigopt.

39:16.280 --> 39:22.440
And was that something that just worked out of the box like, was it, did you have to

39:22.440 --> 39:29.440
do anything special to apply Sigopt or any other other things in your tool chain to deep

39:29.440 --> 39:36.960
learning or was it just work?

39:36.960 --> 39:42.440
The challenge is around getting the machines that have the GPUs that we want in order

39:42.440 --> 39:43.680
to use it effectively.

39:43.680 --> 39:48.480
So that was one thing that we had to change how we do some stuff.

39:48.480 --> 39:53.520
That's me sounds like infrastructure like Kubernetes and scheduling and that kind of thing.

39:53.520 --> 39:54.520
Yes.

39:54.520 --> 39:55.520
Okay.

39:55.520 --> 40:01.240
But as far as Sigopt goes, one of the things that we love about it is how well it works

40:01.240 --> 40:08.280
out of the box and how easily it integrates with so many of our existing platform solutions.

40:08.280 --> 40:14.240
Kind of looking back over the past 12 years are there things that you had approached

40:14.240 --> 40:18.640
very differently knowing what you know now?

40:18.640 --> 40:19.920
That's a great question.

40:19.920 --> 40:27.080
So the one thing that immediately comes to mind is Python and how dominant Python has

40:27.080 --> 40:32.800
become in the data science modeling space.

40:32.800 --> 40:39.200
That is something that certainly wasn't obvious back in 2001, it wasn't obvious when I had

40:39.200 --> 40:40.200
started.

40:40.200 --> 40:43.040
We've been a JVM shot since the beginning.

40:43.040 --> 40:50.240
So built out a lot of tools and infrastructure on the JVM for doing not just for building

40:50.240 --> 40:54.920
or systems, but for doing the analysis that we need to do.

40:54.920 --> 41:03.920
So I would say investing more in Python earlier on, getting on that train would have been

41:03.920 --> 41:10.640
something that I would do differently, but it really has taken over for us, our modelers

41:10.640 --> 41:11.640
come in.

41:11.640 --> 41:17.480
Everybody higher comes in with that as their language of choice, the best library, open source

41:17.480 --> 41:25.440
libraries are out there and obviously the deep learning world is dominated by Python as

41:25.440 --> 41:27.240
well.

41:27.240 --> 41:31.960
So having had that experience, how do you apply that forward?

41:31.960 --> 41:38.080
Does that lead you to wanting to touch and try everything or are there specific things

41:38.080 --> 41:45.440
that you now see were clear in Python and Python's rise that you can apply that pattern

41:45.440 --> 41:48.680
matching to other things?

41:48.680 --> 41:52.940
I don't think so, but I think that the one change is that it's made us approach things

41:52.940 --> 41:59.720
in a more language and like platform agnostic way or not relying on everything happening

41:59.720 --> 42:04.320
on the JVM, so you can't predict what's going to win.

42:04.320 --> 42:09.720
So you have to build something that's flexible and that can support some different things.

42:09.720 --> 42:10.720
It's interesting.

42:10.720 --> 42:17.920
Folks, that end up on both sides of this, it takes a lot more resources to support

42:17.920 --> 42:20.520
something that supports everything, right?

42:20.520 --> 42:27.040
Then it does to build a very targeted solution and be prescriptive.

42:27.040 --> 42:34.600
But this is a great argument for not doing that because you don't know what we all pronounced

42:34.600 --> 42:41.880
in TensorFlow, the winner without any challenges and all of a sudden PyTorch popped up, right?

42:41.880 --> 42:48.720
And now it's totally reasonable to allow your modelers to use that if that's their preferred

42:48.720 --> 42:49.720
kind of interface.

42:49.720 --> 42:54.360
That's just one example of how stuff you think it's a game over and stuff just pops up

42:54.360 --> 42:55.360
out of nowhere.

42:55.360 --> 43:02.000
Yeah, but that said, some of the platform agnostic choices that we make are actually just

43:02.000 --> 43:08.240
the right choice to make any way, or they have additional benefits aside from being somewhat

43:08.240 --> 43:09.240
future-proof.

43:09.240 --> 43:16.320
For instance, data access and doing that through services where we're not having any expectation

43:16.320 --> 43:22.080
of what the consumer is, is an example of a way that we're adapting based off of what

43:22.080 --> 43:24.120
we've observed.

43:24.120 --> 43:30.440
And so if you were talking to a team that has been doing modeling, has been doing data

43:30.440 --> 43:39.280
science, has achieved a level of success there, but is early on in thinking about formalizing

43:39.280 --> 43:44.280
a platform effort, what kind of advice would you give to them?

43:44.280 --> 43:51.600
Well, the world is so different now and also a lot of those places if they're just starting,

43:51.600 --> 43:53.640
they're operating at a very different scale from us.

43:53.640 --> 44:01.600
So the cloud providers and their solutions are wonderful, especially at smaller scales,

44:01.600 --> 44:05.680
but even in some cases for folks at our size.

44:05.680 --> 44:11.120
So picking one and really taking advantage of it and all of the services that they offer,

44:11.120 --> 44:16.120
while being mindful about vendor lock-in is probably what I would say to focus on.

44:16.120 --> 44:25.560
Yeah, when you think about the cloud providers or generally about any kind of package offering,

44:25.560 --> 44:30.240
they often have kind of gaps that get in your way that can be hard to feel like, how

44:30.240 --> 44:37.280
do you balance that concern versus, you know, kind of the ease or just kind of getting

44:37.280 --> 44:41.880
everything all at once, or it is, first of all, is that a challenge that you've had

44:41.880 --> 44:47.440
to kind of engineer around in the past and if so, how do you kind of think through all

44:47.440 --> 44:48.440
that?

44:48.440 --> 44:52.320
It hasn't been so much of a challenge for us because we have been running our own data

44:52.320 --> 44:53.600
centers for so long.

44:53.600 --> 44:59.640
So we've already got things working really well without all of the cloud provider-specific

44:59.640 --> 45:01.440
offerings.

45:01.440 --> 45:07.960
So it made leveraging the cloud provider's compute really easy because we weren't dependent

45:07.960 --> 45:12.920
on any of their bespoke solutions.

45:12.920 --> 45:17.600
More generally than cloud, there are a lot of folks that are a lot of startups and even

45:17.600 --> 45:27.720
mature companies that are trying to offer NN1-size-fits-all platform in a box kind of solutions.

45:27.720 --> 45:34.080
And the route that you've gone is to at least in the case of optimization, identify kind

45:34.080 --> 45:41.360
of a best of breed, you know, focused, targeted thing and kind of plug it into some broader

45:41.360 --> 45:42.440
thing that you're building.

45:42.440 --> 45:48.160
Like when you think about kind of the role of that end-to-end, you know, versus kind of

45:48.160 --> 45:54.160
building based on, you know, individual things that you like, like how do you kind of parse

45:54.160 --> 45:56.840
all of that?

45:56.840 --> 46:00.600
Because we've already been doing it with components, separate components and haven't

46:00.600 --> 46:04.680
been leveraging one end-to-end solution because nobody really does everything that we

46:04.680 --> 46:05.680
need.

46:05.680 --> 46:11.920
I mean, we like to think deeply about every step of the process and find the right solution

46:11.920 --> 46:14.760
for each step.

46:14.760 --> 46:19.960
So that's one of the things that really appealed to us about Sigopt was that it really did target

46:19.960 --> 46:28.200
this one problem and solved it in a way that didn't, that wasn't locking us into any particular

46:28.200 --> 46:34.320
technology before or after that stage, that really was a huge benefit for us.

46:34.320 --> 46:39.680
And then on top of that, the fact that it could be used on any of the cloud providers,

46:39.680 --> 46:44.920
it wasn't just one solution that was like trying to keep us locked in somewhere, it was

46:44.920 --> 46:46.920
also a big benefit.

46:46.920 --> 46:51.280
And Scott, I'm curious if you have any perspective on that, you certainly, I'm sure you get asked

46:51.280 --> 46:52.280
that a lot.

46:52.280 --> 46:53.280
Yeah, definitely.

46:53.280 --> 46:58.920
And I think, again, it's going back to standardizing where it makes sense, but doing so in a way

46:58.920 --> 47:01.280
that's non-constraining.

47:01.280 --> 47:06.600
So as Matt was saying, you don't want to spend all your time being future-proof, but

47:06.600 --> 47:11.720
you definitely need to recognize that the flavor of the month is going to change.

47:11.720 --> 47:16.000
Four years ago, a lot of people using Sigopt were using Psychic Learn primarily, and now

47:16.000 --> 47:20.840
we see a lot of TensorFlow and PyTorch, and we're already starting to see quite a bit

47:20.840 --> 47:25.400
to pick up of reinforcement learning, even in production, with firms like OpenAI that

47:25.400 --> 47:27.400
use us.

47:27.400 --> 47:32.040
And two years from now, it's all probably going to be different, like the landscape continues

47:32.040 --> 47:37.280
to change, and it's not just the landscape of tooling, it's the landscape of infrastructure

47:37.280 --> 47:38.760
providers as well.

47:38.760 --> 47:42.560
Again, four years ago, AWS was so far in front of everybody else that you didn't really

47:42.560 --> 47:47.880
think twice, but now there's other providers out there, people use hybrid solutions, firms

47:47.880 --> 47:52.560
like Dropbox are moving back off of the cloud for cost-saving measures, and it's really

47:52.560 --> 47:58.640
about saying, what can I bring to the problem, what do I differentiate, and then leave optionality

47:58.640 --> 47:59.640
everywhere else?

47:59.640 --> 48:04.160
And that's something that's core to us, being agnostic to the tooling, the infrastructure,

48:04.160 --> 48:05.160
whatever it is.

48:05.160 --> 48:09.800
We want to meet you where you're at and augment what you're doing, not try to fit you into

48:09.800 --> 48:13.000
a sandbox or lock you into something in particular.

48:13.000 --> 48:18.520
Well, Scott and Matt, thanks so much for taking the time to chat about this very, very interesting

48:18.520 --> 48:19.520
topic.

48:19.520 --> 48:22.960
I appreciate having you on the show.

48:22.960 --> 48:24.280
Thanks so much, Sam.

48:24.280 --> 48:25.280
Thank you.

48:25.280 --> 48:31.160
All right, everyone, that's our show for today.

48:31.160 --> 48:35.560
For more information about today's guest, or to follow along with AI Platform Volume

48:35.560 --> 48:40.920
2, visit twemalai.com slash AI Platforms 2.

48:40.920 --> 48:47.040
Make sure you visit twemalcon.com for more information or to register for Twemalcon AI

48:47.040 --> 48:48.040
Platforms.

48:48.040 --> 48:52.440
Thanks again to Sigout for their sponsorship of this series, to check out what they're

48:52.440 --> 48:58.080
up to and take advantage of their exclusive offer for Twemal listeners, visit twemalai.com

48:58.080 --> 48:59.880
slash Sigout.

48:59.880 --> 49:11.360
As always, thanks so much for listening and catch you next time.

