Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
The podcast you're about to hear is the first of a series of shows recorded at the Georgian
Partners Portfolio Conference last week in Toronto.
My guess for this show is Solma's Shah Alizadeh, director of merchant services algorithms
at Shopify.
Solma's gave a great talk at the GPPC, focused on her team's experiences applying machine
learning to fight fraud and improve merchant satisfaction.
Solma's and I dig into, step by step, the process they used to transition from a legacy, rules
based fraud detection system to a more scalable, flexible one based on machine learning models.
We discussed the importance of well-defined project scope, tips and traps when selecting
features to train your models, and the various models, transformations and pipelines, the
Shopify team selected, as well as how they use PMML to make their Python models available
to their Ruby on Rails web applications.
Georgian Partners is a venture capital firm whose investment thesis is that certain tech
trends change every aspect of a software business over time, including business goals, product
plans, people in skills, technology platforms, pricing and packaging.
Georgian invests in those companies best positioned to take advantage of these trends and then
works closely with those companies to develop and execute the strategies necessary to make
it happen.
Applied AI is one of the trends they're investing in as our conversational business and
security first.
Georgian sponsored this series and we thank them for their support.
To learn more about Georgian, visit twimmalai.com slash Georgian, where you'll also be able
to download white papers on their principles of applied AI and conversational business.
Before we jump in, if you're in New York City on October 30th and 31st, we hope you'll
join us at the NYU Future Labs AI Summit and Happy Hour.
As you may remember, we attended the inaugural Summit back in April.
The fall event features more great speakers, including Karina Cortez, head of research
at Google New York, David Venturelli, science operations manager at NASA Ames Quantum
AI Lab, and Dennis Mortensen, CEO and founder of startup x.ai.
For the event homepage, visit AI Summit 2017.futurelabs.nyc and for 25% off tickets, use
the code twimmal25.
For details on the Happy Hour, visit our events page at twimmalai.com slash events.
And now on to the show.
All right, everyone.
I am here at the Georgian Partners Conference and I have the pleasure of being with Solma
Shah Ali Zadeh, who is Director of the Data Team at Shopify.
Solma is welcome to this week in Machine Learning and AI.
Thank you very much for having me, Sam.
It's great to have you, and I especially love when I get an opportunity to interview
folks who have listened to the podcast before, so thank you so much for listening.
You're welcome.
Yeah.
I subscribe, I think, like nine minutes ago or something like that.
Awesome.
Awesome.
Thank you so much.
Why don't we get started by having you tell us a little bit about your background and
how you got into data science and machine learning?
Sure.
In my undergrad, I studied computer science.
And towards the end of my undergrad, one thing I realized is that I'm really interested
and passionate about using computer science and the little bit of machine learning that
I knew at that stage to solve problems in other domains.
So as I figured that out, I did a master's in bioinformatics and I went to Sweden and I studied
there and as part of my thesis project, I actually worked with Sloan Kettering Cancer
Center in New York, trying to predict what happens in the cell after you give it sort
of multiple perturbations or different drug cocktails and how the structure of the cell might
change.
And at the time, we used actually recurrent neural networks which were not hot then because
that was 2006 and everyone was like giving us grief about like, oh, this was 80s, like
this was cool in 80s while you're using it now, but it lent itself very well to our problem.
So we used it there and then after that, I went to McGill in Montreal and I did another
master's that I was like more focused on machine learning and compare science and as part
of my thesis project.
I worked very closely with the department of oncology and molecular biology and we used
my career rate data to print it.
My career rate?
Yeah, so it's like a very simplistic way to explain it is like you have a chip and on
it, you have these probes that you can measure the expression of different genes.
So human body?
My DGPCR.
Sort of yeah, but like scaled that to thousands of genes on it.
So we had the data on that and we were trying to use that data which we had captured at
the moment before people have got any treatment to see if there is any signal in that data
that we can save there likely to have a recurrence of the breast cancer.
And the usage of that is that by knowing it, you can sort of like not give as much of
a hard treatment to people who don't need it and also plan for the recurrence for people
that are likely to get it.
So that worked really well.
But one of the key learnings I had there was that if you are building a computational
solution and you want people in other domains to use it, the first thing you have to learn
is to actually speak their language and understand how they explain their problems.
So remember as like a rookie grad student, I had this presentation, all of my p-values
and all the like statistical factors about why my predictor was awesome is on that presentation
and I started and you could see like after the second slide, the lighting like the eyes
of the molecular biologist is gone, like they are not listening, there's nothing keeping
them going.
And then through that I learned how to understand what's the actual real challenge for my collaborators
and how to explain my solutions in relation to that.
And then like towards the end of my degree it was like really engaging like I felt like
I understand in their domain on top of machine learning and I think that was like a learning
that I kept with myself.
And then for a while I worked there more instantly, an investment bank and then I joined Shopify
four years ago.
So Shopify is a leading cloud-based commerce platform, it allows you to sell on multiple
channels from like brick and mortar stores to huge, huge sales and enterprise like Tesla
Motors or Budweiser, all of these big brands, but also very small merchants as well.
So I joined Shopify, we were in the process of like changing our data warehouse and prepping
for IPO.
So it was really important to make sure we really understand the data we were capturing
and we can also have like clear definitions for the metrics that we were going to share
and all of that.
So I was part of the team that worked on that.
But then after IPO we realized, well, we have all of these machine learning expertise
in house and we have also all of this data about different aspects of commerce.
Right.
And beauty of commerce is that it's like messy, it's real world, you know, there's merchant
trying to fulfill orders in their basement of their home and their people having like thousands
of orders coming to a second and they have to deal with that.
So that brings with itself so many opportunities to take repetitive tasks out of the daily
life of a merchant and an entrepreneur and give them back either the gift of time or
the gift of money that they were spending on something else.
And that's like has been like fascinating part of my job at Shopify.
So last two years I focused mostly on machine learning teams, data science teams that build
products that are powered by data.
So one of this is our order fraud detection that runs on real time on every single order.
The other one is our cash advance product where we basically give cash to our merchants
because we think that's the amount and that's the right time to give them the capital
to help them grow their business and they return that money.
And now we're trying to like bring sort of this like basic level smartness to other
products such as that are shipping and fulfillment and things like that.
Okay.
That's where I am.
We spoke at length about the order fraud problem and your approach and solution to that earlier
today at the conference.
And why don't you tell us a little bit about the problem there, the context that you're
trying to apply machine learning to.
Sure.
So as I was talking today, what happens is like all of our merchants big or small have
one thing in common.
They are there to make sales to succeed.
What happens is that they see an order come through their store, they go ahead and fulfill
it and then because so they look at the order nothing looks suspicious, they fulfill it
like six months goes by and then they receive a charge back from the credit card company.
So they're out of the item because they've already fulfilled it and sent it to the customer.
They are out of the money for the sale because the credit card company refunds the the amount
and they also receive a charge back fee.
That really cuts into their cash flow.
So you don't have to have too many charge backs to feel the impact.
The other side of it is the emotional factor.
So we are saying like, okay, you know, you focus on building the best product that you
can, putting it in front of the right audience.
And now we now we somehow have to tell them to be okay that somebody across the universe
from you is using stolen credit card information to buy from you like that on an emotional
level is unsettling.
The other thing, the other reason we pick it as one of the first areas to tackle with machine
learning is the fact that it's really back office work like becoming an expert in fraud
detection is not going to make you a better product designer.
It's just not a core skills of our emergency.
The customer should have to think about.
Exactly.
So that's where we thought, okay, we have the data, we have the knowledge and we can scale
the solution so that not only the big merchants can benefit from it, but the merchants who
start on the platform and on their very first order, they can get this analysis that
back by a decade worse up their the worth of data.
And that's why we picked this problem as the first one to tackle with machine learning.
Okay, was the problem previously being managed manually where there's some group of analysts
that were doing this or was there not a prior solution in place for order fraud detection?
We had actually a prior solution and we have a group of risk analysts in house, so it's
like a combination of the two, but the prior solution was built like five years ago and
I think it was good for the time it was built, but it had very hard-coded rules.
So it would say things like if the order was placed using a web proxy, then probably it's
fraud.
Right now, we see many people use web proxies.
Even like you're here, you want to order something from US, you probably use a web proxy.
But we're not going to the details of the rules that was there.
The problem was like the rules were not learning on it.
Exactly.
Yeah, exactly.
They were static.
So I think it serves our product for the time it was working, but also we have had this
crazy hockey stick growth.
So as we have had this many more merchants and more visibility into sales, it became
apparent that we can make a difference by using our own data.
So what were the steps in kind of getting to a solution to this?
What was the first thing that you had to figure out?
The first thing that you have to figure out, and it's common across any machine learning
problem, is actually try to define what you're trying for self.
So it sounds very basic, right?
We're trying to fight fraud.
Yeah, exactly.
But then they find like, okay, I want to cash fraudulent transactions, but I actually want
to do it before the merchant has gone ahead and fulfilled the order.
So that brings some practicality requirements to the solution that we offer.
So any machine learning algorithm and system I build, it has to be able to run on every
single order as they go through without slowing down the platform, without having downstream
processes have to wait for it.
So that would itself brought some hard requirements on the kind of solutions we can do.
The next step was, of course, okay, so we have a classification problem, so we want to
classify these transactions to fraud and non fraud.
Let's see if we can actually clearly define what's the fraudulent order.
So we did some digging there to make sure like our definition is correct and also can capture
if they're anomalies or they're changes.
So for example, if we are relying on dispute codes from payment gateway or a bank and
that happens to change, like let's make sure we have automatic detection in place or
some way so we learn, okay, you know what, the fact that I haven't received anymore fraud
is not because fraud has gone down or has gone up, it's just the code that I used to capture
fraud has changed.
Okay.
So that making sure that basically the targets of your prediction are correct and then we
got to like, investigating inputs.
And that's kind of an interesting story because you know, I talked about like the massive
pools of data we have, like over the last year we saw just last year, 100 million customers
placed orders on Shopify store.
So for these customers, we know the path they took to go to the product page, how much
time they spend, what's their color preference, so we have all of this information.
But at the same time, when you tackle a prediction problem, you also have to know, okay, of all
of these features that I have, which of them I'm actually going to have available at the
time that I'm making the decision.
So for example, if I'm going to use the number of orders this customer has placed in the
past, like, how am I going to have that aggregate count of orders available at the time of production?
And that brings again, like another layer of practicality to features that you can put
into your model.
So we went through that right now, we can actually, we've built like internal services where
we can get aggregate data, we can get real-time data aggregations.
And that of course gives a boost to the models.
Part of that is the architectural challenge of making sure that data is available.
But then in your talk, you also mentioned, you know, at one point you were building your
model around some feature that actually wouldn't be available to the model for months later.
Yeah, that's true, actually.
So that's one of the main challenges in fraud is that people can decide to place it charged
back up to a year, credit card companies allow you to take a year and say, like, look,
I believe this was a fraudulent charge at my card.
So we started looking at data and realized, okay, most of the fraud actually comes back
within the six months.
So we define our target as like, okay, has this transaction resulted in charge back in
six months or not?
Okay.
But that also means for any transaction, we actually know the full ground.
We have to wait for the full ground truth for up to six months.
So that's a challenge.
So we have to make sure, okay, are there any leading indicators that we can use?
So if I add a new feature to a model and I want to see how that's going to work for new
orders, I have to actually wait six months to see what the prediction, if the prediction
is correct or not.
So some of it we deal with it by using historical data.
If we have the feature available in past, if not, we depend if the feature is like really
out of nowhere, we don't know we have to wait.
But if the feature, like if we can get a degree of confidence with leading indicators and
we go with that, we say, okay, has the ratio of fraud we've seen within the two weeks gone
off or down.
So we try to do that to have like a faster iteration speed.
Okay.
And even in terms of the definition of fraud, you know, I'm just thinking out, you know,
there's, you know, stolen credit cards, you know, there's people that I always worry
about this as like a, you know, an eBay seller, like you sell something and you ship it
to someone they say they never got it, but they got it.
That's another, like did you did a lot have to go into actually defining the types of
fraud, did you model for every kind of fraud or just a specific subset of the possible
fraudulent universe?
For the first step, we actually decided to select a subset of fraudulent universe, as you
say, because the features and the characteristics that we were studying and trying to understand
were more around the financial fraud.
So people using stolen credit card, we are looking at adding other models and capabilities
to pick things like item that is described and also with better shipping information integration,
we can also see if the item was delivered or not.
But for the first version, it was really important for us to have a very clean cut definition.
Yeah.
So we went with the charges that we thought were fraudulent due to financial reasons.
This also included things that the merchant had gone in the admin and canceled due to fraud
either because of a call they got from the credit card company or the bank and we called
those ones as fraud as well.
So we were very sure and of course we, we have this internal platform built on top of
Spark and Pi Spark and we, like, we made our definitions into jobs with unit tests that
run on schedule.
So as a byproduct of this, regardless of which part of the team you work on and what
day of the week you query the database, you're always going to get the same orders as being
fraud and same orders and being non-frog.
And that helps a lot in being able to sort of validate results and models as we go ahead.
Okay.
So you have your definitions set like what's next?
Okay.
So we have a definition.
So now we basically have labeled data.
So what are the inputs?
What are the features that we're going to put into models?
So we started with very basic things like things around payment gateways and credit cards
and those were like the easiest things to think about.
But we had to also for that do some checks like make sure.
So we had also another feature where we started looking at it and looked predictive.
But then we realized, yeah, but this feature has not been fired for the last six months.
So actually I can't use it because for whatever reason we're not producing it anymore.
Meaning it just got pulled out of the platform somewhere and it needs you.
Okay.
So that's the thing.
Like there's lots of data.
Like there's massive amounts of data.
But you also have to understand like who's producing it?
What's the expectation level unavailability on?
Right.
And sometimes like for example, orders that come from like a point of sale are not going
to have all the features of the orders on the web.
So the features you pick if they're going to run across different gateways and across
different channels have to have a representative value for all of these scenarios.
Otherwise the model is going to be biased towards one versus the other.
So we realized that feature recently mattered a lot.
Feature frequency mattered a lot.
How often are we going to see this feature?
And then the distribution of values and how often it's going to be null or not present.
So what ones we figured out this list, one of the things we always focused on on Shopify
is like, how can we scale this?
So I don't want every single data scientist in the team to every day have to like code
how to figure out these things out and have these checklists in their mind.
So because I think there's like way more interesting things that they can do.
So we made templates like we made Python template codes that they can run a new feature
through and it would produce this sort of descriptive statistics.
So none of it is super complicated, but it's just the fact that we have thought through
this step gives us a boost in the speed for our delivery.
So now if you're a part of the team and you want to add a new feature before you even
put it in a model, you can run it through this sort of scripts or I put on notebook or
a job.
And you would get a report that says, okay, you know, over the last 12 months, this is when
this feature has been null, this is when it has been missing.
These are the distribution of values you see for it is how it looks across different segmentations.
So that's something that's very simple, but in action, it helps a lot.
And is that those tools are they primarily used for kind of exploratory analysis?
Or are they do you have like a whole framework for back testing and things like that?
So this specific alone one we use for exploration because we're trying to add features into new
models and see how they work.
We do exploration by pulling these features that have passed the checks and the target
and then trying a different learning models.
And we started with the simplest ones that are really easy to explain and easy to sort
of debug as well because we knew we had to scale it for 500,000 versions.
So for the first one, it's okay, we're going to go with a random forest.
So then we have a pipeline where you define your features, you define the transforms you
want to apply on those features and the models.
So for example, I want to say, so as an input data, I get where the order is placed.
But I transform it to a feature that says, is the order placed on a tablet or not?
So it's like a level of change you do on top of a feature, but it's really important
that that's transformation is well defined and also well understood by the downstream
parts of the pipeline.
So after the transformation, we also have the model training and then we do our testing.
And then we, so then we optimize for different metrics and we really tie those metrics based
on what's accepted in the fraud detection industry rather than like optimizing for, you know,
for true positives or true negatives.
So you really want to balance, you want them in merchants to be able to take as much
sale as they can with peace of mind.
So that's the optimization.
So elaborate a little bit on the, you know, this difference between the kind of these generally
accepted metrics versus the ones you might otherwise track is the idea that, you know,
you can't report AUC to a merchant because that doesn't mean anything to them or is it
or are there, you know, industry specific terminologies or metrics that you need to kind
of map things to?
Sure.
So there are industry metrics within frauds.
For example, like, it varies a little bit, but above 95% of your orders should be accepted.
Like there's 95% of your transactions.
We actually don't have to worry about, we're not going to be fraud.
It's within the next 5%, how much of it do you ask a person to investigate, call the
customer, try to verify it a little bit of extra steps or it's the ones that you say,
you know, this is fraud and you have to go and cancel it.
So we have built in our pipeline, we can actually pass it the product metrics and see, okay,
this is the bucket size of each recommendation that I want you to have and optimize for metrics
within this bucket sizes.
So that's one way to go because then we actually know what the practical impact of this metric
is going to be on the merchant.
We also look at the losses that they would have.
So the value of accepting this order and receiving a charge rack versus like a loss of
not having like not actually letting the order go through and we try to optimize for
that.
But yeah, I would say like if you want to remember one thing is like the metric that you
use for tuning your model has to be something with very understandable user impact because
this model, this product, going to wild and power real like people.
So we have to have an understanding of what happens if I actually pushed just too much
one way or the other and what the impact is going to be on the user.
So you've got your model now trained up.
What's next?
What's next is what we call production back test.
So in that, what we do is that we say, okay, for the most recent six months of the data,
which as I said, we might not always have all of the prediction results ready.
We are going to still run the model and do the prediction and look at the distribution
of the predictions across the six months and across different merchants.
So we're going to see, okay, we train this model to say put X percent of the orders in
cancel bucket.
Is it doing the same in the most recent orders?
Has something changed in the patterns that's not allowing for that?
And then we even go find our grain and we say, let's look at the model predictions within
the segments of our user rate.
So let's look at people in a specific geography or in a specific channel to see if there are
any problems there.
And then we go once that deeper and that if we have merchants with individual commendations
have changed significantly.
That means that person is going to have a very different experience when they log in
that day.
So if that's the case, we reach out to them ahead of time and we let them know that this
change is happening.
And this is why this model is better and this is why your experience is likely to change.
And they're actually really positive because we have to focus on making their business
better.
Okay.
And what are the things that trigger that last scenario?
Is that when for whatever reason, that merchant or that category becomes a target of fraud
or is it something more statistical, drift in a distribution or is it stuff that you've
done in terms of just tweaking models?
Most of the ones we've seen so far is by addition of features that are totally different.
So for example, if I start looking at historical data on the merchants and I see a cookie this
merchant is more likely to have fraud or less likely to have fraud, that is what's changing.
So most of the time it's introducing not a single new feature, but a class of new features.
Like historical values or when we start looking at the features about the browsing behavior,
then that brings an extra level of detail.
That being said, even when I say that change is drastic, it meets some checks, but it's
just like for one person, maybe one day they get like 10 more orders than they are used
to to investigate or 10 less orders to investigate.
And you'd be surprised how much people build their own workflows and their own understanding
of them, so you want them to feel safe and protected because that's the whole goal of this
product is for the merchant to feel safe and protected like that.
Right.
Right.
Yeah.
Okay.
So you've done your back testing.
Are we done?
Are we?
No, no, no.
So we've done all of these things, but then we've done all of it in mobile data land.
It's like everything in data and everything in data in our company is done with Python,
Python, but Shopify is a very big Ruby and Rails application and even the services we build
inside it, they're all Ruby and Rails.
So we have to find a way to transform this model that we have made in Python to run in
a Ruby application, which is a risk application.
So for that, there are many different ways for us because we know there are many other
applications in Shopify that also use Ruby and Rails.
We wanted to find a way to see if we can actually run our machine learning models in in a Ruby
application.
So what we did is we went with this model serialization definition, which is called PMML.
So it's predicted modeling markup language.
It's been around for decades.
It was mostly used in academia, but now it's having a comeback and different languages
and packages have PMML transformations.
So in scikit-learn, you can save your model in PMML or you can save your model in PMML.
And it's very similar to XML.
So it's like, it has a very well defined spec.
So you define your inputs, you define your transformations, the model and then decision-making
at the end.
And so it kind of ties into our pipeline.
And we got ideas from that for the levels of abstraction that we put in our pipeline.
So what we do is like, once we get the model, we serialize it to PMML.
And then we have built a gem that we're planning to open source next year.
Basically, it's a Ruby interpreter for PMML.
So once that gem is included in the Ruby application, it's able to sort of like understand this
PMML model and evaluate orders using it as they come true.
So then we have the model.
Before we go past that, I've been waiting for us to get to this point because I first
came across PMML probably like four or five years ago or so and couldn't find anyone
who is really doing anything with it.
And there's another one.
I forget the name of it, but it's another kind of model serialization.
I'm sure there are a bunch of model serialization things.
But so you serialize this model and you have your inputs and your outputs, what's the
level of abstraction of the serialized model?
Like are you telling it like a model type, like this is a random forest and then it's up
to the implementation that is interpreting it to actually know how to implement a random
forest or is there some other mechanism for actually implementing the model?
So PMML spec, for example, has something that says, okay, this is a logistic regression.
These are the inputs I need for a logistic regression.
And this is how I'm going to give you the output.
But the actual implementation of the code that does it, it can be done in any language.
So what it is is literally just a transform mechanism for one language to the other one.
So it's an XML document, it can open, then you can see it, then it has like the inputs
and the model definition and the sort of weights and things that go into the model and how
it's going to make the decision at the end.
But it doesn't do anything, right, it doesn't have any execution engine type to do.
Right.
So my question is, do you ever run into issues where I guess if your deployment environment
is all Ruby, it would have to be something like, you know, you've got some weird dependency
thing where you've got one version of, you know, one Rails app that's, you know, pinned
to, you know, some Ruby machine learning library and version and you have another that's
pinned to another version and because you're serializing this model at like a level of
abstraction higher, you get different results based on where you run it.
So right now we have one like one gem that we use across Shopify and right now we are
using it in a single application.
But like any gem, if we don't pay attention to which version of the gem is running, we
can run into problems, but we have checks for that.
So what we do is that when the model is ready, but it's still not ready to meet the user.
So what we do is that we do what we call live model practice.
So we deploy the model in the application.
So as the orders come true, it evaluates them, it makes a score and a recommendation,
but instead of powering the users, but it only writes it to a Kafka topic.
So what we do is that we're also observing the same data in data land, right?
And we're constantly training.
So what we do is we compare what did the, in data we predict using this inputs and what
do we, do we predict in production using this inputs?
I mean, match those and it's only when we match like 100% match between the new systems
that we pick it out of the shadow mode and we power users.
And is this process, is this an automated process or is this data scientist, you know,
kind of manually overseeing this transition into production and making sure that monitoring
the performance over time and then hitting a button to deploy it.
So parts of it are automated.
So like the, the jobs that reconcile the two are automated, the reports that show you
what the reconciliation looks like is automated.
The last step of just like making sure everything reconciled and then pushing the deploy,
no, that's not automated right now.
And I do want us to get a bit more experience before we like fully automate that or at
least learn how to catch things if something goes wrong.
The other benefit of live model practice is that also it gives us some metrics and like
real understanding around performance of this new model.
So as we make more sophisticated models, we are reaching out to more internal services
to get data with different SLAs and models, they're getting more complicated.
So what's the runtime of it going to be?
And I remember I said like we'd really want to have the fraud detection and risk analysis
ready as soon as the order is placed.
So it allows us to see, okay, what's the real performance of this model?
And once all of that are good, then the model is ready to meet the user and hopefully
the user will be delighted by the results.
And do you currently deploy new models out to all users or do you do like A-B testing
or something like that?
Do you feel like all the steps you've taken up to now mean that you don't have to do
like partial deployments and A-B testing?
Right now we deployed to all users, but I foresee there are other things that we want
to do.
We want to be able to look at a voting scheme between different models.
We want to be able to have more specialized models for specific segments of the users.
So when we get to that, we're going to do more A-B testing.
So it's something that I foresee in our future for sure.
And then we have to do holdout sets so that we're not impacting the actual real prediction
and yeah, the fun with that.
How far do you see that going?
I mean, you can, the way you describe that you can or I can envision like, you know, different
models for kind of arbitrarily small customer segments.
Yeah, you can do that.
It's actually with the pipeline.
It's very easy.
Okay.
But then it becomes like just because we can do it, should we do it or not?
Because then it brings also a maintenance.
There's an overhead.
Yeah, overhead associated with it.
But yeah, in terms of like platform and scale, yes, we can do it.
But one of the things that's interesting is like by us going through this problem, specifically
focused on fraud, we actually have built this pipeline that not can be used in other parts
of the platform as well.
So if I want to deploy an algorithm that tells our shipping service, like what are the default
dimensions of something that the user has added, I can use the same pipeline, like the
same encoding of model and then deploying it and running it in another Ruby app.
So those are the steps that we wanted to make easy because I think companies in general,
they use AI or today they encounter AI in two different ways.
Sometimes the company is built as an AI company and the challenge they have is finding the
data.
And sometimes the company is built, the business model is working perfectly.
They have ton of data and now they want to introduce machine learning to parts of existing
company.
And I think we are in the latter one right now.
So what matters for me right now on my team is to be able to sort of unlock that capability
across many services so that we can give the benefit you know at scale to different parts
of the platform.
Right.
Right.
Interesting.
So you've got the, you've got this pipeline going and you said it was a number of times
Spark and Python and is the data, is the data HDFS, is that why you're using Spark primarily
or?
Yeah.
So our internal platform that was initially built as an ETL extract transform load system
uses Spark, PySpark and it's really good because it, well, the volume of our data is really
high and Python is really easy to adapt.
So PySpark was like a sweet spot for us as a company to adapt and we built this like
platform so it abstracts so many things from the user.
So I'm now with the Spark DataFrames that's actually like really easy for anyone who has
worked with tabular data and SQL to like write a Spark job.
So we use that because we also have like an amazing team of data engineers that have built
this platform and by free we get so many things, I was actually just talking to someone upstairs
we get things like metadata on every input data that we have.
So for every model, I have a UUID and by that I can go and see what was the Gitcha of
the code that ran this model.
What was the what?
Gitcha like the, oh, the Gitcha, I've got it.
And then the path to the, the gith commit of the model.
So I can exactly go and reproduce what version of the code was run.
And I also have the information on the sort of snapshot of the data that was run as input.
So all of these capabilities to reproduce, to be able to audit, to be able to like go
back and check or reproduce, those come for free like my team didn't have to build those
are like, yeah, exactly.
Those are amazing things that were built for us and we just use them.
Okay.
Nice.
Is all that infrastructure and Spark is that, is that all deploy time or not deploy time
but like inference time or is that involved in training as well?
It depends.
Okay.
So Spark has machine learning library.
So we use it for training as well, depending on the models that we are using.
But it's well known that like scikit-learn has just like a broader set of machine learning
capabilities implemented.
So I'm hoping that the community, including our company gives back to Spark by adding
this different learning algorithms.
But yeah, we sort of go between like for loading of the data for doing all the transforms.
Also the things are at Spark level, but then for sort of training, we use scikit-learn
heavily.
Okay.
So it's a mix of two.
It also makes onboarding people really easy.
So many people in the data field are picking data or they've already worked in data on Python.
So it's like a familiar interface and it just breaks that barrier very easily.
And it's a lot more accessible than Spark.
Exactly.
Yes, it is.
Okay.
Awesome.
So you would want to leave folks with or left folks with it in your presentation?
So right now I've focused a lot on the mechanics of making this model, be a life.
Part of that is because I had the luxury of being in the same company, being in the same
domain for like two and a half years before we even started tackling this problem.
That means like the understanding of what the features mean or where do I have to poke
to find out what the future means sort of came with me and my team.
So I think one thing that I want to emphasize to people is that try really hard to understand
the domain and the problem you're trying to solve because at the end of the day, so
many times, machine learning sort of builds up on top of heuristics that already humans
that are in that field know.
So having spending that time that does not feel like you're working on a fancy learning
algorithm is actually one of the most important parts of having a successful data product.
Yeah.
Interesting.
Well, thank you so much for taking the time, so much to sit down with me.
I enjoyed your presentation and I enjoyed talking to you about your presentation and
I appreciate it.
Oh, thanks a lot.
And thanks for having me and keep producing this awesome podcast.
Thanks so much.
All right, everyone.
That's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on solmars or any of the topics we covered in this episode, head
on over to twimlai.com slash talk slash 60.
To follow along with the Georgian partner series, visit twimlai.com slash GPC 2017.
Of course, you can send along feedback or questions via Twitter at twimlai or at Sam
Charrington or leave a comment on the show notes page.
Thanks once again to Georgian partners for their sponsorship of the show.
Be sure to check out their white papers, which you can find by visiting twimlai.com slash
Georgian.
Thanks again for listening and catch you next time.
