WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:28.720
I'm your host Sam Charrington.

00:28.720 --> 00:33.040
I want to send a quick thanks to our friends at Cloud Era for their sponsorship of this

00:33.040 --> 00:39.760
series of podcasts from the Stratidata conference, which they present along with O'Reilly media.

00:39.760 --> 00:41.960
Cloud Era is long been a supporter of the podcast.

00:41.960 --> 00:47.760
In fact, they sponsored the very first episode of Twimble Talk back in 2016.

00:47.760 --> 00:51.920
Since that time, Cloud Era has continued to invest in and build out its platform, which

00:51.920 --> 00:57.680
already securely hosts huge volumes of enterprise data, to provide enterprise customers with

00:57.680 --> 01:01.920
a modern environment for machine learning and analytics that works both in the cloud

01:01.920 --> 01:04.240
as well as in the data center.

01:04.240 --> 01:09.680
In addition, Cloud Era Fast Forward Labs provides research and expert guidance that helps enterprises

01:09.680 --> 01:14.080
understand the realities of building with AI technologies without needing the higher

01:14.080 --> 01:16.440
and in-house research team.

01:16.440 --> 01:19.840
To learn more about what the company is up to and how they can help, visit Cloud Era's

01:19.840 --> 01:32.960
machine learning resource center at cloudera.com slash ml.

01:32.960 --> 01:33.960
All right, everyone.

01:33.960 --> 01:35.480
I am here with Shaolin Sam.

01:35.480 --> 01:39.600
Shaolin is a research engineer with Cloud Era Fast Forward Labs.

01:39.600 --> 01:42.840
Shaolin, welcome to this week in machine learning and AI.

01:42.840 --> 01:43.840
Thank you.

01:43.840 --> 01:44.840
Thank you for having me.

01:44.840 --> 01:45.840
Absolutely.

01:45.840 --> 01:48.440
I am really excited about our conversation.

01:48.440 --> 01:54.200
We are going to be diving into some of the work that you've done recently exploring learning

01:54.200 --> 01:56.800
with limited label data.

01:56.800 --> 02:00.160
Before we do that, tell us a little bit about your background.

02:00.160 --> 02:03.720
How did you get started working in AI?

02:03.720 --> 02:09.240
Yeah, so my background is actually in electrical engineering and computer science.

02:09.240 --> 02:10.640
Go doubly.

02:10.640 --> 02:11.640
Yeah.

02:11.640 --> 02:17.640
So after graduation, I worked in the financial industry designing quantitative trading

02:17.640 --> 02:27.200
strategies and then that got a little bit old because I felt like we were using data but

02:27.200 --> 02:31.960
not really for the right reasons or the right thing.

02:31.960 --> 02:39.280
Then I kind of step back and started to focus more on early stage venture investments

02:39.280 --> 02:46.480
where I mostly looked at women-led companies and try to essentially help them through

02:46.480 --> 02:51.600
the whole startup venture fundraising phase.

02:51.600 --> 02:57.000
And after that, I realized that I actually really, really love data and missed programming.

02:57.000 --> 03:05.000
I miss actually building things and that's actually how I found my way back into nitty-gritty

03:05.000 --> 03:07.960
work and Fast Forward Labs.

03:07.960 --> 03:14.600
I remember when I finished, actually I don't think this was the case for undergrad but for

03:14.600 --> 03:19.960
grad school, like we were just at the time when the consulting industry was like sucking

03:19.960 --> 03:24.600
a lot of people out and it was a transition from when consulting was taking everyone to

03:24.600 --> 03:26.800
when financial services was taking.

03:26.800 --> 03:36.320
Yeah, I think my dissertation was in Operation Sweet Shows and Operations Management.

03:36.320 --> 03:41.720
So a lot of us went into management consulting.

03:41.720 --> 03:50.720
And then the next, I guess, popular choice was finance, specifically the Quant Stuff.

03:50.720 --> 03:51.720
Okay.

03:51.720 --> 03:52.720
Yeah.

03:52.720 --> 03:53.720
Nice, nice.

03:53.720 --> 04:01.080
The OR stuff was one of my favorite classes in grad school, like the, what was the,

04:01.080 --> 04:05.200
I'm trying to remember the name of the software that, God, I would be so dating myself to

04:05.200 --> 04:09.360
say that you got the textbook and it was this floppy disk in the back of the book.

04:09.360 --> 04:12.200
I had like a puma on it.

04:12.200 --> 04:13.200
A puma.

04:13.200 --> 04:16.960
Yeah, it was basically a linear program solver thing.

04:16.960 --> 04:20.280
I forget what that thing was called.

04:20.280 --> 04:26.600
But that, I really enjoyed the whole dynamic and stochastic programming and linear programming

04:26.600 --> 04:27.600
and stuff like that.

04:27.600 --> 04:32.440
And every once in a while, I get to kind of geek out with folks that took operations

04:32.440 --> 04:35.120
research stuff.

04:35.120 --> 04:43.160
So you went into, you went to the dark side, the financial services path.

04:43.160 --> 04:47.720
But now you're back doing research and focusing on data and data science.

04:47.720 --> 04:52.880
What exactly do you do at Fast Forward Labs, Cloud or a Fast Forward Labs?

04:52.880 --> 04:54.400
Cloud or a Fast Forward Labs.

04:54.400 --> 05:00.120
So at Cloud or a Fast Forward Labs, our goal is to bridge the gap between research and

05:00.120 --> 05:02.120
industry applications.

05:02.120 --> 05:06.440
And one of the things that we do is publish research reports.

05:06.440 --> 05:10.480
And these reports are meant to highlight capabilities within machine learning that we

05:10.480 --> 05:14.800
think will become important for the business community within the next six months to two

05:14.800 --> 05:16.960
years from when they're published.

05:16.960 --> 05:24.160
So we spend a lot of time focusing on research, focusing on what capability we should actually

05:24.160 --> 05:25.560
write about.

05:25.560 --> 05:30.480
And we also, as part of that, consult with clients.

05:30.480 --> 05:35.000
We are essentially their best friends, best data nerd friends.

05:35.000 --> 05:41.000
So we try to help them implement this capability within the corporation, within the enterprise.

05:41.000 --> 05:43.760
And then they can always come to us with questions.

05:43.760 --> 05:46.440
And that's accomplished through advising hours.

05:46.440 --> 05:50.200
And I've long been a big fan of what you're doing.

05:50.200 --> 05:56.440
Hillary was one of my first interviews on the podcast.

05:56.440 --> 06:04.640
And last year's strata, I interviewed Justin Norman and we talked about the current report

06:04.640 --> 06:07.160
at that time, which was federated machine learning.

06:07.160 --> 06:12.120
I think in that interview, I referenced that one of my favorites was the thing that the

06:12.120 --> 06:13.600
report on summarization.

06:13.600 --> 06:15.680
I thought that was really cool.

06:15.680 --> 06:22.320
But the most recent report is one that you've worked on and that's on learning with limited

06:22.320 --> 06:24.480
label data.

06:24.480 --> 06:29.080
So what motivated that report?

06:29.080 --> 06:31.720
A couple of things.

06:31.720 --> 06:38.880
So we see a lot of enterprises sitting on a large amount of data, real-life data that

06:38.880 --> 06:42.680
they cannot actually leverage and turn into anything useful.

06:42.680 --> 06:48.640
And one of the reasons is that real-life data is very messy and unorganized and unlabeled.

06:48.640 --> 06:55.360
But supervised machine learning, for example, requires precise labels and a lot of data for

06:55.360 --> 06:57.520
you to be able to actually use it.

06:57.520 --> 07:03.680
And because of this, what companies end up doing currently is essentially creating labels

07:03.680 --> 07:07.800
manually for each one of their data.

07:07.800 --> 07:13.200
And that's essentially their attempt to be able to convert the data into anything useful.

07:13.200 --> 07:20.240
But that effort is very brute force, it's also not very effective, it's very expensive,

07:20.240 --> 07:22.280
and doesn't scale very well.

07:22.280 --> 07:24.720
So there has to be a better way to do this.

07:24.720 --> 07:31.400
And active learning is one way that allows you to learn with limited label data.

07:31.400 --> 07:37.520
With active learning, you can actually just smartly select a small set of label data,

07:37.520 --> 07:40.680
use that to build a machine learning model.

07:40.680 --> 07:45.360
And then that essentially opens up capabilities that you weren't able to do before.

07:45.360 --> 07:52.720
You can now build new products that your enterprise can take advantage of using already your

07:52.720 --> 07:54.920
existing data.

07:54.920 --> 08:00.320
So one of the things that's always interesting about kind of the approach you take with

08:00.320 --> 08:07.560
these reports is that it's very much, there's usually something that's happened in the industry

08:07.560 --> 08:12.880
that says, okay, now it's time to do the active learning report, which is sounds like

08:12.880 --> 08:18.440
this learning with limited label data is like the active learning report.

08:18.440 --> 08:25.880
What's the thing that's happening now that is, you know, is it new advancements and

08:25.880 --> 08:30.040
algorithms around active learning or something else?

08:30.040 --> 08:33.240
So the report is called learning with limited label data.

08:33.240 --> 08:36.880
And as part of that, we focus on different approaches.

08:36.880 --> 08:41.440
We looked at different types of approaches that would help you do that.

08:41.440 --> 08:45.520
Active learning was something that we ended up focusing on because it's most mature out

08:45.520 --> 08:47.880
of all these other capabilities.

08:47.880 --> 08:53.560
We also looked at, for example, weeks supervision, meta learning, that that was something that

08:53.560 --> 08:59.400
we mentioned in the report, but it's not something that we actually looked into very deeply.

08:59.400 --> 09:05.320
So the reason we decided to look to focus on active learning is because, first of all,

09:05.320 --> 09:13.400
it is one way that most enterprises can easily use and take advantage of it and essentially

09:13.400 --> 09:15.080
learn from the data.

09:15.080 --> 09:21.440
The second reason is that recently there's been a lot of algorithmic improvements in active

09:21.440 --> 09:25.760
learning that allows it to be used for deep learning.

09:25.760 --> 09:30.040
The classical active learning strategies don't work very well for deep learning, mostly

09:30.040 --> 09:37.400
because deep learning is very highly non-linear and also highly complex.

09:37.400 --> 09:43.640
So many of the existing strategies are hard to translate over and it's not something that

09:43.640 --> 09:47.960
you can just easily take and apply in the deep learning setting.

09:47.960 --> 09:53.240
The other reason is that deep learning trains and batches and an active learning.

09:53.240 --> 09:56.880
What we're doing is you start with a small set of label data.

09:56.880 --> 10:01.800
You build a machine learning model using that small set of label data.

10:01.800 --> 10:06.920
The machine learning model is then used to make predictions on your entire pool of unlabeled

10:06.920 --> 10:08.320
data.

10:08.320 --> 10:14.760
In the process of making prediction, the model also identifies points that are difficult.

10:14.760 --> 10:17.480
And this point is then sent to a human being.

10:17.480 --> 10:19.960
The human provides labels for it.

10:19.960 --> 10:25.640
This label is then added back to the original smaller label data set and then you iterate

10:25.640 --> 10:27.200
this process.

10:27.200 --> 10:32.840
So in active learning, you're adding small chunks of newly labeled data, but the small chunk

10:32.840 --> 10:38.200
of newly labeled data doesn't really make much impact in a deep learning setting.

10:38.200 --> 10:46.160
So recently, there's been just advancement that tackles both of those issues.

10:46.160 --> 10:49.240
Are there different types of active learning?

10:49.240 --> 10:55.960
How you describe generally how it works are there different types of algorithms?

10:55.960 --> 11:03.000
How broad is the field of approaches under the banner of active learning?

11:03.000 --> 11:08.680
Yeah, at the heart of active learning is a model that is able to identify difficult

11:08.680 --> 11:11.680
data points and request labels for it.

11:11.680 --> 11:14.120
So how does it actually do that?

11:14.120 --> 11:20.320
It turns out this model actually relies on strategies and this strategy selects the difficult

11:20.320 --> 11:21.880
data points.

11:21.880 --> 11:26.720
So there are classical strategies and there are strategies that there has been adapted

11:26.720 --> 11:28.160
for deep learning.

11:28.160 --> 11:33.960
So classical strategies, the easiest one is random sampling.

11:33.960 --> 11:37.040
In random sampling, you really not following a strategy.

11:37.040 --> 11:39.960
You're randomly picking data points to get labels for.

11:39.960 --> 11:43.240
So this is the simplest possible way.

11:43.240 --> 11:49.520
And something slightly more complicated, but something that actually works is uncertainty.

11:49.520 --> 11:56.200
So when we say models look for difficult data points, what does that mean?

11:56.200 --> 12:02.280
One way to think about that is the models will look for points that it's uncertain about.

12:02.280 --> 12:07.360
And then when we get there, then we need to actually quantify what uncertainty means.

12:07.360 --> 12:11.440
So there's a slew of strategies that try to quantify uncertainty.

12:11.440 --> 12:18.960
And one way is to look at the distance between a particular data point and its decision boundary.

12:18.960 --> 12:25.120
These data points that are far from the decision boundaries, you can interpret that as points

12:25.120 --> 12:27.440
that the model is very certain about.

12:27.440 --> 12:31.560
Any smog changes in the decision boundary doesn't really affect the classification of those

12:31.560 --> 12:32.560
points.

12:32.560 --> 12:38.400
But when the points are very close to the decision boundary, you can think of that as the

12:38.400 --> 12:43.080
model being uncertain about these points because any slight changes in the decision boundary

12:43.080 --> 12:48.200
due to model refinement will cause these points to be classified differently.

12:48.200 --> 12:53.320
So in margin sampling, for example, we are essentially looking for data points that are

12:53.320 --> 12:57.240
very close to the decision boundary and getting labels for those.

12:57.240 --> 13:03.440
And once we have those labels, they are then added back to our original label data pool

13:03.440 --> 13:06.640
and then used to build a better model.

13:06.640 --> 13:14.960
I guess when I think of active learning kind of non-technically, loosely, I think of you

13:14.960 --> 13:20.840
kind of run, like as you said, you change your model, you have your model make some predictions.

13:20.840 --> 13:28.920
And then when we're doing things like classification, we'll also often see a return value that's

13:28.920 --> 13:36.400
like the probability, a confidence or the probability with which your model thinks that it's

13:36.400 --> 13:38.720
the right classification.

13:38.720 --> 13:43.160
Is that probability the same as can we use that as part of active learning or?

13:43.160 --> 13:48.760
Yes, that's in classical active learning strategies that works.

13:48.760 --> 13:53.720
You can use the prediction probability as a proxy for how confident the model is, for

13:53.720 --> 13:56.080
example.

13:56.080 --> 14:07.600
But this way to measure uncertainty for deep neural networks doesn't really work because

14:07.600 --> 14:13.920
mostly because deep neural networks are very, very complex and there are a lot of parameters.

14:13.920 --> 14:20.720
So there are times when you could perturb the image using some noise and you will get

14:20.720 --> 14:23.640
an unexpected misclassification.

14:23.640 --> 14:29.040
So this is along the lines of the generative adversarial problem.

14:29.040 --> 14:35.720
You can perturb an image that goes into a deep neural network and have it misclassify

14:35.720 --> 14:37.120
unexpectedly.

14:37.120 --> 14:42.400
So for example, you would send an image of a panda with some noise into the deep neural

14:42.400 --> 14:48.480
network and it will classify as a given, but the prediction probably is actually really

14:48.480 --> 14:49.680
high.

14:49.680 --> 14:56.640
So although both images look like a panda to a human being, to the network, the one that's

14:56.640 --> 15:02.760
perturbed looks like a given and the deep neural network is very certain of that.

15:02.760 --> 15:06.920
So it will say, oh, this is not a panda, this is a given with 99%.

15:06.920 --> 15:13.720
So if we use that prediction probability as a gauge of uncertainty in deep neural networks,

15:13.720 --> 15:16.600
that doesn't really work.

15:16.600 --> 15:25.640
So there are many different ways to essentially look at how to judge uncertainty in deep

15:25.640 --> 15:30.640
neural networks and in our report, we look at three major ones.

15:30.640 --> 15:40.400
The first one is essentially to use the adversarial approach and to measure what you would like

15:40.400 --> 15:45.520
to do is to measure the distance between a data point and the decision boundary, similar

15:45.520 --> 15:50.880
to what we do in the classical approaches, but that's not tractable in deep neural networks

15:50.880 --> 15:55.320
because the decision boundary is highly non-linear and you don't really actually know what it

15:55.320 --> 15:57.520
looks like.

15:57.520 --> 16:01.920
So what ends up happening is we could estimate that distance.

16:01.920 --> 16:11.560
For example, we could use the distance between a data point and it's next closest, a different

16:11.560 --> 16:14.200
class object.

16:14.200 --> 16:19.920
But that one will only give you a very coarse estimate of the distance that you're looking

16:19.920 --> 16:20.920
for.

16:20.920 --> 16:25.160
A more creative approach is to use the adversarial approach.

16:25.160 --> 16:31.360
So what you try to do is perturb the data points and perturb it such that it's such

16:31.360 --> 16:34.240
classes unexpectedly.

16:34.240 --> 16:38.760
So when you get a misclassification unexpectedly, the particular data point has switched to a

16:38.760 --> 16:45.120
different class and you use the magnitude of the perturbation to estimate the distance

16:45.120 --> 16:48.720
between that data point and the decision boundary.

16:48.720 --> 16:52.760
So that is one approach that we mentioned in the report.

16:52.760 --> 16:57.200
We also mention a fundamentally different way to think about uncertainty for deep neural

16:57.200 --> 17:04.480
networks and that essentially gets into a Bayesian neural networks and how to estimate

17:04.480 --> 17:08.960
the posterior using dropouts.

17:08.960 --> 17:11.480
Can you elaborate on that a little bit more?

17:11.480 --> 17:14.280
You knew that question.

17:14.280 --> 17:20.040
So like I said before, if you just look at the prediction probability for deep neural

17:20.040 --> 17:25.360
networks and if you use that to estimate uncertainty, that's very misleading.

17:25.360 --> 17:29.640
So we need a different way to think about uncertainty.

17:29.640 --> 17:34.160
Now if you think about it from the Bayesian approach with Bayesian neural networks, what you can

17:34.160 --> 17:40.640
do is to actually get the weight distribution of the neural network given the trading data.

17:40.640 --> 17:46.920
Once you have the probability distribution of the weights, you can then write out mathematically

17:46.920 --> 17:52.400
really nicely the prediction probability of the deep neural network.

17:52.400 --> 18:00.800
But that mathematical expression is pretty but it's not easy to compute.

18:00.800 --> 18:05.720
What you can do then, let me take one step back.

18:05.720 --> 18:09.920
It's pretty and it's not easy to compute and in addition, you actually have to implement

18:09.920 --> 18:12.840
a Bayesian neural network in order to get that posterior.

18:12.840 --> 18:19.040
That posterior is the probably distribution of the weights.

18:19.040 --> 18:21.520
So we don't want to do that.

18:21.520 --> 18:23.560
How do you actually estimate the posterior?

18:23.560 --> 18:28.160
In terms of how you can use dropout as a way to estimate the posterior.

18:28.160 --> 18:34.480
And once you have the estimate for the posterior, the whole mathematical formula for the prediction

18:34.480 --> 18:39.440
probability simplifies down into something that we can easily compute.

18:39.440 --> 18:44.000
How does dropout give you the weight?

18:44.000 --> 18:50.600
So dropout, I think many of us are familiar with dropout as a regularization technique.

18:50.600 --> 18:57.200
When we dropout, you randomly set some weights connecting to the neurons to be zero with certain

18:57.200 --> 18:59.200
probability.

18:59.200 --> 19:03.560
And once that weight is set to zero, you have a smaller neural network.

19:03.560 --> 19:07.760
But you still train that neural network using the initial training data.

19:07.760 --> 19:13.720
So what ends up happening is you have a smaller neural network, you're forcing it to work harder.

19:13.720 --> 19:19.920
One way to think about that is also through the workforce analogy.

19:19.920 --> 19:28.240
If you have a workforce of 100 people, they perform their jobs on a daily basis.

19:28.240 --> 19:33.760
But now when the workforce becomes smaller, you are essentially forcing the workforce to

19:33.760 --> 19:35.640
do the same amount of job.

19:35.640 --> 19:39.520
So everyone can do different types of things.

19:39.520 --> 19:43.880
So that's essentially what regularization is trying to do.

19:43.880 --> 19:49.880
Now, when we use dropout in regular deep neural network training approaches, we train

19:49.880 --> 19:51.920
it with dropout turned on.

19:51.920 --> 19:57.840
But during inference, we turn off the dropout and then we adjust the weight accordingly.

19:57.840 --> 20:03.080
Now if you think about this, if you leave the dropout turned on during inference, what

20:03.080 --> 20:07.840
you're getting essentially as samples of the neural network.

20:07.840 --> 20:11.720
Every time you run inference, you get a different neural network.

20:11.720 --> 20:16.280
So if you run inference multiple times, you get multiple different neural networks along

20:16.280 --> 20:19.920
with multiple different sets of weights.

20:19.920 --> 20:26.280
And you also know with what probability or what likelihood each set of weight will happen.

20:26.280 --> 20:31.960
Why do you have different sets of weights when you're running an inference?

20:31.960 --> 20:36.600
I'm imagining that you go through some process of training your model, you create a model,

20:36.600 --> 20:41.000
it's a set of weights, you deploy that on somewhere and you're running inference against

20:41.000 --> 20:42.000
it.

20:42.000 --> 20:43.600
I don't think of those weights as changing.

20:43.600 --> 20:50.040
But if that's the normal way to use dropout, but if you leave it turned on during inference,

20:50.040 --> 20:55.840
what's going to end up happening is you're going to get some neural networks, some weights

20:55.840 --> 20:57.320
such as zero with probability.

20:57.320 --> 20:58.320
I miss that.

20:58.320 --> 21:00.320
We're leaving dropout turned on during inference.

21:00.320 --> 21:01.320
Yes.

21:01.320 --> 21:04.320
And when you do that, you're essentially sampling from the neural network.

21:04.320 --> 21:10.080
So you get different neural networks that looks different, that has different weight configurations.

21:10.080 --> 21:14.800
And if you do it enough times, what you're going to essentially have is all different sets

21:14.800 --> 21:18.600
of weights along with the probability that it will happen.

21:18.600 --> 21:24.920
So we're essentially trying to build the posterior, which is the probability distribution of weights

21:24.920 --> 21:28.600
over training, over all the training data.

21:28.600 --> 21:31.400
So that that is an estimate of the posterior.

21:31.400 --> 21:35.800
But when you do that, it makes all the computation easy.

21:35.800 --> 21:41.720
And then you can actually simplify the expression for prediction probability.

21:41.720 --> 21:45.880
We can rigorously demonstrate that this allows us to estimate the posterior.

21:45.880 --> 21:46.880
Yes.

21:46.880 --> 21:47.880
That's very cool.

21:47.880 --> 21:53.040
It's a very cool, this is actually a very cool paper, an outcome of a very cool paper,

21:53.040 --> 21:55.880
I think, from a year or two ago.

21:55.880 --> 21:58.160
Do you remember the name or author?

21:58.160 --> 21:59.160
So the paper?

21:59.160 --> 22:00.160
Yeah.

22:00.160 --> 22:09.320
L-G-A-L, first name, Yarin, Y-A-A-R-I-N, it was a pretty big deal at that time.

22:09.320 --> 22:18.400
And is there anything particular about the way we do drop out, meaning the drop out parameter,

22:18.400 --> 22:25.640
like is it constrained in any way or any particular pattern to the way we do drop out or doesn't

22:25.640 --> 22:26.640
matter?

22:26.640 --> 22:33.760
So you essentially drop out each layer of weight, and then you just get a sample of the

22:33.760 --> 22:36.800
neural network that way.

22:36.800 --> 22:43.880
So that is one way to think about uncertainty neural network, and that when you couple that

22:43.880 --> 22:49.960
into the active learning process, what you would do is to run drop out during inference,

22:49.960 --> 22:56.440
run multiple sets of inference, and then each set of inference will give you a prediction

22:56.440 --> 22:57.440
probability.

22:57.440 --> 23:03.160
And you take the average of that as your estimate of the uncertainty, and then that gets fed

23:03.160 --> 23:10.320
into other uncertainty methods like entropy, for example, to figure out which points should

23:10.320 --> 23:11.320
get labeled.

23:11.320 --> 23:12.320
Okay.

23:12.320 --> 23:18.800
So you're not taking that estimate directly, you have to then apply it to an entropy-based

23:18.800 --> 23:20.080
model or something like that.

23:20.080 --> 23:21.920
Can you explain how those work?

23:21.920 --> 23:29.280
Yeah, so entropy, the idea of entropy is that outcomes of uncertain events carry more

23:29.280 --> 23:34.280
information when compared to outcomes of events that we are very certain about.

23:34.280 --> 23:40.920
So if you think about coin toss, the maximum entropy for coin toss is one.

23:40.920 --> 23:42.840
So when does that happen?

23:42.840 --> 23:45.160
That happens when we have a fair coin.

23:45.160 --> 23:50.120
When you toss a fair coin, you don't know what you're going to get, right?

23:50.120 --> 23:55.080
The probability is equally likely of you getting a head or a tail.

23:55.080 --> 23:57.400
So that's when you get an entropy of one.

23:57.400 --> 24:04.280
Now when you think of a biased coin, and in the very extreme case, a double headed coin,

24:04.280 --> 24:09.600
that event, that coin toss event has no uncertainty because you know what you're going to get.

24:09.600 --> 24:15.480
So the entropy for that is zero, and the entropy for a double tail coin is also zero.

24:15.480 --> 24:21.560
So for using, when you use entropy and active learning as a way to quantify uncertainty,

24:21.560 --> 24:27.600
what you do is you compute entropy for all your unlabeled data points, and then you select

24:27.600 --> 24:32.160
the entropy, I'm sorry, you select a data points that has the highest entropy to get labels

24:32.160 --> 24:33.480
for.

24:33.480 --> 24:38.160
And the entropy computation depends on the prediction probability.

24:38.160 --> 24:42.240
So in the deep neural network setting, what you would do is you would estimate that you

24:42.240 --> 24:47.600
would get the prediction probability using dropouts, and you feed that probability into the

24:47.600 --> 24:52.240
entropy approach to get your data points to get labels for.

24:52.240 --> 24:55.240
We've got active learning, working for deep learning.

24:55.240 --> 25:04.080
We've got a set of methods that work, is this something that, you know, your typical kind

25:04.080 --> 25:09.720
of enterprise customer can just pull off the shelf and use or are they building it from

25:09.720 --> 25:10.720
scratch?

25:10.720 --> 25:11.720
How hard is it to do?

25:11.720 --> 25:13.720
You make it sound very easy.

25:13.720 --> 25:19.800
That'll just, you know, just do some dropouts, sprinkle some entropy over here.

25:19.800 --> 25:28.040
How hard or easy is it to actually do in practice, and how robust is it, is it, I mean,

25:28.040 --> 25:31.320
it's not actually changing the performance of your network, but the robustness would be

25:31.320 --> 25:36.400
in the degree to which it's, to which it's a sample efficient, right?

25:36.400 --> 25:44.480
Yeah, so let me talk about the, how you would do it first, and talk about the performance.

25:44.480 --> 25:51.520
So the active learning approach is a workflow approach.

25:51.520 --> 25:58.160
What I mean by that is you have to have a workflow that allows you to first take a small set

25:58.160 --> 26:04.560
of label data, build a model with it, use the model along with the active learning strategy

26:04.560 --> 26:09.520
to find the difficult data points, then get labels for those data points.

26:09.520 --> 26:14.440
The labels are provided by human, and then you have to be able to get those labels back,

26:14.440 --> 26:21.200
and then add them back to your original label data set, and then repeat.

26:21.200 --> 26:25.720
So there are a couple of blocks for this to work.

26:25.720 --> 26:29.880
Obviously, you have to be able to build a model, right?

26:29.880 --> 26:34.480
Building a machine learning model, I think many of our clients can do.

26:34.480 --> 26:39.120
You have to build a model, you also have to be able to implement the selection strategy,

26:39.120 --> 26:44.600
and these can just be, these are not very hard to implement.

26:44.600 --> 26:48.520
In real life, it's just a couple of lines of Python code, for example.

26:48.520 --> 26:52.480
You have to be able to compute entropy, you have to be able to, if you're using a deep

26:52.480 --> 26:59.880
neural network, you have to be able to use some neural deep network, appropriate techniques

26:59.880 --> 27:04.360
like dropout or ensemble approaches.

27:04.360 --> 27:10.080
Once you implement those, what you get at that point is a set of data points that need

27:10.080 --> 27:12.080
to be labeled.

27:12.080 --> 27:17.160
Now in the research setting, what usually happens is the person building the model will create

27:17.160 --> 27:19.800
the labels for that.

27:19.800 --> 27:27.320
But if you were to kind of productionize this, you need maybe more workforce to label

27:27.320 --> 27:32.880
these type of data, and the data that we're talking about can either be images or text,

27:32.880 --> 27:35.560
as humans can label those.

27:35.560 --> 27:41.360
So with active learning, we're kind of limited to use cases where the data comes in the form

27:41.360 --> 27:44.200
of text or images.

27:44.200 --> 27:53.120
If you're building an application for detecting, for medical diagnosis, then you actually need

27:53.120 --> 27:56.840
an intelligent workforce to label those images for you.

27:56.840 --> 28:03.600
If you're building applications for text, sometimes you can do it yourself, sometimes you

28:03.600 --> 28:10.040
also need to farm it out to somebody else that can do a larger scale.

28:10.040 --> 28:15.720
Once you have those, you need to be able to get those data point labels back and integrate

28:15.720 --> 28:22.600
it into your system easily and quickly that will allow you to do a second iteration.

28:22.600 --> 28:27.440
So there's kind of the building blocks of active learning, the key is to obviously make

28:27.440 --> 28:30.560
everything as streamlined as possible.

28:30.560 --> 28:34.880
There are a lot of platforms that will connect your data to the workforce that's providing

28:34.880 --> 28:37.320
the labels seamlessly.

28:37.320 --> 28:43.440
So that part is kind of taken care of, but you just have to be aware that this whole process

28:43.440 --> 28:47.640
should be as seamless as possible because you need to iterate.

28:47.640 --> 28:51.120
So that in practice, that's how active learning would work.

28:51.120 --> 28:55.560
Is there something that active learning is not that good at?

28:55.560 --> 28:57.400
Does that affect their performance?

28:57.400 --> 28:59.240
That was your second question, right?

28:59.240 --> 29:00.240
Essentially.

29:00.240 --> 29:01.240
Yeah.

29:01.240 --> 29:03.040
How robust is it?

29:03.040 --> 29:06.400
Is it reliable as a technique?

29:06.400 --> 29:07.400
Yeah.

29:07.400 --> 29:14.160
So there are a couple of things about active learning that we should always be aware of first.

29:14.160 --> 29:18.320
In active learning, there's a learner or a model.

29:18.320 --> 29:21.920
And then there's also the selection strategy.

29:21.920 --> 29:24.040
So the learner makes the predictions.

29:24.040 --> 29:28.520
The strategy steps in and picks the ones that the learner has the most difficulty with,

29:28.520 --> 29:32.280
and then it feeds it back, gets a label and feeds it back.

29:32.280 --> 29:38.400
So in general, in any machine learning problem, picking the right type of learner is difficult,

29:38.400 --> 29:42.880
but it's made even more difficult under the active learning setting for two reasons.

29:42.880 --> 29:49.000
First, you have to pick the learner when you only have a small set of label data.

29:49.000 --> 29:54.320
Second, the learner is not only used to make predictions, it is used in conjunction with

29:54.320 --> 29:56.880
the strategy to help refine it.

29:56.880 --> 30:00.840
So the tight feedback loop between the learner and the strategy amplifies the effect of a

30:00.840 --> 30:02.360
wrong learner.

30:02.360 --> 30:09.720
What generally happens is we advise clients to use maybe not only a single type of learner,

30:09.720 --> 30:16.280
use an ensemble of learners of different types, and that would essentially prevent your

30:16.280 --> 30:21.040
label data pool from being tied to any particular model in general.

30:21.040 --> 30:23.560
So that's one thing.

30:23.560 --> 30:27.400
A second thing is the impact of these strategies.

30:27.400 --> 30:35.800
Some strategies will result in a biased label data pool when you compare to other strategies.

30:35.800 --> 30:39.280
So we can use a very simple example in the margin sampling approach.

30:39.280 --> 30:43.520
What we're trying to do is to pick data points that's very close to decision boundaries

30:43.520 --> 30:46.560
to get labels for.

30:46.560 --> 30:50.600
The data points that are far from the decision boundary, sometimes are not even used

30:50.600 --> 30:52.680
to build the model.

30:52.680 --> 30:59.120
So essentially your model is built using a set of data that's maybe not very representative

30:59.120 --> 31:01.680
of your original data pool.

31:01.680 --> 31:08.560
It's not representative in that, by definition, it's data that the model has problems with.

31:08.560 --> 31:09.560
Yes.

31:09.560 --> 31:14.800
Yes, because we're only picking data points that the model finds hard and then we're kind

31:14.800 --> 31:18.440
of refining on that set of data points.

31:18.440 --> 31:21.800
And those are the data points that we end up getting labels for.

31:21.800 --> 31:26.320
So data points that the models we're clear about are very far from the decision boundary.

31:26.320 --> 31:30.720
We don't request labels for them and then we don't use them when we built the supervised

31:30.720 --> 31:32.760
machine learning model.

31:32.760 --> 31:39.480
Learning that is like a class of bias are there obvious downsides to over indexing on this

31:39.480 --> 31:41.240
particular class of bias?

31:41.240 --> 31:42.240
Yes.

31:42.240 --> 31:47.400
There is a recognition that active learning will result in some kind of bias.

31:47.400 --> 31:52.120
But there are certain types of selection strategies that will help with that.

31:52.120 --> 31:57.280
The example that I gave was on the margin sampling where we look for data points right around

31:57.280 --> 31:58.600
a boundary.

31:58.600 --> 32:05.080
But when you do an entropy approach, for example, you don't really get data sets as biased

32:05.080 --> 32:08.880
because the entropy approach doesn't just look at the distance.

32:08.880 --> 32:11.800
It is a more balanced approach.

32:11.800 --> 32:17.760
There are other approaches such as density-based approaches, for example, selection.

32:17.760 --> 32:25.080
For those approaches, you're only picking from regions where you have dense data.

32:25.080 --> 32:31.800
So essentially, you're trying to counter the bias issue because I just want data where

32:31.800 --> 32:38.360
I do want to pick data points from regions that I have a lot of other data points around

32:38.360 --> 32:39.360
me.

32:39.360 --> 32:45.760
I'm not focusing on weird little pockets of data.

32:45.760 --> 32:53.200
Our report covers the basic of active learning and obviously that is meant to be an introduction

32:53.200 --> 32:57.760
to active learning for our clients and during our advising hours, what we end up doing

32:57.760 --> 33:02.920
very often is going down different technical paths for different clients based on the problem

33:02.920 --> 33:03.920
and their data.

33:03.920 --> 33:08.800
One of the questions I get all the time is I have problem X. How much data do I need to

33:08.800 --> 33:10.800
collect?

33:10.800 --> 33:17.880
Does active learning help me answer that question?

33:17.880 --> 33:21.360
Unfortunately, it does not.

33:21.360 --> 33:23.640
What it does is...

33:23.640 --> 33:30.880
Does it even help me answer that question relative to not using active learning?

33:30.880 --> 33:31.880
Yes.

33:31.880 --> 33:33.880
I'll give you an example.

33:33.880 --> 33:40.720
For all the reports that we do, we also build prototypes that illustrates the capability.

33:40.720 --> 33:45.720
Our prototype for this particular report is a tool that helps you understand the process

33:45.720 --> 33:52.520
of active learning and how different selection strategies affect your approaches and how

33:52.520 --> 33:54.840
does it work on different data sets.

33:54.840 --> 33:58.240
So we looked at three data sets.

33:58.240 --> 34:01.320
We start with the very simple one, the MNIST data set.

34:01.320 --> 34:03.720
That is the Hello World data set of machine learning.

34:03.720 --> 34:08.640
It's a series of images of handwritten digits from 0 to 9.

34:08.640 --> 34:12.080
And then we looked at a slightly more complex data set.

34:12.080 --> 34:14.200
It is the quick draw data set.

34:14.200 --> 34:16.560
It's a series of hand doodles.

34:16.560 --> 34:19.480
We looked at 10 categories.

34:19.480 --> 34:22.400
The final data set that we looked at is the Caltech data set.

34:22.400 --> 34:29.120
It's a series of real life images, of bridges, of animals, for example.

34:29.120 --> 34:33.920
And we also picked 10 categories from those.

34:33.920 --> 34:40.880
And then we also looked at very different types of selection strategies starting from random

34:40.880 --> 34:45.680
strategy to the more complex ones like drop out an entropy.

34:45.680 --> 34:50.960
And we used active learning to build a model using all of these data sets.

34:50.960 --> 34:56.000
And so for the MNIST data set, if you're familiar with the MNIST data set, the training

34:56.000 --> 35:00.080
data actually has 60,000 points.

35:00.080 --> 35:03.280
But we only start with 5,000.

35:03.280 --> 35:08.600
We build a model using 5,000 data points and we look at the model performance and we

35:08.600 --> 35:15.160
use different selection strategies to select 1,000 points to get labels for.

35:15.160 --> 35:19.200
And then that gets added back to the 5,000.

35:19.200 --> 35:22.480
And that starts the next iteration of active learning.

35:22.480 --> 35:33.440
So with 5,000 data points, we're able to get accuracy somewhere in the, I think, 92%.

35:33.440 --> 35:38.360
And as we increment, I think we did maybe seven or eight rounds of active learning.

35:38.360 --> 35:43.200
And we're able to get to 98% tile of 98% accuracy.

35:43.200 --> 35:46.400
So that's an indirect way to answer your question.

35:46.400 --> 35:52.080
What I'm trying to say is you don't need to use 60,000 data points to build your model.

35:52.080 --> 35:59.280
You can, you only need, we ended up with is 5,000 plus maybe 12.

35:59.280 --> 36:05.720
I think what, what it say, what it is answering is that active learning isn't going, you

36:05.720 --> 36:11.440
know, producing any results that says, you know, either given my problem or independent

36:11.440 --> 36:18.160
of my problem, you know, if I do X kind of active learning, I'm going to require 75% less

36:18.160 --> 36:19.440
data or something like that.

36:19.440 --> 36:20.440
It's all experimental.

36:20.440 --> 36:21.440
It's experimental.

36:21.440 --> 36:23.440
Yes, it's experimental.

36:23.440 --> 36:28.880
But what it's saying is that we know we need a lot of data.

36:28.880 --> 36:30.720
So we random, we don't random.

36:30.720 --> 36:34.480
We just kind of brute force create labels for everything.

36:34.480 --> 36:39.680
It turns out a lot of these labels are not useful to the model, right?

36:39.680 --> 36:45.480
So we kind of, if you think about these labeling projects for autonomous vehicles, they range

36:45.480 --> 36:50.000
and they start at a million dollars and go up from there.

36:50.000 --> 36:54.240
You just label everything and then you hope that it works, right?

36:54.240 --> 36:58.480
But when you use active learning in that kind of setting, you realize that, hey, I don't

36:58.480 --> 37:03.120
have to label so many images, I only need to label the images that will be helpful for

37:03.120 --> 37:04.120
my model.

37:04.120 --> 37:10.240
So that obviously brings down the cost and sometimes it allows us to get to a model faster.

37:10.240 --> 37:14.080
And more importantly, the model performs almost just as well, at least in the research setting

37:14.080 --> 37:18.920
when you compare to a model that you would have built using a much larger data set.

37:18.920 --> 37:21.720
How was that in November, December?

37:21.720 --> 37:22.720
I forget.

37:22.720 --> 37:30.400
AWS re-invent, they announced a new extension to the SageMaker platform, SageMaker ground

37:30.400 --> 37:36.600
truth, which incorporates active learning to some extent.

37:36.600 --> 37:41.120
There are some other tools out there.

37:41.120 --> 37:49.080
The folks behind Spacey, the NLP library have a, what is it, prodigy, prodigy, right?

37:49.080 --> 37:53.200
Have a library called prodigy that incorporates active learning.

37:53.200 --> 37:58.160
Have you looked at kind of these off-to-self solutions and what have you seen there?

37:58.160 --> 38:02.480
Yeah, so all of our reports come with a landscape chapter.

38:02.480 --> 38:08.920
We look at open-source, we look at vendors that help with this particular capability, and

38:08.920 --> 38:12.720
this is usually very helpful for clients who are thinking of implementing any of these

38:12.720 --> 38:14.200
capabilities.

38:14.200 --> 38:23.560
So as part of that, we looked at prodigy, we also looked at a lot of labeling providers.

38:23.560 --> 38:29.840
And the main difference between prodigy and AWS SageMaker is that prodigy, at least

38:29.840 --> 38:36.320
from when we're playing with it, is more of a complete solution for active learning.

38:36.320 --> 38:42.640
And I say that because with prodigy, you can actually modify any of your selection strategies.

38:42.640 --> 38:46.880
I think it comes with uncertainty sampling using entropy out of the box.

38:46.880 --> 38:53.600
But you can essentially link to any selection strategies that you write yourself.

38:53.600 --> 38:57.480
You just have to link it in to the project framework.

38:57.480 --> 39:00.560
AWS doesn't have that flexibility.

39:00.560 --> 39:04.240
It's more of kind of a black box type of solution.

39:04.240 --> 39:05.240
Okay.

39:05.240 --> 39:07.000
So that's the main difference.

39:07.000 --> 39:12.080
If I wanted to play with this, what's the quickest way to get started doing it?

39:12.080 --> 39:14.520
You said it's only one line of Python, right?

39:14.520 --> 39:18.480
There's more than one line.

39:18.480 --> 39:24.240
If you're trying to get an intuition for active learning, I would go to our prototype.

39:24.240 --> 39:25.760
It's public.

39:25.760 --> 39:28.760
It's active learner dot fast for labs.com.

39:28.760 --> 39:29.760
Okay.

39:29.760 --> 39:31.520
And you can kind of play with that.

39:31.520 --> 39:36.200
It's a pretty cool visualization of the process of active learning.

39:36.200 --> 39:42.080
If you're trying to do it yourself, you can definitely try prodigy.

39:42.080 --> 39:47.600
If not, you can just start right building your own model.

39:47.600 --> 39:51.640
Once you have the model, all you need to do is to extract out the prediction probability

39:51.640 --> 39:53.840
and feed it into a selection strategy.

39:53.840 --> 39:59.240
You can use start with, we'll always say you should start with random as a baseline.

39:59.240 --> 40:05.960
And then move up to more difficult strategies and see how they impact your data because

40:05.960 --> 40:08.320
everyone has a different data set, right?

40:08.320 --> 40:10.520
It all behaves differently.

40:10.520 --> 40:17.400
But because active learning tend to introduce bias or can introduce bias, it's helpful to

40:17.400 --> 40:22.400
always start at the base, which is the random sampling and then go up to maybe a more difficult

40:22.400 --> 40:29.840
one, which is the least, which is the margin sampling and then go into the entropy approach.

40:29.840 --> 40:35.600
And kind of just look at the data that's surfaced, the data points that surfaced that's asking

40:35.600 --> 40:37.280
labels for.

40:37.280 --> 40:42.360
You can create labels for those yourself and then you can feed it back to your model and

40:42.360 --> 40:44.480
try iterating it that way.

40:44.480 --> 40:51.320
Having kind of explored this area and learned a bunch about active learning in the landscape,

40:51.320 --> 40:56.240
you kind of see a world where active learning becomes kind of a standard, it's a pretty

40:56.240 --> 40:59.520
exotic thing now, right?

40:59.520 --> 41:04.120
Not everyone is doing it every time because it's just a standard tool that we apply.

41:04.120 --> 41:09.920
Do you think that over time it becomes a standard tool that is applied in most situations

41:09.920 --> 41:18.040
or does the bias or other downsides exist such that you want to be more selective about

41:18.040 --> 41:20.600
when you use it?

41:20.600 --> 41:27.680
I think it should be a tool that you use every time when you're starting to build a model.

41:27.680 --> 41:31.720
When you're trying to get a sense of what works with your data and what doesn't, it's

41:31.720 --> 41:36.800
definitely something that you should use also because you might be labeled constraint

41:36.800 --> 41:38.800
at that point.

41:38.800 --> 41:41.040
So it's a good starting point.

41:41.040 --> 41:46.760
Whether or not it will work as a workflow for all projects, it's hard to say.

41:46.760 --> 41:48.840
But the idea is very simple.

41:48.840 --> 41:53.880
What it's trying to do is reduce this amount of labeling that needs to be done, but at

41:53.880 --> 41:56.840
the same time doesn't affect model performance.

41:56.840 --> 42:01.080
So if you just think of that, I would say that...

42:01.080 --> 42:10.240
It should be something that's attempted at least in the beginning, and that will also

42:10.240 --> 42:15.920
help you discover something about your data that you might not have realized before.

42:15.920 --> 42:25.440
It also will help you verify whether a modeling approach will work or not before you label

42:25.440 --> 42:27.120
everything.

42:27.120 --> 42:30.960
But if active learning works, you actually don't have to label everything.

42:30.960 --> 42:36.520
You just continue with the process and you just end up labeling a selection of unlabeled

42:36.520 --> 42:39.960
data that is the most helpful for the model.

42:39.960 --> 42:43.840
And there are many use cases within enterprises.

42:43.840 --> 42:49.560
I mentioned for autonomous vehicles, it's super helpful, which is because the labeling

42:49.560 --> 42:53.960
causes so high and everyone just indiscriminately labeling.

42:53.960 --> 43:01.440
If you think about text-based examples, companies get, for example, many inquiries from the

43:01.440 --> 43:02.440
customers, right?

43:02.440 --> 43:06.280
And these inquiries need to be routed to the right departments.

43:06.280 --> 43:11.120
But right now, the routing portion is always the bottleneck.

43:11.120 --> 43:17.760
We can build a machine learning model to do the routing, but we need labels.

43:17.760 --> 43:21.200
Now when you use active learning, it's not as daunting because you don't have to label

43:21.200 --> 43:25.000
everything in the beginning, you just start with a small set of label data and then you

43:25.000 --> 43:28.680
iterate on that and then you figure out which ones you have to label.

43:28.680 --> 43:34.840
And that will help you relatively quickly come to a model and then you can, if you want

43:34.840 --> 43:40.640
to productionize it, you might then have to label at a larger scale.

43:40.640 --> 43:41.640
Awesome.

43:41.640 --> 43:44.400
Well, it sounds like an exciting project.

43:44.400 --> 43:48.040
Sheldon, thanks so much for taking the time to share it with us.

43:48.040 --> 43:49.040
You're very welcome.

43:49.040 --> 43:50.040
Awesome.

43:50.040 --> 43:51.040
Thank you.

43:51.040 --> 43:57.680
All right, everyone, that's our show for today.

43:57.680 --> 44:01.760
If you like what you've heard here, please do us a favor and tell your friends about

44:01.760 --> 44:03.080
the show.

44:03.080 --> 44:07.800
And if you haven't already hit the subscribe button yourself, make sure to do so so you

44:07.800 --> 44:11.520
won't miss any of the great episodes we've got in store for you.

44:11.520 --> 44:16.520
For more information on any of the shows in our strata data series, visit twomolei.com

44:16.520 --> 44:19.000
slash strata sf19.

44:19.000 --> 44:22.360
Thanks once again to cloud error for sponsoring this series.

44:22.360 --> 44:26.560
Be sure to check them out at cloud error.com slash ml.

44:26.560 --> 44:56.520
As always, thanks so much for listening and catch you next time.

