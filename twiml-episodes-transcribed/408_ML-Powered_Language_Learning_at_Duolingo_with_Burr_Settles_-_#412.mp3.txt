All right, everyone. I've got Burr settles here. Burr is research director at Duolingo. Burr,
welcome to the Twomo AI podcast. Thanks for having me, Sam. I am super excited to have an
opportunity to chat with you. I am a bit of an avowed lingua file myself and a user of the Duolingo
app, but it has been a while. I don't know what my points are, my XP's are at this point.
But super excited to chat with you about some of the ways that Duolingo uses AI to deliver the
app. Before we get into that, I'd love to hear a little bit about your personal journey,
how you came to work on machine learning and AI and in language education in particular.
Yeah, it's not a straightforward path, as one might imagine. So my introduction, well, I've always
loved languages. My aunt was bilingual in English and French and worked as a translator. So she
started me on French when I was pretty young. I've always enjoyed languages, never really thought
to study it and certainly knew nothing about natural language processing when I was growing up.
I went to a small liberal arts school where I double majored planned to double major in art,
studio art and math. But then quickly kind of fell into computer science as kind of a mathematical
art form. And then ended up going to grad school, thinking I was going to go into distributed
computing because I thought that was cool at the time. Again, first year, took a machine learning
course, fell in love with that. And then language still wasn't in the natural language processing
wasn't really in the scope just yet. I finished my master's degree at the University of Wisconsin,
and intended to kind of graduate and go get a job at that point. But then I met a girl,
and she still had a year left in her program. And so I stuck around and started just taking
linguistics and biology classes and then fell into a, I started doing research with a professor
in the biostatistics department using machine learning to do information extraction on biomedical
texts. And so for several years, I worked on that. So biomedical natural language processing
was a pretty big thing in the mid-2000s. And during that process, the kinds of annotations,
you need to train biomedical, you know, data sets for those models. It's extremely expensive,
because you have to find people with PhD level knowledge of molecular biology and enough
kind of linguistic sense to know like what semantic role labeling might look like in this domain.
It was very slow and expensive, which then tipped my interest into active learning,
which is machine learning algorithms that participate in their own training. So they can ask questions.
The typical way this is done is you've got a bunch of unlabeled data as we did it in this case.
You have an oracle in this case, a human expert who's very expensive. So you have them annotate
a little bit of data, and then the model learns from that, and then it can quickly analyze all
of the unlabeled data. And then say, this I'm pretty squared away on this and this. I understand
these things, but here's some things that confuse me. Can you please label these for me? And then that
way you can kind of steepen the learning curve or flatten the learning curve depending on how
you're thinking about it. And so I switched my I did my PhD and that kind of work.
And in general was just kind of interested, like that was where I did kind of the language
and the natural language processing. And this interaction between humans and machines.
This this interplay using figuring out how to best use a human resource and training machine
learning systems was something that interested me a lot. Then I came to Carnegie Mellon as a post
stock for a few years to work on this read the read the web project. And then the opportunity
dolingo was spinning out of Carnegie Mellon as a company about the time I post stock wrapped up,
which is almost eight years ago now. And I suddenly realized I was interested in flipping the
flipping the script on that learning. And instead of figuring out how to best use people to
teach machines. How can we best use machines to teach people. And the fact that it was kind of
this intersection of AI computer science building apps. I've never been a pure academic. Like I've
always wanted to build things that like create technology that you then built something with.
And then language and cognitive science. It was just like the perfect perfect combination.
So I've been here ever since. That's awesome. That's great. Are you also a language hobbyist and
user of the app? Where did you kind of settle in on French and kind of push deep in that direction?
I've I've dabbled in many languages. I've used the app to learn a bit of German and Spanish and
Portuguese. But French is the only one I could probably have any sort of conversation.
Yeah, I'm probably similar. Although my French is a lot rustier than I like it to be.
I picked that up in in college. I picked up Spanish was like my high school, you know, high
high school language. And it tends to be generally rusty. But it kind of comes out when it,
you know, needs to. But I've also I'm a dabbler as well. And my typical pattern is to,
you know, spend anything from a month to three months before a trip. Kind of, you know, cramming
a language essentially. And then, you know, go use that knowledge to enjoy the trip and then kind
of flush the cash immediately afterwards because it's really hard to keep that, you know, to get
it to stick. But I've learned a little bit of Portuguese. Do Brasil from Duolingo, a little bit of
Mandarin. What else on Duolingo? A little bit of Spanish, I think, with Duolingo. I think I'm
missing missing one. I've also tried to teach myself a bit of umhark and a bit of Arabic quite a
while ago. But it's it's always great to kind of chat with someone else to share that interest.
Yeah, well, it's pretty amazing to work at a company where everybody shares that. I mean,
there, I think there's only imagine. I think there are 30 different languages spoken
among the employees here. So and most of them all not most. I think there's about 30 different
languages that we teach too. Not the same overlap, but there are enough people who speak a variety
of languages that within the company we have these language tables, like a German stomatish
or top of the Portuguese and you know, people get together. Well, back when it was possible and kosher
to like physically together. Right. Actually, that's extended. I think the Portuguese table.
There's a line. Yeah, just everybody orders take out from like this Portuguese restaurant
and then like thousand to a zoom room. Oh, very cool. But that's awesome. And so I could geek out on
just the language side of this, but I'm not sure that many of the, you know, I have no idea how
many of the listeners would be interested in that conversation. So maybe let's shift gears a
little bit to talking about some of the ways that Duolingo uses AI. And it may not be a surprise
to folks that know the some of the history of the company and like spinning out of Carnegie Mellon,
but there's a bit of machine learning focus from the very beginning to the way the company
approached, you know, this problem of language learning as well as the broader issue of building
a business around it. Yeah, I mean, I was one of the first folks. And let's see, I think I was
number 20 or so. I joined about about six months after it spun out. Okay. And so I started working
on machine learning things from day one. And I think a lot of people, this is changing, but like
three or four years ago, if I'd go to an LP or machine learning conference, people would be kind
of like, how does, you know, Duolingo, how would that use any machine learning? And it's sort of
like surprised people. It doesn't seem to be a surprise anymore, but the way we think about it is
particularly for language education, but I think any kind of education. The best kind of education
you can get is a one-on-one tutor. Okay. And but the problem with that is very few people have
access to a good one-on-one tutor subject matter expert for whatever they're trying to learn.
And so we believe the best way that you can scale that kind of experience is with AI. So it's not
necessarily to replace good teachers or good tutors, but most people don't have access to them
anyway. So by using AI, we can sort of democratize education, sort of. And then if you take that
sort of approach and you think, okay, well, what do good tutors do? I claim that they've got
three, at least three properties, important properties. One is that they know the content
really well. In this case, the language that they're teaching and how different aspects of the
language align to different kind of proficiency levels. Two is they know how to keep you engaged
and excited with the material. And then three, and maybe most importantly, they have a way of
getting inside your head. So because of this one-on-one time that they have with you, they see what you
get right. They see what you get wrong. They see what you used to be getting wrong and you're
starting to get right. They get it for how quickly you forget things. I mean, a lot of educational
technology is focused on assessment or like a shorter term semester long sort of learning,
not lifelong learning. So forgetting usually is not baked into the models. And so anyway,
going back to those three things, the content, keeping you engaged and getting inside your head,
we've essentially arranged our AI kind of research program here around those three things. So we've
got machine learning projects going on in those three areas, broadly speaking. So we've got
projects to help develop efficiently like high quality content that's aligned to a proficiency
scale and then to keep people engaged and then also to model what people know so that we can
personalize their experiences. And the story that I was thinking of and I wondered the
extent to which it continues. It was from years ago, it was kind of this idea that
Duolingo had this, you know, the company has a whole wanted to make an impact on language learning
and the recognition was that, you know, yeah, there are a bunch of, you know, folks in wealthy
countries that could like spend a bunch of time, you know, hobby learning languages, but the
biggest impact on them from a global perspective was helping people at massive scale learn languages
that would increase their economic position, you know, languages like English. And there was,
I forget the specific technique or approach that was taken, but there were some things that were
that were being done early on using machine learning to like ingest articles and identify,
you know, turn those articles into into lessons. Again, kind of at a scale beyond the way
a handcrafted, you know, Portuguese language track might work for the Duolingo that many of us
hear. No, you know, does that, does anybody of that sound familiar? Am I making that up?
You're not making that up. We've kind of pivoted since then. So the original business model
for Duolingo, the idea was that we can give away a free education. So the app is free and it's
still free. Yeah. You can go through an entire course without ever, you know, paying us anything.
And the idea at the time was that as part of doing the lessons, some of the exercises that you did
were translating documents. And a lot of it was Wikipedia articles. For a while, we actually did
have clients like, I think CNN was one of our clients BuzzFeed would give us articles written
in English that they wanted translated into their Latin American properties. And so we had,
you know, tens or hundreds of thousands of Spanish speakers who were learning English on Duolingo
that as part of their exercises were translating documents kind of crowdsourced to translating
documents from English into Spanish reports of ease, which then we sold back to CNN and BuzzFeed
and then they published on their website. That was like the original, you know, business model
idea. It ended up not really to be sustainable. So we've kind of like shut that down and pivoted
it, pivoted a bit. The primary business model now is twofold. One is in, I'm trying to remember
exactly when 2014, 2015 or so, we launched the Duolingo English test, which is a high stakes
English language proficiency exam. And the sort of test that if you're an international
student wanting to study in the United States, you could take as proof of English proficiency.
But the idea again, keeping with our mission of being as accessible as possible, it was a test
that was much less expensive. It was, it's $49 and you can take it online anytime anywhere
and it's a computer adaptive test. And so the idea there is that the education is free and then
to certify what you've learned, that is a, a small fee. So that's part of the, the business model.
And then another part of the business model is a subscription service that is not a paywall
on the educational content. But it does unlock certain gamified, you know, features.
Not things that, you know, make it easier. I actually am not familiar with all the sets of features
in the offering right now, like there's a progress quiz that you can take to sort of like track
your own learning over time. I remember the big thing, I remember the big thing for me being,
I think if you are a pro subscriber, you can download your lessons.
Right, offline. We're not on a plane and stuff like that.
Or the subway. Yeah, we're on a subway. Maybe not so big a deal now, you know, with people
quarantined at home. But when I was actively using the app, that was the one thing that
I think maybe even made me do pro for a little while.
Yeah, in fact, lockdown has been really great for us. And a lot of, you know, most of
kind of like tech app most, all the metrics that we look at have at least doubled with the
Duolingo English test. That is, that's gone up, I think 2000 per cent or something like that. Wow.
All the test centers for all, for the traditional tests are closed. So basically, in many parts of
the world, the Duolingo English test was the only option for English language proficiency testing
for people who were hopeful to study in the United States and Canada. Well, so let's push a
little bit deeper into those three areas. I think the first she mentioned was the content side.
Yeah, so a few examples of things that we're doing there. I mentioned earlier proficiency
standards. So there's something called the Common European Framework of Reference. This is a
descriptive framework of the kinds of things you can do at various levels of proficiency.
So it split up into these six levels. There's an A, B and C and then a high and low for each one
of those. So A1 is super, super beginner, survival English or survival Spanish.
And then as you move up, A2, B1, B2 is kind of the threshold for being like you could study
probably in a or work, get a job in a company or university that, where that language is the
medium. You'll probably still have an accent to make your medical errors and stuff, but you can
converse abstractly in that language. And then the C levels are like
much closer to fluent. And so we needed tools to help align our curricula to a standard like that.
So we picked the CEFR for whatever reason. It was as good as any. And there have been lots of
work building vocabulary profiles. This is an A1 level piece of vocabulary like brother and sister
and Monday and Tuesday. And those are kind of A1 level words, whereas contributory,
well actually it's not a word. Or is it? Cropuscular. That's like a C2 level word.
So lots of work had gone into creating these profiles, but only for English.
And we have our in-house language and curriculum developers had put a lot of work into aligning
our English curricula to the CEFR. But we wanted to then duplicate that for French and Spanish.
And the data resources just didn't exist. So we could train machine learning models to project
arbitrary English words onto this onto a CEFR scale, just treating it like an ordinal regression
problem. But we needed to be able to do that for other languages too. And so we had this idea
of using multilingual word embeddings and language normalized frequencies and things like that,
as features in a model that we can essentially train on English data and then make predictions
on Spanish data or French data or Portuguese data or German data. And so we created that. And
there's a tool. We lovingly called it CEPFR, some people pronounce it as CEPFR.
We actually released this publicly. So if you go to CEFR.duelingo.com, you can explore the English
and Spanish versions of that. Internally, we also support I think half a dozen or more languages.
And our curriculum developers use that. We also have features called Duelingo Stories. We
have Duelingo podcasts in French and Spanish. And we have some other kind of top secret things
that we're experimenting with. And the teams that develop content for that are using those tools
to help make sure that, hey, this is stuff we're targeting toward A1 beginner learners.
This is stuff we're targeting toward B1 intermediate learners. And we use these tools to kind of
check the content. And then also highlight the things that are maybe too advanced or too simple.
And those can be nudged in either direction accordingly.
Interesting. So how do you go about training a multilingual word embedding? Are you doing
that based on kind of bi-directional word pairs in a bunch of languages? Or is it
another approach? Yeah, when we were doing this work, we actually didn't train the embeddings
ourselves. I forgot which embeddings we were using at the time. This was about two or three
years ago. We were first building the CEFR tool. But we used some off-the-shelf embeddings. We used
some frequencies from I think Wikipedia and movie subtitles. And that provided enough information
to get pretty accurate scaleings and projections in English data. And then when we qualitatively
inspected the projections in the predictions and the other languages, the content matter experts
found them useful. And was the idea that you would develop a word list for given
course or level of a course and then feed it into the CEFR tool. And it would give you an
approach. It would try to predict the language level of the person who would be taking that
who would successfully complete that level. Right. So a lot of our courses when we were first
developing them, and when I say first developing them, I'm talking about like seven years ago.
So the Spanish for English course was built by our co-founder and CEO Luis Fanon, who's from
Guatemala and bilingual in English and Spanish, as well as some other Spanish-speaking
engineers that we had very early on. Before the linguists and the people with classroom
experience, for example, in Spanish came in. And before I even started, and I was the one who
kind of introduced the CEFR to the company. So for years, we kind of had this like things were
organized in case your listeners haven't actually used a link of lessons are organized into
these skills, which can be their thematic or, you know, they're, you know, like at the restaurant.
And so you learn a few words about ordering food at the restaurant or animals or family or
or in some cases, you know, they're more grammatical in nature. So you'll learn about the past tense
in this skill. And so we had these arbitrary skills that were just like, here are 25 animals.
And no matter, even though you're a beginner, and you don't really need to know how to say
Pangolin in, you know, we're going to teach it to you anyway just because it was just like this
vocabulary core dump. So in the process of going through and revising the courses over time to
make them aligned to the CEFR, tools like this are the kinds of things that we use to both inspire
and sort of quality control check the content as it's being developed. Yeah.
And are there other ways that ML is used in the content development and programming scheme I'm
thinking of trying to think about, you know, either experiences with the app or issues with the app
or I think one kind of recurring theme is or at least something that would be important
for an app like this is kind of the relevancy of the terms and the constructs that you're
learning. And you kind of alluded to this with the Pangolin
content. You want the sentence that the app is teaching you to be representative of the kind of
language that you see out in the wild. Like I imagine that one could create a tool that would
sanity check a curricula like Duolingo's for, you know, relevancy. I remember a lot of folks
might not know this, but you've got there are very active Duolingo forums and entire communities
around like beta versions of languages and stuff like that that most people don't see in the app.
And you'll see folks talking about how, you know, either complaining about the usage in a
a particular question or phrase or, you know, complaining about the relevancy of, you know,
given phrases and it seems like that's something you can develop a tool that would, you know,
compare against, you know, popular media and determine the relevancy.
Yeah, that's something that we have talked about. We actually haven't started a project using
machine learning to do that. But a very similar adjacent thing is in the questions that we give you
particularly the translation exercises and then also sometimes the transcription when you listen and
try to transcribe what you heard. But anytime you're entering in something in the language,
you might, like you may submit something that you're pretty sure you got right, but you're
graded as wrong. And it's also, it's very difficult to capture all possible, you know, language is
very flexible and expressive. So if you're given a prompt in English that can be translated into
Spanish, you know, there's hundreds of valid, you know, Spanish translations of the sentence.
So we write things essentially like regular expressions to sort of capture all the possible
different ways that you can say it. But, you know, the content matter experts who develop this
might miss some or might forget some of there's one particular synonym or, you know,
idiomatic turn of phrase that they skip. So when you grade it wrong, there's a little report
this button. And if you hit that button, then it goes into a big queue. And we get about,
I think, a half a million of those a week. And so it's impossible for the content developers.
And for some of the smaller courses, like Klingon and, you know, Irish Gaelic, there are, you know,
volunteers who actually contributors who maintain. Yeah, we don't have Dothraki. We do have
Hyvalurian. We do have Hyvalurian, which was actually created by David Peterson Patterson,
the linguist who created those languages for Game of Thrones. He approached us and said,
I love the Olingo. I love the incubator, which is the platform where we crowdsource these languages.
I want to create a Hyvalurian course. So we were like, yeah, have at it.
And there's also like, is there Elvish and a bunch of the languages from Lord of the Rings
are? I don't think we have any Lord of the Rings languages. But people, you know,
criticize us all the time for, you know, you're not teaching. For a long time, we didn't have an
Arabic course. We launched that last summer. So people were kind of like, what is your, you know,
you're teaching Klingon, but you're not teaching Arabic. And part of it is the fact that,
you know, a rabid community approached us and volunteered to create the Klingon course, right? So
um, um, but getting back to the, the reports, uh, when those come in, we have a machine learning
model that will probably help prioritize, uh, which are the ones that are likely to be correct
translations, uh, and this uses a combination of kind of like linguistic distance from the things
we do already accept and what this particular submission was, as well as, you know, who was it
that submitted it? Do they have a track record of submitting, you know, good things? And then, um,
uh, and then also like historical data about what did and didn't get approved. And then,
so we have this tool and it's been extremely important. For a lot of the mature courses,
it's, it's less important, but when a new course, like Arabic, for example, when we first launched it,
um, before Latin and Arabic and Scottish Gaelic, which were the first three courses to launch
that we created after we created this tool, even though we didn't have any training data in those
languages yet, it generalized well to those languages. Previous to that, it took about six months
for a course, a new course to graduate out of beta, which among other things, like one of the key
things there is that the number of reports that comes in is below a certain threshold.
Yeah, the number of reports per session or something. And after, you know, this tool was available
after those courses launched and they graduated from beta in like five weeks as of the last
months. Yeah. Oh, very interesting. Um, and so that's the, the content side. I'm also wondering if
there are applications of, you know, language models, GPT-3 types of models on the, the content
side as well. Is that something you're looking at? Uh, potentially. We're, we're looking at those
sorts of models not so much for content creation, but for some, uh, some features you might be seeing
over the next six months or so that are more, uh, kind of interactive feedback on things.
Uh, so currently a lot of the dualingo experiences, translating or transcribing these kind of
rope things and you're not necessarily producing language, uh, spontaneously. Uh, and so we're working
on some, uh, features that would allow you to do that and, and some language models that can give
you feedback. Interesting. Um, this is maybe the last, I'm trying to remember the buckets was
assessment separate from content? Well, assessment is sort of related to the, what I said at the
beginning, I think was content, uh, you know, keeping you engaged with the material and then getting
inside your head. Yeah. And assessment is related to that last one. So if you think about
a good interactive tutor, they're constantly assessing you as they're teaching you. Yeah. Uh,
and so a good interactive, uh, personalized adaptive language system or learning, uh,
educational system of any platform, which is happening to be language, uh, would be assessing
you as well. Um, so we have, and it's actually very related to active learning, which is the,
what I did my PhD in that we talked about earlier. If you think about, um, uh, let's,
uh, like an active learning system, uh, it can look at a bunch of unlabeled data and say,
this I understand, this I understand, this confuses me, please label this. Uh,
a core layer to that is if, if I'm a machine learning system that is trying to teach you something,
I have a, a mental model of what you know, I think you understand this. I don't think you're ready
for this yet. This is in the zone, uh, a proximal development is, is what it's sometimes called.
Uh, so I'm going to give you, uh, material in this area. So it's right on the margin, you know,
this is something that confuses me about what you know and don't. So it probably confuses you in
terms of what you know and you don't. Um, and so we use those kinds of models both for personalized
learning and for computer adaptive testing in the dual and go of most tests.
And one of the things that has really changed language learning over the past, I don't know,
10 years, let's say, for just the number out there is I think the, um, kind of the mainstreaming
of this whole idea of space repetition and kind of systems, a ton of systems that are based on
space repetition. Basically, uh, some system trying to, you know, learn what you know and focus
your learning effort on the things that you don't know as opposed to, uh, you know, continuing
and reinforce the things that you've probably already committed to memory. Um, and this idea that,
you're describing a active learning kind of takes space repetition, you know, once that
further, because it's building this mental model of what you know and what you probably need to
be refreshed of or on, um, versus not as opposed to, uh, I think the original spaced
repetition models were based on this. The idea of a deck of cards and you would put things in
different places in this deck, uh, and that would dictate how often you see them.
Um, yeah, actually, so, uh, I published a paper in 2016 in the association of computational
linguistics about the space repetition model that we use, uh, uh, at Duolingo. Um, so it, that
was actually one of my first projects back in 2013 when I first joined. So when I first joined,
we had one of these flashcard algorithms that was, you know, designed in the 70s back when you
had physical flashcards. So that's what was running in production to, um, to select the exercises
that you would see when you did a practice. So there were, there are lessons where you're being
introduced to new material in Duolingo and then there are practice sessions, uh, where you're
reviewing a old material that can either be within a particular skill that you did or you can
just like the whole course, everything that you've learned so far, you can practice all of it.
Um, and so the, the, the algorithm that was used in production before I started was called the
lightener system. And there's a few different flavors of it, but, but the one that was in production
is this idea that you've got a different box. And there's the one day box and the two day box
and the four day box and they grow up, go up exponentially in powers of two. Uh, and everything
starts in the one day box. And then when you practice something, if you get it right,
it graduates to the two day box. And then you get the way two days to review it. And then if you
practice it again two days later and you get it right, then it can graduate to the four day box
and, and so on and so forth. And, and that way you can, if you consistently keep doing well,
you can push it off longer and longer. Uh, but then if you get it wrong, then it demotes back to
the previous box. And so it cuts the, the spacing in half. Um, so that's what we had in production.
You know, there was, there was some rationale for it, but I quickly realized that that algorithm is
essentially, uh, two to the power of the number of times you got it right and the number of times
you got it wrong. So you can turn that into, you know, a, a model that is like, number of times
right as a feature with a coefficient of plus one and number of times wrong as a feature with
a coefficient of minus one. And you can learn those plus ones and minus ones from data, rather than
just like picking those kind of arbitrary numbers. Yeah. It turns out the Pimsler method, um,
that he published a schedule, Pimsler published a schedule on 67, I think, uh, that also turns out
to be a special case of this. So we, yeah, so we called it half-life regression because it's
essentially a nonlinear regression on with this kind of inductive bias of this half-life.
It's not exactly a slope, but like decay rate. Um, and, and turns out both the lightener system
and the Pimsler method are special cases of that model. And we were able to fit a model
two data and then put, we ran a controlled AB test putting that into production. Uh, and
and to this day, it's probably one of the most impactful, impactful experiments that we've done.
I think there was a 12% boost in, uh, in retention. And by that, I don't mean mental retention.
I mean, like a user used the actual day and going back the next day. Got it.
Or in the very early days in the first year or two of the company, that's the main metric that
we looked at. Well, it can be really frustrating when you're, you've got some set block of time
that you want to spend, you know, learning your language, whether you're on the bus or train,
or whatever, or, you know, you just block out the time. And if you spend half of that time
reviewing cards that you've already committed to memory, that is terribly frustrating.
Yeah. And, and one thing that we're running into now is like, we really, that was an early kind
of successive mine. And I've gone on to do other things and nobody's really been working on that
for seven years. And, and, and our user base has changed. We have more courses now. We've got
orders of magnitude more users now. Uh, the, the amount of content has changed. So that model that
was in production probably is not ecologically valid anymore. Uh, we do have a research scientist
who this past quarter has started revisiting those. And we've got some promising alternatives,
um, that we're about to start running some experiments to try to improve on. So that it,
it's still the case that some people, you know, hey, this is basics one at the beginning of the
tree. Why is it telling me I need to practice this now? I know that stuff backward and forward. And
so it was just an artifact of the fact that when we first fit the models, there were like,
only 100,000 users and, you know, six courses. Yeah. Wow. Um, so you, you, you made this distinction
between human or learner retention and, uh, the, your SaaS metric of retention. Um, I imagine
though that you are also trying to understand the human learner metrics, you know, to what degree
do you, um, you know, to what degree do you go after that? How difficult is it to go after that?
And is there a role, you know, of machine learning and trying to understand, um, you know,
the user experience, the learner experience? Yeah. That's a very, very important question. Uh,
and it's a very difficult question. So most startups, most companies really, there,
there's different families of, of metrics that you look at. All right. And all companies, of course,
look at revenue. That's one that's super important. Uh, most apps and things also look at
engagement. So how growth, you know, how many people are using it. But we have this additional
family of metrics on learning, uh, that most other companies don't have. And it's easy to kind of
look at what other companies are doing to, to optimize, you know, revenue and engagement.
And there's nobody else to look to to really figure out how to, to measure and improve learning.
And, and one way you could do that is, you know, by looking at which exercises they get
right or wrong and are these exercises that they got wrong six months ago or they get in them
right now. Um, and we do do some of that. And, but it's still very challenging because this,
it's this endogenous reasoning, you know, uh, what you're showing them is sort of priming them on,
on what they're, uh, they're getting right and wrong. But, uh, about a year ago, we created a,
an efficacy team, um, within the company that has started to do some longer term sort of research.
And just a month or two ago, we, we published a white paper. Uh, I think if you go to do
lingo.com slash efficacy, you can find a summary of it and, and actually download the white paper.
Um, the main results were essentially for learners of French and Spanish. So English speakers
learning French and Spanish. Um, there is, uh, at reaching checkpoint five, uh, of the course,
which is probably, uh, I don't know exactly how many skills that is, but it's, I'm going to guess
it's on the order of 20 or 25 skills. Um, so after doing 25 skills and do a lingo, you're performing
at least in terms of listening and reading, uh, at the level of somebody who's finished four or
five college semesters. And so there's some, there's mounting evidence that this kind of personalization
that we're doing, uh, through machine learning, uh, to build good content to, you know, create
engagement and to personalize through getting inside your head, um, is working. And do you also
using the, uh, using the separate tools that we talked about earlier, you should be all also
able to relate that level five knowledge base to, uh, a safer level. Uh, yes, you, you could.
I mean, the problem, the difficulty in doing that is you need to get parallel data to do some
mapping and equating, uh, but that's definitely something you could do. I mean, an analogous, uh,
thing we could maybe switch gears and talk about the doing the duolingo English tests because
this is related, um, you know, it is an assessment. And the idea there was, uh, before we do that,
another question on assessment, uh, in some of the tests, there are, um, if I remember correctly,
there are, uh, assessments where you're speaking into the, into your device. And it is, um,
essentially doing a speech recognition and trying to tell you if you've got that correct. And I'm
curious how, um, you know, machine learning oriented that is or technically, you know, sophisticated
that is, um, I remember, I don't remember when I was trying to learn a tonal language like
Mandarin, like if it was really all that sophisticated at telling one tone from another, I know that
when I got there, no one understood a word that I said. So clearly the tones I was not learning
them all that well, uh, but, um, you know, how, how much ML is going into like the speech side of
things. In the, in the duolingo learning app up until very recently, it was fairly rudimentary
and, and to be honest, we were relying on, uh, like the on app services like the Siri and the
Google, uh, ASR systems and, and also relying on, um, TTS that were third party provided. Uh, this,
this year we've actually hired, uh, more people, uh, and in particular, somebody who used to work
on the Siri team who's starting to beef that up. And so over, uh, where, where, we, we have reached
the point where we've hit a ceiling, uh, of what we can do effectively in the learning app using
third party tools. And now we're starting to build it out, uh, in-house with some promising early
results. So you should see that starting to be pushed out into the app, particularly for the,
the more popular courses like English for Spanish and Portuguese speakers or French and Spanish
for English speakers. Um, uh, but on the, on the duolingo English test side of things, there are also
speaking exercises that you have to do, um, both the sort of thing where you're, you're,
you're given a prompt and you have to say this kind of scripted thing as well as just completely
open ended, uh, exercises like you're given, uh, an image and you just have to talk about that
picture for, uh, you know, two or three minutes, uh, there may be one or two minutes. Um, so for those,
we do have more. That sounds like a really interesting NLP machine learning question answering,
potentially type of, uh, challenge. Yeah, yeah. So it's, it's pretty challenging and another thing
that is very important to us, uh, is to make sure that those models are fair across, you know,
that, that, uh, you know, uh, males and females have different vocal registers. Uh, so if you have
lopsided data in some sort of way, it could, you know, be automatically scoring, uh, one group,
uh, with higher scores than the other group for irrelevant reasons, different accents,
so that the speech recognition that is used as a part of that, uh, scoring algorithm features
are extracted from the ASR. Uh, it might work better for some accents than others and you need to
make sure that it's fair. Uh, so we've put a lot of effort into making sure that happens. We've
done a lot of, uh, you know, what's called differential item functioning in the psychometrics, uh,
literature, uh, make sure that the items are not behaving differently, uh, for different groups.
So those are some of the challenges of building the test, but another challenge is,
uh, your traditional, uh, test, and this is true for like, all kinds of educational high-stakes tests.
They're usually done at a test center, uh, and so that means by definition, there's only a few of
them. They're in certain cities. Uh, there's a finite number of seats that are available. They're
not open every day. More expensive to take and deliver. They're more expensive to take and deliver,
and there's this, and the assumption is because it's in this place, it is, you know,
there are security protocols that are sort of in place. Right. The day off will work usually
because they're, you know, businesses. Right. But if you're, if you're from like,
if you're in the rural Amazon, right, then that means you have to take a 14-hour bus ride into,
I don't know, Sao Paulo or something to take the tests, probably spend the night,
and then if there's a trucker strike that shuts down the highway for a day, the day before you're
going to take your test, you're out of luck. Yeah. And so we, we needed, it was part of our goals in
creating this test that is something that anybody anywhere, you know, could take, uh, as, I mean,
as long as they had an internet connected device. Um, that also means that, you know, one way to
cheat on a traditional kind of test is, uh, you know, you take the test and then you, you somehow,
you know, circulate the items that were in that test online so that in very short order,
anybody, you know, who is taking an administration with them in the next 48 hours has access to
the questions and that can get better scores. Uh, this by definition is a test that is in an
uncontrolled environment. So our strategy in combating that was to make it a computer adaptive test
with a huge number of items. Um, the only way to scale it, like, and the reason that makes it
more secure is when you go in and take the test, like, you're, every time you take it, if you take
the test 10 times, it's going to be a different set of questions every time. I think the item exposure
rate is, uh, less than a half a percent, maybe less than a tenth per percent. Yeah, you would have
to take the test a thousand times to see the same item again on average. Uh, and the way we accomplish
that is through machine learning. So all of, all of the items are automatically generated and then we
use the same, we're very similar techniques to create the, the CEFR line tools. We also then project
the items that we automatically generate onto a CEFR line scale, um, using machine learning.
So this is a B2 level question and this is an A1 level question. Um, and then we can adaptively,
you know, we'll start out giving you a B1 level question. If you do well on that, then we'll jump
up to a B2 or a C2, C1 level question. And if you do not as well on that, you know, we'll back off
to a B2 level question and we can zero in on your language proficiency. And when you're starting
this process and generating, uh, questions or texts, um, what type of generation are we talking
about generation from, you know, straight from the model or generation via selection from in the
wild text that you know are valid and make sense and what all goes into that. So right now it's a lot
of the latter, what you just said. Um, it, it depends on the item type. So there are several
different item types. We actually just published a paper this year in transactions of the association
of computational linguistics laying out the approach to the first version of the test. So that paper
even though it was just published a few months ago is already obsolete. Uh, but, um, but there are
several different item types, uh, that go back decades in the language testing literature that
are things that are easy to produce automatically and to grade automatically. Um, and so we'll, we'll
take texts from naturally occurring sources, authentic texts is what the language assessment folks
would say. Uh, and then one of the items is called the C test where every other word you remove
the second half of the word and then the task is to fill in the missing blanks, uh, which seems
like a really simple task really, really hard to do if you don't know the language. Um, and,
and what we can do then is we have a pretty good model to rank order. This is a, an A2 level text
because the language in it is pretty concrete. Uh, it's, it's pretty, um, what, what's, what's
the word I'm looking for? Um, there's not a lot of abstract ideas. It's like informational. And
then as you move up, uh, the levels that can become more academic and more, not only does the
vocabulary become more sophisticated, but the content, you know, it's trying to argue of viewpoint
or, uh, make some abstract cons, you know, uh, discuss abstract concepts. Uh, so those are the
sorts of things that the ML models pick up on. And we use these authentic text to create the
items and then the ML models to project them onto the scale. And then we use adaptive, kind
of active learning type algorithms to then search through that space, um, to, to, uh, efficiently
figure out where you belong on that scale. Interesting. Um, so that's, uh, kind of content and assessment,
a little bit of getting in your head, um, engagement. Yeah, spend a few minutes on that.
Yeah. Uh, so this is probably where to date we've invested the least, uh, and I'm looking forward to,
you know, uh, growing the team and, and doing more work in this. But a good example of that is,
um, we, we send out these, uh, if you've used the dual and go app, you've probably gotten and
maybe been annoyed by, you know, these push notifications that are practice reminders to remind you,
use the app, you know, or you'll lose your streak or something like that. Um, and so we actually
use, uh, machine learning to determine what message to send you when we, we send you those.
We also actually experimented with using machine learning to figure out when to send those to you.
Not, and we had fairly accurate models, uh, that could figure out, you know, that some people
have different patterns on the weekends versus the weekdays or, uh, or whatnot. It turns out that
those, that those, those timing models, even though they were very accurate, they just didn't
make a difference in terms of the metrics compared to some pretty good heuristics that we'd
developed that were simple heuristics. And so in the end, we actually are not using machine learning
to do the timing because it wasn't worth the technical debt of the machine learning. But we do
use it to pick the content and we actually have a KDD paper this year, um, about that. It's a
banded algorithm that's actually a novel algorithm because there are two things in using banded
algorithms for push notifications, at least the way we do it, that are kind of out of the box.
And one of them is that not every template that we could send you is, is viable at every time.
So we might want to send you a message that says, um, you know, uh, keep, keep your streak
or, or don't forget, uh, your streak, which is like the number of consecutive days that you've
been using Duolingo. But if your streak is one or zero or something, we don't want to send you
that, uh, that particular one. Uh, or another one is like, if you are on the leaderboard,
if you're in a certain position in the leaderboard, we might send you a message about that,
but it's only eligible if you're on a leaderboard. That's right. And so that's messes up the statistics,
a little bit. A universe of, uh, what, how big is the message space? I would have imagined that
that would be relatively small, like half a dozen and machine learning wouldn't be all that interesting
as a way to optimize these messages. No, we have hundreds. And we have hundreds. And they also
are translated into all the different languages. Because remember, we've got, uh, about half of
people using Duolingo are learning English from a variety of other languages from, you know,
like, uh, you know, Arabic to Spanish to Chinese. Um, so, so we have to localize all of those. And
the messages culturally perform differently, uh, you know, for, for different groups. Sure.
And so that's one problem is the eligibility criteria, which kind of screws up the statistics
a little bit. And then the other problem is this novelty effect, where, uh, if we send you the same,
like the bandit algorithm figures out, oh, yeah, time for your Spanish lesson. That's the number one
that we should always send that. Right. Very quickly, you'll burn out on that message. Uh, and so
we've had to introduce kind of this cognitive penalty, uh, which is very related to space
repetition. We actually borrowed a lot of the same work from, um, uh, seven years ago on space
repetition and baked it into this bandit model. And so to us, it seemed like it was a, it was a novel
algorithm. So we submitted a paper in it, uh, presented it, I guess last month. Oh, well. And in
terms of the, um, you know, comparison to relatively simple, heuristic type of an approach,
like, how do you characterize, I imagine you're characterized that in terms of engagement lift. Like
what is that, you know, how significant, uh, is the difference of the, uh, uh,
machine learning algorithms and heuristic approach? Uh, I think it was, uh, don't, you know,
don't send a, a streak message if there's no streak, don't send, uh, you know, the other one,
if that doesn't apply, you know, equally, I believe what we had been doing before. Uh, it wasn't
a heuristic as much as like these are all the things that are eligible at this time. Uh, pick one
at random. That's what we were doing. Yeah. Which gave us, you know, really good kind of like
training data to start with because it was a relatively, uh, representative sample. Um,
but we, in general, you try to, with, uh, in production, kind of industry, machine learning,
use heuristics where possible until, until as long as you can fit it in your head. That's,
that's a good heuristic, you know, heuristic can fit in your head or, or, uh, you don't have to
draw a flow chart for somebody else to understand it. Uh, and if it's doing well, stick with that for
now, uh, if it gets to the point where you can't keep it in your head or you have to draw a flow chart
to explain it to somebody else, that's when you should start using machine learning. Uh,
or when you've iterated on the heuristics to the point where you're just getting diminishing
returns, usually like whatever branches are in that heuristic, turn those into features and
apply machine learning, uh, and you'll usually get a lift. At least that's been our experience.
So in this particular case, um, you know, we hadn't really tried any heuristics beyond, uh, randomly
sampling because none of them made sense that wouldn't become immediately very complicated.
So machine learning seemed like the way to go. Interesting, interesting. Any other things that
you're doing on the engagement front? Uh, that's probably the best example to talk about now,
all right. Yeah. I think we're going on for a while. So I don't know. Yeah, no, that is, uh,
I just popped my, uh, my timer up here. We have been, um, but definitely a fascinating conversation.
And, uh, you know, like I said at the beginning, I could continue on, uh, and definitely, but, um,
um, we, uh, want to be conscious of our listeners, uh, attention span as well. Um, but it has been
great chatting with you about some of the things that Duolingo's doing, uh, to, you know, help people
learn languages using machine learning and AI. Well, thanks for having me, Sam. I'm a fan of the
podcast. So it's pleasure to be here. Awesome. Thanks so much for.
