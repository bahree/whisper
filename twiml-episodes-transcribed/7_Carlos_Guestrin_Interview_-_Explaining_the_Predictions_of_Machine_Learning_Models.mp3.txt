Hello everybody and welcome to another episode of Twimble Talk, the podcast where I interview
interesting people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
The recording you're about to hear is the first of a handful of interviews I was fortunate
enough to be able to record live in New York City from the O'Reilly AI and Stratoconferences
that I attended last week.
I'll be sharing these interviews on the podcast over the next several weeks and I think
you'll really, really enjoy them.
I'm especially excited to lead off this series with an interview with Carlos Gastron.
Now if that name sounds familiar it's because I've discussed Carlos's work on the show a number
of times, most recently when I discussed Apple's acquisition of his company Tury back in August.
In addition to Carlos's new role at Apple, he's also the Amazon professor of machine learning
at the University of Washington.
Earlier this year Carlos, along with one of his PhD students Marco Ribeiro and postdocs
Samir Singh, published some very, very interesting research into the explainability of machine
learning algorithms.
My conversation with Carlos is focused on this research and the paper that the group recently
published called Why Should I Trust You, Explaining the Predictions of Any Classifier.
This paper has been on my reading list for a while and I encourage you to take a look
at it.
Of course, you'll find links to Carlos and the paper in the show notes, which you can
find at twimmelai.com slash talk slash seven.
A quick note about the background noise and this and the other onsite recordings, they're
not too bad considering the noisy caverns in which they were recorded, but some of you
might find the murmurs and bumps a bit annoying.
If you find yourself in this camp, please accept my apologies.
And now on to the show.
So hey everyone, I'm here at the strata conference in New York City and I happen to find Carlos
Gestren, who we've talked about on the podcast before.
He's the Amazon professor of machine learning at the University of Washington and we've known
each other for a bit.
So Carlos, say hi.
Hi, thanks for having me here and it was great running into you, it's a great event.
Yeah, absolutely, absolutely.
In fact, I think we probably had a briefing like right at this very table a year or two
ago.
And I think we met at this event in this very place.
Where's that room over there?
Yeah.
Yeah.
Yeah.
So I guess I'll say very briefly to the audience, we're not in the most convenience spot
for podcasting.
So if there's the occasional trolley rolling by, just try to block that out because if you
want some lunch, it's right behind us.
But I'm sure you won't remember that at all because we're going to have a great conversation
here, first of all, congratulations on the acquisition of Toree, Neh, Dada, Neh, Graflab
by Apple.
That was amazing.
Yeah, we're very excited to work with Apple.
It's great.
Awesome.
Awesome.
So why don't we just start with introductions, like introduce yourself, talk a little bit about
your background.
I think a lot of people kind of know what you've been up to.
But sure, I'm happy to share.
So well, I'm Carlos, Carlos Gaston, me working machine learning for a long time.
So I was a professor at Carnegie Mellon for about eight years and then at University of
Washington since about 2012.
And I've been excited about machine learning for a long time and worked on many areas of
machine learning.
Most recently, a couple of years have been exciting to me, really around dealing with
a big data and the two sides of that.
So on one side, algorithms for machine learning that scale to very large data sets.
So how can you scale up to deal with tons of tons of data.
And the second side is what I think about is the human side of machine learning.
So how can a human understand large data sets?
How can a human understand what machine learning algorithm is doing and bringing some kind
of human perspective into the mix.
So I think about those two sides, a computer perspective and a human perspective of machine
learning in large data sets.
And I imagine there's also a fair amount of overlap and intersect between those two.
And of course, right?
So the bigger your data, in a sense, the harder it is to figure out how to make it work,
but it's also hard to figure out what's going wrong with that.
So debugging and machine learning algorithm that requires you to run in a cluster with tons
of machines is just almost an impossible task.
And honestly, the way I think about it is that there's no machine learning without humans
in the loop.
Right.
And this incredibly intelligent application, they're going to be self-sufficient, but
we'll always have humans be part of the process at some point.
And so making that more human is a very important part of machine learning.
It's been understudied in my field, but it's something that I'm very excited to engage
in as well.
Yeah.
So there's humans in the loop in lots of places, actually.
And one of the places that humans are most certainly in the loop is on the back end of a machine
learning recommendation.
And your group has done a lot of interesting work very recently, at least, on explainability.
Can you talk a little bit about how you've arrived at that research area?
Yeah, yeah.
So just in one sentence, we're interested in being able to provide more transparency to
machine learning.
We'll explain why a machine learning model makes a particular prediction, or why it behaves
a certain way.
Now, we fell into this topic kind of interestingly in various ways.
So for example, in academia, we're working with various folks in application domains,
and we said, oh, come use machine learning, solve this problem, it's going to be awesome,
we're going to change your life.
And their response was like, sounds great, but why should I trust this model?
What is it doing?
Right.
And I was like, it's got great accuracy.
So that's one side.
That's somehow unsatisfying.
It's unsatisfying.
Yeah, no, it's not enough.
It's really not enough.
We can talk about why it's not enough.
And then on the other side, once we build a company around machine learning, three day
to graph lab, we start working for a lot of companies that brought machine learning
to production.
And there was always a step that nobody talked about, but it was very fundamental.
They trained the model to do something, recommendations or whatever, predictor and predict fraud.
And you want to deploy it as a service every time you swipe your credit card, it makes
a prediction about fraud.
You don't just make that happen out of the box, you want to make sure that model is working
well and is doing things for the right reason.
Because if it's not, you're going to get fired.
Right.
You really want to understand why that thing is behaving the way it is.
So by talking to those.
And not just that, we've talked about on the podcast before in Europe, there's legislation
that's coming down the line that mandates explainability for machine learning and predictive
applications.
Yes, there's legislation in Europe.
And not just that, even in the US, for certain application domains in the financial sector,
they mandate certain models you are allowed to use versus others because they believe those
models to be more interpretable or more believable or something.
And so for certain tasks in that sector, you have to use a particular kind of model.
And that just blocks a lot of the high accuracy models you might want to use.
So it is a real issue.
And I think that issue gets bubbled up in three areas.
So one is just kind of general user model.
How can they gain trust that service is doing things for the right reason?
So if I go to a movie content recommendation in Netflix, I want to know that I got recommended
Lord of the Rings because I also like Star Wars, that gives me a sense that that thing is
doing the right things, recommending things that make sense to me.
And they can begin to gain a relationship of trust with that artificial intelligence
system underneath.
So that's kind of a more personal consumery thing.
But if you think about from the decision that it's really important, a really high change,
like a doctor making a decision about the treatment of a patient, there you want transparency.
So if you have a system that says the patient is going to have cancer with 90% probability,
most doctors are going to ignore that system because they might not trust it.
And also because there's a holistic approach to medicine that we want to have where it's
not enough to just make that one prediction.
But if the system were to say, you know, this patient is likely to have cancer because
if you look at their MRI results, you see this lump, and if you look at this related cases,
they were diagnosed in the same way, and if you look at this latest study, this all corroborates
to evidence, then I can gain a more holistic view and I can gain trust in the system.
So that's the second piece.
It's kind of getting deeper insights as to what's happening in that prediction.
And the third way, which is more kind of personal for me, is as a data scientist, I want
to be able to make the models always better.
And I want to understand when it's working, what's not working, so they can improve it.
And so those are the areas, public perception of machine learning, making more informed
decisions, not just the prediction for machine learning, and improving the models through
feedback.
And so transparency and explanation are going to be indispensable to make that happen.
And unfortunately, as a field, we haven't invested enough in that topic.
Right.
Right.
But you guys have started to invest in this and you was it a month or two ago, you published
a paper there?
Yeah, the paper came out just a couple months ago, and it's a system called Lyme, that
my student, Marco Ribeiro, and postdoc Samir Singh, wrote, Samir is now a professor you see
or find.
And we wrote this paper based on the feedback we're hearing and the need to do something
more in this area.
And there being some other works in the kind of explainability machine learning, but what
was unique about the perspective that Marco and Samir brought into the world is how they
approach the problem.
So a lot of the working machine learning has been about finding models they're transparent
or explainable.
Right.
Like we talked about in the financial sector.
So these models have to be simple, so this somebody can understand it.
But the problem with that is that simple models tend to be inaccurate.
And so you're compromising accuracy for explainability.
And that's what I think is the wrong compromise to have.
In fact, when you were making the comment that you drawing the analogy with Netflix earlier,
I was thinking to myself, you know, I'd actually kind of rather that Netflix recommends
movies that I want to see and not tell me how it got that, then recommend movies that
I kind of don't really like.
But it says it's read, this is the reason, so, so, so, so, right.
So accuracy is still a good, but for me, the way I'm thinking about it is accuracy is
number one.
You want to have high accuracy for the right reasons, but that's the main thing.
Otherwise, you're not going to be able to solve the image processing task that we've seen
solved really well deep learning today.
If you don't have the most accurate models, if I want to use a very simple model, like
a shallow decision tree, you're never going to be able to detect objects in an image.
So what's the point?
Right.
You know, we're not going to build that kind of artificial intelligence system.
So what we did was say, okay, let's take accuracy as a requirement.
And so that means that we want to be able to give a data scientist the flexibility to
choose any model they want.
And the question is, can we provide an approach that can explain the prediction for any model?
That was like the question, and I think it's a really beautiful question.
And you know, the way that the work came together was really interesting.
Of course, it's only scratching the surface of the possibility.
But what basically, Marcus Mierde, was come up with a system that says, okay, I want
to explain a particular prediction.
Why did you like that movie or why does this patient have cancer?
And the way we're going to explain it is in a simple way that's good just for this
prediction.
And we're going to do it by highlighting the pieces of the input that we believe the
model was most important for the model to make this decision.
So for the doctor's example, it is this particular area of the MRI, this particular studies
are going on, this related cases.
So that's kind of a small explanation.
For the recommendation system analytics that you're unhappy with, it might be that the
underlying system is very complex and very accurate, but the explanation we have to give
you, it's kind of very simple, but somehow has to be faithful.
Right.
It behaves like the model for this particular prediction, it's not like the model only
uses, you know, a lot of the rings for everything, but for this prediction, that's what was
the model.
Right.
And so that's how we went about doing that.
And, you know, we did a bunch of user studies that really showed that this can be very
powerful and can be used in various ways.
So it's pretty cool.
What was the nature of the user studies?
So one of the really cool user studies that Marco designed, let me just step back and say,
it's really hard to evaluate explanations because it's a subjective thing, right?
How do you even figure out it's doing anything interesting?
Right.
And maybe if it's better, I want to get into kind of how it works and what the research
actually showed.
If it's better to do that first, we can do that first and encircle back to the user studies.
It is, you're the boss.
So let's talk about how it works, let's start from there.
So the way it works is to say, I want to have a particular prediction, I want to explain
why it was made, why did the model make the prediction the same way.
And the way we're going to explain it is, let's look at the behavior of the system, the
behavior of this complex model around this prediction.
So not everywhere, but around that.
So for this particular patient, I'm going to try to explain why the model thought it
had cancer.
So let's look at patients around that, some that were predicted to have cancer by the
model, some that were not predicted to have cancer by the model.
And fit a simple explanation that explains the difference between those similar patients.
It doesn't explain the difference between every patient, but just patients that kind of
like you.
So patients that had most of the same characteristics as you that have cancer had this thing going
on, and patients with similar characteristics as you that don't have cancer had these
other things going on.
And that gives me a lot of insights for this particular decision, and that's more or
less how it works.
So as a course approximation of what I'm hearing, take the example of that you use in your
course, predicting real estate prices based on a bunch of different variables, it almost
sounds like you might have a model that's a regression model that's predicting based on
house size, and the Lyme system is almost a reverse regression that's going the other
way, like predicting what the inputs might be based on the output.
In a sense, so one way to think about it is house prices are very complex, depends on
the neighborhood you live in, the characteristics of the house, everything around it.
So there is a simple explanation why your house costs showing you $5 million, right?
That's how much your house costs.
And so, can you give me a raise to it?
So I can pay for my house.
I'll double what I pay you right now.
So why did your house cost five million?
Why did the house cost certain amount?
Who knows, really complex, the models can be very complex, but your specific house can
explain why the model predicted just $5 million, that's very doable.
I can look at your house and I look at similar houses that were like it, and I can fit locally
a simple model with only a few variables, they were most important, that's basically,
let's say linear, and it says around your house, the variables that were most important
were square footage and zip code and number of bathrooms, that's really important for
$5 million house, for $500,000 house, it might be a different thing is more important.
And so that's the kind of thing that the model would say, right, so that's kind of how
it works, it provides the key pieces of the input, they were most distinctive for particular
prediction, and as I said, this is one way to do explanations which scratch in the surface,
there's all sorts of other ways you can imagine doing it, and I think there's a lot of opportunity
to do even more.
But one of the challenges is how do you figure out if this makes any sense whatsoever, if
these kinds of explanations are good, if the algorithm is working at all, like there's
so many dimensions that this can go wrong, how can you even figure out if this is at all
reasonable, right, and Marco Samir and I spent a lot of time buying your head against
the ball to figure out how can you even test this, how can you even measure it, how do you
explain my explanations, and Marco had a couple brilliant ideas that really surprised
me, and so let me give you one of them, so he wanted to know if explanations are good
and intuitive, that would mean that somebody who is not a machine learning expert, a layperson
could look at it and make good decisions from it, so that's what he thought, so how can
we test that hypothesis, so he did two tests of validated hypothesis in a brilliant way,
so the first one was, if I can interrupt you, it sounds like the aim of the research was
not just to spit out, like in order list of features in terms of, you know, waiting or
importance in the output, but more generating human readable description, am I reading
too much into this, or is that correct?
No, no, no, no, no, it was a human interpretable description, interpretable, not necessarily
readable, okay, that's one way to explain, just can be many things, right, we explored
human readable explanations, the visualizations, we explored different ways to explain things,
but yeah, so here's the experiment that he did, which was pretty brilliant, so he took
a data set and just a little background, which is kind of a fun story that I just told,
there's this famous data set called the 20 News Groups Data Set, like 20 News Groups
Data Set has been around for about 30 years in a machine learning community, and it's
from news groups, which you might not know what they are, but they're something they're
called forums, now called Facebook pages, right, they have a topic and people talk about
the topic and they post things, and the data set was famous because the idea was given
the text of the posting, can you predict whether this was about Christianity or atheism
or hockey or computers, whatever the topic was, and basically any modern machine learning
approach gets 94% accuracy, so everybody used this data set in their classes, like I
used in my class, I said, oh machine learning is so cool, I guess 94% accuracy, when
Marco run his explanations on that, it turns out that the main features being used are
things like the email address of the poster, okay, so sam at gmail.com, always post
in the, I don't know what your interest are, sam, but then let's say podcast, news group,
podcasting, yeah, recta podcasting, clearly, and obviously it's a great predictor, but
it doesn't generalize to somebody else, and so it's not a good feature, it's a good
feature for you, but not a good feature for the world, and so if you remove that, those
kinds of features, the accuracy went down from 94% to only 57%, so this data set that
everybody has used for decades, a machine learning is so well, actually it was great so
well, and you were able to see that from the explanations, so the question that he wants
to ask going back to the user study was, as an expert he discovered this with the explanation,
can somebody who's not a machine learning expert discovered this and improved the performance
of a machine learning system, so he took this data set and there was only getting 57%
accuracy, and then clean the data, as much as scrub, scrub, scrub, scrub, remove all
this bad features, then retrain the model and using what to get about 70% accuracy,
by removing all the bad features like sam at gmail.com, and then coming up with a new model
or new model training on that clean data set, so that's the go sound, clean data, and
it was the dirty data, and the question was, using explanations could mechanical turkers
who know nothing about machine learning, identify bad features, we said, look at explanation,
just cross out things that you think should be relevant for this system, we didn't say
anything else, just cross out things that you think are irrelevant, and we thought, okay,
could crossing it out get performance of close to Marko's gold standard, from non experts.
So you ran the line system, the explanation system, against the dirty data set, you came
up with these explanations that included things like emails that should be irrelevant,
and you asked if turkers, yes, turkers if they could figure that out.
Figure out what parts of the explanation, in the sense what features, you thought should
not have been used as parts of this decision, and how did that go?
So after just three rounds of mechanical turkers crossing things out, they were able to
get better accuracy than Marko's gold standard data set.
So they were able to clean the data better than Marko did.
What is around in this case?
Around was showing the, so adding into details, but we showed the explanations to a number
of mechanical turkers, they were able to cross it out and retrain the model, then we showed
the new model to different types of mechanical turkers, the crossing things out and we showed
it again.
So we just did that three times, three iterations with non experts, but looking at explanations,
they were able to find all sorts of problems with data, clean it, and get better performance
than Marko did, like sitting down and like trying to clean the data himself.
Which was surprising, so that means that non experts, this is just an example, so just
a non-experts are able to understand explanations of complex machine learning system and provide
some feedback to that system that can be used to improve the performance of that system.
Which was really surprising.
Wow, that's very cool.
It was very cool.
And then the second user, so then you know, Marko was bold and then went to come up even
more interesting for this, so Mark, and the second one was also really exciting.
So here's what he wanted to ask.
When you train a machine learning model, you usually train it on some data and you evaluate
it on some data that you hold out, it's called a test set, so that you don't kind of get
a biased prediction on how well the model do.
So you can imagine some models might do well on the training set, but don't do well on
the test data set, so we want to throw out those models.
And some models will do well on the test set, and then you want to keep those models.
So that's what you typically do.
If you go back to the training news groups data set, if we had just looked at the training
news groups with the email address in the test set, you do well on the test set, so you
think you're doing well.
But it wouldn't be.
We didn't validate very well.
But if you had tested on some other data set, they didn't have some as email.com, then
it would have them badly, then it would be able to throw it out.
So here's the experiment that Marko did, which I thought was brilliant.
He split the data into a training set and a test set, and he trained a bunch of models,
a lot of models on different random subsets of that input data, of the training data.
And some models did well on the training data, and some models did badly on the training
data.
He threw out everything that did badly on the training data, because we only want to keep
high accuracy models.
So he kept only things that were accurate on the training data.
And then he looked at the test data set, and some models did well on the test data
set, and some models did badly on the test data set.
And then he said, oh, the model is still now, I mean, this is pretty standard, what any
data scientists would do.
Yeah, yeah.
Pretty standard.
Well, Z-Watt sticks against the wall.
Yeah, exactly.
Yeah.
Now he did the following.
Okay.
He took mechanical turkeys with no nothing about machine learning, and he showed them explanations
for the models that did, they didn't say anything about the process, and it was all
randomized and blind and everything, right?
So the explanations for models that were doing well on the test set, and models that did
badly on the test set, and they both looked equally good on the training set.
Yeah.
And the test set, you mean the validation.
Actually, it was a hidden, in this case, it was a hidden test set, but it came out of
all this way.
It was a hidden test set.
So there was something that you wouldn't do as a data scientist, but the test, he held
off some additional data, and he ran both, he ran lots of models, did well on the training
set, and he picked out some that did badly on the test set, and did well on the test set.
And then showed explanations for all those models to make out code turkeys and ask, which
model do you think is going to be better in the real world, based on the explanations of
why they're making their predictions?
Yeah.
And they were asked to pick between one, between two.
Yeah, between two.
And you were comparing them with the coin flip, right?
Yeah.
Compared to the coin flip.
And they did better than a coin flip.
So a coin flip gets 50% accuracy, 87% accuracy.
Wow.
So totally untrained, unwashed mechanical Turk masses are basically creating, you know,
doing feature engineering on models in their head.
So the first part was feature engineering.
The second part was like model selection, basically.
Right.
They were able to look at explanations and figure out this model is stupid.
Yeah.
Even though on the training set, it looked great.
Yeah.
But in the real world, there's going to be bad.
Wow.
That's pretty amazing.
And that was amazing to me.
And the fact that we can do that, as I said, we're only scratching the surface here,
but the fact we can do that, to me says humans will be in the loop in the long run.
There are insights who have, I mean, humans in the loop, because even kind of the statistical
problems that underlie this question, like if we discuss for a long time, we can talk
about why there's a relevant statistical.
Humans might be able to pick those out and they'll be able to do better feature engineering.
They'll be able to understand problems they're going in the data, even on train folks.
And now, if you imagine doing this to get more insight for the doctors or for systems
in the real world, I think you could do really amazing things.
So it's pretty exciting to me to start exploring this further and further.
That's very cool.
Let me ask you this.
This is kind of in the weeds question, but were the features, what you might think of as
natural features or engineered features?
Yeah.
That's the really interesting or a really interesting question.
So the underlying models used the engineered features.
So for example, he also showed this was good for deep learning models for images, which
used really, you know, learn complex features of the data.
But the way that he explained was from pieces of the input.
So the assumption that he made was the input is interpretable.
And by selecting pieces of images or part of the text, I used to look at that and say,
oh, this makes sense.
If we looked at like the seventh player of a neural network, I can say, oh, this is like
a human being like, what is that?
Yeah.
And why do I care?
And so that's why we biased towards this approach.
It doesn't mean that in long run we want to invent something better that looks at the features
because the problem might be down in the features, it might be down in the weeds.
And that's kind of where the research should go.
But as a first step, we looked at pieces of the input.
And it's totally model independent, like you can work on neural network models.
Yeah, yeah.
We've done this with deep neural networks.
We've done this with boosted decision trees.
We've done this with lots of different kinds of models.
Wow.
Wow.
So I know we need to get you off to your next session.
Where can folks learn more about this?
So if you just search for my name and line, you'll find our paper.
It was a KDD this year.
Line LIME.
LIME?
Okay.
We'll find a GitHub project that Marco has been putting together, we hope it's some of
these ideas.
But yeah, it's been a pretty exciting work.
And you just keep tracking, you know, Marco, Samir, there'll be a lot more in the pipeline.
There's a really cool thing.
That's awesome.
That's great.
And if folks want to reach out to you, you're on Twitter or what's the best way to get in touch
with you?
I'm on Twitter, Gastrin.
It's my last name.
It's the handle.
Okay.
And so reach out.
Awesome.
Give me some feedback.
We're also, as you know, on a Coursera teaching machine learning and that's another place
that I interact with folks.
Yep.
And it's a great course.
I highly recommend it.
Very case study focused.
I really enjoyed it.
Okay.
Thanks.
Great.
All right.
Thanks so much, Carlisle.
Yep.
I'll do a handshake here.
On the audio.
Thanks.
All right, everyone, that's it for today's show.
Thank you so much for listening and for your continued support.
A quick story.
If you follow me on Twitter, you know that I recently called out an iTunes review that
I'm actually particularly proud of.
In this review, a user that originally rated the podcast a two out of five based on their
disappointment with the switch to the interview format came back and revised that review to
a four, noting that the interviews were getting better and that the format was really starting
to grow on them.
Now don't get me wrong, please.
I really, really, really appreciate those of you that left five star reviews on iTunes.
And I hope the rest of you go run and do that right now.
But it also felt great to see that in spite of his initial misgivings, the shows just kept
getting better and the user eventually came around.
That kind of feedback is great to read.
Thanks to everyone who stuck with the show through the transition and I hope you're continuing
to learn a ton.
Please join the conversation by commenting on the show notes at the twimmalei.com website
or by reaching out to me on Twitter where you can find me at at Sam Charrington or at
Twimmalei.
All right everyone, thanks again for listening and catch you next time.
