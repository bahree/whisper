1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,320
I'm your host Sam Charrington.

4
00:00:31,320 --> 00:00:36,120
Today we conclude our trust in AI series with this conversation with Katherine Hume, vice

5
00:00:36,120 --> 00:00:39,760
president of strategy at integrate AI.

6
00:00:39,760 --> 00:00:44,360
You might remember Katherine from our interview last year on selling AI to the enterprise,

7
00:00:44,360 --> 00:00:46,960
which was Twimble Talk number 20.

8
00:00:46,960 --> 00:00:52,200
This time around we discuss our newly released whitepaper, responsible AI in the consumer

9
00:00:52,200 --> 00:00:57,120
enterprise, which details a framework for ethical AI deployment in e-commerce companies

10
00:00:57,120 --> 00:01:00,120
and other consumer facing enterprises.

11
00:01:00,120 --> 00:01:04,520
We look at the structure of the ethical framework she proposes and some of the many questions

12
00:01:04,520 --> 00:01:09,480
that need to be considered when deploying AI in an ethical manner.

13
00:01:09,480 --> 00:01:13,760
Thanks once again to Georgian partners for their continued support of the podcast and for

14
00:01:13,760 --> 00:01:15,880
sponsoring this series.

15
00:01:15,880 --> 00:01:20,000
Georgian partners is a venture capital firm that invests in growth stage business software

16
00:01:20,000 --> 00:01:27,480
companies that use applied artificial intelligence, conversational AI and trust to differentiate

17
00:01:27,480 --> 00:01:30,480
and advance their business solutions.

18
00:01:30,480 --> 00:01:35,520
Post investment Georgian works closely with portfolio companies to accelerate the adoption

19
00:01:35,520 --> 00:01:39,560
of these key technologies for increased value.

20
00:01:39,560 --> 00:01:44,600
To help their portfolio companies hire the right technical talent Georgian recently published

21
00:01:44,600 --> 00:01:51,080
Building Conversational AI Teams, a comprehensive guide to lead you through sourcing, acquiring

22
00:01:51,080 --> 00:01:55,440
and nurturing a successful conversational AI team.

23
00:01:55,440 --> 00:02:00,160
Check it out at Twimbleai.com slash Georgian.

24
00:02:00,160 --> 00:02:02,960
And now on to the show.

25
00:02:02,960 --> 00:02:09,840
All right everyone, I am on the line with Katherine Hume, Katherine is the vice president of

26
00:02:09,840 --> 00:02:15,640
strategy with integrate AI, Katherine, welcome back to this week in machine learning and

27
00:02:15,640 --> 00:02:16,640
AI.

28
00:02:16,640 --> 00:02:19,200
It's wonderful to be here Sam, one of my favorite podcasts.

29
00:02:19,200 --> 00:02:25,960
Well, you are one of one of my most favorite people so we are in the right place and we

30
00:02:25,960 --> 00:02:33,760
are going to be talking about broadly speaking the topic of ethics and fairness and trust

31
00:02:33,760 --> 00:02:42,120
in AI, but before we do that, for those who haven't had a chance to listen to the wonderful

32
00:02:42,120 --> 00:02:46,920
podcast that we did a while back, I don't even remember when, but why don't you do a kind

33
00:02:46,920 --> 00:02:53,240
of a brief intro and then we'll refer people back to that previous show in the show notes.

34
00:02:53,240 --> 00:02:54,240
Yeah, for sure.

35
00:02:54,240 --> 00:02:58,440
So as mentioned right now I'm heading up strategy for a Toronto based startup called integrate

36
00:02:58,440 --> 00:02:59,440
AI.

37
00:02:59,440 --> 00:03:04,120
You work with large consumer enterprises, the retail banks and telcos and retailers of

38
00:03:04,120 --> 00:03:09,360
the world and help them do a better job providing relevant experiences for their customers

39
00:03:09,360 --> 00:03:12,920
by applying machine learning into their customer engagement stacks.

40
00:03:12,920 --> 00:03:17,080
And prior to that, I worked at a New York City based machine learning research lab called

41
00:03:17,080 --> 00:03:19,920
fast forward labs that's actually now part of cloud era.

42
00:03:19,920 --> 00:03:25,320
That's where we last met and during my time there I was also working with the enterprise

43
00:03:25,320 --> 00:03:30,200
and sort of sitting between the world of academic research and the world of applied machine learning

44
00:03:30,200 --> 00:03:34,720
in the enterprise to help companies get a handle on what was actually ready for prime time

45
00:03:34,720 --> 00:03:39,400
as they tried to sort through the hype in the space and take them from the point of

46
00:03:39,400 --> 00:03:42,440
prototype conceptualization to actual scaled application.

47
00:03:42,440 --> 00:03:47,040
I guess the other thing to mention and it is relevant for my background for this context

48
00:03:47,040 --> 00:03:52,400
is that I have a doctorate in comparative literature and an undergrad in theoretical mathematics.

49
00:03:52,400 --> 00:03:58,880
And so I sort of bring a mixture of philosophy and epistemology, so the theories of knowledge

50
00:03:58,880 --> 00:04:03,080
to my thinking around applied mathematics and machine learning.

51
00:04:03,080 --> 00:04:07,160
That's been a decent toolkit for my thinking about ethics.

52
00:04:07,160 --> 00:04:11,720
And it's made for some very interesting conversations I will say.

53
00:04:11,720 --> 00:04:19,200
So jumping into this conversation around ethics, you recently spearheaded the development

54
00:04:19,200 --> 00:04:27,760
of a white paper on what you called responsible AI in the consumer enterprise, which is a

55
00:04:27,760 --> 00:04:31,560
really interesting paper.

56
00:04:31,560 --> 00:04:33,800
What bucket do you put that in?

57
00:04:33,800 --> 00:04:39,800
Do you think of it as ethics or fairness or trust there are a lot of words that we kind

58
00:04:39,800 --> 00:04:46,360
of throw around when thinking about some of the ethical issues with regards to AI as applied

59
00:04:46,360 --> 00:04:50,680
to the enterprise in particular, how do you kind of think through all that?

60
00:04:50,680 --> 00:04:57,240
It's a great question and there are so many terms being thrown around these days as basically

61
00:04:57,240 --> 00:05:02,280
we start to grapple with the implications of bringing machine learning algorithms from

62
00:05:02,280 --> 00:05:05,040
the research lab into the real world.

63
00:05:05,040 --> 00:05:11,760
And they sort of hit up against the surprising realities of human behavior and how human

64
00:05:11,760 --> 00:05:18,000
behavior gets reflected in data and then how algorithms basically recycle some of these

65
00:05:18,000 --> 00:05:22,720
reflections and refractions into products that can have sometimes surprising results.

66
00:05:22,720 --> 00:05:28,360
So that's sort of a, I'd say a general statement I'm sure was lots to unpack there, but under

67
00:05:28,360 --> 00:05:34,560
that this sort of general umbrella lies lots of terms that have social implications and

68
00:05:34,560 --> 00:05:39,920
technical implications, terms that you mentioned like responsibility, fairness, transparency,

69
00:05:39,920 --> 00:05:42,240
accuracy, bias.

70
00:05:42,240 --> 00:05:47,480
And I would say we in the paper cover all of those topics because we think they're all

71
00:05:47,480 --> 00:05:53,960
relevant and worth critical attention in the enterprise as well as in society as a whole.

72
00:05:53,960 --> 00:05:59,480
But I'd say what we were trying to do and what's unique about this paper is we didn't design

73
00:05:59,480 --> 00:06:00,920
it as a policy paper.

74
00:06:00,920 --> 00:06:06,320
So it's not providing guidance to government agencies or regulators to help them think

75
00:06:06,320 --> 00:06:12,120
about holistically sort of how to define these concepts at a at a at a large level so

76
00:06:12,120 --> 00:06:14,160
we can agree upon what they mean in society.

77
00:06:14,160 --> 00:06:19,160
It's also not oriented specifically for a data science or machine learning practitioner

78
00:06:19,160 --> 00:06:24,280
audience where we would go into some of the latest and greatest research on say tackling

79
00:06:24,280 --> 00:06:25,280
fairness.

80
00:06:25,280 --> 00:06:31,120
This one is designed for cross functional teams that work in consumer enterprises in particular.

81
00:06:31,120 --> 00:06:33,600
So it's aligned with some of the work we do at integrate.

82
00:06:33,600 --> 00:06:40,680
And it's meant to operationalize ethics as a series of questions and dialogues that teams

83
00:06:40,680 --> 00:06:47,760
need to be asking and having at the various points in building out a machine learning product.

84
00:06:47,760 --> 00:06:49,160
So that was sort of the approach we take.

85
00:06:49,160 --> 00:06:55,160
We really wanted to go from the realm of philosophy to the realm of concrete pragmatic.

86
00:06:55,160 --> 00:06:58,480
What do you need to be thinking about when what are the questions and how do you sort of build

87
00:06:58,480 --> 00:07:05,720
out a framework for developing a company's own ethical posture in the act of applying machine

88
00:07:05,720 --> 00:07:06,720
learning?

89
00:07:06,720 --> 00:07:14,880
And I'm a big fan of the use of the term operationalizing AI ethics.

90
00:07:14,880 --> 00:07:21,280
It's been I think a gap that's been kind of missing in a lot of the conversation that's

91
00:07:21,280 --> 00:07:22,960
been out there.

92
00:07:22,960 --> 00:07:27,520
There's and you talked about some of the various perspectives and things that folks have

93
00:07:27,520 --> 00:07:28,520
done.

94
00:07:28,520 --> 00:07:34,960
And we've seen some of the things that I've seen that have gotten is gotten close to providing

95
00:07:34,960 --> 00:07:39,800
a framework or more kind of checklist-y or manifesto-ish.

96
00:07:39,800 --> 00:07:43,160
You have all these different attempts and they're all wonderful attempts.

97
00:07:43,160 --> 00:07:51,600
But I love that you're kind of pushing it a little bit more formalism without kind of

98
00:07:51,600 --> 00:07:55,920
diving into an academic treaties.

99
00:07:55,920 --> 00:08:03,160
So maybe can you walk us through the overall structure of this framework?

100
00:08:03,160 --> 00:08:04,160
Yeah, for sure.

101
00:08:04,160 --> 00:08:11,520
So we actually used the steps that go into the process of building a machine learning

102
00:08:11,520 --> 00:08:15,640
system as the structure for our ethical framework.

103
00:08:15,640 --> 00:08:20,320
So basically we, you know, there's variations on how a machine learning product might

104
00:08:20,320 --> 00:08:24,600
be built, but more or less companies follow the same process.

105
00:08:24,600 --> 00:08:28,560
So in the beginning there's sort of a design phase where they have to ask, what problem

106
00:08:28,560 --> 00:08:30,480
are we actually solving?

107
00:08:30,480 --> 00:08:35,160
And I think one of the sort of pitfalls or the failure modes, let's say in applied machine

108
00:08:35,160 --> 00:08:40,880
learning and the enterprise and times where enterprises sort of get stuck is they get caught

109
00:08:40,880 --> 00:08:44,240
up with the innovative potential of some cool new capability.

110
00:08:44,240 --> 00:08:47,840
And forget to ask the question, what is the business value?

111
00:08:47,840 --> 00:08:52,360
What is our metric for success and how do we measure baseline performance today and an

112
00:08:52,360 --> 00:08:56,920
improvement upon baseline to make a judgment call as to whether or not machine learning is

113
00:08:56,920 --> 00:08:58,520
really the right solution for our problem, right?

114
00:08:58,520 --> 00:09:01,440
And whether or not we want to invest in this technology in the first place.

115
00:09:01,440 --> 00:09:07,280
So we sort of start off with saying giving some guidance and insight into how to go about

116
00:09:07,280 --> 00:09:10,720
framing and shaping a machine learning problem in the first place.

117
00:09:10,720 --> 00:09:13,920
Next we talk about data collection, where the data comes from.

118
00:09:13,920 --> 00:09:18,480
If it's coming from a product interface, how one can think about the design of that

119
00:09:18,480 --> 00:09:24,240
interface was to collect usable data, data that will be easier to maintain the quality

120
00:09:24,240 --> 00:09:27,480
of in the long term, as well as if there's data that's coming from a third party where

121
00:09:27,480 --> 00:09:29,720
there might be proven evidence issues.

122
00:09:29,720 --> 00:09:34,600
We talk about then some of the data processing, depending on the different algorithmic technique

123
00:09:34,600 --> 00:09:38,440
the company is using, this could include sort of standard feature engineering or more some

124
00:09:38,440 --> 00:09:44,800
of the processing to prepare for a learning representation if one moves into the deep learning

125
00:09:44,800 --> 00:09:46,640
sort of five things.

126
00:09:46,640 --> 00:09:52,160
We talk about how different types of algorithms can present to different issues as it relates

127
00:09:52,160 --> 00:09:54,480
to fairness bias and transparency.

128
00:09:54,480 --> 00:09:59,920
And then we talk about putting the model into production, what it means to scale, how

129
00:09:59,920 --> 00:10:03,440
there's some issues that can arise from sort of overfitting on a training set to extending

130
00:10:03,440 --> 00:10:04,920
that out to larger data sets.

131
00:10:04,920 --> 00:10:09,800
And then we talk a lot about what I actually think is one of the most interesting and underrepresented

132
00:10:09,800 --> 00:10:16,800
portions of fairness in machine learning systems, which is maintenance, DevOps, and auditing

133
00:10:16,800 --> 00:10:18,360
of the system in the long run.

134
00:10:18,360 --> 00:10:24,560
So not just when we first build out that theoretical model, but how we, how one can maintain accountability

135
00:10:24,560 --> 00:10:26,840
over a system over time.

136
00:10:26,840 --> 00:10:30,080
So that's sort of the, those are the steps that we think through and obviously, you know,

137
00:10:30,080 --> 00:10:33,840
there's iterations there and there's feedback loops and it's never quite as linear as

138
00:10:33,840 --> 00:10:35,640
it seems.

139
00:10:35,640 --> 00:10:38,880
But you know, that's sort of how we broke things down.

140
00:10:38,880 --> 00:10:44,600
Considering a maintenance, I think, is underappreciated, not just from an ethics and fairness perspective,

141
00:10:44,600 --> 00:10:47,520
but from a machine learning perspective in general, right?

142
00:10:47,520 --> 00:10:54,680
There's a lot of focus in the industry on modeling and not quite enough on getting that

143
00:10:54,680 --> 00:10:59,960
model into production and the care and feeding that's required to make sure that the model

144
00:10:59,960 --> 00:11:02,960
is fresh and to protect the business.

145
00:11:02,960 --> 00:11:08,520
Yeah, there's sort of a meta point to be made here where, you know, in this framework,

146
00:11:08,520 --> 00:11:14,120
we talk a lot about the interdisciplinary cross-functional questions.

147
00:11:14,120 --> 00:11:17,400
And this is sort of why it's framed more as a, we don't have the right answer.

148
00:11:17,400 --> 00:11:20,600
We can't reduce this down to a checklist where you just have to press, you know, press

149
00:11:20,600 --> 00:11:23,560
X plus button, press Y button.

150
00:11:23,560 --> 00:11:26,800
It's rather, this is the checklist of questions you need to ask.

151
00:11:26,800 --> 00:11:32,400
And the answer is going to depend upon the context and therefore it needs to be a collaboration

152
00:11:32,400 --> 00:11:36,720
between, you know, the legal team, the business team, the data team, the algorithm team,

153
00:11:36,720 --> 00:11:41,120
the engineering team, and that shows up at every point in the process, but if you think

154
00:11:41,120 --> 00:11:45,440
about this endpoint where it's around, you know, maintenance, operationalization of

155
00:11:45,440 --> 00:11:51,600
systems, et cetera, there too, it's, it's a, you know, there's some machine learning scientists

156
00:11:51,600 --> 00:11:56,080
who go so far as to build modular code and then put that out in the scale of into production,

157
00:11:56,080 --> 00:11:59,920
but most of the time this is going to then be collaboration with what Andrew Carpathy

158
00:11:59,920 --> 00:12:04,320
would call software 1.0 folks who, you know, are working in a slightly different code

159
00:12:04,320 --> 00:12:09,760
stack and are, and are representing and modifying the initial mathematics and the models to,

160
00:12:09,760 --> 00:12:12,320
to turn them into scalable, reliable software.

161
00:12:12,320 --> 00:12:16,400
So it's kind of all about, it's all about the handoffs, the collaboration, and then

162
00:12:16,400 --> 00:12:21,000
finding a way to translate between two different language systems, two different modes of

163
00:12:21,000 --> 00:12:27,840
working to, to, to do the hard work of, you know, making the systems thrive in production.

164
00:12:27,840 --> 00:12:36,960
On that point of translation, you also explicitly talk about some of the softer topics,

165
00:12:36,960 --> 00:12:38,960
people and communication.

166
00:12:38,960 --> 00:12:44,320
I don't know that you explicitly mentioned culture, but I imagine that plays a big role

167
00:12:44,320 --> 00:12:47,120
in this as well, a huge role.

168
00:12:47,120 --> 00:12:49,600
Can you talk a little bit about those topics?

169
00:12:49,600 --> 00:12:51,120
Yeah, for sure.

170
00:12:51,120 --> 00:12:53,560
So let's break it down into two.

171
00:12:53,560 --> 00:13:00,520
So in thinking about culture, it's interesting, I recently heard DJ Patil give a talk at

172
00:13:00,520 --> 00:13:05,360
a ethics conference that Scotia Bank hosted up here in Toronto.

173
00:13:05,360 --> 00:13:12,920
And he was mentioning that one of the key requirements for success to deploy AI ethically is to

174
00:13:12,920 --> 00:13:17,840
have a channel of resistance, right, to basically, almost like a, what are those things called

175
00:13:17,840 --> 00:13:20,800
where it's the, it's like the bat line, right, so it's like it's, it's like a channel

176
00:13:20,800 --> 00:13:23,840
where you're allowed to descent, right, there's sort of like the accepted protocol, but

177
00:13:23,840 --> 00:13:27,360
somebody's, somebody, people are empowered and they have the right to raise their hand

178
00:13:27,360 --> 00:13:30,160
and say, I don't think this is okay.

179
00:13:30,160 --> 00:13:38,760
So that's sort of, you know, I think there's a huge cultural implication there where, where

180
00:13:38,760 --> 00:13:43,680
teams are empowered to have a voice and to think critically about what they're building.

181
00:13:43,680 --> 00:13:51,280
And also to be stimulated and challenged to not just accept, you know, the, the base technical

182
00:13:51,280 --> 00:13:54,640
answer and building a machine learning model, namely that, you know, we've decided that

183
00:13:54,640 --> 00:13:56,440
our objective function is X.

184
00:13:56,440 --> 00:14:00,560
We're going to measure accuracy using, you know, sort of the following, the following

185
00:14:00,560 --> 00:14:01,560
measures.

186
00:14:01,560 --> 00:14:08,720
And I guess like one of the idiosyncrasies or the uncomfortable paradoxes of, say, rendering

187
00:14:08,720 --> 00:14:15,000
the model fair is that often to make a model fair requires a little bit of a trade-off

188
00:14:15,000 --> 00:14:20,360
on its accuracy, if one is just reporting on the performance of the model in its, you

189
00:14:20,360 --> 00:14:23,000
know, sort of in its normally functioning fashion.

190
00:14:23,000 --> 00:14:28,480
And that's, that, it's, it comes off as counterintuitive, right, because like the scientists

191
00:14:28,480 --> 00:14:32,800
that are building the models are, are thinking about measuring their performance according

192
00:14:32,800 --> 00:14:36,520
to the, the set of constraints that they, they've learned in school and that they've been

193
00:14:36,520 --> 00:14:40,760
applying and then to come into that and say, okay, we're going to actually design this

194
00:14:40,760 --> 00:14:43,800
so that there's a cut on accuracy and then the question becomes, well, who makes that

195
00:14:43,800 --> 00:14:44,800
call?

196
00:14:44,800 --> 00:14:48,760
Do I make that call or does this hit need to be a cross-functional dialogue with like

197
00:14:48,760 --> 00:14:49,760
the business team?

198
00:14:49,760 --> 00:14:53,920
Did they make the call on some of the trade-offs that we're going to incur if we decide

199
00:14:53,920 --> 00:14:56,840
to make a model fair as opposed to sort of more accurate?

200
00:14:56,840 --> 00:15:05,240
And so, so I think the translation questions are totally intertwined with the cultural

201
00:15:05,240 --> 00:15:10,600
questions, because there needs to be the foundation to enable, to enable these discussions to

202
00:15:10,600 --> 00:15:13,320
occur, to, to start to get traction in the first place.

203
00:15:13,320 --> 00:15:17,800
It's not just a, you know, let me download my fairness module from the cloud and the

204
00:15:17,800 --> 00:15:20,720
next thing you know, the next thing you know, everything's going to be fine.

205
00:15:20,720 --> 00:15:25,880
There's work to try to, to try to build out sort of open source scalable toolkits to

206
00:15:25,880 --> 00:15:30,280
address fairness, but it's still, we're still so early in the process and there's still

207
00:15:30,280 --> 00:15:36,360
not even alignment across the technical community on what fairness means as a statistical problem.

208
00:15:36,360 --> 00:15:39,000
So, we've got a lot of work to do to figure this out.

209
00:15:39,000 --> 00:15:40,960
That makes me think of a couple of things.

210
00:15:40,960 --> 00:15:49,480
This notion of a channel of resistance calls to mind, an article that I came across yesterday

211
00:15:49,480 --> 00:15:56,920
maybe, you know, very contemporary about how Google is experiencing, you know, internal

212
00:15:56,920 --> 00:16:03,320
turmoil is probably too strong, but there are very vocal kind of employee factions on

213
00:16:03,320 --> 00:16:08,400
two sides of the issue of the company's involvement in China.

214
00:16:08,400 --> 00:16:14,440
And that kind of suggests in the context of this notion of a channel of resistance that

215
00:16:14,440 --> 00:16:22,200
they have a very strong culture of, you know, supporting and allowing that kind of resistance,

216
00:16:22,200 --> 00:16:28,640
which kind of bodes well for their ability to, at least in this particular area, you

217
00:16:28,640 --> 00:16:34,520
know, support, you know, making ethical decisions around AI, does that, is that consistent

218
00:16:34,520 --> 00:16:35,520
with what you see?

219
00:16:35,520 --> 00:16:36,520
That's a great question.

220
00:16:36,520 --> 00:16:42,760
I'm certainly not an expert in what's going on inside Google these days, but I do, for

221
00:16:42,760 --> 00:16:43,760
sure.

222
00:16:43,760 --> 00:16:49,880
But, yeah, I think that it's interesting, my father, who worked in technology for 45 years,

223
00:16:49,880 --> 00:16:56,360
he was telling me that back in the 70s and 80s, when he was working, it wasn't even possible

224
00:16:56,360 --> 00:17:01,360
for two managers to be in a room together without a director present, right?

225
00:17:01,360 --> 00:17:05,320
So there was so much control over what might be gossip, what might be dissent, what might

226
00:17:05,320 --> 00:17:09,600
be background blings, what's going on in the company, that they didn't even, like, culturally,

227
00:17:09,600 --> 00:17:13,440
it was not possible for two people to meet, which I have trouble even imagining.

228
00:17:13,440 --> 00:17:16,160
You know, it's like, well, there's always water cooler discussions, right?

229
00:17:16,160 --> 00:17:20,360
Of course, there would be covert backchannels, if not explicitly permissible backchannels,

230
00:17:20,360 --> 00:17:26,800
but there's a difference there where, you know, it's, like, this as a, the organization

231
00:17:26,800 --> 00:17:31,840
coming together and explicitly sanctioning critical thinking, right?

232
00:17:31,840 --> 00:17:38,440
And explicitly, explicitly empowering people from the technical side and as well as voices

233
00:17:38,440 --> 00:17:39,760
external to the company.

234
00:17:39,760 --> 00:17:43,560
So one of the things we talk about in the paper is that I think one of the failure modes

235
00:17:43,560 --> 00:17:48,880
in thinking about operationalizing ethics and, let's say, from an organizational perspective

236
00:17:48,880 --> 00:17:52,480
would be, let's appoint the ethics committee, okay, well, who's on the ethics committee?

237
00:17:52,480 --> 00:17:55,240
And here we're thinking about large enterprises, so you can imagine it would be the chief

238
00:17:55,240 --> 00:17:59,960
data officer, the chief technology officer, somebody from the risk management department.

239
00:17:59,960 --> 00:18:06,000
And the question to be posed is, do the people occupy those positions have the requisite

240
00:18:06,000 --> 00:18:11,920
diversity of opinion and perspective to function well as ethics officers?

241
00:18:11,920 --> 00:18:13,960
And often that's not the case, right?

242
00:18:13,960 --> 00:18:17,360
Because of, just because of the sort of, you know, the issues in diversity, inclusion,

243
00:18:17,360 --> 00:18:18,360
and the workforce.

244
00:18:18,360 --> 00:18:21,840
And so meaning it's really important, not only that there's a channel of dissent, but also

245
00:18:21,840 --> 00:18:27,760
that multiple people's perspectives are brought to the table in particular during the design

246
00:18:27,760 --> 00:18:28,760
phase.

247
00:18:28,760 --> 00:18:32,120
You know, it's not necessarily, you don't need to have sort of community representatives

248
00:18:32,120 --> 00:18:36,040
when it comes to thinking about audit logs and trails on the, on production organization

249
00:18:36,040 --> 00:18:37,600
of the systems.

250
00:18:37,600 --> 00:18:43,280
But in some of the early thinking on doing what we call a pre-mortem, where a team would

251
00:18:43,280 --> 00:18:45,760
not do a post-mortem, where they've done their project, and then they look backwards

252
00:18:45,760 --> 00:18:49,000
and say, okay, what worked, what didn't, what are we going to change, but rather taking

253
00:18:49,000 --> 00:18:55,800
the time to imagine, prospectively, some of the issues that might go wrong in applying

254
00:18:55,800 --> 00:18:58,320
a machine learning application.

255
00:18:58,320 --> 00:19:03,040
And taking that from the embodied perspective of people who are not like, you know, some

256
00:19:03,040 --> 00:19:08,360
of that, you know, not like you, right, not like potentially the composition of an existing

257
00:19:08,360 --> 00:19:09,360
workforce.

258
00:19:09,360 --> 00:19:14,520
And so, so we ask and close this in the paper, should there be customers in that, on

259
00:19:14,520 --> 00:19:18,040
that ethics committee, or could it just be sort of different people's perspective within

260
00:19:18,040 --> 00:19:20,000
the organization?

261
00:19:20,000 --> 00:19:23,880
Another question is, and going back to who makes the call and who's accountable, there's

262
00:19:23,880 --> 00:19:28,520
different opinions across the community as to whether or not there needs to be a neutral

263
00:19:28,520 --> 00:19:34,120
third party, almost auditing function, who comes in and basically has, you know, has

264
00:19:34,120 --> 00:19:40,160
no skin in the game, and can therefore look through system design and audit it for compliance

265
00:19:40,160 --> 00:19:41,760
with some ethical standard.

266
00:19:41,760 --> 00:19:46,760
That makes sense to me, but I don't think it's enough because, as we say in the paper,

267
00:19:46,760 --> 00:19:51,360
if you sort of wait until the end, and then go back through a system and see where it

268
00:19:51,360 --> 00:19:54,880
might have ethical pitfalls, there's a lot of wasted time, and there's so many in the

269
00:19:54,880 --> 00:19:59,720
moment pragmatic decisions that technical teams can make to save themselves the trouble

270
00:19:59,720 --> 00:20:06,440
of, say, implementing a neural network when the use case requires a level of interpretability

271
00:20:06,440 --> 00:20:10,280
and transparency given regulatory requirements, and they could have sort of just selected a

272
00:20:10,280 --> 00:20:16,320
different algorithm earlier on and avoided the subsequent requirement to redo to redo work

273
00:20:16,320 --> 00:20:18,840
to meet regulatory or ethical requirements.

274
00:20:18,840 --> 00:20:25,520
So maybe let's walk through some of the key questions that need to be asked at these

275
00:20:25,520 --> 00:20:29,160
different stages, is that a good way to go through this, you think?

276
00:20:29,160 --> 00:20:30,680
Yeah, let's do it.

277
00:20:30,680 --> 00:20:37,920
So again, you kind of start with this notion of kind of problem definition and scoping.

278
00:20:37,920 --> 00:20:40,000
What needs to be thought about there?

279
00:20:40,000 --> 00:20:44,520
You just kind of talked about some of it, the way you define a problem is going to have

280
00:20:44,520 --> 00:20:48,720
direct implications on, I guess I'm kind of blurring the lines of problem

281
00:20:48,720 --> 00:20:55,400
definition with the whole neural network thing, but let me kind of roll that back and say,

282
00:20:55,400 --> 00:20:59,960
what are some of the questions that need to be considered when we're thinking about problem

283
00:20:59,960 --> 00:21:01,280
definition and scope?

284
00:21:01,280 --> 00:21:07,160
Sure, so I'll talk about two that are partially just about all machine learning products

285
00:21:07,160 --> 00:21:10,640
and also have implications and ethics.

286
00:21:10,640 --> 00:21:17,760
So one is how much error can the business tolerate in its predictions?

287
00:21:17,760 --> 00:21:24,240
And so as I think through this, it's a contextual question where, you know, you can imagine Amazon

288
00:21:24,240 --> 00:21:30,000
using a collaborative filtering algorithm to recommend products and personalize products

289
00:21:30,000 --> 00:21:31,920
to its customers.

290
00:21:31,920 --> 00:21:38,920
So there, you know, what's the worst case scenario if Amazon recommends that I buy a toilet

291
00:21:38,920 --> 00:21:42,280
as opposed to buying told stories were in peace?

292
00:21:42,280 --> 00:21:45,840
I might be, I might be humorous, I might be like, well, this is funny.

293
00:21:45,840 --> 00:21:50,800
It could have implications as a couple of sort of horror stories where I believe somebody

294
00:21:50,800 --> 00:21:57,160
bought an earn for their, after their grandfather died and then subsequently they were sort

295
00:21:57,160 --> 00:22:00,520
of presented with all of these morbid death trinkets.

296
00:22:00,520 --> 00:22:04,960
So obviously that can have some emotional impact on a user, but more or less like, you

297
00:22:04,960 --> 00:22:09,800
know, a recommender in a commercial setting is one that can tolerate a relatively high

298
00:22:09,800 --> 00:22:13,480
level of prediction error and therefore, you know, machine learning team doesn't need

299
00:22:13,480 --> 00:22:18,400
to work and work and work and work until they get to super high precision around accuracy,

300
00:22:18,400 --> 00:22:21,240
recall, et cetera.

301
00:22:21,240 --> 00:22:27,840
So just thinking contextually about that is, does have both pragmatic implications as

302
00:22:27,840 --> 00:22:29,520
well as ethical implications.

303
00:22:29,520 --> 00:22:37,320
Erase Roe has seen this going wrong is in use cases where a business like the end users

304
00:22:37,320 --> 00:22:41,240
have a psychological perception that they need to have certainty from their system.

305
00:22:41,240 --> 00:22:45,280
So this is classic and say auditing and I think we talked about this in the last podcast

306
00:22:45,280 --> 00:22:51,240
where, you know, if you're big for consulting firm and your job is to audit 10k statements

307
00:22:51,240 --> 00:22:56,520
for your clients, you know, there's not a lot of error that can be tolerated in those

308
00:22:56,520 --> 00:23:02,240
predictions because the whole name of the game is to assure whatever that may be, at least

309
00:23:02,240 --> 00:23:06,720
from, you know, from our psychological perspectives that yes, indeed, this 10k statement is in compliance

310
00:23:06,720 --> 00:23:09,560
with the general acceptable accounting principles, right?

311
00:23:09,560 --> 00:23:16,040
So that's saying that these fundamental questions in the output of machine learning algorithms

312
00:23:16,040 --> 00:23:21,720
which aren't deterministic, they can vary over time, but that, you know, that this power

313
00:23:21,720 --> 00:23:25,160
broadens the applicability of what we can do with software assistance today in such

314
00:23:25,160 --> 00:23:30,840
amazing ways, can have impact, can have ethical impact if certain users of your population

315
00:23:30,840 --> 00:23:34,600
are not as well represented in a data set and therefore the performance on the algorithm

316
00:23:34,600 --> 00:23:39,440
as it relates to the accuracy of predictions for products or services or whatever, you

317
00:23:39,440 --> 00:23:45,280
know, whatever for them is less sound than a population that's better represented.

318
00:23:45,280 --> 00:23:48,960
One of the other ones that I really like to think about actually was inspired by Jonathan

319
00:23:48,960 --> 00:23:54,640
Zinger in his super awesome article asking the right questions about AI, where he talks

320
00:23:54,640 --> 00:24:02,240
about how rigorous and precise we need to be in determining what our objective function

321
00:24:02,240 --> 00:24:04,920
actually optimizes for.

322
00:24:04,920 --> 00:24:09,920
So he gives the example, you know, the much cited example from the pro-publica recidivism

323
00:24:09,920 --> 00:24:17,080
prediction algorithms where the impression that the end users of the system had, you

324
00:24:17,080 --> 00:24:21,080
know, the judges was that this system would tell them the likelihood that somebody would

325
00:24:21,080 --> 00:24:24,560
recommit a crime as a function of the sentence that was given to them.

326
00:24:24,560 --> 00:24:27,880
So if they're given a five-year sentence, you know, low likelihood, if it's a two-year

327
00:24:27,880 --> 00:24:30,640
sentence, they haven't been punished enough and there's a high likelihood they're still

328
00:24:30,640 --> 00:24:33,800
going to, they're going to come back and receive us.

329
00:24:33,800 --> 00:24:39,600
He shows in that article that he's so definitely done is that what the system actually measures

330
00:24:39,600 --> 00:24:42,920
is the likelihood of conviction, which is a very different problem, right?

331
00:24:42,920 --> 00:24:46,440
So it becomes clear to us that it's like, oh, God, well, of course, African Americans

332
00:24:46,440 --> 00:24:49,680
are going to have a higher likelihood based on in a past precedent.

333
00:24:49,680 --> 00:24:55,640
And so I see that all over the place in businesses, too, where a business team might think

334
00:24:55,640 --> 00:25:00,800
that they are optimizing for, let's say, long-term revenue from a credit card customer.

335
00:25:00,800 --> 00:25:04,680
And actually, if you look at how the algorithm's been designed, it's just plain old conversion.

336
00:25:04,680 --> 00:25:08,160
So they're not actually trying to get great customers, they're just trying to get more

337
00:25:08,160 --> 00:25:09,160
customers.

338
00:25:09,160 --> 00:25:14,280
And that has both business impact because, you know, you might be getting a lot of customers,

339
00:25:14,280 --> 00:25:16,040
but that might not actually lead to long-term revenue.

340
00:25:16,040 --> 00:25:20,560
And it has ethical impact because the customers that you end up converting might not actually

341
00:25:20,560 --> 00:25:24,560
be sort of like well-served by the lines of credit that are given to them.

342
00:25:24,560 --> 00:25:29,880
And therefore, just being precise on sort of where what our target variable is and what

343
00:25:29,880 --> 00:25:34,960
the implications are and optimizing for that has, again, both business considerations as

344
00:25:34,960 --> 00:25:37,360
well as ethical considerations.

345
00:25:37,360 --> 00:25:44,480
That calls to mind a podcast that I did with Roma Rosales who is Director of AI at LinkedIn

346
00:25:44,480 --> 00:25:49,480
and that one was called Problem Formulation for Machine Learning.

347
00:25:49,480 --> 00:25:55,720
And the example that they gave that was really interesting was, you know, some system that

348
00:25:55,720 --> 00:26:00,960
they had a recommendation system or like something about groups, I think, that kind of

349
00:26:00,960 --> 00:26:05,080
optimized around engagement.

350
00:26:05,080 --> 00:26:10,880
And they, you know, their first versions of this system found that, hey, if we send

351
00:26:10,880 --> 00:26:14,560
more emails, people will be more engaged.

352
00:26:14,560 --> 00:26:18,640
But it didn't really capture kind of the quality of the engagement that they were looking

353
00:26:18,640 --> 00:26:25,920
for or the notion that they can piss people off by sending too many emails.

354
00:26:25,920 --> 00:26:32,000
And the big takeaway there that I found interesting was that while this Problem Formulation

355
00:26:32,000 --> 00:26:38,440
even in your model is kind of the first step, it's a very iterative process, right?

356
00:26:38,440 --> 00:26:44,480
You start someplace, you experiment, you learn, and then you can continue to evolve the

357
00:26:44,480 --> 00:26:53,360
way you define the problem and the nuance that you incorporate into that problem definition

358
00:26:53,360 --> 00:26:57,640
to kind of more closely zero in on what's really important.

359
00:26:57,640 --> 00:27:01,160
I really love that you're saying that it's an iterative process and it's absolutely

360
00:27:01,160 --> 00:27:02,160
right.

361
00:27:02,160 --> 00:27:05,280
So, you know, I mentioned earlier on these pre-mortems where we try to imagine what

362
00:27:05,280 --> 00:27:09,520
might go wrong, but the world is always much more complex than our measly imaginations

363
00:27:09,520 --> 00:27:10,520
can capture.

364
00:27:10,520 --> 00:27:14,880
So, it is really important that you go in with a certain amount of assumptions and then

365
00:27:14,880 --> 00:27:15,880
we learn.

366
00:27:15,880 --> 00:27:20,240
And we learn not only about the algorithm learning and improving its performance by updating

367
00:27:20,240 --> 00:27:27,200
parameters, et cetera, but about our paying careful critical attention to all of the assumptions,

368
00:27:27,200 --> 00:27:31,360
the sort of baked in assumptions that we weren't aware of until we put things out in the

369
00:27:31,360 --> 00:27:32,360
world.

370
00:27:32,360 --> 00:27:33,360
It's funny.

371
00:27:33,360 --> 00:27:39,520
Nick Bostrom has the famous anecdote about the paperclip maximizer in people in the

372
00:27:39,520 --> 00:27:40,520
world.

373
00:27:40,520 --> 00:27:42,280
So, folks aren't familiar with that.

374
00:27:42,280 --> 00:27:45,400
It's basically, you know, we design the system that's optimized to create.

375
00:27:45,400 --> 00:27:48,520
I think it's created as many paperclips as possible or in the fastest amount of time.

376
00:27:48,520 --> 00:27:52,240
I can't remember exactly how it's formulated, but it leads to this, you know, the algorithm

377
00:27:52,240 --> 00:27:56,960
realizes iteration after iteration that humans are the blocker in paperclip production.

378
00:27:56,960 --> 00:28:00,040
So, it's like, right, let's exterminate the humans and then we can just mine all the

379
00:28:00,040 --> 00:28:03,080
aluminum in the world to our hearts delight.

380
00:28:03,080 --> 00:28:06,880
And when I first read that, I was sort of like, oh, God, like it's, it, it, it, it, it

381
00:28:06,880 --> 00:28:12,080
pricked up my sensitivity around what I consider to be the sort of dangerous discussion around

382
00:28:12,080 --> 00:28:13,880
existential risk from AI.

383
00:28:13,880 --> 00:28:18,360
But the more I think about it, if you view it as a fable, it's a powerful fable because

384
00:28:18,360 --> 00:28:26,560
as you just described, optimizing for engagement, optimizing for some, some localizable, measurable,

385
00:28:26,560 --> 00:28:32,320
you know, variable, observable variable, can have drastic implications that we're not

386
00:28:32,320 --> 00:28:33,320
aware of.

387
00:28:33,320 --> 00:28:39,040
And what I like about the, you know, the iteration is it needn't be that we succumb to the all

388
00:28:39,040 --> 00:28:46,080
powerful paperclip maximizer if we are aware of the fact that there are risks in our

389
00:28:46,080 --> 00:28:53,000
reducing down the world to a quantitative, measurable, optimizable frame.

390
00:28:53,000 --> 00:28:59,080
But then keeping our lights in our eyes open as we learn to know that this is indeed a

391
00:28:59,080 --> 00:29:03,720
reduction to know that these systems are, are not super intelligent, you know, they're

392
00:29:03,720 --> 00:29:10,840
correlation gradient descent, you know, search, search space optimizers, right?

393
00:29:10,840 --> 00:29:12,960
So that, and that's it.

394
00:29:12,960 --> 00:29:16,240
And, you know, and obviously there's incredible progress going on in research community.

395
00:29:16,240 --> 00:29:21,520
But if we, if we humble ourselves and humble our interpretation and actually realize how

396
00:29:21,520 --> 00:29:26,520
limited these systems can be, then it forces us to open our minds to think about the critical

397
00:29:26,520 --> 00:29:31,240
questions we need to ask from a values perspective as we, and to empower ourselves operationally

398
00:29:31,240 --> 00:29:36,280
through, you know, through these, like, this iterative learning to, to fix things.

399
00:29:36,280 --> 00:29:40,120
That's hard for consumer enterprises in particular regulated enterprises because they don't

400
00:29:40,120 --> 00:29:41,520
tolerate mistakes.

401
00:29:41,520 --> 00:29:47,720
There's, you know, the whole culture around risk management is around, like, absolute

402
00:29:47,720 --> 00:29:54,440
prevention of, of a bad outcome, adherence to compliance standards and that mindset,

403
00:29:54,440 --> 00:30:00,000
a compliance mindset and a waterfall oriented legal risk management mindset comes into

404
00:30:00,000 --> 00:30:04,560
a lot of tension and friction with a sort of iterative learning, let's fail, but then

405
00:30:04,560 --> 00:30:08,920
we'll, you know, we'll rectify things, I approach that is often required for machine

406
00:30:08,920 --> 00:30:09,920
money.

407
00:30:09,920 --> 00:30:10,920
Right, right.

408
00:30:10,920 --> 00:30:18,320
And it also echoes very strongly back to the, kind of the sixth, sixth step in your

409
00:30:18,320 --> 00:30:23,920
process, this monitoring and maintenance, if you're not monitoring the resource consumption

410
00:30:23,920 --> 00:30:32,400
of your paper clip maker and its secondary implications, you can't really effectively

411
00:30:32,400 --> 00:30:35,240
refine problem definition in the next iteration.

412
00:30:35,240 --> 00:30:40,320
Yeah, my VP of engineering will be delighted to know that, you know, I sort of, I used

413
00:30:40,320 --> 00:30:45,880
to be really bored by things like logging, you know, measurement of stuff, because most

414
00:30:45,880 --> 00:30:51,360
boring stuff I want to think about, like, algorithms and math, and now I am, I'm endlessly

415
00:30:51,360 --> 00:30:58,640
fascinated, I have a lot of respect for the technical specifications and the rigor that

416
00:30:58,640 --> 00:31:01,960
goes into building an auditable system.

417
00:31:01,960 --> 00:31:04,720
I give you an example of one of the problems we've been thinking about.

418
00:31:04,720 --> 00:31:10,440
So in the wake of GDPR, we're working in Canada and so there's still some questions related

419
00:31:10,440 --> 00:31:15,360
to how the privacy regulations might be updated in keeping with, you know, what happened

420
00:31:15,360 --> 00:31:18,680
in European Union and then sort of discussions around the globe or on the appropriate privacy

421
00:31:18,680 --> 00:31:19,920
frameworks.

422
00:31:19,920 --> 00:31:26,840
And the more progressive enterprises are acting as if GDPR were international and global.

423
00:31:26,840 --> 00:31:31,600
And so one of our customers, we had it, was thinking about storing predictions.

424
00:31:31,600 --> 00:31:35,000
So, you know, build out a machine learning model, it's in the marketing space, so it's

425
00:31:35,000 --> 00:31:40,640
recommending which offer, which action would be the next best action or offer for different

426
00:31:40,640 --> 00:31:42,160
customers.

427
00:31:42,160 --> 00:31:45,840
And, you know, when that prediction is made, there's the data went into train the model,

428
00:31:45,840 --> 00:31:49,560
but then there's, you know, the output of the model at a given moment in time.

429
00:31:49,560 --> 00:31:54,160
And so we were thinking, what if sometime in the future, you know, right now we're

430
00:31:54,160 --> 00:32:02,320
November 29, 2018, what if on April 11, 2019, a customer has a question about why they

431
00:32:02,320 --> 00:32:07,400
were served some particular offer, you know, back six months ago.

432
00:32:07,400 --> 00:32:11,720
And so, you know, how would one resolve that if the model is indeed updating over time

433
00:32:11,720 --> 00:32:15,600
and if the output of a, you know, the output of a prediction in April is any different

434
00:32:15,600 --> 00:32:17,720
than it was in November.

435
00:32:17,720 --> 00:32:21,480
And so just keeping that log, right, not only thinking about logging data, not only thinking

436
00:32:21,480 --> 00:32:25,560
about sort of logging, you know, anomalies in the system, but indeed logging this derived

437
00:32:25,560 --> 00:32:30,560
data that, you know, GDPR would call profile, or some sort of, you know, some, some probabilistic

438
00:32:30,560 --> 00:32:34,760
output of a person's likelihood to act on something, it's just like there's a new data

439
00:32:34,760 --> 00:32:39,360
category that companies need to think about when they're, you know, when they're logging

440
00:32:39,360 --> 00:32:42,000
things and they're thinking about oddening their system.

441
00:32:42,000 --> 00:32:48,360
So we should probably move through this list a little bit more quickly.

442
00:32:48,360 --> 00:32:52,000
Number two is design.

443
00:32:52,000 --> 00:32:57,840
What, let's talk about that, that phase with regard to the questions that it should be

444
00:32:57,840 --> 00:32:58,840
prompting.

445
00:32:58,840 --> 00:32:59,840
Yeah.

446
00:32:59,840 --> 00:33:02,000
So we were thinking about design here from a front end perspective.

447
00:33:02,000 --> 00:33:07,480
So it's a big term, but we were sort of narrowed it down to designing product interfaces.

448
00:33:07,480 --> 00:33:11,120
And the questions that we think about here are a lot around how you collect data, both

449
00:33:11,120 --> 00:33:14,840
so that you are reducing some of the ambiguities.

450
00:33:14,840 --> 00:33:18,560
If a question's hard for a person to ask, people are probably going to give answers that

451
00:33:18,560 --> 00:33:23,600
lead to low quality data, as well as some of the representational harm that the way in

452
00:33:23,600 --> 00:33:26,960
which data formats are presented could cause to people.

453
00:33:26,960 --> 00:33:29,560
So an example I like here is gender.

454
00:33:29,560 --> 00:33:33,440
So right now, I think Facebook has 73 and it could be off on the precise number, but

455
00:33:33,440 --> 00:33:37,200
I believe it's 73 different gender categories.

456
00:33:37,200 --> 00:33:41,160
And there are 73 different categories that shows that the variable is continuous and not

457
00:33:41,160 --> 00:33:42,160
discrete, right?

458
00:33:42,160 --> 00:33:48,120
So if there are two or if there are five categories and we can easily be like on one of these five,

459
00:33:48,120 --> 00:33:53,640
it's probably discrete, but here it's this strange mapping of something that is a, it's

460
00:33:53,640 --> 00:33:54,640
easy, right?

461
00:33:54,640 --> 00:34:00,080
It's continuous and nonetheless our systems, we need to put them into a tabular format.

462
00:34:00,080 --> 00:34:05,240
And that it leads to these interesting representational questions underneath the scenes.

463
00:34:05,240 --> 00:34:08,440
And then the third thing we think about in design is, is this a human in the loop system

464
00:34:08,440 --> 00:34:13,720
or is this a completely automated system because there's different legal and liability questions

465
00:34:13,720 --> 00:34:16,040
associated with either a person?

466
00:34:16,040 --> 00:34:24,160
And so is there as part of this, does the cross functional aspect of this framework get

467
00:34:24,160 --> 00:34:27,680
represented in the design phase?

468
00:34:27,680 --> 00:34:34,800
Do you need to be, you know, I guess I'm thinking of, and in fact I think I remember reading

469
00:34:34,800 --> 00:34:40,400
in the paper, you kind of referring to Agile Ethics or Agile, the word Agile came up

470
00:34:40,400 --> 00:34:47,680
somewhere and this in particular kind of calls to mind the whole, you know, do design

471
00:34:47,680 --> 00:34:54,240
with your customers from, you know, traditional software development, Agile applications.

472
00:34:54,240 --> 00:35:02,360
Yeah, so I quoted the term Agile Ethics from Alex Dunn who is the executive director

473
00:35:02,360 --> 00:35:04,960
of a company called the Engine Room based in the UK.

474
00:35:04,960 --> 00:35:07,120
I think she's fantastic.

475
00:35:07,120 --> 00:35:11,600
And I heard her, I actually shared a blog post about the term Agile Ethics that came out

476
00:35:11,600 --> 00:35:16,760
during AccessNow's Rights Con conference in Toronto in May and I met her there and I was

477
00:35:16,760 --> 00:35:17,920
just like, oh my god, I love it.

478
00:35:17,920 --> 00:35:19,400
That's exactly what I'm thinking about.

479
00:35:19,400 --> 00:35:23,840
And so it's funny, I've used the term Agile Ethics with a bank and the, what this does

480
00:35:23,840 --> 00:35:29,000
not mean is that like, you know, we can sort of update our principles on a day-to-day

481
00:35:29,000 --> 00:35:33,640
basis, so one day fairness means, you know, equality of opportunity and the next day it's

482
00:35:33,640 --> 00:35:35,920
equity of outcome, right?

483
00:35:35,920 --> 00:35:39,000
That is not what I mean by Agile Ethics.

484
00:35:39,000 --> 00:35:45,720
What we mean is that, you know, ethics need not be that one-time, final compliance review

485
00:35:45,720 --> 00:35:46,720
of a system.

486
00:35:46,720 --> 00:35:53,040
It should be embedded into the phases of an Agile product development methodology.

487
00:35:53,040 --> 00:35:57,360
And then second is, yeah, as it relates to design, so this is going to be the front end

488
00:35:57,360 --> 00:36:00,920
of the system consent is a big question here.

489
00:36:00,920 --> 00:36:04,400
So in thinking about, you know, how are you going to represent a consent mechanism to

490
00:36:04,400 --> 00:36:05,400
your customer?

491
00:36:05,400 --> 00:36:09,960
Is it going to be, as we know, the sort of standard tacit, you know, here's a 55-page

492
00:36:09,960 --> 00:36:14,680
legal document that nobody reads, but in order to play part in society and the system,

493
00:36:14,680 --> 00:36:20,040
you just sort of tacitly accept it or is there a way to represent it in a, in a friendly,

494
00:36:20,040 --> 00:36:25,240
so not a, like, not sort of cumbersome way, but to make privacy a little bit more legible

495
00:36:25,240 --> 00:36:29,160
and meaningful to users now that we care about it.

496
00:36:29,160 --> 00:36:32,680
I think it's, it's, it's heartening to seeing that more and more people are seeming

497
00:36:32,680 --> 00:36:36,680
to care about it in the wake of, you know, some of the privacy breaches that have occurred

498
00:36:36,680 --> 00:36:38,680
in over the past year.

499
00:36:38,680 --> 00:36:43,080
So speaking of privacy breaches, data collection and retention.

500
00:36:43,080 --> 00:36:44,080
Yeah.

501
00:36:44,080 --> 00:36:47,280
These are big issues here.

502
00:36:47,280 --> 00:36:52,440
And in fact, probably, you know, this is, this is maybe an area that's more obvious to

503
00:36:52,440 --> 00:36:59,560
the people when they're thinking about responsible AI and ethics, or at least it, you know, comes

504
00:36:59,560 --> 00:37:03,880
up frequently, right, from a privacy perspective.

505
00:37:03,880 --> 00:37:06,880
Is it broader than people tend to think?

506
00:37:06,880 --> 00:37:15,600
I think, I think if it is broader, it's that often problems that we associate to ethics

507
00:37:15,600 --> 00:37:20,240
in ML and algorithms actually start with data.

508
00:37:20,240 --> 00:37:26,480
And so, you know, I think by now, it's pretty well understood that it's garbage and garbage

509
00:37:26,480 --> 00:37:27,480
out.

510
00:37:27,480 --> 00:37:32,600
So, you know, if you've got a data set and you've got awesome representation of rich Caucasian

511
00:37:32,600 --> 00:37:40,840
males and very scant representation of not-rich African-American females, then, you know,

512
00:37:40,840 --> 00:37:46,360
the performance of your algorithms can vary across those, across those represented sets.

513
00:37:46,360 --> 00:37:55,160
So I think what's tough is actually solving this, you know, in such a way where companies

514
00:37:55,160 --> 00:38:00,800
think about modifying how they are collecting data and how they're storing it.

515
00:38:00,800 --> 00:38:06,800
So as to sort of address this problem at its roots and its foundation, it's like, it's

516
00:38:06,800 --> 00:38:09,960
one thing to note that it's a problem and it's another thing to go through the changes

517
00:38:09,960 --> 00:38:13,200
required to like update your data collection and retention procedures.

518
00:38:13,200 --> 00:38:19,320
I think the other one is from a risk management perspective and information governance.

519
00:38:19,320 --> 00:38:24,520
Prior to ML, the sort of rule of thumb would be, if you don't need the data, delete it.

520
00:38:24,520 --> 00:38:25,520
Do you know what I mean?

521
00:38:25,520 --> 00:38:26,520
There's, there's of course staff-

522
00:38:26,520 --> 00:38:30,120
You don't need a particular attribute as well.

523
00:38:30,120 --> 00:38:31,120
You don't need an attribute.

524
00:38:31,120 --> 00:38:35,480
Yeah, and if you don't, yeah, and if you don't and think about email, like email retention.

525
00:38:35,480 --> 00:38:41,920
So, like in the information governance community, there's going to be, you discovery and evidence

526
00:38:41,920 --> 00:38:45,080
and production requirements that mean that for a certain period of time, you know, things

527
00:38:45,080 --> 00:38:50,320
need to be saved and stored in case there's some question in case, you know, in case files

528
00:38:50,320 --> 00:38:51,320
need to be reviewed.

529
00:38:51,320 --> 00:38:53,800
But once you're outside of that, it's just like, got rid of it.

530
00:38:53,800 --> 00:38:57,680
And obviously it's cheaper to store massive data at scale now.

531
00:38:57,680 --> 00:39:03,000
So that's the sort of, the economics of that have shifted a lot over the past 20 years.

532
00:39:03,000 --> 00:39:07,640
But from an ML perspective, this came up once in a discussion with a financial services

533
00:39:07,640 --> 00:39:14,280
firm that wanted to capture seasonal irregularities or seasonal regularities across multiple years

534
00:39:14,280 --> 00:39:16,360
in a particular domain.

535
00:39:16,360 --> 00:39:21,200
And if the retention folks going back to cross-functional collaboration say, the week

536
00:39:21,200 --> 00:39:24,720
after year three, and then the machine learning folks come in and they say, oh, no, no,

537
00:39:24,720 --> 00:39:27,840
but we don't want to overfit to, you know, the recent theme when actually have sort

538
00:39:27,840 --> 00:39:33,880
of a more historical view, there can be misaligned objectives, I'd say, in terms of, you know,

539
00:39:33,880 --> 00:39:35,400
what to retain and what not to retain.

540
00:39:35,400 --> 00:39:40,920
Yeah, I was just having a conversation with a manufacturer in the transportation industry

541
00:39:40,920 --> 00:39:46,080
and they were talking about, or we were talking about cultural implications and culture

542
00:39:46,080 --> 00:39:51,120
clashes kind of along these lines and the same exact issue came up, they're collecting

543
00:39:51,120 --> 00:39:58,640
all of the sensor data about, you know, how their vehicles are being used and their retention

544
00:39:58,640 --> 00:40:06,560
people are telling them, you know, throw it away and, you know, do things like kind of

545
00:40:06,560 --> 00:40:16,040
early anonymization and they are, they recognize that they don't really know what the potential

546
00:40:16,040 --> 00:40:19,920
future value of this data is and how they may want to use it.

547
00:40:19,920 --> 00:40:25,560
And they're kind of digging in, I guess, for lack of a better term to kind of fight this

548
00:40:25,560 --> 00:40:33,200
long fight to try to shift this culture or at least come to, you know, some happy medium.

549
00:40:33,200 --> 00:40:38,200
And I don't know that there's any formula for doing that.

550
00:40:38,200 --> 00:40:46,120
You know, I recently became obsessed with an anecdote about Henry Ford who, as a by-product

551
00:40:46,120 --> 00:40:55,040
of his work creating cars, he set up a bunch of sawmills in Michigan and he invested in

552
00:40:55,040 --> 00:41:00,000
this almost so that he could reduce this bottlenecks and his supply chain for wood, so they

553
00:41:00,000 --> 00:41:03,640
could, you know, make steering wheels and baseboards, et cetera, for the cars a lot faster.

554
00:41:03,640 --> 00:41:08,640
And in doing so, he had all these stumps and branches and sawdust, like all this leftover

555
00:41:08,640 --> 00:41:10,760
wasted wood stuff.

556
00:41:10,760 --> 00:41:16,000
And so he ended up combining the wood with tar to create charcoal, briquettes, and created

557
00:41:16,000 --> 00:41:20,440
a separate business line that has now become Kingsford charcoal, which is still an emphasis

558
00:41:20,440 --> 00:41:21,440
today.

559
00:41:21,440 --> 00:41:24,120
I did not know that Henry Ford was the father of Kingsford charcoal.

560
00:41:24,120 --> 00:41:29,080
And what I love about that story is I actually think it helps us sort of rewrite history in

561
00:41:29,080 --> 00:41:33,120
thinking about the legacy of Henry Ford for the data era, where we thought about, you

562
00:41:33,120 --> 00:41:40,880
know, the, like, Henry Ford is all about automation and the assembly line and reducing and squeezing

563
00:41:40,880 --> 00:41:46,200
out every ounce of variable cost from production systems to scale, but like for the data era

564
00:41:46,200 --> 00:41:50,880
and the machine learning era, what's interesting about business processes is that they are

565
00:41:50,880 --> 00:41:57,240
unique vehicles to create data and knowledge about the world that only exists by, as a

566
00:41:57,240 --> 00:41:59,960
byproduct of the activity of doing that work.

567
00:41:59,960 --> 00:42:04,440
And, you know, for me, this is sort of the, had you really, a lot of enterprises struggle

568
00:42:04,440 --> 00:42:07,560
with disruptive innovation in AI.

569
00:42:07,560 --> 00:42:11,200
It's kind of like, okay, we'll optimize something, okay, we're going to automate an existing

570
00:42:11,200 --> 00:42:12,200
job.

571
00:42:12,200 --> 00:42:13,200
But like, is this it?

572
00:42:13,200 --> 00:42:14,200
Is this all we get here?

573
00:42:14,200 --> 00:42:15,200
Is cost savings from AI?

574
00:42:15,200 --> 00:42:20,120
And it's like, not if you do this creative shift in the value of your business process

575
00:42:20,120 --> 00:42:27,040
and start to create a unique information asset, however, as you just said, doing that shifts

576
00:42:27,040 --> 00:42:30,920
around, you know, how we think about data and how we retain it.

577
00:42:30,920 --> 00:42:36,080
And when that data is created, you know, Henry Ford, he didn't, the wood itself did not

578
00:42:36,080 --> 00:42:41,600
suggest charcoal briquettes, it was human ingenuity to find tar with the wood to create a new

579
00:42:41,600 --> 00:42:42,600
business line.

580
00:42:42,600 --> 00:42:45,520
And similarly, it's like, all right, so where does human creativity lie in this whole

581
00:42:45,520 --> 00:42:46,520
AI equation?

582
00:42:46,520 --> 00:42:50,120
Well, the data itself is not worth much.

583
00:42:50,120 --> 00:42:55,840
It takes our designing, our framing, our applying the right algorithm, our lining up the

584
00:42:55,840 --> 00:42:59,560
right back end architecture to scale this to create value.

585
00:42:59,560 --> 00:43:02,760
And you know, that's not an ethical point, that's more of a strategic point.

586
00:43:02,760 --> 00:43:06,720
But I think it's like the essence of innovation with AI.

587
00:43:06,720 --> 00:43:14,880
When it comes to beyond the retention of the data, when it comes to processing the data,

588
00:43:14,880 --> 00:43:19,600
that also has huge ethical implications.

589
00:43:19,600 --> 00:43:24,240
Can you talk through some of the areas that you explore around processing?

590
00:43:24,240 --> 00:43:31,200
I think the most salient point here has to do with the paradox, a paradox of interpretability.

591
00:43:31,200 --> 00:43:37,600
So we tend to think that linear aggressions are more interpretable and easier to control

592
00:43:37,600 --> 00:43:43,320
and govern than deep neural networks, because it's easier for us to align, you know,

593
00:43:43,320 --> 00:43:49,640
which input, which feature, with which weight is correlated to which output, right?

594
00:43:49,640 --> 00:43:52,600
And we say, okay, we feel like we have control over that, that's interpretable.

595
00:43:52,600 --> 00:43:55,480
What I think is happening there, and it's a risk that we haven't really thought enough

596
00:43:55,480 --> 00:44:02,440
about, is that we're actually displacing the lack of transparency from the machine to

597
00:44:02,440 --> 00:44:03,440
the person's mind.

598
00:44:03,440 --> 00:44:09,600
So when a scientist and practitioner comes in and analyzes the data and decides, you know,

599
00:44:09,600 --> 00:44:16,160
which features to focus on, and when they're building out their model, there's lots of

600
00:44:16,160 --> 00:44:21,640
opportunity there for bias of some sort to inflect, you know, which feature ends up

601
00:44:21,640 --> 00:44:22,640
end up being featured.

602
00:44:22,640 --> 00:44:24,280
I'm sorry, a little bit of fun.

603
00:44:24,280 --> 00:44:25,280
Which features matter?

604
00:44:25,280 --> 00:44:33,080
Part of that will be from analytic observation, you know, it will live within the data, but

605
00:44:33,080 --> 00:44:41,240
there's just so much room, you know, for human bias to come in and be shaping the choices

606
00:44:41,240 --> 00:44:46,440
that are made in selection of features without being aware of that.

607
00:44:46,440 --> 00:44:51,800
And so that's, like for me, that's sort of this, it's an overlooked aspect of data processing

608
00:44:51,800 --> 00:44:55,840
that makes us think a little bit more critically about this whole notion of sort of transparency

609
00:44:55,840 --> 00:44:56,840
and interpretability.

610
00:44:56,840 --> 00:45:01,480
And it's one way you have to sort of gut check yourself and be like, you know, are we

611
00:45:01,480 --> 00:45:03,840
imposing our biases here?

612
00:45:03,840 --> 00:45:06,520
The other thing I talk about, and I think this could be a podcast on its own, has to

613
00:45:06,520 --> 00:45:11,920
do with some of the ways to think about data privacy and a machine learning first world

614
00:45:11,920 --> 00:45:16,280
where we're sort of shifting from protecting a data point.

615
00:45:16,280 --> 00:45:20,600
So personally identifiable information in the form of my name, my address, my social security

616
00:45:20,600 --> 00:45:25,480
number to obfuscating an individual within a distribution.

617
00:45:25,480 --> 00:45:29,040
So if you think about sort of, you know, we've got 15 people in a room.

618
00:45:29,040 --> 00:45:30,040
How tall is everybody?

619
00:45:30,040 --> 00:45:31,040
What's the average height?

620
00:45:31,040 --> 00:45:37,120
It would be, you know, the extent to which one individual is contributing to that mean

621
00:45:37,120 --> 00:45:38,720
and the standard deviation from the mean.

622
00:45:38,720 --> 00:45:43,040
And so this is the realm of sort of differential privacy in ways in which we can make it so

623
00:45:43,040 --> 00:45:46,760
that you can't reverse engineer an individual from sort of a large distribution.

624
00:45:46,760 --> 00:45:49,120
So that's, there's a whole section in there on that.

625
00:45:49,120 --> 00:45:52,440
But again, I think it's a topic for another podcast.

626
00:45:52,440 --> 00:46:00,160
And talking about the way we feature our features, you suggested that one of the dangers

627
00:46:00,160 --> 00:46:07,720
is imparting our biases into the models by the way we do feature engineering and the

628
00:46:07,720 --> 00:46:08,720
like.

629
00:46:08,720 --> 00:46:14,360
But it also prompted this question for me around, you know, is there this other paradox

630
00:46:14,360 --> 00:46:19,440
that, you know, there's this class of models that we think of as transparent and so we

631
00:46:19,440 --> 00:46:21,240
don't question them as much.

632
00:46:21,240 --> 00:46:25,640
Whereas there's this other class of models that we think of as opaque and we spend a lot

633
00:46:25,640 --> 00:46:27,480
of energy trying to understand them.

634
00:46:27,480 --> 00:46:29,480
Is there a risk there?

635
00:46:29,480 --> 00:46:30,480
I think so.

636
00:46:30,480 --> 00:46:35,800
I think it goes back to, it's almost like the Nick Bostrom point where it's the risk

637
00:46:35,800 --> 00:46:42,480
of focusing a lot of our attention on, on, on the one area that's been sort of talked

638
00:46:42,480 --> 00:46:46,640
about like that's been discursively accepted as like, this is where the wrist lies.

639
00:46:46,640 --> 00:46:50,760
And people should be working on that like the, all of the work that's going into interpretability

640
00:46:50,760 --> 00:46:54,960
of neural networks is incredibly exciting and important work.

641
00:46:54,960 --> 00:47:00,480
However, I think there's a, there's a larger point that relates to even sort of the social

642
00:47:00,480 --> 00:47:05,200
stratus of and the social like importance of different type classes of algorithms in

643
00:47:05,200 --> 00:47:06,360
the community.

644
00:47:06,360 --> 00:47:12,560
That doesn't mean that linear regressions, logistic regressions, support vector machines don't

645
00:47:12,560 --> 00:47:15,440
have their ethical issues too.

646
00:47:15,440 --> 00:47:21,280
And it also doesn't mean that they're not worthy mathematical functions, you know, for

647
00:47:21,280 --> 00:47:22,280
systems.

648
00:47:22,280 --> 00:47:26,680
And at Fastword Labs, we always used to say, you know, when you're applying ML, start

649
00:47:26,680 --> 00:47:31,040
with the simplest possible algorithm possible to solve your problem that can scale.

650
00:47:31,040 --> 00:47:34,920
And then once you've gotten baseline performance, then maybe experiment up with sort of some

651
00:47:34,920 --> 00:47:38,320
of the more complex models that can process hierarchical features and represent the data

652
00:47:38,320 --> 00:47:39,320
a little differently.

653
00:47:39,320 --> 00:47:44,840
But, you know, don't start with the hammer when all that you need is sort of a chisel.

654
00:47:44,840 --> 00:47:50,920
So yeah, so I think that, I think there's a larger sociological point wrapped up in your

655
00:47:50,920 --> 00:47:58,400
question as it relates to the sort of hierarchy, sometimes of perceived value in the scientific

656
00:47:58,400 --> 00:48:03,320
and engineering community, that, you know, where, and actually, and this shows up in the

657
00:48:03,320 --> 00:48:09,280
paper, in my experience, finding a lawyer, people talk a lot about the sort of scarcity

658
00:48:09,280 --> 00:48:14,200
of PhDs in machine learning, finding a lawyer who knows a lot about the law and also knows

659
00:48:14,200 --> 00:48:18,920
a lot about how machine learning systems work so that they can pave new ground in thinking

660
00:48:18,920 --> 00:48:25,000
about how regulations should evolve or how compliance practices should evolve to encompass,

661
00:48:25,000 --> 00:48:29,520
like the realities of new technical problems at a deep level and not just to sort of a conceptual

662
00:48:29,520 --> 00:48:30,520
level.

663
00:48:30,520 --> 00:48:33,120
Those people are really hard to find.

664
00:48:33,120 --> 00:48:37,120
And I feel like they're heroes whose song needs to be sung a little more, you know?

665
00:48:37,120 --> 00:48:43,600
So in the next step in your framework, which looks at model prototyping and QA testing,

666
00:48:43,600 --> 00:48:52,640
does this go beyond the model evaluation, the, I guess, the statistical aspects of modeling

667
00:48:52,640 --> 00:48:54,440
or is that the focus?

668
00:48:54,440 --> 00:48:56,640
A lot of it is around the statistics.

669
00:48:56,640 --> 00:49:01,440
So Q&A, you know, is this basically testing control, you know, having your test, having

670
00:49:01,440 --> 00:49:03,920
your validation set, seeing how the model performs.

671
00:49:03,920 --> 00:49:09,280
And I think the particular ethical aspect of it has to do with subsampling areas of

672
00:49:09,280 --> 00:49:10,280
the model.

673
00:49:10,280 --> 00:49:14,000
I mean, sort of holistic, you know, how is this performing, but rather are there pockets

674
00:49:14,000 --> 00:49:19,640
of a population where the model seems to be performing well at the, you know, sort of

675
00:49:19,640 --> 00:49:23,520
at the, for the majority group, but if you look at the minority, the performance is opposite.

676
00:49:23,520 --> 00:49:25,800
So this is often referred to as Simpson's paradox.

677
00:49:25,800 --> 00:49:33,000
So it's sort of a particular fine grained comb on Q&A with an eye towards potential fairness

678
00:49:33,000 --> 00:49:34,760
and bias pitfalls.

679
00:49:34,760 --> 00:49:38,920
And then some of the other, one of the other things we bring up in the Q&A testing are inspired

680
00:49:38,920 --> 00:49:42,560
by some of the work that Ian Goodfellow, who I know has been on the podcast, has done

681
00:49:42,560 --> 00:49:48,760
as it relates to basically tricking a model, a malicious actor, tricking a model into thinking,

682
00:49:48,760 --> 00:49:57,720
well, thinking, metaphorical term, into not from a QA tester's vantage point, seeming

683
00:49:57,720 --> 00:50:02,760
to perform well according to the standards that were, you know, measuring the model's

684
00:50:02,760 --> 00:50:07,040
performance by when actually it's been tricked into doing a misclassification or doing

685
00:50:07,040 --> 00:50:09,760
something, you know, or having the wrong output.

686
00:50:09,760 --> 00:50:10,760
Yeah.

687
00:50:10,760 --> 00:50:16,120
First point makes me think of kind of ethical unit tests for models.

688
00:50:16,120 --> 00:50:21,880
Have you come across a scalable framework for doing that or does it not require anything

689
00:50:21,880 --> 00:50:22,880
special?

690
00:50:22,880 --> 00:50:26,520
I personally haven't seen it, but I wouldn't be surprised if someone's working on

691
00:50:26,520 --> 00:50:27,520
that.

692
00:50:27,520 --> 00:50:28,520
And I like, I like the term.

693
00:50:28,520 --> 00:50:29,520
You should.

694
00:50:29,520 --> 00:50:31,520
I like that.

695
00:50:31,520 --> 00:50:36,520
I know we're running out of time, but the sixth step in the framework is deployment

696
00:50:36,520 --> 00:50:37,520
monitoring and maintenance.

697
00:50:37,520 --> 00:50:39,720
We've talked about it a little bit already.

698
00:50:39,720 --> 00:50:41,320
Anything you want to add there?

699
00:50:41,320 --> 00:50:46,320
I'd say the only, we've talked about a lot, you know, advice folks to have a look at

700
00:50:46,320 --> 00:50:48,400
the paper if they're interested in it.

701
00:50:48,400 --> 00:50:52,920
And then I guess the one thing has to do documentation, so not just sort of the, you know, the auditing

702
00:50:52,920 --> 00:50:57,400
and logging controls in the system itself, as well as all the proper DevOps on the tool,

703
00:50:57,400 --> 00:51:02,880
but also the, you know, the hard work of documenting what choices were made when, why.

704
00:51:02,880 --> 00:51:07,280
And there's questions around whether or not that's a sort of internal technical compliance

705
00:51:07,280 --> 00:51:12,240
tool, internal sort of cross-functional compliance tool, or also could be something that is presented

706
00:51:12,240 --> 00:51:16,720
then to end users, so that they, you know, in a friendly, with friendly language, so that

707
00:51:16,720 --> 00:51:21,720
if they have questions around the transparency of a system, you know, they, there's, there's

708
00:51:21,720 --> 00:51:24,880
communication on how frequently the model is updated.

709
00:51:24,880 --> 00:51:29,320
If it's possible to remove, you know, somebody decides to opt out of a system.

710
00:51:29,320 --> 00:51:35,040
The retraining cycles for the model's entail and sort of a guarantee, there's a section

711
00:51:35,040 --> 00:51:38,840
in the paper where we say, well, what if you were to get a confirmation if you opt out

712
00:51:38,840 --> 00:51:43,160
of some data collection mechanism in a machine learning algorithm associated with it that's

713
00:51:43,160 --> 00:51:47,880
like, model's been retrained, you're removed, you're no longer part of this, you know, you're,

714
00:51:47,880 --> 00:51:50,680
you've been forgotten by the system.

715
00:51:50,680 --> 00:51:54,680
That might be overly cumbersome, but it's, you know, something we propose as food for

716
00:51:54,680 --> 00:51:55,680
thought.

717
00:51:55,680 --> 00:51:59,480
Can I talk quite a bit about the paper? Is it easy for folks to find? I just pulled

718
00:51:59,480 --> 00:52:04,920
up the integrate.ai site and it didn't jump out at me. Maybe we can set up a place where

719
00:52:04,920 --> 00:52:07,360
people can kind of easily go to grab it.

720
00:52:07,360 --> 00:52:12,440
Yeah, you bet. If you go on the integrated iSight, there's a, there's a, a tool like a,

721
00:52:12,440 --> 00:52:17,160
a button or whatever called trust, and it's under that trust button and there's a page

722
00:52:17,160 --> 00:52:22,760
for a monthly i, but we can definitely find a way to make it a little bit easier to find.

723
00:52:22,760 --> 00:52:27,120
Because there's a, there's a website that has, like, devoted to it in particular or some

724
00:52:27,120 --> 00:52:31,400
page on our website, but it's accessible through the integrate.ai website under the trust

725
00:52:31,400 --> 00:52:32,400
tab.

726
00:52:32,400 --> 00:52:39,640
Okay. Got it. I see it. Easy enough, any kind of final thoughts or words of wisdom?

727
00:52:39,640 --> 00:52:46,760
Yeah, it's just to sum things up. You know, there's so much attention paid to ethics

728
00:52:46,760 --> 00:52:53,360
these days. I think it's wonderful that we're having large discussions around AI and society

729
00:52:53,360 --> 00:52:59,640
and that this technology is, is encouraging us to question our values so holistically.

730
00:52:59,640 --> 00:53:05,560
However, as we started off, there's, there's still so much work to do in taking this from

731
00:53:05,560 --> 00:53:09,920
sort of the philosophy, the domain of philosophy and the domain of fun conversations that dinner

732
00:53:09,920 --> 00:53:15,600
parties into practical, operational decisions. And, you know, the, what we were trying to do

733
00:53:15,600 --> 00:53:20,960
here is to, is to show that often ethics occurs in the trenches, right? It's like it's the

734
00:53:20,960 --> 00:53:26,480
decisions that teams make in their building systems. They're hard to make and, and they

735
00:53:26,480 --> 00:53:31,440
require accountability and decision rights. And it's within that tension that we think

736
00:53:31,440 --> 00:53:37,360
that we can really do some work to change things. So, so the call to action is to, is to

737
00:53:37,360 --> 00:53:40,760
not only talk about these issues, but to think really critically about how to put them

738
00:53:40,760 --> 00:53:41,760
into practice.

739
00:53:41,760 --> 00:53:46,640
Well, Katherine, thank you so much. Always wonderful to speak with you. We need to find

740
00:53:46,640 --> 00:53:50,000
a way to do it more often. Thanks for being on the show.

741
00:53:50,000 --> 00:53:53,000
Totally concur. It's great to talk to you, Sam.

742
00:53:53,000 --> 00:54:01,000
All right, everyone. That's our show for today for more information on Katherine or any

743
00:54:01,000 --> 00:54:07,640
of the topics covered in this episode. Visit twomla.com slash talks slash 210. Thanks once

744
00:54:07,640 --> 00:54:12,280
again to the great folks over at Georgian Partners for their sponsorship of this series. Be sure

745
00:54:12,280 --> 00:54:18,160
to head over to twomla.com slash Georgian for more information on their building conversational

746
00:54:18,160 --> 00:54:46,160
AI teams guidebook. As always, thanks so much for listening and catch you next time.

