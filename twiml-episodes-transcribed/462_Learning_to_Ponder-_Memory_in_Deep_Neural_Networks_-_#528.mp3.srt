1
00:00:00,000 --> 00:00:16,640
All right, everyone. I am here with Andrea Benino. Andrea is a research scientist at DeepMind.

2
00:00:17,200 --> 00:00:23,120
Andrea, welcome to the Twomol AI podcast. Thanks, Sam. Thanks for having me. So you are

3
00:00:23,120 --> 00:00:30,720
working on artificial general intelligence at DeepMinds. Tell us a little bit about where your

4
00:00:30,720 --> 00:00:37,200
interest in AGI comes from. My interest in AGI comes from my background. I'm actually a neuroscientist

5
00:00:38,320 --> 00:00:45,520
who decided to study how the brain works and in particular memory, how we create our memory,

6
00:00:45,520 --> 00:00:55,120
why we create our memory. And in particular, I'm interested in a subset of the memory field,

7
00:00:55,120 --> 00:01:03,600
which is called a episodic memory. So episodic memory, those memory that relates to some

8
00:01:04,720 --> 00:01:12,400
episode that you experience in the past in a specific location, sometimes, or with specific people.

9
00:01:12,400 --> 00:01:18,560
So they are also referred as autobiographical memories.

10
00:01:18,560 --> 00:01:23,280
An episodic is in contrast to what other kinds of memory?

11
00:01:23,280 --> 00:01:33,360
Another example could be semantic memory. So semantic is like, you know, the meaning of something,

12
00:01:33,360 --> 00:01:39,680
for example, you know, what is a bicycle that could be a semantic memory. But if I ask you to remind

13
00:01:39,680 --> 00:01:48,880
you to remember a specific when when you when you cycle with someone, then maybe now you're

14
00:01:48,880 --> 00:01:53,680
thinking about a specific episode in your life that would be an episodic memory. So you relate

15
00:01:54,400 --> 00:02:05,120
something to a specific moment in time. It's also described as the what when and where of a specific

16
00:02:05,120 --> 00:02:14,160
it went. To some degree, the relationship between memory and intelligence is kind of an obvious one

17
00:02:14,160 --> 00:02:24,080
in the sense that, you know, we we use our memory and our prior experiences and interacting with

18
00:02:24,080 --> 00:02:33,200
the world, making decisions and all that. But is there a kind of broader significance to memory

19
00:02:33,200 --> 00:02:42,160
and the development of AGI? Yeah, I think as you say, it's kind of an obvious one, right? So

20
00:02:42,160 --> 00:02:46,960
we live in a world that is consistent. So if we gain some experience in the world, then we want

21
00:02:46,960 --> 00:02:52,800
to reuse that experience if we don't want to relearn every time from scratch. So you can already

22
00:02:52,800 --> 00:02:59,200
see how that's viable. But something that I actually study over my PhD is that episodic memory,

23
00:02:59,200 --> 00:03:06,480
it's also way to so it's something that enables generalization. In particular, we study how

24
00:03:07,920 --> 00:03:14,000
let's say if you have if you experience two events A and B that are related together. And later

25
00:03:14,000 --> 00:03:20,240
in time, you experience other two events like B and C that are again related together. You are

26
00:03:20,240 --> 00:03:27,200
brain without you doing any effort directly relate A to C together, such that you relate,

27
00:03:27,200 --> 00:03:32,800
you do this kind of inference through your episodic memory. So if you want to give like if I can tell

28
00:03:32,800 --> 00:03:42,480
you like a more precise example would be like, if you see someone going out with a dog in the

29
00:03:42,480 --> 00:03:48,720
morning, and then you see the same dog with a different person in the afternoon, immediately your

30
00:03:48,720 --> 00:03:53,680
brain is going to try to connect the two people that we are going out with the dog. And that's the

31
00:03:53,680 --> 00:04:01,120
kind of inference that episodic memory we know support. So that's that's quite important, right?

32
00:04:02,160 --> 00:04:13,680
Right, right. And what is the the history of memory, you know, in this effort to kind of get us

33
00:04:13,680 --> 00:04:23,120
closer to AGI? How have how have we used memory in the field to facilitate intelligence? Okay, so

34
00:04:23,760 --> 00:04:28,320
this is I think it's a nice question because I think in some sense the kind of memory I'm talking

35
00:04:28,320 --> 00:04:37,600
about is still an open question. How we do it properly in in AI? We have different forms of memory

36
00:04:37,600 --> 00:04:43,520
currently, which we know how to to play with in particular recurrent networks both RNN and the

37
00:04:43,520 --> 00:04:50,720
STM are so-called working memory. So it's kind that gives you like imagine a white white board where

38
00:04:50,720 --> 00:04:56,400
you can write something and reason about that and then erase because it won't stay there forever.

39
00:04:56,400 --> 00:05:04,000
It's like kind of a canvas where you can you can make predictions. That's that's what we know.

40
00:05:04,000 --> 00:05:11,680
Then we have also something called memory augmented neural networks. In that case we we basically give

41
00:05:11,680 --> 00:05:20,560
a neural networks an external memory where they can write a previous computation that they performed

42
00:05:20,560 --> 00:05:26,320
and then read back from there and reuse previous computation and this gets a little bit closer to

43
00:05:26,320 --> 00:05:34,560
the kind of memory I hinted before but we are not there yet and then we have retrieval augmented

44
00:05:34,560 --> 00:05:40,320
models which are those models that basically go back in a if you like in a table where we store

45
00:05:41,040 --> 00:05:47,200
almost everything we have seen in the past like a dictionary if you like and then they try to

46
00:05:47,200 --> 00:05:53,280
look up things but most of the time they look up and they they they use that look up to answer

47
00:05:53,280 --> 00:05:59,440
but they don't consolidate back the knowledge into the into the weights if you like of the system

48
00:05:59,440 --> 00:06:05,760
so they need to do the look up all the time which in some sense it's a waste we don't do as

49
00:06:05,760 --> 00:06:11,440
as I said before we don't do that we kind of make sense of what we are to and use it later.

50
00:06:12,480 --> 00:06:18,960
Right so in that lot of case you can think of it as kind of this fixed boundary between the memory

51
00:06:18,960 --> 00:06:29,200
and the computation in a sense and the computation isn't ever updated the way we think about or

52
00:06:29,200 --> 00:06:35,920
the way the model thinks about its inputs is never updated with regard to the things that it

53
00:06:35,920 --> 00:06:40,560
learns in a memory it's just checking the memory constantly with each each input. Yeah sometimes

54
00:06:40,560 --> 00:06:46,480
there are models for I can I that comes now to my mind a model where that computation is updated

55
00:06:46,480 --> 00:06:52,960
but it's very difficult to scale up those models because they require a lot of computation so we

56
00:06:52,960 --> 00:06:58,000
don't know yet how to make that model big to a scale where they're actually useful to target

57
00:06:58,000 --> 00:07:04,720
a very complex problem and also we have finally the last one will be a very recent

58
00:07:05,360 --> 00:07:11,440
advancement in in AI which is just something that everyone knows I'm sure transformer

59
00:07:11,440 --> 00:07:19,680
this model have shown some of the properties of this kind of memory in language there are what

60
00:07:19,680 --> 00:07:26,960
the sometimes they show a few short learning which is a prerogative of like the memory models I'm

61
00:07:26,960 --> 00:07:31,920
talking about but that happens in language only not in other domains so I'm not sure if it's

62
00:07:32,800 --> 00:07:37,120
big that then that raised the question it's because of the model capable of doing that or it's

63
00:07:37,120 --> 00:07:46,000
because of this specific domain where you train the model that allows that so no one has done that

64
00:07:46,000 --> 00:07:52,560
experiment yet but I think it could be interesting and can you drill down on that how should we

65
00:07:52,560 --> 00:07:59,920
think about transformers from a memory perspective and exhibiting these properties that you

66
00:07:59,920 --> 00:08:04,960
mentioned first of all I think most of the transformer that we use now in language

67
00:08:04,960 --> 00:08:10,720
transformer excel and it's a specific instantiation of transformer whereby you add

68
00:08:11,760 --> 00:08:17,760
a memory and a sort of external memory at each layer of the transformer so we already know

69
00:08:17,760 --> 00:08:23,440
that that's necessary in some sense to overcome some of the limits like not having recurrence for instance

70
00:08:26,640 --> 00:08:32,720
so I would say they are limited in terms of memory also because given that we do this

71
00:08:32,720 --> 00:08:39,920
all to all comparison over the sequence we cannot process very long context there are now papers

72
00:08:39,920 --> 00:08:46,800
trying to deal with that and reduce the complexity to to linear size but still I don't think

73
00:08:46,800 --> 00:08:53,600
we are at the point where let's say we can process several books and ask inference questions

74
00:08:53,600 --> 00:09:01,440
about books without like do like this sort of external retrieval all the time you know oftentimes

75
00:09:01,440 --> 00:09:06,720
there are ideas that we might want to apply in the context of neural networks and a big challenge

76
00:09:06,720 --> 00:09:12,160
is yeah are they differentiable you know so that we can apply techniques like gradient descent

77
00:09:13,760 --> 00:09:20,880
is is memory you know typically differentiable is it typically undifferentiable and like how does

78
00:09:20,880 --> 00:09:28,960
how does that plan to you touch the perfect the perfect downside of like these kind of external

79
00:09:28,960 --> 00:09:36,400
memory where we do what is called k-nears neighbor retrieval and that's a non-differentiable operation

80
00:09:37,760 --> 00:09:47,760
so that obviously has some limitations on on then what you can get and again I don't think

81
00:09:47,760 --> 00:09:54,720
with the current system we know how to backprop or to do backpropagation of a very large memory

82
00:09:54,720 --> 00:10:02,480
which again it's an open problem I think it's a very nice problem to tackle and we should probably

83
00:10:02,480 --> 00:10:09,360
stop now I think we might be able to do it where what is this how does the size of the memory play

84
00:10:09,360 --> 00:10:16,080
into whether we can backprop over it or not because most of the time you use a soft max operation

85
00:10:16,080 --> 00:10:27,040
over the dimension of the memory and you know it becomes quickly impractical to send gradients

86
00:10:27,040 --> 00:10:35,280
over a very large large memory and that's because it's computationally infeasible so it doesn't fit

87
00:10:35,280 --> 00:10:43,920
in memory and also when you get with very large soft maxes also you have problem with gradients

88
00:10:43,920 --> 00:10:52,160
there's another aspect of memory that comes to mind you know we've talked about specific

89
00:10:52,800 --> 00:11:04,000
kind of features and you know architectures that are used to emulate memory but oftentimes one

90
00:11:04,000 --> 00:11:09,920
of the critiques of deep learning is that the networks themselves like remember stuff and you know

91
00:11:09,920 --> 00:11:15,360
that becomes a problem it causes problems with generalization are there ways that

92
00:11:17,120 --> 00:11:24,160
that can be harnessed more directly to achieve some of your goals for episodic memory

93
00:11:25,920 --> 00:11:34,960
yeah okay yes memorization it's definitely an issue like overfitting it's it's it's an issue although

94
00:11:34,960 --> 00:11:48,800
yeah most of what we do as well is like kind of memory so we enlarge our our data set of experience

95
00:11:48,800 --> 00:11:57,280
as we grow right so in some sense we tend to overfeed as well in most of what we do so I don't

96
00:11:57,280 --> 00:12:02,800
think that's that's a huge problem that's also there was actually a nice paper last year called

97
00:12:02,800 --> 00:12:11,040
direct feed to nature which I recommend your listen to to look up from Williamson which basically

98
00:12:11,760 --> 00:12:19,040
poses these the same question you are posing to me and it basically answers in this way so we

99
00:12:19,040 --> 00:12:26,160
we as we grow as we have grown as a species through evolution and also doing our life we kept

100
00:12:26,160 --> 00:12:33,120
enlarging the kind of the the amount of experience that we used to grow our our our brain our network

101
00:12:33,760 --> 00:12:38,000
so in some sense we kept memorizing it's just that we don't know where we start from scratch

102
00:12:38,640 --> 00:12:45,120
and also we have this ability of generalizes that I think it's still a little bit missing from

103
00:12:45,120 --> 00:12:49,840
from deep learning although we shouldn't we should recognize this this generalization

104
00:12:50,720 --> 00:12:55,200
is not that we can generalize to everything right so we have in some sense limited as well

105
00:12:55,200 --> 00:13:01,120
in generalization maybe neural networks are slightly more limited than us but we still we already

106
00:13:01,120 --> 00:13:08,080
see some example of generalization which are starting to to emerge in neural networks and I think

107
00:13:08,080 --> 00:13:12,880
that's a good thing and something we cannot neglect so for instance I had a paper couple of

108
00:13:13,680 --> 00:13:18,640
in two thousand eighty so three years ago in nature where we actually use

109
00:13:18,640 --> 00:13:25,680
and we and view the neural network with representation like the one like the one we have in the brain

110
00:13:25,680 --> 00:13:33,920
in the apocampus and that way the agent was able to travel that was an obligation task the agent

111
00:13:33,920 --> 00:13:39,280
was able to take shortcut and traverse part of the environment that was previously blocked

112
00:13:40,160 --> 00:13:44,960
and the agent was able to do that with the right representation so I don't think the problem is

113
00:13:44,960 --> 00:13:51,520
the back propagation of the model themselves it might be a problem how we train that in terms of

114
00:13:51,520 --> 00:13:58,400
both data we we use in the representation that we force to emerge you mentioned this paper

115
00:13:59,600 --> 00:14:07,200
that implemented something akin to the representation that's used in the apocampus what is that

116
00:14:07,200 --> 00:14:15,680
representation how does how does that work and how the paper we had these we studied these

117
00:14:15,680 --> 00:14:21,840
these things called grid cell so in the apocampus grid cells yes so in the apocampus we have

118
00:14:22,400 --> 00:14:27,280
so it's kind it's a memory machine of the brain but it's also the spatial machine right you can

119
00:14:27,280 --> 00:14:34,080
see also spatial as memory but that's not going to that that's how I feel that I don't want to go

120
00:14:34,080 --> 00:14:41,680
into but basically we have the two cells that are probably the three cells that are more known

121
00:14:42,240 --> 00:14:48,880
like head direction cell so those are cell that fires every time you look in a certain direction

122
00:14:48,880 --> 00:14:54,000
but I'm talking about allocentric direction so every time you face north there will be a cell

123
00:14:54,000 --> 00:14:59,840
firing with a certain probability distribution over north and the same for the other the other

124
00:14:59,840 --> 00:15:06,720
allocentric direction and the north is not the cardinal north the north is the let's say is the

125
00:15:06,720 --> 00:15:14,160
one in with which relate to a certain reference point in the environment so let's say okay

126
00:15:15,840 --> 00:15:20,720
then we have play cell those are neurons that fires every time you are in a particular location

127
00:15:22,320 --> 00:15:27,280
independently aware you're looking at and then we have grid cells which are these

128
00:15:27,280 --> 00:15:34,640
visually and mathematically beautiful cells that basically fires following an hexagonal lattice

129
00:15:36,240 --> 00:15:44,000
they have a 60 degree offset and they are very beautiful and no and there have been several theories

130
00:15:44,000 --> 00:15:51,520
that try to motivate the reason why we have that and one of these was because we can basically

131
00:15:51,520 --> 00:15:56,800
we can use that to calculate shortcut to calculate the shortest vector between two points

132
00:15:58,240 --> 00:16:04,560
and we manage to do two things in that paper first of all we manage to make the representation

133
00:16:04,560 --> 00:16:09,840
emerge in our neural network trained to do path integration so to do our navigation task

134
00:16:09,840 --> 00:16:15,120
and secondly we use these those representations in our in our enforcement learning agent

135
00:16:15,120 --> 00:16:21,680
and we prove through through several ablation as well that the world that was the only agent

136
00:16:21,680 --> 00:16:27,040
able to take shortcut and if we lesion let's say some grid cell the ability of the agent to take

137
00:16:27,040 --> 00:16:34,400
shortcut just wet down so it was kind of an empirical paper to prove what the grid cell are for

138
00:16:36,080 --> 00:16:43,680
is it is the idea in terms of the implementation that i'm imagining you're like adding

139
00:16:43,680 --> 00:16:50,880
signing cosine elements to your loss function or something like that no no no no no no no no no no it's

140
00:16:50,880 --> 00:16:57,040
data driven that our goal was really to be super data driven and we achieved we achieved that by

141
00:16:57,040 --> 00:17:04,800
actually a specific arc it was a recurring architecture two things were really important one was

142
00:17:04,800 --> 00:17:11,360
to introduce a dropout such that no or not all yours were able to fire the same time and the

143
00:17:11,360 --> 00:17:18,320
second one was introduced noise in the gradients and that basically helped if those are two things

144
00:17:18,320 --> 00:17:28,160
we always want to do yeah yeah particularly uh specific to this problem that's that's that's

145
00:17:28,160 --> 00:17:32,400
probably why people like so much that kind of paper because it was kind of a general approach

146
00:17:32,400 --> 00:17:38,480
to to make that one of the although i have to say it was kind of difficult to analyze but

147
00:17:38,480 --> 00:17:46,000
you know one of the reason why noise helps because it helps you moving away from certain solution

148
00:17:46,000 --> 00:17:52,960
in the landscape of the loss and our our our our way to work was to help the network

149
00:17:53,840 --> 00:17:59,600
going down the solution we were we liked so grid cell but that wasn't too difficult actually

150
00:17:59,600 --> 00:18:09,920
to achieve okay that's why i think it it was a nice piece of work yeah interesting interesting um

151
00:18:10,400 --> 00:18:17,760
so we were i'm trying to speak a memory i'm trying to know how we got here actually

152
00:18:17,760 --> 00:18:22,560
in a little the ponder stuff i guess you invited me to talk about the ponder the ponder

153
00:18:22,560 --> 00:18:28,400
the ponder anything inspired by memory exactly from the study that i mentioned at the beginning

154
00:18:28,400 --> 00:18:36,320
the this thing about related doing inference associative inference that was part of my PhD and

155
00:18:37,440 --> 00:18:42,800
one of the paper that came out during that time was disability of the hippocampus to basically

156
00:18:43,920 --> 00:18:50,480
do this recursive ponder before actually being able to do this sort of inference so you see people

157
00:18:50,480 --> 00:18:55,520
when you ask people people when you ask i remember doing this experiment practically with with

158
00:18:55,520 --> 00:19:01,040
people and when when you ask them to late a and c to tell your story about a c they think more

159
00:19:01,040 --> 00:19:09,440
they really spend more time in thinking compared to a b and b c and that's and that's that's

160
00:19:09,440 --> 00:19:14,720
how i got inspired because you know our brain works so the same mechanism because when we did

161
00:19:14,720 --> 00:19:20,560
then FMI study right the same mechanism was involved it's just that the the answer was going out

162
00:19:20,560 --> 00:19:27,200
of the apocampus and then back into the apocampus few times and but then was processed by the same

163
00:19:28,240 --> 00:19:34,960
system if you like and i think that's a nice probability to have in an angry then

164
00:19:36,240 --> 00:19:43,280
does that mean i'm probably you know taking this too far but you know when i hear you say that i

165
00:19:43,280 --> 00:19:50,880
i make associations like the you know the brain knows how to do scanning and it doesn't have

166
00:19:50,880 --> 00:20:05,760
something like an inner join okay no the the problem there is that i what does it mean the brain

167
00:20:05,760 --> 00:20:11,120
no i guess from a computational perspective then i could ask you what's the loss that the brain

168
00:20:11,120 --> 00:20:16,320
is minimizing then to do that which i don't know i don't know the answer maybe i don't know maybe

169
00:20:16,320 --> 00:20:21,280
it could be uncertainty reduction because at the end of the day we want to get better prediction we

170
00:20:21,280 --> 00:20:27,440
want to be able to i think better predict the model the world because it's then it will be less

171
00:20:27,440 --> 00:20:32,400
uncertain and so less risky but i think there are people like much better than me that could explain

172
00:20:32,400 --> 00:20:36,800
that yeah yeah no i think you that was going in maybe a slightly different direction i was

173
00:20:36,800 --> 00:20:47,520
inferring from the what i heard you say was that when we ask people to do these kind of associative

174
00:20:47,520 --> 00:20:55,920
types of tasks or inferences you know where they need to get from a to b and b to c you know that

175
00:20:55,920 --> 00:21:07,120
takes longer which kind of suggests that there's not some built-in associative thing in the brain no

176
00:21:07,120 --> 00:21:14,880
because in the in the fmi study that we did we saw that also if we do longer inference we are

177
00:21:14,880 --> 00:21:20,640
still able to do that it just takes more time even more time so i think that the algorithm we

178
00:21:20,640 --> 00:21:27,680
applies the same which is the ability of making associations is just that the longer the jump

179
00:21:27,680 --> 00:21:35,440
that i ask you to do in doing this sort of associative mechanism the more you need to do you do

180
00:21:35,440 --> 00:21:40,560
it a bit hierarchically right you put i don't know the things that are just two steps separated you

181
00:21:40,560 --> 00:21:45,120
calculate that then three steps and then you might be able to put together two and three and do five

182
00:21:45,120 --> 00:21:53,840
so we went down this particular rat hole in trying to provide some context for ponder net which

183
00:21:55,040 --> 00:22:02,240
you know i'll have you explain but i still don't really see the connection with memory when i

184
00:22:02,240 --> 00:22:11,360
when i read the abstract for ponder net i think about things like you know hyper parameter

185
00:22:11,360 --> 00:22:17,840
tuning and like early stopping like the way we train networks and like pulling that into the network

186
00:22:17,840 --> 00:22:23,440
as opposed to anything having to do with memory and the stuff we were just talking about yeah

187
00:22:23,440 --> 00:22:28,800
so tell me more about the connection two way answer on about the connection the first one is the

188
00:22:28,800 --> 00:22:37,600
more high level maybe and wavy if you like it's generalization so we believe that mechanism like

189
00:22:37,600 --> 00:22:45,360
ponder net that can get you a little bit far in terms of generalization compared to not doing

190
00:22:45,360 --> 00:22:52,400
ponder in deep neural networks and that's the kind of things that i we discussed before right so

191
00:22:52,400 --> 00:22:59,120
one of the benefit of memory is you're giving you the ability to to have the right input to generalize

192
00:22:59,120 --> 00:23:08,640
i guess let me put it this way the second is that a previous work to ponder net was another work

193
00:23:08,640 --> 00:23:14,800
i did during my PhD which was called memo and was a memory of mental neural network where we did

194
00:23:14,800 --> 00:23:22,240
exactly we studied this mechanism of recirculation so you you the network speed out an answer

195
00:23:22,240 --> 00:23:27,760
you compare it with previous answer and you only give the final answer to a certain problem when

196
00:23:27,760 --> 00:23:33,680
you are satisfied so you though you were already like already implementing this sort of ponder

197
00:23:33,680 --> 00:23:42,080
mechanism but there it was really really early stage because we use basic reinforcement learning

198
00:23:42,080 --> 00:23:50,400
to train to train a Bernoulli variable that was basically saying go stop and we saw that

199
00:23:50,400 --> 00:23:57,600
as was really hard to train and very noisy in terms of variance so we decided to do something more

200
00:23:57,600 --> 00:24:06,160
principled and that's that's what led us to to to ponder net okay so taking a step back what's the

201
00:24:06,160 --> 00:24:11,600
problem that you're trying to solve with ponder net yeah i think that's the first line of the

202
00:24:11,600 --> 00:24:18,320
abstract which basically says that the normally neural networks so the amount of computation that we

203
00:24:18,320 --> 00:24:24,480
spend in neural network grows with the size of the input but not with the complexity of the problem

204
00:24:25,680 --> 00:24:32,960
but as we just mentioned like few minutes ago that's not how we reason right the more complex

205
00:24:32,960 --> 00:24:39,920
the problem the more we spend time on it and that's that's what we wanted to get a session

206
00:24:40,480 --> 00:24:47,200
with this this work and also we wanted to to make it fairly general such that it could be applied

207
00:24:47,200 --> 00:24:54,080
to any architecture that was there to be architectural and mystic got it and so the size of the input

208
00:24:54,800 --> 00:24:59,200
we know what that means you were talking about like feature dimensionality yeah we know the

209
00:24:59,200 --> 00:25:05,280
problems that come along with that complexity of the problem what exactly does that mean and how do

210
00:25:05,280 --> 00:25:13,920
we measure that okay so i think empirically again we have like the the example i like in the

211
00:25:13,920 --> 00:25:22,080
paper is that it takes more to divide than to sound so that's exactly the same problem

212
00:25:22,080 --> 00:25:27,920
mathematically speaking but for some reason we spend more time dividing than doing some nation

213
00:25:27,920 --> 00:25:39,200
it's fair to say that we're we're talking about the computational complexity of a given problem

214
00:25:39,200 --> 00:25:47,440
um as opposed to some conceptual type of complexity yeah yeah yeah yeah so if you have to apply

215
00:25:47,440 --> 00:25:54,000
the same algorithm to uh so the algorithm is the same it's just that the specific

216
00:25:54,000 --> 00:25:59,280
instantiation where you have to apply it requires more more compute more compute time

217
00:26:00,560 --> 00:26:05,120
so it's like a computer right so it's the same the same you apply the same function but in

218
00:26:05,120 --> 00:26:11,040
some some circumstances it takes more time so the computer thinks more that's not how neural networks

219
00:26:11,040 --> 00:26:18,640
work it's they imply the same amount of thinking for each input i can give you another example in

220
00:26:18,640 --> 00:26:25,120
a sentence for instance when we read we don't spend the same amount of time of gazing each single

221
00:26:25,120 --> 00:26:31,360
world in a sentence so that's been proved in psychology so we tend to focus our attention

222
00:26:31,360 --> 00:26:36,640
few words in the sentence that are more important to process the whole thing so that's another

223
00:26:36,640 --> 00:26:47,200
that's another practical example okay okay um and so you want to create a neural network that

224
00:26:48,400 --> 00:26:55,920
um you know would you describe it as kind of budgets it's you know computational investment

225
00:26:55,920 --> 00:27:01,040
in solving a problem according to the inherent complexity of the problem is that a way to think of it

226
00:27:01,040 --> 00:27:07,760
yeah i would say it's a fair description and so how did you do that in ponder net

227
00:27:08,720 --> 00:27:15,200
okay so important first of all this is based on our previous work called adaptive computation time

228
00:27:16,480 --> 00:27:22,960
and the the problem there is that they are they they directly minimize the number of pondering

229
00:27:22,960 --> 00:27:33,040
step so the number of step the the network took whereas in our case what we did was to make

230
00:27:33,040 --> 00:27:40,960
this probabilistic so to be to be a slightly more practical on this for each time step in the

231
00:27:40,960 --> 00:27:49,680
sequence we calculate the prediction the probability of alting and the next step so the probability

232
00:27:49,680 --> 00:27:57,760
of alting is just a Bernoulli random variable which tells you the probability of alting at this

233
00:27:57,760 --> 00:28:07,360
particular step given that you have not altered the previous step and then from there what we can do

234
00:28:07,360 --> 00:28:15,120
then is to calculate a probability distribution by basically multiplying the probability at each

235
00:28:15,120 --> 00:28:23,760
time step in order to form a proper distribution a proper geometric distribution once we have that

236
00:28:23,760 --> 00:28:32,960
what we can do is to basically wait each single so we calculated the loss for each prediction in

237
00:28:32,960 --> 00:28:38,880
the sequence that we made and then the loss is then weighted by the probability of having

238
00:28:38,880 --> 00:28:45,120
altered that particular step and that's a critical difference between us and act because in act

239
00:28:45,760 --> 00:28:52,320
they instead output a weighted average of the prediction so they don't output a specific

240
00:28:52,320 --> 00:28:59,360
prediction but a weighted average of the whole prediction and that creates a bias integrated

241
00:28:59,360 --> 00:29:05,440
whereas in our case we can basically just really take the exact loss at each time step

242
00:29:05,440 --> 00:29:12,720
given given the decision of alting and then we take the particular training time we take just

243
00:29:12,720 --> 00:29:19,920
the particular step that has at threshold that basically so past the threshold that we decide for

244
00:29:19,920 --> 00:29:25,600
alting whereas at test time we just sample from the probability that from the probability distribution

245
00:29:25,600 --> 00:29:35,760
that we know okay I mentioned earlier that it calls to mind you're talking about holding here

246
00:29:35,760 --> 00:29:43,760
calls to mind early stopping is the idea that early stopping is kind of like you know we're trying

247
00:29:43,760 --> 00:29:49,440
to conserve training time is the idea here that we're trying to conserve inference time like we

248
00:29:49,440 --> 00:29:55,680
have exactly we're trying to make a prediction yeah instead of going through the whole thing let's keep

249
00:29:55,680 --> 00:30:00,560
going until we're sure what the answer is and then stop early and this is an approach to getting

250
00:30:00,560 --> 00:30:04,560
there yeah I think our hypothesis was that you know you can let's say you want to implement an

251
00:30:04,560 --> 00:30:10,320
algorithm on your phone you can train it in a you know in the cloud no problem a training time

252
00:30:10,320 --> 00:30:17,840
like the amount of commitment for a few people no problem but then at in fact it's time you want

253
00:30:17,840 --> 00:30:24,080
to be quick and that's and that's and that's important but now I'm laughing a little bit but

254
00:30:24,640 --> 00:30:30,000
you know if done proper I think this could also reducing the amount of resources that you

255
00:30:30,000 --> 00:30:34,560
spend a training time because we actually have an experiment in the paper where we see that

256
00:30:34,560 --> 00:30:40,880
the total number of gradient updates for pondernet as more than other methods

257
00:30:40,880 --> 00:30:49,040
even the same final performance so these could also help reducing the amount of resources

258
00:30:49,040 --> 00:30:57,840
that for certain people I guess okay and then circling back the the connection to memory here

259
00:30:57,840 --> 00:31:05,360
is it in storing these probabilities and the Bernoulli variables that kind of stuff or is there

260
00:31:05,360 --> 00:31:11,040
a different connection yeah I guess the we left the connection with memory maybe a little bit

261
00:31:11,040 --> 00:31:19,440
behind because okay however you might think I remember one tweet that actually also inspired

262
00:31:19,440 --> 00:31:24,400
these work a little bit from and I can't party that was basically say one of the limitation of

263
00:31:24,400 --> 00:31:28,800
transformer is actually they spend the amount the same amount of compute for each token in the

264
00:31:28,800 --> 00:31:36,960
sequence so if we treat broadly speaking like transformer has a form of preliminary memory right

265
00:31:36,960 --> 00:31:42,320
you can you can think of applying these on top of transformer and see if that could help right

266
00:31:42,320 --> 00:31:49,280
maybe spending different amount of compute per point in the sequence got it got it got it so

267
00:31:49,280 --> 00:32:00,160
I understand it's a bit stretchy but yeah it's it's it's kind of it's an analogy of some sort

268
00:32:00,160 --> 00:32:04,400
it's not necessarily an implementation of a memory system that we're talking about here

269
00:32:05,200 --> 00:32:10,800
is pondernet is a specific network architecture as opposed to a technique that you could apply

270
00:32:10,800 --> 00:32:17,520
to different architectures or is it the latter it's a technique so it's indeed if you see in the

271
00:32:17,520 --> 00:32:21,920
paper the step function what we call S in the paper could be anything could be an RNN

272
00:32:21,920 --> 00:32:29,360
a CNN of transformer okay on a real agent whatever as long as you return so as long as you

273
00:32:29,360 --> 00:32:35,280
add these extra unit that calculates the probability ofality you can apply it to everything that

274
00:32:35,280 --> 00:32:41,200
that was important for us and indeed in the paper we do that so we applied it three different

275
00:32:41,200 --> 00:32:51,680
architectures okay and how did you evaluate the results so we use one task from the ACT paper

276
00:32:51,680 --> 00:32:59,600
called the parity task so you have a string of of 1 and 0 or 1 and minus 1 whatever and you

277
00:32:59,600 --> 00:33:07,440
need to calculate the parity of that string could so could be either positive odd or even right

278
00:33:07,440 --> 00:33:13,200
and the good thing that we could that that's a nice task because you can also train on

279
00:33:14,640 --> 00:33:22,640
parity up let's say in our case up to 48 integers but then test up to twice as long the length

280
00:33:22,640 --> 00:33:28,480
to test these a bit of extrapolate and indeed we see that our network extrapolates much better than

281
00:33:28,480 --> 00:33:38,000
baselines the other methods then we applied it to a reasoning task which is called a Bobby and

282
00:33:38,000 --> 00:33:43,760
basically you have 20 tasks which you train all in parallel it's a language task where you get

283
00:33:43,760 --> 00:33:50,800
ask questions actually can I can I pause you and go back to parity I'm trying yeah yeah I'm

284
00:33:50,800 --> 00:33:56,000
trying to work this through my head like okay so I guess the first thought that occurred to me is

285
00:33:56,000 --> 00:34:02,880
it was some number of bits that you're trying to calculate the parity for it's not like you're

286
00:34:02,880 --> 00:34:08,400
trying to end the bits together and as soon as you hit a zero or something like that you know the

287
00:34:08,400 --> 00:34:14,000
answer right you still you need to look at all of the bits yes and the input for parity yeah but

288
00:34:14,880 --> 00:34:23,600
there's the premises that independent of that particular fact inside the network

289
00:34:23,600 --> 00:34:30,240
you can still stop early relative to going through some number of computations

290
00:34:31,680 --> 00:34:37,440
and still answering the right question the question correctly we have a baseline we pick this

291
00:34:37,440 --> 00:34:42,640
task exactly because we know this is a task that like a normal RNN is having trouble

292
00:34:42,640 --> 00:34:54,240
okay doing got it that's a that's a kind of a well-known issue in this sort of literature okay got

293
00:34:54,240 --> 00:35:04,240
it yeah on the topic of transformers which have come up a couple times you have you did a workshop

294
00:35:04,240 --> 00:35:14,400
at ICML talking about transformers and reinforcement learning can you talk about taking two topics

295
00:35:14,400 --> 00:35:20,000
that people are really excited about and combining them together tell us a little bit about the

296
00:35:20,000 --> 00:35:28,160
goals of that work so the I think I'm really got inspired by the birth work where they have

297
00:35:28,160 --> 00:35:39,280
their these known causal masking and you know in in RL we we keep using LSTM essentially for doing

298
00:35:39,280 --> 00:35:44,880
most of the task but we know that they suffer from what it's called emergency bias so they tend

299
00:35:44,880 --> 00:35:51,680
to pay attention only to the last bit of the sequence that they are trained on which to be honest

300
00:35:51,680 --> 00:35:59,200
for most of the environment that we we use nowadays it's fine but if you grow in size and the

301
00:35:59,200 --> 00:36:05,600
context you want to pay attention to is quite long then they start to suffer so one option

302
00:36:06,720 --> 00:36:12,480
would be to use transformer because we know that they can end a long term the long context longer

303
00:36:12,480 --> 00:36:21,360
context than LSTM however the problem in RL is that the reward are sparser most of the time and the

304
00:36:21,360 --> 00:36:28,400
gradient being shown to be noisy so it's difficult to train so many ways so what we did in that work

305
00:36:28,400 --> 00:36:35,840
was basically to to generalize the birth training to which you know is done on token so those are

306
00:36:35,840 --> 00:36:42,400
like categorical numbers so that you can apply softmax on the other side we generalize

307
00:36:42,400 --> 00:36:51,360
the the birth masking to real value numbers input so to basically features so we send the

308
00:36:51,360 --> 00:36:58,160
features from the CNN into the into the transformer we mask some of them and then on the opposite side

309
00:36:58,160 --> 00:37:06,720
we basically use a contrastive approach to reconstruct the the masking so we give

310
00:37:06,720 --> 00:37:13,200
the like some negative taken from the same sequence and and and positive and the network has to

311
00:37:13,200 --> 00:37:21,520
discriminate and and also what we did in that work was to combine LSTM and transformer in the

312
00:37:21,520 --> 00:37:26,320
same architecture and and the good thing about that is that it helps you reducing the size

313
00:37:27,200 --> 00:37:35,200
of the the transformer to gain in speed so because the and we let the agent actually learn when

314
00:37:35,200 --> 00:37:41,040
to use transformer sorry when to use a LSTM alone or when to combine transformer and the LSTM

315
00:37:42,000 --> 00:37:47,920
such that in some task it can basically avoid the extra complexity of the transformer and just

316
00:37:47,920 --> 00:37:55,520
use the LSTM whereas in other more complex tasks it will focus on using both. Got it and so in a sense

317
00:37:55,520 --> 00:38:02,720
you there are echoes of the pondernet paper in that you're trying to manage the computation

318
00:38:02,720 --> 00:38:09,280
or let the agent manage the computational investment based on an assessment of complexity.

319
00:38:09,920 --> 00:38:14,640
In some sense yes through the RL gradients there so that was really the agent by playing with

320
00:38:14,640 --> 00:38:21,200
the environment that was deciding what to use because yeah it's although I would love to actually

321
00:38:21,200 --> 00:38:28,480
have agents that stop and ponder that I think will be nice. What types of problems did you

322
00:38:28,480 --> 00:38:34,880
experiment with this paper? Three three different domains the whole of Atari suite

323
00:38:35,600 --> 00:38:41,040
deep mind control suite which I'm particularly proud because that's something that normally

324
00:38:41,040 --> 00:38:49,040
with policy methods like the one that we had there we didn't do so much work and then deep

325
00:38:49,040 --> 00:38:56,400
deep mind laboratory which is a suite of 30 task 3d complex task that you play all at the same

326
00:38:56,400 --> 00:39:01,600
time so it's also free of multi task escape that was the task we escape.

327
00:39:03,040 --> 00:39:11,120
And from a performance perspective what kind of results did you see was this you know promising

328
00:39:11,120 --> 00:39:18,640
enough to keep poking around that or was it you know really good performance that you know it's

329
00:39:18,640 --> 00:39:25,520
kind of challenges state of the art. First of all we always improve on the

330
00:39:25,520 --> 00:39:33,280
efficiency massively compared to baseline and in many domains especially in DM lab so in the

331
00:39:33,280 --> 00:39:41,280
deep mind laboratory we actually also got state of the art performance okay which was so then

332
00:39:41,280 --> 00:39:47,040
that's good because that was the more complex domain where we played so I think and I think that

333
00:39:47,040 --> 00:39:53,040
was kind of not an issue but you know something we could have done slightly better to focus more

334
00:39:53,040 --> 00:39:58,800
and more on complex domain because I think that's where this kind of method we shine so

335
00:39:59,440 --> 00:40:04,800
like complex architecture probably will benefit more from like complex method although I still think

336
00:40:06,240 --> 00:40:12,000
it's something I quite passionate about I think there's lots of stuff we can do to improve

337
00:40:12,000 --> 00:40:18,240
transformer and memory in general in reinforcement learning especially in relation to the

338
00:40:18,240 --> 00:40:25,120
length of the context that we can process that's something I think important and kind of a bottleneck

339
00:40:25,120 --> 00:40:36,320
in my opinion. Meaning the approach you took in this work of you know coupling the LSTM

340
00:40:36,320 --> 00:40:40,880
and the transformer and allowing the agent to choose sounds like you're saying you know that

341
00:40:40,880 --> 00:40:47,200
that's kind of a beginning place but there's a lot more a lot more to be done there. I think so

342
00:40:47,200 --> 00:40:52,400
I think so. As for us we have different sort of memory as I said before we have very long term

343
00:40:52,400 --> 00:40:58,160
memory we have shorter memory I think I'm not the first one to say these there are few papers out there

344
00:40:59,280 --> 00:41:08,240
already and they argue and I argue that agents should be equipped with this sort of different

345
00:41:08,240 --> 00:41:15,680
timescale memory. Awesome awesome. Andrea thanks so much for taking the time to share a bit about

346
00:41:15,680 --> 00:41:20,880
what you're working on. It's been a pleasure Sam and again thanks a lot for for

347
00:41:20,880 --> 00:41:50,800
inviting me. Actually I know. Absolutely thank you so much.

