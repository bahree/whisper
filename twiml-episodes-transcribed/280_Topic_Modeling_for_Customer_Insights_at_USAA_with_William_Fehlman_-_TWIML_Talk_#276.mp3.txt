Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
You are invited to join us for the very first Twimblecon conference which will focus on the
tools, technologies and practices necessary to scale the delivery of machine learning
and AI in the enterprise.
The event will be held October 1st and 2nd in San Francisco and early bird registration
is open today at twimblecon.com, again that's twimblecon.com, I can't wait to see you there.
All right everyone, I am on the line with Bill Felman, Bill is the Director of Data Science
at USAAA, Bill, welcome to this week in machine learning and AI.
Thank you Sam, thank you for inviting me.
Absolutely, so we were originally planning to connect at the Stradda Data Conference
in New York, but you unfortunately weren't able to make it due to hurricane Florence actually.
Did everything work out okay for you with the hurricane?
It actually did, I mean at the beginning of the week we had a zone of uncertainty that
included where I live and then as a week progressed we shifted south so luckily it missed
my area but still it created a lot of uncertainty and drama and preparation for it.
I bet, I bet, so at the Stradda conference you were going to talk about topic modeling
and we're going to dig into that in our conversation today, but before we do that why don't you
tell us a little bit about your background?
Sure, absolutely.
So as you mentioned I'm the Director of Data Science team at USAAA which is the United
Services Automobile Association and my team conducts research and applies advance analytical
methods in support of our context center operations that serves 12.4 million members and how
I got to that position.
My background is in mathematics and I hold a bachelor's in mathematics, a master's in applied
mathematics and a PhD in applied science and my applied science PhD dissertation was
an area of computer vision and so what I did in my dissertation I developed a method
to classify non-heat generating objects in thermal imagery and that work actually led
me to an opportunity to work in an autonomy incubator at NASA Langley Research Center
after I retired from the US Army and while in the autonomy incubator I worked on a multidisciplinary
team I was doing computer vision and what we were doing was developing machine intelligence
software for uncrewed systems and that actually that computer vision work is what led me to
my efforts at USAAA involving natural language processing and so what I'm doing at USAAA
is really looking in large corpus of documents and under trying to identify topics what I
call unknown unknowns really the things we don't know what we don't know but if you go
back to my computer vision really computer vision is looking at frames of imagery in each
frame as a matrix where each value within the cells in the frame are either gray level
values or RGB red, green, blue and when you look at natural language processing I'm really
dealing with large volumes of documents and along with those documents the terms or vocabulary
that are involved so I'm going to take those large corpus documents along with their vocabulary
and I'm going to put them in a matrix and so I'm really either when I'm doing computer
vision or if I'm doing natural language processing I'm applying the similar types of mathematics
which involves the calculus and matrix algebra of probability and statistics to both types
of application so it was a really good fit from what I was doing with computer vision and
then transitioning over to natural language processing type work that I'm doing at USAAA.
Oh nice nice and what is the what's the motivation for doing that kind of work in support
of a context center what are the kinds of documents and data that you're dealing with and what
business outcomes do they help drive for you.
Good question so in our context center so we got a call channel we also had where agents
are talking to our members on the phone we also have a chat channel where members are
talking to our agents as well but also we have digital channels such as.com and mobile
so these are very large volume of both unstructured and structured data and what we want to be
able to do is gain insights in this data to better serve our members and this is where
actually topic modeling comes into play and what you can think of topic modeling as a way
to what I like to say uncover the unknown unknowns you know there's there's the things
we don't know what we don't know and what topic modeling is is an unsupervised method
that helps us discover those topics that are emerging.
You mentioned a chat channel are you using AI as part of delivering that chat and I'm
mostly asking because this concept of discovering the unknown unknowns seems like it would be
really helpful in identifying you know for example topics that come up in chat conversations
that you might need to create you know intense and support for in your chat system.
Yes absolutely so what we want to be able to do and what we're doing is you know you
have chat in the chat channel you have conversations between our agents and our members and
same thing with the call channel and what we want to be able to do is identify topics
that are emerging before we reach a peak call volume and that way we can say mitigate
members concerns before we hit that peak call volume as an example if if there's some
kind of force fire for example and in California or a hurricane you know as we start seeing
conversations about that we can use topic modeling to get a better understanding of our
the needs of our members and we can be able to be ready to provide the services before
we even reach any kind of peak call volume it's really being more proactive than reactive
to the after the fact okay okay so you've got this data set that comes from the is it
coming primarily or solely from these chat and email interactions or are you also transcribing
voice support calls what all how all does your or how do you generate your data set.
So the the chat of course that's through the chat channel and that's that's tax that's
written between you know the conversation between an agent and a member and then the actual
call is a transcription so the call audio is transcribed to text and so we would be
able to obtain that data which when we get it we it's redacted so it's all any kind
of personal information or identifiable information or confidential information of our
members is removed and then we'll be able to run topic modeling on that to be able
to gain those insights.
What approach is to topic modeling do you use.
So the approach or the methodology that I use mainly is non-negative what's called
non-negative matrix factorization so traditionally within the area of topic modeling there has
been mainly like three methods which what the first method is is one of the original methods
which is latent semantic indexing or lean semantics analysis and what that method does it takes
a document term matrix so you can imagine these very large and by the way when I say
document what I'm meaning is a any kind of unit of text under analysis it could be a it
could be one chat it could be a chat or it could be a a call transcript it could even be
a tweet or or some other kind of one structure data but so we're take you can imagine this
document term matrix and along the say rows are all these documents along the columns
are all these terms so there's different methods that you can use to decompose that matrix
and to other matrices and be able to gain insights and like I mentioned the one of the original
methods is latent semantic indexing and what that does it uses singular value decomposition
to decompose that document term matrix into three other matrices and so but the criteria
with that method is that it's creating or the matrices that are decomposed into our orthogonal
vectors within each of those matrices and so that is a concern because a conversation is usually
includes more than one topic you know we could be talking you know about hurricane Florence but
we could also be talking about other topics involved as well and we want to be able to distinguish
those type of topics and with latent semantic indexing it usually has an issue doing that because
of the orthogonality in the in the matrices that it decomposes into another method is before we
continue does that suggest that this latent semantic indexing might be better used for smaller
documents like a tweet or an individual chat message as opposed to an entire transcript of a
call or an entire chat interaction it could be a tweet it could but it's usually it's probably
best applied when you don't have overlap within a given of different topics within a document
which would be sort of it could be a tweet because you're restricted to the number character
so you're usually talking about one topic is that kind of inherent to the the document or
to the set of classes that you are looking to associate these documents with for example
or a meaning if I've got a news site and I've got and I have like some you know strictly delineated
like a set of like categories of news like you know health and finance and sports that are
you know orthogonal does that mean that I can use LSIers it more have more to do with the content
of the documents themselves it is usually it's probably more to do with the content of the document
themselves themselves because you could you could be talking about if you had like a large corpus of
tweets and let's suppose that they're talking about world cup and and the Olympics if you take all
of those tweets and you just put them into one bucket then the latent semantic indexing might
do okay with the the tweets since they may have be talking about different or the different types
of topics and it may be able to distinguish them better than a say a call transcript that could
include multiple different topics and so it makes sense. Just taking a step back these
I'm thinking about the example that I gave where I'm trying to put documents into topics and that's
a different kind of a problem that's more of a categorization type of a problem whereas what we're
doing what you're we're talking about here with topic modeling is like an unsupervised learning
like you just have the data and you're trying to identify what the topics are in the data itself
is that is that right? Absolutely and you can think of it as a form of soft clustering okay so with
the key means clustering for example that's a form of hard clustering where it's actually taking
a specific document and categorizing it within a specific cluster but because we know that a document
could include more than one type of conversation or topic or theme we want to we don't necessarily
want a hard place that do a hard clustering to place that document within one specific cluster
what we would rather do is see how well see how many topics are existing when that document
and then say what is the probability that this document is associated with say topic number one
what's the probability associated with topic number two etc so that's it's more of a soft clustering
with is what topic modeling will do for you okay all right so latent semantic indexing
it's kind of a traditional way that folks might approach this but it's got this limitation
because it enforces that these terms be orthogonal to one another so it doesn't perform while when
your documents have multiple topics in them yes absolutely and so another couple other methods
that work pretty well is one of traditional method it's called latent dirtual allocation which is
more of a probabilistic type method based on the dirtual distributions and it does pretty good
but what we have found is that because it uses Gibbs sampling which is more of a random selection
of the terms and the documents we see a tendency that the topics results vary from one run to another
and so that's where I sort of lean more towards the third method which is non-negative matrix
factorization and what that does it takes the original document term matrix and decomposes it
into two other matrices one being a document topic matrix and a topic term matrix presumably
because this isn't using Gibbs sampling your results given a particular corpus are more repeatable
yes yes so with the non-negative matrix factorization what it's doing is it's more of an optimization
process so what it's doing is decomposing that original document term matrix into the two other
matrix matrices and the goal is to minimize the error between the original matrix and the product
of the other two and it's pretty repeatable I mean it does a pretty good job at providing good
results and also that you can run over continuously run and now when I hear matrix factorization I hear
hard does that is that necessarily the case or you know maybe hard to scale is maybe more
particular do you find that to be the case or are there methods for doing this matrix factorization
that are you know computationally tenable at large scale yeah that so large scale it does pretty
good is you know with using very large corpus sizes I guess the challenge can be when you start
getting it running it on like near real-time data online data okay and and that can be a challenge
but there are different methods that can use the original topics and then do some kind of
resampling of the new topics or new documents that are coming in and integrating them in
so scalability to large corpus isn't really actually a big deal and you know the time it
takes to do this can be a big deal but there are some methods to overcome that yes yes
okay so interesting so you know we've talked about this document term matrix as being the
starting place for you know all three of these topic modeling methods but we haven't talked about
how we even get to that how we represent the documents in this term matrix that's there are
different ways to do that right sure so there's a there's a pipeline for this process and so what we
do is we want to do some kind of pre-processing so we may want to of course tokenize the documents
so each term is a single dimension but we also want to remove things like the stop words so the
common terms like a the high frequency terms that may not be able to give us give us very good insights
with the topics or the subjects so we remove those as well and then we may want to also apply some
dimensionality reduction techniques so there's of course when you're working with a large corpus
of documents the dimensions can be very large and that interferes with the processing time as well
so there's dimensionality reduction techniques that we can apply what I have done in the past is
you know do a single phase or stage where I'm running topic modeling to help reduce the
dimensions across all the the documents as a pre-processing step as well and so what's an example
of a dimensionality reduction technique that you might apply to this document term matrix so what
I what I do usually as far as a dimensionality reduction technique is I will run
non-negative matrix factorization on the entire corpus using two techniques one one is term
frequency forming a term frequency matrix and the other frequent or method is using term frequency
inverse document frequency and so term frequency what that's really doing is forming the original
document term matrix that based on the frequency of terms whereas term frequency inverse document
frequency is is looking at the frequency of terms within each document but also looking at the
the frequency of a term across all the documents so it's sort of like offsetting so a very high
frequency term that is existing across all the document may not receive as much weight as if
it's high frequency but only occurring across a few documents and so what I'm going to do is I'm
going to run the non-negative matrix factorization on the entire corpus at first and get and reduce my
vocabulary size from say hundreds of thousands of terms to possibly down to a few hundred or a
thousand terms and that is just the pre-processing and then I can go ahead and run get more refinement
running topic modeling further on on those documents so how does doing your matrix your
non-negative matrix factorization on the larger document term matrix gets you to reducing the
number of terms so the it goes back to the original decomposition so the non-negative matrix
factorization is decomposing the original document term matrix into the two other matrices which are
the document topic matrix and the topic term matrix and so what it's going to do is it's you can
reduce the number of terms based on the the number of topics so if you have so what it's going to do
is if you look at the topic term matrix it's giving you the strength of association between
the topics in the terms where so that if you have a term that has a higher rate it has a
more of a higher strength of association with a given topic so if I say look at the
top 100 terms within a given topic and just consider those terms as part of my vocabulary
that I'm going to use then that will give me some kind of reduction dimensionality reduction
as opposed to using all of the terms if you're looking at say these top 100 terms on a for a given
topic are you then kind of zeroing out terms back in your document term matrix and kind of
creating more of a sparse matrix or are you changing the shape of that matrix based on this new
information that you have so it sort so it does change the shape so what it's doing is I'm going to
be my initial run with the non-negative matrix factorization is going to create a new document term
matrix but only include those terms that will have the highest association between that I had
originally from the the preprocessing run with topics so it's what it's really doing is it's given
me terms that are more relevant when I run subsequent topic modeling so just to kind of pull on this
one you know one more step so you've got these terms you zero out you know say you've got 100,000
terms and you've got well how many topics would be typical in your case so that's a good question
actually that's a that's a sort of an open problem the number of topics but I do have we have a
technique that we run it's to compute what is a coherent score interpretability of the topics
and what that that running the coherence will do is allow us to identify sort of like the optimal
number of topics that we need to run and so is this on the order of tens or hundreds or thousands
for you know a you know a corpus of documents that well maybe give us kind of general numbers for
each of these things how many documents do you tend to have tend to see you know rough order
magnitude how many terms how many topics oh so it could be I mean we could easily be running
say 50,000 documents through the through the topic modeling and in one run it could be more
but usually around 50,000 the number of terms of course you know that's it's going to vary I mean
if we're running creating a topic term matrix on a call conversations then you know the calls
can vary from you know a couple minutes to many minutes so the size of the corpus is can vary
as well or the documents can vary as well but it's all it's all I mean the only limitation of course
is the the processing speed you know to that we're working with when you are identifying the terms
that are most meaningful is that on a topic by topic basis or is that across the corpus specifically
I'm asking about this this step where we're doing the dimensionality reduction that's so much
like the meaning of the topic itself and the the coherence um specifically when we're looking
doing that dimensionality reduction and trying to get from our 100,000 terms down to 100
is that on a per topic basis or is that are you looking at the 100 terms that have the most
weight across all topics does that question make sense yeah so the the final vocabulary is based
on the the terms across all the topics that were used in the pre-processing phase okay so you've
got these uh you identify these hundred say terms that uh most strongly correlate to these topics
that you've identified and that kind of answered my question I guess that I've been trying to get to
the long way around if you're doing it across the all of the topics then you can just kind of get
rid of all the other terms in your document term matrix as opposed to if you were doing it on a
topic by topic basis then you still have kind of this potentially large sparse matrix yes okay
and and so this this resulting set of vocabulary in this pre-processing that we just ran based on
this um um topic modeling to identify the the top terms across all topics across the entire
corpus this vocabulary says what's going to be used in the subsequent topic modeling and that's the
dimensionality reduction you mentioned tf idf and and term frequency how do those play into here again
so in the in the subsequent steps I'm just going to be using ti tf idf um there's really
so in the pre-processing when I do the initial run for non-negative matrix factorization I run
usable techniques to identify the vocabulary the term frequency and the term frequency inverse
document frequency um but once I identify the vocabulary in the subsequent steps I'll just be running
the the tf idf which the difference between the two methods is that the term frequency it's looking
at it's really assigning a equal weight to each term and so it um it's it's looking at the how
for a given term how frequent that term occurs across the entire corpus but really there's no
difference on how it's applying the the weight that weight value to term a as it is to term b but
you may have a term that it's a high frequency not only in a given document but a very high
frequency across all the documents so that specific term may not really give you much insights on
the topic of the conversation so the other method which is term frequency inverse document frequency
sort of offsets that it'll look at not only the frequency of the terms uh within a document but
also how frequent that frequent that term occurs across other documents so it will give a high value
when terms for example occurs many times within a small number of documents and that's what I
they're using the subsequent topic modeling as well uh then you mentioned the coherence score
what does that help you get to so you can think of coherence as a measure that captures the
semantic interpretability of topics and it's it's based on the co-occurrence of topics or topic
words and where that's important is that when you when the topic model or say non-negative matrix
factorization is creating these categories consisting of these topic categories that are
consisting of terms we want the analyst to be able to look at those topics and say oh I get it
I understand what that topic is about and that starts getting into interpretability and you want
to be able to generate a coherence score that is going to provide you a good semantic interpretability
of the topics and that's it's that sort of like coherence score is a function of the number of topics
and so what we have done is um identify the number of topics that will maximize the average coherence
across the corpus and what's the intuition for kind of the the math you know the score itself what
the coherent score is saying so it's based on co-occurrence of words so there's there's a few methods
that are are used but what it's doing is it's it's looking at the counts of the number of documents
containing like a set of terms words and it's using that and it's also taking consideration
the the logarithm of that of that count and so it's it's you the closer for the specific coherent
score um I thought I'm using the closer that value is to zero the more coherent your topic is
and the more negative the value is the less coherent it is and so what we want to be able to do is
identify the one number of the topics that will maximize coherence and then that's the the
number of topics that we'll use and and be able to compute the the topic categories and also the
coherent scores for each topic category okay and that number of topics that maximizes coherence
is going to be ultimately based on the relationships between these terms in the given documents
in the corpus yes the co-occurrence yes the relationship between terms okay do you have any kind
of rules of thumb that you found for I guess it really depends a lot on the the types of documents
but for Isaiah kind of a standard you know say for call transcripts do you have any rules of thumb
where it you know it ends up being on the order of you know hundreds of hundreds of topics as
opposed to you know tens or thousands so I've actually I mean this is probably surprising but I've
actually a rule of thumb is between five and eight topics is a pretty good value to get
into good interpretability if you start getting larger than that number of topics then you start
losing the coherence how does it fail like are there patterns in the way it fails like does it pick
up words that you know just aren't meaningful in the sense of you know not particularly additive
to understanding what the topic is of a given document or is it trying to you know split topics
to you know find you know find grain so there are words that are kind of similar that should be
one topic like is there a GC patterns in the way it tends to fail if you have too many topics I
think what it will try if you have too many topics what it will try to do is spread yes spread those
terms across more more of those topic categories and therefore you start losing the interpretability
but you know that when you when you do a topic modeling I mean it goes back to the the
purpose of it you know it's doing soft clustering so it's it's definitely possible that you're
going to see a term existing in more than one topic category you know for example if I if I
have seven documents and you know four of the documents are talking about financial banks and
three of the documents are talking about river banks more likely you're going to see the word bank
across those all seven or all all the topics but you're probably going to be able to still be
able to distinguish the topics because you're going to see things that are talking about river
banks and certain topics and you're going to see things that are terms that are associated with
um financial banks and other topics and the reason is is because it's based topic modeling is
based on co-occurrence of terms so you're going to see a lot of um commonality of terms that are
co-occurring together and those given documents placed in a specific topic and it and it does
pretty good it does really good is there a way to identify what the primary topic of a given
document is is that just kind of ranking the the topics based on some score probability
so that that's a very good question and that's actually one of the purposes of doing the topic
modeling as well so if you go back to the the original decomposition it's taken that document
termatrix and decomposing it into a document topic matrix and also the topic term matrix so the
the topic termatrix is giving you the strength of association between each of the terms and a given
topic so that if I say I take the top 10 terms for a given topic which top 10 terms being the ones
that have the highest weight of association with a given topic you know that gives me information
about what that topic or what is the theme of that given topic but the other matrix the document
topic matrix what that is doing is giving you the strength of association between each topic and
a given document and where that's important is it goes back to a conversation so if you and I
are having a conversation we're probably going to be talking about more than one topic but we
may but want to identify well what's the primary topic and that's where that that document topic
matrix is important because it will allow us to identify the topic that has the highest value or
weight which is the highest strength of association between that topic and that that given document
and I can look across a given corpus and run topic modeling on open to transcripts across a given
corpus and I can be able to identify not only the topics of conversations across all the documents
but also I can identify which documents had topic one as the primary topic of conversation.
You've done this factorization you've got this document topic matrix and this topic term
matrix can you then take this topic term matrix and kind of use it as a model that you can run
inference against when you have a new document you kind of put that through your your term pipeline
to get the terms in that document and then you kind of apply you multiply it by this topic term
matrix to get the the topics in that document. So it's sort of like subsequent modeling like subsequent
right right so trying to get the topics in a document that you didn't or maybe the the question
that you know the question before this is does this topic term matrix tend to have strong predictive
value for documents that don't exist within the corpus that it was created on is is not like a
technique like a supervised learning where you're built into the process you're testing against
data that you haven't built your model on. Yeah so I what you can do is so the non the topic
modeling is definitely an unsupervised machine learning method and but you can think of it as being
a precursor to a supervised method so you could really take the top the topic term matrix that is
produced from this non-negative matrix factorization and look at each one of those topics that are
as an output and and label them you can provide a label for each of the topic and then then create a
supervised model say with a deep learning model that is able to classify a new call into one of
those topic categories and then if you say go below a given threshold of probability perhaps
maybe it's a new topic and you might want to put that new topic or that document associated with
that one it fell below a threshold into a bucket and rerun topic modeling on it so it's sort you
can almost think of this process as a feedback you're you're learning new topics and integrating them
into a new model updating the model and then that model is making supervised models making predictions
and then if it isn't able to predict a new document based on a given probability it puts it into
a bucket and reruns topic modeling on it given that you have this limitation of five to eight
topics in order to achieve interpretability or coherence is there is there a process through which
you might you know if you've got a large corpus of say several hundred thousand documents
or even a you know this fifty thousand document corpus cluster your documents or partition your
documents and create separate topic models and so as to maximize the number of coherent topics that
you end up with yeah I think you could you could definitely do as a one of the initial steps
you could create rules that would separate the documents look based on the structure data the
metadata and you know you you might be able to for for example calls you know you if the calls
are going to the bank within the bank you know they might be going to deposits or or checking
or something like that so as an initial step you might be able to categorize those calls and then
runs separate topic modeling on each one of those categories you might be able to do really
just it's a matter of looking at the structure data before you would separate them and then
once you got those bucket of categories run topic modeling on them so you would have different
topic models so what you're suggesting is as opposed to trying to do it in a machine learn kind
of way take advantage of the the metadata if you will that's inherent in your problem to partition
up your documents into you know I guess there's a sweet spot right a small a small of a
corpus as possible but that still allows you to create a robust topic model absolutely you really
what you're working with is semi structure data you know you're you're working with the structure
data in the metadata and also of course the unstructured data which is the text and you're
really combining the two to be able to do the topic modeling effort so you do this topic modeling
you get these topics you know five to eight topics from a corpus of 50,000 transcripts or chat
call or chat transcripts how do you then integrate these back into the call center operations to
improve them so for it goes back to the example I used if we're identifying topics that are
emerging before we reach a peak call volume what we're able to do is be more proactive with the
topics we may be able to create better or inform our agents of these these certain topics that are
coming in so they can be more prepared to mitigate any concerns or better serve our members
we we may be able to even make adjustments if there's if there's confusion about something that's
say on the mobile channels you know either dot com or or mobile we may able to make adjustments
on those on those those different channels because we're starting to see topics and they're
associated with those and then that also in turn could decrease any call volumes associated
with those concerns but it's really being trying to be more proactive to better serve our members
what rather than you know reacting after the fact and do you find it the initial steps of this
process where you're doing dimensionality reduction end up being at odds a bit with trying to use
this as kind of an early warning system because they get rid of emerging terms that might not have
yet achieved some kind of critical mass no not really what I think the how we approach it it's
able to identify the most significant terms and it's able to capture the most relevant topics
it does a pretty good job it's able to surface the things that we didn't even know you know I
go back to the unknown unknowns you know the things we don't know what we don't know you know it's
able to surface those things so we're able to take action on them awesome awesome are there any open
challenges or problems that you see recurring as you try to use these types of models you know I
would say there's definitely open problems I think that choosing the number of topics is still
an open problem I mean I mentioned that we you know use the coherence score to try to maximize
coherence you what is the number of topics that maximize coherence I still think that's an open
problem an appropriate corpus size I think that's still an open problem as well as the the document
term matrix you know there's there's different weights that are used in a document term matrix such
as the term frequency and term frequency inverse document frequency but I think there's more room
for developing new weights and doing more research in that area and then of course you know I talked
about coherence you know coherence is the interpretability of topics but I think also
determining the relevancy of topics is really important because you know something could be
interpretable but is it really relevant and we need to take action on it well Bill thanks so much
for taking the time to chat with us about this I'm sorry that you didn't get a chance to present
this as stratum sure folks would have enjoyed it but it's really interesting stuff well thank you
Sam once again thank you for inviting me absolutely all right everyone that's our show for today
for more information about today's show visit twimmel ai.com be sure to visit twimmelcon.com for
information or to register for twimmelcon ai platforms thanks again to c3 for their sponsorship
of today's episode to check out what they're up to visit c3.ai as always thanks so much for
listening and catch you next time
