WEBVTT

00:00.000 --> 00:10.400
All right, everyone. Welcome to another episode of the Twimble AI podcast. I'm your host,

00:10.400 --> 00:16.640
Sam Charrington. And today I'm joined by Doina Precup. Doina is a research team lead at Deep

00:16.640 --> 00:23.040
Mind Montreal and a professor at McGill University. Before we get going, be sure to take a moment to

00:23.040 --> 00:28.640
leave us a five-star rating and review wherever you're listening to today's podcast. Doina,

00:28.640 --> 00:33.120
this conversation has been a while in the making and I'm super excited to have you on the show

00:33.120 --> 00:37.760
and looking forward to digging into your research into reinforcement learning. Welcome.

00:38.720 --> 00:44.720
Thank you very much. I'm really excited to be on your show and yeah, big fan of your podcast.

00:44.720 --> 00:50.400
So looking forward to our conversation. Awesome. Awesome. Same here. Let's get started. As you know,

00:50.400 --> 00:54.640
we always do by having you share a little bit about your background and introduce yourself to

00:54.640 --> 01:01.120
our audience. Yeah, so I split my time between Deep Mind in the Montreal team as well as McGill

01:01.120 --> 01:07.040
University where I've been a professor since 2000. Before that, I did my PhD at University of

01:07.040 --> 01:14.000
Massachusetts Amherst. Working on reinforcement learning. I was actually lucky enough to take the

01:14.000 --> 01:18.960
first version of the reinforcement learning course taught by Andy Barton and Rich Satin out of

01:18.960 --> 01:27.040
their textbook in 1995 and they got me hooked to the field and I've been working on it ever since.

01:27.040 --> 01:34.800
Fantastic. And what prompted your interest in RL? I really found it a good way to think about

01:34.800 --> 01:39.920
artificial general intelligence. So agents that can learn how to do many different things in an

01:39.920 --> 01:46.880
open-ended environment. And it's really because we have on one hand rewards that give us a way to

01:46.880 --> 01:51.600
express the task that the agent should do. And on the other hand, we really have learning from

01:51.600 --> 01:57.600
interaction rather than let's say learning from being told what to do or not really having a clear

01:57.600 --> 02:03.440
goal. And so it seemed to me to be the right sort of balance between having some interesting

02:03.440 --> 02:09.760
signal in the reward but also having this ability to interact and explore and really learn freely

02:09.760 --> 02:19.920
in the environment. RL is increasingly a broad field. So let's dig a little bit into your research

02:19.920 --> 02:24.640
and have you share a bit about what you're most excited about. So one of the things that I've

02:24.640 --> 02:29.440
worked on for a long time and I'm always excited to think about it is hierarchical reinforcement

02:29.440 --> 02:36.160
learning. This is really about learning abstract representations, especially abstraction over time

02:36.160 --> 02:43.360
because oftentimes in reinforcement learning the problem is phrased at a very small time scale,

02:43.360 --> 02:49.440
very fine grain time scale sort of on the order of let's say muscle twitches. But really to

02:50.080 --> 02:54.800
self-complicated problems you need to think in terms of longer time scales and variable time scales.

02:55.360 --> 02:59.520
So for example, if you're cooking a meal you're not thinking about all the muscle twitches involved

02:59.520 --> 03:07.120
in stirring or putting a pot on the stove you're thinking in terms of larger steps. What ingredients do

03:07.120 --> 03:13.840
I need? Do I need to go to the store to get these ingredients? And so we reason at many levels

03:13.840 --> 03:19.520
of abstraction both in terms of the time scale of our actions as well as in terms of the states and

03:19.520 --> 03:24.800
features that we use. And I would really like reinforcement learning agents to be able to do

03:24.800 --> 03:30.800
the same to really leverage abstraction and learn these abstractions from their interaction stream.

03:30.800 --> 03:37.200
So I've been doing a lot of work in this respect in terms of for example learning a framework

03:37.200 --> 03:43.760
for temporal abstractions called options, learning models that look at different time scales,

03:44.640 --> 03:51.360
understanding how to use these models in planning algorithms and also understanding when agents

03:51.360 --> 03:58.960
can use what actions, what we call affordances. And so this is a long history and a long series

03:58.960 --> 04:04.800
of papers that is associated with it. But one of the important open questions that I still struggle

04:04.800 --> 04:13.600
with and think about is how do agents decide which abstractions to learn about? For us maybe it's

04:13.600 --> 04:18.800
easy. We think about objects for example and that somehow comes naturally but for reinforcement

04:18.800 --> 04:23.600
learning agents this is still something that I would like them to acquire automatically.

04:23.600 --> 04:30.640
Awesome and we'll return back to hierarchical RL. You also spend a lot of time working on

04:30.640 --> 04:35.120
reward specification for RL agents. Can you tell us a little bit about that work?

04:35.120 --> 04:40.720
Yeah so rewards are actually really important because they determine what the agent is really

04:40.720 --> 04:49.840
learning about and thinking about. And actually David Silver, Rit Sutton, Zitinder Singh and I put

04:49.840 --> 04:57.360
out the paper in 2020 which received quite a bit of attention both positive and negative I think

04:57.360 --> 05:05.360
called reward is enough. It really talks about this hypothesis that a reward signal, in fact a

05:05.360 --> 05:12.880
simple reward signal in a complex environment could really lead an agent to develop all the

05:12.880 --> 05:19.120
interesting attributes of intelligence that we might sort of intuitively think about. So

05:19.120 --> 05:25.040
for example in this paper we discussed squirrels. Squirrels have a perhaps a simple reward

05:25.040 --> 05:31.760
function. They like to eat nuts because that helps them to survive. But in that process they

05:31.760 --> 05:37.600
actually develop a lot of interesting abilities like the ability to remember where they put nuts

05:37.600 --> 05:42.480
before and the ability to predict the time of year when these are going to be available and

05:43.200 --> 05:48.480
the ability to deal with other squirrels and maybe you know deceive them or make sure that they

05:48.480 --> 05:57.120
don't get to the hidden stash of their own nuts. And so that's an interesting set of abilities

05:57.120 --> 06:05.200
that involves memory and planning and modeling aspects of the world that can all be seen as

06:05.200 --> 06:12.480
developed from this maximization of reward. And so we were asking ourselves in this hypothesis whether

06:12.480 --> 06:18.720
it could also be true that general age day agents would develop these kinds of abilities also

06:18.720 --> 06:24.560
from maximizing rewards. It's a hypothesis and so you know as any hypothesis it's interesting to

06:24.560 --> 06:31.520
think about how might we might you know confirm or dismiss it. And so following that we did some

06:31.520 --> 06:39.200
technical work trying to understand how we can take sort of intuitive task specifications and

06:39.200 --> 06:46.960
translate them into rewards for reinforcement learning agents. And when is this actually possible?

06:46.960 --> 06:54.960
So this was a paper called on the expressivity of Markov reward that was led by David Able and with

06:54.960 --> 07:02.080
several fantastic collaborators at DeepMind and also at Brown University. And it received the best

07:02.080 --> 07:10.720
paper award at Europe's last year so that was also very wise actually when we when we wrote the

07:10.720 --> 07:16.400
paper we weren't sure what people would make of it. But we were quite excited to see the positive

07:16.400 --> 07:25.360
reception. And really in the paper we aimed to make this hypothesis a bit more concrete and to

07:25.360 --> 07:31.200
try and understand from a mathematical point of view when do Markovian rewards which are a special

07:31.200 --> 07:38.960
class of reward functions capture intuitive notion of tasks. Got it. Let's maybe start with the general

07:38.960 --> 07:47.760
case of rewards. The hypothesis is that you know using this mechanism of reward can lead to all sorts

07:47.760 --> 07:57.840
of intelligence capabilities behaviors. How do you characterize that in a machine learning context?

07:57.840 --> 08:01.200
You know what how do you draw the analogy from the squirrel to the machine learning?

08:01.200 --> 08:11.200
So in the paper we thought of essentially a framework where you might imagine somebody who has a

08:11.200 --> 08:19.120
task in mind let's call this Alice and then you have a learner called the learner Bob and so Alice

08:19.120 --> 08:26.480
has has a task specification in mind and now has to translate this into a reward signal that Bob

08:26.480 --> 08:32.400
would get. So Bob is a usual reinforcement learning agent it inhabits a Markov decision process

08:32.400 --> 08:38.400
this means that on every time that Bob observes the state of the environment can do some action

08:38.400 --> 08:44.000
and then we'll receive as a response an immediate numerical reward signal and there will be a transition

08:44.000 --> 08:51.280
to an X state and there's some discount factor that the values rewards that are received too far

08:51.280 --> 08:57.360
in the future and Alice might have something more intuitive in mind so for example Alice might

08:57.360 --> 09:06.240
have a preference over certain ways of behaving so if Bob is an agent that's in a grid world

09:06.240 --> 09:13.760
navigation task for example Alice might prefer that Bob gets to a designated goal location quickly

09:13.760 --> 09:20.240
but she might also prefer that Bob doesn't step into lava and get burned and so this

09:20.240 --> 09:28.560
imposes now a preference over the set of policies that Bob might have the policy being the mapping

09:28.560 --> 09:38.000
from state to actions and now we can think of no the task specification in Alice's head as being

09:38.000 --> 09:44.240
this preference over the space of policies the reward function is the usual Markovian reward

09:44.240 --> 09:51.680
which is associated with states and actions and we want to understand can Alice efficiently compute

09:52.320 --> 09:58.480
a reward that captures her preferences and so now we can think a little bit about you know

09:58.480 --> 10:04.160
what kind of preferences she might have and how stringent they might be so one kind of preference

10:04.160 --> 10:10.000
which I described is over policies and we can have that be very strict or we can relax it a

10:10.000 --> 10:15.120
little bit we can say you know Alice certainly thinks that some actions are bad like stepping into

10:15.120 --> 10:22.960
lava is always bad but otherwise has no strong preferences in in other situations so whether

10:22.960 --> 10:28.800
you know Bob takes one route or some other route doesn't really matter so long as he doesn't get

10:28.800 --> 10:35.360
burned so that that leads us to ranges of policies or sort of sets of policies being acceptable or

10:35.360 --> 10:43.520
being superior to others and we can also think in terms of trajectories so Alice might consider

10:43.520 --> 10:48.800
trajectories that Bob might do in the environment and just prefer one trajectory over another right

10:48.800 --> 10:55.040
so if Bob is not trying to run into walls too much maybe that's that's a good thing

10:57.120 --> 11:02.720
so now if we have a set of preferences like this the question is can we actually always translate

11:02.720 --> 11:09.600
them into a reward function so can Alice compute a reward function that is Markovian

11:10.800 --> 11:20.960
and if so what's the complexity of of doing this so the the paper has two aspects to it right

11:20.960 --> 11:26.800
there's a negative result and there's a positive result you know the the negative result is in

11:26.800 --> 11:34.720
some in hindsight not so unexpected but it basically says you can't always find a Markovian reward

11:34.720 --> 11:43.120
Markovian in the state of the agent and intuitively in hindsight it's it's not so surprising because

11:44.240 --> 11:51.440
on one hand there may be states that are unreachable where Alice might have certain preferences

11:51.440 --> 11:58.640
but they're irrelevant and so if you have some disconnected state in some corner and Alice has

11:58.640 --> 12:04.800
some contradictory preferences there let's say in terms of the the actions that can impact

12:04.800 --> 12:09.440
the specification of the reward overall but it's a case that we don't really care about so that's

12:09.440 --> 12:15.360
sort of one mode of failure which is perhaps not interesting would it be an example like if Alice

12:15.360 --> 12:21.440
you know if the maze is you know nominally 20 steps to completion and Alice has a preference

12:21.440 --> 12:26.240
that you get there in three steps like it just can't be done that's right so this kind of thing is

12:26.880 --> 12:33.120
you know it's impossible and and therefore you know the math says yes yes it's impossible

12:33.120 --> 12:38.960
there's a more interesting case which is the case of of non-Markovianness so for example

12:38.960 --> 12:47.360
Alice might say I prefer that Bob always go in the same direction so if you're going up or you

12:47.360 --> 12:52.960
should always go up if you're going down you should always go down Markov decisions are ID so you

12:52.960 --> 12:58.480
can't really enforce that so you can't really enforce that if you wanted to enforce that one

12:58.480 --> 13:04.640
possibility would be that you modify the state space right so that we keep track of you know

13:04.640 --> 13:10.320
whether you've been going up and then you know we could we could do a Markovian reward in that but

13:10.320 --> 13:15.520
in this paper we only consider it specifying with respect to the state space that's already there

13:15.520 --> 13:23.200
we didn't consider this larger context of you know trying to modify the states and specify the reward

13:23.200 --> 13:29.680
I think it would be a very interesting avenue for future work to think about how do we go from

13:29.680 --> 13:37.200
observations to a specification that has you know perhaps a new state space and the reward

13:37.200 --> 13:42.080
function that that goes with it but that went beyond the scope of the results that we had in this

13:42.080 --> 13:47.920
paper however one thing that we did show in the paper which is the positive result is that

13:49.520 --> 13:57.920
there is a procedure which is a polynomial time procedure that Alice can use to either return a

13:57.920 --> 14:05.280
reward function that is consistent with her preferences if such a reward function exists

14:06.240 --> 14:14.800
or to determine that no such reward function is possible and the intuition actually of the

14:14.800 --> 14:20.560
of the algorithm is interesting it's based on a linear programming approach

14:20.560 --> 14:31.520
and so the the basic idea is that for any policy we can compute a stationary distribution associated

14:31.520 --> 14:38.080
with that policy stationary distribution over the states of the controlled Markov process

14:38.960 --> 14:45.920
and then we can impose constraints between the policies that our analysis acceptable set

14:45.920 --> 14:52.400
and the fringe policies the fringe policies are identical to those that are in the acceptable set

14:52.400 --> 14:59.520
except at one state they would take a different action and so we can have a set of inequality

14:59.520 --> 15:06.080
constraints the basically say the acceptable policies should be better in value than these

15:06.080 --> 15:14.080
fringe policies and with this kind of linear program either we find a solution and the size of

15:14.080 --> 15:19.920
the linear program is polynomial in the size of the states and actions so we can find a solution

15:19.920 --> 15:26.800
in polynomial time or if the linear program does not have a solution that means that no Markovian

15:26.800 --> 15:31.920
reward function is actually consistent with the original preferences that were expressed.

15:31.920 --> 15:38.400
Is there a notion of like sometimes a linear program is underspecified you've got more degrees

15:38.400 --> 15:46.400
of freedom than you have constraints does that come up in this analysis that's a really

15:46.400 --> 15:58.320
interesting question no in our case we want to find a solution if any solution is is okay

15:58.960 --> 16:05.120
we don't need to find a specific solution and so there will be for example potentially multiple

16:05.120 --> 16:12.800
reward functions that are all consistent with some optimal policy you know for example

16:12.800 --> 16:19.520
scaling of each other and that's that's perfectly fine so long as all of them end up

16:20.560 --> 16:26.400
ensuring that the preferences are satisfied so the good policies are better than than the

16:26.400 --> 16:32.640
others right or that the bad policies are inferior to the others then you know all of these

16:32.640 --> 16:39.280
solutions are are fine and acceptable for more point of view. There's an implicit assumption

16:39.280 --> 16:46.080
that the policy is specifiable kind of mathematically or not the policy sorry the preference

16:47.920 --> 16:54.800
which maybe limits the yeah maybe this is referring back to what we're talking about in terms

16:54.800 --> 17:02.560
of the Markovianness of it but it also kind of speaks to the gap between a preference that's

17:02.560 --> 17:09.200
someone might have in a real-world scenario and then trying to implement it using this method and

17:09.200 --> 17:14.640
how do you get to a mathematical representation of that preference like did you address that at all

17:14.640 --> 17:22.480
on the paper? Yeah so that that's an interesting question and we don't really talk about this in

17:22.480 --> 17:31.520
the paper but I can give you my my perspective on this okay on one hand what you do in the real world

17:31.520 --> 17:39.920
is you have some preferences but you may not have full preferences right so you might for

17:39.920 --> 17:46.880
example in in our grid world say well Bob should never step in lava but you know if Bob has to

17:46.880 --> 17:54.480
go through red squares or green squares I don't really care okay but as you observe behavior

17:55.280 --> 18:00.560
those preferences might actually change right you might start preferring you know the Bob maybe

18:00.560 --> 18:05.920
always steps on green squares because those are soft and you know Bob is like a little toddler

18:05.920 --> 18:14.160
and shouldn't hurt himself right so there these kinds of things may come up over time now our

18:14.160 --> 18:19.280
framework is very much single shot so Alice has a set of preferences they're there from the

18:19.280 --> 18:26.160
beginning she computes the reward and then Bob goes off and optimizes that reward function

18:26.160 --> 18:32.480
I think in practice there's much more of a give and take right there is much more of you know

18:32.480 --> 18:41.120
maybe Alice observes the behavior that might get her to have new preferences right or maybe

18:41.120 --> 18:48.160
revise existing preferences and so I think in in applications one would have to have an interaction

18:48.160 --> 18:56.480
loop where you know Alice observes gives new reward functions Bob continues to optimize over time

18:57.440 --> 19:05.440
and this is you know this continues for a while in order to really get to to the root of what the

19:05.440 --> 19:11.600
task is and you know one thing that I remember for example is when my kids were young you know we

19:11.600 --> 19:17.280
used to reward them for putting away their toys and at some point one of them discovered this

19:17.280 --> 19:21.280
interesting strategy of always taking all the toys out of the torches and putting them back

19:21.280 --> 19:25.920
and taking them out and putting them back you know it's the it's the behavior that gets rewarded

19:25.920 --> 19:32.960
right so it's the classic boat spinning in the middle of the that game yeah so basically now

19:32.960 --> 19:38.160
this is the stage at which one would actually revise what the reward function is by

19:38.160 --> 19:43.360
alerting the quirkiness and the behavior that that is being induced and I think naturally we do

19:43.360 --> 19:50.080
that and our framework in this paper does not does not address that um there's one other aspect

19:50.720 --> 19:59.200
which is also not addressed which is with how good is this reward function in order to get

19:59.840 --> 20:04.640
Bob to learn efficiently so this is not something that we talked about in the paper at all

20:05.360 --> 20:11.760
um we interestingly observed that in the examples that that we showcased in the paper

20:11.760 --> 20:18.800
the inferred rewards did lead to fast learning because you know the reward function is the result

20:18.800 --> 20:24.880
of computing an LP and so it's a dense reward function it's not sparse like what people would

20:24.880 --> 20:31.600
specify in these kinds of tasks and so it does lead as a side effect of that to pretty fast learning

20:31.600 --> 20:37.440
but it's sort of uh it's not clear that that would always happen so the linear program serves

20:37.440 --> 20:45.360
to constrain the search space for the reward function which yields more efficiency yeah exactly

20:45.360 --> 20:50.960
yeah I think my question maybe I'll comment my question a little bit differently the example

20:50.960 --> 20:58.160
you use in the paper is grid world yeah maybe it's the case that any preference about actions

20:58.160 --> 21:02.560
in grid world you know there's a straightforward mathematical representation of those

21:02.560 --> 21:08.960
uh in lots of other you know tasks maybe there's not a straightforward mathematical

21:08.960 --> 21:17.760
representation of the the preferences um you know but I'm I guess I'm supposing that there are

21:18.480 --> 21:25.280
marcovian uh tasks for which the preferences are not easily mathematically represented that

21:25.280 --> 21:35.920
may not be the case but um did you look at like what what tasks or environments this uh all of

21:35.920 --> 21:42.720
the supplies to that's a great question so the paper itself is more of a definitional paper we

21:42.720 --> 21:48.960
were trying to define the problem think about you know what it means to have a a task in mind

21:48.960 --> 21:58.240
and separate that from this issue of how do you then specify it as a reward um there's so it does

21:58.240 --> 22:06.800
assume that you have a marcovian process and the side effect of that is that we have states

22:07.760 --> 22:13.680
um now in the real world we have features right we don't have states but we have some observations

22:13.680 --> 22:20.560
we see images right those are a imperfect representation of what the state might be and that is

22:20.560 --> 22:25.040
not a setting that we have handled in the paper although I think it's really interesting for

22:25.040 --> 22:30.560
for future work so you know Alice might have preferences but might not have access to the state

22:30.560 --> 22:36.400
space she might only have access to observations and Bob also might not have access to the state

22:36.400 --> 22:42.480
space only to observations and this is where uh the framework that we have and the the specifics

22:42.480 --> 22:49.200
of the LP would not carry through I think there is a path to have a similar kind of solution

22:49.200 --> 22:56.240
in this case because generally speaking in reinforcement learning LP formulations have been

22:56.240 --> 23:02.640
used as a as a way to think about the problem of reinforcement learning and there are versions

23:02.640 --> 23:11.680
of these approximate linear LPs infinite LPs that have been proposed for other kinds of reinforcement

23:11.680 --> 23:16.400
learning problems so I you know my gut feeling is we could take some of that methodology maybe

23:16.400 --> 23:22.800
and apply it in the case where you don't have access to state um we do assume that Alice has

23:22.800 --> 23:29.600
preferences and so if she doesn't or if she doesn't know what they are our work doesn't

23:29.600 --> 23:35.760
doesn't help with that um but I think uh you know this limitation for example of not having

23:35.760 --> 23:42.480
necessarily an MDP having features uh these are are things that are more left for future work

23:42.480 --> 23:48.800
but could potentially be overcome it would also be really interesting to think about how preferences

23:48.800 --> 23:56.080
might be uh obtained in practice you know like what if you actually had an agent that is working

23:56.080 --> 24:01.760
in a look to the person the person is is stating some preferences we use this methodology to extract

24:01.760 --> 24:08.000
rewards and then you know maybe again the person observes what's happening and and goes back and

24:08.560 --> 24:13.680
you know is there a way for us to really do this in practical applications it's not something

24:13.680 --> 24:19.120
that we have tried but it would be fascinating to see and part of the problem is the one you

24:19.120 --> 24:27.280
articulated earlier is the evolution of preferences over time that you have to do that plus the

24:27.280 --> 24:34.560
uh trying to figure out how to how to close that loop in real time exactly okay going back to

24:34.560 --> 24:41.440
the work on hierarchical reinforcement learning is there a recent paper in that space that

24:43.040 --> 24:48.880
that comes to mind or or maybe we can talk a little bit about or start from like the

24:50.240 --> 24:54.400
kind of what's the research frontier there how far along are we in thinking about these kinds

24:54.400 --> 25:02.560
of hierarchical problems a lot of the work over time has gone into the question of discovery

25:03.360 --> 25:09.920
how does an agent discover abstractions we have a lot of good understanding about how to represent

25:09.920 --> 25:17.280
for example temporally standard actions um by having an initiation set where where such an action

25:17.280 --> 25:22.720
can start uh by having an internal policy and an determination condition so this is the

25:22.720 --> 25:27.760
a framework called options in hierarchical reinforcement learning that I worked on for a very

25:27.760 --> 25:35.280
long time and what would be an example of that imagine that you have a robot that's in an environment

25:35.280 --> 25:41.200
and uh it has a controller and the controller says you can go forward if there's nothing in front of

25:41.200 --> 25:47.280
you so if you're far enough from any obstacle that's your initiation set um it's almost the entire

25:47.280 --> 25:53.920
environment except for some spaces around your obstacles um the policy is just to go forward so

25:53.920 --> 25:58.880
that's already pre-specified and you stop whenever you're too close to an obstacle so that's the

25:58.880 --> 26:04.960
that's the termination condition and now these policies can be stochastic as well right so

26:04.960 --> 26:09.600
doesn't necessarily have to be that you're always going forward you might be going on any kind of path

26:10.160 --> 26:16.320
right until you get too close to an obstacle and such uh such controllers can be thought of not

26:16.320 --> 26:23.200
just in robotics but you know more intuitively for example in in planning right so if you're

26:23.760 --> 26:29.840
playing let's say a puzzle game like Rubik's cube you might have certain configurations that

26:29.840 --> 26:35.840
you want to achieve that impose sub goals and then you know the the ways to achieve those

26:35.840 --> 26:43.200
configurations can be thought of as as these kinds of options um so we understand how to formulate

26:43.200 --> 26:52.560
the problem the interesting question that is still quite open is how do we actually find these

26:52.560 --> 27:00.720
these sub goals um so you know for example in a in a Rubik's cube uh I don't know if you've ever

27:00.720 --> 27:06.640
done any of these or if you like them but you know when you learn how to do them people tell you

27:06.640 --> 27:13.840
oh there is you know you have to complete a phase right somebody's telling you that's that's your

27:13.840 --> 27:16.000
circle complete the phase that's the first thing you should do and then you know if you have this

27:16.000 --> 27:20.480
kind of configuration on the side here's a here's a little sequence that you should do right

27:21.760 --> 27:29.760
so it's told to you and yes once you know this it really greatly simplifies the problem because

27:29.760 --> 27:36.080
you don't search through all the space of possible moves you're now executing these sequences that

27:36.080 --> 27:44.800
you know are useful um but how do we discover this on our own right that's that's the key question

27:44.800 --> 27:52.560
and it's one that we thought about quite a bit as a community uh it's still pretty open because

27:52.560 --> 27:59.360
it's hard um there are some interesting answers one interesting answer that one of my PhD students

27:59.360 --> 28:06.800
at Miguel Pierre-Luc Bakon did a few years back it's called option critic it's basically an algorithm

28:06.800 --> 28:15.200
that tries to uh use the reward from the environment and and tries to find sub goals that are on the

28:15.200 --> 28:22.560
path to rewarding states and it uses gradient based methods very similar to uh actor critic policy

28:22.560 --> 28:29.520
gradient methods um and so so that works quite well on in certain environments but sometimes we

28:29.520 --> 28:35.760
still observe things like you know the agent using abstractions for some bit and then kind of

28:35.760 --> 28:43.120
collapsing them away like getting rid of them and it again in hindsight maybe one should expect

28:43.120 --> 28:48.640
this because if you think about how people do things oftentimes you need these kinds of hints or

28:48.640 --> 28:56.480
sub goals at the beginning of solving a task but if you sort of if you're only solving one task

28:56.480 --> 29:01.280
and you get used to it you don't have to think about that anymore you'll automatize it

29:01.280 --> 29:06.080
away right so people who do the Rubik's Cube in a few seconds don't really think about the

29:06.080 --> 29:11.120
configurations anymore like you know it's all gotten into their muscle memory and so it's a

29:11.120 --> 29:16.000
similar situation that we observe with our agents if they only have to solve one task they might

29:16.000 --> 29:21.280
use options for a while to try and help them out but then they get rid of them and they obtain

29:21.280 --> 29:30.720
some flat policy that is as close to optimal as possible so one way to uh to think about this

29:30.720 --> 29:36.080
is the agents only have to do one thing and so they're overfitting to that in some sense

29:36.080 --> 29:41.120
if they had a very rich very large environment complex environment with many things to do

29:41.120 --> 29:47.920
they may need to keep these abstractions around so if you know if you're not always having to solve

29:48.480 --> 29:54.640
Rubik's Cube but you have to solve different kinds of puzzles right let's say number puzzles

29:55.360 --> 30:01.840
then you might have certain strategies that you learn and you do keep around about you know how to

30:01.840 --> 30:07.840
manipulate numbers for example in these puzzles or how to do a search that's the kind of richness

30:07.840 --> 30:13.760
that we would need for our agents to keep these abstractions around and so part of it I think

30:13.760 --> 30:19.920
is is thinking about the good the interesting environments that we should use to send and part of

30:19.920 --> 30:25.280
it is to think about the methods themselves and you know for example our gradients enough

30:25.920 --> 30:32.240
should we do something else like more of a generate and test approach right where we think about

30:32.240 --> 30:36.880
useful sub goals and then we test them out and we have some way of curating a collection of

30:36.880 --> 30:45.520
sub goals for the agent I think that's all in substance quite quite open as a as a research direction

30:45.520 --> 30:51.040
interesting one question that's coming up for me is trying to think about the relationship

30:51.040 --> 30:58.480
between hierarchical RL as you've described it in curriculum learning I guess one just kind of

30:58.480 --> 31:06.880
riffing on you know compare contrast like curriculum learning is sequential in nature whereas

31:06.880 --> 31:13.680
hierarchical RL it could be more tree-like in the sense that you've got this or maybe even like an

31:13.680 --> 31:22.000
ensemble like the agent has this portfolio of strategies that it can employ curriculum is

31:22.000 --> 31:31.600
maybe more training time and hierarchical is more inference time I don't know if that's

31:31.600 --> 31:36.320
is that correct there is interesting relationships between curriculum and hierarchical RL so

31:36.800 --> 31:43.520
on one hand you could actually use curriculum learning in order to build a hierarchy so if an agent

31:43.520 --> 31:49.440
for example goes through a curriculum that curriculum could be targeted towards the agent learning

31:49.440 --> 31:57.840
options right so maybe you yeah so and that would be very helpful because it would ensure that the

31:57.840 --> 32:04.000
agent has has a set of options and so when you go to the next stage of the curriculum it's as if

32:04.000 --> 32:10.320
you've changed your action space in some sense right you don't have you know if you've learned

32:10.320 --> 32:17.760
how to do multiplication you don't have to learn that again now you have it as as an option and

32:17.760 --> 32:25.440
you can just employ it whenever it seems to be useful and so curriculum learning can definitely be

32:25.440 --> 32:32.880
a path towards obtaining a hierarchical representation but in hierarchical RL you can do this also

32:33.680 --> 32:39.280
in different ways so for example in a lot of the tasks that people have tried they learn

32:40.000 --> 32:47.520
the options and they learn how to choose the options at the same time and to end so an agent

32:47.520 --> 32:56.480
that let's say plays an Atari game is learning options for this game and using them in order to

32:56.480 --> 33:03.280
generate behavior in order to explore in order to represent its value function now of course it's

33:03.280 --> 33:11.040
harder when you learn many things and to end at the same time and so I think you know that slows the

33:11.040 --> 33:17.360
agent down a little bit sometimes what does harder mean here I mean there's a whole you know as you

33:17.360 --> 33:22.480
well know field around multitask learning that suggests that it can be more computationally

33:22.480 --> 33:27.680
efficient to learn or produce better results to learn multiple things at once yeah so this is

33:28.160 --> 33:34.000
multitask learning the multitask setup and generally speaking non stationary setups I think

33:34.000 --> 33:41.840
is where hierarchical RL can be the most helpful because in such setups it's worth investing

33:42.560 --> 33:49.040
some amount of time slow learning at the beginning so that afterwards you can do many more things

33:49.840 --> 33:56.160
right so you know in a multitask setup you might for example learn efficient exploration strategies

33:56.800 --> 34:00.960
and those will help you because the tasks that come later you'll still have to solve them and now

34:00.960 --> 34:06.480
you know how to walk about your environment or you know you might need to learn to use tools

34:07.680 --> 34:13.040
you spend some time doing that but then if you know how to use the tools then you can

34:13.040 --> 34:18.320
it can go and be much more efficient in in in in later tasks so I think it also depends it depends

34:18.320 --> 34:24.160
on how wide the task distribution is and how long the agent is going to live in this environment

34:24.160 --> 34:30.400
because in some sense we would expect that the benefit of hierarchical RL manifests more

34:30.400 --> 34:35.440
when the environment is more complicated and when the agent is going to have to live longer

34:35.440 --> 34:41.840
in this kind of complicated environment and be able to have it. Is there an analogy between hierarchical

34:41.840 --> 34:50.160
RL and you know thinking about the different layers in a CNN where the low levels are learning kind of

34:50.160 --> 34:56.800
more abstract things shapes textures whatever the higher layers are learning more complex shapes

34:56.800 --> 35:06.560
yes how far would you want to take that so the CNN is hierarchical in feature space right it's

35:06.560 --> 35:15.040
looking at different resolutions in the in the feature space hierarchical RL is similar in spirit

35:15.040 --> 35:22.000
but at the level of actions so lower layers would look at very fine resolution in terms of the

35:22.000 --> 35:29.440
the action actions being taken very quickly and lasting only little bits of time higher layers

35:29.440 --> 35:37.120
may look at actions that take a very long time to complete it's not as sort of clean as a CNN in

35:37.120 --> 35:43.680
the sense that you know these timescales can be variable and we don't necessarily want to separate

35:43.680 --> 35:51.120
them out in in very determined layers right and here's looping and it's not as clean at all

35:51.120 --> 35:57.600
yeah exactly so you know if you're thinking about cooking dinner uh some things take a long time

35:57.600 --> 36:02.640
you know like maybe you have if you're making bread you have to need the bread for a long time

36:02.640 --> 36:06.880
and some things are very fast and sometimes you just have to react like if the cats on the table

36:06.880 --> 36:12.560
you just have to do something immediately right and so the separation is not as fixed as you

36:12.560 --> 36:17.840
would have in in the layers of a CNN but it's the same kind of principle at the time scale we want

36:17.840 --> 36:22.720
to have fine timescales and longer timescales of which we make decisions and at which we also make

36:22.720 --> 36:29.120
predictions about those decisions and is it useful at all to compare it to like an

36:29.120 --> 36:35.440
unsombling type of approach where you've got a portfolio of submodels that you can choose from

36:35.440 --> 36:40.960
in any given time yeah so you definitely have a portfolio of submodels and you can choose between

36:40.960 --> 36:49.040
them um it's not like unsombling in the sense that usually so you have your portfolio make a choice

36:49.040 --> 36:55.200
once you've made that choice let's say to use a particular kind of model you just go ahead and use

36:55.200 --> 37:01.120
it rather than looking at let's say the the entire collection I think there are interesting

37:01.920 --> 37:07.520
things to explore in terms of planning that would be even more like ensemble methods than the

37:07.520 --> 37:14.000
kinds of methods that we have now and so for example right now when we think about planning with

37:14.000 --> 37:21.200
temporarily extended models we think about big jumps over the time scale but a model still usually

37:21.200 --> 37:28.240
predicts pretty much everything that will happen at the end of an option so at the same time we might

37:28.240 --> 37:33.360
you know if you have two hands you might have you know one hand doing one option and the other hand

37:33.360 --> 37:37.840
doing some other option and you might want to make sort of separate predictions and then combine them

37:38.800 --> 37:44.160
so I think there's a lot of room actually to think about what kind of composition operators we

37:44.160 --> 37:50.000
need right now when we use temporarily extended models we use the same kinds of composition operators

37:50.000 --> 37:55.120
as a normal mark of decision processes so think about sequences and we think about stochastic

37:55.120 --> 38:00.480
choice but there may be other interesting things to do like you know thinking about concurrent

38:00.480 --> 38:08.320
execution for example and the outcomes of that got it got it another area that you are exploring

38:08.320 --> 38:14.000
is continual reinforcement learning can you talk a little bit about that yeah definitely so

38:14.000 --> 38:22.000
continual reinforcement learning is really the natural way to do learning right it's really

38:22.000 --> 38:28.960
thinking about an agent that is in an environment that is much much larger than the agent it's

38:28.960 --> 38:36.480
enormous and the agent has a lifetime and has to continue to learn and adapt throughout its

38:36.480 --> 38:42.960
lifetime so in reinforcement learning often when we teach for example reinforcement learning classes

38:42.960 --> 38:48.720
we talk about a mark of decision process that has a finite state space it has a finite action space

38:50.080 --> 38:56.400
and the agent is aiming to go back to certain states understand which of these states are better

38:56.400 --> 39:02.320
and so on but in a continual learning setting there may not be a mark of decision process and if

39:02.320 --> 39:08.960
there is maybe the agent's lifetime is much shorter than the entire environment right so the agent

39:08.960 --> 39:15.600
can't even hope to go to each state once in its lifetime and the side effect of that is that

39:15.600 --> 39:21.760
there is pressure on the agent to use the data that it's seeing and to learn as much as possible

39:21.760 --> 39:27.440
from this data and just to continue learning over time so this is the case where for example

39:27.440 --> 39:32.880
hierarchical reinforcement learning should be very helpful because the agent can learn certain

39:32.880 --> 39:39.360
abstractions right making coffee right going for a walk and so on that would be helpful regardless

39:39.360 --> 39:45.360
of the situations that the agent is going to find itself and later and it's also worth investing

39:45.360 --> 39:53.360
the computational effort in order to to learn these kinds of representations so I sometimes think

39:53.360 --> 40:00.240
about what would it look like to to rewrite reinforcement learning without mark of an assumption

40:00.240 --> 40:05.040
right just imagine you have an agent it's receiving observations it's emitting actions there is

40:05.040 --> 40:10.000
a reward signal because we want the agent to have a task right so there there needs to be some

40:10.000 --> 40:15.520
goal specification that's the reward but otherwise we don't make assumptions about

40:16.160 --> 40:22.800
macovianness about stationarity right about there being a stationary distribution of states that

40:22.800 --> 40:30.960
the agent is visiting and one can still have algorithms that work under these settings

40:31.760 --> 40:38.400
um there are some really simple things that we can think about so for example doing

40:38.400 --> 40:44.240
usual temporal difference learning but with fixed learning rates right fixed learning rates mean

40:44.240 --> 40:49.840
that the agent is always paying more attention to the recent data and that's you know a very easy

40:49.840 --> 40:56.560
way to think about handling the the continual learning setup it's probably not sufficient right because

40:56.560 --> 41:01.360
only sort of encourages the agent to pay attention to the recent data but doesn't necessarily mean

41:01.360 --> 41:09.040
that the agent is trying to build these more abstract useful representations um so I think there's

41:09.040 --> 41:16.560
going to be a lot of interesting uh work in this area we have actually a survey on archive uh that

41:16.560 --> 41:23.280
was called out by my PhD student Kimia Ketropal um on uh continual reinforcement learning never

41:23.280 --> 41:29.200
ending reinforcement learning um and I'm quite excited to to explore uh this further and to think

41:29.200 --> 41:36.640
about problem definitions the mathematical limitations um and good algorithms obviously to handle

41:36.640 --> 41:43.360
these problems do we have a sense for computational computationally uh or you know in terms of

41:43.360 --> 41:50.800
sample complexity like how uh you expect continual RL to play out relative to classical RL

41:51.360 --> 41:57.360
so that's a really great question and it's an area where there is a lot of excitement and a lot of

41:57.360 --> 42:07.280
uh work on the theory side um so the first question to ask maybe is how do we talk about sample

42:07.280 --> 42:17.840
complexity in this case and um in theoretical RL regret has been the measure of um that that

42:17.840 --> 42:22.560
people have thought of traditionally as as a way to uh characterize sample complexity so regret

42:22.560 --> 42:28.320
is the difference between the value of what you're currently doing compared to the optimal value

42:28.320 --> 42:35.200
like how how well uh could you do possibly in an environment but if you're in a continual learning

42:35.200 --> 42:43.120
environment how what could you possibly do uh is up for interpretation right so maybe the environment

42:43.120 --> 42:49.040
is not an MDP there's not a unique optimal policy um you know what do you compare yourself against

42:49.040 --> 42:56.800
and uh there's been some really interesting work by Ben Van Roy's research group at Stanford

42:57.680 --> 43:04.880
recently looking at that question and defining essentially classes of policies

43:04.880 --> 43:10.880
uh and thinking of regret within those classes of policies in this kind of continual learning setting

43:10.880 --> 43:15.520
I think that this is something that we're gonna have to think through like what's the right measure

43:15.520 --> 43:21.680
of of complexity there's also uh tracking this is a different approach that people have

43:21.680 --> 43:26.960
thought of tracking basically says if something changes in the environment how quickly can you

43:26.960 --> 43:31.920
adapt to that change and I think that's a you know it's a little bit of a different perspective

43:31.920 --> 43:36.000
uh resetting than some of his group at University of Alberta have looked at that in the past

43:36.000 --> 43:42.640
what what's the idea with tracking as your environment is changing uh you want to quickly adapt

43:42.640 --> 43:48.480
uh to those changes so for example if you uh let's say you had the grid world and you used to go on

43:48.480 --> 43:53.360
the left path always but now that path is blocked you go and you discover that it's blocked

43:53.360 --> 44:02.400
how quickly can you find an alternative policy um and in some cases uh agents uh can do this very

44:02.400 --> 44:07.840
quickly right some of the agents can do this quickly um it all depends how good your exploration

44:07.840 --> 44:14.800
strategy is but you know an alternative to that is to say the reward signal is changing it's changing

44:14.800 --> 44:22.160
smoothly over time can you smoothly adapt your policy uh to uh to track uh what would be the

44:22.160 --> 44:27.600
the right actions right now so there's these different kinds of changes abrupt changes and

44:28.400 --> 44:34.240
more gradual changes and you want to uh to adapt and of course biological systems do this pretty

44:34.240 --> 44:41.440
well and in our algorithms if we use uh in principle if we use fixed learning rates things like

44:41.440 --> 44:48.800
value functions would also adapt continually um but exploration policies I think may need to be

44:48.800 --> 44:55.440
rethought in this context um and you know specifically a lot of exploration has been

44:55.440 --> 45:02.640
in this context continual or tracking in particular uh continual generally speaking

45:02.640 --> 45:10.320
okay tracking as a special case just because a lot of the work in in our exploration has been

45:10.320 --> 45:15.840
aimed at optimism in the face of uncertainty right so if you don't know something you should

45:15.840 --> 45:22.000
just be optimistic and go for it which is great if you're assuming that eventually you can go

45:22.000 --> 45:28.000
everywhere but if you have an environment that's very large and you can't go everywhere

45:28.000 --> 45:34.560
you might need to to do smarter things so maybe um information theoretic methods are more

45:34.560 --> 45:41.120
interesting in the setup right or explicit the uh keeping track of uncertainty or you know

45:41.120 --> 45:46.000
learning exploration strategies uh that are that are effective in the particular environment

45:46.000 --> 45:51.440
that the agents have so there's a lot of avenues for for future research in that setting

45:51.440 --> 46:04.240
does the continual RL setting lend itself more directly to a time series type of problem uh where

46:04.240 --> 46:09.040
you've got some agent that's uh making decisions that are presented to it presented to it over time

46:09.600 --> 46:15.680
relative to you know exploring environments in any way so I think there are interesting

46:15.680 --> 46:22.880
continual learning problems in time series prediction outside of decision making too for sure

46:24.400 --> 46:32.240
stock market prediction to be very concrete this one such example right uh but you know uh joking

46:32.240 --> 46:38.160
aside there is I think that the the time series prediction setting is really interesting to think

46:38.160 --> 46:47.920
about because um it avoids the problem of exploration but it still allows us to think about how fast

46:47.920 --> 46:53.760
an agent might adapt and to think about how do we characterize the difficulty of these problems so

46:54.800 --> 47:01.440
it's pretty clear intuitively that in some cases an agent may not be able to do anything so if

47:01.440 --> 47:08.560
you're at a time series where at every single time step there's some random bit that happens and

47:08.560 --> 47:13.120
the distribution that you had on the current time step has nothing to do with the distribution

47:13.120 --> 47:18.560
on the next time step well there's nothing the agent can do right so this is the sense that you

47:18.560 --> 47:25.680
know the existence of impossibility results but of course in in the real world there's structure

47:25.680 --> 47:31.840
right so I think they this set up allows us to think about what kinds of structure would allow

47:31.840 --> 47:37.040
an agent to learn successfully even if the environment is changing so of course for example if

47:37.040 --> 47:42.080
if there is gradual change in the environment let's say your bit is drawn from a probability

47:42.080 --> 47:48.080
distribution whose mean is gradually shifting over time then the agent can hope to learn

47:49.040 --> 47:53.600
how this shift is happening right and then perhaps even learn to anticipate what will happen

47:53.600 --> 48:00.880
that's but you know smoothness is only one particular kind of structure there may be others that are

48:00.880 --> 48:05.840
that are more interesting from practical point of view to wrap things up I'd love to have you comment

48:05.840 --> 48:16.000
broadly on the field of RL you know over the years you know RL has it's been hard for folks to

48:16.000 --> 48:20.880
do hard for folks to get up and running how do you see that part of it evolving

48:20.880 --> 48:27.360
what do you think are some of the big challenges going forward beyond the many that we've already

48:27.360 --> 48:35.120
talked about of course yeah thank you for asking about that I think RL has made tremendous progress

48:35.120 --> 48:42.160
and it's again from my point of view it's the closest to biological learning right it's the

48:42.160 --> 48:50.080
closest paradigm that we have in the field of machine learning to biological learning so it's

48:50.080 --> 48:58.480
very important for us to to think about it and deep RL has in fact delivered many very surprising

48:59.520 --> 49:09.360
successes ranging from alpha-go years back to more recently things like work by Marc Chandron

49:09.360 --> 49:17.120
Belmar on rooting loon balloons in the stratosphere using deep RL methods recent work by my colleague

49:17.120 --> 49:23.920
Martin Reed Miller at the mind on plasma control for fusion reactors these are very complicated

49:23.920 --> 49:28.720
control problems and reinforcement learning algorithms can handle them and and do a really great

49:28.720 --> 49:36.560
job so you know that makes me feel very optimistic that that we are able to scale RL and to really

49:36.560 --> 49:44.240
deliver some interesting practical results at the same time I think yes there are challenges and

49:44.240 --> 49:49.200
a lot of them have to do with these problems that we talked about you know discovery and

49:49.200 --> 49:55.680
efficiency how does an RL agent uses data very efficiently how does it do efficient exploration

49:56.400 --> 50:02.480
and how does it construct really good representations and historically reinforcement learning

50:02.480 --> 50:08.080
has relied a lot on other machine learning technology for example in order to build representation

50:08.080 --> 50:14.240
so we use deep nets deep nets are wonderful and they've they've led to a lot of these these great

50:14.240 --> 50:23.680
successes but in some sense that specific set of methods was developed for supervised learning

50:23.680 --> 50:29.360
right and supervised learning is based on a different set of assumptions than RL much more

50:29.360 --> 50:37.520
IID and so the way that we've made progress historically has been to kind of take RL methods and

50:37.520 --> 50:42.800
make them more supervised learning like using replay buffers is a classical example of this in

50:42.800 --> 50:49.680
order to train let's say cue learning in with deep networks I think it would be very useful for

50:49.680 --> 50:56.560
us to think about function approximation and optimization in the context of RL and how do we do

50:56.560 --> 51:02.000
that efficiently one of my students Emmanuel Bengeo who just graduated in fact looked at this

51:02.000 --> 51:09.840
problem last year and he discovered that looking at let's say atom optimizers in the context of RL

51:09.840 --> 51:14.800
doesn't give you that great results right in some cases really does not combine with

51:15.520 --> 51:20.560
temporal difference learning and and similar algorithm so I think one of the challenges for us

51:20.560 --> 51:25.200
is really to think about the function approximation and optimization problem in the context of RL

51:25.200 --> 51:29.920
and also to think about this continual learning setup because that's where we really want to go

51:29.920 --> 51:35.040
before going to build general intelligent agents they have to learn all the time they have

51:35.040 --> 51:41.680
to learn in these settings where you know Markovian structure doesn't hold and so that's that's

51:41.680 --> 51:47.120
the frontier awesome awesome well doing it thanks so much for spending some time chatting with

51:47.120 --> 51:52.400
us and sharing a bit about what you've been up to in the space very cool stuff you mentioned a bunch

51:52.400 --> 51:58.720
of research your own and others and we will try to collect that from you and be sure to include it

51:58.720 --> 52:03.840
on the show notes page but once again it's great chatting with you thank you for joining us

52:03.840 --> 52:30.560
likewise thank you for having me

