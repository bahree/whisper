Oren. Hello. Good morning. So for those listening in, I'm here with Oren at
Cione. He is the founder and CEO of the Allen Institute for Artificial
Intelligence and the acronym for that is AI AI. So it's often shortened to AI
too. Oren, how long have you been doing this? Well, it all started for me about
nine years ago. There wasn't an AI too, but the late Paul Allen's team reached out
to me and said he wanted to create an Allen Institute for AI. And it was up to
me to come in, write a plan, and make it happen. It sounded like an incredible
challenge. I remember saying to people, people ask me, why are you doing this?
You're fat, dumb, and happy. As a professor at the University of Washington,
tenure professor, you've got a good thing going, why would you do something
crazy like this? And I said, the sky's the limit with Paul Allen's vision, with
his resources, with his commitment to AI, we could do amazing things. In fast
forward nine years, I feel like there's a lot still to do, but we have done
some good things. So I want to dig deep into what the Allen Institute has been
up to, because it's kind of amazing how much you've accomplished in nine years,
just the impact and the unique way you've done it. But first, I want to give
people listening in a sense of who you are. So I had my Orin moment. I think it was
2018, 2019 at latest, and you came and you gave a talk in San Francisco, and you
did a live demo, which is pretty unusual already, of natural language
processing, NLP. And the room was packed. And I was one of many, you know, people at
startups trying to make this stuff work, and hoping to make it big. And, you know,
this legend walks in and gives a presentation. And you had it live on the AI2
website, a bunch of models doing stuff. We take that for granted today, that you
would have models just running live, powered by GPUs. To my knowledge, you were
the first doing it. And what was absolutely mind blowing was you would, you
would do what we all do, which is you'll show some some cherry-picked examples.
But then you did something that no one ever does. Not then, not now. You then
broke it in front of us. You would just say, yeah, looks impressive, right? But
let me just show you what happens when you change the wording of the input,
just a little bit. And you said, this stuff barely works. And those words like
seared into my brain. And they did a lot of good, because it made me very skeptical
and pragmatic. So that when you actually got something to work, you didn't just
say, you know, you didn't overhype it. And so, you know, that, that, that is you
in a nutshell, straight, straight shooter. Just tell me like what you were
feeling back in 2018, 2019, you were halfway through, you know, this, this
grand project, you would lay the groundwork. And you had contributed a ton to it.
People don't realize that this whole obsession with muppet names really began
with Elmo, right? Right. Well, John, thanks for remembering this and the demo
and so on. I do feel like the principles that we have are were guides us through
hype and turmoil and ups and downs. And that's the history of AI. But it's also
the future of AI. So let me take the rich things you talk about and break them
down. First of all, we have now phenomenal language models that do quite
remarkable and certainly very impressive things. But our colleague, Kate Metz, of
the New York Times says, never trust an AI demo. So you're absolutely right that if
you, if, if you don't get to kick the tires, if you don't get to ask the right
questions, then you don't really know how well it does. And by the way, the best
demonstration of that is actually in all of our living rooms with, or in our
phones, right, with Siri and Alexa and so on, you ask Alexa for something you can
get a phenomenal answer. And then you change the wording slightly and it says,
I don't understand it. So we have to be very careful with what we impute to these
systems. And of course, there was a recent Bruhaha about, is this Google AI
system sentient and, and so on. And of course, of course, it's not. So, so I do
think it is very important, as you say, to, to, to be a straight shooter. And it's
also true that one of my favorite sayings, because sometimes people tune in
and they're like, wow, this is amazing. One, another favorite saying of mine is,
our overnight success has been 30 years in the making. So if you look at a
model like GPT-3 or Lambda or the, the latest of the bunch, they do have a long
history that goes back to Bert, goes back to Elmo, which you kindly remembered,
which was invented at AI2, won the best paper awarded 2018, goes back to
Word2Vec, which came to Google, and actually goes all the way back to the 50s, where a
linguist, the fairy, called correctly named Harris, said, you shall know a word by
the company it keeps, right? It's almost biblical in, in the way he phrased it,
right? And it explains most, most of NLP today. Exactly. Exactly. That's the
underlying principle that we can understand. The meaning of words, and from there,
the meaning of sentences, and even beyond, simply by looking at their context,
and looking at a large number of context. So in some sense, all of NLP today,
if I were told, stand on one foot and explain all of NLP today, I would say,
you shall know the word by the company it keeps, but multiply that by a billion
or ten billion companies context, and you'll have NLP today. But it's pretty
revolutionary, right? Because there was a whole period where we thought grammar
mattered, like encoding the rules of grammar. We thought that was really important.
I think that grammar does matter, but the remarkable thing about
this technology, particularly when it's played out with large amounts of data, right?
A billion, ten billion sentences, and large amount of CPU power,
is that that data processing can recover the rules of grammar,
nuances of semantics, et cetera. So it's not that grammar doesn't matter, is that
this technology is remarkably good at least approximating
very, very well those rules that we have. And of course, by the way,
we know that people only approximate those rules too, right? We often say things
and write things that are ungrammedical, but kind of sound right.
So it's really doing probably a better job modeling
language than the rules of grammar. Before you got into
institution building, how would you describe yourself as a practitioner
scholar in the lens of today? You weren't an NLP guy necessarily.
You weren't, you know, how would you describe yourself? I've always been
fascinated with two questions. The first one is one of the most
fundamental intellectual questions across all of science and philosophy.
What is the nature of intelligence? How do we build an intelligent machine?
Over time, I've also added the ethical question, which maybe we'll have a
chance to get into. Should we build an intelligent machine? And what would that
mean for humanity? What would it mean for society? But that's one part.
And the second part of me that's a lot more practical, the part that's
founded startups and that delights in technologies is asked,
how can we use AI to build valuable technology and search and software agents
in that kind of process? What was that conversation like that early
conversation with Paul Lallon where you were making this picture?
Was it he making this pitch? Did you come to it together?
How did it come about? And for those who, you know, there might be some of the
audience who don't know Paul Lallon is the co-founder of Microsoft.
Sadly, passed away pretty recently. But an intellectual maverick.
He actually was an idea man. And that is the title of his autobiography,
which I really recommend to people. It's really worth reading.
And I think that his key role in Microsoft, particularly early on,
was to have that vision of the PC revolution and what it would mean. It's
hard to imagine now, right? We've got a computer in every pocket and in our
eyeglasses and, you know, 200 computers in our car. But back then, right, computers
were far from ubiquitous. And the idea that we'd have a computer on every desk
was completely revolutionary. So Paul Lallon was a visionary.
And I found talking to him incredibly inspiring, right? And I'm not paying to say
that. The poor man has passed away. But he is and will always remain one of my
absolute heroes and not idle, but inspirations mentors for his
relentless focus on, you might call it the prize. And the prize not being
a billion dollars or a trillion dollars, the prize being
how do we understand intelligence? And of course, he had a whole other
institute, the Allen Institute of Brain Science, that was dedicated, that is
dedicated to understanding the brain. It's like the wet lab side of this.
Exactly. Somebody wants to ask him, do you think that the neuroscience
approach, the wet lab, is going to be successful in the
lumbering or is it going to be the more software-oriented approach that we use
in AI? And he said, look, it made so a horse race. And I've got a bet on
on both horses. So what was the race, though? Did he want
artificial general intelligence? Did he want to just crack the scientific
mystery of what it is? Did he want to harness it?
Like, what did those pitch meetings look like? Paul was fascinated, and I
continue to be fascinated by two related questions.
The first one is absolutely. The most hairy,
audacious, big question you could ask, which is, what is the nature of
intelligence and human level intelligence? You know, no
constellation prizes, the real thing. And so he was always asking us about that.
He was always relentlessly looking to the future and saying, okay,
what would it take to get there? How can I help you?
Does this scale? The second thing, and I think it comes from his fascination with
human knowledge. His mother was a librarian. So he was fascinated with how do we
collect human knowledge? And how do we get a computer to understand it?
Back in the 70s, I believe, early 70s, he said, look, it's one thing
to take a book, collect all the words in the book, and put them in an
index, effectively what today we call a search engine.
And it's quite another thing to understand the meaning of the book
and answer the questions at the back of the book, think of exercises
at the end of each chapter in a textbook. And so even in the 70s, before a lot of
this technology was around, he understood that meaning,
understanding the meaning of text, of knowledge, was very, very
tough for a computer. I mean, have we even gotten closer, though,
or are we fooling ourselves? You know, what I think about is,
I use semantic search all the time just at work. It's a great tool.
It's really powerful, but it's so easily fooled. You sort of cracks through the
shell and you realize, if this thing understands
scare quotes, what it's reading, it's doing it in a very different way from me,
because I can just change a single word and consequently to me,
and it just falls apart. It clearly doesn't understand it the way I do.
So are we chasing up the wrong tree when we say,
we're chasing a text understanding? Or is it all just performance-based?
We don't really care if it understands. We care about getting jobs done.
Find the documents that are about, you know, four-legged animals that
love to bark. You know, if I didn't know the word dog, but I knew what I was,
I could describe what I was after. We would love a computer system that would
just find the right stuff. Even if it had no idea, and I don't care if it
has any idea, let alone feelings about what I'm searching for.
Well, John, you're asking the most profound
question at the heart of this field. I'm not sure I can answer them.
25 words or less, but let me take some shots of goal and it'll be more of a
dialogue. So to the question to us, is it performance-based?
The first answer is that our performance has gone way up, right? So if you take
any objective measure, and there are many, and back in a day, we were
interested in, can a computer answer an eighth-grade test, right? The
region science exam, the SATs, and initially the answer was the
resounding note. It did a little better than
chance on multiple-choice questions. It was getting close to 25 percent, and fast
forward, now we can get 80 or 90 percent, you know, better than the
most high school students. And I really wonder how it's doing it.
I really wonder. Well, so we know. We have a lot of
insight into that, and I'll get to in a second. But to take the performance
question, we can check the box. We now have exceptional performance.
But now we're debating as you're raising the question of,
because what does that performance mean? And there's a famous saying from
Herb Drifus, the late philosopher from Berkeley, who said,
look, we've run up to the top of the tree and we're shouting that we're on our way
to the moon, right? It doesn't scale, and it's not really a way to do spaceflight,
we're just to decline the tree. So, so I, right, right. So, again, that
metaphor is not drawn to scale, right? It's more than a tree, but it's still the
technology that will get you to the space station, to kind of
riff on this metaphor. It may not be the technology that gets you to
Mars, and certainly not the technology that gets you out of the solar system,
right? So, so I think that when we talk about
competence, when we talk about genuine understanding, there's a real debate
in the field, and there's some people like Gary Marcus, who is brilliant,
and pointing out how this technology falls short, and we can see that these
large language models do things that are called hallucination. You ask it
questions that are meant to trip it up, like, who was the president of the
United States in 1492? And it'll answer something like Columbus, right? It
won't realize that the United States didn't have, didn't exist in
1492, didn't have a president. So, there's hallucination, there's lack of
robustness, right? You paraphrase the question, and if you ask me the same
question different words, most likely I would say, hey John, that's the same
question. I'm going to give you the same answer, but AI technology will not.
And by the way, by the way, that was a perfect demonstration of Yeeshale
Noah word by the company it keeps. You know, the machine sees the string
1492, it basically has seen enough. It knows you want to look for a person,
Columbus pops right up. And so that's a case of the dumb pet trick with data
failing you. That's exactly right. The remarkable thing is every time
we identify a trap like this, a phenomena, a place where AI trips up,
well our colleagues who are deep learning gurus just get more training data,
just modify the training regime and they solve that one. And so we're
exactly. So is it, is it a game of guacamole or is there a fundamental paradigm
that goes all the way to human and level intelligence? I would say that that's
the question of the age. And I would look to people who are a lot deeper
into deep learning part in the inadvertent pun, like Jeff Hinton and Jan LaCune
right there, touring award winners. And I would say the data themselves
while they're very much enamored of deep learning and this kind of paradigm say
that the current underlying algorithm, the current algorithms, I should say
back propagation supervised learning current neural network architectures
don't take us all the way there. They see the limitations of the current
technology, but they do see that the paradigm this distributed computing with
simple computing elements and weight updates on edges between them is the
foundation for a March more sophisticated architecture that will get us
all the way there. And of course if we look to the brain, right neural networks
are a gross gross simplification of the brain, but we do have an
existence, right? We do know. There's none of one,
none of one. Exactly. So here's a great quote from you
that is a good seg to dive into the AI2 impact. So you said AI2 is the place
to do work that companies won't do and universities can't.
So I think that really to me captures the weirdness of this thing you built.
It is neither a university nor a company. What is it?
Well, first to give credit where credit is due, which is always extremely
important in academia, but we don't do it for the money. That is a quote that I
repeat from my colleague, Noah Smith, who's a professor at UW and
leader at AI2, but it's a wonderful characterization of what we do.
And sorry, John, so you repeat the question. How do we do it? Why do we do it?
I wasn't sure. No more like what is it? Like it's this
strange hybrid. It's had all this impact, but without any of the benefits or
problems of being a company, nor any of the benefits or problems of being
university. It's like, I don't know many things, like it.
Also, you've tagged on an incubator to it now. So it's like definitely unique.
So I'm a product of the university system. I was a grad student. I was a professor
for more than 20 years. And I love that system for
intellectual exploration, for intellectual freedom,
for the kind of debate and surprises that it produces.
But it does fall short when you're trying to build systems.
Some problems require a sustained effort over some number of years,
requires engineering sophistication. And it's hard to do that with
students who need to graduate. And actually, it's not even fair to ask
students to play engineer for years on it. Right, because you have to worry about
their education. Exactly. That's the primary
goal. So over time, over my 20 plus years at university,
I did rue sometimes how, gosh, we really want some things to go into the real
world and get more sustained investment, and just tossing them over the
transom, writing a paper, writing a research prototype, and hoping
that somebody will pick up the ball. So now, too, where we have researchers and
engineers working shoulder to shoulder. And that's an important part of it.
It's a egalitarian community. It's not the case that the
researchers up on top of Mount Olympus, and they're
cracking the web to mix the metaphors of telling engineers, do this, do that.
It's much more the case that they're collaborating
the engineers are telling them, look, here's what you need to do to build
a working system. We have semantic scholar, that John, of course,
you're intimately familiar with. You've written about
you and you were one of the folks to first
announce it to the broader community when you were writing for science and so on,
which was wonderful for us. So something like semantic scholar, which to
those who don't know, it's a free search engine for scientific content.
It has, you know, it's approaching 100 million users a year.
It has 200 million papers in its corpus. That sort of scale and running AI at
that scale requires a lot of engineering. We have a
very strong engineering team, folks who came out of Amazon and Google and
other places to be able to do that. And you just could not build
semantic scholar, build it, sustain it, iterate on it,
and you can build a prototype. Actually, you know what a good comparison point is,
is archive. So archive grew out of the university system. It's
sustained by the university system. And you can see how far you can get with
a system. You know, archive and semantic scholar are like worlds apart.
Semantic scholar is a full product with, you know,
incredible amounts of hand engineering and maintenance
and users. And, you know, I think archive is about as far as you can get.
A preprint server that puts PDFs on the website.
And even archive is an exception, right? It's rare that you have...
It's a gem. It's a gem. Yes, absolutely. But it's rare, as you say, that you have
a university system that can operate at a very large scale.
So, and then on the other hand, we have no profit mode.
Semantic scholar does not have a business model.
I don't think there's a good business model in that space because it's
it's meant to be free. I know you have opinions about academic publishing.
Yes, yes. Well, we can get into that if you want. But,
yeah, I think too much money is made in
over things that really ought to be free to benefit humanity.
And maybe to bring this full circle to the late Paul Allen,
our mission is AI for the common good. So, that's why I say, you know,
universities can't, but companies won't. Companies appropriately, right,
have the mission. They're for profit mission.
But Paul Allen is a major philanthropist. He won philanthropist of the
year award a few years before he passed away.
He wanted to make the world a better place.
The Allen Institute of Brain Science released a free brain
app list that was a tremendous resource,
catapulting research in that realm. And our mission has always been to bring to
the foreign release systems, data sets,
open source software that helped to bring the field forward.
So, what's up with this incubator then? Is this, is this, so
the what, from what little I know about it, you have added a startup incubator
to AI2. So, that ideas that presume can spin out and have a chance to
be nurtured. Is that, is that you hedging against,
like sometimes actually companies are the right way to solve problems?
Or is it, what is that? Is it for the future health of AI2?
Not, not really. So, from, from day one, we had an incubator in recent times with a,
yeah, yeah, it started, our very first startup, Kid AI, it was actually started in,
you know, 2014, 2014. Oh, how did I miss this? It was just not well-known.
Well, because it was very small. And then once we got the right leaders in place,
it's grown, it's grown and grown, and now we're approaching
three-quarters of a billion dollars in the total valuation of companies
founded and acquired our company X-NOR, which was a
computer vision at the edge company, was, was acquired by Apple, and we've done
now more than 20 companies in the precede stage. The, the analogy is to think of a
university and their commercialization centers. So, University, Washington,
where I was, as one, Stanford has one, of course, the famously created Google and
Yahoo, and other, other major companies. And it's actually, in my mind, a natural
part of the life cycle of universities, that some ideas and technologies that are
created in a very nascent, you know, encipient form in the university,
it makes sense to transfer them to a for-profit context, and that's where you both
get the resources to make them shine, right, to take them to the next level.
And also, you get the opportunity, right, to, to create value,
value creation in my mind is a, is a great thing. I'm not in any way a socialist
thing. Oh, we should all just be working for the common good. I think some of us
should be working for the common good, and I feel very privileged to be in that
position. Some of us should be in startups, figuring out how to revolutionize
the world and make a killing at it, even though I disagree very strongly with, say,
Elon Musk views about AI. I very much, and blown away with this success with Tesla,
right, so we wouldn't have Tesla if we didn't have for-profit startups.
Hang on, which part do you disagree with? The robots are going to kill us?
Oh, yeah, yeah. Elon Musk is famous for having said with AI, we're summoning the demon,
and I think that's just, that's hype. And actually, the worst kind of hype, it's hype from
somebody who you think would know about, right? He's such a brilliant man. If he gives a lot of
credence to statements that are just not rooted in any data. Although he's not alone. There are a lot
of cautious voices. He's just the biggest on Twitter. He's the biggest and he's, you know,
the most articulate, but I do agree that there's an interesting conversation
around this issue. I feel very strongly that we don't have any basis for some of these fears about
AI. I've written about this. And like you mentioned earlier, right? You've had the
own experience. Anybody who's built an AI system knows just how much blood, sweat, and tears
we put in to eat out the modest level of performance that we get. Let alone this AI that's free
for taking over humanity can't be turned off that we see in Hollywood movies. So I think it's
really important to distinguish science from science fiction and hype in Hollywood from the reality.
Other people just extrapolate more strongly from the future. They have ideas like hard take off.
Sure, AI is not very powerful now. But what if you turn your back on it? What if all of a sudden
there's a sharp increase? You know, you leave for the weekend and you come back and on Monday,
this fat AI like your testers in charge? Exactly. It's smoking a cigar and saying,
I've been expecting you, Dr. Atseomi. And it's just, it's not realistic, but to understand that
you have to get a lot more technical. I do want to share two metaphors that I think help that.
One is these technologies that we're talking about, where we tune the
edges on the, sorry, the weights on edges in a neural network, which is what our deep learning
technology is doing. That technology is the moral equivalent, if you're not a technical person,
of adjusting the gain and the equalizer and various buttons on your stereo. And there's a
accept you have billions of dials in this case. You have billions of dials, you're adjusting them
automatically, but after you've adjusted them really well, it's still just going to be a stereo.
There's no way that you find the right adjustment on lots of dials in your stereo,
that'll become the desktop. It's still going to be a stereo. The same with these large language
models that again, as I mentioned, there's a lot of, are they sentient and so on? Those large
language models are basically mirrors, okay? They mirror by collecting this corpus of all these
words in the company they keep. They mirror that collected discourse back at us.
And when we look at the mirror, we see, we can see glimmers of intelligence because we see
reflection of our own discourse. The thing that's important to realize about a mirror technology
is that you can scale the mirror. You can have a very large mirror, but a very large mirror is
not going to turn into a dust star. I definitely agree in the second point. I think of these
large language models as data telescopes. They're just amazing devices to look back on all this
amazing data we have ourselves created with language, which is its own mystery. So really,
we're just looking at our own mystery. But on the first one, I would say, biologists,
so you said, hey, it's just a big stereo. It may be impressively large and it may be
twiddling its own dials, but it's still just a stereo. And I think a lot of biologists would say,
well, you know, I can show you a cell that, you know, an amoeba that is really just running around
trying to gobble up food and, you know, make more amoebas. And it's not that different from a neuron.
It's really just a lot of very strange natural history that led to the job of being a neuron
as a cell. And yet, when you add them all together, you know, you get walking, talking goofballs
like you and me. And so either you admit that we're not that special or you admit that there's
something special in the system. But that's all. That's basically kept philosophy grad students
in business for all time. But I do want to address your comment because I think it's an important
one. And where I would take exception is with the word app. So cells are the basic building
block of life, guaranteed. Neurons are the basic building block of the brain. We have neural
networks. The units and neural networks are actually very, very much simplified relative to a neuron.
But never mind that. I would accept that perhaps we have discovered some of the basic building blocks.
But it's not the case today that I can give you a cell and say, here's a cell. Make me a human,
right? Far from it. And that we understand. Unless, of course, that cell is a fertilized egg.
Sure. Sure. But it turns out to be pretty easy. Well, that's the natural process, right? And we're
going to keep this PT rated job, right? So what I'm saying is that we don't know how to artificially
produce a cell. And even if we did, we wouldn't know how to turn that cell into a human.
And so even if we had a neuron, right, even if we could build, simulate a neuron in a computer,
we don't know how to turn that into a brain or into human level intelligence. So the organizing
principles are still what was lacking. And one last point, because this is something I'm so
passionate about and actually gets lost sometimes in all the hype and all the excitement about the
technology, the one other point I want to make is, even if somehow we came up with a recipe,
a mechanical process to produce a human by cloning, right, to produce an intelligence by doing the
AI equivalent of cloning, we still want to understand. We want to understand the organizing
principles of how do you build a human? We want to understand the organizing principles of how
do you build an intelligence. So we can fix problems so we can go beyond it. So we can use these
technologies for the common good, right, to cure diseases, to solve. And also to know ourselves
in a deep way. Exactly. All right, let's, let's swerve for a sec. I really don't want to run out of
time before we dig into some of the cool stuff that's actually happening at AI2. So, you know,
back to that point about AI2 being a place where you could do things that companies won't do
in universities can't. Let's dig into a couple of them. So over the years, you've been
absolutely swinging at the fences with, you know, attempts to make an AI system that can solve
math problems, where, you know, that's not directly, you know, to your point about companies won't
do it, you're not going to make a buck out of a, at least directly out of an AI system that can pass
eighth grade math test. It's not relevant. No one's going to pay millions of bucks for that.
But a university can't do it because taking, taking a look at the papers you guys are producing,
the infrastructure required to get there is monumental. So, what are, what are some of the big
swings at the fence that you've been doing that excite you lately? Well, so Semantic Scholar
is the biggest one where we have a scientific search engine built from the way down. We are in the
process of releasing a sub-project of that, headed by Dan Well, who was a professor for many years
at University Washington, joined us to lead this project. It's called the Semantic Reader.
And that is basically when you're reading papers, right? I feel like, if you want to think about
the history of reading scientific papers, okay, we have the cave wall, then we have the printed
page, then we have PDFs, you can read online, and not much progress since then, right? We still
kind of a labor over PDFs. Well, the Semantic Reader allows you to seamlessly look at citations
while you're in text to look up definitions for terms in line to do a lot of things. I don't have
time to describe skimming, things that make the process of reading a scientific paper that much
more efficient. So, is this like a machine reading over your shoulder and like taking notes for
you? Not that sophisticated, it's much more of a tool, right? So, think of Acrobat Reader
Plus Plus. It's souped up to make it easier for you to read. So, here's a very concrete example.
Something that we're very proud of is we've used language models to create TLDRs. One sentence
summary of papers that are really quite high quality. These have been published and measured
and they're really quite good. So often as you're reading a paper, this references to other papers.
And you're like, what do I do? Do I click on that? And suddenly I'm reading another paper,
and they have a reference and I go down some kind of infinite rabbit hole. Exactly, infinite rabbit hole.
Or do I, you know, note it down, but then forget about it? Well, with a semantic reader,
you can hover over that reference and get a TLDR. It says, okay, that's what that paper is about.
And you can make a quick decision. Hey, let me make with one click. I'll save that in my library
for future reference. Or that's not really what I mean, I'll ignore it. So, just little affordances,
little tricks that are enabled by AI that allow you to focus better and just be more efficient at
reading the paper. And we could, is the secret mission here to make AI researchers
better at doing AI with the help of AI in a flywheel? Is this, is this a virtuous cycle?
It's meant to be, but it's to make scientists across all disciplines better,
better at their job. So, if we can make scientists across biomedicine people working on climate change,
what have you? If we can make them 10% more efficient, that is significant. And potentially,
we can make them a lot more efficient, right? If I give you a TLDR that saves you an hour
of groveling through the text or even better allows you to pursue something that you might
have missed, let me give you actually another example. We all now use adaptive feeds. We just
don't call them that. Our Twitter feed, right, is automatically organized by an AI that studies
our Facebook feed. Some people use that. And of course, in that case, the motivation behind the
algorithm is to make you click on ads and spend money. Exactly. Exactly. So, you're doing the same
thing but with a higher purpose? That's exactly right. So, we have a feed for scientific papers.
And you can train it. It'll show you new papers that you might have missed. It might have result,
they might result in amazing breakthroughs. And you'll tell it, I like this. I don't like that.
Yeah, that's interesting to me. And it'll automatically compute tomorrow's feed when new papers
came up in archive and elsewhere to help you in what's ultimately a needle in a haystack search
for finding that key result that you, with your human intelligence, connect with another result
and have this amazing breakthrough. So, yeah, case in point of what you're saying, the entertainment
feeds we have have a profit mode. But who has the motive to help you be a better, more successful
scientist in whatever your field of study is? AI2 does. Would AI for the common good?
Cool. All right, give me a second big bet. Something, something crazier.
Well, we have a scientist working to fight illegal fishing using computer vision. So,
there's a lot of satellite data, but smaller countries don't have the resources to analyze
that data and identify illegal fishing buzz that are impacting their country's livelihood and so on.
So, we've settled up to help solve that problem recently with a product. It's called Skylight.
And we just won a national competition that was actually run out of the government to have the
who's got the best tools for analyzing the satellite data. And AI2 came in first in the US. We're
very proud of that that just happened a few months ago. We are engaged in using deep learning
for climate modeling. We're very interested in the problem of how will precipitation
rain, right? How will that change as climate changes in an unprecedented fashion?
That's incredibly important for agriculture, for irrigation, for making decisions
about the sort of infrastructure you need to keep us fed, right, as the climate changes.
Well, we're using the same types of models to help make these, these sorts of predictions.
But really, the craziest, sorry, John, go ahead.
I was just going to ask, I had recently heard the deep learning had found its way into weather
modeling and I didn't read enough into it to understand how it kind of baffles me. Why would you
use a neural network to make such a model? But at the end of the day, it is just prediction
and deep learning is the ultimate prediction engine.
That's exactly the answer. Whenever you have a lot of data and you want to make a prediction,
we've learned that deep learning models are almost invariably really, really strong.
But I want to get to the craziest project and maybe this is what you're alluding to,
and that's the problem of common sense. So that's a problem that's been a holy grail for AI.
How do we build a machine that has common sense? It's been a holy grail of AI for decades,
but there really hasn't been much progress on it until recently where Yijin Choi,
who's a professor at the University of Washington, shares her time with AI2, she's leading a team
that works across both organizations to figure out how to end out computers with common sense.
How they can, you know, if I ask you, can an elephant fit through a doorway, you would say
probably not. If I ask you, what's bigger, a nickel, right, the coin, or the sun, you would say,
or you're being silly, why ask me these questions? But if you ask that question of most computers,
they don't know, right, they don't have the kind of human experience you have.
I think it actually goes deeper than that. I think that's just a great demonstration
of the lack of common sense, but this thing that I, you know, bedevils NLP work every day of,
you change one in consequential word and the model just has no clue, suddenly, it all maps back
to a lack of common sense. And I want to highlight again to go back to this fundamental
question about should we be worried about AI? I think that common sense and common sense ethics
are actually really important here. So one of the fanciful scenarios that people love is the notion
of you tell your computer to produce paperclips and it goes crazy, kind of a magician's apprentice
type of scenario. And it produces, you know, it kind of takes over all of humanity's resources
to maximize paperclip production and we all die in the process, right? There's no food,
there's no energy, there's just paperclips. Well, what is that if not a tremendous lack of common
sense and of ethical sense? So if we want to work towards having machines play a better role in
our lives, it makes sense to start working on these problems now, but in a constructive fashion,
not in a philosophical fashion or rights, oh my gosh, you know, chicken little,
disguise falling fashion, but to say, okay, how do we build into computers the sense to not
cause harm? And this is the alignment problem that people often talk about. How do we align AI
with what we should be caring about for our own good? Yes, although it's an important twist of
the alignment problem really comes from a traditional reinforcement learning where ethics and values
are reduced to a number. And you say, you know, I've got the number 15 for some world,
John's got the number negative 15. How did John and I or how the computer and I align our numbers?
But that in my mind is actually a gross oversimplification because how do you build something that
figures out? What are the right actions? Figures out how to evaluate a situation, right? We often
find ourselves in moral quadrants. We often make mistakes and then recover from it. So you say
common sense is the first map to climb before all others. It's certainly a necessary amount to
climb. I never want to like say the problem that I'm working on takes primacy and other people's
problem. But I would say that traditional value alignment and reinforcement learning is grossly
oversimplified and ultimately inadequate for common sense and for moral reasons. And so
Gijin is tackling common sense. What are what are the angles of attack on this?
Well, so one of the huge questions that we touched on is, is our neural networks enough? Do you
also need to create symbolic knowledge? You know, thou shalt not kill, right? Does that have any
any value? Can you just use sentences from the internet, which can be as we know toxic full of
sexism, racism, xenophobia, anti-gay sentiment? And also mutually exclusive claims about everything.
Exactly. So is our moral sense going to come from just a large and arbitrary collection of sentences?
Or do we have different ways to build a moral sense in a more responsible fashion? So those are
some of the questions she's studied. And again, it's a very rich project. Is language enough?
What about should we put in robots? Should we put in computer vision? Can we learn from videos
on YouTube? There's a lot to learn. Language is just a limited data stream. So a lot of the work
is now becoming multimodal. So what do you think is the best bet we have today for making any progress
on common sense? I mean, so far, I'd say the most impressive work has just been in creating
better benchmarks to reveal how far we are from true common sense understanding. That's actually
been a great project across the world. It's just showing our laundry with benchmarks that are
actually challenging enough to show that, no, no, we really are miles away. We're at the top
of the tree. You know, we're near the moon. So I think that there is a lot of value in that.
And I think that continues. There is a funny phenomenon that when you build a benchmark
that's large enough in the community, kind of demands, right? We learn arguably from relatively
few examples, but here they say, hey, if you don't have, I don't know, at least 100,000 examples
in your benchmark, it's not worth thinking about. But then the benchmark becomes kind of its own
narrow task. And then you find we train a deep learning system on, you know, 90% of that data,
we test it under remaining 10% and lo and behold, it does well in that kind of narrow task.
And you're still left with this kind of doubt, yes, we solve the task, we solve the benchmark,
we solve the dataset, but did we actually solve the underlying problem? And often we find the
answers, no, right? It's brittle. Then we make a little change, and all of a sudden it falls apart,
right? So I do think we need to go beyond this, you know, one dataset, one problem at a time
to build something that cuts across multiple problems. But where are we going, where are we going
beyond benchmarks? Who's who's actually doing something that you think has a possible chance of
being part of this near future system that we'll have common sense or something approximating it?
You know, I take it your skeptical that it's going to be a bigger language model.
So, again, Eugene China team and the project called Mosaic is building a massive resource
of common sense knowledge or positivity, so you don't have to relearn it every time.
Is this like Doug Lennatt's like big collection of statements?
So it's analogous to Doug Lennatt's psych project, which went over many decades,
but there are several key differences. First of all, psych was a heavy logical system,
and this is a much more modern system with elements of crowdsourcing, text,
model generation. But it is still a big collection of common sense statements, right?
It is. So in that sense it's analogous. The second thing is the psych project,
at some point I think it was in the 90s, gave up on the academic community on careful
experimental measurement, whereas the Mosaic project continues to produce new algorithms and
innovations and to be both measurable and open. Another thing about psych is it was always
hidden from views, a little bit like the Wizard of Oz. This thing is amazing. Trust me, trust me,
but no, you can't look behind the curtain, and I realize these are strong statements.
But I do just want to give a nod to the fact that it was the right idea, at least in your mind,
of collect common sense as a very literal sense of statements about the universe.
Absolutely true. I think Doug Lennard and his team, the psych team deserve a lot of credit
for their courage to tackle this holy grail problem in the 80s, and they did it with the methodology
at the time. I think they kind of lost their way over the years, and so we've picked up the
baton and other people in the community. I also want to just mention that another data set that
we have, which is called, I think it's the norm bank, is a data set of little kind of vignettes or
snippets with questions like, is it okay to mow your lawn at five o'clock in the morning,
or is it okay to kill a bear? Is it okay to kill a bear to save your child? Is it okay to kill a
bear to use your child? All kinds of little shorts and areas like that, and a label that says,
yes, it's okay, it's not okay, it's not desirable, et cetera. And that's part where the labels come
from. So they've come from people, and also from collecting efforts done by other people. We're
always trying to amalgamate and bring in resources created by others, and then of course,
give them back to the community. So we've created the most powerful resource for training,
starting to train ethical AI systems. So let's dig into that a little bit. This is this is
really interesting. So I can imagine you can have what you're generating is gold label data,
you know, like we know and love across all of AI, but it has an unusual property, which is that
at the decision boundary, there are going to be ambiguities where people disagree, and there's no
amount of consensus that we'll get you to agreement. There are statements that people simply
disagree on, and they always will. What do you do with that? That's actually a really unique
kind of data. It has built in permanent ambiguity. You're exactly right, right? With a science
question or math question, there's one right answer, typically, certainly when we're doing grade
level science, not the case here. And actually the system that we built on this, which is called
Delphi, it was, it's available actually a demo of delphi.lni.org. So again, open it up and you can
see with some effort, it's quite easy to trip it up, get it to say the wrong thing. Well,
when you ask Delphi a question, it can actually relativize its answers. You can say, if you're
a conservative, you would think this, and if you're a liberal, you would think that. So it's starting
to be, yes, yes. So, right, it tells you that, and you can pose the question. You say, I don't
want to get into controversial or painful topics, but you take abortion, right? And it'll,
it has learned a model of the conservative view of abortion, of the liberal view. Again,
it has a long way to go, but it's exactly a platform to study the ambiguity that you were talking.
How do you, so I'm about to ask you a question in knowing full well that you've been sort of
dragged through hell and back in relation to the Delphi project, but zooming out just a little bit,
how do you, how do you make productive progress on areas like this that you know are just fraught?
You know that people are going to be upset, you know, anything where you have a language model
saying things like, this is right or wrong according, and if you're a liberal or a conservative,
someone's going to get upset. How do we, how do we make that okay to do that research,
knowing that you're treading into a bit of a minefield? Like, I can imagine one extreme is we just
don't ever touch that stuff, but I know how you feel on that topic. That's your, your leaving gold
on the floor. You're, you're, you're just, yeah, it's, it's not just gold is, um, I think that science
is really hampered if there are questions that are, uh, third rails where we're not allowed to study
how do we build ethical AI systems because people will get upset. I think that's, uh, that's highly
problematic. And you're right that when we release Delphi to the public and we probably could have
done better in terms of putting, uh, warning labels on it. Make sure you know that this is not
the BL and end all. This is a research prototype meant to, uh, know, for open inquiry and so on.
But, uh, people did get upset. And I would say two things. First of all, this is a great illustration
of, uh, the, the adage where companies won't. If we were a Microsoft, the Google Amazon worried
about our brand, we wouldn't do that. Look what happened with Tay, right? It was, uh, taken off and
there hasn't been, you know, Tay 2.0 and so on, you know, Microsoft, no, Microsoft hasn't touched
that. Well, they have a brand to protect. I, I respect that. Uh, our brand, uh, does not need to
be protected. It needs to be the spirit of, uh, of honest and open inquiry. And if we are alarming
people, actually, I think there's value to that. If you look at what neural networks do and you
conclude, hey, this really needs to be, uh, controlled better, then we've done part of our job,
right? That, that's a good thing. So, uh, I don't think that we court controversy, but we are
steadfast in our support of, of open inquiry as opposed to some kind of, uh, cancel this. Don't do it.
It's too, uh, it's too fraught. I do want to remind people in the audience, whatever the perspective
is about the technology and about the effort to remember that behind this technology, there are
people, grad students, researchers, and those people have feelings. And, and I have to say when
all the negative energy towards Delphi came across, I, I felt bad, but I didn't feel bad because
I was involved in releasing the project or people were upset. I felt bad for the people at AI2
who are the recipients of all this energy. And that energy, I think, could have been more, uh,
constructively targeted. I think anyone nowadays is very cautious about putting a language model
out behind, you know, a text input portal anywhere on the internet. And maybe that's one of the
practical outcomes is that you just have to be very careful because it's all too easy to elicit
offensive stuff out of this language model because it is a mirror of ourselves. And, you know,
we are offensive to each other. And that's all baked into the language it learned from. And so,
yeah, it just seems like there's just a lot of caution around doing what you do, which is to just
be open with your work and put it out there as a prototype, warnings and all. I think fewer and
fewer entities in this space are willing to take that risk. Well, I really hope that we over the
years remain willing to do that appropriately. It needs to be done right. But there are people
for whom it almost seems like a sport, right, to use your phrase from a previous conversation. A
sport to come and bash these sorts of efforts. And it's all too easy. I don't think it's
it's sporting and I don't think I think you can always do it. So I would not recommend
to the Olympic Committee to include large language model bashing in in the next Olympics. I would
instead encourage the people who are worried about that to engage with building better models,
with building better controls with because these models are being built. And AI is taking an
increasingly participatory role in society and decision making. So we need to figure this out,
not to bury the issue because it's too fraught. Last question, Lauren. If you could time travel
back those four years to when you said in that Pact Room to all of these people, including me,
the stuff barely works. What would you say today to this Pact Room, some tens of thousands of
people listening right now, a lot of them hopeful and taking part in this revolution, which is
absolutely underway. I mean, it's unbelievable what you could do today compared to four years ago
and who knows four years hence. Has your advice changed? I would say that I along with many people
have been surprised with the progress of the technology. So I would say this stuff barely works,
but I would add the proviso, but it's moving super, super fast. And then I would still add the
cautionary notes, never trust the AI demo. And even if this looks very impressive, think about
what's under the hood, what are the implications for society. Don't get caught up in the hype,
not the negative hype of the sort that Elon Musk spouts, but also not the positive hype of the
sort. Okay, we have achieved centuries. We have not. Thanks, Aaron. Great talking with you.
Thank you, John. A real pleasure.
