1
00:00:00,000 --> 00:00:12,800
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

2
00:00:12,800 --> 00:00:17,520
Hey, what's up everyone. Thanks for joining us for today's Twimal AI Podcast

3
00:00:17,520 --> 00:00:24,080
viewing party in Q&A with Lucas B. Wald, founder of Weights and Biases. Of course, I'm your host,

4
00:00:24,080 --> 00:00:29,760
Sam Charrington. While this is a pre-recorded conversation, Lucas and I are waiting for you

5
00:00:29,760 --> 00:00:36,400
in the live chat. There? Or maybe there? And we encourage you to reach out with your questions

6
00:00:36,400 --> 00:00:40,640
while we're all here together. Weights and Biases provides a great lightweight toolkit for

7
00:00:40,640 --> 00:00:45,120
machine learning practitioners and we thank them for sponsoring today's show. We've used it for

8
00:00:45,120 --> 00:00:49,200
quite a while in the Twimal community as part of our various study groups and have found it

9
00:00:49,200 --> 00:00:53,920
super useful for experiment tracking and collaboration. Now that they're offering dataset

10
00:00:53,920 --> 00:00:58,480
versioning and model management capabilities as well, you've got a one-stop shop for managing

11
00:00:58,480 --> 00:01:03,600
and visualizing your complete machine learning pipeline. If you'd like to learn more, Weights and

12
00:01:03,600 --> 00:01:09,440
Biases is extending a special offer to Twimal listeners and viewers, including unlimited private

13
00:01:09,440 --> 00:01:18,880
projects and priority support. For more details, visit wnb.com slash Twimal. It's w-a-n-d-b.com slash

14
00:01:18,880 --> 00:01:26,400
Twimal. And now on to the party. All right, everyone. I am here with Lucas B. Wald. Lucas is the CEO

15
00:01:26,400 --> 00:01:32,000
and co-founder of Weights and Biases. If Lucas is named sound familiar, that's because we

16
00:01:32,880 --> 00:01:42,000
spoke last on the podcast in August of last year, 2019, episode 295 on managing deep learning

17
00:01:42,000 --> 00:01:49,120
experiments. Lucas, welcome back to the Twimal AI podcast. Thanks, Sam. It's great to be back.

18
00:01:49,120 --> 00:01:55,840
Lucas, if folks want to catch your background, they can check out that episode and we'll link to it

19
00:01:55,840 --> 00:02:03,520
in the show notes. But what have you been up to since last August? I know one big thing has

20
00:02:03,520 --> 00:02:12,720
changed for you. Well, I had a baby. Definitely has changed my life in priorities. Maybe more

21
00:02:12,720 --> 00:02:19,680
work relevant, we've been working on improving the product. And I think one of the reasons I

22
00:02:19,680 --> 00:02:27,440
wanted to talk to you come back on your show is we just recently put out a new product called

23
00:02:27,440 --> 00:02:34,240
Weights and Biases artifacts that we're super excited about. Yep, and that's what we'll

24
00:02:34,240 --> 00:02:41,920
spend our time talking about. Tell us a little bit about the problem that you're trying to solve

25
00:02:41,920 --> 00:02:48,160
with artifacts. Totally, yeah. I mean, you always have such good, what did you call it? You have

26
00:02:48,160 --> 00:02:53,280
such good diagrams, sort of like laying out all the different pieces in the industry. I kind of,

27
00:02:53,280 --> 00:03:00,560
I'd love to know how it exactly fits into your diagram because it's always, it's always insightful

28
00:03:00,560 --> 00:03:08,880
to see that. I think you're referring to the the ebook definitive guide to machine learning

29
00:03:08,880 --> 00:03:16,240
platforms. And I broke out kind of at least my view of the ML tooling and platform landscaping into

30
00:03:16,240 --> 00:03:25,200
data acquisition and management, experimentation, and model development, and then model deployment

31
00:03:25,200 --> 00:03:32,000
and management. And so I would think is artifacts in the the middle of those two or the last of

32
00:03:32,000 --> 00:03:35,840
those two or it is a straddle? Well, I think it might kind of straddle. So let me sort of tell you

33
00:03:35,840 --> 00:03:39,280
the problem that we were trying to solve and you tell me how we should, how we should.

34
00:03:39,280 --> 00:03:48,320
So basically what happened was our experiment tracking platform, I mean, we feel super proud of

35
00:03:48,320 --> 00:03:53,440
it. It's been really popular. It's had faster growth than anything that I've been a part of.

36
00:03:54,320 --> 00:04:01,520
And so we're constantly asking our users, what else do you want? What does it feel like it's missing?

37
00:04:02,240 --> 00:04:08,640
And the biggest request was, from what users, we'll look like this is tracking all the experiments

38
00:04:08,640 --> 00:04:14,720
I do and I can compare all the models I build. But in reality, there's kind of other things that are

39
00:04:14,720 --> 00:04:20,320
really important for me to track. Like I want to track my data sets and I want to check the models.

40
00:04:20,320 --> 00:04:26,560
And I also kind of want to, in some cases, like connect steps together in kind of like a pipeline.

41
00:04:28,400 --> 00:04:34,480
And so, you know, we build artifacts as like in an adjacent product that's that's separate because

42
00:04:34,480 --> 00:04:38,720
we think that, you know, we want to keep our experiment tracking a really tight point solution that's

43
00:04:38,720 --> 00:04:44,160
really good. But we also want to make it really easy to kind of track, you know, more things that

44
00:04:44,160 --> 00:04:48,080
that you might care about. And you know, the main things, like I said, are probably data sets,

45
00:04:50,080 --> 00:04:56,320
models and pipelines. It's important to note, I think some people kind of compare us with like,

46
00:04:57,120 --> 00:05:02,240
like looked at my overview and they say, well, how is it different than something like airflow

47
00:05:02,240 --> 00:05:06,160
that like manages pipelines? And I think it's important to say really different, right? It just

48
00:05:06,880 --> 00:05:10,960
tracks it. So what it would do is like, you know, say you, you know, you have some

49
00:05:11,920 --> 00:05:16,800
set of data and you do some transform on it and then maybe you train a model and then maybe you

50
00:05:16,800 --> 00:05:21,920
like, you know, test that model on a couple different data sets and then maybe you do some quantization

51
00:05:21,920 --> 00:05:27,680
and then you deploy it. What W and B artifacts would do is it would basically save all those

52
00:05:27,680 --> 00:05:34,080
steps and save the fact that they're all connected together. So, you know, sometimes a lot like

53
00:05:34,080 --> 00:05:50,240
management is the difference that you're not the state machine or the graph, you're not managing

54
00:05:50,240 --> 00:05:56,240
that, you're not standing that up, they need an airflow or Luigi or something else that is

55
00:05:56,240 --> 00:06:04,480
actually tracking the state of all the objects in this pipeline and you are kind of doing what you,

56
00:06:04,480 --> 00:06:10,000
something similar to what you did with the experiment management product where you're,

57
00:06:11,840 --> 00:06:15,040
you know, capturing metrics of things as they move through this process.

58
00:06:15,840 --> 00:06:19,760
Yeah, and saving it, right? So, you know, if you want to save something like your data set

59
00:06:20,640 --> 00:06:25,520
or your model, we make it really easy for you to save it and version it. But then what a lot of

60
00:06:25,520 --> 00:06:29,600
our users asked for is, hey, we just want to track it. We don't want to necessarily like,

61
00:06:29,600 --> 00:06:34,000
you know, upload these gigantic files to your servers like we have them in a bucket somewhere,

62
00:06:34,000 --> 00:06:41,920
we have them on prem. And so, you know, we let you, you know, save a pointer to those things

63
00:06:42,640 --> 00:06:49,120
if you want to. So, you know, I think, I think model management and data set management is maybe

64
00:06:49,120 --> 00:06:55,120
the best way of thinking about it. I mean, I think what I know for sure is that, you know,

65
00:06:55,120 --> 00:06:59,920
everybody's asking us for this, so it must be, if it's not a category now, I think it's going to

66
00:06:59,920 --> 00:07:05,360
become a category. And there's certainly lots of tools that, you know, there's lots of tools

67
00:07:05,360 --> 00:07:09,360
that do pieces of this. And in some cases, like, you know, have, you know, are more ambitious

68
00:07:10,240 --> 00:07:13,920
than this. And it's really important to us that we work nicely with all these things. I mean,

69
00:07:13,920 --> 00:07:20,960
you have such a great dichotomy of sort of like, you know, sort of n10 platforms and, you know,

70
00:07:20,960 --> 00:07:27,120
point solutions. And, you know, one of our real core values at which advice is to be, you know,

71
00:07:27,120 --> 00:07:31,920
a set of interoperable point solutions that do kind of one thing really well and play nicely

72
00:07:32,560 --> 00:07:41,760
with all the things around it. So, let me try to put that together. The kind of core problem

73
00:07:41,760 --> 00:07:47,120
that you're trying to solve here is you come at it from the perspective of, again,

74
00:07:47,120 --> 00:07:53,440
experiment management where you had folks using your tools to track different experiments that

75
00:07:53,440 --> 00:07:59,680
they were running as part of their model development process. But you found that that

76
00:08:00,480 --> 00:08:07,520
component was used as part of a pipeline where they were getting data from somewhere. They would

77
00:08:07,520 --> 00:08:13,200
run it through multiple, some series of transformations. And then ultimately run an experiment.

78
00:08:13,200 --> 00:08:23,840
And folks wanted to track more of that process so that, yeah, I often talk to folks that are

79
00:08:23,840 --> 00:08:30,800
trying to do kind of providence solve this data providence problem or decision providence even

80
00:08:30,800 --> 00:08:38,640
where you've got a model that makes an inference and you want to kind of go all the way back from

81
00:08:38,640 --> 00:08:46,320
that inference, the decision to, you know, the model that was deployed, you know, the experiment

82
00:08:46,320 --> 00:08:52,240
that said that that model was the best model, the data that, you know, started in a training set

83
00:08:52,240 --> 00:08:57,680
that, you know, allowed you to train up that model that won. And, you know, the data points

84
00:08:57,680 --> 00:09:01,680
to influence, you know, ultimately the data points that influenced this decision. And it sounds

85
00:09:01,680 --> 00:09:07,280
like you're trying to solve more of that problem than you were doing before. Yeah, and I think like,

86
00:09:07,280 --> 00:09:13,280
you know, there's lots of really like interesting, you know, pain points here that people talk

87
00:09:13,280 --> 00:09:17,600
about all the time, like, you know, model explainability and model reproducibility. And those are

88
00:09:17,600 --> 00:09:23,120
obviously there, they're like huge problems. But I think like the sort of core thing that

89
00:09:23,120 --> 00:09:30,000
we see a lot of our users struggling with is just literally knowing, you know, what data set

90
00:09:30,000 --> 00:09:33,840
the model got trained on and what data said actually or what model actually got deployed into

91
00:09:33,840 --> 00:09:38,480
production. So, so that's like the core focus we have. Like, you know, we work with, you know,

92
00:09:38,480 --> 00:09:43,200
work with a couple of companies that do, you know, kind of retail and are trying to build systems

93
00:09:43,200 --> 00:09:48,800
that, you know, automatically, you know, can like detect like, you know, what you're buying as you

94
00:09:48,800 --> 00:09:53,520
walk out like the Amazon store. And, and, you know, one of the things that those companies

95
00:09:53,520 --> 00:09:56,880
all have in common is they're like constantly getting new label data, right? Because they have,

96
00:09:56,880 --> 00:10:01,680
you know, cashiers in their store that are labeling the data live. And they often have,

97
00:10:01,680 --> 00:10:06,000
they always have new products coming in, right? And so, um, what happens with them is that

98
00:10:06,560 --> 00:10:12,240
they really like never train the models on the same, um, data set, right? So it's actually like

99
00:10:12,240 --> 00:10:16,800
every single time it trains is sort of like a different basket of stuff that the models get, um,

100
00:10:16,800 --> 00:10:21,520
trained on. So not just incremental growth of their training data set, but they're, it's just

101
00:10:21,520 --> 00:10:25,680
different. It's just it, well, yeah, actually in that case, it's incremental growth. I'll say,

102
00:10:25,680 --> 00:10:30,400
okay, we have actually have the opposite thing where it's not incremental growth, it's actually

103
00:10:30,400 --> 00:10:35,600
shrinking. So, you know, some of our customers get, um, kind of privacy takedowns, right? Where,

104
00:10:35,600 --> 00:10:40,160
you know, people will say, um, you actually one, one company we're going to talk to,

105
00:10:40,160 --> 00:10:44,000
this is iRobot, right? Where, you know, people say, hey, take my data set out of, you know,

106
00:10:44,000 --> 00:10:48,640
iRobot data set, of course, they do that, right? But then what happens is now your,

107
00:10:48,640 --> 00:10:53,920
your data set has, um, you know, slightly changed, right? So it's not exactly, um,

108
00:10:53,920 --> 00:11:00,560
um, you know, apples to apples, um, maybe, right? Maybe it's okay, but, um, but, you know, like,

109
00:11:00,560 --> 00:11:07,600
I think that the overhead, it sounds simple, maybe to, to track all that, um, but the important

110
00:11:07,600 --> 00:11:10,560
thing is that you really, really have to do it, right? Like, you have to have a system where

111
00:11:10,560 --> 00:11:14,960
you're always tracking it the same way so that when you do this comparison, it's really easy to say,

112
00:11:15,760 --> 00:11:20,880
um, you know, which is which. I'll say, like another issue, um, is like with our, you know,

113
00:11:20,880 --> 00:11:25,760
a ton of vehicle companies, they don't, they have actually so much data, typically,

114
00:11:26,320 --> 00:11:30,480
um, that they, they never trend in all their data, right? So they're, they're actually,

115
00:11:30,480 --> 00:11:34,800
every time they build a model, they're like selecting pieces of it, um, and they're often selecting

116
00:11:34,800 --> 00:11:39,120
different pieces, like, for different purposes. So I guess like, you know, in some cases of

117
00:11:39,120 --> 00:11:43,120
data sets growing, in some cases it's shrinking, in some cases you're like picking and choosing

118
00:11:43,120 --> 00:11:49,440
from different, um, data sets, but in all these cases, um, you know, we think the really like

119
00:11:49,440 --> 00:11:54,800
core need here, or the, the, the pain that we want to solve is just like, we will keep track of what

120
00:11:56,080 --> 00:12:02,320
data sets your, your model was trained on. And then there's another, like, interesting thing,

121
00:12:02,320 --> 00:12:06,560
which is that, um, you know, data sets have these like pipelines where they might get modified,

122
00:12:06,560 --> 00:12:11,840
and those pipelines can change, right? So, um, you know, with our medical, uh, users,

123
00:12:12,400 --> 00:12:16,480
they often have really small data sets, like, you know, some of our, you know, some of our medical

124
00:12:16,480 --> 00:12:21,280
customers, like, they might say, we have this, we have big data, we have like 1000, you know,

125
00:12:21,280 --> 00:12:26,320
but it's like 3000, like, you know, like, um, chest x-rays, and somebody with like a horrible,

126
00:12:26,320 --> 00:12:30,240
you know, it's like a kind of, you know, must have been like a huge feat, um, you know, to,

127
00:12:30,240 --> 00:12:35,840
to get that data. Um, and so like, what they will do, like, a lot of, um, pre-processing, right?

128
00:12:35,840 --> 00:12:40,720
Because every record is so precious, right? So the, the pre-processing steps almost become,

129
00:12:41,600 --> 00:12:46,080
um, maybe more important than the, the sort of like, model architecture itself, right?

130
00:12:46,080 --> 00:12:50,320
So, you know, it's really important for them to, to track that. And we started to see,

131
00:12:50,880 --> 00:12:55,280
we started to see our users actually kind of, as an entrepreneur, this is like a real sign that,

132
00:12:55,280 --> 00:12:59,280
you know, you know, from the fixer. So we still, I use this sort of like, you know, kind of clinging

133
00:12:59,280 --> 00:13:04,240
together, um, you know, they're, they're sort of like, chains of runs, uh, or, you know,

134
00:13:04,240 --> 00:13:09,280
we call them runs, like, experiments in our, in our tool. Um, and so now we've kind of made that,

135
00:13:09,280 --> 00:13:13,120
with, with the artifact product, we made that like a first-class citizen, where you can say,

136
00:13:13,120 --> 00:13:18,160
you know, okay, I have these asynchronous, you know, steps that I do, um, and it might be some,

137
00:13:18,160 --> 00:13:24,000
you know, complicated pre-processing and then a training. And then sometimes even, um, you know,

138
00:13:24,000 --> 00:13:29,120
like, uh, autonomous vehicles are places that come to mind. You know, the model that you train might

139
00:13:29,120 --> 00:13:34,560
not really be the model you deploy, right? Because you might have to do like a whole bunch of, um,

140
00:13:34,560 --> 00:13:39,120
you know, kind of shrinking of your model to like, to get it to actually run. Um, or so you might

141
00:13:39,120 --> 00:13:45,760
have like tests that you do on the model that got trained, but then you also have, you know, more tests

142
00:13:45,760 --> 00:13:51,520
that you do on the model sort of just before it's deployed. And these might even be like different

143
00:13:51,520 --> 00:13:58,800
teams doing it. So, um, you know, connecting all these different steps, um, in a sane way actually

144
00:13:58,800 --> 00:14:03,280
becomes like a, you know, I think a bigger, um, a bigger problem than you might think if you're,

145
00:14:03,280 --> 00:14:08,240
you know, just like start now, you know, training like M-ness to, you know, a couple of times. Uh-huh. And

146
00:14:08,240 --> 00:14:16,640
so the, I'm curious to hear about the things that they were doing before to clooge it together.

147
00:14:17,280 --> 00:14:21,360
What did that look like? So I only used clooge with, with my own product, because that's the only

148
00:14:21,360 --> 00:14:25,040
place that I, I feel like I can actually say for sure it's clooching, right? So, you know,

149
00:14:25,040 --> 00:14:33,200
you know, you know, you know, you know, you know, you know, you know, you know, you could think about

150
00:14:33,200 --> 00:14:37,760
like, you could just sort of like name things. I was envisioning like, you know, what the way

151
00:14:37,760 --> 00:14:43,440
you would see folks managing experiments by putting hyperparameters and filenames. Like, is it?

152
00:14:43,440 --> 00:14:48,080
Yeah. Well, really common hyperparameter in ways and biases is like, you know, the, the

153
00:14:48,080 --> 00:14:54,320
filename of the, um, the training data, right? And so, um, you know, it's like, that's a great thing

154
00:14:54,320 --> 00:14:57,840
to do. For us, as people want to train there, people want to keep track of their training data

155
00:14:57,840 --> 00:15:02,160
and giving them a way to do it. Exactly. And we're like telling people, hey,

156
00:15:02,160 --> 00:15:07,920
use weights and biases because we don't think it's a good idea to use your, um, file system is like

157
00:15:07,920 --> 00:15:12,960
an implicit record of what you did, right? But then, you know, we're like kind of causing people to

158
00:15:12,960 --> 00:15:18,960
do that in some, um, in some cases, right? So that, that, you know, so yeah, so, so, you know,

159
00:15:18,960 --> 00:15:23,200
you can use the file, you can use like a, a Shah, you know, the file if you want to make sure that it,

160
00:15:23,920 --> 00:15:28,400
it doesn't change. And then there's like a whole bunch of, um, there was a whole bunch of like

161
00:15:28,400 --> 00:15:33,120
interesting tools out there that are like, kind of always evolving and improving, right? There's like,

162
00:15:33,120 --> 00:15:37,760
you know, DVC and pack-a-derm and quilt that do, um, you know, really interesting takes on

163
00:15:37,760 --> 00:15:43,040
sort of data set, um, versioning. And we've done, you know, light integrations with these tools

164
00:15:43,040 --> 00:15:49,120
to kind of make them work, um, with what we have. And then, you know, end-to-end platforms, um,

165
00:15:49,120 --> 00:15:54,960
you know, if you really buy into there, pause there for a second. Yeah. We're doing the, so you

166
00:15:54,960 --> 00:16:01,760
saw folks doing the cluages indicating that they wanted to better track training data. Um, and

167
00:16:02,720 --> 00:16:10,320
it sounds like also the light integrations and, and kind of joint customer work you were doing

168
00:16:10,320 --> 00:16:15,360
with the DVC's and pack-a-derms of the light and these other end-to-end platforms kind of indicated

169
00:16:15,360 --> 00:16:23,440
that, um, I was actually going to ask about that. The, the DVC's and the pack-a-derms of the world are

170
00:16:23,440 --> 00:16:30,640
trying to, uh, that's kind of their, that's their world, right? They're trying to help customers

171
00:16:30,640 --> 00:16:38,800
better track the evolution of training data through transformation processes and, and up to,

172
00:16:38,800 --> 00:16:44,160
up to, and in some cases, including training. All right. And so where do, where does what they're

173
00:16:44,160 --> 00:16:52,960
doing end and what you're doing start? Or are they kind of overlapping like our customers having

174
00:16:52,960 --> 00:16:58,880
to make decisions about, um, you know, which part of which product they want to use for,

175
00:17:00,080 --> 00:17:04,320
which part of the process? Like, it seems like there's some overlap there that I'm trying to thank.

176
00:17:04,320 --> 00:17:12,000
Oh yeah. So there's clearly some overlap and I feel like, um, there's probably, you know,

177
00:17:12,000 --> 00:17:18,240
like we're recording this one April, April 2020. I haven't feeling, you know, like June 2020,

178
00:17:18,240 --> 00:17:24,080
the overlap, you know, it could be more or less a different, um, you know, so like, I think that,

179
00:17:26,080 --> 00:17:31,840
you know, I think like one, you know, one thing, I feel super committed to making our product work

180
00:17:31,840 --> 00:17:36,560
with, um, kind of all the best in class tools. It's like really, really important to me. So,

181
00:17:36,560 --> 00:17:42,400
you know, I think that if, um, you know, if, if DVC or pack-a-derm, you know, solved all the

182
00:17:42,400 --> 00:17:47,840
problems that the customers were having around this, you know, we would just, you know, let them and focus on

183
00:17:48,800 --> 00:17:55,520
experiment tracking. I think that, you know, I think the issue, um, I think that the sort of thing

184
00:17:55,520 --> 00:18:01,760
that we keep finding is that, you know, it kind of reminds me of actually, um, hyper parameter search,

185
00:18:01,760 --> 00:18:06,560
which we also didn't expect to roll out a hyper parameter, um, search tool, but we kept, you know,

186
00:18:06,560 --> 00:18:10,800
we kept talking to customers and like, ah, like, I know what should be doing this, but, you know,

187
00:18:10,800 --> 00:18:15,680
it's like, like, I can't quite figure out how to do it. You know, and so, you know, kind of articles,

188
00:18:15,680 --> 00:18:21,200
like, let's make hyper parameter search just so, like, simple. I mean, that's this really easy,

189
00:18:21,200 --> 00:18:25,680
not like a toy, but just like, make actually the powerful, like, make it easy to make it work in

190
00:18:25,680 --> 00:18:31,280
a distributed fashion on lots of data and really reliable. That's sort of like our, um, you know,

191
00:18:31,280 --> 00:18:35,840
point of view is like, let's just like really make it, um, you know, like, like a solid product of

192
00:18:35,840 --> 00:18:39,840
the easy onboarding. And so, kind of felt the same way with this data set versioning stuff, where we

193
00:18:39,840 --> 00:18:46,320
would, you know, we were pointing people to, um, these different products. And I think that they are,

194
00:18:46,320 --> 00:18:49,920
I think they're a pretty steep learning curve, um, honestly. I mean, it's not like a knock on the,

195
00:18:49,920 --> 00:18:54,320
these are really like, you know, it's kind of complicated stuff and it's hard to get it, um,

196
00:18:55,120 --> 00:18:59,040
you know, it's hard to make it like really easy to, to use, but I think like, that's where I think,

197
00:18:59,040 --> 00:19:06,400
like, because we're like totally focused on, um, making ML researchers and practitioners happy,

198
00:19:06,400 --> 00:19:12,080
I think Winston-Biasis has a much more narrow, um, you know, scope here. Like we're, we're, you know,

199
00:19:12,080 --> 00:19:15,920
I think like, you know, DBC is like native virtual, right? So it's like, there's actually a lot of

200
00:19:15,920 --> 00:19:21,760
different situations you might get into and like, you know, they, they use like, um, I mean, they use

201
00:19:21,760 --> 00:19:26,880
Git and I, you know, I love, do I love Git? I don't know, I kind of love Git. I make a little,

202
00:19:26,880 --> 00:19:34,880
we all have the idea of Git until we need to do something monkey. I imagine like 2021 Lucas,

203
00:19:34,880 --> 00:19:40,800
just totally understands Git and, you know, it's like, but like, 2020 Lucas kind of types and commands,

204
00:19:41,520 --> 00:19:48,640
kind of like feels a little afraid, you know, and that's how, you know, I hope I'm not back

205
00:19:48,640 --> 00:19:52,400
to just doing my tools, I can tool for like idiots. I mean, it's a tool for, you know, ML researchers

206
00:19:52,400 --> 00:19:57,440
who want to do ML research, right? Not like, um, and we know that like, some of them are really deep

207
00:19:57,440 --> 00:20:01,920
on Git, but we don't want to necessarily, we want to hide as much complexity as possible and solve

208
00:20:01,920 --> 00:20:06,720
the core pain point, which I think really here is. I think what you're getting at there is that the

209
00:20:07,840 --> 00:20:18,080
DVC and, and Packaderm as examples are, uh, and some of the, and the platforms there are actually

210
00:20:18,080 --> 00:20:25,520
versioning the data and snapshotting the data as it, uh, transitions through a transformation process

211
00:20:25,520 --> 00:20:30,960
and they can, you know, go to any point in time and any place in a process and allow you to see

212
00:20:30,960 --> 00:20:35,920
the data as it was transformed and that's not quite what you're trying to do. Well, I just say,

213
00:20:35,920 --> 00:20:42,640
we do version, we do version actual data itself. Well, in a really simple way, we make copies of it

214
00:20:42,640 --> 00:20:47,840
and we save them. So, you know, like, like, you know, that's why again, like, you know, it's like,

215
00:20:47,840 --> 00:20:52,960
we keep pointers to, to different places your data was at and we let you like, split it into pieces

216
00:20:52,960 --> 00:20:58,080
if, if, if you think those are going to change, but I think we are not as concerned with making

217
00:20:58,080 --> 00:21:05,760
like really kind of sophisticated dipping. Yeah, the sort of thinking being that, um, you know,

218
00:21:05,760 --> 00:21:11,520
space is pretty cheap and the really important thing is that people actually, um, you know, save

219
00:21:11,520 --> 00:21:16,160
this stuff. And then we just, that's like our first. And if, if, you know, of course, if customers

220
00:21:16,720 --> 00:21:20,480
like really, really want like sophisticated dipping and that's like, you know, necessary,

221
00:21:21,520 --> 00:21:25,120
you know, we'll find a way to, to do it for them. But, you know, the product doesn't exist

222
00:21:25,120 --> 00:21:31,120
today. The idea is just like, you know, you tag every state of your data, um, and, and you can

223
00:21:31,120 --> 00:21:35,600
keep uploading it to us if you want to. Um, and of course, if you, if you use the same exact data

224
00:21:35,600 --> 00:21:40,160
in two different places, we're not going to make two, um, you know, copies of it. But I think,

225
00:21:40,960 --> 00:21:44,960
you know, we're not like using Git in the back end, you know, for example. Got it. Got it. So,

226
00:21:44,960 --> 00:21:54,000
you referenced earlier this, um, it's almost a, uh, I think I gave more energy to it, uh,

227
00:21:54,000 --> 00:22:00,160
in the book, the definitive guide to ML platforms. It wasn't quite a throwaway comment or a footnote.

228
00:22:00,160 --> 00:22:06,720
I, I spent some time with it in the last chapter of the book, but it was this idea, um, that I

229
00:22:06,720 --> 00:22:13,360
proposed of the wide versus deep paradox for machine learning, you know, platform and tool vendors.

230
00:22:13,360 --> 00:22:19,440
And this conversation is such a great example of it. Uh, the, the, the general argument for folks

231
00:22:19,440 --> 00:22:25,920
that haven't, um, taken a look at the book is that you've got these end and platforms that are

232
00:22:25,920 --> 00:22:33,920
trying to kind of own the entire machine learning process at an enterprise. Um, and,

233
00:22:35,680 --> 00:22:41,200
you know, if you start from, from that perspective, you know, you can end up with a simplistic

234
00:22:41,200 --> 00:22:47,040
solution that really is just a workflow engine that's very shallow, you know, in functionality

235
00:22:47,040 --> 00:22:54,720
at any particular point. And you've got specialist providers, um, who, you know, any point in that,

236
00:22:54,720 --> 00:22:59,120
in the end process, have gone very deep, for example, waste and biases went very deep on

237
00:22:59,120 --> 00:23:04,720
experiment management as their initial play. And, and the paradox, the paradoxical element of

238
00:23:04,720 --> 00:23:12,880
this was that the, in the end providers, you know, as we have seen, we've got a lot of end-to-end

239
00:23:12,880 --> 00:23:18,880
solutions and, you know, without going deep in any particular area, that tends to get commoditized.

240
00:23:19,920 --> 00:23:26,960
Uh, and the, uh, and so they're getting kind of pushed to specialize or differentiate in, in a

241
00:23:26,960 --> 00:23:33,280
slice or, you know, particular functionality. Uh, and then with the narrow, uh, providers,

242
00:23:33,280 --> 00:23:38,320
the specialists, you know, they're going in, they're finding that, you know, customers have all

243
00:23:38,320 --> 00:23:43,600
these other gaps that they need to fill in order to get value out of the core product. And so they're

244
00:23:43,600 --> 00:23:51,440
being pushed wider. So you've got these two kind of sides kind of, uh, converging in the middle,

245
00:23:51,440 --> 00:23:56,880
perhaps, uh, or in different directions. And this is such a great example of that. Yeah.

246
00:23:56,880 --> 00:24:03,040
Like you started in the middle, you've got the, the data side. I think you, you mentioned, uh,

247
00:24:03,040 --> 00:24:09,600
extending into the hyper parameter optimization piece. Uh, um, are you working on model deployment

248
00:24:09,600 --> 00:24:16,560
management at all? Uh, we, we, we play with the stuff we, we dabble, we dabble in the sense that

249
00:24:16,560 --> 00:24:21,920
I think like what, what I want, I mean, I, I feel like what I want weights and biases to be

250
00:24:21,920 --> 00:24:26,320
note, like what I want our, our take to be is like, you know, we're not, we don't have like a, um,

251
00:24:26,320 --> 00:24:32,880
we don't know like a, like a, we want to make good tools for people doing, um, machine learning

252
00:24:32,880 --> 00:24:37,280
that they're actually going to use and, and really want, right? So that's sort of like our, our,

253
00:24:37,280 --> 00:24:41,360
our process is like, you know, we spend a ton of time with our customers and we ask them,

254
00:24:42,080 --> 00:24:47,840
you know, what do you need? Whereas I feel like, you know, um, I, I feel like there's a surprising

255
00:24:47,840 --> 00:24:52,560
number of companies that it sort of seems like they have sort of like these grand plans that sort of

256
00:24:52,560 --> 00:24:59,920
make sense in a PowerPoint, um, but then don't really fit what people want. So anytime they're sort

257
00:24:59,920 --> 00:25:04,400
of like, you know, business strategy question, and it's like at all in conflict with what, um,

258
00:25:04,400 --> 00:25:08,880
customers are asking for, like, I'm going to do what people are actually asking for, like, you know,

259
00:25:08,880 --> 00:25:13,120
10 times out of 10. And so that's why I think like, you know, we thought when we started, like,

260
00:25:13,120 --> 00:25:17,280
this is just a point solution, we're just going to do experiment tracking, but then people are like,

261
00:25:17,280 --> 00:25:21,520
you know, what I really want you to do is actually, you know, make a hyper parameter,

262
00:25:21,520 --> 00:25:24,720
research thing for me. So we, you know, we did, and then, and then we were like, okay, what are

263
00:25:24,720 --> 00:25:29,760
they really asking for? Um, you know, they're absolutely like, you know, everyone is focused on

264
00:25:29,760 --> 00:25:34,800
just like, I want to be able to keep track of my data sets and models. I think, um, you know,

265
00:25:34,800 --> 00:25:40,400
some people do ask us about, you know, production monitoring or like, you know, CI, CD for, um,

266
00:25:40,400 --> 00:25:44,080
you know, for machine learning. I think this stuff is really interesting and cool. Um, and I don't

267
00:25:44,080 --> 00:25:48,640
think those problems have been been solved yet. So I'm like intrigued. Um, I'm intrigued by

268
00:25:48,640 --> 00:25:53,440
these directions, but I want to make sure that we do, um, well, and I also want to make sure that we are,

269
00:25:54,960 --> 00:26:00,400
like everything we do, you can, um, pick and choose it because I, I guess like, I really don't think

270
00:26:00,400 --> 00:26:08,960
in the end that, um, these kind of all-in platforms are going to be, um, long-term successful. Like,

271
00:26:08,960 --> 00:26:13,600
I think that, um, you know, right now people are confused. I think a lot of VPs are just like,

272
00:26:13,600 --> 00:26:18,320
look, just come in and solve all my ML problems that wants to that, like, feels, um, really good,

273
00:26:18,320 --> 00:26:22,320
but I think the practitioners don't like them because they don't feel like they're learning

274
00:26:22,320 --> 00:26:28,080
transferable skills. And so I think it's, it's actually hard to track the best talent if you

275
00:26:28,800 --> 00:26:34,240
are using, um, you know, the something that doesn't like work with other tools. And I think at the

276
00:26:34,240 --> 00:26:38,640
end, like, well, companies actually really care about and should care about is getting the best talent.

277
00:26:38,640 --> 00:26:43,680
And so I think you have to make, um, tools that make people feel like they have like,

278
00:26:43,680 --> 00:26:48,320
you know, or learning a transferable skill, um, to other companies. And I will say there's a

279
00:26:48,320 --> 00:26:53,200
natural, like, difference in go-to-market too, right? Where, um, you know, if I was selling an ML

280
00:26:53,920 --> 00:26:58,480
platform, you know, I would go into CIOs and I would go to, um, you know, VPs of

281
00:26:59,120 --> 00:27:02,480
machine learning. What are they called? These days, I feel like you have like chief AI officers and

282
00:27:02,480 --> 00:27:08,480
things like that. Um, and it's funny, you know, I, I guess I like, you know, these days, you know,

283
00:27:08,480 --> 00:27:14,800
is anybody, is that chow? Is anybody coined that? That's awesome. If they didn't, it doesn't, I

284
00:27:14,800 --> 00:27:21,680
think the eyes for John Gendres. And he has, I don't know, I, um, I think, definitely, I'm telling

285
00:27:21,680 --> 00:27:25,360
you, somebody out there is that. And then like, I, you know, Tony is like, my network is getting

286
00:27:25,360 --> 00:27:29,520
more and more senior. And I'm getting, um, you know, people are like introducing me to people,

287
00:27:29,520 --> 00:27:33,360
just like, I don't know, I don't know. You know, like, I just want to like talk to the people,

288
00:27:33,360 --> 00:27:37,920
you know, actually making stuff. And, um, you know, get them to want to use it first,

289
00:27:37,920 --> 00:27:43,760
because that's like a bread, butter for, um, you know, the hardest part about, um, ML tools right

290
00:27:43,760 --> 00:27:48,560
now is, is like actually getting people to, um, use the tools, because there's so many tools out

291
00:27:48,560 --> 00:27:54,560
there. That was part of the end, and argument that I was making as well as the end, and,

292
00:27:56,240 --> 00:28:00,400
platform providers, like in order for them to be successful, they'd have to get

293
00:28:01,200 --> 00:28:05,920
everyone to be, you know, they don't have to get folks to be willing to throw away any of the

294
00:28:05,920 --> 00:28:10,160
point investments they made, whether they were internally developed or, you know, they brought

295
00:28:10,160 --> 00:28:16,720
someone else in, whereas, uh, the, you know, specialists would need to fill these, that's,

296
00:28:16,720 --> 00:28:21,840
that was the other side of specialists filling the, the gaps. Um, yeah. It's a interesting.

297
00:28:21,840 --> 00:28:26,800
I wonder how you feel about this. Like I, so this probably happens a few old times, but like every

298
00:28:26,800 --> 00:28:32,080
week, my investors are like, what do you think about this, you know, ML company? And like, I,

299
00:28:32,080 --> 00:28:35,120
I already can prick like what the website is going to say. It's going to like list all the

300
00:28:35,120 --> 00:28:41,440
pain points in ML, right? And like, I know, you know, repressibility, um, explainability, um,

301
00:28:41,440 --> 00:28:45,760
you know, like maintainability, you know, it's like going to be like, we do ML ops, and it's going

302
00:28:45,760 --> 00:28:50,880
to like, and I'm like, I can't evaluate this, because I have no idea what this thing does.

303
00:28:52,640 --> 00:29:00,240
In the same book that you're talking about, I put, uh, uh, I included kind of a way to figure out

304
00:29:00,240 --> 00:29:09,360
what, uh, what a platform is really good at. And to me, I'm sorry, I haven't, I didn't come

305
00:29:09,360 --> 00:29:12,800
across this. I, I really want to know. It's in that same section that you refer to. It's,

306
00:29:13,600 --> 00:29:19,120
it's basically where did they come from? Uh, yeah, makes right. And so, you know, if you,

307
00:29:19,120 --> 00:29:25,440
if you, you know, if they came out of, uh, you know, data storage and snapshotting and now they've

308
00:29:25,440 --> 00:29:29,680
got a, and, and machine learning platform, they're probably going to be focused on the data side

309
00:29:29,680 --> 00:29:35,120
of things. Okay, but it's not in our case. What's that? You know, we're really good at tracking

310
00:29:35,120 --> 00:29:45,680
artifacts. Don't, let's do everyone else. But, you know, so that was one of several criterias,

311
00:29:45,680 --> 00:29:53,520
but I agree. And that's a lot of the problem with, uh, I shouldn't say problem. That is a challenge

312
00:29:53,520 --> 00:30:00,480
with, um, that I see folks faced with when they're looking at these and the end platforms is that

313
00:30:00,480 --> 00:30:08,000
they all talk about solving the same problems, you know, and workflow, uh, increasing innovation

314
00:30:08,000 --> 00:30:12,480
and cycles, you know, decreasing cycle times, increasing, you know, innovation and a number of

315
00:30:12,480 --> 00:30:19,280
experiments and, um, without going deep in, you know, particular areas, it's really difficult

316
00:30:19,280 --> 00:30:23,040
for them to differentiate. I mean, I'll even give you an example. I mean, you probably can't say

317
00:30:23,040 --> 00:30:27,600
the stuff, but, but I can, you know, like, when I, you know, I remember when SageMaker came out,

318
00:30:27,600 --> 00:30:32,000
and it was basically like a way to have like a nice environment to train your models. And,

319
00:30:32,000 --> 00:30:36,400
and my friends told me it was the fastest growing, um, AWS product that they had seen, right? It

320
00:30:36,400 --> 00:30:40,000
was like, it was awesome. It like really solved this, this pain point that everyone had,

321
00:30:40,000 --> 00:30:45,040
everyone like, you know, really wanted to use it. And, and now I'm like baffled, like what,

322
00:30:45,040 --> 00:30:50,160
like what SageMaker does like SageMaker, where begins and ends like, you know, honestly, you could,

323
00:30:50,160 --> 00:30:56,880
it might be useful for you to write it. You know, guys, like we can like figure this out, but

324
00:30:56,880 --> 00:31:01,360
I feel like if I'm confused, the market must be just like utterly baffled because I'm out there,

325
00:31:01,360 --> 00:31:05,280
like every day talking to people, like using these tools, like thinking about it. And I,

326
00:31:05,280 --> 00:31:10,880
I actually could not tell you, um, you know, where SageMaker begins and ends anymore. But I,

327
00:31:10,880 --> 00:31:15,440
I would say to someone, if you want to quickly stand up an environment to run ML models,

328
00:31:15,440 --> 00:31:24,240
I bet you SageMaker is a consolation still. You know, I think, I don't think anyone who follows

329
00:31:24,240 --> 00:31:32,480
AWS is surprised at how it's evolved because, you know, if you've used AWS at all, you go to their

330
00:31:32,480 --> 00:31:40,080
services page and there's like a thousand different modules, right? And, um, you know, they solve,

331
00:31:40,080 --> 00:31:46,480
you know, they're also a very customer-focused, but their strategy or end, their strategy for doing

332
00:31:46,480 --> 00:31:51,920
that is to, uh, come up with a bunch of point solutions and allow folks to kind of string them

333
00:31:51,920 --> 00:32:02,560
together. And they've kind of done that with SageMaker, uh, an ML in general. Um, and to some extent,

334
00:32:02,560 --> 00:32:08,080
or another, for better or for worse, have taken a little bit of the Watson approach where the

335
00:32:08,080 --> 00:32:13,120
brand gets applied to everything, your many things. And it's like, yeah, it's the one thing if Amazon,

336
00:32:13,120 --> 00:32:17,440
if you're listening to this podcast, dude, really, it would make my life easier if you just name them

337
00:32:17,440 --> 00:32:25,280
like different, different things. That's one.

338
00:32:29,920 --> 00:32:36,400
So going back to actually speaking of naming, so artifacts is called artifacts.

339
00:32:36,400 --> 00:32:42,880
Uh, right. We think, what are the artifacts? You know, so the artifact is, so we think of it as a

340
00:32:42,880 --> 00:32:48,480
data set or model management, um, products. And we call it artifacts. And the artifacts here

341
00:32:49,120 --> 00:32:54,720
is essentially a data set or a model. I mean, it really could be like anything that you can think

342
00:32:54,720 --> 00:33:00,160
of as a file. Um, and we kind of thought we had thought up the name artifacts by ourselves,

343
00:33:00,160 --> 00:33:04,560
but then we looked at, you know, Google pipelines actually calls the same thing that we talk about

344
00:33:04,560 --> 00:33:08,800
as artifacts artifacts. And MLflow actually also talks about these things as artifacts. So it does

345
00:33:08,800 --> 00:33:12,800
seem like everyone sort of independently, or we thought we invented it, but, you know, we were

346
00:33:12,800 --> 00:33:19,280
definitely not the first, we have maybe the first that's calling the product that, the first

347
00:33:19,280 --> 00:33:24,160
maybe to call the product that, but certainly we did not, um, you know, we did, you know, we saw it,

348
00:33:24,160 --> 00:33:28,400
we were kind of intrigued to see that other people are calling it the same thing because I think

349
00:33:28,400 --> 00:33:38,640
when I think of artifacts, I think first and foremost about, um, the instantiation of a model.

350
00:33:38,640 --> 00:33:43,200
So like, when I would, you know, outside of this, the context of this conversation, when I'd

351
00:33:43,200 --> 00:33:48,800
ask folks about, uh, the way they manage, managed artifacts, it would be model artifacts,

352
00:33:48,800 --> 00:33:54,160
like some folks would, you know, pickle, uh, python model and store that, that was the,

353
00:33:54,160 --> 00:33:59,600
the pickle file was the artifact. Yeah, yeah, um, folks would kind of, you know, create a doc would

354
00:33:59,600 --> 00:34:05,040
containerize it and the container itself was the artifact. For some folks, the artifact that

355
00:34:05,040 --> 00:34:12,400
they were tracking was a Shah, uh, Gita. Uh, and so I would ask that question to kind of understand,

356
00:34:12,400 --> 00:34:18,320
to understand, um, you know, what was the fundamental currency of whatever system that they were,

357
00:34:18,320 --> 00:34:27,040
um, building, it sounds like for you, an artifact is, well, what is the fun, what are you actually

358
00:34:27,040 --> 00:34:33,920
tracking? It's practically what we see people tracking, like practically what we see them doing,

359
00:34:33,920 --> 00:34:40,560
is tracking usually data sets and, um, models, right? So that's, that's what you should be like

360
00:34:40,560 --> 00:34:45,920
imagining. What are you tracking the data set for the model? Like, do you, do you, is artifacts,

361
00:34:45,920 --> 00:34:52,560
a content management system that is, you know, taking files, putting them in storage, managing that,

362
00:34:52,560 --> 00:34:59,120
or is it, are you tracking pointers to artifacts? Both and I think that's what's key here, right?

363
00:34:59,120 --> 00:35:04,240
So we do both and we also allow you to tag and version them and that's like the big,

364
00:35:05,360 --> 00:35:09,760
that's the big, that's the infrastructure that we've built, right? So we think that people in

365
00:35:09,760 --> 00:35:15,440
different cases might want to save these things on a third party server and might not want to,

366
00:35:15,440 --> 00:35:19,760
right? And, and in fact, even, you know, we have an on-prem version of this and even there,

367
00:35:19,760 --> 00:35:26,160
you might not want to move around like a petabyte, um, data file, um, but in some cases, you actually,

368
00:35:26,160 --> 00:35:30,240
you know, might want to do that just to make sure that it's like completely saved. So, um,

369
00:35:30,240 --> 00:35:34,560
that's what I think a really important feature, it's just super important to our customers is to be

370
00:35:34,560 --> 00:35:38,880
able to kind of handle both cases where it's like, you know, maybe it's appointed to a file or

371
00:35:38,880 --> 00:35:45,600
appointed to a bucket or literally, um, a file. And I think the other important, um, and maybe this

372
00:35:45,600 --> 00:35:51,040
is getting a little in the weeds of the engineering, but, um, I think the way that our users at least

373
00:35:51,040 --> 00:35:58,160
kind of conceive of it is like, you know, experiments, um, have kind of like, in input and output,

374
00:35:58,160 --> 00:36:02,640
typically, right? And the input might be some data sets, say, and the output might be a model,

375
00:36:02,640 --> 00:36:06,880
right? That'll be like sort of the classic kind of experiment that that that we think of,

376
00:36:06,880 --> 00:36:10,960
right? Like a training, you know, ML training run and there the inputs and the outputs are actually

377
00:36:10,960 --> 00:36:16,800
artifacts, right? In our vocabulary, but one of the things that we saw was actually there are

378
00:36:16,800 --> 00:36:20,320
other things that get general. Well, I should say there's other types of experiments that you might

379
00:36:20,320 --> 00:36:28,080
run, um, like a, um, like a data preprocessing step, right? So, you know, data preprocessing, it

380
00:36:28,080 --> 00:36:33,440
actually does have hyper parameters often, right? So like, you know, like maybe, um, if you're doing

381
00:36:33,440 --> 00:36:38,720
like, um, kind of image preprocessing, like, how much you jitter, you know, the images or, um,

382
00:36:38,720 --> 00:36:44,960
you know, with words of information or, yeah, that kind of stuff. Um, and so, you know,

383
00:36:44,960 --> 00:36:50,320
there are the input artifacts is like a data set and the output artifact is like a, you know,

384
00:36:50,320 --> 00:36:57,600
modified, um, data set. But I mean, you know, also as you know, like, um, sometimes in the course

385
00:36:57,600 --> 00:37:01,600
of training and like the real world, there is like other stuff that gets generated, right? There's

386
00:37:01,600 --> 00:37:06,320
like other files that, you know, might matter to you, like, you know, like profiling, like a huge

387
00:37:06,320 --> 00:37:11,920
kind of profiling, um, data set or, um, you know, in the course of model training, right? Like,

388
00:37:11,920 --> 00:37:17,680
you know, so there's, there's all kinds of, um, files that you, that you might generate. The

389
00:37:17,680 --> 00:37:22,720
idea is that, um, in a way, one really nerdy way to think about this is like, you know, the,

390
00:37:22,720 --> 00:37:27,520
the artifacts are like the nodes in the graph and the experiments are like the edges in the graph,

391
00:37:27,520 --> 00:37:32,080
right? So like, you, you think of this sort of directed, um, you know, graph that usually starts

392
00:37:32,080 --> 00:37:36,960
with data sets and ends with like a deployed model. And that is all kinds of chaos, you know,

393
00:37:36,960 --> 00:37:42,080
a long way. Um, you know, I'll say like one thing that's actually been surprisingly important to

394
00:37:42,080 --> 00:37:46,800
some of our, um, users and it's kind of different than, so a lot of people would call this like a

395
00:37:46,800 --> 00:37:51,040
pipeline, right? Like, um, you know, I think people that do this for production, they'd be like,

396
00:37:51,040 --> 00:37:55,680
okay, that's a pipeline. I think one difference about, you know, our artifacts or one design

397
00:37:55,680 --> 00:38:02,640
choice that we made is allowing people to change this type of thing on the fly. So you don't necessarily

398
00:38:02,640 --> 00:38:07,600
have to know exactly, um, you know, what you're going to do with your model, right? You can, you know,

399
00:38:07,600 --> 00:38:11,760
you can make like a game time decision to be like, okay, I'm going to like run some expert experiments

400
00:38:11,760 --> 00:38:17,760
on it. Um, and that's okay, right? So that's, that's a kind of really common thing, um, that we see.

401
00:38:17,760 --> 00:38:21,520
In fact, you might want to like even look at as like a manager, you might want to be like,

402
00:38:21,520 --> 00:38:27,360
like, I have this data set, I just want to see like every model that got trained on it in my company.

403
00:38:28,400 --> 00:38:32,320
And that's actually an easy thing for our tool to do now if you're using our artifacts product,

404
00:38:32,320 --> 00:38:36,400
we can actually, because we, you know, everyone that uses it gets kind of tracked now, you can say,

405
00:38:36,400 --> 00:38:41,280
okay, here's all the people that use that data set, um, or even, you know, you might like have, um,

406
00:38:42,560 --> 00:38:49,200
uh, you might have a model that gets used by, um, you know, other models downstream of it,

407
00:38:49,200 --> 00:38:53,520
right? And so as long as like every model that's like training up the output of a different model,

408
00:38:53,520 --> 00:38:59,520
as long as they declare it, um, you know, you can actually, um, visualize all that in your,

409
00:38:59,520 --> 00:39:08,560
in your company. You were making a point earlier about, um, the, the reason why you didn't like

410
00:39:08,560 --> 00:39:15,520
the terminology pipeline, because it's more dynamic than that is, is that analogous to like,

411
00:39:15,520 --> 00:39:23,520
I'm envisioning, uh, programmatic access to this or predefined, um, you know, access where

412
00:39:24,400 --> 00:39:32,720
you've got some, some pipeline, uh, you know, coded up and you're executing it versus going in via

413
00:39:32,720 --> 00:39:38,720
the UI and, you know, running, kicking things off manually, accessing files manually. Is, is that

414
00:39:38,720 --> 00:39:42,720
the distinction that you're making there? I think we made a design choice that again was just

415
00:39:42,720 --> 00:39:46,640
really informed by your request from, from researchers that we were working with,

416
00:39:47,520 --> 00:39:53,680
let the, the pipeline get defined dynamically, right? So like, you know, production pipelines,

417
00:39:53,680 --> 00:39:56,800
you tend to just sort of like set them and run them and then you might make a new production.

418
00:39:57,440 --> 00:40:01,920
Um, pipeline, whereas like our notion of pipeline, it's like maybe a little bit more like

419
00:40:01,920 --> 00:40:06,320
realistic kind of real world notion, where it's just sort of like that graph and that graph,

420
00:40:06,320 --> 00:40:10,880
you can keep, that graph can keep changing. And it's not a UI thing, like it's like, you know,

421
00:40:10,880 --> 00:40:15,120
you could, you know, you run things at any point, if you run a thing and you declare like,

422
00:40:15,120 --> 00:40:22,320
hey, I'm using, um, you know, dataset xyz, then in our product at any point, if you look at

423
00:40:22,320 --> 00:40:25,920
dataset xyz and you, you query it for like, what's all the stuff downstream of that? You'll see all

424
00:40:25,920 --> 00:40:33,920
those runs. Um, yeah, I think I was getting at, uh, predefined versus ad hoc. Yeah. And it sounds

425
00:40:33,920 --> 00:40:39,040
like that's kind of the distinction that you're, yeah, ad hoc, exactly, exactly. Not

426
00:40:39,040 --> 00:40:44,160
forgetting. Yeah. I hope I'm making sense. I, it's like, you, you're better explain

427
00:40:44,160 --> 00:40:52,000
the stuff than I am. And so maybe another question on that. So, but is the idea then that while

428
00:40:54,000 --> 00:41:01,760
the, um, you know, while you're not locking, you know, while you're not only tracking changes that

429
00:41:01,760 --> 00:41:10,480
happen as the result of a predefined executed pipeline, and you're allowing folks to do ad hoc,

430
00:41:11,360 --> 00:41:19,920
uh, manipulation of the artifacts that you are also tracking the things that they do to,

431
00:41:19,920 --> 00:41:26,160
that they do ad hoc with the artifacts and kind of they become those actions become part of the

432
00:41:26,160 --> 00:41:30,560
graph and thus part of the pipeline. You know, a core design choice, right? It's the artifacts

433
00:41:30,560 --> 00:41:37,600
themselves are immutable. Um, but what you can't add is like essentially like, um, edges to this

434
00:41:37,600 --> 00:41:41,680
graph, right? So if you decide like you built a model and you want to do one extra test on it,

435
00:41:42,080 --> 00:41:48,640
um, you know, before deployment, you can, um, you know, you can, you can basically do that run,

436
00:41:48,640 --> 00:41:53,200
and that'll notify, um, our system and the background that this is happening and we'll keep,

437
00:41:53,200 --> 00:42:02,080
we'll track that all, um, for you. Cool. Is artifacts out? Is it, uh, yeah, folks using it?

438
00:42:02,080 --> 00:42:06,880
Yeah, books are, they're using it. We've been, um, I mean, it'll say we've been like kind of testing

439
00:42:06,880 --> 00:42:11,680
it on early users for quite a long time. So, you know, it's just coming out and, you know,

440
00:42:11,680 --> 00:42:16,160
I'm sure there are some bugs, but it has been, um, you know, beaten on a fair amount of by,

441
00:42:17,120 --> 00:42:20,800
you know, by existing customers. We, we, we've really been trying to use the fact that we have

442
00:42:20,800 --> 00:42:27,040
a lot of users of our first tool to, um, you know, to kind of build it in the conversation with folks.

443
00:42:28,400 --> 00:42:37,040
And historically, and correct me on this, but when I think of weights and biases and the

444
00:42:37,040 --> 00:42:44,560
experiment management tool in particular in the broader context of experiment management,

445
00:42:44,560 --> 00:42:51,760
the focus was very squarely on deep learning as opposed to, uh, you know, tabular data and kind

446
00:42:51,760 --> 00:42:59,920
of traditional machine learning, uh, to what extent does that, if that is, in fact, true?

447
00:42:59,920 --> 00:43:05,760
So what, does that focus kind of carry on to artifacts as well? That's a good question.

448
00:43:05,760 --> 00:43:09,920
I mean, I think we designed it with sort of modern deep learning techniques in mind,

449
00:43:09,920 --> 00:43:15,040
but the line is not so bright. If you know what I'm saying, like, we have a lot of customers

450
00:43:15,040 --> 00:43:20,640
doing, you know, boosted trees and, and, um, you know, I think, I actually, it's a good point.

451
00:43:20,640 --> 00:43:25,040
I think like, um, you know, for a lot of people that are doing kind of boosted trees at big scale,

452
00:43:25,040 --> 00:43:30,720
this, this could be a really relevant, um, you know, think for them. So I think like, I would say

453
00:43:30,720 --> 00:43:37,360
probably most of the users that we have today are doing, um, deep learning or what you would call

454
00:43:37,360 --> 00:43:43,120
deep learning, um, but I don't know. I'm not so opposed to what? Well, I mean, first of all,

455
00:43:43,120 --> 00:43:48,320
I'm not so pure about this. I think, I mean, I think like XG boost is like a awesome tool.

456
00:43:48,320 --> 00:43:51,200
It's like when I started my, you know, I said, well, I started my career before I could boost,

457
00:43:51,200 --> 00:43:54,320
but I've probably made more boosted trees in my life than anything else.

458
00:43:54,960 --> 00:43:59,760
I think scikit-learn is a beautifully done, um, library that I use like all the time for just

459
00:43:59,760 --> 00:44:05,120
my own, um, data analysis. So, um, is it not about about whether deep learning is better than

460
00:44:05,120 --> 00:44:09,520
anything else, but you said deep learning or what you would call deep learning? Is there a deep

461
00:44:09,520 --> 00:44:16,400
learning that like a super secret deep learning or a deep learning? No, no, no, no, no, no, no.

462
00:44:16,400 --> 00:44:22,880
I mean, like, uh, yeah, sorry, I think about like, I think like some of our users they'll be doing,

463
00:44:24,640 --> 00:44:29,040
I think like, you know, I mean, deep learning, you know, as you know, it's like kind of aspirational,

464
00:44:29,040 --> 00:44:34,000
right? So like, you know, something to like enter a company, um, and I really should not name

465
00:44:34,000 --> 00:44:37,840
names here, but like, you know, we'll come in because like somebody's like, oh, I got to do deep

466
00:44:37,840 --> 00:44:41,840
learning. I got to use like, you know, weights and biases for my deep learning thing. But then

467
00:44:41,840 --> 00:44:45,760
most of the company is like using, you know, scikit-learn. And so they, you know, they pick up the

468
00:44:45,760 --> 00:44:51,600
tool and they use it. And I have no, you know, that's great. That's wonderful. I'm not, you know,

469
00:44:51,600 --> 00:44:57,280
I'm not a leadist about this, you know? Right, right. But yeah, I mean, I just honestly,

470
00:44:57,280 --> 00:45:00,880
I think it's probably like, you know, 80% of the people we're talking to are doing,

471
00:45:00,880 --> 00:45:06,960
oh, some kind of neural mat. So it's, it's okay. That are like core user personas is doing deep

472
00:45:06,960 --> 00:45:15,440
learning. Okay. Um, we're not going to ban you from my tool if you use non-neural methods.

473
00:45:16,720 --> 00:45:25,680
And do you have folks or do you anticipate, uh, users that are using artifacts independent

474
00:45:25,680 --> 00:45:32,640
of experiment management? Or do you see this as, uh, side dish to experiment management?

475
00:45:33,840 --> 00:45:37,280
Uh, I guess I view this as an adjacent product. So yeah, you can, you can use this without the

476
00:45:37,280 --> 00:45:41,280
experiment management, but I'm really proud of our experiment management. What I would try to,

477
00:45:41,280 --> 00:45:49,520
I would try to get you to use that. Um, so I mean, right, right now it's, I think that over

478
00:45:49,520 --> 00:45:53,280
lots of 100% because we've really targeted the people doing, you know, using our experiment

479
00:45:53,280 --> 00:45:58,240
management tool, uh, but you know, over time that might change. I think they're pretty similar to

480
00:45:58,240 --> 00:46:04,720
our hyper parameter, um, sweeps, um, is pretty deeply attached to our experiment management. So,

481
00:46:04,720 --> 00:46:09,360
you know, that's similar, um, overlap. Although I think the, the hyper parameter search and the

482
00:46:10,720 --> 00:46:15,520
artifacts is totally independent, you know, you, you could definitely, um, you know, use one or

483
00:46:15,520 --> 00:46:24,800
the other, or neither. Mm hmm. If folks want to, um, take advantage of, uh, artifacts,

484
00:46:25,680 --> 00:46:31,360
is there thinking that they need to do about the way that they process data that they've likely

485
00:46:31,360 --> 00:46:38,320
not done before? Well, look, I think that people should only use software where they have

486
00:46:38,320 --> 00:46:43,680
actual pain points that they can articulate. So, you know, this doesn't happen very often,

487
00:46:43,680 --> 00:46:48,000
but it happens more than, it happens occasionally, and I'm, I'm always kind of baffled by it where

488
00:46:48,000 --> 00:46:52,720
people are like, oh, I want to use your tool, but like, what does it do? You know, it's kind of like,

489
00:46:52,720 --> 00:46:57,680
okay, you probably don't want to use my tool and, you know, let me give you, let me give you an example.

490
00:46:59,360 --> 00:47:07,040
So, um, you know, business process management, right? It's a, it's a similar space. It's like,

491
00:47:07,040 --> 00:47:12,160
essentially, you know, you want to automate some workflow in your business. Maybe it's like

492
00:47:12,160 --> 00:47:19,600
order to cash or something like that. And you may have, you know, 100 people doing it,

493
00:47:19,600 --> 00:47:24,720
you know, you documented the process 10 years ago. The process has evolved for 10 years, and now you

494
00:47:24,720 --> 00:47:30,080
really have no idea how, you know, at a top level, like how your orders are being processed. They

495
00:47:30,080 --> 00:47:35,600
bounce around the organization and the first thing that you need to do to actually implement that

496
00:47:35,600 --> 00:47:42,720
into a tool is to go in and understand what's really being done, right? What are the connections

497
00:47:42,720 --> 00:47:47,600
from one thing to the next? What are the exceptions that pop up? What are the rules that people are

498
00:47:47,600 --> 00:47:53,440
applying implicitly? You know, all of that stuff. And I'm, you know, curious with this question,

499
00:47:53,440 --> 00:47:59,920
are there similar things that you run into where, you know, folks, you know, there are aspects of

500
00:47:59,920 --> 00:48:04,480
the way that their data is being used in this machine learning process that they haven't really

501
00:48:04,480 --> 00:48:09,360
thought about and need to kind of work out before they can implement something like this or,

502
00:48:10,160 --> 00:48:15,040
you know, it's just so, you know, it's all so new and they were doing something, you know,

503
00:48:15,040 --> 00:48:22,960
that was programmatic or rigid enough before that no, they just need to point your tool at some

504
00:48:22,960 --> 00:48:27,760
stuff and it all just works. It's interesting, is it actually a really interesting question because

505
00:48:27,760 --> 00:48:33,040
I think that, you know, my last company was kind of closer to business process management.

506
00:48:33,040 --> 00:48:40,160
And you, you had to buy into a lot to get it working. I think with this,

507
00:48:41,120 --> 00:48:44,560
you know, going, going back to like what I was saying around like pain point, I think the,

508
00:48:46,240 --> 00:48:52,240
you know, you don't have to, you don't have to have like the perfect setup or track everything

509
00:48:52,240 --> 00:48:58,400
to get the benefit, you know, from a tool like this. So like the, I think the point that you would

510
00:48:58,400 --> 00:49:03,040
want to use a tool like our artifacts is when you're asking questions like our customers are asking

511
00:49:03,040 --> 00:49:09,200
like, you know, do we really know exactly what data all the models are trained on or are we worried

512
00:49:09,200 --> 00:49:14,160
about, you know, like our data sets are changing and we're trying to track it. Are we worried that like,

513
00:49:14,160 --> 00:49:17,600
you know, the person who actually writes down what the different data sets are is going to leave

514
00:49:17,600 --> 00:49:23,600
and it's going to be able to know like what they, you know, what they actually were. And I would say

515
00:49:23,600 --> 00:49:29,360
that's actually the most common entry point is around data set, like worrying about data sets,

516
00:49:29,360 --> 00:49:37,120
right? So, you know, like you could get like all that benefit by just tracking the data sets and,

517
00:49:38,240 --> 00:49:42,400
and that is a fairly light instrumentation. It's actually not quite as light as our

518
00:49:42,400 --> 00:49:46,080
experiment management, which we've really worked hard to get down to like under, you know,

519
00:49:46,080 --> 00:49:52,080
five, six minutes. Like this is like slightly more involved. And, but the benefit is bigger in that

520
00:49:52,080 --> 00:49:58,400
you'll for now, for sure, like always know the data sets that got, you know, that you trained on.

521
00:49:58,400 --> 00:50:04,160
And then, you know, down the road, you may, you know, have another concern, which is like, you know,

522
00:50:04,160 --> 00:50:10,640
do I really know, you know, like my models to plan to production, you know, do I really know

523
00:50:10,640 --> 00:50:14,720
exactly what happened to them? Like, do I know like what the preprocessing was? And now you can,

524
00:50:14,720 --> 00:50:18,880
you know, save the model and you can, you know, start to save pipelines, but you don't have to,

525
00:50:18,880 --> 00:50:25,600
it's not like you have to, you know, buy into a huge process to get the benefit. I think on like

526
00:50:25,600 --> 00:50:32,240
other like Senate organizational things that, that teams do, this is really like, this is really some,

527
00:50:32,240 --> 00:50:42,160
I, in fact, I would like kind of encourage teams to adopt it one step at a time versus like trying,

528
00:50:42,160 --> 00:50:45,600
I mean, there's something about like, I think actually, maybe it's that ML is like sort of less,

529
00:50:45,600 --> 00:50:50,080
you know, understood exactly what all the best practices are. Like, you know, my recommendations

530
00:50:50,080 --> 00:50:55,840
always think working ends, you know, and then iterate and improve it. And, you know, part of

531
00:50:55,840 --> 00:51:00,480
improving it is getting better tracking and, you know, getting more organized. And I think that's

532
00:51:00,480 --> 00:51:09,440
the point where, you know, artifacts would make sense to you as a product. And is there a common

533
00:51:09,440 --> 00:51:19,040
pattern that you saw or that you can envision for folks that, you know, aren't ready to move to

534
00:51:19,040 --> 00:51:25,680
a full-on, you know, tool to do this, but, you know, we'll get them part of the way, you know,

535
00:51:25,680 --> 00:51:30,000
from nowhere. Like, what's the, what's the half step or the...

536
00:51:30,000 --> 00:51:34,880
Well, okay, so there's some half steps that you really, you can do. I mean, like, later on,

537
00:51:34,880 --> 00:51:41,520
doing them already, you should be doing what? Well, I mean, this is, I mean, this is

538
00:51:41,520 --> 00:51:45,360
have done, I mean, I'm like, guilty of not doing this, but it's like, you know, if we're creating

539
00:51:45,360 --> 00:51:49,840
your model on a data set, I mean, I don't know, like, I feel like when I have data sets, right,

540
00:51:49,840 --> 00:51:55,920
I start, I mean, I start naming them like, you know, like, XYZ latest, and then like, XYZ,

541
00:51:55,920 --> 00:52:05,600
like, really latest, like, really latest, you know, V2. And it's like, you don't do that. You know,

542
00:52:05,600 --> 00:52:12,160
storage is actually cheap. It's just cheaper than you think. And, you know, that, you know,

543
00:52:12,160 --> 00:52:17,840
that stuff saves you a few seconds or, you know, a day in the short term, and it really bites you

544
00:52:17,840 --> 00:52:26,000
in the medium, the short to medium term. I mean, you know, I think anyone listening that,

545
00:52:26,000 --> 00:52:30,000
that's trained models will kind of understand that. And I think like, you know, like, I mean,

546
00:52:30,000 --> 00:52:38,800
you can make a spreadsheet, and you can, you know, you can be organized about the, you know, all the

547
00:52:38,800 --> 00:52:42,560
model files that you have and like, what they are and what happened to them. And, you know, I think,

548
00:52:42,560 --> 00:52:47,920
like, most, I mean, most people that we talk to are doing some form of this because it's not like,

549
00:52:47,920 --> 00:52:52,080
it's not like a brilliant insight that we had that, you know, you should be organized about your

550
00:52:52,080 --> 00:52:57,520
training, right? So I think most people find some way to do this. And I think like, most experienced

551
00:52:57,520 --> 00:53:04,560
ML practitioners, as they, you know, gain more experience, they get more and more, you know,

552
00:53:04,560 --> 00:53:08,880
kind of paranoid and organized about, you know, about this kind of, except I think where things

553
00:53:08,880 --> 00:53:15,360
like really break down is like, I just, I don't know, I just remember my first job. There was like

554
00:53:15,360 --> 00:53:19,840
this file, like, we had like this file, it was like all the features that we had. And for some

555
00:53:19,840 --> 00:53:26,480
fucking like the TPS file, oh man, I remember that all the features were named like, it was like,

556
00:53:26,480 --> 00:53:29,840
it was like, as though, like, there's some limit on the number, like, letters you could put in the,

557
00:53:30,560 --> 00:53:34,480
the column name, you know what I mean? So they were like, it would literally be like five letter,

558
00:53:34,480 --> 00:53:39,680
you know, five letters in like a number, you know, and like, and it was like, and someone made

559
00:53:39,680 --> 00:53:44,560
like a wiki page of like what they meant, but like they were kind of wrong, you know, because it

560
00:53:44,560 --> 00:53:48,880
wasn't like the person that like made all the features, it was like someone who like kind of tried

561
00:53:48,880 --> 00:53:54,640
to figure it out, you know, like, and I mean, I don't know, like that, I mean, that's like where we

562
00:53:54,640 --> 00:54:01,040
ended up, you know, and like, you know, this is a Yahoo and in 2006, you know, like building,

563
00:54:01,040 --> 00:54:04,720
and it was like, you know, this was like billions of dollars, it was like the relevant engine,

564
00:54:04,720 --> 00:54:08,160
you know, back when Yahoo served like a ton of search traffic, so it was like, there's no joke,

565
00:54:08,160 --> 00:54:12,880
like, we wanted it to be good, you know, but it's just, it's so natural that like, you know,

566
00:54:12,880 --> 00:54:16,480
one person like makes this thing, it's like obvious to them, and they don't document it very well,

567
00:54:16,480 --> 00:54:21,280
and then, you know, when they leave, and another person picks it up, they don't even really know,

568
00:54:21,280 --> 00:54:27,760
like, what's going on? So, you know, I think, I think, you know, documenting and saving is a,

569
00:54:27,760 --> 00:54:35,120
uh, is it really good? Yeah. Cool. Um, well, Lucas, thanks once again for, uh, you know,

570
00:54:35,120 --> 00:54:41,760
joining us, sharing a bit about what you're up to with me and the listeners, and as always,

571
00:54:41,760 --> 00:54:45,040
it's great to catch up with you. Yeah. Real pleasure. Thank you. Thank you.

572
00:54:47,520 --> 00:54:52,480
All right, everyone. That's our show for today. To learn more about today's guest or the

573
00:54:52,480 --> 00:54:58,640
topics mentioned in this interview, visit twimmelai.com. Of course, if you like what you hear on the

574
00:54:58,640 --> 00:55:05,200
podcast, please subscribe, rate, and review the show on your favorite pod catcher. Thanks so much

575
00:55:05,200 --> 00:55:31,600
for listening, and catch you next time.

