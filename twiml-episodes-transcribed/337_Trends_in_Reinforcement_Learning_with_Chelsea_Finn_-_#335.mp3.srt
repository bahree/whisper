1
00:00:00,000 --> 00:00:25,760
Hey everyone, hope you all had a wonderful holiday.

2
00:00:25,760 --> 00:00:30,520
For the next few weeks we'll be running back the clock with our second annual AI Rewind

3
00:00:30,520 --> 00:00:31,960
series.

4
00:00:31,960 --> 00:00:37,000
Join by a few friends of the show, we'll be reviewing the papers, tools, use cases,

5
00:00:37,000 --> 00:00:43,160
and other developments that made us splash in 2019 in key fields like machine learning,

6
00:00:43,160 --> 00:00:49,520
deep learning, NLP, computer vision, reinforcement learning, and ethical AI.

7
00:00:49,520 --> 00:00:55,720
Be sure to follow along with the series at twomolai.com slash rewind 19.

8
00:00:55,720 --> 00:01:00,120
As always, we'd love to hear your thoughts on this series, including anything we might

9
00:01:00,120 --> 00:01:01,120
have missed.

10
00:01:01,120 --> 00:01:06,640
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via

11
00:01:06,640 --> 00:01:11,520
a comment on the show notes page you can find at twomolai.com.

12
00:01:11,520 --> 00:01:14,480
Happy New Year, let's get into the show.

13
00:01:14,480 --> 00:01:18,080
Alright everyone, I am on the line with Chelsea Finn.

14
00:01:18,080 --> 00:01:23,800
Chelsea is an assistant professor of computer science at Stanford University.

15
00:01:23,800 --> 00:01:27,160
Chelsea, welcome back to the twomolai podcast.

16
00:01:27,160 --> 00:01:28,160
Thank you Sam.

17
00:01:28,160 --> 00:01:32,400
Alright, so we have not spoken since it's been quite a while.

18
00:01:32,400 --> 00:01:39,120
You were at Berkeley at the time, it was back in June of 2017 for twomol talk number 29,

19
00:01:39,120 --> 00:01:43,080
we're over 300 shows now, so quite a while.

20
00:01:43,080 --> 00:01:50,920
Before we jump into our focus for this conversation, which is a look back at 2019 and all the

21
00:01:50,920 --> 00:01:55,120
exciting developments in reinforcement learning, why don't you catch us up on what you've

22
00:01:55,120 --> 00:01:57,200
been up to over the past couple of years?

23
00:01:57,200 --> 00:02:03,000
Yeah, so I finished my dissertation at UC Berkeley, now I'm an assistant professor at

24
00:02:03,000 --> 00:02:07,520
Stanford, and I guess in my research, some of the things I've been really thinking about

25
00:02:07,520 --> 00:02:13,480
recently are how we can build machine learning systems and especially embodied systems such

26
00:02:13,480 --> 00:02:18,040
as robots that can generalize two different objects, two different environments, two different

27
00:02:18,040 --> 00:02:19,040
settings.

28
00:02:19,040 --> 00:02:24,960
And this is through the lens of reinforcement learning algorithms, as well as what's called

29
00:02:24,960 --> 00:02:29,200
metal learning algorithms, where you try to accumulate previous experience in a way that

30
00:02:29,200 --> 00:02:33,280
allows you to quickly learn new things or quickly adapt to new settings, rather than

31
00:02:33,280 --> 00:02:36,840
trying to learn from scratch for every new thing that you might want to do.

32
00:02:36,840 --> 00:02:42,120
So my group at Stanford has been starting to study some of these problems in generalization

33
00:02:42,120 --> 00:02:47,720
and reinforcement learning and robotics, and yeah, excited to be on the show today.

34
00:02:47,720 --> 00:02:51,800
Thanks, and you're also teaching a course at Stanford now on metal learning, is that right?

35
00:02:51,800 --> 00:02:52,800
Yeah, absolutely.

36
00:02:52,800 --> 00:02:55,600
I'm teaching a new course that is just wrapping up.

37
00:02:55,600 --> 00:03:01,400
We have our last class on Monday of next week, and then after actually after the course,

38
00:03:01,400 --> 00:03:07,040
all of the videos are going to be available online for the public to see all the videos

39
00:03:07,040 --> 00:03:08,040
of the lectures.

40
00:03:08,040 --> 00:03:09,360
Oh, that's fantastic.

41
00:03:09,360 --> 00:03:11,040
That is fantastic.

42
00:03:11,040 --> 00:03:18,200
So the format of this particular conversation is going to be, again, focused on 2019

43
00:03:18,200 --> 00:03:25,320
in review and reinforcement learning before we jump into the specific papers or topics

44
00:03:25,320 --> 00:03:28,920
that you thought weren't interesting this year.

45
00:03:28,920 --> 00:03:33,240
Can you kind of characterize the year for us, you know, relative to other years that you've

46
00:03:33,240 --> 00:03:39,080
been following RL, you know, how was 2019 shaped up?

47
00:03:39,080 --> 00:03:43,200
Yeah, I think that research and reinforcement learning has really been picking up, and

48
00:03:43,200 --> 00:03:48,120
there have been an increasing interest in it, so there are more labs that were crucially

49
00:03:48,120 --> 00:03:52,920
focusing on supervised learning or unsupervised learning that are now going into this setting

50
00:03:52,920 --> 00:03:56,520
where agents need to make multiple decisions, and people have different motivations for

51
00:03:56,520 --> 00:03:57,520
doing that.

52
00:03:57,520 --> 00:04:02,640
But I think that really the field is expanding, and that's been an exciting time, and also

53
00:04:02,640 --> 00:04:07,400
with that expansion, I think that people have been studying a broader range of reinforcement

54
00:04:07,400 --> 00:04:08,400
learning problems.

55
00:04:08,400 --> 00:04:13,560
So before people were kind of narrowly focused on a few benchmarks, and I think that now

56
00:04:13,560 --> 00:04:17,000
that is opening up, and people are kind of reconsidering different formulations and

57
00:04:17,000 --> 00:04:21,080
different problem settings within the context of reinforcement learning, and with that

58
00:04:21,080 --> 00:04:25,880
there's also been some of the same big players that have been trying to advance the capabilities

59
00:04:25,880 --> 00:04:28,680
of our reinforcement learning systems as well.

60
00:04:28,680 --> 00:04:33,720
So I think that the, there's been progress on a lot of fronts, and it's been a pretty

61
00:04:33,720 --> 00:04:34,720
exciting year.

62
00:04:34,720 --> 00:04:40,800
Now, the benchmarks that have traditionally been used in RL, at least the ones that come

63
00:04:40,800 --> 00:04:46,840
to mind most immediately for me are video games, and in particular, historically it's been

64
00:04:46,840 --> 00:04:52,280
kind of simple Atari-style video games, but these have been getting a lot more complex

65
00:04:52,280 --> 00:04:53,280
over time.

66
00:04:53,280 --> 00:04:55,280
Yeah, absolutely.

67
00:04:55,280 --> 00:05:00,760
So one of the big focuses in reinforcement learning has been looking at Atari games,

68
00:05:00,760 --> 00:05:05,720
and there's still actually worse and really interesting progress made on those benchmarks,

69
00:05:05,720 --> 00:05:09,280
but I think that also a lot of research has been opening up and focusing on other problems

70
00:05:09,280 --> 00:05:10,280
as well.

71
00:05:10,280 --> 00:05:16,120
Another very common benchmark in previous years has been the, these continuous control tasks

72
00:05:16,120 --> 00:05:22,480
of these simulated robots in the Bajako physics engine, and using like open-air gym, for

73
00:05:22,480 --> 00:05:23,480
example.

74
00:05:23,480 --> 00:05:26,240
An example of that would be like the cart pull.

75
00:05:26,240 --> 00:05:31,960
It's like cart pull, but also what's called the half cheetah or the ant, which is actually

76
00:05:31,960 --> 00:05:37,320
a four-legged creature, these types of locomotion, simulated locomotion tasks.

77
00:05:37,320 --> 00:05:38,320
Wow, okay.

78
00:05:38,320 --> 00:05:43,560
So these are, we'll see these videos with an agent, essentially learning to walk or trying

79
00:05:43,560 --> 00:05:46,040
to get from one place to another most efficiently.

80
00:05:46,040 --> 00:05:48,840
Those types of benchmarks got it, okay.

81
00:05:48,840 --> 00:05:49,840
All right.

82
00:05:49,840 --> 00:05:50,840
Awesome.

83
00:05:50,840 --> 00:05:56,200
So your task in preparing for this session is to come up with a few or was to come

84
00:05:56,200 --> 00:05:59,000
up with a few papers that you thought were significant.

85
00:05:59,000 --> 00:06:04,800
You, in fact, came up with topics, some of which had multiple papers that you thought were

86
00:06:04,800 --> 00:06:05,800
really interesting.

87
00:06:05,800 --> 00:06:08,920
Why don't we just jump in and have you walk us through these?

88
00:06:08,920 --> 00:06:09,920
Yeah.

89
00:06:09,920 --> 00:06:15,080
So I think that it, there's often never just one single paper that really solidifies, and

90
00:06:15,080 --> 00:06:20,800
a result is often many multiple results that actually really show you what has been,

91
00:06:20,800 --> 00:06:25,800
what is capable and what you can, what these algorithms are capable of and what these

92
00:06:25,800 --> 00:06:29,880
what can be done with their current technology, and so that's why I wanted to focus on these

93
00:06:29,880 --> 00:06:34,880
different topics because I think that there are, it's not just one paper for each thing.

94
00:06:34,880 --> 00:06:40,240
So the kind of the first thing that I wanted to highlight was thinking about reinforcement

95
00:06:40,240 --> 00:06:41,160
learning in the real world.

96
00:06:41,160 --> 00:06:46,880
So there's been some pretty impressive progress on reinforcement learning on real robots

97
00:06:46,880 --> 00:06:48,640
for dexterous manipulation tasks.

98
00:06:48,640 --> 00:06:55,920
So think like a five-fingered hand that can do things such as turn, turn Rubik's cube size

99
00:06:55,920 --> 00:07:01,800
or can manipulate to what's called bounding balls in the palm of the hand and rotate

100
00:07:01,800 --> 00:07:04,640
them with a single hand.

101
00:07:04,640 --> 00:07:08,400
And so we've seen reinforcement learning algorithms that are able to learn both of these

102
00:07:08,400 --> 00:07:12,480
very dexterous manipulation tasks with five finger hands in the real world.

103
00:07:12,480 --> 00:07:16,520
So one, but one thing that was interesting, there were actually two papers that showed

104
00:07:16,520 --> 00:07:22,160
these results, one that actually trained completely in simulation and then tried to transfer

105
00:07:22,160 --> 00:07:25,880
what was learned in simulation to the real robot, and one which learned completely in

106
00:07:25,880 --> 00:07:30,280
the real world and was actually efficient enough to run in the real world.

107
00:07:30,280 --> 00:07:33,760
And so one of the things that I found really exciting about these results is it showed

108
00:07:33,760 --> 00:07:39,480
the sort of complexity that we could learn in the real world using reinforcement learning

109
00:07:39,480 --> 00:07:44,840
algorithms, and it was interesting to see how divergent the two approaches were for

110
00:07:44,840 --> 00:07:48,200
accomplishing somewhat similar objectives.

111
00:07:48,200 --> 00:07:51,920
How would you characterize the divergent nature of these two approaches?

112
00:07:51,920 --> 00:07:55,400
What were the key things that they did differently?

113
00:07:55,400 --> 00:07:58,520
Yeah, so the key thing was simulation versus real.

114
00:07:58,520 --> 00:08:03,040
So one of them was, and it's a little bit more that one of them was trying to take a

115
00:08:03,040 --> 00:08:08,040
really powerful reinforcement learning method and train it in a variety of different simulated

116
00:08:08,040 --> 00:08:10,920
settings in a way that allowed it to transfer to the real world.

117
00:08:10,920 --> 00:08:12,760
And one was extremely focused on efficiency.

118
00:08:12,760 --> 00:08:16,520
So if you're running on a real robot, a five-fingered hand that is a bit fragile, it isn't

119
00:08:16,520 --> 00:08:21,040
something that you can really put a lot of wear and tear on, then you need to be extremely

120
00:08:21,040 --> 00:08:22,040
efficient.

121
00:08:22,040 --> 00:08:27,480
You need to be learning in a way that you don't break the hand in the process of reinforcement

122
00:08:27,480 --> 00:08:28,480
learning.

123
00:08:28,480 --> 00:08:33,920
So, and so the two algorithms that were developed here were in many ways just like completely

124
00:08:33,920 --> 00:08:38,600
different from each other, but yet achieved a kind of similar result in the real world.

125
00:08:38,600 --> 00:08:45,240
And the Rubik's Cube paper, that's a relatively recent one in the year, and that was some

126
00:08:45,240 --> 00:08:50,720
results by OpenAI, and those caused a bit of a stir in the community that the results

127
00:08:50,720 --> 00:08:53,200
they presented were overhyped or over marketed.

128
00:08:53,200 --> 00:08:55,640
Do you have a take on that?

129
00:08:55,640 --> 00:09:02,600
Yeah, so I think that the concern was that the kind of the title of the approach or of

130
00:09:02,600 --> 00:09:09,200
the blog post was about solving Rubik's cubes using reinforcement learning, and what they

131
00:09:09,200 --> 00:09:13,680
were actually doing was they were using a Rubik's Cube solver, and then figuring out how

132
00:09:13,680 --> 00:09:19,760
you could do the, they were using a Rubik's Cube solver to figure out which face to turn

133
00:09:19,760 --> 00:09:26,000
in which way, and so that kind of was in some ways pretty specified, and then the reinforcement

134
00:09:26,000 --> 00:09:31,720
learning part was figuring out how you can turn that face with a physical hand.

135
00:09:31,720 --> 00:09:35,960
And so this was, this was a bit controversial because it was people got the impression

136
00:09:35,960 --> 00:09:41,040
that they were learning all of the moves of solving the cube with reinforcement learning

137
00:09:41,040 --> 00:09:45,120
and not just the physical aspect of it.

138
00:09:45,120 --> 00:09:48,720
In many ways, I actually think that the physical aspect of it, while it seems like it should

139
00:09:48,720 --> 00:09:55,480
be simpler, because we are like basic manipulation skills are so intuitive and basic to us.

140
00:09:55,480 --> 00:10:01,760
I think that actually in reality, these sort of physical contact is actually much harder than

141
00:10:01,760 --> 00:10:02,760
solving the Rubik's Cube.

142
00:10:02,760 --> 00:10:05,920
We've had solvers for Rubik's cubes for a very long time.

143
00:10:05,920 --> 00:10:09,440
This is one of the first times that we've actually seen, or robotic hand, be able to have

144
00:10:09,440 --> 00:10:12,520
the dexterity that allows it to rotate one of the paces.

145
00:10:12,520 --> 00:10:18,040
Well, and there are so many more degrees of freedom with the hand than the cube itself.

146
00:10:18,040 --> 00:10:23,240
Yeah, yeah, and so yeah, there was, there was some controversy there, and also just generally

147
00:10:23,240 --> 00:10:29,240
the way that things were handled with like the press and everything, because they also

148
00:10:29,240 --> 00:10:36,360
put a significant effort into producing a video and like marketing the work, whereas

149
00:10:36,360 --> 00:10:41,920
in contrast, a lot of research labs don't do that as you might expect.

150
00:10:41,920 --> 00:10:44,760
So there was also some controversy there.

151
00:10:44,760 --> 00:10:49,320
For the record, we'll include links to all of these papers in the show notes, the ones

152
00:10:49,320 --> 00:10:54,200
we just spoke about were solving Rubik's Cube with a robot hand, and deep dynamics for

153
00:10:54,200 --> 00:10:55,960
learning dexterous manipulation.

154
00:10:55,960 --> 00:11:02,720
Yeah, and the second paper was by Anusha Nagarbadi, who is a PhD student at UC Berkeley, and

155
00:11:02,720 --> 00:11:05,560
was doing an internship at Google Brain at the time.

156
00:11:05,560 --> 00:11:07,280
Okay, great, great.

157
00:11:07,280 --> 00:11:13,760
So lots of progress on applying reinforcement learning to manipulating physical objects

158
00:11:13,760 --> 00:11:15,520
with robotic hands.

159
00:11:15,520 --> 00:11:20,160
So the second approach that we talked about by Anusha was using a technique called model

160
00:11:20,160 --> 00:11:24,040
based reinforcement learning, where you learn a dynamics model of the world, and then you

161
00:11:24,040 --> 00:11:28,840
do reinforcement learning using that dynamics model to optimize a policy or to optimize

162
00:11:28,840 --> 00:11:30,520
over actions.

163
00:11:30,520 --> 00:11:35,320
And so this kind of approach in general, I think, has in some ways received less attention

164
00:11:35,320 --> 00:11:39,600
by the machine learning community than model free reinforcement learning methods.

165
00:11:39,600 --> 00:11:43,160
But this year, I think that we actually saw an increased interest in model based reinforcement

166
00:11:43,160 --> 00:11:46,000
learning methods, and one of the reasons why people do like to use them is they tend to

167
00:11:46,000 --> 00:11:48,520
be more sample efficient.

168
00:11:48,520 --> 00:11:53,000
But the challenge is if you're in a vision based domain, if you only see images, if you

169
00:11:53,000 --> 00:11:58,320
only see image pixels as your observations, then in principle, learning a model involves

170
00:11:58,320 --> 00:12:01,840
potentially even predicting pixels forward into the future, like learning a video prediction

171
00:12:01,840 --> 00:12:04,760
model, and that can be very challenging.

172
00:12:04,760 --> 00:12:10,040
And so this year, we saw some real progress, I think, on model based reinforcement learning

173
00:12:10,040 --> 00:12:15,120
on vision based domains, one approach that predict pixels into the future.

174
00:12:15,120 --> 00:12:19,760
So I was actually learning a video prediction model and showed how you can use this for things

175
00:12:19,760 --> 00:12:24,720
like Atari Games, and one which bypassed the need to predict pixels by pushing forward

176
00:12:24,720 --> 00:12:28,880
in a latent representation learned by neural network, and using that latent representation

177
00:12:28,880 --> 00:12:34,440
to predict things like the future values and the actions of a policy.

178
00:12:34,440 --> 00:12:37,840
And so I guess one of the things that I was really impressed by and thought was interesting

179
00:12:37,840 --> 00:12:42,640
with these two works is that it really showed the viability of model based reinforcement

180
00:12:42,640 --> 00:12:47,440
learning as an approach, even for some of the benchmarks that have been so heavily studied

181
00:12:47,440 --> 00:12:49,480
by model free approaches.

182
00:12:49,480 --> 00:12:53,320
So I think there are a number of benefits of model based, and I wasn't necessarily thinking

183
00:12:53,320 --> 00:12:58,280
that those benefits would come on things like Atari, for example, but we were able to

184
00:12:58,280 --> 00:13:03,400
see significant progress from these works on those domains.

185
00:13:03,400 --> 00:13:08,280
In the case of the Atari work, what does a model look like in that context?

186
00:13:08,280 --> 00:13:13,960
Yeah, so for the first paper, the model was actually generating images into the future.

187
00:13:13,960 --> 00:13:16,360
So it's a big neural network.

188
00:13:16,360 --> 00:13:18,800
It's fully convolutional.

189
00:13:18,800 --> 00:13:26,360
So it's basically taking in an input frame, it produces convolutional feature maps and

190
00:13:26,360 --> 00:13:30,800
produces a representation and then has deconvolutions to produce the next image.

191
00:13:30,800 --> 00:13:33,880
And it also takes us and put the action to produce the next image.

192
00:13:33,880 --> 00:13:37,800
So it's trying to predict what will the next image or the next sequence of images look

193
00:13:37,800 --> 00:13:42,360
like given the current image and a sequence of actions.

194
00:13:42,360 --> 00:13:48,960
And then that future predicted image becomes additional input to the RL learner.

195
00:13:48,960 --> 00:13:53,800
So yeah, so then that image you can use to essentially generate more data for your

196
00:13:53,800 --> 00:13:54,800
reinforcement learner.

197
00:13:54,800 --> 00:13:59,240
So instead of always taking actions in the real environment, you could also take actions

198
00:13:59,240 --> 00:14:03,960
in this learned environment and use that to improve your policy.

199
00:14:03,960 --> 00:14:09,240
Sometimes when we use model-based approaches, the model isn't necessarily a learn model.

200
00:14:09,240 --> 00:14:11,880
Is it a learn model in both of these cases?

201
00:14:11,880 --> 00:14:14,520
Yeah, so in both of these cases, it is a learn model.

202
00:14:14,520 --> 00:14:20,520
There's a field, especially prominent in robotics called model-based control that typically

203
00:14:20,520 --> 00:14:23,640
assumes that you know the model of the environment.

204
00:14:23,640 --> 00:14:26,280
And I guess it's not always, it's not just in robotics as well.

205
00:14:26,280 --> 00:14:28,920
There are a number of approaches where it assumes that you know the model.

206
00:14:28,920 --> 00:14:35,800
You know the true simulation of your system and use that known simulation in order to

207
00:14:35,800 --> 00:14:36,800
learn the task.

208
00:14:36,800 --> 00:14:42,040
And in many ways, in Atari, it's in some ways a little bit silly to actually learn the

209
00:14:42,040 --> 00:14:46,080
model because you actually have the real simulation system.

210
00:14:46,080 --> 00:14:49,640
But I think that in many ways, the reason why people care about these approaches that actually

211
00:14:49,640 --> 00:14:53,960
learn the model is that it means that it's applicable in settings such as the real world

212
00:14:53,960 --> 00:14:56,320
where you don't know the model.

213
00:14:56,320 --> 00:15:02,640
And so in both of these cases are, is the approach essentially the same where you're using

214
00:15:02,640 --> 00:15:06,400
model to predict images into the future?

215
00:15:06,400 --> 00:15:09,560
So the first one is predicting images into the future and the second one is actually

216
00:15:09,560 --> 00:15:12,040
predicting other quantities into the future.

217
00:15:12,040 --> 00:15:18,880
So it's predicting things like values, rewards, and actions into the future.

218
00:15:18,880 --> 00:15:22,360
And this is all, all of these predictions are conditioned on some latent representations.

219
00:15:22,360 --> 00:15:28,760
You can think of it as predicting the only the quantities that are relevant for the game.

220
00:15:28,760 --> 00:15:33,120
But in some latent representation, rather than having to predict all of the pixels, which

221
00:15:33,120 --> 00:15:36,560
include things that aren't necessarily relevant to doing well on the game.

222
00:15:36,560 --> 00:15:37,560
Got it.

223
00:15:37,560 --> 00:15:42,760
And so the pixels that you're predicting here might be pixels that represent things on the

224
00:15:42,760 --> 00:15:48,560
screen that are specific to specific to actions that might be taken, like I don't know

225
00:15:48,560 --> 00:15:54,480
a score or some kind of trying to come up with a good example of what that might be.

226
00:15:54,480 --> 00:16:02,240
But specific symbols on the screen, or are we out of, is the model predicting out of

227
00:16:02,240 --> 00:16:03,240
pixel space?

228
00:16:03,240 --> 00:16:04,240
Yeah.

229
00:16:04,240 --> 00:16:08,200
So in the second one, it's not actually predicting at all in the pixel space.

230
00:16:08,200 --> 00:16:14,440
It's basically, as a neural network that takes the image and produces a vector of representation.

231
00:16:14,440 --> 00:16:18,840
And then it predicts that representation forward, rather than predicting the pixels.

232
00:16:18,840 --> 00:16:25,000
But the fundamental, and both of these cases, we're starting from vision and pixels and

233
00:16:25,000 --> 00:16:31,680
using that to using models based on these pixels to kind of inform an RL learner, an

234
00:16:31,680 --> 00:16:32,680
RL agent.

235
00:16:32,680 --> 00:16:33,680
Yeah.

236
00:16:33,680 --> 00:16:34,680
Okay.

237
00:16:34,680 --> 00:16:35,680
All right.

238
00:16:35,680 --> 00:16:40,880
And so the next category that you wanted to cover is focused on batch off policy RL.

239
00:16:40,880 --> 00:16:41,880
What is that?

240
00:16:41,880 --> 00:16:47,560
Yeah, so, and actually the next two topics are focusing really on generalization and

241
00:16:47,560 --> 00:16:48,920
reinforcement learning in a way.

242
00:16:48,920 --> 00:16:54,320
So what batch off policy RL is, is say that you have, so I guess, let's first talk about

243
00:16:54,320 --> 00:16:55,960
kind of the standard reinforcement learning settings.

244
00:16:55,960 --> 00:16:59,560
So in the standard reinforcement learning setting, your agent collects some data, you learn

245
00:16:59,560 --> 00:17:03,240
from that data, then you collect some more data, you learn from that data, and you iterate

246
00:17:03,240 --> 00:17:05,200
this process.

247
00:17:05,200 --> 00:17:10,680
And what off policy reinforcement learning methods are methods that, I guess, maybe I'll start

248
00:17:10,680 --> 00:17:15,200
with on policy methods, on policy methods, they collect data, learn from that data, and

249
00:17:15,200 --> 00:17:19,560
then they throw away that data and collect a new batch of data and learn.

250
00:17:19,560 --> 00:17:23,240
And so they're always collecting data from their current, their current policy, their current

251
00:17:23,240 --> 00:17:28,000
actor, and they can't, they don't have a way to reuse any data that they collected previously

252
00:17:28,000 --> 00:17:31,600
because they need the data to be from their current policy.

253
00:17:31,600 --> 00:17:35,640
Now off policy methods are ones that can actually leverage data that they collected from

254
00:17:35,640 --> 00:17:36,640
previous policies.

255
00:17:36,640 --> 00:17:42,120
One that can leverage what's called off policy data, data that's not from your current policy,

256
00:17:42,120 --> 00:17:44,080
your current behavior.

257
00:17:44,080 --> 00:17:46,760
And these algorithms tend to be a lot more efficient because you don't, you're not throwing

258
00:17:46,760 --> 00:17:50,880
away data at every single iteration of your algorithm.

259
00:17:50,880 --> 00:17:55,960
Now batch off policy RL algorithms take this to an extreme where they assume that you

260
00:17:55,960 --> 00:18:00,840
actually are just given a batch of data from some, from some policy from, according to

261
00:18:00,840 --> 00:18:05,200
some behavior, and then try to learn from that and I don't have any ability to collect

262
00:18:05,200 --> 00:18:07,480
more data in that environment.

263
00:18:07,480 --> 00:18:12,200
And the reason why this is really important and interesting is that first, if you think

264
00:18:12,200 --> 00:18:17,800
of just the majority of machine learning, you consider like, you often have settings

265
00:18:17,800 --> 00:18:21,240
where you have some data set, maybe something like image net, you're just given a batch

266
00:18:21,240 --> 00:18:23,920
of data and you want to learn from that data.

267
00:18:23,920 --> 00:18:29,960
And if we have algorithms that can just learn behaviors and learn policies from a batch

268
00:18:29,960 --> 00:18:34,480
of data, that means that we can start just accumulating very large and diverse data sets

269
00:18:34,480 --> 00:18:38,720
and allowing algorithms to learn from them without having to kind of have this iterative

270
00:18:38,720 --> 00:18:40,560
data collection process in the loop.

271
00:18:40,560 --> 00:18:44,320
And the second reason why it's important is that if there are a number of settings

272
00:18:44,320 --> 00:18:50,360
where it's just, it's unsafe or not possible to collect more data such as if you imagine

273
00:18:50,360 --> 00:18:53,960
for example, wanting to learn how to make medical decisions.

274
00:18:53,960 --> 00:19:01,200
You want to learn from data of doctors' decisions or maybe you have another system that's interacting

275
00:19:01,200 --> 00:19:08,680
with users in a way that isn't safe to take actions in random ways, then you want to

276
00:19:08,680 --> 00:19:14,240
just be able to use data from doctors, for example, and learn from that data without having

277
00:19:14,240 --> 00:19:20,280
to experiment or collect more data by kind of randomly taking actions or by exploring.

278
00:19:20,280 --> 00:19:21,640
Does that make sense?

279
00:19:21,640 --> 00:19:22,640
It does.

280
00:19:22,640 --> 00:19:30,440
One question I have is, does the sequence of actions, is that necessarily included from

281
00:19:30,440 --> 00:19:35,800
or excluded from the typical problem set up for batch off policy RL?

282
00:19:35,800 --> 00:19:40,080
Yeah, so typically you do assume that you have the actions that were taken as well.

283
00:19:40,080 --> 00:19:45,200
So you know what action or what decision the doctor made in the medical decision making

284
00:19:45,200 --> 00:19:47,200
example.

285
00:19:47,200 --> 00:19:50,840
But one quite interesting setting would be, maybe you have some data with actions, but

286
00:19:50,840 --> 00:19:55,480
some data without actions, like you're just observing humans on the internet doing stuff

287
00:19:55,480 --> 00:20:00,120
and maybe you could try to learn how to manipulate objects from that data and that's a setting

288
00:20:00,120 --> 00:20:03,320
that some students in my lab have actually been studying recently.

289
00:20:03,320 --> 00:20:04,320
Interesting.

290
00:20:04,320 --> 00:20:10,720
Yeah, that question was prompted by your idea that at some point we might just be able

291
00:20:10,720 --> 00:20:18,480
to take some collection of data and learn from it using off policy RL agent, but that

292
00:20:18,480 --> 00:20:23,840
would seem to assume that there's some known sequence of actions that you'd have to

293
00:20:23,840 --> 00:20:30,600
have that and that seems to make it less of a natural data set if that makes any sense.

294
00:20:30,600 --> 00:20:35,640
With that background in mind, what were the specific advancements in the couple of papers

295
00:20:35,640 --> 00:20:37,600
you identified on this topic?

296
00:20:37,600 --> 00:20:43,080
Yeah, so I guess getting back to benchmarks, we actually don't have really good benchmarks

297
00:20:43,080 --> 00:20:48,880
for this setting that actually have meaningful real world settings, but the algorithms themselves

298
00:20:48,880 --> 00:20:54,320
showed a lot of promise towards enabling good learning in these settings.

299
00:20:54,320 --> 00:21:00,120
So the first one actually took the replay buffer of an agent on Atari and when they only

300
00:21:00,120 --> 00:21:07,180
took this replay buffer, they were actually able to outperform the policy in that replay

301
00:21:07,180 --> 00:21:10,840
buffer just by learning from that data.

302
00:21:10,840 --> 00:21:17,320
So that result I think was quite impressive and also is just a suggestion that we should

303
00:21:17,320 --> 00:21:22,080
be able to learn very well from these types of data if we set up our algorithms well.

304
00:21:22,080 --> 00:21:26,920
And the second approach, which actually, the second paper that I linked, which actually

305
00:21:26,920 --> 00:21:30,240
predates the first one, also showed quite strong results.

306
00:21:30,240 --> 00:21:33,600
They weren't looking at the Atari domain.

307
00:21:33,600 --> 00:21:37,880
They're looking more at these continuous control domains, but they also showed the ability

308
00:21:37,880 --> 00:21:47,240
to learn behavior from these batches of data, not quite to the extent of the replay

309
00:21:47,240 --> 00:21:51,480
buffer, if I remember correctly, but we're showing pretty strong results there.

310
00:21:51,480 --> 00:21:57,760
The paper that's looking at the replay buffer, the result seems counterintuitive, if I'm

311
00:21:57,760 --> 00:22:05,600
interpreting it correctly, you basically have this replay data from a RL agent.

312
00:22:05,600 --> 00:22:10,360
So everything the agent kind of saw as it was exploring these games and then you give

313
00:22:10,360 --> 00:22:16,960
it to another agent to learn off of and the agent somehow performs better than the

314
00:22:16,960 --> 00:22:22,960
original agent, but it doesn't see necessarily anything more than the original agent saw

315
00:22:22,960 --> 00:22:28,560
as it that it has access to all of it at the same time, whereas the original agent only

316
00:22:28,560 --> 00:22:32,440
saw it in snapshots.

317
00:22:32,440 --> 00:22:33,680
So yeah, that's a good point.

318
00:22:33,680 --> 00:22:39,400
I think that the reason is that they didn't give it, they didn't train the first agent

319
00:22:39,400 --> 00:22:40,680
completely to convergence.

320
00:22:40,680 --> 00:22:49,280
They took a batch of the log data from that agent before it had reached its max performance

321
00:22:49,280 --> 00:22:54,600
and then showed that there was some room for improvement.

322
00:22:54,600 --> 00:23:01,000
If you give the online agent more data, it would have achieved the full maximum performance

323
00:23:01,000 --> 00:23:04,600
or would have achieved as well as the offline agent.

324
00:23:04,600 --> 00:23:08,680
It was just that they stopped things early and then wanted to test, can you do better

325
00:23:08,680 --> 00:23:11,160
with this batch of data?

326
00:23:11,160 --> 00:23:16,000
This gets to the question of how should we set up these experiments with batch off policy

327
00:23:16,000 --> 00:23:17,000
reinforcement learning methods.

328
00:23:17,000 --> 00:23:22,960
I think that things like training on logged DQN data or logged data from Atari isn't a

329
00:23:22,960 --> 00:23:27,960
great experimental set because it doesn't necessarily test the types of things that we want

330
00:23:27,960 --> 00:23:33,800
from these algorithms necessarily because we may not have that sort of data when we're

331
00:23:33,800 --> 00:23:38,360
performing reinforcement learning in real world settings, but it still I think is a step

332
00:23:38,360 --> 00:23:42,520
in the right direction and at least people are starting to study these kinds of problem

333
00:23:42,520 --> 00:23:43,520
settings.

334
00:23:43,520 --> 00:23:46,520
And proposing benchmarks for them.

335
00:23:46,520 --> 00:23:51,520
Yeah, so I guess one of the reasons why I found it interesting wasn't just the kind of

336
00:23:51,520 --> 00:23:57,320
the results, but also the visualizations and some of the, I think you provided an understanding

337
00:23:57,320 --> 00:24:03,760
of the problem in terms of understanding where what we can do in these problem settings

338
00:24:03,760 --> 00:24:05,240
to do better.

339
00:24:05,240 --> 00:24:10,080
And basically what are the kind of sorts of things that we might try to do and like in

340
00:24:10,080 --> 00:24:11,400
terms of solving this problem.

341
00:24:11,400 --> 00:24:15,400
So when you move from this kind of batch setting to a policy that you're learning from that

342
00:24:15,400 --> 00:24:19,960
batch of data, you have this distribution mismatch between the states and actions visited

343
00:24:19,960 --> 00:24:25,480
by the first policy and that batch of data and the states and actions visited by the policy

344
00:24:25,480 --> 00:24:27,880
that you're learning, the behavior that you're learning.

345
00:24:27,880 --> 00:24:32,240
And so it provides some nice visualizations and to understanding how we might try to handle

346
00:24:32,240 --> 00:24:38,320
this distribution mismatch and in particular they focus on the action setting, the distribution

347
00:24:38,320 --> 00:24:43,320
mismatch and the actions and made some nice visualizations for understanding what is actually

348
00:24:43,320 --> 00:24:46,040
happening under this distribution mismatch.

349
00:24:46,040 --> 00:24:49,920
And this distribution mismatch is what they're calling the bootstrapping error.

350
00:24:49,920 --> 00:24:51,160
I believe so, yes.

351
00:24:51,160 --> 00:24:52,160
Got it.

352
00:24:52,160 --> 00:24:58,160
You mentioned that this work on batch off policy as well as the next paper that you had in

353
00:24:58,160 --> 00:25:05,760
mind, RL with diverse offline data sets are kind of common in that they're both tackling

354
00:25:05,760 --> 00:25:08,760
generalization for RL.

355
00:25:08,760 --> 00:25:13,840
That can mean a lot of things in what sense are these focused on generalization?

356
00:25:13,840 --> 00:25:14,840
Yeah.

357
00:25:14,840 --> 00:25:18,440
So I guess the papers that we just talked about, they actually aren't really focused

358
00:25:18,440 --> 00:25:22,680
on generalization, but I think that if we build better batch off policy reinforcement

359
00:25:22,680 --> 00:25:27,040
learning methods, we'll have the ability to learn from more diverse data sets because

360
00:25:27,040 --> 00:25:32,560
we won't be collecting data for every, like within the context of our algorithm.

361
00:25:32,560 --> 00:25:38,520
So for example, in the context of a robotics, say, if you want to generalize to something

362
00:25:38,520 --> 00:25:42,960
at the level of ImageNet, that would mean that you'd have to collect an ImageNet style,

363
00:25:42,960 --> 00:25:48,320
ImageNet diverse size data set in the context of your reinforcement learning experiment.

364
00:25:48,320 --> 00:25:50,040
That just isn't practical, right?

365
00:25:50,040 --> 00:25:57,120
So we need to think about how we can have algorithms accumulate data into a large data set

366
00:25:57,120 --> 00:26:02,200
and then actually start sharing data just like the rest of machine learning does.

367
00:26:02,200 --> 00:26:06,280
So if we can build these very large data sets by having robots collect data and then store

368
00:26:06,280 --> 00:26:10,080
that into a very large buffer of data and then have algorithms that can learn from that

369
00:26:10,080 --> 00:26:14,800
very large buffer of data without having to kind of recollect it in the loop of reinforcement

370
00:26:14,800 --> 00:26:15,800
learning.

371
00:26:15,800 --> 00:26:21,960
And then we can start to see the generalization that we see in supervised learning settings.

372
00:26:21,960 --> 00:26:26,280
So this next paper is actually trying to start to study that, like can we accumulate,

373
00:26:26,280 --> 00:26:29,280
can we build a very large and diverse data set and then learn from it?

374
00:26:29,280 --> 00:26:34,320
Is it too far of a stretch to say that this is kind of pointing us in the direction

375
00:26:34,320 --> 00:26:40,000
of kind of a transfer learning type of approach as applied to RL?

376
00:26:40,000 --> 00:26:41,720
That's a good question.

377
00:26:41,720 --> 00:26:46,280
So in this particular paper, it is in many ways pointing at a transfer learning setting

378
00:26:46,280 --> 00:26:50,680
where you start, where you learn from this data set and then you learn representations

379
00:26:50,680 --> 00:26:57,560
for control and then you take that representation and then try to transfer that to a new setting.

380
00:26:57,560 --> 00:27:01,160
So analogous to ImageNet pre-training, for example, like ImageNet, you can pre-training

381
00:27:01,160 --> 00:27:05,880
on the ImageNet data set and then fine tune your features to a new task that allows you

382
00:27:05,880 --> 00:27:11,240
to transfer all of the rich diversity from ImageNet to your new problem setting.

383
00:27:11,240 --> 00:27:16,520
And we did something similar in this paper where we collected a very large data set,

384
00:27:16,520 --> 00:27:21,080
learned on it and then transferred to new robots, transferred to new experimental setups.

385
00:27:21,080 --> 00:27:23,200
But it's what I think that it potentially points towards that.

386
00:27:23,200 --> 00:27:28,200
But I also think that we may be able to ideally be able to not just study transfer learning

387
00:27:28,200 --> 00:27:34,000
and also be able to just generalize in zero shot to new domains and new problems.

388
00:27:34,000 --> 00:27:38,240
And so this paper that we're talking about is RoboNet large scale multi robot learning.

389
00:27:38,240 --> 00:27:42,480
If I remember correctly, you did some work in grad school at Google Brain

390
00:27:42,480 --> 00:27:47,360
on a kind of a large scale parallel robot platform. Is this similar?

391
00:27:48,560 --> 00:27:55,040
Right. So the work back at Google in 2017 or like 2016, 2017,

392
00:27:55,040 --> 00:27:58,960
it was actually paralyzing across 10 robots at Google,

393
00:27:58,960 --> 00:28:02,880
but they were all the same robot platform in the same environment.

394
00:28:02,880 --> 00:28:08,400
And so if we want, if we care about, it was really an important step towards this work here,

395
00:28:08,400 --> 00:28:15,680
but it wasn't. And it mainly served as the foundation for the work that we're doing in this paper

396
00:28:15,680 --> 00:28:21,440
in RoboNet. The key question that we're trying to answer now is if we don't have 10 robots,

397
00:28:21,440 --> 00:28:24,880
like which most labs don't, my lab at Stanford does not have 10 robots.

398
00:28:26,320 --> 00:28:30,960
Can we share data across institutions in a way that allows us to get the diversity

399
00:28:30,960 --> 00:28:36,880
of having multiple robots, having many robots. And if we can think about having institutions

400
00:28:36,880 --> 00:28:42,880
in universities share data across robots and across labs and across experimental setups,

401
00:28:42,880 --> 00:28:48,880
then we can get away from having each individual lab to have to collect their own like image net size thing.

402
00:28:49,440 --> 00:28:53,680
What is the nature of the data that we're talking about sharing it? Is this image data?

403
00:28:54,560 --> 00:28:56,480
Is it some other type of data?

404
00:28:56,480 --> 00:29:04,080
Yeah, so in this paper, the data corresponded to trajectories of images and the actions

405
00:29:04,080 --> 00:29:09,440
the robot took. So essentially video plus a sequence of actions that corresponded to that

406
00:29:10,080 --> 00:29:16,800
to that video, to that outcome of images. And is it video kind of off robot video or video

407
00:29:16,800 --> 00:29:20,720
from the perspective of a manipulator or something else?

408
00:29:21,280 --> 00:29:25,600
Yeah, so we actually included multiple camera viewpoints in the data sets such that some of them

409
00:29:25,600 --> 00:29:29,840
were somewhat of a first person's perspective. Some of them were more of a third person perspective,

410
00:29:29,840 --> 00:29:33,120
really corresponding to just different camera placements around the robot.

411
00:29:33,120 --> 00:29:36,720
And the total number of viewpoints we had in the data set was actually over 100.

412
00:29:36,720 --> 00:29:43,600
Okay. And the data set also included data not just from like, so in the Google data set,

413
00:29:43,600 --> 00:29:50,160
it was like 10 robots, one robot platform. And this data set, it's around 10 robots, but it's like

414
00:29:50,160 --> 00:29:55,360
seven robot platforms. So a significant diversity actually, the kinds of robots and the colors of

415
00:29:55,360 --> 00:30:01,360
the robots and the kinematics of the robots across across these different labs and across actually,

416
00:30:01,360 --> 00:30:08,000
across four different institutions. Okay. And the specific learning objective in this set up

417
00:30:08,000 --> 00:30:12,720
is what? Right. So that's where this is where off policy, batch off policy reinforcement learning

418
00:30:12,720 --> 00:30:17,360
methods come in. So if you have, if you have all this data, you need to figure out how to learn from

419
00:30:17,360 --> 00:30:23,600
it. And then yeah, the good question is like, what is the reward function, right? What should you

420
00:30:23,600 --> 00:30:29,600
learn from this sort of data? In this particular setting, we actually use model-based

421
00:30:29,600 --> 00:30:33,760
reinforcement learning methods. So we were learning to predict video based off of the actions that

422
00:30:33,760 --> 00:30:41,840
the robot took. And then once we had this this model of the of the world, then we would actually

423
00:30:41,840 --> 00:30:46,000
use that model of test time to accomplish different tasks. So we would give it a you kind of give

424
00:30:46,000 --> 00:30:50,960
it a new task at test time and then it would use its model to plan to plan to achieve that task,

425
00:30:50,960 --> 00:30:57,520
using its learn model rather than trying to learn a policy during during training. So it actually

426
00:30:57,520 --> 00:31:03,200
is kind of learning behavior on the fly at test time using its learn model. And then the kind of

427
00:31:03,200 --> 00:31:08,880
the tasks that we were testing on correspond to like manipulation tasks like picking up a cup

428
00:31:08,880 --> 00:31:14,880
and positioning it next to other cups or pushing a pencil to be next to some other pencils.

429
00:31:14,880 --> 00:31:21,840
Okay, so in the case of a cup, you've got a model that you've learned off of the data set offline

430
00:31:22,560 --> 00:31:31,440
that has basically kind of built out a view of the world. If I do take some action on the cup,

431
00:31:31,440 --> 00:31:40,880
the future world is probably going to look like this. And then you then give a live robot access

432
00:31:40,880 --> 00:31:49,280
to this and it's also being trained in a RL. Is it is it also an RL agent that's training that

433
00:31:49,280 --> 00:31:56,240
when it's live or is it some other kind of approach? It's so it's it's not quite reinforcement learning.

434
00:31:56,240 --> 00:32:01,600
It's more just in in some ways like model based control in some in some ways as we were talking

435
00:32:01,600 --> 00:32:06,800
about before, but with the learn model. So it's what's called planning. So if you have a model,

436
00:32:06,800 --> 00:32:10,640
if you know what kind of view you have an understanding of what will happen if you take some actions,

437
00:32:10,640 --> 00:32:15,840
and you can use that model to plan a sequence of actions for trying to accomplish a goal. So

438
00:32:15,840 --> 00:32:22,080
some more like an optimization problem across the likely outcomes from the model that you already have.

439
00:32:22,880 --> 00:32:27,360
Yeah, exactly. It's this optimization problem over actions given a goal and your model.

440
00:32:27,360 --> 00:32:32,640
Mm-hmm. Okay. But one of the things that we're doing next is we're we're trying to look into if we

441
00:32:32,640 --> 00:32:39,360
can annotate rewards in the context of in his data set for different tasks. And then that would

442
00:32:39,360 --> 00:32:44,880
allow us to try to do some of this training, do some of this optimization over policies or behaviors

443
00:32:44,880 --> 00:32:50,240
at training time so that we could study algorithms like model free methods. And a reward annotating

444
00:32:50,240 --> 00:32:56,560
a reward in this context means that if the goal is to stack cups that the cup is you know a

445
00:32:56,560 --> 00:33:00,560
picture where the cup is stacked, or are we talking about something more liable than that?

446
00:33:00,560 --> 00:33:05,600
Yeah, it could be something like that. And it may one of the things that's a little bit

447
00:33:05,600 --> 00:33:09,840
challenging in like in robotics context is that if you have a picture of cups that are stacked,

448
00:33:09,840 --> 00:33:13,760
that doesn't mean that you actually have a good reward function for that. So it's actually hard

449
00:33:13,760 --> 00:33:18,480
to compare two images like if you have an image of cup stack, how do you know that another image

450
00:33:18,480 --> 00:33:23,200
also has cup stacked or also isn't achieving that goal in some way. So one of the things we'd

451
00:33:23,200 --> 00:33:29,360
be thinking about is just really simple things like label if an object is being grasped by the robot.

452
00:33:29,360 --> 00:33:34,320
Like one if it's being grasped and zero otherwise. And then we could use that that reward

453
00:33:34,320 --> 00:33:39,200
function for grasping to learn a policy for grasping. So we're thinking about very simple

454
00:33:39,200 --> 00:33:44,000
reward functions like that like are there cup stacks in this image or is there about holding

455
00:33:44,000 --> 00:33:49,520
something things like that. All right, so then the next couple of papers that you called out are

456
00:33:49,520 --> 00:33:56,160
focused on the the broad curriculum learning area. What's been going on there? I remember

457
00:33:56,160 --> 00:34:01,200
correctly we talked briefly about that a couple years ago on that podcast we did. There were two

458
00:34:01,200 --> 00:34:05,840
curriculum learning curriculum based approaches that I found to be quite interesting and exciting

459
00:34:05,840 --> 00:34:11,680
this year. And in both the cases the agent was actually coming up with its own curriculum

460
00:34:11,680 --> 00:34:19,520
for solving tasks. So one of them was this paper from some folks at Uber AI that was actually

461
00:34:19,520 --> 00:34:27,760
using genetic algorithms and evolutionary methods to generate increasingly complex environments

462
00:34:27,760 --> 00:34:33,040
for the agent to learn. So it was a locomotion based task where there was this agent

463
00:34:33,040 --> 00:34:37,280
this legged robot insinuation that needed to move forward. And then it was generating

464
00:34:38,000 --> 00:34:42,320
different different stepping stones and different environments that made it more challenging

465
00:34:42,320 --> 00:34:48,960
for the robot to to traverse the terrain. And the second one was a single environment but

466
00:34:48,960 --> 00:34:55,280
that what it was trying to do it was essentially trying to play sort of a kind of hide and seek game

467
00:34:55,280 --> 00:35:00,880
where there were some agents that were trying to that were trying to hide. And so they were given

468
00:35:00,880 --> 00:35:07,120
like five seconds or some some amount of time to like go hide. And then the secret agents after

469
00:35:07,120 --> 00:35:12,880
that had to go try to find the agents that were hiding. And all this was insinuation. But the one

470
00:35:12,880 --> 00:35:16,720
of the interesting things was that it would kind of learn this sort of curriculum where initially

471
00:35:16,720 --> 00:35:23,200
the hiders were doing very obvious things that were pretty easy to find. But then later they would

472
00:35:23,200 --> 00:35:28,880
actually start making barricades and making it harder for the the secret agents to actually go

473
00:35:28,880 --> 00:35:37,280
and find the hiders. So the curriculum was in that latter case the learning agent was the

474
00:35:37,280 --> 00:35:43,600
hider or the finder. So in this case actually both the hider and the finder were learned.

475
00:35:43,600 --> 00:35:49,440
Okay. They were both being learned simultaneously and the curriculum emerged simply from the fact

476
00:35:49,440 --> 00:35:56,080
that this was a multi agent optimization. And so as the first the kind of hiders would learn some

477
00:35:56,080 --> 00:36:01,040
sophisticated behaviors and then they would be winning and then the the the seekers or the

478
00:36:01,040 --> 00:36:07,200
finders would learn a would start to learn more sophisticated behaviors as well to go find them.

479
00:36:07,200 --> 00:36:13,120
And then you would have this kind of alternating thing where one of them like starts winning and then

480
00:36:13,120 --> 00:36:20,080
the other agents will need to learn more sophisticated behaviors to overcome the types of strategies

481
00:36:20,080 --> 00:36:24,720
that the other set of agents learned. Right. It's probably worth taking a step back and

482
00:36:24,720 --> 00:36:31,120
and kind of providing a high level overview of curriculum learning. As I understand it again,

483
00:36:31,120 --> 00:36:34,960
I think for me, I think you were the first person that explained this to me. The idea is that

484
00:36:35,600 --> 00:36:41,200
in a you know any kind of scenario like that you might have a reinforcement learning agent

485
00:36:41,200 --> 00:36:49,120
in a big part of the problem is that the the state space is huge. And you know, that's not not

486
00:36:49,120 --> 00:36:55,120
unlike how we as humans learn, you know, if we had to learn everything, you know, the possibilities

487
00:36:55,120 --> 00:37:01,200
are huge. So in school, for example, we create a curricula that has us learn, you know, X,

488
00:37:01,200 --> 00:37:07,920
then Y, then Z and in doing so that kind of creates a more narrow path for us to get through this,

489
00:37:07,920 --> 00:37:13,840
you know, the possibilities and curriculum learning is trying to do something similar where first

490
00:37:13,840 --> 00:37:18,240
you teach the agent to do the first thing, then you teach it to do like in the case of,

491
00:37:18,800 --> 00:37:24,080
you know, some of these locomotion examples, you first you teach it to crawl, then you teach it

492
00:37:24,080 --> 00:37:28,480
to stand up, then you teach it to run, then you teach it to jump that kind of thing. Yeah, exactly.

493
00:37:28,960 --> 00:37:34,160
Does it count as a as curriculum learning if it's just something that the agent does without,

494
00:37:34,160 --> 00:37:40,880
you know, some special capability to learn curricula? Does that make sense? Particularly around that

495
00:37:40,880 --> 00:37:46,560
second example with the hiders and the the seekers, you know, what we're doing, we're kind of

496
00:37:46,560 --> 00:37:52,080
observing after the fact, you know, the things that these agents did and calling it curriculum

497
00:37:52,080 --> 00:37:59,520
learning, but was there something specific to the agent that, you know, made it learn in that way

498
00:37:59,520 --> 00:38:06,000
or is that kind of orthogonal to the point anyway? It doesn't not matter. Yeah, so to me,

499
00:38:06,000 --> 00:38:12,080
I think that a curriculum is characterized by an increase in complexity and if for the kind of

500
00:38:12,080 --> 00:38:17,680
that the beginning, you learn very simple things and at the end, you're doing much more sophisticated

501
00:38:17,680 --> 00:38:23,600
and complex behaviors than it seems like there's this there's this progression of simple to complex.

502
00:38:24,240 --> 00:38:29,440
In the case of the first paper, this was fairly explicit. In the case of the second paper,

503
00:38:29,440 --> 00:38:34,080
I think that this was more of an emergent property of the algorithm or an emergent property of

504
00:38:34,080 --> 00:38:39,440
this multi agent optimization and it wasn't necessarily something that was built in to the algorithm

505
00:38:39,440 --> 00:38:45,120
from the start. And I think that just this progression of from very simple behaviors to very complex

506
00:38:45,120 --> 00:38:50,960
behaviors is the thing that's very exciting to me because it means that if we can if we can move

507
00:38:50,960 --> 00:38:55,520
from very simple behaviors to more complex behaviors, then maybe we can also move from very complex

508
00:38:55,520 --> 00:39:01,920
behaviors to even more complex behaviors. And it seems like a path towards agents that are

509
00:39:01,920 --> 00:39:07,200
increasingly sophisticated and increasingly intelligent. And so from that perspective,

510
00:39:07,200 --> 00:39:12,960
it doesn't matter to you whether that's because of a training regime that has curricula built in

511
00:39:12,960 --> 00:39:19,520
that we create or whether it's just something that this complex that emerges in a complex system.

512
00:39:19,520 --> 00:39:25,120
Yeah, and I actually think that potentially it's maybe even more exciting if it emerges or if the

513
00:39:25,120 --> 00:39:30,800
agent creates it itself because that means that it won't rely on us for moving to the next level.

514
00:39:31,360 --> 00:39:34,320
The next couple of papers were focused on exploration problems?

515
00:39:34,960 --> 00:39:41,760
Yeah, so these are kind of getting back to Atari Land. And in terms of the applications that

516
00:39:41,760 --> 00:39:49,600
they were studying. And in general, exploration is kind of a huge challenge and reinforcement

517
00:39:49,600 --> 00:39:56,400
learning. It's the problem of discovering the right thing to do in settings where you're not

518
00:39:56,400 --> 00:40:01,120
given a lot of supervision oftentimes about what the right thing to do is you basically need to explore

519
00:40:01,120 --> 00:40:05,520
your environment and find the parts of your environment, the parts of your state space that give

520
00:40:05,520 --> 00:40:14,800
you high reward. And in the past, one of the kind of classic problems or benchmarks in exploration

521
00:40:14,800 --> 00:40:19,360
and reinforcement learning has been a Atari game called Monizuma's Revenge. I don't think it's

522
00:40:19,360 --> 00:40:24,800
the only problem that we care about in the context of exploration, but it's one that has been notably

523
00:40:24,800 --> 00:40:32,160
challenging for our current reinforcement learning systems. And these two papers were both

524
00:40:32,160 --> 00:40:40,960
proposed means for actually solving Monizuma's Revenge to a very large degree, scoring tens of

525
00:40:40,960 --> 00:40:46,640
thousands of points on this game when I think previous approaches were often only getting zero

526
00:40:46,640 --> 00:40:53,600
points or just a few points on the game. So I think that these approaches were making a

527
00:40:53,600 --> 00:40:57,680
lot of progress there. And the key insight of the first one, which is actually a little bit

528
00:40:57,680 --> 00:41:05,760
controversial, was to find the parts of the game where you're getting some, making some progress.

529
00:41:05,760 --> 00:41:11,920
And then actually, if you then die at those parts of the game, actually reset to that state and

530
00:41:11,920 --> 00:41:16,880
start kind of exploring in that region of the state space again. And so really trying to

531
00:41:17,760 --> 00:41:22,800
remember, really trying to remember the visits states that are promising, returning to those

532
00:41:22,800 --> 00:41:28,480
promising states and then explore from it. And so one of the reasons why it was controversial is

533
00:41:28,480 --> 00:41:32,640
that they were using this reset ability to kind of go to a particular state that you had been to before.

534
00:41:33,760 --> 00:41:37,440
And in many real world contexts, that's of course not something that you can do, but if you're

535
00:41:37,440 --> 00:41:42,080
an Atari game, that is something that you could conceivably do. And then the second paper

536
00:41:42,800 --> 00:41:47,200
took a very similar approach, but they lifted this assumption by using an imitation learning approach

537
00:41:47,200 --> 00:41:53,840
to figure out, to kind of remember how to get back to that state. So both of these papers

538
00:41:54,560 --> 00:42:04,560
are a share in common that they take advantage of ways to spend more time exploring difficult

539
00:42:04,560 --> 00:42:13,440
areas of much as soon as revenge, one using some kind of God mode reset capability. And the other

540
00:42:13,440 --> 00:42:19,120
is just remembering how they got to given states and getting there before they start doing

541
00:42:19,120 --> 00:42:24,400
exploration. Exactly, yeah. Okay, I can see how the first one is controversial because in the

542
00:42:25,360 --> 00:42:30,160
in the paper, they're kind of report their scores. But if you keep doing these unnatural things

543
00:42:30,160 --> 00:42:35,760
like resetting and going to hard places and kind of accumulating more score, it doesn't seem

544
00:42:35,760 --> 00:42:41,440
to be comparable to the other scores that are reported for this game. Yeah, I think it's just

545
00:42:41,440 --> 00:42:46,400
worth briefly mentioning that they're, I focused on a number of, I think, conceptual advances

546
00:42:46,400 --> 00:42:52,160
in various approaches. And at the same time, there have been these fairly large efforts to try

547
00:42:52,160 --> 00:42:58,800
to use reinforcement learning and other approaches to solve harder types of games. So StarCraft,

548
00:42:59,680 --> 00:43:04,240
there's a large team of deep mind that was trying to solve a version of StarCraft

549
00:43:04,240 --> 00:43:10,880
through reinforcement learning and also, and also imitation learning. And there was also a fairly

550
00:43:10,880 --> 00:43:15,760
large team at OpenAI that was trying to study reinforcement learning algorithms on the game Dota 2.

551
00:43:15,760 --> 00:43:19,920
And both of them made quite notable progress in those settings. And I feel like I can't,

552
00:43:20,640 --> 00:43:25,840
I can't summarize 2019 without at least making a very brief mention of those two results.

553
00:43:25,840 --> 00:43:33,360
And OpenAI on the Dota front has been working on this for a while. This goes back to 2018, at least,

554
00:43:33,360 --> 00:43:37,760
first potentially earlier than that. Yeah, and I think that StarCraft actually also goes back

555
00:43:37,760 --> 00:43:45,600
into 2018 as well. I think that they've been long efforts. So one of the other things that we

556
00:43:45,600 --> 00:43:55,040
like to cover in these AI rewind segments is more kind of practical, tangible advances in terms

557
00:43:55,040 --> 00:44:00,640
of new tools and libraries and open source projects, things like that. And you had a number that

558
00:44:00,640 --> 00:44:05,760
that come to mind. Where do you like to start? Yeah, so I'll start with some of the libraries that

559
00:44:05,760 --> 00:44:11,840
have been made available. So in general, reinforcement learning, building reinforcement learning

560
00:44:11,840 --> 00:44:16,880
algorithms is challenging. And we're actually still in the stage where for for deep reinforcement

561
00:44:16,880 --> 00:44:22,640
learning algorithms, many, many algorithms can be, I guess the first is just so many design

562
00:44:22,640 --> 00:44:27,600
decisions when designing a reinforcement learning agent such as when do you collect data, how do

563
00:44:27,600 --> 00:44:33,200
you collect data and like at a per time step level versus like an episodic level? Do you normalize

564
00:44:33,200 --> 00:44:40,160
your state's in actions? Do you how do you kind of estimate your return? There's just all of these

565
00:44:40,160 --> 00:44:45,280
really tiny design decisions that that can actually make a pretty large difference on the result

566
00:44:45,280 --> 00:44:48,320
of the algorithm. And so having implementations of reinforcement learning algorithms that are

567
00:44:48,320 --> 00:44:55,360
trustworthy and and are actually kind of ready to use out of the box for for different applications.

568
00:44:55,360 --> 00:45:01,280
I think it's quite important for the advancement of reinforcement learning. For kind of specific

569
00:45:01,280 --> 00:45:06,080
algorithms, there have been a number of open source implementations released by the authors of

570
00:45:06,080 --> 00:45:10,240
those papers and may wait in may times those those implementations are in some ways the most

571
00:45:10,240 --> 00:45:15,440
trustworthy because they're the ones that should reproduce the results in the paper. But there's also

572
00:45:15,440 --> 00:45:21,280
been a library called TF agents or TensorFlow agents that provides actually a platform of many

573
00:45:21,280 --> 00:45:27,360
different algorithms, many of it reinforcement learning algorithms. And it's trying to basically

574
00:45:27,360 --> 00:45:34,320
provide a kind of a unified code interface and framework for running these different types of

575
00:45:34,320 --> 00:45:40,640
algorithms and making it with the goal of making it easier for people to use these algorithms on

576
00:45:40,640 --> 00:45:48,160
their problems. And so how much of the the problem of getting an RL agent up and running does

577
00:45:48,160 --> 00:45:56,080
does TF agent solve? Does it get you all the way there? It's always struck me with RL there are

578
00:45:56,080 --> 00:46:02,480
just so many moving pieces. You've got your simulation environment or your game environment. You've

579
00:46:02,480 --> 00:46:10,000
got your agents. You've got to figure out your optimization, your loss function. How much of that

580
00:46:10,000 --> 00:46:15,840
does TF agents take care of for you? So if you want to use an environment like OpenAI

581
00:46:15,840 --> 00:46:20,800
Jim, for example, then I think that you should be able to run it like it basically will solve

582
00:46:20,800 --> 00:46:28,320
everything for you. If your environment interface is different from Jim, then of course it you'll

583
00:46:28,320 --> 00:46:34,480
need to do some plumbing essentially to hook it up with that. And it also doesn't solve the

584
00:46:34,480 --> 00:46:38,960
fact that if you have a new environment, these algorithms may not work out of the box on that

585
00:46:38,960 --> 00:46:44,240
environment because of you may need to tune it or maybe just that your problem is harder than

586
00:46:44,240 --> 00:46:49,360
the current kinds of reinforcement learning problems that we can solve. But if you want to reproduce

587
00:46:49,360 --> 00:46:55,760
one of the standard algorithms using a known environment like Jim, you should be able to do it

588
00:46:55,760 --> 00:47:01,280
fairly handily. I think you should be able to do it fairly handily with with this code base.

589
00:47:02,000 --> 00:47:06,720
It's also worth mentioning I think that this is from very late 2018, but there was also a

590
00:47:08,880 --> 00:47:13,200
a framework called dopamine that was trying to do something somewhat similar, but they were

591
00:47:13,200 --> 00:47:18,480
had a more narrow and narrow focus, which was to study Q learning algorithms, various types of

592
00:47:18,480 --> 00:47:23,600
Q learning algorithms for Atari games. So things like deep Q networks, distributional reinforcement

593
00:47:23,600 --> 00:47:27,280
learning, prioritize experience, replay all the bells and whistles that you might want in your

594
00:47:27,280 --> 00:47:33,120
DQ Ed agent, specifically looking at Atari games. That code base I think provided a very reliable

595
00:47:33,120 --> 00:47:37,440
implementation of that kind of suite of algorithms, which are basically with a more narrow

596
00:47:37,440 --> 00:47:43,200
focus than TF agents. And you also mentioned PyTorch higher. What's that all about?

597
00:47:44,000 --> 00:47:50,240
Yeah, so this isn't necessarily a reinforcement learning thing, but it's a library that's been

598
00:47:50,240 --> 00:47:54,640
very useful for metal learning research. So I don't know how much of the details I want to get

599
00:47:54,640 --> 00:47:59,440
into with metal learning, but kind of the goal is to learn, as I kind of mentioned at the beginning,

600
00:47:59,440 --> 00:48:03,280
the goal is to learn priors from previous experience in a way that allows you to learn very quickly,

601
00:48:03,280 --> 00:48:08,640
like learn with only a few data points for new tasks. And one very popular approach for metal

602
00:48:08,640 --> 00:48:14,480
learning is to perform a what's called a bi-level optimization, where you're actually embedding

603
00:48:14,480 --> 00:48:20,400
a optimization process inside another optimization process, and higher provides a way to

604
00:48:21,120 --> 00:48:26,080
perform these higher order optimizations, like for example, if you're embedding one optimization

605
00:48:26,080 --> 00:48:32,800
inside another, you have a second order optimization process. And this this library allows it,

606
00:48:32,800 --> 00:48:38,080
makes it very easy for people to do that. And I haven't used it personally myself, but my PhD

607
00:48:38,080 --> 00:48:41,840
students have been using it and have said have said wonderful things about it.

608
00:48:42,800 --> 00:48:52,480
Is the idea with bi-level optimization and metal learning that you're optimizing whatever

609
00:48:52,480 --> 00:48:57,360
problem you're trying to solve at one level and at another level you're optimizing the way you

610
00:48:57,360 --> 00:49:00,640
learn how to solve that problem? Exactly. Yeah, that's a great way to put it.

611
00:49:00,640 --> 00:49:09,520
And so higher isn't necessarily a metal learning library, but it's a general bi-level optimization

612
00:49:09,520 --> 00:49:15,920
library, or is it more specifically geared towards metal learning? It is a more general thing,

613
00:49:15,920 --> 00:49:20,320
but they also provide a number of optimizers that make it good specifically for metal learning

614
00:49:20,320 --> 00:49:27,440
use cases. Okay. And maybe to further kind of skirt this line of going too deep in on metal

615
00:49:27,440 --> 00:49:35,280
learning, is there a classic problem setup or hello world of metal learning that would help

616
00:49:35,280 --> 00:49:40,400
make it more concrete for folks that aren't familiar with it? It's a good question. Yeah,

617
00:49:40,400 --> 00:49:45,520
so I think that maybe one one very simple like one that kind of very standard problem in metal

618
00:49:45,520 --> 00:49:52,000
learning is can you learn to recognize characters of a new alphabet with a few examples? So can you

619
00:49:52,000 --> 00:49:57,920
give an example of five different handwritten digits? Can you learn a classifier that can

620
00:49:57,920 --> 00:50:02,400
distinguish those five handwritten digits? And the way that these metal learning algorithms work

621
00:50:02,400 --> 00:50:07,840
is that they take a handwritten digits from a number of different alphabets and languages

622
00:50:07,840 --> 00:50:12,880
and learn at the lower level they're trying to learn how to recognize distinguished digits from

623
00:50:12,880 --> 00:50:17,200
a particular alphabet and at the higher level they're trying to change the way that that lower

624
00:50:17,200 --> 00:50:23,440
level learns across alphabets in a way that makes it generalize faster and in a way that makes

625
00:50:23,440 --> 00:50:28,560
allow it to learn from only five examples. So for example, if you trained a deep neural network

626
00:50:28,560 --> 00:50:32,560
on five examples it would probably overfit massively or not be able to learn very much.

627
00:50:33,520 --> 00:50:39,440
And these algorithms try to actually train for the ability to generalize from a few examples by

628
00:50:39,440 --> 00:50:45,520
changing the way that neural networks are learning. So we've talked a little bit about open AI gem

629
00:50:45,520 --> 00:50:50,880
and some of these other simulation environments. There were also some new ones that were introduced

630
00:50:50,880 --> 00:50:55,920
this year. Absolutely. And I think that kind of maybe jump me ahead a little bit too much is

631
00:50:55,920 --> 00:51:00,880
one of my predictions for next year is that we're going to really need better environments for studying

632
00:51:00,880 --> 00:51:05,280
the kinds of problems that we care about. So I think that if we care about things like generalization

633
00:51:05,280 --> 00:51:11,360
like the ability to learn new tasks quickly like the ability to solve longer horizon tasks maybe with

634
00:51:11,360 --> 00:51:17,440
the use of demonstration data, then we need environments that allow us to actually evaluate those

635
00:51:17,440 --> 00:51:21,680
abilities and our algorithms. And there have been a number of environments that were introduced

636
00:51:21,680 --> 00:51:27,520
this year that tried to focus on different aspects of this. So for example, there was the AI habitat

637
00:51:28,080 --> 00:51:32,800
environment that was developed specifically for visual navigation. And I was specifically trying

638
00:51:32,800 --> 00:51:38,720
to target the setting where you want to learn how to navigate environments with photo realistic

639
00:51:38,720 --> 00:51:44,560
rendering where basically you're dealing with images with where your observations are highly

640
00:51:44,560 --> 00:51:49,600
realistic images such that you aren't learning from these kind of really simple game graphics.

641
00:51:49,600 --> 00:51:54,240
So it's such that you're actually studying both the vision aspect of the problem as well as

642
00:51:54,240 --> 00:51:59,600
the control aspect. The second environment is the meta world environment which is an environment

643
00:51:59,600 --> 00:52:04,080
that one of my PhD students and some of my collaborators have been working on where we've been

644
00:52:04,080 --> 00:52:10,000
trying to study or allow ourselves to study generalization across tasks. So we create a benchmark of

645
00:52:10,000 --> 00:52:16,400
50 manipulation tasks and simulation. And with the hope of seeing if it can allow us to study one

646
00:52:16,400 --> 00:52:20,400
whether or not we can have algorithms learn across all of the tasks and two whether or not we have

647
00:52:20,400 --> 00:52:26,960
algorithms that can use say 45 of the tasks in a way that allows you to quickly learn new tasks

648
00:52:26,960 --> 00:52:32,960
like the next five tasks. And that was kind of targeting meta-reinforce and learning algorithms

649
00:52:32,960 --> 00:52:38,720
that can learn how to learn from small amounts of data or small amounts of data for new tasks.

650
00:52:38,720 --> 00:52:45,040
The minor realm competition that I also mentioned has been looking at Minecraft environments which

651
00:52:45,040 --> 00:52:49,280
I think is a really interesting taskbed for reinforcement learning methods because it's more open

652
00:52:49,280 --> 00:52:53,680
ended. And they specifically one of the things that they've been focusing on there was the ability

653
00:52:53,680 --> 00:52:57,840
to learn from example behavior and from demonstrations. And so they collected a very large data set of

654
00:52:57,840 --> 00:53:02,960
humans playing in these Minecraft environments with the hope of building reinforcement learning agents

655
00:53:02,960 --> 00:53:07,760
that can use this data as well as their own data collected that they collected themselves in the

656
00:53:07,760 --> 00:53:16,000
environment in order to learn in order to learn complex and long horizon skills. The recism that I

657
00:53:16,000 --> 00:53:24,400
mentioned is a kind of a simulation platform for studying whether or not you can learn a recommender

658
00:53:24,400 --> 00:53:28,320
system. And I think that one of the things that I really like about this is that it actually allows you

659
00:53:28,320 --> 00:53:34,720
to study a more real world problem that's quite different from things like games and things like

660
00:53:34,720 --> 00:53:38,320
robotics. Yeah, I was not expecting recommender systems to come up here.

661
00:53:39,360 --> 00:53:44,800
Yeah, and I think that people like in many ways the reinforcement learning community has been so

662
00:53:44,800 --> 00:53:50,960
focused on control robotics and video games. And I think that maybe we're overfitting to some of

663
00:53:50,960 --> 00:53:54,720
the challenges in those domains. And if we care about building reinforcement learning algorithms

664
00:53:54,720 --> 00:53:59,520
that are useful in a variety of settings, then we should be testing them on things like recommender

665
00:53:59,520 --> 00:54:04,960
systems. And I hope to see more of that too. Like you could imagine education as I mentioned before

666
00:54:04,960 --> 00:54:09,040
medical decision making. I think that there are a lot of potential applications of a reinforcement

667
00:54:09,040 --> 00:54:13,920
learning where a sequential decision making where you need a really reason about the effect of your

668
00:54:13,920 --> 00:54:21,680
actions on future states. Yeah. And then the last thing is this Google research football

669
00:54:21,680 --> 00:54:29,120
environment. And this one I believe is specifically focusing on the ability to study multi-agent

670
00:54:29,920 --> 00:54:35,040
reinforcement learning in the context. And by football for those of us that are Americans,

671
00:54:35,040 --> 00:54:42,720
they're referring to soccer. Yeah, and actually I did an interview with one of the principles in

672
00:54:42,720 --> 00:54:49,840
this work not too long ago. Actually, this is relatively recent, but it was episode 293 of the

673
00:54:49,840 --> 00:54:55,840
show and it was with Olivier Bachin. Yeah, so those were all the environments that I've seen come

674
00:54:55,840 --> 00:55:01,040
up that look quite promising and really filling a gap that we I think don't have in our current

675
00:55:01,040 --> 00:55:07,600
environments. Well, you started kind of foreshadowing into your predictions there. One of them

676
00:55:07,600 --> 00:55:14,400
being that we need to see even more of these types of environments. What are some of the other

677
00:55:14,400 --> 00:55:19,840
things that you expect to see in the field in the next year? And I guess we're going into 2020,

678
00:55:19,840 --> 00:55:25,600
so we could we could even talk about the next decade if you dare. Yeah, so I think that in addition

679
00:55:25,600 --> 00:55:30,800
to an increase in environments that will hopefully allow us to meaningfully study things like

680
00:55:30,800 --> 00:55:35,920
generalization and batch off policy RL instead of repurposing old environments for those things.

681
00:55:35,920 --> 00:55:40,800
I think we'll also start to see an increase in papers that study settings like batch off policy

682
00:55:40,800 --> 00:55:44,800
reinforcement learning. So I think that this year we saw a pretty big increase in in people

683
00:55:44,800 --> 00:55:48,720
that are studying that and I think that that's probably going to continue because that's really a

684
00:55:48,720 --> 00:55:54,480
problem that matters in the context of real world, the real world deployment of reinforcement learning

685
00:55:54,480 --> 00:56:02,720
systems. So greater focus on batch off policy RL? Yeah, and then I also think that we didn't quite

686
00:56:02,720 --> 00:56:06,720
this was the kind of one of the papers that I was going to mention on meta reinforcement learning

687
00:56:06,720 --> 00:56:12,640
and multitask reinforcement learning, which we didn't quite get to cover in 2019. And I think that

688
00:56:12,640 --> 00:56:16,720
I think that we're also really going to see an increase in papers that study this. So I think

689
00:56:16,720 --> 00:56:22,000
that in general the the community has been fairly focused on like trying to solve individual tasks.

690
00:56:22,000 --> 00:56:25,680
And that's I think in some ways that actually been by nature of the environments that are focusing

691
00:56:25,680 --> 00:56:30,720
on let's learn one Atari game, let's learn to run with this one agent in this one environment.

692
00:56:30,720 --> 00:56:34,720
And I think that a lot of people really do care about generalization and we did actually see an

693
00:56:34,720 --> 00:56:40,320
increase in paper this paper is this year that we're focusing on generalization. To some degree

694
00:56:40,320 --> 00:56:44,080
and I think that we'll see that actually kind of the breadth of generalization that we try to study

695
00:56:44,080 --> 00:56:49,360
increase and in particular trying to study generalization across tasks across goals across objectives

696
00:56:50,400 --> 00:56:54,720
such that we can move towards general purpose reinforcement learning agents rather than these very

697
00:56:54,720 --> 00:57:02,960
narrow and specialized agents. And do you have a sense for what that will likely look like? Is it

698
00:57:03,920 --> 00:57:08,560
analogous to what you were doing with the collaboration across institutions and

699
00:57:09,680 --> 00:57:14,960
in the off-policy work where you were looking at multiple different robotic platforms and trying

700
00:57:14,960 --> 00:57:21,920
to train on different environment simultaneously? Or is it some new I don't know some new training

701
00:57:21,920 --> 00:57:27,280
technique or something that results in greater generalization? Yeah so I think that I would love

702
00:57:27,280 --> 00:57:30,800
for everyone to start studying robotics but I think that in practice people won't be

703
00:57:31,840 --> 00:57:38,240
a little bit scared of robots and not scared in terms of them being dangerous but just scared of

704
00:57:38,240 --> 00:57:42,480
the effort that goes into actually getting things working on a real robot in the real world.

705
00:57:43,280 --> 00:57:47,120
And so I think that in practice what that mean will mean is that people will be studying

706
00:57:47,120 --> 00:57:53,840
simulation agents like simulate control agents, simulate robots, maybe simulate different

707
00:57:53,840 --> 00:58:01,120
like video game levels for example and studying how agents can learn across, can generalize across

708
00:58:01,120 --> 00:58:06,960
these video game levels that can generalize across reward functions of a robot for example.

709
00:58:06,960 --> 00:58:13,760
And yeah I think that basically that the algorithms are mostly ready for this. I think that

710
00:58:13,760 --> 00:58:18,160
there's still advances to be had on interest in terms of the ability for kind of just basic

711
00:58:18,160 --> 00:58:23,120
reinforcement algorithms to be stable and work well with images etc. But I think that many of

712
00:58:23,120 --> 00:58:28,880
the algorithms are ready to take the jump towards these more complex settings where you need to be

713
00:58:28,880 --> 00:58:33,120
doing multiple things and not just one thing. And maybe actually I mentioned Muldi's task RL

714
00:58:33,120 --> 00:58:36,880
that the ability to do these different tasks but it could also be doing tasks and sequence

715
00:58:37,680 --> 00:58:42,160
or kind of hierarchical reinforcement learning where you're performing like picking up an object

716
00:58:42,160 --> 00:58:46,800
and then placing it into a bin and then taking that bin and putting it somewhere else.

717
00:58:47,680 --> 00:58:50,960
And I think that in general people have been studying some of these problems for a long time

718
00:58:50,960 --> 00:58:55,600
but I think that the will actually start to see meaningful advances in these problems

719
00:58:55,600 --> 00:59:01,360
in the deep RL setting and in more complex and challenging environments.

720
00:59:01,360 --> 00:59:02,720
Any other predictions?

721
00:59:02,720 --> 00:59:08,320
I think that the batch of policy multi task RL and metarole in environments are my main ones.

722
00:59:08,320 --> 00:59:12,640
I would guess that I guess we also talked to Affair about model based and model free. I think

723
00:59:12,640 --> 00:59:18,320
that people will continue to show interest in model based methods and will also continue to see

724
00:59:18,320 --> 00:59:23,760
a number of hybrid methods that combine elements of model based and model free algorithms as well.

725
00:59:26,240 --> 00:59:31,200
Yeah, so I think that kind of the things that are going up will continue to actually maybe

726
00:59:31,200 --> 00:59:36,960
kind of increasingly more popular things like batch off policy model based RL and metarole

727
00:59:36,960 --> 00:59:38,800
reinforcement learning and multi task free reinforcement learning.

728
00:59:39,520 --> 00:59:44,000
And one of the things that I guess I'm really excited about is that I think people will actually

729
00:59:44,000 --> 00:59:49,360
really start to meet and study generalization and I think that this is something that's been overlooked

730
00:59:49,360 --> 00:59:51,440
for a very long time in reinforcement learning.

731
00:59:52,480 --> 00:59:57,600
One of the questions that I get all the time about reinforcement learning, particularly deep

732
00:59:57,600 --> 01:00:03,200
reinforcement learning is who's using it and for what? Have you come across any notable

733
01:00:03,200 --> 01:00:07,920
kind of real-world use cases for deep RL this year?

734
01:00:07,920 --> 01:00:12,160
Yeah, that's a really good question. I think that in general people aren't using it

735
01:00:12,720 --> 01:00:19,360
because these algorithms are they work in some context but they are still a long ways away from

736
01:00:19,360 --> 01:00:22,720
being just like a plug and play thing like like deep learning for example with neural networks

737
01:00:22,720 --> 01:00:27,680
we like with batch norm and and all the and like resnets and stuff like that. We've really figured

738
01:00:27,680 --> 01:00:33,200
out a way to kind of really be able to up deploy these deep neural networks in a wide range of

739
01:00:33,200 --> 01:00:38,240
settings if you have enough data and can actually formulate your problem as a supervised learning

740
01:00:38,240 --> 01:00:41,680
problem. In reinforcement learning I don't think that we're quiet at that stage yet

741
01:00:42,560 --> 01:00:47,680
and there have been some some applications so I know that there are some folks that use it

742
01:00:47,680 --> 01:00:55,200
actually for recommender systems. So like Craig Bhutiliyare for example at Google is someone who's

743
01:00:55,200 --> 01:01:01,360
notably who's been studying reinforcement learning in these sorts of settings. There was also one

744
01:01:01,360 --> 01:01:06,480
example I think a couple years ago of using reinforcement learning for like data center power

745
01:01:06,480 --> 01:01:13,520
management but I haven't really seen that see not really expand yeah at least from from what I've

746
01:01:13,520 --> 01:01:17,280
seen. I think it's also hard to say because it's not necessarily people aren't always public about

747
01:01:17,280 --> 01:01:22,160
the sorts of things the sorts of algorithms are using in industrial applications but my guess is

748
01:01:22,160 --> 01:01:28,000
that they really aren't being used much in real world applications. One of the things that makes

749
01:01:28,720 --> 01:01:34,560
this particular question challenging is you'll see a lot of people talking about

750
01:01:34,560 --> 01:01:39,280
using reinforcement learning but when then you go under the covers it's not deep reinforcement

751
01:01:39,280 --> 01:01:46,080
learning it's like single step kind of traditional reinforcement learning which is different.

752
01:01:46,080 --> 01:01:52,080
Yeah I guess one other thing worth mentioning is that there's a startup company by

753
01:01:53,520 --> 01:02:00,960
led by Peter Biel and others that is looking at robotic automation and they're looking at

754
01:02:00,960 --> 01:02:06,160
deep imitation learning and deep reinforcement learning for doing this. That's covariance.

755
01:02:06,160 --> 01:02:11,680
covariant yeah but I also don't know they are very public about the the kinds of techniques that

756
01:02:11,680 --> 01:02:20,560
they're using or the applications. I recently saw the Mark Hammond who has been on this show he

757
01:02:20,560 --> 01:02:25,440
founded a company called Bonsai that was acquired by Microsoft they're still doing interesting

758
01:02:25,440 --> 01:02:32,560
stuff with RL like I think it is happening but in terms of to your point kind of public

759
01:02:33,600 --> 01:02:39,040
detailed case studies about what folks are doing they are difficult to come by.

760
01:02:39,040 --> 01:02:44,720
Yeah and like Osaro is another startup company that is looking at deep reinforcement learning but

761
01:02:44,720 --> 01:02:50,160
they I don't know they're like if they're actually using it for their for the use cases or not

762
01:02:50,960 --> 01:02:55,600
and there's also I guess another kind of maybe more real-world examples that they're but the not

763
01:02:55,600 --> 01:03:01,680
deep is that there was some work I think a couple years ago by Joel Pinou looking at

764
01:03:01,680 --> 01:03:10,160
reinforcement learning for for brain simulation for seizures to try to stimulate the brain

765
01:03:10,160 --> 01:03:16,560
in a pattern that used less stimulation than kind of a standard thing while also still preventing

766
01:03:16,560 --> 01:03:21,680
seizures but that was using that was by no means using a deep reinforcement learning it was just

767
01:03:21,680 --> 01:03:26,640
using reinforcement learning with smaller models because they didn't have enough data in order to

768
01:03:26,640 --> 01:03:33,120
deploy these techniques with deep networks. Cool anything else we should keep our eyes peeled

769
01:03:33,120 --> 01:03:40,800
for in 2020? I think that that's it I'm excited to see what's to come and then I think that there

770
01:03:40,800 --> 01:03:46,000
is maybe one other thing worth mentioning is that there's been increasingly other labs other

771
01:03:46,000 --> 01:03:49,280
research labs that have really been entering the reinforcement learning space and it's been

772
01:03:49,280 --> 01:03:53,840
exciting to see that people are getting more interested in in the problem setting labs that

773
01:03:53,840 --> 01:03:58,640
were traditionally doing things like supervising and unsupervised learning and I guess I mentioned this

774
01:03:58,640 --> 01:04:03,600
also at the beginning of the podcast. Any particular ones that folks should be

775
01:04:04,640 --> 01:04:10,800
keeping an eye on I mean there are the you know deep mind open AI or you know traditional

776
01:04:10,800 --> 01:04:17,440
folks that have been publishing a lot in this space certainly yourself and Sergei Levine and Peter

777
01:04:17,440 --> 01:04:23,920
Biel and many others you know academic labs what who are some of the new ones that might be worth

778
01:04:23,920 --> 01:04:28,400
taking a look at? Yeah so in addition to the folks that you mentioned some of the kind of existing

779
01:04:28,400 --> 01:04:37,040
ones are like the group brain team in addition to to divine efforts and folk labs at McGill and

780
01:04:37,040 --> 01:04:44,640
Montreal like Joanna Precap and Joel Panou folks at Oxford like Ash Shaman Whiteson and then also

781
01:04:44,640 --> 01:04:50,560
folks at Michigan like Satinder Singh and Hungluck Lee and then also at Stanford is also Emma

782
01:04:50,560 --> 01:04:56,640
Brunskill and I think that these folks study different aspects of the problem for example Emma

783
01:04:56,640 --> 01:05:02,720
has been looking at applications like education and really kind of human impact settings and

784
01:05:02,720 --> 01:05:07,600
really actually focusing on like off policy methods for example while Satinder's group and

785
01:05:07,600 --> 01:05:13,040
Hungluck's group are often looking at more video game applications and Joel has done kind of at

786
01:05:13,040 --> 01:05:16,560
McGill has done a variety of different applications so I think that it's been interesting to see

787
01:05:17,120 --> 01:05:21,360
how different methods are using different approaches and then in terms of newer labs I think that

788
01:05:21,360 --> 01:05:28,240
it's hard to list them but I think that some some maybe people who are probably rather recognizable

789
01:05:28,240 --> 01:05:31,760
in the deep learning community and then are starting to do more reinforcement learning or

790
01:05:31,760 --> 01:05:37,280
folks like Gashua Bengeo and Jan Lecune, Jan it's more on the model-based reinforcement learning

791
01:05:37,280 --> 01:05:42,160
side of things and Gashua I think has been been looking at representation learning in the context

792
01:05:42,160 --> 01:05:48,720
of embodied agents and then also folks like Jimmy Baw for example has had some interesting work

793
01:05:48,720 --> 01:05:54,880
in in model-based reinforcement learning recently and then also Tany Maw at Stanford has

794
01:05:55,680 --> 01:06:00,720
Jimmy is at University of Toronto and then Tany Maw at Stanford has done a lot of really great work

795
01:06:00,720 --> 01:06:05,520
on theoretical deep learning and has been starting to move in towards towards the kind of look at

796
01:06:05,520 --> 01:06:11,520
the theoretical aspects of deep reinforcement learning. Well Chelsea that was a ton of stuff to

797
01:06:11,520 --> 01:06:17,760
cover thanks so much for you know doing this show for helping us get caught up on RL and giving

798
01:06:17,760 --> 01:06:22,320
us a peek into what's coming next. Yeah absolutely my pleasure.

799
01:06:26,240 --> 01:06:32,000
All right everyone that's our show for today for more information on today's guest or for links

800
01:06:32,000 --> 01:06:40,240
to any of the materials mentioned check out twimmelai.com slash rewind 19. Be sure to leave us a five-star

801
01:06:40,240 --> 01:06:45,200
rating and a glowing review after you hit that subscribe button on your favorite podcast catcher.

802
01:06:45,200 --> 01:07:12,160
Thanks so much for listening and catch you next time.

