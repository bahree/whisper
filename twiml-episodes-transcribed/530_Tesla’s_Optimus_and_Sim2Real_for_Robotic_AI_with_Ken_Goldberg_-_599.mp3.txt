All right, everyone. Welcome to another episode of the Twemal AI podcast. I am your host,
Sam Charrington. And today I'm joined by Ken Goldberg, the professor of industrial engineering
and operations research, and the William S. Floyd Jr. Distinguished Chair in Engineering at UC
Berkeley. Ken is also the chief scientist at MB Robotics. Before we get going, be sure to take
a moment to hit that subscribe button wherever you're listening to today's show. Ken, it has been a
bit. Welcome back to the podcast. Thank you, Sam. It's a pleasure. I've been enjoying listening to
your shows over the last couple of years. Awesome. Yeah, it is hard to believe that it has been two
and a half years since the last time we spoke. Of course, a ton has been happening in Robotics.
And I guess we're going to use this next next little bit for you to catch us up on everything.
Well, it's been actually an amazing time. I mean, with the pandemic, obviously, we were in the
middle of it when we last talked. And so much has happened. I would say that it's actually been a
very productive time for Robotics. That people have made enormous amounts of progress. There's a
lot of publications, a lot of research that's been going on. And also the world has changed in
interesting ways that I think is as favorable to Robotics as a field. Absolutely. Absolutely.
Now, I'll refer folks back to our conversation that was episode number 359 back in March of 2020,
the third wave of robotic learning for a great conversation in your full background. But for those
who, you know, haven't caught that, why don't you share a little bit about how you came into
Robotics? Well, I was, I've been interested in robots since I was a kid. And back in the old days
of Star Trek and Waston Space. And I've just continued that for, you know, 50 years. And what I
have to say is I rediscovered Robotics as an undergrad. And found that it was just absolutely a
fascinating set of questions. I was lucky to have a great advisor, Regina Bicci. It was at
UPEN. At the time, she took me under a wing, really mentored me. And she's still a good friend.
She was at Berkeley for many years now back at Penn. And then I went to Carnegie Mellon where I
worked with both Matt Mason and also for a little bit with Mark Rayburt. And so I had just the
opportunity to work right at the heart of where robotics has been, had been start growing. And I
took an interest in one particular problem, which was grasping. And I, I've been working on the
same problem for 35 years. Which says a lot about how hard that problem is. Exactly. Exactly.
You know, it's interesting because most, you know, I think everyone on this, who's listening to
this knows that it's a, it's a hard problem for robots. But it's still interesting for the public
that most people, you know, we humans do this effortlessly. It's very easy to pick up almost
anything, right? That we, you're handed. In fact, my dog can pick up almost anything with a parallel
jog ripper, right? It's, it's a, it's a, it's a, it's a, and it quickens out, you know, you can pull
out some very weird shaped object and it'll quickly figure out where to grasp it.
That is a fascinating skill. But if you do that in front of a robot, it is, we're very far from
being able to, to do that reliably. Yeah. I think one of the things, um, and kind of looking over
some of your recent work that most jumped out at me as indicative of the kind of progress we've made
is a paper that you worked on with your team autonomously untangling long cables. And I guess that
struck me because I remembered back to, I don't, it must have been maybe four or five years ago.
I don't think it was, it was your paper. I think it was Peter Abiel had this paper of like,
trying to untie a knot on like a gigantic rope, you know, a single knot. And if the color of
the background change didn't work, if the color of the rope changed, it didn't work. And here you are
with this paper like untangling a hornet's nest of cables. Well, it's not quite the hornet's
nest yet, but, um, but you're, you're, you're, you're right in it. That's, that's a very good
perspective. We were, um, Peter was working with a thick cable. Um, and it was, I believe that was
tying a knot. Okay. To get to create a knot. A few years ago, we started looking at untangling.
And we started with very simple, very small sections of cable. So on the order of eight inches.
And we actually used our surgical robot as an implementation of that. So we had, um, the
Da Vinci, um, trying to untangle these, uh, very small, uh, segments. And they were very simple knots.
They were just overhand knots. And that, that was a, that's a hard problem because it's, you're in
the realm of, of deformable objects. And so, you know, and there's an infinite state space
for those. There's also self-occlusion. And, and, and in fact, though, we minimize the self-occlusion
because of the, the size of the knot, the size of the cable, right? It was fairly small. So you
didn't have all this overlap and slack, if you will. So the key was to identify where the
knots were, which we, we learned with a deep network, lots of examples. And then it would just
start pulling at these knots and then, and then be able to open them. What was, what was exciting
was the, the, we started thinking about longer cables and really expanding this into the macro scale
with a, a full-scale robot, a dual-armed, um, um, UMI robot. And really starting to think about
all the complexities now of, of, of truly trying to untangle this thing, pull it apart and manage
the workspace for a dual-armed robot, which is, there's got a lot of complexities in its own
right, because you have to avoid self-collisions with the two arms. They have to work around each
other. And there's challenges and perception. And also, the workspace of the, of the robot is
surprisingly small. It's usually about the size of a dinner plate, really. And the cable, in this
case, was about three meters. So that greatly exceeds the size of the workspace. So you have to think
about how to move things in and out of the, of the workspace and resolve ambiguities. But that,
that has been a very fun project, I have to say. The team has done a great job. It was actually
led by two, uh, two undergrads who became master students now, uh, Vynavy and Koshek. And they, um,
and, and with, uh, Justin Kerr, a PhD student, they presented this at RSS this year. And they did
such a fantastic job. They won the best systems paper award there. Yeah, congrats to, to you and
them on the best systems paper award. The award calls out the, the systems and nature of this. Uh,
can you talk a little bit about about this as a systems challenge? Definitely. And thanks for asking
about that. In fact, it is a systems paper because you have the, the system of the perception system,
the, the planning system, the actuation system. One of the key things that made this new work possible
was a very, very clever hardware design that Justin came up with, which was to add a little foot
onto the parallel jaw grippers. And if you think of it as just a little L shape, but a very small,
I, almost a toe, if you will, what that allows it to do is that the, the grippers now can be fully
closed and that holds the cable tightly. But if you open them by a little more than the diameter
of the cable, then those two toes basically are interlock and they, um, prevent the cable from
escaping. So we call that caging in, in robotics. Caging is where you have the object contained,
can't escape, but it may not be held immobilized. So it might bounce around. So caging like you put
your hand or, you know, our bird in a bird cage, it can move, but it can't escape. So caging is,
um, is an interesting geometric problem in its own right. But here we, we think about caging,
which is a way of allowing the, the, the gripper to enclose the cable and slide along the cable,
which tends to pull through knots and tangles. So that turned out to be a big, very, very critical
part of the, of the system, that piece of hardware that wasn't allowed us to introduce new primitives.
So what we call cage pinch dilation, which is where we pinch with one jaw, cage with the other
jaw and then pull dilate. And that allows the, uh, the, the system to untangle individual knots.
And then we systematically pull through the slack and there's one other challenges, the depth,
finding the where to grasp a cable is difficult because the cable is not lying perfectly on the plane.
So it tends to loop and sometimes those loops can lift three or four inches off the plane. So if you
go to grasp them, you really do need to have some sense of depth. And so we use a depth camera,
but those are very slow because of the scanning process. So what we are moving to right now,
and this is continuing, is to, is, is, is trying to avoid the depth camera. Is the depth, depth camera
that you've been using? Is it, uh, like a vision-based 3D stereo thing or more like a point cloud,
connect type of camera? More like a point cloud connect. So it's using a laser scanner and, uh,
and, and, and it's, so it's a structure of light, but what, um, that scan is slow. It's about,
it's about one frame per second at best. Now, the, um, and it also has a lot of specularities,
problems with the, just getting that accurate is, is challenging. So here's what we've been thinking
about, Sam, is, um, trying to imagine that we can't know exactly the height of the, of the cable,
but what we can do is we can use geometry and physics. So what we do is place the,
trying to estimate a, a position for the gripper above the cable in such a way that the cable
is somewhere in here. We don't know where, but what we're doing is now lowering the cable down
in such a way that it will again cage the, um, the cable as it moves to the, to the work surface,
and then it closes. And so we're guaranteed to get the cable even though we don't know it's
exact depth. So this is the kind of thing where we're, we're, we're interested in is how to use
geometry and mechanics to perform things where the censoring may not be sufficient. And another example
is that if we, um, if, what we, we, here's something we're very interested in right now,
which is starting to use the subtle clues where I want to, um, determine where, let's say,
do what really care about where the, the cable is. It's, it's sort of moved up and sitting on
over the table. What I can do is move down with a jaws and carefully monitor what's going on.
And when the, when the moment when there's a movement of the cable will be an indication that I've
made contact. So the vision can actually provide a lot more information than we tend to think. We
don't need, we don't need a contact sensor here. We have a vision because the cable will imperceptibly
move just a few pixels. But knowing that is a trigger to say, wait, that's the height. And we know
the height of the robot arm because of the kinematics. So I can determine that. Well, therefore that's
where the, where the cable is. Interesting. Interesting. Uh, so hardware played a, a big role in, uh,
allowing you to do this. Can you talk a little bit about, um, the software or MLAI side of things.
And, uh, with a particular emphasis on, you know, things that have changed over the past,
a couple of years that have really helped to tackle a problem like this. Sure. And the, the,
because we've been, this problem has been evolving. And it's one thing I'm actually just
want to mention, I really believe in. So I came to the students last week and we were saying,
you know, I'm really proud of the, the, the projects that tend to continue over multiple years.
Uh, you know, DexNet, as you, as you know, was our grasping work and started with DexNet 1.0.
And when all the way up to 4.0. And we're not working on 5.0 actually. Because what I like is that
we take a problem and we really try to, to, to, to dig deeper and deeper into it. And in this case,
it's, um, it's really looking at the failure modes, really trying to not be satisfied with the
performance that, you know, and, and, and, and wanting to understand how can this be made better?
How can we really push the envelope? How can we really reduce the failures? In this line of work,
we, we systematically characterize the failures. So, you know, just as an aside, I see
today so many papers that, you know, say, we've bit, we've beat the state of the art. And, uh, you
know, thank you. This is, this is superior and end of story. Um, and it's very frustrating because
I always want to know, well, where, how did you beat the state of the art? What, you know, just
giving me a, a mean success rate doesn't tell me a lot. I want to know, did you, did you succeed on
a number of cases where the prior out, you know, the baselines failed? Or what, what, what did you,
where did you succeed? And where did you fail? And, and that failure, you know, it's interesting
because we have a, I think an instinctive desire not to want to look at that. But it's actually
where the most interesting aspects of the problem are. And so you really want to study those
failure modes. And so we do, we, we, we, we, we characterize them into different categories. Every
single case, we really determine what was the failure mode. Where did that one go wrong? And
then we try and look at those and say, okay, how can we address that? So the, the, the, the ideas
that in each case, we have to, to really develop new primitives and new primitives both for
perception and for action. And then we try and think about how do we sequence between those
primitives? So the, so these are the software aspects. One of the, one of the algorithmic aspect is
learning the, to recognize just where knots are in images. And so we, we did that in real by
sampling lots and lots of examples. We just have the, the system watching the cable and we do
a certain amount of self supervised movement. So the cable, basically tie a knot in it. And then
we allow the system to move itself, move the cable around, taking images over and over again.
And then we, we either manually or what we hope to do is the more and more is to have a self
supervised method that provides ground truth, labels, images, and then we can train a network.
So that's been a core part of it. And that's, you know, that's a huge thing where deep learning
has changed the equation over the last decade that we have that perceptual ability that, that,
that really can solve some of the very, very complex perceptual problems where it's very hard
to analytically determine what you're looking for. Right. And this is to determine what is a knot.
It's actually very hard. One thing that also is interesting is there's a whole body of theory
called knot theory. And mathematicians have been working on this for centuries. It's, it's,
it's very interesting. It's, uh, it's where they, they begin by turning it into a, a graph.
So they abstract away from all the geometry and, and the, the physics. But I'm very interested in,
in combining these. And there's one technique that comes from, from, um, from knot theory,
called a rate of mice to move that is actually analogous to what we do when we pull through
the, uh, the cable from end to end. So we've been applying that. We've been applying new,
new forms of, uh, of, of learning in particular. We're right now, again, trying to remove the,
the reliance on the depth sensor and learn primitives that can determine, they can compensate for
not having depth. And I'm also very interested in having human in the loop. And by that, I mean,
very sporadically, um, um, when the system is stuck where it's not making progress or it determines
that it's in a sufficiently uncertain state, it can call in a human for help.
And this, this is, this is, this is interesting in its own right, in more general sense of,
of when, how do you do this to move? You want to minimize the burden on the supervisor of the human.
And by the way, this is, this is a, this is a, a widespread issue. For example, as you,
as you know, Google is testing an automated, uh, car service, taxi service in the Bay Area.
And my understanding is that they have humans, um, networked in and are standing by.
Now you have one human, probably controlling multiple taxis. So now you have a question, how,
when, when do you call that human in? Right. And you, you want to minimize that because,
ideally, you can have one human supervising 50 taxis, right? So you don't need, you don't need
the person that often. But in, in any robotics case, it can be very tedious to be constantly bothered,
right? So there's a nice problem in when, when do you call human? And also when the human comes in,
when do you transfer control back to the robot? So you, you have these nice dual problems,
and they both have to do it in a sense, a model of confidence. So we've been looking at that. And
I think that that applies to many of the kind of tasks we're interested in, where there are these
failure modes that you're really unrecoverable, at least currently. And so that's where there's no
harm. And, you know, maybe once an hour or so, you want to have a human come over just to just
something and then, um, then, you know, go back to whatever they were doing. Yeah. When I, when I
introduce you, I referenced that you're in the School of IE and OR, and it strikes me in your
description of the, you know, this problem, there's also some interesting kind of classical OR
queuing theory types of problems in there, you know, despite the, the degree to which I enjoyed
working on that kind of stuff in grad school, I have not looked a whole lot into what's happening
to marry machine learning and queuing theory. Do you know of anything interesting out there?
Oh, well, it's quite a bit. I mean, queuing is a, is also a beautiful model. There's, it's,
it's always used a Poisson distribution assumption. And a lot of nice theorems you can prove on that,
but the reality is not, it doesn't behave that way. So how can you generalize that to real empirical
objects or real empirical distributions? And I'm glad you mentioned, um, that you, you know, this
connection with, with queuing, you know, in, in OR, you know, my colleagues say, well, we've
been doing, you know, we've been doing AI for, uh, with the century now. I mean, what, because,
because in some sense, you know, mark up decision problems, um, these have been the core of operations
research for a very long time. And so those are early forms of AI and still, and being rediscovered
in a way, especially where it comes to optimization, which is at the core of deep learning. And so many
of the models that we're using now. So it, it's very natural to have connection between, uh, OR
and AI. And the industrial engineering side of it comes with the other aspect, which is how do you
make these systems practical? And that's where factors like what we're just talking about, the human
interface come into play. You know, there's a, there's a distinction between robotics and automation.
And robotics is obviously much more popular and, uh, enticing and the press labs and robots,
et cetera. And so I've always been amused by the fact that, you know, if you start talking about
automation, it's tends to be, you know, just, uh, that sounds like something, you know, in a,
in a factory, I don't want to really talk about that. But if it's, is robotics, it's really exciting
and energizing and, um, you know, it feels like, you know, science fiction. What's been happening,
I think, in the last few years is that there's a trend toward automation because there's a
recognition that we want to start putting things into practice. And that's where you have to worry
about robustness. You have to put guarantees on performance. You want to worry about
cost, reliability, all those, those factors that are, um, you know, often overlooked when you're
just doing something in a lab. Yeah. Interesting. I thought you were going to go a totally
different, uh, direction with that last, last comment. Um, people often will talk about software
robots. Uh, and I, I'll ask you what, what your take is on that. But to me, like a robot, part of
what fundamentally defines a robot is this bridging of the digital and the physical realms and
something that, you know, purely exists in the digital realm. Unless we're talking about
a simulation of something that exists in the physical realm, uh, I don't like calling those
things robots. The software or it's, you know, some, you know, it's automation, uh, it's supposed to,
you know, software robot. Oh, no, I agree. I can't, I couldn't agree more. I mean, I think that's
actually, it's a misdomer. People often say bots, right? Oh, it's the, you know, bots took down
this website, right? Because it was automated, um, some automated modules that would be able to
do something, but they're just software, right? And they're, I think that is definitely a confusion.
And I, I mean, to my mind, the robot has to have a physical component. In fact, this brings up
another aspect, which is a lot of research has been done just in simulators and then demonstrated
with simulation. And I think there's a danger there that if you, you can, you, you can almost have
a self-fulfilling prophecy. You build the system, you build the simulator, you're retuned,
you work with a simulator, you tune off of the simulator. It's very nice because it gives you
the ability to do, to collect lots of huge amounts of data and you can do resets in the simulation.
But if your simulation is even slightly deviates from reality, when you now take that policy and
put it into practice, you have a performance can, can degrade, you know, dramatically. And so
a lot of the early mijoco demonstrations of walking machines, etc. looked great and they
look beautiful and just surprising how fast they would learn. But then they would not easily transfer
into real machines. So this is the, you know, the, the, the sim to real gap that I think is so
interesting right now. And it is really important to recognize. And you've been doing a bunch of work
in that area as well, you and your, your live feed talk a little bit more about kind of how you
characterize that sim and real gap as a set of research problems and some of the specifics that
you've been working on. Sure. One of the things that I've always been interested in is this,
is the, is the limitations of simulation and in grasping. And I talk about the, the very real problem
of, of the, being able to, there's essentially indeterminacies in physics that are due to friction.
And the example I always like to point out is just pushing as a pencil across your, your,
your desk. And if you, if you do that with just put your index finger and you do, you do, you
do that repeatedly, the position of the pencil will be very different. And so in, it's a chaotic
system. It's basically based on the, the, this complex surface physics, the surface topography.
And that is very difficult. It's not, it changes every single time you, you perform this. So
in a sense, it's impossible to predict how that pencil is going to, with the final state of
the pencil. It's, it's, it's, it's, it's, it's undecidable. I feel like, I think we talked about
this in a fair amount of detail last time. All right. You have a good memory. I, I, I know,
I don't want to repeat myself, but I wasn't saying that because you're repeating yourself. I'm,
I was more saying that because there, there's a part of me that wants to, you know, get into a
philosophical argument about it. And I'm wondering if we, if I got us into that philosophical argument
at last time, you know, the basic question being, is it kind of practically chaotic because we,
we don't have the resolution to incorporate the fluctuations in the surface and the dust particles
and all these things? Or is it, you know, if we could do, if we could capture the microscopic
physics, would we then have a deterministic system? Or are the, whether, whether always be
some element that we can capture, you know, humidity, temperature, what have you?
Mm-hmm. Mm-hmm. No, I, I, I, I love that. We could probably talk for an hour just on that.
I've been using the, the term, thinking about the terms epistemic and aleatory uncertainty.
And this is exactly what you're talking about. So the epistemic is that we just need to model
this better. We have those aspects we don't, we, we don't currently know, but aleatory is what's
inherently uncertain. And that's, you know, it's the same for throwing a dice, right? You, you,
if you're having better and better models, you're still not going to know how, you know,
being able to predict that with certainty is, you know, inherently uncertain. Now, at some point,
you get down to the, uh, the subatomic level and you back to the Einstein and God does not play
dice at the universe, right? So, but, but, but, but any, but any practical sense, you're never going
to be able to predict the position of that pencil. And so you have this, the, the reason I say this
is it's not, doesn't mean that you can't do it. People do it all the time. We pick up pencils.
So what's going on? What's missing? And I think that robots and simulators have a problem because
they're very, they're deterministic. They, the, the simulation has one outcome. And if you perform
the same thing over over, it's doing the same thing. So you tend to, to, to have a system of policy
that's trained on that particular outcome, but it's not trained to be robust to those variations.
Now, the, the trick to doing that is in this idea of domain randomization, right? Which is where
you randomize the outputs in the simulator, but you want to do that very, very methodically.
So the, the, the, the trick there is to have the simulator have ranges of outputs that are
consistent with what you would see and simulate in reality. So this is why what we've been talking
about recently is, is, is real to sim. And we learned this by, through a, a project that we were
doing on, it was very, very, also working with cables, but here the problem is to what we call
cleaner robot casting. And so here you have a robot with a, a weight on the end of a cable.
And the robot is holding the cable above the surface and it basically casts the, the cable out,
like you would a fishing rod, fishing lure. And then you pick a target somewhere on the surface
above where the cable has landed. And then you want the robot to do a motion that will cause the
cable to wind up, but the endpoint to wind up at a particular target point, meaning when it's pulling
it back or when it's casting it out. No, when it's pulling it back. So you want to sort of, you want
to sort of motion like this that will cause a dynamic motion towards a land in, in this particular
point. The, the nice thing is you can do a lot of supervised data collection, self-superbite data
collection. So the system, we have a camera overhead, we have this cable set up and this system
just basically does this, you know, all day long. So we have a, a data on the input control that
we give to the robot and where did the actual cable land. And one thing is there's, there's this
aleatory uncertainty and you can measure it because you give it exactly the same cable motion
and the, the, the endpoint lands in different places, right? And so we can actually draw
an ellipse around that and say, this is the uncertainty that's inherent. Even though this can run
all day long, it's still only capable of generating a few thousand examples. You really need
many more to train a reliable policy. So we wanted to use a simulator. Now, what we found was
that the simulators, they're the number of simulation packages that can do this. Mojoko, Isaac,
Sim and others. And they all look good. They all kind of, when you look at them, they look very
similar to what we're seeing in the physical space. But then we try to actually give it the true
parameters of what we're measuring. And there's a, there's a deviation, right? So this is where
the question is, how do we tune that simulator to closely match reality? So that's the real
system. So you see the difference because if you just use Mojoko and you try to have a walking
machine and you just start with the simulator, right? You, you get this thing and it trains and
it runs and then you say, okay, now I've got a policy. Let me go pull it and try it on a real robot
and it doesn't work. But if you start out by saying, I have a real robot and now I want to make a
simulator that really mirrors what's going on with that, um, that, that robot. And this is
closely related. Maybe it's, it's very similar to the idea of the digital twin. That's very popular
now. And the key is how do you actually do this tuning systematically? And people call that system
identification, right? That's a very common term. But there's a lot of misconceptions about that.
System ID is, uh, is fairly well understood if you have, if you know the structure of the system,
you know, if you have equations that decide the system like a pendulum and you want to identify
what is the mass at the tip of that pendulum, then system ID is very good for telling you that.
But if you have a system like this, this, uh, piece of cable and, uh, and the frictional
interactions of something sliding across the surface, then you don't have a, you don't have a structural
mark. And so it's very hard to figure out what should the, what should the parameters be in the
simulation? By the way, simulation has a lot of parameters. There's, there's, um, things like
torsion of the cable, there's friction of everything. There's inner, you know, when the cable
rubs against itself, there's another frictional property, another frictional parameter for that.
So there's a dozen or more. And now you have a nice optimization problem because you have a
bunch of data that you've collected in real and you want to tune the simulator to match that.
So, um, so we've been exploring that in this context that we actually turned out that in,
we have a paper real to sim to real. We start out with real, we tune the simulator, then we can
generate lots of examples. We combine that with the, the limited number of samples we got in real
and then train a policy and then bring that back into real. And so, and that seems to perform
better, much better than if we just use a very limited amount of real data, which is all we can get,
or if we just use all the simulated data that wasn't tuned to the real system.
So, I've been excited about this because I think it applies to so many problems that we're
looking at in robotics, where we look at a, we have these, these systems and we really want
to have a simulator that's, that's very physical accurate. And what's also been very interesting,
as you, as you know, is that, um, Nvidia has made a major push in simulation. So they've got a huge
team of various student researchers developing Isaac Sim and Variant and also thinking about how to
make those run very fast. And in parallel, um, deep mind acquired Mujoko last year. And that was,
it was almost exactly a year ago, that was a major, major milestone because Mujoko, um, was,
was a, was a very good system, but it was run by a fairly small team. Now it went into deep mind,
which has, you know, much more resources. And they've assembled a fantastic team of physicists
and researchers to basically take Mujoko to an entirely new level of realism. So it's been fantastic
to have these two projects coming along, where they're both getting the simulators better and better.
And that is, I think, is going to lead to major breakthroughs in the field.
You know, along these lines, I wonder if you have been involved in exploring the
possible impact of causal based models here. I'm thinking about the ellipse that you're
describing around kind of some ideal point, you know, where you're casting back to. And you've got
a bunch of different sources of possible, you know, call it noise. You know, you've got
measurement noise, you've got control noise, various other things. You know, is there some kind of
discot that are folks looking at causality as a way to understand that, you know, how these
inputs combine to create uncertainty and to create better models in the robotic realm?
Well, I would have to say one, one answer to that is by trying to figure out how to optimize
the tuning process. And that is that there is a causality inherent in controllable in the sense that
that the robot is able to change its parameters. And you want to be able to do that systematically.
So one way to do it, let's just take the planer robot casting is you pretty much randomly generate
a lot of control inputs, trajectories for the arm. And then you just observe where the end point
of this cable winds up on the surface. Now, you can just generate a big data set and then throw it
in and then try and analyze that to come up with a model. A better way is to do that systematically
where you start doing some random examples, but then you start testing basically values of those
parameters and going back out into the real system and fine tuning it so that also regions of
the state space that you hadn't explored earlier, you can or you haven't explored sufficiently,
you can you can reevaluate and put more, do more experiments in that area.
So that I think is really interesting where the experiments in real are costly, if you will.
They require time and an offer and it's very difficult to reset the system to obtain
the same input to run it again, right? But it's very, so you want to be thoughtful and systematic
and this is related to the theory of the design of experiments. And typically you're trying to
maximize some kind of mutual information gain that you will, by doing this experiments,
you want to choose the experiment that's going to gain you the most information. But it turns out
computing that solving that is a very difficult problem. So there's so many interesting open
problems here and it's very exciting to see how the field is is maturing. I think robotics is
is moving at a remarkable pace, but it's still from the public perception very far from what
what people commonly, you know, commonly think should happen. And people are still thinking, well,
the robot, you know, why don't we have our robot drivers? Why don't we have our robot,
you know, robots in the kitchen and robots at home taking care of us? And these things are
still very far off, unfortunately. Maybe digging into that a little bit where maybe three weeks beyond
Tesla's Optimus Robot announcement, which is, you know, causing people to ask the question again,
oh, hey, are we close to our robot in the home? Elon says we are. What's your take on Optimus? What
was really demonstrated there, you know, the extent to which it demonstrates that we're close to,
you know, practical everyday robotics? Okay, so I do have a take on this. I was very interested.
I watched it right as it came out and it was it was very interesting. I mean, I do have to hand it
to Elon Musk. He's a great entertainer. He has a real knack for doing things and doing stunts
and basically having ideas that are really, you know, out there. But he's also, you know,
has been a visionary. He has actually has succeeded in certain categories. So he's done with
electric cars and with Tesla, with in terms of batteries, in terms of space and landing
an aircraft or a rocket back on the earth, that's remarkable. And those are, you have to put
those into context of everything else that he's doing. And so I have to say, my first reaction is
was, look, you know, what's going on here? He is, he's probably keenly aware that the price
earnings ratio of an auto company is, you know, kind of at one level, but the price earnings ratio
of a robotics company is much higher. So if he transforms Tesla into a robotics company,
there's a very clear benefit for that. Right. So that could be one, one part of what he's thinking.
But I think what's also going on is that he's saying that he really is putting his, you know,
put it putting out there, you know, a bold new idea. And he's not afraid to take risks like that.
Now, what I was happy to see was that, you know, that robot was, was substantial progress from
the past year when they had first announced it. And I remember when he very first announced it,
I thought, what are you talking about a humanoid? No, that's not going to happen. But he has,
he's really put real research behind it. Now, it was, it fell short of anyone's expectation,
if you know about what's going on with agility, robotics, and, and, and, and Boston Dynamics.
But it was, it is a start. I think that the one thing I'm very excited about is that he understands
the this aspect of automation in the sense that it has to make something that's going to run
reliably and cost-effective. So when anybody has been building humanoid over the past
three decades, nobody's really talked about the cost effectiveness of that, right? But he was,
he went out and said $20,000, right? Okay. Well, what does that mean? That means he's going to have
to develop some new motors that are, because he's got a lot of motors in the system. What would you
say the, the list price is on the analog? Oh, the Alex? Oh, it's, I, I, I haven't, I haven't seen,
but it's, it's probably over 200,000. Order of magnitude, a couple order of magnitude, maybe?
Yes. And the, the thing is that these are, you know, you have to amortize all the research
over, over the, over the volume. So if you have, if you're arguably able to produce in volume,
you can do this. The, the challenges in designing new motors, gear trains, sensors that, these
are all things we actually need in robotics. You know, arms, there's a number of arms out there,
but they're really, they're all still, um, fairly expensive and either imprecise or, um, or,
or dangerous, right? So I think there's, I would, I would love to see, and I think Tesla is in a
perfect position to do this, is that they would come out with a new line of motors, and those
could be used for robots. They might even come out, here's something I'd be saying. I think they
could come out with an industrial robot arm, that, um, that Tesla would come out with a new arm,
that would be, that would actually be very high precision, low cost, low mass, and, and we need that.
So particularly if it's one that's based on their own needs and experience as a manufacturer.
Exactly. Exactly. Because that, that's right. So he has a use case right there, right? And there's
all kinds of aspects of the, and they tried to automate with, with existing robots and had
number of challenges. If they build their own robots now, that's very, that really changes
the equation. No, very, no one else, no other company has that big of a use case and production
capability. So they could do it. And the other is sensors. You know, we're just talking about
the connect and all the lines of 3D sensors. They could really put their muscle behind a really
nice compact, uh, uh, sensor that could be, they could give us 3D or very fast 2D sensing.
And that would be very bad. Tactile sensing. By the way, that's also heated up since the last
time we talked that Facebook has really developed a partnership with, um, with Jelsight and come out
with the, um, digit, um, tactile sensor. And that is very interesting. By the way, that's a big
breakthrough. Um, and we've been using that. And now Jelsight just announced a new version of
their sensor, higher resolution. And they're faster. Um, we're very interested in this is like a new
generation of tactile sensing that I think we're going to see lots of applications. Okay. That's
super interesting. I, we may have talked about this last time. I remember as a kid taking, uh,
Paisio electric foam or something like that and slapping it between a couple of circuit boards
printed on one side and using that as like a touch-ish pressure sensor. Exactly. Exactly. We,
you know, good memory. I, I, we did talk about that because that was where I got started as an
undergrad doing trying to build touch sensors. And it's, it's that, you know, you don't have that
yet. And the Jelsight is, is kind of another breakthrough just as connect was that starts to make
that more, more feasible. And because it's using optics, it's kind of riding the, the curve of,
of advances in, in cameras. So it sounds like, uh, kind of the, the, the, to net out your take on
optimists. There's some interesting things that you hope grow out of it. Um, but you didn't
necessarily see anything that, uh, you know, if you were Boston dynamics would make you fear for
the future of your, your own company. No, but I would have to say, I think, well, I think it's good
for the field. So when you have, you know, you have someone with that level of attention
and, and, and mind share, coming out and saying robotics is where we're going to make major
advances. That is good for the whole field. I think it, it talks to young engineers who, you know,
want to take a robotics class or want to maybe go into the field. It also, it speaks to investors
who are, you know, look at his track record and say, hey, maybe he's going to pull us off.
And I think, I think it would be, I don't, I think a bit very unwise to bet against him.
In other words, I'm not saying he's going to come out with a, with a practical humanoid.
I don't think that's, I think he's going to quickly discover how complicated that is.
Yeah, I think that's what, uh, what I'm kind of getting at or trying to get your take on it.
Well, he, you know, I was, I was joking that, you know, uh, it's true, rocket, robotics isn't
rocket science. It's, it's actually, it's much harder. Um, and, and by that, you know, because
this comes back to the things we were talking about, you know, um, basically doing a landing
of a, of a rocket, um, back on, you know, that stabilizing that is a beautiful control problem.
But there's, the, there's only contact at the end of that. In relation, you have continuous
contacts. And those are very difficult and non deterministic for all the reasons we talked
about. So that, that problem is technically harder. And so getting that right is going to require
the next generation. I mean, we're, that's what I'm excited about. So yeah, because I do feel like
we're at the point where the lot of vectors are lining up that we're going, we're going to see
progress and having someone like Elon and his, you know, army of supporters is, uh, is a great
thing for the field. Mm hmm. Uh, I guess one more topic I want to take on before we wrap up,
you're the chief scientist at Ambi Robotics. Uh, what is Ambi up to and, and how is it pushing
the field forward? Well, I've been very impressed with the team, um, at Ambi. The, since we started,
three years ago, the, the team has just been absolutely fantastic, very, very laser focused,
Jeff Mueller as the, as the, the, um, the chief technology officer and really the mastermind
behind Dexnet, he has been leading the technical team and, um, on the software side. And then Steve
McKinley and David Geely have been working on the hardware side. So it's a blend of, of, of very
elegant new software and hardware that are coming together in these systems. And, um, the, the,
the CEO who Jim Leifer has this incredible background in the, in logistics. So this actually
also has happened since you and I talked last. Jim has been, you know, a history working in Wal-Mart
and working with, um, with a number of, uh, of logistics companies. So he really understands
the real problems. And then they, we've been working with the company, uh, named Pitney Bose,
who is a, you know them? Uh, yep. I, uh, I worked at Pitney Bose, uh, as an undergrad for a co-op.
I was doing, um, I was doing, uh, I forget the term, like essentially laying out, uh, custom,
um, custom chips. Really? Oh my God. So you know Pitney Bose. Pitney is very interesting. They're,
they're an old company. They've been a old-school company. Postal, yeah. Postal leaders,
1920. So they just celebrated their 100-year anniversary. And what's been interesting is,
when you look at them, they've, they've always been looking at technology for postage in various
ways. And they, um, they, they, but they, what they do is they, they're sort of behind the scenes
of a lot of the postal sorting systems around the country. And so they installed them for the US
Postal Service, for UPS, for, for FedEx and others. So they really know this technology. And it's been
a pleasure to work with them because they, they're really true engineers. They really try to solve
problems. And so, um, partnering with them has been terrific because we're, we work side-by-side,
where they've, they've essentially, um, you know, become our, our biggest customer. And we,
you know, we're installed 60 of our systems over the summer all across America. And they just
invested in us. So what are the system? Is it, uh, is it a, a hardware system? Yes. The system
is hardware software. It's called ambiSort. And what it does is it takes bins of packages and
sort them into smaller bins according to zip code. So sorting is a little different than just grasping.
You have to grasp the object and then scan it, determine what the code it is, put it on another
a gantry robot that then puts it out to, uh, and drops it into a bin. So there's a quite a bit of
hardware. In fact, it's, it's sort of the size of a living room. Um, it actually fills up at 18
wheel truck. That's, that's the, and what's the form factor of the robot? Is it, uh, an arm type of form
factor or? Yes. So there's a, the standard six-degree of freedom industrial arm at the front of it.
It's picking things up. And then there's a gantry type robot. Like I think of it x, y, um, system that,
that has a pivot that drops the package into the appropriate bin. So that whole system is called
ambi sort and involves lots of cameras, lots of safety features, lots of, of, of, of, of, failsafe
features. It's a, it's a big operation. It's got thousands of parts. But that is, those are the
systems we're talking about. So each one of those can, can, can basically sort through hundreds of
parcels per hour. And that is a big, um, it's an interesting challenge because it's very hard. This
is a, can be back breaking work. Humans are, are prone to making a lot of mistakes. Um, and people,
the turnover in these warehouses is enormous. Uh, you know, all the companies, now Amazon is very
big on this. They're starting to, to try to, um, find ways to automate this. And this is really our
focus. Awesome. Awesome. Um, and the, uh, you know, for that, for, for packages, you often see,
you know, as opposed to hand, uh, you know, gripper types of actuators like suction actuators and
other things. You mentioned Dexnet. So I'm assuming, uh, you're doing kind of more of a gripper type of
actuator. Well, a great question. So Dexnet 3.0 was, uh, suction, where we, we took the same,
same idea and applied it to the suction model. In, in a sense, you have a gripper is a two-point
contact. You have to find two points on the object of pair. In suction, you have to find one.
And so it's just one point contact. But the, but the physics are very different. And so the
resistance to shear forces, for example, are much lower for a suction cup than a gripper. So in
the, the suction cup can be, is, is actually the, the workhorse for this kind of work in industry.
And we can extend Dexnet, um, in a number of ways to make it work in this context. And that's
really been where the team has been pushing the envelope. And we also collect data from every one
of these systems. So it's, it's a, it's a wonderful problem from a machine learning point of view.
Because we have data sets, we have images, we have sensor values, we have all this, we can
characterize every single failure. And analyze it. And then try back testing different algorithms
to be able to reduce those. And that's, that's where there's a, there's a huge opportunity. Because
really, there's this gap. Can we start closing it to really increase the, the throughput,
the pigs per hour? Awesome. Awesome. Uh, well, Ken, I think, uh, we covered a ton of ground,
but also demonstrated that it's really hard to stick to in a half years of robotics,
advancement and innovation in an hour. Uh, so I think that just means we'll have to be sure to
catch up more frequently in the future. Ah, I would love that. Thanks Sam. I have to say I've been
such a pleasure because I listened to your podcast on my bike rides, um, mountain biking. And so
I've just enjoyed them so, so many good hours on a bike, uh, with you. And, um, last thing I want
to say, I don't know if that, when this will air, but the conference on robot learning is going to be
in New Zealand this, in, in December, 14 through the 18th. And it is, um, so I'm, I'm chairing the
conference and we've been, it's been a real pleasure. We have 500 papers submitted a top-notch group
of about 200 papers will be presented there. And you can register as an online, um, online to watch
all the talks and everything else, uh, for, I think it's like close to $200, not too expensive.
And then I will tell you your audience that we're also going to make all this available offline
after the conference. Awesome. Uh, thank you so much, Sam. I really appreciate your great
heartbeat. Thanks so much, Ken. All right. Take care.
