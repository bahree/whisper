WEBVTT

00:00.000 --> 00:12.560
Welcome to the Twimal AI Podcast. I'm your host, Sam Charrington.

00:17.920 --> 00:23.360
All right, everyone. I am on the line with Emily Bender. Emily is a professor of linguistics

00:23.360 --> 00:27.840
at the University of Washington. Emily, welcome to the Twimal AI Podcast.

00:27.840 --> 00:32.400
I'm really excited to be here. Thanks for having me on. I am super excited to chat with you.

00:32.400 --> 00:37.200
We spent some time speaking earlier, kind of the obligatory, how are you doing?

00:37.200 --> 00:44.000
Kind of age of corona conversation and kind of grounded on some of the things that we'll be

00:44.000 --> 00:52.320
talking about. Ethics, practical considerations around AI and NLP ethics, computational

00:52.320 --> 00:57.760
linguistics and a relationship to linguistics to name a few. But I'd love to start with hearing

00:57.760 --> 01:02.880
a little bit about your background and how you came to work in the field of what linguistics

01:02.880 --> 01:08.960
slash computational linguistics. Happy to. So my background in terms of training is linguistics

01:08.960 --> 01:16.160
all the way. I studied at UC Berkeley. Didn't know what linguistics was until I got there and

01:16.160 --> 01:20.960
he did some wonderful advice the summer before going off to read through the course catalog,

01:20.960 --> 01:25.200
which you know back then was a book, looked like a phone book, and just circle anything that looked

01:25.200 --> 01:29.040
interesting. And there was this one class called an introduction to language. And I thought,

01:29.760 --> 01:33.120
that sounds great, because I really liked my language classes I took in high school.

01:34.000 --> 01:37.280
And so I circled it. And then when I was looking for a general education

01:38.000 --> 01:42.000
requirement class, I saw that it fulfilled one of them. So I signed up for it and I was hooked

01:42.000 --> 01:47.440
on the first day. It was like, this is my thing. But it took me the whole term to convince myself I

01:47.440 --> 01:55.840
could major in something that I perceived to be impractical, which is pretty funny in retrospect.

01:55.840 --> 02:00.720
And the class was an intro to linguistics or? Yeah, intro to linguistics. And I majored in

02:00.720 --> 02:06.720
linguistics. I took one computer science class when I was an undergrad. There was the intro

02:06.720 --> 02:12.720
programming class that was taught in Scheme, which is a dialect of Lisp. And then I went on to do a

02:12.720 --> 02:18.640
PhD in linguistics at Stanford, where I worked on. And did you field work in the whole nine? No,

02:18.640 --> 02:23.120
I did some experimental work, but I didn't do like there's there's linguists who do really

02:23.120 --> 02:27.200
interesting work going out and, you know, recording people speaking to observe something with

02:27.200 --> 02:31.360
a variation or working with consultants to understand the structures of languages that are not

02:31.360 --> 02:37.520
well documented. I'm not either of those. But I did do a little bit of experimental work. So my

02:37.520 --> 02:45.920
dissertation was on the interface between sociolinguistics and syntax. So syntax is the study of

02:45.920 --> 02:52.000
what are the rules behind the grammars of languages? How do we get to the strings that are allowed

02:52.000 --> 02:56.320
and what they mean? How do we write those descriptions? And what do we need to be able to

02:57.040 --> 03:00.720
create a formalism that'll work for any language? Because we know that any human can learn any

03:00.720 --> 03:05.520
language if they're exposed to it. And so those are the questions of formal syntax. And there's

03:05.520 --> 03:10.720
this interest in what knowledge do we have. And then sociolinguistics is looking at language

03:10.720 --> 03:15.920
variation and how people speak differently in different contexts based on sort of their

03:16.720 --> 03:21.120
social address and how they want to present themselves to the world. So register and all that kind

03:21.120 --> 03:26.160
of stuff? Yeah, register code switching, right? Lots of people will have multiple different

03:26.160 --> 03:29.760
varieties that they control and they use it to present themselves in different ways in different

03:29.760 --> 03:36.160
contexts and all of that. And the syntax traditions were saying that's not linguistic knowledge.

03:36.160 --> 03:42.960
Linguistic knowledge is just what it is that lets you know that in English we don't say I see cat.

03:42.960 --> 03:47.120
We have to say I see a cat or I see that cat or I see cats, right? And that's a grammatical

03:47.120 --> 03:54.800
constraint of English. And that kind of stuff is knowledge of language. And all of this other

03:54.800 --> 03:59.840
knowledge that people have that allows them to decide when to speak in which way, that's separate.

03:59.840 --> 04:04.640
And my intuition was, well, we're one person. And yet we know all of this and it's connected

04:04.640 --> 04:10.000
with the same thing. I think it fits together. And so I did some research looking at variation in

04:11.040 --> 04:16.800
what at Stanford, we referred to at the time as av, which is an acronym African-American

04:16.800 --> 04:21.680
vernacular English, also known as ebonics. And black English is a whole bunch of names for

04:21.680 --> 04:27.520
the variety, which is a surprisingly actually well studied variety of English among those that

04:27.520 --> 04:34.080
are not considered standard. And so there's really good social linguistics studies of the ways

04:34.080 --> 04:40.240
in which variation is used in app to different purposes. And so I did a perceptual study looking

04:40.240 --> 04:44.960
at how people perceived the meaning associated with saying or not saying the verb B in different

04:44.960 --> 04:53.760
contexts. So that's pretty far field for a podcast on machine learning. So that's what I was

04:53.760 --> 04:59.440
doing in my studies. And alongside I was working as a research assistant on a project building

04:59.440 --> 05:05.360
a computational grammar of English. Okay. So I go on the job market and don't get picked up as

05:05.360 --> 05:11.520
either a syntactician or a sociolinguist. And so I go out into the startup world. I was working

05:11.520 --> 05:20.160
at a startup called YY Technologies. I started there in 2001, which was not a good time to be going

05:20.160 --> 05:24.640
into the startup world if you remember. It was just in time for the dot com first.

05:25.600 --> 05:30.000
Company was gone within seven months of when I started working there that had been around for

05:30.000 --> 05:37.040
almost a decade. Doing grammar engineering for Japanese. So building a grammar that modeled

05:37.040 --> 05:40.640
the rules of Japanese so that we could get from strings of Japanese to semantic representations

05:40.640 --> 05:46.800
in the context of a customer service application. Okay. Kind of precursor with customer service

05:46.800 --> 05:51.760
chatbot. Yeah. Yeah. It wasn't set up in a chatbot mode. It was meant to be email response.

05:52.720 --> 05:57.440
But very much that same idea. Okay. And the company had a product that worked for English and they

05:57.440 --> 06:02.880
wanted to branch out into the Japanese market. And so they hired me to work on the grammar for

06:02.880 --> 06:09.680
Japanese that would fit into that same product. So then I am on the strength of that basically

06:09.680 --> 06:14.640
and a little bit of research I was doing around generalizing grammars across languages got hired

06:14.640 --> 06:18.160
to start the professional master's program in computational linguistics at the University of

06:18.160 --> 06:26.480
Washington. Okay. And yeah. Which felt like a huge stretch at the time I have to say. I was like

06:26.480 --> 06:30.400
really because if you look at my CV, I'm pretty junior and I look like a syntactician slash

06:30.400 --> 06:35.440
sociolinguists with this thin layer of computational linguistics on top. But part of what was going

06:35.440 --> 06:41.200
on there was that the kind of syntax that I was doing was really grounded in computational

06:41.200 --> 06:45.120
modeling of syntax. It was developed to be able to be written on a computer as well as understandable

06:45.120 --> 06:51.040
by humans. And so the linguists in my department saw all of my syntax work as computational linguistics

06:51.040 --> 06:56.560
too. And was that just the representation, the way you represented the syntaxes or something

06:56.560 --> 07:01.360
about the analysis that you were doing? It's about the analysis and the theoretical framework.

07:01.360 --> 07:06.000
So that HPSG is what it's called and it's a framework that's interested in getting down to all

07:06.000 --> 07:11.200
the details so that we actually model not just the small subset of sentences that are interesting,

07:11.200 --> 07:17.680
but everything you might find in running text eventually. And so that's very coherent with

07:17.680 --> 07:23.920
the goals of doing natural language processing, but from a symbolic perspective. And on the other

07:23.920 --> 07:29.680
hand, it's very formalized. It's precise enough that you can do it on a computer. And a lot of

07:29.680 --> 07:34.880
other work that happens in syntactic theory is much more entrusted in theoretical questions to do

07:34.880 --> 07:40.880
with similarities and differences across languages and doesn't put the same emphasis on getting

07:40.880 --> 07:45.040
down to the nitty-gritty both in terms of all of the data and in terms of precise enough a computer

07:45.040 --> 07:50.960
could do it. Is there a way to create an example of the two different tags that would be

07:50.960 --> 08:00.000
elastic and restrictive? It's not so the best examples I can think of are visual examples in terms

08:00.000 --> 08:07.360
of what the analyses look like. And just to give you a sense of it, when we build a grammar in HPSG

08:07.360 --> 08:14.880
for English, there's English. When we do that, for those people just listening, that was a cat

08:14.880 --> 08:18.640
that just jumped into the frame. That's why we're laughing. And the cat did it earlier and I was like,

08:18.640 --> 08:26.080
I hope the cat does it again. So there's the cat. He's helping me out. So in order to get,

08:26.960 --> 08:30.400
there's a resource called the English Resource Grammar that the best thing that I was involved

08:30.400 --> 08:35.440
with as a research assistant in the 1990s. It's been under continual development since 1993.

08:35.440 --> 08:41.280
It is a hand-built grammar of English on a computer that if you give it well-edited English text

08:41.280 --> 08:49.200
from pretty much any genre or register. So chemistry, academic articles, Wikipedia, Linux

08:49.200 --> 08:54.160
manpages, Norwegian hiking, tourist questioners, like broad range of genres, but well-edited.

08:55.120 --> 09:04.000
Their cats went flying. It can come up with correct analyses that give you a good grammatical

09:04.000 --> 09:08.880
structure and a good semantic representation. I can send text tree of the center or something like that.

09:08.880 --> 09:12.960
Send text tree and then like predicate argument structure, like first-order logic type representation,

09:13.920 --> 09:18.800
predicate logic, not first-order of the semantics, for something like 90% of those sentences.

09:19.440 --> 09:24.160
It's a massive project. But in order to do that, its representations internally are actually

09:24.160 --> 09:28.880
quite complex. And if we try to like print one out, not in sort of the abbreviated form that shows

09:28.880 --> 09:32.560
you the general structure of the tree, but all the details, it's pages and pages and pages for one

09:32.560 --> 09:41.520
sentence. If what you're interested in is sort of making generalizations across languages about

09:41.520 --> 09:46.320
what happens with the expression of pronouns versus you just don't say the word at all

09:46.320 --> 09:50.000
in different languages. And how does that correlate with how much morphology is on the verb?

09:50.000 --> 09:56.480
That kind of representation can be cumbersome. And so that's where the theoretical syntax tends to

09:56.480 --> 10:00.320
work more in broad brush, I think, than the kind of details you need to do it on a computer.

10:00.320 --> 10:06.160
Is there an analogy there to like explainability of machine learning models, or is that too much

10:06.160 --> 10:13.200
of a leap? Wow, it's an interesting question. But I don't think it connects to explainability,

10:13.200 --> 10:18.240
because in both cases, it's humans are creating these things. Yeah, I'm not directly like I'm

10:18.240 --> 10:23.840
envisioning the thing that with the pages and pages is like a, you know, a deep model with a lot

10:23.840 --> 10:28.080
of primers that we don't really understand. I guess in this case, you can understand them if you

10:28.080 --> 10:33.680
zoom in. Exactly. Whereas in the other case, it's a little bit more amenable to interpretation.

10:34.160 --> 10:38.480
Yeah, I think at that level, the analogy works. It's that it, but it's not useful.

10:40.480 --> 10:43.920
You can drill down, but it's hard. So when people approach this resource and haven't been involved

10:43.920 --> 10:47.040
with it for years and years and years, it is opaque. And it's like, well, how do I,

10:47.920 --> 10:52.080
like, why is this feature value this here and that there? And where is it coming from? And

10:52.080 --> 10:56.880
why did you make that analytical decision? And there's a lot of work to be done still about,

10:56.880 --> 11:02.240
how do we document this as an incredible resource? But how do we make it accessible and understandable

11:02.240 --> 11:08.640
to people who haven't already been deeply involved with it? And so did you end up taking another

11:08.640 --> 11:15.840
computer class programming class? That's a big question. I left that hanging. So I did take one

11:15.840 --> 11:20.720
class in the computer science department in graduate school. It was a class with Terry Winigrad

11:20.720 --> 11:27.440
on phenomenology. It was a philosophy class. We did not program anything. And it was a great class.

11:27.440 --> 11:35.200
He made us read Heidegger, which was painful. But it was a really interesting class. And that's

11:35.200 --> 11:42.080
the only other computer science class that I took. I did learn along the way how to code.

11:43.840 --> 11:47.280
Done both working in the grammar engineering is a very high level programming language. That's

11:47.280 --> 11:52.480
sort of declarative linguistic knowledge. But I've also done some work with some of the underlying

11:52.480 --> 11:58.720
algorithms. But I am not, you know, not the person who's going to build or deploy a machine learning

11:58.720 --> 12:07.040
system. Rather, my role in this dialogue about linguistics and NLP has been sort of saying,

12:07.040 --> 12:11.280
okay, I see what you're doing with the machine learning. How does that relate to the questions

12:11.280 --> 12:14.880
that we're asking in linguistics on the one hand? And that was sort of my first entry into it.

12:14.880 --> 12:19.840
Was, okay, you're building systems. I see the input. I see the output. I see you're really

12:19.840 --> 12:23.040
interested in the internals of the system. And that's cool. That's fine. But that's not the part

12:23.040 --> 12:27.760
that I'm working on. Right? I'm interested in the framing of the task, the input, the output,

12:27.760 --> 12:33.680
how it's evaluated. Where did that gold standard data come from? How did you create those annotations?

12:35.040 --> 12:40.400
And then how does that task relate to either scientific questions that someone interested

12:40.400 --> 12:45.440
in say how language works might be interested in? Or how does it relate to the practical

12:45.440 --> 12:50.080
applications that you're selling it as a solution to? Right, right. Yeah, you had an interesting

12:50.080 --> 12:53.920
tweet the other day. I meant to look it up so I could reference it here, but it was something

12:53.920 --> 13:00.560
alone, the lines of, I forget the exact framing, but a lot of your perspective being informed by

13:00.560 --> 13:06.320
the idea that, you know, in NLP, we've got this whole field of linguistics that came before it,

13:06.320 --> 13:10.080
and now we're doing all this cool computational stuff. And that you contrasted that with

13:10.080 --> 13:16.080
computer vision, where maybe we didn't have computer visionology or visionology or whatever the

13:16.080 --> 13:22.320
analogous thing would be. Where were you getting out there? So that came out of a fascinating

13:22.320 --> 13:27.840
conversation that I had with Deb Rajee and Emily Denton, who I only know because we've met

13:27.840 --> 13:31.520
over Twitter. And this is one of the wonderful things about this research environment,

13:31.520 --> 13:34.720
especially now that everyone's stuck at home is like, okay, yeah, sure, I have time to

13:34.720 --> 13:37.520
have a meeting with people who are in very different parts of the country,

13:37.520 --> 13:45.440
cool. And they are interested in the sort of benchmarks and how they're deployed to machine

13:45.440 --> 13:51.920
learning broadly and sort of where, where do they come from? How, and this is also Emily Denton's

13:51.920 --> 13:57.120
working with Alex Hanna on these questions of how does this, how does a benchmark become a

13:57.120 --> 14:03.120
benchmark? Why do people care about some and not others? And I think we all share an interest in

14:03.120 --> 14:07.680
how do these benchmarks relate to the broader thing they're supposed to represent?

14:07.680 --> 14:13.120
Particular benchmarks generally or benchmarks like ethics benchmarks or

14:14.080 --> 14:18.480
So it's a great conversation because all of the people in that conversation really care about ethics,

14:18.480 --> 14:21.280
but that's actually not what we're talking about. So it's always a background and

14:21.280 --> 14:24.560
it's about the, yeah, I was just extrapolating from the names.

14:24.560 --> 14:33.120
So, you know, it's things like, so the super glue benchmarks for natural language understanding

14:33.120 --> 14:37.360
and image net for computer vision and image labeling, right? These become these

14:38.480 --> 14:43.120
tasks that then you try different algorithms on. And so one of the things that came out of that

14:43.120 --> 14:48.400
conversation was a framing that I really like, which is this sort of three-part thing. You have

14:48.400 --> 14:55.120
the task definition sort of in the middle. And on the one hand, you have the data set that the

14:55.120 --> 15:00.640
task is represented by. And on the other hand, you have the thing in the world that the task is

15:00.640 --> 15:07.840
supposed to correspond to, right? So if your task is image labeling and then you can say, okay,

15:07.840 --> 15:13.760
how does the construction of image net actually relate to that task as sort of task internally that

15:13.760 --> 15:18.720
I'm trying to do? But then also, how would that task get deployed? What are we using

15:18.720 --> 15:24.080
image labeling for? And how does this particular conception of the task support or not support those

15:24.080 --> 15:29.760
use cases? So we're talking about things like that. And in the context of that conversation,

15:31.120 --> 15:36.480
I was shocked to hear them say that from their perspective, NLP is better off than other parts of

15:36.480 --> 15:43.520
machine learning because NLP has linguistics, keeping it honest. And I was floored by that because

15:43.520 --> 15:47.680
I feel like, yeah, that's what I'm trying to do. And there's a few of us in there kind of conversation,

15:47.680 --> 15:52.640
but it does not feel like we're generally speaking being listened to, which is maybe a little bit unfair.

15:54.000 --> 15:59.280
I think the dynamic there isn't that people are not listening or ignoring whatever, but it's

15:59.280 --> 16:05.520
rather that the way things are set up right now, NLP for some people serves as an application

16:05.520 --> 16:12.240
area of machine learning, right? So if you're, if what you're interested in is learning algorithms,

16:12.240 --> 16:16.240
and that is a wonderful area of research, it's, you know, I'm glad people are working on it.

16:17.120 --> 16:23.120
In order to test them and refine them and understand, you know, what's good at what you need things

16:23.120 --> 16:28.960
to be learning. And a lot of people who are interested in learning algorithms seem to be interested

16:28.960 --> 16:34.960
in as something that can learn genuinely. So they're interested in applying, you know, RNNs in

16:34.960 --> 16:39.360
different contexts. And so you need different contexts. And it's not just sort of a NLP view of,

16:39.360 --> 16:42.800
okay, different registers, different, you know, are we doing speech recognition or machine translation,

16:42.800 --> 16:50.240
but language is one, vision is one, playing chess, you know, things that you would need in

16:51.120 --> 16:56.880
autonomous driving. Like these are all different application areas. And so we get people coming

16:56.880 --> 17:00.720
into NLP who are really interested in the machine learning, and they're just coming to publish

17:00.720 --> 17:05.440
in NLP venues because they're applying it to NLP. And so there's this constant influx of people who

17:05.440 --> 17:12.320
don't have long or deep training in either linguistics or really thinking about language.

17:13.920 --> 17:19.040
And so I sort of feel like I'm almost like chatting into the void. And then I talk to other

17:19.040 --> 17:23.840
people who say, yeah, but, but at least there is linguistics, like there's a field of study there.

17:23.840 --> 17:30.960
And what would it be for computer vision? And I mean, I guess it's something to do, well,

17:30.960 --> 17:32.880
the thing that's strange about is that, yes, there is.

17:32.880 --> 17:37.440
For thing, no processing is the thing that comes to mind. We study that in DSP classes.

17:38.000 --> 17:42.880
Yeah. And that's used also in speech technology, right, for the speech recognition.

17:42.880 --> 17:47.600
But if you're looking at like trying to do what people do, and you're looking at vision,

17:49.040 --> 17:54.000
there's something around both sort of perception. So how does the, and here I'm talking

17:54.000 --> 17:57.360
way outside my expertise, just want to flag that. This is not anything I don't think about.

17:57.360 --> 18:03.360
How does the, how does the eye and the, and the ocular nerve and all of that, like what happens

18:03.360 --> 18:07.600
when the light hits the retina and what happens, and then what happens in the brain that's processing it,

18:08.480 --> 18:14.960
to create maybe some sort of representation that just has to do with the visual stimulus,

18:14.960 --> 18:17.360
but then that gets connected to categories of things.

18:18.320 --> 18:19.280
Yeah, yeah.

18:20.240 --> 18:26.160
And so it's not just about sort of psychophysics and like perception and stuff like that,

18:26.160 --> 18:31.040
but it's also about categories and ontologies and how we understand our world.

18:31.040 --> 18:36.640
And so I don't think one coherent field of study, like maybe it's different parts of psychology

18:36.640 --> 18:41.520
that they should be talking to. But I don't see that interaction apparently happening,

18:41.520 --> 18:44.240
the way at least NLP and linguistics do talk to each other.

18:44.240 --> 18:50.800
Mm-hmm. Interesting. Interesting. Yeah, when I, when I heard you initially describe the,

18:50.800 --> 19:00.400
kind of how the kind of year take on, you know, machine learning folks doing NLP, it almost sounded

19:00.400 --> 19:06.800
a little bit like the, you know, I guess most recently I've come across this in the context of COVID,

19:06.800 --> 19:10.800
like data scientists running off building models and don't know anything about the

19:11.520 --> 19:17.760
virology. Like does it feel to you as like you might be the virologist and you've got all these

19:17.760 --> 19:23.120
data scientists producing all these models that don't correspond to the thing that you actually

19:23.120 --> 19:30.880
understand? Yes. And, and I've had the same reaction to seeing the data scientists jumping in

19:30.880 --> 19:34.960
around COVID. I'm really glad that you had that panel on that. By the way, those were great

19:34.960 --> 19:42.880
comments that your guests had because there, there seem to be something in CS education, I think.

19:42.880 --> 19:47.120
It's so it's somewhere in the people who are attracted to machine learning or the way we train

19:47.120 --> 19:53.600
them or both that basically says you are problem solvers, here are problems you solve them. And

19:54.960 --> 20:00.320
also the way machine learning gets sold, there's this like packaging around it that's all about

20:00.320 --> 20:06.320
denigrating the work it's supposed to be replacing. And so if we're going to apply machine learning

20:06.320 --> 20:11.040
to NLP, it's because oh, it's far too expensive to do by hand, right? You wouldn't want to hire

20:11.040 --> 20:16.720
somebody to write a grammar when you can just learn a grammar, right? So it's better to do it automatically

20:16.720 --> 20:21.200
and that has a way of devaluing the work of the people who actually understand the shape of the

20:21.200 --> 20:26.160
problem. And so part of my message as a linguist and NLP over these years has been, look,

20:26.160 --> 20:30.080
it's great to do things by machine learning, but if you want to know that you've actually done them,

20:30.080 --> 20:34.960
it has to be in conversation with people who understand the shape of the problem and can see

20:34.960 --> 20:43.440
whether your solution actually works. And so when I see data scientists teaming up with

20:43.440 --> 20:50.240
virologists and epidemiologists and clinicians and saying, hey, I have skills. Where do you need

20:50.240 --> 20:56.400
my help? That's fantastic. When I see people jumping in and saying, I'm going to solve this and

20:56.400 --> 21:05.840
they seem to be on their own, I get really worried. And so I don't know that things like language

21:05.840 --> 21:12.640
models, which have shown incredible, you know, innovation and promise and all this kind of stuff of

21:12.640 --> 21:18.720
late. I don't know the extent to which linguistics were involved in their creation, but they're still

21:18.720 --> 21:24.560
interesting. Like what, how do you kind of parse the fact that innovation is happening and we're

21:24.560 --> 21:31.760
doing, you know, good, interesting, useful, at least in some cases, things with kind of the viewpoint

21:31.760 --> 21:37.840
we previously discussed. Yeah. So there's a saying, I wish I knew who to attribute to that

21:37.840 --> 21:43.920
said refers to the unreasonable effectiveness of n grams. And this is before the transformers,

21:43.920 --> 21:49.440
this is old, right? You can do a lot of useful stuff by just counting co occurrences of words.

21:50.240 --> 21:56.000
And as a linguist, that's kind of a bummer. Because when I look at a string of words, it's like,

21:56.000 --> 22:01.200
yeah, the words occur in some order. That's not the interesting part, but it turns out that if

22:01.200 --> 22:06.400
you have enough data and you count words and look at the orders they occur in, that's really useful.

22:06.400 --> 22:10.720
It's extremely useful for speech recognition. It's extremely useful for machine translation.

22:11.920 --> 22:19.760
It's useful for information retrieval. So there's a lot there. And that's fine. And it's useful.

22:19.760 --> 22:25.920
And it sort of comes back around to this, you have the task. And how does it relate to the world?

22:25.920 --> 22:33.040
Right? So if your goal is better transcription of, you know, open domain, a noisy environment,

22:33.040 --> 22:39.040
and you've set up a task that models that well, and you're honest about which language varieties

22:39.040 --> 22:42.880
are represented, and you don't claim to be solving speech recognition in general when it's actually

22:42.880 --> 22:52.000
speech recognition for a particular variety of English. Exactly. Exactly. That's all fine.

22:52.800 --> 22:58.400
And then there's interesting questions of, okay, why is that working? And there's room for

22:58.400 --> 23:03.600
linguistic knowledge to come in and say, okay, what is it that makes this effective? And also,

23:04.320 --> 23:08.560
let's do some error analysis. When it's not working, is there any pattern to that? Is there anything

23:08.560 --> 23:14.640
about what it's missing by only looking at sentences as strings of words that can predict some of

23:14.640 --> 23:18.800
these things? There's a bunch of really interesting work going on right now under the rubric of

23:18.800 --> 23:25.680
Bertology, looking at the transformer language models and trying to figure out how much linguistic

23:25.680 --> 23:30.160
structure they are picking up by doing this essentially to the language modeling task.

23:30.160 --> 23:37.040
And Bertology, the, when you say that, is that the study of those models, or is it the,

23:38.720 --> 23:43.440
kind of the explosion of related models, Roberta, and things like that?

23:44.160 --> 23:48.400
So, Bertology, I think I usually see it used to refer to the study of the models.

23:48.400 --> 23:53.280
Okay. So, Bert and its kin, how is it that they're working and what is it that they're learning

23:53.280 --> 23:58.400
about language structure? So, there's work by, I can't do this off the top of my head. Sorry,

23:58.400 --> 24:04.160
I could look up references for you, but people do probing tasks. Whether it's a, you know,

24:04.160 --> 24:07.600
we're going to take a slice of this model and we're going to use it as a classifier and we're

24:07.600 --> 24:12.000
going to see if that model just sort of as trained as part of Bert has learned something about

24:12.000 --> 24:16.400
predicate argument structure or has it learned something about syntactic categories or syntactic

24:16.400 --> 24:24.400
dependencies. Interesting. And is that the line of work happening primarily from a linguistic

24:24.400 --> 24:30.880
perspective or primarily from a machine learning perspective, or is it both? So, the folks doing

24:30.880 --> 24:38.080
that tend to be teams of people who are, all of them have an interest in linguistics. I think

24:38.080 --> 24:42.240
primarily they tend to be folks who started off with machine learning training and then learned

24:42.240 --> 24:47.120
the linguistics, some of them have linguistics earlier in their training. And then occasionally,

24:47.120 --> 24:52.160
you'll get people who like bring linguists on as collaborators. So, that's, I mean, that's,

24:52.160 --> 24:57.680
that's sort of what I'm usually pushing for when I'm saying, you know, that you can't do

24:57.680 --> 25:02.320
machine learning without domain expertise. It's not a kind, I could sometimes get accused of

25:02.320 --> 25:06.320
doing gatekeeping, which is not my intention. I think that collaboration is a great, more

25:06.320 --> 25:12.640
perspective is great. And what I'm seeing is a lack of inclusion of the domain expertise perspective

25:12.640 --> 25:19.360
sometimes in the machine learning work. And when you look at something like Bert with the

25:19.920 --> 25:25.760
linguistic perspective, are there, is there kind of a laundry list of obvious things that,

25:25.760 --> 25:31.920
you know, you think need to be done or that, you know, should be looked at or that, you know,

25:31.920 --> 25:37.840
things that, you know, shouldn't have been done like that? Or, you know, I'm wondering, is it,

25:40.240 --> 25:45.680
are you speaking primarily out of kind of opportunity? Like, if we pull domain experts in,

25:45.680 --> 25:51.120
then, hey, maybe we could achieve some, you know, greater thing? Or is it because of the specific

25:51.120 --> 25:58.640
things that you see when you look at Bert? Yeah, yeah. So, I don't, can't say I have specific

25:58.640 --> 26:03.040
examples off the top of my head. And it's not, I mean, this is, this is a, a soap box I've been

26:03.840 --> 26:12.960
creeping from for years now, like, prevert. And in prevert, it's more about the, the claims of

26:12.960 --> 26:16.240
what's happening, right? So, if someone creates Bert and they say, hey, this is helping me do great

26:16.240 --> 26:20.880
speed, right, speed recognition, it's helping me do great machine translation. And then you can,

26:20.880 --> 26:24.400
you know, you can test that. That's a product that's a, you know, sort of a practical application,

26:24.400 --> 26:30.480
and it works better. Excellent. If somebody is saying, hey, Bert understands language,

26:32.800 --> 26:40.640
then that's a scientific claim. And I want to say, okay, show me what your tests are that allow

26:40.640 --> 26:44.080
you to make that claim that it understands language. And also, I can tell you on first principles

26:44.080 --> 26:49.440
that it doesn't. Right. Right. And you talked about this in your paper, climbing towards NLU on

26:49.440 --> 26:55.440
meaning form and understanding and the age of data. Yeah. And the basic premise there is that

26:56.000 --> 27:02.320
Bert isn't at all trained on meaning. So there's no way that it could display, you know, meaning or

27:02.320 --> 27:08.320
understanding of meaning. Yeah. Exactly. Exactly. That was a super fun paper to write with Alexander

27:08.320 --> 27:15.760
Thor. We had a blast. And it came out of actually Twitter arguments. So I had this, I think it's

27:15.760 --> 27:21.680
back in the end of 2018, this unending Twitter thread thread is the wrong word for it. So that

27:21.680 --> 27:27.280
makes it sound very linear. And it wasn't where I was basically stating the thesis of that paper,

27:27.280 --> 27:33.120
which is that if your training is only language modeling. So all of the training data is only form

27:33.760 --> 27:38.960
and your task is to predict the next bit of form or, you know, in Bert style masked bits of form

27:38.960 --> 27:44.160
in the middle, meaning as you say, it wasn't in the input signal. So you're not learning meaning.

27:44.160 --> 27:48.320
And there was an unending parade of people who wanted to pick up the other side of that argument

27:48.320 --> 27:55.760
with me on Twitter. And one new paraphrase, was there a best of those arguments? Because when I've

27:55.760 --> 28:03.920
read that, you know, I have often been in a situation of trying to explain to people, you know,

28:03.920 --> 28:12.800
technical or non-technical, whatever. The extent of these models and what they're really capable of

28:12.800 --> 28:18.560
and what they're not capable of, that kind of thing. And I really appreciated the idea of, hey,

28:18.560 --> 28:24.080
we're not the input, the input is not meaning. It's not understanding. So that's not going to be

28:24.080 --> 28:31.120
what comes out. Yeah. So I don't, I don't have a best of because it is sort of this like,

28:31.120 --> 28:35.200
it's not there. So how are you going to learn it? But it tends to take the form of, well,

28:35.200 --> 28:39.920
what if you give it lots and lots and lots of data, right? Or sometimes the arguments were,

28:39.920 --> 28:45.920
I think if I were to try to argue against that, I would try to say that meaning actually was

28:45.920 --> 28:53.600
there, but not in the form that you're used to seeing it, like in a quality, but rather implicit

28:53.600 --> 28:59.520
meaning as opposed to explicit meaning. Yeah. So that is one shape that the argument sometimes

28:59.520 --> 29:06.560
takes, which is, well, the sentences themselves represent meaning. And that one is, well, yes,

29:06.560 --> 29:11.520
they do, but only if you know the linguistic system behind them so that you can pull it apart.

29:11.520 --> 29:15.680
I was going to say, this is a Sam gets creamed by a linguist's segment of the interview.

29:18.080 --> 29:21.760
So at the end of the paper, we actually have a bunch of the counter arguments.

29:21.760 --> 29:26.960
I'm going to say this, but you're right. And so it's, it's in there. But part of what makes it

29:26.960 --> 29:33.120
difficult is we have to really pin down terms, right? So what do we mean by meaning? And then what

29:33.120 --> 29:37.440
do we mean by language model? Because you can certainly imagine language models that are modeling

29:37.440 --> 29:43.040
meaning as well as form. So we say, hey, language model is the only input data it has is the

29:43.040 --> 29:46.880
form of language. And that could be the written form. It could be spoken. It could be signs and

29:46.880 --> 29:54.000
signed language. And that's, that's all it sees. And meaning is a relationship between those

29:54.000 --> 29:58.800
linguistic forms and something outside of language. And we actually break that down into two parts.

29:58.800 --> 30:03.680
So the sort of biggest relation is between the form that I say and my communicative intent.

30:04.240 --> 30:08.720
I'm trying to communicate something to you. So for example, when the cat went jumping and we're

30:08.720 --> 30:12.160
both laughing, and I'm thinking of the people listening to this, like when they're out running,

30:12.160 --> 30:16.880
like, which is how I usually listen to your podcast, I thought, oh, wait a minute, those people don't

30:16.880 --> 30:21.600
know why Sam and I are both laughing. What's going on? I better say something to clue them in,

30:21.600 --> 30:27.200
right? So I have this communicative intent to make apparent to these people who are only listening

30:27.200 --> 30:32.240
audio that there was a cat in the frame. All right. So that's what I want to get across. And so then

30:32.240 --> 30:35.520
I figured out better podcasts than I am because I were just laughing.

30:38.640 --> 30:44.800
I actually have a lot of experience with online teaching. We've been doing our classes, not just

30:44.800 --> 30:49.920
recently, in the master's program, we've had classes in a hybrid online in person format since 2007.

30:49.920 --> 30:55.280
Oh, wow. So I've long years decades, almost experience with this now.

30:55.280 --> 31:01.920
All right. So that goes flying. I want to get that across. So then I think, okay, how can I convey

31:01.920 --> 31:09.120
that to somebody who's listening? And then I pick a string of words that doesn't directly contain

31:09.120 --> 31:14.400
catflies over my shoulder, right? It doesn't, it doesn't, it's not a picture. It's a string of words.

31:15.680 --> 31:21.680
To provide a clue to the listeners that would allow them to reconstruct my communicative intent.

31:21.680 --> 31:27.840
And so the thing about language is that it is this collaborative experience. And interesting,

31:27.840 --> 31:31.840
like you and I are doing some collaboration right now, right? And we're doing a lot in terms of

31:31.840 --> 31:36.080
the general communication stuff, the turn-taking. It helps that we can see each other, right? It makes

31:36.080 --> 31:40.800
the turn-taking race smoother. But we're also collaborating with our listeners. So people who are

31:40.800 --> 31:48.240
not here in this present moment who are listening later, hi people. They are, they are working with

31:48.240 --> 31:55.600
us across that time to work out, okay, given what I know about English and what these sentences can

31:55.600 --> 32:02.560
conventionally mean. What is Emily trying to get across to me as a listener? What is Emily trying

32:02.560 --> 32:07.920
to get across to Sam in the moment and vice versa? So that's what we're saying meaning is about.

32:07.920 --> 32:12.240
And hopefully that makes it clearer to people that it's not there if all you have are the pixels on

32:12.240 --> 32:21.040
the page. Are you aware of any efforts that are trying to get there? I mean, I'm envisioning

32:21.040 --> 32:25.280
something like, I mean, I guess we're all trying to get there or everyone that's in this research

32:25.280 --> 32:30.000
field of NLU is trying to get there in some way. But more specifically, I mean, I'm envisioning

32:30.000 --> 32:36.000
something like a cross between a, you know, a bird that's kind of anger and focused and a

32:36.000 --> 32:42.000
normal machine translation that is pairing, you know, meaning and sentences. Anything interesting

32:42.000 --> 32:46.960
happening there? So, yes. And I think that a lot of the ways in which Burke gets deployed,

32:46.960 --> 32:51.120
you have the pre-training, which is just language modeling. And then you have the fine tuning,

32:51.120 --> 32:55.680
which is giving a tiny little signal about the kind of meaning that's relevant for the present task.

32:56.960 --> 33:01.600
And so it's there in that sense. There's a wonderful paper, this paper by Jonathan Biskin

33:01.600 --> 33:11.920
colleagues, is talking about sort of the stages of the world scope of NLP. So really early on,

33:12.720 --> 33:18.800
you might have had just like hand constructed word lists. And then you got to corpora. So things

33:18.800 --> 33:25.440
like the pen tree bank, which has Wall Street Journal plus the brown corpus in it. And that's

33:25.440 --> 33:32.480
a couple million words of text. And then you get to like web scale. So enormous corpora.

33:33.360 --> 33:38.080
But it's all that is still just form. And then they're saying, well, where can we go next? And so

33:38.080 --> 33:45.840
there's, there's starting to be work on embedding the, the naturalization processing system in some

33:45.840 --> 33:52.000
sort of embodied context where you are talking with a robot. And the robot has to figure out

33:52.000 --> 33:58.960
what motions in the world it should be taking. So there's sort of, there's small amounts of text,

33:58.960 --> 34:06.480
large amounts of text, text plus embodied interactions and grounding in things like vision and

34:06.480 --> 34:12.480
whatnot. And then the biggest one is adding in the social. And so I think that's a really

34:12.480 --> 34:18.880
fantastic vision. And I'm super excited. This is not actually an ACL paper. This is a preprint

34:18.880 --> 34:22.560
right now. I think they're going to keep working on it and publish it somewhere else later. But ACL

34:22.560 --> 34:27.440
has this wonderful theme this year. ACL is the association for computational linguistics.

34:27.440 --> 34:33.440
Supposed to have been in Seattle in July. And I'm sad that we don't get to host people in person.

34:33.440 --> 34:38.640
They'll be online. And the theme session is taking stock of where we are and where we're going.

34:39.280 --> 34:43.040
And so there's a bunch of papers sort of looking at. Interesting. You know, how do we go in a lot

34:43.040 --> 34:46.960
of them seem to be focused on meaning and semantics. So people are thinking about it.

34:46.960 --> 34:53.120
And do you think that that particular theme is driven by the kind of recent progress with

34:53.120 --> 34:57.840
Bert and the like or is that is something that we talk about at these conferences every once in a

34:57.840 --> 35:05.440
while? I guess I'm trying to get a sense of, you know, does it feel from a linguistic and

35:05.440 --> 35:12.880
computational linguist? I guess as a linguist. And then maybe the next level is computational

35:12.880 --> 35:19.920
linguist. Does it, does it? Do you have the same feeling of excitement around what's happening,

35:19.920 --> 35:25.360
you know, with Bert and language models and the like as, you know, folks from a machine learning

35:25.360 --> 35:30.480
perspective have? Because I think from a machine learning perspective, it's kind of akin to

35:31.200 --> 35:36.080
2012 with, you know, deep models and computer vision. It's like, wow, we're making a lot of

35:36.080 --> 35:43.840
progress here. Or is it just trade? So it's actually as a computational linguist in NLP,

35:43.840 --> 35:51.280
it's kind of shouting for it actually. Because when I when I entered the field,

35:52.320 --> 35:56.800
the what I was doing was considered very old school, right? So I came into computational

35:56.800 --> 36:02.960
linguistics in, you know, early 2000s working on grammar engineering and that was already the

36:02.960 --> 36:07.520
height of statistical processing in NLP. And so everyone would frame their papers as well,

36:07.520 --> 36:11.040
you know, in the olden days, people would have to do this by hand. But now we can do it

36:12.320 --> 36:15.360
and I'm like, some of us are still here, still doing it by hand. And you know,

36:16.080 --> 36:20.400
that's okay to have both. Like we don't have to, you know, this room, this should be, you know,

36:20.400 --> 36:26.400
the world is big enough for all of us, right? And then when deep learning sort of swept through

36:26.400 --> 36:31.760
NLP, starting in about 2015, 2016, a bunch of the statistical learning people who have built up all

36:31.760 --> 36:35.680
this expertise around feature engineering were told, Oh, no, no, no, you don't have to do that by

36:35.680 --> 36:39.680
hand anymore. We can do that automatically. And so that's where the shouting for it comes in a

36:39.680 --> 36:50.400
little bit. So it's not, there was a period where it got a little boring because basically,

36:50.400 --> 36:55.120
you had all these existing tasks and turns out you could build those the model for it. And so

36:55.120 --> 37:00.720
there's all these papers that basically take existing tasks and first word embeddings in general

37:00.720 --> 37:03.840
from like word to back or something and then bird into the mix. And hey, look, say to the art

37:03.840 --> 37:08.560
paper. And it's like, does anybody actually enjoy reading these? Like,

37:11.120 --> 37:14.960
what do you get out of this? Right? That was my reaction to that. So,

37:14.960 --> 37:20.320
Bertology is really welcome. The people who start saying, okay, but why? Right? What is it about

37:20.960 --> 37:24.880
the pairing of this methodology and this task that makes such a big difference? That's interesting.

37:25.520 --> 37:30.000
And sometimes it is, hey, look inside of the deep neural net, you can see that this layer is

37:30.000 --> 37:34.800
picking up this kind of information. And then that sort of opens up questions. Okay, so why is

37:34.800 --> 37:38.960
that information available just from distribution and text? Those are scientifically interesting

37:38.960 --> 37:45.120
questions to me. And in other cases, there's a great paper by Niven and Kau at ACL 2019 that said,

37:46.000 --> 37:53.520
actually, guess what? Bert is cheating. And we can construct a data set that sort of hides

37:53.520 --> 37:58.960
the clues that Bert was using that were just artifacts. And then it falls to chance. Really? Yeah.

37:58.960 --> 38:03.040
Yeah. Interesting. Can you elaborate on that a little bit more? I haven't come across that paper.

38:03.760 --> 38:09.600
So this or maybe I should just interview the. Yeah, that would be great. I recommend it.

38:09.600 --> 38:14.480
But in brief, my non-author non-expert understanding of it was that there was,

38:15.680 --> 38:21.040
this was a question. I think it was, it was like textual entailment or reading comprehension or

38:21.040 --> 38:29.600
something. And the way the task was set up, the there was artifacts and the data that made particular

38:29.600 --> 38:37.520
words really good cues for a certain kind of answer. And if you construct a data set that washes

38:37.520 --> 38:44.080
that out, then those cues are gone. And Bert doesn't do well at all. So it wasn't necessarily a

38:44.080 --> 38:48.720
general statement across all tasks. It was for a particular task. For a particular task. Yeah,

38:48.720 --> 38:53.040
exactly. No sort of saying, hey, look, say to the art with Bert, yay, but wait a minute, that makes

38:53.040 --> 38:56.800
no sense. Why should Bert be doing well in this task? Let's look deeper. And for that task,

38:56.800 --> 39:01.920
they found that problem. Okay. Interesting. Yeah. And I think that that kind of result

39:01.920 --> 39:08.080
informed the design of super glue. So Sam Bowman and his colleagues are, there was the glue

39:08.080 --> 39:11.600
task that had to do with a language understanding. And then super glue was, okay, let's bring together

39:11.600 --> 39:16.880
more tasks, but let's pick the ones that are hard for Bert. Okay. So I think that's a that's a

39:16.880 --> 39:22.320
promising way to try to push towards tasks that better represent the questions that we're interested

39:22.320 --> 39:27.200
in. But you asked, you know, why do we have this theme at ACL? And I think you really have to

39:27.200 --> 39:33.360
ask the program chairs. So that's Joyce, Chai, Natalie, Schluter, and Joel, Tetral, who are the

39:33.360 --> 39:38.080
incredibly hardworking program chairs for this conference. And early on, they said, hey, it will

39:38.080 --> 39:43.120
make this more interesting if we solicit papers on this theme. If we actually encourage people

39:43.120 --> 39:48.320
to sort of lift their heads up and look a little bit more broadly at what's going on here. And

39:49.600 --> 39:54.000
I think I recall them saying that that was actually because of the date, basically. It was like,

39:54.000 --> 40:00.240
hey, 2020 round number, it's time to like sort of take stock and think about it more broadly.

40:00.240 --> 40:11.280
Got it. Got it. Interesting. So I'm trying to think if we've kind of fully captured the relationship

40:11.280 --> 40:16.400
between linguistics and computational linguistics, or maybe that's not possible to do it now, we certainly.

40:19.840 --> 40:25.040
Yeah, probably not possible in an hour, but I think that there's there's lots of ways in which

40:25.040 --> 40:30.320
linguistic knowledge can inform work on NLP. And it doesn't have to be directly in terms of encoding

40:30.320 --> 40:33.760
the linguistic knowledge in the system. And when I do grammar engineering, that's what I'm doing.

40:33.760 --> 40:38.720
And I think that's a valid mode of research, but it's not the only mode of research. But it's also

40:38.720 --> 40:43.360
not the only mode of research that needs linguistic knowledge. So if you're doing statistical

40:43.360 --> 40:47.520
machine learning, or if you're doing neural processing for NLP, there's still room for linguistics

40:47.520 --> 40:51.520
on the one hand in feature design. If you're doing the kind of thing, we're not trying to learn

40:51.520 --> 40:57.920
your features automatically. But especially in task design and error analysis and an understanding

40:57.920 --> 41:05.280
sort of the task data set connection and the task world connection. And we barely talked about

41:05.280 --> 41:10.240
the ethics stuff at all, but there's a connection there too, because one of the things that

41:10.240 --> 41:14.160
linguistics tells us from social linguistics is that variation is the natural state of language.

41:14.160 --> 41:21.200
Number one, number two, the fact that a variety is anointed as the standard is all about power

41:21.200 --> 41:26.000
and nothing else, nothing inherently variety. And so that means that if you've got technology that's

41:26.000 --> 41:31.520
working for the standard variety and not others, then you are likely to be setting up a situation

41:31.520 --> 41:36.160
where you're further disempowering, further disempowering marginalized people. So there's a lot

41:36.160 --> 41:41.760
that linguistics can do to inform the ethical deployment of NLP, especially social linguistics.

41:42.640 --> 41:49.120
And is your research in this area looking at the, I know a big part of it is trying to understand

41:49.120 --> 41:56.080
how to engage with people around this idea of ethics in NLP. Are you also looking at the kind

41:56.080 --> 42:02.400
of fundamental problems themselves? So the fundamental problems are broad and diverse and

42:03.920 --> 42:11.280
so I am interested in understanding like as we sort of get these case reports of what's

42:11.280 --> 42:15.440
what's gone wrong or what might go wrong, I'm always interested in how that connects the

42:15.440 --> 42:20.640
language variation. So when I think I mentioned to you beforehand that I've got some unpublished

42:20.640 --> 42:27.280
work trying to do a typology of the kinds of risks of adverse societal impact of NLP,

42:27.280 --> 42:30.480
and that is very much informed by what I know about language variation.

42:31.600 --> 42:35.120
So yeah, so working on that, but also working on, yeah.

42:35.120 --> 42:38.000
Does unpublished mean that you can't talk about it yet?

42:38.000 --> 42:40.080
No, it just means that I can't point to anybody to it.

42:43.280 --> 42:46.480
When you, how do you, what are some of those categories?

42:46.480 --> 42:52.960
So what I'm looking at it in terms of direct versus indirect stakeholders, and this comes

42:52.960 --> 42:58.080
out of value-sensitive design. So work about your freedom and colleagues sort of conceptualizing

42:58.080 --> 43:01.600
the people that we need to worry about as both the users, the people who actually interact with

43:01.600 --> 43:06.960
the technology directly, and other people who are affected. So we're taking that as the overarching

43:06.960 --> 43:11.520
thing and then thinking about the different ways that you can interact with a system as a direct

43:11.520 --> 43:15.200
or indirect, so the direct stakeholders might be doing it by choice or not by choice.

43:15.200 --> 43:20.320
And then the indirect stakeholders, and it could be, well, my words are part of a broad

43:20.320 --> 43:26.080
corpus, or I am a person who's subject to societal stereotypes that the technology might be perpetuating.

43:27.360 --> 43:30.320
Or, dang it, I got a third one over there right now that I can't do off top of my head,

43:30.320 --> 43:33.760
but that's the kind of category. And none of those are specifically about

43:33.760 --> 43:37.120
sociolinguistic variation, but rather within each of those, I say, okay,

43:37.120 --> 43:40.960
if I'm a direct stakeholder choosing to use something, but it doesn't represent my language

43:40.960 --> 43:45.840
variety well, what happens. So that's sort of an example of that.

43:47.040 --> 43:53.360
So the speech recognition does not work well for all Englishes.

43:54.560 --> 44:00.320
And so if I'm trying to use a virtual assistant and it doesn't recognize my variety,

44:00.320 --> 44:04.640
or it only recognizes the variety that I code switch into usually outside the home,

44:04.640 --> 44:08.640
but I have to use it inside the home with this virtual assistant. How does that make me feel

44:08.640 --> 44:14.080
about my language and how I fit into the world, right? Yeah. So there's the things like that.

44:15.520 --> 44:19.840
So I felt like I was going to go somewhere. So sociolinguistics informs that.

44:20.880 --> 44:24.640
And I think that that's an important kind of linguistics that needs to be in the

44:24.640 --> 44:29.440
conversation and visible to people who are working on machine learning and NLP. And I think it's

44:29.440 --> 44:33.840
pretty remote. It's not the sort of thing that maybe people even necessarily know about. They

44:33.840 --> 44:40.080
may have heard of linguistics and they give linguistics as like Chomsky maybe. So.

44:42.160 --> 44:47.520
And so when you talk about kind of techniques for getting folks to engage in these kind of

44:47.520 --> 44:52.480
discussions and think about these issues, are you primarily

44:54.480 --> 45:00.480
thinking about or targeting kind of the laypeople or researchers and builders?

45:00.480 --> 45:06.480
And so both. And I should I can't give you a pointer to a video of a talk that I gave on this.

45:06.480 --> 45:12.160
And it sort of ends with whose job is this, right? And I've got a bunch of categories of people.

45:12.160 --> 45:17.120
And we all fill multiple categories. So the first one is the researchers and developers, right?

45:17.120 --> 45:22.240
Then there's consumers. So when we use a product, we either buy it or we just choose to use it

45:22.240 --> 45:28.080
in a, like, you know, pain with our eyeballs kind of a way. The third one is for cures. So people

45:28.080 --> 45:34.560
who decide to buy and install and stand up a system based on NLP or machine learning technology

45:34.560 --> 45:39.920
more broadly, I suppose. Then we have the role of members of the public who are advocating for good

45:39.920 --> 45:44.160
policy and then we have policy makers. Okay. So you can see that any individual might be in multiple

45:44.160 --> 45:50.800
ones of those roles. And I think it's important to sort of get this knowledge out in all of those ways.

45:50.800 --> 45:58.400
And so that includes, you know, public engagement, things like this, right? It includes teaching.

45:58.400 --> 46:04.160
So I'm running a tutorial at ACL this summer with their coveans and a scotheld on integrating ethics

46:04.160 --> 46:10.080
into the NLP curriculum so that people hopefully early on in their training will start to see this

46:10.080 --> 46:17.680
stuff. And then yeah, talking about it sort of on Twitter, say. How well do you feel the

46:17.680 --> 46:26.800
core, like failure modes, ethical failure modes of NLP, you know, not even a mentioned machine learning,

46:26.800 --> 46:33.120
but NLP are understood. Like I think we throw around the example, like these word-to-vec examples,

46:33.120 --> 46:38.800
you know, man is to doctor as woman is to whatever. You know, I think we understand those failures

46:38.800 --> 46:43.680
pretty well or at least conceptually. But I'm trying to think of other ones that I've heard

46:43.680 --> 46:50.160
thrown around and they're very few. Yeah. So, so they're right. So you have one failure mode,

46:50.160 --> 46:53.600
failure mode, I'm not so it's one of these like it's actually working as intended and we don't

46:53.600 --> 46:59.360
want that kind of failure mode, right? So of these, when the word vector is feature not a bug,

46:59.360 --> 47:05.840
you commented on that podcast recently. I love that episode. Yes, exactly. So it's like that.

47:05.840 --> 47:09.760
It's a feature not a bug because it's doing what we told it to do and maybe we need to rethink

47:09.760 --> 47:15.600
what we're telling it to do, right? Because there is an awful lot of bias in our conceptualization of

47:15.600 --> 47:19.360
the world that comes out and how we talk about the world. And then if you say learn the patterns,

47:19.360 --> 47:23.600
guess what? That's one of the patterns, right? And it's not even necessarily a pattern in the

47:23.600 --> 47:28.080
world that it's learning. It's a pattern in how we talk about the world. And I think that a

47:28.080 --> 47:33.440
mistake that often gets made in these discussions of machine reading, naturally understanding,

47:33.440 --> 47:38.800
is that the text is the world and it's not. It's how some collection of people decided to talk

47:38.800 --> 47:44.880
about the world. So there's a further distinction there. So that's a quote unquote failure mode.

47:45.680 --> 47:49.920
You have cases, right? I mean, it's right because it's a feature not a bug, but we don't want a feature.

47:51.360 --> 47:58.400
You have cases where the system just doesn't do the thing it's supposed to do, right? So

47:58.400 --> 48:03.840
speech recognition and it gives you gibberish instead of what the person said or speech recognition

48:03.840 --> 48:09.120
and it just doesn't give you any words for a little while. Machine translation and it gives you

48:09.120 --> 48:14.160
something that maybe sounds perfectly fluent because the language model is doing its job,

48:14.160 --> 48:20.160
but it's a bad representation of the source language intent. And that's I think a particularly

48:20.160 --> 48:26.880
insidious kind of failure mode because you don't know that it's failed. It looks fluent. And so if

48:26.880 --> 48:30.640
if as a consumer of that technology, you're not savvy to the fact that it's just making a guess,

48:30.640 --> 48:36.000
you might believe that the person whose words you've translated actually said the thing that you're

48:36.000 --> 48:45.920
reading when in fact they didn't. Interesting. And then so what I'm hearing here is you know,

48:45.920 --> 48:53.520
you're saying things that sound like systems or models just breaking. But then when you take those

48:53.520 --> 49:04.160
and put those, you know, let's call them technical failures into the world, they are having effects

49:04.160 --> 49:09.120
and that creates ethical issues, but they're also potentially caused by things like the

49:10.080 --> 49:14.320
style of language that someone's using or accents or things like that. And that also creates

49:14.320 --> 49:23.600
ethical issues. Yeah. So the general framework that I am working towards for thinking about this

49:23.600 --> 49:28.800
is basically whenever we are moving away from this is a fun abstract toy that we're playing with

49:28.800 --> 49:32.880
to this is something that's going to be in the world. We have a responsibility to think about a few

49:32.880 --> 49:38.000
things and one of them is what are the failure modes? That's a question you're asking. Another one

49:38.000 --> 49:45.920
is when this is working as intended, who benefits and who's possibly harmed? And when it's not working

49:45.920 --> 49:51.360
as intended, who was harmed? Because those harms are not going to be evenly distributed

49:53.040 --> 49:56.320
in many, many cases. So this thing about social linguistics and the fact that people speak

49:56.320 --> 50:00.240
differently and that people who are marginalized tend to then be told that their language

50:00.240 --> 50:04.720
variety is not standard and it's not the one that's getting modeled means that when the system fails

50:04.720 --> 50:09.280
to work because of a difference in in accentual linguistic variety, that's going to fall differently

50:09.280 --> 50:15.840
across different groups of people. But I have to always when I get down this part of the line of

50:15.840 --> 50:22.080
reasoning, refer to Alvin Grissom's talk that I heard last summer where he said, you know,

50:22.080 --> 50:27.120
sometimes working as intended is really not anything we want. And so sometimes it's better to

50:27.120 --> 50:31.600
have it not work for you, right? If it's being used for surveillance, then you're happy when it

50:31.600 --> 50:36.800
can't understand you, provided that it's not understanding you just means it says there's

50:36.800 --> 50:43.520
nothing here, as opposed to misunderstanding and then attributing, you know, bad intent when

50:43.520 --> 50:50.080
you've said something completely innocuous. Right, right. I'll refer folks to my interview with

50:50.080 --> 50:56.880
Alvin from February of last year, mythologies of neuromodils and interpretability.

50:56.880 --> 51:06.400
Great, great talk. So when you talk about, again, kind of going back to this promoting engagement

51:06.400 --> 51:16.960
around these issues, have, you know, part of the challenge I think is, you know, generally getting

51:16.960 --> 51:22.960
folks to care about impacts on classes of people that don't involve them. Like, is that something that

51:22.960 --> 51:29.440
you, you know, study or at least study the people that are studying that and can kind of talk

51:29.440 --> 51:36.480
about how folks are thinking about that or who we should look to. So I don't have the expertise

51:36.480 --> 51:40.320
there. I can tell you just what I've learned by teaching this stuff. So I can tell you sort of my

51:40.320 --> 51:44.480
own experience reports about talking to people about this, but you're absolutely right that we

51:44.480 --> 51:50.160
should be looking to, you know, people who know about movement building and about sort of bringing

51:50.160 --> 51:57.680
people in towards working towards shared causes like that. And that's, that's not me. But certainly,

51:57.680 --> 52:01.920
like there was a wonderful podcast episode of Radical AI with Ruha Benjamin on and she had wonderful

52:01.920 --> 52:06.320
things to say about how to connect with existing work going on in the community rather than thinking

52:06.320 --> 52:11.360
you're the one solving it. So I think there is expertise out there that should be called on.

52:11.360 --> 52:18.240
What I've seen is that most people that I talk to and, you know, sample bias, I tend to talk to

52:18.240 --> 52:25.680
people who are interested in this. Both people that I talk to would like to be sure that the

52:25.680 --> 52:31.680
technology they're creating is making the world a better place and not causing harm and sort of

52:32.720 --> 52:37.120
feel uncomfortable with engaging with these issues. And so it's easier to just not think about it.

52:37.120 --> 52:42.560
It's easier to say, oh, I'm only working on the algorithm. And so one strategy is to say, well,

52:42.560 --> 52:46.000
what can you do to make it a little bit better? And that's what we're going with the data statements

52:46.000 --> 52:51.920
work is if you make clear documentation of what's in the data, then you have set it up so that people

52:51.920 --> 52:56.800
can say, hey, guess what? This user is not a good match for my use case. And so that's sort of a

52:56.800 --> 53:01.360
a positive step that's also close to our own research expertise, this dataset creators,

53:01.360 --> 53:06.880
and ought to be close to a research expertise as system developers who are using the data, right?

53:06.880 --> 53:11.600
You'd better understand what's in that dataset that you're using to test the machine learner if

53:11.600 --> 53:16.880
you're going to claim that you're solving some task. So all of that I think is very in domain

53:16.880 --> 53:24.080
and a positive step. And I have my hope, and this is now just like, you know, pure speculation and

53:24.080 --> 53:29.920
hope, is that when people feel like they can do something like this, they then feel more attached

53:29.920 --> 53:36.240
to the longer term goal of making sure that they're building things that don't exacerbate inequities

53:36.240 --> 53:43.760
and maybe actually start being a force for good overall. Yeah. And you just mentioned the data

53:43.760 --> 53:49.040
statements, right? When I haven't looked into that in detail, but when I hear that, envisioning

53:49.040 --> 53:58.000
something very similar to the data sheets for datasets work, the Timnick Ebru is done.

53:59.040 --> 54:03.920
Similar to that, what are the key differences? Absolutely similar. And in fact, it was sort of

54:03.920 --> 54:08.960
like there was something in the air in 2018. So you have Timnick's group doing the data sheets,

54:08.960 --> 54:13.200
Meg Mitchell's group doing model cards. And there was another group at the MIT Media Lab doing

54:13.200 --> 54:19.200
something called nutrition labels for datasets. And yes, absolutely very similar ideas. I think the

54:19.200 --> 54:24.480
thing that distinguishes data statements is that it was very focused on language datasets. So

54:24.480 --> 54:28.560
we're thinking about what do you need to document about a language dataset. The data sheets work

54:28.560 --> 54:34.320
is absolutely wonderful in terms of thinking about what do you need across machine learning datasets

54:34.320 --> 54:40.960
broadly and very, very congruent. And I think I mean, maybe Timnick and Meg were talking to each

54:40.960 --> 54:43.920
other, but there was sort of this moment we all kind of looked around and went, oh same.

54:47.280 --> 54:52.320
So yeah, absolutely, absolutely that sort of the same idea. Got it. Got it. Awesome. Well, Emily,

54:52.320 --> 54:57.600
it has been so wonderful having an opportunity to chat with you. Any parting thoughts

54:57.600 --> 55:02.640
that you'd like to leave us with? This has been a blast. I'm sorry, we only saw the cat the

55:02.640 --> 55:11.280
once. And I really, you know, I hope that your listeners are remain interested in both how do you

55:11.280 --> 55:16.480
create a clever machine learning solution? And how do you think about how it fits into the world?

55:16.480 --> 55:20.400
And if that second part is hard, then who do you need to talk to to figure out how it fits into

55:20.400 --> 55:29.040
the world for the particular task that you're working on? Great. Thanks so much. Thank you. All right,

55:29.040 --> 55:34.400
everyone, that's our show for today. To learn more about today's guest or the topics mentioned in

55:34.400 --> 55:40.880
this interview, visit twimmelai.com. Of course, if you like what you hear on the podcast, please

55:40.880 --> 55:47.120
subscribe, rate, and review the show on your favorite pod catcher. Thanks so much for listening

55:47.120 --> 55:53.120
and catch you next time.

