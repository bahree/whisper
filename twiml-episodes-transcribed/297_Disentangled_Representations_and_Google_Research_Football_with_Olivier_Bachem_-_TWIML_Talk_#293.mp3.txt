Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington, Hey Twimble listeners, if you're a fan of our AI
platforms coverage and especially if you download our first AI platforms ebook, Kubernetes
for machine learning, deep learning and AI, then today is your day.
While we've been working tirelessly on Twimblecom, I have also been working on Book 2 in the
series, the definitive guide to machine learning platforms.
And I am happy to say that it is finally available to download now.
We created the book as a resource for ML, AI and data science leaders and innovators
to help guide their efforts in scaling and industrializing machine learning and deep learning
in their organizations.
In this book, we address questions such as why invest in increasing your organization's
capacity to deliver machine learning and deep learning models.
What are the key barriers to delivering ML and DL models?
How of organizations like Facebook, Airbnb and LinkedIn overcome these challenges and
how can their learnings be applied to your organization?
What are the state of the art, machine learning platforms and tools and how can you put
them to use?
How can you develop an AI platform strategy to support your organization's goals?
To access the definitive guide to machine learning platforms, visit Twimbleai.com slash
AI platforms.
And now onto the show.
All right, everyone.
I am on the line with Olivier Bachem.
Olivier is a research scientist at Google Brain.
Olivier, welcome to this week at Machine Learning and AI.
Hi, Simon.
Thanks for having me here.
Yeah, absolutely.
I'm excited to learn more about the environment that you worked on.
It's called Google Research Football.
We'll dive into that.
But before we do, I'd love to explore your background a little bit.
You did your PhD in large-scale clustering problems.
Tell us a little bit about that, how you got to that and what you've been working on
more recently.
Yes.
As you said, I did my PhD at Dieter Suryk on large-scale clustering.
So one thing, I can't be the main question that I consider my PhD is, how can we go and
apply clustering methods such as k-means clustering to very large data sets?
And I essentially worked on two major topics.
One was called, or is called core sets where the idea is you have a very large data set.
That you want to run your machine learning or your clustering algorithm, not on a full
data set, but on a smaller set because it might just take too long if you have billions
of data points.
And now core sets, they're a subset of your original data that you can find very efficiently.
But that when you train on this smaller subset, you get solutions that are proofably competitive
as if you would have trained on the full data set.
That was one topic, and the second one was related to k-means clustering as well, where
many people use an algorithm called k-means++, which is a smart way to initialize a clustering
algorithm.
And we worked on ways to make it much faster by, again, only looking at parts of the data
or maybe looking at it once or twice instead of multiple times.
Cool.
And when you talk about the core sets and having provably competitive results is that an
exercise in structuring your sample subset so that it has similar statistical properties
as the larger data set.
It is, to some point, it is also about finding the data points that are very important.
Turns out in many machine learning problems, but in particular in clustering, you can
have billions of data points, but some data points, maybe in regions where you have very
few, not a lot of data, and now you want to capture this data because it can contribute
a large part to the area of your model, whereas in regions of the space where you have a
lot of data, you want to take several data points and kind of group them together.
So I would say that's the intuition behind the approach, but then you have to kind of
do this properly in order to actually get the article guarantees.
Okay.
This sounds like in that regard it's somewhat related to active learning.
Well, yes and no.
I think the yes part is it's about finding important points here.
The key idea is you want to do this, I mean, here we're doing this essentially in an unsupervised
setting, right?
If you're doing clustering, you don't get labels, but it's more about actually which data
do you want to look at more closely?
Right.
And I make conceptually as opposed to in practice.
Yes.
Yeah.
And so that was your PhD, what sparked your interest in doing a PhD in machine learning at
all?
Yeah, so I have a bit of a non-traditional background, I did my undergrads in economics.
I worked in different companies.
I did a masters in quant finance, worked in some startups to the statistics master.
And during my masters in statistics, I ran across these cool algorithms where you could
solve statistical problems in a very applied way.
I thought this was pretty cool and immediately after my masters, I thought it would be cool
to do a PhD in the Explorer.
All right.
Cool.
And what are some of the things that you've been working on at Google Brain?
Yes.
So at Google Brain, I've done quite a lot of different projects, which I thought was really
cool, but I would broadly group them into three different categories.
So in the last year, I've mainly led a big effort on research into learning what's
called disentangled representations.
Tell us a little bit more about that, what are disentangled representations?
Yes, so the key idea is that in many settings in machine learning, you see a lot of high-dimensional
observations, such as images or video or audio, but that these observations are not truly
high-dimensional, but that they are the result of a lower-dimensional set of ground-truth
factors of variation.
And now, when you do representation learning, you essentially want to capture a learning
function that takes such a high-dimensional observation and captures some form of information
about that observation.
And disentanglement or disentanglement representations is one property that you might want to get
when you're doing representation learning, which is you want to capture these different
factors of variation in your actual representation.
So one example would be if you have an image with an object in the middle, right, or somewhere
on the image, you could say, for example, one of the factors of variation might be there
is an object at some position, with some size, with some color, and you want to capture
each of these properties independently in representation.
And can you give us a sense of within the realm of disentanglement representations, what's
kind of the state of the art, what are the key tests and research?
Yeah, that's actually a pretty cool question, because this is essentially the question I
asked myself about the year ago, when I started working on this topic, there was a lot of
papers on this topic doing new methods, new metrics, and we started a project actually
benchmarking all of these methods.
It's called a result in a paper called challenging common assumptions in the unsupervised learning
of disentanglement representations, where we actually found that when you actually train
a lot of these models, and we did a large scale study with train more than 10,000 such
disentanglement models, we found that a lot of things happen that are quite surprising.
Now the approach that we took is going to be, we thought of it, okay, what's the approach?
If we are a new researcher, we get a new dataset, we want to find a disentanglement representation,
what are the questions we actually have to answer?
And as you mentioned, I guess the first question you want to answer is which model should
I use, right?
What is the state of the art model?
The second question is how would you select model parameters, hyper parameters to actually
train your model, and thirdly, you might not just train one model, but many different models,
and you might have to select the model that you actually want to use.
And what we kind of found is actually very interesting that the model that you use is
not that important.
It seems that all the methods that were proposed, and when you do a large scale evaluation
on many dataset with many metrics to measure disentanglement, you kind of find out that all
of them can give you very good disentanglement representations at the same time.
So yeah, having said that, if you, what we also found is, it's very hard to select hyper
parameters, because one of the hard parts in learning disentanglement representations
is that when you train these models and you evaluate them, you know what the ground truth
factors of variations are.
But many of these methods that you actually want to use, the call assumption, the whole
point of using them is that you don't know the ground truth factors of variation.
So when you do the training, you have to be kind of strict, you're not allowed to look
at the labels, because that would become cheating in a sense, because in a settings where
you actually want to use such a model, you don't have access to these labels.
So one of the hard parts is it's very hard to select good hyper parameters, because you
cannot compute the disentanglement scores when you're off to it's going to use it.
It turns out models are quite sensitive to hyper parameters, which may not be, may not
be surprising.
There is some hope that you can transfer hyper parameters across different data sets,
but that brings you kind of to the last point, right, how do you actually select a model
when you've trained many models with the same hyper parameters, what turns out there
is even for a given model and the given set of hyper parameters, you can get vastly different
results.
You can, if you train 100 models, you're going to get some that are very, well, doesn't
tell.
You have some that are completely entangled.
And the hard part is without looking at the labels or without looking at the models
yourself as a human, it's very hard to say which of these models you should be using.
We looked at different strategies on how to do this and maybe to illustrate with a kind
of comparison with the best thing that we could find is even if you kind of do a smart
strategy of finding good hyper parameters, you're going to beat a random model across
all the models I've trained only slightly more than at the coin flip.
So slightly more at 50, than 50%, so I mean 60%.
Well, when I hear you describe that work, I hear a couple of different things.
One, I hear this like core focus on the disentanglement versus entanglement, which I'm
kind of conceptualizing as like an orthogonality of these different, I don't know what you'd
even call them, not models, but spaces representation.
Yeah, yeah, yeah, thank you.
Also in the example you gave where you've got an image and the disentangled representations
are, you know, an object and color and size and things like that.
It almost starts to sound like a holy grail of AI or AGI where you're like, there's true
intelligence there.
And the question that comes to mind is, you know, to what degree are these disentangled
representations generally that explainable or aligned with our intuition and how we as
humans would interpret what's happening in an image or are they just some orthogonal
established set of representations that don't necessarily map to the way that we would
interpret an image.
So I think there's maybe two points.
So the first point, this is also one that we're making a paper is that if you look at a task
because it's an unsupervised learning task, it is actually in defined and we have an
impossibility results, which kind of shows that no algorithm can do, I mean, this is my
interpretation, no algorithm, kind of to learn disentanglement representations can do
so consistently for all possible world models of the world or all possible processes that
generate your data.
And I think that brings us kind of to that second point, right?
This essentially means that you require inductive biases onto kind of the models which match
the data that you want to use the models for, right?
And I guess one of the hopes or like the key idea while a lot of people are excited about
disentanglement representations is that there are such good inductive biases, right?
And if you were to find a disentanglement representations, and this is kind of follow-up
work that we have looked into, it seems that you get quite some benefits in terms of the
representations you get, we found that for non-trivial downstream tasks, when you look
at tasks that require reasoning, abstract reasoning about the world that you will do
better if your representation is disentangled versus if you looked at, or at least we
saw that this was correlated with your representation being entangled.
I think that's kind of the hope that some of your algorithms or your models that you train,
they will generalize better to stuff that you have not seen before and things like this.
And I think that's why a lot of people are excited about this.
But I completely agree it matters how you define what is interpretable or not, right?
Yeah, yeah.
Okay.
Well, for the sake of time, we'll skip some of the work you've done on generative
model.
That's the other thing that you spend a bunch of time on at brain and dive into the research
that was recently published on Google Research Football.
Tell us a little bit about that project and its motivation and how it came about.
Sure, sure.
So basically, I think the whole project started in late summer last year.
It basically started as a kind of cool idea in our team where we have, I mean, one thing
you have to see about Google Brain at least in Zurich where I've been, it's very collaborative.
We share a lot of ideas over informal sessions like when you go for lunch, when you go
for coffee, but also in kind of within a team, and remember, we had this idea that we could
use reinforcement learning to play video or kind of video games that model soccer, right?
I guess a lot of people have played in the youth such video games and we started discussing
about this in order to pros, whether to cons, and out of this, we started kind of the
game, a bit of traction within the team.
And we started investigating, would this be possible, would it be feasible?
And we came across this open source football game called Gameplay Football, and we thought,
okay, let's see what we can do with this.
And at the same time, we, as we wanted to do, or some people in our group, were doing
research on reinforcement learning, we started noticing a lot of benefits that would be
had if we would build this into a fully-flanched modern reinforcement learning environment.
So we started, yeah?
Oh, no, I was just going to ask the obvious question, which is, so what are some of those
benefits?
Yeah, so I mean, the first benefit that I personally see is its open source.
So this gave us a lot of flexibility to modify the needs for a modern reinforcement
learning environment.
So it's primarily built for research, and that gives us a whole bunch of benefits.
And the first benefit, and I mean, that is also inherent to the game of football, is
that it is an on-tribal problem, right?
It's actually, if you look at it, it's quite hard to play, and let me if we iterate on
or kind of explain what we're actually doing here.
So this is in the context of reinforcement learning, where you have to teach an agent
to interact with an environment, here the agent has to control, and this is in the basic
form of this environment, has to control the active player on one of the teams, and it
has to do actions such as, okay, I'm going to walk to the right, to the left, to the top
or the bottom, and it has to pass around, shoot as you would, as a human if you were controlling
a team in a football video game.
And now, one of the benefits is that this is actually kind of, if you think of the game
of football, where you start in the middle of game with a kickoff, you have to pass around,
you have to play around your opponent's defense, you have to come up with a strategy, and
you have to score, that is actually a non-tribal task, and it's challenging task.
In particular, if you have different difficulties of opposing teams, as already one of the advantages
we saw, as you change the difficulty of the strength of the opposing team, you can change
the difficulty of the learning task.
Similarly, you can make it much easier, you can say, well, let's say that pitches empty
and they just have to score a goal against no goalkeeper, that is even much easier, right?
So one of the benefits that we saw was that it's very easy to adjust the difficulty of
the game, while still not being trivial, right?
And that is kind of in contrast to maybe other environments that may be rather easy to
solve, or just very hard computation expensive to run stuff on.
The same time, as it's open source, we were able to build a lot of features into the
game that help with doing enforcement learning research.
So one thing we could do is we can turn on and turn off soasticity, we can make the game
deterministic, then it turns out, or this kind of still an open question research is whether
some algorithms generalize from a setting, from an environment that is deterministic to
a sarcastic one, right?
The game is focused on controlling the player that has the ball.
The players that don't have the ball, as well as the opposing team, is there some kind
of traditional, quote unquote, game AI that's controlling those?
Yes.
Maybe the one thing to keep in mind here, so the way we structure this is we build what
we call the football engine, which is a simulation of a game of football, and it supports a lot
of different features.
One is it supports a built-in rules-based AI, as you would probably find in a professional
video game.
And at the same time, it supports a lot of features for research.
And we chose that, OK, so if you have to control all the players in your team, your toss becomes
much harder.
We did some additional experiments and turns out if you have to control more players, this
becomes harder.
So in the basic version on what we call football benchmarks, which is in this setting, we consider
the opposing players and your teammates to be part of the environment and they play with
the built-in rules-based AI.
In that case, you don't control them, right, and these are basically traditional reinforcement
learning problems, and we chose three of them as benchmark problems where people can
compare different algorithms.
But at the same time, the football engine that we built supports actually controlling all
of the players, or you can control all the players of one side, or we can even do a mix.
I think in the paper we, with this experiment, where we control between one and three players.
So that is completely flexible, and that's, again, one of the benefits of having a modern
reinforcement learning environment that is open source.
You mentioned that you chose three scenarios for your benchmarking.
How are those three scenarios distinguished?
Yes, so what we decided is, so we built these football benchmarks, which we considered
the current benchmarks, problems in this environment, and it's essentially you play a game
of 11 versus 11 football against three different of these rule-based opponents, and the only
thing changes is the difficulty of how well these different opponents, how well the opponent
plays.
Okay, so some parameter to the rule-based AI that's driving them.
Yes.
Exactly, I'm kind of how quickly it reacts to you, yeah, how well it plays.
As you would, if you play a, you know, a football game yourself, you can choose, you know,
between an easy medium and a hard, here it's essentially the same thing.
But the key motivation for actually doing this is also to accommodate that different researchers
may have different computational budgets.
So it turns out, if we are running currently with the algorithms that we tried and we found
that if you run on medium and hard problems, it becomes quickly very challenging and you
might have to go to distributed implantations of the funnagrums, you need quite a lot of
computation resources, but at the same time, you also want to provide benchmark problems
where you could go with a single machine, maybe a single GPU, and you could actually test
your ideas and compare against other algorithms, right?
Yeah, and I guess, yeah, there's different areas, right, of reinforcement learning, such
as learning with a few samples, I think this is one of the very hard problem reinforcement
languages, also very relevant.
And even if you don't have a lot of computational resources, you can still go and benefit from
this environment.
Okay.
Yeah, I mean, one thing also, we also added what we call the football academy.
And this is essentially, as I said before, like the having that flexible game engine allows
us to really kind of change what is happening on the pitch.
And one inspiration we took when football teams trained right, they might do some drills,
and we can actually model this, right?
We can say, well, let's not have 11 versus 11 players, maybe let's play 3 versus 2 counter
attack.
And we define a set of different, such academy scenarios as we call them, they start with
very easy ones where you're close to the goal.
There is no goalkeeper and you just have to score, so you have to eventually learn how
to walk into the goal or how to kick the ball into the goal to more difficult problems
where you play with 11 players versus goalkeeper or, as I said before, counter attack scenarios
that kind of accommodate very different game situations, but also which come with very
different difficulty levels.
And maybe the key thing here is the easy ones, right, where you just have to score on
easy, you can like, if you're on a single machine, you can get results with current algorithms
within minutes, or let's say 10, 20 minutes instead of having to wait a full day and block
a full machine before you can tweak your algorithm or check your implementation.
So I think that's also like one of the key advantages of the environment, you can go
and start with very simple cases and kind of see where does it break without always
having to wait a very long time in between.
You can also define your own scenarios, so there is a lot of flexibility and I'm pretty
excited about it.
I suspect this is going to be well beyond the scope of what you have looked at so far,
certainly at this paper, but have you explored kind of the idea of transfer learning or
curriculum learning, like training an agent on these academy scenarios and how that might
impact performance on the game as a whole?
Yes, so in the last few months, we've been very busy pushing out or kind of pushing out
this environment and fixing bugs and running the experiments we added in this paper.
It has certainly been on our mind and one of the motivations actually for the football
academy was to enable curriculum learning.
We didn't include results here, we kind of, in the paper we include benchmark results
for the different academy scenarios to show how hard they are and this role, as I mentioned
before, where you have a different variety of tasks, which are different difficulties,
but you can use this to go and build your own curriculum, right, and you can see does
thus fare better at actually playing the 11-versa-lem game than if I just start to play the 11-versa-lem game.
You've talked about some of the benefits of your environment in terms of it being open-source
and some of the features that you've built in, but I'm curious taking a step back are there,
any thoughts in terms of some unique properties of football slash soccer as a task for reinforcement
learning agents, there are a ton of RL simulation types of environments, everything from the tower
challenges, things to very domain-specific to the more general, teach some agent to walk
kind of environment, plus Atari games, why is football potentially interesting or more
interesting than some of these many other tasks?
Yes, so I think the key benefit of football is twofold, but the first one is I think the
task as we have it now is challenging even for modern reinforcement learning algorithms.
If you look at, for example, the hard scenario, even playing against the fixed rules-based AI
with the general learning algorithm, we need to run a lot of several hundred millions of steps
in a distributed setting where we have several hundred machines actually running this game, right?
So I think having a challenging task is very important. Now, when you do research here,
every research is kind of free how to choose the approach, but one of the hopes is that
if you take a general learning algorithm, which is not specific to football, that it would also
transfer to other under-settings, right? Now, in terms of how does it compare to different
environments? I think the key part where I mentioned before is that we can adjust the difficulty,
this allows you to do research on a variety of different difficulty levels. I think that's
different to prior environments, such as Atari, which I guess now people would consider rather
on the easy side. On the other hand, there has been reinforcement learning research on other
video games, such as Starcraft 2 or Dota 2, which is rather under computationally expensive side.
Here you get kind of everything, right? At the same time, it's also open source, so you don't
need access to a potentially a closed source binary. The other part, and I think that's also one
thing we are very excited about. There is essentially two extensions that we put into the environment,
which is it allows to play multiplayer in a sense that football is inherently a multiplayer game
where you play against an opponent, which brings a completely new dynamic when you have
somebody else acting in the same environment with adversarial goals to you. Similarly, the
environment supports actual research into multi-agents where you could think of every single player
has to make their own decision on what it's going to do, and now you have two teams, right?
And they have to start cooperating. Players within the team have to start cooperating
with each other to play against somebody else. And I think this is just a very broad
range of topics that you can investigate. Similarly, we built into the engine that you can go
and train on different representations or different types of observations, so you can either go
and train about pixels, which may be computation expensive, or we can go at other representations
of the observations, such as the actual positions of the players, right? Not all the data is images,
and you can, again, do something which you cannot do in other environments.
So the representations can be, in addition to just looking at the images, you can get the
kind of more inherent state. What about on the control side? What are the options that you have
on control of the player with the ball? Yeah, so there has been other reinforcement learning
environments, which are more, which are also doing football. This is DeepMind Soccer and Robocup,
the 3D simulation. These are more on the side of doing continuous control, where it's really about
low level control of either the robot in Robocup or the simulator robot in Robocup that you control,
or essentially a kind of players in this DeepM Soccer, where you really have to specify how much
do you turn, how much do you go forward and back, which is much more focused on low level control.
Here, the idea is more, you want to learn how to do these more high level control,
like things like strategy, tactics, become much more important, here you control.
Okay, as you would with a gamepad, essentially you can move right, left, top, bottom,
you can sprint, you can pass, you can try to press when you're in defense, and so on.
Again, the environment is open-source, so this can be modified, right?
So it sounds like the way that you control the character is one of the key distinction
between this environment and some of the others that are out there for this game.
Are there other distinctions? Yeah, I think that is one of them, it's in particular to with regards
to the other environments that do football. That is, for me, the key difference, I think to the
other ones, the actual tasks that you're doing, the fact that you can go from different types of
observations, that you can adjust the difficulty, that is open-source, these are all differences.
I think one of the points here is also, I think there's also, in reinforcement, always a certain
investment required in going and using an environment, and what I like here is, in one
environment, you can essentially do a lot of different types of research.
Maybe share a little bit about where your group goes from here. This has been published,
out on GitHub. When did it go live? When was the environment published and the paper published?
Yeah, so we put this kind of right. We finished a sprint just before ICML, where we first pushed
us to GitHub with an accompanying paper, which describes the environment. Since then, we had a
demonstration at ICML, at the Google booth, where we showed this to people, we got a lot of feedback,
we've worked on the environment actively. Since then, we added a lot of cool features that I'm
excited about, this multiplayer feature, where you can do self-play, for example. We've added
multi-agent support. We've done optimizations of the game engine. It runs now through
ExFaster, and we've just recently updated the paper and put it on archive with more experiments,
where we investigate the first experiments into three different research directions,
which we think are very promising in this environment. One of them is what happens if you do
self-play, what happens second is what happens if you do this multi-agent setting where you
control more players, and third one is what happens if you use different representations of the
actual environment or the observations that the agent interacts with. From this point,
does your group continue to work on the environment itself? Or do you start to
branch off into doing more experimentation work on the environment? Where do you see your work going
with this? Yes. I think there is, as you said, two things that we want to do. The first thing is
we want to make sure it is a useful research or enforcement learning environment. We are
definitely going to continue adding features, make it easier to use the environment. I think
the potential that it has, especially with the multiplayer component or where you have to compete
against other agents, we are definitely looking into options how to make this more accessible.
At the same time, we are also super excited to do research on this environment. Yes.
And also, I think one thing that we should highlight here, this has been a big team effort
in the team here. So it has been over 10 people that have worked actively on this project, right?
Now, I think it's also maybe a time where different people are going to explore different things
in this environment. And so, Olivia, maybe what are three things that you personally learned
in working on this project? Yes. So for me, this has been quite a cool experience. I didn't really
do reinforcement learning research before I came to this project. So for me, I learned a ton
about all different environments that are out there, about different algorithms. And I also learned
that this kind of funny anecdote that even now these algorithms that we have, they're very good
at exploiting kind of loopholes in the game engine. And like one example that people in our team
found and the photo is pretty funny is if you have 11 players playing football and trying to
score against a single goalkeeper, what is kind of the best strategy to do? And turns out in
the version that we have the game that we had at that time, what the agent learned is it would
go take the ball, kick it outside of the playing field so that the goalkeeper has to go to a throw-in
where he doesn't have any teammates, so he's going to throw it somewhere where the other players
have it, and now the players will score. And I think that really it is a pretty cool
example that we found. How long did it take the agent to figure that out?
I don't remember the details, but it's kind of like once it found this, it's a super smart strategy,
right? And it's really exploiting kind of a loophole. We had a similar one where essentially when
you train these agents in the beginning, the opposing team scores a lot and you're kind of learning
not to admit any goals, right? And one thing I learned is for example that once the opposing
player, for example, kicks the ball besides the goal, your goalkeeper when he's doing the kickoff,
he can just wait indefinitely because the engine didn't include any mechanism to
pray with that, and that is a good way to minimize the opponent's goaling goals if the game doesn't
continue. I think these things are super funny, and I think it's also highlights one of the things
which we didn't really talk about yet, but which is that doing research in this football environment
also makes that research to some degree more accessible, right? A lot of people they care about
football, they like watching football is very intuitive, and that's why I also, we hope that
actually having a football environment also helps with actually people maybe demystifying a bit
what is happening in this research so that people can see, oh, you know, this is a random agent
that's doing this, right? And then suddenly it starts learning how not to admit goals, and then
you see how it learns to score. I think people can very much identify with it, students that
are doing master programs that want to learn about reinforcement learning. It's super cool,
you can go with the environment, learn how to pass or kind of score against an empty goal,
then two versus one, and so on. So I think that's maybe one of the benefits that we didn't really
talk about, which is that it is a very accessible environment, right? You don't like like a lot of
people know about football. And of course, you just recently published this, but are you aware of any
efforts or plans to incorporate this into kind of learning curricula? So deep learning and reinforcement
is very fast-moving field. So we chose the approach of releasing the environment and publish,
putting online the paper, so this has not been published yet. It's on the web, everybody can
use it, but we're definitely trying to publish this, but it has not been published yet. Got it. And
the second question for curriculum learning, we did some very initial experiments. To be
clear, more talking about DC application, well, you spoke to seeing applications of this environment
in the education of humans and growing their own knowledge of reinforcement learning,
and I was just curious if you were aware of any plans to formalize that and maybe build it into
some online course or some school-based course to teach students about reinforcement learning
using this metaphor that they're very familiar with. So I'm not aware of this, but I definitely
hope that this happens. We've seen quite a lot of positive feedbacks on things like Twitter,
and where people got very excited about that. So I definitely hope that helps people access
research. Again, also given that it's open-source, it's very easy to use or to get started. If people
are interested in them, even if your listeners are interested in, we go to our GitHub page,
GitHub.com Google-research-football. It's very easy to get started. You can install the
environment, there is the examples, which show you how to train a basic agent. So I'm all for
making it as accessible as possible. You talked about the agents that you've played with finding
these corner cases to exploit the environment. How do you address that? It strikes me that in some
cases, the solution of that is maybe putting in more constraints that are like the human game of
football or better modeling that environment. In other cases, you might need to do something
that doesn't make sense in the context of the actual sport. It creates this unique computer
football sport. Does that resonate at all? How have you addressed those kinds of corner case
issues? We ran a lot of experiments. As you said, it is actually pretty hard to build a good
reinforcement learning environment. I think now we are pretty confident that it works pretty well,
but that doesn't exclude that you go, come with a new learning algorithm. It's really good.
It finds another corner case. I think we then have to address this on a case-by-case basis.
I think it also boils back to the underlying motivation. I think the motivation is not
necessarily playing as well as possible in this environment. It's a primary goal, but the goal
is to do research. If there are things that hind the research in this environment,
then I guess we have to change it. On the other hand, I think there's also the general feeling
our team. There is a benefit to having stability on the environment side, so that research and
results stay comparable across different papers. We definitely don't want to change the
environment too often, but if there is very obvious payload cases, which you can very exclude,
then we might have to do this. It sounds like then that for the most part, these exploitable
corner cases, you've considered them as bugs as opposed to inherent qualities of the game
that you're trying to model. Does that make sense? Yes. I think the example of the goalkeeper
having to go during the throw-in that really defeats the purpose of these scenarios we've
defined. Similarly, the example that I gave where the goal doesn't kick it away. If you do this
in a real game, you would get a delay of game penalty. Something would happen. I think that
really shows, maybe this was really a bug in the implementation. In the other hand,
it also boils back to whenever you're building an environment like this. You have to take some
choices. We've done this with the best faith effort, and we think now it is ready to be used
for research. Olivier, thanks so much for taking the time to share this project with us.
Yeah, thanks for having me here. All right everyone, that's our show for today. If you like what
you've heard, please do us a favor and tell your friends about the show, and if you haven't already
hit that subscribe button yourself, make sure you do so you don't miss any of the great episodes we've
got in store for you. As always, thanks so much for listening, and catch you next time.
