WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.720
I'm your host Sam Charrington.

00:31.720 --> 00:36.520
If you're a Twimble listener, you probably have an opinion about AI.

00:36.520 --> 00:39.760
Are you looking forward to the role AI plays in your life?

00:39.760 --> 00:43.040
Are you fearful of the changes AI will bring?

00:43.040 --> 00:47.400
Or maybe you're just skeptical and don't think AI will make a real difference in the near

00:47.400 --> 00:48.400
future.

00:48.400 --> 00:51.040
Whatever the case, we want to hear your take.

00:51.040 --> 01:00.200
So hit pause right now and jump on over to twimblei.com slash my AI to let us know what you think.

01:00.200 --> 01:05.400
Sharing your thoughts takes two minutes and qualifies you to win some great prizes.

01:05.400 --> 01:09.840
Before we proceed, I want to give a quick shout out to everyone who has submitted a video

01:09.840 --> 01:20.880
so far, so here's to you, Krishnan, Amara, Keith, Shriram, Sharon, Rob, Julian and Chandana.

01:20.880 --> 01:26.680
In today's episode, I'm joined by Roland Mimi Shavitch, co-founder, CEO and chief scientist

01:26.680 --> 01:29.480
at 20 billion neurons.

01:29.480 --> 01:33.960
Roland joined me at the rework deep learning summit in Montreal to discuss the work his

01:33.960 --> 01:40.680
company is doing to train deep neural networks to understand physical actions.

01:40.680 --> 01:46.200
In our conversation, we dig into video analysis and understanding, including how data-rich

01:46.200 --> 01:53.400
video can help us develop what Roland calls comparative understanding or AI common sense.

01:53.400 --> 01:58.480
We touch on the implications of AI systems having comparative understanding and how Roland

01:58.480 --> 02:02.840
and his team are addressing problems like getting properly labeled training data.

02:02.840 --> 02:03.840
All right.

02:03.840 --> 02:04.840
Let's go.

02:04.840 --> 02:08.680
All right, everyone.

02:08.680 --> 02:15.640
I am here at the rework deep learning conference in Montreal and I am with Roland Mimi Shavitch.

02:15.640 --> 02:21.680
Roland is a founder and chief scientist at 20 billion neurons, a company that's based

02:21.680 --> 02:25.680
in Toronto and Berlin and is doing some pretty interesting things.

02:25.680 --> 02:27.840
Roland, welcome to this week in machine learning and AI.

02:27.840 --> 02:29.680
Well, thanks very much for having me.

02:29.680 --> 02:31.960
It's great to meet you.

02:31.960 --> 02:34.960
Why don't we get started by having you tell us a little bit about your background and

02:34.960 --> 02:38.720
how you got interested in the field?

02:38.720 --> 02:44.120
I was interested, I have been interested in AI throughout my career over the last, I would

02:44.120 --> 02:46.160
say almost 20 years.

02:46.160 --> 02:52.200
I was a student in Germany, I'm based from Germany and the neural network research there as

02:52.200 --> 02:54.280
a master student.

02:54.280 --> 03:01.040
And then since the space was fairly sparse in terms of supervision and so on at that

03:01.040 --> 03:08.200
time I decided to go to Toronto to work with Jeff Hinton as an advisor to do a PhD in

03:08.200 --> 03:09.200
2003.

03:09.200 --> 03:10.680
Not a bad choice.

03:10.680 --> 03:11.680
That's true.

03:11.680 --> 03:12.680
Yes.

03:12.680 --> 03:20.040
And it has been a fantastic time and, unsurprisingly, I learned a lot and enjoyed my time very much

03:20.040 --> 03:21.280
in Toronto.

03:21.280 --> 03:26.120
I also like the city incidentally very much and I'm back now in Toronto, partly for that

03:26.120 --> 03:27.120
reason.

03:27.120 --> 03:33.560
And so I spent a few years there doing research on neural networks with a very strong interest

03:33.560 --> 03:36.920
in video understanding specifically.

03:36.920 --> 03:45.400
And like in all the sub areas in deep learning and AI, in video understanding we also experienced

03:45.400 --> 03:54.080
some nice advances and you know a cute, interesting results here and there but not the amazing

03:54.080 --> 03:59.640
breakthrough that we all felt had to be imminent somehow but just never really happened.

03:59.640 --> 04:04.880
And so then I moved around a little bit, I was an assistant professor in Germany for

04:04.880 --> 04:09.960
some time and then at the University of Montreal for the last couple of years.

04:09.960 --> 04:17.240
And then image that happened along the way in 2012 and we realized that the same thing

04:17.240 --> 04:23.840
has to happen for video because there was just no fundamental reason why it wouldn't.

04:23.840 --> 04:31.000
And so we started myself and some from our friends and colleagues from Germany started

04:31.000 --> 04:37.560
to kind of think a little bit about the possibility of just launching a company that would combine

04:37.560 --> 04:44.480
research, perspective and leadership from my side primarily and the ability to build

04:44.480 --> 04:48.800
a company with a strong culture and so on from their side.

04:48.800 --> 04:51.840
So all of these have been colleagues who had been in machine learning for many years

04:51.840 --> 04:55.120
but had successfully built companies and so on.

04:55.120 --> 05:04.520
And we felt like it was the right moment to come together, address the problems head on

05:04.520 --> 05:08.720
by you know building the right kind of engineering environment and operation that would solve

05:08.720 --> 05:14.440
data problems specifically and big engineering problems and see how far we can get.

05:14.440 --> 05:23.720
And so we got together about a year and a half ago and established a lab in Toronto where

05:23.720 --> 05:29.680
there's still a lot of talent in Canada specifically as well as a lab in Berlin where my friends

05:29.680 --> 05:32.200
from there are based.

05:32.200 --> 05:37.400
The company is called 20 billion neurons and we have completely focused on video analysis

05:37.400 --> 05:42.640
and video understanding and have met quite some progress there that I'd be happy to chat

05:42.640 --> 05:45.040
about more in a few moments.

05:45.040 --> 05:50.240
So as part of this I stepped away from my role as an assistant professor at University

05:50.240 --> 05:58.960
of Montreal to dedicate my work full time to this effort because I feel like we are really

05:58.960 --> 06:05.560
really onto something and it takes basically all of your energy to push that through obviously.

06:05.560 --> 06:06.560
Right.

06:06.560 --> 06:07.560
Right.

06:07.560 --> 06:15.160
So you started out with this kind of idea for a grand challenge accumulating a video

06:15.160 --> 06:18.320
data set analogous to ImageNet.

06:18.320 --> 06:19.320
Right.

06:19.320 --> 06:25.360
ImageNet is a huge undertaking you know just for images I can hardly imagine something

06:25.360 --> 06:30.720
of the same scale and impact on the video side what must be involved in doing that like

06:30.720 --> 06:31.720
where are we.

06:31.720 --> 06:36.000
Yeah that's a very good point and it's actually the analogy is not perfect because videos

06:36.000 --> 06:39.320
are a different piece than images.

06:39.320 --> 06:43.720
On various levels obviously the data amount is just ridiculously large back on various

06:43.720 --> 06:48.600
images because there's time and you have like a factor 100 just like there that's going

06:48.600 --> 06:53.840
to increase the amount of pixels that you have to look at basically but then also what

06:53.840 --> 07:00.720
video is really representing and what it's what it's about videos are not about showing

07:00.720 --> 07:07.080
an object you know and the task usually associated with video is not figuring out that there

07:07.080 --> 07:13.360
is an orange in the field of view or a dog or a human or something but it's much more

07:13.360 --> 07:20.640
about verbs, subtle interactions the way people behave in it, relationships between objects

07:20.640 --> 07:24.360
over time how they change and so on and so forth.

07:24.360 --> 07:30.400
So video is really quite a different thing in its own it's own that is though at the

07:30.400 --> 07:36.680
same time a challenge but also an amazing opportunity.

07:36.680 --> 07:41.560
The one of the reasons I would say that actually the reason why I've been drawn to video throughout

07:41.560 --> 07:50.400
my career has been that video can teach you a lot more than images can and then text can

07:50.400 --> 07:51.760
incidentally.

07:51.760 --> 07:57.080
If you have a system that's able to understand some subtle aspects of video and can make

07:57.080 --> 08:02.040
some subtle distinctions like for example understand the difference between putting an object

08:02.040 --> 08:06.800
on top of another object versus next to another object versus behind another object just

08:06.800 --> 08:08.880
simple interactions like that.

08:08.880 --> 08:13.720
If you have a system that can do that then you essentially have a system that has comments

08:13.720 --> 08:19.760
what you would call common sense which humans have you know you have a very natural intuitive

08:19.760 --> 08:25.360
understanding of the world you know unlike incidentally an image net train network that

08:25.360 --> 08:32.600
an object is not just a texture certain distribution of colors in your field of view but it is actually

08:32.600 --> 08:39.760
a thing that has a special extent that is surrounded by air that could be moved from one

08:39.760 --> 08:44.800
place to the other that has a weight associated with it that is subject to gravity that can

08:44.800 --> 08:48.720
behave in certain ways but not others that can teleport for example from one place to

08:48.720 --> 08:49.720
the other.

08:49.720 --> 08:53.440
And all of these things are totally natural to humans humans acquire a lot of that knowledge

08:53.440 --> 09:01.200
throughout the first years in their life and AI systems so far had none of that.

09:01.200 --> 09:06.400
And the interesting part about video is that to the degree that we are able to get systems

09:06.400 --> 09:13.240
to understand video concepts we enable them to get some of those concepts and that means

09:13.240 --> 09:20.160
really equipping AI systems with a certain degree of common sense and that's what's really

09:20.160 --> 09:25.280
exciting in what video and that is the reason why we have been kind of pushing along that

09:25.280 --> 09:29.040
agenda and I have been interested in that for 15 years or so.

09:29.040 --> 09:36.840
Has anyone tried to scope common sense like how much of I don't even know what the unit

09:36.840 --> 09:43.880
would be you know it's not rules it's not necessarily layers or neurons or memory or

09:43.880 --> 09:52.160
storage has anyone in any kind of way tried to get a handle on what the magnitude of what

09:52.160 --> 09:59.520
a computer would need to learn to have common sense or we you know that kind of the you

09:59.520 --> 10:07.240
know the hundred million dollar question kind of the touring test kind of scenario.

10:07.240 --> 10:11.240
There are a few discussions obviously and there are many probably very technical discussions

10:11.240 --> 10:19.920
in the psychology literature. There's one effort school of thought that surrounds common

10:19.920 --> 10:24.440
sense and drives it much further that I completely subscribe to and that has been one of the

10:24.440 --> 10:29.560
main reasons why I've been interested in getting at common sense via video.

10:29.560 --> 10:35.680
That is a school of thought around people like Douglas Hofstadter and George Lekov who

10:35.680 --> 10:43.000
say that our cognitive capabilities are structured around metaphors basically so whenever

10:43.000 --> 10:49.240
you recognize an object or whenever you have some concept in your head of of something

10:49.240 --> 10:55.040
like concept of an argument you do not understand that concept per se but you understand it

10:55.040 --> 11:02.240
by comparison to other things and so one example is that that came from from Lekov back

11:02.240 --> 11:07.480
then is that an argument is like a war basically when people have an argument you think of

11:07.480 --> 11:13.240
it as like a fight between those people or something like that and so specifically people

11:13.240 --> 11:19.080
like Douglas Hofstadter who I have always been a big fan of throughout the years has been

11:19.080 --> 11:25.920
driving this completely to the ultimate end point and he said basically that anything

11:25.920 --> 11:31.160
you do at any point in time is metaphors drawing analogies between different things like

11:31.160 --> 11:37.480
I mean whenever you recognize an elevator as an elevator that you have to walk to you

11:37.480 --> 11:42.400
basically making an analogy at that point and when you understand very subtle complex

11:42.400 --> 11:49.680
concepts like a CEO saying this company will weather the storm you again make an analogy

11:49.680 --> 11:55.560
and all of the cognitive processes that drive human understanding and thinking are just

11:55.560 --> 12:02.640
made of the same stuff which is analogy making and so common sense is kind of on the lowest

12:02.640 --> 12:09.160
level of that but there's a school of thought that says that everything that humans can

12:09.160 --> 12:13.760
do and that makes human thinking amazing is based on the same stuff which is just expan

12:13.760 --> 12:20.680
an expanjan away from from common sense which is focused on very very low level aspects

12:20.680 --> 12:25.560
of the world like what objects are how they move around and so on and so forth.

12:25.560 --> 12:32.280
So what are the implications of that school of thought on machine learning and AI?

12:32.280 --> 12:36.760
I think the implications could be immense and there will be immense to the degree that

12:36.760 --> 12:42.720
we will be able to make them tangible make them actual technologies that actually work

12:42.720 --> 12:48.240
and with what we've been doing on you know understanding how objects relate to each

12:48.240 --> 12:52.080
other what happens to them in a video it's just stretching the surface it's like basically

12:52.080 --> 12:58.920
go getting at the lowest level but there has been a lot of drive towards concepts ideas

12:58.920 --> 13:05.240
like grounding for example so enabling a system that does translation from one language

13:05.240 --> 13:11.440
to another to not just associate semantic patterns with with the word embedding that it

13:11.440 --> 13:17.160
uses but to associate actually something that's connected to a sensor and it's I think

13:17.160 --> 13:21.920
it's quite likely that we will see more and more of that in the future such that a system

13:21.920 --> 13:30.840
that outputs a sentence like there's a cup on the table not only has in its mind if

13:30.840 --> 13:37.400
you want the syntactic structure in the cup the words relate to another like as it learned

13:37.400 --> 13:43.720
from like large large text copper but also has an association of a cup you know some

13:43.720 --> 13:47.680
wake description which is just represented in the feature activation at that moment where

13:47.680 --> 13:53.640
the word is issued of a cup on a on a table and it can distinguish that from a cup under

13:53.640 --> 13:57.800
the table or something just by means of like visual associations I think this is going

13:57.800 --> 14:04.920
to happen more and more of a time are there any particular places or labs that are working

14:04.920 --> 14:13.280
in this area that are worth taking a look at or papers I haven't seen much and I think

14:13.280 --> 14:21.680
one of the reasons is that it is so fundamentally linked to video understanding that that

14:21.680 --> 14:25.680
we haven't seen much of that video understanding positive this own kind of challenges because

14:25.680 --> 14:31.240
or as I just mentioned it because of the really involved technological developments you need

14:31.240 --> 14:35.000
like the lots of engineering you need and the data operation to get all the data and

14:35.000 --> 14:40.840
so on because of that there is a barrier obviously and people have been hesitant to go down

14:40.840 --> 14:49.160
that route there is some interesting work in like common sense reasoning and so on coming

14:49.160 --> 14:56.680
from Josh Tannenbaum from MIT and there have been some efforts happening in the deep learning

14:56.680 --> 15:03.360
community over the last couple of years which didn't fly though like a lot of us have tried

15:03.360 --> 15:08.920
to build systems that can predict the future of a video in order to understand something

15:08.920 --> 15:14.080
about the world that's called unsupervised learning taken and frames and you try to predict

15:14.080 --> 15:20.320
what's going to happen in frames n plus 1 to n plus exactly and exactly and obviously

15:20.320 --> 15:24.320
it is sufficient if you have a system that can do that you could argue that that system

15:24.320 --> 15:28.000
really gets the world you know it really knows what options is otherwise how would it possibly

15:28.000 --> 15:35.040
be able to render the future of a video unfortunately even though it's it's a proof it would

15:35.040 --> 15:41.560
be a proof that a system is basically intelligent it's not feasible so far nobody has been

15:41.560 --> 15:48.360
able to predict video sensibly for up to more than maybe a second into the future and even

15:48.360 --> 15:54.280
then it's not really great and this is a fundamental problem that we all have been facing over

15:54.280 --> 15:59.520
many years when I referred back to the struggles we had a few years ago before image net

15:59.520 --> 16:03.360
and so on we thought that unsupervised learning is going to fly.

16:03.360 --> 16:09.400
It was learning basically means that you take data and you try a system to just represent

16:09.400 --> 16:16.240
that data and then get trained by reconstructing the data by basically inventing images and

16:16.240 --> 16:17.760
videos and so on.

16:17.760 --> 16:21.720
That never flew until now it's not flying.

16:21.720 --> 16:25.800
I have lost phase in that agenda quite to some degree.

16:25.800 --> 16:30.960
Now a lot of that work with the trying to predict the video was using like things like

16:30.960 --> 16:33.960
variational auto encoders and all that.

16:33.960 --> 16:35.880
Well there are many many different techniques.

16:35.880 --> 16:41.640
We had used RNNs back in the day to try to predict synthetic neural videos and that all

16:41.640 --> 16:45.800
worked fairly well and so on and it was nice for the first second.

16:45.800 --> 16:49.040
Yeah for a second or actually for synthetic videos you can go further than that you can

16:49.040 --> 16:54.480
predict like very very very long stretches of video because the structure and the synthetic

16:54.480 --> 16:58.480
data naturally is not that complicated not the real world.

16:58.480 --> 17:05.400
For example there was like a toy data set that showed balls bouncing around in a box.

17:05.400 --> 17:13.040
So very very simple physical simulation without much physics and at some point we became

17:13.040 --> 17:19.360
able to predict that very well and in the grander videos of balls bouncing around in a box

17:19.360 --> 17:23.720
very well but it hasn't gotten us anywhere.

17:23.720 --> 17:29.200
We haven't as a result of that build systems that really understand something about the world.

17:29.200 --> 17:36.840
And I think ImageNet has been a great example of how far supervised learning can actually

17:36.840 --> 17:39.080
push you because the main.

17:39.080 --> 17:44.800
The most interesting part about everything around ImageNet is that it's able to generate

17:44.800 --> 17:52.040
features that people then can use to solve other tasks which are not ImageNet by just

17:52.040 --> 17:56.480
freezing the features that you learned and using like what they call penultimate layer

17:56.480 --> 17:59.840
feature transfer learning or something like that.

17:59.840 --> 18:06.520
And that the fact that this works so amazingly well I think gives us such strong evidence that

18:06.520 --> 18:11.600
there is something to supervised learning and actual tasks that humans have to solve that

18:11.600 --> 18:17.160
it was going down that route and you know pushing the unsupervised agenda to the site

18:17.160 --> 18:21.320
for at least some time.

18:21.320 --> 18:27.480
And so how are you taking on this problem at 20 billion rounds?

18:27.480 --> 18:34.120
So the challenge then is obviously how to get your hands on data that is labeled data

18:34.120 --> 18:38.040
that is video accompanied by a label.

18:38.040 --> 18:43.560
And that is a big challenge in itself because as I just mentioned images can be labeled

18:43.560 --> 18:50.680
by attaching nouns to what's the central object in the image and so on.

18:50.680 --> 18:56.840
For videos it's not quite possible specifically if you want videos as a way to generate some

18:56.840 --> 19:01.960
kind of common sense it's just not going to well you're not going to be able to do that

19:01.960 --> 19:04.480
by just labeling nouns in a video.

19:04.480 --> 19:08.880
What you need is videos that show very subtle concepts people doing things with objects

19:08.880 --> 19:12.440
and interacting and so on.

19:12.440 --> 19:19.720
And the standard way of gathering data just doesn't work in that case either because if

19:19.720 --> 19:24.720
you want to have a video where a person puts an object on top of another object and then

19:24.720 --> 19:28.960
on next to another object so that you can have these two classes.

19:28.960 --> 19:32.560
Good luck if you want to try to find those videos in YouTube.

19:32.560 --> 19:35.840
You're going to have to watch through hundreds of hours of YouTube video until you find

19:35.840 --> 19:39.440
this exact actual happening that you can attach a label to.

19:39.440 --> 19:47.880
So what we do at 20 billion is to have crowd workers not label videos for us but film videos

19:47.880 --> 19:54.880
for us and we build a platform where crowd workers connect and they use their webcams

19:54.880 --> 20:02.320
to do all kinds of stuff in their homes usually that we tell them to.

20:02.320 --> 20:11.320
And what we tell them to do are examples that I just mentioned and many many others behaving

20:11.320 --> 20:15.680
in a certain way do certain things like hide an object behind another cover an object

20:15.680 --> 20:20.360
up and uncover it or push it around or push it so that it falls off the table and so

20:20.360 --> 20:21.880
on and so forth.

20:21.880 --> 20:28.760
And that way we have been able to gather a very, very large amount of data that we now

20:28.760 --> 20:32.800
use to train our networks supervised.

20:32.800 --> 20:37.280
And the name of the game of course is to use the supervised learning not just as a goal

20:37.280 --> 20:42.520
in itself but to cash in on transfer learning there as well.

20:42.520 --> 20:46.720
The idea being that once you have a system that understands what behind and in front

20:46.720 --> 20:52.400
and all of these things are that it can generalize much better to new use cases for which you

20:52.400 --> 20:55.000
have much less data.

20:55.000 --> 21:01.040
And so you've kind of inverted the problem you're having them create the data from the

21:01.040 --> 21:02.040
label.

21:02.040 --> 21:03.040
Exactly.

21:03.040 --> 21:05.840
As opposed to the opposite.

21:05.840 --> 21:10.440
Just going back to the premise though is the idea that with I'm assuming the idea

21:10.440 --> 21:15.120
with is that with you know just going to YouTube videos and like grabbing a video and starting

21:15.120 --> 21:22.280
to label it the your label spaces just too large is just you have to control for the kinds

21:22.280 --> 21:25.280
of things you want to label for any given experiment.

21:25.280 --> 21:26.280
Exactly.

21:26.280 --> 21:27.280
Exactly.

21:27.280 --> 21:32.720
And as opposed to images there are just so many labels that you want to have that are

21:32.720 --> 21:38.120
not just many even but also a compositional.

21:38.120 --> 21:42.600
So it makes much more sense in a video context to have labels that are descriptions like

21:42.600 --> 21:48.720
whole sentences that tell you what's happening in a video clip rather than a single noun.

21:48.720 --> 21:55.400
So videos contain verbs not just nouns and they contain other word classes.

21:55.400 --> 22:00.000
And the labels in our operation take the form of captions typically.

22:00.000 --> 22:04.000
So we tell them because I act out the following concept.

22:04.000 --> 22:09.200
Something like pushing an object over the edge of the table.

22:09.200 --> 22:12.160
So it falls down or something like that.

22:12.160 --> 22:19.560
And so that means the label there right there is structured and it has some similarity

22:19.560 --> 22:26.560
to other labels such as push an object or pushing an object towards the edge of the table

22:26.560 --> 22:30.160
but not so far that it falls down or something like that.

22:30.160 --> 22:36.680
So labels are complex objects in themselves and we don't see any other way of getting

22:36.680 --> 22:40.640
at label data of the type other than inverting this process.

22:40.640 --> 22:45.680
And starting with the label generating them on our site and then just filming what they

22:45.680 --> 22:46.680
represent.

22:46.680 --> 22:54.520
Have there been any efforts to start with a movie for example and just label every scene

22:54.520 --> 23:02.840
or at kind of arbitrary granularities of time label what's happening in a scene?

23:02.840 --> 23:08.520
Yes, there have been a lot of efforts on generating data for video over many years.

23:08.520 --> 23:13.320
As video understanding has been there, people have been trying my included.

23:13.320 --> 23:16.920
But they were usually of that type as you just mentioned.

23:16.920 --> 23:23.200
They're usually involved going through video and labeling video, existing video.

23:23.200 --> 23:27.320
One of the outcomes of that has been that a lot of the video data sets that are available

23:27.320 --> 23:32.520
are a bit funky as training material for networks.

23:32.520 --> 23:37.360
There are data sets out there that are very common and commonly used in the community that

23:37.360 --> 23:44.200
for example contain different sports activities from broadcast TV basically.

23:44.200 --> 23:49.480
And then the task of the networks is to distinguish soccer from basketball or baseball

23:49.480 --> 23:50.960
or something like that.

23:50.960 --> 23:55.840
And so what's problematic with these kind of efforts and data sets is that in many cases

23:55.840 --> 24:00.080
just a single image from the video just reveals what you're looking at.

24:00.080 --> 24:04.480
I can tell you that it's soccer just because I see there's like a green...

24:04.480 --> 24:08.680
Sunderbar, something like that, exactly.

24:08.680 --> 24:14.760
And specifically they don't involve really getting into the nitty-gritty details of the

24:14.760 --> 24:16.400
physics of the scene.

24:16.400 --> 24:22.480
They don't ask a network to really attend to certain objects in the scene and look whether

24:22.480 --> 24:27.800
they are behind another or in front or if they're falling or not falling.

24:27.800 --> 24:34.600
If they are maybe made of liquids or solid or if they're made from cloth or all of these

24:34.600 --> 24:40.960
really, really subtle physical aspects, that video is so great at revealing they are not

24:40.960 --> 24:44.120
basically asked for in these kinds of tasks.

24:44.120 --> 24:48.960
That has been a problem, my opinion, with the way that the video of understanding community

24:48.960 --> 24:53.280
has been rolling over the last couple of years.

24:53.280 --> 25:01.760
The other thing that kind of jumps out at me is if I don't know if like the stage directions

25:01.760 --> 25:10.120
and things like that in a screenplay are rich enough to be used as training data as labels,

25:10.120 --> 25:17.400
but it would be... I wonder if it would be interesting to take a movie and then use the stage

25:17.400 --> 25:21.120
directions as kind of the labels.

25:21.120 --> 25:26.800
But even then, those stage directions, there's so much that's kind of at the discretion

25:26.800 --> 25:33.200
of the director and kind of evolves that's not explicitly specified in the stage direction.

25:33.200 --> 25:34.200
Yes, very true.

25:34.200 --> 25:38.800
And there have been efforts also to generate data sets exactly using that kind of information.

25:38.800 --> 25:43.880
Yeah, using scripts and accompanying movies and making that the source for the labeling

25:43.880 --> 25:44.880
basically.

25:44.880 --> 25:49.520
Still not good enough to convince you that going the other way isn't better.

25:49.520 --> 25:54.520
It's absolutely not because the stage directions, as you just mentioned, are really, really

25:54.520 --> 25:56.440
high level and corporal.

25:56.440 --> 26:04.560
The stage direction is not going to be take a cup, pour slightly next to the glass so

26:04.560 --> 26:07.720
that you spill a little bit or something like that, which is what we want the network

26:07.720 --> 26:08.720
to be understand.

26:08.720 --> 26:13.360
It goes back to this issue that there's so much context that's understood by us as humans

26:13.360 --> 26:14.360
that we don't...

26:14.360 --> 26:15.360
Right, exactly.

26:15.360 --> 26:16.360
Exactly.

26:16.360 --> 26:22.040
We humans already have all that common sense understanding, so the script writer, or whoever,

26:22.040 --> 26:26.040
is going to assume that information is already in the head of the director or something.

26:26.040 --> 26:27.960
They don't need to spell everything out in detail.

26:27.960 --> 26:28.960
Right.

26:28.960 --> 26:34.000
I think to the degree that we advanced on this agenda, and networks get better at getting

26:34.000 --> 26:38.520
this floor right now that we're looking at of basic understanding of physics and so on,

26:38.520 --> 26:43.680
does it agree that we make direction there and it starts to get to work better?

26:43.680 --> 26:54.160
I could see how scripted data from scripts is going to be useful, but right now the foundation

26:54.160 --> 26:55.160
is missing.

26:55.160 --> 27:00.800
We're asking the system to understand something really complex, highly cultural and social

27:00.800 --> 27:05.120
and so on, without even understanding the world in which we're living, and that seems

27:05.120 --> 27:07.440
a bit bizarre to me.

27:07.440 --> 27:11.760
So what have you used this method to accomplish so far?

27:11.760 --> 27:17.000
So we had a breakthrough a few weeks ago, like around three weeks ago, where we trained

27:17.000 --> 27:22.440
a network on a very large amount of data that we've gathered so far, just for the fun

27:22.440 --> 27:23.440
of it.

27:23.440 --> 27:28.640
We have data across many use cases, obviously, but we just thought, okay, let's see how far

27:28.640 --> 27:35.480
we are right now, and much sooner than expected, we saw that a lot of this subtle distinctions

27:35.480 --> 27:38.800
that we were asking the networks to do actually already happened.

27:38.800 --> 27:41.680
And what kinds of distinctions?

27:41.680 --> 27:45.200
Lots of the physical things that I just mentioned, you know, a person throwing an object

27:45.200 --> 27:48.760
in the air and catching it versus throwing an object in the air and it falls on the floor

27:48.760 --> 27:50.760
and it's like that.

27:50.760 --> 27:55.960
And it's still early times, we basically just have the first push through, we basically

27:55.960 --> 28:03.160
saw, okay, there is like a non-rendered performance, the networks make like clearly non-rendered

28:03.160 --> 28:07.880
performance, correct decisions there, which far has been very, very exciting, because

28:07.880 --> 28:10.960
these are incredibly hard tasks.

28:10.960 --> 28:18.160
And so now we have live demos and so on that show how, you know, you can just take the

28:18.160 --> 28:23.400
camera, put it up, and then just do things in front of it, and it'll understand, constantly

28:23.400 --> 28:29.000
describe what you're doing in real time, and get a lot of things that are very subtle.

28:29.000 --> 28:33.600
And it has been very exciting, it has happened just now, basically.

28:33.600 --> 28:34.600
Nice.

28:34.600 --> 28:35.600
Yeah, yeah.

28:35.600 --> 28:37.840
How do you plan to apply it?

28:37.840 --> 28:43.400
So as I mentioned, we're gathering data driven by two considerations.

28:43.400 --> 28:49.200
One being we want to sample the space of things that can happen in video as densely as possible

28:49.200 --> 28:53.720
to catching on transfer learning and so on, but then we also have interactions with prospective

28:53.720 --> 28:58.920
customers and existing customers on a very specific use cases that they have.

28:58.920 --> 29:06.280
And so the data set fills up over time driven by these two considerations, basically.

29:06.280 --> 29:11.880
And so the specific use cases that for which we have gathered data and for which we've

29:11.880 --> 29:16.920
built models and so on are things like human computer interaction.

29:16.920 --> 29:25.320
So for example, we've built one of the first accurate RGB gesture recognition systems that

29:25.320 --> 29:26.320
have ever been around.

29:26.320 --> 29:32.160
So people are sitting in front of the screen doing certain movements with their hands signalling

29:32.160 --> 29:36.800
to the system that they want to turn down the volume or whatever.

29:36.800 --> 29:41.280
You need to talk to this company, I forget the name of the company, but I was at the

29:41.280 --> 29:47.400
Gartner conference last week, it's all a blur at this point, but one of the companies

29:47.400 --> 29:57.040
that I talked to is the company that basically it was founded by the guy that did all of

29:57.040 --> 30:00.360
the gesture stuff for the movie minority report.

30:00.360 --> 30:01.360
Okay.

30:01.360 --> 30:06.200
They commercialized that into a company and so you can have all these screens and you're

30:06.200 --> 30:11.200
kind of dragging and flinging content around on these screens and you're doing it with

30:11.200 --> 30:14.160
this remote and I'm like, yeah.

30:14.160 --> 30:18.240
And they have a conference, conference room set up and there's a camera right there and

30:18.240 --> 30:22.040
I'm like, there's a camera right there, why am I using this clumsy remote to move these

30:22.040 --> 30:23.040
windows around?

30:23.040 --> 30:24.040
Yes.

30:24.040 --> 30:25.360
I can do that with a mouse on a desktop.

30:25.360 --> 30:26.360
This makes no sense.

30:26.360 --> 30:29.800
He's like, well, you know, there'll be a lot of training that would have to happen in

30:29.800 --> 30:33.680
order to get users to make these gestures and like, yeah, I don't buy that.

30:33.680 --> 30:37.280
You need to figure out how to do the gestures and it sounds like you.

30:37.280 --> 30:39.640
Incidentally, we just figured out how to do the gesture.

30:39.640 --> 30:44.880
We have live demos and they just work and we have agreements with customers who want

30:44.880 --> 30:47.080
to use that and so on.

30:47.080 --> 30:49.960
It basically just works.

30:49.960 --> 30:53.920
Just showing once more that there is something to the image and that findings, you know,

30:53.920 --> 30:58.680
if you gather enough data, you can make things work very nicely.

30:58.680 --> 31:03.960
And it's true, there has been a lot of efforts around extra devices that you hold in your

31:03.960 --> 31:10.880
hand or even, even if you get rid of those, like the connect and so on, like the devices

31:10.880 --> 31:15.240
where you have a camera sensor, which is not an RGB sensor, but a depth sensor.

31:15.240 --> 31:20.640
And so it sees something about the depth profile in the scene in order to do its job.

31:20.640 --> 31:27.520
And there has been, you know, almost religious belief in the gesture recognition community

31:27.520 --> 31:29.040
that that's what you need.

31:29.040 --> 31:32.760
You need a funky sensor, otherwise you won't do gestures.

31:32.760 --> 31:37.240
But, you know, if I close one, if I close one of my eyes and you do a gesture in front

31:37.240 --> 31:39.280
of me, I can perfectly recognize it.

31:39.280 --> 31:41.160
There's no problem whatsoever.

31:41.160 --> 31:46.880
So why wouldn't it train neural net that just sees gestures, tons of gestures, be able

31:46.880 --> 31:47.880
to do that as well?

31:47.880 --> 31:50.640
We just showed that it's possible and it is perfectly possible.

31:50.640 --> 31:53.280
So the problem is basically solved.

31:53.280 --> 31:54.280
Yeah.

31:54.280 --> 32:00.080
There's something definitely sticky to this idea of the camera as being kind of the universal

32:00.080 --> 32:06.040
sensor that replaces huge swaths of other types of sensors.

32:06.040 --> 32:11.280
We just need to be able to figure out the intelligence on the back end to allow the camera

32:11.280 --> 32:13.840
sensor to do what we can very easily do.

32:13.840 --> 32:14.840
Indeed.

32:14.840 --> 32:15.840
Indeed.

32:15.840 --> 32:16.840
That's true.

32:16.840 --> 32:23.640
And whenever you can do something with one eye, you have a proof of concept.

32:23.640 --> 32:24.880
It is possible.

32:24.880 --> 32:26.880
And then you know you just have to figure out how.

32:26.880 --> 32:27.880
Yes.

32:27.880 --> 32:28.880
Absolutely.

32:28.880 --> 32:29.880
Awesome.

32:29.880 --> 32:30.880
Great.

32:30.880 --> 32:33.200
Well, I know you did a talk yesterday.

32:33.200 --> 32:37.720
Is there anything that you covered in your talk that we haven't managed to tease out

32:37.720 --> 32:40.680
in our conversation so far?

32:40.680 --> 32:49.720
I showed some demos that I can show to you via audio demos and hand gestures and white

32:49.720 --> 32:52.360
boards are difficult for a podcast.

32:52.360 --> 32:54.160
That is true.

32:54.160 --> 32:57.360
Besides that, we've pretty much covered what I was talking about yesterday.

32:57.360 --> 32:58.360
Awesome.

32:58.360 --> 32:59.360
Great.

32:59.360 --> 33:02.880
Well, Roland, it was great to meet you and great to chat with you.

33:02.880 --> 33:03.880
Thank you.

33:03.880 --> 33:04.880
Great.

33:04.880 --> 33:05.880
Thanks so much.

33:05.880 --> 33:09.200
All right, everyone.

33:09.200 --> 33:11.240
That's our show for today.

33:11.240 --> 33:16.320
For more information on Roland or any of the topics covered in this episode, head on over

33:16.320 --> 33:20.680
to twimlai.com slash talk slash 111.

33:20.680 --> 33:28.760
And remember to submit your thoughts for our My AI contest at twimlai.com slash My AI.

33:28.760 --> 33:31.680
Thanks so much for listening and catch you next time.

