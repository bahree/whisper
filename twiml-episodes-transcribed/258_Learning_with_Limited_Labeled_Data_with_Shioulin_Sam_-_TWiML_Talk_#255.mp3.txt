Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
I want to send a quick thanks to our friends at Cloud Era for their sponsorship of this
series of podcasts from the Stratidata conference, which they present along with O'Reilly media.
Cloud Era is long been a supporter of the podcast.
In fact, they sponsored the very first episode of Twimble Talk back in 2016.
Since that time, Cloud Era has continued to invest in and build out its platform, which
already securely hosts huge volumes of enterprise data, to provide enterprise customers with
a modern environment for machine learning and analytics that works both in the cloud
as well as in the data center.
In addition, Cloud Era Fast Forward Labs provides research and expert guidance that helps enterprises
understand the realities of building with AI technologies without needing the higher
and in-house research team.
To learn more about what the company is up to and how they can help, visit Cloud Era's
machine learning resource center at cloudera.com slash ml.
All right, everyone.
I am here with Shaolin Sam.
Shaolin is a research engineer with Cloud Era Fast Forward Labs.
Shaolin, welcome to this week in machine learning and AI.
Thank you.
Thank you for having me.
Absolutely.
I am really excited about our conversation.
We are going to be diving into some of the work that you've done recently exploring learning
with limited label data.
Before we do that, tell us a little bit about your background.
How did you get started working in AI?
Yeah, so my background is actually in electrical engineering and computer science.
Go doubly.
Yeah.
So after graduation, I worked in the financial industry designing quantitative trading
strategies and then that got a little bit old because I felt like we were using data but
not really for the right reasons or the right thing.
Then I kind of step back and started to focus more on early stage venture investments
where I mostly looked at women-led companies and try to essentially help them through
the whole startup venture fundraising phase.
And after that, I realized that I actually really, really love data and missed programming.
I miss actually building things and that's actually how I found my way back into nitty-gritty
work and Fast Forward Labs.
I remember when I finished, actually I don't think this was the case for undergrad but for
grad school, like we were just at the time when the consulting industry was like sucking
a lot of people out and it was a transition from when consulting was taking everyone to
when financial services was taking.
Yeah, I think my dissertation was in Operation Sweet Shows and Operations Management.
So a lot of us went into management consulting.
And then the next, I guess, popular choice was finance, specifically the Quant Stuff.
Okay.
Yeah.
Nice, nice.
The OR stuff was one of my favorite classes in grad school, like the, what was the,
I'm trying to remember the name of the software that, God, I would be so dating myself to
say that you got the textbook and it was this floppy disk in the back of the book.
I had like a puma on it.
A puma.
Yeah, it was basically a linear program solver thing.
I forget what that thing was called.
But that, I really enjoyed the whole dynamic and stochastic programming and linear programming
and stuff like that.
And every once in a while, I get to kind of geek out with folks that took operations
research stuff.
So you went into, you went to the dark side, the financial services path.
But now you're back doing research and focusing on data and data science.
What exactly do you do at Fast Forward Labs, Cloud or a Fast Forward Labs?
Cloud or a Fast Forward Labs.
So at Cloud or a Fast Forward Labs, our goal is to bridge the gap between research and
industry applications.
And one of the things that we do is publish research reports.
And these reports are meant to highlight capabilities within machine learning that we
think will become important for the business community within the next six months to two
years from when they're published.
So we spend a lot of time focusing on research, focusing on what capability we should actually
write about.
And we also, as part of that, consult with clients.
We are essentially their best friends, best data nerd friends.
So we try to help them implement this capability within the corporation, within the enterprise.
And then they can always come to us with questions.
And that's accomplished through advising hours.
And I've long been a big fan of what you're doing.
Hillary was one of my first interviews on the podcast.
And last year's strata, I interviewed Justin Norman and we talked about the current report
at that time, which was federated machine learning.
I think in that interview, I referenced that one of my favorites was the thing that the
report on summarization.
I thought that was really cool.
But the most recent report is one that you've worked on and that's on learning with limited
label data.
So what motivated that report?
A couple of things.
So we see a lot of enterprises sitting on a large amount of data, real-life data that
they cannot actually leverage and turn into anything useful.
And one of the reasons is that real-life data is very messy and unorganized and unlabeled.
But supervised machine learning, for example, requires precise labels and a lot of data for
you to be able to actually use it.
And because of this, what companies end up doing currently is essentially creating labels
manually for each one of their data.
And that's essentially their attempt to be able to convert the data into anything useful.
But that effort is very brute force, it's also not very effective, it's very expensive,
and doesn't scale very well.
So there has to be a better way to do this.
And active learning is one way that allows you to learn with limited label data.
With active learning, you can actually just smartly select a small set of label data,
use that to build a machine learning model.
And then that essentially opens up capabilities that you weren't able to do before.
You can now build new products that your enterprise can take advantage of using already your
existing data.
So one of the things that's always interesting about kind of the approach you take with
these reports is that it's very much, there's usually something that's happened in the industry
that says, okay, now it's time to do the active learning report, which is sounds like
this learning with limited label data is like the active learning report.
What's the thing that's happening now that is, you know, is it new advancements and
algorithms around active learning or something else?
So the report is called learning with limited label data.
And as part of that, we focus on different approaches.
We looked at different types of approaches that would help you do that.
Active learning was something that we ended up focusing on because it's most mature out
of all these other capabilities.
We also looked at, for example, weeks supervision, meta learning, that that was something that
we mentioned in the report, but it's not something that we actually looked into very deeply.
So the reason we decided to look to focus on active learning is because, first of all,
it is one way that most enterprises can easily use and take advantage of it and essentially
learn from the data.
The second reason is that recently there's been a lot of algorithmic improvements in active
learning that allows it to be used for deep learning.
The classical active learning strategies don't work very well for deep learning, mostly
because deep learning is very highly non-linear and also highly complex.
So many of the existing strategies are hard to translate over and it's not something that
you can just easily take and apply in the deep learning setting.
The other reason is that deep learning trains and batches and an active learning.
What we're doing is you start with a small set of label data.
You build a machine learning model using that small set of label data.
The machine learning model is then used to make predictions on your entire pool of unlabeled
data.
In the process of making prediction, the model also identifies points that are difficult.
And this point is then sent to a human being.
The human provides labels for it.
This label is then added back to the original smaller label data set and then you iterate
this process.
So in active learning, you're adding small chunks of newly labeled data, but the small chunk
of newly labeled data doesn't really make much impact in a deep learning setting.
So recently, there's been just advancement that tackles both of those issues.
Are there different types of active learning?
How you describe generally how it works are there different types of algorithms?
How broad is the field of approaches under the banner of active learning?
Yeah, at the heart of active learning is a model that is able to identify difficult
data points and request labels for it.
So how does it actually do that?
It turns out this model actually relies on strategies and this strategy selects the difficult
data points.
So there are classical strategies and there are strategies that there has been adapted
for deep learning.
So classical strategies, the easiest one is random sampling.
In random sampling, you really not following a strategy.
You're randomly picking data points to get labels for.
So this is the simplest possible way.
And something slightly more complicated, but something that actually works is uncertainty.
So when we say models look for difficult data points, what does that mean?
One way to think about that is the models will look for points that it's uncertain about.
And then when we get there, then we need to actually quantify what uncertainty means.
So there's a slew of strategies that try to quantify uncertainty.
And one way is to look at the distance between a particular data point and its decision boundary.
These data points that are far from the decision boundaries, you can interpret that as points
that the model is very certain about.
Any smog changes in the decision boundary doesn't really affect the classification of those
points.
But when the points are very close to the decision boundary, you can think of that as the
model being uncertain about these points because any slight changes in the decision boundary
due to model refinement will cause these points to be classified differently.
So in margin sampling, for example, we are essentially looking for data points that are
very close to the decision boundary and getting labels for those.
And once we have those labels, they are then added back to our original label data pool
and then used to build a better model.
I guess when I think of active learning kind of non-technically, loosely, I think of you
kind of run, like as you said, you change your model, you have your model make some predictions.
And then when we're doing things like classification, we'll also often see a return value that's
like the probability, a confidence or the probability with which your model thinks that it's
the right classification.
Is that probability the same as can we use that as part of active learning or?
Yes, that's in classical active learning strategies that works.
You can use the prediction probability as a proxy for how confident the model is, for
example.
But this way to measure uncertainty for deep neural networks doesn't really work because
mostly because deep neural networks are very, very complex and there are a lot of parameters.
So there are times when you could perturb the image using some noise and you will get
an unexpected misclassification.
So this is along the lines of the generative adversarial problem.
You can perturb an image that goes into a deep neural network and have it misclassify
unexpectedly.
So for example, you would send an image of a panda with some noise into the deep neural
network and it will classify as a given, but the prediction probably is actually really
high.
So although both images look like a panda to a human being, to the network, the one that's
perturbed looks like a given and the deep neural network is very certain of that.
So it will say, oh, this is not a panda, this is a given with 99%.
So if we use that prediction probability as a gauge of uncertainty in deep neural networks,
that doesn't really work.
So there are many different ways to essentially look at how to judge uncertainty in deep
neural networks and in our report, we look at three major ones.
The first one is essentially to use the adversarial approach and to measure what you would like
to do is to measure the distance between a data point and the decision boundary, similar
to what we do in the classical approaches, but that's not tractable in deep neural networks
because the decision boundary is highly non-linear and you don't really actually know what it
looks like.
So what ends up happening is we could estimate that distance.
For example, we could use the distance between a data point and it's next closest, a different
class object.
But that one will only give you a very coarse estimate of the distance that you're looking
for.
A more creative approach is to use the adversarial approach.
So what you try to do is perturb the data points and perturb it such that it's such
classes unexpectedly.
So when you get a misclassification unexpectedly, the particular data point has switched to a
different class and you use the magnitude of the perturbation to estimate the distance
between that data point and the decision boundary.
So that is one approach that we mentioned in the report.
We also mention a fundamentally different way to think about uncertainty for deep neural
networks and that essentially gets into a Bayesian neural networks and how to estimate
the posterior using dropouts.
Can you elaborate on that a little bit more?
You knew that question.
So like I said before, if you just look at the prediction probability for deep neural
networks and if you use that to estimate uncertainty, that's very misleading.
So we need a different way to think about uncertainty.
Now if you think about it from the Bayesian approach with Bayesian neural networks, what you can
do is to actually get the weight distribution of the neural network given the trading data.
Once you have the probability distribution of the weights, you can then write out mathematically
really nicely the prediction probability of the deep neural network.
But that mathematical expression is pretty but it's not easy to compute.
What you can do then, let me take one step back.
It's pretty and it's not easy to compute and in addition, you actually have to implement
a Bayesian neural network in order to get that posterior.
That posterior is the probably distribution of the weights.
So we don't want to do that.
How do you actually estimate the posterior?
In terms of how you can use dropout as a way to estimate the posterior.
And once you have the estimate for the posterior, the whole mathematical formula for the prediction
probability simplifies down into something that we can easily compute.
How does dropout give you the weight?
So dropout, I think many of us are familiar with dropout as a regularization technique.
When we dropout, you randomly set some weights connecting to the neurons to be zero with certain
probability.
And once that weight is set to zero, you have a smaller neural network.
But you still train that neural network using the initial training data.
So what ends up happening is you have a smaller neural network, you're forcing it to work harder.
One way to think about that is also through the workforce analogy.
If you have a workforce of 100 people, they perform their jobs on a daily basis.
But now when the workforce becomes smaller, you are essentially forcing the workforce to
do the same amount of job.
So everyone can do different types of things.
So that's essentially what regularization is trying to do.
Now, when we use dropout in regular deep neural network training approaches, we train
it with dropout turned on.
But during inference, we turn off the dropout and then we adjust the weight accordingly.
Now if you think about this, if you leave the dropout turned on during inference, what
you're getting essentially as samples of the neural network.
Every time you run inference, you get a different neural network.
So if you run inference multiple times, you get multiple different neural networks along
with multiple different sets of weights.
And you also know with what probability or what likelihood each set of weight will happen.
Why do you have different sets of weights when you're running an inference?
I'm imagining that you go through some process of training your model, you create a model,
it's a set of weights, you deploy that on somewhere and you're running inference against
it.
I don't think of those weights as changing.
But if that's the normal way to use dropout, but if you leave it turned on during inference,
what's going to end up happening is you're going to get some neural networks, some weights
such as zero with probability.
I miss that.
We're leaving dropout turned on during inference.
Yes.
And when you do that, you're essentially sampling from the neural network.
So you get different neural networks that looks different, that has different weight configurations.
And if you do it enough times, what you're going to essentially have is all different sets
of weights along with the probability that it will happen.
So we're essentially trying to build the posterior, which is the probability distribution of weights
over training, over all the training data.
So that that is an estimate of the posterior.
But when you do that, it makes all the computation easy.
And then you can actually simplify the expression for prediction probability.
We can rigorously demonstrate that this allows us to estimate the posterior.
Yes.
That's very cool.
It's a very cool, this is actually a very cool paper, an outcome of a very cool paper,
I think, from a year or two ago.
Do you remember the name or author?
So the paper?
Yeah.
L-G-A-L, first name, Yarin, Y-A-A-R-I-N, it was a pretty big deal at that time.
And is there anything particular about the way we do drop out, meaning the drop out parameter,
like is it constrained in any way or any particular pattern to the way we do drop out or doesn't
matter?
So you essentially drop out each layer of weight, and then you just get a sample of the
neural network that way.
So that is one way to think about uncertainty neural network, and that when you couple that
into the active learning process, what you would do is to run drop out during inference,
run multiple sets of inference, and then each set of inference will give you a prediction
probability.
And you take the average of that as your estimate of the uncertainty, and then that gets fed
into other uncertainty methods like entropy, for example, to figure out which points should
get labeled.
Okay.
So you're not taking that estimate directly, you have to then apply it to an entropy-based
model or something like that.
Can you explain how those work?
Yeah, so entropy, the idea of entropy is that outcomes of uncertain events carry more
information when compared to outcomes of events that we are very certain about.
So if you think about coin toss, the maximum entropy for coin toss is one.
So when does that happen?
That happens when we have a fair coin.
When you toss a fair coin, you don't know what you're going to get, right?
The probability is equally likely of you getting a head or a tail.
So that's when you get an entropy of one.
Now when you think of a biased coin, and in the very extreme case, a double headed coin,
that event, that coin toss event has no uncertainty because you know what you're going to get.
So the entropy for that is zero, and the entropy for a double tail coin is also zero.
So for using, when you use entropy and active learning as a way to quantify uncertainty,
what you do is you compute entropy for all your unlabeled data points, and then you select
the entropy, I'm sorry, you select a data points that has the highest entropy to get labels
for.
And the entropy computation depends on the prediction probability.
So in the deep neural network setting, what you would do is you would estimate that you
would get the prediction probability using dropouts, and you feed that probability into the
entropy approach to get your data points to get labels for.
We've got active learning, working for deep learning.
We've got a set of methods that work, is this something that, you know, your typical kind
of enterprise customer can just pull off the shelf and use or are they building it from
scratch?
How hard is it to do?
You make it sound very easy.
That'll just, you know, just do some dropouts, sprinkle some entropy over here.
How hard or easy is it to actually do in practice, and how robust is it, is it, I mean,
it's not actually changing the performance of your network, but the robustness would be
in the degree to which it's, to which it's a sample efficient, right?
Yeah, so let me talk about the, how you would do it first, and talk about the performance.
So the active learning approach is a workflow approach.
What I mean by that is you have to have a workflow that allows you to first take a small set
of label data, build a model with it, use the model along with the active learning strategy
to find the difficult data points, then get labels for those data points.
The labels are provided by human, and then you have to be able to get those labels back,
and then add them back to your original label data set, and then repeat.
So there are a couple of blocks for this to work.
Obviously, you have to be able to build a model, right?
Building a machine learning model, I think many of our clients can do.
You have to build a model, you also have to be able to implement the selection strategy,
and these can just be, these are not very hard to implement.
In real life, it's just a couple of lines of Python code, for example.
You have to be able to compute entropy, you have to be able to, if you're using a deep
neural network, you have to be able to use some neural deep network, appropriate techniques
like dropout or ensemble approaches.
Once you implement those, what you get at that point is a set of data points that need
to be labeled.
Now in the research setting, what usually happens is the person building the model will create
the labels for that.
But if you were to kind of productionize this, you need maybe more workforce to label
these type of data, and the data that we're talking about can either be images or text,
as humans can label those.
So with active learning, we're kind of limited to use cases where the data comes in the form
of text or images.
If you're building an application for detecting, for medical diagnosis, then you actually need
an intelligent workforce to label those images for you.
If you're building applications for text, sometimes you can do it yourself, sometimes you
also need to farm it out to somebody else that can do a larger scale.
Once you have those, you need to be able to get those data point labels back and integrate
it into your system easily and quickly that will allow you to do a second iteration.
So there's kind of the building blocks of active learning, the key is to obviously make
everything as streamlined as possible.
There are a lot of platforms that will connect your data to the workforce that's providing
the labels seamlessly.
So that part is kind of taken care of, but you just have to be aware that this whole process
should be as seamless as possible because you need to iterate.
So that in practice, that's how active learning would work.
Is there something that active learning is not that good at?
Does that affect their performance?
That was your second question, right?
Essentially.
Yeah.
How robust is it?
Is it reliable as a technique?
Yeah.
So there are a couple of things about active learning that we should always be aware of first.
In active learning, there's a learner or a model.
And then there's also the selection strategy.
So the learner makes the predictions.
The strategy steps in and picks the ones that the learner has the most difficulty with,
and then it feeds it back, gets a label and feeds it back.
So in general, in any machine learning problem, picking the right type of learner is difficult,
but it's made even more difficult under the active learning setting for two reasons.
First, you have to pick the learner when you only have a small set of label data.
Second, the learner is not only used to make predictions, it is used in conjunction with
the strategy to help refine it.
So the tight feedback loop between the learner and the strategy amplifies the effect of a
wrong learner.
What generally happens is we advise clients to use maybe not only a single type of learner,
use an ensemble of learners of different types, and that would essentially prevent your
label data pool from being tied to any particular model in general.
So that's one thing.
A second thing is the impact of these strategies.
Some strategies will result in a biased label data pool when you compare to other strategies.
So we can use a very simple example in the margin sampling approach.
What we're trying to do is to pick data points that's very close to decision boundaries
to get labels for.
The data points that are far from the decision boundary, sometimes are not even used
to build the model.
So essentially your model is built using a set of data that's maybe not very representative
of your original data pool.
It's not representative in that, by definition, it's data that the model has problems with.
Yes.
Yes, because we're only picking data points that the model finds hard and then we're kind
of refining on that set of data points.
And those are the data points that we end up getting labels for.
So data points that the models we're clear about are very far from the decision boundary.
We don't request labels for them and then we don't use them when we built the supervised
machine learning model.
Learning that is like a class of bias are there obvious downsides to over indexing on this
particular class of bias?
Yes.
There is a recognition that active learning will result in some kind of bias.
But there are certain types of selection strategies that will help with that.
The example that I gave was on the margin sampling where we look for data points right around
a boundary.
But when you do an entropy approach, for example, you don't really get data sets as biased
because the entropy approach doesn't just look at the distance.
It is a more balanced approach.
There are other approaches such as density-based approaches, for example, selection.
For those approaches, you're only picking from regions where you have dense data.
So essentially, you're trying to counter the bias issue because I just want data where
I do want to pick data points from regions that I have a lot of other data points around
me.
I'm not focusing on weird little pockets of data.
Our report covers the basic of active learning and obviously that is meant to be an introduction
to active learning for our clients and during our advising hours, what we end up doing
very often is going down different technical paths for different clients based on the problem
and their data.
One of the questions I get all the time is I have problem X. How much data do I need to
collect?
Does active learning help me answer that question?
Unfortunately, it does not.
What it does is...
Does it even help me answer that question relative to not using active learning?
Yes.
I'll give you an example.
For all the reports that we do, we also build prototypes that illustrates the capability.
Our prototype for this particular report is a tool that helps you understand the process
of active learning and how different selection strategies affect your approaches and how
does it work on different data sets.
So we looked at three data sets.
We start with the very simple one, the MNIST data set.
That is the Hello World data set of machine learning.
It's a series of images of handwritten digits from 0 to 9.
And then we looked at a slightly more complex data set.
It is the quick draw data set.
It's a series of hand doodles.
We looked at 10 categories.
The final data set that we looked at is the Caltech data set.
It's a series of real life images, of bridges, of animals, for example.
And we also picked 10 categories from those.
And then we also looked at very different types of selection strategies starting from random
strategy to the more complex ones like drop out an entropy.
And we used active learning to build a model using all of these data sets.
And so for the MNIST data set, if you're familiar with the MNIST data set, the training
data actually has 60,000 points.
But we only start with 5,000.
We build a model using 5,000 data points and we look at the model performance and we
use different selection strategies to select 1,000 points to get labels for.
And then that gets added back to the 5,000.
And that starts the next iteration of active learning.
So with 5,000 data points, we're able to get accuracy somewhere in the, I think, 92%.
And as we increment, I think we did maybe seven or eight rounds of active learning.
And we're able to get to 98% tile of 98% accuracy.
So that's an indirect way to answer your question.
What I'm trying to say is you don't need to use 60,000 data points to build your model.
You can, you only need, we ended up with is 5,000 plus maybe 12.
I think what, what it say, what it is answering is that active learning isn't going, you
know, producing any results that says, you know, either given my problem or independent
of my problem, you know, if I do X kind of active learning, I'm going to require 75% less
data or something like that.
It's all experimental.
It's experimental.
Yes, it's experimental.
But what it's saying is that we know we need a lot of data.
So we random, we don't random.
We just kind of brute force create labels for everything.
It turns out a lot of these labels are not useful to the model, right?
So we kind of, if you think about these labeling projects for autonomous vehicles, they range
and they start at a million dollars and go up from there.
You just label everything and then you hope that it works, right?
But when you use active learning in that kind of setting, you realize that, hey, I don't
have to label so many images, I only need to label the images that will be helpful for
my model.
So that obviously brings down the cost and sometimes it allows us to get to a model faster.
And more importantly, the model performs almost just as well, at least in the research setting
when you compare to a model that you would have built using a much larger data set.
How was that in November, December?
I forget.
AWS re-invent, they announced a new extension to the SageMaker platform, SageMaker ground
truth, which incorporates active learning to some extent.
There are some other tools out there.
The folks behind Spacey, the NLP library have a, what is it, prodigy, prodigy, right?
Have a library called prodigy that incorporates active learning.
Have you looked at kind of these off-to-self solutions and what have you seen there?
Yeah, so all of our reports come with a landscape chapter.
We look at open-source, we look at vendors that help with this particular capability, and
this is usually very helpful for clients who are thinking of implementing any of these
capabilities.
So as part of that, we looked at prodigy, we also looked at a lot of labeling providers.
And the main difference between prodigy and AWS SageMaker is that prodigy, at least
from when we're playing with it, is more of a complete solution for active learning.
And I say that because with prodigy, you can actually modify any of your selection strategies.
I think it comes with uncertainty sampling using entropy out of the box.
But you can essentially link to any selection strategies that you write yourself.
You just have to link it in to the project framework.
AWS doesn't have that flexibility.
It's more of kind of a black box type of solution.
Okay.
So that's the main difference.
If I wanted to play with this, what's the quickest way to get started doing it?
You said it's only one line of Python, right?
There's more than one line.
If you're trying to get an intuition for active learning, I would go to our prototype.
It's public.
It's active learner dot fast for labs.com.
Okay.
And you can kind of play with that.
It's a pretty cool visualization of the process of active learning.
If you're trying to do it yourself, you can definitely try prodigy.
If not, you can just start right building your own model.
Once you have the model, all you need to do is to extract out the prediction probability
and feed it into a selection strategy.
You can use start with, we'll always say you should start with random as a baseline.
And then move up to more difficult strategies and see how they impact your data because
everyone has a different data set, right?
It all behaves differently.
But because active learning tend to introduce bias or can introduce bias, it's helpful to
always start at the base, which is the random sampling and then go up to maybe a more difficult
one, which is the least, which is the margin sampling and then go into the entropy approach.
And kind of just look at the data that's surfaced, the data points that surfaced that's asking
labels for.
You can create labels for those yourself and then you can feed it back to your model and
try iterating it that way.
Having kind of explored this area and learned a bunch about active learning in the landscape,
you kind of see a world where active learning becomes kind of a standard, it's a pretty
exotic thing now, right?
Not everyone is doing it every time because it's just a standard tool that we apply.
Do you think that over time it becomes a standard tool that is applied in most situations
or does the bias or other downsides exist such that you want to be more selective about
when you use it?
I think it should be a tool that you use every time when you're starting to build a model.
When you're trying to get a sense of what works with your data and what doesn't, it's
definitely something that you should use also because you might be labeled constraint
at that point.
So it's a good starting point.
Whether or not it will work as a workflow for all projects, it's hard to say.
But the idea is very simple.
What it's trying to do is reduce this amount of labeling that needs to be done, but at
the same time doesn't affect model performance.
So if you just think of that, I would say that...
It should be something that's attempted at least in the beginning, and that will also
help you discover something about your data that you might not have realized before.
It also will help you verify whether a modeling approach will work or not before you label
everything.
But if active learning works, you actually don't have to label everything.
You just continue with the process and you just end up labeling a selection of unlabeled
data that is the most helpful for the model.
And there are many use cases within enterprises.
I mentioned for autonomous vehicles, it's super helpful, which is because the labeling
causes so high and everyone just indiscriminately labeling.
If you think about text-based examples, companies get, for example, many inquiries from the
customers, right?
And these inquiries need to be routed to the right departments.
But right now, the routing portion is always the bottleneck.
We can build a machine learning model to do the routing, but we need labels.
Now when you use active learning, it's not as daunting because you don't have to label
everything in the beginning, you just start with a small set of label data and then you
iterate on that and then you figure out which ones you have to label.
And that will help you relatively quickly come to a model and then you can, if you want
to productionize it, you might then have to label at a larger scale.
Awesome.
Well, it sounds like an exciting project.
Sheldon, thanks so much for taking the time to share it with us.
You're very welcome.
Awesome.
Thank you.
All right, everyone, that's our show for today.
If you like what you've heard here, please do us a favor and tell your friends about
the show.
And if you haven't already hit the subscribe button yourself, make sure to do so so you
won't miss any of the great episodes we've got in store for you.
For more information on any of the shows in our strata data series, visit twomolei.com
slash strata sf19.
Thanks once again to cloud error for sponsoring this series.
Be sure to check them out at cloud error.com slash ml.
As always, thanks so much for listening and catch you next time.
