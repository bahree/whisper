1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:16,400
I'm your host, Sam Charrington.

3
00:00:16,400 --> 00:00:24,280
Hey, what's up everyone?

4
00:00:24,280 --> 00:00:27,680
Happy Memorial Day to those of you in the States.

5
00:00:27,680 --> 00:00:31,960
Although we might not be able to celebrate holidays like we once would, I encourage

6
00:00:31,960 --> 00:00:37,280
you to find a way to enjoy yourself this weekend, connect with family and friends and enjoy

7
00:00:37,280 --> 00:00:42,320
some good food and fun as best as you can.

8
00:00:42,320 --> 00:00:48,120
I am super excited to be hosting tomorrow's live panel discussion on advancing your data

9
00:00:48,120 --> 00:00:50,840
science career during the pandemic.

10
00:00:50,840 --> 00:00:55,080
This is going to be a great one featuring an amazing lineup and I encourage you to join

11
00:00:55,080 --> 00:00:56,080
us.

12
00:00:56,080 --> 00:01:02,480
You can check out twimalai.com slash DS careers for more information and to register.

13
00:01:02,480 --> 00:01:08,160
I also want to give you a heads up regarding my upcoming webinar with Algorithmia CTO

14
00:01:08,160 --> 00:01:09,640
Kenny Daniel.

15
00:01:09,640 --> 00:01:14,680
The hot topic at last year's TwimalCon on conference was a discussion on whether you

16
00:01:14,680 --> 00:01:19,240
should build or buy an ML or data science platform.

17
00:01:19,240 --> 00:01:23,600
Well, we'll be tackling this topic head on in our upcoming session.

18
00:01:23,600 --> 00:01:27,880
We'll discuss what goes into building a machine learning management platform, how to make

19
00:01:27,880 --> 00:01:33,000
the business case for ML ops at your company and how to evaluate off the shelf machine

20
00:01:33,000 --> 00:01:35,360
learning management solutions.

21
00:01:35,360 --> 00:01:41,440
Be sure to mark your calendar for 10 a.m. Pacific on June 9th and visit twimalai.com slash

22
00:01:41,440 --> 00:01:43,560
Algorithmia to register.

23
00:01:43,560 --> 00:01:50,320
That's twimalai.com slash ALGORITHMIA.

24
00:01:50,320 --> 00:01:53,920
All right, enjoy the show and enjoy the holiday.

25
00:01:53,920 --> 00:01:55,160
All right, everyone.

26
00:01:55,160 --> 00:01:57,400
I am on the line with Joey Gonzalez.

27
00:01:57,400 --> 00:02:02,400
Joey is an assistant professor at UC Berkeley in the EECS department.

28
00:02:02,400 --> 00:02:05,240
Joey, welcome to the Twimalai podcast.

29
00:02:05,240 --> 00:02:06,480
Thank you for having me.

30
00:02:06,480 --> 00:02:11,920
I'm really looking forward to diving into this conversation.

31
00:02:11,920 --> 00:02:18,440
And in particular, talking about ML systems and your recent paper on train large then

32
00:02:18,440 --> 00:02:24,280
compress, but before we do that, please share a little bit about your background and how

33
00:02:24,280 --> 00:02:27,360
you came to work in ML and AI.

34
00:02:27,360 --> 00:02:28,360
Yeah, excellent.

35
00:02:28,360 --> 00:02:30,240
So my story's a bit funny.

36
00:02:30,240 --> 00:02:35,680
I started my PhD at Carnegie Mellon with an interest in actually flipping helicopters

37
00:02:35,680 --> 00:02:39,440
because that was a trendy thing to do back in 2006, a while back.

38
00:02:39,440 --> 00:02:40,440
Flipping helicopters.

39
00:02:40,440 --> 00:02:41,440
Flipping helicopters.

40
00:02:41,440 --> 00:02:42,440
Flipping helicopters.

41
00:02:42,440 --> 00:02:43,440
Flipping helicopters.

42
00:02:43,440 --> 00:02:44,440
Flipping helicopters.

43
00:02:44,440 --> 00:02:45,440
Flipping helicopters.

44
00:02:45,440 --> 00:02:46,440
Flipping helicopters.

45
00:02:46,440 --> 00:02:47,440
Flipping helicopters.

46
00:02:47,440 --> 00:02:52,720
Colleague of mine, Peter Beale, now at Berkeley, when he was finishing up his thesis work,

47
00:02:52,720 --> 00:02:55,680
he was looking at how to do interesting control for helicopters.

48
00:02:55,680 --> 00:02:58,520
I thought that was really cool.

49
00:02:58,520 --> 00:03:03,160
And at CMU, I went to my thesis advice and you've worked on control as well.

50
00:03:03,160 --> 00:03:05,120
I'm kind of interested in flipping helicopters.

51
00:03:05,120 --> 00:03:10,560
I think that's really neat research and you know, I didn't know that was the thing.

52
00:03:10,560 --> 00:03:11,560
Well, it was.

53
00:03:11,560 --> 00:03:14,880
And it actually was some of the pioneering work to what we see today in reinforcement

54
00:03:14,880 --> 00:03:15,880
learning.

55
00:03:15,880 --> 00:03:20,360
But what's kind of cool about the story is my advisor at that time being a real machine

56
00:03:20,360 --> 00:03:21,360
learning researcher.

57
00:03:21,360 --> 00:03:26,520
I was like, you know what, flipping helicopters, that's exciting, but there's something

58
00:03:26,520 --> 00:03:29,640
more important, like we can actually help the world with sensors.

59
00:03:29,640 --> 00:03:34,320
We can build sensor networks to monitor fires and we can use kind of principled machine

60
00:03:34,320 --> 00:03:35,640
learning techniques.

61
00:03:35,640 --> 00:03:38,840
I should add that when I was looking at the flipping helicopters, like, you know what,

62
00:03:38,840 --> 00:03:41,120
we should flip them with neural networks.

63
00:03:41,120 --> 00:03:44,920
And the other thing my advisor said, which was good advice at the time, was neural networks

64
00:03:44,920 --> 00:03:47,280
aren't really serious research.

65
00:03:47,280 --> 00:03:51,800
We use more statistical methods, graphical models, things that have formal foundations

66
00:03:51,800 --> 00:03:57,440
that we can reason about and write kind of detailed analysis and understand what our models

67
00:03:57,440 --> 00:03:58,440
are doing.

68
00:03:58,440 --> 00:03:59,440
And that was good advice.

69
00:03:59,440 --> 00:04:04,760
And so I went down this path of how to build Gaussian processes, Bayesian on parametric

70
00:04:04,760 --> 00:04:08,920
methods to reason about link quality and sensor networks.

71
00:04:08,920 --> 00:04:12,480
And in that process of doing that, I kind of stumbled into a problem.

72
00:04:12,480 --> 00:04:17,560
I was writing a lot of MATLAB code to compute big matrix inverses and then approximations

73
00:04:17,560 --> 00:04:19,000
to that to make it run faster.

74
00:04:19,000 --> 00:04:22,280
And one of the things I enjoyed doing in the process of, you know, exploring these more

75
00:04:22,280 --> 00:04:26,720
efficient MATLAB programs was trying to make them more parallel.

76
00:04:26,720 --> 00:04:30,360
And I think my advisor included is a good advisor is like, you know what, maybe you enjoy

77
00:04:30,360 --> 00:04:31,360
that more.

78
00:04:31,360 --> 00:04:34,080
So maybe instead of focusing on the non-permetrics and the sensor networks, let's start

79
00:04:34,080 --> 00:04:37,160
to think about how to make machine learning more efficient.

80
00:04:37,160 --> 00:04:41,800
And in particular, at that point in time, Hadoop was taking off and said, you know what,

81
00:04:41,800 --> 00:04:44,760
MapReduce, that's going to change machine learning.

82
00:04:44,760 --> 00:04:48,320
And we were thinking, well, we're working on graphs and they just don't fit the MapReduce

83
00:04:48,320 --> 00:04:49,720
pattern.

84
00:04:49,720 --> 00:04:54,760
And the kinds of computation we were doing just, it wasn't, it didn't actually fit the

85
00:04:54,760 --> 00:04:57,120
technology that people were building.

86
00:04:57,120 --> 00:05:00,200
So we started to explore a different design of systems.

87
00:05:00,200 --> 00:05:05,360
So design of systems for computation on graphs, which took us down the design of graph processing

88
00:05:05,360 --> 00:05:06,360
systems.

89
00:05:06,360 --> 00:05:10,120
System that I ended up writing is kind of the end of my thesis was a graph lab for doing

90
00:05:10,120 --> 00:05:13,040
very large analysis of graphs.

91
00:05:13,040 --> 00:05:17,000
And so by the time I finished my PhD, I was actually writing systems papers, not machine

92
00:05:17,000 --> 00:05:18,480
learning papers.

93
00:05:18,480 --> 00:05:20,680
And the field was changing very, very rapidly too.

94
00:05:20,680 --> 00:05:25,960
This around 2012, if anyone's been following the history of machine learning around 2012,

95
00:05:25,960 --> 00:05:28,960
everyone started to realize maybe actually the neural nets were a good idea.

96
00:05:28,960 --> 00:05:33,680
The deep learning, these ideas actually really dated back the 1980s.

97
00:05:33,680 --> 00:05:35,880
They're actually really starting to work.

98
00:05:35,880 --> 00:05:38,760
And they were changing the field of machine learning.

99
00:05:38,760 --> 00:05:42,720
And graphs were also taking off, so we built actually a company around the systems that

100
00:05:42,720 --> 00:05:47,720
I was developing as a graduate student, it was graph lab, that evolved into a company

101
00:05:47,720 --> 00:05:52,800
for building tools for data scientists to do interesting machine learning at scale.

102
00:05:52,800 --> 00:05:56,120
That was ultimately acquired by Apple.

103
00:05:56,120 --> 00:05:59,840
And around that time, I also joined the UC Berkeley Amplab as a postdoc.

104
00:05:59,840 --> 00:06:03,240
And there was, you know, a chance to come out to California and it was a really exciting

105
00:06:03,240 --> 00:06:08,160
opportunity to do research in a different system, a system called Spark, which eventually

106
00:06:08,160 --> 00:06:09,680
became Apache Spark.

107
00:06:09,680 --> 00:06:13,480
And there we started to develop the graph processing foundation for the Apache Spark

108
00:06:13,480 --> 00:06:15,080
system.

109
00:06:15,080 --> 00:06:19,480
And again, as I started to explore more and more into the field, I learned more about

110
00:06:19,480 --> 00:06:25,120
research in data systems and transaction processing and how those connect back to machine learning.

111
00:06:25,120 --> 00:06:28,840
And so after finishing my postdoc, I came to Berkeley.

112
00:06:28,840 --> 00:06:33,080
In fact, I chose not to follow the much more lucrative path of the startup.

113
00:06:33,080 --> 00:06:34,920
It was going to ask about that.

114
00:06:34,920 --> 00:06:37,600
Yeah, I made a terrible financial decision.

115
00:06:37,600 --> 00:06:40,240
But I'm happy because I have a chance to work with students.

116
00:06:40,240 --> 00:06:44,320
I'm a little less happy because I'm not as wealthy as one could have been.

117
00:06:44,320 --> 00:06:49,040
But now I am teaching students that do research at the intersection of machine learning and

118
00:06:49,040 --> 00:06:50,360
systems.

119
00:06:50,360 --> 00:06:55,440
And so we have a pretty broad agenda around how to build better technologies for delivering

120
00:06:55,440 --> 00:07:00,040
models to manage machine learning life cycle, not just training, but prediction, how to

121
00:07:00,040 --> 00:07:04,640
prioritize training experiments on the cloud to use serverless computing to make machine

122
00:07:04,640 --> 00:07:07,720
learning more cost effective and easier to deploy.

123
00:07:07,720 --> 00:07:12,160
We have a big agenda around autonomous driving, building the actual platform that supports

124
00:07:12,160 --> 00:07:16,400
autonomous driving, not necessarily the models, but how they are connected together to make

125
00:07:16,400 --> 00:07:18,240
a reliable car.

126
00:07:18,240 --> 00:07:21,640
And we have work in natural language processing and computer vision.

127
00:07:21,640 --> 00:07:25,200
And one of those papers, one that I'm hoping to talk a bit about today, which is our work

128
00:07:25,200 --> 00:07:28,920
on making a vert models easier to train.

129
00:07:28,920 --> 00:07:33,600
And it, too, has a kind of funny story how we came to actually a realization that what

130
00:07:33,600 --> 00:07:35,840
we were thinking was entirely wrong.

131
00:07:35,840 --> 00:07:38,280
And that's what that paper talks a bit about.

132
00:07:38,280 --> 00:07:42,080
Well, let's get to that funny story in a second.

133
00:07:42,080 --> 00:07:45,400
There's so much interesting stuff that you just mentioned.

134
00:07:45,400 --> 00:07:48,840
It's there, there are at least three or four interesting podcasts in here.

135
00:07:48,840 --> 00:07:52,920
I'd love to dig into some of the stuff you're doing with serverless at some point and how

136
00:07:52,920 --> 00:07:59,320
that intersects with ML and AI, something I've looked at a little bit as well.

137
00:07:59,320 --> 00:08:05,800
But before we jump into even more of that, I'm curious.

138
00:08:05,800 --> 00:08:11,680
Your co-founder at GraphLab and Tury Carlos Gastron was one of my very first guests on

139
00:08:11,680 --> 00:08:12,680
this show.

140
00:08:12,680 --> 00:08:15,640
Tumel talk number seven, in fact.

141
00:08:15,640 --> 00:08:19,400
And I'm curious how you came to know and found the company with Carlos.

142
00:08:19,400 --> 00:08:20,720
Yeah, Carlos is awesome.

143
00:08:20,720 --> 00:08:22,640
So he was my thesis author.

144
00:08:22,640 --> 00:08:23,640
Oh, okay.

145
00:08:23,640 --> 00:08:28,440
When I came to CMU, Carlos was the guy who said, let's not flip helicopters, let's do

146
00:08:28,440 --> 00:08:31,520
something that could make an impact in the world.

147
00:08:31,520 --> 00:08:32,520
He was a great advisor.

148
00:08:32,520 --> 00:08:36,560
He pushed me down the right path in my PhD, the thing that reflected what I was interested

149
00:08:36,560 --> 00:08:37,560
in.

150
00:08:37,560 --> 00:08:41,680
And he was one of the pioneers in the modern field of machine learning and systems.

151
00:08:41,680 --> 00:08:42,680
Yeah.

152
00:08:42,680 --> 00:08:43,680
So that's how I came to know.

153
00:08:43,680 --> 00:08:44,680
He did go to Apple.

154
00:08:44,680 --> 00:08:45,680
He did.

155
00:08:45,680 --> 00:08:50,840
He saw him recently at Nurebs, most recently in Vancouver.

156
00:08:50,840 --> 00:08:52,920
It seems to be really having a good time there.

157
00:08:52,920 --> 00:08:57,040
Yeah, he's had a chance to have a lot of impact doing really cool stuff.

158
00:08:57,040 --> 00:09:02,280
You kind of laid out this broad space of research.

159
00:09:02,280 --> 00:09:05,240
It sounds very broad, actually, tied together by systems.

160
00:09:05,240 --> 00:09:09,800
I'm curious how you kind of, you know, is it rationalized by, hey, you've got a bunch

161
00:09:09,800 --> 00:09:14,040
of, you know, students and you're letting them flip helicopters in the way that they want

162
00:09:14,040 --> 00:09:20,120
to flip helicopters more so than you were, you know, and so it's you or it's challenging

163
00:09:20,120 --> 00:09:23,560
as faculty to decide what is your research agenda.

164
00:09:23,560 --> 00:09:26,880
One likes to imagine you sit there and go, here are the three two things.

165
00:09:26,880 --> 00:09:30,960
I want to study, usually not one, because you have to have enough for a couple of students

166
00:09:30,960 --> 00:09:33,400
to build, you know, their thesis around.

167
00:09:33,400 --> 00:09:37,840
The reality is that students pull you and I actually, I think sort of like artists, it's

168
00:09:37,840 --> 00:09:41,760
hard to compel people to follow the research agenda that you ultimately want.

169
00:09:41,760 --> 00:09:43,160
My advisor did a great job.

170
00:09:43,160 --> 00:09:44,760
It's not about telling you what to do.

171
00:09:44,760 --> 00:09:48,200
It's about showing you the exciting opportunities you can explore.

172
00:09:48,200 --> 00:09:53,800
And so with my students, I've pushed them in directions to think about how we make models

173
00:09:53,800 --> 00:10:01,200
more efficient to not just train, but to serve, how we support new emerging applications

174
00:10:01,200 --> 00:10:06,760
of machine learning that might require innovation both in the system and the modeling techniques.

175
00:10:06,760 --> 00:10:11,160
And actually what's kind of neat about the field of systems and machine learning is, again,

176
00:10:11,160 --> 00:10:12,960
when I started, it wasn't really a thing.

177
00:10:12,960 --> 00:10:17,080
In fact, some of my colleagues at CMU were like, you're just hacking, you're not actually

178
00:10:17,080 --> 00:10:20,800
doing research, you're not proving anything fundamental about machine learning, you're

179
00:10:20,800 --> 00:10:23,960
writing software, a little bit of that was true.

180
00:10:23,960 --> 00:10:25,600
We were definitely writing a lot of software.

181
00:10:25,600 --> 00:10:29,400
We were trying to prove some stuff too, but I think the impact might have actually been

182
00:10:29,400 --> 00:10:30,960
more on the software side.

183
00:10:30,960 --> 00:10:34,800
And one of the funny things about the broader field of systems and machine learning is

184
00:10:34,800 --> 00:10:40,520
that it actually has been kind of the undercurrent of a lot of the recent progress in AI.

185
00:10:40,520 --> 00:10:48,360
When we look at this revolution in deep learning, we can go back to the 2012, the Alex Net paper.

186
00:10:48,360 --> 00:10:51,720
That's actually not the beginning, it goes way back to the 1980s.

187
00:10:51,720 --> 00:10:56,320
In fact, the techniques are from the 1980s, the architectures, the models, even the algorithms

188
00:10:56,320 --> 00:10:58,080
that we're using are from the 1980s.

189
00:10:58,080 --> 00:11:02,400
If you actually read the Alex Net paper, more than half the papers devoted to how they got

190
00:11:02,400 --> 00:11:06,720
it to run on a GPU, how they got it to run on a very large image data set, and some of

191
00:11:06,720 --> 00:11:10,960
the optimizations they made to the training process to make it run at scale.

192
00:11:10,960 --> 00:11:15,200
So it is the movement to scale that really helped launch the revolution that we are on

193
00:11:15,200 --> 00:11:16,200
today.

194
00:11:16,200 --> 00:11:20,400
Now there's the other factor, which I think people overlook, and it's sort of when I was

195
00:11:20,400 --> 00:11:25,000
doing my PhD, we were writing the foretran of machine learning, we were writing MATLAB

196
00:11:25,000 --> 00:11:29,400
code to implement algorithms and debugging gradient procedures, and that's absurd.

197
00:11:29,400 --> 00:11:31,440
Today, it's just too easy.

198
00:11:31,440 --> 00:11:36,640
So a graduate student can pick up PyTorch, TensorFlow, MX Net, one of these packages,

199
00:11:36,640 --> 00:11:42,520
and very easily architect a new model, and train it on TPUs, GPUs, how do they barely

200
00:11:42,520 --> 00:11:47,840
understand, and get it to run at scale on data sets that they don't have to collect.

201
00:11:47,840 --> 00:11:50,480
So that is an enormous jump forward.

202
00:11:50,480 --> 00:11:54,600
And if you look really carefully and a little bit depressingly, the models didn't change

203
00:11:54,600 --> 00:11:55,600
that radically.

204
00:11:55,600 --> 00:11:57,320
The algorithms didn't change that radically.

205
00:11:57,320 --> 00:11:59,760
What changed was it became a lot easier.

206
00:11:59,760 --> 00:12:04,200
We developed the languages, the tools to make machine learning practical, and it really

207
00:12:04,200 --> 00:12:06,320
boiled down to getting the right abstractions.

208
00:12:06,320 --> 00:12:10,640
And maybe if you roll all the way back when Alex Net came out, they didn't quite have

209
00:12:10,640 --> 00:12:11,640
that.

210
00:12:11,640 --> 00:12:12,640
Alex Net came out.

211
00:12:12,640 --> 00:12:17,320
Theanos started to really take off, cafe at Berkeley started to take off, and it became

212
00:12:17,320 --> 00:12:20,760
so much easier to build that next model and the next model, and so on.

213
00:12:20,760 --> 00:12:24,680
And today we're stuck into a flood of archive papers because basically anyone can download

214
00:12:24,680 --> 00:12:28,880
one of these packages and start building state-of-the-art machine learning models.

215
00:12:28,880 --> 00:12:32,360
There's some learning that you go into the process, but the fundamental principles are

216
00:12:32,360 --> 00:12:37,720
to find your objective, to find your decision process, and then tune your decision process

217
00:12:37,720 --> 00:12:40,960
optimize it for that objective.

218
00:12:40,960 --> 00:12:44,440
That's it, and the undercurrent that drive all of this has been a lot of the innovation

219
00:12:44,440 --> 00:12:47,720
in the systems front, not necessarily the machine learning.

220
00:12:47,720 --> 00:12:52,520
And so my research is trying to find those right abstractions, and especially as we look

221
00:12:52,520 --> 00:12:56,000
at new frontiers, not just training models, but how we deliver them, how we support them

222
00:12:56,000 --> 00:13:00,000
and autonomous driving, and how we adjust the architectures of the models themselves

223
00:13:00,000 --> 00:13:02,960
to make them more efficient in these new kinds of applications.

224
00:13:02,960 --> 00:13:07,480
At when I first started doing these interviews, one of my favorite questions was looking

225
00:13:07,480 --> 00:13:16,400
to explore the way folks came up with new models and trying to find the science behind

226
00:13:16,400 --> 00:13:21,440
it, and I think that the takeaways where a lot of it was, the answer was like graduate

227
00:13:21,440 --> 00:13:22,440
student descent.

228
00:13:22,440 --> 00:13:28,240
We would just throw a graduate student at this, and they tweak something that pre-existed,

229
00:13:28,240 --> 00:13:36,040
but there wasn't necessarily a hard science behind how to come up with a new model architecture.

230
00:13:36,040 --> 00:13:40,360
But so we've seen a lot of innovation like and around, you know, Bert and the kinds

231
00:13:40,360 --> 00:13:47,280
of transformer models that we're seeing here, you know, has that, has, you know, would

232
00:13:47,280 --> 00:13:52,040
your answer to that be kind of similar, has it, you know, it changed a lot, or how do

233
00:13:52,040 --> 00:13:57,040
you think of, you know, beyond kind of that high level process, you just laid out?

234
00:13:57,040 --> 00:14:00,720
How do you think of the process for coming up with these new types of architectures?

235
00:14:00,720 --> 00:14:02,920
Yeah, so that's been a struggle for me.

236
00:14:02,920 --> 00:14:07,440
So remember, I start with this religion, this Bayesian philosophy of model development,

237
00:14:07,440 --> 00:14:12,200
they have these principles of priors and likelihoods that gave us at least the basic foundations

238
00:14:12,200 --> 00:14:14,840
of what to think about when building a model.

239
00:14:14,840 --> 00:14:18,960
That's all, that's not gone, but that's, you know, effectively gone for a lot of the

240
00:14:18,960 --> 00:14:19,960
machine learning world.

241
00:14:19,960 --> 00:14:22,240
And so we're left with a lot of the new, though, actually, right?

242
00:14:22,240 --> 00:14:25,880
Like the concept of modeling is on the rise, I think.

243
00:14:25,880 --> 00:14:29,480
So I should say it's not gone, and it's very important to note that a lot of the world

244
00:14:29,480 --> 00:14:34,120
actually runs on these more traditional techniques. It's the research community where we're writing

245
00:14:34,120 --> 00:14:38,680
these new papers for language modeling or speech or driving where there's very specific

246
00:14:38,680 --> 00:14:43,200
cases that have been kind of dominated by deep learning techniques.

247
00:14:43,200 --> 00:14:48,960
But the Bayesian methods are still, you know, fully alive and medicine and even traditional

248
00:14:48,960 --> 00:14:51,280
advertising algorithms.

249
00:14:51,280 --> 00:14:55,400
But with that in mind, so when I start to look at the deep learning work, how do I find

250
00:14:55,400 --> 00:14:56,800
those principles?

251
00:14:56,800 --> 00:15:00,440
And actually, they actually exist. They're a little smaller.

252
00:15:00,440 --> 00:15:04,560
And sadly, we start to embody the model with personality, like the model is trying to

253
00:15:04,560 --> 00:15:08,600
do X, which is sad because that's not how we like to, you know, formally think of things.

254
00:15:08,600 --> 00:15:15,160
But these building blocks, convolution, recurrence, attention, each of these becomes tools that

255
00:15:15,160 --> 00:15:18,720
we can use to architect our models.

256
00:15:18,720 --> 00:15:22,160
When students go, how deep should we make it while we try to go a little deeper?

257
00:15:22,160 --> 00:15:28,600
They start to look at how it affects convergence rates, variation in batch size and its relation

258
00:15:28,600 --> 00:15:29,600
to batch form.

259
00:15:29,600 --> 00:15:31,880
So we have little rules of thumb.

260
00:15:31,880 --> 00:15:36,680
And unfortunately, there's no great, like, my PhD students like two or three years to get

261
00:15:36,680 --> 00:15:39,840
up to speed with the rules of thumb in the area that they're working in.

262
00:15:39,840 --> 00:15:43,000
And once they have that, I hope they teach the next PhD students and so on because it's

263
00:15:43,000 --> 00:15:47,080
hard to really grasp what those are. It's more like, it comes from experience working with

264
00:15:47,080 --> 00:15:48,720
these models and going, ah, right?

265
00:15:48,720 --> 00:15:52,840
So like the transformer, in this particular piece of work that we've been exploring, like

266
00:15:52,840 --> 00:15:55,760
how to make it more efficient, we're like, should we make it deeper, should we make it

267
00:15:55,760 --> 00:15:56,760
wider?

268
00:15:56,760 --> 00:15:57,760
Who knows?

269
00:15:57,760 --> 00:15:58,760
So we start to measure each of these things.

270
00:15:58,760 --> 00:16:02,360
And that's one of the jokes that we make is that machine learnings become more like

271
00:16:02,360 --> 00:16:03,600
a biological science.

272
00:16:03,600 --> 00:16:08,240
It's driven by laboratory experiments, ah, by using compute to understand better the

273
00:16:08,240 --> 00:16:12,520
models that we're building, ah, as opposed to the more, ah, principled approach we might

274
00:16:12,520 --> 00:16:13,520
have had in the past.

275
00:16:13,520 --> 00:16:16,680
We tried to, to frame it in some probabilistic architecture.

276
00:16:16,680 --> 00:16:21,680
You mentioned that there was a story behind the, the work that led to train large than

277
00:16:21,680 --> 00:16:22,680
compress.

278
00:16:22,680 --> 00:16:23,680
Yeah.

279
00:16:23,680 --> 00:16:25,320
So I'm happy to go into that story.

280
00:16:25,320 --> 00:16:26,320
Sure.

281
00:16:26,320 --> 00:16:27,320
Sure.

282
00:16:27,320 --> 00:16:28,320
Yeah.

283
00:16:28,320 --> 00:16:32,320
So the story behind that the train large and compress work, ah, it starts in the following.

284
00:16:32,320 --> 00:16:36,640
So, ah, we've been doing a lot of work in how to make, ah, computer vision models more

285
00:16:36,640 --> 00:16:40,240
efficient, ah, in particular, not for training, but for inference.

286
00:16:40,240 --> 00:16:45,080
Ah, and so we have these skip net architectures, ways of making things more dynamic.

287
00:16:45,080 --> 00:16:47,800
And some of my colleagues go, you know what, maybe we should start thinking about language

288
00:16:47,800 --> 00:16:48,800
models.

289
00:16:48,800 --> 00:16:50,600
They seem to be eating up a lot of cycles.

290
00:16:50,600 --> 00:16:54,840
The, the transformer, the BERT model that's become a kind of a foundation for reason about

291
00:16:54,840 --> 00:16:55,840
text and context.

292
00:16:55,840 --> 00:16:59,040
Well, so that, that model is pretty expensive to run.

293
00:16:59,040 --> 00:17:03,280
Ah, and so we said, all right, maybe we'll explore what we can do in, in the context of

294
00:17:03,280 --> 00:17:05,440
making these BERT models more efficient.

295
00:17:05,440 --> 00:17:09,040
Now, I should say a lot of people are starting to think about that because BERT is, you know,

296
00:17:09,040 --> 00:17:13,840
incredibly expensive to run, ah, on, on text feeds and text is a pretty large, ah, you

297
00:17:13,840 --> 00:17:15,920
know, body of data that we might want to process.

298
00:17:15,920 --> 00:17:20,320
Yeah, I'll mention that, ah, for folks that want to dig into that particular point, I

299
00:17:20,320 --> 00:17:26,200
did an interview with Emma Strubel, ah, who has, um, you know, in, in fair amount of

300
00:17:26,200 --> 00:17:30,440
detail, kind of characterized the, both the cost and kind of environmental impact of

301
00:17:30,440 --> 00:17:33,360
training some of these large scale NLP models.

302
00:17:33,360 --> 00:17:34,360
And it is crazy.

303
00:17:34,360 --> 00:17:38,560
It's, it's crazy, actually, the, the CO2, ah, as narrative was one of the things that

304
00:17:38,560 --> 00:17:42,120
got me, especially, like, I was, ah, maybe we don't touch language, that's, that's

305
00:17:42,120 --> 00:17:45,960
there's plenty of people thinking about it, and then I, I saw Emma's papers, like, wow,

306
00:17:45,960 --> 00:17:49,400
ah, I'm here trying to make autonomous cars so that, you know, a little bit more environmentally

307
00:17:49,400 --> 00:17:52,840
friendly when it comes to driving, when I could go fix a, you know, fundamental prime

308
00:17:52,840 --> 00:17:57,320
right in my field, ah, and so, yeah, so we look at these language models and go, how

309
00:17:57,320 --> 00:18:01,240
can we make these better, ah, and the first step to doing that is we got to understand

310
00:18:01,240 --> 00:18:02,240
them.

311
00:18:02,240 --> 00:18:05,000
So we need to run some experiments, ah, and, and my students go, well, we're going

312
00:18:05,000 --> 00:18:07,480
to need a lot of machines, like, I can't afford a lot of machines.

313
00:18:07,480 --> 00:18:11,440
So if I look at Google at Facebook, they can throw a lot of compute and trying to understand

314
00:18:11,440 --> 00:18:15,240
something, and that's actually one of their, their tools that, that we don't really have

315
00:18:15,240 --> 00:18:19,120
access to, ah, we've actually, ah, started a collaboration with Google so we could get access

316
00:18:19,120 --> 00:18:20,200
to TPUs.

317
00:18:20,200 --> 00:18:23,160
We can't do it at the scale that they're going to run their experiments.

318
00:18:23,160 --> 00:18:24,640
So we had to get lean.

319
00:18:24,640 --> 00:18:29,120
So how can we rapidly iterate on variations in our architectures, ah, we want to look

320
00:18:29,120 --> 00:18:34,000
at different types of, of attention, different architecture sizes, understand the effects

321
00:18:34,000 --> 00:18:37,520
of these hyper parameters, and so my students go, ah, here's what we'll do, we'll make the

322
00:18:37,520 --> 00:18:41,640
models really small and we run a training run every day with different configurations

323
00:18:41,640 --> 00:18:45,160
and we've got a good picture of what's going on, ah, and they did that and so they made

324
00:18:45,160 --> 00:18:49,320
the model really small because that would, in theory, make it really fast to train, ah,

325
00:18:49,320 --> 00:18:53,480
but they also run really small in terms of, ah, good parameters or, yeah, so they made

326
00:18:53,480 --> 00:18:58,160
the model smaller in terms of both the height, ah, the number of layers and the width, the

327
00:18:58,160 --> 00:19:02,040
number of these hidden, ah, hidden layers, hidden parameters inside of each of the attention

328
00:19:02,040 --> 00:19:03,040
heads.

329
00:19:03,040 --> 00:19:06,840
So basically they tried to make the model so it would, ah, it would train faster because

330
00:19:06,840 --> 00:19:12,120
it had less to compute, ah, less to, to update, ah, this is more of a classic way of thinking,

331
00:19:12,120 --> 00:19:15,400
how I would approach a problem too, if it's too big, make the problem smaller, ah, it

332
00:19:15,400 --> 00:19:20,120
should go faster, right, it's less to do, ah, so they did that and it was working, but

333
00:19:20,120 --> 00:19:22,560
one of them was like, well, what if we make it a little bigger just, you know, to get

334
00:19:22,560 --> 00:19:26,760
a point of comparison and they applied the point of comparison on top of the, you know,

335
00:19:26,760 --> 00:19:30,360
the, the smaller models they were training and, and the point of comparison seemed to go

336
00:19:30,360 --> 00:19:33,800
down pretty quickly and I say, well, let's put it in time and you put it in time and

337
00:19:33,800 --> 00:19:39,120
actually the bigger model, the point of comparison was actually getting to a better accuracy quicker

338
00:19:39,120 --> 00:19:43,080
than the smaller models that we were supposed to be running because they're faster to train,

339
00:19:43,080 --> 00:19:46,520
ah, and then we started to wonder, hm, maybe it's the other way around, maybe we had

340
00:19:46,520 --> 00:19:50,080
this backwards all along and if we want to go faster we have to make the problem bigger,

341
00:19:50,080 --> 00:19:54,840
ah, which is really counterintuitive, but it actually turns out to be a really neat intersection

342
00:19:54,840 --> 00:20:00,200
between, ah, how these models converge and how they can take advantage of parallel resources

343
00:20:00,200 --> 00:20:05,120
in the GPUs and TPUs, ah, to get good scaling as you make them bigger, ah, and that sort

344
00:20:05,120 --> 00:20:08,600
of forms the foundation of, of this work that sort of went against what we thought would

345
00:20:08,600 --> 00:20:13,560
be the case, ah, and actually presents a neat way to approach training, ah, these expensive

346
00:20:13,560 --> 00:20:14,560
models.

347
00:20:14,560 --> 00:20:23,640
It is the idea related to the, kind of the rate of change of, ah, of kind of accuracy

348
00:20:23,640 --> 00:20:29,440
for these models and, you know, taking advantage of the idea that the larger models learn

349
00:20:29,440 --> 00:20:37,040
quicker, ah, but the, you know, I guess the, the area under that learning curve is proportional

350
00:20:37,040 --> 00:20:39,800
to your compute cost and you cannot kind of optimize that.

351
00:20:39,800 --> 00:20:41,000
Yeah, so there's a bunch of trade-offs.

352
00:20:41,000 --> 00:20:43,960
Let me, I try to walk through them because they were counterintuitive to me at first

353
00:20:43,960 --> 00:20:44,960
too.

354
00:20:44,960 --> 00:20:48,600
So the first trade-off to think about is, ah, actually let's talk about compute.

355
00:20:48,600 --> 00:20:52,320
So as I make my model bigger, I'm going to compute more, so it's more work and it should

356
00:20:52,320 --> 00:20:53,320
run slower.

357
00:20:53,320 --> 00:20:57,320
But the neat thing is that when I make these models wider at least, ah, I actually expose

358
00:20:57,320 --> 00:21:01,840
more parallelism, and if we look at the execution of these models, it's a little surprising.

359
00:21:01,840 --> 00:21:06,760
We've optimized those, those GPUs and TPUs to have a substantial amount of compute, often

360
00:21:06,760 --> 00:21:11,480
for computer vision models, ah, and so now we have an opportunity when we run a transformer.

361
00:21:11,480 --> 00:21:16,200
If we don't crank the batch, batch size up incredibly high, we actually have a fair amount

362
00:21:16,200 --> 00:21:17,960
of leftover compute that we can use.

363
00:21:17,960 --> 00:21:22,880
So making the model bigger doesn't necessarily translate to a linear increase in runtime.

364
00:21:22,880 --> 00:21:27,800
So we can afford to make the models bigger without, ah, linearly increasing execution.

365
00:21:27,800 --> 00:21:34,640
Parallelism and runtime don't correlate to cost because you're just running more compute

366
00:21:34,640 --> 00:21:35,640
at the same time.

367
00:21:35,640 --> 00:21:36,640
Yeah.

368
00:21:36,640 --> 00:21:38,200
So, ah, this is looking at a single CPU.

369
00:21:38,200 --> 00:21:39,480
Or carbon for that matter.

370
00:21:39,480 --> 00:21:40,480
Yeah.

371
00:21:40,480 --> 00:21:41,480
So, all right.

372
00:21:41,480 --> 00:21:42,480
This is, you're getting to the interesting stuff.

373
00:21:42,480 --> 00:21:45,480
So, so first, let's, ah, in the paper, we actually tried to control for this, because

374
00:21:45,480 --> 00:21:48,960
I was like, ah, on a second, you're, you're just going to increase the cost of compute.

375
00:21:48,960 --> 00:21:52,600
So we looked at one GPU, and what happens is you're not using all the cores on one

376
00:21:52,600 --> 00:21:54,640
GPU when we were looking at the smaller models.

377
00:21:54,640 --> 00:22:00,360
So as we make the models bigger, ah, for a fixed batch size, ah, we can get, ah, an increase

378
00:22:00,360 --> 00:22:02,200
in the utilization of the GPU.

379
00:22:02,200 --> 00:22:06,080
Um, and right now it's not easy to turn off those cores, and you're also paying a fixed

380
00:22:06,080 --> 00:22:10,480
overhead to power the, the actual box that the cores are living in, plus cooling.

381
00:22:10,480 --> 00:22:15,240
So trying to power throttle individual cores on GPUs, generally not a great idea, ah, especially

382
00:22:15,240 --> 00:22:19,120
if we can get better utilization of the cores that we have, ah, now you could say I should,

383
00:22:19,120 --> 00:22:20,120
I should have more GPUs.

384
00:22:20,120 --> 00:22:21,120
We did.

385
00:22:21,120 --> 00:22:26,160
We're going to burn more resources as we turn on more GPUs, ah, but the hope is that

386
00:22:26,160 --> 00:22:27,760
we can get to a solution quicker.

387
00:22:27,760 --> 00:22:32,520
And if, and if those GPUs are already attached to our box, which they often are, there's

388
00:22:32,520 --> 00:22:36,600
usually some incentive to go ahead and try to use those as efficiently as possible.

389
00:22:36,600 --> 00:22:41,400
Um, and so that brings us to the second question, which is if I make my model bigger, is it

390
00:22:41,400 --> 00:22:45,680
really improving any efficiency, which is what we'd like to think of as the improvement

391
00:22:45,680 --> 00:22:50,440
in our, our perplexity, our reduction in perplexity as we, ah, train.

392
00:22:50,440 --> 00:22:55,320
And so we'd like to, to reduce our errors quickly as possible, um, in, in walk clock time

393
00:22:55,320 --> 00:22:57,720
because we have to pay for the power of the building and so on.

394
00:22:57,720 --> 00:23:03,120
Um, so we want to, to train as fast as possible in time, the simpler way to look at that

395
00:23:03,120 --> 00:23:07,480
first is how is it, ah, how is he perplexing or error decreasing as a function of the amount

396
00:23:07,480 --> 00:23:09,120
of data that we touch?

397
00:23:09,120 --> 00:23:11,200
Um, and so there are two knobs there.

398
00:23:11,200 --> 00:23:14,840
So now we're getting to the, the weeds, but there's the batch size, which determines how

399
00:23:14,840 --> 00:23:19,880
much data we look at per step of our algorithm, um, the more data we look at, the better of

400
00:23:19,880 --> 00:23:25,280
an estimate of the direction that minimizes the loss, uh, which in principle should give

401
00:23:25,280 --> 00:23:29,960
us faster convergence, um, it also increases GPU utilization.

402
00:23:29,960 --> 00:23:33,840
So we can use that as another mechanism to get better, uh, utilization out of each of

403
00:23:33,840 --> 00:23:37,640
our piece of hardware, um, but it also has diminishing returns.

404
00:23:37,640 --> 00:23:42,240
So as we increase the batch size, our, our speed at which we're able to converge as a function

405
00:23:42,240 --> 00:23:47,360
of samples we look at, um, doesn't necessarily increase linearly, uh, and one of the other

406
00:23:47,360 --> 00:23:50,760
sort of side effects of this, which if you work in computer vision, you're like, oh,

407
00:23:50,760 --> 00:23:56,200
no, there's a problem, um, that as we increase the batch size, there's some risk of overfitting.

408
00:23:56,200 --> 00:24:00,600
Um, and this is a fact that shows up more in computer vision models where, where it's somewhat

409
00:24:00,600 --> 00:24:06,520
data poor, where in, uh, NLP, it seems at this point, at least that we have opportunities

410
00:24:06,520 --> 00:24:10,760
to, uh, overfit more before we actually are properly overfitting.

411
00:24:10,760 --> 00:24:15,000
So, uh, there's this question of the generalization gap, the gap between how well your models

412
00:24:15,000 --> 00:24:17,680
fitting the training data and the test data.

413
00:24:17,680 --> 00:24:23,280
And in NLP tasks, we're not at a point where we're, that generalization gap is disappearing,

414
00:24:23,280 --> 00:24:27,520
which means that we can increase the batch size quite a bit more, um, without overfitting,

415
00:24:27,520 --> 00:24:30,960
but it also means we can increase the model size quite a bit more.

416
00:24:30,960 --> 00:24:36,000
And so what this paper then does is tries to play off, uh, compare this trade off between

417
00:24:36,000 --> 00:24:39,480
model size and batch size to find the best combination.

418
00:24:39,480 --> 00:24:43,440
And one of the neat results we find is it actually cranking up the model size and a batch

419
00:24:43,440 --> 00:24:47,840
size, uh, to a certain extent as well, kind of gives us the best outcome, uh, it gets

420
00:24:47,840 --> 00:24:52,440
us to a, a model that's more sample efficient, the more samples it sees, the faster it reduces

421
00:24:52,440 --> 00:24:57,800
the, the test air, uh, and it also lets us better utilize our hardware, so we're actually

422
00:24:57,800 --> 00:25:00,240
getting, uh, a gain from parallelism.

423
00:25:00,240 --> 00:25:06,120
And those two forces interact to give us a faster in terms of walk lock time, um, reduction

424
00:25:06,120 --> 00:25:10,000
in the test perplexity or the, the, the air metric that we care about.

425
00:25:10,000 --> 00:25:15,320
In terms of this, uh, generalization gap and the, the differences between it, what you

426
00:25:15,320 --> 00:25:21,920
see in computer vision and, uh, what you see in NLP tasks is that related to the way

427
00:25:21,920 --> 00:25:29,200
the problems are formulated in terms of, uh, supervised versus self-supervised semi-supervised

428
00:25:29,200 --> 00:25:33,680
and the kind of availability of data and labels and that kind of thing.

429
00:25:33,680 --> 00:25:34,680
Absolutely.

430
00:25:34,680 --> 00:25:36,320
So, uh, you're hitting a key point.

431
00:25:36,320 --> 00:25:40,120
So in computer vision, we are largely still focused on supervised tasks.

432
00:25:40,120 --> 00:25:44,840
We need labeled data sets, which, uh, are big, but they're not, uh, as big as we want

433
00:25:44,840 --> 00:25:45,840
them to be.

434
00:25:45,840 --> 00:25:51,560
Uh, whereas in NLP, we can go to really large, uh, unlabeled data sets, uh, cause we're

435
00:25:51,560 --> 00:25:53,240
essentially predicting missing words.

436
00:25:53,240 --> 00:25:57,160
So we've created this self-supervised task, um, and that means that we have so much more

437
00:25:57,160 --> 00:26:02,240
data, we can support bigger models and bigger batch sizes without having this, this generalization

438
00:26:02,240 --> 00:26:03,240
gap disappear.

439
00:26:03,240 --> 00:26:07,560
Uh, we're able to, uh, sorry, without, uh, eliminating, uh, or causing our, our training

440
00:26:07,560 --> 00:26:13,080
error to, um, go to zero and our test error to, to, you know, dominate, uh, so, so there

441
00:26:13,080 --> 00:26:18,280
is, uh, this, this opening created by this, uh, self-supervised training that, that we're

442
00:26:18,280 --> 00:26:19,600
able to take advantage of.

443
00:26:19,600 --> 00:26:23,280
Now, in our experiments, we test both the self-supervised training task as well as the

444
00:26:23,280 --> 00:26:26,840
downstream, uh, translation or, or classification tasks.

445
00:26:26,840 --> 00:26:31,840
It would be applied to actual language modeling, you know, supervised training tasks, but that's

446
00:26:31,840 --> 00:26:36,600
typically done as a small fine tuning step on top of, uh, this extensive pre-training,

447
00:26:36,600 --> 00:26:40,160
which is where all the, the CO2 is going, uh, to pre-training these models.

448
00:26:40,160 --> 00:26:46,640
Uh, and then in, in your description of the, the train large, it, it sounded a little

449
00:26:46,640 --> 00:26:51,800
bit like you're ultimately saying, uh, you know, fully utilized whatever box you're

450
00:26:51,800 --> 00:26:54,920
training on, but there's a lot more nuance there.

451
00:26:54,920 --> 00:26:58,000
I am, yeah, elaborate on, on that.

452
00:26:58,000 --> 00:27:02,040
So this has been a big question in data center design, generally, as a systems person,

453
00:27:02,040 --> 00:27:06,440
like, should I turn off my GPUs, should I turn off my CPUs, uh, or should I try to utilize

454
00:27:06,440 --> 00:27:07,440
them better?

455
00:27:07,440 --> 00:27:12,280
Um, and the general consensus when we think about data centers, uh, is that we really

456
00:27:12,280 --> 00:27:14,920
do want to try to maximize our utilization.

457
00:27:14,920 --> 00:27:18,240
Part because we bought the data center, it's expensive, we should use it, uh, we have

458
00:27:18,240 --> 00:27:20,120
to keep it cool, we have to staff it.

459
00:27:20,120 --> 00:27:24,560
There's a lot of other factors that go into play, uh, and so we want to be able to use

460
00:27:24,560 --> 00:27:28,240
that hardware as much as possible and as efficiently as possible.

461
00:27:28,240 --> 00:27:31,520
Um, and part of the reason we might want to use it as efficiently as possible, you think

462
00:27:31,520 --> 00:27:35,200
of things like serverless, uh, if I'm not using the hardware, I can put something else

463
00:27:35,200 --> 00:27:39,600
there, uh, and we're creating markets for filling in the excess capacity.

464
00:27:39,600 --> 00:27:42,760
So the idea that I would turn off a GPU is sort of silly, I should always have something

465
00:27:42,760 --> 00:27:43,760
running.

466
00:27:43,760 --> 00:27:47,760
Um, now the question is, can I make that thing running on the GPU as efficient as possible?

467
00:27:47,760 --> 00:27:52,480
Uh, and so in our work, we're focusing on trying to maximize that efficiency, uh, in

468
00:27:52,480 --> 00:27:56,440
my lab, for example, students are competing for the GPUs, we, the one GPU experiment is

469
00:27:56,440 --> 00:27:59,960
definitely easier to run because they're not fighting for the entire box.

470
00:27:59,960 --> 00:28:04,240
Um, and so the other GPUs are being used for other experiments, uh, and then when we

471
00:28:04,240 --> 00:28:07,640
go to, to, you know, a GPUs, we're going to again use the whole box.

472
00:28:07,640 --> 00:28:11,840
So the general consensus or at least the thought process today, when we think about the data

473
00:28:11,840 --> 00:28:17,280
centers to really maximize our utilization and not try to, uh, power throttle, um, or

474
00:28:17,280 --> 00:28:19,920
limit the performance of each of the, the course.

475
00:28:19,920 --> 00:28:24,040
So it could be in the future and new kinds of hardware might change that trade off, um,

476
00:28:24,040 --> 00:28:27,360
but the underlying economics would sort of suggest that if you bought the device, you should

477
00:28:27,360 --> 00:28:29,920
really try to find ways to maximize its usage.

478
00:28:29,920 --> 00:28:34,120
And given machine learning has an infinite supply of things that we'd like to train, um,

479
00:28:34,120 --> 00:28:38,040
it's not hard to imagine that I can always fill my excess capacity with more training.

480
00:28:38,040 --> 00:28:44,080
So is the paper fundamentally an economics paper in the sense of, you know, you're trying

481
00:28:44,080 --> 00:28:49,640
to maximize utilization and those kind of things, or do you also get to results that talk

482
00:28:49,640 --> 00:28:54,840
about performance given a set of constraints, like your traditional computer science,

483
00:28:54,840 --> 00:28:56,000
he kinds of papers?

484
00:28:56,000 --> 00:28:57,000
Yeah.

485
00:28:57,000 --> 00:28:58,000
So it's, it's funny.

486
00:28:58,000 --> 00:28:59,000
We hadn't gone down the economics route.

487
00:28:59,000 --> 00:29:00,000
So it's, it's a funny.

488
00:29:00,000 --> 00:29:01,000
I mean, very loosely.

489
00:29:01,000 --> 00:29:02,000
Yeah.

490
00:29:02,000 --> 00:29:05,680
Well, so we, we are actually very much thinking about the economics of computing.

491
00:29:05,680 --> 00:29:09,440
When we look at serverless, that is going to fundamentally change our economics computing

492
00:29:09,440 --> 00:29:13,320
in a way that I think will make things more efficient, more cost effective, um, and

493
00:29:13,320 --> 00:29:14,320
actually easier.

494
00:29:14,320 --> 00:29:18,560
So it's, it's a win for everyone and we actually have a, uh, upcoming paper, um, on

495
00:29:18,560 --> 00:29:22,320
this at a hot cloud about, you know, the economics of serverless are going to generally be favorable

496
00:29:22,320 --> 00:29:26,320
for everyone, um, assuming we get some of the systems problems, uh, you know, ironed

497
00:29:26,320 --> 00:29:31,240
out, uh, this paper was really our students as in, you know, a first effort to really make

498
00:29:31,240 --> 00:29:36,080
progress in the BERT-P training space to find mechanisms that we in academia can use to

499
00:29:36,080 --> 00:29:37,080
go fast.

500
00:29:37,080 --> 00:29:40,840
Uh, and part of that is finding better ways to be more efficient about training.

501
00:29:40,840 --> 00:29:45,400
Um, it allows us to run experiments more quickly and so we can now innovate on BERT.

502
00:29:45,400 --> 00:29:48,680
And one of the things we're actually looking at is, uh, trying to make these models more

503
00:29:48,680 --> 00:29:52,360
non-parametric so they can leverage other data sources.

504
00:29:52,360 --> 00:29:56,240
One of the side consequences of this paper is sort of, uh, you know, if you're out there

505
00:29:56,240 --> 00:29:58,000
thinking about, oh, I should, that's really cool.

506
00:29:58,000 --> 00:30:00,400
I want to play, uh, to play that, but hey, wait a second.

507
00:30:00,400 --> 00:30:04,120
You made the model four or five X bigger, six, seven X bigger.

508
00:30:04,120 --> 00:30:05,120
That's six.

509
00:30:05,120 --> 00:30:06,120
Bensive for inference.

510
00:30:06,120 --> 00:30:07,600
Uh, what am I going to do about that?

511
00:30:07,600 --> 00:30:11,200
Um, and in fact, when we got this result, that was like my first conclusion, yay, we went

512
00:30:11,200 --> 00:30:14,600
on the training front, but we just made inference, which is actually the more expensive

513
00:30:14,600 --> 00:30:20,520
problem, uh, worse by seven, 10 X, uh, and if you think about it, training should only

514
00:30:20,520 --> 00:30:25,000
be a small part of the use of these models, uh, inference is where it really the cost

515
00:30:25,000 --> 00:30:26,000
should be.

516
00:30:26,000 --> 00:30:29,200
And it is when you look at a practical application, we might train it, but we're going to

517
00:30:29,200 --> 00:30:33,720
run that model 24 seven at every single tweet, every single web page that we encounter.

518
00:30:33,720 --> 00:30:35,440
That's a lot of inference.

519
00:30:35,440 --> 00:30:40,760
Um, and if you 100, so you're doing what a thousand, uh, when batch, the optimized, a thousand

520
00:30:40,760 --> 00:30:43,560
sentences per second, uh, which sounds good.

521
00:30:43,560 --> 00:30:47,880
But then you think of the amount of text in the web, that's a lot of expensive GPU hardware.

522
00:30:47,880 --> 00:30:53,080
Um, so making the models smaller after training was one of the questions that we had to solve.

523
00:30:53,080 --> 00:30:55,840
And so this is the second half of this paper comes back and goes, wait a second, so even

524
00:30:55,840 --> 00:30:59,600
the model is bigger to train faster, but now we need a way to squeeze them down.

525
00:30:59,600 --> 00:31:03,600
And maybe actually the bigger insight, which is also maybe a little less counterintuitive,

526
00:31:03,600 --> 00:31:08,560
is that the bigger models, we could actually make smaller more efficiently and actually,

527
00:31:08,560 --> 00:31:11,560
uh, with, with less of a degradation in accuracy.

528
00:31:11,560 --> 00:31:14,400
So we make a, we train a really large model and then we chop it up.

529
00:31:14,400 --> 00:31:18,520
So we, we both explored weight pruning, so eliminating weights, making the model more

530
00:31:18,520 --> 00:31:23,680
sparse, uh, and quantization, reducing the bit precision of each of the, uh, the weights.

531
00:31:23,680 --> 00:31:29,240
And so we are able to take our much larger models and then apply these, these, uh, compression

532
00:31:29,240 --> 00:31:31,040
techniques to make them smaller.

533
00:31:31,040 --> 00:31:35,400
And the, the effect of that is we can make the model actually smaller than the small models,

534
00:31:35,400 --> 00:31:38,040
uh, while retaining higher accuracy.

535
00:31:38,040 --> 00:31:39,040
And so that's something that we're still.

536
00:31:39,040 --> 00:31:43,120
So were you able to use the compression techniques off the shelf, or did you have to adapt

537
00:31:43,120 --> 00:31:48,280
them to this kind of model or, um, the specifics of the way you train them?

538
00:31:48,280 --> 00:31:49,280
Yeah.

539
00:31:49,280 --> 00:31:53,760
So, uh, getting close to the deadline, realizing our models are now 10x bigger, we're like,

540
00:31:53,760 --> 00:31:55,960
right, so how do we quickly figure out a compressed these?

541
00:31:55,960 --> 00:31:56,960
Good news.

542
00:31:56,960 --> 00:32:00,120
One of the students, Shen, uh, who's, uh, working on this project had just finished

543
00:32:00,120 --> 00:32:04,080
to work on quantization, uh, and so we're like, right, Shen, can we use your quantization

544
00:32:04,080 --> 00:32:05,080
technique?

545
00:32:05,080 --> 00:32:06,080
I don't know, maybe.

546
00:32:06,080 --> 00:32:10,160
We started playing math and it turns out that, it worked really well, uh, and so we looked

547
00:32:10,160 --> 00:32:12,560
at the standard quantization and standard pruning.

548
00:32:12,560 --> 00:32:16,280
So we tried not to innovate extensively in each of these pieces, more of an exploration

549
00:32:16,280 --> 00:32:20,720
how they interact with this kind of counter intuitive hypothesis that bigger models might

550
00:32:20,720 --> 00:32:24,760
actually be better, both for training and it turns out for inference as well, if we compress

551
00:32:24,760 --> 00:32:25,760
it.

552
00:32:25,760 --> 00:32:26,760
Got it, got it.

553
00:32:26,760 --> 00:32:29,760
So not that you have any of these numbers like right at your fingertips, but can you give

554
00:32:29,760 --> 00:32:34,440
us a sense for when you say train large, like what large means in this case and how that

555
00:32:34,440 --> 00:32:37,200
compares to what a Google might do typically?

556
00:32:37,200 --> 00:32:38,200
Yeah.

557
00:32:38,200 --> 00:32:41,880
So I think we were looking at like six X seven X bigger than was normally, was normally

558
00:32:41,880 --> 00:32:42,880
published.

559
00:32:42,880 --> 00:32:46,800
Uh, I'm guessing Google actually goes much larger still and they might already be benefiting

560
00:32:46,800 --> 00:32:48,200
from these ideas.

561
00:32:48,200 --> 00:32:53,480
And what, what order of magnitude is that in terms of number of, you know, servers or

562
00:32:53,480 --> 00:32:58,840
CPUs or GPUs or so we were at eight, uh, GPUs, we actually also ran experiments on a

563
00:32:58,840 --> 00:33:05,680
GPU, uh, V three TPU as well, uh, I'm trying to remember the exact sizes, I have a paper

564
00:33:05,680 --> 00:33:07,360
in front of me if I can find it.

565
00:33:07,360 --> 00:33:08,360
Many tabs.

566
00:33:08,360 --> 00:33:09,360
Uh, yeah.

567
00:33:09,360 --> 00:33:14,360
So I think we were at, we were up to like 20 hidden layers or so, uh, our 24 went up

568
00:33:14,360 --> 00:33:20,160
24 layers, um, and we tried, uh, hidden sizes of, you know, like the order of, uh, 15,

569
00:33:20,160 --> 00:33:25,320
36, so 1,536 hidden units, uh, for each of the layers.

570
00:33:25,320 --> 00:33:29,480
So we, we tried a pretty reasonable space, um, we, we built off off the Roberto work,

571
00:33:29,480 --> 00:33:32,960
which is actually if people haven't looked at, it's kind of neat, uh, sort of revisiting

572
00:33:32,960 --> 00:33:37,120
what Bert did, uh, and in some ways, Bert really had the right answer, just this broader

573
00:33:37,120 --> 00:33:40,080
experimentation of the, the trade off space makes a big difference.

574
00:33:40,080 --> 00:33:44,560
Um, so we built off of that and tried different variations on the sizes described in that paper.

575
00:33:44,560 --> 00:33:45,560
Yeah.

576
00:33:45,560 --> 00:33:49,480
The kind of rough magnitudes that I remember reading about and I don't remember if this

577
00:33:49,480 --> 00:33:55,480
was, you know, Bert or Almore, some of the different variations or, or, but there was,

578
00:33:55,480 --> 00:34:02,040
you know, on the order of like 64, you know, tens of GPUs for, you know, yeah, or more.

579
00:34:02,040 --> 00:34:03,040
Yeah.

580
00:34:03,040 --> 00:34:06,240
No, so, so we went, uh, we used a TPU cluster for our big experiments.

581
00:34:06,240 --> 00:34:10,480
So we actually tried to reproduce the Roberta setup, uh, so our, our comparisons are compared

582
00:34:10,480 --> 00:34:11,760
to these standard baselines.

583
00:34:11,760 --> 00:34:16,040
Uh, so we had to use a TPU cluster for several weeks, it's expensive to run the full experiment

584
00:34:16,040 --> 00:34:17,040
for the baselines.

585
00:34:17,040 --> 00:34:20,400
Um, but what was needed is by making the model bigger, we could get comparable results

586
00:34:20,400 --> 00:34:21,400
quicker.

587
00:34:21,400 --> 00:34:22,400
Yeah.

588
00:34:22,400 --> 00:34:23,400
Mm-hmm.

589
00:34:23,400 --> 00:34:27,920
And, um, so we've, we've kind of characterized bigger, now characterized quicker.

590
00:34:27,920 --> 00:34:30,080
What did, what did that mean in practice?

591
00:34:30,080 --> 00:34:36,160
So, uh, we used them, uh, I guess at about a hundred thousand, uh, seconds, are we able

592
00:34:36,160 --> 00:34:38,120
to get fairly competitive accuracy?

593
00:34:38,120 --> 00:34:40,800
Um, you should see the, the final number.

594
00:34:40,800 --> 00:34:45,600
So it sort of depends also on, on which tasks we, we also accounted for the, uh, downstream

595
00:34:45,600 --> 00:34:47,600
fine tuning that you'd need to do as well.

596
00:34:47,600 --> 00:34:51,840
I don't actually remember I'll top my head, uh, uh, uh, well, we'll leave it as an exercise

597
00:34:51,840 --> 00:34:54,480
to the listener to pull up the paper.

598
00:34:54,480 --> 00:35:03,440
So you've increased the size of the model and the number of resources, the CPU resource,

599
00:35:03,440 --> 00:35:10,560
or GPU rather resources that the model is running on and in turn decrease the, the training

600
00:35:10,560 --> 00:35:15,400
time of the model and the aggregate compute cost, right?

601
00:35:15,400 --> 00:35:20,960
Did you need to do anything special to accomplish that or was, is the paper primarily observing

602
00:35:20,960 --> 00:35:26,760
the fact that, you know, the, you know, the, in aggregate, you get the, the preferable

603
00:35:26,760 --> 00:35:30,480
approaches to increase the, uh, the model size.

604
00:35:30,480 --> 00:35:36,160
So, uh, we did small changes to where we placed the batch normalization, uh, we did pre-normalization,

605
00:35:36,160 --> 00:35:39,280
but it's like negligible changes in the underlying architecture.

606
00:35:39,280 --> 00:35:42,960
Most of it is really exploring this, this trade-off space, uh, these different parameters.

607
00:35:42,960 --> 00:35:46,360
So, uh, in some sense, it's a first step towards a bigger agenda.

608
00:35:46,360 --> 00:35:50,400
It wasn't intended to be like a ground, uh, changing work, but what's kind of neat is it

609
00:35:50,400 --> 00:35:54,040
does really make us at least rethink how we approach training of these models.

610
00:35:54,040 --> 00:36:00,320
Well, it's, you know, it's important stuff like, I think a lot of people, uh, will think,

611
00:36:00,320 --> 00:36:04,760
well, you know, if Berkeley's worrying about the cost of training these models, what,

612
00:36:04,760 --> 00:36:13,000
what hope is there for, you know, my lab in, you know, um, a non-Berkeley institution,

613
00:36:13,000 --> 00:36:20,160
you know, that has such close ties to Silicon Valley and, and relatively, awash and resources.

614
00:36:20,160 --> 00:36:26,240
Uh, and so if you can figure out how to make training these models more efficient, then,

615
00:36:26,240 --> 00:36:29,720
um, that's potentially a huge impact for a lot of people.

616
00:36:29,720 --> 00:36:35,560
There's a, an important, uh, sub narrative here, which is that, uh, training, the pre-training

617
00:36:35,560 --> 00:36:39,760
isn't something that everyone needs to do, um, and so, uh, Google has done a great job

618
00:36:39,760 --> 00:36:41,480
of offering these pre-trained models.

619
00:36:41,480 --> 00:36:45,520
So this really expensive part isn't something that every, you know, group in the world has

620
00:36:45,520 --> 00:36:48,160
to address, uh, and that's a good thing.

621
00:36:48,160 --> 00:36:52,720
Um, but if we want to innovate on that pre-training process, if you want to do research in it,

622
00:36:52,720 --> 00:36:57,320
or we want to, in fact, the data suggests that adding more data, uh, that's specialized

623
00:36:57,320 --> 00:36:59,800
to your domain can improve the quality of the model.

624
00:36:59,800 --> 00:37:03,840
So if we want to be able to push pre-training research forward, we do need to find ways

625
00:37:03,840 --> 00:37:09,000
to make it more efficient, uh, and I should say we started out with thinking, oh, we're

626
00:37:09,000 --> 00:37:13,160
going to invent a new bird, um, and discovered in the process and maybe, we don't necessarily

627
00:37:13,160 --> 00:37:16,760
need a new bird yet, but maybe approaches to how we do the training, how we choose our

628
00:37:16,760 --> 00:37:19,920
hyper parameters can make a really big difference.

629
00:37:19,920 --> 00:37:26,800
Your comment just prompted a thought to what degree has the, uh, kind of the, uh, I don't

630
00:37:26,800 --> 00:37:33,960
know, theoretical trade-off space around pre-training versus fine-tuning been explored.

631
00:37:33,960 --> 00:37:40,640
So that, you know, if I know that I have a unique domain, you know, and some, you know,

632
00:37:40,640 --> 00:37:48,360
corpus, uh, of documents or data available to me, you know, is there a, is there any kind

633
00:37:48,360 --> 00:37:55,200
of concrete research I can look to to help me understand, you know, if I should be pre-training

634
00:37:55,200 --> 00:38:00,680
from scratch versus fine-tuning and, or do I just need to try everything and see what

635
00:38:00,680 --> 00:38:01,680
works?

636
00:38:01,680 --> 00:38:07,040
Uh, the try everything is, is not terrible advice, but here's what I would tell my students,

637
00:38:07,040 --> 00:38:08,400
uh, so pre-training is expensive.

638
00:38:08,400 --> 00:38:12,600
So maybe start with fine-tuning, understand what, what is your prediction task, and this

639
00:38:12,600 --> 00:38:17,440
is what, you know, the, the practical world will do, um, so take your, your, your, uh, your

640
00:38:17,440 --> 00:38:21,440
prediction task, whether or not a translation or sentiment tagging, or maybe it's like, which

641
00:38:21,440 --> 00:38:25,800
call center, which call person should this, this, you know, message be forwarded to, um,

642
00:38:25,800 --> 00:38:30,240
focus on, on fine-tuning for that task first, um, there's a little bit of art in choosing

643
00:38:30,240 --> 00:38:34,240
learning rights and stuff to get your fine-tuning to work, so go through that process, uh, understand

644
00:38:34,240 --> 00:38:37,960
how well you can do, uh, by fine-tuning to your, your domain.

645
00:38:37,960 --> 00:38:42,680
And then if you have, and you might, you know, billions of call records from the past,

646
00:38:42,680 --> 00:38:47,120
you think you could really better improve the underlying representation, um, you could

647
00:38:47,120 --> 00:38:52,120
then try to go back to this mass-language model, uh, training, the, the pre-training process,

648
00:38:52,120 --> 00:38:56,280
uh, and then the work that we've done and, and, you know, other work that's, it's, it's

649
00:38:56,280 --> 00:39:01,200
going on around us, um, can help to make that process more efficient so that you can, in

650
00:39:01,200 --> 00:39:07,480
a matter of, of weeks or a week, in our case, um, take your V100 box and really make substantial

651
00:39:07,480 --> 00:39:12,080
progress, uh, towards, uh, pre-trained model that's now pre-trained on, on your domain

652
00:39:12,080 --> 00:39:13,080
as well.

653
00:39:13,080 --> 00:39:18,080
Um, and so where do you see this line of research leading?

654
00:39:18,080 --> 00:39:19,080
Yeah.

655
00:39:19,080 --> 00:39:23,080
So I asked my students this every year, so as we said in this group, like, what's next,

656
00:39:23,080 --> 00:39:24,080
guys?

657
00:39:24,080 --> 00:39:27,800
So we figured the original goal was to, like, be able to develop a new bird.

658
00:39:27,800 --> 00:39:30,240
So now we have the tools to start testing pre-training.

659
00:39:30,240 --> 00:39:31,240
What should we do next?

660
00:39:31,240 --> 00:39:35,960
Um, one of the things that I'm kind of excited about, uh, is, uh, well, the realization

661
00:39:35,960 --> 00:39:40,200
that we're trying to cram a lot of knowledge in the weights of our model, uh, and making

662
00:39:40,200 --> 00:39:43,960
the models bigger certainly helps with that, uh, another way to deal with knowledge is to

663
00:39:43,960 --> 00:39:45,760
not put it in the model at all.

664
00:39:45,760 --> 00:39:49,240
I, I actually have to look up stuff most of the time when I want to remember facts and

665
00:39:49,240 --> 00:39:50,320
terrible remembering facts.

666
00:39:50,320 --> 00:39:53,640
So I use the internet, um, I have a neural net in my head.

667
00:39:53,640 --> 00:39:56,400
It doesn't have to memorize the internet because I have the internet.

668
00:39:56,400 --> 00:39:59,920
So having access to a knowledge base can make a big difference, uh, and how much we need

669
00:39:59,920 --> 00:40:05,160
to encode in our model, make our model perhaps smaller, um, the ability to, uh, synthesize,

670
00:40:05,160 --> 00:40:08,640
uh, decisions or to apply logic on top of knowledge base.

671
00:40:08,640 --> 00:40:14,000
It seems like a really big opportunity for language modeling for, uh, for NLP broadly.

672
00:40:14,000 --> 00:40:17,360
And maybe even for these, these basic representations like Bert.

673
00:40:17,360 --> 00:40:20,640
And so we've been looking at, and starting to look at, and some of their groups actually

674
00:40:20,640 --> 00:40:25,200
have got some early published work on how to bring in a non-parametric or semi-parametric

675
00:40:25,200 --> 00:40:31,560
lens on, on these models so that, uh, we can reference into a large knowledge base, uh,

676
00:40:31,560 --> 00:40:33,880
in, in the construction of our embeddings themselves.

677
00:40:33,880 --> 00:40:37,400
Uh, and that has, you know, the advantage of maybe being more efficient, uh, allowing

678
00:40:37,400 --> 00:40:40,840
us to grow the knowledge base without having to retrain the model, uh, we could get more

679
00:40:40,840 --> 00:40:44,160
data, our model just gets better without having to adjust the model itself.

680
00:40:44,160 --> 00:40:47,360
Um, and maybe even giving us some explainability.

681
00:40:47,360 --> 00:40:51,960
So when we go to make a prediction about, like, how we embedded the sentence or how we represent

682
00:40:51,960 --> 00:40:55,920
this, you know, this decision for, you know, which called a route to, we can now actually

683
00:40:55,920 --> 00:41:00,280
point back at historical data and say, here's the data we use in our embedding to reason

684
00:41:00,280 --> 00:41:01,280
about that.

685
00:41:01,280 --> 00:41:02,440
And you know, that's terrible data.

686
00:41:02,440 --> 00:41:06,560
I don't want that in my data set or, you know, that actually, that makes sense.

687
00:41:06,560 --> 00:41:10,640
And so that, that connection to the data could actually also help with the explainability.

688
00:41:10,640 --> 00:41:14,400
So that's sort of the vision that I, that, that my students and, and I are pretty excited

689
00:41:14,400 --> 00:41:15,400
about right now.

690
00:41:15,400 --> 00:41:20,400
Does that pull from or kind of lead you to like memory based networks or like information

691
00:41:20,400 --> 00:41:22,400
retrieval types of problems or?

692
00:41:22,400 --> 00:41:23,400
Yeah.

693
00:41:23,400 --> 00:41:24,400
Yeah.

694
00:41:24,400 --> 00:41:28,040
So memory nets, I are all of these, these kind of, I say, I are more classic memory nets

695
00:41:28,040 --> 00:41:30,200
that also increase any more classic.

696
00:41:30,200 --> 00:41:32,120
Um, so, so those are our tools.

697
00:41:32,120 --> 00:41:35,040
Uh, one of the things we're looking at right now is something as simple as like, can

698
00:41:35,040 --> 00:41:39,640
we use embeddings from other piece of text and simple text similarity to, to recall those

699
00:41:39,640 --> 00:41:40,640
embeddings?

700
00:41:40,640 --> 00:41:42,880
Um, and, and there's some other work exploring this now.

701
00:41:42,880 --> 00:41:48,200
Uh, ultimately things like, uh, memory nets or pointer, mechanisms to point into a knowledge

702
00:41:48,200 --> 00:41:52,600
base and attend to an entire corpus would be really exciting and, and we're just starting

703
00:41:52,600 --> 00:41:53,600
to explore this.

704
00:41:53,600 --> 00:41:55,680
Uh, so there, there's a lot more to do.

705
00:41:55,680 --> 00:41:59,600
Uh, it does push us in the direction of IR, uh, imagine a neural net that can Google

706
00:41:59,600 --> 00:42:02,000
stuff for you, uh, to answer questions.

707
00:42:02,000 --> 00:42:08,320
So it's certainly, there's a, a well studied area that, that we'll have to build on.

708
00:42:08,320 --> 00:42:14,520
You touched on explainability in there as a possible, uh, benefit here.

709
00:42:14,520 --> 00:42:20,080
Uh, I'm, I'm curious about, uh, you know, maybe you're elaborating on that a little bit

710
00:42:20,080 --> 00:42:25,040
more and then also you've been doing some work on explainability as applied to reinforcement

711
00:42:25,040 --> 00:42:26,040
learning.

712
00:42:26,040 --> 00:42:29,760
Um, um, maybe a little, if you can share a little bit about that work as well.

713
00:42:29,760 --> 00:42:34,960
Yeah, so I'm the co-director of the UC Berkeley RISE lab, which stands for real time working

714
00:42:34,960 --> 00:42:36,640
on that intelligent working on that secure.

715
00:42:36,640 --> 00:42:41,240
So we have an interesting agenda on security, which is another time, um, and then the E.

716
00:42:41,240 --> 00:42:44,040
And then for a long time, I'm like, what does the E really, what, what, what should it

717
00:42:44,040 --> 00:42:45,040
mean?

718
00:42:45,040 --> 00:42:49,000
Uh, we were initially thinking execution, we're going to execute things securely, but

719
00:42:49,000 --> 00:42:51,080
that, that's actually some bad connotations.

720
00:42:51,080 --> 00:42:55,480
So, uh, maybe there's another agenda, which actually came out of our, our exploration

721
00:42:55,480 --> 00:43:00,520
lineage, uh, how we, we track relationship between data and the model development process,

722
00:43:00,520 --> 00:43:04,440
explainable, uh, decisions would be a good thing to be able to support.

723
00:43:04,440 --> 00:43:08,880
Um, and so we had an agenda around how to do X, or we have actually on going agenda,

724
00:43:08,880 --> 00:43:14,120
I had to, uh, explainable machine learning by connecting our training process to our data.

725
00:43:14,120 --> 00:43:18,200
But what's actually pretty exciting is, is pulling in some of the recent work and explainable

726
00:43:18,200 --> 00:43:19,200
AI.

727
00:43:19,200 --> 00:43:23,780
Actually, my, this advisor, Carlos has an exciting work, uh, line, which provides a black

728
00:43:23,780 --> 00:43:27,040
box mechanism for explaining, uh, model's decisions.

729
00:43:27,040 --> 00:43:31,760
Uh, so my students have been also exploring, uh, tremendous staying power.

730
00:43:31,760 --> 00:43:36,840
This is, you know, one of the things we talked about three plus years ago, and that, uh,

731
00:43:36,840 --> 00:43:42,240
went that early twinal episode and Lyme comes up all the time still.

732
00:43:42,240 --> 00:43:43,240
Yeah.

733
00:43:43,240 --> 00:43:47,040
So I, I run the risk of a, of a, another tangent, but the, the world of explainability

734
00:43:47,040 --> 00:43:51,720
is, it's, it's kind of rich and it is created by this need to, to make sense of models

735
00:43:51,720 --> 00:43:56,400
that no longer make sense, uh, and so this idea that I can inspect my model and go,

736
00:43:56,400 --> 00:44:01,360
hey, I like how that makes decisions, uh, that's gone, or at least, you know, to a first

737
00:44:01,360 --> 00:44:02,360
approximation.

738
00:44:02,360 --> 00:44:04,840
So we're left with justify the decision you made.

739
00:44:04,840 --> 00:44:08,240
Let's go back and at least connect it to the data, to even the input.

740
00:44:08,240 --> 00:44:12,040
Uh, so, so my group had started looking at that, and one of the things that we started

741
00:44:12,040 --> 00:44:14,680
to ask is, why can't we have some interpretability?

742
00:44:14,680 --> 00:44:19,480
Uh, and so one of the agendas that, uh, I'm exploring that's actually not in the language

743
00:44:19,480 --> 00:44:25,040
domain, but more in the, the vision domain, um, is how to apply decision trees to connect

744
00:44:25,040 --> 00:44:29,600
decision trees back with our, our, our deep learning models so that we can get the accuracy

745
00:44:29,600 --> 00:44:32,960
we want, but we can also go and interrogate our model and go, well, it's going to call

746
00:44:32,960 --> 00:44:36,240
this a cat, but in order to do that, it has to first identify that it's an animal and

747
00:44:36,240 --> 00:44:40,400
it has to cluster it in animals with, with legs, uh, and then, you know, with fur and

748
00:44:40,400 --> 00:44:41,960
then it gets us to cat.

749
00:44:41,960 --> 00:44:45,880
Um, so there is an opportunity to actually understand what the model is going to do at

750
00:44:45,880 --> 00:44:49,800
the high level, each of these decisions is governed by a neural net, so understanding

751
00:44:49,800 --> 00:44:54,320
that is sort of off the table, uh, but at least we now have an idea of the decision process

752
00:44:54,320 --> 00:44:56,760
the model will go through to make a decision.

753
00:44:56,760 --> 00:45:02,320
So this is our recent work on neural back decision trees, uh, when we look at language, it's

754
00:45:02,320 --> 00:45:05,800
been an interesting question of what, what would an explanation look like in the language

755
00:45:05,800 --> 00:45:06,800
domain?

756
00:45:06,800 --> 00:45:09,720
So there are techniques like grad cam that have been pretty popular in vision that would

757
00:45:09,720 --> 00:45:13,200
give us, you know, highlighting parts of an image that say, you know, this is the part

758
00:45:13,200 --> 00:45:14,520
of the image you're attending to.

759
00:45:14,520 --> 00:45:18,160
We could do that in their explorations of that in the language domain, but one of the

760
00:45:18,160 --> 00:45:22,520
neat things going back to our very beginning narrative is, can I connect my decisions back

761
00:45:22,520 --> 00:45:23,520
to data?

762
00:45:23,520 --> 00:45:26,080
Um, in many cases, that is sort of the ideal explanations.

763
00:45:26,080 --> 00:45:30,560
Like here's the data that informed my decision about what you've given me now.

764
00:45:30,560 --> 00:45:34,080
Um, and so, so that explanation is what we're exploring.

765
00:45:34,080 --> 00:45:37,480
One of the hopes in doing that is you can not only connect it, but you can even fix

766
00:45:37,480 --> 00:45:38,480
it.

767
00:45:38,480 --> 00:45:42,120
Uh, so one of the kind of ideal outcomes of an explainable model is when it gets it

768
00:45:42,120 --> 00:45:44,000
wrong, you go, that's wrong.

769
00:45:44,000 --> 00:45:45,640
Here's what is wrong about it.

770
00:45:45,640 --> 00:45:49,720
And that extra signal could be way more valuable than just some more labeled data.

771
00:45:49,720 --> 00:45:54,120
Uh, and so that's our hope and maybe being able to correct our knowledge base if we are

772
00:45:54,120 --> 00:45:57,360
referencing data so that we don't use that reference data in the future.

773
00:45:57,360 --> 00:46:00,920
That would be one mechanism in the case of decision tree changing the path.

774
00:46:00,920 --> 00:46:04,400
So, you know, cats can't be attached to the thing that are, you know, they're under

775
00:46:04,400 --> 00:46:05,400
water.

776
00:46:05,400 --> 00:46:06,400
That doesn't make sense.

777
00:46:06,400 --> 00:46:09,040
So I want to move my cat class somewhere else in my tree.

778
00:46:09,040 --> 00:46:13,080
So the opportunity to intervene in a model is something that I'm excited about when

779
00:46:13,080 --> 00:46:19,920
we look at explanations at do you draw a hard line between explanations and interpretability

780
00:46:19,920 --> 00:46:22,440
just when you're speaking about them casually?

781
00:46:22,440 --> 00:46:28,320
I do a little bit because at least classically to me, I was again, background being more

782
00:46:28,320 --> 00:46:32,280
in the traditional machine learning, uh, we really cared a lot about interpretal models

783
00:46:32,280 --> 00:46:36,040
and that meant that I could look at the individual pieces of the model and start to reason

784
00:46:36,040 --> 00:46:40,480
about, uh, conditional probabilities, what they would say about the, the priors that I'm

785
00:46:40,480 --> 00:46:42,240
trying to impose on my data.

786
00:46:42,240 --> 00:46:47,040
Um, so interpretability to me means something that is sort of independent of the kind of

787
00:46:47,040 --> 00:46:48,040
intrinsically.

788
00:46:48,040 --> 00:46:49,040
Yeah.

789
00:46:49,040 --> 00:46:50,040
It's intrinsic to the model.

790
00:46:50,040 --> 00:46:54,080
Whereas an explanation could also be called a justification, sort of looks retroactively.

791
00:46:54,080 --> 00:46:56,920
Here's a decision I made, provide an explanation.

792
00:46:56,920 --> 00:47:00,560
If we look at humans, humans are not interpretable, you can't look at their brain well.

793
00:47:00,560 --> 00:47:03,480
Most people can't look at the brain and go, okay, yeah, I know what you're going to do.

794
00:47:03,480 --> 00:47:07,120
But they provide meaningful explanations and that's maybe all we can hope for and we learn

795
00:47:07,120 --> 00:47:08,640
to work with that.

796
00:47:08,640 --> 00:47:12,880
And I hope with the work and explainability that we're explain is to sort of provide a

797
00:47:12,880 --> 00:47:17,520
little bit more of the interpretability, uh, and I think that's important one because

798
00:47:17,520 --> 00:47:21,280
it, you know, if I'm going to deploy a model, I'd like to be able to, in general, understand

799
00:47:21,280 --> 00:47:25,840
how it's going to behave, uh, not just on, on a, you know, candidate piece of data.

800
00:47:25,840 --> 00:47:28,560
The other lens that we're bringing is, be able to adjust the model when I get it, when

801
00:47:28,560 --> 00:47:31,280
it gets it wrong, I wouldn't be able to, to correct it.

802
00:47:31,280 --> 00:47:38,600
So this work with the decision trees is the set up such that the decision tree is, uh,

803
00:47:38,600 --> 00:47:46,200
is an intrinsic part of, like, when you refer to the model there is the model, a superset

804
00:47:46,200 --> 00:47:51,920
that includes the decision tree or is the decision tree a kind of a surrogate model that's

805
00:47:51,920 --> 00:47:56,560
used when you're asking, uh, explainability kinds of questions.

806
00:47:56,560 --> 00:48:00,160
No, so the challenge here was to, to make the decision tree the model.

807
00:48:00,160 --> 00:48:06,000
Uh, and so what we're doing, uh, we don't do really crazy complicated things.

808
00:48:06,000 --> 00:48:11,760
So in this set up, we're taking a standard ResNet 101 to find an embedding of our, our

809
00:48:11,760 --> 00:48:12,760
input.

810
00:48:12,760 --> 00:48:17,320
And then we're using the decision tree as a, a decision tree on top of that embedding.

811
00:48:17,320 --> 00:48:22,080
Um, and so that's allowing us to route our decision process based on that embedding from

812
00:48:22,080 --> 00:48:23,080
ResNet 101.

813
00:48:23,080 --> 00:48:27,720
So there's a part of the model that I can make no sense of that there's no deep, uh, interpretable

814
00:48:27,720 --> 00:48:30,440
or even explainable component of that little piece.

815
00:48:30,440 --> 00:48:33,480
Um, but there is now structure in how decisions are made.

816
00:48:33,480 --> 00:48:34,480
Right.

817
00:48:34,480 --> 00:48:41,320
So the ResNet is basically, um, kind of learning this space of relationships between

818
00:48:41,320 --> 00:48:44,520
the things that it's seeing, at least in a computer vision sense.

819
00:48:44,520 --> 00:48:50,720
And then the decision tree is on top, making decisions about what is what based on the

820
00:48:50,720 --> 00:48:52,840
space that the ResNet has locked.

821
00:48:52,840 --> 00:48:53,840
Yeah.

822
00:48:53,840 --> 00:48:55,320
So it is a funny recipe.

823
00:48:55,320 --> 00:48:59,560
And a lot of vision now, it's take a ResNet like architecture as a backbone and it's

824
00:48:59,560 --> 00:49:06,560
role is to embodying the things, you know, it wants to, but it's role is to extract, um,

825
00:49:06,560 --> 00:49:11,520
pixel information to extract texture, shape, things, image attributes that would be then

826
00:49:11,520 --> 00:49:13,080
used to make a decision.

827
00:49:13,080 --> 00:49:17,720
And it places them in a fairly high dimensional space and then the, the decision tree is, is

828
00:49:17,720 --> 00:49:22,040
constructed in a way that tries to, uh, use that, that space to make decisions.

829
00:49:22,040 --> 00:49:23,360
Now, that actually alone doesn't work.

830
00:49:23,360 --> 00:49:27,720
So you need to take that decision tree and fine tune the neural net, the ResNet backbone

831
00:49:27,720 --> 00:49:31,200
so that it's, uh, compatible with the decision tree we build.

832
00:49:31,200 --> 00:49:35,040
Um, so it's a, you know, a small twist that's needed, but that small twist now allows us

833
00:49:35,040 --> 00:49:39,880
to get competitive accuracy to the original model, we're still using the model, uh, but

834
00:49:39,880 --> 00:49:43,800
now have an interpretable path where like, uh, one of the fun examples is, uh, if we

835
00:49:43,800 --> 00:49:47,440
give it a picture of a zebra, it's a class we've never seen before, it'll route it down

836
00:49:47,440 --> 00:49:50,920
to near the horse, but then it doesn't know what, you know, it'll classify something

837
00:49:50,920 --> 00:49:52,760
in that one of the horse categories.

838
00:49:52,760 --> 00:49:58,200
Um, and so it, it does try to extract some structure to the classes, uh, that is

839
00:49:58,200 --> 00:50:03,280
semantically meaningful, but also a picture, uh, uh, in the, in the image domain meaningful.

840
00:50:03,280 --> 00:50:06,600
So, uh, things that look similar, yeah.

841
00:50:06,600 --> 00:50:16,320
Can you provide a, uh, kind of a quick intuition for why a fix is needed in the neural network

842
00:50:16,320 --> 00:50:22,400
domain, um, as opposed to just throwing the existing decision tree against the existing

843
00:50:22,400 --> 00:50:26,720
embeddings and what the kind of intuition of that fix is?

844
00:50:26,720 --> 00:50:27,720
Yeah.

845
00:50:27,720 --> 00:50:31,600
So, uh, the simple answer is we tried, uh, throwing the simple embedding into the decision

846
00:50:31,600 --> 00:50:36,640
tree and it didn't work, uh, the, the deeper answer is that, uh, it's, the decision tree

847
00:50:36,640 --> 00:50:40,160
wasn't up, sorry, the, the neural that wasn't optimized to give features that have some,

848
00:50:40,160 --> 00:50:44,400
uh, coherent structure that we can build this, this class hierarchy on top of.

849
00:50:44,400 --> 00:50:48,360
And so adding this extra decision tree loss, we can actually force a decision tree to cluster

850
00:50:48,360 --> 00:50:53,200
things using semantically similar structure, like we want horses to be nearby dogs and

851
00:50:53,200 --> 00:50:56,400
farther, like share less common ancestors to fish.

852
00:50:56,400 --> 00:51:01,040
Um, so, so we can impose some structure in our tree, uh, and then we can force the neural

853
00:51:01,040 --> 00:51:03,560
nets and embeddings to reflect that structure.

854
00:51:03,560 --> 00:51:06,880
So that, that is why we need to adjust the neural net to deal, to compensate or to be

855
00:51:06,880 --> 00:51:09,280
able to work in the context of the tree.

856
00:51:09,280 --> 00:51:15,320
So in the, and you're changing your loss function and the decision tree to accentuate kind

857
00:51:15,320 --> 00:51:22,080
of like maybe, you know, I'm envisioning kind of spreading out the embedding space or

858
00:51:22,080 --> 00:51:26,120
something like that so that the decision tree can, so we're more meaningful for the

859
00:51:26,120 --> 00:51:28,080
semantics of the decision tree.

860
00:51:28,080 --> 00:51:29,080
Cool.

861
00:51:29,080 --> 00:51:30,080
Awesome.

862
00:51:30,080 --> 00:51:34,600
Well, Joey, uh, this has been wonderful, uh, learning a little bit about what you're

863
00:51:34,600 --> 00:51:35,600
up to there.

864
00:51:35,600 --> 00:51:39,160
We still never got very deep into serverless, so we're going to have to put a pin in

865
00:51:39,160 --> 00:51:42,720
that one for, uh, next time, but very cool stuff.

866
00:51:42,720 --> 00:51:45,800
And thanks for taking the time to, uh, share it with us.

867
00:51:45,800 --> 00:51:46,800
Thank you.

868
00:51:46,800 --> 00:51:47,800
It's been fun.

869
00:51:47,800 --> 00:51:48,800
Awesome.

870
00:51:48,800 --> 00:51:49,800
Thanks.

871
00:51:49,800 --> 00:51:54,960
All right, everyone, that's our show for today.

872
00:51:54,960 --> 00:52:00,760
For more information on today's show, visit twomolai.com slash shows.

873
00:52:00,760 --> 00:52:16,680
As always, thanks so much for listening and catch you next time.

