All right, everyone. I am here with Francesque Riera. Francesque is an applied machine learning
engineer at the Lego group. Francesque, welcome to the Twomo AI podcast.
Thanks. And thanks a lot for having me as well. I think it's a pleasure and super excited
about the talk tonight as well. Same year. Same year. And thanks for taking the call at night
or being on at night. It's a bit later for you. You're in Denmark. Yeah, that's correct. Denmark,
well, as you know, headquarters for Lego was born in the small town of Pilon,
here in Denmark. So living just across it. Very nice. Very nice. Why don't we get started by
having you share a little bit about your background and how you came to work in machine learning
and at Lego? Yeah. So, well, I think it's, it's not going to be a very long story,
because I've been on the, I guess, on the market for roughly three years now.
But I think my enthusiasm for ML and actually I should say my enthusiasm for computer vision
started back in my bachelor's in industrial electronics in Spain. And that's just because
I was studying the, the bachelor's in electronics. And then the last semester was focusing on robotics
and then robotics. We had an introduction to computer vision. And I don't know why, but I thought
this is then impressive. It's super interesting what you can do well in in reality with mathematics.
And then matrices, right? For all the pictures and pictures and all of these sort of things.
And then that drove me to then actually catching up on, on a master's in Denmark. So that's
when I moved to Denmark to do a full on masters in computer vision and machine learning in the
University of all work, which is in the north of Denmark. And after that, well, I guess I
become sort of an expert in the matter. I hope so at least. And that got me then. I got a,
a small job as a supporting engineer for a couple of months. It was not my thing. And then I
found this splendid opportunity to live where I got to actually work with ML on active products,
running in the cloud as well. Awesome. Awesome. It was speaking of industrial robotics and
computer vision. One of the early, I think this was, I think this was early many years ago,
demos of AI was like somebody built a Lego sorter using a treadmill kind of thing and a paddle
or some kind of robot that would like sort the Legos into different pieces. Did you ever
have you ever seen that one? It's funny. You mentioned it. And we are trying to
and did that one from the internet to maybe exploring the options also to use it as a product
for us. Oh, really? So it's quite funny that you actually mentioned it. I think we talked about it
two, three weeks ago. Oh, that's funny. If I remember correctly, I remember reading a hacker news thread
when this was published. Again, I think it was probably like five years ago or four years ago or
something. There were apparently a bunch of people that would like go on eBay and buy these
gigantic bags of miscellaneous Legos and they were talking about using these kinds of devices
to sort them and then resell them. Apparently, there's a bit of a cottage market, so to speak,
in remarketing Legos. And it's also one of the big campaigns that we're also running is
even though Lego is primarily made of plastic wrap, so you want to give your bricks a
longest life ever, right? And I mean, if you had some Lego set from the 50s or the 60s, you
would still probably use it today. So I think that's also what drives this enthusiasm, right?
On you being able to, all right, let's try and make this, you know, circular economy,
you know, way right for the brick. Yeah. Yeah. Well, I brought that up not thinking that it was
something that you were actually thinking about. But you know, maybe we can jump into how ML is actually
used today at Lego. Yeah, so maybe I can just start putting it aside my area. So my team going
from, I guess, the bottom to the highest. So I'm part of a team where we are three ML engineers
and then full stack and then all of the surroundings right. And in my team, we have two main
products, which I can just quickly describe now. We're going to go into details a bit later if you
want as well. So the main product that started also now to two and a half years ago, approximately,
it's a moderation service. So as you know, Lego is of course a brand for aimed at children, right?
And children are our main responsibility. So of course, all of these, for example, social media
applications or games or anything where children can upload their content, let's say images,
text, videos, anything that can be shared and can be created by them on the phone or on the computer
needs to be moderated by law before it's actually available or online for all the children to
sit right as to avoid the obvious things right. And I mean, we know Facebook does that. We know
Instagram does that. A lot of companies do it, but not in the level I would say as level as it.
And that's because in Facebook, you could post something that is obscene and it's not necessarily
going to be deleted as you upload it, but it's going to be deleted after the fact, maybe because
nobody reported you, right? That cannot happen in our applications because the damage could already
be inflicted and that's not, it's definitely not, you know, the image you want to give to the brand.
So yeah, so our product, what it does is receive the content created by the users and we do an
initial profiting of these content. So on images, on text and on videos, we try to project already
what is the most obvious things that should not be in the app. Like if you uploaded a very bad,
bad image, if you showed your face or some sort of identifiable information, you cannot also
share that. So we also remove that. Of course, the obvious profanities. And we are working also
in extending a couple more detectors. And detectors, by detectors, I mean machine learning algorithms,
actually. So we do this profiltering, what gets rejected by us just gets feedback to the users
and hey, we are very sorry, but your creation is not allowed because of this. Try to take a new
picture or try to be nice writing the text. So some some some post different for that.
If the content is approved by us, it then goes to manual moderation. So there's a company
moderating every single piece of content before it is published. So what we accomplish with our
product, right, is what for the students, right, one of them being monetary aspect because
each piece of content that is very obvious does not need to go to a moderator that
costs per piece, right? So in that sense, what that's what we manage to prove. And of course,
if let's say, you know, there's like, I don't know, a big fair with Lego and people are
asked to upload pictures because they are in the fair. It's like, you need to build a car and then
you need to post a picture of the car. So of course, they would like to post a picture, but if the
picture was bad for some reason, it would be nice that they get the feedback instantly rather than
having to wait. So when when it goes to manual moderation, there is a time frame of five minutes.
I think it's five to 15. I'm not 100% sure because it's not my part. But the thing is our
our service provides a response on 10 seconds for whatever time. So if you uploaded something,
let's say you take a selfie with your car, like, oh, we really like your car, but you cannot have
your face there. So they get the response in seconds, like, oh, my car is still here. I take another
picture, life's good again, right? And then the second product we have, so after everything has
been approved. So when your creation is live on the social media app, then we have an automatic
job every morning that will take all of the creations from the users. And then again, with two
sorts of algorithms, so one for image and one for text, we try to identify what is the theme,
the predominant theme on the on the on the creation. So for example, you have a big Star Wars,
you have a big Star Wars fan, then you have a lot of Star Wars sets, you take picture of your
Star Wars set, we probably will identify it as a Star Wars. And then what we do is like, okay,
this kid has uploaded the Star Wars picture or Star Wars album collection. So what we do is then
we send the like with an NPCs on the application, and we also use different NPCs to send comments,
reinforcing them and encouraging them to keep building, saying, oh, that's a very nice
Star Wars creation you have. I would love to see more from you and see more like that.
And the NPCs are bots or characters and, yeah, they've already existed in the app,
and they've been used. So before we set up this environment for this product, it was used by
my people. So me as a, I guess, consumer, engagement person on the team, I would go in,
I would log in as the, let's say Chihuacca, and then I would comment out on a couple of things.
So now what we're doing is we are automating the job. Of course, there's still some people
looking at the comments and maybe doing more specific comments, depending on the scenario.
But it's been proven also to actually give a lot of value because we have received a lot of
uploads saying, oh, look, legal life, Emmett, like my comment, like my post, and they are super happy
right? So it's also very, it's very fulfilling for us as well to see that the reactions are positive,
right? And you mentioned there are there are three ML engineers on that team.
And how, what's kind of the broader size or scope of ML and data at like a?
Well, so back in March, we had also a big reorganization where we then found that also what
is now the department called the data office. So my team resides within the data office and
then the data science products. But from the data science products, for example, we have a couple
of others. So of course, in the legal.com page, where you go to the shop, right? And you buy
a start-wall set, we have, of course, a recommended engine saying, oh, you like the dits there,
maybe you're all generated in the X-wing, right? So the recommended team is also, I guess, our
sister team working using data as well and developing ML solutions. That's also true. Yeah,
two other product teams, one working on marketing effectiveness. So for example, if you have a
new release of Legonin Chago Netflix, how is that performing compared to, I don't know, Barbie or
or whatever other cartoon is for children at the time, right? And then the last one is the
man forecasting. So trying to be ahead of the curve, which I guess the team had a lot of fun in
Corona times because it's very difficult now. There was no curve. It was just a pick and it's like,
well, we are running out of stock everywhere. So that's it. Yeah. And then a way, of course,
having an incubation team as well. So the incubation team is trying to analyze different
areas, sectors and departments at Lego where automation could be beneficial, for example,
using machine learning. And one of the examples is this brick shorter using a treadmill. Well,
it's a bit more high-scale. That's the idea, right? Nice, nice.
With the, you mentioned the moderation app, it reminds me of a recent interview that we published
with someone who works in cybersecurity. And you know, in that environment, as you can imagine,
it's kind of very adversarial. You, you know, plug one hole, someone's trying to poke another.
And moderation in your environment is it, you know, quite the case that there are bad actors,
as opposed to like, are you, are you, is the task primarily identifying passive,
passive behavior that you don't want, or are there bad actors, so to speak, that are trying to
abuse the system? I think there's a, there's a bit of both actually. Generally speaking, we see
the cases where, you know, maybe, maybe you had the phone in your pocket, right? And accidentally,
it took a picture. I mean, it happens to everybody, right? And then you post a picture by mistake.
And that happens quite often. Sometimes we also actually caught some bugs in the app, because it's
like, well, is it not about we're getting, I don't know, a thousand black images or a full white
images, you know, no, it should not make any sense. It's like, okay, well, maybe you should look
into the latest police, you made, and so that was a funny, funny fact. But also, and that's maybe a
bit above the, where the product, our protocols, the migration product, and that's more on the,
on the manual moderation, because they do track behaviors and bad actors. And as far as I
know, I think there's been a couple of cases where somebody got banned, they opened more accounts,
they keep doing the same, got banned again, the game. So there's a little bit of that. I was curious
if that was, if that was a something that you had to deal with how that played into the way you
approach the model development, but it sounds like that's kind of downstream of what you're doing.
It is because it is, again, we are not Facebook or Instagram, right? We need to make sure nothing
back gets published. Right. So that's why there's the extra layers of security here. Do you
incorporate the input of the human moderators? Like, is it a kind of a human and a loop type of
situation where you're using their judgments to evolve your models? Yeah, yeah, that's right. And
that's something we started doing, I think, at the beginning of this year, maybe the end of last
year, you know, with the Corona year, everything specifically started for me and the calendar. But
what we managed to do, yeah, link was this external moderation company. So when they've
reject an image or a text or a video, we get the feedback that saying, okay, this content was
rejected because of this. And what we did is we made a feature store. And it's a very simple
reason for making a feature store rather than just a database, I guess. It's that the data by
law, so GDPR, it cannot be stored forever. Meaning that let's say you are a user of the social media
and one day you call to consumer service and say, hey, I want all my data to be deleted. We cannot
have this link because we are in a way a standalone product, right? So we are not linked to the app.
Meaning if you call to consumer service, you said, I want to delete it. Your data is deleted.
But nobody's going to tell us, hey, this user called because, well, first of all,
I don't have your user data, for example. So it will be impossible for me to track who you are.
But what we accomplished with this also is we generate features and the features are generated by
well-known machine learning algorithms. For images, we are talking ResNet50. For text samples,
we're talking about multilingual bird, for example. So in that sense, we get the images,
text samples, we get the features out of it, which are anonymized because if you have pulling
layers in a neural network, right, you cannot undo a pulling. You could, but you could not get
a one-mile script. And then those features are saved and then labeled with the human interloop.
Got it. So rather than saving the original images, you're saving these representations from
these neural nets. And one of the driving reasons behind that is to kind of avoid the GDPR
responsibilities if that was personally identifiable data.
Yeah. And whether it is or it is not, if the user calls and wants the data deleted, it needs to be
deleted. Otherwise, yeah, you are reaching the contract. I guess, yeah, I guess that's also within
the GDPR. Meaning if they call whatever the consumer GDPR line is and ask for their data to be
deleted, these representations get deleted from the feature store. Not the representations,
because they have anonymized data. So that's fully anonymized data. There's no way to backtrack
where and what it comes from. Yeah. Yeah. And so you said that, you described that as a rationale
for a feature store versus a database, but you could still put those representations in a database.
Well, yeah. I guess we like to make a fancy words, so I can just call it a feature store rather than
a database, right? Yeah. And I say that to probe a little bit deeper into that and are there other
capabilities that you've built into this feature store that are specific to the way you're using it
for machine learning. Yeah. Well, so as I said, right, so the features, which is anonymized data,
gets labeled by regulators. In this case, it's, so actually we have labels for the moderation part,
but we also have labels for the theme part. Now, and as I said before, so the images that are
approved, they are published on the application right, which I mean are open source, not open source,
but they are open to everybody because they come from a backend that publicly shares the images,
so then what we do for this, for example, is that those rather than keeping the features because
that model is a bit more complex, we keep a pointer to where the image is stored in the database
for the application. And then we'll reuse that if the image was deleted by the customer,
well, we don't have the image. It's a pity that it's not there, but at least it's not our problem anymore.
What we do with this feature store and actually something where currently working on, hopefully,
to be done by the end of the year, it's a sort of retraining pipeline plus a bit testing framework
as well with both actually. So the idea is checking this feature store ever now and then, right?
Okay, we have no features for retraining, you know, the personal detector, right? Yeah, we don't,
all right. Then let's retrain. Before we launch live, even though we think the model is better,
right? Let's have a maybe testing where both models are running the production at the same time.
And then it's up to of course us as developers, but also part of the business side of moderation
to decide, all right, the new model is better or no, the new model is no better.
With the AB testing framework, I'm curious how you're kind of packaging and deploying the models.
Yeah, so for model experimentation and model registry, for example, we use the open
source version of MLflow, which is the, I guess, the package from Databricks. We, what we did is
we created our own, the Moster with the MLflow backend. And then within the model registry,
so we have the most in either production or staging or of nothing. And in the core of moderation,
the core of moderation is built upon a step function in AWS. So what we have in this step
function is, okay, I have the person detector here. And if I have a staging model for the person
detector, then the task is a parallel task where the inputs will flow towards both so that I can
have a one-to-one analysis to say, okay, which model is performing better in all of the images, right?
Okay, and so you mentioned step functions. Do you use serverless technologies pretty broadly
in deploying ML? Yeah, I think, and that's also one of the biggest changes we did. I think
this year, it's been quite a, quite a change here, I think. We are full on serverless, so everything
is deployed with AWS sound, for example. And then everything is, yeah, step function, lambda,
we have also an ECS Fargo running for the biggest model. But that's generally its own. Yeah,
you could say everything is so interesting. And so I'm curious how you're, you mentioned,
it sounds like you're evolving the infrastructure. You've evolved it quite a bit over the past year
with COVID year or so. And I'm wondering if you can share kind of what the prior state was,
and what were some of the drivers for moving to serverless and container service?
Yeah, so, well, as I said, so the moderation products started two and a half years ago,
and when we started, we thought it would be best if we tried to manage everything ourselves,
which meant, all right, let's go full on lambdas, and then rather than, you know, having step
function logic or things like that, let's go SQL, SNSs, Dynamos with streams.
If I had a picture of the old setup, I think it would be scared, and many people may be
listening also to be scared to see that image. But then that evolved a bit into, all right,
let's go ECS, so let's go full on Fargo desks. The problem in that scenario is that
we, in the moderation setup, because the application is, I wasn't going to say deployed,
I guess, the moderation exists in 26 markets, and we get there constantly, all day, everything.
It makes no sense to have a Fargo task that is stopped, and then has to be started when new
data comes, because there's data coming in maybe a bit true for a second, which meant that the
Fargo task I run at 24 hours a day, and then for the Fargo task, which is basically a Docker container,
right, to receive whatever needs to be moderated, is that we needed to then enable a queue,
so then that queue would get some data, and then at some point in the thread looping, right,
you would get the message. I mean, it worked very well, everything was perfectly fine, and
it was not a big deal, but then we consider that whenever we get new employees, new colleagues
in my team, it's hard to explain the flow, it's hard to explain or maybe try to understand what
is happening, right, and I think it was in December 2020 that AWS Sam came with an update,
no, not AWS Sam, Lambda came with an update, where you can deploy your custom Docker images to Lambda,
and that just, yeah, that made our work much easier. Life was better that day,
because what we did is, well, we have to do very, which is really, let's just transfer them to Lambda,
because we know how Lambda works, Lambda is amazing. AWS, not sponsored, I'm gonna say now.
But having the Docker container running in Lambda, then we could also integrate it into
the step functions without having to have task tokens waiting for the callbacks and a lot of
complications, right, and effectively with this meant, and what this means up to today is that
today we can deliver responses under 10 seconds when we talk for, you know, many images with comments
and everything. Whereas before, we were below the minute, so we actually managed to reduce the
average times from a minute to 10 seconds, which is quite the accomplishment, I think, as well.
And do you run into runtime constraints for using Lambda when you're doing image inference?
No, no, and that's because so what we do, and as I said, right, so the feature store, right,
again, and clicking back to the feature store, so the images, they are passed through a resident 50
without the specification layer, which is, I think, a very common approach, right, transfer
learning. You've already got the representation, and you're just doing classification.
And it's funny, because there was this Tesla AI event, a couple of weeks ago, maybe a couple of
months ago, where they mentioned they are doing classification, image classification with something
called Hydra. And what is Hydra? I mean, it's just a resident network, where then you have
different heads, which classify different things. And I was like, well, this is exactly what we
do. Why didn't you think of cool name like that before? We've talked quite a bit about the feature store.
Can you talk a little bit about how that evolved or any challenges you have run into
in bringing that project to fruition? Yeah, so the feature store was, I think it was,
one of the biggest problems was from idea to reality, actually, because we were like, of course,
a feature store makes sense, right? I mean, if you want our models to improve, you need data,
you need labeled data, right? And I guess that is the suffering of every ML engineer, right? It's
not about the data. The spread of data in the world is not, but it's not labeled, right? That's
the problem. So the aviation was there, but we were, it took us quite a, quite a period of time
to try to figure out what's the best approach. How do we do this best, you know, also future thinking?
Because it's like, well, I mean, I could just, you know, get all the data from the manual
moderation and stack it up in some one drive and, you know, let it work there forever,
which is what a lot of people and a lot of companies do with the data, I'm afraid to say.
But then we came to the realization, and I think it was also really some posts on other companies,
how I think maybe it was Cooper, actually, how they did feature stores in Uber.
So then we came to that to making a feature store client, everything we do is in Python,
so it was in Python too. Well, when using the backend as AWS, because we are full on AWS,
but of course, we tried to make it also cloud-agnostic so that if one day we move to
Azure, or we'll move to, I don't know, any other else, then there's not that many problems,
or it can also be integrated, right? And I think one of the learnings also we've got in the
feature store in the first steps is that, all right, I have features, so let's say the feature,
right? It's an image that went through Resident 50, so we go from a couple of megabytes image
to seven kilobytes, a feature array, right? And I know it because I've seen many of them,
so I know it's seven kilobytes.
With all right, because we're going to get a lot of data, I think on average, we get around
10 to 15,000 images per day on the evaluation platform, so you can think, right? So 10 to 15,000
images per day, 10 to 15,000 features. Let's store them in S3, so we store them in S3,
and then we have a catalog in no SQL database, which is DynamoDB and Amazon Work Services.
Everything was fine. The data is starting, it's increasing in numbers, everything looks to be
wonderful. Until the day we want to query the data for the first time, because we sell that.
Let's see how the query works. Well, it turns out that S3 is not the ideal scenario when you want
to query a lot of small files, because that is, and I'm not an expert in this sort of
infrastructure things, but there's all of these handshake things that come between requests and
get into data. That was taken longer than actually fetching the data itself, and because there's a
lot of... Why was the query against S3 and not the Dynamo? Well, the DynamoDB was to keep a catalog,
so in the Dynamo, you would be like, okay, this is the feature path on S3, and this is the label,
so you just need to get the data from S3, because in our eyes, it's where data is supposed to be
stored, it's in a bucket rather than a table. The problem that you ran into was just the latency
for requesting a single file in S3 when your bucket had a lot of files. Yeah, exactly, and also
requesting a big dataset, or a big feature dataset, because I guess for a couple of hundreds,
for features doesn't matter, but I think that one of my colleagues did the calculation right,
so it was, I think, 40 milliseconds per feature, and I'm not exactly sure of the number now,
but what we learned is that we're probably into something else, so what we did is migrate the data
itself as well to DynamoDB, so that the catalog, because I mean, the feature can also be converted
to a bytes array, so then the bytes array can also be stored as, yeah, as no SQL entry write.
So by moving the data into the catalog itself, what we managed to do is that the data now is
fetched 16 milliseconds per feature, which is, you know, howling at the time, and that
that actually proved to be very efficient. So is the data community there at LEGO? You know,
is everyone kind of full stack, or do you have your more traditional data scientists, and then,
you know, folks that are more ML engineers, how does the, what's kind of the range of skill sets
or culture there in terms of full stackness, if that's a thing? We are very happy
we have full stack development in our team, because she's a mastermind in doing our front-end
tool. Before we had him, our think our front-end app was maybe something out of university,
you could say. But so we are split actually, so we have, like myself, so we have machine learning
slash data engineers, you would say, right? We also have the more standard data scientists
as well, and then we have data management, folks, and platform people, so that once that are helping
with the more standard infrastructure, like setting as the account, the security, and of the
basic things that you need when you are an enterprise company. Interesting. We haven't talked
in detail about the engagement tool, or some of the interesting challenges you ran into
in developing that tool. Well, yeah, there's also been a couple, right? Because when you come out,
fresh from university, right? You think data sets are beautiful, clean, and, you know, I think
that's what a lot of people think about them, because when you read research papers, the data is
just beautiful, right? And you have, I don't know, 16 GPUs, and all the money in the world, apparently.
Well, that's not our case, even though it also goes well, I guess monetarily speaking of labor,
but I think one of the, so the main issue, this is this all started, so the whole idea for
this engagement tool started also, maybe two, two and a half, three years ago, and the initial idea
was that we would get the data to then train only image, so we started on with images, to train
an image classifier that could recognize themes. Then the constraint was we want this algorithm
to be on device. So what we did is we went from the smallest network known, which is, well,
at the time was mobile net actuators, and that's three, five megabytes, and that was to be for them.
So, so that we had a problem because it's a wall. We can try to go down, we can try to minimize,
we can try to reduce weights between layers, we can try to do a lot of this sort of optimizations,
but the results were already a bit clumsy. So, you know, you maybe had a Jurassic World build,
and I could recognize it as styles. So it goes far from idea, and I actually the idea of the project
stopped there. Until a couple of months after somebody said, well, when we tried to do this theme,
the texture for actually interacting with the customers or with the users, because then
it's a wall. You have room to do the network you want, we don't really care because it's just
going to run in the cloud, and it's going to run after the fact, right? So it's going to run
at some point in the day to just, you know, get the themes and send some likes and comments, right?
And that went very fine. I think we started with five themes, which is the top five categories,
also, from the application, and probably the biggest issue there again was the data quality, right?
So you can think of our application like the level life app as Instagram, right? So when you're a
user, you take a picture of your level build, you will write a title in the description,
say, you know, this is my cool creation, this is a Star Wars save file. And you can also then
add the hashtags back on Instagram. So those hashtags could be, you know, hashtags, Star Wars,
hashtags, a lot of other things. So because this data is available in the social media
as backend, in this sort of, you know, you could say clumsy labeling, which is all right,
well, we could try maybe running the first iteration, trying to crawl all of this data
with some specific themes and maybe also even keywords. We know are used for specific themes,
right? So let's say, okay, if a user has published a Star Wars image and says, this is Chihuahua,
I know Chihuahua is Star Wars minifigure. Well, I can probably make it in my dataset saying it's
Star Wars. So that's how we collected data the first time for five classes. And it worked
wonderfully well. There were also a lot of misclassifications, but that was also expected from us.
And we tried to clean, so we tried to, I think we did a couple of iterations from the keywords
and the hashtags we used as well. Because you could think, if you said the spaceship,
the spaceship could be Star Wars, but it could also be in Jailor, it could also be City.
There's a lot of different spaces. That's the problem at labor, right? That imagination is the
limit, right? So it's not the real world unfortunately or fortunate. It makes it a bit more fun.
But yeah, so then we got the data. The first model with five classes worked. It was accepted by
the business side. And after that happened, then we extended to 10, no nine classes. And then
now we're up to 13 classes. And I think the latest issues we've been having are not issues,
but the latest challenges we have to overcome was growing to 15 classes. Now we have a three third
by data set of images, which you might think, well, it's not that much. You know, there's like
ImageNet has 15 million images, I guess. It's quite a, it's quite a bunch when it's the first time
we work with such a big data set for a production-ready solution, right? And using SageMaker,
we also learned a lot of SageMaker because we don't have on on premise GPUs. So well,
I have it here, but I have it for gaming on my free time. And so we also have to do some learning
on how to do to let SageMaker the best possible. And to be fair, that also was very helpful when
we also got support from them from the enterprise side. Because then yeah, so we were using SageMaker
notebooks, which is just a full on Jupyter notebook. It turns out that even though you can choose
GPU instance for the notebook, it is not the recommended scenario. The training on a GPU has to
be done on the SageMaker training instead. The notebook is more just for data prep and the
visualization. Well, we didn't know that. Until of course, we needed to run this 13-class training,
actually the 9-class 13 training, we did it on the SageMaker notebook. And I think it took
a week, maybe, a week and a half. Whereas for the 15-class, we then got these estimators
running on the training jobs. And that took, I think, half a day. So more data less time.
It was a good trade-off, I think. Got it. Got it. Very cool. Very cool. What are the,
kind of what's next at LEGO with ML? What are you excited about or looking forward to?
I think it's a very open question, right? Because again, so we are a company that makes a lot of,
I guess, out of the ordinary things, right? And one of the projects, for example, we've
been looking at is a taxonomious, right? So let's say how many cards we have, how many
motorbikes, how many cats we have at LEGO, right? And you could say, well, we are LEGO, we own
all of our data, we know what we have, and that's true. But what is, well, who is telling me that
tomorrow there's not going to be a unicorn with a fish legs, for example, right? So this makes this
taxonomious complicated, right? Because how do you evolve an algorithm, or a set of algorithms
that can get a new class that has never seen before, it does not look like anything, right? Because
you can have a fish in a unicorn, but if they are combined, who's the winner here? How do you
make sure that your algorithm can learn the new class if you're coming? And it's something we're
looking into, and I think it's hopefully coming next year. It requires a lot of prep work, that's
for sure. But there's also a couple of other things. So we're also trying to, to upgrade some of the,
of the A4, which is A4 is an adult fan of LEGO. So all of these adult oriented experiences,
we're also trying to put on a bit more spice to them for making it all exciting for the adults to
use, saying, you can have this specific application here, can you recognize what you're building,
things like that. So trying to interact with the users as well. But it's complicated, I think,
right? Because you can have very big dreams, but it's always the, it's always bureaucratic,
right? And it's always about the data, right? It's like, well, sure, you can ask me to classify
all of the red LEGO bricks you have, but I need to know what a LEGO brick is first, right? And what is
red? Right. Very good, very good. Well, Francesc, thanks so much for joining us. It was great
learning about your projects, and especially how you've built out the platform and infrastructure
to support them at LEGO. Yeah, and well, thanks again for having me. And I guess maybe one last learning,
you don't need to go to Kubernetes, you know, you can always go sellers. Awesome, thanks so much. Yes, thank you.
