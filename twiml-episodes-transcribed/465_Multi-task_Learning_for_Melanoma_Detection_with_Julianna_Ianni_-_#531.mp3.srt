1
00:00:00,000 --> 00:00:12,800
All right, everyone. I am here with Juliana Iyani.

2
00:00:12,800 --> 00:00:17,600
Juliana is vice president of AI Research and Development at Prussia.

3
00:00:17,600 --> 00:00:24,560
Juliana, welcome to the Twomo AI podcast. Thanks, Sam. Got to be here.

4
00:00:24,560 --> 00:00:29,440
I am really looking forward to digging into this conversation.

5
00:00:30,560 --> 00:00:36,320
In particular, we're going to learn a bit about some of the work that you and your team are doing

6
00:00:36,320 --> 00:00:42,800
on cancer diagnosis and as well as the process of commercializing that.

7
00:00:43,600 --> 00:00:48,640
Before we dig into that topic, I'd love to have you share a little bit about your background

8
00:00:48,640 --> 00:00:52,240
and how you came to work in the field. Yeah, sure.

9
00:00:52,240 --> 00:01:01,360
Sure. I guess I have always had an interest in being able to help people.

10
00:01:01,360 --> 00:01:03,760
That's always where I wanted my career to go.

11
00:01:04,720 --> 00:01:08,560
And so I pursued my undergraduate degree in biomedical engineering.

12
00:01:08,560 --> 00:01:10,160
I thought that was the best way to do that.

13
00:01:12,480 --> 00:01:15,680
During my undergraduate, I did several internships.

14
00:01:16,240 --> 00:01:18,880
One of them was in biomedical informatics.

15
00:01:18,880 --> 00:01:26,240
And I actually had kind of a bad mentorship experience during that internship.

16
00:01:26,240 --> 00:01:29,200
But I ended up shaping my career positively.

17
00:01:32,480 --> 00:01:35,840
So I, you know, I didn't have a lot of direction.

18
00:01:36,880 --> 00:01:40,960
I ended up really kind of teaching myself machine learning principles

19
00:01:40,960 --> 00:01:48,080
because of that lack of direction during that internship.

20
00:01:48,080 --> 00:01:54,960
Eventually, you got assigned a fantastic new mentor and learned quite a bit during that.

21
00:01:54,960 --> 00:01:58,480
But during that internship, I was learning all these things about machine learning,

22
00:01:58,480 --> 00:02:04,480
which wasn't really referred to as machine learning and anything that I was reading at the time.

23
00:02:04,480 --> 00:02:12,880
But, you know, all of the same ideas. And I realized that, you know, that is very powerful.

24
00:02:13,840 --> 00:02:18,400
But we have kind of the most, the most data in the medical field.

25
00:02:19,040 --> 00:02:23,840
It's an imaging. And so I got really, really interested in medical imaging.

26
00:02:24,480 --> 00:02:30,720
And, you know, pursued that, you know, thought I need to learn, learn more about that.

27
00:02:30,720 --> 00:02:36,240
And so that's how I got very interested in MRI. I took a whole bunch of classes at Vanderbilt,

28
00:02:36,240 --> 00:02:46,080
my undergraduate school in MRI. And was really fascinated by everything that I went there,

29
00:02:46,080 --> 00:02:55,680
wanted to pursue it. So I wanted to go to graduate school and continue research in MRI.

30
00:02:55,680 --> 00:03:04,000
But actually, I haven't haven't shared this publicly with anyone yet, but I didn't get in the first time.

31
00:03:04,000 --> 00:03:13,280
So I took a little bit of a detour. And I, you know, pursued kind of my second love, which was,

32
00:03:13,280 --> 00:03:20,080
which was neuroscience. I did some research at a lab at Vanderbilt in

33
00:03:20,080 --> 00:03:27,840
a visual attention and memory, studying the way, the way that brain works as, as you're doing

34
00:03:27,840 --> 00:03:35,360
those sorts of tasks. And that curiously kind of, kind of relates a lot to what I'm doing now.

35
00:03:36,400 --> 00:03:42,320
So I was really glad that I did that. Still was really interested in medical imaging. And,

36
00:03:42,320 --> 00:03:50,480
went on to pursue my PhD in that the next year. I tried again, succeeded. So, yeah, so I pursued

37
00:03:50,480 --> 00:03:59,280
my graduate degree in MRI. I joined Wilgerson's lab at Vanderbilt and studied kind of imagery

38
00:03:59,280 --> 00:04:09,200
construction and trajectory correction and trajectory optimization for MRI for non-cartesian trajectories.

39
00:04:09,200 --> 00:04:16,720
So kind of ways to make MRI faster, but still avoid having all these artifacts in your images.

40
00:04:16,720 --> 00:04:24,080
And then sort of the latter half of my PhD, I was pursuing

41
00:04:24,080 --> 00:04:34,720
machine learning algorithms to identify some patient tailored parameters. So for high field

42
00:04:34,720 --> 00:04:40,400
imaging, so for seven to you, which is not what's usually used in hospitals, usually it's

43
00:04:40,400 --> 00:04:47,840
lower field right now. But for seven to you, you can get better images, but you just have to be

44
00:04:47,840 --> 00:04:54,880
a little bit more careful and tailor things to two patients. And so a lot of that...

45
00:04:55,920 --> 00:04:58,800
Sorry, when you say seven to you, is that like a type of MRI?

46
00:04:58,800 --> 00:05:06,560
It's like the strength of the magnetic field, sorry. Yeah, so it's a stronger magnet.

47
00:05:07,440 --> 00:05:13,440
You tend to get kind of on even images, they're kind of unevenly lit, if you will,

48
00:05:14,720 --> 00:05:24,080
uneven signal. And so I was I was working on designing machine learning algorithms to predict,

49
00:05:24,080 --> 00:05:29,920
predict the RF-shin parameters, predict the parameters of the scanner that you need to

50
00:05:29,920 --> 00:05:35,200
to tune that and make the image kind of uniformly lit. Imagine those are, you know,

51
00:05:35,200 --> 00:05:39,440
those parameters were typically the domain of the radiologists, like they would have their hands on

52
00:05:39,440 --> 00:05:47,520
dials trying to figure out how to get the best image for a given patient. And now you're trying

53
00:05:47,520 --> 00:05:54,640
to automate that? Sort of. So it's actually it was be done by another algorithm. The issue was

54
00:05:54,640 --> 00:05:59,840
that it would take a really long time. So you'd have to do a scan. So you already had the patient

55
00:05:59,840 --> 00:06:06,080
in the scanner. And then you had to run this really extensive algorithm to get the parameters.

56
00:06:06,080 --> 00:06:11,360
And then go plug those into the scanner. And then run the scans that you actually wanted to do.

57
00:06:11,360 --> 00:06:19,600
So it's just a, you know, not a very feasible process for most patients and for bringing it kind

58
00:06:19,600 --> 00:06:24,720
of to the clinic. So that was what we were working on. And then doing all this in my PhD, I

59
00:06:24,720 --> 00:06:31,600
was really interested in machine learning, but seeing deep learning take off all around us,

60
00:06:31,600 --> 00:06:39,760
I was I was pretty excited about the potential of that. And so really, really wanted to get in on it.

61
00:06:39,760 --> 00:06:46,800
I was also kind of in my PhD, you know, realizing that what really, really drives me is seeing

62
00:06:46,800 --> 00:06:51,200
something put into practice. I wasn't I wasn't getting that in academia where I was.

63
00:06:52,320 --> 00:06:57,920
Some people do get that from academia, but I wasn't in the right place for that. And so I really

64
00:06:57,920 --> 00:07:03,120
wanted to go into industry and be a part of a company that was that was using deep learning

65
00:07:03,120 --> 00:07:08,000
from medical imaging. And in a way that it could could actually impact patient's lives in the short

66
00:07:08,000 --> 00:07:13,760
term. And was Prussia already doing deep learning on images when you got there?

67
00:07:15,760 --> 00:07:22,320
Yes, yes. So Prussia was very small when I joined. I think less than less than 10 people

68
00:07:22,320 --> 00:07:28,000
at the time. So I got in, got in kind of early days, although the company had been around for a

69
00:07:28,000 --> 00:07:36,000
few years already, I think, was founded in 2014. Now 10 people doing AI data stuff or 10 people

70
00:07:36,000 --> 00:07:42,080
total the entire company. 10 people total the entire company. Okay. Yeah. Yeah.

71
00:07:42,080 --> 00:07:50,320
The like the core products, you say image management like content management system for

72
00:07:50,320 --> 00:07:59,520
radiology or. So pathology images are kind of different different from radiology. So usually,

73
00:07:59,520 --> 00:08:06,960
you know, if you have made a suspected cancer, you'll get like an MRI or a CT scan that will be like

74
00:08:06,960 --> 00:08:12,960
the first thing that happens. But later down the road, if there's still suspicion, you'll get a

75
00:08:12,960 --> 00:08:19,760
biopsy. And that tissue gets sent off to a lab that processes it. They're eventually going to

76
00:08:20,720 --> 00:08:28,720
slice up the tissue into really thin pieces and dead it and wax and stain it and put it on a

77
00:08:28,720 --> 00:08:34,800
slide. So that, that like glass slide, like, you know, like you had in high school biology class,

78
00:08:34,800 --> 00:08:40,320
like gets looked at normally under a microscope to diagnose your cancer.

79
00:08:42,000 --> 00:08:48,720
But what Prosa deals with is actually digitized versions of those glass slides. So very,

80
00:08:48,720 --> 00:08:56,320
very high resolution images of very, very small pieces of tissue. Tell us a bit about where deep

81
00:08:56,320 --> 00:09:03,200
learning comes into the picture. So deep learning as it's kind of taken off in the field of pathology,

82
00:09:04,000 --> 00:09:12,720
in the field of kind of studying this type of image. And there are whole host of potential

83
00:09:12,720 --> 00:09:21,760
applications for it. What we focus on at Prosa is applications that are built in some way to make

84
00:09:21,760 --> 00:09:32,400
the pathologist's lives easier to make labs more efficient. So enable more efficient assigning

85
00:09:32,400 --> 00:09:40,000
of cases to different pathologists or specialists or enabling them to review their cases faster.

86
00:09:40,000 --> 00:09:51,760
We also focus on deep learning systems or as we probably, probably controversially refer to them

87
00:09:51,760 --> 00:10:05,680
AI systems. We also focus on systems that can improve accuracy of pathologists. So in some scenarios,

88
00:10:05,680 --> 00:10:14,560
there are certain diagnoses that are a lot harder to make than others. And pathologists often

89
00:10:14,560 --> 00:10:23,760
disagree on those diagnoses. And so any way that we can, that we can use AI to deliver information

90
00:10:23,760 --> 00:10:29,920
to the pathologists that will improve their accuracy, that is something that we're interested

91
00:10:29,920 --> 00:10:36,640
into. So that's maybe a good segue to talking about the paper that you and your team

92
00:10:37,440 --> 00:10:44,400
will be presenting at the ICCV computational challenges in digital pathology workshop.

93
00:10:45,840 --> 00:10:51,840
If that's not enough of a mouthful, the title of the paper is a pathology deep learning system

94
00:10:51,840 --> 00:11:01,280
capable of triage, of melanoma specimens utilizing dermatopathologist consensus as ground truth.

95
00:11:02,080 --> 00:11:10,320
Did I get all that? You did, you did. Sorry for that. We'll make our next title shorter.

96
00:11:12,080 --> 00:11:19,200
So what's those paper all about? So this paper is about, you know, new new technology,

97
00:11:19,200 --> 00:11:24,880
new AI system that we've developed that's really kind of building on our previous work. So

98
00:11:26,160 --> 00:11:36,080
previously we built a system that could sort of sort and triage pathology cases before pathologists

99
00:11:36,080 --> 00:11:45,520
reviewed them. It's sort of them into four different categories that were not specifically

100
00:11:45,520 --> 00:11:52,320
diagnostic categories, but, you know, related to the diagnosis, kind of diagnostic groupings,

101
00:11:52,320 --> 00:11:59,840
you could think of. And, you know, the reason that we wanted to do that was to kind of make

102
00:11:59,840 --> 00:12:08,000
pathologists work more efficient, you know, be able to have them review certain cases earlier

103
00:12:08,000 --> 00:12:14,000
in the day so they could order additional tests if they needed to earlier in the day if it was

104
00:12:14,000 --> 00:12:19,440
a group of cases that could, that was more likely to need additional testing, things like that.

105
00:12:20,960 --> 00:12:26,240
But that previous system didn't do something that pathologists were really, really interested in,

106
00:12:27,280 --> 00:12:36,560
and that was to classify melanoma. Some melanomas is not the most common form of skin cancer,

107
00:12:36,560 --> 00:12:47,360
but, you know, one of the most deadly. And so, it's also one that pathologists

108
00:12:49,760 --> 00:12:57,760
disagree on a lot. So, if something, if you go in and get a biopsy of your skin and

109
00:12:57,760 --> 00:13:02,560
make it sent in, and it's kind of one of the ones that looks suspicious for melanoma,

110
00:13:02,560 --> 00:13:10,560
it's fairly common for pathologists to disagree on whether, whether it actually is melanoma. And

111
00:13:10,560 --> 00:13:17,920
you tend to, tend to want a subspecialist to review that. So, there are, there matter pathologists is

112
00:13:17,920 --> 00:13:23,920
one of the long words in our title, who are the subspecialists, who review these cases,

113
00:13:23,920 --> 00:13:33,600
and not always, not always, but, you know, sometimes it's not a very obvious, not a very obvious one.

114
00:13:33,600 --> 00:13:38,640
You want that subspecialist to kind of give you a second opinion, essentially, on the case.

115
00:13:39,760 --> 00:13:42,960
So, it would be more efficient if we could route the cases to them first.

116
00:13:43,520 --> 00:13:49,760
Okay, okay. I guess one of the virtues of a long title is that it actually says what the

117
00:13:49,760 --> 00:13:59,840
paper is about. And this one, I think, does a good job of kind of raising one of the core challenges

118
00:13:59,840 --> 00:14:08,960
that it sounds like you must face. If the specialists and, you know, reviewers of these pathology

119
00:14:08,960 --> 00:14:17,920
images tend to disagree, that seems like that would make building datasets and, you know,

120
00:14:17,920 --> 00:14:24,960
collecting ground truth particularly difficult. Yes, yes, absolutely. And I don't think,

121
00:14:24,960 --> 00:14:30,000
I don't think we even realize how difficult when we first set out, set out to build this system.

122
00:14:32,080 --> 00:14:37,280
I don't think the pathologists that we were working with, realized, realized exactly how much

123
00:14:37,280 --> 00:14:45,200
they disagree. When we set out to build this, there's actually surprisingly not that much literature

124
00:14:45,200 --> 00:14:52,560
on the matter. How have you characterized, how would you and how do you, like, have you

125
00:14:52,560 --> 00:14:59,520
characterized the degree of disagreement among the pathologist looking at a given specimen?

126
00:14:59,520 --> 00:15:07,920
Yeah, it's hard. So, there are, I guess, you know, a few different ways of doing that.

127
00:15:07,920 --> 00:15:15,920
What we, what we did for this paper was to have three different amount of pathologists reviewing

128
00:15:15,920 --> 00:15:24,160
the cases. And specifically, the ones that were melanocytic or, you know, basically the category

129
00:15:24,160 --> 00:15:30,640
that, the broader category that melanoma is in. So, that ranges from benigniva, which are just kind

130
00:15:30,640 --> 00:15:39,280
of your basic mole, all the way to, you know, invasive cancer. But we had pathologists review

131
00:15:39,840 --> 00:15:46,800
all of those cases in three different ones and, and characterize, you know, how often,

132
00:15:46,800 --> 00:15:54,400
how often did they disagree for which original diagnosis? And what we ended up doing for this paper

133
00:15:54,400 --> 00:16:01,520
was to just say, okay, we're not going to use it if they didn't all agree, because very often,

134
00:16:01,520 --> 00:16:06,000
they didn't all agree, especially for the ones that were kind of on the more suspicious end of

135
00:16:06,000 --> 00:16:11,840
things. I think it was something like 40 to 50 percent of the time they didn't agree.

136
00:16:11,840 --> 00:16:19,520
Oh, wow. So, the, so the 60 percent of the time, 50, 60 percent of the time where they do agree,

137
00:16:19,520 --> 00:16:26,720
that becomes your training data set. And the outcome that they agreed on is your, your label.

138
00:16:28,160 --> 00:16:34,960
Yeah. But again, from the explicit title, it sounds like you're using some kind of consensus

139
00:16:34,960 --> 00:16:44,720
algorithm on, you know, real data, you know, to kind of predict the outcome.

140
00:16:44,720 --> 00:16:52,000
Um, or where's consensus coming in is just that, that three out of three pathologists agree,

141
00:16:52,720 --> 00:16:57,520
is basically what we're using as our ground truth. Okay. Yeah, I was, I think I was envisioning

142
00:16:57,520 --> 00:17:04,640
some kind of like ensemble weird thing and like doing a consensus among your, among predictions.

143
00:17:05,200 --> 00:17:12,160
Yeah. Well, you're actually, you're not far off from what, what we did end up doing with,

144
00:17:12,160 --> 00:17:17,280
um, one of the models in the system. So the system, um, that actually makes these predictions,

145
00:17:17,280 --> 00:17:26,240
makes these classifications is, is a hierarchy of models. Okay. And, uh, the one that is actually

146
00:17:26,240 --> 00:17:32,880
classifying these, these melanocytic specimens, um, is what we ended up building for that is a

147
00:17:32,880 --> 00:17:39,040
multi task model. Uh, and I think that a lot of the reason that we ended up wanting to do that

148
00:17:39,040 --> 00:17:45,760
was because of this, this coordinates, because the pathologists are disagreeing so often. Um, so

149
00:17:45,760 --> 00:17:52,000
we, what we wanted to do was divide these specimens into three different categories. So kind of,

150
00:17:52,800 --> 00:17:59,360
you can think high intermediate and low risk and, you know, with, with melanoma being the highest

151
00:17:59,360 --> 00:18:05,040
invasive melanoma being the highest and intermediate melanoma in C2 or the lesions that were

152
00:18:05,040 --> 00:18:12,320
severely atypical, uh, and then low being your benign ones. Uh, but so you could do that as

153
00:18:12,320 --> 00:18:19,440
like a three way classifier, right? Um, we didn't end up doing that, doing it that way because,

154
00:18:19,440 --> 00:18:24,160
at least in the early days and most building this, it didn't work very well. Um,

155
00:18:24,160 --> 00:18:30,080
That's a good reason we could do it that way. Yeah. Um, but I think a lot of the reason that

156
00:18:30,080 --> 00:18:35,280
it didn't work while it was because of the, the disagreement. And so what we ended up doing was

157
00:18:35,280 --> 00:18:43,760
building, uh, you know, a multi task classifier that just did, uh, a binary classification at a time

158
00:18:43,760 --> 00:18:50,160
essentially. So it was, can you distinguish the low risk from the high risk? Can you distinguish

159
00:18:50,160 --> 00:18:56,400
the low risk from the intermediate risk? And can you distinguish the intermediate risk from the

160
00:18:56,400 --> 00:19:02,480
low risk? Can we chunk up this classification into smaller tasks? Um, and just do that because,

161
00:19:03,280 --> 00:19:10,400
uh, our ground truth was just too noisy, uh, to do this as it's just a typical like three way

162
00:19:10,400 --> 00:19:17,760
classifier. And the approach to training was kind of an end-to-end approach, you know,

163
00:19:17,760 --> 00:19:27,680
they kind of multi-task trained, uh, this entire system, or did you, um, train, you know, them

164
00:19:27,680 --> 00:19:32,480
independently or hierarchically or something like that, hierarchically? We did end up having to

165
00:19:32,480 --> 00:19:39,040
train them independently. So it's not, not an end-to-end system. Uh, it would be cool if it was, uh,

166
00:19:39,040 --> 00:19:44,880
but the system we built has, uh, has kind of several pieces. So there's, there's, there's an

167
00:19:44,880 --> 00:19:51,040
and better, um, and then there's, there's kind of a hierarchy of three different models, um,

168
00:19:51,040 --> 00:19:58,000
that to arrive at a final classification. Um, so it would have been, would have been a lot to build

169
00:19:58,000 --> 00:20:03,760
this as an end-to-end system. Yeah. Um, probably a lot of issues that we would have run into, but not

170
00:20:03,760 --> 00:20:10,720
impossible. Uh, so tell us about the, the model. What is it, you know, what are the components? I mean,

171
00:20:10,720 --> 00:20:16,000
you mentioned high-level what the components are, but like, what, what are the components? And how did

172
00:20:16,000 --> 00:20:23,520
you, um, how did you arrive at them? You know, were they all custom? Was there some off-the-shelf,

173
00:20:24,240 --> 00:20:31,680
uh, stuff that you used? How did you ultimately arrive at an approach for this?

174
00:20:31,680 --> 00:20:37,200
Yeah. So this system, like I said, was built off a, uh, a system that we, that we had previously

175
00:20:37,200 --> 00:20:43,280
built. So it's a bit of an evolution, many different, many different pieces involved. Um, so one of,

176
00:20:43,280 --> 00:20:48,720
one of the issues I haven't discussed yet that we've run into was that, um, some of the images have

177
00:20:48,720 --> 00:20:58,640
artifacts that are kind of correlated, uh, in many cases with the, the ground truth. Um, and so we

178
00:20:58,640 --> 00:21:05,280
wanted to get rid of those, um, those, namely, the major ones that we encountered were that

179
00:21:05,280 --> 00:21:11,680
pathologists sometimes use penning to mark, um, mark cancer on the tissue. Yeah. Um, so that was,

180
00:21:11,680 --> 00:21:17,760
that was an issue we ran into. Um, so that was kind of a whole, whole other project that we,

181
00:21:17,760 --> 00:21:25,360
that we did and we had, we had a custom model built, um, to eliminate penning from our images.

182
00:21:25,360 --> 00:21:33,040
And did you try to remove it or just kind of, uh, lock it out? I guess the way to, the way to talk

183
00:21:33,040 --> 00:21:39,840
about this is to explain a little bit more about how our models work. Um, so first, um, first thing

184
00:21:39,840 --> 00:21:46,960
we have to do because these are very, very large images, um, we have to kind of chunk them up. Um,

185
00:21:46,960 --> 00:21:52,960
so we're first kind of detecting the tissue regions on the side. Um, and then we just kind of

186
00:21:52,960 --> 00:22:00,240
patch those up into what we call tiles. So there's maybe like a whole bunch thousands of 128 by

187
00:22:00,240 --> 00:22:10,080
128 pixel images, um, that are our tiles. Um, and so when we remove ink, we literally just drop

188
00:22:10,080 --> 00:22:17,280
tiles. Yeah, I realized as I said it that kind of blocking it out wouldn't be much better than

189
00:22:17,280 --> 00:22:24,640
just leaving the ink in if you fed, uh, image with blocked out ink to the downstream model.

190
00:22:24,640 --> 00:22:31,360
Uh, it, yeah, probably not, but it depends. I've seen, I've seen a paper that tried to use

191
00:22:31,360 --> 00:22:38,640
GANS to just, you know, generate, generate, you know, like tissue, look, yeah, in fill. Uh,

192
00:22:38,640 --> 00:22:45,200
but it didn't look like it worked very well. So I don't, maybe there's a strategy there.

193
00:22:46,880 --> 00:22:52,800
Yeah, to what degree was that 128 by 128 kind of a architectural hyperparameter that you played

194
00:22:52,800 --> 00:22:59,920
with? Was that just where that decision come from? Sam there are so many hyperparameters and so

195
00:22:59,920 --> 00:23:08,400
little time. Uh, we didn't, we didn't, we haven't played with that one yet, actually.

196
00:23:08,400 --> 00:23:17,520
It may be taking a step back, you know, we've seen over the, the years kind of many approaches to

197
00:23:17,520 --> 00:23:27,920
trying to detect cancer in, uh, images, um, you know, all different kinds of cancers, all different

198
00:23:27,920 --> 00:23:34,880
kinds of approaches, you know, to the point where like some very, very famous AI people said,

199
00:23:34,880 --> 00:23:40,400
oh, this is a solved problem. Like, you know, we're going to replace radiologists with, uh, with AI

200
00:23:40,400 --> 00:23:49,120
systems. I think we've all, uh, kind of dialed back, you know, that enthusiasm a bit or rhetoric,

201
00:23:49,120 --> 00:23:55,760
if you would go that far, but, um, you know, kind of give me a sense for or give us a sense for,

202
00:23:56,560 --> 00:24:02,720
you know, what distinguishes this particular work from the other work out there, you know,

203
00:24:02,720 --> 00:24:08,000
that is trying to do similar things. Definitely not a solved problem. I feel like, uh, we all

204
00:24:08,000 --> 00:24:14,960
generally in this industry now, I can understand how complicated the, even the self-driving cars problem is,

205
00:24:14,960 --> 00:24:22,160
and uh, this is at least that complicated. And that's actually, that's actually interesting,

206
00:24:22,160 --> 00:24:27,680
because as much respect that I have for the complexity of this problem relative to solution,

207
00:24:27,680 --> 00:24:34,080
to solving it, when I think of a self-driving car, I think of a much more complex system,

208
00:24:34,080 --> 00:24:39,920
uh, with lots of moving parts. And in your mind, they're kind of on the same order in terms of

209
00:24:39,920 --> 00:24:45,600
complexity, you know, elaborate, elaborate on it. Where's the complexity that's not obvious to us,

210
00:24:46,160 --> 00:24:49,920
you know, when we think of problems like the one you're, you're trying to solve here?

211
00:24:49,920 --> 00:24:54,320
Yeah, a great question. I mean, it wasn't obvious to me when I, when I got into this field.

212
00:24:55,040 --> 00:24:59,120
Um, you know, it's like, you know, with image and that is solved, right?

213
00:24:59,120 --> 00:25:06,240
Uh, what could be more complicated, but it is quite complicated. So when you think about what a

214
00:25:06,240 --> 00:25:12,320
pathologist does, they're not just, not just kind of looking at an image and classifying it.

215
00:25:12,320 --> 00:25:17,200
They kind of synthesize a whole bunch of things, but pathologists, when they're looking at an

216
00:25:17,200 --> 00:25:22,880
image, they're not just thinking about the image, they're thinking about the patient's history

217
00:25:22,880 --> 00:25:29,600
and clinical information, they're thinking about the patient's age, um, and they're thinking about,

218
00:25:30,000 --> 00:25:38,240
uh, you know, the last time that they saw a patient like this or, uh, you know, all these other

219
00:25:38,240 --> 00:25:44,160
inputs that they are kind of synthesizing to make a diagnosis. And then, aside from that,

220
00:25:44,160 --> 00:25:53,520
the, uh, you know, the diagnosis itself can be very, very complicated for, for some of these things.

221
00:25:53,520 --> 00:26:04,000
So, um, in dermatopathology, which is skin pathology, um, there are probably over 500 different

222
00:26:04,000 --> 00:26:08,560
diagnoses that you can have. So there's a lot of different types of patterns that you need to be

223
00:26:08,560 --> 00:26:17,280
looking for, uh, in these, in these, uh, type of images that, that we look at, um, they're very,

224
00:26:17,280 --> 00:26:23,040
very high resolution. And so the pathologist is kind of first looking at them, kind of, at a high

225
00:26:23,040 --> 00:26:29,280
level and just, um, you know, seeing, okay, what are the regions that I should focus my attention on?

226
00:26:30,240 --> 00:26:34,240
Um, and what are the reasons I, regions I can totally ignore? Because they,

227
00:26:34,240 --> 00:26:40,800
I probably, in many cases, literally can't go through a pixel by pixel. They have to decide, okay,

228
00:26:40,800 --> 00:26:46,320
what can I ignore? Uh, kind of like a driver does when they're driving a car. Uh, what a,

229
00:26:46,320 --> 00:26:52,000
where do I focus my attention? Uh, and then they're going in at higher power to those regions that

230
00:26:52,000 --> 00:27:00,480
may be concerning, uh, based on, you know, based on all of their knowledge, um, and looking for very

231
00:27:00,480 --> 00:27:06,880
particular associations between like one region of an image at another? Well, it, it sounds like the,

232
00:27:07,520 --> 00:27:15,120
you know, the high level, um, summary is that the problem that you're trying to solve is not the

233
00:27:15,120 --> 00:27:19,920
binary classification that we've been sold in the media. It's a much more complex and nuanced

234
00:27:19,920 --> 00:27:32,400
classification problem, um, and that among other things, increases the level of complexity,

235
00:27:33,040 --> 00:27:38,400
you know, on par with kind of the percept, perceptual problem that a driver, you know,

236
00:27:38,400 --> 00:27:42,640
something that's trying to drive might be experiencing. But that's kind of the origin of your,

237
00:27:42,640 --> 00:27:49,040
your, your comment. I think I interrupted you on a prior question and I'm trying to remember

238
00:27:49,040 --> 00:27:56,480
what that was, uh, but well, what that was was like, what's different about, um, the problem that

239
00:27:56,480 --> 00:28:02,480
you're trying to solve with this paper and, you know, some of the other kind of cancer detection

240
00:28:02,480 --> 00:28:07,920
systems that have been publicized or that are prevalent in other fields or being worked on in

241
00:28:07,920 --> 00:28:12,640
other fields. One of the big differences and other things that we're really excited about with

242
00:28:12,640 --> 00:28:20,560
this work is that, um, there hasn't been, there hasn't yet been any kind of system, even like a,

243
00:28:20,560 --> 00:28:26,400
you know, a different stain or a genetic test or something like that, that can identify melanoma

244
00:28:26,400 --> 00:28:32,400
before a pathologist reviews a case. Uh, and the reason we're really excited about the system

245
00:28:32,400 --> 00:28:38,960
being able to do that is because, uh, it means that you can send that case to the right person

246
00:28:38,960 --> 00:28:45,200
very quickly rather than it having to get passed along from one person to another to, to say, like,

247
00:28:45,200 --> 00:28:50,960
oh, this isn't what I should be diagnosing. Let me send it to my colleague down the hall or across

248
00:28:50,960 --> 00:28:59,840
the city. Um, so we, we think we can speed up, um, the timeline for getting melanoma diagnosed with

249
00:28:59,840 --> 00:29:06,560
a system, uh, which is, is pretty exciting. Uh, should bleed to, should lead to patients getting

250
00:29:06,560 --> 00:29:10,560
their diagnosis faster, like, getting treatment faster ultimately.

251
00:29:11,440 --> 00:29:15,680
Is it the case of other systems that are out there, like rely on some kind of

252
00:29:16,400 --> 00:29:21,040
metadata or coding or something like that that comes from the pathologist in order to make

253
00:29:21,040 --> 00:29:27,440
their decision? Like, good question. With, uh, skin pathology, really aren't that many AI systems

254
00:29:27,440 --> 00:29:34,240
out there at all. Um, so we're, we're one of the first to really explore this and, and definitely

255
00:29:34,240 --> 00:29:41,040
the first to really, uh, explore it at a, at a level where it's, um, you know, this close to the

256
00:29:41,040 --> 00:29:47,040
clinic. There have been, there have been a, a number of papers kind of with a more academic that,

257
00:29:47,920 --> 00:29:56,080
um, exploring the issue. Um, but the only other tests that are out there right now that can detect

258
00:29:56,080 --> 00:30:05,280
melanoma are, um, our tests that are run after the pathologist has lifted the image. So it's, um,

259
00:30:06,080 --> 00:30:12,800
basically, you know, they'll be looking at something that, um, looks suspicious for melanoma and

260
00:30:12,800 --> 00:30:19,680
it's maybe borderline. Um, so the pathologist is not sure. Is this melanoma? Is this not, you

261
00:30:19,680 --> 00:30:28,480
know, should I recommend that they get a really deep oxygen, uh, or not? Um, and so they'll send it off

262
00:30:28,480 --> 00:30:37,280
to, to a lab, um, that does, like, genetic testing or does, um, an additional stain on the tissue,

263
00:30:37,920 --> 00:30:42,800
um, so that they can tell a little bit more about it and, and those tests will kind of like give

264
00:30:42,800 --> 00:30:51,760
you like a score, um, for the likelihood of melanoma, um, but you can't run it on just like any

265
00:30:51,760 --> 00:30:58,000
skin specimen. Um, they're only intended for after the pathologist has, has looked at the case.

266
00:30:58,000 --> 00:31:07,040
Talk a little bit about the, the specimen versus image issue, you know, when we read about,

267
00:31:07,040 --> 00:31:12,640
you know, a system that might, you know, one of the early ones was, uh, identifying breast cancer

268
00:31:12,640 --> 00:31:20,800
in a, you know, a radiological image. Um, it sounds like one of the big distinctions you're

269
00:31:20,800 --> 00:31:27,200
making is that your system is looking at the specimen level. Why is that important?

270
00:31:27,200 --> 00:31:36,800
So with, with pathology, uh, there are usually a couple different, um, whole-side images that belong to a,

271
00:31:36,800 --> 00:31:44,160
a given specimen, um, and that specimen, by the way, might correspond to like a biopsy that has

272
00:31:44,160 --> 00:31:48,720
been taken, like, of a mole. So your, your mole might, might end up being kind of a specimen,

273
00:31:49,360 --> 00:31:53,760
uh, and it might end up with, uh, tissue on a couple different slides that the pathologist has to

274
00:31:53,760 --> 00:32:03,120
look at. Um, and some of those, some of those slides might not even have, um, any, any cancer,

275
00:32:03,120 --> 00:32:09,760
any mole on them. They might be totally clean, totally normal skin. Um, and so the pathologist,

276
00:32:09,760 --> 00:32:14,080
when they're making a diagnosis, they look at, they look at all the slides that are part of this

277
00:32:14,080 --> 00:32:23,120
specimen with a case. And we wanted, wanted our ASS to be able to do that too. Um, I, it was a challenge

278
00:32:23,120 --> 00:32:28,640
because there's so much tissue on a single slide and not, uh, you know, not to mention multiple slides.

279
00:32:29,360 --> 00:32:36,240
Um, but we're, we're excited to be able to do that, um, because it means that, uh, our, our system

280
00:32:36,240 --> 00:32:41,680
is a little bit closer to, to reality, to what the pathologist is, is actually doing.

281
00:32:41,680 --> 00:32:49,200
Is the model's ability to work at the specimen level kind of part of its training routine?

282
00:32:49,200 --> 00:32:54,640
And I guess in contrast, what I'm imagining is that there's a simple heuristic that is,

283
00:32:54,640 --> 00:33:00,160
hey, if any of these slides has cancer, then the specimen has cancer. Is that not? Yeah.

284
00:33:00,160 --> 00:33:04,960
Actually true. Yeah, it's a good question. And what you just mentioned, if any of these slides

285
00:33:04,960 --> 00:33:09,920
have cancer, then the specimen has cancer. That's what, that's what we've done for our previous system.

286
00:33:09,920 --> 00:33:16,080
Um, so it was kind of aggregating the, the classifications at the end. Yeah. Um, but it's,

287
00:33:16,080 --> 00:33:23,040
it's a, I guess it's a less accurate way of doing it. It introduces more room for, for error

288
00:33:23,040 --> 00:33:28,960
in your model. And so we did, um, we did develop a way to train basically with, with all the

289
00:33:28,960 --> 00:33:36,640
slides in this specimen. Um, and we, we do that basically by including all of the tiles from all

290
00:33:36,640 --> 00:33:44,560
of the, all of the slides, um, in one, um, one bag under the multiple instance learning paradigm.

291
00:33:45,680 --> 00:33:52,240
So what that's doing is basically, um, you know, within your network, you're kind of,

292
00:33:52,240 --> 00:33:58,240
you're processing the features from, from each of the tiles, um, kind of individually. And then you

293
00:33:58,240 --> 00:34:07,440
have this aggregation function. That's, um, basically kind of, um, combining, combining, um,

294
00:34:07,440 --> 00:34:12,080
what the model has learned from all those tiles in it, you know, in some kind of learned weighted

295
00:34:12,080 --> 00:34:18,960
fashion. Kind of continuing on this particular point is the idea that if a model makes this kind of

296
00:34:18,960 --> 00:34:26,400
simple aggregated decision, the decision, um, you know, the error rate is higher, uh, because that

297
00:34:26,400 --> 00:34:33,600
decision is just wrong sometimes. And, you know, the, the aggregated heuristic is just not correct,

298
00:34:33,600 --> 00:34:43,440
or is it more nuanced like by doing aggregation, the training is more efficient, or the model learns

299
00:34:43,440 --> 00:34:48,800
different things. And so because it learns those different things, it makes better decisions.

300
00:34:48,800 --> 00:34:57,680
I think the answer is both. Um, so yeah, I think it is a more error prone process. If you have to do

301
00:34:57,680 --> 00:35:02,640
that aggregation at the end, because you have the chance to like, like, you know, let's say you have

302
00:35:02,640 --> 00:35:07,600
three different images in a single specimen, you, you have three different chances for your model

303
00:35:07,600 --> 00:35:14,080
to make an error on one of those, and then you're going to aggregate them. Um, but the other side of

304
00:35:14,080 --> 00:35:21,760
the coin is that, uh, the way we do this shouldn't prove the training process as well, um, in some

305
00:35:21,760 --> 00:35:30,160
former fashion. So, um, we could do training by having a pathologist go in and label each of the

306
00:35:30,160 --> 00:35:38,320
slides. So, um, I have three slides from the specimen. Um, the specimen is a melanoma specimen.

307
00:35:38,320 --> 00:35:43,040
Two of the slides have melanoma and one of them, you know, is just normal skin. I could have a

308
00:35:43,040 --> 00:35:49,040
pathologist label that, but it's not how, um, the medical records are, the medical records just have,

309
00:35:49,040 --> 00:35:54,240
say, you know, this specimen has three slides and it's melanoma. Um, so it's like an extra

310
00:35:54,240 --> 00:36:00,800
data curation step. Um, if we can't directly use that, um, you know, the other thing that we could do

311
00:36:00,800 --> 00:36:06,960
is, you know, train a system with those three slides kind of independently and say they're all melanoma,

312
00:36:06,960 --> 00:36:11,760
but then you're introducing noise into your system. What's the overall structure of your

313
00:36:11,760 --> 00:36:20,240
model or was the architecture look like? So we talked a little bit about how, uh, you know, our first step

314
00:36:20,240 --> 00:36:29,040
is kind of removing ink from the slides. Yeah. Um, after we do that, um, what we want to, what we

315
00:36:29,040 --> 00:36:38,560
want to do is, um, basically create embeddings. Um, and so we actually just use an off-the-shelf

316
00:36:38,560 --> 00:36:46,480
model for that. We use, um, ImageNet trained on ResNet, uh, for this, for this work. Uh, and so we,

317
00:36:46,480 --> 00:36:52,640
we basically create feature embeddings for each of the tiles independently. Uh, and then we have

318
00:36:52,640 --> 00:36:58,800
a hierarchy of models that actually makes the classification, um, based on those embeddings. Um,

319
00:36:58,800 --> 00:37:07,920
and the first, I guess there's one model at the top of the hierarchy, um, and that is basically

320
00:37:07,920 --> 00:37:14,080
distinguishing melanoma from everything else, um, or I guess melanoma and suspected melanoma

321
00:37:14,080 --> 00:37:21,600
from everything else. Uh, the reason why we did that, uh, is because, uh, we wanted this system to

322
00:37:21,600 --> 00:37:29,440
be highly sensitive to melanoma. And since melanoma is only about two percent of the overall cases,

323
00:37:30,480 --> 00:37:40,480
um, we had quite a class of balance problem. Um, so, so that's the first step of the hierarchy.

324
00:37:40,480 --> 00:37:49,040
And, and once we have kind of, we have the sort of suspected melanoma group, um, we have a subclassifier

325
00:37:49,040 --> 00:37:59,280
that, um, is, uh, kind of refining that classification. Um, the, the, the, this is the multi-task model

326
00:37:59,280 --> 00:38:04,320
that I talked about before that's distinguishing the high, low, and intermediate risk specimens.

327
00:38:05,360 --> 00:38:12,240
Um, and then on the other side of things, um, you know, for those specimens that were, um,

328
00:38:12,240 --> 00:38:21,280
basically, uh, classified as not, not at risk for melanoma, um, we, we are providing four

329
00:38:21,280 --> 00:38:26,800
different classifications. So we have a separate classifier that's doing that. Um, and it's,

330
00:38:26,800 --> 00:38:34,320
it's basically, uh, distinguishing, um, the two most common types of, of cancer, um,

331
00:38:34,320 --> 00:38:44,080
so squamous cell carcinoma and basal cell carcinoma, uh, and then those, um, those lower risk

332
00:38:44,080 --> 00:38:50,320
melanocytic specimens. So basically, you're benigny of our moles. Uh, and then we have this

333
00:38:50,320 --> 00:38:59,600
wonderful group called other. And that, that is literally everything else, uh, which, um,

334
00:38:59,600 --> 00:39:05,600
I was like, I think I talked about earlier is there are hundreds and hundreds of diagnosis and

335
00:39:06,320 --> 00:39:12,560
in skin pathology. And so we couldn't possibly classify them all. And so we have this

336
00:39:12,560 --> 00:39:20,960
catch all group. I guess it'd be, it'd be really interesting to, um, you know, understand kind of the,

337
00:39:20,960 --> 00:39:29,440
uh, you know, cell images through or sent, you know, specimen images through the, the eyes of

338
00:39:29,440 --> 00:39:34,720
image and that I guess is what I'm thinking of here. Yeah. Yeah. It would be really interesting.

339
00:39:34,720 --> 00:39:41,120
And so you kind of put all this stuff together. You mentioned that you trained the components,

340
00:39:42,240 --> 00:39:48,640
um, independently or at least the classifier independently. Is that like the features of the,

341
00:39:48,640 --> 00:39:57,920
uh, that your training data is the, the feature in the embedding for, yeah. Yeah. Yeah.

342
00:39:57,920 --> 00:40:03,600
Okay. The embeddings for each of the three models in the hierarchy. Yeah. Okay.

343
00:40:04,560 --> 00:40:09,600
Cool. And so, you know, tell us about how it all, how it all worked and, and, you know,

344
00:40:09,600 --> 00:40:16,560
what kind of results you saw and, um, you know, any challenges you ran into that kind of thing.

345
00:40:16,560 --> 00:40:24,640
Yeah. Yeah. So this, this worked pretty well. We were, we were able to distinguish, uh, the melanoma

346
00:40:24,640 --> 00:40:33,440
specimens from, uh, from, you know, the others pretty well. We had area under the curve of, of point

347
00:40:33,440 --> 00:40:42,080
nine, three and classifying the melanocytic suspect specimens, um, and, uh, you know, reproduce that

348
00:40:42,080 --> 00:40:48,240
pretty well in, in the two labs that we had for, for validation. So we were pretty happy with

349
00:40:48,240 --> 00:40:55,760
this result. I'm sure I'm sure it could be improved on with, with more data even, um, the fact that we

350
00:40:55,760 --> 00:41:03,520
could get, um, we could get a, a decent number of the, the melanomas kind of pulled out into this,

351
00:41:03,520 --> 00:41:12,240
this, this suspect classification, uh, already means that you can prioritize those in your, in

352
00:41:12,240 --> 00:41:17,680
your case lists, if you're a pathologist. So, um, you know, versus, they're just kind of like

353
00:41:17,680 --> 00:41:26,320
randomly interspersed throughout your, your day. Otherwise, um, we thought this was, was, was pretty

354
00:41:26,320 --> 00:41:35,680
exciting. And now you're the head of AI research and development at Procia, like, how does this

355
00:41:35,680 --> 00:41:44,320
translate into something that has impacts, uh, you know, at health centers and, and with users of

356
00:41:44,320 --> 00:41:53,600
your system? Yeah, yeah, that's a great question. Um, so kind of have to, have to demonstrate, uh,

357
00:41:53,600 --> 00:42:00,560
that, you know, the system that we've built actually has utility in practice, um, and that you,

358
00:42:00,560 --> 00:42:07,120
you kind of see similar results in practice. Um, so we've already done, uh, a couple of

359
00:42:07,120 --> 00:42:14,480
deployments of this particular system at a couple, uh, academic medical centers. And you'd just kind

360
00:42:14,480 --> 00:42:21,360
of, uh, a trial, uh, so our first, our first step was to see, you know, make sure that it wasn't

361
00:42:21,360 --> 00:42:25,520
biasing the pathologists. So you don't want to, you don't want to impact their diagnosis in a

362
00:42:25,520 --> 00:42:30,880
negative way. So we were, we were able to show that that it didn't have a, uh, any significant

363
00:42:30,880 --> 00:42:37,280
impact on their diagnosis. The next step is showing that it does indeed allow us to reduce the

364
00:42:37,280 --> 00:42:44,160
turnaround time for these cases. And for that, I think we need to, um, you know, we need to show

365
00:42:44,160 --> 00:42:51,200
that at a, at a larger site. Um, and one that's, that's kind of actually, you know, experiencing these

366
00:42:51,200 --> 00:42:57,440
challenges that, um, you know, they have, you know, multiple dermatopathologists or subspecialists

367
00:42:57,440 --> 00:43:02,720
and multiple general pathologists. And, um, how do they, how do they allocate their cases in the

368
00:43:02,720 --> 00:43:09,120
most ideal way that makes them the most efficient? Got it. Got it. So some, um, it beyond the research,

369
00:43:09,120 --> 00:43:16,640
some promising results, kind of in practice, but, you know, work to be done to, I guess show that

370
00:43:16,640 --> 00:43:23,920
it has the desired benefit at scale. Yeah, yeah, exactly. Awesome. Awesome. Well, Juliana,

371
00:43:23,920 --> 00:43:31,120
thanks so much for sharing, uh, a bit about what you're up to and the, the paper. I'm sure there's

372
00:43:31,120 --> 00:43:37,440
lots more interesting stuff. I'm, I'm, this last question that I asked about kind of productizing it,

373
00:43:37,440 --> 00:43:43,840
um, you know, I can imagine going on for a long time, just talking about that, uh, particularly in a,

374
00:43:43,840 --> 00:43:55,280
kind of mission critical, highly regulated, uh, space like healthcare. Um, yeah. But, uh, super

375
00:43:55,280 --> 00:44:01,760
interesting project and, and thanks so much for sharing it. Awesome. Thanks, Sam. Got to be hearing

376
00:44:01,760 --> 00:44:10,400
and glad to share with you guys. Awesome. Thank you.

