1
00:00:00,000 --> 00:00:13,760
All right, I think we are live. Hey, everyone, welcome to the broadcast. I am super excited

2
00:00:13,760 --> 00:00:21,080
for our conversation today. Since the very beginning of the coronavirus pandemic, we've

3
00:00:21,080 --> 00:00:26,800
seen an outpouring of interest on the part of data scientists and AI practitioners wanting

4
00:00:26,800 --> 00:00:33,960
to make a contribution. At the same time, some of the resulting efforts have been criticized

5
00:00:33,960 --> 00:00:39,440
for promoting the spread of misinformation or just being disconnected from the applicable

6
00:00:39,440 --> 00:00:51,880
domain knowledge. The question that we're here to address today is how can data scientists

7
00:00:51,880 --> 00:00:58,600
get involved and do so in a responsible manner. And I've got a great panel lined up to help

8
00:00:58,600 --> 00:01:05,760
us do that. Before we dive in, a couple of housekeeping notes. We really, really want

9
00:01:05,760 --> 00:01:11,720
to ensure that this is an interactive discussion. So your comments, whether you're viewing on

10
00:01:11,720 --> 00:01:21,160
YouTube, Facebook, or Twitter, are visible to me, and I'll relay them to our panel. And

11
00:01:21,160 --> 00:01:28,040
it's really my sincere hope that you will help drive a good part of the discussion today.

12
00:01:28,040 --> 00:01:35,240
Next, this is the first of many discussions that we'll be bringing you on a wide range

13
00:01:35,240 --> 00:01:41,720
of topics. To be notified when we schedule future discussions, please just follow or subscribe

14
00:01:41,720 --> 00:01:48,640
to us on whatever channel you are watching right now. Before introducing our panelists,

15
00:01:48,640 --> 00:01:54,360
I'd like to send a shout out to our friends at IBM for their huge support of what we're

16
00:01:54,360 --> 00:02:00,160
doing here at Twilmo generally and this discussion in particular. IBM has been at the forefront

17
00:02:00,160 --> 00:02:05,680
of the fight against coronavirus. Through their leadership and the COVID-19 high performance

18
00:02:05,680 --> 00:02:10,320
computing consortium, they've helped bring together the federal government and industry

19
00:02:10,320 --> 00:02:16,720
and academic leaders in support of COVID-19 research. In addition, they're bringing together

20
00:02:16,720 --> 00:02:23,040
trusted data from governments, the World Health Organization, and the CDC to provide detailed

21
00:02:23,040 --> 00:02:29,920
virus tracking on the weather channel. And they're offering free access to Watson Assistant

22
00:02:29,920 --> 00:02:37,760
to respond to common COVID-19 related questions. For more information on these resources and links

23
00:02:37,760 --> 00:02:44,880
to dig in deeper, as well as to learn how you can get 30 days of free Coursera access when you

24
00:02:44,880 --> 00:02:52,480
join the IBM data science community, visit the resource page for this program, twilmolyi.com slash

25
00:02:52,480 --> 00:03:01,600
RDS COVID. With that said, I am really excited to introduce myself and our panelists for this

26
00:03:01,600 --> 00:03:09,600
discussion. I'm Sam Charington, founder of Twilmo and host of the Twilmolyi podcast. Rex Douglas

27
00:03:09,600 --> 00:03:16,720
is a computational social scientist and director of the machine learning for social science lab,

28
00:03:16,720 --> 00:03:24,160
MSSL, at the Center for Peace and Security Studies at the University of San Diego.

29
00:03:27,360 --> 00:03:31,840
Rex's research focuses on applying advanced technologies to research problems in

30
00:03:31,840 --> 00:03:40,720
social sciences and policy world, particularly on issues of human conflict. Rob Monroe is a

31
00:03:40,720 --> 00:03:46,960
former guest of the podcast and a long time friend of the show. He's currently CEO of the

32
00:03:46,960 --> 00:03:53,120
machine learning, he's currently CEO of machine learning consulting. Hi, Rob.

33
00:03:54,320 --> 00:03:58,960
Hey, everyone. Rob has extensive experience in crowdsourced data,

34
00:03:58,960 --> 00:04:07,280
disaster response and the intersection of these areas. Leah Shanley is a senior fellow with the

35
00:04:07,280 --> 00:04:13,680
Nelson Institute at the University of Wisconsin Madison, where she focuses on open science

36
00:04:13,680 --> 00:04:21,920
information, innovation strategy and policy, responsible AI and data ethics and expanding diversity

37
00:04:21,920 --> 00:04:31,200
in data science. Welcome, Leah. And GG UN Read is a distinguished engineer and executive

38
00:04:31,200 --> 00:04:37,200
responsible for data science innovations in the healthcare space at IBM, with an emphasis on

39
00:04:37,200 --> 00:04:46,480
the use of predictive models and clinical applications. All righty. So let's jump right in and get

40
00:04:46,480 --> 00:04:51,440
started with a round of questions so that we can get to know our panelists a little bit better.

41
00:04:52,240 --> 00:04:57,520
We'll start with you, Rex. You recently wrote an interesting piece on how to be curious

42
00:04:57,520 --> 00:05:05,280
instead of contrarian about COVID-19, eight data science lessons from a coronavirus perspective.

43
00:05:06,720 --> 00:05:12,000
We're not necessarily going into the specific lesson that you presented that article. We'll come

44
00:05:12,000 --> 00:05:16,800
back to those. Tell us a little bit about the origin of that article and what prompted you to

45
00:05:16,800 --> 00:05:24,720
write it and how that relates to your work at MSSL. Sure. So the big idea is that my note is

46
00:05:24,720 --> 00:05:30,080
ostensibly a takedown of a specific piece that Richard Epstein wrote posted to Hoover,

47
00:05:30,080 --> 00:05:34,720
where he predicted only 500 deaths in the US and made a long list of other false and

48
00:05:34,720 --> 00:05:40,560
sophist arguments. In reality, though, my note is a Trojan horse intended to try to sneak some

49
00:05:40,560 --> 00:05:46,480
basic social science research design methods to a broader audience who may not otherwise

50
00:05:46,480 --> 00:05:53,120
receive that kind of information. I wanted to show by example that these kind of gatekeeping

51
00:05:53,120 --> 00:05:59,280
or lanekeeping arguments are themselves another kind of anti-intellectualism. I argue that

52
00:05:59,280 --> 00:06:04,160
during a crisis, it's exactly when we should be encouraging people to become more curious

53
00:06:04,160 --> 00:06:08,960
and more scientifically rigorous. Right now, a good chunk of the planet actually wants to know

54
00:06:08,960 --> 00:06:14,240
about causal inference and about confidence intervals and about Bayes rule. For God's sakes,

55
00:06:14,240 --> 00:06:24,320
we should be telling them, especially before theaters reopen. Awesome. Rob, a lot of your

56
00:06:24,320 --> 00:06:29,680
perspective on this issue comes from your experience working in disaster response. Share with us

57
00:06:29,680 --> 00:06:34,720
a little bit about your background in this area and the concerns that it raises for you.

58
00:06:34,720 --> 00:06:40,240
Yeah, absolutely. So I've worked as a disaster responder for about 20 years for about the same time

59
00:06:40,240 --> 00:06:46,080
that I've been working as a data scientist. And in every major disaster I've responded to,

60
00:06:46,800 --> 00:06:53,360
I've seen people die as a result of people cutting corners, including actions taken by well-meaning

61
00:06:53,360 --> 00:06:58,640
data scientists, often grabbing media attention when experts should have had that attention.

62
00:06:58,640 --> 00:07:05,520
But like I've also said, I think some of the biggest impact I've had is not when I've been working

63
00:07:05,520 --> 00:07:11,200
in refugee camps or directly disaster response. Some of the biggest impact I've had

64
00:07:11,200 --> 00:07:15,600
has been when I've been at large companies. So I ran AWS's and on the service for a while,

65
00:07:15,600 --> 00:07:20,160
it was on comprehend. And it's when I've been working with large companies like that and

66
00:07:20,160 --> 00:07:25,120
getting them to expand into more languages and think about general ways that tools can be adapted

67
00:07:25,120 --> 00:07:30,640
to different parts of the world. I think that's built a foundational layer that I'm seeing right

68
00:07:30,640 --> 00:07:37,280
now a lot of people responded to COVID using. So I'd love to share on this panel some of the ways

69
00:07:37,280 --> 00:07:43,040
that you can take your existing skills as data scientists and help plug gaps in the COVID response.

70
00:07:44,240 --> 00:07:53,120
Awesome. Leah, much of your work including co-founding citizen science.gov has focused on encouraging

71
00:07:53,120 --> 00:07:58,320
everyday citizens to get involved in science and help accelerate innovations in this country.

72
00:07:59,040 --> 00:08:05,440
My guess is that scientists don't always welcome the input of the quote unquote unwashed masses.

73
00:08:05,440 --> 00:08:07,840
Can you bear a little bit about your experience in this area?

74
00:08:09,200 --> 00:08:13,680
Well, like Rob, I've worked at the intersection and served as a connector between the

75
00:08:14,480 --> 00:08:18,720
academic research community, the tech innovation community, crowdsourcers,

76
00:08:18,720 --> 00:08:25,680
citizen scientists and disaster management practitioners. Like Rob over the last 10 years,

77
00:08:25,680 --> 00:08:34,560
we saw a lot of enthusiasm and well-meaning open data, data scientists and others try to

78
00:08:34,560 --> 00:08:39,920
contribute and innovate in the middle of a crisis. But they lacked an understanding of what the

79
00:08:39,920 --> 00:08:47,600
disaster response community actually needed on the ground. And so we tried to work to build that

80
00:08:47,600 --> 00:08:51,280
bridge between the practitioners, the research community and the digital volunteers.

81
00:08:52,480 --> 00:08:58,880
For citizen science and crowdsourcing, yes, one of the first questions we get when we propose

82
00:08:59,680 --> 00:09:06,080
crowdsourcing or citizen science approaches is what about data quality and that field has

83
00:09:06,080 --> 00:09:11,840
become rather mature and there are a lot of great approaches now for assuring the data quality

84
00:09:11,840 --> 00:09:16,880
levels that you need for a project just as you would with any scientific research or data analysis.

85
00:09:18,480 --> 00:09:24,000
Awesome. And Gigi, your work at IBM involves what you describe as translational science,

86
00:09:24,000 --> 00:09:31,200
so translating between cutting edge, machine learning and AI research and making that accessible

87
00:09:31,200 --> 00:09:37,600
and scalable to your customer and end users. Can you share a little bit about your experience

88
00:09:37,600 --> 00:09:43,840
coming from the clinical and traumatic side of things and how that applies to COVID?

89
00:09:45,840 --> 00:09:53,440
Yeah, it's a very interesting time. I think based on my experience in trying to take cutting

90
00:09:53,440 --> 00:10:00,640
edge technologies to our end users, sometimes a very sophisticated clinician or a consumer who's

91
00:10:00,640 --> 00:10:05,200
trying to understand the risk for the health problem. Thinking through that really,

92
00:10:05,200 --> 00:10:11,520
really drive home the need for transparency. I think near and resonable, we all kind of touch

93
00:10:11,520 --> 00:10:17,280
on palm data. The fact that we got to be transparent, not only on the data and its quality,

94
00:10:17,280 --> 00:10:24,000
but the assumptions that we can use. I think that tends to get overlooked by the end user,

95
00:10:24,560 --> 00:10:29,520
and if it's our responsibility to make sure we work with the domain experts to validate those

96
00:10:29,520 --> 00:10:36,080
assumptions. That's kind of the biggest piece that I'm advocating. I do think data science,

97
00:10:36,080 --> 00:10:40,560
that's what we can do, even to overcome why the issues of data security, privacy,

98
00:10:41,200 --> 00:10:46,560
and how to leverage the streaming data from IoT. I think that's a lot we can contribute on,

99
00:10:46,560 --> 00:10:48,480
with the matter of how to restrict the balance.

100
00:10:50,160 --> 00:10:57,680
Awesome. Once again, I want to remind our viewers to contribute questions via the chat. We will

101
00:10:57,680 --> 00:11:07,040
incorporate those into the conversation, but I'd like to start out really exploring the idea of

102
00:11:09,040 --> 00:11:18,800
what's at stake when data scientists get involved and talk through your examples of where you

103
00:11:18,800 --> 00:11:25,280
seeing help, where you've seen harm, and what folks are really worried about when they kind of

104
00:11:25,280 --> 00:11:32,160
push out their arms and say, hey, data scientists shouldn't really be involved in this without

105
00:11:32,160 --> 00:11:39,600
working with domain experts, whether that be epidemiologists, virologists, what have you.

106
00:11:40,560 --> 00:11:45,120
I'll let you start with this Rob, and then we'll incorporate others.

107
00:11:46,160 --> 00:11:51,520
Sure. Happy to. First off, this isn't you. It's not new that we're seeing data scientists

108
00:11:51,520 --> 00:11:59,120
stepping to disaster response efforts. We have a lot of past examples already, where there

109
00:11:59,120 --> 00:12:04,880
have been very negative effects. I'll give three examples. I also presented all three of these

110
00:12:04,880 --> 00:12:09,760
at the KDD conference last year, one of the largest data science conferences. These should

111
00:12:09,760 --> 00:12:15,040
already be part of the dialogue that people are having around data scientists responding to

112
00:12:15,040 --> 00:12:20,480
disasters. This is not something new, and if you haven't read the papers and the dangers in the

113
00:12:20,480 --> 00:12:26,800
past, you should think carefully about the kind of work that you're undertaking. One of the

114
00:12:26,800 --> 00:12:31,840
biggest negative impacts I've seen was during the Ebola outbreak in West Africa,

115
00:12:31,840 --> 00:12:35,760
having worked in epidemic tracking and having lived in West Africa in the past,

116
00:12:35,760 --> 00:12:42,960
and a lot of work involved in this. One of the biggest causes for unneeded deaths were data

117
00:12:42,960 --> 00:12:48,400
scientists in the West coming up with really like doomsday prediction models about how bad a

118
00:12:48,400 --> 00:12:54,160
Ebola could be. The people who were going on the big media programs during the Ebola outbreak,

119
00:12:55,360 --> 00:12:59,920
they weren't the professional epidemiologists. These were data scientists from adjacent fields,

120
00:12:59,920 --> 00:13:05,120
and the media just eats up their story like this could kill us all. As a result of the international

121
00:13:05,120 --> 00:13:10,320
pressure from the media, filled by people who did not have that expertise, but who looked like

122
00:13:10,320 --> 00:13:15,440
there were experts, there was a lot of misinformation, a lot of people being too scared,

123
00:13:15,440 --> 00:13:19,760
and so we calculated that for every person who died from Ebola in West Africa,

124
00:13:20,400 --> 00:13:25,680
10 more people died from treatable illnesses because they were avoiding hospitals and clinics

125
00:13:25,680 --> 00:13:34,080
unnecessarily. This was a very clear example of where so many people died because of misinformation

126
00:13:34,080 --> 00:13:41,760
that started from well-meaning non-experts. Some of the others relate to open data and data

127
00:13:41,760 --> 00:13:46,560
security, like people have mentioned already. When I was hoping to respond to the previous

128
00:13:47,760 --> 00:13:54,880
coronavirus outbreaks, the SARS-CoV-1 or MERS, we found that because a lot of these outbreaks were

129
00:13:54,880 --> 00:13:59,680
in countries where the rule of law is very different to the West, people go into different

130
00:13:59,680 --> 00:14:05,760
assumptions. For example, Saudi Arabia wanted to find people in social media who were reporting

131
00:14:05,760 --> 00:14:10,960
the actual numbers of patients in hospitals that might have differed from the official

132
00:14:10,960 --> 00:14:15,840
government numbers. We evaluated this at that time and decided that even if this project was

133
00:14:15,840 --> 00:14:21,520
well-meaning and even if it only used open data from people's public tweets, these people could

134
00:14:21,520 --> 00:14:26,880
be seen as disagreeing with the government. It could be persecuted as a result, and so we decided

135
00:14:26,880 --> 00:14:33,440
not to go forward with this particular project. Then the last of three examples, also working in

136
00:14:33,440 --> 00:14:39,280
the Middle East during the uprising in Libya when Gaddafi was overthrown, I was hoping the UN

137
00:14:39,280 --> 00:14:44,320
track displaced people. It seems like a use case that just suggests to be very non-controversial,

138
00:14:44,320 --> 00:14:50,640
like who's internally displaced or refugee in another country and needs help, but people

139
00:14:50,640 --> 00:14:56,480
within the region saw the UN's involvement as being a Western involvement, and our security was

140
00:14:56,480 --> 00:15:02,400
compromised and some people treated as collaborators with a Western invader, not as people helping

141
00:15:02,400 --> 00:15:10,000
in humanitarian efforts. Again, this was set up by data scientists who didn't have a background

142
00:15:10,000 --> 00:15:15,600
in disaster response, technologists who thought they could do good, and ultimately did far more harm

143
00:15:15,600 --> 00:15:21,920
than good. Those are three very negative examples. I'll pass it over to the other panelists here.

144
00:15:21,920 --> 00:15:26,800
I do want to assure everyone watching that. I do have a lot of good examples of ways that people

145
00:15:26,800 --> 00:15:31,520
are hoping and can help that I want to come back to. I want a second Rob's comments. We saw the

146
00:15:31,520 --> 00:15:36,880
same thing and working the disaster space curation of social media for who needs medical care,

147
00:15:36,880 --> 00:15:44,160
who needs water, who needs rescue, etc. It's not just in other countries. It's here in the U.S.

148
00:15:44,160 --> 00:15:51,040
we're seeing retaliations against certain vulnerable populations. While we espouse open data,

149
00:15:51,040 --> 00:15:55,920
I think we need to be careful. As Gigi mentioned, ensure the privacy and security, particularly

150
00:15:55,920 --> 00:16:05,520
around vulnerable populations. Yeah, and this conversation reminds me of it's about six months ago,

151
00:16:06,720 --> 00:16:13,440
where there is a commonly used population health analytics to help stratify individuals for

152
00:16:13,440 --> 00:16:21,040
intervention, to prevent hospital wear mission, where that work was built upon a national

153
00:16:21,040 --> 00:16:26,800
national data set, to understand individuals' health first. What was overlooked is the fact that

154
00:16:26,800 --> 00:16:31,360
the training data set was balanced. Certain demographic tends to have richer data than other.

155
00:16:32,560 --> 00:16:37,680
The mathematics were great. The sensitivities, the specificity were

156
00:16:39,680 --> 00:16:44,240
high quality. Unfortunately, there's a piece of my data bias that we overlook.

157
00:16:46,320 --> 00:16:50,480
That's a piece that we should really think through. Pandemic is really

158
00:16:50,480 --> 00:16:57,920
evolving very quickly. Every local county has very different data composition and data coverage.

159
00:16:58,800 --> 00:17:03,120
As data scientists, I think it's responsible to really think hard about what is the data

160
00:17:03,120 --> 00:17:09,360
that we bring into our models and being very type of local, one of the strengths and weakness

161
00:17:09,360 --> 00:17:15,600
of doing this. We don't keep that in mind and have the discipline to do the bias and diversity

162
00:17:15,600 --> 00:17:27,040
check at the end. We could really sort of advance the allowance. I think those are some great

163
00:17:27,040 --> 00:17:36,160
examples, Rob. It's clear, I think, to everyone that the stakes are high. I'm curious for you,

164
00:17:36,160 --> 00:17:41,040
Rex, based on a conversation from a comment you made in our earlier conversation.

165
00:17:41,040 --> 00:17:48,560
When a lot of people see these kinds of, or think of these kinds of potential harms,

166
00:17:49,280 --> 00:17:54,720
the immediate reaction is, if you don't have an epidemiology degree or a virology degree,

167
00:17:55,280 --> 00:18:01,280
you shouldn't be involved. You should take us a backseat to the experts.

168
00:18:04,480 --> 00:18:10,800
You're a data scientist who is involved in this work, but you don't have a degree in

169
00:18:10,800 --> 00:18:17,760
epidemiology or background and disaster response. How does that occur for you and

170
00:18:18,720 --> 00:18:21,760
how have you reacted to those kind of commentary?

171
00:18:25,360 --> 00:18:31,680
The way I tried to lay out the piece is to make an argument that the difference between good work

172
00:18:31,680 --> 00:18:38,320
and bad work is not your level of expertise in that field or the strength of the quality of your

173
00:18:38,320 --> 00:18:44,800
priors. I define science as a curiosity about the true state of the world and the application

174
00:18:44,800 --> 00:18:51,840
of evidence and methods to form true beliefs than we held yesterday. That holds for every

175
00:18:51,840 --> 00:18:56,400
level of skill and every kind of question or form of empirical inquiry we could form.

176
00:18:56,960 --> 00:19:02,160
There are parts about COVID that the experts don't understand currently. There are things we'd

177
00:19:02,160 --> 00:19:05,760
like to know about what's going to happen in the future that almost no one understands,

178
00:19:05,760 --> 00:19:10,800
and these are probabilistic forecasts. There are smaller things that individuals would like to

179
00:19:10,800 --> 00:19:17,440
understand of the expert recommendations which ones should I follow. What happens when they contradict

180
00:19:17,440 --> 00:19:22,560
each other? What assumptions are the models making and are they getting better or worse over time?

181
00:19:22,560 --> 00:19:28,960
How do I be a smarter consumer of expert information? I don't know a single scientist no matter what

182
00:19:28,960 --> 00:19:35,040
field they're in who isn't climbing up the walls right now trying to understand this global

183
00:19:35,040 --> 00:19:40,480
pandemic and how it affects them, how it affects their family, what they can contribute or better

184
00:19:40,480 --> 00:19:48,000
understand themselves. My big takeaway is that we're not going to be able to prevent bad work

185
00:19:48,000 --> 00:19:52,960
through gatekeeping. The people who are doing bad work before COVID are doing bad work now and

186
00:19:52,960 --> 00:19:57,840
will do bad work in the future. What we can try to do is we can try to encourage people to be

187
00:19:57,840 --> 00:20:04,160
curious and methodologically rigorous and then keep giving them concrete examples and feedback to

188
00:20:04,160 --> 00:20:11,040
positively support them on that journey as they move up the curve. Earlier you brought up a couple

189
00:20:11,040 --> 00:20:18,480
of examples of even the so-called experts to epidemiologists, credentialed epidemiologists

190
00:20:19,440 --> 00:20:28,720
publishing studies that were later shown to be later taken down. Can you mention a couple of those?

191
00:20:28,720 --> 00:20:36,320
Sure, so there's a really good paper called Learning as We Go, which does sensitivity analysis

192
00:20:36,320 --> 00:20:44,080
on that IHME is two for health and metrics evaluation forecast. That is an example of a

193
00:20:44,080 --> 00:20:50,080
credentialed group according to the Lane Keepers, who is producing the most culturally dominant

194
00:20:50,080 --> 00:20:56,960
forecast in the US right now. But they're 95% confidence intervals or write only about 27% of the

195
00:20:56,960 --> 00:21:04,320
time and their accuracy doesn't improve when you shorten the time horizon for their predictions.

196
00:21:04,320 --> 00:21:08,720
And if you actually eyeball the confidence intervals, they get more confident the further out in time

197
00:21:08,720 --> 00:21:12,640
they go. And there's all sorts of problems with their model and all sorts of weird assumptions,

198
00:21:12,640 --> 00:21:18,960
but this kind of picking apart someone else's work, if you have a background in statistics or

199
00:21:18,960 --> 00:21:25,120
causal inference or research design, anyone with that training can start to see the pieces and

200
00:21:25,120 --> 00:21:30,480
understand the underlying problems and maybe contribute some feedback and some review that will

201
00:21:30,480 --> 00:21:39,840
help others in picking between models. Just because they were first to be pitched or they were

202
00:21:39,840 --> 00:21:44,400
able to culturally position themselves as the most influential forecast doesn't mean that they're

203
00:21:44,400 --> 00:21:49,840
the only forecast within epidemiology and it doesn't mean that we should just accept them

204
00:21:49,840 --> 00:21:59,120
because they're provincial. So we had a really interesting question come in via the chat. David

205
00:21:59,120 --> 00:22:06,880
Clement is referring to the peer review process that we've historically used in academic publications

206
00:22:06,880 --> 00:22:16,880
to ensure the quality of research. What's the role of peer review and why or why isn't it helping

207
00:22:16,880 --> 00:22:24,560
in this environment? I can speak to that briefly. So peer review is absolutely helping in this

208
00:22:24,560 --> 00:22:30,400
environment. In many disasters in the past, which came on much faster than COVID,

209
00:22:31,840 --> 00:22:36,800
there was no advantage in cutting the corners of science. So the science itself couldn't be rushed.

210
00:22:38,720 --> 00:22:44,720
What can be sped up is the review process itself. And so this is something that I have seen

211
00:22:44,720 --> 00:22:50,480
different in COVID to previous disasters. I think because it is so global that a lot of major

212
00:22:50,480 --> 00:22:56,880
publications are speeding up this process and letting it be open. So for example, at the largest

213
00:22:56,880 --> 00:23:02,640
computational linguistics conference, ACL, which is coming up, there's now a workshop specifically

214
00:23:02,640 --> 00:23:08,400
focused on COVID. It's an open review. So it's allowed itself to be the right place for these

215
00:23:08,400 --> 00:23:14,400
kinds of discussions to happen. And papers are being reviewed on a rolling basis. So as quickly

216
00:23:14,400 --> 00:23:21,760
as possible, as soon as we've been submitted. So we've been able to really speed up a process

217
00:23:21,760 --> 00:23:26,560
that taken nine months waiting for the right conference to come around in the past and

218
00:23:26,560 --> 00:23:30,720
bring it down to a couple of weeks without cutting corners on the science itself. It's just

219
00:23:30,720 --> 00:23:34,000
been great to see that across a lot of different disciplines at the moment.

220
00:23:35,040 --> 00:23:40,320
And when you've worked to scale up these kinds of citizen science efforts, have you incorporated

221
00:23:40,320 --> 00:23:46,000
or what kind of review processes or checks and balances or peer review have you been able to

222
00:23:46,000 --> 00:23:52,240
incorporate into those kind of programs? Well, with citizen science, you would go through the same

223
00:23:52,240 --> 00:23:58,160
kind of peer review that you would with any scientific research if the intent is to do scientific

224
00:23:58,160 --> 00:24:04,800
research versus say strictly education. So you would still produce published papers that go

225
00:24:04,800 --> 00:24:11,280
through the normal peer review process. And in fact, these days, a lot of citizen science projects

226
00:24:11,280 --> 00:24:17,840
are including the citizen scientist as co-authors on those papers. And in some open peer review process,

227
00:24:17,840 --> 00:24:23,040
you know, people can contribute to the peer review as well. I want to tag on though to Rob's comment

228
00:24:24,000 --> 00:24:30,480
about, you know, a lot of this preprints of the research is being made available and speeding up

229
00:24:30,480 --> 00:24:36,560
the peer review. The White House has issued a call to action to the tech community. They've made

230
00:24:36,560 --> 00:24:46,160
the publishers have made the literature now openly available around COVID. It made a team of

231
00:24:46,160 --> 00:24:50,880
different organizations have gotten to including Microsoft, the National Library of Medicine,

232
00:24:50,880 --> 00:24:57,840
the Chan Zuckerberg initiative to make those articles machine readable. So they now have 29,000

233
00:24:57,840 --> 00:25:02,320
scholarly articles on COVID that are machine readable. The White House now would like the data

234
00:25:02,320 --> 00:25:09,600
science community to develop text and data mining techniques to help speed up, you know, synthesizing

235
00:25:09,600 --> 00:25:16,720
these materials. And they are also provided a set of research questions to help guide some of that.

236
00:25:16,720 --> 00:25:26,480
And that is available on Kaggle. Awesome. Awesome. So Ayadela gets us to our core question, you know,

237
00:25:26,480 --> 00:25:33,760
what can data scientists do to help? And she's specifically asking for resources, but I think there's

238
00:25:33,760 --> 00:25:40,080
a lot to dig into just on, you know, what are things that data scientists need to be thinking about

239
00:25:40,080 --> 00:25:47,760
when they're considering getting involved? Rob, do you want to get us started on that one?

240
00:25:47,760 --> 00:25:52,880
Yeah, sure. I can get through a low new list of things which are really important and for which

241
00:25:52,880 --> 00:25:58,160
they're very little negative consequences. There's a lot of ways that you can jump in and help

242
00:25:58,960 --> 00:26:03,920
without potentially endangering people. So content moderation is huge. If you work at a large

243
00:26:03,920 --> 00:26:08,000
company doing content moderation or if you want to take one of the existing open data sets,

244
00:26:08,800 --> 00:26:14,560
that's really important. So obviously there's a big NLP component to it, but also computer vision.

245
00:26:15,200 --> 00:26:20,960
So for example, a lot of even the bigger companies right now can't do matching on a fake news image

246
00:26:20,960 --> 00:26:26,160
if there's crops slightly differently or it's like in, you know, on below format, etc.

247
00:26:26,160 --> 00:26:30,800
So there's some low hanging fruit there that already exists in open data sets that any research

248
00:26:30,800 --> 00:26:38,080
would help us. Unfortunately, during any disaster, while crime goes down in general, some particularly

249
00:26:38,080 --> 00:26:46,160
nasty people ramp up in scams and exploitation, especially exploitation of miners. So again,

250
00:26:46,160 --> 00:26:50,720
if you're working in content moderation or content health, when you're able to identify scams,

251
00:26:50,720 --> 00:26:55,920
artists or people looking to exploit children, that's incredibly important right now. And again,

252
00:26:55,920 --> 00:27:03,520
this will exist in initiatives for that. Any work in low resource languages is really valuable

253
00:27:03,520 --> 00:27:11,760
right now. So the majority of course that I've had with organizations like WHO have been especially

254
00:27:11,760 --> 00:27:16,400
worried about how to get information out, how to start using natural language processing in low

255
00:27:16,400 --> 00:27:21,680
resource languages, they're not captured in any of the large libraries that we have. Some of the

256
00:27:21,680 --> 00:27:26,160
existing open source data sets that I've helped create for disaster response have low resource

257
00:27:26,160 --> 00:27:33,600
languages or any research into those would be valuable. You see them in AI for all, some Stanford

258
00:27:33,600 --> 00:27:39,920
MIT classes have used them. Udacity is nano-degree in data science uses a set that I created too.

259
00:27:40,640 --> 00:27:46,160
Any of that research would be valuable. And then it's just like a ton of things around the edges.

260
00:27:46,160 --> 00:27:50,960
Like cleaning data is always valuable. I mean, if you turn up to volunteer at your local hospital,

261
00:27:51,600 --> 00:27:54,960
you're not going to be invited into do surgery, you might be given a mop and bucket.

262
00:27:56,400 --> 00:27:58,480
Expect the same thing. If you turn up to...

263
00:27:58,480 --> 00:27:59,520
Technology.

264
00:27:59,520 --> 00:28:03,760
Yeah, so if you turn up to a big organization, they're not going to be sitting there with a

265
00:28:03,760 --> 00:28:08,560
well structured data set ready to go and they just need 10 lines of high torch. But data cleaning

266
00:28:08,560 --> 00:28:14,000
is really important. Scentitation in hospitals is equally as important, especially at the moment.

267
00:28:14,000 --> 00:28:19,840
And so yeah, like that's how I started out for years to do an all the grunt work in disaster

268
00:28:19,840 --> 00:28:28,800
response. And it's a very noble, valuable way to start. The last example, anything around

269
00:28:29,680 --> 00:28:36,080
supply and logistics. So there is no world database of who has what kinds of medical supplies.

270
00:28:36,080 --> 00:28:40,720
And a lot of the world governments are now implementing really basic crawlers and machine learning

271
00:28:40,720 --> 00:28:46,000
systems to work out who has a supply of medical masks or respirators and things like this.

272
00:28:46,560 --> 00:28:52,240
And in almost any disaster response, the supplies and logistics component of it ends up being

273
00:28:52,240 --> 00:28:58,160
the biggest part of it, the most complicated part. And there's a ton of machine learning in supply

274
00:28:59,040 --> 00:29:03,680
supply chains and logistics. But not enough because it hasn't been a popular area for people to

275
00:29:03,680 --> 00:29:08,320
research. And again, like any research in this area would be really valuable for those of us in

276
00:29:08,320 --> 00:29:16,560
disaster response. Anthony Garland asks, what's the role of IOT devices in this pandemic? He's

277
00:29:16,560 --> 00:29:22,880
speaking specifically about personal IOT devices and health trackers like Fitbit, but GGM wondering,

278
00:29:23,760 --> 00:29:30,880
you know, either personal or kind of within hospitals and health care systems or other IOT use

279
00:29:30,880 --> 00:29:37,920
cases are there. Interesting opportunities for data scientists that arise in the intersection

280
00:29:37,920 --> 00:29:47,040
of COVID and IOT? Right. I mean, to ask that to it, right, in terms of personal IOT device,

281
00:29:47,040 --> 00:29:52,640
right? One is there's a lot of interest around better understanding more social policy,

282
00:29:52,640 --> 00:29:58,800
social distancing adherence. The people actually following the orders or guidelines that

283
00:29:58,800 --> 00:30:04,560
is local government. So the personal IOT device has rich rich data to support that.

284
00:30:04,560 --> 00:30:10,960
But of course, we've got to be vigilant with the security and privacy. But we do, we are seeing,

285
00:30:10,960 --> 00:30:17,520
there's a strong correlation to adherence to some other disease curves, right? So I think that's

286
00:30:17,520 --> 00:30:24,880
an area that will be of interest. And then the area will be to help support some of the symptom

287
00:30:24,880 --> 00:30:30,800
checks, right? As opposed to trying to get on a phone call and talk to someone and understand

288
00:30:30,800 --> 00:30:35,040
this sometimes and putting it in the CDC website, there's a lot of opportunities in leveraging

289
00:30:35,040 --> 00:30:41,440
the data that is being connected in this personal fitness devices to help supplement that.

290
00:30:42,560 --> 00:30:47,120
I think about, you know, the fever monitoring devices,

291
00:30:48,400 --> 00:30:54,320
think about, especially for one of the highest good is folks with hypertension and diabetes,

292
00:30:54,320 --> 00:30:59,040
but about those IOT devices, how they can be incorporated to help detect people at

293
00:30:59,040 --> 00:31:05,920
high risk. And trace together is another one where they're looking at the connectivity is

294
00:31:05,920 --> 00:31:14,400
who's exposed to whom. And I'm going to bring it in a privacy way that secures people's privacy,

295
00:31:14,400 --> 00:31:17,760
but alerts them if they've been exposed potentially, if people who've been diagnosed.

296
00:31:18,800 --> 00:31:26,800
Yeah, yeah, that's a great one. In fact, I just had a ethics conversation,

297
00:31:26,800 --> 00:31:31,600
internally, and I'd be on how to have a handle of that. But you may have seen in the latest

298
00:31:31,600 --> 00:31:37,680
White House guidance, there's a point to as employer to support the employees' content resources.

299
00:31:37,680 --> 00:31:43,760
So if you and I have gone to work, and in the next stage, we should be alerted by employers,

300
00:31:43,760 --> 00:31:50,160
someone else on my floor has been tested possible. So how do we do that while protecting the

301
00:31:50,160 --> 00:31:58,480
individual privacy and the instability? So Rob, you took on that question of how can data

302
00:31:58,480 --> 00:32:06,080
scientists help from a use case perspective? Rex, maybe you can take it on from a practices

303
00:32:06,080 --> 00:32:15,440
perspective. Sure. Well, there's two big ways that at least our shop is trying to contribute,

304
00:32:15,440 --> 00:32:20,320
and I think are low-hanging fruit and opportunities for other people in this space.

305
00:32:20,320 --> 00:32:26,240
The first is in the aggregation of existing data, not just cleaning, but munging different

306
00:32:26,240 --> 00:32:35,040
data sources together creates a lot of problems. And my shop day-to-day, we're a knowledge graph

307
00:32:35,040 --> 00:32:40,560
shop that works on aggregating conflict panel data or political science data measured across

308
00:32:40,560 --> 00:32:46,720
countries or localities. And so when COVID counts started coming out of confirmed deaths,

309
00:32:46,720 --> 00:32:52,240
number of tests, it looks just like the data we are working with already. And so we've rapidly

310
00:32:52,240 --> 00:32:58,080
been prototyping a system to aggregate all of the COVID data globally at at least eight province

311
00:32:58,080 --> 00:33:03,360
or state level and preferably below. So we're up to about 300,000 observations, a couple dozen

312
00:33:03,360 --> 00:33:09,360
data sets, and doing that kind of work is something I think any data science shop can contribute to.

313
00:33:09,360 --> 00:33:15,600
The other one is evaluation of models and predictions. If there's anything ML people know how to do,

314
00:33:15,600 --> 00:33:21,840
it's how to create a test set and take a split. And I just want to do a shout out from one project,

315
00:33:21,840 --> 00:33:29,120
I know of Nicholas Reich's lab at UMass Amherst, has been collecting the predictions and forecasts

316
00:33:29,120 --> 00:33:34,400
from different groups, normalizing them, putting them into the same sheet, and then seeing how they

317
00:33:34,400 --> 00:33:40,080
perform over time and even creating an ensemble prediction based on all of them. And that is something

318
00:33:40,080 --> 00:33:45,200
we can immediately start trying to do is evaluate and combine the forecasts we have already. And the

319
00:33:45,200 --> 00:33:49,680
faster we do that, the more informed decisions we'll be able to make in the next couple months

320
00:33:49,680 --> 00:33:58,720
about reopening in the US or other places. Any other takes on that? Things that data scientists need

321
00:33:58,720 --> 00:34:05,440
to be thinking about when they're considering jumping in and helping out?

322
00:34:07,280 --> 00:34:11,280
Listening to the panelists, right, two things come to mind. I think we're out your earlier point

323
00:34:11,280 --> 00:34:21,280
about NLP in addition to helping with clinical and genetic discovery, looking at various literature,

324
00:34:21,280 --> 00:34:26,880
that there is a strong desire to better understand local government's policy. It's changing on a

325
00:34:26,880 --> 00:34:33,360
daily basis. One of us, you know, to deal with bus models, we had to look at cost geography

326
00:34:33,360 --> 00:34:38,800
while really being careful about local differences. So if you could spend more time

327
00:34:40,000 --> 00:34:45,040
enabling policy insights to the NLP, it could be very valuable.

328
00:34:45,040 --> 00:34:56,640
Oh, go ahead. Go ahead. GovLab did kind of a rapid assessment of different topic areas that

329
00:34:56,640 --> 00:35:05,680
could be looked at and proposed at least 12 areas, going beyond tracking disease spread and supply

330
00:35:05,680 --> 00:35:11,360
chain of the PPE to protecting human rights and promoting accountability, you know, oversight of

331
00:35:11,360 --> 00:35:17,760
governments and their actions with COVID, alleviating pandemic related and unemployment and poverty,

332
00:35:18,480 --> 00:35:24,560
education, upskilling, identifying which businesses may be at most risk and helping inform

333
00:35:24,560 --> 00:35:30,720
policymakers of that as they make their decisions. And as we talk together in the pre-planning

334
00:35:30,720 --> 00:35:32,960
meeting, identifying and tracking misinformation.

335
00:35:32,960 --> 00:35:46,000
So misinformation is, you know, didn't come up in specifically in talking about some of the

336
00:35:46,000 --> 00:35:52,560
potential harms, but it's kind of the backdrop of a lot of the, you know, angst that comes up in

337
00:35:52,560 --> 00:35:59,920
this conversation. You know, any specific examples of misinformation or disinformation relating

338
00:35:59,920 --> 00:36:10,240
to COVID that any of you have seen? Yeah, I've seen a lot related to, I guess, basically you could

339
00:36:10,240 --> 00:36:16,160
you could call folk remedies and that's in every country in the world. So whether it's something

340
00:36:16,160 --> 00:36:24,560
of no known benefits like COVID-opathy here in the United States or other local remedies in other

341
00:36:24,560 --> 00:36:31,440
parts of the world, this happens in every disaster and every disease outbreak. It can be very

342
00:36:31,440 --> 00:36:37,040
dangerous when that misinformation relates to poisonous things. So as soon as we introduce

343
00:36:37,040 --> 00:36:41,040
something like bleach as a cleaning agent and then in disaster, we know some percent of people

344
00:36:41,040 --> 00:36:46,640
are going to drink it. As what happened with, I think it was chloroquine or certainly some other

345
00:36:46,640 --> 00:36:53,920
quinine derivative here where people drank it who didn't even absorb symptoms and then people died.

346
00:36:55,680 --> 00:37:01,520
So yeah, this is, you know, off the back of well-meaning people, people who think there are solutions

347
00:37:01,520 --> 00:37:07,760
or there are types of protective measures which they believe are way better or way worse than they

348
00:37:07,760 --> 00:37:15,120
truly are. And this becomes the biggest worry for us when celebrities get behind it because no

349
00:37:15,120 --> 00:37:20,160
public health organization can compete, you know, with the Hollywood celebrity with tens of

350
00:37:20,160 --> 00:37:27,120
millions of followers on social media. So it's really tough for us when a lot of these celebrities

351
00:37:28,080 --> 00:37:35,760
will double down, you know, maybe on remedies that they have a financial stake in, like that

352
00:37:35,760 --> 00:37:43,520
stupid good product. And so that's, we see a lot of the danger because, you know, we can't

353
00:37:43,520 --> 00:37:48,400
compete with those kinds of people in social media and a lot of them are being kicked off in social

354
00:37:48,400 --> 00:37:53,360
media, YouTube and Facebook especially have been great at this. But it's always trailing. The

355
00:37:53,360 --> 00:37:58,800
damage has often been done by the time this content gets removed. I think they also thought that 5G

356
00:38:00,800 --> 00:38:07,040
causing coronavirus so people were burning down the 5G towers. There's an organization news guard

357
00:38:07,040 --> 00:38:12,080
which has a misinformation tracker around COVID and I think the credibility coalition and the

358
00:38:12,080 --> 00:38:18,560
missing folk on folks are also have efforts on this regard. Yeah, so I think as a data scientist,

359
00:38:18,560 --> 00:38:22,560
this is one area where you can use your critical judgment. Similar to what we're actually saying

360
00:38:22,560 --> 00:38:27,040
earlier, you know what a good report, what versus a bad report looks like. You should be able to

361
00:38:27,040 --> 00:38:32,960
evaluate objectively whether a public official has experience in this area, whether they're

362
00:38:32,960 --> 00:38:37,280
calling on peer-reviewed articles when they present their evidence versus someone who's

363
00:38:37,280 --> 00:38:42,560
share in their end of total experience. And as the data scientist and your family, you probably

364
00:38:42,560 --> 00:38:48,000
have a better critical thinking capacity than most other people. And so I hope in your family

365
00:38:48,720 --> 00:38:54,160
become better critical thinkers and how to evaluate any advice skeptically and choose the best

366
00:38:54,160 --> 00:38:58,560
advice when there's conflicting advice. That's a really valuable thing that you can do in your

367
00:38:58,560 --> 00:39:04,240
community. There's also a big need for data scientists in this research space. So when I ran the

368
00:39:04,240 --> 00:39:08,640
big one of the big data innovation hubs for the National Science Foundation, we had a community

369
00:39:08,640 --> 00:39:15,440
of a hundred researchers, data scientists, digital anthropologists, political scientists, all

370
00:39:16,080 --> 00:39:21,760
bringing to bear their different disciplines to understand, track and develop mitigation strategies

371
00:39:21,760 --> 00:39:30,800
for misinformation. You know, I would like to comment too. Sometimes the information is true,

372
00:39:30,800 --> 00:39:35,440
but it's not actionable to your situation. I think that's the piece that I tend to see often.

373
00:39:36,480 --> 00:39:42,080
For instance, we hear how a lot of local hospitals are having resource storage,

374
00:39:43,040 --> 00:39:50,160
patients that we turn away, or that's in a call to ask them on critical or elected surgeries to

375
00:39:50,160 --> 00:39:57,760
take a pause. And it's very, it really depends on the local health system. And we're seeing that

376
00:39:57,760 --> 00:40:03,280
there are a few conditions that shouldn't have gone down, are going down in areas. Like there,

377
00:40:03,280 --> 00:40:09,520
less heart attack, there's less asthma attack, and there's less trauma cases. And in some sense,

378
00:40:09,520 --> 00:40:12,800
it could make sense because people are not traveling as much, but the rate of the increase is a

379
00:40:12,800 --> 00:40:18,400
little alarming. So hospital resource storage does apply to your local area. I think that's,

380
00:40:18,400 --> 00:40:23,360
you know, we've got to help our community to think through that. It's true. If I have someone

381
00:40:23,360 --> 00:40:30,880
city, it's a stature for the way I live, and that's one of the things. We started off talking about

382
00:40:31,920 --> 00:40:38,880
examples of the potential harms, but what about specific examples of where data science is

383
00:40:38,880 --> 00:40:47,040
helping out and making an impact? Leah, you had a few. There's like, there are lots of different

384
00:40:47,040 --> 00:40:52,800
opportunities, I think, for data scientists and other data-related folks to plug in,

385
00:40:52,800 --> 00:40:58,000
in very positive ways. I think in our discussions, we talk about, you not only need the data

386
00:40:58,000 --> 00:41:02,480
science at the table, but you need the epidemiologists, the virologists, immunologists,

387
00:41:03,360 --> 00:41:07,920
economists, and other kinds of professionals. So there's several opportunities where they're

388
00:41:07,920 --> 00:41:14,400
bringing these kind of teams of folks together. MIT COVID challenges one where they're looking at

389
00:41:14,400 --> 00:41:22,160
innovating, using data and other approaches for solutions in Africa. The DevPost Global Hackathon

390
00:41:22,160 --> 00:41:26,880
is another place. We'll put provide those links afterwards. They've got to think something like

391
00:41:26,880 --> 00:41:34,400
18,000 people signed up to contribute. The US Digital Response Team, which is started by some

392
00:41:34,400 --> 00:41:40,880
former White House deputy chief technology officers, has now got over 4,000 volunteers,

393
00:41:40,880 --> 00:41:47,760
and they're pairing those data science back end engineers, product managers with local government

394
00:41:47,760 --> 00:41:54,400
folks to help build websites, things that are free, open source, and extensible or replicable

395
00:41:54,400 --> 00:42:01,040
to other locations. And then in the citizen science space, we've got Folding at Home, which is

396
00:42:01,040 --> 00:42:07,760
using distributed computing. So if you want to, like, study at home, donate your computing resources

397
00:42:07,760 --> 00:42:13,680
to help solve some of the medical challenges around COVID, there's the coronavirus binder

398
00:42:13,680 --> 00:42:20,560
design game, which is developed by Foldit. So we've got a serious gamers figuring out how a protein

399
00:42:20,560 --> 00:42:25,920
related to coronavirus works, and figuring out how they might disrupt that protein.

400
00:42:26,560 --> 00:42:31,840
Eternet is another one of these online series games that requires some complex problem-solving

401
00:42:31,840 --> 00:42:36,880
that having a data scientist tell probably would be helpful. And then, you know, as I think Gigi

402
00:42:36,880 --> 00:42:42,720
mentioned, there's not just COVID we have to worry about, but now we've got some fabulous air

403
00:42:42,720 --> 00:42:48,080
quality, right? So doing things like monitoring air quality, doing things like monitoring water

404
00:42:48,080 --> 00:42:53,520
quality, it's Earth Day, and how is this impact of COVID on our environment? There's a big

405
00:42:53,520 --> 00:42:59,520
opportunity for folks with IoT skills, data science skills, Earth observation, geospatial mapping

406
00:42:59,520 --> 00:43:09,360
to help with those efforts too. Other examples of where data scientists, non-epidemiologists,

407
00:43:09,360 --> 00:43:16,560
virologists are jumping in and adding value. So no one's mentioned kind of the efforts from

408
00:43:16,560 --> 00:43:22,480
journalism, and a lot of newspapers have data scientists on staff now, so I just wanted to

409
00:43:22,480 --> 00:43:29,040
point out that the main source of tracking for testing in the US is the COVID tracking project,

410
00:43:29,040 --> 00:43:37,040
which came out of an effort by the Atlantic and two journalists in India. COVID-19 India is a

411
00:43:37,040 --> 00:43:44,640
massive crowdsourced effort. If you go on GitHub, you can go country name by country name and find

412
00:43:44,640 --> 00:43:52,240
at least one data scientist or beginner who has started scraping the local health ministry website

413
00:43:52,240 --> 00:43:58,240
and getting unstructured data into a structured form that we can then use. I think one of the

414
00:43:58,240 --> 00:44:06,960
one of the main comparisons here is not between epidemiologist and expert scientific experts

415
00:44:06,960 --> 00:44:14,480
and civilians or data scientists or amateurs. It's between government resources and capabilities

416
00:44:15,040 --> 00:44:21,440
and the civilian capital, social capital to work on that. So if the New York Times is the one

417
00:44:21,440 --> 00:44:26,880
who's putting together county level counts of COVID in the US and not the federal government,

418
00:44:26,880 --> 00:44:32,560
that tells you something. New York Times and Rooters are both tracking that at the county level

419
00:44:32,560 --> 00:44:37,680
because there isn't a strong federal system with reporting all the way up the chain and all the

420
00:44:37,680 --> 00:44:43,520
way down the chain. It's newspapers collating this information for us. And so we have to step in

421
00:44:43,520 --> 00:44:48,320
where the institutions fall short and that can only be done by regular people.

422
00:44:48,320 --> 00:44:59,760
David Clement asked another really interesting question about how the role of data science shifts

423
00:44:59,760 --> 00:45:07,760
as this pandemic continues. Early efforts have been involved in modeling and dashboards and the

424
00:45:07,760 --> 00:45:13,840
like. But what does the role of data science need to evolve to as time goes on? Any thoughts on that?

425
00:45:13,840 --> 00:45:18,880
Yeah, I can I can speak to some part of it. I certainly can't speak for for all the data

426
00:45:18,880 --> 00:45:25,680
science and how it can respond. One of the most constant things I'm seeing working with really

427
00:45:25,680 --> 00:45:33,360
large organizations doing public health and misinformation is the areas where we can get help

428
00:45:33,360 --> 00:45:38,400
from data scientists. So almost all the systems I'm looking at don't have much data to begin with

429
00:45:38,400 --> 00:45:43,600
and they need to bootstrap from not many examples. So human in the loop systems, smart active

430
00:45:43,600 --> 00:45:48,560
learning, adapting to low resource languages with with inconsistent spellings and more complicated

431
00:45:48,560 --> 00:45:54,160
suffixes and prefixes. Lots of really important problems that just aren't part of what most

432
00:45:54,160 --> 00:45:59,600
data scientists learn. Most data scientists will learn about purely automated methods where the

433
00:45:59,600 --> 00:46:06,240
data is constant and the data is never constant in in disaster response. So a lot of the things

434
00:46:06,240 --> 00:46:12,400
that might look useful for multilingual NLP like zero-shot learning really aren't that useful.

435
00:46:12,400 --> 00:46:17,680
Zero-shot learning requires fairly large unsupervised, unchanging corpus and the percentage difference

436
00:46:17,680 --> 00:46:24,080
doesn't doesn't change a whole lot. What we need more is people who can think about the productization

437
00:46:25,280 --> 00:46:33,440
human in the loop systems where it's the human either expert or non expert for example giving

438
00:46:33,440 --> 00:46:38,560
medical advice who can look up a database or maybe the right answers and give that advice.

439
00:46:38,560 --> 00:46:43,280
And so this is one of the areas that I've been given advice to. So a lot of major companies working

440
00:46:43,280 --> 00:46:47,680
in this area have been introduced to a lot of data scientists. So data scientists that all the big

441
00:46:47,680 --> 00:46:51,840
tech companies are willing to step up and do the right work. But they're going to introduce to

442
00:46:51,840 --> 00:46:56,880
the wrong people. They get introduced to someone who can do a cutting edge paper in zero-shot or

443
00:46:56,880 --> 00:47:01,920
one-shot learning. Whereas actually the data scientists who probably have the best knowledge,

444
00:47:01,920 --> 00:47:10,080
they work in content moderation or they help out with customer success for customer service systems.

445
00:47:10,720 --> 00:47:15,280
So I think a lot of that human computer interaction side of data science which I find really

446
00:47:15,280 --> 00:47:21,680
fascinated. I think that becomes way more useful in a disaster response environment and

447
00:47:21,680 --> 00:47:28,400
I would love to see more people picking that up. Gigi, I think you had something to add to this one.

448
00:47:28,400 --> 00:47:39,680
Yeah, I was smiling because let's on the technology aspect. But as we think through the use cases,

449
00:47:39,680 --> 00:47:43,920
as many countries are trying to figure out how to recover and get back to the new normal,

450
00:47:44,560 --> 00:47:48,880
I think we have to think through like care delivery will not be the same anymore.

451
00:47:49,680 --> 00:47:53,840
Like half of the care today in America is delivered to a telemedicine and I think that's

452
00:47:53,840 --> 00:48:00,560
statistically even higher than that. So what can we do for the scientists to help improve

453
00:48:02,640 --> 00:48:10,080
not only how monitoring but the experience between the clinician and the patient as they are

454
00:48:10,080 --> 00:48:15,360
more longer able to physically touch right. So the signs of haptic skin tell me, the signs of

455
00:48:16,960 --> 00:48:22,560
how the share and transmit monitoring data all that comes in. And another

456
00:48:22,560 --> 00:48:29,040
we have to allow us whereas as a teacher, as an educator, we will be delivering our education

457
00:48:29,040 --> 00:48:35,520
material in a very different way. How can we make them more interactive and how can we learn from

458
00:48:36,480 --> 00:48:40,160
and imagine the data that we are collecting these online systems. How can we learn from that

459
00:48:40,160 --> 00:48:47,200
and really provide it in much more rich experience for our students. So my mind actually went

460
00:48:47,200 --> 00:48:53,120
in a way of, okay, in a new normal, not only in health, but in many many disciplines,

461
00:48:53,120 --> 00:48:58,000
on their normal opportunities for us. The idea of the new normal is an interesting one. I saw

462
00:48:58,000 --> 00:49:05,600
an article the other day that talked about how in Milan, they're taking advantage of the change in

463
00:49:06,800 --> 00:49:12,720
the traffic situation in the core city to implement some pro cycling measures. So they're

464
00:49:12,720 --> 00:49:18,720
going to be producing the vehicular traffic in the city core and adding bike lanes and things like

465
00:49:18,720 --> 00:49:27,120
that. And you know, the new normal is going to be new and it will provide a lot of opportunities

466
00:49:27,120 --> 00:49:35,280
for folks to jump in and help with the kind of modeling that they've done in non epidemiology domains.

467
00:49:35,280 --> 00:49:48,160
So we've got a number of questions that have come in that relate to the kind of data that

468
00:49:49,120 --> 00:50:00,160
we're seeing being published by media and experts talking about. I think a lot of these questions

469
00:50:00,160 --> 00:50:07,920
are getting at kind of the, you know, a couple of things, a lack of confidence on the part of

470
00:50:07,920 --> 00:50:15,200
the information consumer as in terms of what media we can trust as well as the rapidity with which

471
00:50:15,200 --> 00:50:23,360
viewpoints are changing. We've got, you know, folks showing data on different sides of, you know,

472
00:50:23,360 --> 00:50:31,440
the utility of masks and the position that local governments and organizations like WHO CDC

473
00:50:31,440 --> 00:50:41,440
is shifting over time. We've got similar types of, you know, data on different sides of social

474
00:50:41,440 --> 00:50:51,840
distancing. Any thoughts on like cutting through the, you know, cutting through these differing

475
00:50:51,840 --> 00:50:56,560
perspectives and more importantly, how do data scientists help their communities cut through

476
00:50:56,560 --> 00:51:05,520
these differing perspectives? Well, I'll say be very wary of the media for one, you know, most

477
00:51:05,520 --> 00:51:10,320
journalists are great people and they want to report things honestly, but media companies are not

478
00:51:10,320 --> 00:51:18,560
aligned with providing you the most important information that you need. A lot of media companies

479
00:51:18,560 --> 00:51:22,800
have the same narratives in every disaster and it's that the responders aren't doing the right

480
00:51:22,800 --> 00:51:28,880
job. Why is this being ignored? Why did somebody cover this up? Why is there a conspiracy here?

481
00:51:29,440 --> 00:51:34,880
And even when journalists know that they, it's not a real story, it's part of the narrative

482
00:51:34,880 --> 00:51:41,440
they have to tell. And you can be smart about this. If you see stories come out and there's no author

483
00:51:43,120 --> 00:51:47,840
in a big publication that they normally know that they're spreading misinformation or they get

484
00:51:47,840 --> 00:51:52,320
an invited expert opinion who is no expert at all, I'd just say something outrageous.

485
00:51:53,520 --> 00:51:58,800
But it can be, yeah, it can be tough. I mean, I haven't said that like, I mean, like Rex said,

486
00:51:58,800 --> 00:52:02,560
like some of the most useful sources of information have come from

487
00:52:03,120 --> 00:52:08,240
den scientists and journalists working together. So yeah, I'll flag that there's a lot of variety,

488
00:52:09,360 --> 00:52:12,960
but yeah, the media is tough to trust a lot of the time.

489
00:52:12,960 --> 00:52:19,200
So you mentioned working with local communities. I had mentioned the US Digital Response,

490
00:52:19,200 --> 00:52:23,200
which is working with local government, but there are other examples like the Data Science,

491
00:52:23,200 --> 00:52:30,000
Federation, and Southern California. That was started by Gene Holm, who's in the CIO's office

492
00:52:30,000 --> 00:52:36,320
at the City of Los Angeles, and they've partnered with 18 universities and organizations in the

493
00:52:36,320 --> 00:52:44,960
area. So the students and faculty and staff are working closely with the City. Students and faculty

494
00:52:44,960 --> 00:52:51,840
and the other volunteers bring the Data Science expertise. The City brings the problem space,

495
00:52:51,840 --> 00:52:57,520
the use cases where they need that help, and that's kind of a local on the ground use case.

496
00:52:59,520 --> 00:53:03,760
You know, I was listening to you, Rob, and I think what do I do? What do I advise myself

497
00:53:03,760 --> 00:53:09,040
in my choosing to do when you're exposed to this different news channel and information? I think

498
00:53:09,040 --> 00:53:13,200
the first thing I go to, of course, the author and the citation. I think the second thing I'll

499
00:53:13,200 --> 00:53:21,040
advise you to pay attention to is the metric. You can very quickly tell the thoughtfulness of the

500
00:53:21,040 --> 00:53:25,440
numbers behind the scene in how do we put the metric, right? It's just a number of cases, so a number

501
00:53:25,440 --> 00:53:31,040
of cases per capita. That is a basic, for example, but many ways you can ask you if you focus on

502
00:53:31,040 --> 00:53:37,920
the metrics that's being used and being reported, and you ask yourself, this is translatable.

503
00:53:37,920 --> 00:53:44,160
I think that is a good indicator for me. A corollary is to look at the why

504
00:53:44,160 --> 00:53:49,280
access of the graph and whether the numbers are jumping around like some of the ones that we've

505
00:53:49,280 --> 00:53:56,240
seen going around. There's been some interesting variations on log scales. I thought that was

506
00:53:56,240 --> 00:54:04,160
complicated. It's a log scale and check the units, right? Things that you learn in elementary

507
00:54:04,160 --> 00:54:09,440
school, it's very, very relevant. Well, then there's even a scale. If there's not a scale in the

508
00:54:09,440 --> 00:54:16,000
first place, that's a really good indication. It's something going on. It's actually, if I could

509
00:54:16,000 --> 00:54:19,840
make one point, I think there's a piece that someone else could overlook their thoughts, right?

510
00:54:20,800 --> 00:54:24,880
Especially coming into a new field, what is the right metric to use? What is the right

511
00:54:24,880 --> 00:54:29,520
scaling path to use? We've got to be careful of that, and coach looks at what we work with.

512
00:54:30,560 --> 00:54:36,640
One quick point on this that I raised in my piece is that not just the media, but some of these

513
00:54:36,640 --> 00:54:44,000
political actors are producing speech that they know is wrong, or they're incurious about what the

514
00:54:44,000 --> 00:54:50,720
right answer is. They just have to produce a particular viewpoint. Their mistake here is to

515
00:54:50,720 --> 00:54:56,560
try to respond to that point for point. There's something called the Gish Gallup, which is a

516
00:54:58,160 --> 00:55:04,160
yeah, it's a debating tactic where you just spam bad arguments as quickly as you can, because

517
00:55:04,880 --> 00:55:09,760
the time it takes to explain why those are wrong is longer than it takes to make the argument

518
00:55:09,760 --> 00:55:17,040
in the first place. We have a risk here where certain media or certain political actors are

519
00:55:17,040 --> 00:55:24,640
producing false speech at a rate faster than we could possibly debunk it. And frankly, if you're

520
00:55:24,640 --> 00:55:28,640
an expert in these fields, you shouldn't be spending your time debunking this. You should be

521
00:55:28,640 --> 00:55:35,280
producing work and getting us to the right answer independently. So the only response that I found

522
00:55:35,280 --> 00:55:42,160
has either been through speech like comedy and satire, where the daily show used to take apart

523
00:55:42,160 --> 00:55:48,400
bad arguments for you in a very creative way quickly, or through positive education. We have to

524
00:55:48,400 --> 00:55:55,120
provide scientific literacy to our families and to our population so that they can tell a bad

525
00:55:55,120 --> 00:56:01,440
argument no matter which day or year or decade it's made in on what topic. That's getting it at

526
00:56:01,440 --> 00:56:07,040
the bud is the only way to protect them. It's like a vaccine against bullshit. And it's the only

527
00:56:07,040 --> 00:56:13,120
hope. Otherwise, you'll bankrupt yourself trying to go beat for beat with these people. On the satire

528
00:56:13,120 --> 00:56:20,320
front, there was an article that was responding to another article that purported that public

529
00:56:20,320 --> 00:56:28,560
transportation was one of the key spreaders of the virus in New York City, based on correlation

530
00:56:28,560 --> 00:56:34,880
between, you know, I forget what the specifics were when the city shut down and public transit usage

531
00:56:34,880 --> 00:56:43,680
or some random correlation and the satirists and author basically took the same argument and

532
00:56:43,680 --> 00:56:49,680
article and rewrote it with the correlation being to bike sharing using the bike sharing data.

533
00:56:50,400 --> 00:56:56,960
It's poke fun at the argument. We're coming up to the top of the hour. I want to take another

534
00:56:56,960 --> 00:57:04,880
couple of audience questions to help us close out. So Megan asks a question referring to

535
00:57:06,720 --> 00:57:14,560
COVID-19 data sets but brings up the broader issue of bias. And this relates to something that

536
00:57:14,560 --> 00:57:22,000
you open up with Gigi as well as the idea of transparency. Can you speak a little bit to the

537
00:57:22,000 --> 00:57:28,720
importance of bias and transparency and things that we need to be thinking about both generally and

538
00:57:28,720 --> 00:57:39,120
when dealing with these data sets? Yes, absolutely. Really being, it's okay, I call the gating

539
00:57:39,120 --> 00:57:45,040
requirement. It's first take a look at the completeness coverage and distribution of your data,

540
00:57:45,040 --> 00:57:48,640
right? And I think it's more than important especially when we're dealing with public

541
00:57:48,640 --> 00:57:55,440
health situation to consider the dimensions of not just demographic groups but different

542
00:57:56,800 --> 00:58:04,560
local diversity sets. So I think that's the piece that we're very passionate about.

543
00:58:04,560 --> 00:58:08,560
And secondarily, I'll go beyond just the data set. I think it goes back to my earlier point

544
00:58:08,560 --> 00:58:14,160
about being transparent on the assumptions that we are making. It's amazing, right? If you look at

545
00:58:14,160 --> 00:58:19,520
the epidemiology model, if you include as a atomic patient or if you don't include as a

546
00:58:19,520 --> 00:58:24,160
tonic patient, you could get two different times differences in results. If you don't mention that

547
00:58:24,160 --> 00:58:30,640
to your reader, then I think we're not doing our job. So transparency in data and transparency

548
00:58:30,640 --> 00:58:37,760
in assumptions. Any additional thoughts from our panel on bias and transparency or data sets?

549
00:58:37,760 --> 00:58:45,520
Stay tuned, we're working on a piece called how to be careful with COVID counts that's going to

550
00:58:45,520 --> 00:58:53,280
investigate that in question exactly. And if I could add one thing, right? It's interesting how

551
00:58:53,280 --> 00:59:00,160
we often report on cases and deaths as opposed to recoveries. It's also interesting how we don't

552
00:59:00,160 --> 00:59:05,840
report on testing availability and data in a driver of what we see in the results. So I think not

553
00:59:05,840 --> 00:59:14,880
that. I look forward to the article. Bernard asks, and this is a great question to help us wrap up,

554
00:59:14,880 --> 00:59:21,120
is there a responsible data science 101 out there that folks can go to and have a look at?

555
00:59:23,360 --> 00:59:28,800
I start with the article I wrote about five ways that data scientists can help and five mistakes

556
00:59:28,800 --> 00:59:35,920
to avoid. There's a lot of links in there. If you want to do deeper reading about things that

557
00:59:35,920 --> 00:59:42,480
worked and things that didn't work in the past too. And Rex's article as well. And in fact, we'll

558
00:59:42,480 --> 00:59:50,000
link to all of these articles and other resources that have been mentioned in this conversation and

559
00:59:50,000 --> 00:59:58,880
that our panelists have to share on the program page for this, which is at twomolei.com slash

560
00:59:59,600 --> 01:00:08,160
rdscovid. Matume asks about that term that you used Rex in the context of debating. We'll get

561
01:00:08,160 --> 01:00:15,760
that in there as well. But at this point, I'd like to thank all of our panel for participating in

562
01:00:15,760 --> 01:00:25,440
this great discussion. Thanks, everyone. Thanks for having us. Thanks for joining us.

