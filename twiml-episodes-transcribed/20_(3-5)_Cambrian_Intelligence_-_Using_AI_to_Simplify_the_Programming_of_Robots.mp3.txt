Hello everyone, I am back here at NYU Future Labs at the AI Nexus Accelerator speaking
with Mika Perra and Hamid Zaheri from Cambrian Intelligence.
All right guys, say hi.
Hi.
Great to be here.
Hi everyone.
So tell us about Cambrian Intelligence, what are you guys up to?
Yes, the Cambrian Intelligence, the word Cambrian comes from Cambrian Explosion that happened
some 60 million years ago.
So we are developing robot always made of AI modules.
And that can be used to do various tasks and solve to enable robotics in every industry
possible.
Okay.
And with regards to the analogy to the Cambrian Explosion, what's being exploded?
Like what have we seen proliferate here?
Is it the intelligence itself?
Is it robotics?
Is it?
Yeah, so we believe that sticking this narrow AI applications into modules, we can achieve
much, much more and we can more rapidly develop different applications for robotics.
So basically, what we are doing is making robots more intelligent.
And so maybe tell us a little bit about your personal backgrounds, how did you get to
where you are?
Yeah, sure.
So I have a background in aerospace and from shoot guard in embedded systems and robotics.
Okay.
And I moved to London, then there are MedMika, Mika has a background in cognitive science
and deep learning.
And there we figured that we both share passion for robotics and for artificial intelligence.
And we decided to start this company together.
Oh, nice.
And how long have you been doing it?
A little bit over all that year.
Okay.
If you've been at it for around a year, where are you at as a company?
Yeah.
So right now we are working for pilots for a large auto manufacturer and like to implement
the first application out of our operating system on industrial robots.
So we had started on developing that and we planned to release the platform more pro-dera
late this year.
What specific problem are you going after that an auto maker would have?
So right now robots are programmed point by point to accomplish different tasks.
The robot is blind.
You put some specific parts into some specific locations with the accuracy of sub-millimeter.
And the robot is programmed to just go there and pick it apart and place it some place.
It's already defined for a robot, but robot has no knowledge about the environment or the
task that it's trying to accomplish.
But we're doing, we're trying to make that robot more intelligent.
So the robot can have understanding of the environment and can accomplish the task by self-learning.
Then there won't be any need for programming the robot.
We're basically changing the abstraction level of robot programming from low level programming
of the robot to be decode to high level, just specifying the task and the specific location
and the initial location, the target location and the robot can figure out on its own how
to accomplish the task.
So I've seen a bunch of videos from a variety of different sources actually with showing
robots trying to do these pick and place types of operations and there are a number of challenges
including, you mentioned the millimeter resolution with what you need to program the robot.
What are some of the other challenges that you see folks having with the traditional approach
to programming these robots?
A beauty program in this robot is really time-consuming and it's not just the programming itself,
but the robot changes the behavior with environmental factors like even life and conditions or
temperature of the environment and so on can affect how the robot behaves.
So every time these conditions are changing they need to reprogram the robot.
So our approach is to add that level of intelligent to the robot that the robot can adapt to the
new changes of the environment and it basically can learn how to optimize for different tasks.
Current challenges, it includes localizing the part with the accuracy that we mentioned,
extracting the pose of the part, grasping the part from the right location and with the right timing
and so on. And these are the challenges we want to remove from the programming and let the robot
to learn.
And why the auto industry?
So other industry, currently 2.3 million industrial robots out there operating as we speak and 40
percent of those in car industry, automotive industry and kind of out of car industry,
about 70 percent operating the body shop which is where they make the chassis of the car.
So these are the pictures and videos you see with the car kind of going down the line
starting from the frame and then putting all the, and those are massive, massive robots that are
open. Yeah and they require like massive holes to operate on.
And it's a big, big task to configure like when you can imagine when a new car model comes in
with all the different variants and how many parts the car is made of. So you can imagine the
task of programming all this in which all robot excels down at the production line.
So it's massive task. Do you have a sense for how long that usually takes
for when a new, you know, once I've completed design on a new car to complete the,
you know, the programming of the line to get it to market?
Yeah so typically they practice the production like they have 100 cars, 200 cars and so on and
often even with the production run practice like you typically change the part in dimension of form
to fit in better, but it's like you're talking in days of like very, very hard ramp up production.
I've also seen where, you know, there are companies that are trying to,
there are companies that are trying to solve this robot programming problem by,
as opposed to, you know, the traditional programming approach basically have the robot like,
you know, shadow a human or you kind of move the robot the way you want the move to move the robot
to, you manually manipulate the robot in order to teach it. It doesn't sound like that's what
you guys are doing. That's part of our platform. Okay so tell us more about, you dig into the platform
a little bit and how, how a company would use it. So the basic idea of the platform is to enable
people to have natural interaction with the robot. If there are tasks that the robot can self-learn,
the robot can do it using a platform if there's a necessity for the human to have a simple
interaction with the robot to show some specific tasks. Okay. Still possible using our platform.
Basically we are trying to remove the need for the human to have expertise in the robotics
in order to interact with the robots. We want normal human without that kind of expertise to be
able to interact with the robot and be able to teach it simple tasks or even more communicate in
the future hopefully. So kind of like we see that all like teaching by demonstration or even like
teaching by scripting or like even voice commands like we see this like blend in in our platform
and some point in the future. So you can have any industrial, any robotic arm on your table and you
can ask it to grab your coffee cup and then it's able to do that. So that's that's where we are
aiming at. And now today the robots aren't even typically or to what degree do the robots
typically already have visual sensors or is that something that that has to be outfitted in
order for a company to use your approach? Yeah so we kind of for industrial robots in the kind
of so we're going to retrofit robot industry robots with a specific set of sensors so it can
sense our own it's environment. Okay. We're not trying anything to manufacture any specific
sensor we're trying to use off the shelf sensors and not really expensive ones. Okay. And
what kinds of sensors primarily visual or are there other types of sensors that go into?
We started by visual sensors but they're going to be a sense of fusion using different inputs
to enhance the performance. Okay. I had a conversation with someone actually from one of the
large auto manufacturers where they mentioned that one of the challenges that they were having in
this space was that the robotics manufacturers are increasingly trying to you know they see one
of the big they see the data that the robots are producing in terms of the sensors that they
already put on the data and and they're operating data as kind of proprietary and they're trying
to sell that now to the automakers and are you finding have you come across anything like that
where the the manufacturers are you know having issues automating because the robotics suppliers
are making it more difficult for them. Yeah of course like many especially big manufacturers
like they are very sensitive about the data like what they have inside the inside this factory
so they don't obviously want it to go away anywhere. What when you go when you're working with a client
like this large auto manufacturer what challenges do you run into as they seek to use the your product?
Yes obviously like with a for quite small company like us startup face and then
scaling up to a multiple factories around the world so that's like that's something like where
you need to ramp up your operation force and so on and so but we are not like we are iterating there
so in order to gain the trust of those big manufacturers they definitely need certain level of
safety and precision. That should be proven over time and they need to trust the product
and make sure it works before they can deploy manufacturing. So what's their experience with the
product when they you know you guys come in and get them set up with I imagine a small use case
to start to build that trust. What does it look like from their perspective?
Yes at the moment we are still so like our product is not yet in operation so yeah so it's going
there. What do you envision the user experience to be when you get a new customer onboard?
Yes so the user experience will be very simple so like for them like instead of now programming
the robot they can just upload the cat model of that specific part they want to be creeped and
then they specify the end location where it needs to go and so it's that simple.
And you mentioned so you as opposed to having to explicitly tell the robot you know give it
coordinates of a pick location you're giving them what instead or are you giving them coordinates
but just not the dimensions of the item or yeah yeah so the problem we are solving is not
them so typically that's where the part needs to go for example most of the operations are
welding these parts or gluing these parts together. So the end point is typically could be
pretty determined but where do you grasp like that part if you now they need to manufacture
custom manufacturer chicks so that they hold stack of parts very very accurately and with every
new part if you have new model or something like you need to custom manufacturer a new chick and
this is obviously a pick so we can just like stack paths arbitrarily and so they still leave the
creeping location is specified still the robot can figure out from exactly that location but
the parts doesn't have to be in that exact orientation for the robot in order to find given if
they have some randomness in their orientation the robot can still extract the pose of the object
and recognize that part and the creeping location and grasp from there. So there are two
two problems in there one is kind of the bucket of parts problem right so how do I know what
the orientation is but then there's also are you also enabling them to use general grippers as
opposed to custom made grippers for a given part or for the specific task of manufacturing they
usually have their own specific tools or groupers the systems to be integrated with that but in the
future the robot can learn to use different kind of tools. So maybe let's dive into the technology
a little bit you know what machine learning and AI techniques are you using to make all this work?
Sure so in the robot industry at the moment you don't have a kind of common operating system
that the robot can use every manufacturer to make their own custom programs it's really low level
programming we're trying to make it a bit more general something that imagined operating system
that has a low level interface that can interface different kind of robots and that are high level
modules like recognizing the parts grasping the parts or collision awareness or any intelligent
that you need in the robot behavior that are abstracted from the low level communication and they
can grow and get better over time and this makes our operating system. And so how would you characterize
where you are with the product so far? So far we've integrated with three different robot
manufacturers I can name it it's Kuka, Universal Robots and Stubli and we're implementing some
basic AI modules but later we're going to open up this platform for people to add more skills
to the robot and expand the platform over time. So what are some examples of the AI modules
that you've implemented? So right now we are working on the very accurate localization so that's
the key value adding module in our product at the moment and then we've added our recognition
and then simple robot controllers. So what goes into localization? What are the challenges there
and how are you attacking those? Yeah so basic is kind of like where the uncertainty comes from
is like what you can receive the resolution from your sensors and then kind of how you can fit
that into your model and kind of get accurate enough prediction over time and like over multiple
samples like and what do you do and about like going back to the modules like you can think
different kind of modules like you can think of like safety modules where you can detect people
if people somehow enter this cell where the robot is operating so you can have these sorts of
all you can have force feedback where you can recognize different forces when you're installing
some components and so on and so are you are you building everything from the ground up from scratch
or are you using various open source libraries for the analytics parts? It's a combination of both
we need some certain tools we cannot develop everything from scratch but there is a certain level
of modification that we need to make to the existing tools to make them usable for that precision
required for manufacturing and that's the work that we need to accomplish. Anything that we might
know in terms of tools are you running on top of yeah on the deep learning side we use TensorFlow
so that's great yeah we also use on the robotic side kind of like low level commands we use
ROS so in the robotic research there's this platform called ROS a robot operating system and it has
kind of distributed architecture no it can run in parallel and they're coming to come and get
into the master mode and so on we're using the idea the whole platform like what has been developed
for research is not suitable for manufacturing in terms of safety and so on but the idea is
great so we're trying to adapt those ideas but implement our own version that makes it suitable
for precision required for manufacturing. Okay that was great great anything else you'd like to
share about what you guys are up to? Yeah we'll share like in the bit later this year just follow us
on Twitter and check out website and you'll hear more about us. Okay awesome how can folks what is
your Twitter and working folks find you online? Yeah it's called Keynes or C4-1 NT. C4-1 NT. Okay and
that's the Twitter or is that the website? Yeah and the website is C-A-I-N-T that's O-O
okay got it awesome well thanks so much for being on the show it's great to hear about what you
guys are up to. Thanks for having me. Yeah thanks for having us.
