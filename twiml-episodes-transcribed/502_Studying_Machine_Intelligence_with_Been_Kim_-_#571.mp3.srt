1
00:00:00,000 --> 00:00:10,560
All right, everyone. Welcome to another episode of the Twomo AI podcast. I'm your host, Sam

2
00:00:10,560 --> 00:00:17,200
Charrington, and today I'm joined by Bean Kim, a staff research scientist at Google Brain,

3
00:00:17,200 --> 00:00:23,600
and an ICLR 2022 inviting speaker. Before we get into today's conversation, though, I encourage

4
00:00:23,600 --> 00:00:29,040
you to head over to Apple Podcasts or your listening platform of choice and leave us a five-star

5
00:00:29,040 --> 00:00:35,040
rating and review if you enjoy the show. Bean, welcome to the podcast. Thank you, Sam. Thanks for

6
00:00:35,040 --> 00:00:41,680
having me. It's honor to be here. I'm really looking forward to our conversation, and I would love to

7
00:00:41,680 --> 00:00:46,640
have you start by introducing yourself to our audience, sharing a bit about your background,

8
00:00:46,640 --> 00:00:53,440
and letting us know how you came into the field of machine learning. Yeah, sure. I'm a staff research

9
00:00:53,440 --> 00:01:01,040
scientist at Brain. I grew up in South Korea. I actually did, I majored in mechanical engineering

10
00:01:01,040 --> 00:01:07,200
back in the day, until I came to MIT for grad school. I made some robots initially, then until I

11
00:01:07,200 --> 00:01:14,960
realized robots are so cool, however, limited by physical, like just the time that you have to

12
00:01:14,960 --> 00:01:20,880
spend to make the robot. And I thought, you know what, if I just remove the physical aspect of

13
00:01:20,880 --> 00:01:25,680
their research and just go with the brain, I can go faster. I can do many things at the same time,

14
00:01:25,680 --> 00:01:31,600
computers can do things while I'm sleeping. So I switched to machine learning and did a PhD

15
00:01:31,600 --> 00:01:37,120
from MIT, then I went through some other jobs and landed in brain. And so tell us a little bit

16
00:01:37,120 --> 00:01:43,280
about your current research focus. What are the things that you enjoy thinking about? Yeah, so for

17
00:01:43,280 --> 00:01:52,480
many years, I've been singularly focused on interpretability, which is kind of an umbrella term

18
00:01:52,480 --> 00:01:58,880
for things starting from how do you understand the data distribution that you're about to train

19
00:01:58,880 --> 00:02:06,320
your model on to building inherently interpretable models. You design model from scratch, embed rules

20
00:02:06,320 --> 00:02:11,600
or logic to make sure that you can understand when the model is trained. And although it to

21
00:02:11,600 --> 00:02:16,720
postdoc interpretability methods, like you have a led your model, somebody gave it to you and you

22
00:02:16,720 --> 00:02:22,000
can't really change the model, then what do you do? You can use gradients and other things to extract

23
00:02:22,000 --> 00:02:28,320
some explanations. So that's what I've been really poorly focused on for like last 10 plus years.

24
00:02:29,200 --> 00:02:36,160
And recently, I've made a little bit of transition thinking about, okay, we've come this far

25
00:02:36,160 --> 00:02:42,800
developing lots of engineering tools to provide explanations. What are we missing? What do we have

26
00:02:42,800 --> 00:02:49,360
to do? What is the gap to fill in order for us to move to the next level? And that's what I talk

27
00:02:49,360 --> 00:02:56,000
about in I clear keynote this year about, well, what we really need to do is to paralyze what we do

28
00:02:56,000 --> 00:03:03,920
with engineering effort and science. So engineering effort is making tools, perhaps in absence of

29
00:03:03,920 --> 00:03:08,720
principles, you just kind of try things out and see what works out, build a lot of tests around

30
00:03:08,720 --> 00:03:14,880
that. Whereas science is approaching this complex mechanism called neural network or machine learning

31
00:03:14,880 --> 00:03:23,280
models or AI as an organism that we want to study. So go ahead and probe them, analyze their behaviors

32
00:03:23,280 --> 00:03:29,120
to see what comes out. Like pretend that this is something that we build the hypothesis and build

33
00:03:29,120 --> 00:03:36,880
tests and data analysis structure around it in order to gain more understanding about this unknown

34
00:03:36,880 --> 00:03:43,760
object. So I talk a lot about that in my keynote. Your keynote, which is titled Beyond Interpretability

35
00:03:43,760 --> 00:03:49,280
as you're describing, developing a language to shape our relationships with AI. Before we do go

36
00:03:49,280 --> 00:03:54,960
beyond interpretability, though, I wonder if it makes sense to start by having you characterize

37
00:03:54,960 --> 00:04:03,680
where we are as an industry and community with interpretability. What's the best way to characterize

38
00:04:03,680 --> 00:04:11,120
the current state of the art or the state of the field? There's a lot of needs and passion

39
00:04:11,120 --> 00:04:17,920
and requirements. We're not fully there yet, right? We're definitely not fully there yet. In fact,

40
00:04:17,920 --> 00:04:27,920
we're quite far away from it. Some of my work, I kind of took this course, if you call it a journey

41
00:04:27,920 --> 00:04:33,040
together with the community. I feel like I exactly was at the position where this is so exciting,

42
00:04:33,040 --> 00:04:37,680
we can do so much things and did a lot of things. And then I was at one point in 2018 in Eurip's

43
00:04:37,680 --> 00:04:44,480
paper. We shared a little shocking discovery that showed that a lot of interpretability methods

44
00:04:44,480 --> 00:04:50,400
that we've been using and deployed are not actually showing what you think it's showing.

45
00:04:51,840 --> 00:04:58,880
To TLDR of that paper is that when you show explanation from a trained model and you randomize

46
00:04:58,880 --> 00:05:04,240
that model, so go in there, just mess it up. And so now the model is garbage and show explanations

47
00:05:04,240 --> 00:05:10,080
from that model. You can't tell them apart. As a human, you can't tell them apart. As a computer,

48
00:05:10,080 --> 00:05:16,000
oftentimes you also can't tell them apart. That was a big paper. Right. And that was, you know,

49
00:05:16,000 --> 00:05:24,080
it received both very enthusiastic and people, some people who didn't like to, didn't like what

50
00:05:24,080 --> 00:05:28,720
they're reading, right? Because we had this trust and passion around these methods. And including

51
00:05:28,720 --> 00:05:36,720
myself, I test the own method that I developed and to my surprise and disappointment that the

52
00:05:36,720 --> 00:05:41,200
methods that I worked on also don't really pass that sanity check test to what we call.

53
00:05:42,800 --> 00:05:48,880
And since then, I think we've been, as a community, many smart people, people smarter me,

54
00:05:48,880 --> 00:05:55,120
joined to think really carefully about what does it mean to make progress? It's not just about

55
00:05:55,920 --> 00:06:01,600
making a reasonable and convincing picture of a bird and show an explanation of why that was a bird.

56
00:06:01,600 --> 00:06:07,360
Much more beyond that, we have to rigorously validate what we're seeing. We have to conduct human

57
00:06:07,360 --> 00:06:13,920
experiments to make sure that what we hope to happen actually happens with the human. So if,

58
00:06:13,920 --> 00:06:18,080
for example, this might be a good segue to also talk about one of my people or this conference

59
00:06:18,080 --> 00:06:25,680
that I clear where we argue that if your goal is to detect spurious correlation, then existing

60
00:06:25,680 --> 00:06:31,440
methods won't work for you unless you know what that spurious correlations are or let it.

61
00:06:32,400 --> 00:06:37,040
So just the community has been thinking a lot about, that's just a example of many other work

62
00:06:37,040 --> 00:06:42,640
that start to think about what is a test that we can use this explanation for and what are the

63
00:06:42,640 --> 00:06:48,960
tasks we simply cannot and should not. In terms of, do you have a sense for

64
00:06:48,960 --> 00:06:56,320
in practice and in industry practice, kind of what are the techniques that folks are

65
00:06:57,520 --> 00:07:03,600
using and the kind of state of the field from that perspective and I ask that mostly because

66
00:07:04,720 --> 00:07:08,880
you know, some of the first methods that I remember hearing about are like lime and

67
00:07:08,880 --> 00:07:15,360
chapp and those are still the ones that I hear about. You know, is that I'm not hearing and there's

68
00:07:15,360 --> 00:07:19,920
this rich field of folks of things that folks are doing or are those still the ones that are

69
00:07:19,920 --> 00:07:26,480
most used in practice? Do you have a sense for that? Yeah, I've seen many methods used across

70
00:07:26,480 --> 00:07:32,400
Google and other industry just, you know, people picked their favorites and I should mention that

71
00:07:33,120 --> 00:07:38,320
you know, the conclusion of our study that I just mentioned in 2018, I need to check paper,

72
00:07:38,320 --> 00:07:44,800
isn't to conclude, okay, all these methods don't use it, it's not that. The conclusion is that

73
00:07:44,800 --> 00:07:50,560
we don't yet understand what these explanations are showing, what information these explanations

74
00:07:50,560 --> 00:07:58,720
have and so if your goal is to, for example, just look at candidates of features that might have

75
00:07:58,720 --> 00:08:04,480
been used in model prediction, then perhaps these methods do work for you. In fact, we have

76
00:08:04,480 --> 00:08:10,800
plenty of other papers that showed they present these explanations in from doctors, like actual

77
00:08:10,800 --> 00:08:18,080
doctors and show that yes, they do help. So it's not to say, you know, all these family of methods,

78
00:08:18,080 --> 00:08:22,320
we should throw them away, it's not, it's just to say we should be very careful, we should build a

79
00:08:22,320 --> 00:08:27,360
lot of tests around it to make sure that it fits your bill, what you're trying to do. So to your

80
00:08:27,360 --> 00:08:34,000
question, I think they're, and exactly to follow to what I just said, it really depends on what

81
00:08:34,000 --> 00:08:41,280
you're trying to do. So sometimes Lyme is a very simple method, elegant, elegant simple,

82
00:08:41,280 --> 00:08:48,320
but that simplicity also implies that if your decision boundary is very curvy, for example,

83
00:08:48,320 --> 00:08:54,560
very complex, fitting a linear function in any of the local area is not going to be a good

84
00:08:54,560 --> 00:09:00,080
approximation of how your model works. So in that cases, it will not, it will not be the best

85
00:09:00,080 --> 00:09:05,440
friend for you. Other cases, you know, there are computationally really expensive methods,

86
00:09:05,440 --> 00:09:10,640
and if you are a task has to be something that decision has to make very quickly,

87
00:09:10,640 --> 00:09:14,480
that's not going to work for you, because it's just simply so slow, it doesn't work for what

88
00:09:14,480 --> 00:09:20,080
you're trying to do. So I've seen many family of methods like those, you know, Greg came,

89
00:09:20,080 --> 00:09:26,080
in fact, SmoothGrad also used in many domains, like TKW used many. There isn't, I haven't seen a

90
00:09:26,080 --> 00:09:31,920
single method that works for everybody, and I suspect there will be, there will never be

91
00:09:31,920 --> 00:09:38,720
a single method that's going to work for everybody. Awesome, awesome. And so your talk

92
00:09:38,720 --> 00:09:48,160
beyond interpretability, you kind of organize that around these core ideas of studying machines

93
00:09:48,160 --> 00:09:56,240
and isolation, and you spoke a little bit to that, studying machines in conjunction with humans,

94
00:09:56,240 --> 00:10:00,960
and then this kind of broader idea of alignment. You know, let's maybe go through those when you

95
00:10:00,960 --> 00:10:07,440
talk about studying machines with in isolation, you're talking about applying the scientific method

96
00:10:07,440 --> 00:10:14,080
to studying the machine learning methods we're building. You know, speak more about some of the

97
00:10:14,080 --> 00:10:19,280
methods that you've explored and the way you think about that category. Yeah, this is the

98
00:10:19,280 --> 00:10:25,360
category that I'm super excited about, because there's so much work we can do in this, and just

99
00:10:25,360 --> 00:10:31,680
so little has been explored. And the things that I talk about in this talk is just one of many,

100
00:10:31,680 --> 00:10:37,600
many, many centuries old studies after studies about humans. This is just one tiny bit of it,

101
00:10:37,600 --> 00:10:42,400
not even point zero zero one percent. And that should just kind of give, hopefully give you a

102
00:10:42,400 --> 00:10:49,520
scope of what I'm talking about when I say studying machines as a scientific object and leverage

103
00:10:49,520 --> 00:10:56,400
studies done on humans. We've done many, many studies on humans, biases, perceptual biases,

104
00:10:56,400 --> 00:11:01,760
cognitive biases, or just capability, what we can see and what we can cannot see.

105
00:11:01,760 --> 00:11:07,600
So there's a Gestalt study that was a historical Gestalt study from psychology.

106
00:11:07,600 --> 00:11:14,400
Was it psychology and you applied similar ideas to studying machine learning models?

107
00:11:14,400 --> 00:11:18,720
Yeah, that's right. So I can tell you a little more about what I exactly did. So Gestalt

108
00:11:18,720 --> 00:11:25,200
phenomenon for those who are familiar with it is something that is very strong phenomenon.

109
00:11:25,200 --> 00:11:30,400
That for example, I show you this triangle. Actually, just think about Amazon logo, right?

110
00:11:30,400 --> 00:11:36,480
The Amazon logo has this arrow hidden inside of it. It's called A to Z and it's point at it.

111
00:11:36,480 --> 00:11:42,640
FedEx logo actually has also this arrow embedded inside. And those design decisions are

112
00:11:42,640 --> 00:11:48,960
essentially based on a group of strong perceptual phenomenon called the Gestalt phenomenon.

113
00:11:48,960 --> 00:11:55,360
And designers to this day leverage those very strong phenomenon to make visually appealing

114
00:11:55,360 --> 00:12:02,240
things. The one specific rule of Gestalt, Gestalt rule, if you make your Gestalt principles

115
00:12:02,240 --> 00:12:07,520
or law, that we study is a closure effect, which is if I show you a pack, my three pack

116
00:12:07,520 --> 00:12:13,760
pens that align nicely, humanized can now help yourself, help yourself, but seeing a triangle,

117
00:12:13,760 --> 00:12:19,120
even though there's no triangle. And we are, our core question of that paper was,

118
00:12:19,120 --> 00:12:26,160
well, does neural network have the same thing? Because if it does, that says something about how,

119
00:12:26,960 --> 00:12:31,920
how our brain works versus how neural network brain works. And you can imagine that's just

120
00:12:31,920 --> 00:12:37,280
the beginning of so many other studies that we've done on humans. If I give you a high number,

121
00:12:37,280 --> 00:12:42,560
for example, a really famous example, Danny Cannonman spoke, talks about, if I give you a high

122
00:12:42,560 --> 00:12:47,280
number of any kind, like I just say, how many things, how many trees do you think we have in Seattle,

123
00:12:47,280 --> 00:12:53,760
and you give me a number? Then I ask you completely on related question, let's say, how many people

124
00:12:53,760 --> 00:12:58,560
cross borders between United States and Canada or something? My answer is biased higher.

125
00:12:58,560 --> 00:13:05,040
Exactly. Yeah. And it's such a robust statistical phenomenon that it's so embedded in us.

126
00:13:05,040 --> 00:13:11,200
So then the question is, what are those embedded inherent biases for phenomenon that neural network

127
00:13:11,200 --> 00:13:17,600
has? We have no idea. We just have no idea. And there's so much studies we can do to just literally

128
00:13:17,600 --> 00:13:23,120
leverage what we did in humans and just test to ask machines, craft a design experiment really

129
00:13:23,120 --> 00:13:29,200
well and do that experiment to see what happens in machines. Now, when you're talking about the

130
00:13:29,200 --> 00:13:36,800
Gestalt phenomenon in particular, and this example of the Pac-Man and the triangle, are you

131
00:13:38,720 --> 00:13:47,440
trying to conduct like literally that specific experiment, meaning a visual experiment and things

132
00:13:47,440 --> 00:13:53,440
like Pac-Man and triangles? Because it raises for me the question, what does it even mean for the

133
00:13:53,440 --> 00:13:57,280
machine to be doing what the human is doing? I thought you were going to ask a slightly different

134
00:13:57,280 --> 00:14:01,600
question also to both our interesting questions. So let me ask you a question first.

135
00:14:02,320 --> 00:14:05,520
What does it even mean the machine is to be doing what humans are doing?

136
00:14:06,080 --> 00:14:12,960
I just elaborate, you know, I'm thinking of, you know, as we first started to build deep networks,

137
00:14:12,960 --> 00:14:17,840
you know, some of the early studies like deep dream and others like looked at the early layers

138
00:14:17,840 --> 00:14:22,320
of the network and found, you know, textures here and shapes here. Is it like trying to

139
00:14:22,320 --> 00:14:27,440
introspect networks and see if we've got to triangle somewhere or or something else?

140
00:14:27,440 --> 00:14:33,520
No, it's a lot more global understanding of of the object. So let's say let's we didn't do this

141
00:14:33,520 --> 00:14:37,920
in the paper, but let's say we did the following and I can tell you what that might imply. So

142
00:14:37,920 --> 00:14:45,360
let's say I showed in the paper, yeah, the network has guessed all a fact and which means that we

143
00:14:45,360 --> 00:14:52,000
can we don't even have to occlude things in an image in order for it to learn how to fill in the

144
00:14:52,000 --> 00:14:57,200
gap because you know what, if I put a triangle and put something on top of it and now network can

145
00:14:57,200 --> 00:15:03,440
only see parts of the triangle, we already know that network will fill in that gap and what that

146
00:15:03,440 --> 00:15:08,320
means that might have an implication for the way that we augment the data, the data augmentation,

147
00:15:08,320 --> 00:15:14,880
we intentionally rotate and shift or blow up the image or put occlusion on top of it to make

148
00:15:14,880 --> 00:15:19,920
sure that network can be robust to these changes. Perhaps you don't have to do that if you know

149
00:15:19,920 --> 00:15:25,040
enough about what the network is already capable of, maybe that could inform data augmentation.

150
00:15:25,040 --> 00:15:33,360
And that, that's just a small example of many, many things that will guide our developing,

151
00:15:33,360 --> 00:15:38,400
our decisions when they we develop these neural networks. What are the gaps that we should really

152
00:15:38,400 --> 00:15:42,880
teach them to? Because they don't have it a LED in inherently. And what are the things we should

153
00:15:42,880 --> 00:15:48,880
rather, we don't have to worry about because it already has some architecture, something about the

154
00:15:48,880 --> 00:15:56,160
way we built it, already builds in those capabilities. And so what are some examples of experiments that

155
00:15:56,160 --> 00:16:00,720
you ran to try to demonstrate this? And maybe before that, what was the question that you thought I

156
00:16:00,720 --> 00:16:06,080
was going to ask? Yes, so I thought you were going to ask, you know, in traditional Gestalt

157
00:16:06,080 --> 00:16:13,280
phenomenon, the way that they test humans is response time. So they put some distracting objects

158
00:16:13,280 --> 00:16:17,200
in the screen and they put a Gestalt triangle and say where's the Gestalt triangle or where's

159
00:16:17,200 --> 00:16:22,000
this anger thing that how fast they can find it. And so I thought you're going to ask, how did

160
00:16:22,000 --> 00:16:27,360
you test that in neural network? Because that's, I think, the core challenge of doing what I'm,

161
00:16:28,240 --> 00:16:34,800
what I'm incurred, what I am excited about doing to in order to bring test experiments that's

162
00:16:34,800 --> 00:16:40,640
done on humans to be done for neural networks. You would have to think very carefully about,

163
00:16:40,640 --> 00:16:45,520
okay, we can't really measure response time because there's no such thing in neural network.

164
00:16:45,520 --> 00:16:50,640
How would you design an experiment such that you can still measure the effect robustly,

165
00:16:50,640 --> 00:16:57,520
but convincingly? So things that we did, or we did a lot of testing into, okay, the way that we

166
00:16:57,520 --> 00:17:03,120
measure is the simple metric of closure measure, what we call closure measure. Now, what could go wrong?

167
00:17:03,120 --> 00:17:09,040
Would it go wrong? If I have, these are some, some of the details, but there are some confounding

168
00:17:09,040 --> 00:17:14,640
factors that we should really rule out and test that metric with those potential confoundings to

169
00:17:14,640 --> 00:17:19,440
make sure that we're not fooling ourselves, that this metric actually fakes fully measure what we

170
00:17:19,440 --> 00:17:25,280
want to measure. Can you explain the metric before talking through some of the confounding factors?

171
00:17:25,280 --> 00:17:32,640
Yeah, it's actually pretty simple. So metric itself is a cosine similarity between two set

172
00:17:33,280 --> 00:17:41,680
two pairs. So first pair is a gestal triangle, which invokes that closure phenomenon to full

173
00:17:41,680 --> 00:17:46,480
triangle. So this measures how similar is it to the neural networks embedding space, how similar.

174
00:17:47,520 --> 00:17:55,280
And we subtract that cosine similarity with a, what we call disorder triangle. So the same

175
00:17:55,280 --> 00:18:01,120
Pac-Man, but Pac-Man are now slightly rotated so that you no longer see the triangle.

176
00:18:01,120 --> 00:18:06,960
And similarly between that disorder triangle to the full triangle. And we just subtract two.

177
00:18:06,960 --> 00:18:12,880
Then that's all there is really. And we do this for every layer and every, every bottom like

178
00:18:12,880 --> 00:18:17,600
layers. For the rest of that, if there's speed connection, you make sure that every information

179
00:18:17,600 --> 00:18:24,960
is coming to that layer. For the confoundings, now many things can go wrong. So for example,

180
00:18:24,960 --> 00:18:32,640
if I make, if I don't, if I don't control for the location of this triangle, for example,

181
00:18:32,640 --> 00:18:38,400
when I compare the gestal triangle to full triangle, if they're, they're literal pixel overlap,

182
00:18:38,960 --> 00:18:44,560
is a lot more than this order triangle, that's a problem, right? Because it's literally the same,

183
00:18:44,560 --> 00:18:49,120
it's just this pure pixel overlap is going to be a confounding factor.

184
00:18:50,000 --> 00:18:54,800
So the, because in the journal or same thing in the same place is going to think of, I think that

185
00:18:54,800 --> 00:19:00,240
these two things are similar. And it's not going to abstract out to read the read our intention,

186
00:19:00,240 --> 00:19:05,520
which is to express, well, here are incomplete triangle versus complete triangles.

187
00:19:05,520 --> 00:19:10,560
So what we do is we change the background color. We rotate them around. We, in this way,

188
00:19:10,560 --> 00:19:17,360
and both in, in local data, which we call, we control for all that and measure the average effect.

189
00:19:17,360 --> 00:19:22,800
I'm not sure that we cover this, but the, the presumption being the result is you identified the,

190
00:19:23,600 --> 00:19:29,280
that this effect is present in neural networks. That's right. But how do you characterize it to,

191
00:19:29,280 --> 00:19:35,440
to a certain degree, or what, what's the interesting, you know, what's the degree, I guess?

192
00:19:36,000 --> 00:19:43,680
Yeah. So the interesting nugget in this is that it only exists when the network is able to

193
00:19:43,680 --> 00:19:51,680
generalize. So if let's say I have a network that was overfitted to some random labels,

194
00:19:51,680 --> 00:19:56,080
which we can totally do, we can achieve up to 90 something present accuracy. If I give you random

195
00:19:56,080 --> 00:20:00,720
label, it's happily overfit. But test time, it doesn't know how to generalize. It never learn

196
00:20:00,720 --> 00:20:06,560
what is the meaning of say frog or cat. Those networks do not show the closure effect.

197
00:20:07,120 --> 00:20:14,640
It's only the regularly trained network. We also try to keep the label correct, but instead

198
00:20:14,640 --> 00:20:20,880
shuffle the pixels. So that means I am destroying the local features and train the network.

199
00:20:20,880 --> 00:20:27,120
Again, it is happy to overfit the data because labels are correct, but pixels are shuffled.

200
00:20:28,320 --> 00:20:35,120
If there is some statistical patterns across the classes, for example, all the cats,

201
00:20:35,120 --> 00:20:41,680
the average value of pixels or 0.5, whereas all the dogs are 0.3. So our question was,

202
00:20:41,680 --> 00:20:49,360
can, would neural network leverage that neural network train those destroyed local structure?

203
00:20:49,360 --> 00:20:55,920
Will it still have closure effect? Because if it does, it's not the spatial

204
00:20:55,920 --> 00:21:01,120
meaningfulness that causes closure effect. It's something else. It's something else that we don't

205
00:21:01,120 --> 00:21:06,160
know. And that's something we should, we must investigate. It turns out that if you destroy

206
00:21:06,160 --> 00:21:12,400
the local feature, it does not have closure effect. So you might ask, okay, so what does that mean?

207
00:21:12,400 --> 00:21:18,320
What does that mean? Ultimately, why are we doing this? So ultimately, my studies like this,

208
00:21:18,320 --> 00:21:24,800
I think will lead to insights such as, for example, does that mean that if we build in more

209
00:21:24,800 --> 00:21:29,680
human-like perception in neural network, when we design this network, LLM's, vision language model,

210
00:21:29,680 --> 00:21:37,680
whatever, maybe that means we will help generalization. Now, there's a huge logical jump that I made

211
00:21:37,680 --> 00:21:43,120
there because error this way, causal analysis, this way, doesn't mean the other way. However,

212
00:21:43,120 --> 00:21:48,560
a lot of studies that could perhaps show that relationship or absence of that relationship

213
00:21:48,560 --> 00:21:52,400
could really guide us designing these models because at the end of the day, we're the ones

214
00:21:52,400 --> 00:21:58,000
making decisions what to make next. Within this broad category of studying machines and isolation,

215
00:21:58,000 --> 00:22:01,920
that's just one of the studies that you've conducted. What are some others?

216
00:22:02,720 --> 00:22:10,640
Yeah, so in another study that I share, it's preliminary results, but this is in that work,

217
00:22:10,640 --> 00:22:18,640
we think about the saliency methods, which is the target of the topic of discussion in

218
00:22:18,640 --> 00:22:22,960
the sanity check paper that where we show the training or on training or you can tell the

219
00:22:22,960 --> 00:22:29,760
difference in this saliency map based family of explanation. So we kind of take a deeper dive

220
00:22:29,760 --> 00:22:36,160
into that. So then our question in that paper was, okay, so humans can tell the difference,

221
00:22:36,160 --> 00:22:42,320
but maybe the information is there. It's just that we can tell. And perhaps the other way around

222
00:22:42,320 --> 00:22:48,560
is true too. What if there's information that human can tell the machines can't? So in that

223
00:22:48,560 --> 00:22:55,600
paper, we show both of the information. So it turns out that if I train a model with the same

224
00:22:55,600 --> 00:23:00,400
data, same architecture, but just different seeds. So now I have two models that roughly

225
00:23:00,400 --> 00:23:03,840
achieve the same accuracy, but just literally different seeds and different weights.

226
00:23:03,840 --> 00:23:08,400
When I get explanations, humans can tell the difference. They look the same. Like I looked at

227
00:23:08,400 --> 00:23:15,520
many, many of those same, but for neural network, it can tell which which explanation came from

228
00:23:15,520 --> 00:23:22,800
which model close to perfection, close to 99% 96, the more than 96% accuracy. So that's like

229
00:23:22,800 --> 00:23:28,160
concrete evidence of some information that humans cannot read off of these explanations,

230
00:23:28,160 --> 00:23:34,400
but it's there. This paper characterizes that effect as a fingerprinting of the model.

231
00:23:35,600 --> 00:23:43,200
Does that have implications in kind of adversarial data leakage and questions like that,

232
00:23:43,200 --> 00:23:47,520
or are those tangential to the kinds of things you're thinking about?

233
00:23:48,240 --> 00:23:56,000
I think it has implications to that. But in this work, we focus on explanations alone

234
00:23:56,000 --> 00:24:04,640
that what sort of fingerprints of the model we show that one can read off of the explanation.

235
00:24:04,640 --> 00:24:09,920
So for example, you can imagine actually now you said it. Now I think actually there's

236
00:24:09,920 --> 00:24:14,560
better more link to that. So for example, there has been work that shows, if you just give me

237
00:24:14,560 --> 00:24:19,680
explanations, I can actually completely reconstruct a model. I think this has worked from many years

238
00:24:19,680 --> 00:24:25,520
ago, Barry Moritz-Hardt's group in when he was in Berkeley. And what does that mean? Well,

239
00:24:25,520 --> 00:24:32,000
that means that if somebody requires me to provide older explanations for the input data that

240
00:24:32,000 --> 00:24:39,760
customers are using, then that could be used to reverse engineer preparatory information

241
00:24:39,760 --> 00:24:46,080
and intellectual property gets really a big question. I think that's what you're referring to.

242
00:24:46,080 --> 00:24:51,360
And further, there's other results that suggest that because if we're talking about large models

243
00:24:51,360 --> 00:24:56,000
that can memorize data, there's actually training data that can leak into the model. And if that

244
00:24:56,000 --> 00:25:02,720
leaks out through explanations, then you're also exposing the potentially private information.

245
00:25:02,720 --> 00:25:08,160
That's right. And Catalina Uler's work from MIT showed that this also happens in VAE models

246
00:25:08,160 --> 00:25:14,240
where you can completely reconstruct every single training data because they're attracting points

247
00:25:14,240 --> 00:25:21,520
of the network. You can 100%. And apparently, this phenomenon is widely common. Nicholas

248
00:25:21,520 --> 00:25:28,800
Papernod's paper, Rikus Karlin's paper, all show that this liquid not about explanation,

249
00:25:28,800 --> 00:25:35,040
but just the liquid of the training data completely is possible. So that all should really hint us

250
00:25:35,040 --> 00:25:40,880
that there's just so much we don't know about neural network. And all these studies are super

251
00:25:40,880 --> 00:25:46,560
useful. But somebody should also think about, okay, well, let's just go back to the drawing board,

252
00:25:46,560 --> 00:25:53,040
zero, beginning. And consider this as an alien. And let's think about how we can study this alien

253
00:25:53,040 --> 00:25:57,840
because there's just simply so much to study, we need a lot of different approaches to study

254
00:25:57,840 --> 00:26:04,080
this important alien. And so this first category that we've talked about is, hey, we've got the alien

255
00:26:04,080 --> 00:26:11,200
in a room and we're just kind of, you know, probing and studying the alien. Is the studying machines

256
00:26:11,200 --> 00:26:18,080
with humans? Is that more looking at the alien and the context of collaborating or working with

257
00:26:18,080 --> 00:26:24,160
humans? That's right. To extend the analogy. Yes, exactly. That's right. And I think we should go

258
00:26:24,160 --> 00:26:30,400
one step forward. So a lot of people talk about collaborating with machines and how we can leverage

259
00:26:30,400 --> 00:26:38,960
each other's intelligence and augmented intelligence and so on. But in fact, I think what's perhaps

260
00:26:38,960 --> 00:26:46,480
more important to recognize is that as we interact with these machines and work with AI and live

261
00:26:46,480 --> 00:26:52,800
in this society where every single thing that we do or somehow some AI models are behind them

262
00:26:52,800 --> 00:27:00,000
or most of it anyway, we should know that that is changing our behavior. That's changing us.

263
00:27:00,000 --> 00:27:05,360
It's changing the society. Take Twitter, take YouTube, whatever. People spend a normal

264
00:27:05,360 --> 00:27:11,760
amount of time on social media. Who decides what you see? Who decides what you see from which you

265
00:27:11,760 --> 00:27:17,360
decide you make a decision about your life, about your career? How what you think? How angry you

266
00:27:17,360 --> 00:27:25,520
are or how happy you are? That ecosystem between machines and humans, it's not just collaborations

267
00:27:25,520 --> 00:27:33,040
and leveraging each other. It's a whole world that we're now living with AI. And the language

268
00:27:33,040 --> 00:27:41,280
aspect of my talk, how I think that this language must be developed that we should put

269
00:27:41,280 --> 00:27:45,840
force together to develop this language. And I even think that once we develop this language,

270
00:27:45,840 --> 00:27:50,640
this might be something we are taught in school. For kids as they learn math in high school and

271
00:27:50,640 --> 00:27:55,920
middle school, maybe this is another language that they will learn in schools because it's so

272
00:27:55,920 --> 00:28:01,920
important so that we have effective ways to manage and communicate with these machines.

273
00:28:01,920 --> 00:28:08,960
So pulling back again. I was curious why you characterize it as a language as opposed to

274
00:28:08,960 --> 00:28:16,000
a set of principles or foundational understanding about using machines. What makes you think of it as

275
00:28:16,000 --> 00:28:23,600
as a language or communication vehicle? It representational differences. So if we have same

276
00:28:23,600 --> 00:28:28,400
representation as a human, we can agree on our principle because the underlying

277
00:28:29,280 --> 00:28:35,840
vocabulary that exercises those principles, we agree. We know that you and I know we live a life

278
00:28:35,840 --> 00:28:42,320
and we live and die. We live under the constraints of gravity. We all know that. We have common

279
00:28:42,320 --> 00:28:47,920
understanding. With the machines, we don't have that. So that means even if we have a principle

280
00:28:47,920 --> 00:28:54,480
defined in our own language, we may not be able to communicate what that really means to machines.

281
00:28:54,480 --> 00:29:00,080
We even have hard time communicating with each other what, for example, fairness, fair justice means.

282
00:29:00,640 --> 00:29:06,240
And the challenge is so high, I believe that in order to just really feel that gaps in our

283
00:29:06,240 --> 00:29:11,680
representational spaces, it would have to be back and forth. It would have to be a conversation

284
00:29:11,680 --> 00:29:17,600
and to have a conversation, we need rich language that both of us can express what we really mean.

285
00:29:17,600 --> 00:29:22,000
And that's why language is important. Tell us a bit about your research program that explores

286
00:29:22,000 --> 00:29:28,800
some of these ideas. Yeah, so I must say that this is weird at the beginning of a huge effort

287
00:29:28,800 --> 00:29:34,080
that we, as a community, I hope that we will push down, push on. And lots of folks already have

288
00:29:34,080 --> 00:29:41,440
thought about this. So to build this language, we have to study airlines as we're AI models,

289
00:29:41,440 --> 00:29:48,400
as an object of scientific study. And not just that in isolation, but also in this whole ecosystem

290
00:29:48,400 --> 00:29:53,840
between machines and humans. We live in an ecosystem. We influence each other. We should study

291
00:29:53,840 --> 00:30:01,920
that interaction very carefully. But on top of, but in addition to that, we need to humans have

292
00:30:01,920 --> 00:30:09,040
amazing capacity to learn new things. We should expand what we know. This is a great opportunity

293
00:30:09,040 --> 00:30:16,720
to meet a coworker who is sometimes smarter than me, thinking about your network,

294
00:30:16,720 --> 00:30:22,720
who could really show us how to view a problem that we've been trying to solve from a different

295
00:30:22,720 --> 00:30:28,080
angle. And that could really inspire us to see the solution that we didn't perhaps previously

296
00:30:28,080 --> 00:30:34,800
solve before. So expanding what we know is another branch of this research program that I am

297
00:30:34,800 --> 00:30:40,720
envisioning. And some of those work that I talked about in keynote is Alpha Zero, thinking about Alpha

298
00:30:40,720 --> 00:30:47,200
Zero, the superhuman network that plays chess better than any other humans. How do we go about,

299
00:30:47,200 --> 00:30:53,920
can we do in-depth study in this amazing network to see how do we interpret them? What does it know

300
00:30:53,920 --> 00:30:58,320
and what does it not know? How much of our representational space overlap?

301
00:30:58,320 --> 00:31:05,840
What's your sense for how far we are there? How far we are with that in the case of Alpha Zero?

302
00:31:05,840 --> 00:31:11,760
We are far, we're far away. I think that answers to a lot of those questions. All of the far

303
00:31:11,760 --> 00:31:18,160
questions were far. Yeah, it's why it's a research in which why I'm excited to pursue

304
00:31:18,160 --> 00:31:23,200
distraction for, you know, probably first. I hope that before I, before end of my day,

305
00:31:23,200 --> 00:31:29,760
I would see some a lot different than than where we are now. But we are far away, but the things

306
00:31:29,760 --> 00:31:35,120
that we discovered in that work is already really interesting and frankly encouraging. So when

307
00:31:35,120 --> 00:31:40,400
we started that project, we have this Alpha Zero Network. Only thing that we knew about that is

308
00:31:40,400 --> 00:31:45,120
that it plays chess really, really well. It can be the stock fish engine, which is arguably the

309
00:31:45,120 --> 00:31:52,240
best chess engine in the world. And note that the chess engine, human player, it's very hard for

310
00:31:52,240 --> 00:31:58,240
human player, any human players to beat the chess engine. So this is arguably a very, very good

311
00:31:58,240 --> 00:32:03,200
machine. So when we started that project, we weren't sure what we're going to find. We had ideas

312
00:32:03,200 --> 00:32:09,520
that, okay, well, let's first try to detect human chess concepts in this network and see if it's

313
00:32:09,520 --> 00:32:14,160
there at all. And we honestly thought we don't know what will happen. How much like we will,

314
00:32:14,160 --> 00:32:22,080
will we find 30% of human concepts, 20%, 80%, and we ended up finding quite identifying these

315
00:32:22,080 --> 00:32:28,240
human concepts a lot in Alpha Zero, which is encouraging news, because that means that at least

316
00:32:29,440 --> 00:32:36,080
some concept that at least correlates with how we understand chess, how we play chess,

317
00:32:36,080 --> 00:32:43,760
exists in Alpha Zero. That's a great news as a common ground to start to expand what we know.

318
00:32:44,320 --> 00:32:49,360
So in the follow-up work that we're going to do this summer and next year kind of a long term

319
00:32:49,360 --> 00:32:58,640
project is with this amazing performer professional chess player also PhD student in machine learning.

320
00:32:58,640 --> 00:33:06,480
She's going to join, her name is Lisa Shutt. She's going to join us to work on this problem of,

321
00:33:06,480 --> 00:33:12,160
okay, now we have an expert chess player who also know machine learning. How can we discover new

322
00:33:12,160 --> 00:33:18,720
concepts that we didn't know before? What are some other work that you're seeing that in the

323
00:33:18,720 --> 00:33:27,520
machines with humans category? You mentioned that you're early on in that part of the research.

324
00:33:27,520 --> 00:33:32,240
Are there other examples of things that you're looking at there? Yeah, so I would love to talk

325
00:33:32,240 --> 00:33:39,600
about artist art project that we've done in as a part of expanding what we know. This is human and

326
00:33:39,600 --> 00:33:47,120
machine collaborating together to inspire each other. So I have a long passion for art related

327
00:33:47,120 --> 00:33:54,160
projects because my aunt is an artist in Paris, my mom is an artist. I was just naturally dragged

328
00:33:54,160 --> 00:34:01,920
to something that could, that are just beautiful, beautiful to see, beautiful to feel and think

329
00:34:01,920 --> 00:34:12,080
about. So this project came out of the art and culture team in at Google and with a nor project

330
00:34:12,080 --> 00:34:20,560
who are amazing three designers and artists in London and they, we had this idea of, okay,

331
00:34:20,560 --> 00:34:28,160
what if we use the bizzalignment between how machines view the world when I show you this flower,

332
00:34:28,160 --> 00:34:32,480
would machine also think it's pretty? Would machine think it's very small? Would machine think

333
00:34:32,480 --> 00:34:38,880
it's purple and is it inspiring? Does it inspire it spring time of the year? Or would it think about

334
00:34:38,880 --> 00:34:44,000
something else? And what if I excommunicate something to the machine and say, hey, machine,

335
00:34:44,000 --> 00:34:50,400
I think this is pretty. I think this has, Phil makes me think about babies because it's a small plant.

336
00:34:51,600 --> 00:34:56,800
What do you think machine? And machine comes back with other images from different pool that say,

337
00:34:56,800 --> 00:35:02,800
hey, I think these are images that inspires me when you talk about that flower. And there's this

338
00:35:02,800 --> 00:35:08,640
communication back and forth from which I get inspired by say, oh, interesting. So you are thinking

339
00:35:08,640 --> 00:35:13,440
ocean for some reason from this flower. I can kind of say that inspires me this one time I was

340
00:35:13,440 --> 00:35:19,280
in the lotion blah, blah, blah. So that communication, what which we call moodboard, well, it's a standard

341
00:35:19,280 --> 00:35:24,480
word, but we call moodboard search and concept camera is some tools that we built to enable this

342
00:35:24,480 --> 00:35:30,800
communication, which we are all open sourcing in a couple of weeks. And I think directions like

343
00:35:30,800 --> 00:35:35,840
that, like deep dream, you talked about deep dream, there has been our projects around deep dream

344
00:35:35,840 --> 00:35:43,040
to inspire humans. Directions like that, our project, music, we are hoping to also do some

345
00:35:43,040 --> 00:35:49,840
music projects, or exciting because it's also about not just thinking about task tasks,

346
00:35:49,840 --> 00:35:53,600
task prediction, prediction prediction, it's also about how can we enrich

347
00:35:55,040 --> 00:36:01,920
humans, us and artists and other people behind a machine learning community to leverage this tool.

348
00:36:01,920 --> 00:36:07,760
I think that's extremely exciting. In the case of that heart project concept camera, I'm envisioning

349
00:36:07,760 --> 00:36:15,040
you've got some embedding space of images and you've got your little plant, you find where it is

350
00:36:15,040 --> 00:36:20,480
in that embedding space or what's closest to it. And then you kind of walk the neighborhood to see

351
00:36:21,440 --> 00:36:26,080
what's around it. And that's what the machine is thinking of is, is that along the lines?

352
00:36:26,080 --> 00:36:30,720
Very similar. Okay. Yeah, it's very similar. So I can give you an example of what artists,

353
00:36:30,720 --> 00:36:37,680
one artist, we hired artists to build artistic concept. It's not just a one flower. And one of the

354
00:36:37,680 --> 00:36:43,360
concepts that she built was, this was actually one of our engineers, she built a concept of father.

355
00:36:44,320 --> 00:36:51,680
It reminded me of all the images she collected, reminded her of her father who passed. And it

356
00:36:51,680 --> 00:36:57,920
consists of images like ocean, maybe some blurry leaves, it's very diverse set of images. And from

357
00:36:57,920 --> 00:37:02,880
that such an abstract concept, what machines, we take all those abstract concepts and build the

358
00:37:02,880 --> 00:37:08,080
concept vector from that those images in the embedding space as you described. And then find

359
00:37:08,080 --> 00:37:14,800
other images from a different pool, maybe by cropping or blowing up some part of the image or

360
00:37:14,800 --> 00:37:20,880
removing some part of an image to identify where does this concept present in other set of images.

361
00:37:20,880 --> 00:37:26,160
And that's how we create the conversation. Talk a little bit about the, is there an interactive

362
00:37:26,160 --> 00:37:31,840
element of the conversation? Is that just sequentially kind of choosing another image or is there

363
00:37:32,320 --> 00:37:37,200
another framing for that? Yeah, yeah. So we do, there's a lot of interactions. So after

364
00:37:37,200 --> 00:37:43,520
machines came back in with set of images, you can up vote or down vote to indicate, no, no, no,

365
00:37:43,520 --> 00:37:49,840
that's not what I meant. Yes, yes, more of these. You can crop and scale the images to show better

366
00:37:49,840 --> 00:37:56,320
showcase this is really what I meant. And you can go back and force infinitely. Images are such a powerful

367
00:37:56,320 --> 00:38:06,480
means for visual art in terms of communication, because how quickly you can digest number of images

368
00:38:06,480 --> 00:38:11,920
at the same time. So we leverage that in this work. And is there also an explanations element to

369
00:38:11,920 --> 00:38:19,040
this work? Yeah, good question. So we tried a few things in the tools that we broadcast. One of

370
00:38:19,040 --> 00:38:26,880
which we do is focus mode, which represent we simply do very simple things because again,

371
00:38:26,880 --> 00:38:31,520
we weren't sure if the saliency maps would hold up to this task. So instead, we decided to do

372
00:38:31,520 --> 00:38:37,840
something very simple. So whenever there's a concept present, we, if you click a box, then it will

373
00:38:37,840 --> 00:38:43,760
show a box in the whole image to show what machine, which part of the image that was most activated

374
00:38:43,760 --> 00:38:49,200
by this concept from machines perspective. So that's the only explanation part of this project.

375
00:38:50,320 --> 00:38:54,880
Okay. There's also heap met mode, but that's that's rather

376
00:38:55,920 --> 00:39:04,480
stealth mode for now. That's what? It's a stealth part of the project. Not in an exciting way.

377
00:39:04,480 --> 00:39:12,240
I should. Yeah. It's some some more work has to be done there. Got it. Got it. Other other aspects of

378
00:39:12,240 --> 00:39:18,320
this machines, studying machines in conjunction with with humans that we can talk about.

379
00:39:19,200 --> 00:39:25,360
Let's see. Some of the aspects that I am excited about and I can tell you a little bit about

380
00:39:25,360 --> 00:39:31,360
few projects that I'm currently working on. And I think that's that's really exciting next step. So

381
00:39:31,360 --> 00:39:39,680
one, we are looking at what sort of behaviors emerge in multi-agent reinforcement learning setting.

382
00:39:39,680 --> 00:39:43,520
So if you, if you just let them wild and learn and give them

383
00:39:44,480 --> 00:39:48,880
pretty high level rewards and they are team working at trying to work as team,

384
00:39:49,600 --> 00:39:58,160
what are what's the division of labor? How do they learn what to do? So if I build say an agent that

385
00:39:58,160 --> 00:40:06,720
has four legs, intuitively if we give two people a task of let's try to run as fast as we can and

386
00:40:06,720 --> 00:40:11,600
you you have this this thing, then we would naturally the first person would grab the first leg

387
00:40:11,600 --> 00:40:16,720
and then the second person will grab the letter to two back legs, right? Will that what does that

388
00:40:16,720 --> 00:40:23,040
happen in in multi-agent RL setting? You don't know. What if we have 10 legs? What if we also have

389
00:40:23,040 --> 00:40:30,400
legs and a wheel and a wing? What happens? Those are just really out there curiosity-based questions

390
00:40:30,400 --> 00:40:37,920
that we're thinking about. It reminds me a little bit of a conversation I had a couple of years ago

391
00:40:37,920 --> 00:40:45,600
with Blaze, Agroira Yarkas, also at Google and some of his thoughts on kind of this concept of

392
00:40:45,600 --> 00:40:51,600
social intelligence emerging between agents. Is that a collaborator on this project or is that

393
00:40:51,600 --> 00:40:58,960
that? Oh Blaze, Blaze very busy. I would love to collaborate with you Blaze if you were watching

394
00:40:58,960 --> 00:41:06,720
the fun, but he's very busy. It is similar. I think in many years ahead of time that's completely

395
00:41:06,720 --> 00:41:12,240
feasible idea. I wouldn't be surprised that there are some efficient behaviors that are maybe

396
00:41:12,240 --> 00:41:17,200
similar, maybe this similar with how humans work together, they would emerge in this important

397
00:41:17,200 --> 00:41:23,200
thing is though that we have to look out for it. We have to have our tool right tools to see when

398
00:41:23,200 --> 00:41:30,240
that emerges we notices and we confirm or just confirm that it's the desirable thing that we want

399
00:41:30,240 --> 00:41:34,080
them there and that there's something to be done or maybe if you should do something about it.

400
00:41:37,040 --> 00:41:42,560
I was going to also tell you about this second project that briefly might be interest people.

401
00:41:42,560 --> 00:41:48,320
There has been a lot of, as everyone knows, that large language model or vision language model has

402
00:41:48,320 --> 00:41:55,840
been just impressive. Frankly, it might be fair to say there's some change of paradigm to some

403
00:41:55,840 --> 00:42:01,840
extent in the way that we work with these different modalities or one modality in different tasks.

404
00:42:02,640 --> 00:42:10,480
So we've been thinking about in the case of, say, palm model from Google and work with Jeff

405
00:42:10,480 --> 00:42:18,960
Dean and others, how does it do what it does in that big giant yet sparse model, where does it come

406
00:42:18,960 --> 00:42:24,560
from? And if you, for example, fine tune the model to some other tasks, what changes, what really

407
00:42:24,560 --> 00:42:31,760
changes, what information resides and what right information gets lost. That's been something that's

408
00:42:31,760 --> 00:42:37,120
been on my mind too. And do you have early results from that work that you can talk about?

409
00:42:37,120 --> 00:42:47,440
No. Clearly an exciting and important area given the, as you describe the paradigm shift that

410
00:42:48,800 --> 00:42:57,840
LLMs are bringing about. Yeah. And I think, I think, I know you also, you also talk to Emily Bender

411
00:42:57,840 --> 00:43:02,400
about about whole thing, think about LLM. I think we need to be careful. And which is why I don't

412
00:43:02,400 --> 00:43:07,440
want to, you know, make claims about, oh, we can't have this preliminary results until I really

413
00:43:07,440 --> 00:43:12,560
confirm as a scientist, you know, really confirm, rigorously confirm, and triple confirm to see,

414
00:43:12,560 --> 00:43:18,640
okay, this is, this is really it. And then I share it. Otherwise, there's just a lot of information

415
00:43:18,640 --> 00:43:26,400
that may be lacking or to, to only fuel the hype around AI when sometimes it's fair to have

416
00:43:26,400 --> 00:43:35,040
the hype sometimes isn't fair. One of the things on that note that LLMs and transformers

417
00:43:35,040 --> 00:43:42,240
more broadly are unlocking is this whole opportunity to do multimodal models.

418
00:43:44,480 --> 00:43:50,320
What's the, the state of play around applying some of the work around explainability and

419
00:43:50,320 --> 00:43:59,200
understanding to multimodal? Yeah, it is in the, it is infancy. I think the difficulty, it's

420
00:43:59,200 --> 00:44:04,800
difficult to answer that question because while I see a lot of tools that we developed are on

421
00:44:04,800 --> 00:44:09,280
embedding space. For example, this concept activation vector or concept vector, we do everything

422
00:44:09,280 --> 00:44:15,120
on embedding space because we know that that's an efficient, tightly learned space that we can do

423
00:44:15,120 --> 00:44:21,360
stuff about intervene, do causal inference, it's a nice space. And that still exists in this

424
00:44:21,360 --> 00:44:27,520
model model. So it should extend. Ideally, it should, but then in a way, it kind of shouldn't

425
00:44:27,520 --> 00:44:33,760
because do the things that what they can do is so different, it must be that some different

426
00:44:33,760 --> 00:44:39,040
way of encoding information has happened in this large language model in one way or another.

427
00:44:39,040 --> 00:44:44,240
So in a way, I'm optimistic to where baby we can extend these techniques, but in other,

428
00:44:44,240 --> 00:44:50,400
on the other hand, we have to be very careful, especially careful because our human biases

429
00:44:50,400 --> 00:44:57,040
are amazing, something that we have and we cannot get over, can escape. And that means because

430
00:44:57,040 --> 00:45:03,840
that's excited about these advances, because I, the developer, already biased, when we work on these

431
00:45:03,840 --> 00:45:10,480
models, we have to be very careful, probably double blind myself when I do research to make sure

432
00:45:10,480 --> 00:45:15,360
that I'm not fooling myself. So that introduces additional challenge.

433
00:45:15,360 --> 00:45:19,440
It seems like appropriate caution coming from the author of paper that said,

434
00:45:19,440 --> 00:45:25,040
hey, all these things that we learned about explainably don't explain what we thought they explained.

435
00:45:25,040 --> 00:45:26,560
I learned my lesson in a hard way.

436
00:45:29,760 --> 00:45:36,640
You mentioned under the final category that you described that you were talking through in your

437
00:45:36,640 --> 00:45:43,280
talk around alignment. One of those core elements there is this idea of concept vectors and some

438
00:45:43,280 --> 00:45:49,440
of the work there. Can you speak a little bit to that? Yeah, yeah. It's a line of work that I'm

439
00:45:49,440 --> 00:45:54,880
hugely excited about and have been working on for years. So the overarching idea is the following.

440
00:45:54,880 --> 00:46:00,800
So prior to concept-based explanation, what we've been looking at are things like saliency map,

441
00:46:00,800 --> 00:46:05,600
where I put a number in the pixel, and that's what you're looking at. So if you want an explanation

442
00:46:05,600 --> 00:46:11,920
for a picture of a bird, then you look at a picture of a bird and maybe 10,000 other pictures of birds

443
00:46:11,920 --> 00:46:19,680
to see, okay, what are the common patterns among the class of birds. What concept-based explanation

444
00:46:19,680 --> 00:46:24,000
suggests is that let's not do that because that's not even how humans communicate to each other.

445
00:46:24,000 --> 00:46:29,360
I talked to you in words that in abstractist words and you understand you're nodding, right?

446
00:46:29,360 --> 00:46:34,720
I can tell you your glasses, you sit on a chair. I don't have to explain everything about each

447
00:46:34,720 --> 00:46:41,120
pixel in you that I'm looking at you to communicate what I'm saying. So we have a more efficient

448
00:46:41,120 --> 00:46:45,920
high band with communication. So why don't we build that high band communication with machines

449
00:46:45,920 --> 00:46:51,360
and humans? And this concept-based explanation is a first step towards that. So instead of pixels,

450
00:46:51,360 --> 00:46:57,920
let's abstractize and use the language that we used with the between humans. So fluffy bird,

451
00:46:57,920 --> 00:47:05,360
fluffy dog, or a striped, or a circular shaped biopsy and then cell that looks a little bit

452
00:47:05,360 --> 00:47:10,720
oval shape. That's how doctors can talk to each other. So let's use those extra-type concepts,

453
00:47:10,720 --> 00:47:18,960
extra abstract, more abstract concepts to have machines communicate to us. And the way we do it

454
00:47:18,960 --> 00:47:25,680
is to by building this concept activation vectors. So you say doctors, give me, I have this medical

455
00:47:25,680 --> 00:47:33,520
concept of irregular boundaries in the biopsy samples. I will give you some examples and we will

456
00:47:33,520 --> 00:47:38,720
map those examples in an embedding space, which gives us a vector. There are lots of different ways

457
00:47:38,720 --> 00:47:43,520
to do it, but we do it many, many times so that we can get some robust measure of this direction

458
00:47:43,520 --> 00:47:48,720
and the vector. And once you have this vector, you can do a lot of things. You can ask the model,

459
00:47:48,720 --> 00:47:53,680
okay, here's a direction in the embedding space. Do you care about this direction? If I move things

460
00:47:53,680 --> 00:47:59,840
a little bit, would your prediction change a lot? Because that means that then the direction

461
00:47:59,840 --> 00:48:05,120
influence system model in terms of first order derivative, in terms of sensitivity test.

462
00:48:06,160 --> 00:48:12,240
If I remove that direction altogether, what do you do? Do you do you explode and give up and

463
00:48:12,240 --> 00:48:19,040
go home? Or you're just fine, like it's as if it didn't have to exist. So by the core concept is

464
00:48:19,040 --> 00:48:28,240
simply the building in this complex concept around medical or human ideas into this one vector

465
00:48:28,240 --> 00:48:32,960
and then so that we can build that this vector can be a bridge between machines and humans'

466
00:48:32,960 --> 00:48:41,120
communication. In thinking about the concept we discussed earlier around language that

467
00:48:41,120 --> 00:48:48,960
linguistic constructs to enable machines and humans to communicate and these abstract ideas

468
00:48:48,960 --> 00:48:56,560
around concepts, does it support at all the call that some industry are making for us to kind

469
00:48:56,560 --> 00:49:04,320
of marry symbolic methods for AI and statistical methods for AI? Yes, absolutely. I know Rao

470
00:49:04,320 --> 00:49:14,560
is a proponent of this and I agree with Rao on this that at some level I think

471
00:49:16,080 --> 00:49:21,200
when our principles are just the only way and perhaps safest way to do a task,

472
00:49:21,920 --> 00:49:28,800
then I think symbolic based representations would be inevitable because that's just we decided

473
00:49:28,800 --> 00:49:34,640
as a human, we're the human, we're making AI machines. As a human we decided that that's just how

474
00:49:34,640 --> 00:49:40,400
it's going to go, especially might be important in high-stake decisions where we know that this

475
00:49:40,400 --> 00:49:47,840
is the way that we've been doing and we don't yet have better way to do it or we need more

476
00:49:47,840 --> 00:49:53,920
empirical evidence until we switch the way that we do things. Then I think that those representation

477
00:49:53,920 --> 00:50:00,000
would have to be part of the equation. We can't just do everything in freely distributed

478
00:50:00,000 --> 00:50:03,680
representation unless there's some breakthrough that could convince me otherwise.

479
00:50:04,800 --> 00:50:10,880
We'll stay tuned for that. Being, it's been wonderful chatting with you. Thanks so much for joining

480
00:50:10,880 --> 00:50:16,160
us and sharing a bit about what you're working on and what you're excited about and what you

481
00:50:16,160 --> 00:50:27,120
share it at. I clear. Thank you, Sam. Thanks for having me. Thank you.

