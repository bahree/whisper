All right, everyone. I am here with Luke Stark. Luke is an assistant professor at Western University in London, Ontario. Luke, welcome to the Twimble AI podcast.
Thank you. It's great to be here.
It's great to have you on the show. You are a ethics and AI researcher among other things and you've been a vocal critic of computer vision and facial recognition in particular, and I'm looking forward to digging into those topics with you.
But before we do, I'd love to have you share a little bit about your background and how you came to work in the field.
I was trained by training, by academic training, and I was not a big computer kid. I liked science fiction a lot, but I was mostly reading books. It wasn't too much coding or too much electronical engineering.
I ended up doing my PhD in media studies because I kind of got sick of the fact that historians sort of by convention often kind of stopped their stop their work about 20 to 30 years prior to the present.
That's changing a little bit, but that was that's kind of the tradition.
And so I did a PhD in media studies at NYU and ended up working with a philosopher of technology there named Helen this and bomb, who was at the time thinking about digital privacy until you just come up with a book about privacy and context, the idea that privacy is one of these things that isn't just about control of your data, but it's about kind of socially appropriate or socially contextual data transfers and data flows.
And this was about 2010. And as it turned out, Helen had been thinking about not just privacy, but other human values and computing systems since the mid 90s, which she was kind of part of a, you know, a fairly small niche bunch of folks who were in law schools in information schools in media studies departments who had been had been thinking about these questions.
And, and that's why I got along, I work with her, did a bunch of work with her.
And, and about 2016, 2017, all of a sudden, right, there was this huge upsurge in interest in machine learning, right, and also a huge interest in the social impacts of machine learning ethical AI.
And, and all of a sudden, it felt like all the things we have been talking about in this fairly kind of constrained corners of academia were things that, you know, government's interested in companies are interested in the general public is interested in.
And, and so that's that's been sort of where my work has been ever since I was sort of just lucky lucky to be in in on the ground floor, say, say.
And, that you have been critical of facial recognition, that seems like a good place to start one of your articles from a couple of years ago, describe facial recognition as the plutonium of AI.
And, in the, in the introduction to that piece, you kind of acknowledge that it comes off to some as alarmists, but then you proceed to back it up the analogy.
You know, tell us about that what, why that analogy and more importantly, why do you think that facial recognition is so dangerous.
That piece was inspired by some earlier work I had been doing with a wonderful friend and colleague and Lauren Hoffman at Washington on thinking about metaphors in first big data and then eventually AI.
And how often these natural metaphors get used right to talk with, you know, data mining data lakes, you know, people called data, the new bacon.
I mean, it was kind of a bit of a new oil, the new oil, exactly, I'm from bacon.
And, and so I was thinking a lot about data metaphors and I was thinking, okay, so, so you can use metaphors in a way, you know, to make a product seem or make data seem appealing.
But can thinking about these metaphors do something else, can it, can it kind of orient people towards, you know, some of some of the challenges and maybe problems with, with different kinds of AI technologies.
And the reason I came to plutonium was because I was reading a lot of it, I was really inspired by work in in race and technology studies right work by amazing scholars who have been thinking about the impacts of digital technologies on race and racism on racial categorization.
And how the internet has shaped, you know, has reshaped those conversations over the last 20 years, people like Lisa Nakamura, Simone Brown, Wendy Chun, you know, really, really amazing scholars.
And I want to say, I mean, I think my work is absolutely indebted to and builds on this really path breaking work of these other other people, many of whom are women of color, right, especially black women.
So, so right and what they've been arguing for a long time for decades, right, is that the, you know, the way that digital technologies kind of encourage categorization, right, either new categorizations of people or, or they kind of build in old suspect problematic, you know, racist categorizations.
That that has a kind of toxicity because these, these technologies are now so ubiquitous right there in our pockets, they're, they're making decisions about hiring and, you know, bail and, and all these important things in our lives.
So, and so, I take that, that, that evidence, you know, historical evidence, empirical evidence from sociologists really seriously, and I think the analogy of plutonium is being physically toxic is to facial recognition is socially toxic, right.
And, and then the, you know, at the time that I wrote the piece, I guess that was 20, early 2019, like 2018, the conversation around facial recognition was very different than it is today.
I just shifted a lot. In 2018, there were a few people in media studies sort of saying, this is a really challenging problematic technology.
What he heard so I'm going to have an challenger to legal scholars wrote a piece about the kind of the kind of the kind of democratic danger, the danger to democracy of facial recognition.
I wrote this piece and other people had also written other pieces, but, but I think in the last two to three years, you know, more and more jurisdictions, right, especially municipalities in the states and elsewhere have really come to say we, you know, we don't want this kind of this kind of surveillance being used by government.
There's often mandates to that the police don't use it for obvious reasons, right, because it is so tied up in this history of, of, you know, of unjust policing of policing of minorities, especially black Americans.
And, and so I think I think that social toxicity of facial recognition has become more people become more aware of it, right, they see it in more parts of their of their everyday lives, their, you know, there are stories about how how it screws up, right, how it doesn't it doesn't do the job of capturing certain kinds of faces.
And how it can be used to, you know, repress dissent. And so I think all of those things have meant have meant that social toxicity is increasingly clear.
The, the distinctions that you make in the piece in your work generally is that the, your critique is not so much that, you know, it's used improperly like we've seen examples of local policing organizations use facial recognition improperly and they could have used it better perhaps.
And, you know, it's not necessarily that it's used in the wrong domains, but rather your, your take is that it's kind of fundamentally, it's fundamentally dangerous and so dangerous that it shouldn't be used or should only be used in very, very specific highly regulated situations analogous to plutonium.
You elaborate on on where that, where that contention comes from.
Yeah, sure. And, and I, I have this, I have this, this kind of kind of folksy folksy analogy, I'm becoming analogous, I guess, of, of thinking about a rotten onion. So take a take a rotten onion, right.
You got the onion, you know, it's got these layers and let's say, you know, you take the skin off the onion and the first, the first layer of the skin is of the onion is rotten. That's, that's kind of analogous to the kind of concerns about use and deployment official recognition, right.
So, okay, so, right, we have all these problems with it being being used by the wrong people in the wrong hands for the wrong reasons.
So let's say you say, okay, I accept those, those problems, I accept those critiques, right. I'll peel that first rotten layer of the onion off, right. Hopefully, maybe the next layer down will be better.
Then the second layer of the onion are the, the kind of concerns about technical bias that that have been now lively are articulated about about facial recognition.
So, right, so, so Joe Boilwini and Tindy Jebrew's work on gender shades, right. You know, makes the point that a lot of computer vision data sets and facial recognition data sets are not trained on a diverse set of faces.
So, they're, they're picking up, especially dark skin women, much less precisely than they are picking up white men, right.
And there have been lots of studies, and this is a big critique, not just recognition, but things like analysis, right. One of a great scholar Lauren Rue at Institute of Maryland has talked about this and the kind of emotion recognition that emotion recognition recognition recognition systems recognize white smiles better than they recognize black smiles, right.
So similar, similar thing. That's a much harder problem to solve, right. Because of the, you know, the challenges of getting contextually appropriate data, the challenges of having having diverse data sets, the challenges of our privacy, right.
You know, so there's a whole bunch of technical problems. But let's hypothetically say, and I don't actually think you could fix this, but let's hypothetically say you could fix it. Let's have hypothetically say you could peel that second layer of the onion off that rotten onion.
So now you got like the third, like the core of the onion, right. And to me, this layer of the onion is also rotten in the context of facial recognition because conceptually facial recognition systems, right.
Even though they're not, you know, they're not making any, you know, the people are used to making the judgements about what the technology is doing. But the system itself is designed to put, put numbers to faces, right.
Put, quantify the face and put that face that's quantified in conversation with other faces, right. So, so even if you are doing simple straightforward identification, right.
You're still, you know, you're still identifying the fact that there's a face pattern there in the first place, right. And when you start putting that into like databases, and you start looking at how much that kind of comparison of facial vectors, I guess you could say
what happens, that kind of quantification of the face, even if it's not intended to be, is really, you know, a big chunk of the textbook definition of racism, right. Racism is about, you know, using a kind of external physical quantification, physical, physical features, and making judgements about an individual based on those features, right.
But in my view, almost every kind of kind of facial recognition technology in some way participates in that kind of quantifiable judgment.
And that's why I think that's the third part of the onion is rotten. And I think right if you, and then if you get rid of that, if you get rid of that third part of the onion, you have no onion left.
And so, you know, there's, there's no point and there's no point in investing in a rotten onion. If every layer at every level, there are these instrumental problems.
So that third part of the onion, the argument there is that kind of digitizing faces and just any kind of judgment or analysis or decision making based on the digital digital model of faces is.
Yeah, the argument is that any kind of digital model of, really, it's not just the face, it's the human body.
More generally, right, is open to exploitability as a kind of around racism, right. So it's, it's, it's, so the textbook definition of racism, right, is using exterior physiological traits, physical traits to make judgements about.
You know, the inner, the inner self, right.
And so all of these systems are doing that first part of that definition, they're all, they're all mapping, they're dependent on the kind of quantified mapping of extra physiological traits of the face of the body of whatever.
And unless you're not doing anything with that mapping, unless you, unless unless it's purely a descriptive matching mapping, right, which is not, I don't think really how, you know, that's not very useful in a lot of the ways these, these systems that deployed.
And then the system, you know, is being asked by its designers, is being designed by its designers to make judgements about the faces, the bodies, the digital profiles that it, that it's, it's taken in.
And potentially to, to make put them into a hierarchy, right, potentially to classify them, potentially to, you know, to add, in the case of facial analysis, right, to say, you know, to make a prediction about, about age or gender or whatever.
And I just think, I just think right that, that, that so right, you have, you have there, both the kind of, the kind of textbook definitions of racism, it's quantified external features, and, you know, being, being being judged in some way, and put into kind of hierarchies, whether deliberately or not, right.
I think one thing important to say here is that I don't, I don't think many, you know, technologists and designers think they're being racist, right, they're not, they don't necessarily have racial animus.
But, but one of the, one of the things I take from, from the work of somebody like Wendy Chun, who I mentioned, right, this amazing theorist of race and technology is, is that racism itself, race as a concept is a kind of technology, right, it's kind of a technique, right, it's a technique that the powerful have always used in lots of different context, of course, in the American context, very specifically often around black Americans, but, you know, racism more broadly, right, between to non white people.
It's a technique, a tech, a classificatory technique, right, it's a race, race as a concept, you know, puts people into particular orders, right, it, it, it, it works a bit like, you know, conceptually, what facial recognition systems or any, any kind of recognition system that is, that is mapping the body and mapping many bodies and putting them into kind of into these databases is doing, doing in a dumb way.
Right, doing in a way that, that is, that is just, that's what it's meant to do. It's, and I think it's important to think about that, that, that kind of systemic definition of race and racism, right, that idea of race as a technique, because, you know, you can, you can be using that technique and not be realizing it, right, you can not maybe not realize that technique is threaded through, you know, the, the kind of classification systems that you're, that you're working with.
So you, you seem to be suggesting that that, you know, third core layer of the onion is problematic because it's, you know, or maybe, maybe put another way that facial right, all facial recognition is fundamentally racist.
Yeah, yeah, if you, if you, yeah, basically, if you want to, if you want to boil it down, yeah, just trying to get to, like, so does that extend to, like, face unlock for your phone, is that fundamentally racist.
I think it is, I think it is because just by the act of deciding to put numbers to your face, right, Apple is, or whoever's using face unlock, not to pick on Apple face, you know, these facial unlock systems are,
doing a kind of judgment about what a face is, right, they're saying, okay, I'm going to, I'm looking for a particular pattern of light and dark of, whatever.
And I'm going to define that pattern as a face.
And that's, that's the edge case, right, the edge case that that kind of simple one to one recognition.
um is is like the probably the the kind of right that the the the argument that that that's that
that's racist is intrinsic to the the quantification aspect right it's it's not because it's it's one
to one you're not necessarily putting it in conversation with other faces right and we talk about
that you know we talk about this in in this paper physiognomy AI that I recently um written
with um a great legal scholar Joanne Hudson we talked about that as the edge case we say there are
some I think some good kind of philosophical reasons why even just the identification of the
face with numbers you know is it kind of kind of kind of suggest right it kind of it kind of pushes
you towards this kind of racializing classification scheme and certainly when you put multiple faces
together then that that kind of produces and what is it about the numbers that inherently gives rise
to racism what about other types of judgment based on faces like we go through life all day
making judgment based on faces does that mean that human existence is fundamentally racist
no but I think human existence is fundamentally fundamentally based on on these kind of these
kind of inferences about people these conjectures about people this is a great question right this
is actually often often a response that you get when you talk about these these problems okay you say
okay um you know yeah people do this all the time people are always making judgments about people
based on on what they look like and their face and there are two a couple things to say about that
first right there they're sort of right these these two battling battling uh folks saying right
you know you can't judge a book by his cover versus you know you you never get a second chance to
make a first impression right so there's this kind of there's this kind of um opinion is divided
right about just how useful these individual inferences about people are right and there's been
lots of work in psychology that talks about how much stereotype and other kind of other kind of
incorrect inference permeates what we're doing a lot at the time in in in our daily lives
we're much better at making inferences about people if we know them well right so I uh you know
I know I can I can kind of infer things about my my mother what she's thinking or what how she's
feeling that I wouldn't be able to infer from a total stranger and the the real problem right then be
kind so there are so there are all these problems with this kind of individual personal
inference um lots of books have been written about it right lots of songs were written about
missing etc okay but when you try to scale that right when you try to say okay we're we're going to
make a science out of this we're going to find kind of regular and repeatable patterns in these
in in in faces we're going to infer from one particular person's you know whatever you know some
some set of things about them with some degree of accuracy right that's that's what that's when
it all breaks down right because it's too brittle right it's it's not you you're never going to be
able to um capture the you know capture the the kind of the potentially the reason why that that
inference works and that inference even if it's 70 percent you know accurate at a population level
is also never going to be really applicable down to the individual level right this is what
called it's called the ecological fallacy and statistics just to to be clear so you're you're
shifting focus in the conversation to the paper you did which is the physiognomic AI and that is
broadly a critique of um you know these kinds of studies that will try to look at faces and determine
you know any number of things what are some examples of like the most egregious things you've seen
sure well so so race is one of them right I think I think so in terms of my own trajectory right so
thinking about race and racial race and facial recognition right well well it's not just race or
ethnicity that these systems report to predict claim to be able to predict they claim to do be able
to do things like predict your gender your age your emotion uh your sexual orientation your
political leanings right there's a hole there's been a whole raft of of kind of dubious
machine learning you know kind of kind of prediction studies um and I think I think you know I think
race is a really critical example here because it's you know it's exactly what we're talking about
but but at a bigger you know that physiognomic AI and the problem is that at a scale that's not
just about race right it's looking at these um you know these these these various identity categories
and trying to make assumptions about that identity category um in ways that at that macro level at
that level of populations that then gets translated back down to the individual um can often be
wrong right it can it can be it can be an accurate um and if it and whether or not it is accurate or
not it can you know if it's used in various ways it's it's it's hugely damaging my general sense is
that most of the technologists I interact with look at these studies you know very critically
to say the least um and wonder how they get published uh but of course people write them and
publish them yeah you know and and I think there's a there's a whole conversation to have about
an interesting conversation to have about you know basic machine learning research and applied
machine learning research and how how even when researchers don't quite realize it the line
between them is very very narrow right um I so I was a I was a postdoctoral researcher at
Microsoft research in their fairness group for two years and I mean I really saw this first hand
right a lot of researchers you know I have good intentions they um they actually really want
tools and scaffolding to do a good job and to not not you know trace been to these problematic
deployments um and and so there is a kind of a kind of a set of structural questions around
research but it's just right it's so easy to to productize this stuff right um I'm thinking of
I'm thinking especially of of these various smartphone filters and apps right where you know
they they say oh we'll we'll turn your face into an 80 year old man's face or we'll we'll more
fit so that you look you're you look uh you know stereotypically female or something right um and
that's that's um you know that's really easy to do but I think it's also another example of how how
ubiquitous like this this this this kind of trying to kind of this digitizing of the features and
and kind of and kind of this kind of um this categorizing of features is becoming because of facial
recognition and I and I think you know that those those filters okay maybe they're doing good fun
except for the you know when uh you have a filter that says make you know the beautifying filter
and that beautifying filter um the actual um visual effect is to lighten your skin right or it's
to change the shape of your face uh so that's that's a huge problem um yeah so that's that's that's
that's I mean I and I and I agree I mean there are there is a lot of a lot of good internally critical
work in machine learning and in the tech sector and I think increasingly large amounts of that I
think um I I was asked to give a give a talk about this stuff at CVPR um really this year and
yeah and there was and there was a lot of you know a lot of interest and a lot of a lot of thoughtful
conversation about it um I think that the you know I think I think that there there is a real anxiety
on the part of a lot of researchers about censorship written about sort of being told what to
research and what not to research um and I think that anxiety you know I think that anxiety
is understandable but I think if we all stop to think for a little bit there are lots of boundaries
on on what kinds of research we do right and there are lots of kind of ethical rules and guidelines
and in every academic discipline nobody's just kind of just you know out there kind of plowing over
uh you know every every every social constraint and so I think um you know because there's been
this body of work um that thinks things seriously about about things like classification um I think
it's time and I think it is I think it's good that it's it's now being thought about in the theater.
Yeah we we've talked quite a bit about kind of the I don't know if you call it changing
perceptions but kind of the broader context in which um we're evaluating facial recognition
we've alluded to some of the shifts that have happened in the acceptability of using facial
recognition and policing how would you characterize the I guess the regulatory landscape or the
the legal landscape I know that's a topic that you're interested in and of course it's you know
there's many many municipalities it's hard to do broadly but what do you think are the kind of
the major landmarks um recently yeah so yeah municipal bands and moratorium on facial
recognition have been a big a big kind of part of the regulatory landscape of the last couple of
years in the US um uh cities like I believe like Oakland like Cambridge, Massachusetts um and then
and then also uh you know new focus on uh laws that protect broadly things like biometric privacy
right um the Illinois biometric privacy law um is just one of the strongest in the states
and and it has been like very very heavily attacked by various um big tech firms because it stops
you know various kinds of um of filters various kinds you know various kinds of of facial facial
tech from from being deployed or used in Illinois so I think that's been notable I think that
um there has of course been in the the American context a lot of conversation in Congress about
tech regulation broadly um you know unclear if we'll actually see any legislation but um what
has been notable is the kind of recent set of high rings at the FTC right the Federal Trade Commission
which has sort of become the kind of de facto tech regulator um it has been for a long time around
privacy because of of its mandate to um you know to deal with unfair or deceptive business practices
of which there have been sort of many many in the context of of kind of privacy breaches and
dark patterns and that kind of thing and so I think that that the FTC is really gearing up to think
about all these questions kind of holistically not just privacy but also um and this is something we
argue in we argue for in in this in this recent paper but for um you know understand understanding
things like biometric classification and physiognomy classification as potentially being
per se unfair and deceptive um precisely because right you're doing kind of a classification
on on an individual right that is is is is making assumptions about the individual right that
that may not be accurate right no no classified classifier is 100% accurate um and so that may be
both unfair and deceptive right depending on uh on the context yeah yeah when you looked at the
physiognomic AI landscape what did you find was the kind of the motivator behind that work was it
you know people with the data set and just playing around was it um yeah I don't know you know
I look at a lot again you know we look at these and like you know you see them pop up on Twitter
every few months and it's like okay you know once again correlation not equal causation blah blah blah
um and I I'm curious you know where they come from in terms of thinking yeah and I I think
and I think there's a there's a kind of complex like overlapping set of motivation I think that
the kind of the world of academic prestige and reputation and controversy and and PR
uh does a lot in in feeling some of this work right I think you know if you right if you publish
this study that makes this kind of big claim about being able to tell you know tell somebody's
uh sexual orientation via via a data set or by by facial recognition or political orientation
right you're gonna get written up in all the tech press right um you know it's part of the kind
of the kind of ecosystem of content and clickable headlines and everything in it you know I think
I think that I think right I think they're so so sometimes there's there's there's more more
serious work or more interesting work um that gets written up in a sensational way and then
sometimes there's work that kind of that kind of I think is is aimed at kind of getting
sensational headlines um so I think that's part of it I think um and I you know I also I'm really
aware of of the of the kind of very the very heated nature of of ML research right I mean these are
there's a lot of money involved um you know there's there's there's all this back and forth between
the private sector and universities um everybody's incentivized to make big claims right and and
and and especially the press is sort of incentivized to not be careful about either the claims that
they make or or checking the claims that people make um and then you know and and then governments
pick up on it and they sometimes they sort of believe it right or or you know people people
you know companies believe it customers believe it you want customers to believe it because you
want to sell them the technology at a certain level and I think you know I think that's that's a
lot of a lot of where this is all coming from and this kind of this kind of kind of swirling hype
world um around machine learning um and and this desire right this desire there's a real desire to
want to um peel back the skin and like get to the truth of of things about people right that's
that's not a that's not a desire that started with machine learning machine learning is just the
latest tool that that kind of keeps that that kind of desire going right um in the paper that
you mentioned the dynamic AI you know we talk a lot about the 19th century um you know
computers then but um a similar set of desires to kind of try to like identify people from their
exterior traits right a similar set of desires um to uh to you know to to categorize people to
kind of understand who people are and what they're doing um and I think I think that um I think that
that longstanding that long sun and kind of desire for that certainty I think that actually that's
actually the core problem right it's that these technologies are the symptom but this huge huge
unease about about that you might have to like actually you know engage with somebody and find out
their perspective as opposed to running some sort of sensor over them and getting the information
about them I think that's that's a much longer history um and that's and that's why I think
actually being a historian in this context is really helpful because right the new the tech is new that
that kind of ideology or that kind of that kind of you know um wish is not
in the in that paper did you find examples or in your research broadly in the area did you
find examples of commercial products that are based on uh this research ideas there have been
lots of applications of of of the the kind of simple emotional analysis for instance right
and things like hiring right so higher view it was a famous case right they they they have claimed
that they've stopped doing um a lot of the the kind of facial recognition um or facial analysis
work on emotion but they um you know there there's still a lot of concerns about hiring in general
um there are um a lot of really alarming um examples in the in the kind of sort of law enforcement
national security space both both in the end states and overseas right about around kind of doing
these analyses making judgments and assumptions about suspects um and and there's an increasing
interest in in using these technologies in education right um the pandemic has really meant
right a lot more screen time and and a whole bunch of companies have have sort of moved into the
space promising analytics to help children learn right which which involves analyzing their
attentiveness analyzing there you know their demeanor uh stuff like that so you know it's it's a
kind of it is a quite a heterogeneous set of technologies and you know not not every technology
um is seeking to classify or identify the same thing uh and so in the paper we we tried to kind of
get a as high level a kind of definition as we could to kind of capture the broad range of um
um of where we see the tech going so I wanted to kind of introduce into the conversation some
late breaking news this is not characteristic for the this podcast but since we started our
recording Facebook announced that they're going to be shutting down their facial recognition system
obviously you haven't been reading that but you've been following what the company's been doing um
and any thoughts on on that yeah that's interesting um I think that's I think that's first of all
indicative of how toxic the term facial recognition has gotten um also apparently how like
how ubiquitous somebody was somebody the other day was mentioning that they
that that people sort of sometimes he kind of use facial recognition as synonymous for any kind
of computer vision right oh we use facial recognition to identify this guitar that's interesting
but any who facial recognition you know as a as a as a as a term is clearly really really toxic and
clearly something that the tech companies don't want to be involved with um Facebook uh is of
course um rebranding itself right it's it's it's it's it's it's made had a lot of um a lot of press this
week uh going to to meta um talking about about the the metaverse what's the metaverse the metaverse
is virtual reality right the metaverse is about about um it which includes creating digital
avatars of of users right Facebook um has been researching how to do physiological scans of bodies
to produce more more precise for accurate avatars for some time and so uh Facebook may be shutting
down some aspect of its photo facial recognition right but Facebook is still very interested in
collecting data about the body and using that data to collect information about its users to
to to model it um in fact VR technologies collect enormous amounts of of of data not just about
the face right but about heart rate movement uh right you name it right there they're they're
doing a very granular um uh scan of of of uh of what what you know how you're moving how you're
acting um it's interesting that this comes up actually because there is a whole connection with
these conversations around facial recognition and physiognomy and stereotypes with animation
actually right animation in terms of you know cartoons in terms of CGI right um and one of the big
challenges with animation is that the easiest way to convey emotion and animation is through
a stereotype you know it's through some kind of physical you know big eyes or or it's tiny mouth
right and so you get all of these conventions all these stereotypes um from animation that
that begin to inflect how we think about people you know how we think about people's emotional
expression and the other kinds of expression um and now we're getting to the point right where
where social media companies are trying to uh collect you know to try to animate
users right for their own for their own ends um so um an interesting move by Facebook but but
not not maybe totally shocking or unexpected you also raised an interesting point that
while they are shutting down facial recognition in many ways their stated direction requires
a lot requires them to do what you find most uh most frustrating about facial recognition which
is digitize the body and uh make representations of that and maybe in some other universe or some
digital world right some some digital representation exactly and and you know when a bunch of companies
have done this over the last couple of years right so uh what did Amazon said they were going to
do a moratorium on using their facial recognition systems and systems for police departments okay
let's if that's very narrow you know all these all these announcements from various companies sounded
good they got good headlines right if you dug into it they were you know either they weren't really
in that business anyway and and so there was no skin off their back to say they weren't going to
be in any more right or they or it was a very limited part of of what they were what they were
inviting so um yeah yeah I um I think that's I think that's deflection right on Facebook's part
and I think again that's actually partially why um why we we really wanted to go beyond facial
recognition right in this paper and say um there's this whole class of of classifying
technologies and classificatory technologies and um and because you know you know because we
we don't you know we don't want to play black and roll all the time right people in tech off
and complain that that regulators can't keep up with the technology but part of that is because
of the argument that every technology is new even if some of these technologies have a lot of
common denominators in terms of conceptual basis or the technical basis um and so that's that's
kind of I think you know I think I think a ban on on physiognomic AI analysis yeah would you
know you could apply that to thinking about virtual reality avatars especially if those are
grounded in digital data um and if they're being you know if if if if machine learning is being
used to extrapolate or you know that animation do you have a goal in terms of kind of a regulatory
framework or or thought in terms of what that needs to look like to to be effective yeah so again
this is where the historian hat comes in kind of handy uh I think that right as I sort of have been
saying this problem this problem of of trying to infer things about people at scale is a pretty old
problem right it it it's at least a couple hundred years old and it it you know it for a long time
was was sort of felt by two groups of people broad groups of people experts in the lab right
scientists of various kinds and the kind of the kind of often marginalized populations who got
experimented on right whether that's very broadly right whether that's people you know people
people in colonial you know in in colonized societies who were who were getting you know getting
you know 19th century scientists administrators you know going in and doing stuff to them
whether it's poor people in you know in in Europe and North America right so right those marginalized
people you know understood the in the kind of injustices of these sorts of classifications very well
but you know the kind of the kind of the kind of privilege you know and I count myself as part of
this part of this group right kind of privileged in North America you know white white middle class
people they didn't really see you'll always see the kind of the kind of negative side of classification
and modification so now with AI being so ubiquitous in being being deployed right you know in white
color professions right in in all in all sorts of different spaces um I think it it just means
that that the injustices and and power asymmetries of of classification of the of the body are just
are now more apparent to people for whom they weren't always apparent before um and so right what
so that means it's an old problem that that people are suddenly waking up to not some sort of new
problem and so I think the the end game in terms of regulation is to think really carefully about
when you do inference about humans at all right what you know what how what kind of
if you do is that inference being done solely for research right is is our certain kind of
inferences that are done for research you know is that does that put that research off limits for
commercialization um how does you know how to how to governments how to states you know handle the
kind of data that they have under their control and the kind of the kind of you know inferences they
make based on that on that data about humans um I think that's a conversation that that is
happening has happened happening sort of in patches but I think as an overarching theme needs
to happen more um which is something I'm thinking about um right because not not all
um inferences physiognomic but all physiognomy is inference right so there's a so there's
physiognomy AI that's a class of category we we know there's a problem that's what we're talking
about in the paper there's a higher level class of course category about about doing any kind of
inference about data about humans that I think there's worth the beginning of a bigger conversation
about maybe going all the way back to the onion to kind of wrap things up
uh uh what would you point you know me or or anyone else who's not quite bought into that third layer
of the onion as being you know fundamentally fundamentally wrong like I can buy that you know
MLAI facial recognition is misused all the time I can buy that people you know that the technology
is not quite there yet and is problematic in its current state um you know of applicability
particularly in some use cases I'm not sure that I'm fully uh fully bought into you know digitizing
faces digitizing bodies is fundamentally wrong or fundamentally racist like for for that matter
what what what the best book or books or articles for us to take a look at yeah so you could start
there's a famous book by Stephen J. Gould called the mismeasure of man that's a good introduction
to thinking about chronology and physiognomy in 19th century and and kind of thinking about that
this history of of why people's bodies get quantified in that way um the work of yeah so the work
of some of the people I've already mentioned um of Wendy Chun who's uh at Simon Fraser University
up in Vancouver talking about race and racism as a as a technique or as a technology um work of
Simon Brown um at UT Austin who talks about the sort of history of classification and racial
surveillance of black Americans going back to the slave um you know the slave people and to
and to kind of anti anti anti escaped escape slave laws uh it's been a long time since I've come
across the the gold book but my my immediate reaction to those is that we're kind of circling back to
layer one of the onion which is the use like I get racist use to facial recognition as racist
but like fundamentally any digitization digitization of a face as being racist like
right so well so if you again right if you if if you take the definition of racism as being
assigning you know assigning some some metric to an external feature
and using it to um using it to make a judgment about the person right I mean that's that's in
some ways maybe yeah maybe maybe we've done a kind of like a full circle loop right that's that
is both a use case but it's also fundamental to what these systems do right so it's it's
conceptual it's the concept that they can the point it's almost technological right the point
of facial recognition is to assign a set of numbers to a phase and make a judgment about that phase
right that's that's it and if that and that's like that is the textbook definition of racism racism
being you know making a judgment about about you know you're you know your you know your skin
your your your external your external self so yeah so I think it in some ways it does loop around
but I think that's part of the problem is that is that you know people people don't always get
that that's what that really at bottom that's what racism is right it's it's not it's not being
you know you know being a rationally you know it's not being you know being a rationally
bigoted or something right racism is is a technique right it has this like tech you know this kind
of materiality to it and so you can you know yes you go west why you can be you can be racist
without realizing it right you can be racist you know without with the best of intentions which
kind of ties into bigger conversations for having these days in society and so I think you're
you know your point is right but I think I think it does come around to that to that that
fundamental question of what right what conceptually is putting numbers to humans about
and I think I think and I think I think look I think there's a there's a question here interesting
question here about um you know I mean okay so you could say let's say like in some medical
cases right you want it you maybe you want it you want to you know quantify quantify the
spleen or something or along right right is that is that racist um well there's certainly a lot
of potential for race racism or racial disparity to come into those medical data and there's
a lot of studies of that is that is that quite the same
jury's still out right so I think right so I think I think that's a fair point right that
that that you know quantifying it a lot of a particular organ on you know we that's something to
talk about um but the face right the face is so central to the way we identify each other and
the way we understand each other socially that I don't think you can I don't think you can quantify
the face without without stumbling into that problem. Well there's no question that uh it's
dangerous no question that it is uh you know lends itself or is available for misuse no question
that it's easy to do wrong uh and that we've seen a lot of examples where you know data sets and
now rhythms and all these things contribute to to misuse um so uh I'm glad that food food for thought
food for thought for the audience folks like you're out there you know keeping an eye on it uh and
I'd love to get your tape once you get a chance to actually dig into this Facebook news
um interesting timing. Go kidding well it's never never a dull moment in this business I have to say
boy kind of wish kind of wish there'd be a down week but hasn't happened yet.
Awesome thanks so much Luke. Thanks so much.
