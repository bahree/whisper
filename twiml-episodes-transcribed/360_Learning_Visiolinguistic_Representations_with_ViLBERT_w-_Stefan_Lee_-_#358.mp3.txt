Welcome to the Tumel AI Podcast.
I'm your host Sam Charrington.
Hey, what's up everyone?
Unless you're Jared Lido or on the cast of Big Brother Germany, you've undoubtedly seen
your world shift dramatically over the last few weeks.
For most of us, this is simply uncharted territory.
And unfortunately, we aren't quite able to see the finish line just yet.
Now more than ever before is the time for us all to lean into our various communities.
We encourage you to take this opportunity to connect with your fellow human, albeit virtually.
And please know that the Tumel community, which is over 3,500 strong on Slack, is here
for you, and we welcome you to join us.
Take this chance to join a study group, work on a Kaggle competition, share your thoughts
on a paper you're reading.
We'd love to hear what you're working on and offer our support.
For those who might just need a friendly hello, I'm here for that too.
If you're so inclined, feel free to reach out via Twitter, our Slack channel, or just
shoot me an email to Sam at Tumel AI dot com.
Please be safe and take care of your health and that of those around you.
Peace and love, and on to the show.
All right, everyone.
I am here at NURPS 2019, and I am with Steffen Lee.
Steffen is an assistant professor at Oregon State in the School of Electrical Engineering
and Computer Science.
Steffen, welcome to the Tumel AI podcast.
Yeah, thanks for inviting me.
Absolutely.
Let's talk a little bit about your background before we dive into some of the many things
that you're presenting here at the conference.
Sure, sure.
So I did my PhD at Indiana University, and most of my work there was sort of on the core
computer vision side.
So how do I use computer vision to help scientists do various tasks?
So a lot of it was replacing what would otherwise be human labeling tasks.
Okay.
I get bored somewhat quickly, so in my postdoc I extended out to vision and language,
so I'm thinking about problems where an agent has to reason about visual input and linguistic
input.
And not only understand the visual content, but it has to express that understanding by
creating language or responding to language, things like that.
And is visual question answering one of the tasks that is interesting to you?
Yep.
So visual question answering, I've got a number of pieces of work on that topic.
Okay.
There's also things like captioning or doing visual dialogues from multi-round Q&A style
dialogues.
Okay.
Cool.
And then lately I've been extending out to tasks that not only have vision language,
but also some form of action.
So these are vision and language tasks situated in embodied context, where an agent tests
to see and talk and move to accomplish some sort of task.
And are the agents that we're referring to here, are they simulated agents?
Uh, so mostly they're simulated agents, but some recent work has sort of extended out
of the simulator into physical platforms with some surprising success.
Okay.
What's an example of a platform that you're using for simulation or for the robotic lab?
For the robotics on the robotic side, is they, uh, what's the kind of dimensionality,
how complex are they?
Yeah.
So most of the work we've been working on so far has been on this pie robot platform, which
is something that Facebook has recently released, which is this wonderful low-cost, robotic
platform, a low co-bot, I think they call it.
Okay.
That has a really nice interface for machine learning practitioners.
Oh, really?
So I have not come across that yet.
It's worth looking at.
So it's, you know, you can say from pie robot, import robot, and then robot, go forward
one meter.
And that's sort of the level of the interface, so it's a real machine learning persons robot
for things like navigation and grasping.
So here at the conference, you've got a number of presentations that you're, and posters
you're involved in, uh, we'll talk about, uh, one of them in most detailed billboard, uh,
but what are some of the others that you've been focusing on?
Sure.
So this year I'm presenting Ember things that you said, uh, one of them is one of these
embodied tasks.
Mm-hmm.
We work on what's called vision and language navigation, where an agent is sort of spawn
in a never-before-seen environment, and given a natural language and navigation instruction.
So it was something like go down the hall, turn left at the wolf head, and stop on the
third bedroom, the one with the yellow bed spread or something like this.
So instructions are this mix of trajectory clues, like go forward and turn left, and, and
visual grounding landmarks, and then the goal is to have an agent that can reason about
all this while actually following that path in a simulated, uh, world.
So I am.
And what are the, uh, priors that the agent's bringing into this environment?
Does it already know what a wolf head is, or is it need to figure that out from training?
So it's a tricky question, um, a lot of vision and language, and we'll touch on this in
Villebert, uh, sort of starts from a set of pre-trained image features, and a set of
pre-trained language features, uh, what it doesn't have is a sense of grounding.
So it may be able to represent a wolf head visually, and it may understand the word
wolf head, but the connection between sort of the visual incarnation of a wolf head and
then the term, um, isn't there.
And that you expect to learn during the specific task, um, though in Villebert, the point
is that we're trying to pre-trained grounding itself.
And so you've got the agent's paper.
Yep.
And then there's Villebert, and then I'm also giving a talk at a workshop on emergent
communication.
So if you have agents that interact to perform some task, and you want them to share
information between each other or communicate, uh, how could you make that communication
protocol more interpretable to humans?
Mm-hmm.
And one way to do that is to make them actually use sort of discrete symbols.
So something that looks like a word, uh, more or less, that doesn't necessarily have
that meaning.
And then the talk is about how do we make these, uh, these communication protocols more
interpretable?
You know, this whole field was kind of, I don't know, if popularized as the right word
or, uh, villainized a few years ago with the Facebook, uh, those two agents that were
said to have developed their own coded language.
Yeah.
Uh, similar kind of vein of, of research here.
Um, yeah, except for the fact that we, we want to understand what the code is.
Right.
Um, yeah.
Um, yeah, that was this, uh, dealer, no, dealer paper, um, right.
I had some similar, similar goals.
Yeah.
Yeah.
Um, and so it's, in, in a sense, kind of merging the field or the desire for explainability
into this emergent communication work.
Yeah.
Yeah.
And it, um, if you're thinking about building an agent that eventually will work with
a human, uh, communication ends up being a big part of that.
Right.
That's how we organize ourselves.
So making sure that we can understand what the agent is saying and that the agent can
understand what we're saying is sort of one of the goals of my research.
But not constrained necessarily by English allowing the agent to come up with its own stuff.
If we can map it back to some space that has human-like structure, that would be fine.
Right.
Um, so rather than having a unique, unique word for the concept of red ball would prefer
the agent to have a word for red that modifies a word for ball.
So even if it's not English, it's something we can map back to a way that we understand,
uh, how language works.
And so, Wilbert, uh, what's Wilbert all about?
Yeah.
So I hinted on this a little bit ago, but Wilbert's about learning the associations between
the visual incarnation of a concept and the linguistic incarnation of a concept.
And this is sort of tricky because most people instantly hallucinate the visual part
of something whenever they hear the other word when it's when I said wolf head, I assume
a lot of people immediately thought of like Game of Thrones type of things.
Um, and there was some visual concept that got brought to mind.
But the machines don't by did it sort of automatically have this, right?
You can, I mean, you can learn language in just in a linguistic context.
And in fact, that's what most of national language processing does.
Yeah.
Is just learn it, it's an association with other words.
Uh-huh.
Likewise, on the visual side, you're just sort of learning to represent some sparse set
of classes and those classes often relate to specific nouns, but they don't have a sense
of closeness, right?
So there's no idea that the feature for a cat should be close to the feature for a tiger
because they're related linguistically or in certain taxonomies.
So the point of Villebert is to try to learn these associations between vision and language
directly.
And this is something we'll usually call a visual grounding of a word.
Okay.
And so what's the, the general approach there, presumably it involves, uh, Burt, transformer
models.
Yeah.
The name is not as well.
Um, yeah, so if you, this, if you look at the Burt models and some of the big successes
in an LP, it's these large self-supervised tasks.
So they take a large language corpus and they learn certain little things to build supervision
from unlabeled data.
So they'll, you know, they'll mask out a few words and have them reproduct it based
on the other linguistic context or their last of a sentence follows another sentence
in text.
And what we do is we, we find analogs for that in the vision and language space.
Okay.
So there's this data set called conceptual captions that came out recently, which is this
massive data set.
I think it's about in the order of about three million, uh, image text pairs.
Oh, wow.
Where they just found images online that had alt text.
So some human had provided some alternative text for people, usually for people with visual
impairment.
You might interact with this by mousing over an image.
Sure.
And it produces some, some text tag.
Uh, they did some processing on top of that, but it's this sort of Webly supervised data
where they scraped all this down.
You know, the images filtered for kind of simple, simple kind of image net types of depictions
or they just raw whatever's out there on the web.
They're across the board from everything from, you know, flicker style images to pictures
of maps and things like this.
So it's pretty, pretty broad, but that's sort of where we start as our data source and
we perform similar self-supervised trainings where we're masking out certain image parts
of the image, just at random, and then asking it to reconstruct those given the rest of
the image and the language.
And likewise, we're asking, does this sentence match with this image or not or masking out
parts of the language and having it reconstruct from the image and the text?
So we're designing this sort of self-supervised multi-modal task with this large, weekly supervised
data source.
And what we get at the end is a model that has built some representations that bridge between
vision and language and then we fine tune that for a wide variety of other tasks.
Now randomly masking out parts of the image sounds like a very blunt instrument to apply.
Yeah, it is.
So part of it is we don't know which parts of the image are being described by the associated
alt text.
So it's not clear that we should mask out certain regions versus certain other ones.
So yeah, it is a blunt instrument, but I think if we were going to be more precise, we
would already need to know the grounding.
And that's exactly what we're trying to learn.
Yeah, I think the picture that came to mind was doing some kind of object detection in
the image and then masking out one or more of the objects.
It kind of almost predicates the image net kind of image that I initially asked about
as opposed to more natural scenes.
Well, not only does it predicate that, it's also a good question.
How many objects can we detect until recently it was something the biggest data set for
detection was something like 80 or 100 classes.
I think open images brought it up to one or 2,000.
But if you actually look at the richness of text, we use all sorts of visual words all
the time.
Even if you're looking around where we're recording right now, there's lots of objects
that you know a word for.
You could describe, but they would never show up in a detection challenge.
So the question is, how do we build groundings for those things?
Not just for things we can already detect, because if we can already detect them, we sort
of how do you have the grounding?
Talk a little bit more about how you've kind of adapted the process for training a
BERT-like model, transformer model, to incorporate this additional information.
Yeah, absolutely.
So one of the sort of core parts of the BERT model is that it treats this sequence of
words, a sentence, sort of as a set of independent inputs and then does a bunch of self-attention
in between to process this.
And the actual position of a word in that sequence is just denoted by some positional embedding.
So it's really just a set of words with some sequentiality added as a feature representation.
And you could think about an image the same way.
You could think about an image as a set of bounding boxes, for instance, of just various regions
in the image that have been pulled out and some associated visual feature and a positional
encoding.
And this box out from over here, so it's just a set, it's an unordered set, in fact.
So the actual input API for BERT looks the same in our model for the visual side and the
linguistic side.
As we're just assuming we have a set of features with some sort of encoding put in.
And then we're sort of directly copying over a number of the self-supervised losses that
BERT is actually designed for.
So masking out parts.
The image we're asking about alignment between the text and the image.
In BERT, it's a sentence and the next sentence, and you're predicting whether one comes
after the other.
But in our case, it's an image and a sentence, and we're asking, does this align or not?
Is this actually a pair from conceptual options?
And then sort of if you're asking about more internal mechanisms, we rely on a co-attentional
transformer, which is something we introduce here, but it's been introduced by a number
of people simultaneously, because it's sort of intuitive idea, that rather than doing
self-attention, you're attending one modality based on queries from the other modality.
So you're using the language to attend over the image, and then you're taking that sort
of pooled image feature back into the language to inform the rest of that computation.
And we have those sort of co-attentional layers periodically through this structure.
The result is kind of a jointly trained model that is kind of trained on the relationship.
It's not you've kind of trained on some visual stuff, and then some language stuff, and
you found a way to kind of merge them together.
Yeah, I mean, I think that's a pretty fair assessment.
We aren't training the vision from scratch, so we are sort of starting with some frozen
representation, but we are allowing it to learn the association between the two together.
But it's a very dynamic back and forth process, so it's not just like a direct assignment.
So what's the frozen representation that you're starting from?
We're starting from a faster RCNN trained on visual genome, so it's just sort of a standard
practice in the vision and language space.
Okay, and so how is that incorporated into the virtual model?
So that's the thing that actually outputs these bounding boxes.
Okay.
So it's sort of like your object detection idea, except for that we are not stopping at
the classes it actually predicts.
We're looking for the associations more broadly.
Okay, so the bounding boxes aren't random random there driven by this.
They're driven by the faster RCN model.
You can see arbitrary, more so than random perhaps.
Or not even arbitrary, but there is picking out.
You can also sample from them.
So the faster RCN model can produce quite a lot of bounding boxes, and you can sample
from it if you'd like to increase the randomness there.
If you actually look at some of the bounding boxes that come out, many of them aren't really
object aligned, because if you look at the visual genome data, it's actually fairly noisy.
Some of it's very clean, some of it's very noisy.
So sometimes you'll have, for instance, an object is like a road very often or something
like this.
It doesn't do a great job of honing in on specific objects.
Where does Wilbert fit into kind of the broader scope of your research?
What do you want it to take you?
Is it a means or an end or a little bit of both?
Yeah, I mean, so it's both.
Right.
So there's a fundamental question of how do we align language and vision?
And that's the sort of interesting research question, and then why would we want to do
that is perhaps a useful follow-up.
And there's a number of things that are sort of concrete applications.
If you could align vision and text really well, you can train models to do things like
VQA or Visual Question Answering, which can help people to visual impairment ask about
the world around them and get answers back, or you could do image captioning to provide
descriptions of what someone would be seeing if they didn't have the same visual impairment.
Yeah.
So the ideas that someone could take this Wilbert model off the shelf and kind of VALA transfer
learning, just drop it into their application, and what would the inputs be and what are
the outputs?
Yeah.
Yeah.
So in our case, the inputs are sort of image features that they're looking at, and then
some sort of text query.
So we transfer image features, meaning the bounding boxes extra from some model.
So, since the Wilbert paper, we've applied it to something on the order of 12 different
vision and language tasks.
So you can pre-train this and then use it as a base to perform fairly well, fairly quickly
on a wide range of these visual and language reasoning tasks.
So the inputs, some bounding boxes, and some text.
So let's say I was doing question answering, and I would take my image, and I would extract
these bounding boxes and feed it in, and then I would feed my question as the text.
And I can train output that just predicts some set of answers, and that trains quickly
and trains well, rather than training it from scratch on this status set.
And so you can train it.
You can kind of fine tune for specific task types.
Presumably you can also fine tune it with specific types of data or kind of data language
relationships.
Yeah.
How might that work?
So this is actually what comes up often in these tasks, right?
So Wilbert's pre-trained on this huge set from conceptual captions that covers like we
talked about earlier, quite a few things.
But if you go to something like VQA, which is based off Coco, there's 80 major object categories
and the images are all things people were proud enough to put on Flickr, so that they
could eventually get downloaded.
So that's a much more constrained image set.
But when you're fine-tuning, you're fine-tuning the whole of Wilbert, and you are feeding in
those images and they search for a question.
So it can fine-tune.
Got it.
So you're kind of doing both at the same time.
You're fine-tuning the visual data set, but also the, you know, for the specific task
that you're asking it to do.
Yep.
That's exactly right.
Okay.
Interesting.
Interesting.
And so kind of going back to the previous question, you know, where does this lead do
you think from the broader perspective of kind of visual, kind of this integration between
visual and language tasks?
Yeah.
So I think the grounding problem itself could use sort of more attention.
We've been very tasked-driven as a community, and we're learning sort of myopic groundings,
right?
So if I train a model for VQA, it understands what Coco images look like, and it understands
what questions are, or likewise if I do a captioning model.
Sometimes this is even more shocking.
You've trained a model to caption Coco images, and Coco images have a really small set
of classes.
For instance, one example that I like to show from a recent paper is they don't have any
guns.
So if you've fed this caption, we're trained on Coco with an image of a man in a red
hat with a red shirt holding a shotgun.
And the caption is a man in a red hat and a red shirt holding a baseball bat.
Okay.
Because he's wearing what looks like a baseball uniform, and he's got something in his
hands.
Might as well be a baseball bat.
And if we talk back to these potential applications of helping people with visual impairment, that
kind of mistake doesn't seem justifiable.
Yeah.
There was a tweet going around just yesterday of the day before where someone passed a
chart into a captioning system, and it was like a chart with like three, you know, three
trend lines or three graph, three data sets that were kind of diverging.
And it was like something with two scissors.
Yeah.
Yeah.
So there's that whole side of things.
Right.
And there's this other paper that I recently did with a co-author Peter Anderson called
No Caps, which is a novel object captioning at scale, and it addresses these sort of problems.
Where how do I caption objects that maybe I've never heard talked about before?
So I don't have linguistic data about shotguns, for instance, but I might have an object
detector that can tell me that that object exists, and then how do I incorporate that into
language descriptions?
Tell me more about that process.
Sure.
How does that work?
How does that work?
So that's one of the open questions.
So we presented a data set and ran some initial baselines from techniques that had sort
of already been floating around on this.
Did you also propose a model that performs well on the task?
No.
Not in that paper.
Okay.
So, not yet.
Coming soon.
Potentially.
But the way it typically works is the model will consider regions in the image as potential
words that it could use in its output.
So even if it doesn't really know what the right language for that specific part of the
image is, it might recognize that that's an important thing to talk about and want to
try to put it into the caption wholesale, and then you rely on the object detector to
tell you what that's...
Maybe an action in a red hat with some object that I can't identify that seems important.
A man with a red hat holding a that, where that is the image region that contains the
gun in this case.
And then you rely on object detectors to tell you, or image classifiers, to tell you what
that image patch actually was, just some subset of already defined kind of things.
So basically, it's saying, if I already know how to use language to describe the world,
but I don't know specific objects, it seems silly that I should have to go collect new
descriptions that contain that object.
You know how to talk, but you may not know what a widget is, but if I point you to a widget,
you can then talk about the widget, you just need that first grounding.
So this line of work is really about given that grounding.
How do you incorporate it into language models or how do you actually talk?
And it kind of, it likes what, likewise, seems silly to pick some other random object.
You know, it probably isn't this thing and stick it in this place.
Yeah.
Confidence estimation is an open problem in this space.
I mean, does the model know it's not a baseball bat, or does it really think it is?
It's something that needs to be explored, where we're going to actually deploy these things.
His other thoughts on kind of interesting challenges in this visual language domain or
the grounding problem in particular.
I mean, I'll use the question as an opportunity to talk about some other recent work that I'm
excited about.
Yeah.
So, for a long time, in the community, grounding has been on static images.
But there's actually a lot of concepts that rely on motion, that rely on interaction
to ground.
I don't know if I could, I could give you a photo and you could tell me it looks like
people are talking, but they could just be sitting there quietly as well, right?
It's interaction that actually sort of tells you what's going on.
So lately, I've been moving, are we primarily in this or definitionally talking about kind
of verb concepts as opposed to noun concepts or is it independent of that particular distinction?
I think that's a fine generality, I think that's a fine way to understand it in general.
Though some verb concepts can be learned from static images.
Sure.
So it's like jumping, right?
We sort of defies gravity, so we know what they're roughly doing.
Yeah.
They'll actually jumping versus falling.
That nuance doesn't really get captured very easily for a lot of images.
Yeah.
So, I've been interested in settings where agents have to demonstrate their visual and
linguistic understanding in simulated environments by acting, this was sort of like the VLN stuff
I talked about earlier, the first work.
But there's also things like if I drop an agent in an unseen environment and I ask them
a question, what color is the car, how does an agent navigate, see the car, know that
it now has enough information to answer, and then offer up an answer in return.
So these sort of problems are things that I'm increasingly interested in.
Is the problem there as you've framed it up, passing a model of sequence of images and
having it identify either caption or identify an action or something else, or is it more
the lot of things that you describe seem different, like the agent navigating the environment
etc.
What you described sounds a lot like what image, video captioning or video question answering.
Yeah.
But I'm much more interested in sort of situations with this active perception, so embodiment
of some kind.
So when I ask what color is the car, if the agent starts in a bedroom, it needs to understand
the cars who are outside, it needs to get into the hallway because it knows hallways tend
to lead outside doors, right?
It shouldn't go to the bathroom attached to the bedroom, there's no outside doors there.
Right.
So it has to start reasoning about structural priors in the world and navigating according
to those to actually decide and output an answer in the end.
So it's more about controlling the agent towards a goal that's specified in natural language.
And do you have existing data sets that will allow you to get there or is that part of the
challenge is coming up with new data sets?
Yeah.
So I think data sets are obviously a big driver of progress in machine learning a couple
about a year ago, the team in Georgia Tech, where I was working, output this embodied
question answering project, which revolved around this, and then there's also the vision
and language navigation task from Peter Anderson, which relies on sort of falling instructions
in environments again by controlling the agent.
So it's recent, but people are producing these sort of things.
Anything else you want to cover?
No, I think that's fine for right now, if you have more questions, I'm happy to answer
more.
So we're keeping our eyes peeled, we'll be keeping our eyes peeled for your upcoming publications.
Thanks so much, Stefan, for taking the time to chat with us.
Yeah, thanks for having me.
All right.
Thanks.
All right, everyone, that's our show for today.
To learn more about today's guest or any of the topics mentioned in this interview, visit
twimmelai.com.
Of course, if you like what you hear in the podcast, please, please, please subscribe,
rate, and review the show on your favorite pod catcher.
Thanks so much for listening, and catch you next time.
