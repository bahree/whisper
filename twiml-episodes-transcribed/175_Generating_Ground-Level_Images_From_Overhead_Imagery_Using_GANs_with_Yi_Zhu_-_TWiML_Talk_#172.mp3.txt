Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington, a quick update for our faithful listeners and fans.
We recently asked you to help us in our bid to secure our nomination in the People's
Choice Podcast Awards.
Well, thanks to you, we're a finalist in the best technology podcast category, along
with other noteworthy shows like Riko Diko and the Verge Cast.
The award ceremony will be stream live on September 30th, which turns out to be international
podcast day.
Keep your fingers crossed for us and a huge thanks to everyone who voted.
You have our utmost gratitude.
Today we're joined by EZU, a PhD candidate at UC Merced focused on geospatial image analysis.
In our conversation, E and I discussed his recent paper, what is it like down there?
Using dense ground level views and image features, from overhead imagery, using conditional
generative adversarial networks.
E and I discussed the goals of this research, which is to train effective land use classifiers
on proximate or ground level images, and how he uses conditional gans along with images
sourced from social media to generate artificial ground level images for this task.
We also explore future research directions, such as the use of reversible generative networks,
as proposed in the recently released open AI glow paper to produce higher resolution
images.
Enjoy.
All right, everyone, I am on the line with EZU.
E is a PhD candidate at UC Merced.
E, welcome to this week in machine learning and AI.
Hi, Sam.
Hi, everybody.
So thank you for having me.
It's really excited to be here.
Absolutely.
I'm really looking forward to digging into your work today to get us started.
Why don't you tell us a little bit about your background and how you got started in machine
learning.
So actually, my major at first is wireless communication, because my undergraduate study
was on like signal processing and information theory.
But then at the year of 2012, when deep learning first the show is great potential on image
net challenge, so I was fascinated by simplicity and surprisingly good results.
So I started to attend some like cargo challenges using deep learning, including the famous dog
and cat classification challenge.
So although at that time, most of the challenge winners are still using random forest of XG
boost.
But later I found myself like attractive to this machine learning field and especially
deep learning.
So I'm thinking so why not change a major?
So in my understanding, so images are still signals captured by optical sensors.
So they are not that different from my previous study.
So I switched my major to computer vision and start my PhD study at UC Merced in 2014.
So basically, I have two lines of researcher directions.
Right now, one is geospatial image analysis.
The other is a video analysis, but I think today I would just focus first one.
You mentioned that you started competing in cargo competitions.
What were some of the competitions that you competed in?
So the first one will be the dog and cat challenge.
So that's the object, like a recognition problem, like a binary thing.
And later, I also competing the challenge of the eye contact of the driver to try to
understand if the driver is sleepiness or is it a safe driver or something.
So there's also an autonomous driving challenge.
And then also for the insurance to see a car's image.
So whether it is like a damage or not, is it a new car?
So how much price should it sell for a second hand car?
So that's for the insurance company.
So there are several more, but I cannot remember the details.
Yeah.
Okay.
Awesome.
I don't know the last two, the driver eye contact or the insurance one, but the dog and
cat one is one that we have gone through.
I'm working with a group of folks to kind of go through the fast AI course and they talk
about that dog and cats one in there quite extensively.
How do you do and the Kaggle challenges you did?
So firstly, I just tried like a linear regression.
So that's the most basic model.
I think most people still use it today for normal questions.
And then I also tried like a random forest and XG boost.
So at that time, the most winners are using those two techniques and then model assembling
to get to the best score they can.
But then I also tried like deep learning, right?
So convolutional neural networks for this image recognition task.
And at first, they cannot compete with like random forest because the images are not that
large.
So you know, like deep learning is a data hangry model, right?
So if you have more data and if you have like a clean data, so you can get a very good
result.
But if you don't have enough data, so maybe sometimes it doesn't compete with those traditional
algorithms.
Yeah.
That basically the four algorithms I'm using for Kaggle challenges.
How did you rank in the competitions?
Did you rank highly in any of them?
I didn't rank like a top one or top 10 thumbs in, but I'm usually in the top 10%.
So.
Okay.
Nice.
And so your research now is on understanding images using deep learning.
And you recently published a paper.
The paper is called what is it like down there generating dense ground level views and
image features from overhead imagery using conditional Gans.
That there's a ton in there.
Yeah, yeah, the title is quite long.
The title is quite long.
I mean, maybe let's get started.
I think, you know, we've talked about Gans quite a bit on the podcast, but why don't you
start us out by talking about conditional Gans?
Okay.
Yeah.
So yeah, I have seen those like a Gans episode here and there.
So it's very hot topic like last year and there's a trend is still going on.
So as we know, so Gans consists of two components.
So one generator, one discriminator.
But the thing is when you generate something, so you're generating from a random noise.
So each time you will get different results, different output, but sometimes you really
want to have some like fixed output, right?
So that is what conditional Gans for.
And for like, for example, sometimes a like a fashion, like a shoe designer, they want
to design using again to design a new shoe.
But if you didn't control the output, sometimes it will begin something really random.
So if we can give the model some information like prior information, like some texturing
bad things like I want a shoe.
So then the output will definitely be a shoe image.
So that's what Gans for.
So see again.
So the conditional again, learn the distribution condition on some auxiliary information.
So those auxiliary information can be class labels, or which is the most standard form,
or texting bad things for generating images, or like image to image translation.
So here in our work, so we use conditional again because we know what we want, right?
We want ground level images corresponding to the overhead image.
So given me overhead image, I want to generate a similar looking ground level images.
So the overhead image will be considered as a prior information.
Yeah.
So that's why we choose a conditional again.
And what you're going for kind of visually is if you send in to the system, the image
of, you know, what an area that looks like farmland, you want to generate ground level
pictures that look like farmland.
Similarly, if you're sending in overhead images of an urban setting, you want it to generate
urban images.
Yes.
Exactly.
Tell us about the different data sources that you use to make all this happen.
Okay.
So the ground level images, the problem there is their spars.
So what we can have is a satellite images, right?
Satellite images is dense.
So in everywhere.
So it has all the satellite images at every spatial location.
So that is our image input source.
So the ground chooses and cover map is from the LCM data set.
So our study region is a 77 by 71 kilometer region containing that in London.
So the geographic images are labeled as urban or rural based.
So then we can do a very simple like a binary classification problems.
So our input sources are like a lot.
So from like Google maps, statistics and a geographic API and also the satellite images.
So those are the three image sources we're using.
The LCM data set is that your overhead images or those your ground images.
Those are the overhead images.
Okay.
And the ground level images is from the geographic.
And that is a website and by some London researchers, so those are for the whole United Kingdom.
So the length of our classes are provided on one kilometer grade.
So for every grade, they have the user provided ground level images.
So that's pretty accurate.
So that is our ground level images.
And the LCM is the overhead images.
So we have the corresponding relationship to train our models to do some experiments.
And so I realized that what you're doing here with Gantt is fundamentally generative.
But part of the way the problem is set up sounds like an information retrieval problem,
like you have this overhead image and you have this corpus of ground level images kind
of find the best one.
Are those related in any way?
Oh, so there are there is difference there.
So for like a image retrieval problem is so given me overhead images, I want to find
the best matching like ground level images, right?
So there is like a there's a database of the ground level images.
So we try to find the like most matching one.
So but for generate models, it's not where we want some specific images.
So we don't want some specific ground level images to corresponding to the overhead image.
We want the whole data distribution to fit.
So the we want the generator to generate some real looking images to fit the data distribution.
We don't care about so which images we're generating, we just want it to look realistic
for that category.
So that's the only difference.
And do you do any kind of information retrieval as part of the solution?
In other words, is your GAN conditioned only on the overhead image or is it conditioned
on the subset of images that you retrieve from a database based on the overhead image?
That's a good question.
So actually right now we only like based our model on GAN, so we're not we're not using
any information retrieval thing.
So but in the future, we want to use that because the land use classification problem is
really challenging.
It's really, really hard.
So maybe later I will introduce my another work.
So it's also about large-scale land use classification, but the classification accuracy
is only like 20%, 30%, so it's pretty challenging.
So we need to have some like a human prior knowledge inside it.
So like the information retrieval, like exciting information you mentioned about.
So we will do that in the future work.
So in this work, what are some of the big challenges that you had to overcome in the
spaper and this research?
Okay.
So can I like a backup a little bit to talk about the motivation of this work?
So that would be more clear.
Okay.
So the motivation to like all of our work is because although we have like enormous
amount of online images like from Flaker, Instagram, so all those images, all those videos,
we could use it to do some geospatial analysis.
But if we want to do a detailed land use classification map, so land use I mean is, for
example, so whether a building is a hospital or whether it's a shopping mall or something.
So how do we use the land?
So whether it's residential or for office use.
So but overhead images cannot handle such case because overhead images is from like above,
right?
So from above, you can only see a building.
So you cannot see inside of the building.
So you don't know what that design for.
So that's what we come up with.
So we want to use social multimedia to do this kind of like a land use classification
because social media is captured by phones or cameras.
So they can see inside the building to easily infer what it is of this building.
So where it is.
But to the challenge, the most big challenge of using social media is it's uneven spatial
distribution.
So simple speaking because most images are coming from the famous landmarks, right?
So not to the general spatial locations, like there were tons of people having the golden
gate bridge in San Francisco, like after towering Paris.
So it's very easy to tell these landmarks from the photographs, but for like residential
areas or privacy, preserve the regions, which so we don't have enough images.
So that's a very serious problem if we don't have images.
So how can we infer the location and all those land use classification information?
So that's the challenge we really face about.
So traditional zero several ways to do that.
So one simple way is to interpolate the features.
So if we have, for example, we have several images at location A and we have several images
at the land location B and these two locations are close to each other.
But between these two locations, there is no images at all.
So in this case, we just computed the image features of these all the images at those
two locations and interpolate them along this line.
But here we made assumption that the assumption is we hope the image features will change
smoothly in the spatial domain, but actually in most cases, it's not a case.
So if we have several images of like a residential and at the location A and a residential and
location B, then if you do the interpolation things, it will interpolate.
So between these two locations, all the areas are residential based.
But actually in most cases, the space in between is like forest or park or just a river
or something.
It's like natural things.
So the other kind of method is, so we try to use other information like remote sensing
images or Google Street Wheels because those two information sources are like a dance
so everywhere in the Earth.
So there is a work from like Professor Nason Jacob's lab in ICCV 2017, so they just
use Google Street Wheels to do these kind of things.
But we find that so all those techniques are based on image features and are based on
the assumption I just said.
So the image features change smoothly in the spatial domain, but usually that's not
the case.
So that's why we do this work.
So if we cannot use the image features, why not we just use images?
But the images are missing at those locations.
So how do we come up with new images?
So fortunately, we have guns.
So that's the motivation of the work.
You mentioned that early in your research career, you spent some time looking at
information theory and the like and it strikes me in that context that this is a pretty
difficult problem and that there's just not enough information in these satellite images
to do a very good job coming up with ground level images.
How do you get around that?
Yeah.
So both like a overhead images and ground level images has this like advantages and drawbacks.
So as I said, so the overhead images is very accurate and it stands in everywhere.
So we can totally use this kind of information.
But if you want to see inside the building, so overhead images cannot do that.
But ground level images, we can see inside the building, but the biggest problem is the
uneven distribution.
So from the information theory side of like point of view, so we should use both information
sources to how to combine them in the best way to get the best result.
Yeah.
This is a good point.
Yeah, maybe to make my question more concrete, it has to do with what you considered your
error function to be or something along those lines.
And I guess the thing that I am trying to articulate here that strikes me as being particularly
interesting and challenging here is with a ground level image, you know, beyond just
trying to generate like high, you know, whether a particular image looks kind of urban or
looks like, you know, greenery or forest or things like that, you know, there are things
that you're trying to generate that aren't at all represented in your input.
So for example, your satellite image, you know, has no building facades.
But if you're trying to generate imagery around an urban area, those ground level images
will have building facades, so it's just kind of making that stuff up.
And I'm wondering, does that fact play into kind of how you build out the model and what
the loss function is and stuff like that?
Does that make any sense?
Yeah.
Yeah, it totally makes sense.
Yeah.
That's a good point.
So, but actually our work is in the very like a initial stage.
So currently we're just using a very traditional conditional again to do that.
And that the loss function is just like generating realistic looking images.
So we're not considering like whether there is a discrepancy between the ground level
images or overhead images or any other like objective function.
We're just using conditional again without other stuff.
So the loss function only thinking about realism versus not realism.
Is that kind of axiomatic for conditional gains?
Yes, it is.
So, yeah, for most of conditional gains, it's just a, so for the discriminator as job
is just to say, so whether this image is fake or real.
So it's a binary problem and the objective function is just that.
So entropy loss.
Okay.
Got it.
So you built this GAN based system to produce these images.
We were just talking about loss functions like what did you find in terms of its performance?
How did the system do?
Yeah.
So in terms of the image quality, so I don't like have a monitor to show the quality here,
but basically it makes sense.
So we can generate like some realistic images, like ground level images according to the
overhead images.
And but we don't have like a evaluation metric.
So how real they are or how accurate they are.
So we use another task.
So that's a land use classification, right?
Because if we can do a better land use classification, given these like fake images, so we can create
fake images all over the ground, right?
So if we can use these images to like to do better land use classification.
So that of accuracy can be a good indicator of how our model works.
So let me like share the performance of our land use classification problem.
So if you if we are using the conditional GAN generated features to do the land use
classification, we can achieve like a land cover classification accuracy with like 73% accuracy.
So actually it's kind of like it's a reasonable, it's not high, but it's reasonable.
The problems we're thinking here is because our generated images are not realistic enough.
So that's some of our future work.
So we have like a three future directions to go.
So I can talk about later like if you like want.
In the land use classification, how many classes are there?
It's only binary classification.
So like as I mentioned, it's a rural and urban areas.
So because that's that's only the ground truth we have.
Okay.
So you've got a bunch of satellite data.
You feed it into your conditional GAN.
It generates an image that is meant to be representative of a ground level view of the
area that you indicate.
And then you are sending that into a classifier that is meant to determine whether it's
urban or rural.
And the is the 73% is that the accuracy of your classifier or the accuracy of the generator
based on a trained classifier?
Oh, it's a it's a the classifier.
It's a classifier.
Okay.
The classifier itself.
That's what I thought you were saying.
And so, you know, how did the GAN itself perform relative to with that classifier?
Okay.
So the baseline of that is, so we compare to like a traditional approaches, right?
So the first approaches I'm talking about is interpolated features.
So if we have some images, we just do the feature extraction first and interpolate the features
on those regions without images.
So the baseline is like 65%, okay, so 65.
So if we're using GANs without conditional, like we're getting like a two or three percent
improvement, like to the almost the 68% and if we're using conditional again, we're
having like 70% and if we're using conditional again, generated features, we're having the
best accuracy like 73%.
So that's the progress of our work.
And I just want to make sure I understand this.
I thought what you were saying was that the classifier itself, like totally separate
from the GAN part of the system, had a 73% accuracy.
Did you model the land use classifier separately and that was 73%.
Yes.
So the land use classifier is totally separate from the GAN, right?
So the GAN, the GAN is about generated discriminator.
The job is to like create, create these images, yes.
And then we use the features from the discriminator as the input to the land use classifier,
okay?
Right.
I guess what strikes me as odd is that, or at least curious, is that if I understood
you correctly, your ultimate accuracy of your GAN turned out to be exactly the same as
the accuracy of your classifier, 73%.
No, maybe I'm saying wrong.
So because for GAN, there's no accuracy, right?
So GANs for the discriminator is just a real or fake.
So we don't care about that accuracy or not.
So it's usually very high, so like 80% or 90% is very high.
But what we are concerned about is the land use classifier, yeah.
So what I'm saying, the accuracy is all for land use classifier, it's not for GAN
classifier.
Okay.
I think I'm confusing the issue here and not doing a good job explaining.
So I guess I'm thinking that there are, as we established, there are two separate systems.
There's the generating system.
It's input is an overhead image and its output is a ground level image.
And there's a classifying system and its input is a ground level image and its output
is a binary land use classification.
And so the performance of the GAN is a subset of that first system.
And it's responsible for generating these images and it's kind of judged on whether
the images are realistic or not.
But that whole first system, the generator system as a whole, right?
You're giving it a satellite image and it's spitting out a ground level image.
You can measure that its accuracy with respect to producing kind of the correct land use,
right?
Yes.
So that's one kind of accuracy measure and then there's another which is given any kind
of image, whether it's generated from our generator or not, you know, is this land use
classifier model accurate in classifying the input image correctly.
And then there's like a third performance metric, which is the end to end, right, given
the satellite image, can we identify the land use correctly?
And so which of these is the 73 percent?
It's the second one.
So for the third one, we're not doing end to end right now.
So we're just, we're doing two stage.
So first stage is GAN, the second stage is then to use, we're not doing end to end at
this point.
Yeah, I guess if you're not doing end to end, you're not really looking at the first,
the performance of the generator either because that really is the end to end.
Uh-huh.
Okay.
So the main focus of the research around modeling and testing this, the classifier then
or is it the generator system, uh, the classifier, right, because we're, um, so for geospatial
analysis, we're more caring about the land use system.
So basically it's a zoning system, right?
So usually the government and city will make a zoning map so every year or so.
So that is the most helpful things.
So for the generator, it's just a technique we use.
So if we don't use GAN, we can use other generator to generate images.
Yeah, it's, it's kind of funny in that like, you know, GANs are so popular and quote unquote
sexy term.
It's almost like a head fake that kind of pulls your attention away from what you're actually
trying to do in this paper.
Yeah, yeah, yeah.
Actually it is.
So, um, okay.
So I'm not sure you've even talked much about the classification model.
So for land use, it's a pretty basic, like a, like a convolutional network.
So like we use ResNet 101 to do that.
Um, so actually, um, we have like a previous work like a three years ago.
Um, so it, it's also doing land use classification with like a convolutional neural network.
I think at that time where the first batch of work that used deep learning for this kind
of work, we also received the best postal word in the SEM six spatial conference.
Yeah.
Hopefully it's you, um, at that time, it's also the similar problems we're using social
media.
We use deep learning.
We do the land use classification, but the biggest problem is we don't have the ground
juice, right?
If we don't have ground juice, we cannot evaluate our model.
So at that time, which is to check and like a university campus to do the land use classification
task.
It's a tribute example.
It's a toy example, but we get like a very good accuracy.
So since that time, we start to build a very large data set and actually we took like
two years to build a data set and continue this line of land use classification system.
Yeah.
But then in this work in this game paper, we're just using a very standard resident one
on one network to do the land use classification.
Does the social media images come into play in this paper?
Oh, yes.
So for the ground level images from the geographic data set.
So those are all like the ground level images.
So social media, right?
Those social media are contributed by just resident in United Kingdom.
So anybody can submit an image to the website and the website will show it.
So where the photo is taken and what it's about.
So yeah, for that data set, it is totally like a social multimedia.
And remind me how where that comes into play in the system?
The system is when you do the discriminator, right?
You need to tell this image is fake or real, right?
But how do you determine whether it's real or fake?
You have to look at some ground level images to know it is real or fake, right?
Oh, right.
So yeah, that's what they're coming to play.
So although the input is overhead imagery, but the ground shows like something you compare
to is the ground level images.
So that's when the social multimedia coming to play.
And those images are they're not labeled at all with regard to land use or anything
like that.
It's just that's the training the discriminator to understand real versus fake images.
Yes.
That's the beauty of gain, right?
So we don't care about the labeling.
We don't need to know the land use process.
We just need to know this is a real image that is a fake one.
So I.
Okay.
Cool.
I think I've got it now.
So the to sum it all up, right, you've got this challenge of being able to develop accurate
land use classifiers, but you've got this problem of sparse data, right?
So you've got all of this area that you might want to classify based on satellite images,
but that you don't have specific ground level data for.
So you generate some realistic looking ground level data using the conditional gain and
then use that to train your classifier.
And you've been able to kind of incrementally improve the performance of the classifier
using this kind of data as opposed to previous data sources that are trying to approximate
this data that you don't have.
Yes.
Yes.
Correct.
Yeah.
Got it.
Exactly.
Got it.
Okay.
Awesome.
Yeah.
I don't know why this one was so difficult for me to wrap my head around, but I think a part of it
had to do with this, this, this GAN head fake.
Yes.
So most of the people just focus on the GANs.
Oh, what's GAN can structure you're using?
Are you using the state of art again?
So yeah.
So actually, we're doing something like for geospatial analysis, so for the land use classification.
So that's how we're finally aimed.
Awesome.
Are there other interesting aspects of this paper that we haven't touched on yet?
Yeah.
So for this paper, there's, no, not, but for the future work, right?
I want to talk about the future work a little bit.
So that's very interesting.
So the first thing about the future work is, so right now, our generated ground level
images, like, it's not real enough.
So they lack the image details.
So for some like houses or like animals, they don't, they don't look real enough.
So there's like a plenty of room for improvement.
So currently we're thinking so we want to use a technique called a progressive GAN.
So from immediate, so that method is the key idea is to grow both the generator and
discriminator progressively.
So from a small network, from a small resolution, we add new layers to the model and
increasing the model in a progressive manner.
So this can both speed up the training procedure and stabilize the model.
Because GAN is really hard to train, so sometimes it's just a model collapse.
So this could eventually lead us to a very good image resolution, because currently our
generated images only like 32 by 32 or 64 by 64, so it's very coarse.
But eventually, we want to have some images like 1K by 1K or 2K or even 2K.
So because most of the remote sensing images is like 2K by like 3K.
So it's very large and it's very details.
So we want our generated ground level images is also large and details.
So I think that will bring up our performance by a large margin.
Okay.
So that's the first direction.
And the second direction is, so there is also a recent work by OpenAI called GLOW.
So it's there using reversible generated models.
They're not using GANs, but reversible generated models.
So that model, the most, the good thing about it is their latent space, I mean the features.
So the features are useful for downstream tasks.
Because in GANs, the data points can usually not be directly represented in a latent space.
So because they have no encoder and they don't have all the data distribution.
And for reversible generated models, they can, they can interpolate between these features
space.
So it's very smooth.
So in that case, we can directly use the features.
We don't need to use images.
So I hope that can be a better solution.
And then I think like a lot of people are interested in this paper as well.
So the GLOW one.
So it can generate very realistic images.
And the third direction will be using more information as you talk about.
So maybe using like a information retrieval thing, we're using some like a text.
Text information, right?
Because for example, if we want to generate like a forest like image.
So if we just give him the overhead image, so maybe he cannot like give us a good image.
But if we can say so I want the forest to wheel with like with one house inside it.
So maybe in our generated GLOW images, we will see exactly one house in a forest.
Like scenery.
So that's very promising.
So that's the third direction to go.
So so this works, this can work is our initial attempt to do this dance dance interpolations
in this direction.
So we have a lot of work coming up.
So hopefully we can get better results in the future.
You mentioned with regard to the GLOW paper, I've seen it.
But I haven't looked at it in any detail at all.
You mentioned that part of what it allows you to do is to get these smooth representations
in a feature space.
And are you would you then be trying to use that feature space or is the only benefit
to you of that that it produces better images?
I think we will try both.
So better images and the features.
I think maybe the image maybe the features makes more sense because for GLOW work, they're
reversible, right?
But reversible means if you have some input and go into the output, it can also use output
to get you the input.
So the reversible makes the features more robust.
So it makes sense.
It is like explainable.
So in that case, the features might be more powerful than the again features.
Are the features in this feature space?
Are they semantically related?
Like is it kind of an embedding in the feature space where you can get these semantic relationships
between these different types of generated images?
Yeah, definitely.
I think the features should be like semantic related.
So if we do like a feature space visualization, we can see like a tree forest cluster together
under the river water lake, those images are clustered together.
So the features are definitely semantic related.
And so do you think you'll get to a point where you can start with a ground truth image
of a field and say like I want a little bit more, you know, rivers and then get a stream
and a little bit more rivers and get like a bigger body of water or something like that?
Well, you're really like smart.
So that's something we're trying to do right now.
So that's more like a manipulative of the image, right?
So yeah, if you can do that, so that's very interesting to the geospatial communities.
Just to kind of wrap things up, I'm curious you mentioned how difficult training the
GANs has been.
Can you maybe share with us some things that you've learned as you try to work with GANs
and conditional GANs?
Okay, sure.
So for GANs, so because we're using a very standard conditional GANs, which is proposed
like 2015 or 2016, so it's a very earlier stage of the GAN.
So the training is like on stable, so I have a lot of problems.
So as mentioning another paper, I forgot the exact name, but it should be like a good
practice is during like training, implementing GANs.
So they propose like we should use straight-ed convolution, not pooling because pooling can
hurt the like image resolution thing during the down sampling.
And we should like using smaller crop size and the smaller learning rates, something
like that.
But I don't think that's a major problem right now because most of GAN models is
like easier to train at this moment.
So because the loss matrix changes to the loss system loss, so that is a very stabilized
loss function and also the network architecture change.
So right now we can train very deep networks using GANs.
So for example, like a ResNet 101 and the output can be several hundred resolutions
or even one K resolutions.
So the training of the GAN is not that hard right now.
Awesome, well E, thank you so much for taking the time to share with us what you're working
on.
It's really interesting stuff.
Yeah, no problem.
Alright, everyone, that's our show for today.
For more information on E or any of the topics covered in this episode, head over to twomolei.com
slash talk slash 172.
As always, thanks so much for listening and catch you next time.
