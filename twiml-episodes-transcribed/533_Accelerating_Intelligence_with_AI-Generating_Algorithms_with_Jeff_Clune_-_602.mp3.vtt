WEBVTT

00:00.000 --> 00:04.880
All right, everyone. Welcome to another episode of the Twimmel AI podcast. I am your host,

00:04.880 --> 00:10.560
Sam Charrington, and today I am joined by Jeff Kloon. Jeff is an associate professor at the

00:10.560 --> 00:16.960
University of British Columbia and a faculty member at the Vector Institute. Before we get into

00:16.960 --> 00:21.280
today's conversation, please be sure to take a moment to head over to Apple Podcasts or your

00:21.280 --> 00:26.400
listening platform of choice. And if you enjoyed the show, please leave us a five-star rating and

00:26.400 --> 00:31.680
review. Jeff, welcome to the podcast. Thank you. It's a pleasure to be here. I'm looking forward

00:31.680 --> 00:39.200
to our conversation. We will be talking about some of your work relating to the path towards AGI.

00:39.200 --> 00:45.600
In particular, this idea of AI generating algorithms. But before we dive into that topic,

00:45.600 --> 00:50.080
I'd love to have you share a little bit about your background and what brought you to the field.

00:50.080 --> 00:55.520
Yeah. Well, thanks again for inviting me. So my background actually going way back is philosophy.

00:55.520 --> 00:58.720
So when I got through, you know, as an undergrad at University of Michigan,

00:58.720 --> 01:02.800
I was trying to figure out kind of how does thinking work, you know, and how do we think about

01:02.800 --> 01:07.360
thinking and how maybe could we be recreated in a machine. And I thought, you know, who's got

01:07.360 --> 01:11.760
the market cornered on thinking about thinking, and I thought that was philosophy. So it was a

01:11.760 --> 01:16.000
fascinating, you know, a few years. But ultimately, I started to get frustrated because I couldn't

01:16.000 --> 01:22.240
test my hypotheses and try to learn by building. And so over time, I came to realize that the best

01:22.240 --> 01:26.480
place for me would be in machine learning and trying to build intelligence to learn more about it.

01:27.040 --> 01:31.600
And another quest that I've been on my whole life, which I was also fascinated about then,

01:31.600 --> 01:36.320
still to this day, is kind of the explosion of complexity we see in the natural world.

01:36.320 --> 01:41.280
You know, how does such an unintelligent algorithm like Darwinian evolution produce jaguars at hawks

01:41.280 --> 01:46.560
and human brain and three-toed sloths? This amazing explosion of marvels in the natural world.

01:47.680 --> 01:52.000
And I am super excited to think about could we create computer algorithms that can

01:52.000 --> 02:00.080
replicate that creativity and endless innovation. So I went back to graduate school and started

02:00.080 --> 02:03.440
studying, you know, machine learning, got a PhD in computer science, actually got a master's

02:03.440 --> 02:09.200
degree in philosophy. That's a little bit of a different story. I went, that one did a postdoc

02:09.200 --> 02:14.160
at Cornell and became a professor at the University of Wyoming. And then my friends and I had a

02:14.160 --> 02:20.640
startup called geometric intelligence. Uber acquired that to create, you know, Uber artificial

02:20.640 --> 02:26.160
intelligence lab, Uber AI labs. And so they asked us to move out to San Francisco to stand that up.

02:26.160 --> 02:30.320
So we built up Uber AI labs and it was a wonderful place, a great intellectual

02:31.200 --> 02:34.880
environment. And then I went over to open the eye and let a research team there.

02:35.840 --> 02:39.440
And now I am a professor at the University of British Columbia.

02:39.440 --> 02:42.720
Awesome. Awesome. Now geometric intelligence, was that with Ken Stanley?

02:42.720 --> 02:48.000
It was. Ken Stanley is a long-standing friend and collaborator of mine. We still talk every week.

02:48.000 --> 02:52.960
And he is, you know, part and parcel all the ideas that I've been working on throughout my career.

02:52.960 --> 02:57.200
So yeah, he is the person who brought me, one of the people that brought me geometric and

02:57.200 --> 03:00.560
we get to work alongside each other standing up Uber AI labs.

03:00.560 --> 03:09.200
Awesome. Awesome. You mentioned that you went to philosophy to kind of learn about thinking.

03:09.200 --> 03:13.440
You know, what's your top list of, hey, if you want to think about thinking, these are the things

03:13.440 --> 03:20.080
you need to go read from philosophy. I think one of the most fascinating questions in all of science

03:20.960 --> 03:25.360
is this idea that shows up in philosophy and we're starting to wrestle with it and think about

03:25.360 --> 03:31.440
it and machine learning as well. And that is, when do you go from rocks to feeling? You know,

03:31.440 --> 03:37.360
originally this planet was rocks and weather. And now we have beings that feel pain and fall in love

03:37.360 --> 03:43.520
and love the taste of chocolate and they below the taste of something else. And we, you know,

03:44.080 --> 03:48.800
philosophers call this qualia. And it's kind of this extra thing that gets layered onto the world

03:48.800 --> 03:53.600
at some point. And we have very little understanding of how that might happen physically,

03:55.040 --> 03:59.920
despite, you know, talking about it for millennia. And a central question that we're starting to

03:59.920 --> 04:07.280
grapple with in machine learning is when might such feelings show up in AI systems? When might we

04:07.280 --> 04:12.720
start to have to recognize the pain and suffering of AI agents? And when might we have to start

04:12.720 --> 04:18.480
treating them as beings that have ethical worth that we can't say enslave or torture or make suffer

04:19.360 --> 04:24.640
for our own ends? And this is the kind of stuff that I used to be afraid to even say out loud,

04:24.640 --> 04:28.640
on a public podcast like this, because I thought I was career suicide to let people know that I was

04:28.640 --> 04:33.120
thinking about these things, even though I deeply think that they're true. But increasingly,

04:33.120 --> 04:37.040
you see more and more mainstream figures in the field worried about this. You know, I actually

04:37.040 --> 04:41.120
recently saw a tweet from Richard Sutton that brought this idea up just like last week.

04:42.080 --> 04:46.080
And there are many other people in the field that are becoming concerned. I think nobody really

04:46.080 --> 04:50.320
thinks we're there yet. But also, I think nobody knows when we're going to cross that threshold.

04:50.320 --> 04:54.080
And so we probably want to get ready so we don't accidentally cause tremendous amount of

04:54.080 --> 05:00.000
suffering before we realize it and then have to feel really bad about it. So that's one example

05:00.000 --> 05:05.280
of thinking by thinking. I think that, you know, I think I could probably make up some other

05:05.280 --> 05:10.480
examples. But for the most part, it's probably why I went over to machine learning is that I wasn't

05:10.480 --> 05:14.480
finding a lot of, I mean, there's a lot of fascinating topics for sure, but I wasn't finding a lot

05:14.480 --> 05:20.000
of the answers to how do we really build the thinking machine and how does thinking really happen

05:20.000 --> 05:24.800
in our head that I couldn't, that I wasn't seeing better address in the sciences than in philosophy.

05:24.800 --> 05:31.680
Coming back to machine learning, you've talked about this idea of AI generating algorithms as,

05:31.680 --> 05:40.240
you know, one of the possible paths to getting us to to generalize artificial intelligence.

05:40.240 --> 05:46.000
Why don't you talk about that and introduce that idea in the context of the other paths and how

05:46.000 --> 05:50.800
you think about the role each of them will play? Sure. So I think that if you look out into

05:50.800 --> 05:57.360
machine learning, it's pretty clear that there is kind of a dominant paradigm. And this was

05:57.360 --> 06:00.800
certainly true a couple of years back. It's increasing a little less tree, but for the most part,

06:00.800 --> 06:07.440
look at any Neurops ICML-ICLEAR conference and what are most of the papers doing? They're saying,

06:07.440 --> 06:13.040
hey, I think that there's a building block that is important, you know, and I'm going to try to

06:13.040 --> 06:18.320
either introduce a new building block that we may not have had in our system before or create an

06:18.320 --> 06:23.360
improved version of an existing building block. You know, different types of neural activation

06:23.360 --> 06:28.960
functions are ways to normalize weights or layers, you know, better optimizers, maybe better

06:28.960 --> 06:33.120
recurrent cells of memory and writable memory and, you know, all these different things.

06:33.120 --> 06:37.680
And the thing that I find interesting about that is what we don't usually do is take a step back

06:37.680 --> 06:41.360
and think about the really grand emissions of the field and think about how are we going to get

06:41.360 --> 06:47.360
all the way to our grandest ambitions, you know, which is maybe making AGI. There's kind of this

06:47.360 --> 06:52.320
assumption of one that in this first phase, we'll be able to manually identify all of these building

06:52.320 --> 06:57.120
blocks and the right version of them. And then two, at some point, there's got to be some phase two

06:57.120 --> 07:02.480
where we put all these pieces together. And that is just a herculean challenge if you really

07:02.480 --> 07:06.320
think about it. And I think we should be clearied about how hard that might be to take all of the

07:06.320 --> 07:10.000
different things that all of the community is building and stick them all together and tune all

07:10.000 --> 07:14.800
their hyper parameters and make them interoperate perfectly. I mean, just think about debugging that thing,

07:14.800 --> 07:19.680
let alone building it in the first place. It would require an Apollo scale or Manhattan project

07:19.680 --> 07:26.320
scale effort, in my opinion, to pull that off. However, I think that if you look in the history of

07:26.320 --> 07:32.320
the last 10, 20 years of machine learning, there is a undeniable trend. And that is that hand

07:32.320 --> 07:38.960
design pipelines give way to learn pipelines as you have more compute and data. So let's take some

07:38.960 --> 07:46.080
examples. The classic and first one, the shot herred on the world, with features inside of pipelines,

07:46.080 --> 07:51.840
like vision, language, speech detects, et cetera. You know, we used to hand design all of our

07:51.840 --> 07:55.840
features and then learn a little bit on top of those features. People like Jan LeCune and Yasha

07:55.840 --> 07:59.840
Benjio and Jeff Hinton were saying, let's just learn the whole thing. They were right and everybody

07:59.840 --> 08:04.240
knows that now. But that same exact trend has applied over and over again on other topics. So

08:04.240 --> 08:08.400
look at architectures. We used to design them by hand, we still mostly do. But increasingly,

08:08.400 --> 08:13.520
the best architectures are learned or at least they're competitive. RL algorithms themselves,

08:13.520 --> 08:19.120
like learning to reinforcement learn, hyper parameter tuning, even data augmentation pipelines,

08:19.120 --> 08:22.640
et cetera, et cetera, et cetera. The writing is on the wall. Once we get a lot of compute and

08:22.640 --> 08:27.280
data, we let machine learning do the heavy lifting, we should just learn the whole thing. So what I

08:27.280 --> 08:32.160
argue is that we should apply that thinking to the process of our grandest ambitions of creating

08:32.160 --> 08:36.400
AGI itself and say, instead of trying to manually design all of the pieces of some

08:36.400 --> 08:41.840
Rube Goldbergian thinking machine, let's just try to get out of the way, set up the system such

08:41.840 --> 08:46.400
that compute and data and machine learning can do the heavy lifting for us. And let's try to

08:46.400 --> 08:50.800
learn the whole thing. And so if you want to make progress on that, that would be this thing

08:50.800 --> 08:54.720
that I call an AI generating algorithm. It's basically AI that makes better versions of itself.

08:54.720 --> 08:59.840
It starts simple and it bootstraps itself up from simple origins all the way potentially to

08:59.840 --> 09:04.320
AGI. And if we want to make progress, I think we have to push on three pillars. One, we have to

09:04.320 --> 09:08.720
meta-learn the architectures. Two, we have to meta-learn the learning algorithms themselves.

09:08.720 --> 09:13.680
And three, we have to automatically generate the learning environments so that the system can

09:13.680 --> 09:18.080
learn forever and it's not trapped within one specific domain. And so that's the idea of an AI

09:18.080 --> 09:23.920
generating algorithm. And I think it might be the fastest path to produce AGI. And even if it's

09:23.920 --> 09:28.960
not, it's still scientifically interesting because it teaches us how can a system like Darwinian

09:28.960 --> 09:33.920
evolution have produced us. It strikes me that the way AI generating algorithm, the way you kind of

09:33.920 --> 09:42.800
pose that problem, it is maybe there's a pushback a little bit. It's kind of an accelerator to building

09:42.800 --> 09:52.320
the building blocks, but to kind of switch rails from kind of this broader, more holistic approach,

09:52.320 --> 09:59.760
we need to, we kind of have to figure out what's the objective function on general intelligence.

09:59.760 --> 10:05.200
I'm like, we have no idea how to do that. Completely agree. I'm not saying it's going to be easy.

10:06.320 --> 10:13.440
But and I think that you hit upon one of the central grand challenges in an effort like this.

10:13.440 --> 10:17.840
Like, if we want to make such a system work, we have to figure out, yeah, what is the reward

10:17.840 --> 10:22.720
function or the loss function for the overall system? What is it trying to optimize? And how,

10:23.680 --> 10:29.600
how can we create what's called an open-ended algorithm that will keep going forever

10:30.320 --> 10:35.280
and not stagnate? But I don't think that we lack ideas on this front. I think this is a

10:35.280 --> 10:38.800
place where we can do a lot of research. And I think there's probably a touring award out there

10:38.800 --> 10:44.240
for somebody who can figure out the right answer to that question. So I think it's a fascinating

10:44.240 --> 10:48.800
kind of grand challenge of science to figure out what kind of a loss function could you put in

10:48.800 --> 10:53.360
such a system such that it would bootstrap itself all the way up from, you know, almost being

10:53.360 --> 10:58.480
completely unintelligent all the way past human intelligence. Yeah, and, you know, I kind of pose

10:58.480 --> 11:05.200
that as a way to kind of fine tune my understanding of what you're trying to accomplish with your

11:05.200 --> 11:12.160
work around AI-generating algorithms. Do you see that work as being applied to kind of evolving,

11:12.160 --> 11:18.080
you know, that reward function in such a way that, you know, you get us beyond building the

11:18.080 --> 11:24.160
individual components or, you know, are you focused on kind of evolving those individual components

11:24.160 --> 11:29.120
more quickly without really understanding how they get us to, you know, this broader way of thinking

11:29.120 --> 11:38.240
about, you know, getting to AGI. Yeah, so that very interesting question inspires me to make

11:38.240 --> 11:43.760
three different comments because there's three things that are related there. So one thing is that

11:43.760 --> 11:47.680
I think that one of the hardest things is trying to figure out how do you take all these different

11:47.680 --> 11:52.640
pieces that like the community might be building and put them all together in the way that really

11:52.640 --> 11:57.760
works with the kind of elegance and beauty and effectiveness of, say, the human mind.

11:59.360 --> 12:04.000
And so I think rather than trying to separately create components even with a system that was

12:04.000 --> 12:09.520
learning the components and then later like figure out how to put them together, the algorithm should

12:09.520 --> 12:15.120
do all of that work for us, you know, it should say, you know, you've got system one type thinking

12:15.120 --> 12:19.280
fast and slow, you know, system one, system two, fast and so you've got hierarchical RL with

12:19.280 --> 12:23.520
different levels of abstraction, you know, that you have long term planning and short term planning

12:23.520 --> 12:28.160
and continual learning and all these things have to work well together and we should just basically let

12:28.160 --> 12:34.800
the an AIGA figure out how to create those pieces and make them work well together. So this path

12:34.800 --> 12:40.080
would not be trying to separately create these components onto the reward function issue. There's

12:40.080 --> 12:44.480
actually two fascinating questions there. One is kind of what would the reward function be for an

12:44.480 --> 12:52.240
individual, individual agent in the system? And one thing that I think is fascinating is the system

12:52.240 --> 12:59.680
itself would probably end up creating at least all of the intrinsic rewards for the agent. So evolution,

12:59.680 --> 13:04.080
there's debate about exactly what it's what it's reward function is, but let's assume itself

13:04.080 --> 13:09.520
replication, for example, with that simple kind of higher level reward function, look what it did

13:09.520 --> 13:15.360
in your brain, you know, if you're like me, you love ice cream and you love guy, you know, seeing

13:15.360 --> 13:21.600
beautiful views and you love running up mountains and you might be interested in romantic partners

13:21.600 --> 13:26.480
and you probably really don't like putting your hand on a hot stove. You're probably also curious

13:26.480 --> 13:31.680
and you like to play and learn and you probably care about what your peers think of you. You've got

13:31.680 --> 13:37.360
all of these different rewards kind of baked into that motivates you to do the things that you do

13:37.360 --> 13:42.800
every day, every week, every year, right? Where did those come from? Well, basically evolution figured

13:42.800 --> 13:50.320
out that if it makes beings that have those kind of internal rewards, they are more effective at

13:50.320 --> 13:55.440
this other thing which is self replication. And so I also don't think we should be trying to hand

13:55.440 --> 13:59.760
specify like we do often in machine learning, oh, it should be curious, it should try to go to new

13:59.760 --> 14:05.760
states, it should probably try to maximize its reward, it should probably try to interact with other

14:05.760 --> 14:09.280
people and maybe it should try to communicate with other people, we'd like manually design all

14:09.280 --> 14:14.320
these like hack down rewards that we inject in the system in the hopes to get to do what we want.

14:14.320 --> 14:19.440
I think instead we probably need to be looking for that outer loop meta reward, it's really kind of

14:19.440 --> 14:25.120
general and probably simple. And with enough compute, the internal stuff all kind of shows up in

14:25.120 --> 14:29.600
the system and it kind of knows, it kind of creates a ton of intrinsic rewards like paying in

14:29.600 --> 14:34.320
pleasure and curiosity, et cetera. So then the question is what is that outer loop reward function,

14:34.320 --> 14:39.440
which is this is the third thing I wanted to talk about. We don't know exactly what that is,

14:39.440 --> 14:43.280
but I will just give you a sketch and I'm sure this is wrong, it's not the right answer,

14:43.280 --> 14:47.040
but it feels like it's pointing in the right direction and suggests the research we could do.

14:47.040 --> 14:54.960
What if you had an agent that was motivated to learn somehow, it's like wants to continually

14:54.960 --> 15:01.760
learn new things and the things that it wants to learn, we find interesting or useful and by

15:01.760 --> 15:07.040
we, I mean humans. And so maybe somehow some way it's grounded to our world, maybe it has to

15:07.040 --> 15:12.320
get better and better at solving real world problems or making money on earth or just making humans

15:12.320 --> 15:18.720
think that what it's doing is worthwhile or imitating YouTube and things that we do.

15:19.360 --> 15:25.680
That basically allows broadly writ a system that can learn forever and not learn to memorize

15:25.680 --> 15:30.640
white noise patterns, but instead learn things that we find useful. Now that's overly simplistic,

15:30.640 --> 15:34.160
it won't work in practice, but that is I think where a lot of the research should be focused,

15:34.160 --> 15:38.960
which is can we figure out those kind of general reward functions plug it into an AIGA

15:38.960 --> 15:42.880
and good things continue to happen forever. And I think that's a really interesting thing to

15:42.880 --> 15:48.160
wait a frame it. If you look out into the natural world, we have seen two at least open-ended

15:48.160 --> 15:53.280
processes that innovate forever. And one of them is a natural evolution that's been going

15:53.280 --> 15:59.280
about 3.5 billion years, continues to surprise us with things like COVID. And human culture is the

15:59.280 --> 16:06.240
other one, right? We continuously, we solve problems and in the process we create new opportunities,

16:06.240 --> 16:09.600
new problems. You create one technology and suddenly it has a cascading effect that will

16:09.600 --> 16:13.840
open up new opportunities and doors. We solve those problems and it generates et cetera, et cetera.

16:13.840 --> 16:16.960
And so the question that I think is fascinating is one of my colleagues put it is,

16:17.520 --> 16:21.680
could we create the computer algorithm that was worth running for a billion years?

16:22.240 --> 16:28.720
That continues to innovate and delight, surprise, and be creative for a billion years. Right now,

16:28.720 --> 16:32.880
when I started my career, the best algorithms weren't worth running for more than a few hours.

16:32.880 --> 16:37.360
I'd say we're now at a point where we have some algorithms that are worth running for about

16:37.360 --> 16:43.600
a month to three, and not much beyond that. But could we create something that would truly innovate

16:43.600 --> 16:47.840
forever? And if we can, then we've made a lot of progress, I think, towards some really

16:47.840 --> 16:53.280
fascinating scientific questions. So you've referenced a couple of times, you've got

16:53.280 --> 17:00.720
a reward function, AIX or scenarioX, you stick it into AIGA and AI Generating Algorithm.

17:02.640 --> 17:08.160
Let's talk about the state of AI Generating Algorithms. Let's make that a little bit more

17:08.160 --> 17:16.400
concrete. What AI Generating Algorithms are out there, how far along are we? How do you think

17:16.400 --> 17:22.800
about the state of that line of work? Yeah, great question. So as I mentioned, there are three main

17:22.800 --> 17:28.960
pillars to an AIGA. You have to learn the architectures, you have to metal learn the learning algorithms

17:28.960 --> 17:33.280
themselves, you have to automatically generate the environments. So we could separately say,

17:33.280 --> 17:36.640
what kind of major work has been done in each of those pillars? And then we could talk about what's

17:36.640 --> 17:40.800
even more fun as like has anything been trying to put those pieces together? Yeah, right.

17:43.360 --> 17:48.720
So first in the automatically learning the architectures has been an explosion of work in

17:48.720 --> 17:52.960
neural architecture search. It's kind of a thriving field and it's doing really well. I don't think

17:52.960 --> 17:55.600
I need to mention the work that's done there. Your readers are probably familiar with it,

17:55.600 --> 18:00.400
but suffice to say that many of the best architectures now are being learned, not hand design.

18:01.040 --> 18:07.120
And I expect that soon will be most. In the second pillar, there is a lot of work that's

18:07.120 --> 18:10.800
been happening over the last couple of years in this area called, often it just goes by metal

18:10.800 --> 18:15.440
learning, but it's basically learning to learn. I was kicked off by these two amazing papers,

18:15.440 --> 18:19.920
RL squared, and learning to reinforcement learning by Jane Wang. You've got the work by Chelsea

18:19.920 --> 18:23.680
Finn and Sergey Levin on mammal. And you have some of my favorite work is the work I'd

18:23.680 --> 18:28.720
open the eye, which happened before I was there on Rubik's Cube, but they basically just take

18:28.720 --> 18:33.360
a big neural net. They asked it to solve Rubik's Cube in a variety of different situations with

18:33.360 --> 18:38.560
different like friction and weight of the cube and the size of the hand, et cetera. And what they

18:38.560 --> 18:43.760
showed, which is just mind blowing, is that it inside of a giant recurrent neural network

18:43.760 --> 18:48.640
figures out a learning algorithm such that when you want to use it at the end of the day,

18:48.640 --> 18:52.560
you've turned off your own SGD. You're not doing a hand design in the algorithm anymore.

18:52.560 --> 18:57.040
You're completely handing it off to the recurrent neural net, which is invented its own way

18:57.040 --> 19:01.840
to conduct experiments in the world, figure out kind of what type of world I'm in,

19:02.400 --> 19:06.560
take the information it has learned from the world, and use it to then get very efficient

19:06.560 --> 19:11.440
in solving the task. And we don't know how it did that. And that's an example of where it figured

19:11.440 --> 19:15.520
out how to like do the thing we wanted to do without us having to hand design it and probably

19:15.520 --> 19:21.920
a very complicated way that we wouldn't have been able to hand engineer. And so that is fascinating

19:21.920 --> 19:27.360
work. And then the final pillar is automatically generating environments. So one of the first

19:27.360 --> 19:32.320
big works in this area was Poet. This worked that I did with Ken Stanley and Joel Layman

19:32.320 --> 19:39.040
and many others at Uber. AI Labs and Ray Rang was the lead on this. And the here the idea is

19:39.040 --> 19:42.880
typically a machine learning. We pick the problem and then we try to like solve it for a

19:42.880 --> 19:46.800
long time. Then we move on to the next problem. So we work on chess for a while, then we go to

19:46.800 --> 19:52.240
go and then we go to starcraft and then we go to dodo. The problem there is that no matter how much

19:52.240 --> 19:57.280
time you run that algorithm on go or starcraft, all you're going to get is a go playing agent

19:57.280 --> 20:01.120
for a starcrafted playing agent. That's it. It's not going to do anything else more interesting

20:01.120 --> 20:05.520
than that. And so the idea behind code is we don't want that. We want the system to produce

20:05.520 --> 20:10.240
the learning challenges forever. And so the way the Poet works is it basically starts out

20:10.240 --> 20:13.520
with an environment. And then once the agent is pretty good at solving that environment,

20:13.520 --> 20:18.400
it creates an entirely new environment that it thinks is close enough that the skills learned

20:18.400 --> 20:22.480
in the first environment will help in the second environment. And it basically keeps going forever

20:22.480 --> 20:26.800
and kind of adding more and more learning challenges to the agent. And as the agent masters them,

20:26.800 --> 20:31.120
it basically has it switched to harder and harder stuff. But not in a linear chain. It's not

20:31.120 --> 20:36.480
headed in any direction, trying to solve one particular final target challenge. It's more like

20:36.480 --> 20:40.320
what you see on earth or in human culture. It's just fanning out and getting better and better

20:40.320 --> 20:44.080
and better and more not more knowledgeable at a variety of different things.

20:44.080 --> 20:47.840
Do you think of that as kind of deriving from some of the work that's been done around

20:47.840 --> 20:53.520
curriculum learning where you're kind of staging out a set of learning objectives to

20:53.520 --> 21:02.160
accelerate an agent's ability to zero in on the specific things and then build on those things?

21:02.160 --> 21:06.240
I do. Yeah, of course, there has been, obviously, decades of work in curriculum learning.

21:06.240 --> 21:10.960
I would say that in my opinion, the history of most curriculum learning work however is,

21:10.960 --> 21:15.280
I have this thing that I want to solve. What is the right curriculum that will get me there?

21:15.280 --> 21:20.640
Yeah, there's a pre-determined target. Whereas Poet is saying, I want to learn everything.

21:20.640 --> 21:29.200
How do I learn forever? There's a wonderful body of work and principles and kind of almost

21:29.200 --> 21:34.720
like a philosophical paradigm that dates back to Ken Stanley and Joel Lehman. I'm this idea that

21:34.720 --> 21:38.560
oftentimes when we pick an objective ahead of time and we try to get there, if it's a simple

21:38.560 --> 21:42.960
objective, we can do it. If it's a really hard objective, we'll probably fail. Usually,

21:42.960 --> 21:48.320
the best way to solve a really, really hard challenge is not to try to get anywhere in particular,

21:48.320 --> 21:52.800
but to try to just learn and go everywhere. Go in any direction, follow any stepping stones,

21:52.800 --> 21:58.880
kind of have serendipity inside of your algorithm. Then, eventually, you will learn the skills and

21:58.880 --> 22:03.920
the knowledge to solve this task that maybe you originally did want to solve, but you shouldn't

22:03.920 --> 22:08.400
try to solve it. There are a lot of examples from the history of technology on this. For example,

22:08.400 --> 22:14.800
if you went back to Melania and you said, I have cooking over a fire and you said, all right,

22:14.800 --> 22:19.120
I'm the king of the universe and I'm the king of earth and I'm only going to found scientists

22:19.120 --> 22:23.120
in my kingdom that will give me better and better cooking technology that will cook things

22:23.120 --> 22:29.040
faster and with no smoke. Well, you will never invent the microwave because to invent the microwave,

22:29.040 --> 22:33.200
you had to have been working on radar technology and notice that a chocolate bar melted in your pocket.

22:34.800 --> 22:38.240
Similarly, if you wanted to invent, you know, the modern computer, go back to the abacus.

22:38.240 --> 22:41.920
Really good device. It's pretty good at computing at the time and you're like, I will fund anybody

22:41.920 --> 22:45.920
in my kingdom and only people who will give me more computing and all of your grant proposals

22:45.920 --> 22:52.000
would be like longer rods, more beads, you know, maybe a 3D abacus, but you would never invent

22:52.000 --> 22:56.880
the modern computer because to do that, you had to have been working on electricity and vacuum

22:56.880 --> 23:02.560
tubes and those are technologies that were not invented because they help with computation.

23:03.920 --> 23:09.040
And so, we're trying to capture that kind of serendipity inside of these AI-generating algorithms

23:09.040 --> 23:13.200
and open-ended algorithms. This is building on a subfield that's now thriving called

23:13.200 --> 23:18.640
quality-diversity algorithms of which poet is one. And the idea is that you basically want

23:18.640 --> 23:26.800
as many high-quality, yet diverse things in a growing archive or library of skill sets or

23:26.800 --> 23:30.720
innovations, whatever it is you're trying to do. And so, poet is like that. It basically says,

23:30.720 --> 23:33.840
I want to create creating environments, totally different environments, and agents that know how to

23:33.840 --> 23:38.000
solve those environments. Overall, the system is getting smarter, the skill level is going up,

23:38.000 --> 23:41.520
the amount of things we know how to do is going up, and that will continuously unlock

23:41.520 --> 23:45.120
new stepping stones and new things that we can do. And eventually, you bubble out,

23:45.120 --> 23:49.440
and in theory, if you did that in the right way, you might get all the way to human-level intelligence.

23:51.200 --> 23:54.400
Now, if you don't mind, I want to go back to finishing my thought on the question of putting

23:54.400 --> 23:59.040
pillars together. I think probably the most exciting work I know of that is put two of the pillars

23:59.040 --> 24:06.960
together, not three, is X-land by Max Yeterbury out of DeepMine. They have this wonderful paper

24:06.960 --> 24:11.440
where they build on the ideas of open-endedness and AI-generating algorithms. And even Max said

24:11.440 --> 24:16.080
on Twitter in exchange, I had that this is an AI-generating algorithm, or I think that's in the paper as

24:16.080 --> 24:20.240
well. And I think it's one of the best first examples of somebody going really big toward this

24:20.240 --> 24:25.600
idea. And what they do there is they basically describe a whole, they say, sample from this huge

24:25.600 --> 24:31.680
space of possible tasks that agents have to play against each other. And so, they sample a task,

24:31.680 --> 24:35.920
they train an agent to solve the task, as the agent levels up, they give it new tasks and some kind

24:35.920 --> 24:40.640
of fanning out curriculum, and they basically try to make it as good as possible as in as many as

24:40.640 --> 24:45.920
possible. And in the end, the system itself trains agents that go from initially not knowing how to

24:45.920 --> 24:53.120
do anything to ultimately being able to zero-shot solve tasks like hide and seek and capture the flag,

24:53.120 --> 24:59.120
and all other kind of games that you might think would be good for agents to just know how to

24:59.120 --> 25:03.760
to solve if they were generally intelligent. So, I think that was great work in that direction.

25:03.760 --> 25:12.240
And in that particular case, how are they demonstrating that generalizability? Are they doing

25:13.040 --> 25:16.160
something analogous to pull it where they're creating new environments, or are they

25:17.280 --> 25:23.280
more hand-picking new environments, and showing that this agent that they've created is generalizable?

25:23.280 --> 25:28.320
Yeah, good question. So, what they did is, like Poet, they basically said, we're going to be

25:28.320 --> 25:32.960
able to describe an environment in a parameter vector. So, normally, we're used to think

25:32.960 --> 25:36.640
of a parameter vector describing the weights of a neural net. Now, the environment is being

25:36.640 --> 25:43.280
described by some descriptor, and they have a search space. So, basically, the huge space of

25:43.280 --> 25:48.720
possible parameter vectors is specify all these environments, and they can sample an environment

25:48.720 --> 25:53.120
and train an agent on it. One thing that they did, which I really like, is they also show that

25:53.120 --> 25:58.960
vector to the agent. So, the agent knows what task it's trying to solve, which is a really good

25:58.960 --> 26:04.640
idea, because it can then learn to generalize and even zero-shot new tasks. So, for example,

26:04.640 --> 26:09.200
if it has learned over time the language of how the worlds are being specified. So, in there,

26:09.200 --> 26:14.400
what it's like, your job is to get the blue ball and take it to the green area,

26:16.080 --> 26:20.240
and not let your enemy get the green ball and take it to the red area. Well, that basically

26:20.240 --> 26:24.640
describes capture the flag. I have to go get the thing, and I have to take it to my area,

26:24.640 --> 26:29.360
and you know, prevent you from doing likewise. And so, they train on enough of these tasks, where

26:29.360 --> 26:33.520
maybe some of the simpler tasks are get the green ball, and then some of the harder tasks are get

26:33.520 --> 26:39.440
the green ball and move it to, you know, move it anywhere. And eventually, it's like this more

26:39.440 --> 26:45.200
complicated thing. And so, in the end, they can basically sample a new task from a hell.task set,

26:45.200 --> 26:51.040
like hide and seek or capture the flag. Give it to the agent. It has learned how to read the

26:51.040 --> 26:56.640
description of the world, and immediately go and solve that relatively complicated game that's

26:56.640 --> 27:02.560
never seen before. What you're describing here, and to some extent, what you're describing with

27:02.560 --> 27:08.240
Poet reminds me of the work that's been done around NetHack, which is kind of specifying,

27:08.960 --> 27:13.040
you know, trying to, and it's been a while since I had that conversation, but I'm thinking about

27:13.040 --> 27:18.960
like, you know, creating this one vector that kind of specifies an environment. Are you familiar

27:18.960 --> 27:24.320
with that work? And if so, how do you think about them relative to, how do you think about NetHack

27:24.320 --> 27:30.800
relative to X-Land and Poet? Yeah, so I did hear about the competition, and I remember that

27:30.800 --> 27:35.280
it's like a symbolic hybrid B.D. learning, which caused quite a splash. That's about the extent of

27:35.280 --> 27:39.840
what I know about it. I will, I'll be able to just say just a few things that one is that I think,

27:39.840 --> 27:46.000
in general, having agents be told the task that trying to solve is very, very helpful. It

27:46.000 --> 27:50.640
shaves off probably orders of magnitude in terms of exploration difficulty. The example I love to

27:50.640 --> 27:56.640
give is like, my dog is very capable, athletic, right? And if I wanted my dog to go around my

27:56.640 --> 28:01.520
apartment and like, pick up all the green things and put them in a bin, my dog could do it if they

28:01.520 --> 28:05.120
knew that was the task that would give them the treat, but I can't just tell that to my dog,

28:05.120 --> 28:09.040
because we don't have the ability to communicate that kind of high level task descriptor. So,

28:09.040 --> 28:12.880
in our old agent, it's in the same boat. If you just put it in a room and say, go, it's never

28:12.880 --> 28:16.320
going to figure out to pick up all the green things and put them in a bin. But if you could tell

28:16.320 --> 28:21.120
the agent that and knew what you meant, and it had some basic athletic skills, that's probably

28:21.120 --> 28:25.760
no longer that hard of a task. So, I like that thing. And also, I think, you know, just the fact

28:25.760 --> 28:30.240
that NutHack is being done in text is a great accelerant, because, you know, as we've seen with

28:30.240 --> 28:34.080
so many things in the last few years, operating in text is cheap and quite powerful because you can

28:34.080 --> 28:42.560
bring, you know, things like GPT to bear on. In the case of X on this vector,

28:42.560 --> 28:49.760
is it specifying the task in specifying the environment or? I think the right, the easiest way

28:49.760 --> 28:54.160
to think about it is that they do create a world, and then they create the description of what you're

28:54.160 --> 28:58.960
supposed to do in that world. And so, the agent is basically told, oh, in this world, you're supposed

28:58.960 --> 29:05.840
to get the green things and take them over to this bin. And it learns over time that, like, basically,

29:05.840 --> 29:12.320
what those words mean, and that learns the skills to do that. Now, one thing you might be

29:12.320 --> 29:17.440
thinking, and I think a lot of people ask me about in, when we talk about the agias, is, you know,

29:17.440 --> 29:22.560
this is what Josh said in one one asked, once asked me, it's just like, this sounds super great,

29:22.560 --> 29:26.240
but how are you going to do this without a planet-sized computer? Because that's what Earth had.

29:28.000 --> 29:35.760
Fair point, Josh. I think that what we have to look at is, where are we going to get abstractions

29:35.760 --> 29:40.960
or innovations that shave many orders of magnitude off of what was required to make Earth work?

29:40.960 --> 29:44.960
Darwinian evolution is very, very unintelligent, and I'm not advocating we even necessarily need

29:44.960 --> 29:49.600
evolutionary algorithms, just to be clear. I'm inspired by evolution, but not advocating that

29:49.600 --> 29:54.320
that technology into the hood. But I do think there's probably many places where we could look

29:54.320 --> 29:59.280
to things that could, you know, save orders of magnitude. So, if you have the right abstraction,

29:59.280 --> 30:03.040
if you have the right domain, where you don't have to simulate chemistry and physical,

30:03.040 --> 30:06.720
like low-level physics, you can simulate things higher level, or even be in an environment,

30:06.720 --> 30:11.360
maybe like net hack, a text only environment. Those things can really help. And another thing I

30:11.360 --> 30:19.040
think that it can help is to, you know, steal a quote from Newton and riff on it, that I think AI

30:19.040 --> 30:24.240
will go farther if it stands on the shoulders of giant human data sets. And so we've seen that

30:24.240 --> 30:30.080
with GPT and all the unsupervised pre-training right now is that we can basically take a huge step

30:30.080 --> 30:34.240
forward just by, you know, starting from where humans are at. Now that we know how to learn from

30:34.240 --> 30:38.880
human data. And so my team at OpenAI recently had a paper that was accepted at NURPS this year

30:38.880 --> 30:44.400
called Video Pre-Training, which is a nod to GPT. And the idea there is that we have a very

30:44.400 --> 30:49.440
simple way that you can go on to the internet and have AI learn by watching YouTube,

30:50.400 --> 30:55.440
and learn how to act by watching YouTube. So mostly in games like using your computer or,

30:56.240 --> 31:00.720
you know, maybe like playing Minecraft or whatever, there's all sorts of video tutorials online

31:00.720 --> 31:05.040
trying to do that. The trick was that we didn't know how to get the labels. So if you're trying to

31:05.040 --> 31:11.040
generate text or generate pixels or images or generate music, all the labels are out there, right?

31:11.040 --> 31:14.240
From the previous text, you just have to predict the next word. The next word is right there on

31:14.240 --> 31:19.200
whatever web page you're scraping. The problem in videos of people acting, whether they're humans

31:19.200 --> 31:24.080
or robots or video game characters, is you see the agent doing its thing, but you don't know what

31:24.080 --> 31:28.800
buttons it's pressing, like how it's moving its mouse or what keyboard, what keys it's pressing

31:28.800 --> 31:34.560
as keyboard. So the simple idea behind VPT is that we just train a simple model that basically learns

31:34.560 --> 31:39.920
to predict, oh, in Minecraft, if all of a sudden the character like, you know, jumped, you must have

31:39.920 --> 31:44.560
hit the jump button. Or if it placed a block, you hit the place block button. That's a pretty simple

31:44.560 --> 31:49.440
task. We train a little model to look at, to like learn how to do that. Then we run it across,

31:49.440 --> 31:55.920
you know, years and years of video of people playing Minecraft on YouTube. We then get label data

31:55.920 --> 32:00.320
for all of these videos on YouTube of how to play the game. We then pre-trained a model just like

32:00.320 --> 32:06.960
GPT to go from past to what's the next action. And now we can zero shot have the agent out of

32:06.960 --> 32:12.640
pre-training do very complicated behaviors. And with that baseline, we can then fine tune the

32:12.640 --> 32:17.280
agent to solve any task we want it to do in Minecraft or, you know, very hard tasks. And what we

32:17.280 --> 32:21.760
show in the paper is that, you know, zero shot, the agent's able to do really complicated things

32:21.760 --> 32:27.360
to take humans like two to five minutes. But once you give it a challenge, if you don't pre-trained,

32:27.360 --> 32:31.520
you have no hope of solving the challenge. But with this pre-training, you can end up learning

32:31.520 --> 32:35.920
things like getting diamond tools in the game that take humans more than 20 minutes. And I think

32:35.920 --> 32:41.120
it's like 24,000 sequential actions to achieve. And you can learn it relatively easily with

32:41.120 --> 32:46.480
reinforcement because you started off by standing on a shoulders of giant human datasets. So to

32:46.480 --> 32:52.080
me, that is kind of like how the era of pre-training fits into AAGAs is that human datasets can be

32:52.080 --> 32:57.760
this huge catalyst or this huge level up that allows us to skip all the bootstrapping. Maybe the

32:57.760 --> 33:01.680
first couple billion years of evolution get right to the good stuff where we have an agent that

33:01.680 --> 33:06.320
understands language, understands our world, maybe knows how to act in that world. And then now we're

33:06.320 --> 33:10.400
challenging it to go off and like learn new types of science and math and learn new skills and

33:10.400 --> 33:14.880
like solve cancer and do things that we aren't even able to do yet. And so I think that's kind of

33:14.880 --> 33:19.200
one of these exciting accelerants that might mean we don't need a platform as computer to pull

33:19.200 --> 33:23.760
us off. In what ways do you see the VBT work as being generally applicable, meaning

33:24.400 --> 33:29.680
are is it a set of techniques for teaching an agent to get really good at playing Minecraft or

33:31.440 --> 33:39.920
is it something that we can generally apply to video understanding or video, you know, inferring

33:39.920 --> 33:44.160
you know actions from videos? Yeah, I think it's very general. I think that you could use this

33:44.160 --> 33:52.720
technique to learn how to do whatever it is that you see people on YouTube doing. Now there's

33:52.720 --> 33:57.440
some major caveats. First of all, it's much easier, you know, if you're talking about a computer

33:57.440 --> 34:02.480
usage. One of the things that we did in the Minecraft work is very intentionally we said we're

34:02.480 --> 34:07.600
not going to go for a handcrafted Minecraft action, you know, a space where you have like a macro

34:07.600 --> 34:13.200
where I hit a button and it like does this big complicated thing like chop sound a tree or places

34:13.200 --> 34:19.600
of lock or crafts a crafting table or whatever. We basically within the game of Minecraft most humans

34:19.600 --> 34:24.960
play it by using a mouse and keyboard. And so we said our agent has to learn the same thing, it has

34:24.960 --> 34:29.680
to learn how to use a mouse and use a keyboard. And it has to just remind craft figure out how to

34:29.680 --> 34:34.160
not only play the game but also within the game there's all these little tiny graphic user interfaces

34:34.160 --> 34:38.400
where you have to drag and drop little icons and click on buttons and browse through like recipe

34:38.400 --> 34:42.240
books and all sorts of stuff. It looks a lot like just learning how to use a computer like changing

34:42.240 --> 34:46.960
your preferences and like dealing with menus and Microsoft Word and things like that. So I think

34:46.960 --> 34:52.960
this technique is very general and basically could for example have an agent learn how to generally

34:52.960 --> 34:58.480
use a computer if we got enough tutorial videos of how to use a computer. There's some unsolved

34:58.480 --> 35:03.120
issues like different keyboard shortcuts and different programs do different things. So the

35:03.120 --> 35:06.400
challenge is a little harder for that thing that has to predict what action must have been taken

35:06.400 --> 35:12.080
given to video I've seen. But I think that's like a research problem that is solvable. You could even go more

35:12.080 --> 35:16.400
crazy to something like robotics and say I can maybe learn by watching robotics videos but now you

35:16.400 --> 35:20.800
have to solve a couple of extra problems like how do I infer what action must have been taken for whatever

35:20.800 --> 35:24.240
rub but I end up wanting to use it with other actions in that rub that are a little bit different.

35:24.800 --> 35:29.120
So there are some things to solve but I think it's um directionally right that you know this is

35:29.120 --> 35:33.440
probably an easy path to extract a lot of the knowledge of learning how to act by watching

35:33.440 --> 35:38.800
video demonstrations. And at a minimum anything that's in the computer states mouse and keyboard

35:38.800 --> 35:43.920
that covers that covers so much. Think of all the things you can do on a computer with you know

35:43.920 --> 35:48.000
just a mouse and keyboard and this system you know in theory should be able to learn how to do all of

35:48.000 --> 35:53.920
that as well. And so how do you say a tying into the idea of AI generating algorithms?

35:53.920 --> 35:58.880
Yeah well I you know so imagine for example that we like right when we did poet or when they did

35:58.880 --> 36:06.320
X land the agent basically starts out knowing almost nothing and it has to learn how to pick up

36:06.320 --> 36:11.360
the green ball and go you know get you to take it to the to the blue area or in poet it has to

36:11.360 --> 36:15.520
learn how to walk or if it was Minecraft it has to learn from you know if we didn't pre-trained

36:15.520 --> 36:19.840
to learn from scratch how to do all this stuff that's very computational inefficient.

36:19.840 --> 36:26.400
So if you had this pre-trained model that you can use to seed the agents you know give the

36:26.400 --> 36:31.680
agent some kind of common sense understanding we might refer to it as about its environment and

36:31.680 --> 36:37.920
how actions correspond to outcomes then it could stand on the shoulders of giants as you put it

36:37.920 --> 36:43.760
paraphrasing Newton. Exactly so imagine if it comes with GPTA skills so it knows all of human

36:43.760 --> 36:47.920
language it knows how to talk to you it knows how to get instructions on what it should do it

36:47.920 --> 36:52.080
also knows how to move around the world it knows how to do things on a computer or walk around

36:52.080 --> 36:58.400
a 3D video game it knows how to play Starcraft, Dota, Doom, Chess go it knows all that stuff and

36:58.400 --> 37:04.400
then you kick off this process. Now you're basically you're so far down the road that you probably

37:04.400 --> 37:08.640
don't need that many more pieces to create a system that's truly kind of can auto-catalycally

37:08.640 --> 37:12.960
learn tremendously interesting things and so you're skipping over that really inefficient thing

37:12.960 --> 37:16.720
where it's learning to see and it's learning to understand language and it's learning how to

37:16.720 --> 37:22.720
move its body so just like GPT went from you know think about the world like the pre-NLP world

37:22.720 --> 37:27.360
before GPT you know we had hacked together all this manual stuff and it kind of worked but not

37:27.360 --> 37:32.560
really and all of a sudden the GPT paradigm just like wow AI is exciting and it can do amazing

37:32.560 --> 37:37.040
things that it's metal-learning and it can almost be human level but now apply that to learning to

37:37.040 --> 37:43.840
like act in three dimensional worlds on computers you know I think that that is such a huge

37:43.840 --> 37:49.120
place you know such a huge leapfrog forward as a place to start from when you're trying to do

37:49.120 --> 37:57.840
something that create an open-ended system in an AI generator. Going back to Poet, a key element

37:57.840 --> 38:08.960
of what you're trying to do there is you're creating new environments that serve the purpose of

38:08.960 --> 38:21.440
teaching the agent new things and I want to try to to kind of dive into how direct it is

38:21.440 --> 38:30.080
is that the you know the n plus 1th environment is that you know designed again some you know

38:30.080 --> 38:36.000
reward function that is optimizing for learning or tries to predict the amount the agent will

38:36.000 --> 38:40.320
learn or is it more of our random we're going to create a bunch of environments and hope the

38:40.320 --> 38:47.840
agent learns something like tell me more about the algorithmic aspect of that. Yeah great question

38:47.840 --> 38:52.400
so in Poet we were in trying to produce an initial proof of concept with a lot of these ideas

38:52.400 --> 38:57.200
a lot of the individual pieces are pretty simple so in that work it was a simple evolutionary

38:57.200 --> 39:02.240
based system you take a description of an environment you mutate it which means you just change

39:02.240 --> 39:05.520
some of the numbers in the parameter vector that describes the environment that produces a

39:05.520 --> 39:09.840
slightly different environment and then you basically keep it if it's not too hard and not too

39:09.840 --> 39:15.440
easy for the agent which is basically a proxy for will it learn on it or not. Now you definitely

39:15.440 --> 39:19.280
could get more intelligent and I think this is a fantastic direction to go where you train a

39:19.280 --> 39:25.440
neural network model whose job it is to look at the history of what this agent has learned so far

39:25.440 --> 39:30.080
and intelligently give it the next learning challenge that it thinks is right at the cusp of

39:30.800 --> 39:34.320
I think you could like you're ready for this you know and this is where you're going to have

39:34.320 --> 39:39.200
maximal learning progress but also that the task itself is worth learning is interesting.

39:39.200 --> 39:45.280
Well even even in the case of Poet are you are you predicting whether it's too hard or too

39:45.280 --> 39:51.760
easy or are you letting the agent try and kind of measuring progress and you know doing things

39:51.760 --> 39:57.600
like early stopping or does the agent actually have to do it and that's when you know if it's too

39:57.600 --> 40:01.760
hard or too easy. Yeah so in that work we took a pretty easy approach which is we created the

40:01.760 --> 40:08.720
environment we tested the current agents that we had so far and we had hand to find for this

40:08.720 --> 40:13.760
particular type of environment if you're basically have a score that's not too low and not too high

40:13.760 --> 40:18.080
we'll say that's like that's in the middle and then after a while if you haven't been learning on it

40:18.080 --> 40:23.120
I think we can get out but that's you know that's kind of in in retrospect since then there have

40:23.120 --> 40:27.760
been people that have been working on versions that don't have these kind of hand defined limits

40:27.760 --> 40:32.160
they're automatically basically trying to estimate learning progress and saying if I'm seeing

40:32.160 --> 40:37.360
learning progress then keep going. For example so work we had out of OpenAI before VPT

40:38.160 --> 40:43.440
it came out of Ingmar was the Lolita author came out of our team at OpenAI basically was

40:43.440 --> 40:47.120
doing a different technique that was trying to measure learning progress on a task so this was

40:47.120 --> 40:52.560
also in Minecraft we give you a challenge and we say you know initially we'll give you a challenge

40:52.560 --> 40:56.720
and we'll search your records statistics on it and if it looks like after a few trials

40:56.720 --> 41:01.360
that you are in fact in the sweet spot of learning then we'll keep it and if not we'll basically

41:01.360 --> 41:04.880
stop sampling that as much so you're trying to basically it's almost like a bandit problem you're

41:04.880 --> 41:08.640
pulling all these arms and if you start to get learning you keep pulling that until you run

41:08.640 --> 41:12.480
you stop getting learning and then you go on to find something else and there's lots of different

41:12.480 --> 41:16.720
ways that you could approach this probably this problem but I want to point out what I think is

41:16.720 --> 41:20.960
probably the most interesting challenge which is I mean learning progress is hard to get right no

41:20.960 --> 41:27.600
question but then the other question is is it interesting is the challenge worthwhile because

41:27.600 --> 41:32.720
almost all of the learning progress based systems have pathologies that when you start to optimize

41:32.720 --> 41:38.400
for learning progress you start to get environmental challenges the agent shows huge learning progress

41:38.400 --> 41:45.680
on that are totally worthless imagine if I told you that your job was to I don't know like you

41:45.680 --> 41:50.800
let's say you're you're running an obstacle course your job is to get I invented a whole obstacle course

41:50.800 --> 41:55.440
and it had like you know balance beam and like a wall at a rope you know ladder you had to climb and

41:55.440 --> 42:00.240
you did really well okay so you're pretty good on that task and then I just tack on at the end

42:00.240 --> 42:05.840
like wiggle your arms in exactly this particular way but it's just a little bit of that at the end

42:05.840 --> 42:10.320
well okay so you first did the obstacle course maybe we considered that interesting and at the end

42:10.320 --> 42:13.920
you can get some learning if you happen to luckily figure out how to do wiggle your arm in

42:13.920 --> 42:17.760
exactly the right way okay and now you're no longer after you learned that you no longer

42:17.760 --> 42:21.440
learning anything because you figured out the right wiggling I just tack on a little bit more

42:21.440 --> 42:25.680
different random wiggling this is kind of the equivalent of of memorizing white noise but in

42:25.680 --> 42:31.120
motor space and I could probably endlessly add just a little bit more stuff for you to memorize

42:31.120 --> 42:35.200
that is not going to generalize to our world or to even other environments it's just totally

42:35.200 --> 42:40.720
uninteresting now that's super that's super interesting how do you characterize that interestingness

42:40.720 --> 42:47.520
so that you can optimize for it yes welcome like decades of thinking in like open-endedness trying

42:47.520 --> 42:51.360
to figure out the answer to this problem the short answer is we don't know yet but I have some

42:51.360 --> 42:56.720
ideas we're currently working on in my lab now so stay tuned but I'll give you a flavor of some ideas

42:56.720 --> 43:01.840
that are not exactly our current best ideas but you know in the spirit of them imagine for example

43:01.840 --> 43:06.960
if you had to learn some skills in one environment that give you learning progress and they also

43:06.960 --> 43:11.600
help you generalize to other environments that's already a little bit better because you're not

43:11.600 --> 43:17.760
going to memorize random motor twitches another idea you could do is you could grind into the real world

43:17.760 --> 43:23.840
so you have say a held up set of tasks from real games or the need to make money in the world or

43:23.840 --> 43:29.520
to imitate you know animals on YouTube or something and you have to like practice if they the

43:29.520 --> 43:33.920
the environment has obstacle courses maybe you have to learn to break your client if you never see

43:33.920 --> 43:40.240
a human wiggling their arms in in the wild then it's probably not useful is that kind of the idea

43:40.240 --> 43:45.840
or if learning that skill practicing on that wiggling task doesn't make you better at like being

43:45.840 --> 43:53.120
American ninja or imitating you know American like humans in the American ninja obstacle course game

43:53.120 --> 43:57.440
or imitating orangutan breaky eating or something like that if you don't get better at doing

43:57.440 --> 44:01.120
something we care about by training on these tasks then maybe we consider those tasks to be

44:01.120 --> 44:05.520
uninteresting so the system that's generating those tasks then has an extra challenge it's not

44:05.520 --> 44:10.640
just trying to get learning progress because that's not sufficient it's necessary but not sufficient

44:10.640 --> 44:15.760
it also has to generate tasks that are learning useful skills that transfer and help us solve

44:15.760 --> 44:20.160
all the problems that we do care about and that all of a sudden starts to feel like okay maybe I

44:20.160 --> 44:25.280
can see how that wouldn't have massive pathologies it might actually get us to worry why yeah it it

44:25.280 --> 44:31.040
uh to your point it strikes me that this is a super interesting problem because

44:33.440 --> 44:40.880
there's a big risk in like a big part of what we're trying to do is use algorithms and the

44:40.880 --> 44:48.320
computer you know compute and data to allow the system to you know create and learn from you know

44:48.320 --> 44:58.560
discontinuous patterns right that you know that we're not going to produce um but part of what

44:58.560 --> 45:05.920
you're trying to do with this interesting the interestingness you know regularization let's call

45:05.920 --> 45:15.760
it is like um it is to kind of dampen just kind of random things so it's like you you you want to

45:15.760 --> 45:24.720
capture and you know amplify randomness that's useful but uh but kind of suppress you know

45:24.720 --> 45:30.400
randomness it's not useful it sounds really hard without some kind of signal or ground truth or

45:30.400 --> 45:36.320
something that's going to tell us and part of the point is like you don't if you think about it

45:36.320 --> 45:42.400
from an evolutionary perspective you don't know what kind of step evolutions are going to

45:42.400 --> 45:48.400
ultimately produce the success right yeah I it's super hard I think that's why it's fascinating

45:48.400 --> 45:53.120
I consider this to be one of many grand challenges it's kind of laying over here in this general

45:53.120 --> 45:58.480
area of research and AI generating algorithms um but it also doesn't feel hopeless um I feel like

45:58.480 --> 46:03.040
maybe take your net hack domain or a text domain if you could if the system could generate the right

46:03.040 --> 46:08.640
kind of problems basically like a teacher generating like lesson plans like work on this math problem

46:08.640 --> 46:12.960
work on write this essay etc and periodically you were testing how they're doing out there in

46:12.960 --> 46:17.920
the world at like helping us with our text-based problems or is it a better thing for an API of

46:17.920 --> 46:23.920
people who want to use a language model um maybe there's some maybe maybe there's enough learning

46:23.920 --> 46:27.840
signal there that could figure out the right sort of challenges to generate especially that again

46:27.840 --> 46:31.520
if it's pre-training on what human teachers provide to students right and that's where you get

46:31.520 --> 46:36.240
that big catalyst where you're not just trying to like do it from some from like from just randomly

46:36.240 --> 46:40.400
generating problems and hoping that one of them happens to help you but you can take advantage

46:40.400 --> 46:45.520
what we know about teaching maybe just get better at it yeah maybe switching gears a little bit um

46:46.320 --> 46:54.480
but continuing in the broader theme of AGI how do you think about the the safety challenges of AGI

46:54.480 --> 47:02.880
and the context of AI generating algorithms great question so I am in general very

47:02.880 --> 47:09.760
concerned with the safety challenges regarding AGI so putting aside the specific concerns that

47:09.760 --> 47:15.120
might come up with AI generating algorithms I think we as a community are playing with fire and

47:15.120 --> 47:21.200
we're doing we're creating a very powerful technology and we have to be very careful that we do

47:21.200 --> 47:27.520
it in the right way um and I'm not even honestly sure that you know if you gave me a button and

47:27.520 --> 47:31.840
said could you pause a AGI development so we can figure out the right way to make it benefit

47:31.840 --> 47:36.560
all humanity would you hit that button I probably would but I think that humanity doesn't have a

47:36.560 --> 47:44.240
good track record I would because I'm so concerned about like the potential negative impacts that we

47:44.240 --> 47:50.400
need time to get this right however I think that a humanity basically doesn't have a very good

47:50.400 --> 47:55.600
track record of not inventing things when they can and like you know it's going to be the case

47:55.600 --> 48:00.800
as somebody makes progress so condition on that I think that we need to make sure that we do the

48:00.800 --> 48:07.760
best job that's possible maximize the potential upside and minimize the downside that's just my

48:07.760 --> 48:14.240
comments on AGI in general now there are specific ethical concerns that come up with AI GA's that I

48:14.240 --> 48:21.360
think are interesting one of them we get to that if I can okay I'm sure okay is in what ways is

48:22.480 --> 48:29.600
is your working on AI not the exist you know proof that you wouldn't actually hit that button

48:29.600 --> 48:38.880
well it's because of that very important caveat that I don't think that humanity will stop working

48:38.880 --> 48:43.840
on it and so therefore I think that we mean it doesn't really matter if you individually hit the

48:43.840 --> 48:48.480
button unless everybody hit the button hits the button so it's like a game theory kind of problem

48:48.480 --> 48:55.840
in a sense yeah yeah yeah I don't know that I like the words it doesn't matter because I think

48:55.840 --> 49:00.240
another way to say it is it's inevitable and so we might as well I might as well do my best to

49:00.240 --> 49:04.240
try to make sure that it goes as positively as possible that's kind of how I think about

49:06.000 --> 49:09.280
that's why if you gave me this button that I could stop everybody from working on it so we could

49:09.280 --> 49:14.480
take 20 years and discuss the right way to do it I'd probably hit that button so if you're

49:15.440 --> 49:19.360
I could tell you about what I think are some AI GA specific ethical concerns but also this is

49:19.360 --> 49:25.360
an important topic so I'm happy just just to talk about AI safety as well and now we can you know

49:25.360 --> 49:31.760
we'll we'll continue to talk about it but we can jump into the AIGA part okay so you might say

49:31.760 --> 49:37.600
all right well is something there's something more risky about AIGAs than the manual path to AI

49:37.600 --> 49:44.080
for example I think there are some things that might be more risky one of them is that it's almost

49:44.080 --> 49:49.840
by its nature trying to create kind of a auto catalytic process that's getting bootstrapped thing

49:49.840 --> 49:54.960
itself up and so and any given instance of the algorithm you might finally stumble upon the right

49:54.960 --> 49:59.600
ingredients and boom you create this kind of lightweight process so you're kind of looking in

49:59.600 --> 50:06.400
some sets for something that is like a fast takeoff or at least a medium takeoff in a way that

50:06.400 --> 50:12.000
maybe the manual path won't have I think increasingly that's less less obvious that the manual path

50:12.000 --> 50:17.440
also might not have that you know maybe GPT-6 just suddenly has the power to do amazing you know

50:17.440 --> 50:24.160
like this AGI and can control the world's digital systems so who knows but at least in AIGA

50:24.160 --> 50:28.000
as you're looking for that I think there's another one going back to this issue of qualia

50:28.000 --> 50:34.320
and philosophy and that is you know what if I what if we could create a system that had like a

50:34.320 --> 50:38.080
whole bunch of agents learning a whole bunch of different tasks interacting with each other

50:38.080 --> 50:43.680
and we're basically simulating an entire human civilization or at least maybe sorry it's

50:43.680 --> 50:48.000
simulating an earth up to the dawn of female civilization and then AGI is like once you get

50:48.000 --> 50:53.040
humans in this simulation well how much untold suffering what you have caused in all of these

50:53.040 --> 50:58.240
digital beings you know like like who is responsible for all the pain and suffering of dinosaurs

50:58.240 --> 51:02.960
and hyenas and lions and their ancestors and is that a price we're willing to pay I think it's

51:02.960 --> 51:08.320
interesting if we knew that these creatures were actually suffering is it worth it to get to AGI

51:09.120 --> 51:13.680
that's something we should consider and a final thing that I think is really interestingly unique

51:13.680 --> 51:21.360
to AGI is more so than many other attempts is that if and this is a big if but if you tried to create

51:21.360 --> 51:27.360
your outer loop meta reward function that we talked about to be similar to the one on earth which

51:27.360 --> 51:33.280
might be just like kill and don't be killed survival then maybe you recreate the red and tooth and

51:33.280 --> 51:40.080
claw situation that happened on earth and so the values and the instincts of the entities that

51:40.080 --> 51:45.120
come out of it are also potentially like us maybe they're selfish and they're violent and they

51:45.120 --> 51:50.800
are deceptive etc you I think it'd be really awesome and interesting if we could create

51:50.800 --> 51:54.800
versions of AGI's where the outer loop thing is very cooperative and you don't end up with those

51:54.800 --> 52:00.240
kind of vices and you get more virtues and I also don't know to what extent the manual path

52:00.240 --> 52:05.840
is going to create vices instead of virtues but thinking about kind of the values of the the

52:05.840 --> 52:12.160
AGI that we create is absolutely essential like we need its values to be aligned with ours and

52:12.160 --> 52:17.200
to the extent that it doesn't have our vices that dramatically increases the chances that things

52:17.200 --> 52:25.680
go well once we make AGI. Yeah I think there's maybe an argument that of all of the ways the folks

52:25.680 --> 52:33.760
are trying to get to AGI the AIGA is probably the most dangerous and that you are specifically

52:34.720 --> 52:42.080
trying to enable the machine to you know create its own environments and as we discussed

52:42.080 --> 52:47.120
earlier you know that may lead to creating its own reward functions right because that's going

52:47.120 --> 52:55.040
to be the big challenge and that seems specifically like a point where you lose control.

52:55.040 --> 52:58.880
Totally agree and I make that exact point in the paper on AGI generating algorithms

53:00.800 --> 53:05.360
however if that is a con an important con I think we should need to do research to try to

53:05.360 --> 53:10.080
like minimize the possible downsides of that con but let me also express what the pro is that

53:10.080 --> 53:16.720
comes along with that and that is that if we believe that the manual path is kind of designing

53:17.840 --> 53:22.240
AI it's probably we're probably going to be designed in our image and it's similar to us and

53:22.240 --> 53:26.320
it's probably at least in the the the current path is going to be consuming human data so it's

53:26.320 --> 53:32.480
going to look a lot like us. AIGA is because they could go off in so many different directions

53:32.480 --> 53:40.880
is all gives us the amazing possibility of effectively doing alien cultural travel we get to see

53:40.880 --> 53:46.960
all these totally different types of intelligences and societies and cultures that might pop out

53:46.960 --> 53:51.440
of the system because it's so like possibly going in all these different directions that don't

53:51.440 --> 53:55.680
have a lot to do with the biases that were making either the system and so yes some of them might

53:55.680 --> 54:00.960
be unsavory and we need to make sure we have like the safety in place to like not like that be a

54:00.960 --> 54:07.360
problem or try to minimize that for being a problem but at the same time like coming to understand

54:07.360 --> 54:12.000
intelligence in general the space of possible culture and the space of intelligence like what is

54:12.000 --> 54:17.200
the music and the math of an alien culture look like well here is a system that might actually show

54:17.200 --> 54:22.240
it to us with greater probability than anything we're doing in the manual path AI and you know I

54:22.240 --> 54:25.680
don't think we're going to invent interstellar travel for quite some time so this is kind of our

54:25.680 --> 54:31.920
best shot to you to explore the space of possible intelligences in the universe and that

54:32.800 --> 54:37.600
sounds like the science it's the stuff of science fiction but it's probably not as far off as

54:37.600 --> 54:47.920
many people would imagine yeah it it does seem that even even kind of optimizing or or kind of

54:47.920 --> 54:57.200
pursuing this AIGA oriented path it's still in the context of you know all of the data that we have

54:58.960 --> 55:05.120
you know which has its buy I guess I'm I'm pushing back in that I'm not sure the AIGA to the

55:05.120 --> 55:10.880
extent that it's built on you know access to all of our data and and that's going to be an

55:10.880 --> 55:18.080
accelerant that it necessarily divorces us from the biases that the manual path will have totally agree

55:18.080 --> 55:25.440
so the first AIGA is probably consume human data and look a lot like us and in that sense I

55:25.440 --> 55:30.320
totally agree you don't get this this cultural travel you know seeing this massive diversity of

55:30.320 --> 55:35.200
possible cultures but I think once we figured out how to make that system we probably then can go

55:35.200 --> 55:39.440
back and try to create the system without consuming human data without that big step forward

55:39.440 --> 55:45.760
um standing on the shoulders of human data sets giant human data sets and also maybe not as

55:45.760 --> 55:50.960
much stuff built in our image and that's when you can start to kind of explore the space of

55:50.960 --> 55:56.640
possibilities much more and so there's kind of a spectrum even within AIGA between like to what

55:56.640 --> 56:00.160
extent they're going to look at us and it's not just consuming human data if we use the tricks that

56:00.160 --> 56:05.360
I described for grounding what counts as interesting as solving problems in our world especially on

56:05.360 --> 56:10.080
earth you know making money in our economy or whatever then that also is grounding it too

56:10.080 --> 56:15.520
basically it would look like a lot like something that is quite human but maybe we figure out a way

56:15.520 --> 56:20.080
to figure out tasks that are intrinsically interesting that are just internal to that world

56:20.080 --> 56:23.840
like I said you have to learn tasks that we're here that generalize to other tasks within that

56:23.840 --> 56:29.120
system within that kind of bubble maybe that still gives you intelligence but now it's totally

56:29.120 --> 56:38.800
unhinged from earth so we've we've covered a lot of ground I think if we it's easy for me

56:38.800 --> 56:43.440
you know relatively easy for me to wrap my head around kind of the you know the near term at

56:43.440 --> 56:49.440
least the the kind of work that you're doing now and generating environments and training agents

56:49.440 --> 57:00.880
on those environments and the the kind of the impulse that that you're using and creating

57:00.880 --> 57:06.640
these environments to maximize learning is something that's going to you know accelerate

57:08.080 --> 57:14.560
the evolution of intelligence let's say and then the long term is you know if you know if that

57:14.560 --> 57:21.440
continues and we throw lots more compute at it lots more data at it you can see how there's an

57:21.440 --> 57:32.000
argument that that's going to going to create an intelligence that is far beyond what the manual

57:32.000 --> 57:38.000
path might do or there's a lot there in that statement that I don't like but at least that you

57:38.000 --> 57:44.720
know there's some there's the opportunity to kind of to serendipitously stumble onto some

57:44.720 --> 57:53.520
discontinuous innovation that gets us there that the manual thing might not at least what's in

57:53.520 --> 57:59.440
that middle what do you see in that kind of middle term you know beyond the stuff that we're doing

57:59.440 --> 58:06.720
now you know not quiet at the you know then the miracle occurs like what what what what what's the I

58:06.720 --> 58:14.480
don't know three to five year kind of path for AIGA yeah good question so I think one thing that

58:14.480 --> 58:18.400
we haven't seen people do a lot of some people have but not a lot of is putting all the pillows

58:18.400 --> 58:23.360
together so for example we haven't seen a lot of architecture search kind of the learning

58:23.360 --> 58:28.080
in conjunction with the learning to learning algorithm stuff in conjunction with environment

58:28.080 --> 58:32.880
generation so I think we're going to see I think we're going to see more of that I also think

58:32.880 --> 58:37.600
you're probably going to see more stuff like poet and like excellent you have these agents

58:37.600 --> 58:42.400
who are just increasingly training on increasingly diverse challenges and becoming very general

58:42.400 --> 58:48.720
intelligent and so what you're starting going to start to see I think is more and more sample

58:48.720 --> 58:54.480
efficient reinforcement learning for two reasons one is that the algorithms the architectures

58:54.480 --> 59:00.960
etc are better optimized to learn quickly but also that you're not always starting from scratch

59:01.680 --> 59:06.800
you know people complain about the sample efficiency on like Dota or Atari or chess or whatever

59:06.800 --> 59:12.160
compared to a human but like a human doesn't learn to play chess waking up in the middle of a

59:12.160 --> 59:15.760
chess plane universe they start learning planning learning to play chess when they're six or seven

59:15.760 --> 59:20.000
they've had years of data coming in so I think increasingly what you're going to see are

59:20.000 --> 59:26.560
internal to these systems agents that basically already know a lot as they approach the next problem

59:26.560 --> 59:31.040
and maybe you via pre-training maybe even the system itself being pre-trained and maybe you

59:31.040 --> 59:36.000
can also models that are come from somewhere else that are dropped into these systems that are

59:36.000 --> 59:39.840
trained and also models that are exported from these systems to become pre-training that are used

59:39.840 --> 59:44.720
for other things so increasingly I think the era of starting from random weight initializations

59:44.720 --> 59:49.760
will go away we're starting to see that now almost all many many many AI papers now are starting

59:49.760 --> 59:56.880
with pre-trained components right and so this is kind of a quiet revolution that is basically AI

59:56.880 --> 01:00:03.200
is no longer being as sample inefficient because we are starting not from scratch starting from

01:00:03.200 --> 01:00:09.600
from a good foundation so I think that's probably what the next you know three to five years look

01:00:09.600 --> 01:00:15.040
like increasingly capable sample efficient learning agents that can generalize and another

01:00:15.040 --> 01:00:20.240
one that I would mention I think text in a conditioning on the task knowing what I'm supposed to do

01:00:20.240 --> 01:00:24.960
is another way that they get much more sample efficient so I just think you'll see more and more

01:00:24.960 --> 01:00:29.680
things like excellent and Rubik's Cube where these systems can do quite impressive things in

01:00:29.680 --> 01:00:34.160
generalizing big distributions of environments instead of narrow agents trained from scratch for one

01:00:34.160 --> 01:00:39.280
environment awesome well Jeff it was wonderful wonderful speaking with you and getting to learn a

01:00:39.280 --> 01:00:45.360
little bit about your research and looking forward to keeping in touch likewise thank you very

01:00:45.360 --> 01:00:49.680
much for having me I really enjoyed the conversation and it's a great fantastic podcast so it's

01:00:49.680 --> 01:01:17.840
an honor to be on thanks so much Jeff

