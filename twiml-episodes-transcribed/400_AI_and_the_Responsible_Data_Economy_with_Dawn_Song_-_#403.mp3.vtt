WEBVTT

00:00.000 --> 00:27.080
Hey everyone, just a quick announcement before we get you over to today's show.

00:27.080 --> 00:33.640
I'd like to invite you to join me on Thursday, August 27th for my conversation with Dylan

00:33.640 --> 00:39.720
Urb, co-founder and CEO of PaperSpace, as we discuss machine learning as a software

00:39.720 --> 00:41.720
engineering discipline.

00:41.720 --> 00:45.520
We've been having a ton of fun with these watch parties and of course Dylan and I will

00:45.520 --> 00:51.240
be in the chat answering all of your questions about the relationship and differences between

00:51.240 --> 00:57.040
traditional software development practices and MLOPS and how to scale up and manage machine

00:57.040 --> 00:59.560
learning pipelines.

00:59.560 --> 01:07.720
We'll begin at 1pm pacific time, register at twimmelai.com slash watch 404 for reminders

01:07.720 --> 01:09.400
and updates.

01:09.400 --> 01:11.520
And now on to the show.

01:11.520 --> 01:15.800
Alright everyone, I am on the line with Don Song.

01:15.800 --> 01:23.600
Don is a professor of computer science at UC Berkeley and CEO and founder at OASIS Labs.

01:23.600 --> 01:26.040
Don, welcome to the Twimmelai AI podcast.

01:26.040 --> 01:27.720
Thanks a lot for having me.

01:27.720 --> 01:31.480
Yeah, I'm really looking forward to digging into our conversation.

01:31.480 --> 01:36.960
We will be talking quite a bit about responsible data and what that means.

01:36.960 --> 01:41.760
But before we do, let's start off with a little bit of background.

01:41.760 --> 01:44.600
What got you started in working in AI?

01:44.600 --> 01:53.680
So my undergraduate was in physics and I switched to computer science in my graduate school.

01:53.680 --> 02:00.600
And in my graduate school and also later on as a professor I spent a lot of time actually

02:00.600 --> 02:06.080
focusing in security and privacy, thinking about how to build security systems and so

02:06.080 --> 02:07.080
on.

02:07.080 --> 02:15.320
But at the same time, I've always been really interested in building intelligent machines.

02:15.320 --> 02:22.080
So yes, I'm really glad that I have had the opportunity and the experience to really

02:22.080 --> 02:25.440
try to see how we can make progress in that space.

02:25.440 --> 02:30.480
AI and trying to figure out how to build intelligent machines.

02:30.480 --> 02:34.240
I think it's really probably the ultimate goal.

02:34.240 --> 02:39.200
If we can build intelligent machines, there are so many problems that we can solve and

02:39.200 --> 02:40.200
so on.

02:40.200 --> 02:50.000
So it's just really, really exciting and I would say probably one of the most important

02:50.000 --> 02:51.000
pursuits.

02:51.000 --> 02:52.000
Awesome.

02:52.000 --> 02:53.000
Awesome.

02:53.000 --> 02:55.400
And you're currently working on a startup now, Oasis Labs.

02:55.400 --> 02:57.400
How long have you been working on that?

02:57.400 --> 02:59.600
Oasis Labs is about two years now.

02:59.600 --> 03:06.320
Tell us a little bit about the genesis of Oasis Labs and the problem that you're looking

03:06.320 --> 03:07.320
to solve there.

03:07.320 --> 03:13.800
So at Oasis Labs, we focus on building what we call a platform for a responsible data

03:13.800 --> 03:14.960
economy.

03:14.960 --> 03:21.280
So as we know, Internet has really changed everybody's lives and mostly for the better,

03:21.280 --> 03:27.240
but at the same time, we do see many challenges in particular.

03:27.240 --> 03:30.160
As we know, data is critical.

03:30.160 --> 03:35.840
It's a key driver for a modern economy, but a lot of this data is also really sensitive

03:35.840 --> 03:41.960
and handling the sensitive data poses many challenges for the, on the user side as well

03:41.960 --> 03:44.200
as on the business side.

03:44.200 --> 03:49.800
So for the user side users are losing control of their data, they don't really know what

03:49.800 --> 03:54.000
their data has been used for, how their data has been used and so on.

03:54.000 --> 04:00.120
And also, they are now getting direct sufficient direct benefits from their data.

04:00.120 --> 04:06.160
And on the business side, businesses continue to suffer from large-scale data breaches

04:06.160 --> 04:11.720
and also it's becoming more and more expensive and cumbersome for them to comply with, for

04:11.720 --> 04:16.440
example, privacy regulations like GDPR and CCPA.

04:16.440 --> 04:22.280
And some more importantly, it's still really difficult for business to utilize data due

04:22.280 --> 04:25.760
to data silos and privacy concerns and so on.

04:25.760 --> 04:33.760
So the hope is that we can build a new platform for a responsible data economy that helps

04:33.760 --> 04:40.240
address many of these challenges that helps users to better maintain control of their data

04:40.240 --> 04:46.120
and rise to data and also help businesses to better utilize data, but in a privacy

04:46.120 --> 04:51.840
preserving and responsible way, essentially to enable a new paradigm to address many of

04:51.840 --> 04:54.320
the challenges that I mentioned.

04:54.320 --> 04:59.320
We've talked quite a bit on the podcast over the past few years about differential privacy

04:59.320 --> 05:01.960
and related techniques.

05:01.960 --> 05:07.160
Is that a core piece of this vision of a responsible data platform?

05:07.160 --> 05:09.960
Yeah, that's a great question.

05:09.960 --> 05:16.080
So in order to enable something like what I just mentioned, this responsible data and

05:16.080 --> 05:21.800
responsible data economy, essentially, it needs to address a number of different

05:21.800 --> 05:25.480
questions, different types of questions.

05:25.480 --> 05:33.640
So first of all, we need to ensure that users' rights to data is properly maintained as

05:33.640 --> 05:37.360
well logs and so on.

05:37.360 --> 05:43.840
So for that, we actually utilize blockchain to maintain an immutable ledger for users'

05:43.840 --> 05:49.920
rights to data and also the log of how the data has been utilized.

05:49.920 --> 05:58.320
And then also, we need to ensure that when the data is used, it's used in a way that we

05:58.320 --> 06:06.120
call it a controlled use that actually satisfies users' privacy requirements and their policies

06:06.120 --> 06:09.440
of how their data should be used.

06:09.440 --> 06:14.840
So for that, essentially, we need to address at least two separate types of questions.

06:14.840 --> 06:21.320
One is typically today, for example, when you talk about data markets, the buyer usually

06:21.320 --> 06:23.320
buys a copy of the data.

06:23.320 --> 06:26.360
And they essentially get a raw copy of the data.

06:26.360 --> 06:32.720
And once they buy or get raw access to the data, they essentially, they can do anything

06:32.720 --> 06:33.920
they want with the data.

06:33.920 --> 06:39.880
They can go ahead and resell the data, they can use the data for other purposes that

06:39.880 --> 06:44.720
may not be for the best interests of the data owner and so on.

06:44.720 --> 06:50.320
So that's also one of the big challenges today for data use.

06:50.320 --> 06:57.440
So in contrast, ideally, for responsible data use, what we need to enable is what we

06:57.440 --> 06:58.760
call controlled use.

06:58.760 --> 07:05.240
So in this case, for example, the buyer of the data, they don't just buy the data itself,

07:05.240 --> 07:09.320
what the buyer is the use of the data.

07:09.320 --> 07:17.360
To enable this, essentially, they shouldn't get a copy or a raw access to the data itself.

07:17.360 --> 07:27.160
So what we enable in this platform in this controlled use is enable the buyer or the user

07:27.160 --> 07:33.120
of the data to actually use the data in a confined environment.

07:33.120 --> 07:36.440
You can also think of it as a black box.

07:36.440 --> 07:44.280
So the data will only be computed over in this black box so that the data itself doesn't

07:44.280 --> 07:52.240
leak out and then the buyer or the user of the data doesn't actually ever get raw access

07:52.240 --> 07:55.120
to the data, doesn't get a direct copy of the data.

07:55.120 --> 07:59.920
They can only use the data in this black box confined to environments.

07:59.920 --> 08:00.920
So that's number one.

08:00.920 --> 08:02.840
We call that secure computing.

08:02.840 --> 08:12.720
There are a number of techniques that you can use to enable this secure computing to essentially

08:12.720 --> 08:16.480
you can use it as a way of computing over encrypted data.

08:16.480 --> 08:24.320
So basically, right, so you can use cryptography-based approaches such as homomorphic encryption or

08:24.320 --> 08:29.160
multi-party computation and so on.

08:29.160 --> 08:36.320
So you can use secure hardware which also essentially provides this type of black box-based

08:36.320 --> 08:43.720
black box like confined execution environments so that the data can only be used inside this

08:43.720 --> 08:45.240
black box environment.

08:45.240 --> 08:46.240
Okay.

08:46.240 --> 08:54.760
And a lot of ways it sounds like there's a much lower tech analogy here in like old school

08:54.760 --> 08:55.760
direct marketing.

08:55.760 --> 09:00.400
You have these list aggregators that would collect people's mailing addresses and they

09:00.400 --> 09:08.680
don't want to give the catalog vendors access to their mailing addresses because then

09:08.680 --> 09:11.120
why would they need to license them again?

09:11.120 --> 09:16.840
So instead, the catalog vendors would send them the things that they want to send out.

09:16.840 --> 09:20.920
I think the same thing happens in email as well, right?

09:20.920 --> 09:26.320
So the list company won't give someone the list, they'll say, we'll send your email for

09:26.320 --> 09:27.320
you.

09:27.320 --> 09:30.520
And a lot of ways you're saying, we're not going to give you the data but we'll do your

09:30.520 --> 09:32.240
compute for you.

09:32.240 --> 09:38.040
And now you need to come up with interesting ways to allow people to actually do the compute

09:38.040 --> 09:43.720
on these personal data without giving them access to it.

09:43.720 --> 09:45.960
That's an interesting analogy.

09:45.960 --> 09:52.520
So of course, computing on a data is much more complicated than sending out emails to

09:52.520 --> 09:54.360
your email address and so on, right?

09:54.360 --> 10:01.520
So that requires an entirely new type of technology to enable the secure computing so that you

10:01.520 --> 10:06.400
can only compute on the data you can now actually get access to the data.

10:06.400 --> 10:11.040
So there's one part and the other part is then you want to ensure that the computation

10:11.040 --> 10:19.480
itself on the data also complies with the user's policies of how their data should be utilized.

10:19.480 --> 10:25.280
For example, you want to ensure that when someone uses the data to train a machine learning

10:25.280 --> 10:33.840
model, then in the end, the machine learning model is being used for queries, for inference,

10:33.840 --> 10:34.840
and so on.

10:34.840 --> 10:38.800
You want to ensure that the machine learning model itself doesn't leak, individual usage

10:38.800 --> 10:41.000
information.

10:41.000 --> 10:46.480
And so you mentioned about differential privacy earlier, differential privacy is one example

10:46.480 --> 10:50.640
technology that can help address this problem.

10:50.640 --> 10:58.040
So speaking of which, like the privacy challenges for training machine learning models, I can

10:58.040 --> 11:00.040
give you one example.

11:00.040 --> 11:06.960
This is a recent work that we did in collaboration with the researchers from Google and so on.

11:06.960 --> 11:13.200
And the question here where trying to explore is the following.

11:13.200 --> 11:19.800
As we know, that neural networks has very high capacity and they can, so the question

11:19.800 --> 11:25.600
is whether when you train a machine learning model, it actually remembers a lot about the

11:25.600 --> 11:34.880
original training data and if it does, it can actually exploit this issue and try to actually

11:34.880 --> 11:40.920
learn sensitive information about the original training data in this case, even just through

11:40.920 --> 11:46.560
requiring the machine learning model without even getting a copy of, for example, the parameters

11:46.560 --> 11:47.800
and so on of the model.

11:47.800 --> 11:48.800
Right.

11:48.800 --> 11:52.920
If I remember correctly, there have been papers where they were able to demonstrate that

11:52.920 --> 11:59.720
you can reconstruct images that were part of a training set of the machine learning model.

11:59.720 --> 12:07.480
So that's one example and so the example work that we did is in the language model setting.

12:07.480 --> 12:14.560
So we showed that if you train a language model, for example, using an email data sets,

12:14.560 --> 12:17.400
in our case, it's called the Unreal email data sets.

12:17.400 --> 12:24.680
The Unreal email data sets naturally contains uses social security numbers and critical

12:24.680 --> 12:25.680
numbers.

12:25.680 --> 12:32.560
And we showed that when you train a language model on this email data sets, an attacker

12:32.560 --> 12:40.320
by devising new attacks and just by querying this language model without even knowing the

12:40.320 --> 12:45.120
details of the model, such as the parameters of the model, the attacker actually is able

12:45.120 --> 12:51.760
to recover the original uses credit card and social security numbers that were embedded

12:51.760 --> 12:55.600
in the original data sets.

12:55.600 --> 13:01.160
So this is another example showing that as we train machine learning models, it's really

13:01.160 --> 13:08.360
important to pay attention to privacy protection to use this data.

13:08.360 --> 13:14.120
And in this case, actually, we showed that for this particular case, we actually can have

13:14.120 --> 13:17.160
a very good solution to the problem.

13:17.160 --> 13:23.840
The solution is that instead of a training vanilla language model, if we train a different

13:23.840 --> 13:31.120
language model, then in this case, we can still have pretty good utility, but at the same

13:31.120 --> 13:41.080
time, we can significantly enhance the privacy protection of the user's data in this language

13:41.080 --> 13:42.080
model.

13:42.080 --> 13:43.080
Right.

13:43.080 --> 13:49.520
So essentially, so that's why, as I mentioned, in order to build a platform for a responsible

13:49.520 --> 13:57.200
data economy, we also need to ensure that the computation on data itself doesn't leak

13:57.200 --> 14:00.120
users sensitive information.

14:00.120 --> 14:06.560
And in this case, utilizing technologies like differential privacy can help us to ensure

14:06.560 --> 14:12.600
that the computation results itself doesn't leak sensitive information about individual

14:12.600 --> 14:13.600
users.

14:13.600 --> 14:22.120
OK, in the example that you are describing to what degree are you being fine-grained about

14:22.120 --> 14:27.360
what you consider sensitive information versus what isn't sensitive information?

14:27.360 --> 14:30.400
Everyone's talking about GPT-3.

14:30.400 --> 14:36.280
And there's a very coarse-grained argument that says that what GPT-3 is doing is kind

14:36.280 --> 14:41.760
of remembering all of the text on the internet and kind of regurgitating it in creative ways

14:41.760 --> 14:44.080
based on the prompt.

14:44.080 --> 14:49.080
And so from that perspective, all it's doing is leaking information, but in a constructive

14:49.080 --> 14:50.080
way.

14:50.080 --> 14:54.320
And trying to draw a parallel between that and the data leaking that we're talking about

14:54.320 --> 14:55.320
in this case.

14:55.320 --> 14:56.320
Right.

14:56.320 --> 14:58.320
Yeah, that's a very good question.

14:58.320 --> 15:05.000
And exactly, even in a work that we did, studying the privacy challenges of language

15:05.000 --> 15:11.640
models, one of the main issues here, when the vanilla language model leaks sensitive

15:11.640 --> 15:18.120
information about user's data, it is memorization, it is remembering those credit card numbers

15:18.120 --> 15:23.280
and social security numbers that were infected in the original training data set.

15:23.280 --> 15:27.560
So in this case, what we want is when you train a language model, you want a language

15:27.560 --> 15:34.640
model to really learn essentially how to predict, for example, the next words, the next

15:34.640 --> 15:43.480
character and so on, for the things that's essentially in this probabilistic way, but now

15:43.480 --> 15:48.440
actually remembering all these individual social security numbers and credit card numbers

15:48.440 --> 15:49.440
and so on.

15:49.440 --> 15:53.200
So essentially, it is how to address this memorization issue.

15:53.200 --> 15:59.760
And in our work, we actually showed studies and that's a, this language model, they

15:59.760 --> 16:06.760
do remember these occurrences for social security numbers and credit card numbers in this particular

16:06.760 --> 16:07.760
case.

16:07.760 --> 16:14.600
And also, we proposed the measures of code exposure to actually measure the, like the degree

16:14.600 --> 16:19.760
of the memorization that the language model has essentially has occurred in the language

16:19.760 --> 16:20.760
model.

16:20.760 --> 16:27.280
And for GPT-3, so we actually, we have further extensions of our work that is studying

16:27.280 --> 16:34.760
these type of models and like you said, these really, really large models, they can actually

16:34.760 --> 16:36.400
remember a lot.

16:36.400 --> 16:42.400
And, and oftentimes, these really large models, they are changed in the public data, but

16:42.400 --> 16:48.240
when you actually have private data, it can really potentially, could remember a lot

16:48.240 --> 16:53.320
about sensitive information and that's, these are the kind of issues that we really need

16:53.320 --> 16:59.120
to pay attention to because otherwise, these private data, they can contain both individual

16:59.120 --> 17:03.120
users' sensitive information and also when you're trained to over business data also

17:03.120 --> 17:05.680
can be a lot of proprietary information as well.

17:05.680 --> 17:13.720
Yeah, I think the, the issue that I was trying to form a question around is, it seems to

17:13.720 --> 17:20.520
me a lot easier to figure out how to get a model to not remember or, or not leak or

17:20.520 --> 17:27.480
not even learn from Social Security numbers because those are, you know, very fixed in

17:27.480 --> 17:28.480
nature.

17:28.480 --> 17:29.480
They've got a fixed format.

17:29.480 --> 17:33.960
It's easy to identify them in the training data, it's potentially easy to teach the

17:33.960 --> 17:37.760
model not to, to remember them in some way.

17:37.760 --> 17:43.720
But if you've got a language model and you're trying to maybe fine tune it on a, on a private

17:43.720 --> 17:50.080
data set, there's potentially a ton of sensitive information, you know, say about the inner

17:50.080 --> 17:55.240
workings of a business or, you know, past, you know, deals or contracts or things like

17:55.240 --> 18:02.160
this that it, I'm envisioning a lot of scenarios where it's hard to separate the information

18:02.160 --> 18:07.000
that you want the model to learn from and the information that you want the, the model

18:07.000 --> 18:08.000
to not leak.

18:08.000 --> 18:10.040
Right, that's a very good question.

18:10.040 --> 18:15.760
So when I talked about the solution of linear, differential private language model, so

18:15.760 --> 18:21.440
in this case, the solution is not just particular for, for example, credit current numbers or

18:21.440 --> 18:25.400
Social Security numbers, it's in, right, it's general.

18:25.400 --> 18:32.640
You don't pre-specify the type of sensitive information that you need to protect.

18:32.640 --> 18:37.560
Differential privacy is a very general notion of privacy.

18:37.560 --> 18:42.520
It's not specific to a particular type of sensitive information.

18:42.520 --> 18:48.640
So the idea there is essentially, so when we say, for example, an algorithm is differential

18:48.640 --> 18:55.600
private, it follows the following, a high level definition is essentially, if you consider

18:55.600 --> 19:03.200
you have two, we call it neighboring data sets, where one data set has one more data points,

19:03.200 --> 19:10.600
then the other, for example, one data point about them, that's not in the other data

19:10.600 --> 19:11.800
sets.

19:11.800 --> 19:17.800
And then we compute an algorithm over these two neighboring data sets.

19:17.800 --> 19:20.600
The algorithm is randomized.

19:20.600 --> 19:24.880
And in this case, we say that the algorithm is differential private.

19:24.880 --> 19:32.320
If the computation results of this algorithm over these two neighboring data sets is very

19:32.320 --> 19:33.320
close.

19:33.320 --> 19:39.480
Essentially, the probability distribution of the computation results from this algorithm

19:39.480 --> 19:48.240
over these two neighboring data sets is communistically very close, then what this says is essentially

19:48.240 --> 19:54.200
from the computation results, the attacker wouldn't be able to tell what their sense data

19:54.200 --> 19:57.440
point has been used in the computation or not.

19:57.440 --> 20:02.960
And this is a way to essentially talk about showing that the computation result has not

20:02.960 --> 20:08.120
really leaked much information about, for example, sense data.

20:08.120 --> 20:11.840
So similarly, when you carry over to the machine model, it's similar.

20:11.840 --> 20:17.160
So essentially, from the machine model, the attacker wouldn't be able to tell whether

20:17.160 --> 20:22.320
a particular, for example, social security number has been used in the data sets and

20:22.320 --> 20:25.640
hence, then in this case, you won't.

20:25.640 --> 20:31.760
So in this way, the training machine model can help enhance the privacy protection for

20:31.760 --> 20:34.200
the original training data set.

20:34.200 --> 20:41.280
Okay, do you draw a distinction between techniques like differential privacy that are preventing

20:41.280 --> 20:47.520
information being leaked and techniques that are preventing the network from memorizing

20:47.520 --> 20:49.520
the information in the first place?

20:49.520 --> 20:50.520
I see.

20:50.520 --> 20:56.440
So in this particular case, what the differential privacy, that's, for example, on your training,

20:56.440 --> 21:02.240
the differential privacy machine model, in this case, what you are doing is actually trying

21:02.240 --> 21:08.200
to, it is actually reducing the memorization that the network is doing.

21:08.200 --> 21:16.560
And in our work, we actually showed, as I mentioned, that with our measure of this exposure,

21:16.560 --> 21:20.960
which measures the degree of memorization, we actually showed that when you train a differential

21:20.960 --> 21:29.600
a private language model, in this case, actually, you are reducing this type of memorization.

21:29.600 --> 21:36.160
In the case of differential privacy, typically in an application that involves differential

21:36.160 --> 21:43.160
privacy, you're limited to doing computation or analysis on an aggregate level.

21:43.160 --> 21:47.960
Does that mean that the kinds of applications you'll be able to support in a, this kind

21:47.960 --> 21:54.480
of, in a responsible data platform or scenario or only these kinds of aggregate types of

21:54.480 --> 21:55.480
computations?

21:55.480 --> 21:57.880
That's a very good question.

21:57.880 --> 22:02.760
So I think it can support different types of computations, for example, if usage data

22:02.760 --> 22:11.640
is only used to, like, the computing results is only used for users' own consumption,

22:11.640 --> 22:17.240
then you can essentially do arbitrary computation on users' data and then just review the computation

22:17.240 --> 22:20.000
results to the user's self.

22:20.000 --> 22:26.160
But when you want to compute over multiple usage data and then release the results, the

22:26.160 --> 22:31.360
computation results, then in this case, oftentimes you are already computing some kind of aggregate

22:31.360 --> 22:37.800
sense, or you're actually machine models of a different user's data and then in this

22:37.800 --> 22:38.800
case, yes.

22:38.800 --> 22:44.320
So you actually, you really do need techniques like differential privacy and so on to ensure

22:44.320 --> 22:45.320
that.

22:45.320 --> 22:49.120
So when you are computing over different users' data, then the computing results doesn't

22:49.120 --> 22:53.080
leak sensitive information about individual users.

22:53.080 --> 22:54.880
You mentioned homomorphic encryption.

22:54.880 --> 22:59.200
When you talk a little bit about where that comes into play, I've not spent a lot of

22:59.200 --> 23:04.280
time looking at it, but I understand that in that case, the set of operations that you

23:04.280 --> 23:09.560
can apply that can retain this homomorphic property is somewhat limited.

23:09.560 --> 23:12.040
Does that is that a big barrier?

23:12.040 --> 23:17.720
So homomorphic encryption is one type of cryptographic techniques to enable your two essentially

23:17.720 --> 23:25.400
to computing over encrypted data, and it's one type of solutions to the problem that

23:25.400 --> 23:27.560
I mentioned, the secure computing.

23:27.560 --> 23:30.160
So the goal of the secure computing is following.

23:30.160 --> 23:38.080
We oftentimes talk about this simulation of ideal worlds with trusted third party.

23:38.080 --> 23:45.720
So with the secure computing, the goal is that, let's say we have trusted third party

23:45.720 --> 23:51.280
in this ideal world, and what you do is that you gave data to this trusted third party,

23:51.280 --> 23:57.280
and also you gave an algorithm in this case to the trusted third party.

23:57.280 --> 24:01.760
And the trusted third party will compute this algorithm function, let's say a function

24:01.760 --> 24:07.760
f over your input x, then there's the computing result f of x.

24:07.760 --> 24:12.520
So in this case, only f of x will be revealed, nothing else.

24:12.520 --> 24:16.320
So this is what happens in the ideal world with this trusted third party.

24:16.320 --> 24:23.400
So you trust that this trusted third party will not leak any sensitive information about

24:23.400 --> 24:29.120
your data only, the computing, the computation result f of x is revealed.

24:29.120 --> 24:33.800
But of course, then how to find this trusted third party?

24:33.800 --> 24:38.800
In the past, essentially, essentially people rely on trust of a particular party based

24:38.800 --> 24:45.920
on business contracts, and so on. But of course, that's suboptimal.

24:45.920 --> 24:52.920
So even in the best case, the trusted third party tries to comply to the contract,

24:52.920 --> 24:56.200
their own system may be compromised, and so on.

24:56.200 --> 25:00.760
So essentially, the question is how we can have technical solutions,

25:00.760 --> 25:07.520
ideally even with the provable guarantees to ensure this ideal world,

25:07.520 --> 25:10.560
essentially, be able to simulate this trusted third party.

25:10.560 --> 25:15.840
And in order to do that, essentially, the community has been developing

25:15.840 --> 25:17.920
different types of solutions.

25:17.920 --> 25:21.400
This is a homomorphic encryption, it's one type of solutions,

25:21.400 --> 25:26.680
utilizing cryptography, where you essentially, you encrypt the inputs,

25:26.680 --> 25:32.640
and then you compute over the inputs, and then you generate this encrypted

25:32.640 --> 25:37.480
computing results, so that only the original user is able to decrypt.

25:37.480 --> 25:42.680
And so then they only learn the computing results.

25:42.680 --> 25:46.560
And another way, as I mentioned, is that you can use secure hardware,

25:46.560 --> 25:52.840
where based on certain hardware and software combined solutions,

25:52.840 --> 25:55.680
you simulate this black box environment.

25:55.680 --> 25:58.440
We call it a secure execution environment,

25:58.440 --> 26:04.840
and also called a secure enclave, where, again, you put data into a black box,

26:04.840 --> 26:11.120
and also you put the function of the program into this black box.

26:11.120 --> 26:14.600
And the hardware and software combined solution

26:14.600 --> 26:19.400
ensures that when the program is executing inside this black box,

26:19.400 --> 26:22.880
nothing else, nothing outside this black box,

26:22.880 --> 26:27.000
including the operating system or other applications,

26:27.000 --> 26:29.760
we'll be able to see what's running inside,

26:29.760 --> 26:32.720
or we'll be able to modify what's running inside.

26:32.720 --> 26:38.920
And hence this black box ensures the confidentiality and integrity

26:38.920 --> 26:40.400
of the computation.

26:40.400 --> 26:45.240
And the secure enclave also provides a capability

26:45.240 --> 26:50.360
called remote at a station, so that a remote verifier is able to

26:50.360 --> 26:52.240
remove the verifier.

26:52.240 --> 26:57.520
The right computation has happens on a particular piece of data,

26:57.520 --> 27:02.320
so essentially, it can verify the initial state of the secure enclave

27:02.320 --> 27:05.200
and the program that will be run in the secure enclave.

27:05.200 --> 27:08.160
So with this method, essentially, it's another way,

27:08.160 --> 27:12.280
another practical way to simulate this trusted third party,

27:12.280 --> 27:16.720
to ensure that when the program computes over the data,

27:16.720 --> 27:20.120
nothing else gets leaked.

27:20.120 --> 27:23.000
And there has been various commercial solutions,

27:23.000 --> 27:26.960
including Intel Asjacks and AMD,

27:26.960 --> 27:30.360
different hardware manufacturers have built

27:30.360 --> 27:34.520
their different types of solutions for us.

27:34.520 --> 27:37.280
But however, all these solutions are closed sourced,

27:37.280 --> 27:42.880
and there has been some security issues discovered,

27:42.880 --> 27:46.320
even though they have been patched and so on.

27:46.320 --> 27:48.760
But being a closed source, it's difficult

27:48.760 --> 27:53.520
for the community to really know what kind of security guarantees

27:53.520 --> 27:56.360
that this type of solution can provide.

27:56.360 --> 28:03.520
So as a research project, a Berkeley in my research group,

28:03.520 --> 28:05.560
with other colleagues and so on, we have

28:05.560 --> 28:10.400
been building what's called a keystone secure enclave.

28:10.400 --> 28:13.400
It's an open source secure enclave, essentially

28:13.400 --> 28:17.320
provides an open source version of this black box

28:17.320 --> 28:21.760
that helps you to do this type of secure computing.

28:21.760 --> 28:27.080
And it's built on top of risk 5 open source risk architecture.

28:27.080 --> 28:31.280
And we have demonstrated that you can actually

28:31.280 --> 28:36.920
build machine learning models inside the secure enclave

28:36.920 --> 28:43.160
that you can do inference and other types of computation.

28:43.160 --> 28:45.200
It can be really efficient.

28:45.200 --> 28:49.160
And so on. So in the future, also, I think

28:49.160 --> 28:53.560
there has been discussions, even like GPUs and CPUs

28:53.560 --> 28:58.880
in the future, there can be this notion of a secure enclave.

28:58.880 --> 29:02.960
So that the data is encrypted going into the chip

29:02.960 --> 29:06.720
and only is decrypted inside the chip

29:06.720 --> 29:10.200
and then you compute over the data essentially

29:10.200 --> 29:12.600
in this black box manner.

29:12.600 --> 29:15.720
When you talk about the risk architecture,

29:15.720 --> 29:18.360
is this something that you're able to build

29:18.360 --> 29:20.480
with off-the-shelf components?

29:20.480 --> 29:24.520
Or does it require custom hardware to implement?

29:24.520 --> 29:26.440
So right, so they came with risk 5.

29:26.440 --> 29:29.640
There are chips already off-the-shelf to enable you

29:29.640 --> 29:30.720
to do that.

29:30.720 --> 29:34.240
And right, I was just actually going to sound that.

29:34.240 --> 29:36.640
OK, cool, cool.

29:36.640 --> 29:42.080
You also are active in exploring different adversarial

29:42.080 --> 29:45.040
attacks on machine learning and that whole space

29:45.040 --> 29:47.360
of adversarial machine learning in general.

29:47.360 --> 29:50.760
Can you talk a little bit about some of your work in that area?

29:50.760 --> 29:52.520
Right, yes.

29:52.520 --> 29:56.800
Well, as we tried to deploy machine learning

29:56.800 --> 29:59.640
AI systems in the real world, one issue we've already

29:59.640 --> 30:03.040
discussed is in the privacy and responsible data

30:03.040 --> 30:05.680
used to ensure that the model actually does

30:05.680 --> 30:07.560
a leak-sensely-faint information about users

30:07.560 --> 30:12.120
and also is used in a way that's for users best interests.

30:12.120 --> 30:14.320
And then another important issue is

30:14.320 --> 30:18.720
to ensure that these models are actually not easily

30:18.720 --> 30:21.200
hacked by attackers.

30:21.200 --> 30:25.120
So that relates to the problem of adversarial machine learning

30:25.120 --> 30:27.640
where in adversarial machine learning,

30:27.640 --> 30:30.680
so one example we have said is essentially

30:30.680 --> 30:33.920
looking at how the attackers can actually

30:33.920 --> 30:37.760
feed, for example, modified inputs,

30:37.760 --> 30:39.520
like with adversarial perturbations that

30:39.520 --> 30:41.520
can fool the machine learning system

30:41.520 --> 30:45.160
to give the wrong answers, for example, the wrong predictions.

30:45.160 --> 30:47.640
And one example that we have stated

30:47.640 --> 30:52.240
is in the self-driving car setting,

30:52.240 --> 30:56.240
we're looking at also, well, there's some of these adversarial

30:56.240 --> 30:59.520
example attacks can even happen in the real world.

30:59.520 --> 31:02.160
It's like putting a picker on a stop sign and...

31:02.160 --> 31:04.240
Right, right, right, exactly.

31:04.240 --> 31:07.960
Right, demonstrating that this type of attacks

31:07.960 --> 31:10.600
can even happen in the real world where the attackers

31:10.600 --> 31:13.640
are putting just stickers on stop signs.

31:13.640 --> 31:16.600
It can, for humans, we can still recognize

31:16.600 --> 31:19.200
these stop signs with no problems.

31:19.200 --> 31:22.120
But for the image classification systems,

31:22.120 --> 31:23.920
or these computer vision systems,

31:23.920 --> 31:27.080
they can be very easily thought to give wrong answers

31:27.080 --> 31:30.560
and also to give the target answers that the attacker wants.

31:30.560 --> 31:34.360
And the important part is that this type of attack

31:34.360 --> 31:39.040
can even remain effective as, even from different viewing

31:39.040 --> 31:42.800
distances, different viewing angles, and so on.

31:42.800 --> 31:44.680
Right, so with my collaborators,

31:44.680 --> 31:49.560
we have demonstrated that this is feasible,

31:49.560 --> 31:52.120
and we develop these real world stop signs,

31:52.120 --> 31:53.840
archefic signs, and so on.

31:53.840 --> 31:58.120
And some of these artifacts actually have been

31:58.120 --> 32:03.120
exhibits at the Science Museum in London.

32:03.120 --> 32:06.720
So it's actually quite fun.

32:06.720 --> 32:10.880
Oh, well, to what degree are you seeing or hearing about

32:10.880 --> 32:15.960
kind of real world examples of these types of adversarial attacks?

32:15.960 --> 32:17.320
Are we there yet?

32:17.320 --> 32:22.240
Is it something that people are practically faced with

32:22.240 --> 32:24.000
and worried about today?

32:24.000 --> 32:25.200
That's a very good question.

32:25.200 --> 32:29.200
I think, definitely, we have seen

32:29.200 --> 32:33.760
the attackers try to fool machine learning systems.

32:33.760 --> 32:38.840
So in particular, for example, there are a lot of these clouds

32:38.840 --> 32:42.960
APIs for different machine learning services.

32:42.960 --> 32:47.560
For example, this cloud APIs may try to identify

32:47.560 --> 32:51.680
whether a certain content is deemed safe.

32:51.680 --> 32:56.640
For example, and it's actually very easy for attackers

32:56.640 --> 32:59.200
to write through these type of attacks

32:59.200 --> 33:02.080
to try to fool this type of cloud APIs.

33:02.080 --> 33:04.960
In our own work, we have demonstrated this as well.

33:04.960 --> 33:07.120
And we call this actually Black Box attacks.

33:07.120 --> 33:10.880
So in this case, we don't even need to know the details

33:10.880 --> 33:14.520
of the machine learning model of the cloud API,

33:14.520 --> 33:17.720
including the architecture or the parameters

33:17.720 --> 33:23.240
of these machine learning models, but through Black Box attacks.

33:23.240 --> 33:27.760
And they're from what's called transferability attacks

33:27.760 --> 33:32.520
where we can build a local model and then

33:32.520 --> 33:37.440
try to generate these adversarial examples by just attacking

33:37.440 --> 33:38.160
the local model.

33:38.160 --> 33:42.160
And then due to this transferability phenomena,

33:42.160 --> 33:44.840
they generated the adversary examples

33:44.840 --> 33:47.640
from the local model actually has high likelihood

33:47.640 --> 33:51.760
to actually be successful against the remote victim model

33:51.760 --> 33:52.480
as well.

33:52.480 --> 33:54.760
And we demonstrated that this type of attacks

33:54.760 --> 33:57.680
can be effective for this cloud APIs.

33:57.680 --> 33:59.560
On the work that you were just mentioning

33:59.560 --> 34:04.880
is the ability to generate these effective local models

34:04.880 --> 34:09.480
incorporating some of the parameter and architecture

34:09.480 --> 34:12.880
leakage that we've talked about previously?

34:12.880 --> 34:15.400
So in this case, we assume that we actually

34:15.400 --> 34:19.040
don't really know anything about the remote model,

34:19.040 --> 34:21.080
like what actually, what parameters

34:21.080 --> 34:24.880
it actually uses.

34:24.880 --> 34:27.240
So we just built a separate local model.

34:27.240 --> 34:28.920
And because of this transferability,

34:28.920 --> 34:31.640
the attacks that we found, this local model

34:31.640 --> 34:37.680
has high likelihood to actually succeed on the remote model.

34:37.680 --> 34:40.120
And also, so in the past, we've done work

34:40.120 --> 34:43.600
and also the community has done work studying

34:43.600 --> 34:47.560
in the computer vision fields for this type of attack.

34:47.560 --> 34:53.200
And recently, we've also studied in the natural language

34:53.200 --> 34:54.160
space.

34:54.160 --> 34:57.800
So in this particular case, for machine translation.

34:57.800 --> 35:01.560
So we actually looked at, for example, Google Translate

35:01.560 --> 35:06.440
and a few other these cloud APIs for machine translation.

35:06.440 --> 35:08.600
So first, we showed that by just creating

35:08.600 --> 35:11.520
this cloud APIs for machine translation,

35:11.520 --> 35:15.920
you can call it an imitation attack or model steaming attack.

35:15.920 --> 35:19.200
We can actually build a local model that

35:19.200 --> 35:24.080
has very high essentially performance

35:24.080 --> 35:27.240
that close to this cloud APIs when you evaluate it

35:27.240 --> 35:29.440
over standard benchmark.

35:29.440 --> 35:31.920
So by creating these cloud APIs, we

35:31.920 --> 35:35.680
are able to build these imitation models locally.

35:35.680 --> 35:41.120
And then by developing attacks, these adversarial attacks

35:41.120 --> 35:44.800
on this local imitation model, we

35:44.800 --> 35:48.000
are able to generate these adversarial examples.

35:48.000 --> 35:51.800
As a simple example, we show that if we try to translate,

35:51.800 --> 35:54.320
for example, from English to German,

35:54.320 --> 35:56.520
the English sentence says, as I said,

35:56.520 --> 36:00.040
today is the temperature is very high.

36:00.040 --> 36:05.480
In this case, the language model generates the correct translation.

36:05.480 --> 36:10.000
But we are able to show that by finding essentially,

36:10.000 --> 36:12.080
we are able to find the attack.

36:12.080 --> 36:15.200
In this case, if we just change six Fahrenheit

36:15.200 --> 36:18.560
to seven Fahrenheit, just changing the number.

36:18.560 --> 36:20.280
And otherwise, it's the same sentence.

36:20.280 --> 36:23.240
And then one way to give the sentence to the language model

36:23.240 --> 36:26.480
actually translates into the temperature

36:26.480 --> 36:29.560
is 21 Celsius, for example.

36:29.560 --> 36:34.360
So it gives the wrong translation.

36:34.360 --> 36:37.760
And in this case, we only change the one

36:37.760 --> 36:39.840
character in the original sentence

36:39.840 --> 36:44.200
and show that the result in translation is very different.

36:44.200 --> 36:46.160
And this is just one of many examples

36:46.160 --> 36:50.560
which shows how using different essentially

36:50.560 --> 36:53.840
last functions using different methods,

36:53.840 --> 36:56.720
we can generate different types of attacks

36:56.720 --> 36:59.480
to try to fool the translation model.

36:59.480 --> 37:03.080
And also, when we transfer this attack

37:03.080 --> 37:06.920
to the remote model, for example, to Google Translate

37:06.920 --> 37:12.920
and to other types of APIs, we show

37:12.920 --> 37:17.240
that these attacks can still work.

37:17.240 --> 37:22.000
So these are other examples demonstrating

37:22.000 --> 37:25.760
that when we deploy machine learning systems

37:25.760 --> 37:27.840
in the real world, it's really important

37:27.840 --> 37:30.040
to think about these types of issues as well.

37:30.040 --> 37:31.960
And we call them the integrity attacks,

37:31.960 --> 37:35.800
essentially trying to see how attackers may

37:35.800 --> 37:39.520
fool the machine learning systems to try

37:39.520 --> 37:42.920
to have the machine learning systems to give the wrong answers.

37:42.920 --> 37:47.360
And as we know in the future, more and more critical decisions

37:47.360 --> 37:50.880
will be made by these learning systems

37:50.880 --> 37:56.120
autonomously, almost potentially every minute,

37:56.120 --> 38:00.320
we really need to be very, very careful

38:00.320 --> 38:03.320
about ensuring that these machine learning systems

38:03.320 --> 38:08.520
cannot be easily attacked, they do give the rights,

38:08.520 --> 38:14.040
and so they have high security assurance and so on.

38:14.040 --> 38:17.120
You also have done some interesting work

38:17.120 --> 38:20.600
on the topic of program synthesis,

38:20.600 --> 38:24.320
so trying to use machine learning models

38:24.320 --> 38:26.320
to generate computer programs.

38:26.320 --> 38:29.560
Can you talk a little bit about your work in that area?

38:29.560 --> 38:32.520
Yes, I think the reason I'm working in program synthesis

38:32.520 --> 38:34.480
is because I think program synthesis

38:34.480 --> 38:38.640
is the ideal playgrounds for trying

38:38.640 --> 38:45.160
to build what we call agatic artificial general intelligence.

38:45.160 --> 38:47.960
I think it's really the ultimate task.

38:47.960 --> 38:49.520
Talking about building intelligent machines,

38:49.520 --> 38:51.920
ideally you want this intelligent machine

38:51.920 --> 38:54.920
to actually be able to do the program,

38:54.920 --> 38:57.600
and hence it's actually able to, in the future,

38:57.600 --> 39:00.440
maybe even build itself and so on

39:00.440 --> 39:06.440
to help building programs to solve many real world problems.

39:06.440 --> 39:11.040
Also, I joke with my colleagues in robotics

39:11.040 --> 39:14.720
is that program synthesis is like doing robotics,

39:14.720 --> 39:19.720
but without the physics constraints of nature

39:20.760 --> 39:25.200
without having to obey laws of physics.

39:25.200 --> 39:28.320
So in program synthesis, essentially, you need to solve

39:28.320 --> 39:30.280
many of the similar problems.

39:30.280 --> 39:33.040
You need to understand goals.

39:33.040 --> 39:36.280
You need to be able to decompose problems.

39:36.280 --> 39:38.240
Into some problems, you need to be able

39:38.240 --> 39:40.240
to do very effective search.

39:40.240 --> 39:45.040
The search space of programs is huge.

39:45.040 --> 39:47.040
As we all know, you know,

39:47.040 --> 39:50.280
comparing to playing Go, playing Starcraft

39:50.280 --> 39:53.280
and so on, the search space of programs

39:53.280 --> 39:58.280
is even larger, even for small, simple programs and so on.

39:59.920 --> 40:04.160
So you need to do planning and also,

40:04.160 --> 40:06.360
you need to better understand semantics

40:06.360 --> 40:09.320
to know what you want the programs to do and so on.

40:09.320 --> 40:12.960
So essentially, all the problems that you need to solve

40:12.960 --> 40:15.560
in robotics and also just in general building

40:15.560 --> 40:20.560
education machines, they all fight in program synthesis.

40:20.560 --> 40:22.960
You need to really solve program synthesis.

40:22.960 --> 40:25.680
You really need to solve all these problems.

40:25.680 --> 40:28.240
So it's a really exciting domain,

40:28.240 --> 40:30.280
but also at the same time, it's a domain

40:30.280 --> 40:32.560
that you can experiment much more easily.

40:32.560 --> 40:36.400
You don't need to build robots, you don't, right?

40:36.400 --> 40:38.280
And what you need to do, like what we did,

40:38.280 --> 40:41.560
is oftentimes you just build these synthetic,

40:41.560 --> 40:43.160
you can build synthetic environments.

40:43.160 --> 40:48.160
You can easily also build this program, you know,

40:48.160 --> 40:50.840
analytics or even just execute the program

40:50.840 --> 40:52.760
to know exactly what it does.

40:52.760 --> 40:55.440
And so also from in terms of evaluation,

40:55.440 --> 40:58.720
the experiments, it's much, much easier.

40:58.720 --> 41:01.960
So in the program synthesis space,

41:01.960 --> 41:04.560
in particular program synthesis by Lenny,

41:04.560 --> 41:07.520
it's still a nascent field.

41:07.520 --> 41:10.760
I remember a few years ago,

41:10.760 --> 41:12.600
when we started working in it,

41:12.600 --> 41:14.800
there were very few people actually working in a space.

41:14.800 --> 41:18.680
And then when you look at new ribs,

41:18.680 --> 41:21.480
I clear, I say now, these conferences,

41:21.480 --> 41:23.880
there were very few papers actually

41:23.880 --> 41:26.360
doing program synthesis by Lenny.

41:26.360 --> 41:28.160
But the way you look at the most recent,

41:28.160 --> 41:30.960
for example, new ribs conference and so on.

41:30.960 --> 41:34.720
And I clear conference, you actually see,

41:34.720 --> 41:36.280
now there are specific sessions,

41:36.280 --> 41:40.560
even dedicated to a program synthesis by Lenny.

41:40.560 --> 41:45.040
I think it is a great progress for the community.

41:45.040 --> 41:49.640
But still, we are at a very early age.

41:49.640 --> 41:53.440
The program that in general, the community can synthesize

41:53.440 --> 41:55.240
is still very small.

41:55.240 --> 42:00.240
And in general, typically we focus on the program synthesis

42:00.720 --> 42:03.240
for certain vertical domains,

42:03.240 --> 42:06.040
is there to make progress that way.

42:06.040 --> 42:08.960
And I think also program synthesis can have,

42:08.960 --> 42:12.600
already can have a huge impact in the real world.

42:12.600 --> 42:16.360
So for example, some of the tasks that we have done,

42:16.360 --> 42:18.720
and translating natural language

42:18.720 --> 42:20.720
into simple programs.

42:20.720 --> 42:23.080
And so this includes translating natural language

42:23.080 --> 42:26.640
into if this and that's type of programs.

42:26.640 --> 42:29.200
For example, if it rains tomorrow,

42:29.200 --> 42:31.960
so any text message,

42:31.960 --> 42:34.480
or translating natural language description

42:34.480 --> 42:36.200
into SQL queries.

42:36.200 --> 42:41.200
So this can enable more people who cannot code

42:41.440 --> 42:42.960
to be able to utilize data.

42:42.960 --> 42:45.840
So for example, with a huge database,

42:45.840 --> 42:47.560
people who cannot code,

42:47.560 --> 42:49.440
who cannot write SQL queries,

42:49.440 --> 42:50.640
they may have a lot of questions

42:50.640 --> 42:54.760
that they want to get answers from these large database.

42:54.760 --> 42:59.240
And when we enable this natural language description,

42:59.240 --> 43:02.120
get translated into SQL queries directly,

43:02.120 --> 43:05.800
we can really enable more people to benefit from data,

43:05.800 --> 43:08.280
you know, talking about responsible data,

43:08.280 --> 43:10.960
responsible data economy, and so on.

43:10.960 --> 43:13.720
So I think it's, right,

43:13.720 --> 43:16.520
it's a really exciting domain.

43:16.520 --> 43:21.760
And deep learning has really been hugely helpful in the space.

43:21.760 --> 43:25.360
It's, well, we first started working in the space.

43:25.360 --> 43:29.360
Who was your first paper in program synthesis?

43:29.360 --> 43:31.760
So our first paper in program synthesis

43:31.760 --> 43:35.240
by learning is actually using deep learning

43:35.240 --> 43:38.360
to enable natural language description

43:38.360 --> 43:40.640
to be translated in this,

43:40.640 --> 43:43.040
this then that's called ifTT programs.

43:43.040 --> 43:43.880
Okay.

43:43.880 --> 43:48.880
And at the time, also there has been some other approaches

43:48.880 --> 43:53.080
using more traditional natural language approaches,

43:53.080 --> 43:54.480
like semantic pricing, and so on,

43:54.480 --> 43:56.760
to try to address a problem.

43:56.760 --> 43:58.960
And we were the first one to actually demonstrate

43:58.960 --> 44:00.000
that using deep learning,

44:00.000 --> 44:02.320
you can actually get much better results.

44:02.320 --> 44:05.160
And we were able to get state-of-the-art results

44:05.160 --> 44:07.320
using deep learning, and so on.

44:07.320 --> 44:10.800
And then since then, we have explores, as I mentioned,

44:10.800 --> 44:13.760
translating natural language description into SQL queries

44:13.760 --> 44:18.200
and also building even like language translated,

44:18.200 --> 44:21.240
translating programs written in one program in language

44:21.240 --> 44:26.240
into another, and many other application domains and so on.

44:26.680 --> 44:31.360
And also trying to develop a method that's essentially

44:31.360 --> 44:35.800
make it the learning process more sophisticated

44:35.800 --> 44:39.760
to be able to leverage more about execution semantics

44:39.760 --> 44:44.760
and also better learning from its past mistakes.

44:44.840 --> 44:47.760
So many interesting techniques in the space.

44:47.760 --> 44:51.600
I think the field overall has really flourished

44:51.600 --> 44:54.080
over the last few years.

44:54.080 --> 44:56.800
I imagine you've seen this demo, again,

44:56.800 --> 44:59.200
referencing back to GPT-3.

44:59.200 --> 45:02.680
There's one demo that's kind of making the rounds

45:02.680 --> 45:05.720
of someone who built a web layout generator

45:05.720 --> 45:08.960
that spits out JSX code, and you can tell it,

45:08.960 --> 45:11.280
make me a web page with a red button, a green button,

45:11.280 --> 45:15.360
and a blue button, and it creates the code to do that.

45:15.360 --> 45:17.040
Have you seen that one?

45:17.040 --> 45:21.320
I mean, now the particular, but I see similar applications.

45:21.320 --> 45:26.040
And in the past, actually, we also tried to do

45:26.040 --> 45:28.800
some of this type of applications as well.

45:28.800 --> 45:32.360
I think, yes, you can do that.

45:32.360 --> 45:35.440
Gigantic language models will play a huge role

45:35.440 --> 45:37.720
in the field of program synthesis,

45:37.720 --> 45:42.280
so do you think it's going to be maybe more constrained

45:42.280 --> 45:46.400
techniques that allow us to make big progress there?

45:46.400 --> 45:47.400
That's a very good question.

45:47.400 --> 45:49.960
I think in general for program synthesis to really work,

45:49.960 --> 45:54.000
we need different components, different types of techniques

45:54.000 --> 45:54.840
and so on.

45:54.840 --> 45:56.680
So certainly, for this type of language models,

45:56.680 --> 46:01.200
we know certain network architectures, like a transformer

46:01.200 --> 46:03.440
and so on, they just have been so powerful

46:03.440 --> 46:07.080
they can solve so many different types of problems

46:07.080 --> 46:09.680
in so many different types of applications and so on.

46:09.680 --> 46:12.680
So I think it's different types of network architectures

46:12.680 --> 46:17.680
and certain components are definitely very, very helpful.

46:18.720 --> 46:21.280
But on the other hand, I think with program synthesis,

46:21.280 --> 46:25.760
there's also another reason that I really like the domain

46:25.760 --> 46:29.760
is that for solving problems in this area,

46:29.760 --> 46:31.560
you also really need to understand

46:31.560 --> 46:35.280
semantics in a much deeper level to write,

46:35.280 --> 46:37.640
to know what you want the program to do

46:37.640 --> 46:40.320
and hence what type of programs you should generate.

46:40.320 --> 46:43.480
I don't think just having a transformer itself

46:43.480 --> 46:44.920
can help us solve program synthesis,

46:44.920 --> 46:47.560
but definitely, I think it's an important component.

46:47.560 --> 46:50.600
And that's also why it's a really exciting field

46:50.600 --> 46:53.720
we need to, for the explore and develop

46:53.720 --> 46:57.080
other types of techniques and approaches.

46:57.080 --> 47:01.680
As I mentioned, trying to learn more about the semantics

47:01.680 --> 47:05.320
and also trying to learn from past mistakes,

47:05.320 --> 47:08.840
also trying to understand if the program

47:08.840 --> 47:10.840
that you've generated is now the right one,

47:10.840 --> 47:13.400
you try to identify the type of issues

47:13.400 --> 47:15.480
that it's why it's not working

47:15.480 --> 47:17.480
and try to learn how to fix it,

47:17.480 --> 47:19.720
very much like a how humans program,

47:19.720 --> 47:23.840
how we can crack sense, right, like the programs and so on.

47:23.840 --> 47:27.520
So we hope that the tools for program synthesis

47:27.520 --> 47:32.520
can leverage many of these types of capabilities.

47:33.720 --> 47:36.960
You've also done a bit of work on contact tracing

47:36.960 --> 47:40.960
as it applies to coronavirus and COVID-19.

47:40.960 --> 47:43.960
Can you share a little bit about what you've done in that area?

47:43.960 --> 47:47.920
That's another concrete example

47:47.920 --> 47:50.880
of what we call responsible data use.

47:50.880 --> 47:51.960
So in particular,

47:51.960 --> 47:53.840
you focused on the privacy side there?

47:53.840 --> 47:55.240
Right, right, right, exactly.

47:55.240 --> 47:58.400
As we know, with this pandemic,

47:58.400 --> 48:01.120
it's changed the world.

48:03.120 --> 48:06.880
Nobody has, I think, really, really expected

48:06.880 --> 48:09.400
that it can change the world so fast

48:09.400 --> 48:12.840
and as such a global scale and so on.

48:12.840 --> 48:15.520
And hence, of course, finding solutions

48:15.520 --> 48:18.080
to fight the pandemic is really important.

48:18.080 --> 48:22.120
And of course, data is the key driver

48:22.120 --> 48:23.840
in fighting the pandemic.

48:23.840 --> 48:26.080
Then the question is, as we use data

48:26.080 --> 48:27.960
to fight the pandemic,

48:27.960 --> 48:31.120
how we also at the same time need to ensure

48:31.120 --> 48:34.280
that the data is being used in a privacy-preserving way

48:34.280 --> 48:37.560
and used in a responsible way.

48:37.560 --> 48:42.560
Because otherwise, you can really have huge compromise

48:43.360 --> 48:44.760
of individuals privacy,

48:44.760 --> 48:49.760
which is really sensitive for individuals and so on.

48:50.120 --> 48:52.280
And also, if users are concerned

48:52.280 --> 48:54.080
about their privacy,

48:54.080 --> 48:56.760
this will also limit their participation

48:56.760 --> 48:59.880
in whatever apps that you try to have users to use

48:59.880 --> 49:03.080
or in whatever systems that you are trying to deploy.

49:03.080 --> 49:07.440
And hence, as a concrete example and application

49:07.440 --> 49:10.760
of this approach of building a platform

49:10.760 --> 49:13.440
for a responsible data economy,

49:13.440 --> 49:18.280
we are investigating how we can utilize data

49:18.280 --> 49:20.560
in a more responsible way.

49:20.560 --> 49:24.400
And at the same time to enable, for example,

49:24.400 --> 49:27.880
fasts and effective contact tracing.

49:27.880 --> 49:32.680
So we have been exploring different approaches

49:32.680 --> 49:34.040
and developing different techniques,

49:34.040 --> 49:39.040
including improvements on cryptographic protocols

49:39.080 --> 49:43.160
for more privacy-preserving contact tracing.

49:43.160 --> 49:46.120
That's extension to the Google Apple Exposure Notification

49:46.120 --> 49:49.720
Protocol to provide more privacy protection

49:49.720 --> 49:54.720
as well as enabling what we call a secure distributed

49:55.480 --> 50:00.160
computing fabric to enable this type of contact tracing

50:00.160 --> 50:03.360
work to be done over different data sources,

50:03.360 --> 50:05.320
but not in a centralized manner,

50:05.320 --> 50:08.680
in a more distributed and decentralized manner.

50:08.680 --> 50:13.040
And also, I wanted to mention that, of course,

50:13.040 --> 50:15.680
this is really important and time-tapping.

50:15.680 --> 50:20.440
We are actually launching summits called

50:20.440 --> 50:22.400
responsible data summits.

50:22.400 --> 50:27.240
And we dedicate one day just on responsible data

50:27.240 --> 50:32.240
in the time of pandemic to discuss the various issues

50:32.440 --> 50:36.680
about how we can use data in a more responsible way

50:36.680 --> 50:41.400
while providing effective solutions for fighting pandemic.

50:41.400 --> 50:46.400
And also another day talking about the cutting edge technologies

50:47.160 --> 50:52.000
and also the latest thinking in legal frameworks

50:52.000 --> 50:56.680
for responsible data technology and policy in the real world.

50:56.680 --> 51:01.360
And I think that that can provide more exciting details

51:01.360 --> 51:03.720
for people who are interested.

51:03.720 --> 51:06.920
And responsible data.AI.

51:06.920 --> 51:07.760
Right, great.

51:07.760 --> 51:10.360
And will the, I believe this will be published

51:10.360 --> 51:12.320
after that conference,

51:12.320 --> 51:15.520
but will the videos be available for folks to check out?

51:15.520 --> 51:17.720
Yes, the videos will be online.

51:17.720 --> 51:20.840
And we have a great line of speakers,

51:20.840 --> 51:24.240
including Yushua Benjou, actually we'll talk about how

51:24.240 --> 51:29.240
he uses data to fight pandemic in contact tracing

51:31.560 --> 51:35.160
and in the privacy preserving a responsible way as well,

51:35.160 --> 51:37.800
as well as many other great speakers.

51:37.800 --> 51:39.080
Awesome, awesome.

51:39.080 --> 51:41.560
Well done, thanks so much for taking the time

51:41.560 --> 51:44.800
to share with us a bit about what you're up to.

51:44.800 --> 51:46.440
Great, thanks a lot for having me.

51:46.440 --> 51:47.280
Thank you.

51:47.280 --> 51:48.280
Thanks, thanks a lot.

51:51.280 --> 51:54.400
All right, everyone, that's our show for today.

51:54.400 --> 51:55.960
To learn more about today's guest

51:55.960 --> 51:58.040
or the topics mentioned in this interview,

51:58.040 --> 52:00.360
visit twimmelai.com.

52:00.360 --> 52:03.120
Of course, if you like what you hear on the podcast,

52:03.120 --> 52:06.000
please subscribe, rate, and review the show

52:06.000 --> 52:08.160
on your favorite pod catcher.

52:08.160 --> 52:11.200
Thanks so much for listening and catch you next time.

