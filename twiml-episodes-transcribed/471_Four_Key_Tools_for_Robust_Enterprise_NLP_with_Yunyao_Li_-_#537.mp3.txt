All right, everyone. I am here with Yoon Yao Lee. Yoon Yao is a senior research manager with IBM research working on natural language processing.
Yoon Yao, welcome to the Twoma AI podcast. Thank you so much for having me here.
I'm looking forward to digging into our chat. We'll be talking of course about NLP and some of your experience on the research side as well as making it useful for IBM's enterprise customers to get us started.
I'd love to have you share a little bit about your background and how you came to work in the field.
Sure. So as you mentioned right right now, I'm a senior research manager at IBM research before that.
How did I get there? It's actually a long story. So I have to say, I grew up in a small town in China, right?
Before I went to college, I didn't even see a computer. But when I picked my major, I picked the major automation because I thought if I automated everything, I don't have to do anything.
It's like a part of really think about AI helping people.
So when I went to college, I also did a secondary degree, I did a dual degree, I did automation and economics because I want to understand how technology impacting people, impacting the society.
Then I went down to come to US to pursue my graduate studies. So I did two master degree again, I did a line in computer science, another line in information science because again, I want to understand the technology itself, but I also want to understand how technology impacting people.
So my information science, more focused on HCI side and information economics. Then I pursue my PhD in database. However, it's not a traditional database, PhD.
So what I did was to enable people to query database using natural language. So that's basically the longest topic today in natural language processing.
So I will say my connection with AI basically started from the very beginning of when I started my higher education.
Nice, nice. And now as a senior research manager, talk a little bit about your research interests and more broadly, the role and.
It sounds like you you're focused on kind of traditional research, but you also have these customer engagements element to your role. Can you talk a little bit about that?
Sure. So my general research interest is quite broad, but in general, I'm very passionate about building systems and the tools to enable people wide range of users to be able to use technology.
In particular, since I work in the field of natural language processing, my passion is really to empower people to harvest information from text to build the next generation of AI applications.
So within IBM, you know, we have a lot of products based on natural language processing capability, like once in L, L, U, once in discovery, and so on. So behind this technology, I'm very proud to say a lot of the products are part of our technology developed by my team, so really empower the product themselves.
Meanwhile, you know, when we have since in the lab, they may not always mature enough to put in a product yet. So then we also often engage with customers and also a technical system to say, OK, for this particular problem, we don't have a product yet. What can we do for the customer?
And that really helped me to understand what are the key challenges that we need to address from research point of view.
I think that in my mind, you know, as someone working in industry research lab for over 10 years, right? I really enjoyed that.
Because when I was in school, how do you always need to think about what's the motivation behind the work here in industry research? I don't have that problem. The problem come to me.
And then the interesting part is, how do I generalize one person's specific problem into a research problem that can be solved in I stage the fashion, like both very ambitious from scientific point of view, but at the same time with the nervousness.
Yeah, well, I want to dig into some of the specific challenges that you've identified in the NLP domain, but projecting from individual customers challenge to kind of a broader, interesting research problem is one that I'm also interested in.
Can we maybe start start there and I'd love to get your take on that.
So maybe I can share a story, right? When I joined IBM, I work on a project called a system T. So system T is a declarative system for natural language processing.
And I have to see to a certain extent, I'm still working on the system. The reason is that when we have a larger research agenda, we cannot solve the problem at once.
So for example, how system did come into being that because at the beginning, we want to build a better enterprise search system.
So when we trying to support enterprise search internally within IBM, we found the traditional Ion system do not understand the document enough.
But once in that it differentiate internet search and the internet search is in internet search, when you try to find the answer to a question, maybe it's owning a one page that has the answer.
Unlike when you do a Google search on the internet, when you search for certain question, right, you may have hundreds of pages.
So then internet is really important to understand every single document. And that really motivated the team to build a natural language processing system. And then at that point, what we found is there are a lot of problems in creating natural language understanding system.
But the most basic one is really, for example, the two information instruction identify important entities, identify important events and so on. But at that point, we don't have a very scalable system to perform the task.
So then we start building the system T where we kind of abstract out what are the common text analytics operations that we need to capture. Then we build a system at the beginning, our concern is about expressivity and round hand performance to be able to express the kind of task that are important for supporting enterprise search.
Then as we go, we found out, okay, we can build a system that is really good to building extractors for supporting enterprise search. But then we want to enable more people to use them, right.
Then we kind of say, okay, now we have a problem of how do we build a better tuning into it? How do we incorporate more advanced machine learning capability into it?
So kind of at a different period of time, we have different focus, but overall because we have this one bigger project, we can build one thing on top of the other.
So I think that's really the advantage of working in our industry lab where our research is not bounded by a fixed period of time for grant, right.
But it is kind of generalize the overall life cycle, like we have a concrete business problem, for example, we started of how do we support a better enterprise search, right.
Then we turned that into how do we build a system to support a class of such problems.
And then we saw some of them, we build some prototype, we evaluate with concrete use cases, and then we get a feedback.
When we can solve certain kind of concrete problems, we can push it out into the wild, and then we get additional feedback from the wild.
Then we incorporate to say, okay, now we have the system should we focus on improving the system itself, or should we focus on the tuning, should we focus on other things.
I think a lot of the work from the team really kind of come with the same life cycle.
Maybe jump into some of the challenges that we referenced earlier, you know, when you are thinking about and trying to help teams productize NLP in the enterprise.
What are some of the challenges that you run into and kind of how do you think about those, those challenges, how have you organized those in your head.
Yeah, so that's a very good question. I will say we can think of the four category of major challenges.
The one is complexity, complexity in terms of the, we can talk about not long processing, right.
In terms of the complexity of the document, we have to deal with, and the complexity of the tasks themselves, we have to deal with.
The second category is small data. So again, we want to talk about enterprise-facing challenges, right.
We often have two new things that works for everybody, but we don't necessarily have data that are representing everyone's problems, right.
So therefore, we need to be able to address the small data problem either based on what we have internally or have some way to enable people to provide data for us to help themselves.
The third part is customization. No matter what we build, right, how well we build it, the other vast capability may not be sufficient for a customer specific use case or works as well as it could be on their own data.
Then how do we enable the customers to quickly customize what we build to fit into their use case, their data very quickly, and it usually do not necessarily require someone like me, right.
Someone with a big degree to work with them. So instead, maybe someone with a business knowledge to be able to do this work on their own.
And then the fourth part of explainability. I know, you know, everybody talk about trusted AI and so on these days, right.
One big part of trusted AI is explainability. Then here we need to understand what kind of explainability makes sense to what kind of audience, right.
And how do we provide variety of explainability to help people. So I would say, yeah, those are the four major challenges we are focused on from enterprise, not along with process importantly.
When you think about those challenges, do you think about them independently, meaning does.
Do you have a set of solutions or a set of research directions to try to address complexity and a separate set to try to address small data, et cetera, et cetera.
Or, you know, is there a way that you kind of unify these in the solutions that you're trying to apply to these various problems.
Yeah, I will say we have a few common approach, a combination of them to solve all the problems, maybe with different combinations.
I think the four things we often use as a phone number one is data augmentation, right.
The second is have a very powerful declarative language that captures some of the main primitives that are important for conduct the task.
And the third one is neural symbolic AI really leverage the best of both word in neural network, as well as the symbolic system. And the fourth one is human look, how do we involve human beyond just provide labor data.
So those are the four things that the basic secret formula that we have been following and then for 12 different challenges, we may use a combination of two of them or three of them.
Got it. Got it. I want to go through those in turn, but maybe to set some context, it'd be helpful to have you talk about, you know, some specific challenges and where each of those come comes in.
Sure, I think maybe I can describe a particular system we have recently built and I think the system kind of representative for all the challenges.
So what we have been working on with the Washington Discovery team was to build something called Washington Discovery content intelligence.
What this system does is to enable lawyers and not professionals to quickly review contract documents. If you think about contract, right, it's probably one of the most important business documents in the business world.
It's very expensive to review those contracts because you need to have legal professionals and the contract to often have many, many pages and contain a lot of information that is challenging even for you and me to understand, right.
So to build such a system, what is required? First of all, the first step is a lot of contract in PDF format. So we need to convert the PDF into a form of a consumable by the machine, right.
Then secondly, we need to build models to simulate what the no professionals would do so that can help them, right. So what contract understanding involved on hardware is a form.
Then every single document identify the clause for simplicity content of sentences for every single sentence identify what are the categories of that particular sentence and what are the parties involved in the sentence.
If I say the buyers share pay supplier one million dollars, right. So in legal terms, this is an obligation and the party involved is a buyer and supply, right.
And then there is some amount. Then we turn this into a sentence, first of all, identify the sentence, right. Identify sentence from the document and then do classification so that we can categorize the sentence and also do extraction to extract what are the parties involved like supplier and buyer.
So now, then once we have the system build, it will not work perfectly, right. So we need to surface it to a legal professional so the legal professional can say, okay.
Now I want to review all the clause related to obligation and then look at those sentences and to be able to say, okay, this is indeed an obligation.
I want to do something like market as high risk or low risk or no problem. But think of the challenge we just mentioned, like complexity.
First of all, the complexity involved in the document itself, the document is the multi-page PDF document that we need to preserve all the structure, all the information and also be able to identify, like assuming the document conversion is correct, we also need to identify the document structure and to be able to identify individual sentences and the context, right.
Then why is that small data problem because we come to contact nobody going to share the contact with IBM for us to create training data, right.
If we want to view that model, we have to rely on IBM contact data, but at the same time what we build need to work for other people, right.
Then why is that customization problem because even though we're talking about the legal professionals follow some general taxonomy related to the classification, every single company they may have some difference.
For example, if I sentence mentions patent, right, or a trademark, it can be viewed as a clause related to intellectual property, but some company want to separate them.
They want to say, okay, I only identify all the clause related to trademark as trademark, but everything else related intellectual property is intellectual property.
And then finally explain infinity, when we surface the results to the legal professionals, we also need to, especially when there's a mistake from the prediction, we need to explain to the legal professional, like why we predict this sentence as something, right.
Like this is obligation rather than something else, we need to explain that. And also what that will help us is when they say this is not correct, we can also look into potentially how to fix them, and they can explain to us, okay, why certain sense is not labeled correctly, like what I just described earlier related to customization, right.
When they come back to say, oh, this sentence should be labeled as trademark, instead of intellectual property, they can explain to us to say, okay, for this particular thing, you identify this clause related to trademark, but in our company, this is regarded as trademark rather intellectual property.
So they also have the model developers to make some changes. Yeah, so I hope this example kind of initiated all the different thread, but I will also be happy to talk about, you know, how do we solve them.
Yeah, yeah, well, we'll definitely do that one question that does come to mind. Yeah, I've talked in the past with folks that are, you know, working on these production, NLP systems, and over time, the need to supplement the learn models with rule based systems and heuristics has decreased.
And I'm curious in the cases that you deal with, you know, in addition to the data augmentation declarative neuro symbolic human and a loop, you know, do you also need to, you know, are you still worrying about, you know, rules and exceptions and all of these things as well.
And do you see those decreasing over time?
And depending on the use case, so for example, for some use case, when full explainability is important, we actually see requirement to combine both.
So let me give you an example, right. So if we think about model development, it's a life cycle, right, we have data gathering, we have the developing the model self, we have past and validation.
For some industry, for example, for retail banking.
The fact, you know, the training test and the validation, what we learned is if you use a block box model, right, like a neural approach, assuming you have sufficient data, the training is very fast.
Test also will not take too much time, but the validation will take a lot of time.
So what they do in validation really ensures the model work as predicted under different variety of conditions and also be able to explain why certain predictions produce the particular results versus what is expected.
Right. So this takes a lot of time.
But if we use rule based approach, right, it's kind of opposite. So the developing of this rules take a lot of time.
Testing also takes some time, validation doesn't take too much time. So overall, you cannot see the time and effort distributed in a different way.
So what I might seem has been working on is actually kind of get the best of both work. So basically, what we do is we take a neural network.
We leverage all the advantage of neural network to be able to learn from larger amount of data, right, but then we produce several rules that are completely transparent and explainable.
So the advantage of this and then I don't my expert can actually come in to do further inspection and augmentation to ensure the model not just learn from the data, the model also learn from the domain experts, the corresponding domain knowledge, right.
So the benefit of this approach is the training will not take as much time as the purely rule based system, right.
It takes similar time as that whatever approach we are taking for black box model testing is also similar, but the validation takes significant and shorter amount of time than if you take a black box model.
Depending on the use case, right. In some use cases, you really don't need to have full explainability. Then I think we are using neural model.
Probably is a way to go, but in the use case is where explainability is important.
Because it's important to hold somewhere accountable when the system make mistake, right. So then this approach works really well. Another aspect is remember what we talk about about small data problem, right.
Actually, a lot of the area we don't have sufficient data. So when you learn from the data, it will not capture the whole domain knowledge, right.
So I think what our experience, the domain experts really like the fact that they can not just provide input by labeling.
They can also provide input by inspecting what learned from the model and make changes not just to understand how model performed, but able to make changes.
For example, sometimes like what I just described earlier, right, to say, okay, share, pay, supply, I mean $1.
Like a potential rule that we can learn is when there is a predicate indicating like a verb indicating purchase indicating some business transaction.
And then there is a model, there is a modality of necessity modified on a particular verb, then it's potentially a indication of obligation. This is something completely transparent, right. We can learn such a rule.
But what are considered as important business transaction verb, this can be given by the domain experts, not just by learning from the data, right. We can also let the domain expert to inspect the needs of dictionaries, renowned, miso rules, renowned so that we can ensure their knowledge is captured by a model.
And then we can combine, right. So this model can be then combined with a block box model so that we can really ensure, you know, the overall performance is as good as possible.
But at the same time, we're able to predict or explain significant portion of the prediction and how the two models are combined together, depending on use case, like for example, if you want to say, I want to have as good performance as possible, then if I can explain some of the results, that's good.
Then we can combine it in one way or you want to say explainability is most important to me, right. Then with that, I also want how good performance in you can combine this to in a different way.
So I think with this particular approach, we really can get a best of both words, you know, be able to leverage the fact neural network is very powerful to be able to really capture a model nuances from the data.
That is very hard to express in rules, right. But at the same time, capture as much as possible, the domain knowledge, that can be clearly articulated through rules.
Another thing I want to mention is the rules are not based on syntactical pattern, right. The rules are based on abstraction, we obtain through natural language understanding.
For example, given the sentence, fair share pay supplier through natural language understanding, shadow semantic parsing, we will know there is an action of pay.
And then the performer of the action is the buyer and the target of the action is the supplier and then the manner is pay with, you know, a million dollars. So this kind of semantics is captured through advanced natural language understanding.
We can express rules in a very concise but very powerful way. I think that's another thing people need to think about. Rules are not necessarily one token for another, right.
But if we build a rules on top of semantic abstraction, they can be much more powerful. And then behind the thing is also leverage the deep learning, right.
So behind is in leverage in neural network model to do not long understanding.
Got it kind of. And now just to make sure that one, the same page is what you describe, we kind of came at it from the perspective of rules, but is this the neural symbolic AI that you spoke about earlier.
Correct. Correct. Yeah. So I think neural semantic, you can basically think of that two approaches, right. One approach is you embed symbolic information or expression into neural model, right.
That helps people can ingest some domain knowledge into neural model. But then the neural model coming out still not completely transparent, right.
The other way you can do this is you can use neural model to produce symbolic model or symbolic abstraction, then we can build a symbolic model on top of it.
So right now, imagine we actually have both approach, but when I'm describing more of a ladder, where we leverage neural model to empower symbolic model because the symbolic model has a lot of advantages that people is fully aware, right.
Explanability and the, and the part where, you know, it's easier for people in the art ways and understand.
Yeah. So this is one particular neural semantic approach we have been taking.
And then one of the other techniques that you mentioned was the use of declarative languages, imagining that, you know, you've used this neural model to create
kind of this abstract rule, let's say about a particular scenario, you want to make that accessible to all the humans in the loop that we also talked about.
A way to do that is to kind of represent this abstraction using some kind of declarative language.
Exactly. So I think, yeah, I think that's a very, very good point, right. So you can express rules in grammar, but the challenges that grammar also detect how the rules will be executive, right.
Then that makes the real creation much more challenging and less expressive and also how runtime performance issue.
So we can overcome that by leveraging the declarative system we have mentioned earlier, right. Like the system I have been working with since I joined IBM is we can separate execution from the actual semantics.
So you can specify the semantics using declarative language, but how it will be executed.
It determined by the optimizer, the optimizer would determine what's most efficient way to execute. So for example, if my rule is to identify all the sentence,
this business transaction and it's an include the modality indicating necessity, I can do it in two different way, right. One is I can identify all the sentences with necessity and then see whether there is the business transaction next to it, or I can do the other round, which was more efficient or decided by the optimizer.
When I created a rule, whether manually or automatically, I don't need to worry about execution. The execution will be done automatically by the declarative system to ensure we get the most optimized plan in terms of execution.
And when we talk about execution in this context, what do we mean we typically think of, you know, neural network, inference or, you know, even more broadly ML model inference as kind of this one, you know, you make a request you get a prediction back.
What do you think about execution more from the perspective of like information retrieval and queries and that kind of thing.
So here, when we have a rule based, like based on this kind of model, right. So for example, for simplicity, necessity, we do two part. One part is just to not to language understanding. The second part is just enforce the predicates, right. So the execution composed of a few different options. One option is that you perform this not to language understanding operations.
The neural model we have right on every single sentence. Then for every single sentence, you also check whether it contains any business transaction mentions and also some words indicating necessity.
That's a long way of execution. Another way of execution is that I can actually do some optimization. Instead of looking at every single sentence, I can first retrieve only the sentence contains the business transaction. Then I perform not to language understanding, then I check my predicates.
So it's really how we produce a final result. So that's the executive time talking about it. Got it, got it. So to maybe kind of recap this and put it back in the context of this original problem where you're trying to enable legal discovery across lots of contracts.
One approach one my take is to use traditional deep neural networks and maybe a supervised learning kind of thing or even unsupervised and kind of clustering the different clauses or sentences.
But you know what you've found is that by combining by using the neural network instead of to classify, but rather to kind of do something akin to like entity extraction or or to or semantic parsing.
And you can use that to generate this essentially a rule set that's kind of expressed in this declarative language. And then you've got a system that executes those rules against the given contract and uses that to essentially make sense of it.
So you can identify whatever it is that you might want to identify it, you know, at a given time. Right. Yeah, on the high level, you can view it that way. But we can also do combination right. So you can basically think of I can either build a block box classifier or I can build a white box or I combine the block box and a white box together so that I can get it the best of both words.
But in reality, we actually do both. So I think the challenge of doing this block box one another another challenge is especially when we start with building the product, we don't have even a lot of labor data.
And also, as I mentioned, right, we have this small data problem. We have data from IBM, but we don't have data from other company. How do I ensure the model I build works well. So here, the other domain performance is very important. Right.
So what we actually started with one of the different approach, right. You can think about in IBM research, we also build a lot of different neural networks. Right. That's really a thing that accessible to other people. We also have it. But we really don't side by side. What we found is that the
domain performance for the deep neural network works really, really well. But then it drops very significantly when we do auto domain. So that really passionately motivated us to build more of this transparent model so that the auto domain performance is very similar to
the intonement, then we can augment this particular model if we want to have a better performance to say if we have additional training that we can also kind of augment this with a block box model. But at the same time, we can benefit from the
identity, transferability, and also be able to really quickly start with a particular customers use case without worry about training data. Because I think a lot of times, I mean, I'm deeply involved in not
processing research, right. So if you if you look at the papers, people often are shown we already have labeled that are very nicely available. Right. Then if you don't have a label that you can just do crowdsourcing. But in the case of enterprise applications, like I cannot even label this data, right.
The contact information is really, really rich. So I remember at the beginning, when we build this product, we look at all the sentences and like true sentence looks almost identical. We go to ask the lawyer to say, okay, why? Why those true sentence looks
identical label differently. The lawyer tells us, oh, because one is within this section with this information, but the other do not. Right. Again, like, how do you even captured like, let's see, you have this information.
I can quickly basically code that rule to say, okay, take this section information into account. Right. I can do that in a few minutes. But think about how do I return my model to do that? Like, now I have to.
Yeah. So I think in general, I would say depending on the use case, right. There is the in my opinion, there is no one solution. We need to really use the combination of all these technology techniques available in your
toolkit and take the practical challenges and also what's the final goal in mind. Right. Like, for example, in our case, our goal is not to say, build the best possible model for IBM contract. If that's my goal, I don't need to have this approach. Right.
I can just do this training and then have a model that works really, really well for Indomie. But our goal is to build a good model for other people and enable them to customize and not really come to the solution that I mentioned.
So we want to take advantage of both.
In describing that solution, you mentioned a lot of different options, you know, the white box, the black box, et cetera.
How do you, I'm wondering like where you're making those decisions, is it, you know, it's not at the level of product because you want, I'm imagining a platform that kind of does, you know, they can handle multiple use cases.
You know, do you have different paths that you go down for different use cases or am I thinking about this incorrectly and, you know, it's more of you have this toolkit and you have to apply this toolkit kind of more in a consulting or orientation when you're faced with a particular use case.
I think that I feel different ways to approach it, right. One is like, for example, when we build the one I just described this particular offering in Washington Discovery County Intelligence, it's one, it's one offering, right.
And then in that offering, we take this particular approach and then we show it's really useful.
But assuming like the customer comes to me with something else, then one thing we're actually doing right now is to take more of our auto AI approach where they usually give us the constraint.
We will figure out based on the constraints they give to us, what's the most, like what's the optimal combination.
One of a very trendy topic. So, for example, having faced recently pushed out a new feature called the auto AIP, right. Because even with the block box, you still have a lot of challenges like which models you use, how do you choose the hyper parameter and so on.
Right, so here is actually think about the main parameter we introduce is we have additional type of models and we have additional ways of ensemble them together.
And how do we do those could be automated when the user specified constraints. So, for example, the user can say, I want to have best performing model with some explainability, right.
Then with those constraints, we can automatically figure that out. This is something we are building in our project called auto AIP for text. So, we're building all these operators that support different kind of model and different model have different properties, right. So then based on the user constraints, we can decide which combination models we use and then which is the same.
The model to use and then which model to refine further look at it, you know, refine the hyper parameter tuning and refine, how do we do the final ensemble.
Got it, got it, got it. Since you mentioned the auto NLP and hugging face, I'll quickly mention to folks that I interviewed one of the folks who worked on that.
I'll be check thicker and we'll drop a link to that interview in the show notes. But I want to get back to data augmentation, because we haven't really talked about that so much here. And I think it's, you know, it's.
Often easier to think about how to do that in the context of computer vision, you know, add noise or change the orientation or flip or rotate or that kind of thing. What does that mean in the context of NLP and how have you included it or or incorporated it into your projects.
Yeah, sure. So talking about data augmentation in NLP, actually, there is an excellent tutorial or survey paper come out very recently and summarize some of the techniques I can, I can still be a link later on.
But in the context of what we're working on, we do that augmentation in a few different ways.
The first one is, so I mentioned we do not learn with understanding, right? We do, you know, give you a sentence we pass into some semantic structure.
So we have enough data for English to do that. However, for other languages, we don't have this nice data, right?
We don't have labor that are to perform the same task, but we want to also support other languages. So one technique we have been using in terms of data augmentation is we can leverage the fact there's the bi text we have sentencing one language like English and sentencing another language that is translation of the English sentence.
We can do sentence alignment, right, align each token with each other and then we can project the annotation will produce an English onto other language.
So now we have some very noisy label that are to start with, right? Because you can think about the always lost in translation, right?
So when you do this projection problem is going to be some mistakes. So we also do additional filtering to ensure that the data will produce as good as possible.
And then we can use this data to train a semantic posture. I challenge a man posture in the other language to further augment the data, kind of repair the data. So maybe after projection, for example, in my sentence, I only have some portion with annotation, the other portion with do not.
Then we can do this with strapping to other additional ones. So this way we can automatically at scale produce larger amount of data. So this is like one way to do that augmentation.
Another way you can also do it is, so for example, while my colleagues they have been working on dependency parsing for other languages, right? So again, we may not have sufficient data for other languages.
So what they have done is they translate the English sentence of other language into English sentence and then back and forth.
So this way you can also produce larger amount of what we call silver data. It's not good data because it's still contain good amount of noise. But when you train from this data, the neural model is able to distill the noise and to be able to learn a pretty reasonable model.
So a lot of the data augmentation in natural language, kind of doing this synthetic data, all annotation projection, annotation transfer.
There are also ways to generate the data, like for example, if you have this kind of a recent paper we published in ACL earlier this year.
So one of the parts that are challenging for parser is handling questions because most of the training data for natural language processing are statement, right?
News report or something. But often we have data load that actually are questions. So what we find is those the parser trend using the typical data set do not work well very well for questions.
So then what we did, but then labor question is very expensive, right? Producing labor data for question, right?
So what we did is there is a data set called a question bank is are some maybe a couple of thousand the sentence and so on.
So we basically generalize from those questions from from the question bank to generalize into template. So with the template, we can generally generate a lot of labor data for question and then we can add those generated question back into the training data so that the parser can handle question very well.
So that is actually to great extent similar to what people do in computer vision right is just how you manipulate the data is different because well you don't deal with tax care instead of vision right.
Nice, nice, we haven't talked much about human in the loop. I think a lot of folks are familiar with some of the common uses of human in human in the loop.
You know, of course, exception handling is one that comes to mind. Is that the kind of thing you are referring to when you mention it? That's a very good question because I can have very strong opinion about that.
Human in the loop is actually very broad and I can describe a few different aspects in my opinion human can be very important as part of the model development process.
We think about human in loop is more about labeling right. How do we involve people to provide labor data? That's very important. In fact, we have spent a few years trying to figure out how do we enable average person to be able to provide high quantity data for shallow semantic parsing because our initial way of doing this shows that the human cannot perform as good as our machine learning model.
Then we need to come up with a more intelligent flow to enable average people to do as much as possible and then enable experts to do the rest.
So I think that's one big part when we talk about a human loop is how do we enable human to generate high quality data but not necessarily all be done by crowdsource worker because not every task is possible. But then at the same time, we don't necessarily always fall back to the expert in the loop.
So we need to kind of have crowd in the loop but at the same time involve experts or automatic method as much as possible. Then one thing people don't really talk too much related to human in loop is the model development.
The reason is very simple. Most of the time when people talk about model development, you are talking about block box model. But when we talk about white box model or gray box model, we want to involve humans in loop in a few different ways.
One is what I described as before. When you define your network, learn a transparent model. Now human can inspect the model, augment the model, modify the model. That's one big part of human and loop.
The second part is maybe the user do not need to give us a lot of data at the beginning. Instead, we can have the model creation that are gathering in one intelligent simplified mode.
For example, when I talk about classification of this particular cross. Instead, the user give me a few thousand labels that are a trim model and then we write what we can do.
The user may not even specify a path up from the user to say, hey, this sentence is interesting. I will label a few interesting sentences as an example. Then that is the machine figured out what are other similar sentences.
Again, the machine may not fully understand what I mean by similar. So the machine will propose some potential interpretation and ask the user to provide feedback. Then as the user provide feedback, the machine can learn a bit more and refine what the machine has learned.
In the end, it will produce a model, but it's a very collaborative process. In between, the user may go beyond just give me an example. The user may even give me a rationale to say, hey, why I think this sentence is interesting or why I think what you propose is wrong.
Then it's very interesting to incorporate this kind of what we call reach feedback into the machine learning process. Again, this is like a model development process.
Another approach is, as I mentioned before, we have this declarative system. The declarative system is actually very good for tasks that have very obvious patterns. So rather than trying to let the black box model figure it out, we can specify that.
But specify that can be expensive. So we can also have this process of user giving example. I can explain to the user in my declarative language what I have learned and then the user can give me either additional feedback or modify that particular declarative statement directly.
That's one big part of human loop and model development. Finally, about user feedback. How do we enable the user to provide additional input to the system when the system already deployed?
Again, that's also an important part. We're thinking about today in a lot of systems. If the user is not happy, you cannot really do anything.
So the system produces whatever it produces, that's it. But you can imagine a system where, for example, the contract intelligence system I mentioned before. When the user sees outputs, they use the not company happy with it. The user can explain why she's not happy with the results.
And then can also maybe provide some additional input to say, okay, here, let's say, for example, in my company, we were differentiated pretty much and IP.
So we can learn from those and that can come back to our development team to be able to say, okay, is there something we need to address in our baseline model?
Is there something we need to address by enabling the user to be able to do some customization? So that allows to do much more intelligent model improvement and maintenance.
It sounds like the elements of this that you feel most strongly about are kind of a combination of closing the loop. So you put the human in a loop, but then close the loop so that it improves your model, but also an idea of like intelligent human in a loop use machine intelligence to optimize the way the human intelligence is being used in the loop itself.
Exactly. I think we need to give human more agency. We need to empower human rather than just, you know, sometimes if you think about it, if all the user can do it, give you labor data and then leave everything to the machine.
The human is almost powerless. You cannot do anything. You know the machine is not cracked, but other domain experts, you don't know how to modify the model.
You don't know how to do, right? You're kind of waiting waiting waiting. Hopefully someone will incorporate the data or the knowledge properly.
But when we actually give human agency, I think they have, you can do much more. So for example, actually in recent, in 2021, we can see a very interesting trend.
So in 2021, we see three workshops from top ARP conference that are related to human and like human and ARP. So we have a workshop interactive machine learning.
We have a workshop and ARP and HCI and we have a workshop and data science with human and loop language advancement. And next year in 2022, we're also going to have a special theme in NACO that is focused on HCI for ARP.
So basically more and more, like when ARP become more and more used in practice, human become an important aspect. When it's just in the lab, you know, all you need to do is produce some numbers and then compare with benchmark data.
You don't need to consider human, right? But now when the ARP actually impacting business, when it's actually used by people, you know, how do we use people to help with the entire development lifestyle or including evaluation and so on become one more important and you can see the trend.
Well, you know, thanks so much for taking the time to share a bit about what you're up to and walk us through these four different tools that you've had some success within delivering enterprise and LP.
Thank you so much for having me here.
Thank you.
