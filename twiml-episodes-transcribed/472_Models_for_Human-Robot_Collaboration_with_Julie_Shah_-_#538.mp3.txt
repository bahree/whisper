All right, everyone. I am here with Julie Shaw. Julie is a professor at MIT. Julie, welcome to the
Twomo AI podcast. Thank you so much for having me. So you are a professor of robotics in particular,
and we're going to dig into your work in that field. But to get us started, I'd love to have you
share a little bit about your background and how you came to work in the field. Wonderful, yeah.
So I'm a professor of aeronautics and astronautics at MIT. And I also lead a robotics lab at MIT as
a part of the computer science and artificial intelligence laboratory. And I have a special love of
like time, critical, safety, critical applications, which is why I'm in aerospace. And so my
story of how I ended up in robotics has to start with how I ended up in aerospace. And I was just
like born loving airplanes and rocket chips. I always wanted to be an astronaut. And I looked at
there was like a I went to space camp. I was a super nerd. And space camp that gave you a book
of all the astronauts and where they all went to school. And you know, they always stress you need
to be really good at math and science. And I was like, Jack, that can be good at math and science.
And a very large number of them either went to the military academies or MIT. And I was like,
MIT sounds like a good place. So whenever anybody would ask what I wanted to do, I would say,
I want to go to MIT and study aerospace engineering. And everybody would say, oh, that's so specific.
What's so specific? And I'd say, okay. And then I came to MIT and I in the aerospace department.
And then, you know, pretty, pretty soon after in the department, I'd be asked, okay,
see your aerospace job. But what are you going to specialize in? And I was like, oh, no, I have to
specialize further. Because it turns out that, you know, aerospace, much like robotics is a systems
discipline. And I really loved control theory and control systems and really loved learning about,
you know, aircraft auto pilots. And how those really had to be designed to the capabilities of
pilots of humans and, you know, how they could affect control over a system. Having to design for
the human machine system was really exciting to me and needing to understand both sides of that
equation. So I actually did my master's degree in human factors engineering to kind of go deep
on the human side. And it wasn't until my PhD, I switched gears and joined the computer science
and artificial intelligence laboratory and did my PhD in automated planning and scheduling.
And so, you know, this was 2010. I finished up my PhD. So before the big, the big era change
into, you know, to machine learning. And throughout have just been really, really interested in
how you design automation and technologies and, you know, AI and machine learning to like fit
like a puzzle piece against human capabilities so that you can achieve some end objective.
And that's why I, you know, started a robotics lab, the interactive robotics group and my lab
includes human factors engineers and aerospace engineers and computer scientists and,
um, uh, others as well. So we bring kind of across disciplinary perspective to, uh, developing the
technology and showing its benefit. Nice, nice. And your lab, do you still have that leaning
towards flying robots as opposed to arms or walking robots or humanoid or other types of form factors?
Yeah, it's interesting. It's interesting. You ask that because when I, um, when I applied for
faculty positions, it was straight out of PhD and, um, and I got the offer from MIT from my home
department. I did all my degrees at MIT and then I had this amazing offer from MIT from my home
department, uh, but they strongly recommended slash required that I go away for a year,
was before starting on the faculty. And they, um, my PhD was funded under an NSF fellowship. So it
was, uh, maybe not, not super closely tied to, you know, real applications of real systems.
And, um, the department head at the time suggested that I go out to Boeing and he said, you know,
they have interesting applications and robotics for manufacturing. I said, oh, it sounds great.
It sounds like a great way to spend a year out in Seattle. So, um, and, uh, and I just, I just,
I just caught the bug of, you know, uh, manufacturing. I just loved watching, you know, 737 or
737 being built in front of me and then thinking about the challenges. Again, it's like an integration
challenge. Like robots can do pieces of that work, but actually the vast majority of building a large
commercial airplane is still done manually, like a beehive of manual work. And so they're, um,
their challenges in how you design the technology, the robotics, the intelligent robot to integrate
into manual workflow. Um, and so when I came back, I had us, uh, I wanted to keep doing that. I kind of
wanted to keep working in robotics for manufacturing and collaborative robots. And I, I asked, uh,
a different department head on the one that hired me at the time. So, is it going to be okay if I,
you know, if I work in manufacturing in aeroastro or do I need to be flying things? And he said,
you know, pursue, pursue the problems that you think are important. And, um, and I, and I really
appreciated that. I work, I work on aerospace and, um, you know, applications, but I also work in,
um, uh, yeah, in, uh, you know, automotive, you know, um, I work in decision support for fighter pilots,
but also like nurses and doctors. Um, but the key thread is, uh, I'm just, you know, I think it's
really exciting to develop technology that really has to be flawless to add value and has to fit
with the human in a way that, you know, doesn't, doesn't add friction, but really eases a very
challenging job of a dominean expert. And so- So you can kind of envision that as a, uh,
venn diagram of, you know, mission critical in one circle and requires human interaction
and the other and your sweet spot is in the middle. Yeah, yeah, that's a, that's a good visualization
of, uh, of whether or not I take on a project. Yes. Okay. And so, uh, let's dig a little bit deeper
into that. What are some of the types of projects that you work on? Or maybe even more broadly,
how do you craft a research agenda in that, uh, that intersection? So, uh, I started on the faculty
at MIT just a little over 10 years ago, um, or maybe 11 years ago now. And, um, and from the, um,
sort of like the vision behind our work then and still now is to be intentional about developing
intelligent machines or intelligent robots or even kind of computing more broadly that enhances,
um, human capability, um, and well-being. And, um, and, you know, there are many ways to do that,
but, you know, starting out in the lab and going back to my PhD, I was interested in, you know,
how, how far can you get in, um, developing a robot, um, that can emulate the, the capabilities
of an effective human team member. Um, and that's not the only model of collaboration or teamwork
for sure. Um, and it's not even, it was always a hypothesis. It's not even a given that if you
develop, you know, um, a system with some of the, you know, capabilities or features of an
effective human team member, that that'll even be useful. So we've always, you know,
conducted experiments to try to determine, you know, whether or not that's useful. But my lab
focuses on developing novel AI models and algorithms where the AI is modeling people. Um, and so
there are, you know, decades of studies in what makes for effective human team work, you know,
in sports psychology and studying pilots in the cockpit or doctors and nurses in, in the operating
room. And it's very, it's a very practical field of study because you want to be able to train
up new people to come into this profession and work effectively in a team. Um, and, uh, but I'll just
brush that all aside and summarize many decades of very rigorous study in human teamwork and
human team coordination, um, in terms of, you know, like, there's, uh, three capabilities that we
as humans bring to teams that make us really, um, make some of us exceptional team members.
And that's the ability to know what your partner is thinking, to be able to anticipate what they'll
do next and then be able to use that information online, um, a circumstances unfold to, um,
uh, uh, to, to sort of change your plan. And so, um, my lab has focused on doing those three things,
developing models that can infer human cognitive state, predictive models of human behavior and
workflow, uh, and, um, and then also, uh, set of techniques around dynamic plan execution,
to be able to take those predictions and use them online to have a, what kind of play the game
with you. It's primarily in, in, in development of, of novel AI models and methods, but everything
we do needs to be tested and evaluated with a system actually working with a person to know whether,
you know, I'm in gelating these capabilities is, is even a value.
Well, one of the things that you said in there that, uh, that I don't want to be as hyperbolic
as blows my mind, but yeah, it is the idea of the robots or the AI kind of predicting what's going
on in their human collaborators heads. That's hard enough for a human. Uh, what does that even mean
for a robot or an AI? Yeah, that's a, that's a, that's a great question. So, um, and you have to,
to pursue that in an academic fashion, it needs to be a well-defined problem and the, like, the,
the human cognitive state that matters in a particular setting and, you know,
particular setting A is very different than the state that matters in a different setting.
Sometimes when I give talks, like, sometimes the question afterwards is, is there a unifying,
you know, model or approach to being able to infer human cognitive state? And I've always just been like,
I don't, you know, I don't, I don't think so, but we have to maybe ask the cognitive scientist,
the neuroscientist. But, uh, I think that's, that's a really, like, for me, that, the fact that
there isn't, you know, want, you know, one well-defined set of states that you're really after
that you're aiming to infer is actually, um, one of, like, the key research questions. And I think
it is one of the key research challenges, which is, um, like, we as humans, we want to be able to
shape the machine learning model or the AI's model of our world or of us, um, much like a human
brings, like, different considerations or a different mental model of interacting with another human
or doing one task versus another task. Um, so, um, like a, a, a key, um, the direction of our
research is, is, uh, first of all, recognizing that you will never succeed at that with an unsupervised
learning approach. There's, I mean, you might get super, super lucky in the latent states you
infer, happen to correspond to the human's mental model of, you know, and, and, you know, what's,
what's particularly important in that particular context, but practically that's not going to happen.
And we, you know, um, theoretically, you know, it's, uh, it's a challenge. So, um, so the question is,
what inputs can you listen from a person that's easy for a person to provide, um, that can help
lock in or, uh, sort of shape the latent space or improve the efficiency of inference for the system
to, uh, learn those, you know, the, um, the, the latent states are their dynamics of, uh,
of the human that, that's interacting with it. Um, and, like, providing labels, like a supervised
approach will work, but then where did those labels come from and how practical is it in every
setting to be able to provide labels of, of relevant human cognitive state? I mean, it's just like
an unreasonable request. Um, so, uh, yeah, what, on the other hand, there are easy, there are
inputs that are easy for people to provide. Like, um, I can, maybe I, maybe it's helpful to ground it
in a, in a concrete example. So, um, say, I have the most simple system. I'm trying to, you know,
develop my own mental model of, and it's just a subway. So just go to the end of line, it turns
around and it comes back. Uh, I'm originally from New Jersey. So in New York, the subway goes
uptown and downtown, and that's the mental model I hold is, uh, someone from New Jersey.
But in Boston, um, the subway becomes exactly the same way. It goes to the end of line, it turns
around and it comes back, but we say the subway goes inbound and outbound, and the switching point
is some arbitrary, like, stop in the center, which is weird. So like, you're going inbound,
as long as you're going to Park Street, and as soon as you pass through Park Street, you're going
outbound. And then when it turns around, it comes back, you're going inbound again until you pass
in Park Street, and then you're going outbound. So it's a, it's like, my mental model of the subway
in New York, in Boston, it's a two-state switching model, but it's, but my mental model is different,
depending on where I'm from. Um, and so, um, you know, we, we employed like non-parametric
Bayesian techniques to try to infer latent states, um, of, you know, some black box systems, say,
like, a human, um, but what are the odds you're going to learn an inbound outbound switching model
for Boston? Um, it's like, you know, um, so, um, but here's the thing, like, you know, maybe I don't
want to label for you inbound outbound, and then New York is uptown, downtown, um, but if I can
just tell you the change point, the switch point. So in New York, the switch point of my mental
model that's a way is that it switches at the end of the line, and then Boston, it switches at Park
Street. Um, that's enough to sort of lock in, and now we're going to hold the same mental model
of how to talk about the behavior of that, that subway. Um, so, um, looking for these ways in
which it's very easy for a person to provide high-level input into a machine learning model,
and then enabling the machine learning model to use those inputs within its computational framework.
Um, is, uh, really, really exciting to me.
There are the, the inputs that you're talking about, uh, and in particular, in this case of the
subway, is this an interaction time input, or is this a training input? Um, so the, the particular
work that I'm, that I'm talking about there, that's done at training time. So, um, the, the idea is,
um, you know, you have, uh, you have your, um, time series data, but rather than work with it in
an unsupervised fashion, if you have some labels, but it's not fully labeled, and then you take
this sort of high-level input from the person in the form of partial policies or partial dynamics,
and then what we do is we formulate those as constraints on a, on a variational inference
process. In that work, it's still done at, um, training time, and I think your question is really
exciting, because, okay, so, you know, or even you have a trained model, and now you're taking,
you're, you, you task to do together, new environment, or, um, uh, how it is you, first of all,
identify the differences between how you want the system to behave in the new environment,
or the model it holds, and be able to adapt it is, um, you know, uh, online is, uh, you know,
a really exciting research direction.
Yeah, I think where the question was coming from was, uh, in the context of training, it,
and, you know, me trying to construct, you know, what this trained model might, uh, be, might
look like, it almost sounded like a feature engineering task that, uh, a, you know, subject matter
expert might apply, um, to map, you know, some raw data of whatever the data is we're looking at
about these trains to, uh, you know, their mental model of the world in order to get the model to,
to kind of attach to that in the training process to use, use loose terminology. Um, you know,
you might, if, you know, you have your, uh, if you're kind of moving away from some center,
you might, the, your feature might be the distance from that center point, or the distance from
your endpoint, or something like that. And I was trying it to, I was curious whether the way that
you were building that knowledge into the system was similar to feature engineering, or was it
a different process altogether? That's interesting. So you're highlighting that, like, um, the,
the person's role can be in, uh, you know, identifying or synthesizing the features that are
meaningful to, you know, their understanding of, um, uh, you know, of, of the functioning of the
system. There's, there's definitely a role there, and also, uh, support, you know, obviously
support there in, um, enabling a person to, um, uh, and enabling the proper elicitation of, of
those features. Um, the, the particular work that I'm referring to, it's not in the feature
engineering, um, space. It's, um, so we, we, um, employ, uh, like a Bayesian graphical model,
a factored model, um, with, uh, with, with particular assumptions made on, um, you know,
on the structure of the model to be able to support efficient inference. So you have, um,
instead of observable states about the world, like, there's things I can see about the subway,
right? I can see its movement, um, uh, there's actions that are, that are taken, um,
that are observable, but then there's, like, there's, there's something in the latent space
that clearly impacts, you know, the behavior, the observable behavior of the system as I want
to be able to predict it and describe it. Um, and I think what's interesting is either of those
two-state switching models can be informative, right? Especially if you want to, if you want to
predict the behavior of the system, um, uh, you know, it's, you just kind of need a two-state
switching model and you'll be able to predict the behavior of the system. Um, but, uh, the, um,
the, the question of, like, which, which, which one of those, um, sometimes it can help in, uh,
in, in predictive power of your model, but it's critically important for being able to
support a person in inspecting and understanding and doing their own projection of the behavior
of this learn model in a, in a different context. Um, so, um, in, in the, in, in the approach,
it's really, uh, it's really a flat, we assume a flat, um, discrete, um, latent state space.
But, uh, the feature engineering question is, is, you know, is, is, is equally important.
Um, like, um, I, I worked for a while on my pilot's license, which, which was really fun,
and then I was a grad student and I ran out of money before I could solo, which is, I thought I
had it all planned out, but you never fly for an hour. You always fly for an hour and 15 minutes,
and it didn't, and I'm working out. And the irony is now that I'm in the Arrow Astro Department,
like, I think there's like, actually, like a benefit, like a professional education benefit,
where I could do flying lessons, and now I don't have time, and I have young kids, so,
but, you know, from my, um, from my days of learning how to fly, like, it's just like, much like a,
like a poor machine learning model. It's like a lot of input, and, you know, like, the spurious
correlations, like, you're like, oh, maybe I should do this when I see that over there, and it's
like the role of the instructor to be like, here's what you attend to. These three features mean
this, like, put your fingers up, you know, do you see the horizon above or before four fingers?
Now, you know, if you're flying straight and level, and, you know, like that, that role of guiding
the meaningful, the meaningful use of, of, you know, the features that the system could attend to,
is, is enormously valuable. Without a causal model, it can be, it can be challenging to ensure
you're learning, a model that'll produce the behavior that you hope it'll produce in a, in a novel
environment. Is this subway example? Is this kind of a toy example for illustration, or was there
an actual problem that you were solving an actual data set? And what exactly is that problem?
Yeah. Yeah. Yeah. The subway, the subway example was the very, the very most simple synthetic
domain in the paper, to like illustrate the idea of, we applied it to be able to, so in, in
in this larger goal of being able to observe a human and develop a predictive model of their
behavior, I think a person holds certain priorities or preferences and how they want to do their work.
And this is something like that is practically a challenge for us into playing robots in industrial
environments. So on an assembly line, you know, someone might say, oh, they're standard work.
There's like a standard way of building this car. There's a standard way of building this plane.
And I believe that, too, I went in and watched how people built things. But there's actually,
like a whole lot of variation in how the plane is assembled or how the car is assembled.
And, you know, different people for different reasons, different biomechanical models will have
different ways of performing the work. And in in something like an automotive, you know, an automotive
factory, a collaborator once told me that like half a second of efficiency could like make a break
the business case for introducing a new technology. So once you're looking at a robot to support a
person and being like half a second faster, like small changes in the ordering of how they install,
you know, something on the dashboard of a car or or their motions, being able to predict where
they'll be in a fine grain way for the robot to assist, that'll make a break the success of that
system in in doing that. So being able to quickly kind of lock in and model an individual person's
priorities or preferences for performing a task improves the predictive power of that model.
And but the key is to be able to do it with relatively little data. So the concrete use case is
the person does their assembly task in a factory and you can collect data of how they
how they do their work. And then you want to when the robot comes in, you want to be able to model
like their intent like where they're going to reach next on the table or where they're going to
walk next in in the cell environment. And different people have different again priorities and
preferences. Work load or sort of fatigue level can impact, you know, your model of your predictions
for the person. But rather than hand specifying a threshold based on, you know, some some measures,
it would it's better to be able to learn that and tune that threshold based on the value it gives
you in order in order to predict where the person will be in space and time after some number of
hours. So that's the those are the those are more concrete use cases of of inferring these latent
states and their and especially their dynamics. Got it got it. In the case of the the shop floor
and the the preferences, it kind of going back to that earlier question I asked about run time
versus training time, you know, training time you've kind of built in the capability to identify
this but the identification is happening at run time, is that right? Yeah, yeah, so um okay, so
this is this is this is this is fun. Okay, so like what do you need for a robot to be able to
collaborate with a person online? You needed to have a model of human behavior, which had and it's
maintaining a belief right over the person's, you know, latent states and a belief over, you know,
what they're going to do next. And then you need a task model and maybe you're lucky enough to have,
you know, a clearly specified task model, but even then you have you have a partially observable
model that the robot is reasoning about in order to be able to collaborate with the human partner.
So that gives you as a Palm D.P. And then you want to be able to solve that Palm D.P. online.
And that and what does it mean to solve it online? It means to give the robot the ability to
choose its actions, whether it's like physical actions or in some cases, even communicative actions
to be able to reason on the uncertainty of what the person will do, but also to be able to
reduce uncertainty based on the old action. And this is quickly interject Palm D.P. is partially
observable Markov decision process. That's it, yep, yep. And the
going back to the beginning of your explanation, do you have one partially observable model,
like you have this task model that is, you know, which there's some noise of what the human might,
how the human might perform that task, or do you have two distinct models, one about the human?
And well, it sounds like one is the human's cognitive state, which you're using to influence
the task model. Yes, yeah. So the human's cognitive state is not directly observable.
Therefore, the model of your human is partially observable, like you can see physically what they're
doing, but there's some elements about what they're doing that you can't directly observe.
And because of that, you know, the robot is, you know, is aiming to reason about, you know,
about how it should be behave or after what actions it should take considering this partially
observable model of human behavior. The task model, in some of our works, we assume it's fully known
and fully observable. But, you know, just as it's really important to lower the barrier,
to quickly and easily learning a human model to work with a person, it's really important in
many contexts to lower the barrier of enabling like a shop floor or a line worker or a domain expert
to teach a robot a task without an applications engineer as the intermediary, or in like in our lab
without, you know, your AI researcher as the intermediary. And when you, when you aim to do that,
you know, you're no longer directly specifying the task and it's often advantageous for the robot
to maintain a belief over the true task specification as well. So it may not definitively know, you know,
the, the specification for how to form the task, but to hedge against its uncertainty, maybe in
some particular context, it really needs to be very, very conservative if it's a safety critical
context. In other contexts, it might have more flexibility without there being a major consequence.
But that's another form of, you know, partial observability that might, that would go into a
Palm DP model that includes both an agent model and a task model.
Yeah, I've come into contact with, well, not physically, but there are like these co-robots or
co-bots that are in use. I think there's a backster robot like there are models for humans interacting
with robots. But I imagine they're fairly brittle when the human does something kind of outside
of the script. Maybe the robot just waits for human to get back in the right position or something
like that to continue the script. And what I'm hearing is that this is a direction for
building robots or robot AI's that can more gracefully interact with humans. Is that the big picture idea?
Yeah, that, that, that is the big picture idea. Like the big, the big picture idea is to figure out
how you can develop and field robots that don't require a human to be a robot to work with it.
So, because that's, that's, that often doesn't work very well. But, you know, there are actually
many of the applications out there today. I mean, humans are very, very adaptable, but it comes
at a cost. So, a cost in many different dimensions. There could be one angle where you look at how you
develop and play intelligent robots to take over or supplant elements of what's being done by a human
today. And there is an alternate approach where you sort of take, take what can easily be done
physically by a robot today, but then recognize it's, it's, it's little pieces of existing manual
work that can be done by a robot. And now you have this integration challenge and then developing AI
to help ease that those integration challenges. And a lot of those integration challenges occur
because humans are the ultimate uncontrollable entity. So, being able to adapt or reason explicitly
over the uncertainty and what a human will do becomes really important. And a key part of doing
that is being able to, for the robot, specify like, what are the true constraints that underlie,
the tasks that needs to be done, and what's acceptable for working with the person.
And then if you have a predictive model, what the person will do, that's all the better.
Because then you can, you know, you can, you can plan over a time price and whether it be less than
a second or tens of seconds. And the, and the interaction can be a little, a little more fluid.
You, the, the, like the backster, the backster is an example of like one of these collaborative robots
that are really game changing because they're safe enough to be right alongside a person. You
don't need a cage. You don't need to remove them physically from the same space as people.
And there's, we've used the universal robot and some of our deployments. We really enjoy the
Franca, Amica robot. But many of the installations of these robots are still working independently of
people in, in production environments. They're not working interdependently. And because of that,
they're limited in, in the value they can provide or the places they can be easily deployed. And so,
a key motivation of our work is, is enabling that interdependence, but rather than forcing the
person to like adhere to the fixed, you know, robot motion and a robot schedule, allowing the robot to
accommodate the natural flexibility or uncertainty that a person brings. And, and now the scenario that
we've talked about thus far kind of creates this picture of a human with their kind of partner robot
working on some tasks together. But does it, to what extent do you envision it extending to
teams of, you know, one robot embedded within a team of humans or multiple robot, one human
with embedded within a team of robots or, you know, a more diverse mixture of humans and robots
kind of working together on, on some kind of tasks? Yeah. The work that I've been describing so far
around the, the techniques for the non-perject-based and techniques for inferring latent state and
their dynamics, that, that is the PhD work of Bayblab on Holcker, who is currently an assistant
professor at, at Rice, at Rice University. And, actually, even though, you know, many of those
studies end up being, you know, one to one, one person, one robot, the, the motivation for a
lot of our work is the recognition that, you know, a lot of our work is done in teams. And we do
need to know more than just the cognitive state or mental state of, you know, one, one partner
in order to succeed. So then that race is really interesting questions about like, what is a team
cognitive state? And how do you model that? And how can the techniques developed for, you know,
modeling like one black box agent, like a person, extend to modeling the sort of, like, the team
concept, the prior work on team, team performance and, and team coordination and communication.
You know, those settings, maybe other than the pilot, co-pilot, you know, narrow scenario, those
are all team settings involving more than two, two people. And there is this notion that team
cognitive state is, is something different. And a lot of thinking and study about like what,
what, what's encompassed in a team cognitive state? And they've, and I have a really exciting
collaboration with Harvard Medical School looking at their operating room and like cardiac
surgeries and the very complex dynamics involved in that. And the, the researchers there, they have
this like super futuristic, you know, surgical simulation environment that they train the residents
and, and grants from NASA to study, you know, human teamwork and failures and human teamwork
from a human factors perspective. And we're in, you know, in discussion currently. And they both
unhulk her in particular has a really exciting recent working paper out of his new lab in,
again, these synthetic scenarios, but in extending these techniques to model some aspects of
team cognitive state, flow, workflow disruptions, sort of miscommunication or signals among a team
that, that have adverse outcomes for, for a team's performance. And again, you have this really
interesting question of training time versus online and what, you know, like, what, what can be predicted
with batch versus online and with what fidelity and how does that flow in to actually training
on you, surgeon to identify the team factors that affect performance because this is done like
an action review or is there a way to bring these techniques actually online in an operating room
and show some aspects of, like, current coding or rating of the team state that can help, you know,
potentially help potentially spur sort of a repair action online that might be useful.
Yeah, it's a great question and work that is newer and early underway.
Yeah, it sounds like there are elements of that that are kind of fundamental psychology
research. Like, you know, what is, you know, what is team cognition separate from the individual
cognition, the cognition of the individuals on the team and what would a representation of that,
you know, even start to look like. Yeah, yeah, the work on that is, so the frameworks that
exist for it are not computational frameworks currently, but we've had a, you know, one of the
great joys and, you know, I think from the work in our lab is talking with, you know, researchers
and cognitive psychology and also cognitive science and sometimes those works really have like the
sort of the basis for that sort of feature, that feature identification problem that you raised.
We've had success in, for example, taking dialogue acts related to,
to shared understanding developed in behavioral psychology and cognitive science literature
and use that to develop an inference system to be able to infer a team's state of understanding
of like a common plan that they're, they're discussing. And so there, there is a lot of opportunity
to leverage the decades of research and insight developed in these other fields and work together
to develop computational models that can advance the automated capability of systems to, to shore up,
you know, weaknesses that human teams sort of naturally have in performing these challenging tasks.
You've also done some work on in the domain of cross-training between humans and robots.
When I hear that, I think a little bit of like imitation learning, but I think your approach is,
you know, in a fairly in a different direction than that. Can you talk a little bit about that work?
Yes, yes. So my lab explored this idea of cross-training or team training applied to
human robot teams. And this goes back to the very start of the lab. And the researcher that,
that did this research is, was my, was a master student in my lab at the time,
Stephen O's Nikolatus. And he's now an assistant professor at USC. And the motivations of that work
was, if you looked, you know, then a technique from, from the literature and learning from
demonstration. It would be like a person showing a task to a robot or walking a robot, you know, through,
through the, the steps of the task. But if your goal was interdependent action between a person
and a robot, like, how do you demonstrate that, right? Like, it's like, you'd be like, okay, robot,
you do this, okay, now I'm going to do this, okay, and then what you're going to do is do that,
and then I'm going to do that. It becomes kind of awkward to think through. And when you look
at, again, the teaming literature, we have, we have many well-honed techniques, like tried and
true techniques for training humans to work interdependently and very challenging tasks. And the,
it's not always possible, but the gold standard is this thing called cross-training. So I do my part
of the task, and you do your part of the task, and we try it out together, but then we switch roles,
then like, I do your part of the task, and you do my part of the task, and we do that for a little
while, and then we switch roles again. And it turns out from, you know, from, from a perspective
of optimizing human learning, for a human, humans learning how to work in teams. This works better
than just about anything else. And you, you can only really do it with small homogenous teams,
like a nurse really can't take a surgeon's role. But when you have, it's kind of small homogenous
teams, like, this is the, this is the way to train your team. I imagine part of the rationals that
you kind of develop empathy and understanding of the other role, and it helps you, like we talked
about before, kind of subtly adapt the way you do your thing so that the needs of the partner.
Exactly, exactly. Like, when I do your job, I suddenly realize, like, you know, the challenges
of you doing your job when I, you know, and then you come back and you adjust the way you do things
for the benefit of the team. Exactly. And so, one of our, one of our goals early on was to think
about, well, how can you optimize human robot team performance in following this type of learning
curve? And so the first, you know, the, the first try at this was to say, well, what if,
what if a human just gives the robot inputs the way, you know, the way a human would experience
inputs when working with another person through cross-training? So in this, in that paradigm,
first the human did their role and the robot did its role. But then the human took the robot's role
in a simulation environment and, like, pretended to be the robot. And then the robot, you know,
did, you know, something that the human would do. The ability to improve the quality of the
human's mental model of the robot was substantial. We were able to show that in experimentation.
But there was also some evidence that the inputs the person was giving in this,
we important this modified reinforcement model where of higher quality than you would get through
you know, standard approaches to reinforcement learning with just positive and negative feedback
or reward. And that's because if a robot does something and you say, like, good robot, like,
there's, there's questions as to what you mean by good robot. Was it good robot for the thing
the robot just did or is it good robot for the thing that you think the robot is about to do next?
Whereas when you take the equivalent of those inputs from the person actually doing the robot's
job, that's more like a direct demonstration of the person for the robot. Was this project
implemented in simulation or is there a physical implementation of this? Yeah, so the cross-training
was implemented in simulation. So the person and robot kind of played together in this game
environment and the robot, you know, in simulation showed the movements it would make. But they weren't
physically working together. It was in a in the 2D simulation environment. And then we had the
person walk over to the lab to the physical robot and do the task with the physical robot after
training under different paradigms. And when the person robot trained via cross-training and simulation,
there were both objective and subjective measures benefits to the physical interaction between
the person and the robot. So that leads to an exciting direction where you can imagine, you know,
a person robot or a person teaching a robot and then learning to collaborate with a robot entirely
in simulation. Off the assembly line without money flying by and then you just walk out onto the
assembly line and like you're good to go, you built your mental model of how to work with this
robot and it's learned its model of how you'll behave and when working with it. Nice. What are some
of the things you're most excited about looking forward? That early work in the lab in cross-training,
we still have the goal of optimizing a human and robots ability to co-learn how to work together.
That early work in cross-training was, you know, a part of the reason we pursued it in that particular
way was because like what does it mean to optimize a human's learning of how to work with a
learning robot? Like I didn't know, but I knew that via certain structured interactions a human
should learn better. And you know, so we implemented cross-training and we're able to show this,
you know, really strong benefit. Much of our work since has been looking at trying to more
directly support and optimize the human's learning via the approach I mentioned earlier about
enabling a person to guide a machine learning's model ability to align its learned model with
the human mental model. So how can we learn a machine learning model that a latent model that
is well aligned with the human mental model directly? Not indirectly via exercising certain
forms of interaction. And the question you raised about how you do this at runtime is really
exciting and really interesting. On the physical side, I have a group in the lab focused on this
because if you say you're on an automotive line which we were and you want to gather your
model of a person doing the task so that you can introduce the robot, you have a chicken and an
egg problem because when you collect your data of the person doing the task on their own
and then you introduce the robot, suddenly the person does their task entirely differently
because the robot is there. It's like out of distribution. So then we're like, okay, so we get
these real associates come through and then we're like, okay, here's what we're going to do.
I'm going to be the robot here. Look, I'm moving like a robot. You do your task like you would with
me as the robot supporting you and then you try to get your data that way and learn the model and
then you introduce the actual robot and then the person does it differently again. And so the ability
of a system to be able to follow or even guide that that learning process of a person to work with
the robot becomes very important, very practically important to successful deployment of the system.
And then we have a line of research also looking at deep models, neural models and
following a similar line of research. Like, what are the inputs a person can give to shape that
latent space so that it better aligns with a human-held mental model that's useful for some
specific task. So moving from the Bayesian graphical model to deep models and, you know,
and enabling that same capability there. But all towards making these systems much more easily
shapeable and adaptable to the needs of someone who's not a machine learning expert.
Well, Julie, thanks so much for joining us and sharing a bit about what you're up to.
Thank you so much for having me.
