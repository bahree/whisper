WEBVTT

00:00.000 --> 00:10.720
Hey, what's up everyone? Welcome to another episode of the Twomo AI podcast. I am your host,

00:10.720 --> 00:17.680
Sam Charrington, and today I'm joined by Sharred Goyle. Sharred is a professor of public policy

00:17.680 --> 00:23.440
at Harvard University. He and I last spoke just over a couple of years ago about his paper,

00:23.440 --> 00:30.480
The Measure and Mismeasure of Fairness, and today we'll be talking about a follow-on work that

00:30.480 --> 00:38.080
recently won the ICML 2022 Outstanding Paper Award, causal conceptions of fairness and their

00:38.080 --> 00:44.640
consequences. I'll say that this paper surfaced some really surprising results for me, and I'm looking

00:44.640 --> 00:50.960
forward to digging into it and sharing a bit of those with you. Sharred, welcome back to the podcast.

00:50.960 --> 00:56.880
Thanks, Sam. Thanks for having me back. Awesome. For those who didn't catch our last conversation,

00:57.440 --> 01:04.000
how about you spend a little bit of time introducing yourself and talking about how you came to work on

01:04.960 --> 01:10.640
understanding machine learning and fairness in their intersections? Yeah, so I've had a pretty

01:11.600 --> 01:19.360
roundabout path towards this. I would say recovering mathematician. I did my PhD in math,

01:19.360 --> 01:28.080
many years ago, and then slowly drifted towards the policy realm over the last 10, 15 years,

01:28.080 --> 01:34.480
and somewhere in between started to think more seriously about discrimination and human

01:34.480 --> 01:41.120
decisions, and particularly in the criminal justice system. Then all of these discussions about

01:41.120 --> 01:46.720
algorithmic fairness, not just discrimination and human decisions, but how do you conceptualize

01:46.720 --> 01:54.080
discrimination, fairness, and machine systems had started coming to light, and that's where

01:54.080 --> 02:00.960
me and my collaborators came into this emerging area and debate about how do we conceptualize

02:00.960 --> 02:05.200
discrimination, how do we measure it, how do we design more equitable systems?

02:07.200 --> 02:15.680
Yeah, I think if I can maybe summarize our last conversation and how it leads into this one,

02:15.680 --> 02:22.240
I think a big theme out of that last paper in conversation was that, hey, we've been spending

02:22.240 --> 02:30.400
a lot of time as a community, as a field, trying to characterize fairness in mathematical terms,

02:30.400 --> 02:43.040
and you found those lacking in a lot of ways. More recently, this idea of causality and causal

02:43.040 --> 02:48.880
approaches has been presented as, in some ways, almost sounded like a silver bullet, like,

02:48.880 --> 02:54.320
hey, we didn't get it before, but we just sprinkle causality dust on everything, and that's

02:54.320 --> 02:59.280
going to fix all of our problems, and I think this paper saying not so fast.

02:59.280 --> 03:09.040
Yeah, causality dust is nice often, I think it definitely brings nuance and often it's important

03:09.040 --> 03:13.840
nuance to add, and so I'm glad that people are thinking about causality in these discussions,

03:14.640 --> 03:20.560
but I think you said it well that there's still this idea that the solution to designing

03:20.560 --> 03:26.480
equitable algorithms is to just come up with a better mathematician of fairness.

03:28.000 --> 03:33.520
That, again, I don't want to make a too strong claim, but I would say what we're finding is at least

03:33.520 --> 03:39.120
for the existing definitions, causal or non-causal, those definitions, when you use them as design

03:39.120 --> 03:45.360
principles to help you create algorithms, those can end up in many cases causing more problems

03:45.360 --> 03:51.200
than they solve, and so I don't think this closes the door on that approach, but it definitely

03:51.200 --> 03:56.560
should give people pause on pursuing this formalization of fairness.

03:56.560 --> 04:04.640
And it sounds like embarking on this endeavor, a big part of the challenge you ran into is

04:04.640 --> 04:11.920
just understanding what it means to apply causality to this idea of fairness in machine learning

04:11.920 --> 04:18.400
or algorithmic fairness. Yeah, I mean, you're exactly right. I mean, this is this kind of subfield

04:18.400 --> 04:24.720
has been progressing at a breakneck pace. There's lots of papers out there, people quite

04:24.720 --> 04:29.520
understandably are using different terms, different ways of describing the same phenomenon or

04:29.520 --> 04:35.920
the same ideas, and so a large part of this project was to go through that literature and make

04:35.920 --> 04:41.040
sense of it for ourselves. And in the process, what we hope to do is help other people who are new

04:41.040 --> 04:47.520
to this area understand it themselves. And at the end of the day, I think we ended up finding a

04:47.520 --> 04:54.960
reasonably concise way of describing this area of causal fairness. And I would say roughly,

04:55.360 --> 05:00.960
there are two classes of definition, there are two classes of approaches to causal fairness,

05:00.960 --> 05:04.880
and that's kind of the shorthand that I'm using to describe this subfield. So the first is

05:06.160 --> 05:14.080
the idea that that we want to eliminate or reduce the effect of legally protected traits,

05:14.080 --> 05:22.480
like race or gender on our decisions. It's a very natural idea that we don't want race, for example,

05:22.480 --> 05:29.760
if we're in some sort of hiring context, we don't want race to affect our decisions. That's

05:29.760 --> 05:34.960
again, one big class of definitions, either directly or indirectly. And so maybe race indirectly

05:34.960 --> 05:40.400
affects our decisions through opportunities that are afforded to folks, and then that changes

05:40.400 --> 05:46.000
the amount of experience that people have. So even if some sense hiring decisions are based

05:46.000 --> 05:50.800
on Leon, quote unquote, experience, race still might enter the equation through these indirect

05:50.800 --> 05:57.040
paths. And so there's a sense that we want to reduce those types of direct or indirect effects.

05:57.040 --> 06:03.360
And in the second, big class of causal fairness definitions aims to reduce what we call counterfactual

06:03.360 --> 06:10.320
disparities. And so in a world where different decisions were made, what would these different,

06:11.600 --> 06:17.360
what would the effect of those decisions be, for example, on error rates? And so that's another

06:17.360 --> 06:22.240
kind of big class of definition. So these are the two ways that we've conceptualized

06:22.240 --> 06:30.400
this subfield of causal fairness. Can you characterize the things that you saw in the literature,

06:30.400 --> 06:36.160
like the ways in which they differed underneath these two broad classes of intent?

06:36.160 --> 06:46.320
Yeah. So even saying what does it mean to limit the effect, the direct or indirect effect of race

06:46.320 --> 06:52.480
on a decision, that statement is very, very hard to make precise. And so there are many reasons for

06:52.480 --> 06:58.640
that. I mean, one is this kind of mantra of no causation without manipulation. This kind of famous

06:58.640 --> 07:04.720
statistical mantra. And the idea there is this kind of point to this fact of, if I can't manipulate

07:04.720 --> 07:11.680
something, what do I even mean when I say the cause of this thing on something else? And so in our

07:11.680 --> 07:18.240
setting, what does it even mean to say the cause of the effect of race on some decision? How do we

07:18.240 --> 07:25.440
manipulate race? And that is not a thing that we can easily manipulate. And so even saying what,

07:25.440 --> 07:30.480
you know, what are we talking about when we're talking about the effect of race? That's kind of a

07:30.480 --> 07:35.760
big open question. Now, even if we were to say, okay, but let's pretend we understand what that

07:35.760 --> 07:39.760
means. And we might pretend we understand what that means by saying there's these kind of famous

07:39.760 --> 07:45.120
audit studies that say, well, the way we're going to manipulate it is by changing somebody's name.

07:45.120 --> 07:51.200
We're going to imagine an employer or someone who's screening CVs. And we can say we're going

07:51.200 --> 07:56.800
to change the name on a somebody's CV. The only thing that the employer gets to see is a CV.

07:56.800 --> 08:01.600
And we're going to change the name to suggest to the employer's mind and the employer's mind

08:01.600 --> 08:06.880
that this person is black or white, for example. And then if we see differences in decisions,

08:06.880 --> 08:14.480
we're going to say that, well, this is what the effective race means. It's really the effect

08:15.200 --> 08:20.480
a name change on the perception of race, which again induces this change in the decision. But

08:20.480 --> 08:26.960
that's at least one way of narrowly describing what we mean there. Okay, so there are ways that we

08:26.960 --> 08:33.680
can understand what is the effect of race, at least if we kind of suspend a little bit of disbelief

08:33.680 --> 08:39.840
and kind of go down the sort of accept these proxies. So there's that. But now what do we mean when we

08:39.840 --> 08:49.200
say, let's limit the indirect effect of race through certain paths. So now this is an even more

08:49.200 --> 08:56.080
nuanced idea of what exactly doesn't mean to say, well, race could have affected where

08:57.040 --> 09:02.720
what opportunities you had in high school, which affected what colleges you went to, which affected,

09:02.720 --> 09:08.160
which test scores you got, which affected all this other stuff. And so now we might want to say

09:08.160 --> 09:15.200
what we don't want race to affect decisions through some of these pathways. But we are going to

09:15.200 --> 09:20.480
allow race to affect decisions through other pathways. So for example, we might allow, again,

09:20.480 --> 09:26.160
let's go back to this hypothetical of college admissions. We might say, well, we're going to allow

09:26.160 --> 09:32.240
for affirmative action. So we're going to allow race to directly be taken account into account

09:32.240 --> 09:40.000
by admissions officers when deciding whom to enact. But we're not going to allow race to affect

09:40.000 --> 09:46.560
decisions through these quote unquote unfair pathways. And now we have to define exactly what those

09:46.560 --> 09:52.000
unfair pathways are. For example, reduced opportunity resources, which means potentially

09:52.000 --> 09:58.400
lower test scores or whatever those other other potential pathways are that we want to block.

10:00.480 --> 10:07.200
And so that was a lot for us. We had to kind of really unpack all of these formal ways of

10:07.200 --> 10:12.080
making those statements precise. And then in our paper, we tried to convey those to people.

10:12.080 --> 10:16.480
We don't want to kind of read through all of that literature, but you know, what the two-page

10:16.480 --> 10:23.520
summary of how you might do this. Are you saying that all of the definitions that are out there

10:24.640 --> 10:32.240
boil down to two definitions or are there two broad classes of ways that people are defining

10:32.240 --> 10:39.600
or treating the topic? Yeah, so I would say they're really two broad classes. There are many,

10:39.600 --> 10:46.880
many, there are dozens of sub definitions in each of these categories. But at the end of the day,

10:46.880 --> 10:53.120
I think the kind of statistical structural properties really depends on which of these two classes

10:53.120 --> 10:58.960
you're talking about. But I would say there really are. If you were to enumerate them,

10:58.960 --> 11:04.960
you would get to dozens pretty quickly. And a part of what you are doing is presenting

11:05.680 --> 11:12.960
mathematical formulations for these definitions or these formulations that were generally present

11:12.960 --> 11:18.640
in the literature that you were reviewing or were they where they, you know, scenarios that were

11:18.640 --> 11:24.080
proposed less rigorously and you're trying to bring some of that rigor to them.

11:24.080 --> 11:29.120
I would say a little bit about that. I would say maybe 80% there were already out there in various,

11:29.120 --> 11:35.520
you know, using various notation and kind of various setups. And so we definitely tried to consolidate

11:35.520 --> 11:41.600
these, put them on a common footing and then present them all coherently. And in the process,

11:41.600 --> 11:47.440
we, I would say we did clarify some of some of these definitions and hopefully make them a

11:47.440 --> 11:54.480
little bit more rigorous. But I'd say by and large, that part of the paper was bringing in these ideas

11:54.480 --> 11:59.280
that other people had had put out there. And so, you know, these could be extended in various ways,

11:59.280 --> 12:03.440
but we were really trying to represent what other people had already said rather than

12:03.440 --> 12:09.520
introduce our new conceptualizations of these ideas. Got it, got it. And were there particularly notable

12:09.520 --> 12:17.360
areas where you kind of added to definitions or clarified or extended definitions?

12:18.000 --> 12:24.160
Yeah. So again, I mean, I think much of this was already out there. And in many of the authors

12:24.160 --> 12:28.640
of the papers that we cite, you know, probably for various reasons, including space, didn't give

12:28.640 --> 12:34.640
all the details. And so we tried to give many of the details. But I would say especially these kind

12:34.640 --> 12:42.240
of past specific notions of fairness where I'm trying to say, here is I want, I want to limit

12:42.240 --> 12:50.320
the effective race along certain causal paths. That is a definition, a style of definition,

12:50.320 --> 12:56.800
which we found to be quite hard to make rigorous. Once you write it all down, it's pretty compact.

12:56.800 --> 13:02.880
You can say it in a page, but actually getting all that machinery and writing it down and saying

13:02.880 --> 13:08.400
this is exactly what we mean by this definition, that certainly took us a fair amount of time.

13:08.400 --> 13:12.960
And while we were figuring out, hopefully, we were able to communicate that to others.

13:12.960 --> 13:23.200
And so there's on the one hand, there's defining these particular problems in terms of

13:23.200 --> 13:33.920
in kind of using the machinery of causality. There's also defining policies that, you know,

13:33.920 --> 13:40.640
that act on those problems using the same causal machinery. Were you also looking to kind of

13:40.640 --> 13:46.240
clarify and define the policies or are you promoting or discussing, you're not really proposing

13:46.240 --> 13:51.120
new policies. This is also kind of a surveying aspect of the paper, is that right?

13:51.120 --> 13:56.720
So I think this is a really good distinction that you're raising. I mean, there's the use of

13:56.720 --> 14:02.640
causality to define very mathematical notions of fairness. And then there's this way of using

14:02.640 --> 14:08.880
causality that is core to how we are thinking of this project, of saying, you a policy maker

14:08.880 --> 14:17.840
need to do something. And now, the way I think about that is you're asking, what is the causal

14:17.840 --> 14:24.480
effect of implementing a certain policy on the world? And so again, using this kind of idea of

14:24.480 --> 14:30.000
admissions, college admissions, we can say, if I were to admit students according to this policy,

14:31.200 --> 14:35.600
what would happen? What would the demographic composition of my class look like?

14:35.600 --> 14:40.400
You know, what would the graduation rate look like? You know, what would the world very broadly

14:40.400 --> 14:51.680
look like? And that, I would say, is there a notion of a causal policy or is it rather a causal

14:51.680 --> 14:57.760
approach to analyzing the impacts of a policy? Yeah, I would say the latter. And so I would say

14:57.760 --> 15:06.000
here, it's what is the causal effect of implementing a policy? And here, this is a very different

15:06.000 --> 15:12.320
style of reasoning than one that we regularly see in computer science. It's very kind of, I would say,

15:12.320 --> 15:19.280
an economic style or a policy style or reasoning where, you know, you have a menu of options,

15:19.280 --> 15:23.280
and we can say we can do A, B, or C, what is the world going to look like? Or what do we think the

15:23.280 --> 15:29.280
world will look like under each of these potential policies that we could implement? And that's very

15:29.280 --> 15:38.880
different from, I would say, the style of causality or the use of causality in creating these

15:38.880 --> 15:45.120
these algorithmic definitions of fairness. And you mentioned this distinction between the way

15:45.920 --> 15:53.760
causalities treated in economic and statistical context and kind of computer science and algorithmic

15:53.760 --> 16:05.040
context. Did your, your review kind of focus on or find more of one or the other? Yeah, I mean,

16:05.040 --> 16:10.720
I would say we, you know, our perspective was was really coming from the policymaker

16:10.720 --> 16:16.880
of saying we would like to design a more equitable world, you know, whatever that means,

16:16.880 --> 16:23.120
it's a hard amorphous concept, but that is what we are going for. And so the person a question

16:23.120 --> 16:30.240
to a policymaker to a set of stakeholders is what do we do? Now, that is our kind of overarching

16:30.240 --> 16:36.240
perspective when you're coming to this problem. I would say the use of causality in the formal

16:36.240 --> 16:43.200
definitions that we're reviewing are not asking about the consequences of implementing certain things.

16:43.200 --> 16:53.440
They're really saying we want to procedurally reduce the impact of race on certain decisions.

16:53.440 --> 16:58.560
You know, that's kind of roughly what that style of computer science, algorithmic fairness

16:58.560 --> 17:04.240
is doing at least one broad class of things that's happening there. And the key distinction is that

17:04.240 --> 17:10.160
one is procedural. This is the computer science notion of it. And, you know, if you want to be

17:10.160 --> 17:14.640
philosophical about this, you know, sometimes it's called a dantological perspective of saying these

17:14.640 --> 17:19.920
are rules. There are certain things that we think of as being good or bad. And we might say that

17:19.920 --> 17:28.800
it is it is bad to allow race to affect decisions through certain pathways. And then there's this

17:28.800 --> 17:37.680
alternative consequentialist perspective of saying the procedure is really only important to

17:37.680 --> 17:43.760
the extent that it brings about good outcomes. And that's where we were really coming from. And

17:43.760 --> 17:48.800
we're saying we want to create this. We care about certain outcomes being equitable.

17:49.920 --> 17:56.560
And what we found though is quite surprising is that by constraining yourself procedurally

17:57.440 --> 18:04.000
to certain kind of definitions of fairness, even though on the surface, they might seem quite

18:04.000 --> 18:12.160
reasonable. In fact, those constraints are so strong that almost always in a kind of rigorous sense,

18:12.800 --> 18:21.280
they would lead to bad outcomes. And so there's this this dark tension between process fairness

18:21.280 --> 18:26.720
and outcome fairness. And in general, I think, you know, in the criminal legal system,

18:26.720 --> 18:31.360
this is something that often comes up. And I think there's a lot of nuance and whether or not you

18:31.360 --> 18:38.880
want to favor a quote unquote fair process or a quote unquote fair outcomes. Here, the process

18:38.880 --> 18:47.600
in my mind, at least, was was an unintuitive enough that I would prefer having better outcomes

18:47.600 --> 18:52.240
rather than saying, let's equalize error rates or something like that. So where it's unclear to

18:52.240 --> 18:58.400
me, why do you want that thing? Like, you know, presumably you want that thing because precisely,

18:58.400 --> 19:02.640
it leads to these better outcomes. And to the extent that it doesn't lead to these better outcomes,

19:02.640 --> 19:08.560
maybe we should revisit those definitions, those rules that we bound ourselves to at the beginning.

19:08.560 --> 19:15.680
Yeah, and this was a big result in the paper. Can you talk a little bit more about how you

19:15.680 --> 19:24.560
demonstrated that? And it sounds like you did so both the experimentation as well as kind of

19:24.560 --> 19:31.840
mathematically that, you know, these these process definitions were insufficient relative to

19:32.720 --> 19:37.840
the outcome definitions. So the way we did that is that that we kind of use this idea of

19:38.880 --> 19:45.040
pre-dodominance, which is popular in the economics literature. And roughly, this is saying, you

19:45.040 --> 19:52.240
imagine you have a set of stakeholders. And you know, to ground everything, let's let's go back

19:52.240 --> 19:56.640
to our kind of college admissions example. All of what we wrote in the paper is quite general,

19:56.640 --> 20:01.040
but I think it's helpful to keep things grounded with this one example. And so imagine we have an

20:01.040 --> 20:08.160
admissions committee and they all have different preferences, but they agree on two things. So they

20:08.160 --> 20:14.560
agree that they want to have, they want to admit students who are likely to succeed in their

20:14.560 --> 20:19.120
program. And so you might operationalize this by saying that you that they that they have a

20:19.120 --> 20:24.080
preference for people graduate. That's like one way to to say that that we can operationalize

20:24.080 --> 20:30.960
success. And the second thing that they agree on to some extent is a preference for diversity.

20:31.840 --> 20:35.680
And, but they disagree on the extent to which they're willing to trade these two things off.

20:36.720 --> 20:42.640
And so some people are saying, okay, it's like we really want to prioritize, you know, one over

20:42.640 --> 20:49.200
the other and other people say, no, let's not really, let's, you know, kind of we really mostly care

20:49.200 --> 20:55.200
about one and not the other. And so we have this kind of, I would say realistic scenario of stakeholders

20:55.200 --> 21:00.480
who have differing opinions, but they're kind of reasonable in the sense that their preferences aren't

21:00.480 --> 21:05.760
all over the place. They kind of have rough alignment on these critical dimensions. And so now,

21:05.760 --> 21:13.840
what we say is that I am going to in some sense randomly draw a state of the world for you.

21:13.840 --> 21:19.040
And so what does that mean? I'm going to kind of, you know, tell you the relationship between,

21:19.040 --> 21:24.480
you know, demographic groups and all these covariates that are observable and everything that

21:24.480 --> 21:28.640
admissions committees can make their decisions on. And there's kind of this infinite

21:28.640 --> 21:33.600
dimensional space of distributions that I can consider. And we're saying just pick some of

21:33.600 --> 21:41.600
these things randomly. And our result in the nutshell is saying almost always, no matter what

21:42.320 --> 21:47.920
state of the world you randomly draw, if you were to limit yourself to these causal definitions

21:47.920 --> 21:54.320
of fairness, the policies that will result, the only policies that satisfy your causal definition

21:54.320 --> 21:59.440
of fairness will be perito-dominated, meaning that everybody on the admissions committee

21:59.440 --> 22:06.240
will think it's a bad idea. And so what does that mean to be concrete? So if I give you any policy

22:06.800 --> 22:12.960
that satisfies your causal definition of fairness, I can come up with another policy that has both

22:13.520 --> 22:21.040
greater student body diversity and higher admissions or graduation rates. And because it has both

22:21.040 --> 22:26.240
of these two things, everybody on the admissions committee will think that it's better than the

22:26.240 --> 22:31.600
policy that you generated through this causal fairness mechanism. With that result in mind,

22:31.600 --> 22:42.560
you're kind of suggesting here that these causal policies traded in the context of causal

22:42.560 --> 22:51.040
definitions are suboptimal broadly. But we've talked about all of the work that's gone into these

22:51.040 --> 23:01.280
causal policies. Certainly, those folks thought they were onto something. Like, is it, what is your

23:01.280 --> 23:09.280
results, what light does your result shed on all the results that you looked at when you were

23:10.800 --> 23:16.880
kind of coming up with your definitions? I completely agree that there's been a lot of effort,

23:16.880 --> 23:24.160
probably thousands of hours, since thousands of hours of effort collectively going into this

23:24.160 --> 23:30.640
subfield of causal fairness. And I don't think that was wasted effort in any way. I mean, I think

23:30.640 --> 23:39.200
these are this is an emerging field. And we have to learn and understand what we're doing. That's

23:39.200 --> 23:45.680
a messy process. And so in some sense, not at all surprising that our first stab at this is going

23:45.680 --> 23:55.680
to fall short. That being said, I do think, and again, this is my own view, is that our results

23:56.640 --> 24:06.640
really cast them down on the value of this approach going forward for informing policy decisions.

24:06.640 --> 24:12.000
I don't think that that's not a mathematical statement. It's not a formal statement. And I

24:12.000 --> 24:16.160
would encourage other people who are interested in this area, you know, to continue pursuing it

24:16.160 --> 24:20.640
and to read carefully the literature, to read carefully what we've written and make up their

24:20.640 --> 24:29.040
own mind. But for me personally, having gone through that process, I do think there's a real risk

24:29.680 --> 24:40.560
of causing harm and particularly causing harm to groups that, ostensibly, we are trying to not

24:40.560 --> 24:48.720
harm by using these definitions. If we were to go down this path of mathematicizing fairness in

24:48.720 --> 24:56.880
these ways, you mentioned in the paper that you show the Pareto dominance both analytically

24:56.880 --> 25:04.480
and empirically, is the are you empirical results based on simulation or yes, this is simulation.

25:04.480 --> 25:12.720
So we mostly use this as is an intuition pump. And so this is how we started the mouth turns out

25:12.720 --> 25:19.760
to be hard. And so this is the way that we started building a tuition for the problem is let

25:19.760 --> 25:26.800
simulate things and we hope that that approach is also illuminating to other people who are coming

25:26.800 --> 25:31.520
to this field. Can you talk a little bit more about how you built out the simulations?

25:31.520 --> 25:39.280
Yeah, and so it's pretty straightforward. We basically formalized the setting that we've

25:39.280 --> 25:44.640
been talking about about college admissions. And it's very much mirror discussion. So we just

25:44.640 --> 25:53.440
imagine there's some population of people. We imagine that there are various things that

25:53.440 --> 25:59.840
admissions can be considered. For example, test scores or GPA and race. And we imagine some

25:59.840 --> 26:05.360
relationship between these things. And then once we write this thing down, and again, I don't

26:06.560 --> 26:12.720
even pretend that this is realistic in a literal sense, but I think it captures some of the structure

26:12.720 --> 26:18.640
of these problems in the real world. But once we write it down, then we can just kind of draw

26:20.080 --> 26:25.280
outcomes from this generative model. We can say, given that you observe these outcomes and you

26:25.280 --> 26:30.080
implement this policy that is derived through these causal fairness definitions, what would you

26:30.080 --> 26:35.360
get? How would that compare to things that you could do alternatively? And like I said, you know,

26:35.360 --> 26:41.040
if you, anything you do, if you restrict yourself to the class of policies that are attainable

26:41.760 --> 26:48.720
through a causal fairness definition, you are guaranteed in a mathematical sense to have

26:48.720 --> 26:53.920
lower student body diversity and lower graduation rates than what you can do if you don't restrict

26:53.920 --> 27:01.760
yourself to that class of policies, which Toss was a kind of surprisingly strong result.

27:01.760 --> 27:08.960
Yeah, yeah. I mentioned when we were chatting earlier that it was very surprising for me reading

27:08.960 --> 27:15.360
the abstract you used to seeing results that are talking about how something is better.

27:15.360 --> 27:24.160
And I had to read Pareto dominance, you know, though that the causally defined results were

27:24.160 --> 27:30.400
Pareto dominated by other policies several times before I realized what you were saying. And hey,

27:30.400 --> 27:37.760
these are always worse. They're always worse. Yeah, they're always worse. And even formalizing,

27:37.760 --> 27:43.280
what does that mean that these are technically almost always worse, like going back to the

27:43.280 --> 27:47.840
statement I made before it's like if you were to draw these policy, draw the state of the world

27:47.840 --> 27:53.680
at random, you will almost always end up with something that is Pareto dominated, meaning it's

27:53.680 --> 27:59.680
actually strongly pre-dodaminated, meaning everybody on our admissions committee and that hypothetical

27:59.680 --> 28:06.000
would disfavor that policy. So what does that even mean? And this is where we had to kind of dig

28:06.000 --> 28:13.040
into the decades old mathematical literature on formalizing what does that mean almost every

28:13.040 --> 28:19.360
policy. And so that kind of goes down this whole rabbit hole, which again, to us at a technical

28:19.360 --> 28:23.920
level was super interesting because from our previous work, we had this intuition that often

28:23.920 --> 28:30.080
these things are bad. But we didn't quite have that language to say, well, what does that mean

28:30.080 --> 28:35.600
often? You know, what does that mean typically? What does that mean bad? And here the insight was,

28:35.600 --> 28:40.560
I mean, putting together this idea of Pareto dominance, you know, coming from the economics literature

28:40.560 --> 28:46.240
and this idea of almost everywhere on these kind of infinite dimensional spaces, coming for

28:46.240 --> 28:52.080
the mathematics literature, and kind of putting all these pieces together, we, you know, found a

28:52.080 --> 28:57.680
this, you know, surprisingly strong result. I'm trying to think back to the previous results,

28:57.680 --> 29:06.160
and if you could have said something similar, or did you need the constraint of causality to

29:06.160 --> 29:10.480
provide a mathematical framework for you to even get this far? Yeah, that's a really good

29:10.480 --> 29:19.840
question. And so I would say yes and no. So yes, much of what we, many of the causal, money

29:19.840 --> 29:24.480
of the non-causal definitions that are out there are subject to the same critique that we're

29:24.480 --> 29:28.800
offering in our current paper. And so at the time, you know, a few years ago when we were writing

29:28.800 --> 29:35.520
those older papers, we just didn't understand what we now understand. And so in that sense,

29:35.520 --> 29:42.960
these results translate. But in another sense, there is something about these causal definitions

29:42.960 --> 29:50.160
of fairness, which is does not apply to the non-causal definitions. And so it turns out that these

29:50.160 --> 29:58.240
causal definitions of fairness, at least many of them, have such strong requirements on what they

29:58.240 --> 30:06.080
allow that the only policies which satisfy them end up being kind of random policies that say that

30:06.800 --> 30:12.640
the only policy that will satisfy, you know, one of these particular definitions of causal fairness

30:12.640 --> 30:20.000
has to admit everybody with equal probability, regardless of qualification, regardless of

30:20.000 --> 30:26.480
demographics, everybody's admitted with the exact same probability. And that result was extremely

30:26.480 --> 30:33.760
surprising to us. And it really stems from the fact that these definitions are implicitly

30:33.760 --> 30:39.200
imposing so many constraints on your policy that you kind of have no way out because this is

30:39.200 --> 30:44.720
the only thing left to do. And as far as I know, I don't know of any non-causal definitions

30:44.720 --> 30:49.280
that impose so many restrictions that this is the only thing that you end up with.

30:49.280 --> 30:55.200
Okay, so that kind of gets at the second question I had, which was kind of some intuition around

30:55.200 --> 31:05.680
why causality or causal policies perform so poorly. And it's this idea of restriction,

31:05.680 --> 31:13.440
it sounds like, can you kind of speak more directly to this? Yeah, that's exactly right,

31:13.440 --> 31:18.720
that's exactly right, that this is really about constraints. And so every time you add a constraint,

31:18.720 --> 31:25.600
you're restricting the space of policies that you can consider. And it turns out that especially

31:25.600 --> 31:31.040
for these, you know, past specific notions of fairness, or any of these things where I say,

31:31.040 --> 31:36.880
if I change your race, counterfactually, I have to see exactly the same outcomes in my decision.

31:37.440 --> 31:45.280
It turns out if you unpack that definition, that really implies a huge number of constraints.

31:45.280 --> 31:50.080
It sounds like one constraint, but really it's many, many, many constraints that are tied

31:50.080 --> 31:57.280
into that one notion. And so when you do that and you kind of, you know, internalize that bit any

31:57.280 --> 32:06.560
further, like what is it that is what's the mechanism of kind of that constraint mapping to many

32:06.560 --> 32:11.200
other things? Yeah, so one way to think about it, and again, this gets a little bit technical

32:11.200 --> 32:18.480
and pictures is worth a thousand words here, but I'll try to do it through audio is that you

32:18.480 --> 32:26.000
can imagine a graph, a DAG, a causal DAG that says, well, we have various features, attributes,

32:26.000 --> 32:31.600
and we're trying to say, you know, we're drawing arrows between everything that has an effect

32:31.600 --> 32:39.040
on everything else. And now we have, you know, race is one of these nodes in this graph and race

32:39.040 --> 32:46.080
affects all these other things. And now if you think about all these kind of pads that race can

32:46.080 --> 32:51.680
go through to affect your decisions, if you kind of look at that graph, it turns out that this

32:51.680 --> 32:58.160
definition is saying that your decisions can only depend on things that are not downstream of race.

32:59.520 --> 33:05.440
But mostly when people write down these types of DAGs, everything is downstream of race.

33:05.440 --> 33:10.960
And so you have to say, well, you know, it's like where you went to school and then, you know,

33:10.960 --> 33:16.160
your test scores because of where you went to school. And then all these other things like the

33:16.160 --> 33:22.400
neighborhood you live in, all of these things are somehow tied to race. At least this is how

33:22.400 --> 33:30.480
people conceptualize it in many of these papers. And once you write that down, that is implicitly

33:30.480 --> 33:39.040
constraining your policy in a way that leaves no other option. But you have to make decisions that

33:39.040 --> 33:44.800
don't depend on that race or anything downstream of race, which basically doesn't leave you with much.

33:44.800 --> 33:53.600
Is there a notion that makes sense of kind of rating or parameterizing,

33:53.600 --> 34:01.200
now less parameterizing than rating or categorizing, I guess, these causal policies based on

34:01.200 --> 34:07.840
their degree of constraint? Yeah. So definitely, this one that I've been describing right now,

34:07.840 --> 34:15.520
I would say it's the most extreme and that literally the only thing you can do is make decisions

34:15.520 --> 34:20.000
purely randomly, no differentiation. And so I would say that's the most extreme. There's more or

34:20.000 --> 34:27.200
or less unique policy that satisfies this causal definition of fairness. For everything else that we

34:27.200 --> 34:32.400
looked at, there's a little bit of wiggle room. But all of that wiggle room, all of those policies

34:32.400 --> 34:38.960
that are allowable are still pre-dominated. And so they're still all, you know, quote-unquote bad

34:38.960 --> 34:46.080
in some sense, but they're, I would say not as bad, they're not as, you know, there's more than one

34:46.080 --> 34:54.000
choice. And you kind of touched on this earlier in saying that, you know, this work isn't necessarily

34:55.040 --> 35:02.400
meant to, you know, say that there's no value in this research, but do you see, does this work

35:03.600 --> 35:10.560
point you in a direct, you know, if you're interested in causal policies, do you see, you know,

35:10.560 --> 35:16.640
something in this work that says, well, maybe we should be looking here, or is it more, you know,

35:16.640 --> 35:20.080
is it not really providing you that that shining light, so to speak?

35:20.080 --> 35:23.920
No, I mean, I think there are at least a couple things that are that are important at first,

35:23.920 --> 35:30.160
just to highlight the introduction of causality into this conversation is an important one, I think.

35:30.720 --> 35:38.480
And it's, it was too much about, I would say pure prediction before, without thinking about

35:38.480 --> 35:44.720
the effects of interventions. And so I think that, that idea at a very high level is an important one.

35:44.720 --> 35:49.600
Now, the other thing that I think is at least in some of this literature, but, you know,

35:49.600 --> 35:54.800
perhaps not highlighted the extent that I would like it to be, is, is let's think about outcomes.

35:55.440 --> 36:00.160
And so there are these process oriented views of fairness. And in some context, those might make

36:00.160 --> 36:07.200
a lot of sense, but I think it's important to understand what we're, we're leaving on the table,

36:07.200 --> 36:14.640
when we think in terms of pure process, as opposed to at least acknowledging the effects of policies

36:15.280 --> 36:21.760
generated by using these design principles that are being put out there. And at the end, I think

36:21.760 --> 36:26.560
a reason we're going to disagree about whether or not one should focus on processor outcomes,

36:26.560 --> 36:30.000
but what I would personally like to see is more discussion, more engagement,

36:30.880 --> 36:35.040
around these two different ways of thinking about these problems. But that is something I think

36:35.040 --> 36:39.600
this literature at least is alluding to, even if it's also directly engaged with.

36:39.600 --> 36:47.280
Is there some idea in which if you start with outcomes, then there is some role of causality as

36:47.280 --> 36:57.200
a tool to taking from those outcomes to policies that aren't limited by the constraints that we've

36:57.200 --> 37:02.160
talked about and are dominated in the way that we talked about. Definitely. I think it's very hard,

37:02.160 --> 37:09.200
but at least in theory, you can do this. And so here, the idea is if we agree, and this is a big if,

37:09.200 --> 37:15.280
if we agree what we want the world to look like, and again, different people are going to disagree

37:15.280 --> 37:21.120
about that. But let's assume that we have some consensus on where we want to get to.

37:21.760 --> 37:27.360
Then you can say, well, we could implement all these different interventions. How do we get to

37:27.360 --> 37:33.120
this place that we agree we want to get to? Which one of these interventions is going to get us

37:33.120 --> 37:38.800
closest to that place or fastest to that place? And that's a causal question. That's purely a

37:38.800 --> 37:46.000
causal question. It's a hard question, but I don't think it's impossible in all scenarios to

37:46.000 --> 37:54.080
answer. And some of our other work is actually directly trying to take that perspective to design

37:54.080 --> 38:00.320
policy interventions. What's next for you in this line of research? Yeah, so there's a lot of

38:00.320 --> 38:05.920
stuff kind of that we're actively pushing on right now. And the one thing that I'm quite excited

38:05.920 --> 38:13.120
about is designing interventions that can get us to a better place. And so this is a theme of

38:13.120 --> 38:18.560
much of my work. And so to give one specific example, we are working with a public defender

38:18.560 --> 38:24.800
in California, the Santa Clara County Public Defender office to help their clients make it to court.

38:24.800 --> 38:30.320
And so why is that? Well, we, when you miss court, all sorts of bad things happen. And so you can

38:30.320 --> 38:34.960
know a bench ward is often issued. And now if you're pulled over for a minor traffic violation,

38:34.960 --> 38:42.160
you could get arrested and thrown into jail. And so obviously, I would say the right solution here

38:42.160 --> 38:47.360
is to not have those outcomes given the, you know, if you miss court, in my mind, I think of that

38:47.360 --> 38:51.840
as like a tantamount to missing a doctor's appointment. It's not like people are trying to flee

38:51.840 --> 38:56.320
the jurisdiction. They have complicated lives. They, they forget about their appointments, all

38:56.320 --> 39:01.520
sorts of things happen. And I don't think it's kind of, it's, it's, it's not humane. It's not just

39:01.520 --> 39:06.960
to lock somebody up for that. That's a harder policy discussion. And so the way that we're addressing

39:06.960 --> 39:11.040
it is we're saying, well, given that this is the way the system works, we're not happy with it,

39:11.040 --> 39:16.400
but given the way that this is the way the system works, let's try to help people make it to court.

39:16.400 --> 39:22.880
And so we're doing that through two different means where we're working to send out text message

39:22.880 --> 39:30.800
reminders so people can help them plan. And we're also providing free door to door ride share

39:30.800 --> 39:37.680
from someone's home to court and back. And that is now, now the question is, how do you,

39:38.880 --> 39:43.840
how do you allocate this limited benefit of ride? So unfortunately, we can't offer the ride

39:43.840 --> 39:49.600
to everybody. And so who do we offer the rides to? And this is fundamentally a causal question.

39:49.600 --> 39:54.240
It's like there are various policies that we can implement. Which ones of these policies

39:54.240 --> 40:00.480
are we happier with? Which ones are going to get us closer to a world that we want to be in?

40:00.480 --> 40:09.040
Awesome. Awesome. Well, Sharad, congrats again on the outstanding paper award. That's huge.

40:09.040 --> 40:14.720
And it was great catching up and learning a bit about your latest work. Thanks. It's always fun chatting.

