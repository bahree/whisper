1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:22,400
I'm your host Sam Charrington.

3
00:00:22,400 --> 00:00:25,280
Hey what's up everyone, producer Amari here.

4
00:00:25,280 --> 00:00:29,600
Before we get to the show, I want to remind you that our study group for the Fast AI

5
00:00:29,600 --> 00:00:35,560
course, a Code First Introduction to Natural Language Processing, begins this Saturday,

6
00:00:35,560 --> 00:00:37,800
December 14th.

7
00:00:37,800 --> 00:00:43,040
This course will cover NOP applications like Topic Modeling, Classification, Language

8
00:00:43,040 --> 00:00:45,560
Modeling, and Translation.

9
00:00:45,560 --> 00:00:51,000
To join the study group and the broader Twimal Online community, head over to TwimalAI.com

10
00:00:51,000 --> 00:00:52,840
slash community.

11
00:00:52,840 --> 00:00:57,040
Submitting that form will trigger an invitation to our Slack, and once you're there, join

12
00:00:57,040 --> 00:00:59,680
the appropriate study group channel.

13
00:00:59,680 --> 00:01:06,160
Hope to see you online.

14
00:01:06,160 --> 00:01:08,800
Alright everyone, I am on the line with Stephen Merritti.

15
00:01:08,800 --> 00:01:13,240
Stephen is an NLP and deep learning researcher.

16
00:01:13,240 --> 00:01:16,600
Working on a start-up, I'll let him mention that.

17
00:01:16,600 --> 00:01:22,600
Stephen's a long time friend of the show and was a participant in our conversation back.

18
00:01:22,600 --> 00:01:31,800
One exactly was that, it was TwimalTalk 234, but that was all around the OpenAI GP2 release

19
00:01:31,800 --> 00:01:37,120
and we had that great panel discussion about the controversy that surrounded it.

20
00:01:37,120 --> 00:01:43,120
But now Stephen's back for a standalone interview, particularly on the topic of his recent

21
00:01:43,120 --> 00:01:48,160
paper, single-headed attention, RNN, stop thinking with your head.

22
00:01:48,160 --> 00:01:50,320
Stephen, welcome to the TwimalAI podcast.

23
00:01:50,320 --> 00:01:57,320
Yeah, I'm looking forward to diving into this, but before we do, you didn't have an opportunity

24
00:01:57,320 --> 00:02:01,120
to give us a full background when we did that panel.

25
00:02:01,120 --> 00:02:06,400
So why don't you share a little bit about how you got to working in this area?

26
00:02:06,400 --> 00:02:12,600
Absolutely, I was lucky to actually start working in natural language processing in late

27
00:02:12,600 --> 00:02:13,600
high school.

28
00:02:13,600 --> 00:02:18,200
Brilliant professor by the name of Dr. James Kern helps run a summer camp for high school

29
00:02:18,200 --> 00:02:24,520
kids, and there he introduced, not just programming, but also natural language processing.

30
00:02:24,520 --> 00:02:29,280
And back in those days, well before deep learning, it was far more of the traditional methods.

31
00:02:29,280 --> 00:02:34,680
But I got obsessed with it from then, so basically the second I was at university, I was interested

32
00:02:34,680 --> 00:02:40,760
in working in the field and published some interesting papers primarily on maximum entropy

33
00:02:40,760 --> 00:02:47,600
models and on combinatorial, categorial grammar pausing if any of the readers' viewers

34
00:02:47,600 --> 00:02:48,600
are aware.

35
00:02:48,600 --> 00:02:57,920
But since then, yeah, it's been a bit of a well-end, so when I came over to the US, I went

36
00:02:57,920 --> 00:03:03,840
to Harvard University for masters, and there I was really obsessed with large data sets,

37
00:03:03,840 --> 00:03:10,160
thinking that was the right way to kind of machine intelligence or artificial intelligence.

38
00:03:10,160 --> 00:03:16,760
And since I left then, I worked at a nonprofit called CommonCroll, who they basically

39
00:03:16,760 --> 00:03:21,200
download billions of pages from the internet every month to turn into a data set, so think

40
00:03:21,200 --> 00:03:26,280
of it as your kind of own small version of Google's data store.

41
00:03:26,280 --> 00:03:30,920
Then I worked at Metamind, which was eventually acquired by Salesforce and became Salesforce

42
00:03:30,920 --> 00:03:32,040
Research.

43
00:03:32,040 --> 00:03:36,800
And that was where I got most interested in language models.

44
00:03:36,800 --> 00:03:43,720
And since then, I've left Salesforce Research and started my own startup called DDX times.

45
00:03:43,720 --> 00:03:49,560
And the idea with that is that I think that language models are amazing tools, just kind

46
00:03:49,560 --> 00:03:55,120
of new ways of handling a lot of the computation and linguistic costs that we think of every

47
00:03:55,120 --> 00:03:56,120
day.

48
00:03:56,120 --> 00:04:00,600
And I'm really interested in kind of unlocking language as a data set, you know, beyond

49
00:04:00,600 --> 00:04:04,760
the kind of surface level work that we traditionally do.

50
00:04:04,760 --> 00:04:09,720
So the tagline of the company is, you know, language is humanity's longest running program.

51
00:04:09,720 --> 00:04:13,400
And it's kind of accumulated all of this craft and all of this information over time and

52
00:04:13,400 --> 00:04:17,600
hopefully language models will be the way to start unpackaging that and helping humans

53
00:04:17,600 --> 00:04:22,280
actually look at millions of web pages or, you know, hundreds of years of documents

54
00:04:22,280 --> 00:04:24,880
without having to read through it all themselves.

55
00:04:24,880 --> 00:04:29,600
Well, since language models will be playing such a prominent role in this discussion, maybe

56
00:04:29,600 --> 00:04:34,560
we should start from the top and have you share for those that aren't familiar with

57
00:04:34,560 --> 00:04:38,640
the terminology, what a language model specifically is trying to do.

58
00:04:38,640 --> 00:04:44,280
Absolutely, so the kind of most likely example of language models that everyone has probably

59
00:04:44,280 --> 00:04:48,640
run into is the order suggests on your mobile phone.

60
00:04:48,640 --> 00:04:53,400
So as you're typing some phrases, kind of highly likely and instead of you're hitting

61
00:04:53,400 --> 00:04:59,320
each of the keys on your mobile phone, it'll just pop up a word and suggest that instead.

62
00:04:59,320 --> 00:05:03,400
So that's kind of the example of language model that most people see.

63
00:05:03,400 --> 00:05:07,840
But it turns out if you keep scaling these language models up, if you use, you know,

64
00:05:07,840 --> 00:05:12,480
a far more powerful device than your phone, just trying to predict the next token, whether

65
00:05:12,480 --> 00:05:19,760
that is a character, a word, or a piece of a word, then the machine learning model that

66
00:05:19,760 --> 00:05:25,320
you produce there ends up encapsulating a lot of kind of interesting pieces of knowledge

67
00:05:25,320 --> 00:05:31,680
about not just language, but kind of the structure and uses of language itself.

68
00:05:31,680 --> 00:05:38,440
So there have been amazing examples of just predicting the next character on language modeling

69
00:05:38,440 --> 00:05:40,640
data sets.

70
00:05:40,640 --> 00:05:46,560
One example is OpenAI's sentiment neuron, where they, without giving the machine learning

71
00:05:46,560 --> 00:05:52,440
model any extra information, they gave it a number of Amazon review reviews and they

72
00:05:52,440 --> 00:05:54,360
asked it to predict the next character.

73
00:05:54,360 --> 00:05:57,320
So imagine you're just sitting at the keyboard trying to guess the next character that someone

74
00:05:57,320 --> 00:05:58,320
is typing.

75
00:05:58,320 --> 00:06:00,560
That's all this machine learning model is doing.

76
00:06:00,560 --> 00:06:05,000
But from the knowledge that's distilled in that, it's able to accurately predict whether

77
00:06:05,000 --> 00:06:08,480
it's a positive or a negative review.

78
00:06:08,480 --> 00:06:13,480
And in fact, when you're generating, you can basically also in a language model, continue

79
00:06:13,480 --> 00:06:16,080
generating as if it's a positive or a negative review.

80
00:06:16,080 --> 00:06:19,720
So if I was reviewing my headphones, I would say, well, make it positive.

81
00:06:19,720 --> 00:06:24,400
And like the headphones have a beautiful sound, crisp, and a very long battery life to

82
00:06:24,400 --> 00:06:31,720
draw, but then you could flip it negative and language models, say, the headphones have

83
00:06:31,720 --> 00:06:35,520
static and are constantly annoying and slight ringing in my ears.

84
00:06:35,520 --> 00:06:39,840
So these language models, as you keep scaling them up, seem to be capturing more and more

85
00:06:39,840 --> 00:06:46,120
at the very least surface knowledge of text, if not kind of deeper, interesting connections

86
00:06:46,120 --> 00:06:48,960
that humans might not be fully aware of.

87
00:06:48,960 --> 00:06:55,920
And so this example you gave, and examples like at GPT-2 and others where you're predicting

88
00:06:55,920 --> 00:07:02,200
paragraphs and paragraphs, it's all coming from this fundamental ability to predict the

89
00:07:02,200 --> 00:07:03,520
next character.

90
00:07:03,520 --> 00:07:04,520
Yes.

91
00:07:04,520 --> 00:07:08,760
And in those cases, it's usually the next token where that might be a word piece, so you

92
00:07:08,760 --> 00:07:18,000
might break a word down into, let's say, running because running or so on, but exactly

93
00:07:18,000 --> 00:07:20,720
that point is just predicting this next token.

94
00:07:20,720 --> 00:07:25,240
And you can, of course, make the task more complex, but it turns out even just providing

95
00:07:25,240 --> 00:07:30,200
a huge chunk of text without any additional information about, say, the domain of the

96
00:07:30,200 --> 00:07:36,760
text or where the text comes from, even the language of the text, you're able to stop

97
00:07:36,760 --> 00:07:39,320
bling out these interesting details.

98
00:07:39,320 --> 00:07:46,160
And so this paper that you just published, the SA R&N paper, tell us a little bit about

99
00:07:46,160 --> 00:07:49,480
the motivation for the paper.

100
00:07:49,480 --> 00:07:53,040
Yeah, there are two main motivations for the paper.

101
00:07:53,040 --> 00:08:00,600
One is that the language modeling field of late has primarily been dominated by just

102
00:08:00,600 --> 00:08:02,680
a single type of neural architecture.

103
00:08:02,680 --> 00:08:09,080
So a single type of machine learning model that's being created to kind of solve the problem.

104
00:08:09,080 --> 00:08:13,560
And as I've seen from both language modeling itself, but almost every other field in machine

105
00:08:13,560 --> 00:08:18,200
learning, there's usually many different ways to tackle a problem.

106
00:08:18,200 --> 00:08:25,480
And just as one technique has gotten a lot of advantage in recent works, doesn't necessarily

107
00:08:25,480 --> 00:08:29,840
mean we should be focusing all of our time, all of our effort on that.

108
00:08:29,840 --> 00:08:36,920
So what I did was rather than using the kind of dominant neural architecture, which is

109
00:08:36,920 --> 00:08:43,320
called the transformer architecture, I used an older architecture called the LSTM,

110
00:08:43,320 --> 00:08:49,200
long short term memory, many of your listeners probably would have heard of, to get basically

111
00:08:49,200 --> 00:08:52,280
quite similar state-of-the-art results.

112
00:08:52,280 --> 00:08:55,960
It's still not quite at state-of-the-art, but I also used far less resources, which was

113
00:08:55,960 --> 00:08:57,160
the second part.

114
00:08:57,160 --> 00:09:02,280
I really wanted a language model that was trainable by the majority of people, because

115
00:09:02,280 --> 00:09:07,240
a lot of the more recent language models have required tens or hundreds of thousands of

116
00:09:07,240 --> 00:09:12,640
dollars worth of either cloud compute or equipment and so on.

117
00:09:12,640 --> 00:09:18,120
So it's really asking the question of what would this field look like if we headed potentially

118
00:09:18,120 --> 00:09:23,040
a different direction in the past or saw different results and everyone pushed on a different

119
00:09:23,040 --> 00:09:24,040
direction.

120
00:09:24,040 --> 00:09:30,040
And also are we sure that we can't achieve many of these results with far less time, far

121
00:09:30,040 --> 00:09:34,880
less compute, which will mean that it's more available for more people, more researchers,

122
00:09:34,880 --> 00:09:40,180
more practitioners to actually use these results and try it on their own data sets and

123
00:09:40,180 --> 00:09:41,180
tasks.

124
00:09:41,180 --> 00:09:48,080
You mentioned a couple of things, which I'll take a quick second to point folks to some

125
00:09:48,080 --> 00:09:52,480
references from here on the podcast.

126
00:09:52,480 --> 00:10:01,120
Back in July, I did a show with Emma Strubel, who did a paper on the environmental impact

127
00:10:01,120 --> 00:10:08,120
of just the kind of large-scale language model training that you're referring to.

128
00:10:08,120 --> 00:10:14,880
As you mentioned, these models take tons of compute to train and that is all powered

129
00:10:14,880 --> 00:10:21,640
by energy and has a huge environmental impact and she traced that down to the actual carbon

130
00:10:21,640 --> 00:10:26,640
load required to train some of these models and made some comparisons about them.

131
00:10:26,640 --> 00:10:32,280
So that is an interesting show for folks to check out and then we'll be talking a little

132
00:10:32,280 --> 00:10:40,760
bit about LSTM's and of course the authority on that is JÃ¼rgen Schmidhuber and I just

133
00:10:40,760 --> 00:10:48,040
show with him about the LSTM back in August of 2017, until we'll talk 44.

134
00:10:48,040 --> 00:10:55,360
But before we talk specifically about the model you created based on the LSTM, you know,

135
00:10:55,360 --> 00:11:00,960
walk us through the transformer architecture that folks are spending so much time on

136
00:11:00,960 --> 00:11:03,800
now, what is it doing?

137
00:11:03,800 --> 00:11:04,800
How does it work?

138
00:11:04,800 --> 00:11:05,800
Absolutely.

139
00:11:05,800 --> 00:11:12,720
So the transformer architecture takes a quite different look at how to do language modeling.

140
00:11:12,720 --> 00:11:18,800
I might start with LSTM's or RNN's recurrent neural networks and the word they're recurrent

141
00:11:18,800 --> 00:11:24,160
is that it takes a word at a time and kind of imagine your reading left to right over

142
00:11:24,160 --> 00:11:25,560
a page.

143
00:11:25,560 --> 00:11:28,720
Well, transformers have a very different take on this.

144
00:11:28,720 --> 00:11:32,120
You get up to a current word, let's say I've just thrown a page in front of you and I

145
00:11:32,120 --> 00:11:38,400
ask you to kind of guess what the next word would be at the bottom of the page.

146
00:11:38,400 --> 00:11:45,200
Your eyes might dart around to multiple different places on the page and in fact, you know,

147
00:11:45,200 --> 00:11:50,320
you might look at first maybe the general topic of the language without looking at many

148
00:11:50,320 --> 00:11:52,840
of the words individually or how they're composed.

149
00:11:52,840 --> 00:11:56,840
You might skirt around and look whether or not you see specific persons name.

150
00:11:56,840 --> 00:12:02,360
So the general idea behind transformers is that they actually use attention over dozens

151
00:12:02,360 --> 00:12:06,880
or hundreds or thousands of the past kind of words or wordpaces that you're looking at

152
00:12:06,880 --> 00:12:12,080
on the page and each of these words end up kind of doing the exact same thing.

153
00:12:12,080 --> 00:12:15,720
They look at all the surrounding words as context using attention.

154
00:12:15,720 --> 00:12:20,600
So a lot of the time, the most important thing for attention is it'll just look at words

155
00:12:20,600 --> 00:12:21,600
that are local to itself.

156
00:12:21,600 --> 00:12:25,120
It'll look at the word previous to itself for the last few.

157
00:12:25,120 --> 00:12:30,080
It might also focus on other most likely related concepts.

158
00:12:30,080 --> 00:12:36,720
So if you saw there were maybe a pet name on not petting a animal species on the page,

159
00:12:36,720 --> 00:12:40,440
you might look for other animal species on the page or something else like that.

160
00:12:40,440 --> 00:12:44,720
So this isn't potentially the best example of describing transformers.

161
00:12:44,720 --> 00:12:49,080
It's been a while since I've had to try and think through it from kind of the ground

162
00:12:49,080 --> 00:12:50,480
up.

163
00:12:50,480 --> 00:12:56,760
But the idea is each word in the page would perform attention to get context about itself

164
00:12:56,760 --> 00:13:01,920
and about the other words on the page, which also enables you to do some very interesting

165
00:13:01,920 --> 00:13:04,560
differences to traditional models.

166
00:13:04,560 --> 00:13:09,560
These models usually don't have any concept of what word is necessarily before or after

167
00:13:09,560 --> 00:13:10,560
it.

168
00:13:10,560 --> 00:13:13,800
It will usually learn that or have some amount of that knowledge injected.

169
00:13:13,800 --> 00:13:19,320
That also means you can extend it quite readily to tasks such as images where you might be

170
00:13:19,320 --> 00:13:23,520
talking about pixels looking around orals or audio as well.

171
00:13:23,520 --> 00:13:30,280
So they're not kind of fundamentally sequential models like an RNN or LSTM.

172
00:13:30,280 --> 00:13:38,560
The relationships between the words are built up solely based on this attention mechanism.

173
00:13:38,560 --> 00:13:39,560
That's absolutely correct.

174
00:13:39,560 --> 00:13:40,560
Yeah.

175
00:13:40,560 --> 00:13:44,160
There's no sequentially necessarily forced on it.

176
00:13:44,160 --> 00:13:48,160
You can add in a few kind of hints or ideas like that.

177
00:13:48,160 --> 00:13:52,600
But it suddenly doesn't have the same idea of sequentiality as like a recurrent neural

178
00:13:52,600 --> 00:13:55,960
network where you're kind of walking one word at a time.

179
00:13:55,960 --> 00:14:01,840
And because of that, one of the main advantages is that you can parallelize a lot of this.

180
00:14:01,840 --> 00:14:06,720
So rather than having to walk one word at a time when you're doing processing, transformers

181
00:14:06,720 --> 00:14:13,040
can usually perform all of these computations across these words in parallel as well.

182
00:14:13,040 --> 00:14:16,720
So there are certain advantages that's primarily during training, but there are certain

183
00:14:16,720 --> 00:14:22,360
advantages about the way in which you can parallelize the work too because you make less

184
00:14:22,360 --> 00:14:25,720
of this constraint on kind of sequentiality.

185
00:14:25,720 --> 00:14:28,680
You kind of mentioned the couple goals.

186
00:14:28,680 --> 00:14:37,080
One was to kind of just take on the idea of transformers and explore whether you could

187
00:14:37,080 --> 00:14:41,000
achieve similar goals with other types of models.

188
00:14:41,000 --> 00:14:49,360
But that is kind of a starting point of inspiration. How did you end up at a specific model this

189
00:14:49,360 --> 00:14:51,320
Shah RNN?

190
00:14:51,320 --> 00:14:57,560
In the past, most of the language models were kind of fixed around either recurrent neural

191
00:14:57,560 --> 00:15:00,920
networks, but they're also some convolutional neural networks.

192
00:15:00,920 --> 00:15:05,360
So almost as similar purchase you would have for handling a vision task.

193
00:15:05,360 --> 00:15:12,080
But for me, in particular, I appreciate and like the aspect of the LSTM, I think it's

194
00:15:12,080 --> 00:15:21,360
more in line with how, at least I can visualize how I would read or think about reading text.

195
00:15:21,360 --> 00:15:25,520
And that's suddenly nothing to do with how the human brain thinks or whatever else, but

196
00:15:25,520 --> 00:15:29,640
it's suddenly at least an abstraction that I can think about and reason through.

197
00:15:29,640 --> 00:15:34,360
As you're reading text usually, at least I don't know about you, but I usually go woodward.

198
00:15:34,360 --> 00:15:38,920
And I might glance back at a different part of the page, but I'm suddenly not glancing

199
00:15:38,920 --> 00:15:42,080
around the page for every word that I'm looking at.

200
00:15:42,080 --> 00:15:46,440
And so the LSTM or the RNN are far better fits for that.

201
00:15:46,440 --> 00:15:52,560
You have an interior kind of hidden state, so some sort of memory, and you're storing

202
00:15:52,560 --> 00:15:57,040
aspects of the current text that you're reading in that memory and using that to predict

203
00:15:57,040 --> 00:15:59,640
the next word.

204
00:15:59,640 --> 00:16:05,600
And that I feel is, at least in my mind, a kind of more intuitive fit for how I read compared

205
00:16:05,600 --> 00:16:11,400
to, say, these multi-headed attention mechanisms used by transformers, where it's equivalent

206
00:16:11,400 --> 00:16:17,520
of every word you look at on the page glancing for some of these models, hundreds of times

207
00:16:17,520 --> 00:16:21,840
at different locations on the page to pull information back and forth.

208
00:16:21,840 --> 00:16:26,600
And they don't generally have this concept of memory as well.

209
00:16:26,600 --> 00:16:32,760
Some of the most recent transform of paper papers have added memory, but it's suddenly

210
00:16:32,760 --> 00:16:39,320
nothing quite like the LSTM's at the moment, which I feel has kind of, as I said, a far

211
00:16:39,320 --> 00:16:44,120
better, intuitive understanding of how we'd go through and reading text.

212
00:16:44,120 --> 00:16:47,920
You know, talking about these two different types of models, you know, we're contrasting

213
00:16:47,920 --> 00:16:57,240
the attention mechanism versus the sequentiality, but they're not necessarily mutually exclusive.

214
00:16:57,240 --> 00:17:03,160
In fact, you use attention as part of the Shah RNN, and attention has been used in conjunction

215
00:17:03,160 --> 00:17:07,120
with RNNs for quite some time, isn't that right?

216
00:17:07,120 --> 00:17:08,120
Yeah.

217
00:17:08,120 --> 00:17:14,240
And that was one of the aspects that I was particularly interested in, and actually researching as well.

218
00:17:14,240 --> 00:17:21,440
The question of how much of the most recent results for transformers are mostly results

219
00:17:21,440 --> 00:17:28,240
of these attention mechanisms themselves, versus, you know, maybe what aspects are missing

220
00:17:28,240 --> 00:17:30,320
from previous models.

221
00:17:30,320 --> 00:17:38,200
So transformers, they are entirely this multi-headed attention concept, versus LSTM's

222
00:17:38,200 --> 00:17:43,040
in the past, have generally, mostly just being the LSTM and only added maybe a layer of

223
00:17:43,040 --> 00:17:46,440
attention here or there, very sparsely.

224
00:17:46,440 --> 00:17:54,680
But the research seemed to have stopped in probably 2017-18 on extending LSTM's direction.

225
00:17:54,680 --> 00:18:02,200
So rather than adding attention heads to these existing RNN models, it was kind of literally

226
00:18:02,200 --> 00:18:06,600
the paper that started, a lot of this was called attention is all you need, which gave

227
00:18:06,600 --> 00:18:09,080
kind of rise to this transformer approach.

228
00:18:09,080 --> 00:18:14,320
And people have seemed to stick pretty close to the paper's title, using almost purely

229
00:18:14,320 --> 00:18:17,080
attention for a lot of these tasks.

230
00:18:17,080 --> 00:18:25,680
So before transformers took off two or three years ago now, I was also exploring using

231
00:18:25,680 --> 00:18:31,960
a very simple form of attention on top of these RNN models.

232
00:18:31,960 --> 00:18:35,320
And it was referred to as a point of sentinel model.

233
00:18:35,320 --> 00:18:40,560
And the idea is it was that it would allow your LSTM, which is mainly just looking at words

234
00:18:40,560 --> 00:18:45,520
one at a time, to look back with like one passer of the page to try and pull some information

235
00:18:45,520 --> 00:18:46,520
back.

236
00:18:46,520 --> 00:18:51,360
So the kind of intuitive idea was imagine that I give you a highlighter and you're allowed

237
00:18:51,360 --> 00:18:54,880
to highlight some of the words on the page that you think are relevant to predicting the

238
00:18:54,880 --> 00:18:56,200
next word.

239
00:18:56,200 --> 00:19:01,960
And that had incredibly good results, but it wasn't kind of properly integrated into the

240
00:19:01,960 --> 00:19:07,120
model. It was mainly about pulling in kind of rare words that the model might not understand

241
00:19:07,120 --> 00:19:13,080
from an kind of information point of view, but it might understand from a positional point

242
00:19:13,080 --> 00:19:14,080
of view.

243
00:19:14,080 --> 00:19:16,960
So my last name might be a good example of that.

244
00:19:16,960 --> 00:19:21,680
Maybe the model has never seen Stephen Merritti, but with an attention mechanism, you highlight

245
00:19:21,680 --> 00:19:25,800
just Merritti and say, well, just pull that word copy and paste it here.

246
00:19:25,800 --> 00:19:29,680
So that was the idea initially behind my point of sentinel.

247
00:19:29,680 --> 00:19:34,000
And that was based off of a lot of work on point networks, which were kind of precursors

248
00:19:34,000 --> 00:19:39,360
to these transformers and multi-headed attention.

249
00:19:39,360 --> 00:19:45,640
But allowing the RNN itself to actually use that to update its kind of internal repository

250
00:19:45,640 --> 00:19:51,280
of knowledge to update its internal memory wasn't heavily used beforehand.

251
00:19:51,280 --> 00:19:54,920
And so that was the direction that I was interested in taking it.

252
00:19:54,920 --> 00:20:01,080
And also asking, well, these transform models have had great success with these dozens

253
00:20:01,080 --> 00:20:07,920
or hundreds of heads, how much, how many heads do you actually need for doing this work

254
00:20:07,920 --> 00:20:10,480
and how complex do they need to be?

255
00:20:10,480 --> 00:20:17,640
Is it enough to literally just do one highlight of the page or do you require 20, 100 different

256
00:20:17,640 --> 00:20:22,040
colors for your highlighter and different levels of granularity?

257
00:20:22,040 --> 00:20:26,440
And the most surprising result for me is that a single head of attention, so allowing

258
00:20:26,440 --> 00:20:30,800
the model to kind of glance back at the page once, is enough to get near state of the

259
00:20:30,800 --> 00:20:35,720
art results when you kind of combine it with an LSTM.

260
00:20:35,720 --> 00:20:43,760
And now is there a kind of measure of the complexity of the LSTM network, like the number of time

261
00:20:43,760 --> 00:20:51,360
steps or something like that that is relevant here to get a sense for, well, complexity.

262
00:20:51,360 --> 00:20:54,800
Yeah, there are a few different directions we could look at.

263
00:20:54,800 --> 00:20:58,240
And that was also one of the ones that I was interested in thinking about.

264
00:20:58,240 --> 00:21:04,200
A lot of the time, these larger models aren't actually kind of sensible to put into production.

265
00:21:04,200 --> 00:21:09,160
They either make a great deal of references or they require a great deal of memory for

266
00:21:09,160 --> 00:21:15,040
keeping, you know, those references in memory or a great deal of like flops, floating

267
00:21:15,040 --> 00:21:22,120
point operations are basically the certain amount of compute for your GPU.

268
00:21:22,120 --> 00:21:27,880
And yeah, part of the idea was that because I'm using only a single head of attention,

269
00:21:27,880 --> 00:21:33,160
and it's a far simpler one, that's the reference to stop thinking with your head, I'm actually

270
00:21:33,160 --> 00:21:40,720
able to add in far more tokens into this, the past reference kind of memory window.

271
00:21:40,720 --> 00:21:45,240
So the size of the page, if you want to think of it that way, that you're able to look back

272
00:21:45,240 --> 00:21:52,800
over the text, but also that LSTM's, because it's just got this one time step, when you're

273
00:21:52,800 --> 00:21:56,040
generating, you're interested in just producing one time step.

274
00:21:56,040 --> 00:22:00,960
And LSTM by itself, you have a very small amount of computation you need to do.

275
00:22:00,960 --> 00:22:05,280
Versus a transformer, as we mentioned, transform is at least so far, don't have any aspect of

276
00:22:05,280 --> 00:22:06,440
this memory.

277
00:22:06,440 --> 00:22:11,240
So every word that they look at, you have to do this full computation over again, and look

278
00:22:11,240 --> 00:22:15,440
back all the way back over the page, doing a great deal of compute usually.

279
00:22:15,440 --> 00:22:18,520
Is that at both training and inference?

280
00:22:18,520 --> 00:22:20,480
Yes, basically.

281
00:22:20,480 --> 00:22:24,800
And it also gets particularly interesting when it comes to inference, because a lot of

282
00:22:24,800 --> 00:22:31,440
the parallelization advantages that you had earlier on for training kind of disappear.

283
00:22:31,440 --> 00:22:36,440
Even training, you already have all the text kind of, you can cheat and imagine that you're

284
00:22:36,440 --> 00:22:41,520
able to predict the next word, because you've got this written down text exactly as it is.

285
00:22:41,520 --> 00:22:47,600
So it's equivalent of, I guess, you or me like reading over an existing paper or article,

286
00:22:47,600 --> 00:22:51,520
versus when you're generating, you obviously can't kind of glance forward to what you would

287
00:22:51,520 --> 00:22:56,080
have written five or ten sentences ahead.

288
00:22:56,080 --> 00:23:00,560
And so the model has to step very slowly, one step after the other.

289
00:23:00,560 --> 00:23:05,280
And when you have a kind of parallel model, as you do with the transform.

290
00:23:05,280 --> 00:23:09,400
And so the model, you're able to train it on just a single GPU.

291
00:23:09,400 --> 00:23:10,400
Yeah.

292
00:23:10,400 --> 00:23:13,720
So all of this lived on a single GPU.

293
00:23:13,720 --> 00:23:17,240
Thanks to Nvidia, they donated to me some time ago.

294
00:23:17,240 --> 00:23:23,240
But yes, a single GPU and almost all the experiments were under 24 hours.

295
00:23:23,240 --> 00:23:29,320
So there's this being this huge push towards cloud compute and everything else like that,

296
00:23:29,320 --> 00:23:31,480
which there's absolutely no problem with.

297
00:23:31,480 --> 00:23:34,120
But they're certainly generally not cost effective.

298
00:23:34,120 --> 00:23:42,120
And I generally want my research to be replicable by, say, a grad student or a freshman or just

299
00:23:42,120 --> 00:23:47,120
someone who's interested and maybe just has the GPU and their gaming laptop.

300
00:23:47,120 --> 00:23:53,040
I think there are so many interesting different potential directions for research that it's

301
00:23:53,040 --> 00:23:58,640
a shame to limit the field to people who have huge amounts of compute or some way of getting

302
00:23:58,640 --> 00:24:01,920
free credits on these cloud services.

303
00:24:01,920 --> 00:24:04,120
I've certainly been offered enough of them.

304
00:24:04,120 --> 00:24:08,800
There are many companies that offer these credits to try to encourage people to play around

305
00:24:08,800 --> 00:24:12,480
and they'll usually give it to the researchers or universities or so on.

306
00:24:12,480 --> 00:24:17,520
But at the end of the day, if not everyone has access to those same kind of free credits

307
00:24:17,520 --> 00:24:22,880
and free resources, then we're limiting the field and limiting the breadth of people

308
00:24:22,880 --> 00:24:24,680
who can contribute.

309
00:24:24,680 --> 00:24:33,160
So you created this model based on attention and LSTM and then you ran it against a benchmark

310
00:24:33,160 --> 00:24:41,920
in this case with the NWIC 8, Hunter Prize Wikipedia data set and you found reasonable

311
00:24:41,920 --> 00:24:43,280
performance with that.

312
00:24:43,280 --> 00:24:47,520
Tell us a little bit about the approach to benchmarking you took here.

313
00:24:47,520 --> 00:24:51,960
So the NWIC 8 data set, it's also referred to as a Hutter Wikipedia Prize.

314
00:24:51,960 --> 00:24:58,280
And the idea was that it's just grabbing a possible dump of Wikipedia.

315
00:24:58,280 --> 00:25:03,120
So that's the first 100 million bytes also of English Wikipedia from it's many years

316
00:25:03,120 --> 00:25:04,120
back now.

317
00:25:04,120 --> 00:25:06,320
I think it was 2006.

318
00:25:06,320 --> 00:25:09,840
And this is kind of an interesting data set for a number of reasons.

319
00:25:09,840 --> 00:25:13,360
One is that it's traditionally being used for traditional compression.

320
00:25:13,360 --> 00:25:18,800
So if you think of zip files or that type of thing, this was the data set that was frequently

321
00:25:18,800 --> 00:25:25,200
used to show whether or not a new compression technique was better than past ones.

322
00:25:25,200 --> 00:25:30,960
So it has been kind of heavily thought through by a number of different people in terms

323
00:25:30,960 --> 00:25:33,040
of traditional data compression.

324
00:25:33,040 --> 00:25:38,000
And one of the fascinating things about language models is that they are actually also data

325
00:25:38,000 --> 00:25:43,640
compresses that certainly not used generally to actually compress files that we might deal

326
00:25:43,640 --> 00:25:45,240
with every day.

327
00:25:45,240 --> 00:25:49,640
But by being able to better predict the next token, it actually means you're able to spend

328
00:25:49,640 --> 00:25:54,160
less and less bits storing the data set itself.

329
00:25:54,160 --> 00:26:01,120
And this has been considered a potential demonstration of kind of the idea that compression and machine

330
00:26:01,120 --> 00:26:05,440
learning and artificial intelligence kind of all wrap up into one thing.

331
00:26:05,440 --> 00:26:09,440
Because compression at the end of the day is being able to produce a larger sequence

332
00:26:09,440 --> 00:26:13,880
from a smaller sequence because you have some knowledge about the structure of how the

333
00:26:13,880 --> 00:26:15,560
data goes together.

334
00:26:15,560 --> 00:26:21,880
And in the exact same way, if you have a knowledge about the structure of a given language

335
00:26:21,880 --> 00:26:28,600
that you're talking in, let's say it's English, or the structure of XML, which is also part

336
00:26:28,600 --> 00:26:33,160
of this data dump, you're able to better predict the next token, which means you're able

337
00:26:33,160 --> 00:26:35,280
to better compress the file.

338
00:26:35,280 --> 00:26:38,200
So this is a really interesting data set in my mind.

339
00:26:38,200 --> 00:26:44,080
And it also, of course, runs over Wikipedia, which has already a great kind of collection

340
00:26:44,080 --> 00:26:48,240
of all of the world's information in it.

341
00:26:48,240 --> 00:26:52,160
So that was the data set that I was interested in exploring.

342
00:26:52,160 --> 00:26:57,160
And yes, it's been one of these standard data sets to explore for language models for

343
00:26:57,160 --> 00:26:58,480
quite some time.

344
00:26:58,480 --> 00:27:02,760
And it turns out with this relatively simple architecture that kind of jumped off in a

345
00:27:02,760 --> 00:27:07,880
different direction, my sharp LSTM, I was able to achieve results that would have been

346
00:27:07,880 --> 00:27:10,720
instead of a year or two ago, unfortunately.

347
00:27:10,720 --> 00:27:18,320
But in able to achieve these results in 12 to maybe 20 hours, depending on your GPU or

348
00:27:18,320 --> 00:27:21,280
depending on the exact formulation of the model.

349
00:27:21,280 --> 00:27:27,880
So far, faster than a lot of these kind of larger models and almost with a minimal size

350
00:27:27,880 --> 00:27:28,880
as well.

351
00:27:28,880 --> 00:27:37,560
So does the result and performance that you've seen in the benchmarking generalize to other

352
00:27:37,560 --> 00:27:39,840
types of tasks?

353
00:27:39,840 --> 00:27:47,000
For example, many might be familiar with these transformer models and language models like

354
00:27:47,000 --> 00:27:53,120
GPT-2 through just a text prediction type of a task where you give it a prompt and you

355
00:27:53,120 --> 00:27:55,760
have it generate some number of words.

356
00:27:55,760 --> 00:28:02,240
And to expand on that, often, we're surprised at how coherent that text reads.

357
00:28:02,240 --> 00:28:07,080
Would you expect that this model performs similarly on that type of task and generate

358
00:28:07,080 --> 00:28:12,920
text that feel so kind of spookily human?

359
00:28:12,920 --> 00:28:18,760
Yeah, it's a great question and it's one in merely, I don't have direct evidence for

360
00:28:18,760 --> 00:28:24,440
yet, but at least for this model and this data set, when you compare it against the other

361
00:28:24,440 --> 00:28:29,960
models, which are all transformer based almost, the numbers are quite similar and when I'm

362
00:28:29,960 --> 00:28:32,080
looking at the output, it's quite similar.

363
00:28:32,080 --> 00:28:34,200
The difference would be in scaling.

364
00:28:34,200 --> 00:28:39,000
So many of these existing transformer models have run over far larger data sets.

365
00:28:39,000 --> 00:28:45,400
The N-Wik8 data set is only, unfortunately, about 100 megabytes of Wikipedia.

366
00:28:45,400 --> 00:28:50,680
So far smaller than many of these models that are now training over dozens, hundreds of

367
00:28:50,680 --> 00:28:54,680
gigabytes of quite varied text.

368
00:28:54,680 --> 00:29:00,440
So the model I use, it certainly produces some fun and hilarious and interesting Wikipedia

369
00:29:00,440 --> 00:29:01,440
pages.

370
00:29:01,440 --> 00:29:06,720
In fact, I love using it to get the kind of reception, like the New York Times writer

371
00:29:06,720 --> 00:29:11,680
said that this article was the most terrible idea in all of existence.

372
00:29:11,680 --> 00:29:15,040
Usually this model ends up with beautiful, pithy quirks, but unfortunately, because it's

373
00:29:15,040 --> 00:29:19,520
mainly focused on the Wikipedia domain, it isn't as expressive as a lot of these larger

374
00:29:19,520 --> 00:29:22,560
models trained over far more varied texts.

375
00:29:22,560 --> 00:29:26,720
Now that isn't an issue with the model itself, that's an issue with the data set that I have.

376
00:29:26,720 --> 00:29:31,680
And the fact that I was mainly trying to get it to run quite quickly.

377
00:29:31,680 --> 00:29:37,480
But I don't think there is necessarily a limit or going to be an extreme difference in how

378
00:29:37,480 --> 00:29:42,800
this model would perform for a lot of these kind of text completion tasks.

379
00:29:42,800 --> 00:29:45,160
But you would have to scale it up.

380
00:29:45,160 --> 00:29:50,080
And one of the other main centerpieces of the article, the paper that I was talking

381
00:29:50,080 --> 00:29:56,320
about is the fact that because there was so much attention on performance for the

382
00:29:56,320 --> 00:30:03,680
past few years, no pun intended, there's models that ended up getting a lot of the focus

383
00:30:03,680 --> 00:30:08,800
when it comes to both the research, but also the practical engineering behind how to make

384
00:30:08,800 --> 00:30:10,560
these models scale.

385
00:30:10,560 --> 00:30:16,160
There are indeed certain advantages that just make it far easier to scale transformers.

386
00:30:16,160 --> 00:30:21,920
But there are also many known techniques to making other neural networks, formulations

387
00:30:21,920 --> 00:30:27,440
faster such as the LSTM, that we haven't actually pursued because we're all going off in

388
00:30:27,440 --> 00:30:29,920
this transformer direction.

389
00:30:29,920 --> 00:30:35,360
And I feel like that's a bit of a shame, because as more people spend more time working on

390
00:30:35,360 --> 00:30:41,920
a very particular brand or a particular style of solution to a task, these other solutions

391
00:30:41,920 --> 00:30:45,200
which might not actually be worse kind of fall to the side.

392
00:30:45,200 --> 00:30:51,280
And so that's really part of what I was interested in kind of pointing out with this research,

393
00:30:51,280 --> 00:30:55,520
but up until kind of fairly recently, these results would have been steady art.

394
00:30:55,520 --> 00:31:02,080
And maybe people have been focusing more on how do you optimize an LSTM or how do we improve

395
00:31:02,080 --> 00:31:05,280
the way and we can form or how do we scale it up.

396
00:31:05,280 --> 00:31:09,280
But that kind of tension instead went to transformers.

397
00:31:09,280 --> 00:31:16,840
When I've played with models like GPT-2 in the past, it becomes very clear, very quickly

398
00:31:16,840 --> 00:31:22,520
that the model, it's not creating new texts, it's kind of stitching together things that

399
00:31:22,520 --> 00:31:28,280
it has memorized or seen in the past.

400
00:31:28,280 --> 00:31:33,800
And these transformer models, because they're huge models, they have a huge parameter

401
00:31:33,800 --> 00:31:36,520
space and they can memorize a lot of stuff.

402
00:31:36,520 --> 00:31:42,920
So does a smaller model, you've talked about scaling it up, can it scale up in the same

403
00:31:42,920 --> 00:31:47,240
way so that it can remember as much stuff?

404
00:31:47,240 --> 00:31:53,360
Yeah, the concept of scaling up the model, I guess you can go in two different directions.

405
00:31:53,360 --> 00:31:58,560
One is you can scale up a small model to handle a larger data set or you can of course scale

406
00:31:58,560 --> 00:32:04,000
up the kind of model architecture from this smaller model to a larger one and also at the

407
00:32:04,000 --> 00:32:06,000
same time throwing more data.

408
00:32:06,000 --> 00:32:07,520
I guess let's pull that apart.

409
00:32:07,520 --> 00:32:11,080
So one is scaling up a small model to a larger data set.

410
00:32:11,080 --> 00:32:16,200
And it is going to be a limit on how well, at least in the kind of traditional ways that

411
00:32:16,200 --> 00:32:19,600
language models operate, a small model will work.

412
00:32:19,600 --> 00:32:24,320
There's just, you know, some limit, it's not going to be able to perfectly memorize all

413
00:32:24,320 --> 00:32:29,320
of Harry Potter if you, you know, all of these different books, if you're feeding them

414
00:32:29,320 --> 00:32:34,680
in, just because there's a finite amount of space, a finite number of parameters that

415
00:32:34,680 --> 00:32:38,240
this language model is able to use.

416
00:32:38,240 --> 00:32:43,960
But that's also kind of a different question compared to can a small model be able to

417
00:32:43,960 --> 00:32:50,080
coherently generate text, maybe especially if you gave it, you know, the first few pages

418
00:32:50,080 --> 00:32:54,840
of Harry Potter to thumb through because we wouldn't call someone unintelligent if say,

419
00:32:54,840 --> 00:33:00,400
I don't know, I gave the first few pages of Harry Potter to my friend who would never

420
00:33:00,400 --> 00:33:03,520
read the book and say, try and continue writing along.

421
00:33:03,520 --> 00:33:07,760
If they don't make a reference to Snape, that's a very different kind of problem than

422
00:33:07,760 --> 00:33:11,320
them not being able to actually coherently generate text.

423
00:33:11,320 --> 00:33:14,480
So we are kind of conflating two aspects of language models.

424
00:33:14,480 --> 00:33:17,800
How much can the language model memorize from the world around it, you know, how much

425
00:33:17,800 --> 00:33:24,640
of the text that's been written or topics in the world can it kind of cram into its memory?

426
00:33:24,640 --> 00:33:28,560
And that's usually the form of parameters, that's where the lot of these models scale.

427
00:33:28,560 --> 00:33:33,840
And the secondary question of, well, can it coherently generate text, you know, how much

428
00:33:33,840 --> 00:33:39,200
of it is actually some level of understanding of the structure of text or how text might

429
00:33:39,200 --> 00:33:44,640
work and how much of it is actually just kind of, as you said, reproducing what it's already

430
00:33:44,640 --> 00:33:47,440
read, what it's already kind of memorized.

431
00:33:47,440 --> 00:33:53,400
And I'm personally, hopefully, not convinced that we have to keep scaling these models

432
00:33:53,400 --> 00:33:58,400
up, you know, as a human being, if I, you know, gave my friend that task of completing

433
00:33:58,400 --> 00:34:02,240
some Harry Potter fanfiction and they'd never read the books before, maybe I could point

434
00:34:02,240 --> 00:34:06,960
them to the Harry Potter Wikipedia page and maybe that would be enough.

435
00:34:06,960 --> 00:34:10,720
Maybe they want to thumb through a Wikipedia that's specific to Harry Potter.

436
00:34:10,720 --> 00:34:14,120
Maybe they need to read through the book, but I suddenly wouldn't have to, you know,

437
00:34:14,120 --> 00:34:19,560
get them an extra chunk of brain just in order to handle that type of task.

438
00:34:19,560 --> 00:34:24,520
And so that's the other kind of question and hope in my work.

439
00:34:24,520 --> 00:34:27,800
Maybe we're scaling up in the direction of being able to memorize more and more of this

440
00:34:27,800 --> 00:34:34,360
data far too early compared to, you know, having these models potentially be able to refer

441
00:34:34,360 --> 00:34:40,040
to new data when it needs to and thus be able to generalize better.

442
00:34:40,040 --> 00:34:44,640
So yeah, I do think that the two quite different questions, which isn't to say that they also

443
00:34:44,640 --> 00:34:50,880
don't interoperate and interact as well, because one of the fascinating things about text

444
00:34:50,880 --> 00:34:55,560
is that it's really, if you think it was like memorizing results, humans have spent

445
00:34:55,560 --> 00:35:01,600
a lot of time generating the text and usually text is beautifully distilled, condensed,

446
00:35:01,600 --> 00:35:03,160
kind of form of knowledge.

447
00:35:03,160 --> 00:35:05,960
And so there is obviously a lot of benefit to that.

448
00:35:05,960 --> 00:35:10,960
And there are obviously ways in which that can help a model train, but, you know, how

449
00:35:10,960 --> 00:35:16,640
much of that is necessary or maybe a completely different question is, do we think that these

450
00:35:16,640 --> 00:35:20,240
models can generate something that it hasn't read about before?

451
00:35:20,240 --> 00:35:24,000
Because that is potentially, you know, there are two different directions we can think

452
00:35:24,000 --> 00:35:29,880
about with language models, one is kind of minimizing the complexity of these data sets.

453
00:35:29,880 --> 00:35:34,240
The idea that I'll never be able to read all of the internet or all of Wikipedia, but

454
00:35:34,240 --> 00:35:39,480
hopefully I don't necessarily need to get to the result or to the information that is

455
00:35:39,480 --> 00:35:41,480
interesting or useful for my task.

456
00:35:41,480 --> 00:35:46,360
But then the second one is, well, if you want it to generate something, as we've been

457
00:35:46,360 --> 00:35:51,840
using with GPT-2 and Transform Excel and a lot of these language models, if we want

458
00:35:51,840 --> 00:35:56,920
to get it to generate something that's actually quite intelligent, is it enough to be able

459
00:35:56,920 --> 00:36:00,800
to kind of copy together other pieces of knowledge from the past?

460
00:36:00,800 --> 00:36:01,800
And I don't know.

461
00:36:01,800 --> 00:36:03,600
It's an interesting kind of very open question.

462
00:36:03,600 --> 00:36:04,600
Cool.

463
00:36:04,600 --> 00:36:10,920
There's a section of the paper where you go into a discussion around tokenization attacks

464
00:36:10,920 --> 00:36:12,240
and that kind of thing.

465
00:36:12,240 --> 00:36:14,080
What was that bit all about?

466
00:36:14,080 --> 00:36:15,080
Yeah.

467
00:36:15,080 --> 00:36:20,480
So I'll admit this is a section where I'm still having to rethink a lot of what I'm

468
00:36:20,480 --> 00:36:28,320
pondering, but one of the questions is that I published one of the standard data sets

469
00:36:28,320 --> 00:36:31,360
for this task called WikiTax103.

470
00:36:31,360 --> 00:36:37,520
And generally, when you're trying to determine whether or not a language model is performing

471
00:36:37,520 --> 00:36:44,200
well or not, you have a specific data set and you get it to try and predict an under like

472
00:36:44,200 --> 00:36:47,520
a test section, a section that hasn't seen before.

473
00:36:47,520 --> 00:36:53,000
And you asked how confused were you by say, you know, all of these words in succession.

474
00:36:53,000 --> 00:36:58,480
And tokenization is the idea that you can break these up into kind of different components.

475
00:36:58,480 --> 00:37:03,240
You might be able to say, just split on spaces and you would consider those words.

476
00:37:03,240 --> 00:37:08,400
So running, ran, and so forth end up all being separate, but the other concept is you

477
00:37:08,400 --> 00:37:11,000
could break it up into parts of words.

478
00:37:11,000 --> 00:37:16,560
So the example I have is specialized because you have special, you have specialized,

479
00:37:16,560 --> 00:37:17,560
specialized.

480
00:37:17,560 --> 00:37:21,960
If you break it up into those three different chunks, you can say, well, okay, the word

481
00:37:21,960 --> 00:37:24,080
special means this type of thing.

482
00:37:24,080 --> 00:37:30,000
When I add the suffix, eyes, you know, I specialize in a field or I specialized adding

483
00:37:30,000 --> 00:37:34,920
the D, the additional suffix on top of that, you can see you break the word up into these

484
00:37:34,920 --> 00:37:39,280
components where even if you don't necessarily understand all of one part, you can better

485
00:37:39,280 --> 00:37:41,120
understand other pieces.

486
00:37:41,120 --> 00:37:45,840
So that was part of the idea behind the tokenization attack, whether it was actually fair for these

487
00:37:45,840 --> 00:37:52,040
different models to compare the numbers by breaking up across these different tokens.

488
00:37:52,040 --> 00:37:56,600
Turns out I think it actually is, I've been turned smarter people than me have pointed

489
00:37:56,600 --> 00:38:02,320
out many of the issues in my thinking, but there's also still an issue when it comes to

490
00:38:02,320 --> 00:38:03,320
generating text.

491
00:38:03,320 --> 00:38:07,920
So if you've seen from GPT2 or if you've been playing around with that or these other

492
00:38:07,920 --> 00:38:13,240
online models, most of the time these models have been trained with what we would consider

493
00:38:13,240 --> 00:38:14,240
gold text.

494
00:38:14,240 --> 00:38:19,440
So text that we would hopefully have pulled from an intelligent source so it actually makes

495
00:38:19,440 --> 00:38:20,440
sense.

496
00:38:20,440 --> 00:38:25,880
And if the language model say guesses the wrong word next, it can recover because you're

497
00:38:25,880 --> 00:38:31,000
telling it well actually know that word was wrong, you actually meant to say this.

498
00:38:31,000 --> 00:38:34,840
But if you've played around with these models and you've had it start to generate something

499
00:38:34,840 --> 00:38:40,360
incorrect, it'll usually start to say getting into this weird loop or start to repeat the

500
00:38:40,360 --> 00:38:46,000
wrong fact over and over and over again because it says, well, okay, I've seen that text

501
00:38:46,000 --> 00:38:47,000
generated.

502
00:38:47,000 --> 00:38:51,400
I assume that everything before it was correct and I'm not going to go back and fix it.

503
00:38:51,400 --> 00:38:57,680
So the different sizes of these text chunks can also impact that when it comes to generation.

504
00:38:57,680 --> 00:39:02,280
But yeah, the tokenization attack I've got to admit is less of a well formed argument

505
00:39:02,280 --> 00:39:03,680
as it turns out.

506
00:39:03,680 --> 00:39:09,160
It did kind of feel a little bit like you were geeking out and having fun after the

507
00:39:09,160 --> 00:39:11,560
real work of the paper was done.

508
00:39:11,560 --> 00:39:16,760
I think it was less well formed than the rest of the paper.

509
00:39:16,760 --> 00:39:18,920
So I'll have to admit that much.

510
00:39:18,920 --> 00:39:19,920
Yes.

511
00:39:19,920 --> 00:39:26,200
You know, is your hope that folks will build on the sharp RNN or you know, put it in the

512
00:39:26,200 --> 00:39:31,160
production, put in the actual use like what do you, you know, what's your ultimate hope

513
00:39:31,160 --> 00:39:33,640
for this work and where do you see it going?

514
00:39:33,640 --> 00:39:38,880
Yeah, I've had people already contact me about pushing something like it into a

515
00:39:38,880 --> 00:39:42,960
production, because there are a number of advantages, especially for production usage

516
00:39:42,960 --> 00:39:45,480
or for if you're running small models.

517
00:39:45,480 --> 00:39:49,600
There are some advantages to it that you don't get out of the transformer.

518
00:39:49,600 --> 00:39:55,000
But honestly, my main hurt is for us as a field to kind of think through these two different

519
00:39:55,000 --> 00:39:57,200
directions that we're going.

520
00:39:57,200 --> 00:39:59,680
One is that we have something that works.

521
00:39:59,680 --> 00:40:03,440
We have language models and they seem to work well, turns out scaling them up.

522
00:40:03,440 --> 00:40:05,280
They generally seem to work better.

523
00:40:05,280 --> 00:40:10,520
But as we scale them up, we are kind of locking ourselves into two different things where

524
00:40:10,520 --> 00:40:14,880
we're focusing all of our research on this one specific formulation of how to solve the

525
00:40:14,880 --> 00:40:18,800
task, which we don't know is necessarily correct yet.

526
00:40:18,800 --> 00:40:23,600
And we're locking in a lot about engineering effort into solving the task using that particular

527
00:40:23,600 --> 00:40:28,160
formulation, which means that it's more and more difficult for different potential

528
00:40:28,160 --> 00:40:31,720
approaches to actually even get a start.

529
00:40:31,720 --> 00:40:36,360
The model slower, or it's just not able to be scaled because the frameworks aren't there

530
00:40:36,360 --> 00:40:37,800
and so on.

531
00:40:37,800 --> 00:40:44,400
And I am hoping that the community kind of really thinks about that, because if we kind

532
00:40:44,400 --> 00:40:49,040
of followed this example, I'm not sure if your readers are aware, but one of the first

533
00:40:49,040 --> 00:40:57,280
deep learning examples was the cat example that Google did in, I think it was 2012, where

534
00:40:57,280 --> 00:41:04,120
they used, it was along the lines of 16,000 CPU cores to kind of look at small images in

535
00:41:04,120 --> 00:41:09,040
order to better classify our number of small classes amongst them, whether or not a cat

536
00:41:09,040 --> 00:41:10,720
was in the image.

537
00:41:10,720 --> 00:41:14,560
And that kind of sounds crazy now because you can do that on your mobile phone.

538
00:41:14,560 --> 00:41:18,960
But that was because we also went a different direction and we realized GPUs were better

539
00:41:18,960 --> 00:41:23,560
for kind of solving this type of task and we have improved the architecture and so on over

540
00:41:23,560 --> 00:41:24,560
time.

541
00:41:24,560 --> 00:41:30,960
Imagine kind of a different history where we instead focus just on using CPUs like this.

542
00:41:30,960 --> 00:41:36,160
Maybe the only people who'd be able to do machine learning at this point were the Googles

543
00:41:36,160 --> 00:41:41,480
or the Microsofts or what have you that have the 16,000 CPUs hanging about.

544
00:41:41,480 --> 00:41:48,040
And so the more we focus on a particular direction and forget about the fact that these

545
00:41:48,040 --> 00:41:52,520
models are becoming larger and larger and more unwieldy to train and that there are potentially

546
00:41:52,520 --> 00:41:55,280
other more efficient ways to solve the problem.

547
00:41:55,280 --> 00:42:00,120
While we might find ourselves locked into a future where only a handful of like large organizations

548
00:42:00,120 --> 00:42:04,560
are able to do this research and the worst part would be we'd be locked into this future

549
00:42:04,560 --> 00:42:08,880
not because it's the only way forward but because it's just the way forward that we've

550
00:42:08,880 --> 00:42:10,960
all pushed.

551
00:42:10,960 --> 00:42:13,360
So that's really my hope for this paper.

552
00:42:13,360 --> 00:42:18,920
But additionally if people use the model itself I'll be glad to.

553
00:42:18,920 --> 00:42:26,480
So kind of put another way your general senses that we've kind of over indexed on exploit

554
00:42:26,480 --> 00:42:30,760
in this particular case and you're kind of encouraging us to explore more yet there's

555
00:42:30,760 --> 00:42:33,480
more there to to try.

556
00:42:33,480 --> 00:42:40,520
That's a beautiful way of putting it yes we have exploited a great deal and that doesn't

557
00:42:40,520 --> 00:42:44,320
necessarily mean it's the best solution to the task and the the worrying part is you

558
00:42:44,320 --> 00:42:48,800
know the more we exploit in one direction the more it seems that all other paths are useless

559
00:42:48,800 --> 00:42:52,280
but that's not necessarily because that useless but because of the amount of effort put

560
00:42:52,280 --> 00:42:56,360
into just you know making this one design work.

561
00:42:56,360 --> 00:43:03,400
For folks that want to play around with this you've got code available on GitHub is that

562
00:43:03,400 --> 00:43:04,400
right?

563
00:43:04,400 --> 00:43:09,240
Indeed there's code available on GitHub it isn't the most beautiful card but I am also

564
00:43:09,240 --> 00:43:15,480
going to be rewriting it some point soon with a particular focus on using it in production

565
00:43:15,480 --> 00:43:19,880
and using it for a number of different tasks because the code base as it stands is really

566
00:43:19,880 --> 00:43:24,080
my research code base where you know you have a number of different options in here you're

567
00:43:24,080 --> 00:43:29,480
poking and prodding and trying things out and certainly haven't optimized it for speed

568
00:43:29,480 --> 00:43:34,240
even though it's you know able to train in only 12 or 20 hours on a GPU.

569
00:43:34,240 --> 00:43:38,400
But yes there's code available if anyone's interested in playing around with it as a few

570
00:43:38,400 --> 00:43:43,200
have already started doing please do so and I'll help as much as I can but yeah the

571
00:43:43,200 --> 00:43:45,920
code is available so get to playing.

572
00:43:45,920 --> 00:43:53,720
Yeah and I just pulled up the GitHub page and noticed that one of the kind of assumptions

573
00:43:53,720 --> 00:43:59,800
lies assertions that I made earlier was that this model would have fewer parameters than

574
00:43:59,800 --> 00:44:04,680
the transformers and at least in the case of the ones that you've mentioned in the table

575
00:44:04,680 --> 00:44:07,880
that's not necessarily the case.

576
00:44:07,880 --> 00:44:12,520
Yeah the number and on that remake page is actually a little higher than it actually

577
00:44:12,520 --> 00:44:18,840
is in the paper 53 million parameters versus I think it's 41 and yes I mentioned that

578
00:44:18,840 --> 00:44:25,120
in the paper as well there are a few different directions for that.

579
00:44:25,120 --> 00:44:30,440
One is that LSTMs they generally need to have a fairly large hidden state in order to

580
00:44:30,440 --> 00:44:33,000
kind of retain memory.

581
00:44:33,000 --> 00:44:37,960
If your readers are aware of how it works the recurrence the kind of forget gates means

582
00:44:37,960 --> 00:44:43,080
that it kind of continues left to right I won't go into too much detail about that but

583
00:44:43,080 --> 00:44:49,040
yeah the exact in terms of competing purely on parameters there are models that are slightly

584
00:44:49,040 --> 00:44:51,840
better and that's really quite reasonable.

585
00:44:51,840 --> 00:44:56,000
I also don't think I mentioned the adaptive transformer which does a fair degree better

586
00:44:56,000 --> 00:45:01,120
as well but this is more of a proof of concept paper it's possible that you could kind of

587
00:45:01,120 --> 00:45:08,760
trim the shot LSTM down to try to to catch these as well but that's also then a question

588
00:45:08,760 --> 00:45:13,600
of you know do these necessarily train faster to the use less compute when it comes prediction

589
00:45:13,600 --> 00:45:18,560
time to so it's really a bit of a trade off on a number of different directions.

590
00:45:18,560 --> 00:45:23,280
Yeah yeah I guess the biggest thing to jump out to me is that my intuition that the reason

591
00:45:23,280 --> 00:45:28,240
why transformers you know these large scale transformers were better at kind of pulling

592
00:45:28,240 --> 00:45:31,720
all these texts out is because they had more parameters but that's not necessarily the

593
00:45:31,720 --> 00:45:32,720
case.

594
00:45:32,720 --> 00:45:37,240
No the multi-headed tension like that's the only thing that this paper is not saying at

595
00:45:37,240 --> 00:45:41,160
all it's not saying that my method is better it's just kind of asking like you know how

596
00:45:41,160 --> 00:45:47,280
many attention heads are important how do these necessarily work because you know I could

597
00:45:47,280 --> 00:45:51,000
have well and fully found that you know attention heads were the only thing that was necessary

598
00:45:51,000 --> 00:45:53,960
for this but that didn't end up being the case.

599
00:45:53,960 --> 00:45:58,880
But one of the interesting things about these transformables is you if you see those results

600
00:45:58,880 --> 00:46:04,680
the LSTM is only full layers deep so there are four LSTMs that you kind of pass the information

601
00:46:04,680 --> 00:46:10,920
through versus these transform models the two that we were talking about that have lower

602
00:46:10,920 --> 00:46:14,480
parameter accounts that have 12 layers each.

603
00:46:14,480 --> 00:46:21,840
So you can also either reuse parameters or perform a great deal more computation to potentially

604
00:46:21,840 --> 00:46:26,480
get similar results by kind of maybe doing the same task you know a few times in a slightly

605
00:46:26,480 --> 00:46:32,320
more intelligent way and I guess the comparison with that would be either certain compression

606
00:46:32,320 --> 00:46:36,920
algorithms you know the more time you spend compressing the smaller file size so there

607
00:46:36,920 --> 00:46:39,480
are kind of these intuitive ideas as well.

608
00:46:39,480 --> 00:46:44,640
So it's yes possible that by doing a far deeper model that requires more compute we'd

609
00:46:44,640 --> 00:46:49,640
be able to get better results but that's all that's all kind of open research questions

610
00:46:49,640 --> 00:46:50,640
yeah.

611
00:46:50,640 --> 00:46:54,960
Are you surprised by the reception to it or was it in line with what you thought?

612
00:46:54,960 --> 00:47:01,000
I mean also the reception has been bit more than I was expecting the other issue which

613
00:47:01,000 --> 00:47:05,200
is interesting is you know you probably noticed in the paper I was just having fun with

614
00:47:05,200 --> 00:47:06,200
it.

615
00:47:06,200 --> 00:47:12,040
I mean that's obvious it's a real research paper but like there's this whole secondary

616
00:47:12,040 --> 00:47:16,720
discussion some people have gotten very angry talking about the scientific voice and whether

617
00:47:16,720 --> 00:47:18,160
or not I was breaking it.

618
00:47:18,160 --> 00:47:25,400
The paper is how do you put quite literary and fun and lighthearted it doesn't take

619
00:47:25,400 --> 00:47:31,960
itself seriously and you know that makes it I think rather accessible and interesting

620
00:47:31,960 --> 00:47:36,720
but it sounds like others you're getting some flak for that.

621
00:47:36,720 --> 00:47:43,840
I mean it suddenly ended up being a polarizing thing I need or in retrospect it I do think

622
00:47:43,840 --> 00:47:49,280
I've suddenly gone too far in some directions and the main one the main point I really

623
00:47:49,280 --> 00:47:58,160
wish I had done better was it is less accessible to non-English speakers and that's something

624
00:47:58,160 --> 00:48:05,200
that I in no way ever wanted to have happen and so that's the main thing I've suddenly

625
00:48:05,200 --> 00:48:10,560
decided on I want to make it far more accessible which you know that might just be additional

626
00:48:10,560 --> 00:48:16,840
material very simplified explanations as some papers have like blog posts that really

627
00:48:16,840 --> 00:48:22,160
distill everything out or worth through examples one step at a time but yeah they're kind

628
00:48:22,160 --> 00:48:26,000
of two other directions that I was thinking about this from one is that I'm now an independent

629
00:48:26,000 --> 00:48:31,320
researcher I don't have these large companies backing me and papers are really intensive

630
00:48:31,320 --> 00:48:37,120
things and my natural writing style the way in which I enjoy writing is you know along

631
00:48:37,120 --> 00:48:41,560
the lines of in the paper some directions as I said I've gone too far but other directions

632
00:48:41,560 --> 00:48:47,600
people have really enjoyed and it kind of provided me a lot of a lot of room to think

633
00:48:47,600 --> 00:48:53,080
about what I was writing why I'm doing language modeling like what it means to me and and

634
00:48:53,080 --> 00:49:01,880
so on but the other direction as well is that I think in this professionalized setting

635
00:49:01,880 --> 00:49:07,040
a lot of I have this entire kind of section in there where I talk about how you'd have

636
00:49:07,040 --> 00:49:12,600
to present what I was writing in order for it to be kind of accepted at conferences whilst

637
00:49:12,600 --> 00:49:17,600
whilst I I have gone too far I think it's also worth it for the academic community to really

638
00:49:17,600 --> 00:49:22,160
reflect on what they're expecting and what they're enforcing when it comes to papers one

639
00:49:22,160 --> 00:49:26,840
friend said I should you know and rightly so you should be dispassionate when it comes

640
00:49:26,840 --> 00:49:33,560
to scientific voice you shouldn't put your opinion into it but just because it isn't

641
00:49:33,560 --> 00:49:38,160
obvious on the surface that given paper has an opinion doesn't mean that it that there

642
00:49:38,160 --> 00:49:44,520
is no bias within that paper and whether that's the the papers it compares against the the

643
00:49:44,520 --> 00:49:49,920
approaches it tries the results it does or doesn't show I think this is this is something

644
00:49:49,920 --> 00:49:54,840
that the field should actually think about and you know a friend of mine said maybe I'm

645
00:49:54,840 --> 00:49:59,920
not the right person to think about that or do it but I don't think that science has

646
00:49:59,920 --> 00:50:08,320
to have a single voice and I don't know I the main thing as well is that you know as

647
00:50:08,320 --> 00:50:12,320
I said I'm an independent researcher I enjoyed writing it that's probably the best way

648
00:50:12,320 --> 00:50:19,440
for me to to produce more content so if the if you want more content that's kind of

649
00:50:19,440 --> 00:50:24,800
the way that I'm writing myself right and you know writing pages of text and getting these

650
00:50:24,800 --> 00:50:29,800
results is no easy effects are anyway that that's part of the battle as I said I'm still

651
00:50:29,800 --> 00:50:36,600
I'm still really just thinking through it myself the next paper I will I'll try simultaneously

652
00:50:36,600 --> 00:50:41,080
turning it down within the main section of the text but maybe I'll do something ridiculous

653
00:50:41,080 --> 00:50:46,720
like put all the all my funnicides and footnotes or into the appendix or something else like

654
00:50:46,720 --> 00:50:52,400
that maybe it maybe you need to train a language model on a bunch of boring archive papers

655
00:50:52,400 --> 00:50:59,200
and then run your paper through it or something like that to turn it into academic speak yes

656
00:50:59,200 --> 00:51:05,760
the the room of all of the benefits of these is yeah maybe all the optional you're sure

657
00:51:05,760 --> 00:51:11,760
the hidden text and sections explored and fill out with all the the ridiculous expansions

658
00:51:11,760 --> 00:51:19,800
yeah right right yeah yeah I hadn't seen that that aspect of the the response to it that's

659
00:51:19,800 --> 00:51:26,080
another corner of scientific Twitter I guess well yeah it's it's interesting it kind of split

660
00:51:26,080 --> 00:51:34,320
depending on community lines reddit was the one that was most kind of 50 50 when it came to that

661
00:51:34,320 --> 00:51:41,680
hacken years had a little bit of a a discussion on it but wasn't that heavy and Twitter was you know

662
00:51:41,680 --> 00:51:47,520
mercilessly support the other thing is it's all sort of private channels as well that there are

663
00:51:47,520 --> 00:51:54,480
some some friends of mine who didn't want to you know say it publicly just because they they

664
00:51:54,480 --> 00:52:00,240
considered this a mistake I made and that they just didn't want to they though as long as I you

665
00:52:00,240 --> 00:52:07,840
know don't do it again sort of thing yeah as I said I'm I'm thinking through it all like a great

666
00:52:07,840 --> 00:52:15,440
deal you know maybe maybe I just need to separate out everything and keep the papers boring but

667
00:52:16,400 --> 00:52:21,760
I don't know I'm still thinking through a little bit well that would be a great loss but you

668
00:52:21,760 --> 00:52:27,680
know I certainly you know there are certainly as you've mentioned aspects of the argument that

669
00:52:27,680 --> 00:52:34,880
resonate in terms of accessibility you know to folks that are less familiar with the particular

670
00:52:34,880 --> 00:52:39,760
idioms and things like that you're using I mean at the same time it's it's opening up to a

671
00:52:39,760 --> 00:52:46,080
broader like as you said it's opening up to a broader audience I had like you know venture

672
00:52:46,080 --> 00:52:51,440
capitalists or students that I helped teach in Australia who are still high school students or

673
00:52:51,440 --> 00:52:59,040
so on being able being able to read the paper and get the vast majority of the content you know

674
00:53:00,320 --> 00:53:04,320
and then at the same time a lot of these papers if we're talking about reproducibility or

675
00:53:04,320 --> 00:53:10,080
understandability you know I've included my curd and people have already reproduced it a lot of

676
00:53:10,080 --> 00:53:16,320
the more complex works I've spent like there was one paper that I spent almost a year trying to

677
00:53:16,320 --> 00:53:22,400
replicate it was some of the worst worst chunks in my life because this paper was doing better on

678
00:53:22,400 --> 00:53:29,680
a task that I was really trying to solve it was a language modeling paper I ended up wasting so

679
00:53:29,680 --> 00:53:34,640
much time so much compute so much energy and that was because the authors even though it was a

680
00:53:34,640 --> 00:53:39,200
professionally written and well received and published paper the authors didn't include all the

681
00:53:39,200 --> 00:53:46,560
details to reproduce the work that's kind of what I mean by like a lot of this is kind of you know

682
00:53:46,560 --> 00:53:51,600
as I said my paper is not necessarily the right balance of it but that there's this reality behind

683
00:53:51,600 --> 00:53:57,680
the papers that is just papered or go yeah they're fun intended but like that there's this

684
00:53:57,680 --> 00:54:01,440
full throughout it says like you know oh we only tried this thing it's actually you know you tried

685
00:54:01,440 --> 00:54:06,640
10 different things and you only reported on that or we don't quite know how this works but we're

686
00:54:06,640 --> 00:54:11,520
going to pretend we do for the sake of like we're worried about reviewer number two saying that

687
00:54:11,520 --> 00:54:16,240
this is the obviously don't understand how it works it's not right in our approach or what

688
00:54:16,240 --> 00:54:24,240
happier so at the very least you know I think my paper makes it obvious that writing styles are

689
00:54:24,240 --> 00:54:31,520
thought about and enforced but maybe they're not thought about and forced as as publicly as as we

690
00:54:31,520 --> 00:54:37,920
might might need to think about them yeah well you know I think at the end of the day for a paper

691
00:54:38,720 --> 00:54:47,760
who's one of whose primary goals is to ask questions of the research community you know

692
00:54:47,760 --> 00:54:52,960
it strikes me as totally appropriate that it you know ask those questions in a way that

693
00:54:52,960 --> 00:55:00,080
itself ask questions yeah I think that's the other thing I wanted the off sheet when you when

694
00:55:00,080 --> 00:55:04,640
you're talking about the community as a whole like I can't decide on the direction of the community

695
00:55:04,640 --> 00:55:09,520
and I can't necessarily there there are certain things that you can't scientifically prove one way

696
00:55:09,520 --> 00:55:15,120
over the other like I can't say that we took their own direction by focusing on transformers I can

697
00:55:15,120 --> 00:55:21,440
just ask the question but maybe there's the right way to do that as well yeah I mean it's you know

698
00:55:21,440 --> 00:55:26,000
I don't know that it's a right or wrong thing it's you know it's kind of what we've done and I think

699
00:55:26,000 --> 00:55:33,040
you know you're you're touching on that it's not the only way and I would hope people would get

700
00:55:33,040 --> 00:55:39,520
that whether they can get those other ways funded at this point because everything is so focused

701
00:55:39,520 --> 00:55:44,720
on transformers that's a whole other question but yeah maybe you've given someone a data point

702
00:55:44,720 --> 00:55:51,040
that you know that says that you know their thing should be funded yeah that's the other I mean

703
00:55:51,040 --> 00:55:57,440
the one of the other first is as well like training language models quickly my postcard based

704
00:55:57,440 --> 00:56:04,800
AWD LSTM because it was so quick to train similar times it was like 12 hours for many of these

705
00:56:04,800 --> 00:56:10,480
different datasets you know experiment is we're able to they say do it in like dozens or hundreds

706
00:56:10,480 --> 00:56:16,800
of languages or dozens or hundreds of variants versus these large language models I know friends

707
00:56:16,800 --> 00:56:23,600
who they set off the training job and even with you know the 50 or $100,000 spent on some cloud

708
00:56:23,600 --> 00:56:30,720
compute thing they're waiting one two three four months in order for to get just one result I

709
00:56:30,720 --> 00:56:34,160
that's the other thing as well not just being an independent research but even when I was a

710
00:56:34,160 --> 00:56:40,080
researcher who was funded I don't like the idea that the research in the community is just

711
00:56:40,080 --> 00:56:45,360
becoming more and more predicated on you having access to all this equipment I think there are

712
00:56:45,360 --> 00:56:50,880
fascinating questions that you know your listeners or even like the high school students I teach

713
00:56:50,880 --> 00:56:55,680
in Australia anyone else would be able to ask and actually answer in a completely scientific

714
00:56:55,680 --> 00:57:01,280
and fascinating way but only if we keep making sure that these models are actually

715
00:57:01,840 --> 00:57:08,000
inner trainable the purchable applicable to the normal person and a lot of these pre-trained

716
00:57:08,000 --> 00:57:13,600
language models that were being getting recently where the training that for example GPT2

717
00:57:13,600 --> 00:57:20,640
the Salesforce reaches such as language model as well many of these these different

718
00:57:20,640 --> 00:57:24,400
approaches they're no longer releasing the training code for it so you get these huge

719
00:57:25,280 --> 00:57:32,880
model blogs you know GP2 GPT2 excel what have you but no one's necessarily able to reproduce it

720
00:57:32,880 --> 00:57:38,240
so any research that's on top of that is predicated on either that company or those research

721
00:57:38,240 --> 00:57:45,600
organizations continuing to release those results of those models or again on the idea that

722
00:57:45,600 --> 00:57:49,200
you don't have control of the underlying architecture like you can't decide whether it's a

723
00:57:49,200 --> 00:57:54,640
transformer or an LSTM or whatever else you have to live with whatever else is put out in front of

724
00:57:54,640 --> 00:58:03,840
you and I don't know it just I I I'm ready to admit one day that you know we have to like

725
00:58:03,840 --> 00:58:10,160
focus on one direction or you know the compute resources do indeed get this better result and

726
00:58:10,160 --> 00:58:13,920
you know we can't try all these different things but I don't think that day is yet and I don't

727
00:58:13,920 --> 00:58:21,360
think that day is necessarily soon well thanks so much for taking the time to share what you're up to

728
00:58:21,360 --> 00:58:27,040
and talk through this paper really interesting stuff and I am really looking forward to seeing

729
00:58:27,040 --> 00:58:36,560
what the community does and what you do to kind of extend it all right everyone that is our show

730
00:58:36,560 --> 00:58:44,160
for today for more information on today's show including our guest heads to tomalei.com slash shows

731
00:58:44,800 --> 00:58:51,520
once again the co-first introduction to NLP study group begins this Saturday December 14th

732
00:58:51,520 --> 00:58:59,120
head to tomalei.com slash community to get signed up now thanks for listening and we'll see you

733
00:58:59,120 --> 00:59:29,040
next week.

