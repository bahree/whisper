1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,720
I'm your host Sam Charrington.

4
00:00:31,720 --> 00:00:36,520
A few weeks ago I had a chance to attend the TensorFlow Developer Summit on the Google

5
00:00:36,520 --> 00:00:39,520
Cloud campus in Sunnyvale, California.

6
00:00:39,520 --> 00:00:43,800
I had a great time at the summit, meaning a bunch of Twimble listeners and friends of the

7
00:00:43,800 --> 00:00:50,040
show and learning about all the new features released as part of the TensorFlow 2.0 Alpha.

8
00:00:50,040 --> 00:00:53,880
This week on the show, we'll be bringing you a series of conversations from the event,

9
00:00:53,880 --> 00:00:58,040
which we'll be kicking off with today's interview with Paige Bailey, a TensorFlow Developer

10
00:00:58,040 --> 00:01:00,160
Advocate at Google.

11
00:01:00,160 --> 00:01:04,280
Paige and I sat down to talk through the latest TensorFlow updates and we cover a ton of

12
00:01:04,280 --> 00:01:09,920
ground, including the evolution of the TensorFlow APIs and the role of Eager mode, TF.Kiris

13
00:01:09,920 --> 00:01:15,600
and TF.Function, the introduction of TensorFlow for Swift and its inclusion in the new fast.ai

14
00:01:15,600 --> 00:01:22,760
course, new updates to TFX or TensorFlow Extended, Google's end-to-end machine learning platform,

15
00:01:22,760 --> 00:01:28,800
and the emphasis on community collaboration with TensorFlow 2.0 and a bunch more.

16
00:01:28,800 --> 00:01:34,040
Now Dev Summit attendees received an awesome gift box from Google that included some really

17
00:01:34,040 --> 00:01:38,400
fun toys that will appeal to the hackers and builders in our audience, including the

18
00:01:38,400 --> 00:01:45,480
new Coral Edge TPU device and the Spark Fun Edge Development Board powered by TensorFlow.

19
00:01:45,480 --> 00:01:49,600
If you're a geek like me and get excited about toys like this, I'm excited to share that

20
00:01:49,600 --> 00:01:54,160
we've got our hands on a few of these for you, our dedicated listeners.

21
00:01:54,160 --> 00:02:00,480
To enter to win one, just visit twimmelai.com slash TF giveaway and let us know what you

22
00:02:00,480 --> 00:02:03,320
would do with this kit if you got your hands on it.

23
00:02:03,320 --> 00:02:07,680
Of course, we'd like to send a huge thanks to the TensorFlow team for helping us bring

24
00:02:07,680 --> 00:02:10,880
you this podcast series and giveaway.

25
00:02:10,880 --> 00:02:14,200
With all the great announcements coming out of the TensorFlow Dev Summit, including the

26
00:02:14,200 --> 00:02:20,400
2.0 Alpha, you should definitely check out the latest and greatest over at TensorFlow.org,

27
00:02:20,400 --> 00:02:23,880
where you can also download and start building with the framework.

28
00:02:23,880 --> 00:02:26,680
And now on to the show.

29
00:02:26,680 --> 00:02:31,520
All right, everyone.

30
00:02:31,520 --> 00:02:32,920
I am here with Page Bailey.

31
00:02:32,920 --> 00:02:38,680
Page is a developer advocate for TensorFlow with Google, and we're here at the TensorFlow

32
00:02:38,680 --> 00:02:39,680
Developer Summit.

33
00:02:39,680 --> 00:02:41,880
Page, welcome to this week in machine learning and AI.

34
00:02:41,880 --> 00:02:42,880
Excellent.

35
00:02:42,880 --> 00:02:43,880
I'm delighted to be here.

36
00:02:43,880 --> 00:02:48,920
I'm delighted to have you on the show and looking forward to diving into all that's new and

37
00:02:48,920 --> 00:02:50,680
interesting with TensorFlow.

38
00:02:50,680 --> 00:02:54,440
But before we do that, a little about your background, how did you get started working

39
00:02:54,440 --> 00:02:55,440
and all this stuff?

40
00:02:55,440 --> 00:02:56,440
Yeah.

41
00:02:56,440 --> 00:03:01,480
So I got started working in machine learning and data science before data science was

42
00:03:01,480 --> 00:03:05,160
really a term, I guess.

43
00:03:05,160 --> 00:03:13,520
My background in undergrad was very geophysics applied math, I did research in planetary

44
00:03:13,520 --> 00:03:21,600
science, doing data analysis on large amounts of data from NASA equipment, so I got to

45
00:03:21,600 --> 00:03:26,200
work at places like the laboratory for atmospheric and space physics and Southwest Research Institute

46
00:03:26,200 --> 00:03:30,160
doing lunar ultraviolet research.

47
00:03:30,160 --> 00:03:35,560
And that today, doing statistical analysis on multivariate data would be called data science.

48
00:03:35,560 --> 00:03:39,560
But back then, it was just called research.

49
00:03:39,560 --> 00:03:42,480
So that was when the bug first started.

50
00:03:42,480 --> 00:03:48,640
And then I started doing machine learning for work when I worked at Chevron for predictive

51
00:03:48,640 --> 00:03:51,560
modeling in the oil and gas industry.

52
00:03:51,560 --> 00:03:56,680
So I guess in terms of data science things, I've been doing it for about 10 years for machine

53
00:03:56,680 --> 00:03:58,800
learning about five to six.

54
00:03:58,800 --> 00:04:01,560
Did you jump from Chevron to Google?

55
00:04:01,560 --> 00:04:02,560
No.

56
00:04:02,560 --> 00:04:07,760
So that was an interesting sort of career progression, I guess.

57
00:04:07,760 --> 00:04:12,440
I had done planetary science research as an undergrad for my first two internships and

58
00:04:12,440 --> 00:04:15,240
then my third was with Chevron.

59
00:04:15,240 --> 00:04:21,320
And they had me on a really cool project I got to create a database and do some scripting,

60
00:04:21,320 --> 00:04:24,480
some geospatial analysis using Python.

61
00:04:24,480 --> 00:04:28,200
And after it, they said, hey, do you want to come work for us?

62
00:04:28,200 --> 00:04:32,680
And that sounded like a cool job opportunity, especially since they were offering to pay

63
00:04:32,680 --> 00:04:34,800
for grad school.

64
00:04:34,800 --> 00:04:39,280
Because I went to go work there for about five years.

65
00:04:39,280 --> 00:04:47,560
And after Chevron, it was time to kind of leave and to explore new opportunities.

66
00:04:47,560 --> 00:04:48,960
And that was Microsoft.

67
00:04:48,960 --> 00:04:54,360
So I worked for Microsoft doing machine learning developer advocacy and then as a senior software

68
00:04:54,360 --> 00:04:56,720
engineer in the office of the Azure CTO.

69
00:04:56,720 --> 00:05:00,680
For those that aren't familiar, what exactly is a developer advocate and how does that translate

70
00:05:00,680 --> 00:05:02,840
into how you spend your day-to-day?

71
00:05:02,840 --> 00:05:03,840
Absolutely.

72
00:05:03,840 --> 00:05:05,560
That is a great question.

73
00:05:05,560 --> 00:05:10,240
And developer advocacy means different things to different companies.

74
00:05:10,240 --> 00:05:18,720
Here at Google, developer advocacy is very focused on improving the experience for our end

75
00:05:18,720 --> 00:05:22,920
users and for basically anybody who would go about using our tool.

76
00:05:22,920 --> 00:05:28,600
And that can include everything from improving documentation to user experience research,

77
00:05:28,600 --> 00:05:33,320
to building tutorials and quick starts, to creating curriculum materials like the Udacity

78
00:05:33,320 --> 00:05:40,240
and Deep Learning AI courses that we just released, to also engaging with the community via

79
00:05:40,240 --> 00:05:44,280
social media, but also through things like special interest groups and making sure that

80
00:05:44,280 --> 00:05:49,680
if there is a problem with the tool, it's surfaced, fed back to the engineering team and we can

81
00:05:49,680 --> 00:05:53,200
build a plan to resolve whatever that issue happens to be.

82
00:05:53,200 --> 00:05:59,560
So if anything is frustrating, so if maybe a higher level API isn't intuitive, it might

83
00:05:59,560 --> 00:06:07,000
require making a pull request and resolving that or say we're missing a symbol in our

84
00:06:07,000 --> 00:06:14,120
documentation, then making a pull request and fixing that in the docs.

85
00:06:14,120 --> 00:06:20,960
So it's really a variety of things, but as I mentioned, it's all very focused on communication,

86
00:06:20,960 --> 00:06:28,600
improving the developer experience, education, and also very much engineering tasks.

87
00:06:28,600 --> 00:06:32,320
And if you're a person who enjoys all of the above, then maybe developer advocacy is

88
00:06:32,320 --> 00:06:33,320
right for you.

89
00:06:33,320 --> 00:06:34,320
Awesome.

90
00:06:34,320 --> 00:06:35,320
Awesome.

91
00:06:35,320 --> 00:06:40,160
So, there was a bunch of interesting TensorFlow news yesterday.

92
00:06:40,160 --> 00:06:42,640
Yes, there was so much.

93
00:06:42,640 --> 00:06:46,680
It felt like a fire hole was almost and that's coming from so many new, it was all about

94
00:06:46,680 --> 00:06:47,680
to happen.

95
00:06:47,680 --> 00:06:49,680
So many new products.

96
00:06:49,680 --> 00:06:50,680
Yep.

97
00:06:50,680 --> 00:06:56,040
Yeah, so I'm really interested in kind of digging into them, one of the things that I've

98
00:06:56,040 --> 00:07:00,160
mentioned this to a couple of times, maybe yesterday, one of the things that's been a little

99
00:07:00,160 --> 00:07:05,520
bit difficult for me to parse is an outsider is like, what exactly is new and all of these

100
00:07:05,520 --> 00:07:07,240
things that are kind of new?

101
00:07:07,240 --> 00:07:13,120
Like, eager mode was highlighted very extensively yesterday, but that was announced last year.

102
00:07:13,120 --> 00:07:17,320
There were other things that I'm like, I know I've seen this before, there was a paper

103
00:07:17,320 --> 00:07:23,480
that was a couple of years ago, you know, so for me, you know, probably the best thing

104
00:07:23,480 --> 00:07:26,480
to do is to have you kind of walk through from your perspective, you know, what were the

105
00:07:26,480 --> 00:07:31,000
highlights and what was new and then we can kind of dig in from there.

106
00:07:31,000 --> 00:07:32,000
Right.

107
00:07:32,000 --> 00:07:41,000
So, it is difficult to kind of place what's actually new and then what's just been kind

108
00:07:41,000 --> 00:07:42,680
of bundled and packaged, right?

109
00:07:42,680 --> 00:07:49,280
So the biggest announcement yesterday by far was the alpha release of TensorFlow 2.0.

110
00:07:49,280 --> 00:07:57,280
And TensorFlow 2.0 is a new version of TensorFlow focused on developer productivity, ease of

111
00:07:57,280 --> 00:08:04,400
use, carous as the recommended higher level API, an eager execution by default, which enables

112
00:08:04,400 --> 00:08:13,280
you to do things like better debugging, but also the pathonic experience that you, especially

113
00:08:13,280 --> 00:08:18,960
folks coming from scikit-learn and the data science community would expect.

114
00:08:18,960 --> 00:08:25,800
So TensorFlow 2.0 is, I mentioned it's eager execution by default, and you're correct

115
00:08:25,800 --> 00:08:31,320
in that eager execution was announced last year, but in order to enable it, you had to

116
00:08:31,320 --> 00:08:36,320
do something called like tf.enable eager execution at the top of your script.

117
00:08:36,320 --> 00:08:41,240
So you had to kind of specify that at the beginning and you had the option of sticking

118
00:08:41,240 --> 00:08:47,120
in graph mode if you didn't enable eager execution, tf2.0 is eager by default, which means

119
00:08:47,120 --> 00:08:54,080
that you don't have to have that line, but you're thrown directly into this experience

120
00:08:54,080 --> 00:08:58,880
where you don't have to create a static graph, you don't have to have sessions, you don't

121
00:08:58,880 --> 00:09:03,280
have to create the thing before you can interact with it, before you can actually start using

122
00:09:03,280 --> 00:09:04,280
it.

123
00:09:04,280 --> 00:09:08,880
It feels much more pathonic.

124
00:09:08,880 --> 00:09:14,920
So that's kind of a new thing in the eager execution by default.

125
00:09:14,920 --> 00:09:20,360
Another very, very new thing is that TensorFlow 2.0 was created entirely in collaboration with

126
00:09:20,360 --> 00:09:22,760
the community through the RFC process.

127
00:09:22,760 --> 00:09:27,880
So every change that was made to the API had to be proposed by an engineer or proposed

128
00:09:27,880 --> 00:09:36,040
by someone endorsed by the entire community, discussed, and before it was incorporated.

129
00:09:36,040 --> 00:09:42,380
So if you have any curiosity about these RFCs, you can go and take a look at it on github.com

130
00:09:42,380 --> 00:09:44,520
slash TensorFlow slash community.

131
00:09:44,520 --> 00:09:49,000
We have a list of all of them, and if you care about details, they're all important.

132
00:09:49,000 --> 00:09:53,000
We also have a community of special interest groups, everything from TensorBoard, which

133
00:09:53,000 --> 00:09:59,440
is a visualization tool, to build, which is sort of building TensorFlow for various Linux

134
00:09:59,440 --> 00:10:04,720
distributions, and then also with Nvidia graphics card strivers and the like.

135
00:10:04,720 --> 00:10:07,600
We have a special interest group for testing.

136
00:10:07,600 --> 00:10:12,400
So if you're trying out TensorFlow 2.0 and you're running into Snags, or if you need migration

137
00:10:12,400 --> 00:10:15,520
support, there's that option for you as well.

138
00:10:15,520 --> 00:10:20,640
And then others are on networking and data ingestion and lots and lots of different ways

139
00:10:20,640 --> 00:10:21,880
to get involved.

140
00:10:21,880 --> 00:10:28,720
And really, as I mentioned, the focus on TF2.0 is community, sort of building something

141
00:10:28,720 --> 00:10:34,760
that people would love to use, as opposed to the first TensorFlow release, which kind

142
00:10:34,760 --> 00:10:40,120
of felt like, hey, here's some code, like go forth and prosper, okay?

143
00:10:40,120 --> 00:10:46,320
And not exactly the most straightforward sort of code, so that's the general idea.

144
00:10:46,320 --> 00:10:53,680
A lot of the kind of top line takeaway is around kind of APIs and developer experience.

145
00:10:53,680 --> 00:10:59,440
Eager by default, you can kind of think of that as, okay, so there's one line of code

146
00:10:59,440 --> 00:11:01,000
less that I have to write.

147
00:11:01,000 --> 00:11:08,240
But is it deeper than that in terms of the underlying architecture and the way that TensorFlow

148
00:11:08,240 --> 00:11:09,240
supports that?

149
00:11:09,240 --> 00:11:10,600
That's a great question.

150
00:11:10,600 --> 00:11:11,600
And absolutely.

151
00:11:11,600 --> 00:11:18,080
So Carmel, during the keynote yesterday, she had this great slide, where, so if you're

152
00:11:18,080 --> 00:11:24,720
a carous developer, if you have experience using TF.carous, you can create a neural network

153
00:11:24,720 --> 00:11:26,480
with about 10 lines of code.

154
00:11:26,480 --> 00:11:28,280
And it's very straightforward.

155
00:11:28,280 --> 00:11:29,280
It's intuitive.

156
00:11:29,280 --> 00:11:31,480
It's easy to understand.

157
00:11:31,480 --> 00:11:33,520
And again, feels very private.

158
00:11:33,520 --> 00:11:38,640
She showed a slide saying, here's what your model would look like if you're using TensorFlow

159
00:11:38,640 --> 00:11:44,200
like 113 or TensorFlow 112 or whatever version.

160
00:11:44,200 --> 00:11:45,960
And it's 10 lines of carous.

161
00:11:45,960 --> 00:11:48,880
And she said, and here's what it would look like in TensorFlow 2.0.

162
00:11:48,880 --> 00:11:50,480
It's the same code.

163
00:11:50,480 --> 00:11:52,560
The same code expressed both ways.

164
00:11:52,560 --> 00:11:58,520
But the reality is that TF2.0 is doing a lot of things under the hood to improve model

165
00:11:58,520 --> 00:12:06,920
performance and to take that same carous code and make it scalable in a way that it wasn't

166
00:12:06,920 --> 00:12:09,080
experienced before.

167
00:12:09,080 --> 00:12:14,160
So even though the code looks exactly the same, there are a lot of optimizations under

168
00:12:14,160 --> 00:12:20,200
the hood that makes it a bit more performant, especially with distributed architecture,

169
00:12:20,200 --> 00:12:22,360
so with distribution strategies.

170
00:12:22,360 --> 00:12:28,480
One of the things that was presented yesterday was there was a fair amount of detail that

171
00:12:28,480 --> 00:12:30,960
was gone into around TF.function.

172
00:12:30,960 --> 00:12:32,800
Is that totally new?

173
00:12:32,800 --> 00:12:35,800
Yeah, TF.function is new.

174
00:12:35,800 --> 00:12:44,120
TF.function is basically a way for you to take any sort of Python function, rapid

175
00:12:44,120 --> 00:12:48,120
and define it as a core TensorFlow operation.

176
00:12:48,120 --> 00:12:54,440
So you could have anything from adding two numbers, so you could have a function that would

177
00:12:54,440 --> 00:13:00,440
take, you would define something that would add A and B. And if you preface it with an

178
00:13:00,440 --> 00:13:05,600
at TF function, it would define it as a core TensorFlow operation that you could then

179
00:13:05,600 --> 00:13:08,160
use in your graph.

180
00:13:08,160 --> 00:13:10,880
That is pretty new.

181
00:13:10,880 --> 00:13:16,920
People have had a lot of positive feedback about autograph and TF function both when using

182
00:13:16,920 --> 00:13:23,320
TensorFlow 2.0, and it makes it a lot easier to take things that traditionally would have

183
00:13:23,320 --> 00:13:32,480
been very difficult to do with static computation, static graphs and sort of use them in a friendly

184
00:13:32,480 --> 00:13:33,480
dynamic way.

185
00:13:33,480 --> 00:13:34,480
Okay.

186
00:13:34,480 --> 00:13:36,000
And what is autograph?

187
00:13:36,000 --> 00:13:39,720
Autograph helps you write complicated graph code using normal Python.

188
00:13:39,720 --> 00:13:44,680
So it automatically transforms your code into the equivalent TensorFlow graph code, and

189
00:13:44,680 --> 00:13:48,160
it supports a lot of the Python language.

190
00:13:48,160 --> 00:13:53,680
So that is that's a kind of one-liner explanation of autograph.

191
00:13:53,680 --> 00:14:01,640
2.0 introduces some significant changes to the API, but that's not the only kind of developer

192
00:14:01,640 --> 00:14:04,840
facing developer experience kind of announcement.

193
00:14:04,840 --> 00:14:09,680
One of the ones that's gotten a lot of conversation going is around the Swift for TensorFlow

194
00:14:09,680 --> 00:14:14,440
and curious what your thought, do you deduce, do you know Swift?

195
00:14:14,440 --> 00:14:16,440
I have to play with that at all.

196
00:14:16,440 --> 00:14:24,080
So I have fiddled with it, but only since I got to Google, I hadn't really experimented

197
00:14:24,080 --> 00:14:30,120
with Swift before kind of, well, I get to sit next to Chris Latner, which is Chris

198
00:14:30,120 --> 00:14:32,600
Latner and Brennan and the entire Swift team.

199
00:14:32,600 --> 00:14:35,160
So that's kind of rad.

200
00:14:35,160 --> 00:14:43,560
And the idea behind Swift for TensorFlow is that if you're a Python developer, Python

201
00:14:43,560 --> 00:14:46,720
is great in terms of user experience.

202
00:14:46,720 --> 00:14:52,760
As a user interface, as a programming language, it's a great way to build out products.

203
00:14:52,760 --> 00:14:55,720
But it also has a lot of limitations and drawbacks.

204
00:14:55,720 --> 00:15:02,480
So for example, gradient tape is something that causes a great deal of frustration whenever

205
00:15:02,480 --> 00:15:04,240
you're using TensorFlow.

206
00:15:04,240 --> 00:15:05,240
gradient tape?

207
00:15:05,240 --> 00:15:06,240
gradient tape.

208
00:15:06,240 --> 00:15:07,240
What is that?

209
00:15:07,240 --> 00:15:12,280
It's how you deal with variables as you're creating your graph.

210
00:15:12,280 --> 00:15:18,200
And so as a Python developer, it's often very frustrating to try to manage all of these

211
00:15:18,200 --> 00:15:20,520
different aspects.

212
00:15:20,520 --> 00:15:31,320
But with a lower level tool like Swift, a language that's a little bit closer to C++,

213
00:15:31,320 --> 00:15:39,080
you get afforded a lot more control over what you're creating.

214
00:15:39,080 --> 00:15:40,560
And that's kind of the idea.

215
00:15:40,560 --> 00:15:45,240
And I think that's what captured the imagination of Jeremy Howard.

216
00:15:45,240 --> 00:15:49,080
So I'm sure you saw that in Alzheimer's as well.

217
00:15:49,080 --> 00:15:54,920
He's using Swift for TensorFlow for his latest iteration of the FAST AI curriculum, which

218
00:15:54,920 --> 00:15:56,680
we're all very excited about.

219
00:15:56,680 --> 00:16:03,080
And I'm very curious to see how that progresses.

220
00:16:03,080 --> 00:16:08,020
Another really cool thing about Swift is, or Swift for TensorFlow rather, is that you

221
00:16:08,020 --> 00:16:13,520
can use Python within Swift just within port Python.

222
00:16:13,520 --> 00:16:21,640
And there is a great demonstration from Brennan and from Chris yesterday where you could create

223
00:16:21,640 --> 00:16:28,560
something like a plot of performance or accuracy over time with Matplotlib, just as simply

224
00:16:28,560 --> 00:16:33,440
as you would with Python, except all you have to do is practice it with LAT for your

225
00:16:33,440 --> 00:16:34,440
variable.

226
00:16:34,440 --> 00:16:40,560
So I think Swift for TensorFlow is a really interesting space.

227
00:16:40,560 --> 00:16:44,720
They're still very new, so they're still developing their story and they're still developing

228
00:16:44,720 --> 00:16:46,520
the product.

229
00:16:46,520 --> 00:16:50,000
But I'm curious to see how it progresses this next year.

230
00:16:50,000 --> 00:16:52,040
And they certainly have a vibrant community.

231
00:16:52,040 --> 00:16:58,640
If you have any, I mentioned the special interest groups a little bit before, but they also

232
00:16:58,640 --> 00:17:04,240
have these really active mailing lists where people are asking questions and sharing products

233
00:17:04,240 --> 00:17:10,800
that they've created and sort of ideas that they have about changing designs of API and

234
00:17:10,800 --> 00:17:12,800
technical things.

235
00:17:12,800 --> 00:17:19,960
And the Swift for TensorFlow user group and mailing list is consistently one of the most

236
00:17:19,960 --> 00:17:20,960
active.

237
00:17:20,960 --> 00:17:28,320
So if you have curiosity about it, absolutely join and send your questions to swiftittensorflow.org.

238
00:17:28,320 --> 00:17:29,320
Awesome.

239
00:17:29,320 --> 00:17:30,320
Awesome.

240
00:17:30,320 --> 00:17:31,320
Yeah.

241
00:17:31,320 --> 00:17:35,880
People keep asking me like what I think about it and like, I don't know, I've never seen

242
00:17:35,880 --> 00:17:40,400
Swift outside of a slide on the presentation today.

243
00:17:40,400 --> 00:17:41,400
Yeah.

244
00:17:41,400 --> 00:17:48,800
It's very, when I first, because it was announced last year, Swift for TensorFlow, but in

245
00:17:48,800 --> 00:17:50,800
a very sort of low key way, right?

246
00:17:50,800 --> 00:17:51,800
Okay.

247
00:17:51,800 --> 00:17:52,800
I missed that.

248
00:17:52,800 --> 00:17:53,800
Yeah.

249
00:17:53,800 --> 00:17:54,800
It was a presentation.

250
00:17:54,800 --> 00:17:59,280
It was also the best non-leak of TensorFlow history.

251
00:17:59,280 --> 00:18:05,720
I think like nobody anticipated that it was coming, but so it was announced last year.

252
00:18:05,720 --> 00:18:12,040
And all of the progress that you've seen since has been made since that initial announcement.

253
00:18:12,040 --> 00:18:13,040
But yeah.

254
00:18:13,040 --> 00:18:17,520
Like usually you would think Swift and think, you know, iOS or, you know, something.

255
00:18:17,520 --> 00:18:18,760
That's exactly what I think.

256
00:18:18,760 --> 00:18:19,760
Yeah.

257
00:18:19,760 --> 00:18:20,760
Yeah.

258
00:18:20,760 --> 00:18:25,200
You know, I tell people, I'm kind of excited about it because Jeremy's excited.

259
00:18:25,200 --> 00:18:31,280
Part two of the new course is starting up soon and sounds like he and Chris will be collaborating

260
00:18:31,280 --> 00:18:34,400
on a couple of lessons in that course to kind of highlight.

261
00:18:34,400 --> 00:18:35,400
Yeah.

262
00:18:35,400 --> 00:18:40,840
Well, not just kind of highlight the Swift for TensorFlow, but also this may be a little

263
00:18:40,840 --> 00:18:46,680
bit of kind of inside baseball or whatever, but like the way that the way that he builds

264
00:18:46,680 --> 00:18:51,920
the courses is basically by building out the framework, right?

265
00:18:51,920 --> 00:18:55,840
And that becomes the process of building out the framework becomes the course.

266
00:18:55,840 --> 00:19:01,720
And so it sounds like what he and Chris will be doing in this course is like starting

267
00:19:01,720 --> 00:19:12,360
to build out the fast.ai Swift version as kind of part of this course is really fascinating

268
00:19:12,360 --> 00:19:15,680
kind of approach for multiple reasons.

269
00:19:15,680 --> 00:19:20,800
I took a look at his blog post about kind of why he was so excited about Swift and one

270
00:19:20,800 --> 00:19:27,360
of the reasons that that he's real playing about is like, this is the ground floor opportunity

271
00:19:27,360 --> 00:19:33,400
like Swift, you know, TensorFlow or rather Python, you know, they're all of these challenges

272
00:19:33,400 --> 00:19:40,320
associated with like the barrier between Python and C when you have to get really deep

273
00:19:40,320 --> 00:19:44,360
and not being able to debug all the way through and not be able to trace variables all

274
00:19:44,360 --> 00:19:46,960
the way through that kind of thing.

275
00:19:46,960 --> 00:19:53,160
And needing to kind of shim out underneath Python to get performance and one of the reasons

276
00:19:53,160 --> 00:19:58,160
why he's excited about Swift is because it is high performance.

277
00:19:58,160 --> 00:20:03,880
And you know, to the extent that over time kind of the entire stat gets built in a high

278
00:20:03,880 --> 00:20:08,840
performance language, you don't have these barriers that you have to figure out.

279
00:20:08,840 --> 00:20:09,840
Absolutely.

280
00:20:09,840 --> 00:20:17,480
Like it's a fascinating space and also so one of the things that shocked me the most

281
00:20:17,480 --> 00:20:23,960
I guess about Swift was how intuitive it was to understand as a language.

282
00:20:23,960 --> 00:20:29,720
Like if you look at the Swift code used to generate a neural network, it feels very

283
00:20:29,720 --> 00:20:32,880
similar to Keras.

284
00:20:32,880 --> 00:20:39,320
It's not because I took some people as well as classes in college and I am not a fan

285
00:20:39,320 --> 00:20:40,960
and will not be shy about saying that.

286
00:20:40,960 --> 00:20:46,720
Like I love the idea of it conceptually as a language, but it's something that's very

287
00:20:46,720 --> 00:20:49,400
it doesn't feel natural for me to use.

288
00:20:49,400 --> 00:20:51,920
Swift feels like a great deal more natural.

289
00:20:51,920 --> 00:20:59,880
And one of the challenges I think that they're going to face is the that Swift as you mentioned

290
00:20:59,880 --> 00:21:04,320
hasn't traditionally been a data sciencey language.

291
00:21:04,320 --> 00:21:10,400
So they don't have all of the vast libraries of tools that Python has.

292
00:21:10,400 --> 00:21:15,840
So things like matplotlib and psychic learning for traditional machine learning tasks and

293
00:21:15,840 --> 00:21:17,640
the like.

294
00:21:17,640 --> 00:21:23,400
But it's really interesting and that so we're doing Google Summer of Code for the very

295
00:21:23,400 --> 00:21:25,680
first time this year.

296
00:21:25,680 --> 00:21:31,480
And five of our projects or five of our proposed projects are Swift for TensorFlow related.

297
00:21:31,480 --> 00:21:37,320
Summer of Code for those that don't know as a program where open source projects can get

298
00:21:37,320 --> 00:21:42,440
matched with like college students or students in general maybe and get some funding to pay

299
00:21:42,440 --> 00:21:44,000
them to work on the project.

300
00:21:44,000 --> 00:21:45,000
Absolutely.

301
00:21:45,000 --> 00:21:48,680
So the Google Summer of Code is one of my favorite things about Google.

302
00:21:48,680 --> 00:21:55,880
And it was also when I applied for it a long time ago when I was still doing the grad school

303
00:21:55,880 --> 00:21:56,880
thing.

304
00:21:56,880 --> 00:22:03,640
I applied to a project called AIMA and got a response back from Peter Norvig who was

305
00:22:03,640 --> 00:22:09,840
apparently the person who who was sponsoring the project and that was my first email from

306
00:22:09,840 --> 00:22:10,840
a Googler.

307
00:22:10,840 --> 00:22:11,840
And it was Peter Norvig.

308
00:22:11,840 --> 00:22:12,840
Wow.

309
00:22:12,840 --> 00:22:13,840
Yeah.

310
00:22:13,840 --> 00:22:14,840
It was amazing.

311
00:22:14,840 --> 00:22:22,320
But it's it's so enchanting in that open source projects can apply to be part of this

312
00:22:22,320 --> 00:22:29,040
GSOC program if they're selected that they can have a partnership of mentors who are

313
00:22:29,040 --> 00:22:32,760
directly working on the project and students.

314
00:22:32,760 --> 00:22:38,360
The mentors work collaboratively with the students, doesn't matter where they're located.

315
00:22:38,360 --> 00:22:42,760
And it's there's certainly encouraged to be remote.

316
00:22:42,760 --> 00:22:47,000
And the students are kind of guided through making their first substantial open source

317
00:22:47,000 --> 00:22:51,760
contribution and also paid for it, which you know as a student that's kind of like a

318
00:22:51,760 --> 00:22:52,760
dream come true, right?

319
00:22:52,760 --> 00:22:56,520
Like you get to work on open source, you get to be paid and potentially you get to be

320
00:22:56,520 --> 00:23:00,160
mentored by like in the Swifferents for TensorFlow case.

321
00:23:00,160 --> 00:23:05,840
Like you would be mentored by Chris Ladner or yeah, or the AIMA case you would be mentored

322
00:23:05,840 --> 00:23:10,720
by Peter Norvig or for the rest of our for the rest of our project opportunities, right?

323
00:23:10,720 --> 00:23:18,000
We've got autograph and TF function and sort of TensorFlow data sets, you're mentored

324
00:23:18,000 --> 00:23:23,440
directly by the often the people, the engineers who've created those products.

325
00:23:23,440 --> 00:23:30,360
So at least in TensorFlow's case and in the other projects that are sponsored, it's really

326
00:23:30,360 --> 00:23:32,480
a phenomenal opportunity.

327
00:23:32,480 --> 00:23:36,240
And I'm excited to see what the GSOC students build out for Swiffer TensorFlow.

328
00:23:36,240 --> 00:23:38,240
Interesting.

329
00:23:38,240 --> 00:23:44,400
You mentioned something earlier that kind of tied to a question that I had in kind of seeing

330
00:23:44,400 --> 00:23:50,560
the Swiffer TensorFlow announcement, how would a Swiffer TensorFlow kind of play or not

331
00:23:50,560 --> 00:23:52,400
play with a carous?

332
00:23:52,400 --> 00:23:53,400
Yes.

333
00:23:53,400 --> 00:24:00,680
So Swiffer TensorFlow, like you can use carous within it, but for the most part it's like

334
00:24:00,680 --> 00:24:03,000
you can use any Python within Swift.

335
00:24:03,000 --> 00:24:09,840
Yes, but it's recommended that you that you would program using Swift.

336
00:24:09,840 --> 00:24:14,880
But again, the Swift syntax looks very, very similar to carous syntax.

337
00:24:14,880 --> 00:24:20,320
And we can, if folks are interested, we can share some of the Colab notebooks that like

338
00:24:20,320 --> 00:24:23,240
links to it that were shared yesterday, right?

339
00:24:23,240 --> 00:24:26,600
But yeah, I would love to see that.

340
00:24:26,600 --> 00:24:31,080
And another really cool thing about Swift for TensorFlow is that it works in Jupyter

341
00:24:31,080 --> 00:24:34,200
notebooks, it works in Google Colab.

342
00:24:34,200 --> 00:24:38,360
So really the notebook experience, if you're a data scientist, feels very familiar as

343
00:24:38,360 --> 00:24:39,360
well.

344
00:24:39,360 --> 00:24:45,960
And the big kind of announcements in addition to kind of the under the cover stuff was kind

345
00:24:45,960 --> 00:24:54,600
of a rationalization of the APIs, it sounds like it's more than just, hey, curious is the

346
00:24:54,600 --> 00:25:01,880
default and more than even, I don't know the right way to ask this, but we're getting

347
00:25:01,880 --> 00:25:02,880
organized.

348
00:25:02,880 --> 00:25:04,880
Is that what it is?

349
00:25:04,880 --> 00:25:09,840
I think so, or at least that's what it feels like.

350
00:25:09,840 --> 00:25:15,560
So the original, I started using TensorFlow, like not for real substantial work, but just

351
00:25:15,560 --> 00:25:21,000
for, you know, personal projects and prototyping things in 2015 when it was released.

352
00:25:21,000 --> 00:25:26,400
And it was, oh man, like it was, it was not fun, like it was, it was not good at all.

353
00:25:26,400 --> 00:25:31,920
Like TensorFlow, the original version, like you, you had to write so much boilerplate in

354
00:25:31,920 --> 00:25:35,240
order to get something to work.

355
00:25:35,240 --> 00:25:40,920
And then since the initial release, it's just, it had grown just sort of exponentially.

356
00:25:40,920 --> 00:25:47,960
We had this module called Contrib, TF Contrib that was kind of a generalized catch-all for

357
00:25:47,960 --> 00:25:50,320
things that folks wanted to add.

358
00:25:50,320 --> 00:25:55,880
So if you were a grad student and you had created, you know, a collection of loss functions,

359
00:25:55,880 --> 00:26:02,040
you would, you would add those in Contrib or we had TF dot Contrib dot GAN, which was

360
00:26:02,040 --> 00:26:11,600
really sort of beloved, you know, GAN, GAN creation tooling for like a straightforward

361
00:26:11,600 --> 00:26:14,200
path to creating GANs using TensorFlow.

362
00:26:14,200 --> 00:26:18,080
And we'll still have it just migrated to its own repo.

363
00:26:18,080 --> 00:26:23,360
But the, but the sad reality of Contrib was that none of those contributions had a support

364
00:26:23,360 --> 00:26:24,360
plan.

365
00:26:24,360 --> 00:26:28,440
None of them had a defined owner, so like a proxy maintainer.

366
00:26:28,440 --> 00:26:33,680
And a lot of them may be functioned in an earlier version of TensorFlow, but failed to work

367
00:26:33,680 --> 00:26:34,680
later.

368
00:26:34,680 --> 00:26:39,520
And there was no clear way to understand what worked, what didn't, what was a duplicate,

369
00:26:39,520 --> 00:26:45,280
and like how would you migrate from what, like a symbol to the core API.

370
00:26:45,280 --> 00:26:46,880
So that really wasn't defined.

371
00:26:46,880 --> 00:26:53,120
We also had a lot of mathematical operations and sort of more traditional statistics capabilities

372
00:26:53,120 --> 00:26:58,960
that were in various places in the API, but not really kind of structured and grouped together.

373
00:26:58,960 --> 00:27:03,800
And it was, gosh, I don't want to, I don't want to count how many symbols there were,

374
00:27:03,800 --> 00:27:05,360
but there were certainly thousands.

375
00:27:05,360 --> 00:27:11,040
There were thousands of symbols without any clear structure or consistent aiming conventions

376
00:27:11,040 --> 00:27:17,440
and a lot of duplication and a lot of sort of things that kind of failed to work together

377
00:27:17,440 --> 00:27:19,240
nicely.

378
00:27:19,240 --> 00:27:23,560
So now we've taken a more modularized approach.

379
00:27:23,560 --> 00:27:27,960
So all of the statistics and mathematical capabilities have been kind of bracketed

380
00:27:27,960 --> 00:27:31,680
off and turned into something called TF probability.

381
00:27:31,680 --> 00:27:38,560
So if you don't want to use all of the things in TensorFlow, like you don't really care

382
00:27:38,560 --> 00:27:44,680
about, you don't really care about, you know, Keras, or you don't care about some other

383
00:27:44,680 --> 00:27:49,160
portion of the API, you can just use TF probability and you can download it as a

384
00:27:49,160 --> 00:27:53,480
pit package or estimators, you can just download estimators.

385
00:27:53,480 --> 00:28:00,360
So all of these things, we've removed duplication, we've kind of taken all of the symbols that

386
00:28:00,360 --> 00:28:06,200
were just kind of dispersed to the winds and grouped them together in a way that seems

387
00:28:06,200 --> 00:28:15,040
more logical and also applied a lot of sort of well thought out renamed to make things

388
00:28:15,040 --> 00:28:22,400
more consistent and to make the sort of calls feel more intuitive.

389
00:28:22,400 --> 00:28:28,040
But that's kind of the idea is that instead of one monolithic application that you have

390
00:28:28,040 --> 00:28:33,760
to download in its entirety in order to use a single bit of functionality, you just take

391
00:28:33,760 --> 00:28:38,760
the pieces that fit for you in your organization and that helps a lot with the build and deployment

392
00:28:38,760 --> 00:28:39,760
process as well.

393
00:28:39,760 --> 00:28:46,200
I definitely saw that in the discussion yesterday and one of the analogies that kind of popped

394
00:28:46,200 --> 00:28:52,080
up in my head is like Hadoop and not all the negative associations with Hadoop but like

395
00:28:52,080 --> 00:28:56,920
people think of Hadoop as, you know, they think of it as this one thing, MapReduce, right?

396
00:28:56,920 --> 00:29:01,240
When really it was like a storage system and MapReduce and like this whole ecosystem

397
00:29:01,240 --> 00:29:02,240
of stuff.

398
00:29:02,240 --> 00:29:03,240
All of the Apache products.

399
00:29:03,240 --> 00:29:04,240
Yeah.

400
00:29:04,240 --> 00:29:05,240
All of the Apache stuff, right?

401
00:29:05,240 --> 00:29:09,520
And TensorFlow, like I think we're transitioning from the point where you would think of TensorFlow

402
00:29:09,520 --> 00:29:14,000
as this monolithic thing to like, you got an ecosystem, right?

403
00:29:14,000 --> 00:29:19,800
With the probability stuff, which we can talk maybe a little bit more about the TFX stuff,

404
00:29:19,800 --> 00:29:25,000
which I'm really excited about, which is almost its own ecosystem.

405
00:29:25,000 --> 00:29:30,760
So that I think that's an interesting kind of transition and one that, I mean, it's kind

406
00:29:30,760 --> 00:29:36,640
of a, there's a natural point in a large open-source project where you start to see that kind

407
00:29:36,640 --> 00:29:37,640
of thing happen.

408
00:29:37,640 --> 00:29:43,320
Yeah, and it's really interesting to, at least from a developer advocacy perspective,

409
00:29:43,320 --> 00:29:47,680
to see the different communities that build up for each one of those tools.

410
00:29:47,680 --> 00:29:54,360
So for example, we mentioned Swift for TensorFlow and usually the folks who gravitate towards

411
00:29:54,360 --> 00:29:59,880
that camp are very interested in things like algorithmic fusion, they're very interested

412
00:29:59,880 --> 00:30:04,240
in compilers and, you know, sort of, performance, and then you have folks in the TensorFlow

413
00:30:04,240 --> 00:30:12,080
JS community that are, so TensorFlow JS is JavaScript in the browser that are very focused

414
00:30:12,080 --> 00:30:17,840
on creating these sort of artistic experiences often.

415
00:30:17,840 --> 00:30:24,320
Like you would have code pens to do neural drum machines or you would have, you know, really

416
00:30:24,320 --> 00:30:31,120
interesting sort of, you know, like puppy ears placed on people heads that, you know,

417
00:30:31,120 --> 00:30:36,480
would follow you around as you, as you look in your, your phone camera or your browser,

418
00:30:36,480 --> 00:30:43,200
or, you know, even things like, sort of in a more product-centric way, like if you're

419
00:30:43,200 --> 00:30:49,440
a developer who wanted to include a text box in your application, automatically, including

420
00:30:49,440 --> 00:30:51,680
something like sentiment analysis within it.

421
00:30:51,680 --> 00:30:57,040
So as a person is typing, yeah, it would be like, hey, man, you know, your tone's kind of,

422
00:30:57,040 --> 00:31:00,040
your tone's kind of negative there, maybe you should make it a little bit happier or

423
00:31:00,040 --> 00:31:03,880
maybe you should be a little bit, like, change this word to be this other word and that

424
00:31:03,880 --> 00:31:05,200
should be fine.

425
00:31:05,200 --> 00:31:11,160
So those sorts of tools and then TensorFlow Core, you know, you get a lot of academic researchers

426
00:31:11,160 --> 00:31:16,480
sure, but you also get businesses who are really interested in productizing machine learning

427
00:31:16,480 --> 00:31:22,600
and deep learning and then they also are interested in TFX and, yeah, it's really cool to see.

428
00:31:22,600 --> 00:31:23,600
Yeah.

429
00:31:23,600 --> 00:31:26,960
Well, let's maybe talk a little bit about TFX.

430
00:31:26,960 --> 00:31:27,960
Yeah.

431
00:31:27,960 --> 00:31:32,760
So that was one of the things that I was most excited about.

432
00:31:32,760 --> 00:31:34,800
TFX is TensorFlow Extended.

433
00:31:34,800 --> 00:31:35,800
Yes.

434
00:31:35,800 --> 00:31:38,200
And there's a workshop for it today, is it?

435
00:31:38,200 --> 00:31:41,480
And there's a workshop for it today that I will be popping in on.

436
00:31:41,480 --> 00:31:43,800
I'm excited about that.

437
00:31:43,800 --> 00:31:47,040
So my, I've mentioned this to a couple of people.

438
00:31:47,040 --> 00:31:52,200
Yes, I, I've, you know, we've done a series of podcasts on the show about machine learning

439
00:31:52,200 --> 00:31:57,360
infrastructure and I've been writing ebooks about machine learning infrastructure and it's

440
00:31:57,360 --> 00:32:03,600
a space that I'm really excited about and I've looked at TFX in the past and it was hard

441
00:32:03,600 --> 00:32:08,400
to really like wrap my head around it, it's just like this kind of random collection of

442
00:32:08,400 --> 00:32:09,400
tools.

443
00:32:09,400 --> 00:32:10,400
Yeah.

444
00:32:10,400 --> 00:32:16,960
But what was announced yesterday seemed like the beginning of making it more coherent and

445
00:32:16,960 --> 00:32:22,840
kind of like, when you read the TFX like the papers, like it's clearly Google has this

446
00:32:22,840 --> 00:32:28,480
kind of very elaborate sophisticated infrastructure internally.

447
00:32:28,480 --> 00:32:33,240
And I think looking at that and then seeing what was previously available made it even

448
00:32:33,240 --> 00:32:34,240
more visible.

449
00:32:34,240 --> 00:32:37,480
It's like, that's all you're giving me is like, do you have a melody?

450
00:32:37,480 --> 00:32:38,480
Yeah.

451
00:32:38,480 --> 00:32:39,480
And certainly.

452
00:32:39,480 --> 00:32:40,480
Yeah.

453
00:32:40,480 --> 00:32:41,480
Yeah.

454
00:32:41,480 --> 00:32:42,480
Yeah.

455
00:32:42,480 --> 00:32:43,480
Yeah.

456
00:32:43,480 --> 00:32:44,480
Yeah.

457
00:32:44,480 --> 00:32:47,840
So maybe kind of what you, it's your, you know, what's the kind of the top line on TFX

458
00:32:47,840 --> 00:32:50,520
or what are you excited about from that perspective?

459
00:32:50,520 --> 00:32:55,320
I'm so glad that you mentioned TFX because TFX is one of the reasons why I wanted to

460
00:32:55,320 --> 00:32:57,360
come work at Google in the first place.

461
00:32:57,360 --> 00:33:03,000
Like I read the paper and I think I mentioned a little bit before I worked as a data scientist

462
00:33:03,000 --> 00:33:07,880
in the, in the energy industry for a long time and the hard bit like it was, it was always

463
00:33:07,880 --> 00:33:11,960
delightful to create the models, you know, it was cool to have this like crunchy engineering

464
00:33:11,960 --> 00:33:17,000
challenge and to take data and to figure out how to solve it in a way that, in a way that

465
00:33:17,000 --> 00:33:21,680
it sort of made sense and that would, and that would be performant and explainable.

466
00:33:21,680 --> 00:33:24,360
But then the hard part was always, okay, cool.

467
00:33:24,360 --> 00:33:25,600
Now how do I deploy it?

468
00:33:25,600 --> 00:33:27,240
Like how do I get it placed?

469
00:33:27,240 --> 00:33:31,120
How do I have this bottle placed into a format that somebody can use in a software application?

470
00:33:31,120 --> 00:33:34,920
And then once it's released, how do I make sure that it's doing its job and that it's

471
00:33:34,920 --> 00:33:36,240
kept up to date?

472
00:33:36,240 --> 00:33:40,440
And how do I make sure that the incoming data stays consistent with the data that was used

473
00:33:40,440 --> 00:33:41,840
to train the model initially?

474
00:33:41,840 --> 00:33:45,720
And if I want to do online training, how would I go about doing that?

475
00:33:45,720 --> 00:33:51,400
So all of these like really interesting questions where the model was just like this tiny piece

476
00:33:51,400 --> 00:33:59,560
of this big, big overall, like sort of continuous integration, continuous deployment strategy,

477
00:33:59,560 --> 00:34:01,600
like DevOps for data science.

478
00:34:01,600 --> 00:34:13,040
And the, and TFX, the paper sort of gave, it sort of elaborated on a strategy that Google

479
00:34:13,040 --> 00:34:17,840
has been using successfully over the past 10 years to do that effectively.

480
00:34:17,840 --> 00:34:23,120
Because most businesses they create custom glue code, they, they don't really, they might

481
00:34:23,120 --> 00:34:29,560
have a component of DevOps for their machine learning models.

482
00:34:29,560 --> 00:34:37,320
But a lot of it might be manual, so not necessarily an example would be if your model falls

483
00:34:37,320 --> 00:34:42,520
below a given accuracy for a stent of time.

484
00:34:42,520 --> 00:34:43,520
How would you?

485
00:34:43,520 --> 00:34:49,440
Yeah, you know, like, or it's like, and if you're at the other, the other piece, right,

486
00:34:49,440 --> 00:34:54,680
is that data scientists, they're probably creating their model in Python or R. In order

487
00:34:54,680 --> 00:35:01,080
to get it integrated with a software application, that model, at least where I worked previously,

488
00:35:01,080 --> 00:35:06,560
it would have to be refactored into something like Java or C++, which means that you're,

489
00:35:06,560 --> 00:35:12,920
you know, usually your accuracy goes down, like, it just, it's, it's not a good situation.

490
00:35:12,920 --> 00:35:17,440
And then if your model needs to be updated, how are you going to take that C++ and Java,

491
00:35:17,440 --> 00:35:21,360
give it back to your data science team and be like, well, fix it now.

492
00:35:21,360 --> 00:35:25,240
You know, there's, there's no straightforward path to do that.

493
00:35:25,240 --> 00:35:30,200
With TensorFlow, you know, it's, it's the same framework for everything.

494
00:35:30,200 --> 00:35:32,480
So the data scientists can create it.

495
00:35:32,480 --> 00:35:36,720
It can be deployed by your, you know, your DevOps team. It can be maintained by them as

496
00:35:36,720 --> 00:35:37,720
well.

497
00:35:37,720 --> 00:35:41,320
And then if it needs to be modified, it's the same TensorFlow code that your data scientists

498
00:35:41,320 --> 00:35:43,080
know how to use.

499
00:35:43,080 --> 00:35:48,880
So TFX, you're right in that when it was announced last year, they only had like two little

500
00:35:48,880 --> 00:35:57,040
components of this, of this sort of toolbox of things, the date, everything from data validation,

501
00:35:57,040 --> 00:36:02,440
which allows you to do those data checks that I was mentioning before, transformation.

502
00:36:02,440 --> 00:36:08,960
So TFT, TensorFlow transform, which allows you to do pre-processing on data pipelines,

503
00:36:08,960 --> 00:36:15,520
to make sure that an in full past data processing, to make sure that your statistical distributions

504
00:36:15,520 --> 00:36:19,840
for each feature are similar as they're coming through your pipeline.

505
00:36:19,840 --> 00:36:23,800
Model analysis, which gives you insight into how your model is performing over time.

506
00:36:23,800 --> 00:36:28,200
There's even a GUI to show you sort of metrics associated with that.

507
00:36:28,200 --> 00:36:34,160
Of course, they're serving, and then there's also sort of additional tools that I believe

508
00:36:34,160 --> 00:36:44,120
Clemens mentioned yesterday, collaboration with Cubeflow, but all of these sort of products

509
00:36:44,120 --> 00:36:48,280
that bundled up together, alleviate the need for you and your company to have to write

510
00:36:48,280 --> 00:36:52,520
Blue Code and to have to refactor models.

511
00:36:52,520 --> 00:36:56,720
It gives you the tools to deploy models in a maintainable way.

512
00:36:56,720 --> 00:37:03,160
Yeah, there's also an interesting bit in there about a metadata repository where you're

513
00:37:03,160 --> 00:37:13,440
able to track all of the configs and parameters associated with various data linux and data

514
00:37:13,440 --> 00:37:14,440
problems.

515
00:37:14,440 --> 00:37:15,440
Right.

516
00:37:15,440 --> 00:37:18,120
So you have your experiments, and then when you have a model that you're pushing out to

517
00:37:18,120 --> 00:37:22,840
production, all that information, and there were some use cases presented to your point

518
00:37:22,840 --> 00:37:29,520
that shows how you can use that information to go kind of work backward from a model decision

519
00:37:29,520 --> 00:37:36,280
to what training data impacted that model decision and what the experiments were that kind

520
00:37:36,280 --> 00:37:39,680
of drove to those sets of model parameters and stuff.

521
00:37:39,680 --> 00:37:40,680
Absolutely.

522
00:37:40,680 --> 00:37:44,240
And that's huge in terms of model explainability.

523
00:37:44,240 --> 00:37:48,400
So if you're working in a high impact industry, again, like the oil industry, you had to be

524
00:37:48,400 --> 00:37:53,240
able to explain why you made a decision just in case it was the wrong one, and then also

525
00:37:53,240 --> 00:37:54,240
reproducibility.

526
00:37:54,240 --> 00:37:59,520
So if somebody wants to take your results and try to replicate them on their own data,

527
00:37:59,520 --> 00:38:06,160
or try a different model and see if the performance is, if the performance varies, then having

528
00:38:06,160 --> 00:38:09,920
all of that metadata associated with the input is huge.

529
00:38:09,920 --> 00:38:12,920
Anything else that we've not talked about yet?

530
00:38:12,920 --> 00:38:17,480
You left excited about yesterday or while you knew about it already, but it was still

531
00:38:17,480 --> 00:38:19,440
exciting to hear.

532
00:38:19,440 --> 00:38:23,920
So I'm really, it's not a secret.

533
00:38:23,920 --> 00:38:31,840
Like I am most excited about the community focus for TensorFlow, and I really love, I really

534
00:38:31,840 --> 00:38:38,920
love that 2.0 has been pushing for community involvement and like not making any changes

535
00:38:38,920 --> 00:38:45,920
unless everybody who's using the tool is, you know, is informed and has had the opportunity

536
00:38:45,920 --> 00:38:48,640
to give feedback.

537
00:38:48,640 --> 00:38:54,600
So the original release of TensorFlow felt very much like, you know, delivering tablets

538
00:38:54,600 --> 00:38:56,640
down from a mountain kind of almost.

539
00:38:56,640 --> 00:39:02,680
It's, you know, the code was released and over time, like community could get involved,

540
00:39:02,680 --> 00:39:06,400
but it was really only through this thing called Contrib.

541
00:39:06,400 --> 00:39:10,480
And for the most part, the engineering was done by the engineering team at Google.

542
00:39:10,480 --> 00:39:15,920
And since then, it's, it's developed into more of a partnership between individuals in

543
00:39:15,920 --> 00:39:20,440
the community, but also companies who are using TensorFlow extensively.

544
00:39:20,440 --> 00:39:28,640
And you know, students or application developers or, you know, not just deep learning researchers,

545
00:39:28,640 --> 00:39:32,360
but really people coming from all walks of life who want to incorporate machine learning

546
00:39:32,360 --> 00:39:33,360
into their tools.

547
00:39:33,360 --> 00:39:37,440
I have crunchy problems that machine learning can help solve.

548
00:39:37,440 --> 00:39:44,800
So that is, that is my favorite thing is that TensorFlow 2.0 is all about community.

549
00:39:44,800 --> 00:39:52,720
You mentioned the RFC process and that's a, that's a new process since that's a new,

550
00:39:52,720 --> 00:39:59,520
the RFC process is a new process that was begun, I believe, halfway through last year.

551
00:39:59,520 --> 00:40:03,040
So maybe July, August of last year, okay.

552
00:40:03,040 --> 00:40:09,480
And again, if you, if you want to make a change to the API, anybody can do it.

553
00:40:09,480 --> 00:40:13,760
So if you have an idea that you think would be solid, you just propose it.

554
00:40:13,760 --> 00:40:19,800
The engineering team, both at Google and then also externally reviews it and delivers commentary.

555
00:40:19,800 --> 00:40:24,120
And if everybody agrees, then development work can start.

556
00:40:24,120 --> 00:40:25,120
Awesome.

557
00:40:25,120 --> 00:40:27,920
Well, Paige, thanks so much for taking the time to chat with me.

558
00:40:27,920 --> 00:40:29,240
Thank you so much for having me.

559
00:40:29,240 --> 00:40:30,240
This was fun.

560
00:40:30,240 --> 00:40:31,240
Absolutely.

561
00:40:31,240 --> 00:40:32,240
Absolutely.

562
00:40:32,240 --> 00:40:38,400
All right, everyone, that's our show for today.

563
00:40:38,400 --> 00:40:44,600
For more information on Paige or any of the topics covered in this episode, visit twimlai.com

564
00:40:44,600 --> 00:40:47,560
slash talk slash 242.

565
00:40:47,560 --> 00:40:52,000
Thanks again to the TensorFlow team for their sponsorship of this series.

566
00:40:52,000 --> 00:40:57,480
Make sure you check out the tensorflow 2.0 alpha at tensorflow.org and enter our TensorFlow

567
00:40:57,480 --> 00:41:02,360
Edge giveaway at twimlai.com slash TF giveaway.

568
00:41:02,360 --> 00:41:29,120
As always, thanks so much for listening and catch you next time.

