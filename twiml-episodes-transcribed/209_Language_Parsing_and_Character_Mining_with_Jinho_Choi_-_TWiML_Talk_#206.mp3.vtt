WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.480
I'm your host Sam Charrington.

00:31.480 --> 00:36.080
Today in the second episode of our reinvent series, we're joined by Geno Choi, assistant

00:36.080 --> 00:39.400
professor of computer science at Emory University.

00:39.400 --> 00:45.040
Geno participated in the conference's AI Summit, presenting on ELIT, which is short for evolution

00:45.040 --> 00:50.920
of language and information technology, which is a cloud-based NLP platform.

00:50.920 --> 00:54.720
In our conversation, we discuss some of the key NLP challenges that Geno and his group

00:54.720 --> 01:00.520
are tackling, including language parsing and character mining, as well as their vision

01:00.520 --> 01:05.920
for ELIT, which makes it easy for researchers to develop access and deploy cutting-edge

01:05.920 --> 01:09.320
NLP models to the cloud.

01:09.320 --> 01:13.080
If you're hearing this on Wednesday, I'm currently at NURRIPS.

01:13.080 --> 01:18.080
If you're also here, please join the listener meetup I'm hosting tonight at 630 at Tavern

01:18.080 --> 01:19.080
Midway.

01:19.080 --> 01:24.440
Be sure to RSVP via the social activities thread in the conference's Hoover app.

01:24.440 --> 01:26.880
Next week, I'll be in Seattle at CUBECON.

01:26.880 --> 01:30.480
I'd love to connect with any listeners in the area or in attendance.

01:30.480 --> 01:35.080
Feel free to shoot me a message via at Sam Charrington on Twitter, via email, or the

01:35.080 --> 01:57.880
Twomo website, CURM, and now on to the show.

01:57.880 --> 02:04.880
So we are sitting here in Las Vegas, the occasion that brought us together is the reinvent conference

02:04.880 --> 02:11.160
here, where you delivered a presentation on some of the work that you're doing at Emery.

02:11.160 --> 02:15.120
But before we dive into that, tell us a little bit about your background and how you got

02:15.120 --> 02:16.640
involved in machine learning.

02:16.640 --> 02:17.640
Right.

02:17.640 --> 02:22.760
I actually got into the natural language processing back in 2002, so that's when I was

02:22.760 --> 02:28.920
doing my master's degree at University of Pennsylvania, and so I've been always interested

02:28.920 --> 02:34.680
in studying languages and having the language understanding by computers.

02:34.680 --> 02:40.240
So since I was young, I was always telling my mom that I wanted to make a robot that

02:40.240 --> 02:47.280
understands me, and I guess I'm stepping towards to that goal still, and I don't know if

02:47.280 --> 02:52.080
it's going to happen before my time or not, but I'm going to try to strive to it.

02:52.080 --> 02:57.560
So once I actually got into this field, I was fascinated by all these machine learning

02:57.560 --> 02:58.560
approaches.

02:58.560 --> 03:03.800
The machine is capable of learning the patterns that we don't really even teach, and you actually

03:03.800 --> 03:08.760
can pick up the patterns that we don't even think about, especially with the latest

03:08.760 --> 03:11.200
technologies with dim learning and neural networks.

03:11.200 --> 03:16.920
It's actually started discovering some linguistic phenomena that has not been really discovered

03:16.920 --> 03:18.600
by humans yet.

03:18.600 --> 03:23.400
So this is actually a very exciting time of the year to study natural learning processing

03:23.400 --> 03:24.880
along with machine learning.

03:24.880 --> 03:29.920
What are the research areas that you focus on in your work?

03:29.920 --> 03:30.920
Right.

03:30.920 --> 03:36.000
So there are many different fields in natural learning processing that I do, but there

03:36.000 --> 03:39.240
are like mainly three projects I work on.

03:39.240 --> 03:42.440
One is the core level natural learning processing.

03:42.440 --> 03:50.080
What we call given an unstructured raw free text, we try to analyze the text into structures.

03:50.080 --> 03:54.760
So this is all prototypical since you learn from the elementary school.

03:54.760 --> 04:00.040
So you analyze the language into grammatical categories, synthetic patterns, semantic

04:00.040 --> 04:02.480
processing, and some understanding.

04:02.480 --> 04:08.400
So we now try to have computers to generate the similar structures that we have been

04:08.400 --> 04:10.360
teaching in the decades.

04:10.360 --> 04:16.560
So that's the one part of the research I work on, which we call as a language parsing.

04:16.560 --> 04:22.960
And the second part is we also try to show how the language represent.

04:22.960 --> 04:28.560
So especially we are trying to present this into the setting of the machine comprehension.

04:28.560 --> 04:35.800
So the project that I actually created since I came to Emory was a project called Character

04:35.800 --> 04:42.600
Mining, and the goal of the project is to understand human conversation involving multiple

04:42.600 --> 04:43.800
parties.

04:43.800 --> 04:50.680
So now the scenario I'm designing right now is if you have a machine such as Alexa or

04:50.680 --> 04:58.120
any of this client conversation agent machines, it can just listen to our conversation.

04:58.120 --> 05:03.120
And later on, if we actually forget some contents about this conversation, we can, instead

05:03.120 --> 05:09.280
of calling multiple people around about this, we can ask the machine, so what did I talk

05:09.280 --> 05:10.280
about?

05:10.280 --> 05:11.280
What did I talk about?

05:11.280 --> 05:16.080
And if I forgot your last name, so what was Sam's last name, it should be able to tell

05:16.080 --> 05:19.960
if that kind of content was brought up during the conversation.

05:19.960 --> 05:24.280
So that's one project I call Character Mining, that's what I work on most.

05:24.280 --> 05:29.920
And also along the application size, since Emory has a huge hospital, we have so much

05:29.920 --> 05:35.640
records, years of years of data from patients and all this different kind of electrical

05:35.640 --> 05:37.640
healthcare data.

05:37.640 --> 05:42.840
So we actually also try to apply natural links processing to help people to facilitate

05:42.840 --> 05:45.240
better for their medical needs.

05:45.240 --> 05:50.440
So especially what we have done is we have tried to done the classification of the radiology

05:50.440 --> 05:57.240
reports and see the machines capable of seeing what kind of severity level of this patient

05:57.240 --> 06:00.720
has based on the text and the report.

06:00.720 --> 06:04.920
And I think we have been reaching to the similar accuracy as humans do.

06:04.920 --> 06:07.440
So that's what the technology is involved.

06:07.440 --> 06:13.520
And also another new project that we start working on is, given the language patterns,

06:13.520 --> 06:20.440
or you basically ask the person to speak freely about certain things, about what or two

06:20.440 --> 06:21.440
minutes.

06:21.440 --> 06:28.560
And analyze the style of the language or some patterns of the language, and we can detect

06:28.560 --> 06:34.600
if the person has some kind of cognitive impairment and leading to the Alzheimer's disease.

06:34.600 --> 06:39.400
So we are actually trying to catch a very early stage of Alzheimer's disease by using

06:39.400 --> 06:43.840
our language technology, and this has been actually working really exciting project for

06:43.840 --> 06:44.840
me.

06:44.840 --> 06:45.840
Oh wow.

06:45.840 --> 06:49.120
There's a lot of interesting stuff to dig into.

06:49.120 --> 06:57.240
So the first one that you talked about is kind of language parsing, and we have a long

06:57.240 --> 07:05.160
history of trying to do this with computers, both using more traditional rules, model-based

07:05.160 --> 07:12.640
approach to doing it, and more recently statistical-based approaches to doing it.

07:12.640 --> 07:18.800
What are the main elements of the way that you're going after this problem?

07:18.800 --> 07:19.800
Right.

07:19.800 --> 07:26.240
So during my PhD, I graduated from my PhD even in 2012, which is not that long ago.

07:26.240 --> 07:32.480
Amazing thing is every approach I actually learned during the PhD actually has changed.

07:32.480 --> 07:37.120
One side story, one side story about this is, when I came to Emory, I spent a year to

07:37.120 --> 07:41.960
make slides for my natural language processing graduate level class.

07:41.960 --> 07:47.800
That was year 2015, and I cannot use any of those slides anymore, because technology evolved

07:47.800 --> 07:52.920
so much, which is great thing for the field, but for faculty members like us, we just have

07:52.920 --> 07:55.400
to keep making new slides every year.

07:55.400 --> 08:02.240
Well, I mean, it's a happy thought, but so the ladies break through, I have to say,

08:02.240 --> 08:04.440
it's a neural network and dim learning.

08:04.440 --> 08:10.080
So at the moment, we are trying to rely on the supervised learning, which you need to

08:10.080 --> 08:17.120
require an enormous amount of manually annotated data from the linguist, and you basically

08:17.120 --> 08:21.680
the machine tried to learn the patterns among the human annotations and tried to produce

08:21.680 --> 08:25.120
a very similar result for the unseen data.

08:25.120 --> 08:31.680
So this approach has been working for over 20 years in natural language processing lately,

08:31.680 --> 08:36.760
because of this evolved amount of neural network, we can actually use a lot of unsupervised

08:36.760 --> 08:37.760
data.

08:37.760 --> 08:43.440
So free text without human annotations can actually take a huge role into this field now.

08:43.440 --> 08:46.800
So all of a sudden, let me give you a very solid example.

08:46.800 --> 08:51.640
So there was a synthetic parsing test case, one of the tests that natural needs processing

08:51.640 --> 08:54.560
field has been investigated for a long time.

08:54.560 --> 09:03.560
And the accuracy of 92 or 93% was the barrier that had not been broken for over 15 years.

09:03.560 --> 09:09.800
And after this neural network, it got broken up to 96% now.

09:09.800 --> 09:16.120
As of last month, one of my pitches actually broke the record to the 96%, which is something

09:16.120 --> 09:19.360
that we couldn't even dream of just two years ago.

09:19.360 --> 09:24.600
So yeah, I think all this involvement of the diminuting and machine learning technology

09:24.600 --> 09:28.040
really played a huge role in natural needs processing.

09:28.040 --> 09:35.800
And so is there a particular type of model or model architecture that you're using to

09:35.800 --> 09:38.120
do the syntax parsing?

09:38.120 --> 09:39.120
Right.

09:39.120 --> 09:42.800
So there are multiple different ways of doing it.

09:42.800 --> 09:46.760
So for parsing, there are two main streams of parsing.

09:46.760 --> 09:51.720
One is known as a transition-based parsing, and there is known as a graph-based parsing.

09:51.720 --> 09:57.800
So in all days, just like it's not even all days in five years ago, a lot of people focused

09:57.800 --> 10:04.600
on the transition-based parsing approach, which doesn't search for the entire, search spaces

10:04.600 --> 10:09.480
and entire possibilities, but it actually prints out a lot of search space so it will run

10:09.480 --> 10:11.000
much faster.

10:11.000 --> 10:14.040
And speed is really essential for the big data analysis.

10:14.040 --> 10:18.120
So everyone was going for a transition-based parsing.

10:18.120 --> 10:25.280
What was actually amazing was now because of all this power of the GPU, which is known

10:25.280 --> 10:31.120
for parallel processing, even if we actually make an exhaustive search using graph-based

10:31.120 --> 10:36.280
parsing, it can actually still run as fast as a transition-based parsing.

10:36.280 --> 10:41.440
So this actually is another movement that I didn't actually dream of during my PhD time.

10:41.440 --> 10:46.080
But now my students actually are discovering graph-based parsing, which usually is more

10:46.080 --> 10:49.480
accurate, and now can even run as fast.

10:49.480 --> 10:56.480
So the algorithm is basically making the exhaustive search and giving all the possible probabilities,

10:56.480 --> 11:00.880
now you can re-rank to find the best structure out of it.

11:00.880 --> 11:05.560
So which is getting much more popular approaches these days.

11:05.560 --> 11:14.920
And so in that model and the graph-based processing, what are the nodes of the graph?

11:14.920 --> 11:17.880
No, self-the-graph is the individual word.

11:17.880 --> 11:20.320
And the connection?

11:20.320 --> 11:27.240
The connections will be what we call dependency relations, so like subject, object, preposition

11:27.240 --> 11:28.240
relations.

11:28.240 --> 11:34.600
So if you have a simple sense of like John but a car, so John is a subject of both and

11:34.600 --> 11:39.280
car is an object of both, and all is a determiner of car.

11:39.280 --> 11:42.440
So that kind of relation is the synthetic parsing.

11:42.440 --> 11:47.120
You start with just nodes then, right, because the problem that you're trying to solve is

11:47.120 --> 11:50.160
define what the connections are.

11:50.160 --> 11:55.440
So now in this example, there are four words, John but a car.

11:55.440 --> 12:00.400
And so you have four nodes, and you have one external called root root of the sentence.

12:00.400 --> 12:05.680
So it depends on your approach, but root of the sentence in this case will be a bot.

12:05.680 --> 12:10.280
So bot will be connected to root, and everything else will have a connection to itself.

12:10.280 --> 12:17.400
So the current approaches that we are developing is you try to find if every pair will be compared

12:17.400 --> 12:19.440
if there is a relation.

12:19.440 --> 12:25.160
And now you run the optimization algorithm to find which is the most probable tree you

12:25.160 --> 12:28.160
will generate out of this.

12:28.160 --> 12:31.120
And you're doing this via neural network.

12:31.120 --> 12:33.200
Now we are doing this neural network, yes.

12:33.200 --> 12:35.480
Is it a supervised learning problem?

12:35.480 --> 12:42.160
At the moment, so I would say like the most latest approaches are semi-supervised.

12:42.160 --> 12:47.400
So we do use all the training data from the human annotation, but we also may take advantage

12:47.400 --> 12:50.800
of unstructured data on annotated data.

12:50.800 --> 12:55.840
So this is the, well, everyone uses an object called word embeddings.

12:55.840 --> 13:00.440
So this word representation is trained on a large amount of data.

13:00.440 --> 13:04.520
So the data that we have is about 62 gigabytes of the text.

13:04.520 --> 13:11.040
So 62 gigabytes of text is actually equivalent to so much more if you compare to images.

13:11.040 --> 13:13.160
Because it's only the character in level.

13:13.160 --> 13:20.640
So yeah, and we can actually learn all the, what's the best representation for this word

13:20.640 --> 13:26.240
out of all this text and apply that knowledge into this supervised learning passion?

13:26.240 --> 13:33.600
So that means so then each of the nodes then isn't a symbolic representation of the word

13:33.600 --> 13:37.400
rather it's an embedding that has some semantic space.

13:37.400 --> 13:38.400
Right.

13:38.400 --> 13:39.880
What we call distributional semantics.

13:39.880 --> 13:40.880
Okay.

13:40.880 --> 13:42.560
Distributional semantics, right.

13:42.560 --> 13:49.080
And so this, this corpus that you describe the 62 gigabytes is that your own corpus

13:49.080 --> 13:51.200
or is that like love vectors or something like that?

13:51.200 --> 13:52.600
It's all publicly available.

13:52.600 --> 13:58.080
So the most popular one, the entire Wikipedia is available to everyone.

13:58.080 --> 14:03.400
Amazon wonderfully released all their review, a lot of like 10 years of their review.

14:03.400 --> 14:04.400
Reviews.

14:04.400 --> 14:05.400
We have those.

14:05.400 --> 14:08.680
We have a lot of Twitter data as well.

14:08.680 --> 14:12.720
And there are some medical domain Wikipedia that you can crawl.

14:12.720 --> 14:13.720
Okay.

14:13.720 --> 14:15.680
And so those are also in there too.

14:15.680 --> 14:16.680
Okay.

14:16.680 --> 14:22.080
And also, there is also New York Times corpus from LDC.

14:22.080 --> 14:26.600
So that's actually you have to buy from the organization what we have is so we are also

14:26.600 --> 14:27.600
using it.

14:27.600 --> 14:33.560
Can you describe kind of the algorithm and the way that you're using embeddings and kind

14:33.560 --> 14:41.080
of the network architecture that you're using and how you're training these models to ultimately

14:41.080 --> 14:43.480
produce these graphs?

14:43.480 --> 14:44.480
Right.

14:44.480 --> 14:49.120
And so in the very low level, the generations who are already embedding, they're like so

14:49.120 --> 14:51.080
many different approaches coming out.

14:51.080 --> 14:54.880
It's basically you try to develop a language model.

14:54.880 --> 15:01.640
So for given each all this text, for every word in the text, you are basically trying

15:01.640 --> 15:06.800
to build a model that actually can predict the contextual words.

15:06.800 --> 15:08.360
So you can go the other way too.

15:08.360 --> 15:12.680
Given the contextual word, you can try to predict that word of the target.

15:12.680 --> 15:17.440
So once you develop this kind of language model and you can develop in many different ways.

15:17.440 --> 15:22.560
So the most popular approach called word to bag is just using a very simple feed forward

15:22.560 --> 15:25.160
neural network with just one layer.

15:25.160 --> 15:30.520
But the latest approach that people are using is more complicated as a bidirectional LSTM

15:30.520 --> 15:37.600
kind of approaches, which tends to give better higher accuracy, but it's probably slower.

15:37.600 --> 15:43.400
So given this language model, the language model tells you this word, for this word, this

15:43.400 --> 15:47.080
is the best vector representation of the word.

15:47.080 --> 15:51.160
So now every word will be represented as a vector.

15:51.160 --> 15:57.880
So given that, now we pair, we tokenize every word into sentences.

15:57.880 --> 16:03.080
And for every pair of word in the sentence, you try to see if there is a dependency

16:03.080 --> 16:04.760
relation or not.

16:04.760 --> 16:07.160
And that's the supervised version.

16:07.160 --> 16:14.720
So now we have a large corpus, about 2.5 million words corpus that actually has an annotated

16:14.720 --> 16:17.520
for this kind of dependency relations.

16:17.520 --> 16:25.560
So the end is that the annotation, is it contextual or is it like engram?

16:25.560 --> 16:26.560
It's a contextual.

16:26.560 --> 16:28.160
It's an actual text.

16:28.160 --> 16:34.520
So a lot of the most famous one, what everyone knows in the field of NLP is called Pantry

16:34.520 --> 16:35.520
Bank.

16:35.520 --> 16:38.760
So that's the 1 million words one.

16:38.760 --> 16:44.080
After that, there is another project called Antonut, which is a big one to note.

16:44.080 --> 16:49.480
So it's yeah, ON to ONTO note.

16:49.480 --> 16:56.240
So this is also another project, and it actually has 2.5 million words, so it's a bigger corpus

16:56.240 --> 16:58.120
with much more genre.

16:58.120 --> 17:04.080
So Pantry Bank was only for the new swire, but Antonut's project had a broadcasting conversation,

17:04.080 --> 17:09.680
broadcasting news, telephone conversation, web blogs, and even the Bibles.

17:09.680 --> 17:14.120
So it has much more different kind of genres, these bigger corpus.

17:14.120 --> 17:19.600
And we also internally have some of this annotation on the medical domain as well.

17:19.600 --> 17:23.320
So all this combined together is making a supervised learning.

17:23.320 --> 17:24.320
Okay.

17:24.320 --> 17:27.480
So that's kind of the graph model.

17:27.480 --> 17:32.920
You also mentioned the transition model, which confused me at first because when I think

17:32.920 --> 17:37.280
of graphs, I think of transitions as these variations between the nose, but it sounds

17:37.280 --> 17:43.680
like what that, that's more of like a, I don't know if you'd call it stateful or stateless,

17:43.680 --> 17:49.760
but like you're looking at the current word at the time, as opposed to the entire relationship

17:49.760 --> 17:51.480
between everything and you're trying to.

17:51.480 --> 17:53.080
You've got the exactly right.

17:53.080 --> 17:54.080
Okay.

17:54.080 --> 17:59.400
So graph based algorithm basically doesn't take in each node as an individual state, it

17:59.400 --> 18:03.640
just take a whole holistic view of the entire graph.

18:03.640 --> 18:09.320
The transition based parsing, however, is looking at a word at a time.

18:09.320 --> 18:14.760
So it's making the transition between one word to the other word and try to see if there

18:14.760 --> 18:19.840
are some word, contextual words around it has the dependency relations.

18:19.840 --> 18:22.440
So that's how the transition based parsing works.

18:22.440 --> 18:27.240
And the advantage of transition based parsing is the parsing complexity is much lower.

18:27.240 --> 18:33.880
So we have, many people actually have written several papers about the parsing complexity

18:33.880 --> 18:37.360
on average, can be as low as the big of n.

18:37.360 --> 18:43.120
So basically, if you have n number of words in the sentence, you need to make about n number

18:43.120 --> 18:45.280
of comparisons to be done.

18:45.280 --> 18:49.360
Whereas graph based parsing, you have to make n squared comparisons.

18:49.360 --> 18:55.960
So that's why transition based parsing is tend to be much faster.

18:55.960 --> 19:04.120
But because it's not comparing every possible pairs, it tends to be a little less accurate.

19:04.120 --> 19:09.680
So, but people were buying the speed over accuracy for a long time period.

19:09.680 --> 19:14.880
So I also focused a lot on the transition based parsing in the past because of the efficiency

19:14.880 --> 19:16.040
reason.

19:16.040 --> 19:22.200
But these days, because of this GPU, now comparing n number of things versus n squared,

19:22.200 --> 19:27.040
actually it's not that much different because everything is done in parallel now.

19:27.040 --> 19:33.760
So a graph based parsing, I couldn't even believe, but it actually is performing as fast

19:33.760 --> 19:35.200
as a transition based parsing.

19:35.200 --> 19:36.200
Okay.

19:36.200 --> 19:42.760
And was the transition based parsing also being done with neural networks?

19:42.760 --> 19:44.840
It can be trained on the neural networks.

19:44.840 --> 19:49.720
So transition versus graph is depending on the parsing algorithm.

19:49.720 --> 19:55.240
So it's a different way of processing the text.

19:55.240 --> 20:01.120
And once you define your parsing algorithm, you can use any kind of machine learning algorithms.

20:01.120 --> 20:07.800
So you can use as easy as logistic regression, like one layer, simple or support vector

20:07.800 --> 20:09.640
machines in all days.

20:09.640 --> 20:15.840
But now everyone is moving on to using neural networks to take advantage of the more accurate

20:15.840 --> 20:17.000
deck.

20:17.000 --> 20:21.360
And so that could be the language parsing piece.

20:21.360 --> 20:27.240
And then the, I can't even read my own hand right here, character mining, character

20:27.240 --> 20:28.240
mining.

20:28.240 --> 20:29.240
Yes.

20:29.240 --> 20:30.240
Yes.

20:30.240 --> 20:34.640
Well, it's on usual title too, so yeah.

20:34.640 --> 20:38.200
And so character mining, so you describe this.

20:38.200 --> 20:44.160
If I'm remembering the scenario, like you've got two or multi-parties in a conversation

20:44.160 --> 20:49.440
and you're trying to basically have the machine understand what there is, like a machine

20:49.440 --> 20:50.440
comprehension.

20:50.440 --> 20:51.440
Right.

20:51.440 --> 20:55.400
But it's, it's like a third-party comprehension, you're trying to comprehend a dialogue.

20:55.400 --> 20:56.400
Exactly.

20:56.400 --> 21:00.880
So yeah, I want to, thanks for pointing that out, actually, you understood this correctly.

21:00.880 --> 21:06.720
So I, yeah, I want to distinguish this project versus like a traditional other, what everyone

21:06.720 --> 21:11.760
is going for as a conversation agent that's like a conversation between the agent, the

21:11.760 --> 21:17.120
computer agent and the human, that's not my goal, because I want to have a third-party

21:17.120 --> 21:20.800
agent that's listening to our conversation.

21:20.800 --> 21:25.160
And if we need something that, if you're missing from this conversation, it can actually,

21:25.160 --> 21:30.280
so the best application for this kind of approach is maybe in your business meeting.

21:30.280 --> 21:35.680
So business meeting, you always have a security writing notes for you.

21:35.680 --> 21:41.400
And this conversation, this character mining project can really help to automate the process.

21:41.400 --> 21:47.600
Or like if you had some birthday party with your son last time, and you forgot what kind

21:47.600 --> 21:53.760
of twist you were going to buy for him, then you are pretty embarrassed to ask to your son

21:53.760 --> 21:54.760
about it.

21:54.760 --> 21:56.760
Then you can ask this to assist you.

21:56.760 --> 21:57.760
Right.

21:57.760 --> 22:00.960
So you can see for all your embarrassment as well.

22:00.960 --> 22:01.960
Yeah.

22:01.960 --> 22:06.480
I'm not sure if it's a good application or a bad application, but I'm imagining all kinds

22:06.480 --> 22:12.560
of domestic applications between husbands and wives.

22:12.560 --> 22:15.760
Alexa, you said they were going to buy the milk?

22:15.760 --> 22:16.760
No.

22:16.760 --> 22:20.800
You know, actually Sam, you said you were going to buy the milk, right?

22:20.800 --> 22:23.600
You have a hard proof phrase.

22:23.600 --> 22:27.680
So, I mean, all this privacy issue actually will come out.

22:27.680 --> 22:32.040
So I don't imagine people would like it to be listening to you all the time.

22:32.040 --> 22:33.040
All the time, right?

22:33.040 --> 22:38.120
Well, I mean, like our conversation like the interview like this is a perfect situation.

22:38.120 --> 22:42.280
If the machine is listening to us, if later on, I wouldn't actually make sure if I said

22:42.280 --> 22:46.520
things right, I don't have to bother you, right?

22:46.520 --> 22:48.880
I can just ask the machine too.

22:48.880 --> 22:51.480
The word character there.

22:51.480 --> 22:52.480
What is the character?

22:52.480 --> 22:53.480
Character is people.

22:53.480 --> 22:54.480
Okay.

22:54.480 --> 22:56.760
Character as a people.

22:56.760 --> 23:01.800
So the project actually is rather fun because it's actually my student's favorite project.

23:01.800 --> 23:07.040
Whenever they come to my lab, research lab, they always want to work on this project instead

23:07.040 --> 23:12.880
of some other one because it's a more high level project.

23:12.880 --> 23:20.160
So I started this because I believe conversation data is going to be the key to natural and

23:20.160 --> 23:22.280
gender understanding in the future.

23:22.280 --> 23:31.240
So I have a kind of evidence for this too because how often do you text all the time?

23:31.240 --> 23:32.560
You can even count, right?

23:32.560 --> 23:36.440
But how often do you, so compare to email, right?

23:36.440 --> 23:40.720
Email you probably pretty frequently, but not as much as text, but if you think about

23:40.720 --> 23:44.400
like blogging, even Twitter stream, right?

23:44.400 --> 23:48.720
All these things, what people are focusing on to do the data money, doesn't even come

23:48.720 --> 23:53.080
close to the conversation you're making on the text.

23:53.080 --> 23:59.000
So conversational data, the amount of the conversation data is growing way, way faster

23:59.000 --> 24:07.400
than any other type of data, but natural language processing is one of the genre NLP is missing

24:07.400 --> 24:08.400
so much.

24:08.400 --> 24:17.120
NLP does very well for newswire kind of formal data, or also even like a Twitter kind of social

24:17.120 --> 24:20.640
media data, people have been vaccinated for past 10 years.

24:20.640 --> 24:25.880
So they are doing very well for those domains at the moment, but conversation data is still

24:25.880 --> 24:27.560
hugely lacking.

24:27.560 --> 24:31.840
That's why this Alexa actually, Alexa Price, this kind of challenge is a very challenging

24:31.840 --> 24:32.840
task.

24:32.840 --> 24:33.840
And what's the Alexa Price?

24:33.840 --> 24:44.080
Oh, Alexa Price is, so we're in the, we're in the, the win, or one of the win hotels

24:44.080 --> 24:45.080
and...

24:45.080 --> 24:46.080
What are you looking for?

24:46.080 --> 24:47.080
Okay.

24:47.080 --> 24:48.080
Oh, you guessed it.

24:48.080 --> 24:49.080
Yeah, yeah, yeah.

24:49.080 --> 24:51.840
Stop, let's just stop.

24:51.840 --> 24:52.840
It's not there yet.

24:52.840 --> 24:55.720
So I've just muted it again.

24:55.720 --> 25:00.560
So we're in the, the win hotel, and some folks may remember that a couple of years ago,

25:00.560 --> 25:06.600
I think it may have been here at a reinvent, Amazon and the win hotels announced that they

25:06.600 --> 25:09.560
were putting an Alexa in every hotel room.

25:09.560 --> 25:10.560
Really?

25:10.560 --> 25:11.560
They did.

25:11.560 --> 25:12.560
Yeah.

25:12.560 --> 25:13.560
That's just part of the hotel room.

25:13.560 --> 25:14.960
Oh, actually, you didn't bring this.

25:14.960 --> 25:17.240
No, no, it was a part of the hotel.

25:17.240 --> 25:19.560
It's actually pretty awesome.

25:19.560 --> 25:21.800
Alexa, open the drapes.

25:21.800 --> 25:24.800
Oh my God.

25:24.800 --> 25:25.800
That's pretty cool, isn't it?

25:25.800 --> 25:26.800
Full motivation.

25:26.800 --> 25:27.800
Yeah.

25:27.800 --> 25:28.800
Hotel automation, in this case.

25:28.800 --> 25:29.800
Yeah.

25:29.800 --> 25:35.600
So yeah, Alexa Price is a competition that allows research to show off their research skills

25:35.600 --> 25:36.600
on the conversation agent.

25:36.600 --> 25:37.600
Okay.

25:37.600 --> 25:43.040
So they had a huge competition, and one of my colleagues from Emory actually participated

25:43.040 --> 25:45.680
and they were very close to win the third place.

25:45.680 --> 25:47.720
I think they won the fourth place.

25:47.720 --> 25:52.280
So they were not one of the finalists, but yeah, next year, probably.

25:52.280 --> 25:54.760
And what's the problem, problem formulation of...

25:54.760 --> 25:57.680
The problem formulation is you are supposed to be...

25:57.680 --> 26:04.880
So there are many judges for this competition, and each judges basically talk freely in

26:04.880 --> 26:05.880
open domain.

26:05.880 --> 26:07.680
It's not necessarily even questions.

26:07.680 --> 26:11.880
You're just trying to make a conversation with Alexa.

26:11.880 --> 26:18.040
And if judge feels it actually is in the into the level of the satisfaction, they give

26:18.040 --> 26:24.240
some kind of scores, and they gather all those scores to see who actually has the best

26:24.240 --> 26:25.240
model.

26:25.240 --> 26:30.680
Because it's open-ended, kind of like gymnastics in the Olympics, you're trying to impress

26:30.680 --> 26:32.480
the judges with what Alexa can understand.

26:32.480 --> 26:33.480
Exactly.

26:33.480 --> 26:34.480
Oh, wow.

26:34.480 --> 26:35.480
That's...

26:35.480 --> 26:36.480
And it sounds like it's an ongoing...

26:36.480 --> 26:37.480
It is an...

26:37.480 --> 26:38.480
It's been there for...

26:38.480 --> 26:41.000
I know it's been there at least for two years.

26:41.000 --> 26:42.480
I think it's been...

26:42.480 --> 26:45.480
And I know they are going to do again for sure next year.

26:45.480 --> 26:52.080
The first year was the winner of the competition actually gets $1 million, so it's a serious

26:52.080 --> 26:53.080
deal.

26:53.080 --> 26:57.760
From the faculty point of view, it's not just a prize, but it's also a very intellectually

26:57.760 --> 27:00.200
challenging problem too, so it's very interesting.

27:00.200 --> 27:06.960
Plus, I mean, they actually give out the support for AWS Cloud Computing and also the PhD students.

27:06.960 --> 27:10.800
If you are selected, one of the top-aids teams, so...

27:10.800 --> 27:14.120
You were talking about how we've got all this news.

27:14.120 --> 27:20.640
You know, we have a mature set of practices for applying this type of analysis to news.

27:20.640 --> 27:28.120
We've got kind of these news data sets where much further behind and conversational.

27:28.120 --> 27:30.240
Is it like a chicken and an egg kind of problem?

27:30.240 --> 27:34.320
Like, we don't have the data set because we haven't been working on it as hard and...

27:34.320 --> 27:41.280
And so, I mean, I think it was basically based on the initial interest of the NLP.

27:41.280 --> 27:47.160
So initially, as I mentioned, the Pantry Bank was the largest corpus that we had for a

27:47.160 --> 27:48.160
while.

27:48.160 --> 27:54.320
And it's not anymore, but it's still one of those initial tribank that was...

27:54.320 --> 27:59.440
That allowed us to do any meaningful statistical based natural image processing.

27:59.440 --> 28:00.440
So...

28:00.440 --> 28:04.200
And I think their intention originally was to analyze news data.

28:04.200 --> 28:07.400
So that's why they chose the genre of the news.

28:07.400 --> 28:11.720
If they chose the genre of like conversation data, it would have been probably a good

28:11.720 --> 28:13.680
guarantee to that side.

28:13.680 --> 28:17.000
But I mean, government had their own reasons, right?

28:17.000 --> 28:22.600
So and the tricky part about the conversation data though, would you be willing to give

28:22.600 --> 28:24.360
out your text data to other people?

28:24.360 --> 28:31.600
I was thinking about this as you're describing this like, you know, if, yeah, I'm thinking

28:31.600 --> 28:36.040
about like what kind of incentive system, you know, some app might have to put into place

28:36.040 --> 28:41.480
so that, you know, a user would buy into the idea of using this messaging system knowing

28:41.480 --> 28:43.440
that they were sending all their data someplace.

28:43.440 --> 28:44.440
That's a tough ask.

28:44.440 --> 28:52.120
Yeah, yeah, so I mean, I even tried to ask my own student and I actually even tried to

28:52.120 --> 28:56.800
sign up some kind of form to them that I would not reveal this data to anyone else.

28:56.800 --> 29:01.560
But I mean, there were actually students who were willing to just give to me, but most

29:01.560 --> 29:03.640
of them said no, obviously.

29:03.640 --> 29:09.560
So that was the first barrier we couldn't find a good large amount of data that has a human

29:09.560 --> 29:10.920
conversation.

29:10.920 --> 29:19.320
So the closest data that we found is Twitter, actually coming from the TV shows.

29:19.320 --> 29:27.920
So we got the 10 years of the transcript from the show called Friends, which, so that

29:27.920 --> 29:33.120
show does have some use, exactly, and basically everyone knows that you and they're making

29:33.120 --> 29:36.280
reruns for another 10 years now, right?

29:36.280 --> 29:42.760
So we actually made the effort to make the transcript, it was transcript actually was done

29:42.760 --> 29:47.000
by all the fan-based, but none of them was structured.

29:47.000 --> 29:51.480
So we actually had to spend a year to make the data structurized.

29:51.480 --> 29:58.720
After that, we have a fairly clean data for this script and we particularly liked this

29:58.720 --> 30:02.720
show because this show is not talking a particular thing.

30:02.720 --> 30:03.720
It's just not jargonous.

30:03.720 --> 30:04.720
Exactly.

30:04.720 --> 30:05.720
Life.

30:05.720 --> 30:06.720
Yeah.

30:06.720 --> 30:07.720
For people.

30:07.720 --> 30:12.160
But so that's why we chose this over some other shows.

30:12.160 --> 30:18.240
So even that is involving a lot more like humorous sarcasm.

30:18.240 --> 30:19.240
Right.

30:19.240 --> 30:24.840
So when we talk about it right now, so we don't make so as much humor or sarcasm or all

30:24.840 --> 30:28.600
this metaphors, they intentionally do so much time.

30:28.600 --> 30:33.280
So people think these scripted languages are actually easier to process, but they're

30:33.280 --> 30:36.640
actually harder because of all these reasons.

30:36.640 --> 30:42.880
So anyway, so we have that data now and given the data, the first experiment we ran is,

30:42.880 --> 30:47.800
is it possible to tell like when you talk, when these people are talking to each other,

30:47.800 --> 30:54.280
is it possible to tell who is talking to others or if, say, to one of the character in the

30:54.280 --> 31:01.200
friends and named Ross, his sister is also in the show called Monica and they are Ross

31:01.200 --> 31:06.000
and Monica are talking together and they're talking about their mom and dad.

31:06.000 --> 31:10.040
And their mom and dad actually appears some other episodes.

31:10.040 --> 31:15.400
So we were trying to experiment, is it possible when they talk mom and dad, if the machine

31:15.400 --> 31:19.840
will be able to figure out who that mom actually is?

31:19.840 --> 31:22.680
So like entity disambiguation.

31:22.680 --> 31:24.360
So what we call entity linking.

31:24.360 --> 31:25.360
Entity linking.

31:25.360 --> 31:28.360
Entity disambigation is also right terminology.

31:28.360 --> 31:34.600
So yeah, so basically you have to listen to the entire show, pretty much the entire show,

31:34.600 --> 31:39.440
but usually those people are gonna appear within next few episodes, so you don't have

31:39.440 --> 31:45.920
to search that much, but your search space is still out of like 400 characters, out

31:45.920 --> 31:52.480
of 400 characters, which one of these is mom, Ross's mom, Monica's mom.

31:52.480 --> 31:59.400
So that's the first test we ran, we actually even ran the international, what we call

31:59.400 --> 32:04.640
semi-veils, the semantic evaluation test last year and I think it was well received.

32:04.640 --> 32:09.720
A lot of people get very much interest and I'm still getting a lot of requests from the

32:09.720 --> 32:10.720
people too.

32:10.720 --> 32:17.160
So that was the first test, we need to figure out like who these people are talking about.

32:17.160 --> 32:20.840
So that was the first part of the character mining project.

32:20.840 --> 32:24.320
So second task that we tackled.

32:24.320 --> 32:29.520
And if I can just ask a question about that, you're doing that in a totally model free

32:29.520 --> 32:30.520
way.

32:30.520 --> 32:37.320
It's all happening with kind of a flat algorithm as opposed to pre-processing to identify

32:37.320 --> 32:41.880
people and then remembering the people that are talked about and kind of linking.

32:41.880 --> 32:47.960
It's not bad, it's like just running, you know, kind of training a neural network on

32:47.960 --> 32:48.960
this.

32:48.960 --> 32:53.200
So having the neural network, you send in a sentence and having the neural network point

32:53.200 --> 32:57.960
you to the sentence that refers back to the people that you're talking about.

32:57.960 --> 32:58.960
It could be so.

32:58.960 --> 32:59.960
For example.

32:59.960 --> 33:00.960
For example.

33:00.960 --> 33:01.960
Exactly.

33:01.960 --> 33:05.920
So you sent it different from traditional entity linking task which is also known as

33:05.920 --> 33:07.560
a wikipication.

33:07.560 --> 33:13.440
So wikipication is another entity linking task, much more well-known, which is given

33:13.440 --> 33:19.920
like a name, like Lincoln, you find a wikipedia page for that link.

33:19.920 --> 33:22.200
So you have to do some disambigation as well.

33:22.200 --> 33:26.640
Lincoln can be a link on hall or a link on bridge or actually every link.

33:26.640 --> 33:27.640
Yeah.

33:27.640 --> 33:28.960
So so many different links, right?

33:28.960 --> 33:34.240
So that, but those task has significant advantage that all those entities are already

33:34.240 --> 33:35.240
well-known.

33:35.240 --> 33:39.760
So you already have some good knowledge base established for this entities.

33:39.760 --> 33:43.120
But for this show, we don't have any of these knowledge.

33:43.120 --> 33:48.080
We are not assuming any of these knowledge base, basically the machine has to listen

33:48.080 --> 33:53.240
to these conversations and figure out if there's a link between these mentions.

33:53.240 --> 33:57.560
So it's a much more challenging task as in sense, yeah.

33:57.560 --> 34:04.280
And so you talked about another couple of projects, the kind of what I have is what I got

34:04.280 --> 34:08.520
out of that was like interpreting structure and kind of semi-structured reports, like

34:08.520 --> 34:09.520
medical reports.

34:09.520 --> 34:10.520
Oh, yeah.

34:10.520 --> 34:11.520
Yes.

34:11.520 --> 34:16.760
And another one, which is a really interesting application of identifying the early indicators

34:16.760 --> 34:21.480
of cognitive impairment through speech.

34:21.480 --> 34:26.600
But as we're kind of coming up on the end of our time here, I did want to ask you about

34:26.600 --> 34:33.240
one of the presentations you gave here at re-event about this NLP platform that you built.

34:33.240 --> 34:37.000
What is the NLP platform that you built?

34:37.000 --> 34:41.200
So this is a project called Elite ELIT.

34:41.200 --> 34:47.000
So it's short for evolution of language and information technology.

34:47.000 --> 34:53.080
We call this evolution because I think the way that people are doing language technology

34:53.080 --> 34:58.440
has to be evolved into the modern computing technology.

34:58.440 --> 35:05.080
So what I mean by that also is all these natural language processing models lately, as I said,

35:05.080 --> 35:08.240
are making heavy use of deep learning.

35:08.240 --> 35:15.080
Deep learning is a neural network with multiple layers and it requires a lot of computations.

35:15.080 --> 35:21.200
Because of this, to run, to train or decode any of the state of our models, you really

35:21.200 --> 35:25.400
need to have a very high computing GPU power.

35:25.400 --> 35:32.240
All days, when I was actually my PhD dissertation was purely focused on the efficiency of NLP

35:32.240 --> 35:39.640
models, how to make the model as small as possible while not losing accuracy.

35:39.640 --> 35:41.840
So you can actually run on your laptop.

35:41.840 --> 35:46.560
So all my models were able to, you could run on your laptop, as long as you have like

35:46.560 --> 35:48.640
a three gigabyte sub-ramp.

35:48.640 --> 35:55.080
But now, because after all this neural network error, it's almost impossible to run this

35:55.080 --> 36:03.400
into your local machines. So a lot of researchers are having difficulties of making use of this

36:03.400 --> 36:04.400
technology.

36:04.400 --> 36:09.400
And when we're talking about the size, are we talking about training or inference?

36:09.400 --> 36:10.400
Both.

36:10.400 --> 36:11.400
Okay.

36:11.400 --> 36:16.800
So for both training and inference, you do need a GPU to run this neural network models.

36:16.800 --> 36:21.600
At least if you want to run the state of the art models, not the simple toy models,

36:21.600 --> 36:23.280
but really good models.

36:23.280 --> 36:27.760
And in all days, before neural network with the linear models, it was still possible

36:27.760 --> 36:31.200
to optimize those models to make it very small.

36:31.200 --> 36:36.480
So without losing so much accuracy, so you could just run any researchers like linguist

36:36.480 --> 36:39.880
or historian, they could just run on their laptops.

36:39.880 --> 36:44.840
So these people still want to take advantage of all these latest technology that we are

36:44.840 --> 36:48.000
developing, which are tons and tons.

36:48.000 --> 36:53.760
But now it's literally impossible for anybody who doesn't have these high computing machines

36:53.760 --> 36:56.360
to run this kind of models.

36:56.360 --> 37:02.280
So there is a solution for it, which we found a solution from a cloud computing.

37:02.280 --> 37:09.440
So the platform, the elite platform that we are developing is a SaaS architecture.

37:09.440 --> 37:14.720
So we are bringing all our natural language processing models to the cloud.

37:14.720 --> 37:18.800
So anybody in the world can actually call this as a web API.

37:18.800 --> 37:24.840
Just like the way you use Google search, you can just send your text and it will send

37:24.840 --> 37:30.160
you back your NLP result with a simple HTTP call.

37:30.160 --> 37:36.520
And so is this a generic platform that you can then plug in these different projects

37:36.520 --> 37:42.280
as the types, the calls that you would make and you would get back?

37:42.280 --> 37:49.120
And so what I just described about web API in SaaS environment, that was the original

37:49.120 --> 37:51.560
intention for our lab.

37:51.560 --> 37:57.440
So because we actually started having so much needs of running this NLP models.

37:57.440 --> 38:02.240
So we actually developed the SaaS architecture for ourselves first.

38:02.240 --> 38:07.480
So my students can actually just use that without installing anything on their local machines.

38:07.480 --> 38:13.280
But we tried to bring that up to the cloud so everyone in the world can actually use it.

38:13.280 --> 38:17.880
And thanks to AWS, who has been supporting us for making this happen.

38:17.880 --> 38:21.800
But now the intention actually became much more interesting.

38:21.800 --> 38:29.600
So the motivation at the moment now is we want to invite all NLP community, people to

38:29.600 --> 38:33.440
contribute their models to this cloud.

38:33.440 --> 38:39.640
So this is actually huge because how many papers do you think are getting published every

38:39.640 --> 38:42.640
year to like top three conferences?

38:42.640 --> 38:47.280
NLP or NLP, like top three conferences in LLP.

38:47.280 --> 38:56.000
Gosh, how many papers a year to the top three conferences in NLP, 2,500?

38:56.000 --> 38:57.520
It's still 1,000.

38:57.520 --> 38:58.520
All right.

38:58.520 --> 38:59.520
Yeah.

38:59.520 --> 39:00.520
I didn't do the count.

39:00.520 --> 39:15.880
But even the workshop papers and all NLP, I would imagine like a 5,000 of those.

39:15.880 --> 39:23.080
So in every single paper, basically claims they have the best model of some sort, right?

39:23.080 --> 39:25.320
Otherwise, you wouldn't be published.

39:25.320 --> 39:30.480
So every paper has developed some kind of model that has some value.

39:30.480 --> 39:36.160
How many of them are actually getting used, right, less than few percent?

39:36.160 --> 39:38.280
So why is that though?

39:38.280 --> 39:43.520
These PhD students are spending at least five years of their life dedicated to develop

39:43.520 --> 39:47.760
these models and after they graduate, nobody uses it.

39:47.760 --> 39:53.200
So I think this is a issue and it's a very depressing issue to me at least because I want

39:53.200 --> 39:58.920
all like the models that we develop are has to be getting used in some sense, but it's

39:58.920 --> 40:01.600
not getting used at all, right?

40:01.600 --> 40:09.360
So it strikes me that some of that issue is accessibility and cloud and platforms dress

40:09.360 --> 40:10.360
that.

40:10.360 --> 40:16.400
But if there's also the assumptions that the model makes, a lot of them are rather limiting

40:16.400 --> 40:18.400
for actual use.

40:18.400 --> 40:19.400
Excellent.

40:19.400 --> 40:20.400
That's an excellent point.

40:20.400 --> 40:25.440
That's what I want to actually open up the free platform for people to actually show

40:25.440 --> 40:28.200
off your model actually works well.

40:28.200 --> 40:29.200
Right?

40:29.200 --> 40:34.760
So yes, you have proof that your model has value in your paper in this sense, but does

40:34.760 --> 40:40.560
it actually do the right thing for real application or your application?

40:40.560 --> 40:49.040
So if the developers or the researchers truly believe their model has value, they can deploy

40:49.040 --> 40:54.480
their model to our platform and anybody in the world can test it out.

40:54.480 --> 41:00.080
And see, okay, this model actually does work really well, then it gets promoted.

41:00.080 --> 41:05.040
So we will actually have to show the ranking of the usage of the models as well.

41:05.040 --> 41:10.520
So anybody in the world can actually see, okay, this model does well for this kind of

41:10.520 --> 41:11.520
event.

41:11.520 --> 41:15.040
And we can actually include to the discussion forum to see, okay, this model didn't do

41:15.040 --> 41:19.200
well for this genre, but this genre it did exceptionally well.

41:19.200 --> 41:22.640
So users can actually write their reviews about this.

41:22.640 --> 41:26.240
It's like that's what the technology should be, right?

41:26.240 --> 41:31.120
When you have a new phone, when you have new technology, there's so many reviewers write

41:31.120 --> 41:33.320
the reviews about it.

41:33.320 --> 41:39.280
But our in academic world, what happens is you saw me on paper, the paper gets reviewed

41:39.280 --> 41:45.040
by three reviewers and that was it later on people forget about it.

41:45.040 --> 41:48.800
But actual usage can be now reviewed by people.

41:48.800 --> 41:53.480
So this is I think a very exciting part, also another part that I really want to focus

41:53.480 --> 41:59.480
on is I believe there are a lot of models that are really good, but are not getting published

41:59.480 --> 42:01.400
for some reason.

42:01.400 --> 42:07.160
So for many different reasons, we all know this, sometimes this review system doesn't work

42:07.160 --> 42:08.800
on some others.

42:08.800 --> 42:16.200
So and also another thing is somebody actually is not really intended to invent some algorithm,

42:16.200 --> 42:21.760
but they replicate someone else's work, but happen to have a better performance.

42:21.760 --> 42:26.560
And the academic conferences don't allow you to really write papers about this, replicating

42:26.560 --> 42:28.160
someone else's work.

42:28.160 --> 42:31.800
There are some venues, but usually it's not as valued.

42:31.800 --> 42:36.760
But now these people can actually deploy all their models to our platform, then they

42:36.760 --> 42:41.720
can actually show, okay, so I use the same algorithm, but my model actually works more

42:41.720 --> 42:47.640
efficiently or better in some sense, so please take a try out and see if it actually works

42:47.640 --> 42:48.640
well.

42:48.640 --> 42:54.760
So you have to open the door to the developers and researchers to show off how well of engineering

42:54.760 --> 43:00.320
or like researchers they have done and prove it to the world from the practical point of

43:00.320 --> 43:01.320
view.

43:01.320 --> 43:07.120
How does a researcher format their model to submit it to your platform?

43:07.120 --> 43:08.120
Right.

43:08.120 --> 43:14.280
So I tried so many different ways, I have an NLP platform called, it used to be called

43:14.280 --> 43:17.480
clear NLP, now it's called NLP4J.

43:17.480 --> 43:23.080
So this platform has been used quite a bit by industry and academics.

43:23.080 --> 43:27.960
So I used to get at least 500 downloads every month about this software.

43:27.960 --> 43:30.760
So it was a huge project.

43:30.760 --> 43:37.640
The lesson I learned from that is you should not force developers to write their code

43:37.640 --> 43:39.560
into some kind of architecture.

43:39.560 --> 43:40.560
They are not going to fall.

43:40.560 --> 43:44.680
They have their own architecture already, and they are not going to follow whatever you

43:44.680 --> 43:47.120
think is the best for them, right?

43:47.120 --> 43:50.160
So I'm going to give complete freedom to develop.

43:50.160 --> 43:52.440
You can write your model in any way you want.

43:52.440 --> 43:56.440
The only thing that I want to standardize is the IO format.

43:56.440 --> 44:00.840
Input and output format, which in natural and its processing is pretty standard.

44:00.840 --> 44:03.440
The input is text, right?

44:03.440 --> 44:07.440
Output is some kind of generated labels that you have.

44:07.440 --> 44:14.840
So basically, we are putting, given the input text, you are actually generating a dictionary

44:14.840 --> 44:17.760
that has a label of your task.

44:17.760 --> 44:25.080
So once that part is standardized, then all these models can actually even work together.

44:25.080 --> 44:30.520
So this is another unique part about our platform, or rather about natural and its processing.

44:30.520 --> 44:34.040
A lot of models need to work together.

44:34.040 --> 44:36.280
They're not working independently.

44:36.280 --> 44:40.840
So this is a very unique thing, and because of this, if you actually standardize their

44:40.840 --> 44:48.160
IO format, they can actually take advantage of all the other models and extract features

44:48.160 --> 44:51.280
from those models to make even better models.

44:51.280 --> 44:58.920
So I think from the research and user point of view, they can choose any of the models

44:58.920 --> 45:04.040
that deploy to the platform, and you can actually try out which one works better for you.

45:04.040 --> 45:08.000
And the best thing is you don't even have to install anything on your machine.

45:08.000 --> 45:11.080
So you can just call it out from the web and get all the results.

45:11.080 --> 45:18.200
So is the developer, the researcher, are they zipping up a Python folder, or whatever?

45:18.200 --> 45:23.560
Are they zipping something up, and maybe some kind of JSON to tell you where they need

45:23.560 --> 45:31.560
to get their IO, or is it containers, or what's the actual format of the algorithms

45:31.560 --> 45:32.560
that they're submitting?

45:32.560 --> 45:39.320
Right. So we have released SDK, as of yesterday.

45:39.320 --> 45:41.320
So we have SDK for Python.

45:41.320 --> 45:43.960
Oh, you did a big launch here at EOS.

45:43.960 --> 45:52.000
Well, we didn't make an official announcement, but the packages are on the release now.

45:52.000 --> 45:53.000
People can use it.

45:53.000 --> 45:56.000
They don't know where the link is probably.

45:56.000 --> 46:00.760
So as soon as I go back to Atlanta, I'm going to actually try to make a big announcement

46:00.760 --> 46:01.760
about this.

46:01.760 --> 46:07.440
But yeah, so we have SDK that provides you abstract classes, basically.

46:07.440 --> 46:13.040
So as a matter of fact, there's only one abstract class with four abstract functions.

46:13.040 --> 46:16.200
And that's all we're asking you to overwrite.

46:16.200 --> 46:19.760
So you can completely independently develop your model.

46:19.760 --> 46:21.200
Sounds very job-ish.

46:21.200 --> 46:22.200
Yeah.

46:22.200 --> 46:28.200
I'm coming from object-oriented programming background, so it's not really Python-like, but I get

46:28.200 --> 46:29.200
that.

46:29.200 --> 46:34.160
Well, yeah, so we have an environment developed for both Python and Java.

46:34.160 --> 46:36.840
So if you are a Java programmer, you can just submit.

46:36.840 --> 46:40.520
And that was also another great thing about cloud computing, right?

46:40.520 --> 46:45.600
So now these models that's written in either Python or Java can work together, because

46:45.600 --> 46:52.840
it will be connected by the network and communicating between the AWS instances is really fast.

46:52.840 --> 46:53.840
Right.

46:53.840 --> 46:55.760
Do you have a web page for it yet?

46:55.760 --> 46:56.760
Yes.

46:56.760 --> 47:00.280
The project is called at least in the web page is elite.cloud.

47:00.280 --> 47:01.280
Oh, nice.

47:01.280 --> 47:02.280
So E-L-I-T.cloud.

47:02.280 --> 47:03.280
All right.

47:03.280 --> 47:04.280
Sounds interesting.

47:04.280 --> 47:10.560
Well, thank you so much for taking the time to chat with me about what you're up to.

47:10.560 --> 47:12.560
It was a really interesting stuff.

47:12.560 --> 47:13.560
Thank you very much.

47:13.560 --> 47:15.560
And yeah, time flies.

47:15.560 --> 47:16.560
It was very exciting.

47:16.560 --> 47:17.560
Yeah.

47:17.560 --> 47:18.560
Thank you.

47:18.560 --> 47:19.560
Yeah.

47:19.560 --> 47:20.560
I had a lot of fun.

47:20.560 --> 47:21.560
Thank you.

47:21.560 --> 47:26.560
All right, everyone, that's our show for today.

47:26.560 --> 47:33.160
For more information on Geno or any of the topics covered in this episode, visit twomolei.com

47:33.160 --> 47:36.520
slash talk slash 206.

47:36.520 --> 48:05.240
As always, thanks so much for listening and catch you next time.

