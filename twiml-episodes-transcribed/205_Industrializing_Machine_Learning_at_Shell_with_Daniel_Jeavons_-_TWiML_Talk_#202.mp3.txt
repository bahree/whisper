Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington. For those challenged with promoting the use of machine learning
in an organization and making it more accessible, a key to success is to support data scientists
and machine learning engineers with modern processes, tools and platforms.
This is a topic we're excited to address here on the podcast with the AI Platforms podcast
series that you're currently listening to, as well as a series of e-books that will be
publishing on the topic.
The first of these e-books takes a bottoms up look at AI platforms and is focused on the
open source Kubernetes project, which is used to deliver scalable machine learning infrastructure
at places like Airbnb, booking.com and open AI.
The second book in the series looks at scaling data science and ML engineering from the top
down, exploring the internal platforms, companies like Airbnb, Facebook and Uber have
built and what enterprises can learn from them.
If these are topics that you're interested in and especially if part of your job involves
making machine learning more accessible, I'd encourage you to visit Twimbleai.com slash
AI platforms and sign up to be notified as soon as these books are published.
In this episode of our AI platform series, we're joined by Daniel Jevins, General Manager
of Data Science at Shell.
In our conversation, Daniel and I explore the evolution of analytics and data science
at Shell and cover a ton of interesting machine learning use cases that the company is pursuing,
like well drilling and charging smart cars.
A good bit of our conversation centers around IoT related applications and issues, such
as inference at the edge, federated machine learning and digital twins, all key considerations
for the way they apply machine learning.
We also talk about the data science process at Shell and the importance of platform technologies
to Daniel's organization and the company as a whole.
And we discuss some of the technologies that he and his team are excited about introducing
to the company.
And now on to the show.
All right, everyone, I am on the line with Daniel Jevins.
Daniel is the General Manager for Data Science at Shell.
Daniel, welcome to this weekend machine learning and AI.
Thanks so much for having me.
Before we jump in, I'd love to hear a little bit about your background and how you got
to working in data science at Shell.
I know previously you were the General Manager for Advanced Analytics at the company and
you've worked in that capacity for a while.
Tell us a little bit about your background.
Yeah, so it's not a straightforward story.
When I left university, I joined Accenture as many of us did in that era and spent a
bunch of time developing big systems.
And I guess the Eastward described what I was doing was data engineering without any of
the tools.
We used a lot of SQL.
That sounds fun.
It was tough back then.
We hacked a lot of stuff together in Excel and made some crazy business insight for executives
based on core business processes and taking raw data out of SAP.
It was a fun time.
We spent most of the evenings extracting data and then run it overnight and come back and
hope the results worked in the morning.
And that worked pretty well, but it was a fun start and I learned a lot from that.
So I also got really interested in the interface between data and process.
And I guess the easiest way to describe my career is that I've always been in that interface.
And so then I then spent a lot of time learning about things like Six Sigma and Lean and trying
to apply some of those techniques into the business processes that I was extracting
data for and trying to improve the way that our business ran.
And I went through a whole series of roles in that sort of area from data architecture
to process architecture in the CI office in a more strategic role.
And then eventually got to the point where I saw back in about 2012 that increasingly
there are a whole series of new tools coming out that were starting to apply data science
in really interesting ways to improve the way that the business ran.
And what I saw was that this was going to be a big thing and it was going to change
the way in which industry did business.
And so I went to my manager who was at the time Shell's lead architect and I said to him,
look, I see a big opportunity in this space.
Would you let me go and start something?
And he said, actually, we've just created a new role.
The IT executive wants to do something as well.
Why don't you apply?
And so I applied for what we then called the predictive analytics center of excellence
lead position.
And I think they didn't really know what it was or what it was supposed to be.
So they appointed me.
And I sort of went back to my roots and I said, well, what do I need to make this successful?
Well, I need some good data engineers and I need some people who understand stats.
And so I hired a statistician and I went back to some of my old colleagues and brought
in a data engineer and we got started.
And we didn't really know what we were doing back then.
We were sort of making it up as we were going along.
We got a lot of input from others and talked to a lot of different companies.
So what was really interesting was we figured out pretty early on that the way this works
is you focus on the value and actually getting good at articulating where the value is from
a business use case is really fundamental.
And what we did was we effectively built out some cases around the customer space.
We also focused on some work in our finance space and some work in the asset space.
And all three of those paid pretty modest dividends pretty quickly.
And that gave us a bit of a mandate and a bit of a momentum to start to grow.
And I think maybe skipping forward a little bit, we went through a couple of years of sort
of modest growth.
But what were really changed was we hit on something that really resonated and we ran
our first big project, which was focused on spare part inventory optimization.
And we were able to deploy that quite quickly as a minimum viable product and it suddenly
started to make millions of dollars.
And it was at that point that the game shifted and suddenly this became a big thing and it
became very much on the agenda of the executives and they got very interested in what we could
do.
And from there, really the team has been growing very, very rapidly.
And I've been through various incarnations of positions, as you mentioned, to my current
position where I'm now running about a team of about 130 people worldwide based in four
global locations, combination of data scientists and data engineers.
And we're running use cases right across the business.
Interesting.
I'm really looking forward to digging into some of those use cases.
I didn't want to ask though, seeing this transition from advanced analytics, COE to data
science, COE, tell me about the kind of background behind that change.
Is it more significant than rebranding the role and the mission or does it have a significant
meaning to the way you do business?
Well, I think if you go back to the beginning, we had the idea really of putting statistical
methods into software applications and deploying them to end users.
That was if you like the initial mission that we came up with.
But I think we always saw how this could evolve because even back in, we're talking back
in 2013 now, we could see the early writing on the wall around the development of machine
learning and also things like natural language processing and AI and the way in which this
could evolve.
And so we had that in mind, but we have focused very much on the simpler things and trying
to deliver value with what we're building out then.
I think it's been a pretty natural evolution, but of course, as you evolve, you need to
describe what you do a bit better.
And where it started off with, as I said, deploying those statistical methods, we're now
deploying advanced machine learning at scale.
And so we need a title that reflects that.
So I think it's not that the mission or the vision has changed, but just we've managed
to follow the roadmap and hence needed to rebound.
One of the things that you mentioned in your background is this.
Actually, there are a couple of things that I wanted to probe into.
One is around this idea of kind of the interface between process and data and analytics and
how you apply these techniques at that interface.
But you also mentioned really needing to understand the value that these types of projects can
bring to the table.
Starting with that, what have you learned about kind of capturing the essence of the value
proposition of these kinds of projects?
So I think there's a couple of things I would say.
So just linking to that process and data point.
I find that people tend to look at the world through one or other lens.
So business folks often tend to look at it through the lens of the process and forget
about the data.
And data practitioners often come at it from the data side and forget about the process.
And the answer is you've got to understand both to really be successful.
If I talk about that inventory use case, which I talked about as our big launching case,
the key there was that interface of process and data.
So what we understood was the data allowed us to develop certain recommendations around
the stock levels that we were operating.
And off the back of that, we could improve statistically the recommendations we were
giving to the industry analyst.
And that was relatively straightforward.
But what we then had to do was work really closely with the experts in the business through,
in this case, the materials management center of excellence internally.
So the team that looked after materials management as a discipline and try and work with them
to say, how are we going to fit a tool into your business process?
And the reason it was successful was because we made a tool which made it really, really
easy for the inventory analyst to do their job.
And ultimately, that's why it was so successful.
So I think that's one element of how you get the value.
I think the other elements of getting to the value is understanding the friction.
So again, if I use this as an example, what we understood was the inventory analyst could
probably get to the same recommendations that the algorithm was making or close to, but
it would take them several weeks.
And that's not scalable, and it's not even doable because what that means is it doesn't
get done.
So what we quickly figured out was that actually understanding that friction point, which
was this saves the inventory analyst time, was key to getting the value of which also leads
to better decision making and off the back of that bottom line impact.
So it's that friction understanding that's been really key to our journey.
I think the other thing is, and maybe one of the other big learnings and learn from some
of my failures here as well, often the business will ask questions which are thinly veiled.
I have lots of data looking for a problem problems.
And that's the worst type of definition because ultimately more data is not better.
What's better is to have better insight, and that typically means small data.
And so the challenge of any analytics project is taking big data into small insights, which
allows someone to make a better decision.
You mentioned that better insights typically mean small data.
Can you elaborate on that?
I think that is counterintuitive to the way we think about a lot of these problems nowadays.
Yeah, I think so.
I mean, it's not saying that you don't use a lot of data in coming to the insights,
but ultimately, if you look at, let's take an operator in the North Sea, for example,
we're currently asking them to deal with probably 10,000 data points on any given day.
So they should be looking at the incoming variables from 10,000 different sensor feeds about
their plans.
No one can process that amount of data.
And ultimately, you end up with just information overload, which means you shut down or you switch
off or you manage on intuition or gut feel.
And the challenge of data science is, can I take all of that incoming data that I have
and turn it into the three things that that operator needs to look at and make sure that
they really pay attention to those three things because that's what we're comfortable
with dealing with as human beings.
So a lot of my team and the thinking that we try to instill in people is, look, you've
got to make it easy for your user.
We like our iPhones because the app is typically only giving you one or two or three pieces of
information, that's what a well designed app does.
And you might deal with a lot of apps in a day, but actually each one is pretty targeted
in the way that it surfaces information to you.
And that's really the same sort of thinking we try and bring into the design of the things
that we build for our business users.
So I recently had an opportunity to hear one of your colleagues talk a little bit about
the various use cases of machine learning at Shell at a conference.
And I'd love for you to kind of run through that list.
I mean, there were, you've got a ton of things going on in this space.
Can you give us a taste?
Yeah, of course.
So we're looking at the application of machine learning into areas in the subsurface,
for example, like well drilling where we're trying to automate the way in which we do
geo-steering using machine learning, we're also doing work in production looking at predictive
maintenance.
So in other words, how can we predict failure on piece of equipment?
That's everything from valves to compressors to heat exchangers.
And going back to the example I gave on the operator, trying to give our operators really
meaningful insight around the areas they need to focus on.
We're also looking at optimization of those assets.
So how can we use machine learning and other techniques to optimize the throughput of our
manufacturing equipment, our production equipment?
We're working with trading in a number of areas, trying to improve the way in which we manage
our trading portfolios, the way we manage risk, the way in which we also make bets and take
positions in the market.
We are looking at optimizing our lubricants in the way that we blend our lubricants, effectively
trying to leverage larger data sets to understand how we can make our products more effectively
or more efficiently.
We're working in retail, trying to develop new insights for our customers, but also trying
to make our sites safer by using things like machine vision.
And actually machine vision is an area that we're looking at far more broadly because leveraging
video footage across our very physical value chain is a huge opportunity.
And then we're also working in the new energy space where we're starting to try to develop
algorithms to charge cars more smartly and also save our customers money in the process.
So that's just a few examples that I could go on for a long time.
How does your organization support such a broad portfolio of application areas?
Are you kind of embedded into the different business units or business processes or do
you operate in a more centralized way?
So I think it's a combination.
So the way I describe my team is we try and provide a technical backbone to these projects
to make sure that we do them consistently.
We operate with common standards and we deploy them to a high level of delivery.
So we're trying to act as a true center of excellence if you will.
We're often bringing in resources, I would say almost always bringing in resources from
other parts of the organization.
We partner very closely with our business service centers because they have some great
data scientists there.
We also partner closely with our IT colleagues because we need them to help us operationalize
the things that we're building.
And of course we want business people embedded in these projects because a lot of this
doesn't work unless you're very close to the user and the person with the problem who
really understands the case that you're trying to develop.
So our most successful stories are those where all of those ingredients are there.
So I think it's the best way to describe it is we're involved in a lot of these projects.
We're providing technical assurance, expertise, we're often hands-on in terms of delivery,
but we're not doing it on our own.
We bring together cross disciplinary working teams to deploy the outcome.
You mentioned operationalization as well as technical standards to what degree have you
established a standard set of tools, practices or a concrete platform upon which you build
these types of projects?
That's a great question.
So I think I'll answer that in two ways.
We certainly have established tools and best practices.
So I talk about my center of excellence having three core functions.
Number one, we develop the underlying platform.
We work with our IT colleagues to do that.
The second thing is that we develop a series of use cases on top of that platform to demonstrate
the value of it, but we also democratize that platform to allow others to use the platform
as well.
And we run a network to share best practice with about 2,000 people across Shell around
how we do this.
So that's the operating model, if you will.
If you talk about the platform specifically, it's always evolving.
So we built the first platform in about 2014, early 2015, I guess.
And of course, the technology now has moved on significantly since then.
And so we're in the process of working through some of that and refreshing some of our tools
and our ways of working.
But we've had a number of core components that have been very successful and will be with
us for a long time to come.
Things like Altrix and Databricks have been the standards as well as R and Python.
And we've done a lot of work in those tools.
Increasing though, we're also looking at others, things like C3 IoT.
We do a lot of work with math works as well.
So it's very much, we've brought together a best of breed toolset.
And of course, we're really excited as you may have heard around Microsoft and working
very closely with Microsoft now to move things into more of a paths set up in the public
cloud.
And so that's very much the direction that we're heading in.
And a lot of the use cases that you support are, you know, what we've come to start calling
edge applications.
So can you talk a little bit about how the IoT and edge use cases impact the requirements
that you have on the underlying platform?
Yeah, of course.
So I think a couple of things.
I think in the strictest sense, Shell has been an IoT player for a very, very long time.
And we've had a consolidated sensor infrastructure for many years.
It's based on a technological OSI, so PIE and we effectively aggregate all of our sensor
data into centralized repositories.
And that's been a huge enabler for us because of course, that means you don't have to deploy
all the sensors and you don't have to deal with some of the IoT problems.
We just pick up the brilliant work that others have done to aggregate all that data for
us.
So that's been a big play and the challenge there has been making that available in the
cloud and starting to leverage machine learning on top of that.
And that's been a really exciting journey.
But increasing needs as we deploy new solutions, we also need to deploy things to the edge.
And in particular, deploying machine learning.
So the best example is some of the work we've been doing in retail recently, where we're
now deploying cameras into retail sites in Singapore and in Thailand.
And into those cameras, we actually, because we've got six cameras generating about 200
megabytes per second in data volumes, we need to be able to filter that before we pass
it into the cloud, because otherwise it just becomes unmanageable.
So what we're doing is we're using effectively edge deployment.
And we're allowing the containerized cloud based environment to push machine learning
into the edge to act as a filter to allow us to only retrieve the elements that we need
into the cloud for type processing in a scaled environment like Spark.
So that's pretty, that's pretty cutting edge and that's pretty exciting.
And that's one of the areas that I'm really enjoying working in.
The other example, of course, is some of the new devices that are coming online, so things
like charge posts.
So we're doing a lot of work around how can we deploy algorithms, which have full connectivity
into charging posts, so we can optimize the way in which we provide electricity to electric
vehicles.
The retail application you mentioned was one that was highlighted at the Ignite conference.
Can you talk about that in more detail?
What are its goals?
Of course.
So I mean, our retail businesses is huge.
We're the largest single branded retailer in the world.
We operate in about 70 different markets, we have about 44,000 sites and we deal with
about 30 million customers every day.
That's an incredible scale.
And of course, in that business, we want to make sure that our customers are safe and
secure and that we protect them from risks.
And as you will know, I mean, having hydrocarbons there has an elements of risk to it.
And so we need to make sure that we're able to intervene in situations like smoking,
which is remarkably still an occurrence on some of these sites.
As well as looking at other risks like people speeding or going in the wrong direction,
which may have an impact on pedestrians going into the store or situations where we may
have robberies having a bid, we want to be able to make sure we can detect and ideally detect
the symptoms of those things about to happen so we can make an intervention.
And so what we've done is we've developed a machine learning system that's looking for
some of those well-known symptoms that are about to cause issues.
And then providing those as alerts back to the service champion.
So that's the person who looks after the customer in the store.
And we're currently in a pilot phase with that.
We're testing it out in Singapore and in Thailand, as I mentioned.
It's going pretty well.
It's early days, but it's a really exciting area for us because there's so many use cases
we can think of that can help improve our retail business, but way beyond that into our
production business and into our refining business and elsewhere.
Because at the end of the day, what we've built is a standard platform that runs in the
public cloud that allows us to operate all of these cases at scale on a common infrastructure.
And so this particular application, as you mentioned, is based on capturing video.
You mentioned that the video that's being pushed to the centralized processing in the cloud
is reduced bandwidth.
Are you also doing inference at the edge or are you, tell us a little bit more about how
that's what's happening there?
Yeah, of course.
So I mean, if you think about it, what we have is about six different cameras that are
all pushing footage into an edge device.
We've then deployed a thin machine learning model, so a thin, deep neural network, if
you will, to that edge device based on TensorFlow, but it's kind of a yolo type model.
What it's doing is fast and loose inference, pulling out frames of interest and passing
those frames into the cloud, we're then loading them into a blob store environment, but
then automatically bringing them up into the memory of the spark cluster and the spark
cluster is then using effectively a Kafka stream, passing those through and identifying
potentially interesting events that we want to notify the service champion about.
Those events are then pushed into a database which provides an alert back then into the
service champion in a dashboard which runs on an iPad.
So that's a very brief overview of the architecture, but the core elements are inference at
the edge and tighter inference in the cloud to reduce false positives.
And then Kafka acting as the bus to pass those alerts through into the service champion
in the form of a mobile device.
And so the inference that's happening in the cloud is based on a more robust model presumably.
Yeah, it's deeper, of course, because what we've got is a smaller data volume and you
can do tighter inference, so you're using effectively deeper neural nets to make more
robust recommendations back to the service champion.
And are you in any way federating the training or the models, you know, pushing the knowledge
in the centralized models out to the edge, because periodically or training them in concert
with one another, are you doing anything in that space?
So we're continually retraining based on new data and feedback from the service champion
because both of those two things are important in refining the models.
We're also working with offline training data to also try new things to try and improve
the models as well and new ideas based on the new footage that we now have.
I think the, I mean, I'm sure you'll hear this from a lot of people, but training machine
learning is obviously it's a bit of trial and error.
You've got to try a bunch of things and iterate quickly and that's very much the phase that
we're in now to try and make our models more robust.
The beauty is that we have these things fully containerized though, and as they improve,
we can pass those down to the edge to improve the way in which they perform.
On that note, how do you do experiment management?
Well, it's a great question.
It's a tough one.
Maybe the best way to describe it is what we're trying to do across the department.
I mean, of course, we have design of experiments training that we roll out for a lot of our
data scientists.
I think the other thing is that we have a set up around quality where we put in place
a bunch of peer reviews to try to make sure that we're doing things in a sensible way across
the team.
I think though as well, and this is maybe a bit of a non-answer that the challenge with
this is, some of this is quite new for us.
And so in some ways, as I said, it's a bit of trial and error as to what works, particularly
given the architecture that we're using is quite new.
So we're trying to learn a lot from the way that others are handling this.
We're learning a lot from other organizations who have perhaps ahead of us, some of the
tech giants, for example, but I think it would be unfair to say we've got this right.
We're still very much learning.
One other thing I'm curious about is the degree to which you've instilled formalized processes
around screening for a bias in data sets or results in, for example, in this application
that's very much consumer facing and based on video, which is we've seen recent examples
of bias creeping into the use of object detection, image detection, facial recognition, things
like that.
How mature would you say your data science practices are in terms of understanding and
building processes in place to protect against that kind of bias, and where do you see that
going?
Yeah, it's a really good question.
I'll answer that in two ways, I think we've merged in my group, the statistics group and
the data science group, and I think that has really helped in the way we think about
quality of models, because as you can probably appreciate, the statistics group have a much
deeper understanding, if you will, of the potential for bias to cause issues.
So we've had a stats group since the 1970s, it's a very mature group, and I think they've
really helped in terms of bringing the challenge into some of the data scientists who are perhaps
more focused on some of the more emerging technologies and have a deeper knowledge there.
So I think it's that cross-pollination between different disciplines that's helping us
in the way that we develop these models.
And we're trying to encourage that across the team, and in particular, I mentioned the
quality initiative that we're starting to roll out.
We're trying to make sure that we have peer reviews to look at this and look at things
like systematic bias in the way that we're developing our models.
I think the other thing that's really important that we're talking a lot about at the moment
is estimating uncertainty and distribution at the source of the data sets that are coming
in, and really trying to understand that so you're not building in systematic bias through
basing your models on a small set of observations.
And I think in particular, one area that I think has a lot of promise here is using synthetic
data to train the algorithms.
We're not there yet with that, but I think that's an area that we're really interested
in because we think it has strong potential to remove some of those biases.
Is there a particular use case for which you see that as being most promising?
Well, I think it's more of a... I mean, it's very early days with this, right?
And we really don't know if this is going to work, by the way.
Because the question is, can you accurately generate enough realistic data to be
representative?
I think what we're excited about is, if you look at the occasional event from a machine
vision perspective, it's very hard to get a representative set of footage around that,
right?
So you end up very easily overfitting, whereas I think what's quite interesting is, if you
think about synthetic data, you can potentially generate a vast quantity of data in a VR world
that gives a real indication of potentially all the scenarios in which this could be
observed and creates a much more representative data set to do a first-pass training on your
model.
Now, we still don't think that synthetic data will take us to real world in one step.
It's going to be a two-step process, but if we can build that into the way we think
about machine vision, it could be a real game changer.
We think.
But like I said, I can be wrong on this one.
I think you're right, I agree. I wonder what the timeframe is for all of the pieces to
come together.
There's been some interesting results in combining synthetic data with real world data and
applying techniques like domain transfer, but a lot of that stuff is really bleeding
it.
And I'd love to hear if there are any tools or papers or things like that that are top
of mind for you in having looked at this.
Yeah, I mean, we're like I said, we're really early stages, and we're kind of going
in alone because we haven't found anything that's bang on point yet for not the use cases
that we are interested in.
And so I think this is actually an area where we think we might be able to publish some
useful material that's going to drive some of the discussion in this space.
So watch this space is what I would say.
So we talked a little bit about the experiment management side of things.
How do you approach model management and deploying models out to production and managing their
performance over time?
What kind of tooling have you built up around that and processes?
That is a great question as well.
So this is a real headache.
So let me just be completely honest with you.
Look, I can answer it with a simple answer, which is, you know, we containerize our models,
we use things like Azure Container Services and Kubernetes to deploy them.
We have, we embed metrics so we can manage it.
We're doing some work looking at things like ML flow in Spark to help with some of this.
However, I'll be completely honest and say, we're actually looking at, we're working
with C3 IoT to try and crack this problem.
And I'll explain in the scenario that we're working through, we have thousands of valves
across our business, literally thousands of them, and we want to be able to run machine
learning models for every single valve.
Now every single valve is slightly different.
The flow is different.
The temperature, the pressure is different, right?
So you basically need a model per valve, but you don't want to manually manage that because
it's just unmanageable.
So you need drift management, you need the ability to automatically transfer between different
types of models.
You need human override on those valves to actually replace the model with one that you think
is going to perform better.
You need to be able to retrain those models on a regular basis based on new incoming data
sets.
And you need methods to actually manage the overall performance of the system in a simple
way that gives an end user the ability to look across the entire set of valves.
So if you're giving an idea on a refinery, you might have 10,000 of these things.
So that is a real world problem, it's a great real world problem, it's a really exciting
one.
But it's also a real challenge.
And so like I said, we've gone and we've worked with C3 IoT to look at, can we leverage
their platform and their type system to help us to manage that level of machine learning
deployed into an asset where we have to be able to support a user interacting with that
every day?
Right.
Adding the edge components to the model management challenge makes it a lot more complex.
And you're relative to serving up models to some centralized website or via centralized
website.
I can definitely see where the complexity comes in there.
We mentioned the valves and building individual models for these valves.
I'm assuming that's kind of this digital twin type of use case.
How do you end up using those digital twins?
And do you like that terminology?
Do you use that internally or not really?
Yeah, we do.
I think the problem with the terminology is that everyone means something different by
it.
And it's everything from a simulated set of the physics of the operations of the system
through to a 3D model based on CAD drawings and all the variations in between.
So you have to be a bit careful with it.
And we're trying to standardize on a definition internally so that we can say this is what
we mean.
And then it has these attributes.
That's the current discussion that we're going through.
So interested if anyone else has had a similar problem.
But what we're trying to do really here is it is kind of a digital twin setup.
We're taking a hierarchy of the equipment that we have on the site and then we're effectively
tagging the things we want to monitor with a model against that hierarchy.
And then we're able to manage that basis the way in which the asset is set up.
But it tends to be specific pieces of equipment.
And of course, within the context of that, we're taking the historic data feeds about that
piece of the asset.
And then we're running an algorithm, typically a machine learning model, not always a deep
learning model, by the way, against that particular piece of equipment in order to provide
results back to the user.
Do you ultimately end up with for a complex piece of equipment as you're trying to make
predictions against the performance of that complex piece of equipment and ensemble
of thousands or tens of thousands of submodels representing the individual parts?
And have you built a framework that allows you to do that kind of thing repeatedly or
are we not there yet?
Well, so you know what's really interesting is actually we're going the other way right
now.
Really?
So one of the things that we found is the difference in performance between the submodels that
we're building out in the complex system versus basically having a master model that cuts
across all the pieces of equipment is not differentiating.
And of course, having the master model is much, much easier.
We're currently playing around with that, but we actually think that more and more will
be moving towards master models for certain things.
But to be honest, it depends because it's not a one-size-fits-all for certain pieces
of equipment we found having one model monitoring a whole sort of train, if you will, works
really well.
And for other things, you've really got a monitor that piece of equipment using a very
specific model.
So I think I would say we're learning all the time with this because we're sort of looking
at it.
We're learning as we go.
And it's something we believe quite passionately in, right, which is you go through a process
and you iterate on that and you don't overthink it.
And you may try three, four different approaches, even within the same team.
And that's OK, because I don't think anyone's done this, not at the scale that we're trying
to do it now.
So it was actually featured pretty prominently at this conference we keep referring to, the
Microsoft Ignite conference in the Satya's key notes.
And one of the examples was the use of not just the work with C3 IoT, but their recent
acquisition of bonsai and some of the work you're doing around reinforcement learning
and the application of that, that's also, you know, like all of the things we've talked
about very new, but can you talk a little bit about that work and what you've seen there?
Well, yeah, absolutely.
I mean, what this came out of was basically the idea of the self-driving car.
And the idea was basically, well, if they're making self-driving cars, why can't we have
self-driving wells, basically?
Because if you think about it, in some ways it's a simpler problem to solve.
There's less dimensionality to it, there's less uncertainty to it in some ways, there's
less unexpected events that you still have them.
And so effectively, what we've been trying to do is develop based on a three-dimensional
model of the subsurface, a set of automated geosteering algorithms that allow us to move
the drill bit through the subsurface in line with the well plan, and that's the simplest
way to describe it.
Now, if you think about that, it's actually a very, very suitable problem for reinforcement
learning, because it's got a very simple penalty function, right?
Out of well plan, you assign a penalty and off the back of that, you can very, very quickly
start to bring the well back into line as you train the algorithm over time.
And what we found was that the bonsai solution effectively gave us the opportunity to do
that very easily at scale.
And it's one of those things, you can do this yourself, but actually, if someone's built
a framework for doing this at scale, it's going to be much easier for us to do that as
we go forward, rather than having to maintain our own software.
And that was something that was very appealing about bonsai, that's why we started working
with them.
And so we're now looking at them to help us to scale this as we look at applying it into
different scenarios across our business.
So in formulating this self-driving well problem, what are the different control variables?
I imagine there are tons, but what are the kinds of things that we're talking about,
like rotational speed of the drill bits, and that kind of thing?
Yeah, exactly, and sort of the geospatial positioning, the azimuth, et cetera, right?
You've got a whole series of different measurements that you're taking all the time, and the best
way to describe it, is it's like driving forward, but you're only getting the data in retrospect.
And that's the challenge with it, because what happens, of course, is the data comes
off the bit, and you get it kind of a foot behind where you are if you see what I'm saying.
And so that's quite an interesting dilemma in the whole process.
But certainly, it's looking at a multi-dimensional, three-dimensional problem set based on your
understanding of the subsurface continually, and constantly iterating and adjusting, as
well as trying to really look at the optimum way of doing it, because it's not always
intuitive, because obviously you'll go, this is directional drilling, it's not only sort
of linear drilling.
So your speed also changes depending on the way in which you're drilling, and obviously
that has knock-on effect from an optimization perspective.
And so with reinforcement learning in general, bonds are in particular, the notion of
using simulation is key.
Did you already have a simulation of this in place, and were you able to easily use that
with reinforcement learning?
So I think the beauty of this is we have a lot of internal simulations set up for this
of course, because more or less everything that we do here has been looked at and has
been simulated in the past for optimization purposes.
So if you take something like rate of penetration, we have endless simulations of the way in
which that can be optimized.
And so we can bring some of those things into something like bonsai by having that very
good understanding of the first principles, simulators that we've developed over many
years, as well as of course, simulators of the subsurface, which is another element
of the simulation.
So I think that's been the real benefit, and it's the bringing together our existing
scientific knowledge with the new technology of reinforcement learning that's really driving
this change.
Well, Dan, we covered a ton of ground here.
I appreciate you taking the time to chat with us any kind of final thoughts or words of
wisdom to folks that are maybe at an enterprise that doesn't quite have as much experience
with this intersection of data and process as you and Shell.
What should they be thinking about?
I think it always comes back to really understanding your business problem.
I mean, I know I talked about that earlier on in the call, but the thing I'd encourage
you is that I talked about a lot of sophisticated techniques.
I talked about machine vision, I talked about reinforcement learning, a lot of the biggest
value things we've done are actually the simplest things.
And so if you're feeling like you're just getting started with this, and I know a number
of companies are actually the biggest thing is understanding where the big value is for
your business, and trying to solve the problem in the simplest way you possibly can.
I've got a lot of time for deep science, I absolutely love it, as you can probably tell,
I love some of the things we're doing, but I do think there's so much low hanging
fruit in this space that getting started.
And also, I would say there's some great tools out there that make it very accessible
to get stuck in very, very quickly.
So we've had a great relationship with Ultrix over many years, and we've had huge value
from them, just because it makes it so easy for citizen data scientists in the business
to get stuck in and start to add value.
And I think a lot of this is about a cultural change, trying to drive cultural change across
an organization where data and data science is central to the way that we do business.
That's something which is exciting, it's essential for any company I believe, but it's
also more and more accessible with the latest advances in technology.
Yeah, we didn't get into the cultural side of this, but your colleague Yuri spent a
lot of time talking about that in a session that I had with him at Ignite, clearly something
that's very central to the way you think about scaling data science and machine learning
at Shell.
Yeah, it's massive.
I mean, I think the biggest problem of this is adoption and belief and adoption in the
sense that how do you persuade people to use the output of a data science project?
And the second is do you believe that this is going to improve the way in which you do
your work and make your life easier?
And I think it also comes down to, we've got to learn as an organization to work in a way
that's for more familiar to software houses.
So things like developing minimum viable products, having strong product ownership, iterating
quickly, failing fast, being able to pivot, having the setup in which you are willing
to work with a minimum viable product, not tell the people that are developed it that
is rubbish, those sorts of things are not commonplace and not comfortable for an organization
like ours.
And so it's as much a challenge of building and understanding on both sides, building
and understanding on the business side of the way in which we need to work and building
and understanding from the way that we're working with the challenges that the business
people have every day and bringing those two worlds more closely together.
And it's an exciting challenge, but it's one that's going to take time and it's core
to how we're trying to get the value out of what we're doing.
Fantastic.
Well, Daniel, thank you so much.
Thank you.
All right, everyone, that's our show for today.
For more information on Daniel or any of the topics covered in today's show, visit
twimmelai.com slash talk slash 202.
To learn more about our AI platform series or to download our eBooks, visit twimmelai.com
slash AI platforms.
As always, thanks so much for listening and catch you next time.
