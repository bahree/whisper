WEBVTT

00:00.000 --> 00:09.600
Alright everyone, welcome to another episode of the Twilmal AI podcast.

00:09.600 --> 00:15.480
I'm your host Sam Charington and today I'm joined by Vidyut Napire, director of AI and

00:15.480 --> 00:17.920
machine learning at PayPal.

00:17.920 --> 00:21.280
Before we get going, be sure to take a moment to hit that subscribe button wherever you're

00:21.280 --> 00:23.640
listening to today's show.

00:23.640 --> 00:25.640
Vidyut, welcome to the podcast.

00:25.640 --> 00:26.640
Thanks Sam.

00:26.640 --> 00:27.640
Happy to be here.

00:27.640 --> 00:28.640
Big fan of usual.

00:28.640 --> 00:29.640
Thanks so much.

00:29.640 --> 00:34.120
I'm really looking forward to this conversation and learning a bit more about what you're up

00:34.120 --> 00:43.640
to both from the perspective of the hat you wear leading applied ML and AI at PayPal but

00:43.640 --> 00:48.640
also some of the work you're doing supporting ML ops and the machine learning platform

00:48.640 --> 00:50.440
teams there.

00:50.440 --> 00:53.360
But before we jump into all that, I'd love to have you share a little bit about your

00:53.360 --> 00:56.320
background and how you came to work in the field.

00:56.320 --> 01:03.040
I joined PayPal three years ago and I was, before that, I used to work in the autonomous

01:03.040 --> 01:04.040
driving area.

01:04.040 --> 01:09.880
I used to work at a company called Neo where my team was working on building kind of very

01:09.880 --> 01:14.560
interesting anomaly detection algorithms for catching crazy driving scenarios out there

01:14.560 --> 01:15.560
in the field.

01:15.560 --> 01:21.560
Before that, I used to work at Qualcomm where I led several interesting projects in modern

01:21.560 --> 01:27.320
design, sensor system design and so on, that's where I started getting my hands and feet

01:27.320 --> 01:29.920
wet with AI ML if you will.

01:29.920 --> 01:35.720
For the past three years, I've been at PayPal where I'm leading, as you said, the AI R&D

01:35.720 --> 01:41.960
function applied AI R&D function and in addition, off late, I also started leading some of

01:41.960 --> 01:45.120
our ML ops efforts in this area.

01:45.120 --> 01:51.920
So really, really interested in sharing what we've found in our journey so far.

01:51.920 --> 01:52.920
Awesome.

01:52.920 --> 01:53.920
Awesome.

01:53.920 --> 01:59.600
Just thinking about some of the places you've been on your personal journey, you kind

01:59.600 --> 02:05.360
of started out in these very, you know, edge scenarios, hardware driven scenarios and

02:05.360 --> 02:08.600
I'm imagining that PayPal is not like that at all.

02:08.600 --> 02:13.360
I'd love to hear you kind of comparing, contrast some of your prior experiences to what

02:13.360 --> 02:14.680
you see now in PayPal.

02:14.680 --> 02:15.680
Yeah.

02:15.680 --> 02:16.680
That's a great question.

02:16.680 --> 02:22.760
Actually, you know, as you rightly pointed out, when I was at Qualcomm, you know, it's

02:22.760 --> 02:27.920
a very different vertical, very different space and the kinds of problems that you encounter

02:27.920 --> 02:34.320
there, you know, have more to do with having very, very limited access to compute, limited

02:34.320 --> 02:40.160
access to memory, you know, so the focus there entirely is on how do you kind of build

02:40.160 --> 02:46.560
the most, you know, energy efficient algorithms from an AIML perspective versus when you move

02:46.560 --> 02:53.600
into a company like PayPal, which is primarily kind of like a cloud-based, you know, platform,

02:53.600 --> 02:55.600
it's a very different situation altogether.

02:55.600 --> 03:01.480
I mean, you're no longer strictly limited by the amount of compute, memory, resources

03:01.480 --> 03:03.000
you have.

03:03.000 --> 03:07.280
It becomes a slightly different environment where you are trying to, you know, bring

03:07.280 --> 03:11.600
AI ML to production, so, you know, just as an example, right?

03:11.600 --> 03:18.240
Like, you know, when at Qualcomm, when we were building, you know, various algorithms,

03:18.240 --> 03:19.560
you know, in the sensor area, right?

03:19.560 --> 03:25.480
So this is where we are trying to use Excel's and gyros, you know, to figure out whether

03:25.480 --> 03:30.800
what kind of activity a person is engaged with, with sensors.

03:30.800 --> 03:34.760
You know, we have a very, very small power budget, energy budget to work with.

03:34.760 --> 03:40.000
So we really, really have to focus on making our algorithms very, very efficient.

03:40.000 --> 03:45.840
In PayPal, it's kind of like a different, I think here it's more about how do we, you

03:45.840 --> 03:53.360
know, make sure that we are deploying the algorithm to solve, you know, to improve performance,

03:53.360 --> 03:57.160
but cost is a consideration, but it's not the most important thing, right?

03:57.160 --> 04:03.000
So it's, it's an interesting, it's an interesting set of experiences I've had.

04:03.000 --> 04:06.480
And then autonomous driving kind of that area falls somewhere in the middle of both of

04:06.480 --> 04:11.880
this, because, you know, on the one hand, you're still doing all of your inferencing on

04:11.880 --> 04:18.120
the edge, because ultimately all your computer vision processing has to happen on the car.

04:18.120 --> 04:23.200
But, but so in that sense, it is energy constraint because you have to do all of that, you

04:23.200 --> 04:24.920
know, kind of battery constraint environment.

04:24.920 --> 04:31.360
But on the flip side, the performance that you need, the bar that you need is very high.

04:31.360 --> 04:35.480
And also, the complexity is extremely high, right?

04:35.480 --> 04:40.480
Because you're processing very, very high bit rate video streams from multiple cameras

04:40.480 --> 04:41.480
concurrently, right?

04:41.480 --> 04:47.200
So the amount of compute you need to solve those problems is, you know, is much higher than

04:47.200 --> 04:50.800
what you would, what I was dealing with when I was at Qualcomm, for example.

04:50.800 --> 04:57.160
So it's just like each area has added its own, you know, difficulties and challenges

04:57.160 --> 05:02.920
to deal with, but it's also made me aware of, you know, very, very interesting perspectives

05:02.920 --> 05:04.520
on solving the same problem.

05:04.520 --> 05:10.320
You know, and I think about, again, financial services or PayPal in particular, fraud is

05:10.320 --> 05:15.640
probably the first thing that, you know, jumps to mind for me and for other folks.

05:15.640 --> 05:21.520
What are some of the other use cases that kind of fall in your portfolio and that are important

05:21.520 --> 05:22.520
at PayPal?

05:22.520 --> 05:23.520
Yeah.

05:23.520 --> 05:29.240
It's a good segue into, into kind of what, what overall data science at PayPal looks

05:29.240 --> 05:30.240
like.

05:30.240 --> 05:34.600
Before going there, maybe I'll just spend a few minutes talking a little about PayPal,

05:34.600 --> 05:35.600
right?

05:35.600 --> 05:41.640
Like what we do, you know, and where we are at in terms of scale and so on.

05:41.640 --> 05:47.240
So as you know, PayPal is probably one of the most widely recognized kind of payments,

05:47.240 --> 05:55.880
payment services company, we have last count, maybe around 395 or so million customers,

05:55.880 --> 05:59.560
maybe another 30 to 35 million merchants on the platform.

05:59.560 --> 06:06.560
So basically what we enable is this two-sided network where we have on the one hand, you

06:06.560 --> 06:14.080
know, all these customers who can use PayPal products to move money, to manage money.

06:14.080 --> 06:18.760
We also offer credit services to customers, for example.

06:18.760 --> 06:23.320
And then on the merchant side of the house, we have, you know, our checkout product.

06:23.320 --> 06:29.960
We have, we have several other, we have a whole laundry of products, right, where we enable

06:29.960 --> 06:33.960
our merchants to improve their own metrics, their own business metrics.

06:33.960 --> 06:38.320
We also help them with risk and fraud management, for example.

06:38.320 --> 06:41.560
And last but not least, we also provide merchants with credit, right?

06:41.560 --> 06:49.200
So as and when they need it, also not to forget, but it's, it's probably not widely known

06:49.200 --> 06:54.480
probably to your audience, but Venmo Zoom, they are also actually PayPal products, very,

06:54.480 --> 06:55.480
very popular.

06:55.480 --> 07:00.920
For example, Venmo with Gen Z, and you know, Zoom is a cross-border, cross-border remittance

07:00.920 --> 07:02.400
is right, like super popular.

07:02.400 --> 07:04.400
Zoom with an X, not Zoom with a Z.

07:04.400 --> 07:06.800
Zoom with an X, exactly.

07:06.800 --> 07:12.480
And several more, right, brain tree, for example, which is, you know, a payment gateway solution

07:12.480 --> 07:18.320
he offers, yeah, which not just a caring payment, but basically it's a payment, you know,

07:18.320 --> 07:23.800
gateway for merchants, right, similar to what stripe offers to merchants.

07:23.800 --> 07:27.960
So anyway, so I think the first thing to understand is PayPal is a very diverse ecosystem with

07:27.960 --> 07:30.920
a lot of different services and products.

07:30.920 --> 07:36.200
And you know, each one of them have their own, I would say, flavor of AIML, you know,

07:36.200 --> 07:37.720
embedded within them.

07:37.720 --> 07:42.400
But again, just zooming out, if you, if I were to broadly categorize all of the AIML,

07:42.400 --> 07:47.280
I think driven products, I think the number one would be definitely fraud prevention, that

07:47.280 --> 07:55.320
is the area where we have the most advanced, most complex AI systems in use and deployed.

07:55.320 --> 07:57.440
After that, you know, there is credit.

07:57.440 --> 08:04.040
So several, several of our credit product also is powered by AIML, especially in the

08:04.040 --> 08:09.400
underwriting process in the marketing of those kinds of products to prospective customers

08:09.400 --> 08:11.760
or merchants.

08:11.760 --> 08:19.120
Then there is things like sales and marketing, you know, when we are trying to essentially

08:19.120 --> 08:24.960
cross sell or upsell, you know, existing products to an existing customer or merchant,

08:24.960 --> 08:30.600
we will typically try to use AIML to identify those customers who are most likely to, you

08:30.600 --> 08:35.800
know, basically propensity models, which will help our business teams to identify the

08:35.800 --> 08:42.400
best set of next customers to target on the both sales and marketing side.

08:42.400 --> 08:44.800
There's also customer service, right?

08:44.800 --> 08:50.560
So we, in our, you know, PayPal is a big company, right?

08:50.560 --> 08:55.120
And many people reach out to us whenever they have some issue with the transaction that

08:55.120 --> 08:59.400
didn't go through or a transaction that was put on hold or stuff like that.

08:59.400 --> 09:01.640
So people contact us a lot of the time.

09:01.640 --> 09:09.520
We actually use NLP bird-like models in many of these use cases to identify, for example,

09:09.520 --> 09:14.160
in a chat context, the intent with which potentially a customer is calling us or contacting

09:14.160 --> 09:15.160
us.

09:15.160 --> 09:17.160
And then there is compliance.

09:17.160 --> 09:20.560
So again, this is where the regulatory side comes into the picture.

09:20.560 --> 09:27.440
So, you know, we are expected to catch, you know, any kind of illegal activity on the

09:27.440 --> 09:31.880
platform, things like money laundering or drug trafficking, and so like there are all

09:31.880 --> 09:37.760
these kinds of use cases that we are supposed to catch and flag to regulatory bodies.

09:37.760 --> 09:43.040
So we again use AIML there as well, because ultimately, if you map all of some of these

09:43.040 --> 09:48.320
problems, they all become AIML problems at some level.

09:48.320 --> 09:53.440
And again, this is just, I would say that probably the top five or six use cases, but there

09:53.440 --> 09:59.040
is many, many, I would say there is again kind of like a longish tale of other areas within

09:59.040 --> 10:04.840
PayPal where there is a lot of exploration going on around where AIML can be used to solve

10:04.840 --> 10:06.960
a certain business problem and so on.

10:06.960 --> 10:13.440
So hopefully that gives you a full view of what AIML looks like in PayPal today.

10:13.440 --> 10:14.720
No, that does.

10:14.720 --> 10:22.000
And now if you could then kind of use that as a segue to map to your applied R&D portfolio

10:22.000 --> 10:29.840
and you think about the projects and investments you're making there and what you think is most

10:29.840 --> 10:36.520
promising for the sets of problems that you have in terms of approaches.

10:36.520 --> 10:40.360
Yeah, it's a great question.

10:40.360 --> 10:47.120
So I'll probably provide my perspective on how I look overall at kind of the area of R&D

10:47.120 --> 10:55.040
where the PayPal director and R&D hat, right? So for me, like the overall AIML landscape,

10:55.040 --> 11:00.640
like I break it into four categories, there's kind of like the hardware compute layer,

11:00.640 --> 11:03.400
you know, where I feel there's a lot of innovation happening.

11:03.400 --> 11:08.440
Then there's the core algorithmic layer, which is kind of the focus of most of the academia

11:08.440 --> 11:12.480
and the Facebook's and Google's of the world.

11:12.480 --> 11:16.120
And finally, there's the application layer, right, where a customer is interacting with

11:16.120 --> 11:21.280
the product and he gets some experience driven by AI, right?

11:21.280 --> 11:25.080
So each of these areas, I feel there's a lot of innovation happening.

11:25.080 --> 11:30.120
And so my role as an artist, say four, yeah, sorry, fourth one is tools frameworks and

11:30.120 --> 11:31.120
platforms, right?

11:31.120 --> 11:36.000
I thought you were going there, but just, yeah, I missed it, sorry, thanks for catching it.

11:36.000 --> 11:39.920
So the tools frameworks and platforms are, you know, this is where I look at the overall

11:39.920 --> 11:40.920
ecosystem, right?

11:40.920 --> 11:43.720
And this is where, for example, MLOPS enters the picture, right?

11:43.720 --> 11:49.280
So how do we make sure that not just each cog in this wheel is doing well?

11:49.280 --> 11:53.680
But as a whole, how does everything kind of work well together in the most efficient

11:53.680 --> 11:54.680
manner, right?

11:54.680 --> 11:59.480
So MLOPS, things like interoperability, platforms, because, you know, there is a, you know,

11:59.480 --> 12:03.720
there are all these different machine learning frameworks, you know, there's TensorFlow,

12:03.720 --> 12:04.720
there's PyTorch.

12:04.720 --> 12:10.360
Now, how do we actually, on X, how do we, yeah, so on X is, for example, something that

12:10.360 --> 12:13.920
we are investigating for our use cases.

12:13.920 --> 12:19.680
So now, let me kind of like, you know, kind of like drive deeper into three main, like

12:19.680 --> 12:22.240
I would say, areas of R&D, right?

12:22.240 --> 12:25.920
The first is the kind of hardware compute layer.

12:25.920 --> 12:32.120
Here I think of two primary initiatives where we already made some investments, we are

12:32.120 --> 12:35.080
really, really hopeful about what, where this is going to go.

12:35.080 --> 12:40.080
The first one is this whole idea of federated learning and how do we essentially benefit

12:40.080 --> 12:47.280
from being able to offload some of our inferencing and hopefully in the future, the future training

12:47.280 --> 12:53.240
workloads directly to the customer on merchant devices, right?

12:53.240 --> 12:59.760
So we have some ongoing work in this area where we feel it's the right time.

12:59.760 --> 13:04.520
It's going to be very important from a cost management perspective, also from a privacy

13:04.520 --> 13:12.240
and regulatory perspective, that the closer we move the compute and the lesser we move

13:12.240 --> 13:19.880
data to our data centers, I think the better it is for everybody in the ecosystem.

13:19.880 --> 13:28.120
The second one is around how, again, this is something that we are extremely positive

13:28.120 --> 13:39.160
about, which is how do we essentially make sure that we use the data that we use, right?

13:39.160 --> 13:43.920
Fundamentally, the data layer and the compute layer in the most efficient manner.

13:43.920 --> 13:50.240
And for example, this could be things like, you know, how do we, for example, store data

13:50.240 --> 13:54.960
at 16-bit resolution, do we really need to store data at 32-bit resolution, do we really

13:54.960 --> 13:59.440
need to do compute at 32-bit, can we do it at 16-bit?

13:59.440 --> 14:09.080
Because again, the impact, the impact that this has on the overall cost of building an

14:09.080 --> 14:14.840
AIML solution, especially for a company like PayPal where we have such humongous amounts

14:14.840 --> 14:19.760
of data and to deal with, right?

14:19.760 --> 14:24.840
So we are investing in this area as well, like how do we optimize our compute layer

14:24.840 --> 14:29.360
and how do we optimize our data layer.

14:29.360 --> 14:32.960
And then, so those two, I think, are the kind of, I would say, primary two things we are

14:32.960 --> 14:34.960
focusing on.

14:34.960 --> 14:43.800
Yeah, the hardware silo, we have, for example, done some pilots in the quantum computing

14:43.800 --> 14:44.800
area.

14:44.800 --> 14:51.440
We have done, we are also looking at, for example, in the area of graph compute, right?

14:51.440 --> 14:59.280
You know, what is the best possible hardware layer for graph?

14:59.280 --> 15:05.320
Because as you know, graph computing, I think the underlying hardware ecosystem today

15:05.320 --> 15:10.800
is not optimized for graph because of the fact that, you know, the memory access patterns

15:10.800 --> 15:17.560
when you are basically trying to do graph compute are not well suited to today's, I would

15:17.560 --> 15:19.520
say, computer architecture.

15:19.520 --> 15:27.120
So, so in this particular instance, for example, we are, you know, working with some external

15:27.120 --> 15:31.480
partners trying to understand, you know, where this might go.

15:31.480 --> 15:36.120
But there's several different lines of things at the hardware acceleration layer that we

15:36.120 --> 15:37.360
are also considering.

15:37.360 --> 15:44.120
So kind of in the context of hardware, you're looking at federated learning, I'm curious

15:44.120 --> 15:50.520
when you think about federated learning or most of your efforts focused on what I think

15:50.520 --> 15:56.120
of as kind of the algorithmic elements of federated learning, like differential privacy

15:56.120 --> 16:04.360
and privacy preserving machine learning or more like deployment challenges, logistical

16:04.360 --> 16:10.200
challenges of just getting models, you know, you know, model slash data to and from devices

16:10.200 --> 16:15.200
and coordinating all of that, you know, distributed training, things like that.

16:15.200 --> 16:16.200
Yeah.

16:16.200 --> 16:21.280
So it's a journey for us, Sam, when federated learning, you know, has many components,

16:21.280 --> 16:26.800
pieces, some of them, as you said, more AIML algorithmic in nature, some of them more

16:26.800 --> 16:29.880
engineering and infrastructure oriented.

16:29.880 --> 16:35.000
So in terms of our journey, right now, we are focusing on first, you know, it's kind

16:35.000 --> 16:37.680
of like, you know, crawl first, then walk, then run right.

16:37.680 --> 16:46.480
The first goal is start to actually do inferencing of some models, you know, on the device.

16:46.480 --> 16:53.920
Gradually, as, you know, in parallel, of course, as you said, it just doesn't stop there.

16:53.920 --> 16:57.720
We have to start thinking about, you know, differential privacy, can we, you know, how

16:57.720 --> 17:06.040
do we make, you know, our models differently private, you know, that's one angle to it,

17:06.040 --> 17:11.240
distributed learning where the full, uh, ambit of edited learning actually is also eventually

17:11.240 --> 17:13.160
doing training on device.

17:13.160 --> 17:18.400
Um, but I think that's a bit far out, uh, from our current, uh, from where we are right

17:18.400 --> 17:19.400
now.

17:19.400 --> 17:24.440
But certain things, there are opportunities here, uh, to do that.

17:24.440 --> 17:30.000
I would, uh, I would just say that, uh, that's where we are at basically, uh, in this particular

17:30.000 --> 17:31.000
area.

17:31.000 --> 17:35.480
mentioned a couple of the kind of accelerated hardware

17:35.480 --> 17:41.800
acceleration opportunities that are out there.

17:41.800 --> 17:44.680
Is there anything in particular that you're finding

17:44.680 --> 17:50.000
exciting or interesting among the things

17:50.000 --> 17:51.600
that you've been looking at?

17:51.600 --> 17:55.760
I would say there are definitely interesting things,

17:55.760 --> 17:59.160
but I feel there are a bit like from an overall technology

17:59.160 --> 18:00.400
maturity standpoint.

18:00.400 --> 18:03.360
I think they're not quite there for prime time.

18:03.360 --> 18:07.440
For example, our recent work on quantum computing,

18:07.440 --> 18:08.840
let me just share some context.

18:08.840 --> 18:14.360
We, one of the steps in basically the AML workflow

18:14.360 --> 18:15.360
is feature engineering.

18:15.360 --> 18:18.440
And one of the sub steps there is feature selection.

18:18.440 --> 18:20.440
So let's say you have a catalog of features

18:20.440 --> 18:24.160
and you're trying to find the best possible subset of features

18:24.160 --> 18:27.480
for a given ML problem.

18:27.480 --> 18:31.040
That problem is fundamentally a combinatorial optimization

18:31.040 --> 18:34.960
problem with the complexity of the number of features

18:34.960 --> 18:37.320
that we have.

18:37.320 --> 18:40.640
Today we use greedy approaches, traditional classical greedy

18:40.640 --> 18:41.160
approaches.

18:41.160 --> 18:44.040
So one of the POCs we did on quantum was,

18:44.040 --> 18:49.400
and this was with some external vendors like IBM and D-Wave

18:49.400 --> 18:51.000
in this space.

18:51.000 --> 18:56.080
And what we found is that we didn't get exactly

18:56.080 --> 18:58.400
very, very conclusive results that demonstrated

18:58.400 --> 19:03.240
that the feature selection that you can get with today's

19:03.240 --> 19:09.400
quantum computers can actually outdo classical approaches

19:09.400 --> 19:10.680
yet.

19:10.680 --> 19:19.240
So that was a sobering, I guess, a revelation for us.

19:19.240 --> 19:21.760
And we feel OK, so there's a few more years

19:21.760 --> 19:24.880
for this particular area to evolve.

19:24.880 --> 19:26.680
That's a specific case in point.

19:26.680 --> 19:30.400
But I feel that's a kind of broader industry trend.

19:30.400 --> 19:33.400
I know there's a lot of startups right now

19:33.400 --> 19:38.680
in overall the hardware acceleration space, companies

19:38.680 --> 19:42.400
are building custom silicon to accelerate AI workloads,

19:42.400 --> 19:43.960
either for training or inference.

19:46.800 --> 19:51.360
But we don't yet feel the need to go there yet,

19:51.360 --> 19:52.760
but maybe in the future.

19:52.760 --> 19:55.800
So that's probably the, I would say,

19:55.800 --> 19:58.040
in terms of overall hardware space,

19:58.040 --> 20:01.200
that's probably the closest to, from a technology

20:01.200 --> 20:04.880
maturity standpoint, custom silicon for AI workloads.

20:04.880 --> 20:06.720
But I think that probably is not what

20:06.720 --> 20:09.040
we are the most interested in right now.

20:09.040 --> 20:12.920
And I've got to imagine if you're a significant user

20:12.920 --> 20:17.080
of cloud computing, you also are, to some degree,

20:17.080 --> 20:20.480
banking on the cloud providers, investments

20:20.480 --> 20:23.600
and custom silicon decreasing your costs

20:23.600 --> 20:25.040
and providing better performance.

20:25.040 --> 20:27.040
So maybe it's not the thing that you

20:27.040 --> 20:30.000
need to spend your efforts for focused on.

20:30.000 --> 20:32.960
Spot on, again, absolutely right.

20:32.960 --> 20:35.200
I think for, especially for quantum,

20:35.200 --> 20:37.480
right, like again, as a case in point,

20:37.480 --> 20:41.640
I think with quantum, it's a kind of steep learning curve

20:41.640 --> 20:44.800
to understand how quantum computing works.

20:44.800 --> 20:47.200
And so we actually went ahead a little bit

20:47.200 --> 20:49.720
and because it's a long lead time activity,

20:49.720 --> 20:51.640
we decided to do some work by ourselves.

20:51.640 --> 20:53.680
But as you said, maybe in a few years' time,

20:53.680 --> 20:59.640
quantum computing and all these graph and other hardware

20:59.640 --> 21:04.720
acceleration silicon might all be abstracted away

21:04.720 --> 21:05.600
by cloud providers.

21:05.600 --> 21:07.360
And we kind of don't have to worry

21:07.360 --> 21:11.080
about all of the early technology maturity issues

21:11.080 --> 21:15.000
that otherwise, you know, that you face,

21:15.000 --> 21:17.480
typically when you go down to the hardware layer.

21:17.480 --> 21:19.640
So absolutely, yeah.

21:19.640 --> 21:23.600
What was that second area that you mentioned?

21:23.600 --> 21:24.720
Was that algorithms?

21:24.720 --> 21:26.920
Yeah, algorithms, yeah.

21:26.920 --> 21:30.600
So in the algorithms bucket, Sam,

21:30.600 --> 21:33.400
I think we have, as you can imagine,

21:33.400 --> 21:35.560
the sets of things that we are doing here

21:35.560 --> 21:38.680
are pretty very familiar to your audience,

21:38.680 --> 21:40.560
things around representation learning,

21:40.560 --> 21:44.560
sequence to label learning, transformers.

21:44.560 --> 21:47.040
How do we use these in our most complex use cases?

21:47.040 --> 21:49.960
So this is one area of focus for my team.

21:49.960 --> 21:52.520
Another, like, I would say,

21:52.520 --> 21:55.440
increasing area of focus for the team is,

21:55.440 --> 21:58.440
you know, around causal ML, causal inference.

22:01.440 --> 22:05.880
I think this is, there are two primary use cases here.

22:05.880 --> 22:09.600
One is many problems that we encountered

22:09.600 --> 22:11.920
typically in the marketing area.

22:11.920 --> 22:13.400
You know, they are more causal in nature

22:13.400 --> 22:15.560
where you're trying to influence behavior

22:15.560 --> 22:19.920
of customers and merchants by taking certain actions.

22:19.920 --> 22:21.880
So we believe that instead of looking

22:21.880 --> 22:24.640
at these problems as predictive problems,

22:24.640 --> 22:27.440
they are much better formulated as, you know,

22:27.440 --> 22:30.560
treatment and effect kind of problems, right?

22:30.560 --> 22:33.520
So we're doing some early research here,

22:33.520 --> 22:37.320
partnering with some teams to, you know, to figure out

22:37.320 --> 22:40.960
if there is, you know, an improvement in performance

22:40.960 --> 22:43.800
by changing our approach to those problems.

22:43.800 --> 22:47.480
The other interesting application of causal ML is,

22:47.480 --> 22:52.000
in what we believe is essentially identifying causal features,

22:52.000 --> 22:55.800
right, in going back to that feature selection problem, right?

22:55.800 --> 22:59.320
A lot of today's feature selection problems

22:59.320 --> 23:02.400
fundamentally are rooted in correlation, right?

23:02.400 --> 23:05.040
You're trying to understand how features are correlated

23:05.040 --> 23:06.040
with labels.

23:07.240 --> 23:11.440
But I think with causal ML, there are approaches,

23:11.440 --> 23:15.000
which have actually been shown

23:15.000 --> 23:20.000
where you can try to learn more causative features

23:20.000 --> 23:22.560
for why a certain thing might be happening,

23:22.560 --> 23:27.560
which in turn helps with making our models more robust, right?

23:27.760 --> 23:31.640
Because to our earlier discussion that we were having

23:31.640 --> 23:33.760
around, you know, the fact that, you know,

23:33.760 --> 23:36.400
data distribution is drifting and, you know,

23:36.400 --> 23:38.720
as the data distribution is drifting,

23:38.720 --> 23:40.520
so is the model score, right?

23:40.520 --> 23:44.400
But if we can fundamentally identify features,

23:44.400 --> 23:49.080
which are causal in nature, we expect the model's output

23:49.080 --> 23:52.960
to be more robust to this kind of noise, right?

23:52.960 --> 23:55.720
So this is another area where we feel causal ML could play

23:55.720 --> 23:58.960
a big role, and it should help, for example,

23:58.960 --> 24:02.640
with keeping, you know, our performance of our models

24:02.640 --> 24:07.080
table over longer periods of time, then it is today, for example.

24:07.080 --> 24:10.840
Are there any references you can point us to on the causal

24:10.840 --> 24:12.080
features work?

24:12.080 --> 24:14.840
Yeah, there is some prior work done by Professor Susan

24:14.840 --> 24:20.120
Atty at Stanford, you know, who actually we had done a pilot

24:20.120 --> 24:24.920
with her team back in 2020 on this area.

24:24.920 --> 24:27.920
But again, that was one single pilot

24:27.920 --> 24:30.160
with one single use case, so we are continuing

24:30.160 --> 24:33.960
to explore other areas and other use cases.

24:33.960 --> 24:39.520
But the basic idea is how do you identify causal features

24:39.520 --> 24:42.200
using some causal ML techniques?

24:42.200 --> 24:44.640
So moving on, maybe to the third area.

24:44.640 --> 24:49.240
I imagine you didn't mention it, but I imagine

24:49.240 --> 24:54.320
under algorithms you're looking at various graph machine

24:54.320 --> 24:55.960
learning techniques as well.

24:55.960 --> 24:57.240
Absolutely, absolutely.

24:57.240 --> 24:59.520
We are looking at graph machine learning.

24:59.520 --> 25:03.240
It's a big area of research for us.

25:03.240 --> 25:08.200
This is where we are, you know, using GCNs to learn embeddings

25:08.200 --> 25:12.200
for several kinds of use cases, like collusion detection,

25:12.200 --> 25:15.520
like one form of fraud is collusion, where the buyer

25:15.520 --> 25:20.880
and the seller collude to essentially gain the system.

25:20.880 --> 25:24.920
So in these kinds of use cases, we have found

25:24.920 --> 25:28.480
that, you know, GCN based embeddings can actually really

25:28.480 --> 25:31.400
help improve the performance, you know,

25:31.400 --> 25:33.920
to catch this kind of fraud.

25:33.920 --> 25:41.280
But going beyond that, right, there is graph is such a rich

25:41.280 --> 25:49.120
representation of our user interaction data, and coupled

25:49.120 --> 25:52.200
that with the fact that graph ML itself is evolving so rapidly

25:52.200 --> 25:56.720
right now in the academia, there's so many great papers

25:56.720 --> 25:58.880
and research is coming out in this area,

25:58.880 --> 26:01.720
that it's a continuous area of exploration for us.

26:01.720 --> 26:06.200
How do we is evolving really quickly?

26:06.200 --> 26:10.480
So we are exploring it in several context,

26:10.480 --> 26:12.600
be fraud detection.

26:12.600 --> 26:16.880
We can also, you know, another use case

26:16.880 --> 26:18.800
that comes to mind is compliance, as I said,

26:18.800 --> 26:23.480
like because there are very, very, when you look at how

26:23.480 --> 26:28.440
money laundering occurs on a graph, it is almost,

26:28.440 --> 26:30.960
you'll immediately see how you can actually use graphs

26:30.960 --> 26:32.760
to catch it, right.

26:32.760 --> 26:35.320
But the problem with graph, which I was

26:35.320 --> 26:38.840
alluding to earlier, is the scale of the graph

26:38.840 --> 26:39.800
that we have at PayPal.

26:39.800 --> 26:44.800
We have like millions, you know, millions of customers, right?

26:44.800 --> 26:48.720
And like billions of transactions happening every year.

26:48.720 --> 26:52.360
So to model all of that, to keep that graph up to date,

26:52.360 --> 26:56.960
to be able to train, you know, models on this kind

26:56.960 --> 27:02.360
of evolving graph and so on, it's a very challenging problem.

27:02.360 --> 27:03.800
So that's one part of it.

27:03.800 --> 27:06.440
And then at the algorithmic layer graph also,

27:06.440 --> 27:09.160
as I keep saying, a lot of interesting work happening

27:09.160 --> 27:10.080
in this area.

27:10.080 --> 27:13.280
So we are doing our own pilots in this area.

27:13.280 --> 27:19.000
And just maybe taking a step back, do you

27:19.000 --> 27:25.440
think about kind of applied R&D in the context of algorithms

27:25.440 --> 27:30.800
as taking the stuff that the Stanford's and Google's

27:30.800 --> 27:33.640
and Facebook's are doing from a research

27:33.640 --> 27:35.760
and academic publishing perspective

27:35.760 --> 27:40.800
and trying to understand how they fit into your problem domains?

27:40.800 --> 27:46.160
Or are you also doing academic style research

27:46.160 --> 27:48.560
into the problems that interest you?

27:48.560 --> 27:53.160
Yeah, I think it's more of the former in the sense

27:53.160 --> 27:54.480
we had an applied R&D group.

27:54.480 --> 27:59.200
So we are not focused on kind of fundamental research.

27:59.200 --> 28:02.640
I would call it, we are not, our goal

28:02.640 --> 28:08.720
is not to, let's say, come up with novel problems

28:08.720 --> 28:11.840
and publish them to conferences and journals.

28:11.840 --> 28:14.520
It's definitely an applied R&D function.

28:14.520 --> 28:19.040
In terms of how we, to refresh your question,

28:19.040 --> 28:21.720
how do we kind of through our backlog?

28:21.720 --> 28:24.440
How do we decide what to work on and what not to work on?

28:24.440 --> 28:26.480
It's a combination of things.

28:26.480 --> 28:30.280
I think we're definitely monitoring what the Google's

28:30.280 --> 28:32.000
and the Facebook's are doing.

28:32.000 --> 28:34.600
We are also monitoring conferences,

28:34.600 --> 28:40.520
like top tier conferences, like ICML, ICLR and so on.

28:40.520 --> 28:45.480
And trying to, I think some of the differences

28:45.480 --> 28:48.960
that we see from academia, when you look at everything

28:48.960 --> 28:53.080
from an academia lens versus a applied R&D lens

28:53.080 --> 28:57.160
is, for example, number one, like a lot of AIML research

28:57.160 --> 29:02.960
is focused on NLP, CV, unstructured, kind of,

29:02.960 --> 29:06.920
I would say data, versus R is focused on,

29:06.920 --> 29:09.160
I would say, some portion of it's structured,

29:09.160 --> 29:12.440
some portion of it unstructured.

29:12.440 --> 29:20.200
And I think the other thing is financial services space,

29:20.200 --> 29:24.360
we have our own unique, hard, I would say,

29:24.360 --> 29:27.240
attributes to our data sets, which are not, like,

29:27.240 --> 29:30.320
typically found in kind of academic conferences.

29:30.320 --> 29:33.800
So one of the key things that we and my team do

29:33.800 --> 29:35.800
is really connecting those dots, right?

29:35.800 --> 29:39.280
So you will see a lot of things that show up in CV

29:39.280 --> 29:43.480
or in NLP land, but the key is like to identify,

29:43.480 --> 29:46.960
how do you map, you know, let's say the problem

29:46.960 --> 29:49.680
and the solution to our domain, right?

29:49.680 --> 29:52.960
And to see if there is some potential, right?

29:52.960 --> 29:55.360
It sounds very easy to say, but it's actually

29:55.360 --> 29:56.920
very hard to do in practice, right?

29:56.920 --> 30:02.400
So, but I think this is really the key,

30:02.400 --> 30:04.600
I would say, role that we play in addition

30:04.600 --> 30:08.920
to running the pilots and doing the R&D itself.

30:08.920 --> 30:12.360
I think the key part is ideating and understanding

30:12.360 --> 30:17.840
what trends are happening in these related,

30:17.840 --> 30:20.760
what separate domains and how do we kind of like map it

30:20.760 --> 30:22.600
to our domain, right?

30:22.600 --> 30:26.080
That is the real secret sauce, I think,

30:26.080 --> 30:27.520
to our group and what we do.

30:27.520 --> 30:29.440
So the third area?

30:29.440 --> 30:31.600
Yeah, the third area is kind of more

30:31.600 --> 30:34.520
on the application side, as I mentioned.

30:34.520 --> 30:39.120
So this is primarily, this is where I think, you know,

30:39.120 --> 30:44.120
most of our focus area here is on the responsible AI side.

30:44.120 --> 30:50.040
So things around explainability, fairness, privacy,

30:50.040 --> 30:52.040
adversarial learning, you know,

30:52.040 --> 30:57.240
how do we essentially look at these problems

30:57.240 --> 31:00.840
and solve them for our customers, basically,

31:00.840 --> 31:02.040
ultimately for our customers,

31:02.040 --> 31:04.920
but also, you know, you know,

31:04.920 --> 31:07.240
make sure that we are complying with regulations

31:07.240 --> 31:12.400
and laws and we have a pretty, I think,

31:12.400 --> 31:15.720
in the last year or two, we've actually taken steps

31:15.720 --> 31:21.200
at PayPal to make sure that we really are, you know,

31:21.200 --> 31:25.960
building our models in a very responsible manner.

31:25.960 --> 31:30.280
You know, we have governance and I would say processes

31:30.280 --> 31:33.400
in place to make sure that as and, you know,

31:33.400 --> 31:35.960
as we build models, we are paying close attention

31:35.960 --> 31:37.600
to how do we make them explainable?

31:37.600 --> 31:40.960
How do we ascertain that they are fair?

31:40.960 --> 31:43.200
In some areas, for example,

31:43.200 --> 31:45.720
we are already doing it because it's required by law,

31:45.720 --> 31:48.240
but we are also now starting to think about all the new areas

31:48.240 --> 31:51.640
where we need to start paying attention to all of these things.

31:51.640 --> 31:54.760
I already mentioned a little bit about privacy.

31:54.760 --> 31:57.720
And then finally, on the security side,

31:57.720 --> 32:02.720
we are starting to really look at adversarial learning

32:02.720 --> 32:05.680
as, you know, how do we, for example,

32:05.680 --> 32:07.200
in a front detection context,

32:07.200 --> 32:09.520
how do we use adversarial learning

32:09.520 --> 32:11.480
to make our models even more robust?

32:13.240 --> 32:18.120
So ultimately, I think our customers interact

32:18.120 --> 32:19.560
with us through the application layer.

32:19.560 --> 32:22.480
And so we really need to make sure that everything

32:22.480 --> 32:25.520
that we are doing as a customer sees it

32:25.520 --> 32:27.120
is done in a responsible manner,

32:27.120 --> 32:28.880
which complies with law and so on.

32:28.880 --> 32:33.360
So this is an area that my team is heavily invested in.

32:35.320 --> 32:37.960
I would say that would probably be the number one.

32:37.960 --> 32:41.040
On the second area here is probably more

32:41.040 --> 32:43.720
around the conversational AI side,

32:43.720 --> 32:45.600
you know, where we want to build,

32:46.800 --> 32:51.800
for example, chat parts, which basically delight our customers

32:53.400 --> 32:57.040
in terms of giving them a very good experience.

32:58.200 --> 33:03.200
And so here, there is some R&D happening within my team

33:03.360 --> 33:06.600
where we are trying to look at how do we use techniques

33:06.600 --> 33:08.920
and reinforcement learning, you know,

33:08.920 --> 33:10.720
to essentially train a chat part

33:10.720 --> 33:13.960
with all the previous chat logs that we have

33:13.960 --> 33:17.680
of our customers with our agents, for example, right?

33:17.680 --> 33:22.000
So that's a very interesting area of research for us as well.

33:22.000 --> 33:25.840
And the fourth area was around tools

33:25.840 --> 33:30.160
and ML ops platform technologies, that kind of thing.

33:30.160 --> 33:33.440
Exactly. So I think that actually is the glue

33:33.440 --> 33:36.440
that eventually tries all these three layers

33:36.440 --> 33:37.280
on the top.

33:37.280 --> 33:42.200
So definitely ML ops, interoperability platforms,

33:42.200 --> 33:45.920
how do we make sure that both on the deployment side

33:45.920 --> 33:47.200
as well as on the training side,

33:47.200 --> 33:50.920
we have an ecosystem that you can train

33:50.920 --> 33:55.520
in any framework of your choice, deploy on any kind of device,

33:55.520 --> 33:59.960
be it like a phone or a browser, right?

33:59.960 --> 34:01.920
So these kinds of things are also,

34:01.920 --> 34:04.920
we have various pilots ongoing in this area as well.

34:04.920 --> 34:07.000
And then the big one is ML ops, of course,

34:07.000 --> 34:12.000
which we have been on our own journey, I guess,

34:13.600 --> 34:18.600
but this is basically the place where we try to automate

34:18.840 --> 34:23.680
as many of the steps involved in model development

34:23.680 --> 34:27.160
and delivery to the extent possible, right?

34:27.160 --> 34:28.640
Like that's the ultimate goal.

34:28.640 --> 34:33.640
So this helps, as you said, with, you know,

34:35.240 --> 34:38.160
making our development cycles shorter

34:38.160 --> 34:41.200
and basically increasing deployment velocity, right?

34:41.200 --> 34:46.200
So this is the primary objective of the ML ops area.

34:47.840 --> 34:52.840
And is ML ops from your perspective

34:52.840 --> 34:57.840
from your perspective, is it still applied R&D into ML ops,

34:59.600 --> 35:03.240
meaning looking at kind of the, you know,

35:03.240 --> 35:05.800
have you characterized the time horizon, you know,

35:05.800 --> 35:07.680
hey, what are we gonna be doing in ML ops

35:07.680 --> 35:09.840
three to five years from now?

35:09.840 --> 35:11.920
And then exploring those things

35:11.920 --> 35:15.480
and transitioning them to other internal teams

35:15.480 --> 35:18.000
that are focused on, you know, one of the three years

35:18.000 --> 35:22.880
or this year is that ML ops component,

35:22.880 --> 35:24.800
like you have all this kind of forward looking stuff

35:24.800 --> 35:26.920
and you're also responsible for the platform

35:26.920 --> 35:29.040
that your teams use today in production.

35:29.040 --> 35:30.400
It's a great question.

35:30.400 --> 35:32.800
I think I would say it's a combination of both.

35:32.800 --> 35:36.000
I think there is some part of that team's bandwidth

35:36.000 --> 35:39.920
that goes into more forward looking R&D type of stuff.

35:39.920 --> 35:41.440
So collecting back, for example,

35:41.440 --> 35:45.560
to our first conversation on, you know, unsupervised

35:45.560 --> 35:49.440
versus semi-supervised approaches to, you know,

35:49.440 --> 35:50.360
for fraud detection.

35:50.360 --> 35:54.000
So that team, for example, will look into something like that

35:54.000 --> 35:57.920
because ultimately it's a way for us to improve,

35:58.880 --> 36:03.880
it's a way for us to automate the process of a model refresh

36:04.720 --> 36:08.480
by looking at a technique like semi-supervised learning, right?

36:08.480 --> 36:11.640
That's the R&D side, but as you can imagine,

36:11.640 --> 36:16.640
this is also a very kind of product and execution heavy function.

36:16.960 --> 36:19.880
So I would say 50% of the bandwidth goes in kind of like

36:19.880 --> 36:23.760
the day-to-day, you know, building off of the pipelines

36:23.760 --> 36:27.120
onboarding, you know, new models and customers

36:27.120 --> 36:29.920
onto the ML ops rails and so on.

36:31.240 --> 36:33.960
But and actually, this was also,

36:33.960 --> 36:37.480
by the way, the reason why this function, you know,

36:37.480 --> 36:39.480
was synergized with the R&D group

36:39.480 --> 36:42.640
because we already see that there is,

36:42.640 --> 36:45.400
there's plenty of synergy in terms of the fact

36:45.400 --> 36:48.760
that some of the work happening in this group is R&D nature.

36:49.760 --> 36:54.280
And so, you know, it makes, it just made a lot of sense

36:54.280 --> 36:59.280
for me to actually, you know, basically look at both

36:59.280 --> 37:01.720
the R&D side of the house as well as the ML ops R&D side

37:01.720 --> 37:03.800
of the house so that we can even synergize more.

37:06.600 --> 37:08.960
Well, I feel like we need a whole separate conversation

37:08.960 --> 37:12.640
just to dig into the details around your platform

37:12.640 --> 37:15.760
and tooling choices and the way you think about

37:15.760 --> 37:18.880
automating the machine learning workflow.

37:18.880 --> 37:19.880
I know.

37:19.880 --> 37:22.480
We didn't even get to it.

37:22.480 --> 37:23.520
Even get to it.

37:23.520 --> 37:26.720
That said, it has been wonderful, you know,

37:26.720 --> 37:30.240
exploring the way you're thinking about applied research

37:30.240 --> 37:34.640
and development at PayPal and some of the use cases

37:34.640 --> 37:36.320
that you're looking at and the things

37:36.320 --> 37:38.160
that you're excited about.

37:38.160 --> 37:40.720
Even there would be great to go into a lot more detail.

37:40.720 --> 37:43.120
So we'll have to make sure to, you know,

37:43.120 --> 37:45.880
stay in touch and have some fall on conversations.

37:45.880 --> 37:46.720
Thank you, Sam.

37:46.720 --> 38:16.680
It was great to be on the show.

