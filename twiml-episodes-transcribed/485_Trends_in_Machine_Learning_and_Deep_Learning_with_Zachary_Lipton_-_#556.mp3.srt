1
00:00:00,000 --> 00:00:16,760
All right, everyone. Welcome to another episode of AI Rewind 2021. Today, we are joined by Zachary

2
00:00:16,760 --> 00:00:22,400
Lipton, an assistant professor in the machine learning department and operations group at Carnegie

3
00:00:22,400 --> 00:00:28,400
Mellon University to talk through all things machine learning and deep learning. Zach last joined

4
00:00:28,400 --> 00:00:35,040
us on the show for the 2019 edition of Rewind and I'm super excited to have him back once again,

5
00:00:35,040 --> 00:00:40,320
Zach. Welcome back to the Twimal AI podcast. Well, thanks for having me, Sam. Great to see you again.

6
00:00:41,040 --> 00:00:47,680
It is great to see you again. I think the last time we physically had the opportunity to hang out

7
00:00:47,680 --> 00:00:55,680
was also 2019 in Vancouver. I think that's probably a story shared by a lot of folks in our field,

8
00:00:55,680 --> 00:01:01,920
like that was the last opportunity that folks had to hang out in person. Right. How have the last

9
00:01:01,920 --> 00:01:11,440
couple of years been for you? Oh, man, it's been a good full. You know, I'm not going to pretend it's

10
00:01:11,440 --> 00:01:17,520
all been smooth, but I mean, some things are nice. Like, my students are great and I think how

11
00:01:18,640 --> 00:01:24,480
been, I think it's not very easy for everyone like some people, some people got sick, some people

12
00:01:24,480 --> 00:01:30,960
lost, some people didn't get to see their family for a couple of years. I feel like it's a

13
00:01:30,960 --> 00:01:35,680
weird thing where people managed to be startlingly productive or at least, you know, maybe I don't

14
00:01:35,680 --> 00:01:39,840
want to shame anyone who doesn't feel productive. I feel like from just my feeling in my life,

15
00:01:39,840 --> 00:01:44,000
I feel like people are, I maybe it's just kind of like CMU culture design. I feel people have been

16
00:01:44,000 --> 00:01:51,120
really, yeah, like locked in and researched, but I think there's a kind of like emotional

17
00:01:51,120 --> 00:01:56,320
wear and tear of just not seeing anyone, like, especially like some folks are like living by

18
00:01:56,320 --> 00:02:00,000
themselves, you know, and then like when they're quarantined, it's not seen in another human for

19
00:02:00,000 --> 00:02:04,320
six months. And I and for others of us just like catching up now. So it's been, it's been a little

20
00:02:04,320 --> 00:02:11,520
bit wild, but you know, interesting on the research side and you know, we got a puppy. So like

21
00:02:11,520 --> 00:02:20,720
interesting personally. Nice, nice. So as I mentioned in the lead up, we are here to review the

22
00:02:20,720 --> 00:02:30,880
year in ML and deep learning. This is the third rewind that we'll publish. The first couple were

23
00:02:30,880 --> 00:02:37,120
an NLP in computer vision or computer vision and NLP in the order that they were published. And

24
00:02:37,120 --> 00:02:44,720
so far a couple of key themes have emerged. One, which was common in those first couple of

25
00:02:44,720 --> 00:02:56,000
episodes is this idea that as John Bahan and put it, NLP eating machine learning, kind of like

26
00:02:56,000 --> 00:03:03,040
in the same way we would say, you know, AI eating the software or what have you. The idea that

27
00:03:03,600 --> 00:03:10,720
computer vision is adopting transformers and things like that. You have echoed one of the other

28
00:03:10,720 --> 00:03:19,440
observations that John made in that NLP conversation and it is that particular point is kind of a

29
00:03:19,440 --> 00:03:27,520
slowing down of the field and a little bit of a respite from that kind of breakneck pace of

30
00:03:29,280 --> 00:03:36,560
of change that we were experiencing for a while. So maybe that is a place for you to jump in and

31
00:03:36,560 --> 00:03:45,600
riff for a bit. Yeah, I'm happy to riff. You know, like just you know, maybe I may be like

32
00:03:45,600 --> 00:03:50,080
choking a contrarian or something. I'll start by, you know, pushing back. I want to think it's

33
00:03:50,080 --> 00:03:56,800
kind of interesting of like, you know, that phrase of like NLP eating ML is kind of cute because

34
00:03:57,600 --> 00:04:06,080
it's sort of, well, among other things, right? Like in some sense, there's like the

35
00:04:06,080 --> 00:04:12,560
deline for for the long as time for the last seven years. It's sort of been machine learning

36
00:04:12,560 --> 00:04:21,120
eating NLP and that like if you look at the seven people going into like sort of NLP oriented

37
00:04:21,120 --> 00:04:25,120
like Radpar, there's a point where like NLP sat really close to like, you know, there were like

38
00:04:25,120 --> 00:04:30,960
NLP and computational linguistics like sort of two sides of a coin and they sat not so far from

39
00:04:30,960 --> 00:04:36,400
their like philosophers of linguistics, whatever. And now you have a moment for the last however many

40
00:04:36,400 --> 00:04:42,320
years where the the median person in NLP knows absolutely nothing about language. It's nothing

41
00:04:42,320 --> 00:04:47,760
interesting to say about language that couldn't just as easily and it's hard to say nobody or that

42
00:04:47,760 --> 00:04:52,080
there isn't anyone with something interest, interesting observations or interesting experiments

43
00:04:52,080 --> 00:04:55,920
that are that are kind of hitting off both sides, but say that like the center of gravity of the

44
00:04:55,920 --> 00:05:03,200
field has moved to this way that almost there there's almost no L in NLP, you know, it's just like

45
00:05:04,240 --> 00:05:10,640
it's just sort of like, you know, a set of tools where if if the like commercial demand was more

46
00:05:10,640 --> 00:05:15,680
on music than on NLP, you would use almost, you know, conceivably like the same set of mosques,

47
00:05:15,680 --> 00:05:20,640
all they care is just like a sequence of tokens, like a very generic sort of approach. And so in

48
00:05:20,640 --> 00:05:25,120
some sense, it's sort of just been like deep learning eating NLP has been the story for a while,

49
00:05:25,120 --> 00:05:30,880
and I think that like this version of NLP eating ML is, well, I guess one, they don't really mean

50
00:05:30,880 --> 00:05:37,200
NLP, but really more it's like other application areas. I think, you know, they don't really mean

51
00:05:37,200 --> 00:05:42,080
NLP as much as the thing that eight NLP, which is Transformers.

52
00:05:42,080 --> 00:05:47,600
Right, whatever is like that like new organism that displaced NLP is now coming across,

53
00:05:47,600 --> 00:05:52,800
but right, it's it's more like, you know, there was like a discipline of computer vision where

54
00:05:52,800 --> 00:05:57,600
you had people that like the typical person who was in there knew something about like the physics

55
00:05:57,600 --> 00:06:02,960
of light and optics and like was sort of like doing this sort of like, you know, that that was the

56
00:06:02,960 --> 00:06:07,760
angle. They were like a real expert on like the the modality of vision and the person NLP knew

57
00:06:07,760 --> 00:06:12,960
something about language. And I think they both got eight by deep learning in such a way that,

58
00:06:12,960 --> 00:06:17,760
you know, over the last seven years, ideas that would hit on one side could very easily

59
00:06:17,760 --> 00:06:25,120
poured across to the other. And from most of that history, I think it's hard to say precisely why

60
00:06:25,120 --> 00:06:29,920
if there's some reason or if it's just sort of the the order in which things happen, like because

61
00:06:29,920 --> 00:06:33,600
like the those breakthrough image net results are really caught people's attention where we're

62
00:06:33,600 --> 00:06:37,840
envisioned first. But I think from most of that history, it's been very one dimensional,

63
00:06:37,840 --> 00:06:46,000
like very one directional. I think things going from, you know, mostly in the direction of

64
00:06:46,000 --> 00:06:51,440
computer vision to NLP. And I think if anything, this is not really NLP eating vision, but it's just

65
00:06:51,440 --> 00:06:56,880
notable that maybe one of the bigger things happening in vision is crossing in the other direction,

66
00:06:56,880 --> 00:07:04,080
like contrary to the pattern. But yeah, you know, broadly on the like things slowing down pattern,

67
00:07:04,080 --> 00:07:08,400
I think I've been I've been noting this and writing about this for I don't know, maybe like four

68
00:07:08,400 --> 00:07:16,640
years now, but I think there's those definitely a moment where like if we were to look through

69
00:07:16,640 --> 00:07:24,800
the history and say like 2012 image net 2014, like sequence to sequence models, 2015, Alpha Go,

70
00:07:26,480 --> 00:07:37,520
2016, 2017 big advances and like the kind of like perceptual quality of like generative models,

71
00:07:37,520 --> 00:07:48,800
2017, transformers, 2018, Bert. There was a kind of change that, you know, I don't think these

72
00:07:48,800 --> 00:07:54,720
are necessarily all profound in a sense of like some big intellectual move, but they are like

73
00:07:54,720 --> 00:07:59,440
qualitative changes in capabilities. They it's like there's a big move in a sense of like what

74
00:07:59,440 --> 00:08:03,840
set of problems do I think are best tackled with these tools and what sort of performance can I

75
00:08:03,840 --> 00:08:09,760
expect from them? And a big difference in the sense of if I'm a practitioner in the field and

76
00:08:09,760 --> 00:08:15,520
somebody hits me with a typical like industry problem, what's my go-to tool? And if we look at like

77
00:08:15,520 --> 00:08:20,480
2021 and now we're saying, well, if someone hits you with a classification task, what are you

78
00:08:20,480 --> 00:08:24,720
going to do? If I'm just going to use a resonant from 2014 and 2015, you know, someone hits you

79
00:08:24,720 --> 00:08:31,840
with an LP task, like basically fine tuning Bert or a Bert like, you know, very Roberta Alberto,

80
00:08:31,840 --> 00:08:36,000
you know, whatever it's like, you know, right, like there's this moment where, you know,

81
00:08:37,360 --> 00:08:45,040
like I think that in some sense, I think it's okay in that like research is not now need to like

82
00:08:46,000 --> 00:08:49,440
start looking somewhere else other than just like what if I tweaked the architect here a little

83
00:08:49,440 --> 00:08:54,000
bit? Like there's a and as I was telling you like when we were ripping before, is that I think

84
00:08:54,000 --> 00:08:59,040
a research is there is some aspect that people like grow up it around in the dark looking for,

85
00:08:59,040 --> 00:09:03,520
looking for a way in, it's almost like they're like swinging out a pinnata with a blindfold on

86
00:09:03,520 --> 00:09:08,160
and trying to find like where's where's where's there an angle that like where's there something big?

87
00:09:08,800 --> 00:09:14,640
And I think you want to have a lot of researchers in that mindset of like I'm looking for the blind

88
00:09:14,640 --> 00:09:20,000
spot, like I'm looking for the big prize that other people aren't looking for. And look,

89
00:09:20,000 --> 00:09:24,880
every now and then someone gets, every now and then someone really lands a mark and the pinnata

90
00:09:24,880 --> 00:09:29,280
rips open and a bunch of candy falls on the floor. And then everybody rushes on and there's some

91
00:09:29,280 --> 00:09:33,200
period of time where nobody's worried about like nobody even knows where the bat is, everybody's

92
00:09:33,200 --> 00:09:40,000
just picking candy off the floor. And you know, I think where we saw that period of like people

93
00:09:40,000 --> 00:09:45,120
finding all this, you know, every, it wasn't like you didn't need a big intellectual breakthrough

94
00:09:45,120 --> 00:09:52,000
to have a big, to have an impactful breakthrough in all those years. And I think we're getting to

95
00:09:52,000 --> 00:09:58,160
the point where like, you know, there is some amount of stagnation because like most of the good

96
00:09:58,160 --> 00:10:03,760
candies can take up and people are like looking at the old grimy like moldy stuff that's like,

97
00:10:03,760 --> 00:10:08,160
you know, maybe there's still something in there. It's like it's like the like the sloppy seconds

98
00:10:08,160 --> 00:10:17,280
on the the research, you know, pinnata. With that analogy in mind, you know, where should research

99
00:10:17,280 --> 00:10:23,280
be swinging the bat? Do you have, is that a, is that it, you know, it's a crystal ball kind of question,

100
00:10:23,280 --> 00:10:29,040
but what is your intuition tell you where opportunities might be?

101
00:10:29,920 --> 00:10:33,920
Well, I always look for, you know, what, what is it that we actually care about? When people

102
00:10:33,920 --> 00:10:40,400
are selling a story, an aspirational story about, you know, I'm not like a mathematician first,

103
00:10:40,400 --> 00:10:44,640
I'm not just like, what's a hard problem? Like, which is all for that reason, you know, I got into it

104
00:10:44,640 --> 00:10:49,680
too late, you know, I kind of back into it from like, what's the, like, what's the dream?

105
00:10:49,680 --> 00:10:52,960
And if you look at like the dream people are selling people, even people with like existing

106
00:10:52,960 --> 00:10:57,040
companies right now, the claim they're making, like if you looked at like IBM Watson, which

107
00:10:57,040 --> 00:11:00,880
went up in flames, if you look at the claims they were making, like, what are we going to do for you?

108
00:11:00,880 --> 00:11:04,000
It's like, we're going to, you're going to make better decisions, you're going to provide

109
00:11:04,000 --> 00:11:08,560
personalized health care, you're going to help people to, you know, have better health outcomes

110
00:11:08,560 --> 00:11:13,840
and otherwise would have without our AI. If you look at this kind of stuff, you know,

111
00:11:13,840 --> 00:11:18,000
what are people selling? What are people hoping to actually achieve? And then you look at like,

112
00:11:18,000 --> 00:11:22,320
what are people actually doing? And if I like, I always kind of look back and forth between those

113
00:11:22,320 --> 00:11:27,040
and say, like, what's, what's like the missing part? Like, if you actually want to realize the dream

114
00:11:27,040 --> 00:11:31,600
of what, you know, people have to seem to want, what's, what's gone? Like, what's, what's not even

115
00:11:31,600 --> 00:11:37,040
being addressed in a, in a mature way? And so I think one, one thing that sort of jumps out

116
00:11:37,040 --> 00:11:41,680
is that everybody's sort of premise, you know, everyone says like these things are good,

117
00:11:41,680 --> 00:11:46,720
it's always based on some notion of like accuracy or the return of an RL system as evaluated on

118
00:11:46,720 --> 00:11:50,880
some fixed static environment. And, and then you look at what people actually do and it's like,

119
00:11:50,880 --> 00:11:55,840
they're taking some model, trained in some context on some set of data and deploying a crop in

120
00:11:55,840 --> 00:12:02,160
some different environment, which is changing in unpredictable ways. And where the whole environment

121
00:12:02,160 --> 00:12:08,240
is not just changing like in a kind of benign or passive way, often it's changing in direct

122
00:12:08,240 --> 00:12:12,720
response. So like, you know, like think about Google search, right? You deploy Google every single

123
00:12:12,720 --> 00:12:16,400
time they tweak their algorithm. What's the first thing that happens? And it's like all the message

124
00:12:16,400 --> 00:12:22,080
boards light up and all the SEO goons are like, oh, like SEO change the algorithm. Now you need to add

125
00:12:22,080 --> 00:12:27,360
this keyword. You need to do this. And I think that ML doesn't address that kind of stuff. I might

126
00:12:27,360 --> 00:12:32,640
say ML doesn't. I don't mean nothing that we aspire to in a hell, but I mean like the, the main

127
00:12:32,640 --> 00:12:38,000
thing, you know, the main thing that practitioners do, the toolkit, the mature one, the like, you

128
00:12:38,000 --> 00:12:43,200
know, I know how to use PyTorch and train, you know, ResNet and ResNet and whatever, whatever.

129
00:12:44,080 --> 00:12:50,160
That world, it's like completely set in an environment of like, I train a model, evaluate on a,

130
00:12:50,160 --> 00:12:55,120
so like, I ID holdout set. Or even if you evaluate on some kind of challenge set, it's not like

131
00:12:55,120 --> 00:12:59,360
with any coherent principle for why you should expect this model to do well on that challenge set,

132
00:12:59,360 --> 00:13:02,640
or why you should think the performance on that challenge that's representative of what you should

133
00:13:02,640 --> 00:13:09,040
encounter in the world. So I think performing in a dynamic world, making decisions and not just

134
00:13:09,040 --> 00:13:12,080
predictions, right? Because everybody's sort of saying, ultimately, if you think you're going to

135
00:13:12,080 --> 00:13:15,840
make money off of this, or you think you're going to affect some kind of like, societally

136
00:13:15,840 --> 00:13:21,360
beneficial outcome by using AI, if you think you're going to do anything, then ultimately it's like

137
00:13:21,360 --> 00:13:26,800
what you're, the claim at some some point, what you're hoping to do is guide some kind of decision

138
00:13:26,800 --> 00:13:30,320
or automate some kind of decision, right? You're actually hoping to have an outcome, not just

139
00:13:30,320 --> 00:13:35,360
to like be a passive observer to the world and make accurate predictions about what would happen,

140
00:13:35,360 --> 00:13:40,400
were you not to take any action at all? And, and this kind of setting of like actually providing

141
00:13:41,040 --> 00:13:46,720
guidance for what you should do in the world is, is we are, you know, it's, it's a thing like,

142
00:13:46,720 --> 00:13:50,480
yeah, there are people working on causal inference. There are people who are trying to bring

143
00:13:50,480 --> 00:13:55,360
reinforcement learning closer to the real world by, you know, maybe incorporating some ideas

144
00:13:55,360 --> 00:14:00,160
and causal inference like to think about confounding that might exist in the data to be able to

145
00:14:00,160 --> 00:14:04,400
build models and off policy kind of way, you know, so that you're not just saying I'm just going

146
00:14:04,400 --> 00:14:10,480
to deploy some randomly acting system in an important application and have it stuck for two

147
00:14:10,480 --> 00:14:16,800
million years until it learns. But they're relatively mature and then they get relatively

148
00:14:16,800 --> 00:14:20,160
little attention. You're going to look at like, you know, what are people, you know, what is,

149
00:14:20,160 --> 00:14:23,280
you know, I'm not going to beat up on our buddies in the press, we say, what is like, you know,

150
00:14:23,280 --> 00:14:27,920
Cade going to write a big article about in the New York Times, it's not typically like the,

151
00:14:27,920 --> 00:14:32,720
the, the, the, the slog of like scientific advances and making robust machine learning or,

152
00:14:33,040 --> 00:14:39,440
um, you know, off policy RL or something like this, it's, it's, uh, you know, there's a big

153
00:14:39,440 --> 00:14:43,520
neural network that, you know, is nine trillion parameters and a billion dollar investment from

154
00:14:43,520 --> 00:14:50,560
Microsoft and this kind of, and, and so I think right, um, decision making, uh, robustness and

155
00:14:50,560 --> 00:14:56,800
dynamic environments and, and actually addressing certain societal desert or audit, you know,

156
00:14:56,800 --> 00:15:04,000
people have sort of noticed the problems that arise in terms of the, um, ways sort of,

157
00:15:04,000 --> 00:15:09,760
like AI system can affect whether it's like, um, on ethical sort of outcomes if you plug them

158
00:15:09,760 --> 00:15:12,880
and night evenly into certain decision making systems, but the sort of like,

159
00:15:14,560 --> 00:15:19,280
field of actually like developing systems that could, in some coherent way,

160
00:15:19,280 --> 00:15:23,440
align with societal desert is quite primitive, right? So there's like a recognition that there's

161
00:15:23,440 --> 00:15:30,800
a problem, but we're, we're very early stages on getting towards solutions. So I think that,

162
00:15:30,800 --> 00:15:37,200
to me, like, these are the areas I think are, are more interesting, um, you know, in that like,

163
00:15:38,000 --> 00:15:43,840
like, if you can get, if you can squeeze half a point out on like, you know, all the NLP benchmarks by

164
00:15:43,840 --> 00:15:48,240
like making a slight variation on birth, you'll get a lot of citations that everyone will use it,

165
00:15:48,240 --> 00:15:54,320
and they should, and it is useful, but it's, I feel like not, it's, it's like a direct, it's a,

166
00:15:54,320 --> 00:15:59,680
a slight change in degree, it's not a change in kind. And so I think, you know, when you,

167
00:15:59,680 --> 00:16:04,160
when I look at it the field, I think the, the luxury of being an academia, the reason of being an

168
00:16:04,160 --> 00:16:09,680
academia is to think that like, I don't have to just think, how do I do epsilon better than someone

169
00:16:09,680 --> 00:16:15,200
at the same crop we're all doing tomorrow? But like, what, what actually is something that,

170
00:16:15,200 --> 00:16:21,600
you know, addresses, you know, some problem that nobody's even engaging with intellectually right

171
00:16:21,600 --> 00:16:29,840
now? It sounds like your answer then to the where to swing the bat is, um, and getting closer to,

172
00:16:29,840 --> 00:16:37,760
you know, real world problems that that folks are having. And you mentioned a lot of different

173
00:16:37,760 --> 00:16:45,600
elements, um, you know, I heard some aspects of domain generalization than there. I heard, um,

174
00:16:45,600 --> 00:16:50,800
you know, aspects of even like user interface, like how you're presenting the information,

175
00:16:50,800 --> 00:16:56,960
heard aspects of fairness in there, but broadly, it sounds like you're kind of also calling

176
00:16:56,960 --> 00:17:02,240
into the question kind of the simplification that often happens in research of problems that

177
00:17:02,240 --> 00:17:08,880
removes them from, you know, all of the constraints and fuzziness of the real world. Yeah, and look,

178
00:17:08,880 --> 00:17:15,840
it's tricky, right? Because everybody's, everybody thinks they're doing that? Well, it's more like

179
00:17:15,840 --> 00:17:21,360
everybody's got to choose the focus on something and, and to focus on the thing they want to focus

180
00:17:21,360 --> 00:17:26,800
on, they got to compromise on something else. Um, and, and it's not that like one thing is right

181
00:17:26,800 --> 00:17:30,560
or wrong. Like, I don't think it's wrong if there's people out there building bigger language

182
00:17:30,560 --> 00:17:37,280
model. I don't think that's fundamentally wrong. Um, I think you got to be like, um, you know,

183
00:17:37,280 --> 00:17:42,240
maybe like you got to use a brain to think about how, how, how, what kind of claims I can make

184
00:17:42,240 --> 00:17:47,360
about these things or, or how should they be used in the real world? But I think like, look,

185
00:17:47,360 --> 00:17:51,280
it's interesting to turn that knob and say, what if I make this big or what, you know, what

186
00:17:52,000 --> 00:17:59,200
happens? Um, there, there, there's plenty of work to be done. Um, you know, like, like one

187
00:17:59,200 --> 00:18:05,600
cutting on like trade off that you often have is that, um, you know, you have like, if I want to

188
00:18:05,600 --> 00:18:10,320
get close to what real data looks like, you know, and I want to get close to things I can actually

189
00:18:10,320 --> 00:18:17,680
do in a bunch of domain, often like all that's available is, you know, uh, data is, um, like,

190
00:18:17,680 --> 00:18:22,480
there's, there's a way that like predictive modeling and like the status quo, you know, is closer

191
00:18:22,480 --> 00:18:28,480
to the real world and that it, it touches real data and it gets within its like narrow aspiration

192
00:18:28,480 --> 00:18:34,560
of like, just predict well on like, IID data, like under a naive assumption of how the world doesn't

193
00:18:34,560 --> 00:18:41,280
change, it's able to do that on really complex, high dimensional real world data. Um, on the other

194
00:18:41,280 --> 00:18:47,920
hand, what you give up when you focus only on just that problem is any kind of consideration of,

195
00:18:47,920 --> 00:18:52,240
you know, I think people just only think about how do I get better predict, you know, building

196
00:18:52,240 --> 00:18:58,640
predictive models, um, is, you know, like, they're getting close to like the dealing with real data,

197
00:18:58,640 --> 00:19:02,480
but they're asking a very narrow set of questions about it, which is like, how do I get higher accuracy?

198
00:19:03,440 --> 00:19:10,480
Then on the other side, you know, you have folks say, for example, trying to get at like, um, fundamental

199
00:19:10,480 --> 00:19:17,440
questions about, um, what sort of like causal query is I can make. And very often in order to flesh

200
00:19:17,440 --> 00:19:24,080
out those questions, now they're taking on like a more ambitious set of like, kinds of queries

201
00:19:24,080 --> 00:19:28,160
that I can ask, but in order to make progress, like understanding the fundamental form of those

202
00:19:28,160 --> 00:19:33,360
questions is often starts with, well, I got to analyze like fundamentally one of these questions

203
00:19:33,360 --> 00:19:36,960
even answerable from the data sets that I have. And in order to like maybe get some of that

204
00:19:36,960 --> 00:19:40,560
analysis to go through, I have to make some simplifying assumptions about the form of the data,

205
00:19:40,560 --> 00:19:44,960
like, I assume that the whole world is linear and it's not high dimensional and it's not whatever.

206
00:19:44,960 --> 00:19:50,400
So, you know, you have plenty of people who are, who are doing, you know, work that's like really

207
00:19:50,400 --> 00:19:54,880
more like ambitious and expansive on the front of like the kinds of questions I can ask,

208
00:19:55,600 --> 00:20:01,680
but they're making really simplifying assumptions, uh, in terms of like the source of data I have

209
00:20:01,680 --> 00:20:06,960
and the number of variables I have and, uh, not worried about that, but I mean, that's to compromise,

210
00:20:06,960 --> 00:20:13,520
they make it as other folks building predictive models and trying to get close to like, do something

211
00:20:13,520 --> 00:20:18,800
that works on real data, but be naive about the kind of questions you can ask and, you know,

212
00:20:18,800 --> 00:20:22,880
not worry too much about just how limited is like what you could do with those predictions or

213
00:20:22,880 --> 00:20:27,600
their power to guide decisions in the real world. And, and then I think once you have that kind of

214
00:20:27,600 --> 00:20:31,760
tension of like, okay, everybody's looking at something and not looking at something else,

215
00:20:33,040 --> 00:20:37,840
you know, I think the question you have is like a research community is like, are you overleveraged

216
00:20:37,840 --> 00:20:44,000
somewhere? You know, I think oftentimes people, there's like a naive form of a criticism,

217
00:20:44,000 --> 00:20:48,880
which is like, oh, this thing sucks and this thing is good, but like there's a more mature version

218
00:20:48,880 --> 00:20:56,240
of it, which is like, we're way overleveraged on this thing and paying way too much attention

219
00:20:56,240 --> 00:21:01,600
to this thing and neglecting these other things. So it's like, you know, like more matter of like

220
00:21:01,600 --> 00:21:05,840
moving the needle. It's not that like nobody should be building a bit of bigger language model,

221
00:21:05,840 --> 00:21:12,240
or or tinkering with architectures, but it's sort of like, okay, like we're at a point where

222
00:21:12,880 --> 00:21:20,800
we're not getting nearly as much juice per squeeze doing that. Why do we have 99% of the community

223
00:21:21,680 --> 00:21:27,360
engaged in this? Why do we have so many papers that are being submitted that most of which know,

224
00:21:27,360 --> 00:21:32,880
you know, are not actually contributing anything either as an idea or as a result? And so,

225
00:21:32,880 --> 00:21:39,680
yeah, I think we're sitting in like some funny terrain like that. So were there notable

226
00:21:40,320 --> 00:21:47,680
papers or research advances that you think kind of poked at some of these issues that you're

227
00:21:47,680 --> 00:21:53,760
raising or you think are swinging the bat in the right direction? Yeah, I think there's

228
00:21:54,560 --> 00:21:58,160
this very weird climate now, which is like, I think for a lot of these questions, you have sort of

229
00:21:58,160 --> 00:22:08,800
like a growing recognition that there are problems, but then you have like a subset of people that

230
00:22:08,800 --> 00:22:15,200
are just kind of like taking advantage of the way in which the like peer review system is like

231
00:22:15,200 --> 00:22:21,520
overtaxed and scattered and like sort of just using the language of those problems, but not

232
00:22:21,520 --> 00:22:24,720
actually addressing them. And I think you see this in all of these and you see this in the

233
00:22:24,720 --> 00:22:28,800
fairness literature. I think you see it in the robustness literature. I mean, you see in the

234
00:22:28,800 --> 00:22:32,880
causality, like in that people submitting papers that sound like they're addressing causal problems,

235
00:22:32,880 --> 00:22:37,280
they're not actually people just saying this model's robust in a way where it's like, by the way,

236
00:22:37,280 --> 00:22:44,240
you can never just say a model is robust. Like if you state nothing about the ways in which the

237
00:22:44,240 --> 00:22:51,280
environment is allowed to change and, you know, there's no such thing as like general robustness,

238
00:22:51,280 --> 00:22:57,440
right? Because I can pose two different assumptions about the world where in one of them when the

239
00:22:57,440 --> 00:23:01,040
environment changes, you know, this is what I should be doing. And the other one I want to change

240
00:23:01,040 --> 00:23:05,600
is that's what I'm should be doing. I have no way of discerning which world I'm in, right? Like

241
00:23:05,600 --> 00:23:08,960
a class would be like, do I live in the labelship assumption? Like if I make that assumption,

242
00:23:08,960 --> 00:23:14,000
that like that distribution of categories is changing, but the class conditional literature,

243
00:23:14,000 --> 00:23:18,720
what does it, what does, you know, COVID versus not COVID look like is what's not changing,

244
00:23:18,720 --> 00:23:23,040
but the prevalence is changing. First of all, I assume that like the covariate distribution is

245
00:23:23,040 --> 00:23:27,520
changing, but that the label, the conditional, the probability of the label given the input is

246
00:23:27,520 --> 00:23:33,040
different. I might have no way of discerning whether I'm in world A or world B, but, you know,

247
00:23:33,040 --> 00:23:37,920
one thing is the right, like the robust model in this setting at, you know, should do this,

248
00:23:37,920 --> 00:23:43,040
and the robust model in that setting should do that. So you have like a set of people that are

249
00:23:43,040 --> 00:23:47,200
just kind of doing the deep learning thing, which, you know, like prediction lets you get away

250
00:23:47,200 --> 00:23:51,280
with it, like let me throw spaghetti at the wall, because I get to evaluate on the whole

251
00:23:51,280 --> 00:23:55,760
doubt data, and like how well I'm doing is identified. So I don't have to be able to state in

252
00:23:55,760 --> 00:23:59,440
terms of any principle. If you get to like a causal effect, you don't get to observe the causal

253
00:23:59,440 --> 00:24:02,800
effect. So it's like, if what you're doing doesn't actually identify the causal effect, you could

254
00:24:02,800 --> 00:24:06,400
call it a causal, you could, you could, you could just like use the language of causality,

255
00:24:06,400 --> 00:24:11,520
and it'd be quite in favor of not actually like addressing causality in any kind of sound way,

256
00:24:11,520 --> 00:24:16,240
and fool a reviewer, but not necessarily be doing it. And so, you know, I think the thing that,

257
00:24:16,240 --> 00:24:20,080
like a lot of these other problems are kind of more foundational, like they're not problems where,

258
00:24:20,080 --> 00:24:23,600
like we know how to evaluate systems, let's have people try stuff and whatever their problems were.

259
00:24:24,800 --> 00:24:31,360
So I'll give you an example, like in, you know, the distribution shift world, I mean,

260
00:24:31,360 --> 00:24:35,280
there's, I think a handful of things people are doing that are a little bit more interesting

261
00:24:35,280 --> 00:24:43,120
or sound or actually giving a path forward. There, there's a group at Stanford, some of Percy's

262
00:24:43,120 --> 00:24:51,280
students, Shiori and Pongway among down, put together this really expansive benchmark called

263
00:24:51,280 --> 00:24:58,400
Wilds, and it's a, like a collection across a whole lot of different application domains of a whole

264
00:24:58,400 --> 00:25:02,720
lot of different settings where you have some kind of subpopulation shift or some other kind of

265
00:25:02,720 --> 00:25:09,680
distribution shift, and it provides like a sort of unified resource for a whole bunch of settings.

266
00:25:09,680 --> 00:25:14,960
Again, like you still need to have some kind of, you know, you can't just like use the data set

267
00:25:14,960 --> 00:25:18,800
and say, oh, I tried this thing in this one domain and it generalize well to these two others,

268
00:25:18,800 --> 00:25:25,760
therefore it's robust, but at least it gives you some, like unified resource for asking a question,

269
00:25:25,760 --> 00:25:29,200
you know, like if you compare to the world where basically people are just saying, like I have

270
00:25:29,200 --> 00:25:35,360
pictures of eminus images and then eminus images on funky backgrounds, I think it's like a

271
00:25:35,360 --> 00:25:40,480
big advance towards like a nice sanity check and putting people in touch with the sorts of

272
00:25:40,480 --> 00:25:46,400
problems that are arising in the real world. There's one formulation of these domain

273
00:25:46,400 --> 00:25:55,040
adaptation settings is that there's a version called domain generalization and here it's like

274
00:25:55,040 --> 00:26:00,000
sort of saying I have like a bunch of different environments that I collected data from and now I

275
00:26:00,000 --> 00:26:06,080
have, you know, I want to generalize well to target environments, possibly using the fact that like

276
00:26:06,080 --> 00:26:09,760
I can look at the different source environments that I've had and they're actually marked out of

277
00:26:09,760 --> 00:26:14,160
different environments. I could try to see something like what's stable versus unstable across

278
00:26:14,160 --> 00:26:21,280
environments. And there've been some interesting papers. So by the way before we met, I ping some

279
00:26:21,280 --> 00:26:24,960
of my students be like, what do you think are some of the interesting papers so I want to give some

280
00:26:24,960 --> 00:26:30,880
credit to my students who are now like the extension of my memory. So my students are appointed

281
00:26:30,880 --> 00:26:34,480
out. There's a lot of interesting work where you have a whole lot of methods that are proposed

282
00:26:35,440 --> 00:26:43,200
but it turns out that if you set up a really rigorous baseline and listen papers, some from CMU

283
00:26:43,200 --> 00:26:53,440
from my friend Alon Rosenfeld and is advisor under SESCI, some from David Lopez Paz at Fair,

284
00:26:53,440 --> 00:26:58,320
but where they've shown things that like for a lot of these setups, it's really, really hard

285
00:26:58,320 --> 00:27:02,960
to be like really stupid baselines, like just dump the data together and just do ERM on it,

286
00:27:02,960 --> 00:27:08,240
like just train on all the data together and don't use the environmental labels in any sophisticated

287
00:27:08,240 --> 00:27:14,240
way. You know, in our own lab, my students are, I've been making a lot of progress on these

288
00:27:14,240 --> 00:27:19,360
distribution shift problems and we have some results that we've been excited about, like among

289
00:27:19,360 --> 00:27:26,000
other things, working out when you're presented with, you know, you've trained on data from,

290
00:27:26,640 --> 00:27:30,480
you've seen, you have some classes you've seen before and then suddenly at test time, you have

291
00:27:30,480 --> 00:27:36,080
some data that shows up from some additional class that like you never saw before. Can you actually

292
00:27:36,080 --> 00:27:41,840
on the fly, look at, you know, this previously seen data from from some classes and now additional

293
00:27:41,840 --> 00:27:47,600
data from some from some unknown class and identify like, oh, I can I say exactly precisely what

294
00:27:47,600 --> 00:27:52,560
fraction of the new data is from some previously unseen class and even develop a classifier that

295
00:27:52,560 --> 00:27:57,200
can now start predicting it so to say, oh, I think these samples have this probability of

296
00:27:57,200 --> 00:28:02,320
belonging to that class. So you'd imagine that like in the context of like a model monitoring pipeline,

297
00:28:02,320 --> 00:28:06,880
you'd eventually like live in that world where if the world changes in some way, the model could

298
00:28:06,880 --> 00:28:11,840
go back to you and say, hey, I think with high probability, like, you know, at least 20% of your

299
00:28:11,840 --> 00:28:16,320
new data actually belongs to some new class that you've never seen before and here are some examples

300
00:28:16,320 --> 00:28:21,280
that I think belong to that class and then you could sort of, you know, take some kind of corrective

301
00:28:21,280 --> 00:28:31,760
action if you think that the model's wrong. So that's on the robustness side. On the causality side

302
00:28:31,760 --> 00:28:37,120
and I got some of these tips from my student, Shun Senua, some of the work that we've been talking

303
00:28:37,120 --> 00:28:46,560
about and going over, I think there's so causality research is really exciting because it actually

304
00:28:46,560 --> 00:28:50,960
gets to the question we care about, which is like, what would happen if I did this versus what

305
00:28:50,960 --> 00:28:55,360
would have happened if I were just a passive observer watching the decisions get made as they

306
00:28:55,360 --> 00:29:00,560
always are and and causality gives like a philosophically coherent way for answering those kinds of

307
00:29:00,560 --> 00:29:07,040
questions. But the danger is that those answers are almost always predicated on some pretty strong

308
00:29:07,040 --> 00:29:14,320
assumptions that like our, you know, I can learn this like the parameters of my causal model,

309
00:29:14,320 --> 00:29:20,560
but the structure of the causal model is given like a priori and I know it exactly and there's

310
00:29:20,560 --> 00:29:26,000
no one observed confounding that, you know, can make all my results invalid. And so there's a lot

311
00:29:26,000 --> 00:29:33,120
of interesting things happening among them. There's some folks like Carlos Chinelli who just started

312
00:29:33,120 --> 00:29:37,680
a faculty job in the physics department at UW have been doing a lot of interesting work on

313
00:29:37,680 --> 00:29:44,000
sensitivity analysis. So if there's measurement error or if there's some, some omitted variable bias

314
00:29:44,000 --> 00:29:50,800
or something, just how, you know, like frameworks for being able to say just how much would there

315
00:29:50,800 --> 00:29:55,520
have to be for me to change my causal conclusion, right? So getting towards like, I'm not just saying,

316
00:29:55,520 --> 00:30:01,600
oh, if I'm nailing all these ridiculously precise assumptions about how the world is,

317
00:30:01,600 --> 00:30:06,480
then this is the answer to your public query, but saying like, you know, this is how far off those

318
00:30:06,480 --> 00:30:12,240
assumptions would have to be for like me to like have to totally change my mind. Eric Chuchin Chuchin

319
00:30:12,240 --> 00:30:18,640
is a researcher at Wharton's statistician who does a lot of exciting work in this area and

320
00:30:18,640 --> 00:30:22,960
Johnson to hit me to a paper that he's doing which addresses a specific problem of, you know,

321
00:30:22,960 --> 00:30:29,680
people often make this assumption that there's no unobserved confounding. And, you know, that is

322
00:30:29,680 --> 00:30:33,360
such a strong assumption because it's like, even if you have the right confounder, if you just

323
00:30:33,360 --> 00:30:38,400
measure it in a slightly noisy way, then there's unobserved confounding. And so he's gotten to

324
00:30:39,360 --> 00:30:43,360
this formulation we call proximal causal learning. And it's like, you can allow that, okay, I have

325
00:30:43,360 --> 00:30:49,840
some proxies for the underlying confounders, but they're not perfect proxies. And what can I do

326
00:30:49,840 --> 00:30:55,600
in that situation? And finally, one thing on the causal inference side that I've been

327
00:30:55,600 --> 00:31:02,480
really excited about is, you know, a whole lot of machine learning just sort of takes

328
00:31:02,480 --> 00:31:09,920
this stance, which is like, I've got this set of variables and I've got a collection of examples.

329
00:31:09,920 --> 00:31:13,360
It's like something like, you know, my data looks something like a table. Now it could be kind

330
00:31:13,360 --> 00:31:17,440
of complicated because if it's like texts, the different documents could be different length,

331
00:31:17,440 --> 00:31:22,640
but it's sort of the typical formulation that people work with don't usually allow for

332
00:31:22,640 --> 00:31:27,440
the setting where it's like, oh, I have a collection of a bunch of different data sets and I observe

333
00:31:27,440 --> 00:31:30,720
this thing in this data set and this other thing in that data set. But I feel like a lot of real

334
00:31:30,720 --> 00:31:37,200
world decision making is actually governed by that kind of process. I think we've all gotten a

335
00:31:37,200 --> 00:31:41,600
little bit of a crash course in this from just watching like the like COVID response, like

336
00:31:41,600 --> 00:31:46,480
unfold in the public eye. And it's like, oh, I've got this data from the CDC, but it has these

337
00:31:46,480 --> 00:31:49,520
features, but it doesn't, you know, it tells you how many reported cases, but it doesn't tell you

338
00:31:49,520 --> 00:31:54,720
how many tests are run, you know, but oh, I have this other data from the manufacturers of diagnostic

339
00:31:54,720 --> 00:31:59,440
equipment and that data actually tells me what fraction of tests are positive, not just what number

340
00:31:59,440 --> 00:32:04,720
of tests are positive. And I have this other data from, you know, the local municipalities.

341
00:32:04,720 --> 00:32:09,360
And so you get to these questions where it's like, if I have some question where, you know,

342
00:32:09,360 --> 00:32:13,440
I think very often we have queries, especially in economics, this comes up to call like data fusion

343
00:32:13,440 --> 00:32:20,320
type problems, but where like the answer can't come like directly, I have no one data set that

344
00:32:20,320 --> 00:32:24,080
can necessarily answer my query, but I have a whole bunch of different data sets and it's possible

345
00:32:24,080 --> 00:32:28,400
that if I combine them intelligently, I could sort of like try and relate to the answer to the

346
00:32:28,400 --> 00:32:34,320
question that I have. We talk about that on the infrastructure side as well. Is there kind of

347
00:32:34,320 --> 00:32:38,960
terminology evolving on the machine learning side for thinking about problems like this?

348
00:32:38,960 --> 00:32:46,160
For some reason, it also calls to my graphical kinds of things in that you want, you'd imagine

349
00:32:46,160 --> 00:32:50,480
some kind of connectedness in the data and the way they represented to one another.

350
00:32:50,480 --> 00:32:55,200
Well, you know, in the econ world, they call these like data fusion problems and

351
00:32:56,560 --> 00:33:01,360
someone who's done some really interesting work on that from like the AIML side is a researcher

352
00:33:01,360 --> 00:33:07,760
named Elias Ferenboim. So he's a professor at Columbia and he was a Udapuril grad student and

353
00:33:07,760 --> 00:33:13,040
now he's a prophet, he's doing a bunch of, I think a lot of the, you know, super exciting work

354
00:33:13,040 --> 00:33:19,280
in this area. And, you know, he's gotten to these sort of questions where, like is this paper from,

355
00:33:19,280 --> 00:33:24,320
I don't know if it was technically 2020, but I read it in 21, so we can call it 2021.

356
00:33:25,920 --> 00:33:31,280
But on an algorithm, what he calls like the general, general identifiability problem.

357
00:33:31,280 --> 00:33:36,720
So it's not just saying like, oh, I've got this one data set, is this thing, you know, can I answer

358
00:33:36,720 --> 00:33:40,960
my causal query? But it's like, oh, I've got this collection of data sets. And in this data set,

359
00:33:40,960 --> 00:33:46,080
these are variables are observed. And this other set, this other variables observed. And maybe this

360
00:33:46,080 --> 00:33:51,120
data set was collected by someone doing a particular kind of experiment on one of the variables.

361
00:33:51,120 --> 00:33:54,960
And this other, you know, it's like, I might have different data sets from different experiments.

362
00:33:54,960 --> 00:33:59,440
They're not even necessarily just different views of like the same data. One of them, someone was

363
00:33:59,440 --> 00:34:03,520
intervening in some way. But it's, if you kind of have this collection and data sets and some

364
00:34:03,520 --> 00:34:09,520
underlying causal structure, now can you tell me precisely, how can I combine all these data sets

365
00:34:09,520 --> 00:34:13,680
to answer the question that you have? Or, or, or I guess like, you know, with causal questions,

366
00:34:13,680 --> 00:34:19,600
always the first step is, is it possible to identify, you know, the, the answer to the question

367
00:34:19,600 --> 00:34:24,320
that you have based on the data that's available? And then, you know, if you can, well, give me,

368
00:34:24,320 --> 00:34:28,400
give me the formula such that if I plug in the data, different data sets, I could, you know,

369
00:34:28,400 --> 00:34:33,600
it would give me that, that estimate. So it's like, is it estimable? And if so, like, how do I

370
00:34:33,600 --> 00:34:40,960
produce such an estimate? Yeah, so there's, you know, I, these are general, exciting areas. There's

371
00:34:40,960 --> 00:34:48,000
also a lot of work happening now in causal discovery. So this is a really ambitious problem.

372
00:34:49,280 --> 00:34:53,200
Because, so in causal inference, you basically say, I know the structure of the causal

373
00:34:53,200 --> 00:34:57,280
graph. I know which variables potentially cause which other variables. But I just don't know the

374
00:34:57,280 --> 00:35:03,120
functions that determine, you know, so it's like x, you know, x and y together influence z.

375
00:35:03,120 --> 00:35:08,480
I don't know what is the function by which the values of x and y determine z. But I know that like

376
00:35:08,480 --> 00:35:13,280
z listens to x and y. Like if I were to intervene on x that could potentially change the value of z,

377
00:35:13,280 --> 00:35:17,840
whereas if I were to intervene on z, it wouldn't change the value of x, right? And so if you have this

378
00:35:17,840 --> 00:35:22,480
kind of structure, causal inference says, well, how do I like figure out, you know, basically,

379
00:35:22,480 --> 00:35:28,000
what are those functions that I can then answer a causal query? But there's like a sort of,

380
00:35:29,040 --> 00:35:33,920
like in that by itself, it's super hard. And, you know, we always never agree on causal effects

381
00:35:33,920 --> 00:35:38,080
because it's like, well, if you assume the graph looks like this, and it's slightly different,

382
00:35:38,080 --> 00:35:43,280
or, you know, than all bets are off, causal discovery basically says, what if I don't even know

383
00:35:43,280 --> 00:35:47,760
the graph that for you, or I have some partial knowledge of the graph, but I don't actually know

384
00:35:47,760 --> 00:35:54,080
fully which arrows, you know, go from which variables to which other variables. So in that case,

385
00:35:54,080 --> 00:35:59,280
you know, you, you ask this question of like, well, what, when is it possible to recover the graph?

386
00:35:59,280 --> 00:36:03,520
And in general, you can only recover the graph up to like something called an equivalence class.

387
00:36:04,640 --> 00:36:08,080
But now there's a whole bunch of other papers and I don't have all the links that I can send them

388
00:36:08,080 --> 00:36:12,960
to you offline, but things really start asking questions that says, okay, like, well, if I'm able to

389
00:36:12,960 --> 00:36:17,520
use causal discovery to get the graph up to like equivalence, well, now I can ask

390
00:36:17,520 --> 00:36:22,480
question like, what sort of experiments should I run in what order to as efficiently as possible,

391
00:36:22,480 --> 00:36:27,680
resolve any lingering ambiguities? So like, right, just the observational data might at least tell

392
00:36:27,680 --> 00:36:31,600
me something like certain variables aren't connected to other variables, and I'm able to

393
00:36:31,600 --> 00:36:35,680
orient for some edges in the graph, like which direction do they point, but others I can't,

394
00:36:36,320 --> 00:36:41,280
you know, but then, you know, the hope of causal discovery is to be able to additionally do that.

395
00:36:41,280 --> 00:36:45,600
So that's one, you know, exciting thing. And my student Charlton who's been working a lot in that

396
00:36:45,600 --> 00:36:51,040
area, yes, we have a paper that gets at this question of if you get to make these decisions about

397
00:36:51,040 --> 00:36:55,040
kind of like I was telling you before, if you have different data sets and by combining them,

398
00:36:55,040 --> 00:36:59,440
you can answer a question, but not necessarily by using one of them alone, or even if you can

399
00:36:59,440 --> 00:37:03,600
combine them to answer the question, there's still an unresolved question of how much data should

400
00:37:03,600 --> 00:37:08,080
I collect from this source versus that source in order to like as efficiently as possible pin down

401
00:37:08,080 --> 00:37:12,320
the causal effect? We've been working on that problem of basically, how do I like, you know,

402
00:37:12,320 --> 00:37:16,160
imagine you're working at a company and you have some third-party data provider that charges you

403
00:37:16,160 --> 00:37:22,320
for, you know, I participate this much per thousand examples, right? Like, you know, how would I make

404
00:37:22,320 --> 00:37:27,200
the decision sequentially of like, okay, based on what I know now, which data source should I query

405
00:37:27,200 --> 00:37:32,080
next and for how many samples? And okay, now I update my beliefs, I make a subsequent decision,

406
00:37:32,080 --> 00:37:36,800
which I think this decision process is always going on in the background, right? Like if you're

407
00:37:36,800 --> 00:37:43,280
a company that's buying data from people or going out and actively, you know, doing some kind of

408
00:37:43,280 --> 00:37:48,560
monitoring effort data collections, you're making decisions on the fly about, oh, I want to collect

409
00:37:48,560 --> 00:37:53,120
data from here. Oh, now I know something I didn't know before. This is going to guide my decision

410
00:37:53,120 --> 00:37:58,160
of what to collect next, but we don't usually formally model that process. We usually sort of assume

411
00:37:58,160 --> 00:38:02,960
the data is already there and then focus on how do you estimate, you know, something given that the

412
00:38:02,960 --> 00:38:10,240
data is there. So, yeah, those are, you know, some, some are so excited about that direction.

413
00:38:10,240 --> 00:38:16,240
And I think on the fairness side, I think one way that things are maturing is that people have been

414
00:38:17,840 --> 00:38:22,080
posing these questions in ways that are maybe, I don't know if you're familiar with this, a

415
00:38:22,080 --> 00:38:27,840
philosopher Charles Mills who passed away recently. Yeah, so he's, he's, this is great, like,

416
00:38:27,840 --> 00:38:38,160
moral and political philosopher. And he writes about, sort of like ideal approach to theorizing

417
00:38:38,160 --> 00:38:45,760
about questions of justice. And in, and I think it, you know, and my student, I had a post-doc

418
00:38:45,760 --> 00:38:49,680
in graduate, Cena Fuzzle, who was now a professor at Northeastern, but we wrote a paper recent,

419
00:38:49,680 --> 00:38:54,240
like a couple of years ago, just, just making a connection between what's going on in ML and,

420
00:38:54,240 --> 00:38:58,400
and this sort of framing of ideal versus non-ideal theorizing about justice that comes from,

421
00:38:59,760 --> 00:39:05,120
among other folks, Charles Mills. But, you know, he has this point that, you know, when you start

422
00:39:05,120 --> 00:39:10,720
posing like a question about equity or question about justice as a sort of, like, technical problem,

423
00:39:10,720 --> 00:39:17,200
and you, you make up a toy model, there's this danger that, you know, you, you sort of highlight

424
00:39:17,200 --> 00:39:24,240
as, like, salient and relevant, those parts of the problem that are captured by, like, your toy model.

425
00:39:24,800 --> 00:39:31,280
And you relegate as, like, not even of, you know, academic consideration, everything that doesn't

426
00:39:31,280 --> 00:39:37,200
show up in your model, right? So I think that, and the danger here is that, like, if the things that

427
00:39:37,200 --> 00:39:42,000
you're just, like, completely forgetting about are actually, like, the, everything that really

428
00:39:42,000 --> 00:39:46,800
matters, that you wind up in a situation where you could do a lot of academic tinkering and you could

429
00:39:46,800 --> 00:39:52,800
even develop, like, elegant mathematical theories, but they have almost nothing to say about the

430
00:39:52,800 --> 00:39:56,800
underlying question of justice that you care about. And I think this is sort of the situation that

431
00:39:56,800 --> 00:40:01,440
we've been in to some degree, and it's not to implicate everyone, but it's, say, like, the, the main

432
00:40:01,440 --> 00:40:05,920
thing, right? Is that we've been posing these questions of equity in the form of, like, say,

433
00:40:05,920 --> 00:40:12,240
I have a data set, say, I have a particular feature, let me just sort of start enumerating different

434
00:40:12,240 --> 00:40:17,200
things that should be equal. And I'm saying, oh, it's not possible to make them all equal simultaneously.

435
00:40:17,200 --> 00:40:23,440
So let's either just, like, naively pick one and then flesh out an algorithm for it, or just kind

436
00:40:23,440 --> 00:40:30,080
of, like, pying about how fairness is impossible. And I think, like, what gets lost in that whole

437
00:40:31,600 --> 00:40:35,280
kind of discussion is that it's almost all of it. It's, like, picking for granted, just,

438
00:40:36,400 --> 00:40:40,160
I've got a data set. There's a bunch of anonymous features. I don't really say anything about

439
00:40:40,160 --> 00:40:46,720
what they actually mean, or what real world processes they correspond to, or how disparities arise,

440
00:40:46,720 --> 00:40:56,800
and how that consideration really bears on what is sort of the appropriate response from a

441
00:40:56,800 --> 00:41:02,400
standpoint of, like, affecting justice. Like, when is it, you know, we don't look at every single,

442
00:41:02,400 --> 00:41:10,960
like, we don't look at, I don't know, you don't look at, like, the major league baseball, for example,

443
00:41:10,960 --> 00:41:15,200
and say, like, oh, I said, I noticed there were more players from some country that, you know,

444
00:41:15,200 --> 00:41:18,800
than from some other countries, you know, relatives that are, you know, and say, okay, let me just

445
00:41:18,800 --> 00:41:23,920
equalize it instead of quote a system. But it's also because I don't, you don't believe that, like,

446
00:41:23,920 --> 00:41:27,120
I don't know, say some country that, like, you know, excels in basal, like Puerto Rico,

447
00:41:27,120 --> 00:41:30,720
so you don't think they've been giving an unfair advantage in getting to the major leagues

448
00:41:30,720 --> 00:41:34,480
or something. And so, like, this whole backstory of, like, what actually are the,

449
00:41:37,200 --> 00:41:41,520
what actually do these variables mean, and to the extent that there are disparities reflected

450
00:41:41,520 --> 00:41:48,960
in the data, like, where do they come from, and how do they correspond to some political,

451
00:41:50,080 --> 00:41:57,360
some coherent political stance or theory that sort of makes the straight line from that,

452
00:41:57,360 --> 00:42:01,440
to who has responsible to remediate, who has a responsibility to remediate it. These are,

453
00:42:01,440 --> 00:42:04,960
like, fundamentally, the concerns that we sort of always have when we speak, I think,

454
00:42:05,600 --> 00:42:11,600
in the law or in a broader sense about questions of justice. That's some, for some reason,

455
00:42:11,600 --> 00:42:15,440
I think there's something I think maybe just about the fact that it's a new field or something

456
00:42:15,440 --> 00:42:19,200
like that, but that for some reason, I've been just kind of completely sidelined or completely

457
00:42:19,200 --> 00:42:24,400
as a strong word, but by, like, the main branch of fairness research. And I think there is a

458
00:42:24,400 --> 00:42:31,360
number of people who are, who are doing interesting work here to try to actually ask the critical

459
00:42:31,360 --> 00:42:38,400
questions. And I think, like, Lily, who, and, um, is to call their houseman, uh, or two people who,

460
00:42:38,400 --> 00:42:45,280
I think, just have been, like, kind of, asking the right kinds of questions, um, for, for a while,

461
00:42:45,280 --> 00:42:51,520
and, and kind of framing that critique in a way that I think is what's so rare. It's like,

462
00:42:51,520 --> 00:42:55,840
both really understands what's happening in, like, the sort of fair ML world, and also really

463
00:42:55,840 --> 00:43:01,840
understands the sort of context and, um, like, the people actually understand ethics and actually

464
00:43:01,840 --> 00:43:08,160
understand, like, legal principles of justice and our, um, I think, able to speak from some

465
00:43:08,160 --> 00:43:12,480
degree of authority to sort of what's missing in a way we're opposing those questions and tackling

466
00:43:12,480 --> 00:43:21,280
them. Before, before you jump into their work, the things that come to mind for me are this idea of, um,

467
00:43:22,800 --> 00:43:27,520
you know, techno-solutionism being part of the problem, like, we're trying to, you know, throw

468
00:43:27,520 --> 00:43:33,600
technology at the problems that technology is, is creating for us. Yeah, we talked about that

469
00:43:33,600 --> 00:43:43,200
a couple of years ago. We did. We did. Uh, and also there's kind of a nod in, in the way you talked

470
00:43:43,200 --> 00:43:50,560
about the problem of fairness to causality, and, you know, when we all got really excited about

471
00:43:50,560 --> 00:43:58,000
causality a couple of years ago, you know, and, uh, I think it was that same NURPS, uh, it was,

472
00:43:58,000 --> 00:44:03,040
you know, where everyone left excited about causality. Like, it was supposed to be the savior of

473
00:44:03,040 --> 00:44:09,520
fairness, and it was, you know, applying causal modeling to machine learning more broadly was going

474
00:44:09,520 --> 00:44:16,800
the, you know, give us, you know, transparency, give us fairness, give us, uh, interpretability,

475
00:44:16,800 --> 00:44:23,120
and, you know, break open all of the black boxes and all of that. Like, where is that?

476
00:44:24,240 --> 00:44:28,400
You know, and I think I just, I couldn't more highly recommend, um,

477
00:44:28,400 --> 00:44:34,240
issa and lilies work here that I think, you know, there's a, there's a handful worse, you know,

478
00:44:34,240 --> 00:44:40,000
there's some work by Elias Barron-Boyam and by Ilya Spitzer that has, and, and before some earlier

479
00:44:40,000 --> 00:44:46,960
work by like Matt Kuzner that sort of posed different, um, notions or of that, that are, like,

480
00:44:46,960 --> 00:44:51,760
within a causal framing kind of like coming out of like pearls, causal modeling. So some notions of

481
00:44:51,760 --> 00:44:58,320
like, you know, the earliest versions of that say something like, well, there's a lot of different

482
00:44:58,320 --> 00:45:01,760
versions. If someone wants to say something like, okay, it's not just a question of is race or,

483
00:45:01,760 --> 00:45:06,400
or gender or whatever is considered as protected attribute. Does it turn out to be correlated with

484
00:45:06,400 --> 00:45:10,240
some outcome? We want to ask some question of like, is it what causes the outcome?

485
00:45:11,600 --> 00:45:16,800
Um, and there, there's a way that like these questions have been paused, and it's actually,

486
00:45:16,800 --> 00:45:22,080
you know, the causal framing is not unique to machine learning. It's actually something that, um,

487
00:45:23,360 --> 00:45:28,720
I think like the legal scholarship itself often expresses things in causal terms, right?

488
00:45:30,400 --> 00:45:37,200
You know, um, and before them, you know, I think like economists, for example, like, you know,

489
00:45:37,200 --> 00:45:41,280
there's a famous experiments, right, where people say, well, recent, like, the resume experiment,

490
00:45:41,280 --> 00:45:45,680
that I think, Sindel Millenathan and some others ran, or they, they, they randomized names to be,

491
00:45:45,680 --> 00:45:53,200
uh, tip more, more likely sort of, um, uh, you know, like black Americans sounding as more likely

492
00:45:53,200 --> 00:45:57,360
to be white Americans sounding as, you know, and then they, they, they send the resumes to people

493
00:45:57,360 --> 00:46:01,520
and they measure the response. And it's, it's an interesting experiment. It's certainly like,

494
00:46:02,400 --> 00:46:08,160
uh, uh, valuable research. And the fact that there is, in certain contexts, a difference in

495
00:46:08,160 --> 00:46:16,720
the response race, like, you know, does jump out as, um, you know, problematic, right? On the other

496
00:46:16,720 --> 00:46:23,760
hand, if you sent them out and there was no difference in response race, um, should be conclude in

497
00:46:23,760 --> 00:46:28,640
the other direction, like, oh, there's, there's nothing wrong, right? And, and I think the answer is,

498
00:46:29,360 --> 00:46:32,560
you know, obviously, different people will have different opinions and, and, and the answer might

499
00:46:32,560 --> 00:46:37,120
be answered different in different contexts. But I think that there's a lot of context so much,

500
00:46:37,120 --> 00:46:41,360
I think many of us or most of us would say that that's not necessarily the case, right?

501
00:46:41,920 --> 00:46:48,480
Um, for example, if, uh, right, like if you, if you, if you, if you, like, if you're like, oh,

502
00:46:48,480 --> 00:46:51,920
you know, like, because, you know, it took, what does it mean to just change the name, right?

503
00:46:51,920 --> 00:46:56,240
Like, I could change your, I could change your name, but, um, and that one make a difference,

504
00:46:56,240 --> 00:47:04,000
but if I change what college you went to from, say, like, HBCU to a, um, you know, say some,

505
00:47:04,000 --> 00:47:08,720
some, some other school, and that made a difference, even if your name by itself conditioned on

506
00:47:08,720 --> 00:47:12,640
everything else didn't make a difference. So, so there's this notion that's baked into a lot

507
00:47:12,640 --> 00:47:18,960
of literature that tries to pose questions about, um, discrimination through a causal lens that sort

508
00:47:18,960 --> 00:47:25,840
of tends to adopt a rather like narrow notion of what could constitute discrimination as like the

509
00:47:25,840 --> 00:47:31,280
direct effect of some attribute, like the direct effect of gender, the direct effect of race on a

510
00:47:31,280 --> 00:47:37,840
decision. And the problem is that, well, what about like all of these sort of potentially

511
00:47:37,840 --> 00:47:41,280
indirect effects that could still be, you know, if I were to make, if someone were to make a

512
00:47:41,280 --> 00:47:48,880
decision based on some factor that is super correlated with race and also irrelevant to the decision

513
00:47:48,880 --> 00:47:55,200
otherwise, well, like, would you say that that's not discrimination? Um, and so there's this,

514
00:47:55,200 --> 00:47:58,960
you know, and then there's some work by Elias Barron-Moham and Ily Schbitzer, which is,

515
00:47:58,960 --> 00:48:04,080
I think a sort of step at least conceptually in a, in a more interesting direction, whereas

516
00:48:04,080 --> 00:48:10,480
what they try to do is sort of, you know, if you have like a causal model over all the variables,

517
00:48:11,440 --> 00:48:20,560
you could say something like, well, let me disentangle the, like, the, how the effect of some attribute

518
00:48:20,560 --> 00:48:26,080
of interest, whether it's race or gender, comes to influence some outcome of interest along all the

519
00:48:26,080 --> 00:48:29,840
different sort of plausible causal paths that it might take. And I could sort of attribute,

520
00:48:29,840 --> 00:48:34,640
to what extent does this, you know, influencing the outcome via that variable versus via this,

521
00:48:34,640 --> 00:48:40,560
via this path versus via this other path. Um, now keep in mind though, like, that sort of,

522
00:48:40,560 --> 00:48:45,360
I think what's cool about it is it's like a thinking tool. Like in practice, do we actually expect

523
00:48:45,360 --> 00:48:49,840
that we would have a causal model that captures all the variables of interest and actually says

524
00:48:49,840 --> 00:48:55,680
exactly, we would know precisely which variables influence, like every variable that goes from

525
00:48:56,400 --> 00:49:02,960
somebody's gender, you know, to whether or not they got hired, you know, we'd be able to,

526
00:49:02,960 --> 00:49:06,960
like, we're going to trace like over what scale, like over the scope of someone's entire life,

527
00:49:06,960 --> 00:49:12,960
we're going to, you know, build into our graph, every opportunity, right, every, every, every,

528
00:49:12,960 --> 00:49:16,880
every decision, every opportunity that someone was given or not given on that account,

529
00:49:16,880 --> 00:49:20,960
like, that we're going to have a graph that is so rich as to capture all of that, you know,

530
00:49:20,960 --> 00:49:26,320
it seems unlikely, but at least it gives you maybe like a thinking tool as like, okay, it's it,

531
00:49:26,320 --> 00:49:30,240
you know, at least I can conceptualize and step back and think about the fact that there are these.

532
00:49:31,600 --> 00:49:35,520
But among other things, it outsources the normative work, right? At the end of the day,

533
00:49:36,800 --> 00:49:40,720
presumably that the reason to disambiguate these different pathways is to say

534
00:49:41,440 --> 00:49:45,440
someone believes that, you know, they usually put as like in the terms of like some paths

535
00:49:45,440 --> 00:49:50,880
are permissible, like maybe they run through like unambiguous qualifications for the

536
00:49:50,880 --> 00:49:56,800
position being hired for, versus other paths are sort of like impermissible paths because all

537
00:49:56,800 --> 00:50:01,760
they're really doing is telegraphing information, but they're not actually influencing,

538
00:50:02,800 --> 00:50:08,080
you know, they're not actually, say, relevant to the job qualifications or whatever the context is,

539
00:50:08,080 --> 00:50:13,440
but there's still outsourcing the normative work of someone, well, someone has to go and say

540
00:50:13,440 --> 00:50:18,000
which paths are permissible and which paths are impermissible, and Lily has a really sharp

541
00:50:18,000 --> 00:50:24,400
critique. She also has a nice set of blog posts that are called like disparate causes, I believe,

542
00:50:24,400 --> 00:50:31,200
on this blog phenomenal world, but it goes into this problem kind of critically, and among other

543
00:50:31,200 --> 00:50:36,240
things, you know, getting at this question of what we call like a direct effect or an indirect

544
00:50:36,240 --> 00:50:44,480
effect is partly an artifact of the representation that we have, and there are some causal questions

545
00:50:44,480 --> 00:50:51,520
where for any kind of process that we describe, there's multiple different valid causal representations

546
00:50:51,520 --> 00:50:57,520
conceivably, right, because you can always like zoom into a via this variable in this variable

547
00:50:57,520 --> 00:51:02,080
in an edge between, you know, I can always zoom into it and say, oh, like it's not just that like,

548
00:51:02,080 --> 00:51:07,600
you know, someone's college influences their internship, it's actually that college influences

549
00:51:07,600 --> 00:51:13,360
this subtle decision that's made by some recruiter, which influences this, which influences that,

550
00:51:13,360 --> 00:51:20,720
and so you can always zoom into it and sort of bring more into focus on whether or not someone

551
00:51:20,720 --> 00:51:24,080
would like look, you know, now like a very generic question, like what is the average

552
00:51:24,080 --> 00:51:29,520
treatment effect? Maybe as long as you had, whether you had a very sort of granular or very sort of

553
00:51:29,520 --> 00:51:36,800
um, you know, coarse representation of some process, if they're both valid, you'll have the same

554
00:51:36,800 --> 00:51:42,080
answer for a question like that, but this question about like what are the pathways taken

555
00:51:42,720 --> 00:51:49,040
is sort of like, um, and are they permissible or not? Is sort of an artifact partly of

556
00:51:49,040 --> 00:51:54,240
at what resolution do you zoom into this process and do you capture it? And something might look

557
00:51:54,240 --> 00:51:59,440
okay, you know, if you zoom way out and you subsume a whole lot of mediators into just like an arrow,

558
00:51:59,440 --> 00:52:04,000
but if you zoom in closely and you knew more about how that process took place, then maybe you

559
00:52:04,000 --> 00:52:11,760
would say, oh, this isn't kosher. Um, so I think, you know, at a high level, um, I think that like

560
00:52:11,760 --> 00:52:17,920
causality gives us like a set of thinking tools for thinking critically about some of these

561
00:52:17,920 --> 00:52:24,400
problems, and they are maybe in some way like a partial step in the right direction.

562
00:52:25,200 --> 00:52:32,320
Um, but at the same time, you know, I don't think it's like a magic bullet that like sort of addresses

563
00:52:32,320 --> 00:52:38,960
all questions of, of, of fairness or justice or discrimination, and I think that often,

564
00:52:40,160 --> 00:52:45,360
you know, they're, they're, that the, the sort of like model that was sufficiently rich to be able

565
00:52:45,360 --> 00:52:49,520
to, you know, even if you believe that they were, like you couldn't, you wouldn't actually have,

566
00:52:50,400 --> 00:52:54,880
you know, uh, you wouldn't actually be able to like produce the causal models that you could

567
00:52:54,880 --> 00:52:59,360
fully resolve those questions. And I think what, one nice point that Lily makes, um, and I think

568
00:52:59,360 --> 00:53:06,080
I might have been, uh, um, in joint work with, with Issa, but, but I remember one of the points is

569
00:53:06,080 --> 00:53:11,280
is that, you know, I think that there's a lot of times like a danger is that it could be a little

570
00:53:11,280 --> 00:53:15,920
bit of a distraction, you know, like you said, there's like heroic amount of, you know, I have to know

571
00:53:15,920 --> 00:53:20,400
every single variable and every single thing and estimate every single relation. Uh, and before I

572
00:53:20,400 --> 00:53:25,760
can make any kind of conclusion about whether there's discrimination, um, that there might not

573
00:53:25,760 --> 00:53:29,360
actually be necessary and it's not in general what we do. I think there are situations where we can

574
00:53:29,360 --> 00:53:36,080
size up, um, at, at a bird's eye view that there's, there's some fundamental like inequity in

575
00:53:36,080 --> 00:53:40,960
society and, and, and conclude that we all, that we, we have some responsibility to do something about

576
00:53:40,960 --> 00:53:46,640
it and that that doesn't need to be contingent upon saying that like I've exactly estimated every

577
00:53:46,640 --> 00:53:54,160
single possible, you know, uh, causal functional on the pathway of every single factor that, you

578
00:53:54,160 --> 00:53:59,920
know, plays any role in, uh, you know, on the path to some decision that's made about someone in

579
00:53:59,920 --> 00:54:06,240
their life that that might set like a, um, at the end of the day, like too high a bar that, you know,

580
00:54:06,240 --> 00:54:13,600
you kind of, um, that, that I think that I think we're, we're able to recognize cases of discrimination

581
00:54:13,600 --> 00:54:20,480
and plenty of cases where, where we're not able to do this kind of like, you know, um, hurtfully and

582
00:54:20,480 --> 00:54:27,600
like numerical feed. Let's maybe shift gears and talk a little bit about, uh, use cases or

583
00:54:27,600 --> 00:54:36,640
application areas that have made notable progress in 2021. Um, anything come to mind there?

584
00:54:38,960 --> 00:54:45,280
You know, well, look, um, one, one obvious one and, you know, as much as I might, you know,

585
00:54:45,280 --> 00:54:51,280
be kind of called on as a contrarian, but like, uh, well, one more, I think I'd give some credit is

586
00:54:51,280 --> 00:55:00,080
I think, um, alpha folds from deep mind, um, like I'm not a protein folding expert, but I know

587
00:55:00,080 --> 00:55:05,600
some people that are not just like gullible deep learning boosters, um, who do work in the area

588
00:55:05,600 --> 00:55:14,320
and as far as I can tell, it's like actually a pretty significant, um, leap forward, um, that,

589
00:55:14,320 --> 00:55:18,480
you know, it's work that, you know, could very well have, you know, one like a significant,

590
00:55:18,480 --> 00:55:25,280
you know, science prize, you know, uh, like that level of accomplishment, um, and, you know,

591
00:55:25,280 --> 00:55:29,760
that's a little bit, you know, hearsay and that like, I'm not, I'm not, uh, an expert in,

592
00:55:29,760 --> 00:55:34,880
in protein folding, but as far as I understand it, it really is, uh, a legitimate significant

593
00:55:34,880 --> 00:55:39,120
contribution. And I think an area where, you know, maybe deep learning wasn't quite as,

594
00:55:39,120 --> 00:55:46,240
you know, inlined as a essential tool. So that that's certainly, um, a use case. I think you're

595
00:55:46,240 --> 00:55:55,840
starting to see, um, a lot of the use cases that were maybe obvious ones, um, but not necessarily,

596
00:55:56,800 --> 00:56:01,840
um, you know, for example, like radiology, it's sort of like an obvious initial thing because

597
00:56:02,880 --> 00:56:06,000
and harkening back to like our earlier conversation about the difference between prediction and

598
00:56:06,000 --> 00:56:12,080
decision, part of why radiology is like people see it as like this big target is that, um, you

599
00:56:12,080 --> 00:56:16,000
know, there are certain roles of the radiologists where they really are involved in decision

600
00:56:16,000 --> 00:56:20,720
making and, uh, recognition is part of a weird, it's like an interventional radiology,

601
00:56:20,720 --> 00:56:25,280
but there are also our lots of people who literally are looking at images and making classifications

602
00:56:25,280 --> 00:56:31,040
and medical imaging is a case like a diagnostic kind of imaging, right? And I think that's a case where

603
00:56:32,160 --> 00:56:37,840
we've known since the moment, you know, the kind of big image recognition results started

604
00:56:37,840 --> 00:56:43,280
hitting and say 2012, 2013 that radiology was a potential target and he had some maybe

605
00:56:43,280 --> 00:56:49,440
overly, uh, optimistic statements from like Jeff had been like, if you're, you know, if I were,

606
00:56:49,440 --> 00:56:54,640
if you're a medical school now, do not specialize in radiology, it hasn't quite gotten to that point,

607
00:56:54,640 --> 00:56:58,640
it hasn't taken the radiologist out of the loop, but I've been, I've been chatting with a lot of

608
00:56:58,640 --> 00:57:04,400
radiologists recently and I've been surprised to find, well, sort of two things, one, on one side,

609
00:57:05,120 --> 00:57:11,040
that some of the systems really are quite good and you actually have some systems being

610
00:57:11,040 --> 00:57:17,120
deployed already, actually, like, actually piping information into patient records.

611
00:57:18,400 --> 00:57:22,320
And at the same time that I think some of the problems that we discussed earlier about what can

612
00:57:22,320 --> 00:57:27,200
go wrong are happening on the ground and you do have a situation where, for example,

613
00:57:28,640 --> 00:57:35,600
systems that work well on one set of equipment are not performing well on, you know, some new scanner,

614
00:57:35,600 --> 00:57:41,040
which, you know, you know, it's efficiently similar to all the other scanners that a human

615
00:57:41,040 --> 00:57:45,680
radiologist would have no problem. And these are not adversarial examples, nobody's out there

616
00:57:45,680 --> 00:57:49,520
designing a scanner, there's not like, there's like a radiologist on the, I'm going to, I'm going

617
00:57:49,520 --> 00:57:54,320
to build a scanner that just fucks up all the previous, fucks all the deep learnings, so that way

618
00:57:54,320 --> 00:58:01,920
we can like keep our dogs. So I think you sort of have, you know, both, uh, a moment of the

619
00:58:01,920 --> 00:58:09,360
technology actually kind of like making landfall. But I think you also have some, some moment of

620
00:58:09,360 --> 00:58:17,440
like the rubber hitting the road and people, people sort of seeing up front some, you know, up close,

621
00:58:17,440 --> 00:58:24,000
some, some of the ways in which the technology is brittle and dangerous. Yeah. And I think that

622
00:58:24,000 --> 00:58:28,400
largely, if this might take an unsexy story because it's like what's like the big sexy application,

623
00:58:28,400 --> 00:58:34,880
I think largely this story now. And I think that overall the biggest like, if I were to like,

624
00:58:34,880 --> 00:58:39,760
take a, like a bird's eye view of the economy and just like, I'm just watching like, what is AI

625
00:58:39,760 --> 00:58:45,280
doing? You know, I think, right, the story of like 2014, 2015 is like these new use cases,

626
00:58:45,280 --> 00:58:50,320
like fundamental new things popping up, like things we weren't doing with deep learning suddenly,

627
00:58:50,320 --> 00:58:54,720
like machine translation, people swapping out the old guts and sticking into deep learning

628
00:58:54,720 --> 00:59:00,560
systems. And, um, suddenly like every single mobile phone having the capacity to run some kind

629
00:59:00,560 --> 00:59:06,240
of small deep learning model because it's being used for recognizing objects in the cameras and

630
00:59:06,240 --> 00:59:10,560
doing the face recognition that unlocks your phone and all of that. I think the bigger story of

631
00:59:10,560 --> 00:59:18,000
the last couple of years has been more on the side of, um, more on the side of like, deployment

632
00:59:18,000 --> 00:59:25,040
and like diffusion and like maturity of the, like, uh, like operations around ML. Like, I

633
00:59:25,040 --> 00:59:28,640
notice more and more companies that like their pain point isn't that they need someone who could

634
00:59:28,640 --> 00:59:32,640
train a model. Their pain point is they need an ML ops president. They need someone who,

635
00:59:32,640 --> 00:59:38,800
who can actually keep the crap running day in, day out that someone who knows, you know,

636
00:59:38,800 --> 00:59:44,480
it's like, there's some specialized, it's not, it's like a pure like ML researcher, like someone

637
00:59:44,480 --> 00:59:49,840
like me even, like, I don't have this skill set. I haven't spent my life in like, you know,

638
00:59:49,840 --> 00:59:54,640
there's, there's a real serious discipline and like keeping software working day in, day out.

639
00:59:54,640 --> 00:59:58,640
Like the people, it's, it's amazing what we could do when Eastern companies that like have a

640
00:59:58,640 --> 01:00:03,920
software that, you know, product that like 400 million people use every day and they like go

641
01:00:03,920 --> 01:00:09,760
seven years without a single, you know, hour of downtime. You know, it's absolutely bonkers how

642
01:00:09,760 --> 01:00:14,960
difficult that is. And machine learning has rose in a whole, you know, I think researchers don't

643
01:00:14,960 --> 01:00:18,800
have that. But machine learning grows like a weird set of complications because all kinds of ways

644
01:00:18,800 --> 01:00:23,200
that things go wrong, even if there aren't software bugs. And so they need to understand something

645
01:00:23,200 --> 01:00:28,240
about enough about statistics. They have some sense of what could go wrong and ways that you need

646
01:00:28,240 --> 01:00:33,120
to model things that aren't software glitches. They're like, the world changing glitches.

647
01:00:33,120 --> 01:00:37,680
There's like the world is the bug. Even if, even if everything's coded precisely and need to be

648
01:00:37,680 --> 01:00:43,680
able to interface back and forth between like software developers, ML engineers and researchers.

649
01:00:43,680 --> 01:00:51,280
So I think like the maturity of ML ops and also just broadly like the use of ML not just in,

650
01:00:51,280 --> 01:00:57,120
you know, I think there was a moment right when there was Google, Amazon, Facebook, Microsoft.

651
01:00:58,240 --> 01:01:05,280
And I don't know if you ever read this, you know, but like I wrote this like satire that just

652
01:01:05,280 --> 01:01:09,440
because like when everyone was making a big deal about like, oh, whatever professor left to go to

653
01:01:09,440 --> 01:01:13,760
whatever company and just their salary and they were kind of writing about it, like, you know,

654
01:01:13,760 --> 01:01:18,800
almost like like football players getting traded or something. And so I wrote this stupid post

655
01:01:18,800 --> 01:01:23,040
that was just like announcing that I had been hired as like the intergalactic head of the

656
01:01:23,040 --> 01:01:27,520
machine learning by Johnson and Johnson or something. And I'd be, you know, for some, you know,

657
01:01:27,520 --> 01:01:33,520
astronomical sum. And it was just a stupid joke. But the final is like a year later,

658
01:01:33,520 --> 01:01:38,160
I forget where I was and I met someone and they worked at like Johnson and Johnson AI research.

659
01:01:38,160 --> 01:01:44,800
Right. And I think that this is part of like what's going on now is that like there was a moment in

660
01:01:44,800 --> 01:01:49,920
time. I'm sure like I'm making this up. I researched it before though, you know, but you know,

661
01:01:49,920 --> 01:01:54,160
this is what you come here to speak from academic authority to make up crap on your podcast.

662
01:01:55,360 --> 01:02:00,720
I'm sure there was a moment in time where like there were only a small number of like elite tech

663
01:02:00,720 --> 01:02:06,000
firms that were using like modern SQL databases. You know, when it was fresh, I think it was at IBM,

664
01:02:06,000 --> 01:02:08,560
when it was developed, but there's probably a moment. It was just like a really hot,

665
01:02:09,120 --> 01:02:13,600
fundamentally new technology that really changed its business operations at places. And there were

666
01:02:13,600 --> 01:02:18,160
like a handful of like super technical firms that knew how to do it. And now it's like the most

667
01:02:18,160 --> 01:02:24,480
boring technical firm in the world uses SQL. Right. And I think that this is a huge part of what's

668
01:02:24,480 --> 01:02:29,120
happening in AI if you were to size up like commercial environment. So I think there are exciting

669
01:02:29,120 --> 01:02:34,320
startups that are using this technology in new ways. There are, you know, interesting things going

670
01:02:34,320 --> 01:02:39,520
on at like the sexy tech companies. But I think there's a lot of like, you know, there's no company

671
01:02:39,520 --> 01:02:45,360
that you go to, you know, whether it's like, you know, I'm sure if you went to like a waste management

672
01:02:45,360 --> 01:02:50,800
company, like they're finding, you know, they're using AI for something or forecasting demand or

673
01:02:50,800 --> 01:02:56,000
trying to figure out how to route their trucks or something. And I think that like this sort of

674
01:02:56,000 --> 01:03:05,680
just general like progression of AI from like a luxury good to a commodity is like an essential

675
01:03:05,680 --> 01:03:12,400
part of what's going on. And like the fact that like, yeah, like every company has, you know,

676
01:03:12,400 --> 01:03:19,440
this is becoming their concern. And I think I think part and parcel of that is the way that the

677
01:03:19,440 --> 01:03:25,040
tooling is getting better and better and better. A whole lot of companies, you know, like what are

678
01:03:25,040 --> 01:03:30,320
they offering? It's like things that make it that like the stuff that everyone's already been doing

679
01:03:30,320 --> 01:03:37,200
for a while that anyone could do it, right? And it's easy to track and, you know, it's easy to

680
01:03:37,200 --> 01:03:42,400
organize. Like, you know, I think this movement of like AI from a concern of like what's the new

681
01:03:42,400 --> 01:03:48,640
model to like what is like a stable workflow that we can adopt such that a company that can't

682
01:03:48,640 --> 01:03:52,800
spend half a million dollars for engineer can still use this technology like successfully and

683
01:03:52,800 --> 01:03:58,080
profitably. I think that's a major part of the story of like the commercial application of AI

684
01:03:58,080 --> 01:04:03,200
right now. And it's kind of like, it's a pretty unsexy story. Maybe I've just like, oh, this is

685
01:04:04,000 --> 01:04:08,640
becoming right. But I think this is what happens to everything, right? Like, I don't know,

686
01:04:08,640 --> 01:04:13,040
I'm like, sorry, if you're a AI researcher, but if you're an MLOPS, it's pretty cool.

687
01:04:13,760 --> 01:04:19,040
Oh, yeah, absolutely. I don't really cool stuff happening in that field. And there's like a lot more

688
01:04:19,040 --> 01:04:25,680
jobs at every company in the world together than there is at like whatever it is, like Apple

689
01:04:25,680 --> 01:04:31,840
Microsoft, Amazon, Facebook, whatever. So I think that moved. Before we run out of time,

690
01:04:32,960 --> 01:04:38,080
I'd love to have you kind of dust off the crystal ball a little bit more and kind of share some

691
01:04:38,080 --> 01:04:45,280
of your predictions for the upcoming year, years. We've talked a little bit about where you'd

692
01:04:45,280 --> 01:04:55,520
swing the bat from a research perspective, but how do you think 2022? With the backdrop of

693
01:04:55,520 --> 01:05:02,800
kind of the, I don't know if you'd call it a cooling or slowing or boring a vacation or whatever

694
01:05:02,800 --> 01:05:12,640
you'd want to call it. Are there innovations that are, you know, you kind of see the silhouette

695
01:05:12,640 --> 01:05:20,880
emerging from the shadows and you think something's there? It's not. The funny thing about that

696
01:05:20,880 --> 01:05:26,880
is it's not like it's all cooling or it's all heating up. It's like the coolest or weirdest

697
01:05:26,880 --> 01:05:31,440
interesting thing about it is that whenever you sum up something like a complex

698
01:05:32,080 --> 01:05:38,720
phenomena with a single number, you lose a lot of information. I think it's like more like

699
01:05:38,720 --> 01:05:44,160
follow the Roman Empire, right? It's like like the like Rome's all right. Like Rome is still partying

700
01:05:44,160 --> 01:05:49,680
and like the borders are still expanding, but you also have like, you know, like you have like cities

701
01:05:49,680 --> 01:05:54,080
being lost and whole country is going off the map and it's like, I think that's happening,

702
01:05:54,080 --> 01:05:59,680
right? Like you have like like Uber AI shutting down. I research you have like hiring

703
01:05:59,680 --> 01:06:04,880
freezes at major companies. The big leader is like having major hiring freezes not offering

704
01:06:04,880 --> 01:06:10,080
the kinds of salaries in 2022 that they were offering in 2018 to like, it's kind of like well-known

705
01:06:10,080 --> 01:06:14,800
researchers. And at the same time, you have like whole companies where like the shockwave hasn't

706
01:06:14,800 --> 01:06:19,440
even hit them yet. And they're like first getting into the like like major health systems starting

707
01:06:19,440 --> 01:06:24,800
to adopt like, you know, deep learning and I think that yeah, there's that going on. If I had to

708
01:06:24,800 --> 01:06:35,040
predict what's going to happen, I'm going to double down on decision-making. So, you know, I think

709
01:06:35,040 --> 01:06:40,960
it's like that I'm already seeing a lot of is, you know, like, you know, you could think of like

710
01:06:41,600 --> 01:06:48,160
like two things came, a few things came together that made AI so hot, which is like one was suddenly

711
01:06:48,160 --> 01:06:55,600
the fact that like the existence of easily queriable, well-organized curated data at every single firm

712
01:06:55,600 --> 01:06:59,760
in the world, you know, the fact that like health companies are using electronic health records,

713
01:06:59,760 --> 01:07:04,480
every company being basically an internet company, everyone having a digital trace of all their

714
01:07:04,480 --> 01:07:08,320
customer interactions. Now, you know, we can get to a separate like normative point about whether

715
01:07:08,320 --> 01:07:12,480
we want to live in that world or like whether we're irked by the surveillance state or from a

716
01:07:12,480 --> 01:07:18,400
standpoint, like an economic standpoint, that happened together with like advances in both the

717
01:07:18,400 --> 01:07:22,400
tooling and algorithms around statistical modeling. And so the question became we have this data,

718
01:07:22,400 --> 01:07:28,400
we have statistical tools, how do we do this like analytics on the data, right? But there's

719
01:07:28,400 --> 01:07:32,400
another side which is like, how do we guide, how do we use the data to guide actions? And I think

720
01:07:32,400 --> 01:07:39,840
people, I think one one thing that is underutilized by most firms and I think only a small number of

721
01:07:39,840 --> 01:07:46,080
people are really sophisticated about it is, is really focusing on this like the decision problem.

722
01:07:46,080 --> 01:07:50,800
And part of that is, part of that is, you know, offline causal inference, which is some of the

723
01:07:50,800 --> 01:07:55,280
stuff we were talking about, like how can I use some some causal background knowledge together with

724
01:07:55,280 --> 01:08:00,960
the data that I have to infer a causal effect and use that to guide decisions. But a huge part of

725
01:08:00,960 --> 01:08:07,200
that is experimentation. And I think that this is a huge thing that not enough companies do that

726
01:08:07,200 --> 01:08:12,880
you're going to start seeing, you know, become, you know, obviously like Amazon, you know, has,

727
01:08:12,880 --> 01:08:17,360
has the what they call like web labs, you know, where the, you know, Google, like, you know, does

728
01:08:17,360 --> 01:08:22,400
randomized control trials for, you know, which shade of green the G should be in Google or something.

729
01:08:23,280 --> 01:08:27,680
But I think most companies grossly underutilized experimentation, like really methodical

730
01:08:27,680 --> 01:08:32,880
experiments, because, you know, that plays into the data fixer. Online experiments in particular?

731
01:08:32,880 --> 01:08:38,960
Well, online, not necessarily in the sense, I mean, I think online is part of, you know, in a sense of

732
01:08:38,960 --> 01:08:42,720
like, you know, doing like reinforcement learning or having a policy that's adaptive as you're getting

733
01:08:42,720 --> 01:08:48,720
the results, but even experimenting at all, right? Like just like we've been guiding, if you look

734
01:08:48,720 --> 01:08:54,560
at how we guide personalized decisions, it's often in the context of I just take passively collected

735
01:08:54,560 --> 01:08:58,800
traces of people's data. I do some kind of latent factor analysis or whatever to build a

736
01:08:58,800 --> 01:09:04,480
recommended system versus actually I'm going to randomize choices and try to estimate the sort of

737
01:09:04,480 --> 01:09:08,880
like, you know, potentially like heterogeneous treatment effects of how different people will

738
01:09:08,880 --> 01:09:14,560
respond differently to different things, but actually to estimate the effects on, you know,

739
01:09:14,560 --> 01:09:18,400
whether it's people's behavior or whatever, you know, I don't mean this is sort of like a sound like

740
01:09:18,400 --> 01:09:23,920
I'm, I'm advising that we like really nearly experiment on people without thinking about the

741
01:09:23,920 --> 01:09:29,120
considerations or which decisions or which experiments are potentially like of ethical import,

742
01:09:29,120 --> 01:09:34,160
and obviously there's a lot that needs to, there's a lot of considerations that need to go into

743
01:09:34,160 --> 01:09:43,440
how you do that and doing it right. But yeah, I think that, you know, the reckoning we're seeing,

744
01:09:43,440 --> 01:09:48,720
I think is over and over again, it's like people claiming the AI is going to, you know,

745
01:09:48,720 --> 01:09:53,760
going to personalize this, personalize that, it's going to lead you to make all these different decisions

746
01:09:53,760 --> 01:09:59,040
in better ways, and then people find like, oh, I just now newly trained a predictive model came up

747
01:09:59,040 --> 01:10:04,800
with some heuristic for how to operationalize that its decision, and something didn't go as planned.

748
01:10:04,800 --> 01:10:10,800
And I think that people actually getting more into this world of both using offline, you know,

749
01:10:10,800 --> 01:10:15,760
kind of causal inference on observational data, but also actually experimenting in the real world,

750
01:10:15,760 --> 01:10:21,600
and, you know, developing more mature processes for saying, how do I, how do I test hypotheses?

751
01:10:21,600 --> 01:10:27,280
How do I see what the impacts are of, you know, different actions that I have? I think that that's

752
01:10:27,280 --> 01:10:33,200
going to become more and more and more important, and you're going to start seeing the like hiring

753
01:10:33,200 --> 01:10:42,480
focus, you know, and just like where teams start moving, you know, towards those kinds of

754
01:10:42,480 --> 01:10:47,600
problems, and again, I don't think this is like overnight, you're going to go from people like

755
01:10:47,600 --> 01:10:53,040
hiring 90% which, you know, deep learning, you know, like, like pie towards jockey to like 90%

756
01:10:53,040 --> 01:10:59,280
hiring, you know, experts in like, you know, bounded algorithms and causal inference,

757
01:10:59,280 --> 01:11:05,280
but I do think that there is a, there's a shift here, I'm seeing it at every level, I'm seeing it

758
01:11:05,280 --> 01:11:10,480
in what, what looks, you know, interesting among new students, what looks interesting among

759
01:11:10,480 --> 01:11:17,280
our folks hitting the hiring market, I think that this sort of intersection of like, you know,

760
01:11:17,280 --> 01:11:25,200
CS, operations, research, economics, and bringing to bear, like, you know, tools of predictive

761
01:11:25,200 --> 01:11:33,920
modeling that we've gotten, but also more sophisticated processes of experimentation and

762
01:11:33,920 --> 01:11:40,720
estimating causal effects, and principles of just guiding, you know, intelligent decision-making.

763
01:11:40,720 --> 01:11:47,360
I see there's like a, I think there's like a growing up process happening there, and you know,

764
01:11:47,360 --> 01:11:55,440
I think the other thing though is, well, one thing I add on, and this is not like a specific

765
01:11:55,440 --> 01:12:03,040
prediction, but a meta prediction, is, you know, like the internet, like web 2.0, web 3.0,

766
01:12:03,040 --> 01:12:08,400
whatever the hell we're doing, like very little, there's a lot of like new, there's a lot of new

767
01:12:08,400 --> 01:12:12,640
stuff that we're seeing, and like the way companies are behaving in the way they're interacting with

768
01:12:12,640 --> 01:12:18,720
people, that isn't technologically new, right? There's a lot of stuff that like you could have done

769
01:12:18,720 --> 01:12:24,640
from the late 1990s, the tooling wasn't in there, which restricted how many people could develop it,

770
01:12:26,000 --> 01:12:30,000
but it was something else, it was something about like, you know, there was a capability that came,

771
01:12:30,000 --> 01:12:34,000
and a few people have figured out some very like, some early players that figured out like, you know,

772
01:12:34,000 --> 01:12:37,840
how to conquer e-commerce, like Amazon, whatever, but it took a long time before you got to Uber,

773
01:12:38,480 --> 01:12:43,840
right? And so there are certain innovations there that were like, you know, it's like there were

774
01:12:43,840 --> 01:12:48,240
a bunch of pieces that need to fit together, like a certain understanding of like markets,

775
01:12:48,240 --> 01:12:52,880
or a certain understanding of like usage patterns of phones with the technological capability that

776
01:12:52,880 --> 01:13:00,240
had been there all along, and I think that like, there's a kind of like innovation in deployment

777
01:13:00,240 --> 01:13:04,720
that doesn't actually correspond, like I think when people have been stoked about ML recently,

778
01:13:04,720 --> 01:13:10,960
right? It's been like, oh, BERT is like good at classifying texts, or you know, like, you know,

779
01:13:10,960 --> 01:13:16,080
seek to seek, you know, LSTMs, and then Transformers are good at, you know, this one thing.

780
01:13:16,080 --> 01:13:20,400
There's like single-purpose models, but like, I'll give you an example, so I'm an advisor for

781
01:13:20,400 --> 01:13:27,760
a company, so it's full COI. I'm an advisor for a company called a bridge AI, and a bridge is

782
01:13:27,760 --> 01:13:34,640
a company that like is sitting between like doctors and the patients, and sitting in this like

783
01:13:34,640 --> 01:13:42,240
interaction where patients, it turns out patients are recording their visits on their cell phones,

784
01:13:42,240 --> 01:13:45,760
and they're doing this already. Sometimes sort of viciously, sometimes the doctors can

785
01:13:45,760 --> 01:13:51,120
send, and may or may not be wired up, and depending upon like what your particular state, it's like,

786
01:13:51,120 --> 01:13:57,040
you know, two-party sends, so their idea is like, let's inline this as a normal part of the

787
01:13:57,040 --> 01:14:01,520
doctor-patient interaction. Like, let's have permission, let's have both parties get in,

788
01:14:01,520 --> 01:14:06,000
they'll agree to record the conversation, they'll pull out a bridge, and then they record it, and

789
01:14:06,000 --> 01:14:10,320
there's all kinds of different things you could do, right? Like, you can help the doctor to draft

790
01:14:10,320 --> 01:14:16,240
the summary of the visit, you can help the patient to understand, like, oh, like, you know, don't

791
01:14:16,240 --> 01:14:21,200
forget, like you mentioned that you would be starting this new medication, have you picked it up,

792
01:14:21,200 --> 01:14:25,600
or have you, you know, called in that prescription, or did you schedule out this follow-up?

793
01:14:25,600 --> 01:14:31,360
So there's like a million different places to plug in models, and any one of them by itself may

794
01:14:31,360 --> 01:14:38,320
or may not be like, you know, a single purpose, like, major innovation, but the ways that you can

795
01:14:38,320 --> 01:14:42,160
mix and match these, like, okay, I've got the conversation, I've got to send it to an ASR model,

796
01:14:42,160 --> 01:14:46,160
like, in fact, the text of the text, I need to flag out, like, well, what are the interest,

797
01:14:46,160 --> 01:14:51,680
what are the relevant or salient parts of the conversation? How do I then take that, turn it into,

798
01:14:51,680 --> 01:14:57,600
like, an interface feature that, you know, provide some value or a mixing's useful to the patient?

799
01:14:57,600 --> 01:15:01,040
And I think that, like, a lot of things, like, when Alexa works really well, right?

800
01:15:01,760 --> 01:15:05,600
You know, or Google Home, or any of these things, why don't work really well? It's usually not

801
01:15:05,600 --> 01:15:11,680
because there's one model that's, like, magnificent. It's, like, the magic is in the clever way that they

802
01:15:11,680 --> 01:15:18,320
stitch together, you know, some astute observations about what are the common interaction patterns

803
01:15:18,320 --> 01:15:22,880
together with, like, what are the right little places you can patch in machine learning,

804
01:15:22,880 --> 01:15:28,240
and the right ways that you can, like, patch in some intelligent heuristics and rules around it,

805
01:15:28,240 --> 01:15:35,280
such that, like, you know, you have, like, an end-to-end product that feels like it's magic, right?

806
01:15:35,280 --> 01:15:39,040
Um, you know, Shazam even is a little bit like that, but when these things are like, there's a few,

807
01:15:39,040 --> 01:15:43,520
like, like, a little heuristics, where if you start thinking, how do I decompose this into, like,

808
01:15:43,520 --> 01:15:46,800
something that works? It's like, you can make a pipeline where every single step of it's kind of

809
01:15:46,800 --> 01:15:51,520
simple, but, like, the end of the end result is something where it feels a little bit magical, right?

810
01:15:51,520 --> 01:15:56,320
And I think that, like, this is going to be a, I think, a major part of this is, like, I think

811
01:15:56,320 --> 01:16:00,000
we've been looking at, like, people who are really good at building single-purpose models,

812
01:16:00,000 --> 01:16:04,960
then turning that into a big start-up, and, or trying to turn, parlay that into a start-up,

813
01:16:04,960 --> 01:16:08,880
and I think there will be some amount of, like, the single-purpose models are mature,

814
01:16:08,880 --> 01:16:14,400
and they'll get a little bit better, but what's maybe under-explored a bit are ways that you mix

815
01:16:14,400 --> 01:16:20,320
and match models together with cool interaction patterns, and, you know, some clever understanding

816
01:16:20,320 --> 01:16:26,000
of, like, what people want, and, you know, what data's available, et cetera, to build, like, kind of,

817
01:16:26,000 --> 01:16:32,000
user experiences that maybe under the hood are invoking, like, seven different models, and

818
01:16:32,000 --> 01:16:36,560
seven different contact, and, but it's, like, kind of hidden from the user in a clever way, where it

819
01:16:36,560 --> 01:16:41,600
just feels like you're having, like, it adds up to a new capability that no one model,

820
01:16:42,480 --> 01:16:47,360
or piece of software, like, by itself would provide. And so, I do think that there is some

821
01:16:47,360 --> 01:16:53,200
element of this, like, you know, like, we've built a bunch of cool Legos, and we haven't given

822
01:16:53,200 --> 01:16:57,520
people that many years, so, like, you know, instead of, like, you know, some innovation comes

823
01:16:57,520 --> 01:17:01,120
from, like, I designed a new Lego piece, but, like, I think a lot of innovation will come from

824
01:17:01,840 --> 01:17:08,800
people that are, you know, you know, don't have to be, like, have, like, off-the-chart skills at

825
01:17:08,800 --> 01:17:14,400
building Legos, but they're really, they have a kind of design sentence for, you know, what are

826
01:17:14,400 --> 01:17:21,200
cool ways to put them together? I think that's a, it's a natural consequence of the, the broader

827
01:17:21,200 --> 01:17:26,240
maturity conversation we've had, we've been having, right? The Lego pieces are, you know, that

828
01:17:26,240 --> 01:17:30,560
that we've come up with every Lego piece that's ever going to be created, and there aren't some

829
01:17:30,560 --> 01:17:36,080
cool ones to come, but all the basic pieces required to build really cool stuff is in place, and

830
01:17:36,080 --> 01:17:41,360
now it's all about, how do you put them together? Yeah, and even more so than the pieces themselves,

831
01:17:41,360 --> 01:17:46,800
the tools to easily put them in place, you've got your hugging faces, you've got your ML ops tools,

832
01:17:47,600 --> 01:17:52,960
like, it's a great time to be a builder. Right, yeah, it also, like, it takes some of that work

833
01:17:52,960 --> 01:17:58,720
away that it allows you to focus, right? I think music's like that a little bit, right? Like,

834
01:17:58,720 --> 01:18:01,840
you know, there's this way, like, when you're learning an instrument, and you're like, I got to

835
01:18:01,840 --> 01:18:05,760
practice articulation, I got to practice rudiment, I got to practice scales, I got to do that, and you,

836
01:18:05,760 --> 01:18:08,640
you know, you're sitting here, you're going to do, do, do, do, do, do, do, do, do, do, do, do, do, do, do, do,

837
01:18:08,640 --> 01:18:12,400
and then you're playing this kind of shit over and over and over again when you're like 10,

838
01:18:12,400 --> 01:18:17,120
11, 12, 13, 14 years old, but you get to some point where like, maybe you still practice like that

839
01:18:17,120 --> 01:18:22,960
one hour a day, but when you go to play, you're not even thinking at that level at all, like,

840
01:18:22,960 --> 01:18:27,280
it's not, and I think there is some element of that, of like, people using machine learning recently

841
01:18:27,280 --> 01:18:30,560
have been thinking like, just like, how do I get the data and train a single model? I think,

842
01:18:30,560 --> 01:18:33,760
once you have a lot of these contexts where maybe you don't even need to train a model,

843
01:18:33,760 --> 01:18:36,880
maybe there's an off-the-shelf model that's sufficiently good at this task and it works better

844
01:18:36,880 --> 01:18:40,880
than anything you could train, even if you're applying it sort of on slightly different

845
01:18:40,880 --> 01:18:44,320
domain-shifted data, right? Then you start getting at this point where,

846
01:18:45,920 --> 01:18:52,480
right? Like the difference between like a great artist and like a super boring artist isn't

847
01:18:52,480 --> 01:18:58,400
that like the great artist is better at scales, you know? It's not, it's at a point, you know,

848
01:18:59,600 --> 01:19:03,280
right? So it's not like, oh, like, like, mild Davis played like better, you know,

849
01:19:03,280 --> 01:19:07,200
played like cleaner scales than, you know, like, I don't know.

850
01:19:07,200 --> 01:19:10,640
He's hated the artist staying on key or something like that. It's not like that.

851
01:19:12,000 --> 01:19:19,760
Yeah. So, I think there is, you know, a lot of, a lot of iteration that did be had on that side.

852
01:19:21,120 --> 01:19:28,080
Yeah. Awesome. Awesome. Well, Zach, it has been wonderful catching up. Let's make sure it's not

853
01:19:28,080 --> 01:19:34,160
two years, until the next time. Yeah. Right. Who knows what pandemic will be in full swing

854
01:19:34,160 --> 01:19:44,160
by that time. Awesome. Well, thanks so much for helping us reflect on 2021 and the ML&D

855
01:19:44,160 --> 01:20:00,080
LDM and catch you next time. Yeah, thanks for having me sound. Great to see you.

