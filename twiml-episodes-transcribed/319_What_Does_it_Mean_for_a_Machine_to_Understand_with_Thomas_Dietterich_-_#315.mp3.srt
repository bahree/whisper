1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:24,240
I'm your host Sam Charrington.

3
00:00:24,240 --> 00:00:27,360
Did you miss TwimalCon AI platforms?

4
00:00:27,360 --> 00:00:32,880
If so, you'll definitely want to check out our TwimalCon Video Packages.

5
00:00:32,880 --> 00:00:38,320
Featuring over 25 sessions, discussing expert perspectives on ML and AI at scale and in

6
00:00:38,320 --> 00:00:43,800
production, you'll hear from industry leaders such as Facebook, Levi's, Zappos, and more

7
00:00:43,800 --> 00:00:50,400
about their experiences automating, accelerating, and scaling machine learning and AI.

8
00:00:50,400 --> 00:00:56,160
In each video package, you'll receive our keynote interviews, the exclusive team Teardown

9
00:00:56,160 --> 00:01:05,000
panels featuring Airbnb and SurveyMonkey, case studies, and more, over 13 hours of footage.

10
00:01:05,000 --> 00:01:10,920
Once again, visit twimalcon.com slash videos for more information or to secure your advanced

11
00:01:10,920 --> 00:01:17,440
purchase today.

12
00:01:17,440 --> 00:01:20,440
Alright everyone, I am on the line with Tom Dietrich.

13
00:01:20,440 --> 00:01:25,640
Tom is a Distinguished Professor Emeritus at Oregon State University.

14
00:01:25,640 --> 00:01:28,080
Welcome to the Twimal AI Podcast.

15
00:01:28,080 --> 00:01:29,080
It's nice to be here.

16
00:01:29,080 --> 00:01:30,360
Thank you for inviting me.

17
00:01:30,360 --> 00:01:31,360
Absolutely.

18
00:01:31,360 --> 00:01:34,560
I'm really looking forward to digging into this conversation.

19
00:01:34,560 --> 00:01:39,840
I'd love to, before we dive in, get a little bit of your...

20
00:01:39,840 --> 00:01:44,480
I'm really looking forward to diving into our conversation, which is going to be focused

21
00:01:44,480 --> 00:01:46,720
on a recent blog post that you wrote.

22
00:01:46,720 --> 00:01:51,600
There's really exploring what it means for a machine to understand.

23
00:01:51,600 --> 00:01:55,600
But before we do that, I'd love to learn a little bit about your background.

24
00:01:55,600 --> 00:02:00,600
And the context to which you bring to this conversation.

25
00:02:00,600 --> 00:02:06,200
Okay, well, I'm started out as one of the very first graduate students working in machine

26
00:02:06,200 --> 00:02:07,720
learning.

27
00:02:07,720 --> 00:02:12,560
My advisor, Richard McCalsky, was one of the three professors, along with Tom Mitchell

28
00:02:12,560 --> 00:02:19,080
and Jaime Carbonell at Carnegie Mellon, who launched the series of workshops and now

29
00:02:19,080 --> 00:02:22,640
conferences, the International Conference on Machine Learning.

30
00:02:22,640 --> 00:02:28,120
So back in 1980, there were about 30 of us in a classroom in Carnegie Mellon, and I've

31
00:02:28,120 --> 00:02:30,760
been working in the field since then.

32
00:02:30,760 --> 00:02:31,760
Wow.

33
00:02:31,760 --> 00:02:39,120
Over that time, I made some technical contributions in ensemble methods and in hierarchical

34
00:02:39,120 --> 00:02:45,560
reinforcement learning and various applications, for example, in pharmaceuticals, and I work

35
00:02:45,560 --> 00:02:52,840
in applications in weather network data and data cleaning, internet of things, kinds

36
00:02:52,840 --> 00:02:58,960
of things, but my primary research focus these days is on reliable, robust artificial

37
00:02:58,960 --> 00:03:04,320
intelligence, particularly in safety critical applications.

38
00:03:04,320 --> 00:03:11,400
So I've done a lot of service activities for the field I edited the machine learning

39
00:03:11,400 --> 00:03:17,200
journal for six years, and then moved into a position as the founding president of the

40
00:03:17,200 --> 00:03:21,560
International Machine Learning Society, which is the organization that runs the machine

41
00:03:21,560 --> 00:03:24,160
learning conference, the ICML conference.

42
00:03:24,160 --> 00:03:32,280
I also chaired the NURP's conference in 2000, and I served a term as president of the

43
00:03:32,280 --> 00:03:35,760
Association for the Advancement of Artificial Intelligence.

44
00:03:35,760 --> 00:03:40,960
My main service activity these days is I'm one of the moderators on archive for machine

45
00:03:40,960 --> 00:03:46,320
learning, which is between machine learning and computer vision, the two most active categories

46
00:03:46,320 --> 00:03:49,280
for the archive pre-print server.

47
00:03:49,280 --> 00:03:56,280
And so I mentioned this in the opening, but we're really digging into this topic of what

48
00:03:56,280 --> 00:04:01,440
it means for a machine to understand, a recent blog post of yours, and I thought to get

49
00:04:01,440 --> 00:04:05,920
things kicked off, I'd read your couple of the opening sentences.

50
00:04:05,920 --> 00:04:10,840
You wrote critics of recent advances in artificial intelligence complained that although these

51
00:04:10,840 --> 00:04:15,400
advances have reduced remarkable improvements in AI systems, these systems still do not

52
00:04:15,400 --> 00:04:19,160
exhibit real true or genuine understanding.

53
00:04:19,160 --> 00:04:23,920
The use of words like real true and genuine imply that understanding is binary, a system

54
00:04:23,920 --> 00:04:27,880
either exhibits genuine understanding or it does not.

55
00:04:27,880 --> 00:04:31,760
The difficulty with this way of thinking is that human understanding is never complete

56
00:04:31,760 --> 00:04:33,560
and perfect.

57
00:04:33,560 --> 00:04:39,400
So certainly the way you've laid that opening argument out, it resonates with me.

58
00:04:39,400 --> 00:04:46,680
I recently had Gary Marcus on the show, this was back in September, and we spoke about

59
00:04:46,680 --> 00:04:56,640
the book that he recently launched rebooting AI, and he's very outspoken as a critic of

60
00:04:56,640 --> 00:05:01,640
deep learning, and maybe that's not the way he would put it, maybe he'd say a critic

61
00:05:01,640 --> 00:05:09,360
of deep learning as a standalone path to artificial general intelligence, but reading

62
00:05:09,360 --> 00:05:16,120
the blog post, I couldn't help but think of Gary Marcus as being, although you didn't

63
00:05:16,120 --> 00:05:21,560
name him kind of the person in absentia that you were writing this to.

64
00:05:21,560 --> 00:05:29,760
Maybe talk a little bit about the broader context for this post and maybe how what prompted

65
00:05:29,760 --> 00:05:35,440
you to write it, if Gary was part of what you were thinking about or not, I'd love

66
00:05:35,440 --> 00:05:38,280
to kind of get a sense for where you were coming from here.

67
00:05:38,280 --> 00:05:45,680
Well certainly Gary and I have had an opportunity even to engage in formal debates, and Gary's

68
00:05:45,680 --> 00:05:50,600
as I was saying, I think Gary's main points I generally agree with, which is that there

69
00:05:50,600 --> 00:05:56,400
are lots of obvious shortcomings in our existing AI systems and in particular these systems

70
00:05:56,400 --> 00:06:03,600
based on deep learning, but Gary and other people can't seem to stop saying things like

71
00:06:03,600 --> 00:06:10,160
well, when we look at the behaviors say of Google Translate, it's clear that it doesn't

72
00:06:10,160 --> 00:06:17,280
have, it's not exhibiting real understanding of the language it's translating, or when

73
00:06:17,280 --> 00:06:23,800
we talk about Siri, that Siri doesn't really understand what we're talking about.

74
00:06:23,800 --> 00:06:30,160
And I've been making the counter argument, yes, these systems are understanding and it's

75
00:06:30,160 --> 00:06:34,160
real understanding, but it is narrow understanding.

76
00:06:34,160 --> 00:06:41,400
So I am criticizing the use of the word real to mean deep and complete understanding, because

77
00:06:41,400 --> 00:06:47,680
that denies that these systems are doing anything that is intelligent or that is exhibiting

78
00:06:47,680 --> 00:06:52,200
real understanding, and I think that puts you in the position that you will never be

79
00:06:52,200 --> 00:06:57,640
happy with any AI system because no matter how good it gets, it will make mistakes and

80
00:06:57,640 --> 00:07:00,120
exhibit failures and it's understanding.

81
00:07:00,120 --> 00:07:05,760
And are you going to say, well, when it understands 95% of what people say, that it's still

82
00:07:05,760 --> 00:07:11,080
not real understanding, I mean, you're pushing yourself into a belief that there's some

83
00:07:11,080 --> 00:07:15,560
magic threshold, that if you could somehow cross it, you would have a system that had

84
00:07:15,560 --> 00:07:17,520
real understanding.

85
00:07:17,520 --> 00:07:19,200
And I don't think that's the way it works.

86
00:07:19,200 --> 00:07:24,560
I think that the way it works is that we make incremental progress, sometimes bigger

87
00:07:24,560 --> 00:07:30,080
leaps, sometimes no progress for periods of time, as we were doing in speech recognition

88
00:07:30,080 --> 00:07:36,440
for a while in the 90s, but our systems get better, they are able to understand something.

89
00:07:36,440 --> 00:07:43,440
So as I say, when I tell Siri, please call Dan and it calls the right person, it has understood

90
00:07:43,440 --> 00:07:47,400
me for the purposes of that utterance for that task.

91
00:07:47,400 --> 00:07:53,120
Now if I said, you know, Siri, tell me what Dan means to me, and it doesn't really know

92
00:07:53,120 --> 00:08:01,760
anything about what it means, say, to be a best friend, but many people, of course, have

93
00:08:01,760 --> 00:08:07,680
remarked that it's impossible for two human beings to fully understand each other.

94
00:08:07,680 --> 00:08:12,240
So that brings me back again to what is it we're really trying to achieve when we build

95
00:08:12,240 --> 00:08:13,880
AI systems.

96
00:08:13,880 --> 00:08:20,600
And as an engineer, I would say, I want systems that can make the appropriate response when

97
00:08:20,600 --> 00:08:26,480
I ask them to do something, or if they're warning me of some situation in the world that

98
00:08:26,480 --> 00:08:28,920
I should be paying attention to, and so on.

99
00:08:28,920 --> 00:08:33,800
And to the extent that they do that correctly, I would say they understand what I want them

100
00:08:33,800 --> 00:08:34,800
to do.

101
00:08:34,800 --> 00:08:38,880
Yeah, when we have these kinds of conversations, I think it's, you know, there's a slippery

102
00:08:38,880 --> 00:08:47,160
slope of kind of devolving to defining every term in the argument.

103
00:08:47,160 --> 00:08:51,520
But in this case, I wonder the extent to which we're talking about different types of

104
00:08:51,520 --> 00:08:52,520
understanding.

105
00:08:52,520 --> 00:08:56,600
Do you think that that is the case at all here?

106
00:08:56,600 --> 00:09:00,200
Well, I don't know that there are different types, but certainly different definitions

107
00:09:00,200 --> 00:09:01,200
or degrees.

108
00:09:01,200 --> 00:09:04,240
Well, obviously we are arguing about definitions.

109
00:09:04,240 --> 00:09:11,920
And in my blog post, I was supporting the view that instead of arguing about our definitions,

110
00:09:11,920 --> 00:09:16,680
we should be trying, we should be asking ourselves, well, what tests would you give to

111
00:09:16,680 --> 00:09:21,960
a system in order to evaluate whether it's understanding, doing a particular type of

112
00:09:21,960 --> 00:09:22,960
task well?

113
00:09:22,960 --> 00:09:23,960
Right.

114
00:09:23,960 --> 00:09:32,120
If you say, well, the system is narrow, show me all the things that you would like it

115
00:09:32,120 --> 00:09:35,160
to do that it is failing to do now.

116
00:09:35,160 --> 00:09:41,720
And I drew the analogy to test driven development in software engineering, write the tests first,

117
00:09:41,720 --> 00:09:46,240
and then use those to decide how to engineer the system to try to meet those tests.

118
00:09:46,240 --> 00:09:49,280
And then keep writing more tests.

119
00:09:49,280 --> 00:09:54,920
And I think Gary has actually jumped on that.

120
00:09:54,920 --> 00:09:59,800
And on Twitter, he's been asking people, you know, what's wrong with our current natural

121
00:09:59,800 --> 00:10:03,040
language understanding tasks.

122
00:10:03,040 --> 00:10:09,880
And because it seems that we can often get an AI system to do well on a particular benchmark

123
00:10:09,880 --> 00:10:10,880
task.

124
00:10:10,880 --> 00:10:16,720
And yet, again, it turns out that it's very narrow and it's not doing well on any like

125
00:10:16,720 --> 00:10:22,480
immediately adjacent tasks that we would like it to do.

126
00:10:22,480 --> 00:10:25,960
And so some people have been, there's been a bit of a discussion now about that.

127
00:10:25,960 --> 00:10:30,400
And I think that's really where the discussion needs to go is, you know, and Gary himself

128
00:10:30,400 --> 00:10:35,760
in our Twitter conversations, I thought articulated beautifully, said, okay, I want the computer

129
00:10:35,760 --> 00:10:41,920
to be able to say, read a story and tell me the answers to the journalist questions.

130
00:10:41,920 --> 00:10:44,320
Who, when, where, why, how?

131
00:10:44,320 --> 00:10:46,000
So why did this person do this?

132
00:10:46,000 --> 00:10:47,000
What did they do?

133
00:10:47,000 --> 00:10:48,000
When did they do it?

134
00:10:48,000 --> 00:10:51,160
You know, ordered these events for me correctly in time.

135
00:10:51,160 --> 00:10:57,200
And those are way beyond what we can, the state of the art in AI and natural language understanding.

136
00:10:57,200 --> 00:11:00,920
Some of the people in the natural language community said, just stop using the word understanding

137
00:11:00,920 --> 00:11:06,160
at all, we just call it language processing, because we know that, that this word understanding

138
00:11:06,160 --> 00:11:12,280
sets expectations for something that is very broad and deep.

139
00:11:12,280 --> 00:11:17,560
You know, Doug Hofstetter had a very interesting piece that came out last year, where he analyzed

140
00:11:17,560 --> 00:11:24,680
Google Translate and showed how many, many cases where Google translates understandings

141
00:11:24,680 --> 00:11:28,160
clearly extremely surface oriented.

142
00:11:28,160 --> 00:11:34,240
And often it can't understand anything about did, you know, did John do something to Mary,

143
00:11:34,240 --> 00:11:39,440
was Mary doing something to John, it just knows that John and Mary need to both be translated

144
00:11:39,440 --> 00:11:41,720
into the different language.

145
00:11:41,720 --> 00:11:47,320
And it certainly doesn't have any of the, you know, connotations and depth to say that

146
00:11:47,320 --> 00:11:53,200
would be required to translate more poetic language or metaphorical language.

147
00:11:53,200 --> 00:12:02,280
It really has no understanding of human social relationships, what might make Mary angry,

148
00:12:02,280 --> 00:12:08,240
what might make John happy, you know, just, it's completely clueless about that, because

149
00:12:08,240 --> 00:12:13,440
all it has been taught to do is to translate from say Chinese into English or English into

150
00:12:13,440 --> 00:12:20,640
Chinese for fairly straightforward everyday sentences, and certainly not trying to translate

151
00:12:20,640 --> 00:12:23,360
Shakespeare at all.

152
00:12:23,360 --> 00:12:30,320
And one can imagine that it could make very serious mistakes as a result in say highly

153
00:12:30,320 --> 00:12:36,720
emotional and complex social situations.

154
00:12:36,720 --> 00:12:41,800
It's fine for where is the nearest bus stop, but not so good for, you know, why aren't

155
00:12:41,800 --> 00:12:44,480
you talking to me anymore or something.

156
00:12:44,480 --> 00:12:48,840
I do want to encourage us to move beyond saying, well, it either understands or it doesn't,

157
00:12:48,840 --> 00:12:54,120
and this understanding is true or it isn't to say, well, this understanding is incomplete

158
00:12:54,120 --> 00:12:58,200
in these important ways and what we would need these to do.

159
00:12:58,200 --> 00:13:03,960
And so for example, when we think about reading a story or just engaging in a dialogue,

160
00:13:03,960 --> 00:13:08,560
we need AI systems that can be building and maintaining an interpretation of the dialogue.

161
00:13:08,560 --> 00:13:12,000
And this is well known in the natural language community, we just don't know really how

162
00:13:12,000 --> 00:13:13,560
to do it at scale.

163
00:13:13,560 --> 00:13:20,120
We can build applications in a narrow domain, say purchasing airline tickets or something,

164
00:13:20,120 --> 00:13:27,480
where we can cover a lot of different linguistic phenomena and have quite good performance,

165
00:13:27,480 --> 00:13:31,080
but as soon as you step out of that narrow domain, that breaks down.

166
00:13:31,080 --> 00:13:37,480
Yeah, maybe kind of returning to the definition or debate over what understanding a self

167
00:13:37,480 --> 00:13:45,600
means, you refer to Cyril's Chinese room argument and an article by Cole in your blog.

168
00:13:45,600 --> 00:13:48,240
Can you talk a little bit about those?

169
00:13:48,240 --> 00:13:57,840
Well, yes, so there is this famous article in which Cyril put forward the sort of following

170
00:13:57,840 --> 00:14:00,640
thought experiment, right, which was his Chinese room.

171
00:14:00,640 --> 00:14:08,200
So a person is in a room and in this room are, you know, files and files or books and

172
00:14:08,200 --> 00:14:14,600
books full of rules that basically say if you are given these Chinese characters as input,

173
00:14:14,600 --> 00:14:19,840
you should produce these Chinese characters as output and maybe their intermediate rules

174
00:14:19,840 --> 00:14:20,960
and so on.

175
00:14:20,960 --> 00:14:25,080
And there's a human being in there and what that human being is essentially like the central

176
00:14:25,080 --> 00:14:29,640
processing unit of a computer, it takes the input from the outside world that matches

177
00:14:29,640 --> 00:14:34,720
it somehow against all these rules, follows the rules until it produces an output and outputs

178
00:14:34,720 --> 00:14:35,720
it.

179
00:14:35,720 --> 00:14:42,280
And Cyril was arguing that obviously this room could not, that was not truly or generally

180
00:14:42,280 --> 00:14:46,760
understanding Chinese, even though to an outside observer, it looked like it was doing

181
00:14:46,760 --> 00:14:49,360
just fine.

182
00:14:49,360 --> 00:14:56,200
And several people criticize this at the time and it has been a source of lots of interesting

183
00:14:56,200 --> 00:15:06,040
analysis over the years, but one of the main points I would like to link this back to

184
00:15:06,040 --> 00:15:16,320
my previous discussion is that the person inside the box, who he, Cyril asserts is just

185
00:15:16,320 --> 00:15:21,200
an English-only speaker and doesn't understand any Chinese at all, will still not understand

186
00:15:21,200 --> 00:15:24,920
any Chinese even though they are executing this computer program.

187
00:15:24,920 --> 00:15:29,280
And that's what we should expect, I mean, he doesn't make this point, he tries to pull

188
00:15:29,280 --> 00:15:35,720
out the intuition, you know, why doesn't this human understand Chinese and even argues,

189
00:15:35,720 --> 00:15:41,000
well, suppose the human wouldn't actually be using books full of rules but had memorized

190
00:15:41,000 --> 00:15:45,480
all of these rules, but still they would just be executing rules and they wouldn't be understanding

191
00:15:45,480 --> 00:15:46,480
Chinese.

192
00:15:46,480 --> 00:15:52,200
Well, a lot of people then criticize this as saying, well, the intuition is somehow that

193
00:15:52,200 --> 00:15:56,400
I'm looking, if the box is the entire system and I'm looking at some subpart of the box

194
00:15:56,400 --> 00:16:02,840
like the central processing unit of a computer, I'm not going to be able to find a locus of

195
00:16:02,840 --> 00:16:08,440
understanding, looking at any single component inside the box, it's a sort of system-level

196
00:16:08,440 --> 00:16:14,280
property that, given inputs, the box produces appropriate outputs, which would be my sort

197
00:16:14,280 --> 00:16:18,640
of functional definition of understanding.

198
00:16:18,640 --> 00:16:23,800
And I would say that understanding is deeper and broader to the extent that you can ask

199
00:16:23,800 --> 00:16:28,600
it a wider and wider of more challenging and deeper questions and it produces the appropriate

200
00:16:28,600 --> 00:16:31,360
responses to those also.

201
00:16:31,360 --> 00:16:36,120
But we shouldn't expect when we open up, let's say some day we have produced an omni-intelligence

202
00:16:36,120 --> 00:16:40,880
system that is broad and deep intelligence and we open it up, what are we likely to find

203
00:16:40,880 --> 00:16:41,880
inside?

204
00:16:41,880 --> 00:16:46,200
Well, at some level, we might find things we would describe as reasoning and memory and

205
00:16:46,200 --> 00:16:52,440
knowledge and if we go down deeper, we might find pattern matching and probabilistic, you

206
00:16:52,440 --> 00:16:54,400
know, random sampling or something.

207
00:16:54,400 --> 00:16:59,280
If we go down lower, we're just going to find, you know, and in or gates turning on

208
00:16:59,280 --> 00:17:05,680
and off, why should we expect to look inside the box and find intelligence there?

209
00:17:05,680 --> 00:17:10,800
It's going to be the entire system that exhibits this behavior.

210
00:17:10,800 --> 00:17:16,160
Just as with us, when we open up our brains, we just see neurons firing and we keep trying

211
00:17:16,160 --> 00:17:18,920
to find, well, how does that produce intelligent behavior?

212
00:17:18,920 --> 00:17:25,080
Yeah, I think part of this, you know, that most recent argument reminds me of my conversation

213
00:17:25,080 --> 00:17:31,280
with Greg Brockman of OpenAI, where I think towards the end of that conversation, you know,

214
00:17:31,280 --> 00:17:32,600
we pause it.

215
00:17:32,600 --> 00:17:38,080
I forget whether this was, you know, his perspective or mine or, you know, how we arrive at it,

216
00:17:38,080 --> 00:17:44,320
but, you know, one way of kind of projecting what AGI might look like is that it's not

217
00:17:44,320 --> 00:17:50,760
some single universally intelligent system, but something that looks more like an ensemble

218
00:17:50,760 --> 00:17:53,720
of narrower things.

219
00:17:53,720 --> 00:18:00,560
And I think that kind of ties into your functional argument in the sense of, you know, at the

220
00:18:00,560 --> 00:18:06,880
end, the individual subcomponents of this thing won't necessarily have broad understanding,

221
00:18:06,880 --> 00:18:10,320
but they could produce something that, you know, if you look at it, it functionally

222
00:18:10,320 --> 00:18:14,720
has a broader understanding.

223
00:18:14,720 --> 00:18:15,720
Yes.

224
00:18:15,720 --> 00:18:19,720
And of course, this has also been a topic of long discussion in the AI community.

225
00:18:19,720 --> 00:18:25,000
I think researchers and artificial intelligence want to believe that there are some common core

226
00:18:25,000 --> 00:18:30,200
mechanisms that apply across a wide range of intelligent behaviors.

227
00:18:30,200 --> 00:18:37,360
And certainly we, one of the things we have a commitment to or have had since the 50s

228
00:18:37,360 --> 00:18:43,400
in reaction to behaviorism is that there should be some sort of mental models or mental representations

229
00:18:43,400 --> 00:18:45,720
of the world.

230
00:18:45,720 --> 00:18:51,920
And for instance, I should have a representation, someone in my head of you and your goals

231
00:18:51,920 --> 00:18:55,400
and questions in this conversation, what we've already talked about, what we might talk

232
00:18:55,400 --> 00:18:57,000
about, and so on.

233
00:18:57,000 --> 00:19:03,440
And there's been a almost a reflex reaction against the idea that intelligence is just

234
00:19:03,440 --> 00:19:08,600
a big switch that, given the task, I switch on a different expert in my head, and I just

235
00:19:08,600 --> 00:19:10,000
have all these narrow experts.

236
00:19:10,000 --> 00:19:12,600
There should be some shared mechanism.

237
00:19:12,600 --> 00:19:19,560
On the other hand, you know, when you look at, say, Nobel Prize winners in physics, who

238
00:19:19,560 --> 00:19:26,800
decide to start talking about artificial intelligence, and they don't have any deeper insight

239
00:19:26,800 --> 00:19:33,200
than any of the rest of us when they move out of their field of expertise, and this

240
00:19:33,200 --> 00:19:37,760
has been established across many different, you know, fields of expertise is that, you

241
00:19:37,760 --> 00:19:42,600
know, you can be an expert in one thing, but you're basically a novice in other things.

242
00:19:42,600 --> 00:19:47,800
And yet, of course, we do see that some people seem to be faster learners than others.

243
00:19:47,800 --> 00:19:53,720
And so that argues, well, maybe there are some shared mechanisms, and certainly our

244
00:19:53,720 --> 00:19:57,880
perception, as we get older, maybe we're fooling ourselves, is that as we learn more and

245
00:19:57,880 --> 00:20:03,160
more about more things, we're able to go into a new area and learn faster because

246
00:20:03,160 --> 00:20:10,140
we have some frameworks that we can plug new knowledge into that help us learn faster.

247
00:20:10,140 --> 00:20:20,200
So there's definitely this conflict in AI between generality and narrow specificity.

248
00:20:20,200 --> 00:20:25,400
And I don't think it's a big switch that just completely switches from one expert to

249
00:20:25,400 --> 00:20:26,400
another.

250
00:20:26,400 --> 00:20:31,880
But nor do I think that there is some universal module that if we just get that right,

251
00:20:31,880 --> 00:20:38,760
we'll be able to learn everything, because certainly we see in ourselves and in other

252
00:20:38,760 --> 00:20:44,040
people that different people have different strengths and weaknesses in terms of the kinds

253
00:20:44,040 --> 00:20:47,880
of knowledge that they can master, the kinds of skill they can exhibit.

254
00:20:47,880 --> 00:20:52,040
So there's a lot of heterogeneity, and that suggests that there isn't a single thing

255
00:20:52,040 --> 00:20:56,360
like our heart or our lungs that has the same function across everyone.

256
00:20:56,360 --> 00:21:05,760
There's certainly elements of your argument that resonate with me very strongly, the idea

257
00:21:05,760 --> 00:21:10,360
that understanding isn't binary just makes sense to me.

258
00:21:10,360 --> 00:21:16,880
At the same time, when I think about what Gary is reacting to and trying to speak out

259
00:21:16,880 --> 00:21:27,160
against, it's this hype engine that wants to portray with intent or not.

260
00:21:27,160 --> 00:21:33,800
Maybe it's not the initiators of the news that want to intent, but once you put it through

261
00:21:33,800 --> 00:21:43,600
the media filter starts to portray this idea that we're at already systems that exhibit

262
00:21:43,600 --> 00:21:52,400
some kind of intelligence or that we should be worried about, or that have understanding.

263
00:21:52,400 --> 00:21:59,240
I think that's really what he's trying to put a damper on for the benefit of the industry

264
00:21:59,240 --> 00:22:07,840
as a whole, and certainly when I talk to laypeople about what's happening in artificial intelligence

265
00:22:07,840 --> 00:22:16,720
and what a theory really is, what I most immediately refer to as a misconception that these

266
00:22:16,720 --> 00:22:26,280
systems do have understanding our intelligent, and my immediate reaction is to try to contextualize

267
00:22:26,280 --> 00:22:33,600
that, and most often I'm saying, no, they're not really, they don't really understand.

268
00:22:33,600 --> 00:22:42,680
It seems like the right counterbalance to what the hype creates or just a lack of understanding

269
00:22:42,680 --> 00:22:47,960
of what's really happening in these systems conveys.

270
00:22:47,960 --> 00:22:54,040
It seems like the right way to kind of guide their understanding of the degree of understanding

271
00:22:54,040 --> 00:23:00,280
of modern AI, so why do you kind of object to that?

272
00:23:00,280 --> 00:23:07,200
Well, I don't object to that, and I am 100% with Gary on the trying to dampen down the

273
00:23:07,200 --> 00:23:08,200
hype.

274
00:23:08,200 --> 00:23:12,200
I think it creates those misconceptions that you're talking about.

275
00:23:12,200 --> 00:23:19,920
I think it's leading investors to put money behind things that are not going to work out.

276
00:23:19,920 --> 00:23:26,960
I think that it's also an issue of intellectual honesty in the computer science field that

277
00:23:26,960 --> 00:23:32,120
as we do research on these things, we need to point out not only the new capabilities

278
00:23:32,120 --> 00:23:37,480
that we can create, but also their weaknesses.

279
00:23:37,480 --> 00:23:44,200
I totally agree with Gary, and I think Gary would make another point in his argument against

280
00:23:44,200 --> 00:23:48,280
a sort of all deep learning approach.

281
00:23:48,280 --> 00:23:53,280
He's making essentially the ladder to the moon argument, which you may have heard before,

282
00:23:53,280 --> 00:23:59,520
which is that if our goal is to get to the moon, and we can demonstrate that we went

283
00:23:59,520 --> 00:24:03,560
from six-foot tall ladders to ten-foot tall ladders to fifty-foot tall ladders, and

284
00:24:03,560 --> 00:24:08,360
we say, look, we're making progress and extrapolate from our ladder technology to claim that

285
00:24:08,360 --> 00:24:12,640
it's going to get us to the moon, and it just isn't.

286
00:24:12,640 --> 00:24:22,080
We have to, of course, as researchers and engineers, we place our bets on certain technologies,

287
00:24:22,080 --> 00:24:25,920
and we want to see how far we can push them.

288
00:24:25,920 --> 00:24:31,440
In the past, people have placed bets on explicit knowledge representation in reasoning systems

289
00:24:31,440 --> 00:24:36,800
where they're hand-coding as much of human knowledge as they can, and now deep learning

290
00:24:36,800 --> 00:24:44,800
is placing its bets on deep neural networks and, more generally, undifferentiable programming.

291
00:24:44,800 --> 00:24:49,480
This certainly gives you a way of writing programs that you can now train in an end-to-end

292
00:24:49,480 --> 00:24:53,720
fashion, or you can train individual modules, and then glue them together with end-to-end

293
00:24:53,720 --> 00:25:04,240
fine-tuning, and there's a feeling among the connectionist or deep learning community

294
00:25:04,240 --> 00:25:09,960
that there's still a lot of headroom to go in the things that we could do with this technology.

295
00:25:09,960 --> 00:25:15,040
So people are putting in memory, and they're connecting this technology to external knowledge

296
00:25:15,040 --> 00:25:21,680
sources. They're doing the meta-learning, which allows them a system that's been trained

297
00:25:21,680 --> 00:25:27,400
on some initial tasks to also then very rapidly learn new tasks and transfer their knowledge

298
00:25:27,400 --> 00:25:34,600
from one to another, and so partly the people inside the deep learning community feel

299
00:25:34,600 --> 00:25:41,080
like this is still a productive research paradigm or research program, which is another

300
00:25:41,080 --> 00:25:48,360
another thing I mentioned in my blog post is this idea that if we think about the development

301
00:25:48,360 --> 00:25:53,960
of science in terms of Thomas Coons analysis in terms of paradigms, or this analysis in

302
00:25:53,960 --> 00:26:02,960
terms of research programs, that these programs continue until they cease to be productive

303
00:26:02,960 --> 00:26:08,520
and fruitful, and Gary, on the one hand, is arguing, well, there are all these things

304
00:26:08,520 --> 00:26:13,040
that deep learning systems, today's deep learning systems cannot do, and that he doesn't

305
00:26:13,040 --> 00:26:15,880
see how they're ever going to do.

306
00:26:15,880 --> 00:26:21,480
So he's arguing, well, we know we have these other systems, these symbolic reasoning systems

307
00:26:21,480 --> 00:26:26,120
that can do some of these other things, so the obvious path forward is to build hybrid

308
00:26:26,120 --> 00:26:31,240
systems that combine both symbolic and connectionist components.

309
00:26:31,240 --> 00:26:36,240
But I would say that the connectionist deep learning reply to that is, well, we're going

310
00:26:36,240 --> 00:26:43,120
to keep pushing, because we see that our methods are still fruitful in giving us new capabilities,

311
00:26:43,120 --> 00:26:47,200
we're going to keep pushing on them, and we want to maintain an open mind.

312
00:26:47,200 --> 00:26:53,480
I mean, maybe we can't reach the moon with only connectionist ladders, but maybe we can

313
00:26:53,480 --> 00:26:58,400
build rocket ships out of connectionist material, and maybe we can get there.

314
00:26:58,400 --> 00:27:04,040
And so, of course, the connections have from, for decades, criticized symbolic systems

315
00:27:04,040 --> 00:27:11,200
for their inability to capture nuances and similarities, and because they're very symbolic

316
00:27:11,200 --> 00:27:18,960
and Boolean, and I don't think we have a good, that the symbolic community has a good

317
00:27:18,960 --> 00:27:21,400
response to that criticism.

318
00:27:21,400 --> 00:27:26,360
So both methods have their weaknesses, and maybe, I mean, if I were building a system

319
00:27:26,360 --> 00:27:29,520
today, I would build a hybrid system.

320
00:27:29,520 --> 00:27:33,400
And certainly, if you look at the successful AI systems out there, like let's look at

321
00:27:33,400 --> 00:27:41,840
Google search or Bing, these systems are hybrids of deep learning and symbolic reasoning

322
00:27:41,840 --> 00:27:43,160
systems.

323
00:27:43,160 --> 00:27:49,080
They are also collections of experts, so that an incoming question is routed to multiple

324
00:27:49,080 --> 00:27:53,720
subquery engines that say, well, is this a question about stock prices?

325
00:27:53,720 --> 00:27:56,080
Is this a question about a product for sale?

326
00:27:56,080 --> 00:27:59,120
Is this a question about geography, right, and so on?

327
00:27:59,120 --> 00:28:04,400
The candidate answers bubble up and are ranked and assessed, and then one or more of them

328
00:28:04,400 --> 00:28:06,600
are displayed to the user.

329
00:28:06,600 --> 00:28:10,360
And so these are certainly, if you look at what is the state of the art in terms of actual

330
00:28:10,360 --> 00:28:13,640
practical systems, they are all hybrid systems.

331
00:28:13,640 --> 00:28:23,920
It strikes me that the critique of deep learning isn't going to get us to AGI is similar

332
00:28:23,920 --> 00:28:32,840
to, or there's maybe an adjacent critique that kind of touches on your own area of research

333
00:28:32,840 --> 00:28:39,560
into robust and safe AI that kind of says that, you know, a lot of the research in that

334
00:28:39,560 --> 00:28:47,120
area kind of presupposes some super intelligence, ALA, Nick Bostrom.

335
00:28:47,120 --> 00:28:48,240
And we're nowhere near there.

336
00:28:48,240 --> 00:28:51,000
We have no idea how we're going to get there.

337
00:28:51,000 --> 00:28:59,400
And therefore, how useful really is that whole line of reasoning is kind of that the parallel

338
00:28:59,400 --> 00:29:04,600
critique there, part of what you're responding to or in what ways do you see that playing

339
00:29:04,600 --> 00:29:05,600
out?

340
00:29:05,600 --> 00:29:11,880
Well, so my interest in robust AI is much more immediate than Bostrom's paperclip maker

341
00:29:11,880 --> 00:29:15,080
or any of those things.

342
00:29:15,080 --> 00:29:16,080
Okay.

343
00:29:16,080 --> 00:29:18,680
I think those are all fantasies, basically.

344
00:29:18,680 --> 00:29:25,520
But we have very, very practical issues confronting us right now, because all of the AI systems

345
00:29:25,520 --> 00:29:30,840
that we build today are limited to fairly closed worlds.

346
00:29:30,840 --> 00:29:36,840
I mean, I guess I'd have to say the Google search engine is much more open, but it has

347
00:29:36,840 --> 00:29:43,400
a fallback, which is to just go and do a search of the web and try to find matching documents.

348
00:29:43,400 --> 00:29:47,440
So if it can't understand something, that's kind of what it falls back on, just like Siri

349
00:29:47,440 --> 00:29:48,440
does.

350
00:29:48,440 --> 00:29:55,320
But when we think about, say, a self-driving car, and we train it to recognize basketballs

351
00:29:55,320 --> 00:30:00,600
bouncing across the street, children on bicycles and tricycles and dogs and deer and what

352
00:30:00,600 --> 00:30:05,280
have you, but there's always the possibility that something new, there'll be some new kind

353
00:30:05,280 --> 00:30:08,360
of obstacle that the car hadn't seen before.

354
00:30:08,360 --> 00:30:13,440
And so I used to use the made up example, well, suppose we train the system in North America,

355
00:30:13,440 --> 00:30:17,760
but then we deployed in Australia, what does it do the first time it sees a kangaroo?

356
00:30:17,760 --> 00:30:24,560
It turned out that was actually a real case, and the kangaroos were confusing some of

357
00:30:24,560 --> 00:30:30,640
the self-driving car systems that had been engineered in Europe, and then were being

358
00:30:30,640 --> 00:30:32,840
tested in Australia.

359
00:30:32,840 --> 00:30:38,080
So this is known as kind of the open world problem or the open category problem.

360
00:30:38,080 --> 00:30:41,120
There's some new kinds of objects out there.

361
00:30:41,120 --> 00:30:47,240
And our machine learning systems, say for supervised learning, are trained on some fixed

362
00:30:47,240 --> 00:30:48,520
set of categories.

363
00:30:48,520 --> 00:30:56,680
So the most famous benchmark, which is ImageNet, has 1,000 categories of objects, and it's

364
00:30:56,680 --> 00:31:00,360
trained to discriminate among those 1,000 categories.

365
00:31:00,360 --> 00:31:05,120
So it basically assumes every new image it sees contains one of those, objects from

366
00:31:05,120 --> 00:31:07,320
one of those 1,000 categories.

367
00:31:07,320 --> 00:31:11,320
And so if there's something new there, it will just classify it as one of the thousand

368
00:31:11,320 --> 00:31:13,400
things it knows about.

369
00:31:13,400 --> 00:31:19,240
So if it hadn't been trained on kangaroos, maybe it classifies it as a rabbit or something,

370
00:31:19,240 --> 00:31:23,160
maybe it's confused about the scale, I don't know, or a deer, who knows what kind of mistake

371
00:31:23,160 --> 00:31:28,320
it might make, maybe because of the way it moves it classifies it as a paper bag blowing

372
00:31:28,320 --> 00:31:30,720
across the road.

373
00:31:30,720 --> 00:31:38,040
And in that case, the automatic car might make a mistake and not break to stop for it.

374
00:31:38,040 --> 00:31:44,320
So I'm interested in this question of can we build systems that can work in open environments?

375
00:31:44,320 --> 00:31:48,880
Can we get probability estimates, confidence estimates out of the system that we can

376
00:31:48,880 --> 00:31:51,240
trust in open worlds?

377
00:31:51,240 --> 00:31:52,400
And in close worlds too.

378
00:31:52,400 --> 00:31:58,000
So if we think about, there's been a lot of discussion of face recognition and these

379
00:31:58,000 --> 00:32:04,480
face recognition systems, and it's well established that they are not equally accurate on all

380
00:32:04,480 --> 00:32:10,120
people, right, and particularly black people and darker skinned people, the systems are

381
00:32:10,120 --> 00:32:17,640
not nearly as accurate on, and particularly black women are very inaccurate on these, and

382
00:32:17,640 --> 00:32:23,120
partly that is because the images of black people are underrepresented in the training

383
00:32:23,120 --> 00:32:27,280
data, but they are also just a minority of the population.

384
00:32:27,280 --> 00:32:33,600
So if you belong to a subpopulation that is a tiny minority, machine learning systems

385
00:32:33,600 --> 00:32:40,120
tend to go for the common case, and they tend to be less accurate on the edge cases.

386
00:32:40,120 --> 00:32:42,680
You know, you can say, well, I'm 98% accurate.

387
00:32:42,680 --> 00:32:46,960
It's just that every one of my mistakes turns out to be a black woman.

388
00:32:46,960 --> 00:32:50,280
And so if you're a black woman, you're really going to have problems with these face recognition

389
00:32:50,280 --> 00:32:52,240
systems.

390
00:32:52,240 --> 00:32:58,320
And you know, this has been really called to attention, I think, really nicely, by a

391
00:32:58,320 --> 00:33:08,160
joy-blown lenient MIT and Timnick Gibru, who I think is still with Google, and their collaborators.

392
00:33:08,160 --> 00:33:16,680
And I want these systems to be able to give confidence scores that say, well, when I see

393
00:33:16,680 --> 00:33:22,200
an image of a black woman, I have a confidence score that is very low, so that we know not

394
00:33:22,200 --> 00:33:25,880
to trust these systems in these situations.

395
00:33:25,880 --> 00:33:32,960
Even for the self-driving car, same for, say, using computer vision in medicine, in reading

396
00:33:32,960 --> 00:33:40,200
X-rays and so on, we need systems that can give us confidence numbers that we can trust.

397
00:33:40,200 --> 00:33:45,720
The computer vision system that Amazon sells for face recognition gives confidence numbers,

398
00:33:45,720 --> 00:33:49,680
but they don't tell you what they mean.

399
00:33:49,680 --> 00:33:56,000
And in any case, you would need to calibrate your confidence numbers to the data you're

400
00:33:56,000 --> 00:34:00,280
using the model on, which is different from the data it was trained on.

401
00:34:00,280 --> 00:34:02,720
And so that's one of the things I'm studying in my work.

402
00:34:02,720 --> 00:34:10,000
Yeah, I think at the end of the day, it strikes me that, you know, what Gary's saying is,

403
00:34:10,000 --> 00:34:16,240
hey, you know, we're calling these systems intelligent, talking about their understanding,

404
00:34:16,240 --> 00:34:22,440
but at AGI, deep learning, you know, almost certainly isn't going to get us to AGI by

405
00:34:22,440 --> 00:34:29,240
itself, which is kind of the benchmark of, quote unquote, you know, capital I intelligence

406
00:34:29,240 --> 00:34:35,920
capital you understanding at the same time, what you're saying is, hey, let's not throw

407
00:34:35,920 --> 00:34:42,800
the baby out with the bathwater, there's still significant value in deep learning as well.

408
00:34:42,800 --> 00:34:48,880
And also a lot of room for additional research and exploration and uncovering future value

409
00:34:48,880 --> 00:34:50,280
there.

410
00:34:50,280 --> 00:34:56,400
And it, you know, it sounds like both of these are correct, both of these are great and

411
00:34:56,400 --> 00:34:59,080
right perspectives to take.

412
00:34:59,080 --> 00:35:00,080
Do you agree with that?

413
00:35:00,080 --> 00:35:08,000
Well, I mean, I'm not willing to say, oh, well, you know, deep learning style technology

414
00:35:08,000 --> 00:35:11,880
can't get us to say broadly intelligent systems.

415
00:35:11,880 --> 00:35:15,200
I don't like them from AGI because it's an undefined term.

416
00:35:15,200 --> 00:35:25,960
But so I think we should continue to push in that direction with our deep learning systems.

417
00:35:25,960 --> 00:35:31,160
But we obviously should not be declaring victory prematurely.

418
00:35:31,160 --> 00:35:37,240
And we do get a lot of press releases out of companies and out of academic labs that

419
00:35:37,240 --> 00:35:44,560
exaggerate the significance of the work and, and, you know, maybe they don't explicitly

420
00:35:44,560 --> 00:35:52,280
say it, but they are open to the misinterpretation that, that, that, you know, broad, I'm the intelligence

421
00:35:52,280 --> 00:35:57,080
systems are right around the corner when, when, in fact, of course, we are nowhere near

422
00:35:57,080 --> 00:35:58,080
having those.

423
00:35:58,080 --> 00:36:06,440
Yeah, and I should be clear, Gary's perspective is that deep learning as a style of computation

424
00:36:06,440 --> 00:36:07,440
won't get us there.

425
00:36:07,440 --> 00:36:14,520
And it needs to be supplemented by symbolic computing and, or other techniques.

426
00:36:14,520 --> 00:36:22,080
My personal view is that, you know, there's some discontinuous innovation that's going

427
00:36:22,080 --> 00:36:25,040
to be required to get us there.

428
00:36:25,040 --> 00:36:27,960
Deep learning may be a big part of it, but there's something else.

429
00:36:27,960 --> 00:36:30,120
And we don't really know what that something else is.

430
00:36:30,120 --> 00:36:36,000
It may be some other style of compute that, you know, quantum computing, you know, heaven

431
00:36:36,000 --> 00:36:43,680
forbid or something else that allows us to, you know, to, to far surpass where we are

432
00:36:43,680 --> 00:36:46,000
today.

433
00:36:46,000 --> 00:36:49,600
But I don't think we know what that is today.

434
00:36:49,600 --> 00:36:55,280
And, you know, it's certainly, I think it's a very safe bet that we're going to need

435
00:36:55,280 --> 00:36:58,280
more innovation, discontinuous innovation, as you say.

436
00:36:58,280 --> 00:36:59,280
Exactly.

437
00:36:59,280 --> 00:37:04,520
And then, you know, but also there, there's, you know, it's very clear that we're just

438
00:37:04,520 --> 00:37:14,600
at the beginning of unlocking the value that deep learning offers to society and that,

439
00:37:14,600 --> 00:37:17,520
you know, there's more work to be done and that we should be doing that work.

440
00:37:17,520 --> 00:37:18,520
Yes.

441
00:37:18,520 --> 00:37:24,920
I think we could continue this discussion for quite some time, but I think we've covered

442
00:37:24,920 --> 00:37:30,760
a lot of good ground and it was great checking with you on your perspective on this.

443
00:37:30,760 --> 00:37:34,200
And I really appreciated the blog post, Tom.

444
00:37:34,200 --> 00:37:35,960
Well, thank you very much.

445
00:37:35,960 --> 00:37:41,000
It's always fun to talk about these, these challenging questions in artificial intelligence.

446
00:37:41,000 --> 00:37:42,000
Awesome.

447
00:37:42,000 --> 00:37:45,840
Thanks so much.

448
00:37:45,840 --> 00:37:47,960
That's our show for today.

449
00:37:47,960 --> 00:37:53,480
To learn more about today's show, visit twomolei.com slash shows.

450
00:37:53,480 --> 00:37:58,360
Once again, if you missed twomolei or want to share what you learned with your team, be

451
00:37:58,360 --> 00:38:04,280
sure to visit twomolei.com slash videos for more information about twomolei.com video

452
00:38:04,280 --> 00:38:05,880
packages.

453
00:38:05,880 --> 00:38:32,600
Thanks so much for listening and we'll see you next week.

