1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,560
I'm your host Sam Charrington.

4
00:00:23,560 --> 00:00:28,360
A big thanks to everyone who participated in last week's Twimble Online Meetup and

5
00:00:28,360 --> 00:00:31,160
to Kevin T from SIGUP for presenting.

6
00:00:31,160 --> 00:00:35,400
You can find the slides for his presentation in the Meetup Slack channel as well as in

7
00:00:35,400 --> 00:00:37,320
this week's show notes.

8
00:00:37,320 --> 00:00:41,680
Our final Meetup of the Year will be held on Wednesday, December 13th.

9
00:00:41,680 --> 00:00:46,880
Make sure to bring your thoughts on the top machine learning and AI stories for 2017

10
00:00:46,880 --> 00:00:49,040
for our discussion segment.

11
00:00:49,040 --> 00:00:54,680
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing

12
00:00:54,680 --> 00:01:01,160
the paper understanding deep learning requires rethinking generalization by Shi Huang Zhang

13
00:01:01,160 --> 00:01:04,560
from MIT and Google Brain and others.

14
00:01:04,560 --> 00:01:09,680
You can find more details and register at twimbleia.com slash Meetup.

15
00:01:09,680 --> 00:01:14,000
If you receive my newsletter, you already know this, but Twimble is growing and we're

16
00:01:14,000 --> 00:01:19,720
looking for an energetic and passionate community manager to help expand our programs.

17
00:01:19,720 --> 00:01:24,120
This position can be remote, but if you happen to be in St. Louis, all the better.

18
00:01:24,120 --> 00:01:27,840
If you're interested, please reach out to me for additional details.

19
00:01:27,840 --> 00:01:31,800
I should mention that if you don't already get my newsletter, you are really missing

20
00:01:31,800 --> 00:01:37,040
out and should visit twimbleia.com slash newsletter to sign up.

21
00:01:37,040 --> 00:01:43,120
Now the show you are about to hear is part of our Strange Loop 2017 series brought to you

22
00:01:43,120 --> 00:01:46,120
by our friends at Nexusos.

23
00:01:46,120 --> 00:01:50,520
Nexusos is a company focused on making machine learning more easily accessible to enterprise

24
00:01:50,520 --> 00:01:51,520
developers.

25
00:01:51,520 --> 00:01:56,040
Their machine learning API meets developers where they're at, regardless of their mastery

26
00:01:56,040 --> 00:02:01,280
of data science, so they can start cutting up predictive applications immediately and

27
00:02:01,280 --> 00:02:04,120
in their preferred programming language.

28
00:02:04,120 --> 00:02:08,480
It's as simple as loading your data and selecting the type of problem you want to solve.

29
00:02:08,480 --> 00:02:13,360
Their automated platform trains and selects the best model fit for your data and then outputs

30
00:02:13,360 --> 00:02:14,920
predictions.

31
00:02:14,920 --> 00:02:19,280
To learn more about Nexusos, be sure to check out the first episode in this series at

32
00:02:19,280 --> 00:02:27,520
twimbleia.com slash talk slash 69 where I speak with co-founders Ryan Sevy and Jason Montgomery.

33
00:02:27,520 --> 00:02:32,960
Be sure to also get your free Nexusos API key and discover how to start leveraging machine

34
00:02:32,960 --> 00:02:38,200
learning in your next project at nexosos.com slash twimble.

35
00:02:38,200 --> 00:02:43,600
In this episode, I speak with Matthew Taylor, open source manager at Numenta.

36
00:02:43,600 --> 00:02:47,840
You might remember hearing a bit about Numenta from an interview I did with Francisco Weber

37
00:02:47,840 --> 00:02:54,440
of cortical.io on twimble talk number 10, a show which remains the most popular show of

38
00:02:54,440 --> 00:02:56,720
the podcast to date.

39
00:02:56,720 --> 00:03:02,000
Numenta is basically trying to reverse engineer the neocortex and use what they learn to

40
00:03:02,000 --> 00:03:08,480
develop a neocortical theory for biological and machine intelligence that they call hierarchical

41
00:03:08,480 --> 00:03:10,120
temporal memory.

42
00:03:10,120 --> 00:03:14,640
Met joined me at the conference to discuss his talk, the biological path towards strong

43
00:03:14,640 --> 00:03:16,160
AI.

44
00:03:16,160 --> 00:03:21,960
In our conversation, we discussed the basics of htm, its biological inspiration and how

45
00:03:21,960 --> 00:03:26,160
it differs from traditional neural networks, including deep learning.

46
00:03:26,160 --> 00:03:30,760
This is a nerd alert show and after you listen, I would also encourage you to check out the

47
00:03:30,760 --> 00:03:35,320
conversation with Francisco, which we'll link to in the show notes.

48
00:03:35,320 --> 00:03:44,800
And now on to the show.

49
00:03:44,800 --> 00:03:49,600
Hi everyone, I am here at the strange loop conference in St. Louis.

50
00:03:49,600 --> 00:03:55,320
And I am joined by Matt Taylor, who is an open source community manager at Numenta.

51
00:03:55,320 --> 00:03:58,480
And I am super excited to have you here with me, Matt.

52
00:03:58,480 --> 00:04:04,560
You just delivered a talk here at the conference and I'm looking forward to us diving into that.

53
00:04:04,560 --> 00:04:06,560
But before we go anywhere else, welcome.

54
00:04:06,560 --> 00:04:07,560
Thank you.

55
00:04:07,560 --> 00:04:08,560
It's a pleasure to be here.

56
00:04:08,560 --> 00:04:10,000
Pleasure to have you on the show.

57
00:04:10,000 --> 00:04:13,720
So why don't we get started by having you tell us a little bit about your background and

58
00:04:13,720 --> 00:04:16,720
how you got into machine learning and AI?

59
00:04:16,720 --> 00:04:19,720
Yeah, I don't know how far back to go.

60
00:04:19,720 --> 00:04:23,720
But I mean, in computers, I got interested in computers when I was enlisted in Air Force.

61
00:04:23,720 --> 00:04:26,960
I was an intelligence analyst in the Air Force.

62
00:04:26,960 --> 00:04:31,320
And then that turned into a department of defense job in the same place and I was doing a lot

63
00:04:31,320 --> 00:04:38,840
of simulation, like air defense simulations in like Fortran and Shell scripts.

64
00:04:38,840 --> 00:04:41,760
It was kind of our cake, but it was a very powerful simulation.

65
00:04:41,760 --> 00:04:43,680
So that's sort of what got me into programming.

66
00:04:43,680 --> 00:04:49,000
I didn't really think much about artificial intelligence until I read on intelligence,

67
00:04:49,000 --> 00:04:54,000
which is a book that our founder Jeff Hawkins wrote, I think in 2005.

68
00:04:54,000 --> 00:04:56,600
And I was working in the software industry at that point.

69
00:04:56,600 --> 00:05:03,040
I got away from, you know, old defense industry and moved here to St. Louis to work in software

70
00:05:03,040 --> 00:05:05,840
after I got my software degree.

71
00:05:05,840 --> 00:05:06,840
Oh, wow.

72
00:05:06,840 --> 00:05:10,960
And so I would just consult it around St. Louis, worked at a bunch of different places and

73
00:05:10,960 --> 00:05:12,560
did a bunch of different jobs.

74
00:05:12,560 --> 00:05:13,560
And I read that book.

75
00:05:13,560 --> 00:05:17,880
I remember reading it on intelligence and another book called The Singularity is Near by

76
00:05:17,880 --> 00:05:18,880
Ray Kurzweil.

77
00:05:18,880 --> 00:05:19,880
Sure.

78
00:05:19,880 --> 00:05:23,440
And reading those two books at the same time really like flipped the script for me.

79
00:05:23,440 --> 00:05:29,320
And I was like, it made me start wondering all these big questions like, what is consciousness?

80
00:05:29,320 --> 00:05:30,320
What is intelligence?

81
00:05:30,320 --> 00:05:31,920
How do we even define these things?

82
00:05:31,920 --> 00:05:37,520
Is it really possible that we could build intelligence systems out of non-biological materials?

83
00:05:37,520 --> 00:05:42,640
But at the time, you know, I was just working here doing mundane software programming and

84
00:05:42,640 --> 00:05:43,640
stuff.

85
00:05:43,640 --> 00:05:50,640
But I don't have a math degree, I didn't have an experience in deep learning or artificial

86
00:05:50,640 --> 00:05:51,640
neural networks.

87
00:05:51,640 --> 00:05:55,560
Even at that point deep learning wasn't even a big deal.

88
00:05:55,560 --> 00:05:58,240
So I just gave it up as a pipe dream.

89
00:05:58,240 --> 00:06:04,040
But at some point, I got a job at Yahoo as a front-end engineer, which is odd because

90
00:06:04,040 --> 00:06:08,840
I'd never done front-end engineers before, but I got a job at Yahoo and moved out to the

91
00:06:08,840 --> 00:06:09,840
Bay Area.

92
00:06:09,840 --> 00:06:13,760
Worked there for a couple years and out of the blue, I got a call from a recruiter for

93
00:06:13,760 --> 00:06:16,400
a front-end physician at Nementa.

94
00:06:16,400 --> 00:06:18,600
And I was like, wow, what?

95
00:06:18,600 --> 00:06:19,600
OK, sure.

96
00:06:19,600 --> 00:06:20,600
Wow.

97
00:06:20,600 --> 00:06:24,720
So I jumped on board at that point and just started doing web stuff.

98
00:06:24,720 --> 00:06:27,120
Eventually moved up to do web services.

99
00:06:27,120 --> 00:06:29,160
It was the manager web services.

100
00:06:29,160 --> 00:06:34,680
And when my boss, Jeff, decided he wanted to take all of their algorithms, open source,

101
00:06:34,680 --> 00:06:37,400
I was like, I want to help with that.

102
00:06:37,400 --> 00:06:38,800
Because I like open source.

103
00:06:38,800 --> 00:06:43,000
I've always been an advocate of open source and been a part of different communities.

104
00:06:43,000 --> 00:06:46,760
And I was like, sign me up, so I did.

105
00:06:46,760 --> 00:06:48,120
That's fantastic.

106
00:06:48,120 --> 00:06:54,200
So maybe for folks that aren't familiar with Nementa, you can kind of walk us through

107
00:06:54,200 --> 00:06:58,480
the company and its position in the machine learning space.

108
00:06:58,480 --> 00:07:03,640
I think the company has kind of a unique approach to machine learning.

109
00:07:03,640 --> 00:07:09,000
And folks that have been around with the podcast for a while and listened to Francisco

110
00:07:09,000 --> 00:07:14,840
Weber's podcast might recall Nementa and Jeff Hawkins' work coming up in that context

111
00:07:14,840 --> 00:07:21,160
because the work that cortical is doing is related to what Nementa is doing.

112
00:07:21,160 --> 00:07:25,320
Yeah, that podcast, I think, was a great primer for us for Nementa.

113
00:07:25,320 --> 00:07:30,680
It was a partner of ours, and Francisco is a brilliant guy, you know, is exactly what

114
00:07:30,680 --> 00:07:32,320
he's talking about.

115
00:07:32,320 --> 00:07:37,280
So our mission at Nementa is different than I think most companies, and it's always been

116
00:07:37,280 --> 00:07:39,200
this ever since that I've been at the company.

117
00:07:39,200 --> 00:07:41,000
It's two things.

118
00:07:41,000 --> 00:07:45,320
Understand how intelligence works in the Neo Cortex, and the second thing is implement

119
00:07:45,320 --> 00:07:49,720
those things outside of biological systems, like try and build it, because basically

120
00:07:49,720 --> 00:07:53,720
reverse engineer the Neo Cortex as our mission.

121
00:07:53,720 --> 00:07:56,320
And hopefully, you know, we'll make money out of that at some point.

122
00:07:56,320 --> 00:08:00,120
But honestly, we're like really kind of R&D focused right now.

123
00:08:00,120 --> 00:08:01,120
Yeah.

124
00:08:01,120 --> 00:08:05,240
Very small company, very focused on the research.

125
00:08:05,240 --> 00:08:08,720
And is it primarily just funded by Jeff?

126
00:08:08,720 --> 00:08:11,720
Yeah, it's privately funded by Jeff Donah.

127
00:08:11,720 --> 00:08:12,720
Yeah, yeah.

128
00:08:12,720 --> 00:08:17,200
A group of contributors that have been long time associates of Jeff and Donah, you know,

129
00:08:17,200 --> 00:08:22,640
they built Palm and Handspring, and so there's a crew of board members that I think help

130
00:08:22,640 --> 00:08:25,080
with the funding, but I don't know the details of all that.

131
00:08:25,080 --> 00:08:30,760
And is the implication of that mission, though, that the company is not under your traditional

132
00:08:30,760 --> 00:08:33,560
kind of venture commercialization pressures?

133
00:08:33,560 --> 00:08:39,200
Is it better to think of Nementa as like an open AI than like a, you know, machine learning

134
00:08:39,200 --> 00:08:40,200
company X?

135
00:08:40,200 --> 00:08:41,200
I guess.

136
00:08:41,200 --> 00:08:44,480
I've never thought of it in comparison to open AI, but I guess it would be similar in

137
00:08:44,480 --> 00:08:46,200
that we're not building products.

138
00:08:46,200 --> 00:08:48,840
You know, we're not selling services.

139
00:08:48,840 --> 00:08:51,880
What we're doing is we're trying to make discoveries.

140
00:08:51,880 --> 00:08:58,680
So all of our discoveries are based on neuroscience research, you know, our research engineers are

141
00:08:58,680 --> 00:09:03,640
always reading the most recent neuroscience papers that come out there, they're interacting

142
00:09:03,640 --> 00:09:08,440
with different neuroscientists in the community, trying to answer questions that are relevant

143
00:09:08,440 --> 00:09:11,680
to how we understand intelligence in the context.

144
00:09:11,680 --> 00:09:16,160
And what we do is as we make these discoveries and we test them out, you know, we'll prototype

145
00:09:16,160 --> 00:09:18,320
them and software and think, oh, this is how it works.

146
00:09:18,320 --> 00:09:23,000
It actually does, our theory seems to work in software the way we thought.

147
00:09:23,000 --> 00:09:27,440
Then we will create patents around those discoveries.

148
00:09:27,440 --> 00:09:31,240
So there, you know, specific ones about things that we've discovered about how the brain

149
00:09:31,240 --> 00:09:36,600
is working and how we've implemented it and currently in software, but it could be implemented

150
00:09:36,600 --> 00:09:37,600
in hardware too.

151
00:09:37,600 --> 00:09:38,600
Okay.

152
00:09:38,600 --> 00:09:43,000
The idea being, you know, the monetization strategy is in the value of the IP itself.

153
00:09:43,000 --> 00:09:49,040
So we don't want to be distracted by consulting, by providing services or by creating applications

154
00:09:49,040 --> 00:09:50,040
at this point.

155
00:09:50,040 --> 00:09:54,600
We really want to focus on the discovery, on the brain, trying to figure out how it works

156
00:09:54,600 --> 00:09:57,560
and we think that good things will come of that.

157
00:09:57,560 --> 00:09:58,560
Okay.

158
00:09:58,560 --> 00:10:06,680
So for your talk, one of the big things that I think you talked about at least from the

159
00:10:06,680 --> 00:10:11,480
perspective, from what I got out of the abstract was you kind of premised it on, you know,

160
00:10:11,480 --> 00:10:15,720
hey, there's a lot of excitement out there about neural nets and deep learning and things

161
00:10:15,720 --> 00:10:20,760
like that, but these are all based on a model of a neuron that is, you know, rather

162
00:10:20,760 --> 00:10:21,760
dated.

163
00:10:21,760 --> 00:10:22,760
Right.

164
00:10:22,760 --> 00:10:27,000
And I presume you then walk through some of the new things that we've learned since then.

165
00:10:27,000 --> 00:10:28,000
Yeah.

166
00:10:28,000 --> 00:10:32,560
You kind of walk us through your talk and the ideas that you wanted to share with folks.

167
00:10:32,560 --> 00:10:33,560
Sure.

168
00:10:33,560 --> 00:10:37,720
So I'd like to say, first off, that I don't have anything bad to say about artificial

169
00:10:37,720 --> 00:10:39,400
neural networks or deep learning.

170
00:10:39,400 --> 00:10:44,080
I sure that's necessary technology that we needed to build in.

171
00:10:44,080 --> 00:10:49,840
But one of my main points is that it's not going to naturally evolve into what people

172
00:10:49,840 --> 00:10:51,400
call strong AI.

173
00:10:51,400 --> 00:10:56,960
And the first thing I say in my talk is weak AI is not intelligent and won't become intelligent.

174
00:10:56,960 --> 00:11:03,560
There's not going to be some, this like exponential growth and suddenly, you know, sentience.

175
00:11:03,560 --> 00:11:09,960
There's some core things about the an end point neuron models specifically that don't have

176
00:11:09,960 --> 00:11:14,960
the capacity for intelligence as we understand those are what are some of those core things.

177
00:11:14,960 --> 00:11:18,800
Let's just dive right in there's there's there's two main things.

178
00:11:18,800 --> 00:11:23,760
One is that the neuron needs to have three states and current neurons have two states active

179
00:11:23,760 --> 00:11:25,360
or not.

180
00:11:25,360 --> 00:11:28,240
And we add the idea of a predictive state.

181
00:11:28,240 --> 00:11:34,240
So the neuron goes into a predictive state to indicate that it thinks based upon the

182
00:11:34,240 --> 00:11:38,800
context of its input that it's going to be active soon.

183
00:11:38,800 --> 00:11:44,160
And that prediction is core to everything about our theory that and we, you know, we take

184
00:11:44,160 --> 00:11:46,400
that from understanding how the brain works.

185
00:11:46,400 --> 00:11:50,760
Your brain is constantly making predictions about what it's going to see next when it's

186
00:11:50,760 --> 00:11:52,760
going to feel next all the time.

187
00:11:52,760 --> 00:11:58,280
And you can see that by investigating these depolarized pyramidal neurons in the neuroscience

188
00:11:58,280 --> 00:12:03,160
they call these cells a depolarized, which means that they're primed to fire.

189
00:12:03,160 --> 00:12:05,440
And we're missing that in the an end neuron model.

190
00:12:05,440 --> 00:12:06,880
There's no, there's no concept of that.

191
00:12:06,880 --> 00:12:08,200
So there's that.

192
00:12:08,200 --> 00:12:12,680
And the other thing is pyramidal neurons have different integration zones.

193
00:12:12,680 --> 00:12:17,040
It's not, they don't just have one group of connections to other neurons.

194
00:12:17,040 --> 00:12:22,440
They've got apical dendrites that kind of provide feedback from layers that are either

195
00:12:22,440 --> 00:12:25,000
above it or different parts of the cortex.

196
00:12:25,000 --> 00:12:29,280
There is distal, a distal zone that's kind of, that's lateral.

197
00:12:29,280 --> 00:12:33,800
So that's getting connections from, it could be from another layer, it could be from within

198
00:12:33,800 --> 00:12:36,320
a layer itself, but that provides context.

199
00:12:36,320 --> 00:12:39,360
And these both provide context for the proximal input.

200
00:12:39,360 --> 00:12:43,280
The proximal input is really the driver input.

201
00:12:43,280 --> 00:12:46,880
That's typically coming from the direction of the senses.

202
00:12:46,880 --> 00:12:51,240
And so that's, that's like the sensory input that we need to understand, we need to process.

203
00:12:51,240 --> 00:12:55,800
And the pyramidal neurons do that in the context of these other zones and the context of distal

204
00:12:55,800 --> 00:12:57,880
input and apical input.

205
00:12:57,880 --> 00:13:01,680
So those are the two things I think we're really missing from that point neuron model.

206
00:13:01,680 --> 00:13:07,760
So I get that the neuroscience research has identified these things in human biology,

207
00:13:07,760 --> 00:13:13,040
but it's not clear to me how we've demonstrated that those are required for intelligence, or

208
00:13:13,040 --> 00:13:19,240
even that those things can't be approximated with artificial neural networks as we currently

209
00:13:19,240 --> 00:13:23,760
know that like the last thing, the different zones, you know, maybe think of well, you

210
00:13:23,760 --> 00:13:26,560
know, we just have different inputs and different weights, right?

211
00:13:26,560 --> 00:13:33,840
And then as far as predictions are concerned, if we're able to predict at a network level,

212
00:13:33,840 --> 00:13:38,480
you know, who's to say that the neuron itself has to have that predictive state in order

213
00:13:38,480 --> 00:13:40,240
to create intelligence?

214
00:13:40,240 --> 00:13:47,200
Well, it's, it's true that current artificial neural networks in deep learning could potentially

215
00:13:47,200 --> 00:13:51,560
put together models that replicate the parts of the things about the neuron that we're

216
00:13:51,560 --> 00:13:53,080
saying are required for intelligence.

217
00:13:53,080 --> 00:13:54,080
I think we use it all.

218
00:13:54,080 --> 00:13:55,040
It's for prediction all the time.

219
00:13:55,040 --> 00:13:56,040
Yeah.

220
00:13:56,040 --> 00:14:00,480
But I don't know that that's, it doesn't feel natural to me.

221
00:14:00,480 --> 00:14:05,840
And think about this recently, there's been this big discussion in the deep learning community

222
00:14:05,840 --> 00:14:11,920
about back propagation because Jeff Hinton has recently said, let's give up on back propagation,

223
00:14:11,920 --> 00:14:15,520
go back to the drawing board and try and figure out what's really going on.

224
00:14:15,520 --> 00:14:17,520
We did that 12 years ago.

225
00:14:17,520 --> 00:14:19,600
So we never tried back propagation.

226
00:14:19,600 --> 00:14:23,320
We've always tried to do this because we don't see back propagation happening in the brain.

227
00:14:23,320 --> 00:14:28,520
And for the, for the longest time, you know, Hinton and, and Ben Gio were insisting that back

228
00:14:28,520 --> 00:14:31,280
propagation is happening in the brain, we just don't see it.

229
00:14:31,280 --> 00:14:34,320
So I think, so that's kind of a move in, in our direction.

230
00:14:34,320 --> 00:14:39,160
And even from like the deep mind crew, they recently had this blog post about how important

231
00:14:39,160 --> 00:14:43,560
neuroscience is to contributing to artificial intelligence.

232
00:14:43,560 --> 00:14:46,520
It feels to me like the community started to move in our direction.

233
00:14:46,520 --> 00:14:52,120
And maybe they will be able to hack these properties that we're saying we need in, in

234
00:14:52,120 --> 00:14:55,480
the neuron model into deep learning systems that, that could happen.

235
00:14:55,480 --> 00:14:59,160
But I don't think that it will happen without them doing something to incorporate those

236
00:14:59,160 --> 00:15:00,160
ideas.

237
00:15:00,160 --> 00:15:06,000
And Ben Gio, just this week published a paper that talked about, I forget the exact

238
00:15:06,000 --> 00:15:07,000
title.

239
00:15:07,000 --> 00:15:08,000
Something about consciousness.

240
00:15:08,000 --> 00:15:09,000
I don't know if you saw that.

241
00:15:09,000 --> 00:15:10,920
I did not see that.

242
00:15:10,920 --> 00:15:17,120
And it was controversial might be strong, but it raised a lot of questions because he,

243
00:15:17,120 --> 00:15:22,840
you know, proposed that somehow we need to take into account some notion of consciousness

244
00:15:22,840 --> 00:15:23,840
in our models.

245
00:15:23,840 --> 00:15:28,000
But the paper didn't present any experimental results or whatever it was just like a prod

246
00:15:28,000 --> 00:15:29,000
to the community.

247
00:15:29,000 --> 00:15:33,440
Anything about consciousness is going to be controversial because what is consciousness

248
00:15:33,440 --> 00:15:34,440
Sam?

249
00:15:34,440 --> 00:15:35,440
Right.

250
00:15:35,440 --> 00:15:36,440
What is intelligence?

251
00:15:36,440 --> 00:15:37,440
Exactly.

252
00:15:37,440 --> 00:15:42,920
I was asking people in the audience who believes humans are intelligent and they all raise their

253
00:15:42,920 --> 00:15:43,920
hands.

254
00:15:43,920 --> 00:15:44,920
Who believes chimpanzees are intelligent?

255
00:15:44,920 --> 00:15:49,200
And they just go down the evolutionary ladder and see hands going down.

256
00:15:49,200 --> 00:15:53,320
By the end of it, I'm asking who thinks plants are intelligent and there's still one or

257
00:15:53,320 --> 00:15:56,040
two people that think plants are intelligent and they may be right.

258
00:15:56,040 --> 00:15:57,640
You know, I don't, we don't know.

259
00:15:57,640 --> 00:15:58,640
Yeah.

260
00:15:58,640 --> 00:16:00,160
Permissium, whatever.

261
00:16:00,160 --> 00:16:02,200
So there's a lot of disagreement.

262
00:16:02,200 --> 00:16:05,480
The thing is everybody believes humans are intelligent.

263
00:16:05,480 --> 00:16:06,960
So at least we can start with that.

264
00:16:06,960 --> 00:16:11,560
So we have been, and I think by association, we can include most primates and not two because

265
00:16:11,560 --> 00:16:14,400
they have the same neocortical structure that we have.

266
00:16:14,400 --> 00:16:18,800
So we've focused that on that neuroscience and what we think, what we all know is intelligent

267
00:16:18,800 --> 00:16:22,600
and that's the neocortex of the mammalian brain.

268
00:16:22,600 --> 00:16:26,880
So you started off talking about like level setting on intelligence and just how open

269
00:16:26,880 --> 00:16:31,160
and that is and then talked about kind of the evolution of the neuron.

270
00:16:31,160 --> 00:16:34,240
Like how do you get from there to systems?

271
00:16:34,240 --> 00:16:35,240
Oh, okay.

272
00:16:35,240 --> 00:16:40,480
So you think about the pyramidal neuron, like I said, and it has these integration zones.

273
00:16:40,480 --> 00:16:44,440
It's hard to visualize without a picture, but Francisco said the same thing.

274
00:16:44,440 --> 00:16:45,440
I know.

275
00:16:45,440 --> 00:16:46,440
I know.

276
00:16:46,440 --> 00:16:49,720
But my talk will be online at some point.

277
00:16:49,720 --> 00:16:54,320
So if you could find Matt Taylor talking strangely, but I've got a bunch of drawings and

278
00:16:54,320 --> 00:16:55,320
stuff.

279
00:16:55,320 --> 00:16:57,720
But if you look at a pyramidal neuron and it's got, we'll link to it if you should

280
00:16:57,720 --> 00:16:58,720
us a link.

281
00:16:58,720 --> 00:16:59,720
Okay.

282
00:16:59,720 --> 00:17:02,800
It has these integration zones, distal, which is lateral to the side, proximal, which comes

283
00:17:02,800 --> 00:17:05,760
from below apocode, which comes from on top.

284
00:17:05,760 --> 00:17:09,280
The cortex has this homogenous structure.

285
00:17:09,280 --> 00:17:14,520
If you took your neocortex and you unwrinkled it and unfolded it and flattened it all out,

286
00:17:14,520 --> 00:17:16,040
it's a sheet of cells.

287
00:17:16,040 --> 00:17:19,640
It's about the size of a dinner napkin, it's about the thickness of a dinner napkin.

288
00:17:19,640 --> 00:17:21,040
And it's homogenous throughout.

289
00:17:21,040 --> 00:17:26,280
It has the same structure and what that, there's sort of like this computational unit in

290
00:17:26,280 --> 00:17:28,640
the cortex called a cortical column.

291
00:17:28,640 --> 00:17:32,840
This is something that is more recent of a neuroscience discovery.

292
00:17:32,840 --> 00:17:36,840
We've known for like a hundred years that the cortex had layers, like there is these

293
00:17:36,840 --> 00:17:41,840
distinct little layers in the sheet and that their structure was different enough that

294
00:17:41,840 --> 00:17:45,200
we thought, well, they're doing different things, but we're not exactly sure what they're

295
00:17:45,200 --> 00:17:46,200
doing.

296
00:17:46,200 --> 00:17:50,120
Now that we know they're not just layers, there's also columns.

297
00:17:50,120 --> 00:17:54,120
And we can take that, each column and say, okay, each one of these is some individual

298
00:17:54,120 --> 00:17:56,560
computational unit, right?

299
00:17:56,560 --> 00:18:00,840
And maybe they can share their computation or the output of their computations with their

300
00:18:00,840 --> 00:18:02,560
neighbors and stuff.

301
00:18:02,560 --> 00:18:08,640
So this idea that a column can have layers within it and every layer is full of these

302
00:18:08,640 --> 00:18:10,560
primal neurons, okay?

303
00:18:10,560 --> 00:18:16,080
So imagine a column that's cut up into layers and this is sort of a cylindrical column,

304
00:18:16,080 --> 00:18:17,520
cut up in layers.

305
00:18:17,520 --> 00:18:21,600
Each one of those layers is full of primal neurons that have these integration zones,

306
00:18:21,600 --> 00:18:26,480
apical up and down to the north sort of and proximal to the south and distal to the side.

307
00:18:26,480 --> 00:18:32,200
So each layer itself has the same integration zone properties as an individual neuron

308
00:18:32,200 --> 00:18:35,080
because they're all oriented in exactly the same way.

309
00:18:35,080 --> 00:18:39,040
So you can treat that layer as a computational unit.

310
00:18:39,040 --> 00:18:43,800
So a layer gets proximal input, a bunch of proximal input that all gets piped into its

311
00:18:43,800 --> 00:18:45,720
neurons in different ways.

312
00:18:45,720 --> 00:18:51,280
From some space that's representing generally some spatial sensory features changing over

313
00:18:51,280 --> 00:18:53,120
time or something like that.

314
00:18:53,120 --> 00:18:57,960
So you can think of the layer itself as a computational unit.

315
00:18:57,960 --> 00:19:01,560
Depending on where it gets its proximal input, where it gets its distal input and its

316
00:19:01,560 --> 00:19:04,200
able to input, it does different things.

317
00:19:04,200 --> 00:19:09,560
And also there's a bunch of different layers in the cortex, somewhere between six and

318
00:19:09,560 --> 00:19:11,880
ten, depending on which neuroscience you talk to.

319
00:19:11,880 --> 00:19:15,160
But each one of those layers is structured a little bit differently too.

320
00:19:15,160 --> 00:19:20,080
So there's some minor deviation in the organization of those primal neurons within layers that

321
00:19:20,080 --> 00:19:25,360
also give them a little bit of different computational aspects, organization in what sense.

322
00:19:25,360 --> 00:19:30,120
For example, we have these algorithms that we're saying are happening in these layers.

323
00:19:30,120 --> 00:19:34,920
One is called a spatial pooling algorithm that takes some input and kind of spreads it,

324
00:19:34,920 --> 00:19:38,120
normalizes it while retaining the semantics of the input.

325
00:19:38,120 --> 00:19:42,200
And these create these mini column structures of neurons.

326
00:19:42,200 --> 00:19:44,120
And some layers have this.

327
00:19:44,120 --> 00:19:49,140
And typically the distal connections from each one of those neurons as it's perceiving

328
00:19:49,140 --> 00:19:53,160
proximal input, they start connecting to each other over time.

329
00:19:53,160 --> 00:19:57,480
When you take that distal input to a layer and you say, okay, we're not going to get that

330
00:19:57,480 --> 00:20:02,240
distal input from somewhere else, we're going to have all of the primal neurons within

331
00:20:02,240 --> 00:20:05,560
the layer give each other distal input.

332
00:20:05,560 --> 00:20:09,560
What you're doing is just naturally creating a temporal context.

333
00:20:09,560 --> 00:20:15,800
Because when your only context is some input is what state you've been in in the past,

334
00:20:15,800 --> 00:20:18,160
then that's the temporal context.

335
00:20:18,160 --> 00:20:22,120
If you're getting that input from somewhere else, who knows, that context could mean any

336
00:20:22,120 --> 00:20:23,320
number of things.

337
00:20:23,320 --> 00:20:27,600
But if you're just giving yourself context, that's you're looking at your own past.

338
00:20:27,600 --> 00:20:31,820
That layer has context of its own history when you loop them back to itself.

339
00:20:31,820 --> 00:20:33,760
So that's one of the core things that we discovered.

340
00:20:33,760 --> 00:20:36,120
We call this a temporal memory algorithm.

341
00:20:36,120 --> 00:20:43,440
And it relies on these little mini column structures that takes the input, the bits of input

342
00:20:43,440 --> 00:20:48,640
that are coming in from some sensory organ or perhaps from another part of the cortex,

343
00:20:48,640 --> 00:20:54,080
normalizes it into these column activations and then activates cells within each column

344
00:20:54,080 --> 00:20:57,120
based upon the distal context that it's getting.

345
00:20:57,120 --> 00:21:00,920
So what you get is it's starting to tie sequences together.

346
00:21:00,920 --> 00:21:07,240
When you see a pattern repeating over and over, over and over, you get these distal connections

347
00:21:07,240 --> 00:21:09,240
that are being reinforced.

348
00:21:09,240 --> 00:21:13,040
Because they see the pattern and the distal connection will create a connection to the active

349
00:21:13,040 --> 00:21:17,560
cells that it just saw that represented the previous spatial input.

350
00:21:17,560 --> 00:21:20,040
And then we get another input and there may be a prediction.

351
00:21:20,040 --> 00:21:22,840
So I saw that last time I'm going to be next.

352
00:21:22,840 --> 00:21:27,160
So it makes a prediction and if it's right and the next input activates a column that

353
00:21:27,160 --> 00:21:30,600
that that cells in, then if it comes active, it was a correct prediction.

354
00:21:30,600 --> 00:21:35,020
Well, the context you're creating for me is how I felt when Francisco was explaining

355
00:21:35,020 --> 00:21:36,020
something.

356
00:21:36,020 --> 00:21:37,020
It's like, whoa.

357
00:21:37,020 --> 00:21:39,200
It's a lot easier with visuals.

358
00:21:39,200 --> 00:21:43,720
And hence, that's why I created this bunch of videos on our YouTube channel to try and

359
00:21:43,720 --> 00:21:46,480
explain it all visually.

360
00:21:46,480 --> 00:21:52,760
So you explain kind of the microstructure than the macrostructure and then like what's

361
00:21:52,760 --> 00:21:53,760
next?

362
00:21:53,760 --> 00:21:57,240
So Strange Loop is a developer conference, like how do you get from there to, okay, how do

363
00:21:57,240 --> 00:21:58,240
I build something?

364
00:21:58,240 --> 00:22:00,920
Well, there's two questions there, I guess.

365
00:22:00,920 --> 00:22:05,320
Strange Loop is a developer conference, however, it's also like a weird conference, you

366
00:22:05,320 --> 00:22:06,920
know, it's all granted.

367
00:22:06,920 --> 00:22:07,920
It is.

368
00:22:07,920 --> 00:22:11,080
And so you can like, yes, it's very eclectic.

369
00:22:11,080 --> 00:22:15,360
So you can get in, if you have something that's like on the fringe, but very interesting,

370
00:22:15,360 --> 00:22:16,360
you can get in and talk to them.

371
00:22:16,360 --> 00:22:18,360
So I think that's why I got this talk.

372
00:22:18,360 --> 00:22:25,640
But there's, as far as from a, but still, I mean, in addition to developing IP and all

373
00:22:25,640 --> 00:22:31,520
of that, as I understand it, Numenta's company offers tools that allow people to actually

374
00:22:31,520 --> 00:22:32,520
use this stuff.

375
00:22:32,520 --> 00:22:33,720
Is that correct or no?

376
00:22:33,720 --> 00:22:35,240
All open source tools.

377
00:22:35,240 --> 00:22:40,440
So all of our code is open source and anybody can try and use it if they want to.

378
00:22:40,440 --> 00:22:46,280
I've created a lot of tutorials and code samples and I try and make it as approachable as possible

379
00:22:46,280 --> 00:22:47,280
for our community.

380
00:22:47,280 --> 00:22:52,360
We've got a very active forum, lots of discussions about the theory and about code and stuff.

381
00:22:52,360 --> 00:22:59,480
So as a user of these open source tools and things like, am I, do I need to think about

382
00:22:59,480 --> 00:23:05,160
columns and dendrites and all of that stuff or am I thinking about other representations?

383
00:23:05,160 --> 00:23:06,920
So it can go either way.

384
00:23:06,920 --> 00:23:08,360
It depends on what you're trying to do.

385
00:23:08,360 --> 00:23:12,880
So we have a pretty diverse and eclectic community that are interested in this, typically

386
00:23:12,880 --> 00:23:16,520
people who are really interested in how the brain works or, you know, let's say they

387
00:23:16,520 --> 00:23:17,880
can be a little off.

388
00:23:17,880 --> 00:23:22,960
But I mean, they're always very smart and inquisitive and curious and it, it amazes me the types

389
00:23:22,960 --> 00:23:27,400
of things that people try and do with, with our stuff and I always encourage it and I

390
00:23:27,400 --> 00:23:28,400
was like, yeah, try it.

391
00:23:28,400 --> 00:23:29,400
Give it a try.

392
00:23:29,400 --> 00:23:30,400
Who knows?

393
00:23:30,400 --> 00:23:31,400
We don't know what it's going to have.

394
00:23:31,400 --> 00:23:36,280
It's a software that we open source is called the new pick, the Numenta platform for intelligent

395
00:23:36,280 --> 00:23:37,680
computing.

396
00:23:37,680 --> 00:23:43,720
And we just released 1.0 of that a few months ago and that includes up to what I just talked

397
00:23:43,720 --> 00:23:48,800
about, the temporal memory part of it, and a few years back after we, you know, went

398
00:23:48,800 --> 00:23:52,640
through this research cycle and made the temporal memory discovery, that was a big discovery

399
00:23:52,640 --> 00:23:57,920
for us to see how sequences were memorized in the brain or in the cortex.

400
00:23:57,920 --> 00:24:02,200
We kind of just dumped it all open source and we started like building these potential

401
00:24:02,200 --> 00:24:05,400
sample, you know, which is brainstormed about what could we make with this that people

402
00:24:05,400 --> 00:24:06,800
might want to use.

403
00:24:06,800 --> 00:24:10,640
And we made all these sample applications, there's one that was like rogue human behavior

404
00:24:10,640 --> 00:24:15,200
detection, which is something you can install on a computer and it monitors the different

405
00:24:15,200 --> 00:24:20,320
metrics that are coming out of the computer over days and weeks and given indication

406
00:24:20,320 --> 00:24:21,800
about a user's behavior.

407
00:24:21,800 --> 00:24:26,400
Are they behaving oddly or differently based on the time of day and the thing that they're

408
00:24:26,400 --> 00:24:29,200
doing and the metrics that are coming out of the computer?

409
00:24:29,200 --> 00:24:31,280
So that's a sort of thing that you can do.

410
00:24:31,280 --> 00:24:40,240
We also had a IT analytics program that hooked up to AWS and we actually licensed that to

411
00:24:40,240 --> 00:24:42,040
another company called crock.

412
00:24:42,040 --> 00:24:48,760
And so they are actively selling that to IT companies that have a bunch of servers on Amazon

413
00:24:48,760 --> 00:24:53,760
and it will automatically like through CloudWatch connect to all the different metrics coming

414
00:24:53,760 --> 00:24:57,600
out of your servers and it will create models for all of them and it'll just start streaming

415
00:24:57,600 --> 00:25:01,520
the data into them and you don't really have to do anything, they're all sort of pre-configured.

416
00:25:01,520 --> 00:25:03,680
And then it'll give you anomaly indications over time.

417
00:25:03,680 --> 00:25:08,640
So after it's seen that server data for a while, it gets an idea of what's normal and

418
00:25:08,640 --> 00:25:13,600
what's not normal, then it notifies you that something's wrong with the server.

419
00:25:13,600 --> 00:25:17,440
It doesn't know what's wrong with the server, but it can tell you that something abnormal

420
00:25:17,440 --> 00:25:23,680
is happening and even with this server and this server and the combination of those.

421
00:25:23,680 --> 00:25:27,920
So there's anywhere that there's streaming analytics that you need anomaly detection,

422
00:25:27,920 --> 00:25:33,120
I think that there's a potential application for what we have right now with new pick 1.0.

423
00:25:33,120 --> 00:25:38,800
There's also this really interesting thing that I think is still a big opportunity for

424
00:25:38,800 --> 00:25:40,960
people who want to try and build something novel with this.

425
00:25:41,600 --> 00:25:47,440
We figured out a way to encode geospatial location into a format.

426
00:25:47,440 --> 00:25:52,800
Remember when Francisco and you talked a lot about SDRs about sparse distributed representations.

427
00:25:52,800 --> 00:25:55,680
So corticals, I was all about that, they call them semantic fingerprints.

428
00:25:57,040 --> 00:26:00,400
We found a way to encode location information like latitude,

429
00:26:00,400 --> 00:26:03,040
longitude, altitude, into an SDR.

430
00:26:03,040 --> 00:26:09,200
So we can take something that moves through time and space and give the algorithms,

431
00:26:09,200 --> 00:26:14,960
the intelligence algorithms a way to understand the patterns in the movement of that object.

432
00:26:15,600 --> 00:26:21,840
So for an example that I always do is I go walk my dogs on the same dog walking around every day.

433
00:26:21,840 --> 00:26:26,800
And if I take a tracker with me and then I go put all my points back through the algorithm,

434
00:26:26,800 --> 00:26:29,840
the first time it sees the walk, it's like all anonymous.

435
00:26:29,840 --> 00:26:33,280
It doesn't think, none of it's familiar because it's brand new.

436
00:26:33,280 --> 00:26:35,440
The second time I do it, it's a little bit less familiar.

437
00:26:35,440 --> 00:26:37,680
The third time I do it, it's like no big deal.

438
00:26:37,680 --> 00:26:38,880
This is normal, right?

439
00:26:39,520 --> 00:26:42,560
As soon as I deviate from the path that I've taken,

440
00:26:42,560 --> 00:26:45,040
and even if I just go walk on the other side of the street,

441
00:26:45,040 --> 00:26:47,360
or if my dogs decide they don't want to stop at that tree,

442
00:26:47,360 --> 00:26:49,200
they want to stop at some other tree,

443
00:26:49,200 --> 00:26:52,080
I get anomaly indications coming from my path.

444
00:26:52,080 --> 00:26:55,680
So I think this has big applications in fields like logistics,

445
00:26:55,680 --> 00:26:58,640
air traffic control, human tracking, pet tracking,

446
00:26:58,640 --> 00:27:03,120
stuff like that, where you've got normal routes of things that normally happen.

447
00:27:03,120 --> 00:27:06,240
And you don't necessarily, to the T, want to say,

448
00:27:06,240 --> 00:27:09,280
oh, if they deviate right now, or if they're not at this point at this time,

449
00:27:09,280 --> 00:27:13,040
there's something wrong, you just want to get an idea of their general movement,

450
00:27:13,040 --> 00:27:14,960
whether it's strange or not, or whether it's,

451
00:27:14,960 --> 00:27:19,280
have been seen or not, then it can do that sort of thing. That's really interesting.

452
00:27:19,280 --> 00:27:23,920
Certainly for the network and server anomaly detection,

453
00:27:23,920 --> 00:27:26,480
and the example you gave before that,

454
00:27:26,480 --> 00:27:31,280
there are things that you can do with a variety of different techniques.

455
00:27:31,920 --> 00:27:40,240
Are there things that you've found that either the approach you take,

456
00:27:40,240 --> 00:27:44,800
because of the approach you take, it's just kind of besting class,

457
00:27:44,800 --> 00:27:49,120
or if you need to do X, Y, Z, this is the best way to do it,

458
00:27:49,120 --> 00:27:54,960
or either from a complexity of creating the solution,

459
00:27:54,960 --> 00:27:59,360
or computational costs, or some other metric.

460
00:27:59,360 --> 00:28:01,200
Well, we wondered the same thing,

461
00:28:01,200 --> 00:28:07,440
but the problem we had several years ago is that there are no standard benchmarks

462
00:28:07,440 --> 00:28:11,040
for streaming temporal anomaly detection,

463
00:28:11,040 --> 00:28:13,760
or just for temporal anomaly detection.

464
00:28:13,760 --> 00:28:15,920
Most of the benchmarks are on spatial data,

465
00:28:15,920 --> 00:28:20,080
and most of the machine learning techniques work on spatial data online.

466
00:28:20,080 --> 00:28:23,360
We didn't find anything so we could compare what we did with what,

467
00:28:23,360 --> 00:28:28,320
like LSTM, for example, has some abilities to do temporal analysis on things,

468
00:28:28,320 --> 00:28:31,680
it'd be sort of in batches that move along.

469
00:28:31,680 --> 00:28:34,240
So we created a benchmark we called the anomaly,

470
00:28:34,240 --> 00:28:40,320
the numente anomaly benchmark, and we've set up R as one of them in the running,

471
00:28:40,320 --> 00:28:43,600
and we set up an LSTM one, and we set up, there's one from Twitter,

472
00:28:43,600 --> 00:28:46,400
there's one from Etsy, that does streaming anomaly detection,

473
00:28:46,400 --> 00:28:49,360
like they've got open source projects that do that sort of thing.

474
00:28:49,360 --> 00:28:54,000
And we created these input data sets, things like how many taxi calls

475
00:28:54,000 --> 00:28:58,240
where they're in New York City over an entire period of time or something.

476
00:28:58,240 --> 00:29:01,280
And you can look at that data, and you can say something weird happened there,

477
00:29:01,280 --> 00:29:05,120
for sure, and you go look it up and you're like, oh, there was a big game in town,

478
00:29:05,120 --> 00:29:07,920
or there's stuff like that, you can find that data.

479
00:29:07,920 --> 00:29:10,800
So we'd find data sets like that that had a good amount of data,

480
00:29:10,800 --> 00:29:13,760
and had obvious anomalies that were labeled and marked,

481
00:29:13,760 --> 00:29:16,000
and we've run all of these algorithms against them,

482
00:29:16,000 --> 00:29:19,600
and score them based on how well they detected the anomaly.

483
00:29:19,600 --> 00:29:23,680
And waiting it, I think we waited it pretty heavily on,

484
00:29:23,680 --> 00:29:27,040
not providing false positives, I can't remember exactly,

485
00:29:27,040 --> 00:29:33,440
but it's open source, it's on GitHub, it's a nimenta-slash-nab for nimenta-anomaly-finchmark.

486
00:29:33,440 --> 00:29:36,640
Okay, we have that at least, and of course we're the winner,

487
00:29:36,640 --> 00:29:40,160
because we always mean, but we had like this contest,

488
00:29:40,160 --> 00:29:42,960
we're like, if anybody beat us at this and somebody came and beat us at it,

489
00:29:42,960 --> 00:29:45,520
we're like, okay, we're gonna fix it, so we fixed it,

490
00:29:45,520 --> 00:29:50,320
we're like, we're beating again, but there's always some tweaking that you can do,

491
00:29:50,320 --> 00:29:53,040
you know, to try and get that last few percent.

492
00:29:53,040 --> 00:29:58,880
Ah, okay, it kind of leads me with an impression that, like, this is a tool that,

493
00:29:58,880 --> 00:30:03,040
you know, we've, or a set of tools, that you have a strong feeling that,

494
00:30:03,920 --> 00:30:09,040
you know, closely models the inner workings of the brain as we understand it,

495
00:30:09,040 --> 00:30:14,000
and that over time, that will lead to, you know, I'm assuming you're banking on,

496
00:30:14,000 --> 00:30:18,640
like, order of magnitude, you know, capabilities over current approaches.

497
00:30:18,640 --> 00:30:21,520
Like, the things you can do using nimenta and the things you can do,

498
00:30:21,520 --> 00:30:24,160
using other things will diverge over time, but today,

499
00:30:24,880 --> 00:30:28,560
it doesn't sound like there's a, you know, a bang on the table, like,

500
00:30:28,560 --> 00:30:32,880
if you need to do x, y, z, these tools will get you there,

501
00:30:32,880 --> 00:30:35,440
you know, a hundred times faster and a hundred times cheaper,

502
00:30:35,440 --> 00:30:38,960
or even 10, or, you know, it sounds like, you know,

503
00:30:38,960 --> 00:30:42,560
it's an interesting approach and something that's worthwhile for people to learn,

504
00:30:42,560 --> 00:30:45,440
and take a look at and understand and thinking around.

505
00:30:45,440 --> 00:30:47,440
But it's, we don't have a killer.

506
00:30:47,440 --> 00:30:48,320
There's no killer.

507
00:30:48,320 --> 00:30:50,000
I guess that's what I'm, what I'm getting.

508
00:30:50,000 --> 00:30:53,280
Yeah, there's no killer app, but we're patient, too.

509
00:30:53,280 --> 00:30:56,960
There's a lot of things about the brain that we don't, we still don't understand.

510
00:30:56,960 --> 00:30:57,280
Right.

511
00:30:57,280 --> 00:31:02,880
And what we have currently in nimenta 1.0 is just, you know, temporal memory stuff.

512
00:31:02,880 --> 00:31:05,600
All of our other work that we're doing is in research repository,

513
00:31:05,600 --> 00:31:08,320
is that kind of attach on top of that.

514
00:31:08,320 --> 00:31:11,280
So we're taking those core algorithms, which aren't going to change,

515
00:31:11,280 --> 00:31:13,440
and we're building new and different things with them,

516
00:31:13,440 --> 00:31:16,160
because the core algorithms in your brain don't change,

517
00:31:16,160 --> 00:31:19,840
but we discover that it can do lots of different things with those core algorithms.

518
00:31:19,840 --> 00:31:20,240
Right.

519
00:31:20,240 --> 00:31:24,720
So we're building structures now, because we think we understand how

520
00:31:24,720 --> 00:31:28,400
sensory motor integration happens with sensory input and movement.

521
00:31:28,960 --> 00:31:32,720
But it is the integration of two layers in one of those columns.

522
00:31:32,720 --> 00:31:34,800
Remember I told you about the layers having these integrations.

523
00:31:34,800 --> 00:31:35,600
Yeah.

524
00:31:35,600 --> 00:31:40,960
So we could have one layer that is running the same temporal memory algorithm that I described

525
00:31:40,960 --> 00:31:43,520
earlier, with the mini columns and everything.

526
00:31:43,520 --> 00:31:45,680
But we don't send it its own distal input.

527
00:31:45,680 --> 00:31:47,600
We don't give it a temporal context.

528
00:31:47,600 --> 00:31:48,160
Okay.

529
00:31:48,160 --> 00:31:51,040
We can pipe in the context, the distal connection,

530
00:31:51,040 --> 00:31:52,320
comes from somewhere else in the brain.

531
00:31:52,320 --> 00:31:54,320
It comes from a different layer down.

532
00:31:54,320 --> 00:32:00,720
And if we assume that the output of that layer is providing us with

533
00:32:00,720 --> 00:32:04,480
location information associated with a sensory input

534
00:32:05,200 --> 00:32:07,920
that's proximal coming up to the layer from the bottom.

535
00:32:07,920 --> 00:32:10,640
So that's the driver signal as this sensory input.

536
00:32:10,640 --> 00:32:14,720
The distal signal is going to represent the object being touched

537
00:32:14,720 --> 00:32:19,040
and what location on the object that sensory feature was sensed.

538
00:32:19,760 --> 00:32:25,600
Then we can have a layer that can represent every object we've ever

539
00:32:25,600 --> 00:32:29,360
touched and what sensory input we've felt where on it.

540
00:32:29,920 --> 00:32:34,400
And so that layer now provides that information to another layer,

541
00:32:34,400 --> 00:32:35,600
which we call an output layer.

542
00:32:36,320 --> 00:32:38,960
This output layer has a little bit of a different structure,

543
00:32:38,960 --> 00:32:41,680
because it doesn't have the mini columns like the one underneath it.

544
00:32:41,680 --> 00:32:46,960
But it represents, over time, a library of every object we've ever learned.

545
00:32:46,960 --> 00:32:50,560
So we can train this thing and say, okay, this is a coffee cup.

546
00:32:50,560 --> 00:32:51,760
Touch it all over the place.

547
00:32:51,760 --> 00:32:52,480
Right, right?

548
00:32:52,480 --> 00:32:52,800
Okay.

549
00:32:52,800 --> 00:32:53,840
Here's a banana.

550
00:32:53,840 --> 00:32:55,200
Touch it all over the place.

551
00:32:55,200 --> 00:32:59,920
And we can build a library of objects that that top layer represents.

552
00:32:59,920 --> 00:33:03,280
So the bottom layer is basically just going to represent all the sensory input

553
00:33:03,280 --> 00:33:06,400
you've felt on every location on every object that you've touched.

554
00:33:06,400 --> 00:33:08,720
And this is the temporal memory concept.

555
00:33:08,720 --> 00:33:11,840
It's the temporal memory concept, but it's not doing temporal memory anymore.

556
00:33:11,840 --> 00:33:16,000
It's doing sensory feature and location association,

557
00:33:16,000 --> 00:33:18,080
just because we've changed the distal input.

558
00:33:18,080 --> 00:33:20,080
So it's no longer giving itself distal input.

559
00:33:20,080 --> 00:33:21,440
It's getting it from somewhere else.

560
00:33:21,440 --> 00:33:22,960
And it does something entirely different.

561
00:33:22,960 --> 00:33:25,120
And so it sounds like the idea there is,

562
00:33:25,120 --> 00:33:28,400
like if you think about using deep learning,

563
00:33:28,400 --> 00:33:33,520
object recognition, like our best guess at the way the different layers work now.

564
00:33:33,520 --> 00:33:37,360
So you've got layers that kind of figure out edges and layers that figure out colors.

565
00:33:37,360 --> 00:33:41,840
And so, you know, when the inputs, the banana,

566
00:33:41,840 --> 00:33:45,760
you know, we'll get kind of the curvy layer firing and the yellow layer.

567
00:33:45,760 --> 00:33:48,080
So that's kind of the deep learnings couldn't do.

568
00:33:48,080 --> 00:33:51,280
No, but I'm saying what you're describing sounds like,

569
00:33:52,080 --> 00:33:54,720
you know, maybe in the, kind of in the internals,

570
00:33:54,720 --> 00:33:57,840
is capturing a richer representation of these various things.

571
00:33:57,840 --> 00:33:59,280
That's clear what the big difference is.

572
00:33:59,280 --> 00:34:01,760
Is our model incorporates movement?

573
00:34:02,400 --> 00:34:04,080
And that's the big difference.

574
00:34:04,080 --> 00:34:08,240
So can you name anything that is intelligent that cannot move?

575
00:34:08,240 --> 00:34:09,040
Nothing comes.

576
00:34:09,040 --> 00:34:10,880
I've never, nobody ever does.

577
00:34:10,880 --> 00:34:13,680
Because there's nothing intelligent that can't move.

578
00:34:13,680 --> 00:34:14,320
Yeah.

579
00:34:14,320 --> 00:34:17,680
So we believe that's a core feature of intelligence.

580
00:34:17,680 --> 00:34:21,600
The ability to interact with your environment has to be baked in

581
00:34:21,600 --> 00:34:23,520
to the architecture of the intelligent system.

582
00:34:23,520 --> 00:34:24,960
It's not something that you can just add.

583
00:34:24,960 --> 00:34:28,160
You can't just add behavior to a system that you're building.

584
00:34:28,160 --> 00:34:30,720
It has to be baked into the flow of information.

585
00:34:30,720 --> 00:34:34,560
So like I said, when, when you move your finger to touch an object,

586
00:34:34,560 --> 00:34:36,560
you know where your finger is going to move.

587
00:34:36,560 --> 00:34:38,320
Because you just commanded it to move there.

588
00:34:38,320 --> 00:34:40,560
So that information is available to your brain.

589
00:34:40,560 --> 00:34:42,160
That loop has to be baked in.

590
00:34:42,160 --> 00:34:43,760
So that every time you touch something,

591
00:34:43,760 --> 00:34:46,720
you know where it's going and you know what you expect to feel.

592
00:34:46,720 --> 00:34:48,960
If you don't feel that, something's wrong.

593
00:34:48,960 --> 00:34:49,280
Okay.

594
00:34:50,160 --> 00:34:53,360
And so Matt is demonstrating all this with a glass of water.

595
00:34:53,360 --> 00:34:56,400
And we've been experimenting with a video camera set up here.

596
00:34:56,400 --> 00:35:00,480
So we may be able to show the visual aids

597
00:35:00,480 --> 00:35:02,000
of with motion.

598
00:35:02,000 --> 00:35:04,000
It helps with the visual aids.

599
00:35:04,000 --> 00:35:06,720
But like I said, if you want visuals, go to nemento.org.

600
00:35:06,720 --> 00:35:07,200
Yeah.

601
00:35:07,200 --> 00:35:08,240
I got lots of stuff.

602
00:35:08,240 --> 00:35:08,720
Nice.

603
00:35:08,720 --> 00:35:09,040
Nice.

604
00:35:09,040 --> 00:35:09,840
Awesome.

605
00:35:09,840 --> 00:35:12,320
Well, anything else that you covered in your talk

606
00:35:12,320 --> 00:35:14,400
or last kind of final thoughts that you want to leave us with?

607
00:35:15,200 --> 00:35:19,680
I guess I just want to emphasize that we have a really nice

608
00:35:19,680 --> 00:35:20,400
community.

609
00:35:20,400 --> 00:35:21,920
And like I'm the community manager.

610
00:35:21,920 --> 00:35:23,280
So of course, we're going to say that.

611
00:35:23,280 --> 00:35:26,080
But honestly, there's some really bright people that have even shown up

612
00:35:26,080 --> 00:35:29,760
just in the past year that are doing some really interesting things with HTML.

613
00:35:29,760 --> 00:35:31,600
All of our papers are open access.

614
00:35:31,600 --> 00:35:34,880
So all this theory is everything that we theorize about.

615
00:35:34,880 --> 00:35:37,600
We write papers about and we put it out there.

616
00:35:37,600 --> 00:35:39,040
And we do it with code.

617
00:35:39,040 --> 00:35:40,640
So we're like, here's a paper.

618
00:35:40,640 --> 00:35:41,840
Here's a simulation.

619
00:35:41,840 --> 00:35:42,560
Here's the code.

620
00:35:42,560 --> 00:35:45,120
You can run it yourself if you want to try to run it yourself.

621
00:35:45,120 --> 00:35:47,360
So if you don't believe us, you can try it yourself.

622
00:35:47,360 --> 00:35:50,960
And there's lots of people in our community that have decided they're going to

623
00:35:50,960 --> 00:35:55,040
write their own HTML system and their own favorite language of their own environment.

624
00:35:55,040 --> 00:35:57,760
So there's a lot of people doing new and interesting things.

625
00:35:57,760 --> 00:35:59,680
They're creating their own visualizations.

626
00:35:59,680 --> 00:36:01,840
Last one was thesis from this guy in Turkey.

627
00:36:01,840 --> 00:36:07,520
He did this amazing sensory motor sort of simulation in a 3D game environment where he's

628
00:36:07,520 --> 00:36:12,560
got a player trying to find a point and he wrote his whole thesis on his brain.

629
00:36:12,560 --> 00:36:16,800
But he used our theory and then attached some stuff on top.

630
00:36:16,800 --> 00:36:20,800
He theorized further and he's like, well, what have I got this and this and this?

631
00:36:20,800 --> 00:36:26,400
And trying to create a more complete idea of the brain, not just the cortex.

632
00:36:26,400 --> 00:36:28,960
Because we're really just working on cortex right now.

633
00:36:28,960 --> 00:36:32,960
And he's trying to incorporate some other things like some like real behaviors or real

634
00:36:32,960 --> 00:36:38,560
drivers of what is the motivation for that agent that is running the intelligence.

635
00:36:38,560 --> 00:36:44,080
And we're not quite there, but we're focusing our research right now on location,

636
00:36:44,080 --> 00:36:45,760
like that location signal I'm telling you about.

637
00:36:45,760 --> 00:36:49,040
We've got a really good idea of how that location signal is generated.

638
00:36:49,040 --> 00:36:50,640
And it's super interesting.

639
00:36:50,640 --> 00:36:55,520
Like the way that your brain rocks location of things is amazing.

640
00:36:55,520 --> 00:36:58,640
You probably don't, I don't have knowledge to explain it.

641
00:36:58,640 --> 00:37:02,320
But it's about grid cells, location cells and place cells and stuff like that.

642
00:37:02,320 --> 00:37:05,600
If anybody wants to go research that, there's some really interesting neuroscience papers

643
00:37:05,600 --> 00:37:07,200
coming out about grid cells.

644
00:37:07,200 --> 00:37:09,040
Okay, for example, I'll give you a little example.

645
00:37:09,040 --> 00:37:15,520
If you put a mouse in a box and you let it run around the box and you're monitoring its neurons,

646
00:37:15,520 --> 00:37:19,920
you'll see as it runs around the box and you trace where it goes,

647
00:37:19,920 --> 00:37:23,040
certain neurons will fire when it's in certain places.

648
00:37:23,040 --> 00:37:28,240
And those fire, and you can identify those cells that are greening whenever it's in place.

649
00:37:28,240 --> 00:37:29,360
Yeah, X, Y.

650
00:37:29,360 --> 00:37:31,200
Yeah, the specific neurons going to fire?

651
00:37:31,200 --> 00:37:31,680
Yes.

652
00:37:31,680 --> 00:37:32,400
Wow.

653
00:37:32,400 --> 00:37:36,240
And if you look at it, it forms this hexagonal grid.

654
00:37:36,240 --> 00:37:41,680
So there's this hexagonal pattern of neurons that are firing as you move through space,

655
00:37:41,680 --> 00:37:45,360
representing where you're at in the space that you're occupying.

656
00:37:46,000 --> 00:37:52,080
And we think that that interplay of neurons and that idea of neurons representing locations

657
00:37:52,080 --> 00:37:58,000
in space plays out at a bigger level to even represent objects in space too.

658
00:37:58,000 --> 00:37:58,240
Okay.

659
00:37:58,240 --> 00:38:03,120
Like you have an allocentric representation of any object that you can imagine.

660
00:38:03,120 --> 00:38:07,520
Allocentric meaning not related to where you are, not egocentric,

661
00:38:07,520 --> 00:38:09,760
but just like imagine a cup.

662
00:38:09,760 --> 00:38:11,840
I mean, that's an object that you have.

663
00:38:11,840 --> 00:38:15,600
And if you like used its center of gravity for whatever, as its center,

664
00:38:15,600 --> 00:38:20,080
you could define it entirely based upon all the sensory input that you've ever received

665
00:38:20,080 --> 00:38:23,520
about those objects that you've felt or seen or whatever.

666
00:38:23,520 --> 00:38:26,800
And we think that that has something to do with grid cells.

667
00:38:26,800 --> 00:38:32,720
That how those objects are stored, like the plate that how in 3D space they're defined

668
00:38:32,720 --> 00:38:36,080
is linked to the sensory input that we receive about them.

669
00:38:36,080 --> 00:38:40,560
And what cells are firing in space as we're imagining where we're touching on the object.

670
00:38:40,560 --> 00:38:40,800
Okay.

671
00:38:41,520 --> 00:38:42,080
Wow.

672
00:38:42,080 --> 00:38:43,760
Super, super interesting stuff.

673
00:38:44,400 --> 00:38:49,360
I will definitely make a note for folks to listen to the conversation with

674
00:38:49,360 --> 00:38:51,760
Francisco a couple of times before this one.

675
00:38:52,800 --> 00:38:55,760
Or maybe this one should be the prerequisite for that one, I don't know.

676
00:38:55,760 --> 00:38:58,560
Well, it's hopefully it's standalone, hopefully it's standalone.

677
00:39:00,080 --> 00:39:00,400
Awesome.

678
00:39:00,400 --> 00:39:01,360
Well, thanks so much, Matt.

679
00:39:01,360 --> 00:39:01,840
You're welcome.

680
00:39:01,840 --> 00:39:03,120
I appreciate the opportunity.

681
00:39:03,120 --> 00:39:03,760
Absolutely.

682
00:39:07,520 --> 00:39:08,560
All right, everyone.

683
00:39:08,560 --> 00:39:10,720
That's our show for today.

684
00:39:10,720 --> 00:39:15,360
Thanks so much for listening and for your continued feedback and support.

685
00:39:15,360 --> 00:39:19,920
For more information on Matt or any of the topics covered in this episode,

686
00:39:19,920 --> 00:39:24,080
head on over to twimlai.com slash talk slash 71.

687
00:39:25,040 --> 00:39:28,800
To follow along with our Strange Loop 2017 series,

688
00:39:28,800 --> 00:39:31,840
visit twimlai.com slash ST loop.

689
00:39:32,480 --> 00:39:36,800
Of course, you can send along your feedback or question via Twitter

690
00:39:36,800 --> 00:39:42,560
to at Twimlai or at Sam Charrington or leave a comment right on the show notes page.

691
00:39:42,560 --> 00:39:46,000
Thanks again to Nexosis for their sponsorship of the show.

692
00:39:46,000 --> 00:39:53,120
Check out twimlai.com slash talk slash 69 to hear my interview with the company founders.

693
00:39:53,120 --> 00:40:00,240
And visit nexosis.com slash twimble for more information and to try their API for free.

694
00:40:00,240 --> 00:40:12,720
Thanks again for listening and catch you next time.

