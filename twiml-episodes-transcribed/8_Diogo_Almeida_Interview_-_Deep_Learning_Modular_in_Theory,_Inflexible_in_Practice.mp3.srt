1
00:00:00,000 --> 00:00:16,060
Hello and welcome to another episode of Twimmel Talk, the podcast where I interview

2
00:00:16,060 --> 00:00:21,640
interesting people doing interesting things in machine learning and artificial intelligence.

3
00:00:21,640 --> 00:00:25,480
I'm your host Sam Charrington.

4
00:00:25,480 --> 00:00:29,480
The recording you're about to hear is part of a series of interviews I recorded live

5
00:00:29,480 --> 00:00:34,520
from the O'Reilly AI and Stratoconferences in New York City last month.

6
00:00:34,520 --> 00:00:38,520
I'll be sharing these interviews on the podcast over the next several weeks and I'm sure

7
00:00:38,520 --> 00:00:40,880
you'll enjoy them.

8
00:00:40,880 --> 00:00:46,160
This time I interview Diogo Almeida, senior data scientist at healthcare startup and

9
00:00:46,160 --> 00:00:47,680
LITIC.

10
00:00:47,680 --> 00:00:52,400
Diogo and I met at the AI conference where we delivered a great presentation on in the

11
00:00:52,400 --> 00:01:01,000
trenches deep learning titled deep learning modular in theory inflexible in practice.

12
00:01:01,000 --> 00:01:07,040
Diogo and I discussed the ideas he presented which are centered on the data, software,

13
00:01:07,040 --> 00:01:12,400
optimization and understanding issues surrounding deep learning.

14
00:01:12,400 --> 00:01:18,200
Diogo is also a past first place Kaggle competition winner and we spend some time discussing

15
00:01:18,200 --> 00:01:22,880
the competition he competed in and the approach he took to win it.

16
00:01:22,880 --> 00:01:25,760
Before we jump in, a bit of a listener warning.

17
00:01:25,760 --> 00:01:29,400
Our conversation gets pretty technical pretty quickly.

18
00:01:29,400 --> 00:01:34,120
I do try to make sure to summarize key points from time to time and I really think that

19
00:01:34,120 --> 00:01:37,280
if you hang in there, I'm sure you'll learn a ton.

20
00:01:37,280 --> 00:01:41,480
Of course, let me know how you like this level of detail.

21
00:01:41,480 --> 00:01:45,480
I'll be including links to Diogo and a bunch of the data sets and other things that we

22
00:01:45,480 --> 00:01:53,640
discuss in the show notes, which you can find at twimmolai.com slash talk slash eight.

23
00:01:53,640 --> 00:01:58,380
Also as is the case with my other field recordings, there's unfortunately a bit of unavoidable

24
00:01:58,380 --> 00:02:01,360
background noise, sorry for that.

25
00:02:01,360 --> 00:02:04,120
And now on to the show.

26
00:02:04,120 --> 00:02:11,040
Alright, hey everyone, I'm here at the O'Reilly AI conference and I'm sitting with Diogo

27
00:02:11,040 --> 00:02:16,080
Almeida, who just did a really interesting talk on deep learning and he was kind enough

28
00:02:16,080 --> 00:02:21,120
to sit down with us and talk a little bit about what he talked about.

29
00:02:21,120 --> 00:02:23,320
Diogo, why don't you introduce yourself?

30
00:02:23,320 --> 00:02:24,320
Cool.

31
00:02:24,320 --> 00:02:25,320
I'm Diogo Almeida.

32
00:02:25,320 --> 00:02:30,760
I work at this super cool medical deep learning startup where we work on giving like really

33
00:02:30,760 --> 00:02:35,600
accurate, really fast, really safe medical diagnoses and this is something we hope will completely

34
00:02:35,600 --> 00:02:38,080
change the world.

35
00:02:38,080 --> 00:02:41,360
Before that, in past life, I was a math lead.

36
00:02:41,360 --> 00:02:45,600
So I broke a 13 year losing streak for the Philippines in the international math Olympiad

37
00:02:45,600 --> 00:02:52,680
was in the top team in the world at the interdisciplinary competition in modeling and there's a website

38
00:02:52,680 --> 00:02:57,080
for machine learning competitions called Kaggle that I won first place on in one competition

39
00:02:57,080 --> 00:02:58,080
as well.

40
00:02:58,080 --> 00:02:59,080
What was that?

41
00:02:59,080 --> 00:03:03,760
This was in 2013 because the cause effect bears challenge.

42
00:03:03,760 --> 00:03:04,760
Tell us about that.

43
00:03:04,760 --> 00:03:10,240
Oh, it's just a very weird challenge where in most machine learning, you have like tabular

44
00:03:10,240 --> 00:03:11,240
data.

45
00:03:11,240 --> 00:03:15,520
So you know, like you have columns of features, rows of observations.

46
00:03:15,520 --> 00:03:20,200
And in this problem, your data was pairs of sequences.

47
00:03:20,200 --> 00:03:24,760
So you have something like altitude and like one observation is like altitude and height

48
00:03:24,760 --> 00:03:30,320
and you have like a pair of, sorry, a sequence of pairs of like which altitudes correspond to

49
00:03:30,320 --> 00:03:32,680
which heist in some unordered manner.

50
00:03:32,680 --> 00:03:38,000
The idea was given this, you're supposed to predict whether altitude is causes height

51
00:03:38,000 --> 00:03:40,240
or height, sorry, the altitude and height were the same thing.

52
00:03:40,240 --> 00:03:41,600
I meant altitude and temperature.

53
00:03:41,600 --> 00:03:42,600
Right.

54
00:03:42,600 --> 00:03:45,800
So you're supposed to predict if altitude causes temperature, it causes altitude and obviously

55
00:03:45,800 --> 00:03:48,760
that altitude causes temperature right for us.

56
00:03:48,760 --> 00:03:54,360
But there's a lot of like very complicated tasks that we don't know the answer to and

57
00:03:54,360 --> 00:03:58,720
it's kind of like the basic task is to, if you know the saying correlation doesn't apply

58
00:03:58,720 --> 00:04:01,880
causation, it's supposed to do the opposite of that.

59
00:04:01,880 --> 00:04:06,680
You're supposed to figure out how the correlation implies causation, which is, it's extremely

60
00:04:06,680 --> 00:04:08,800
useful because you have like lots and lots of observational data.

61
00:04:08,800 --> 00:04:09,800
Right.

62
00:04:09,800 --> 00:04:11,320
It's very hard to have like a controlled study.

63
00:04:11,320 --> 00:04:15,560
So the more accurate we can get a view of the world from purely observational data,

64
00:04:15,560 --> 00:04:21,920
the more we can either have informed priors before running the control study or figure out

65
00:04:21,920 --> 00:04:24,200
how to order the controlled study in an appropriate way.

66
00:04:24,200 --> 00:04:25,200
Okay.

67
00:04:25,200 --> 00:04:30,600
And this is also the kind of analysis you would use for like a root cause analysis or something

68
00:04:30,600 --> 00:04:33,800
in like an IOT use case where you've got all these observations and you're trying to

69
00:04:33,800 --> 00:04:36,800
figure out what the underlying condition is or.

70
00:04:36,800 --> 00:04:40,360
I'm not as familiar with that.

71
00:04:40,360 --> 00:04:46,400
There are, there was traditional statistical work and there actually was a background for

72
00:04:46,400 --> 00:04:50,360
this topic, but I kind of didn't pay much attention to that because I kind of went my

73
00:04:50,360 --> 00:04:54,400
own way and it was much more for fun than for winning.

74
00:04:54,400 --> 00:04:58,520
And winning was a very nice side effect.

75
00:04:58,520 --> 00:05:04,840
And I went through a much more like software oriented way of just like build a really

76
00:05:04,840 --> 00:05:10,000
complicated powerful model and have it solve this based on like rather than like hand

77
00:05:10,000 --> 00:05:11,320
engineering stuff.

78
00:05:11,320 --> 00:05:16,040
Why not just like automatically engineer a lot of informative variables and then solve

79
00:05:16,040 --> 00:05:17,040
it with that.

80
00:05:17,040 --> 00:05:18,040
Okay.

81
00:05:18,040 --> 00:05:24,200
So can you walk us through the process like how do you, how did you formulate a methodology

82
00:05:24,200 --> 00:05:25,200
for attacking them?

83
00:05:25,200 --> 00:05:28,280
Was this your first Kaggle competition or had you been doing that for a while?

84
00:05:28,280 --> 00:05:29,280
My first serious one.

85
00:05:29,280 --> 00:05:33,440
I've done like one or two before that I didn't really like really spend much time on.

86
00:05:33,440 --> 00:05:36,440
But like you know you quit like after two days because it turns out your teammates were

87
00:05:36,440 --> 00:05:39,040
in useful or something like that.

88
00:05:39,040 --> 00:05:44,160
So I have like played with it before but I've never really gone all out until this one.

89
00:05:44,160 --> 00:05:49,120
So my methodology was, well some background is that there are like statistical tests that

90
00:05:49,120 --> 00:05:55,880
people use that did very well in this task and or sorry that people used to use in this

91
00:05:55,880 --> 00:06:00,680
task and put it roughly in perspective these got like 0.6ish AUC.

92
00:06:00,680 --> 00:06:04,840
So if you see a paper in nature science about a new test for causality it probably gets

93
00:06:04,840 --> 00:06:05,840
around 0.6ish AUC.

94
00:06:05,840 --> 00:06:06,840
Okay.

95
00:06:06,840 --> 00:06:10,720
AUC for those that don't know is the area under the curve and that's a performance metric.

96
00:06:10,720 --> 00:06:11,720
Yeah.

97
00:06:11,720 --> 00:06:16,280
So we were solving a ranking problem or we were trying to rank the outputs given that we

98
00:06:16,280 --> 00:06:20,600
know which ones were, which ones caused each other to a little bit of complicated metric

99
00:06:20,600 --> 00:06:22,720
because we actually had three output classes.

100
00:06:22,720 --> 00:06:26,840
So we did like a bidirectional AUC but that doesn't really matter much.

101
00:06:26,840 --> 00:06:30,840
And so these tests we did like 0.6ish they're roughly a single feature because it's just

102
00:06:30,840 --> 00:06:34,320
the prediction you extracted directly from the data.

103
00:06:34,320 --> 00:06:40,480
The most of the other competitors in like the top 10 had you know tens of features or

104
00:06:40,480 --> 00:06:46,560
something like that and the second place where I think had like a whooping like 100 something

105
00:06:46,560 --> 00:06:47,560
features.

106
00:06:47,560 --> 00:06:48,560
Okay.

107
00:06:48,560 --> 00:06:58,040
I had 50,000 so what I did was I found like a very simple way of determining causality

108
00:06:58,040 --> 00:07:05,960
which would be the rationale would be if x causes y then y is a function of x you know

109
00:07:05,960 --> 00:07:07,880
there's noise in there somewhere.

110
00:07:07,880 --> 00:07:13,160
So roughly you can tell how good one is a function of the other based on how well they

111
00:07:13,160 --> 00:07:15,720
can be approximated by functions.

112
00:07:15,720 --> 00:07:21,000
And this is kind of like a very vague like recipe for how to create these features but the

113
00:07:21,000 --> 00:07:25,320
idea is rather than you know hard coding statistical tests like you know like add a Gaussian

114
00:07:25,320 --> 00:07:29,840
integrate this thing out whatever I just figure that we have an entire field of curve fitting

115
00:07:29,840 --> 00:07:34,720
which is called machine learning right and these are often like built after natural like

116
00:07:34,720 --> 00:07:36,360
very natural priors.

117
00:07:36,360 --> 00:07:40,320
So the idea would be try like a ton of machine learning algorithms all of the ones that were

118
00:07:40,320 --> 00:07:41,840
computationally feasible.

119
00:07:41,840 --> 00:07:47,440
Try a different metrics for what fit means because fit is it's it's kind of like a not like

120
00:07:47,440 --> 00:07:52,720
a very exact term and like throw like these are all the features now throw them all into

121
00:07:52,720 --> 00:08:01,240
like big boost decision tree train this thing for a week on like a 50 core machine and

122
00:08:01,240 --> 00:08:07,320
then you know take a nap the entire time so that was roughly my solution.

123
00:08:07,320 --> 00:08:14,520
Wow and so the solution was was primarily based around the boosted decision tree as opposed

124
00:08:14,520 --> 00:08:21,200
to some super complex ensemble or something like that actually it's a weird story that

125
00:08:21,200 --> 00:08:26,520
for this competition I was so far ahead for almost all the competition I didn't even

126
00:08:26,520 --> 00:08:27,520
try.

127
00:08:27,520 --> 00:08:35,960
So the what was it like for basically everything beyond the last week like yep like maybe

128
00:08:35,960 --> 00:08:39,240
a month or a month and a half before I even started the competition late I was like

129
00:08:39,240 --> 00:08:43,240
so far ahead that the gap between like me and second place was like the equivalent of

130
00:08:43,240 --> 00:08:46,280
like you know second and like 50 and there's something like that.

131
00:08:46,280 --> 00:08:49,960
So I was like feeling really confident and I actually stopped paying attention to this

132
00:08:49,960 --> 00:08:54,480
because I felt that like oh this is going to be easy right.

133
00:08:54,480 --> 00:08:57,720
But then during the last week you know someone you know people started sharing their solutions

134
00:08:57,720 --> 00:09:03,360
like I only got 10 or something here the features I used in all of a sudden like everyone

135
00:09:03,360 --> 00:09:08,080
started rising and this is definitely basically by creating ensembles of everyone's

136
00:09:08,080 --> 00:09:12,240
everyone's a solution like people like Lee kind of hinted at what I think it was only

137
00:09:12,240 --> 00:09:14,960
one person but like they had like a lot of good stuff in there that other people started

138
00:09:14,960 --> 00:09:15,960
using.

139
00:09:15,960 --> 00:09:19,440
And once people were getting performance they like make more of it or something like that.

140
00:09:19,440 --> 00:09:24,600
So people are starting to rise right and like I didn't have even ensemble this far and

141
00:09:24,600 --> 00:09:29,160
I unfortunately had a model that took out like a week to train like I said so and I only

142
00:09:29,160 --> 00:09:31,240
had a one week left for the competition.

143
00:09:31,240 --> 00:09:38,280
So I decided that I tried like a few last minutes attempts at ensembling but nothing beat

144
00:09:38,280 --> 00:09:41,520
my like my super big one week long model.

145
00:09:41,520 --> 00:09:44,960
And so I just stuck with that thing and that ended up actually winning and it actually

146
00:09:44,960 --> 00:09:50,920
was very scary because people ended up passing me on the training on the validation leader

147
00:09:50,920 --> 00:09:51,920
board.

148
00:09:51,920 --> 00:09:52,920
Yeah.

149
00:09:52,920 --> 00:09:56,040
But in test leader board it was like it was completely flipped because by they overfit

150
00:09:56,040 --> 00:10:00,000
yeah they like they had like hundreds of submissions while like my best submission was

151
00:10:00,000 --> 00:10:05,760
like my sub 10th because like it was a very like hands off competition for me.

152
00:10:05,760 --> 00:10:13,320
I cared about it a lot and I like I wrote like lots of software that was I thought nice

153
00:10:13,320 --> 00:10:17,320
but like I was really I really really thought that would have been like an absolute slam

154
00:10:17,320 --> 00:10:18,320
down.

155
00:10:18,320 --> 00:10:19,320
Okay.

156
00:10:19,320 --> 00:10:20,320
So it's exciting though.

157
00:10:20,320 --> 00:10:21,320
Okay.

158
00:10:21,320 --> 00:10:25,680
So where did the 50,000 features come from?

159
00:10:25,680 --> 00:10:29,640
So you can imagine like exponential growth when you're just trying like every combination

160
00:10:29,640 --> 00:10:31,160
of this with every combination of this.

161
00:10:31,160 --> 00:10:32,160
Yep.

162
00:10:32,160 --> 00:10:35,760
There was like every combination of metric that I can think of every combination of machine

163
00:10:35,760 --> 00:10:38,960
learning algorithm that was like computationally tractable.

164
00:10:38,960 --> 00:10:43,280
There was like symmetric features so you could like augment your thing with like difference

165
00:10:43,280 --> 00:10:48,120
features because like it doesn't matter which extra bias right.

166
00:10:48,120 --> 00:10:53,880
There was a a nuanced thing that I don't normally explain when I talk about the competition

167
00:10:53,880 --> 00:10:59,280
which is not all of the input was numerical some of it was categorical.

168
00:10:59,280 --> 00:11:02,720
And like it you just can't like throw categorical data into a numerical algorithm right.

169
00:11:02,720 --> 00:11:03,720
Right.

170
00:11:03,720 --> 00:11:04,720
So it becomes actually a complicated problem.

171
00:11:04,720 --> 00:11:07,640
How do you compare numerical different ways of calibrating your bands or something like

172
00:11:07,640 --> 00:11:08,640
that?

173
00:11:08,640 --> 00:11:12,280
Well, I mean you can it's very easy to convert numerical to categorical but you lose a lot

174
00:11:12,280 --> 00:11:13,800
of information from that drive.

175
00:11:13,800 --> 00:11:19,960
So what I did was I did different ways of converting from like like this is like a categorical

176
00:11:19,960 --> 00:11:21,880
numerical pair metric.

177
00:11:21,880 --> 00:11:27,400
So this stuff like compare you know compute sorry convert numerical to categorical via

178
00:11:27,400 --> 00:11:29,840
like clustering or binning or something right.

179
00:11:29,840 --> 00:11:33,280
And then you know when you want to convert categorical to numerical you do something like

180
00:11:33,280 --> 00:11:39,880
the PCA you know like get the first principle components or something like that or projection

181
00:11:39,880 --> 00:11:41,760
to the first principle components.

182
00:11:41,760 --> 00:11:44,360
And I basically are just looping through all of these things.

183
00:11:44,360 --> 00:11:48,280
So you can imagine like a lot of less before loops and the end I had a bunch of them.

184
00:11:48,280 --> 00:11:51,080
So like that ended up with like 50,000 ish.

185
00:11:51,080 --> 00:11:55,920
And I also skipped a detail there which is I also used a feature selection algorithm in

186
00:11:55,920 --> 00:11:59,960
Earth like make it a little bit smaller, which help performance a bit but it ended up not

187
00:11:59,960 --> 00:12:00,960
being important.

188
00:12:00,960 --> 00:12:04,760
So I usually am it but for the sake of clarity that was also done.

189
00:12:04,760 --> 00:12:05,760
Okay.

190
00:12:05,760 --> 00:12:06,760
Okay.

191
00:12:06,760 --> 00:12:07,760
Wow.

192
00:12:07,760 --> 00:12:08,760
That sounds pretty cool.

193
00:12:08,760 --> 00:12:10,760
And now that was a little bit of a digression.

194
00:12:10,760 --> 00:12:11,760
Yeah.

195
00:12:11,760 --> 00:12:12,760
Complete digression.

196
00:12:12,760 --> 00:12:13,760
Yeah.

197
00:12:13,760 --> 00:12:15,160
Interesting story though.

198
00:12:15,160 --> 00:12:16,160
Yeah.

199
00:12:16,160 --> 00:12:17,160
Absolutely.

200
00:12:17,160 --> 00:12:18,160
Absolutely.

201
00:12:18,160 --> 00:12:19,680
It's actually generalized to new problems as well.

202
00:12:19,680 --> 00:12:23,760
I believe the competition organizer was applying it to some sort of biology problems and

203
00:12:23,760 --> 00:12:26,160
they were showing that they'd actually predict causality in that as well.

204
00:12:26,160 --> 00:12:27,160
Oh really?

205
00:12:27,160 --> 00:12:31,720
So yeah, hopefully that kind of thing could be really useful.

206
00:12:31,720 --> 00:12:32,720
Oh nice.

207
00:12:32,720 --> 00:12:33,720
Nice.

208
00:12:33,720 --> 00:12:36,120
But what you were talking about here was deep learning.

209
00:12:36,120 --> 00:12:37,120
Yeah.

210
00:12:37,120 --> 00:12:38,880
And it was not deep at all.

211
00:12:38,880 --> 00:12:42,560
And I didn't catch all of your talk.

212
00:12:42,560 --> 00:12:44,440
I caught the last bit of it.

213
00:12:44,440 --> 00:12:50,160
But it seemed like what you were going through was kind of a bunch of war stories lessons

214
00:12:50,160 --> 00:12:56,000
learned like, you know, you hear a lot about deep learning, you know, but there are a lot

215
00:12:56,000 --> 00:13:00,400
of things that people broadly believe about deep learning that actually are false.

216
00:13:00,400 --> 00:13:06,160
And why don't you explain kind of what your intent was for the talk and kind of walk

217
00:13:06,160 --> 00:13:09,160
us through, you know, an overview of what you're presenting.

218
00:13:09,160 --> 00:13:10,160
Cool.

219
00:13:10,160 --> 00:13:13,800
So the way I see it is like there's these two competing these views on deep learning,

220
00:13:13,800 --> 00:13:18,200
like extreme views, which is deep learning will solve all our problems and deep learning

221
00:13:18,200 --> 00:13:19,200
is complete garbage.

222
00:13:19,200 --> 00:13:23,240
Sorry, it's all hype, kind of exaggeration, but maybe for exaggerating views, you can

223
00:13:23,240 --> 00:13:24,240
say that.

224
00:13:24,240 --> 00:13:27,640
And there's evidence for each of these views, you know, like there's some amazing results

225
00:13:27,640 --> 00:13:28,640
of deep learning.

226
00:13:28,640 --> 00:13:31,280
There's some made like extremely poor results on deep learning.

227
00:13:31,280 --> 00:13:32,280
Right.

228
00:13:32,280 --> 00:13:35,320
And the idea is that like these are not as informative of the stuff in the middle.

229
00:13:35,320 --> 00:13:38,280
So the idea is like you draw all of this evidence in like this one dimensional plane.

230
00:13:38,280 --> 00:13:39,280
Yeah.

231
00:13:39,280 --> 00:13:42,480
And you like try to like draw like a max margin hyper plane.

232
00:13:42,480 --> 00:13:45,320
You might get like you this interesting decision boundary because like this is where the

233
00:13:45,320 --> 00:13:46,320
interesting stuff lies.

234
00:13:46,320 --> 00:13:49,560
Like this is stuff that's going to be moving slowly over time if deep learning is doing

235
00:13:49,560 --> 00:13:50,560
well, right?

236
00:13:50,560 --> 00:13:55,200
Or the other way if people are starting to like find all sorts of failure cases.

237
00:13:55,200 --> 00:14:00,480
And the idea would be if we talk about like these examples and like the edges of our understanding

238
00:14:00,480 --> 00:14:04,520
or the edges of our everything or like edges of you know, like all the things that are limiting

239
00:14:04,520 --> 00:14:09,560
deep learning nowadays and like keeping us from solving all of our dreams, that can hopefully

240
00:14:09,560 --> 00:14:14,280
give people an impression of like what everything else is like because it's like just very extreme

241
00:14:14,280 --> 00:14:15,600
on the other end to this spectrum.

242
00:14:15,600 --> 00:14:20,360
And I feel like that's just not very much talked about because like you said, like a lot

243
00:14:20,360 --> 00:14:25,120
of people are on the deep learning hype drain or kind of being sad at home and like being

244
00:14:25,120 --> 00:14:31,840
grumpy because now all of the all of the questioners are silent stride.

245
00:14:31,840 --> 00:14:37,280
So if we kind of map out what the corner cases are and the failure mose and things like

246
00:14:37,280 --> 00:14:40,840
that to help us push forward our understanding of this thing is the basic premise.

247
00:14:40,840 --> 00:14:41,840
Yeah.

248
00:14:41,840 --> 00:14:44,960
And kind of like acknowledging it also helps.

249
00:14:44,960 --> 00:14:48,720
I don't think what I did was the greatest acknowledgement of it, but I think it was a

250
00:14:48,720 --> 00:14:55,200
more thorough one than I've seen before and realistic especially in that I think that

251
00:14:55,200 --> 00:15:01,040
sometimes just understanding your problem really well really helps you to solve that problem.

252
00:15:01,040 --> 00:15:08,000
So I know now that I mean like I do research as well and the stuff's very important to

253
00:15:08,000 --> 00:15:09,520
me.

254
00:15:09,520 --> 00:15:14,720
And by looking at it from like a kind of a higher level, I can kind of see better like

255
00:15:14,720 --> 00:15:19,240
this seems like something that looks really promising to me or this doesn't seem promising

256
00:15:19,240 --> 00:15:20,240
at all.

257
00:15:20,240 --> 00:15:21,240
Right.

258
00:15:21,240 --> 00:15:25,840
Like for example, one of the problems with deep learning nowadays is everything's very

259
00:15:25,840 --> 00:15:26,840
local.

260
00:15:26,840 --> 00:15:27,840
Right.

261
00:15:27,840 --> 00:15:31,480
Like you get local and what's that use the gradient, right?

262
00:15:31,480 --> 00:15:38,960
Or maybe higher order driven things, but they for practical purposes use the gradient

263
00:15:38,960 --> 00:15:45,520
and this can be insufficient for some applications, right?

264
00:15:45,520 --> 00:15:49,920
Going to a higher level, maybe it can start with a lower level, right?

265
00:15:49,920 --> 00:15:53,880
Like S3D doesn't work for my spatial transformer network.

266
00:15:53,880 --> 00:15:54,880
This is unfortunate.

267
00:15:54,880 --> 00:15:58,840
Like, let me try Adam, let me try RMS prop, but if you go to a higher level, you realize

268
00:15:58,840 --> 00:16:02,520
that the problem is the local learning, the spatial transformer network, not necessarily

269
00:16:02,520 --> 00:16:03,520
the gradient descent.

270
00:16:03,520 --> 00:16:08,800
So to tell us about spatial transformer networks, yeah, so this is just one example I

271
00:16:08,800 --> 00:16:14,840
use of a kind of network that it's very easy to see the issues of local learning with.

272
00:16:14,840 --> 00:16:17,120
It's very nice because it's a, it's a differentiable network.

273
00:16:17,120 --> 00:16:21,120
It's very easy to see exploration problems in reinforcement learning domains, but this

274
00:16:21,120 --> 00:16:27,720
is one that you have a derivative of and it should be easier to optimize and it is, but

275
00:16:27,720 --> 00:16:33,160
you sometimes don't get what exactly you, it doesn't like fulfill its full potential.

276
00:16:33,160 --> 00:16:38,360
So are you kind of seeing that there are a lot of people coming into the space that,

277
00:16:38,360 --> 00:16:42,280
you know, that, you know, try to throw deep learning at a given problem.

278
00:16:42,280 --> 00:16:47,160
The common way of solving it is using stochastic gradient descent and they don't really think

279
00:16:47,160 --> 00:16:52,160
about, you know, how that's working and that it's, you know, finding a local optimization

280
00:16:52,160 --> 00:16:55,920
and there are some problems that, you know, for which they get kind of stuck in that local

281
00:16:55,920 --> 00:16:56,920
and...

282
00:16:56,920 --> 00:16:58,160
That is unfortunately the case.

283
00:16:58,160 --> 00:17:02,720
Like, I have seen many people introduce to deep learning who think that let's stitch

284
00:17:02,720 --> 00:17:06,760
together an architecture that's differentiable, you know, bingo, bingo, call it a day.

285
00:17:06,760 --> 00:17:08,920
They've like solved problem X, right?

286
00:17:08,920 --> 00:17:17,600
Like, they realize the limitations of requiring large data sets, but they think that that's

287
00:17:17,600 --> 00:17:18,600
what it amounts to.

288
00:17:18,600 --> 00:17:21,600
And I think often, very often times, it doesn't.

289
00:17:21,600 --> 00:17:26,440
So back to spatial transformer networks, what they are is basically, instead of like a single

290
00:17:26,440 --> 00:17:29,960
network that learns how to classify an image, you have two networks.

291
00:17:29,960 --> 00:17:33,680
One of them learns which part of the image to look at and the other part takes what

292
00:17:33,680 --> 00:17:37,600
that network looked at and does the classification on it.

293
00:17:37,600 --> 00:17:41,920
And this is a huge advantage because a lot of the times your input image might be really

294
00:17:41,920 --> 00:17:45,120
large and you don't want to run the network overall, all of it.

295
00:17:45,120 --> 00:17:48,600
It might have like unnecessary information.

296
00:17:48,600 --> 00:17:52,280
It might be really useful to like, co-localize, so like, have the where as well as the

297
00:17:52,280 --> 00:17:53,280
what.

298
00:17:53,280 --> 00:17:57,600
So there's really good reasons to use this and in fact, for medical problems, if it worked

299
00:17:57,600 --> 00:17:59,040
well, I would use it for everything.

300
00:17:59,040 --> 00:18:01,880
Number one, the number two is if it worked well, I would use it for every computer vision

301
00:18:01,880 --> 00:18:02,880
problem.

302
00:18:02,880 --> 00:18:06,600
Because what these spatial transformer networks can do is not only find the region, but it

303
00:18:06,600 --> 00:18:11,480
can also transform the region into a canonical location.

304
00:18:11,480 --> 00:18:15,880
So rather than having to learn filters of like cats at every orientation, you might have

305
00:18:15,880 --> 00:18:20,000
to learn filters of cats at only one orientation, which like would reduce and result in like

306
00:18:20,000 --> 00:18:24,000
much better data and parameter efficiency.

307
00:18:24,000 --> 00:18:29,320
But back to the issue here is that you have these two networks that are, they're not competing,

308
00:18:29,320 --> 00:18:34,040
but they're working together, but they're only using the current network, the current

309
00:18:34,040 --> 00:18:37,680
other network as its source of signal basically.

310
00:18:37,680 --> 00:18:42,040
So if your classification network gets really good early on in training, your localization

311
00:18:42,040 --> 00:18:43,720
network gets stuck in this optimal, right?

312
00:18:43,720 --> 00:18:47,160
Because like if it changes anything at least a little bit, your classification network

313
00:18:47,160 --> 00:18:48,160
will do worse.

314
00:18:48,160 --> 00:18:51,880
So like the gradient tells it like, hey, hey, just stay where you are, you're pretty good

315
00:18:51,880 --> 00:18:54,080
or move you all around the small region, right?

316
00:18:54,080 --> 00:18:56,400
Which might be very far from the intended purpose, right?

317
00:18:56,400 --> 00:18:59,960
Like correctly like zooming all the way into the thing you care about and like rotating

318
00:18:59,960 --> 00:19:01,240
it a lot.

319
00:19:01,240 --> 00:19:06,640
And on the other hand, if the spatial transformer network converges early, so imagine the classification

320
00:19:06,640 --> 00:19:12,120
network is garbage, it might zoom into like regions of the image that are just independent

321
00:19:12,120 --> 00:19:15,160
of the class, but makes the classification network tends to perform a little bit better

322
00:19:15,160 --> 00:19:16,440
on.

323
00:19:16,440 --> 00:19:21,080
So it might like, for example, if you're trying to classify kinds of dogs or like image

324
00:19:21,080 --> 00:19:26,640
net, and it turns out like your classifier starts out like just being good at telling

325
00:19:26,640 --> 00:19:30,920
grass means dog, and the localizer notices and like just zooms into the grass, right?

326
00:19:30,920 --> 00:19:32,840
Like those zooms in, zoom grass.

327
00:19:32,840 --> 00:19:35,000
And basically you've cut the dog out of the image.

328
00:19:35,000 --> 00:19:38,360
And the moment you've cut the dog out of the image, you get no gradient signal.

329
00:19:38,360 --> 00:19:41,560
And when you have no gradient signal, you're stuck there forever.

330
00:19:41,560 --> 00:19:47,040
And this is a problem that people just don't really like to acknowledge in networks, right?

331
00:19:47,040 --> 00:19:52,440
That's actually a very complicated relationship, because now you need to like maintain a balance

332
00:19:52,440 --> 00:19:53,760
and all of that.

333
00:19:53,760 --> 00:19:55,640
And I don't think people even know how to do that.

334
00:19:55,640 --> 00:19:58,040
Like people don't know how to do it with a generative adversarial network either, which

335
00:19:58,040 --> 00:19:59,760
is another example I gave of this.

336
00:19:59,760 --> 00:20:00,760
Yeah.

337
00:20:00,760 --> 00:20:01,760
Yeah.

338
00:20:01,760 --> 00:20:02,760
Huh.

339
00:20:02,760 --> 00:20:08,040
So what was the overall structure of your talk?

340
00:20:08,040 --> 00:20:12,680
So the title of the talk was deep learning, modular, and theory, and flexible in practice.

341
00:20:12,680 --> 00:20:19,160
So I first wanted to talk about the successes of deep learning, rather to show that deep learning

342
00:20:19,160 --> 00:20:22,280
is very modular and it can do a lot of things.

343
00:20:22,280 --> 00:20:25,760
And you know, get them into the mode like, wow, we can solve everything.

344
00:20:25,760 --> 00:20:31,400
And I actually think that I had a somewhat bold claim to end that first part, which is

345
00:20:31,400 --> 00:20:39,120
that deep learning, today's deep learning components can solve any problem, any like

346
00:20:39,120 --> 00:20:45,240
a computable problem, if you ignore the practical aspects, which would be, I mean, I think

347
00:20:45,240 --> 00:20:46,840
it's interesting to point out, right?

348
00:20:46,840 --> 00:20:49,840
Because then now that you isolate that, you know that the practical aspects are the issue,

349
00:20:49,840 --> 00:20:50,840
right?

350
00:20:50,840 --> 00:20:51,840
Right.

351
00:20:51,840 --> 00:20:57,320
And those practical aspects are data software optimization, in probably order of difficulty

352
00:20:57,320 --> 00:21:00,640
of how to understand them.

353
00:21:00,640 --> 00:21:05,680
And the latter part of the talk I talked about, these issues with deep learning like specifically

354
00:21:05,680 --> 00:21:11,040
data software optimization and a final section of understanding, just because I wanted to

355
00:21:11,040 --> 00:21:16,440
point out that while understanding is not necessary for like getting things to work,

356
00:21:16,440 --> 00:21:21,320
which maybe is what we care about, understanding is very necessary to make progress, right?

357
00:21:21,320 --> 00:21:24,720
And we just, it's amazing how little we understand about anything.

358
00:21:24,720 --> 00:21:27,720
Well, let's come back to that and maybe walk through the different sections.

359
00:21:27,720 --> 00:21:33,600
So data, walk us through the points that you were driving home around that.

360
00:21:33,600 --> 00:21:39,840
Okay, so from a super high level, it's that neural networks are extremely data-efficient

361
00:21:39,840 --> 00:21:42,040
and they don't have to be that way.

362
00:21:42,040 --> 00:21:45,440
And data efficiencies, the root cause of all problems, because if we were data-efficient,

363
00:21:45,440 --> 00:21:48,240
the size of data sets wouldn't matter, right?

364
00:21:48,240 --> 00:21:54,200
The data sets we use are kind of flawed in that, like they have known issues that, you

365
00:21:54,200 --> 00:22:00,680
know researchers know about, that they're noisy or like what kinds of known issues.

366
00:22:00,680 --> 00:22:05,680
Like, Pantry Bank is a very small data set, therefore making bigger networks is not

367
00:22:05,680 --> 00:22:11,360
very helpful, because it overfits, therefore you should generally only publish regularization

368
00:22:11,360 --> 00:22:13,920
research on it or something like that.

369
00:22:13,920 --> 00:22:19,480
So you're referring primarily to kind of the known data sets, that kind of thing.

370
00:22:19,480 --> 00:22:22,720
That's the kind of things that, you know, like the mainstream deep learning researchers

371
00:22:22,720 --> 00:22:27,280
publish on to keep into them, hey, I have something cool, use my thing.

372
00:22:27,280 --> 00:22:31,600
And that is, I mean, it's important, right?

373
00:22:31,600 --> 00:22:35,120
Like the alternative is publishing and they said no one knows about, which is also very

374
00:22:35,120 --> 00:22:37,160
hard to get any information from.

375
00:22:37,160 --> 00:22:43,480
But one has kind of, it's almost like a reproducibility kind of issue where there are elements that

376
00:22:43,480 --> 00:22:49,680
are inherent to the data set that, you know, drive towards or require a certain class

377
00:22:49,680 --> 00:22:50,680
of solution.

378
00:22:50,680 --> 00:22:58,000
Yeah, it's a horrible state of affairs where, like you need to, like, if you, you know,

379
00:22:58,000 --> 00:23:00,280
you read a paper, the paper usually has the high level, it doesn't have all the low

380
00:23:00,280 --> 00:23:03,640
little details, that's what the code is for, and you implement the paper exactly as it

381
00:23:03,640 --> 00:23:04,960
says.

382
00:23:04,960 --> 00:23:09,120
And it gets not anywhere near close to what they had, right?

383
00:23:09,120 --> 00:23:14,440
And you're like, yo, what the F. And then, you know, you, maybe you email the authors,

384
00:23:14,440 --> 00:23:16,800
maybe they eventually reach the source code, and you run the source code, because you

385
00:23:16,800 --> 00:23:17,800
don't believe them.

386
00:23:17,800 --> 00:23:22,040
Wow, this reproducibility is exactly what the author said, and it turns out, like, it just

387
00:23:22,040 --> 00:23:23,720
has like a bunch of magic hyper parameters.

388
00:23:23,720 --> 00:23:27,200
Like you said, you know, L2 regularization to this, you need this learning rate schedule

389
00:23:27,200 --> 00:23:28,200
for sure.

390
00:23:28,200 --> 00:23:33,320
Use this optimizer, and also preprocess your data set in this way and sample it in this

391
00:23:33,320 --> 00:23:34,480
way.

392
00:23:34,480 --> 00:23:37,920
And like, these are all things that you really want to be robust to, right?

393
00:23:37,920 --> 00:23:40,320
And you just, you just aren't, right?

394
00:23:40,320 --> 00:23:45,520
Like that is, it's a very unfortunate, like, aspect of the world, right?

395
00:23:45,520 --> 00:23:51,160
Like, you're put into this position where, um, if you don't do, you know, if you don't

396
00:23:51,160 --> 00:23:55,160
play the game, you never get to the art results, and people don't listen to you.

397
00:23:55,160 --> 00:23:59,880
If you do play the game, um, I mean, some people listen to you, but some don't, because

398
00:23:59,880 --> 00:24:05,000
they know the game, but then, like, it's the only way to get people to see your thing.

399
00:24:05,000 --> 00:24:09,800
And then by the game, you mean in terms of the researchers, like they're driven to publish,

400
00:24:09,800 --> 00:24:14,000
you know, you know, win in the competitions for whichever data set that they're looking

401
00:24:14,000 --> 00:24:15,000
at.

402
00:24:15,000 --> 00:24:17,440
And then you're in the competition, but it's usually like, you want to get people interested

403
00:24:17,440 --> 00:24:18,440
in your papers.

404
00:24:18,440 --> 00:24:19,440
Yeah.

405
00:24:19,440 --> 00:24:22,440
And it's very different if you just didn't care and you wanted to publish interesting things,

406
00:24:22,440 --> 00:24:23,440
right?

407
00:24:23,440 --> 00:24:27,440
But if you want to get eyeballs, sometimes, like, unless you're already a respected person,

408
00:24:27,440 --> 00:24:29,680
it's kind of what you have to do, right?

409
00:24:29,680 --> 00:24:35,040
So, um, like, I did, like, uh, it's sometimes that kind of thing is important.

410
00:24:35,040 --> 00:24:40,080
I think that it's kind of very qualitative thing, um, which is unfortunate in the data world

411
00:24:40,080 --> 00:24:43,640
that they get to get a feel of a data set, like when this data set's starting to get, like,

412
00:24:43,640 --> 00:24:47,080
really overfit, um, that, um, perhaps it's not useful anymore.

413
00:24:47,080 --> 00:24:51,800
And I feel like some researchers like qualitatively feel that about, like, CIFAR 10 and CIFAR

414
00:24:51,800 --> 00:24:52,800
100.

415
00:24:52,800 --> 00:24:53,800
Especially CIFAR 10.

416
00:24:53,800 --> 00:24:57,240
I'm not 100% sure about CIFAR 100 as much as that data set.

417
00:24:57,240 --> 00:25:02,880
Um, this is a data set of 32 by 32 RGB images.

418
00:25:02,880 --> 00:25:03,880
Okay.

419
00:25:03,880 --> 00:25:08,920
It's a popular use baseline because, um, it's a very small baseline and images of anything

420
00:25:08,920 --> 00:25:09,920
in particular.

421
00:25:09,920 --> 00:25:11,120
CIFAR 10 has 10 classes.

422
00:25:11,120 --> 00:25:12,120
Okay.

423
00:25:12,120 --> 00:25:18,320
And common classes, um, and they are, um, it's a popular data set because it's a really

424
00:25:18,320 --> 00:25:21,520
small data set, 32 by 32 images, you barely see anything.

425
00:25:21,520 --> 00:25:25,600
And it's not MNIST because people have, like, basically decided, like, MNIST research is

426
00:25:25,600 --> 00:25:26,600
not enough.

427
00:25:26,600 --> 00:25:29,400
So, like, they just don't listen to MNIST research at all, right?

428
00:25:29,400 --> 00:25:33,040
And it's starting to be that way for CIFAR 10, just because we're getting to be so good

429
00:25:33,040 --> 00:25:34,040
on it now.

430
00:25:34,040 --> 00:25:35,040
Okay.

431
00:25:35,040 --> 00:25:41,840
Um, and, yeah, there's just known limitations that makes it, it makes it hard if you

432
00:25:41,840 --> 00:25:46,400
have a genuinely good result to tell people that you have a genuinely good result, especially

433
00:25:46,400 --> 00:25:49,520
because, like, as you scale up, like, it's also very computationally demanding, right?

434
00:25:49,520 --> 00:25:58,040
So, um, you describe the data sets as being overfitted, which, um, explain, elaborate

435
00:25:58,040 --> 00:26:04,320
on that because I tend to think of data as being inherently, well, the community is

436
00:26:04,320 --> 00:26:05,320
overfitted.

437
00:26:05,320 --> 00:26:06,600
They said, not even the algorithm itself.

438
00:26:06,600 --> 00:26:10,640
There's actually this cool test that someone did, I can't remember who, where they showed

439
00:26:10,640 --> 00:26:16,280
us, like, four pictures of images, and they asked, like, these are, these are the four

440
00:26:16,280 --> 00:26:20,040
data sets, or, sorry, maybe not, they said, like, do you know what, they set these pictures

441
00:26:20,040 --> 00:26:22,560
from, these pictures from, these pictures from, these pictures from, right?

442
00:26:22,560 --> 00:26:28,000
Like many people did, like CIFAR is a very canonical data set, um, uh, there's a place

443
00:26:28,000 --> 00:26:31,640
of data set, there's a large team understanding one, right?

444
00:26:31,640 --> 00:26:34,200
And there's ImageNet, which is, like, more general.

445
00:26:34,200 --> 00:26:38,240
So, but, and so you're basically saying that if someone can recognize these data sets so

446
00:26:38,240 --> 00:26:43,320
well, we're designing solutions to them, they are not generalizable or not adequately

447
00:26:43,320 --> 00:26:44,320
generalized.

448
00:26:44,320 --> 00:26:47,400
And, like, people have actually reported, like, native results are generally not reported

449
00:26:47,400 --> 00:26:49,000
as much, because it's just so much of it, right?

450
00:26:49,000 --> 00:26:50,480
It's a very empirical field.

451
00:26:50,480 --> 00:26:54,720
So maybe this is uninteresting now, but, um, this just happens so much, like, people have

452
00:26:54,720 --> 00:26:59,800
noted that, um, the inception architecture seems to work much better in ImageNet than

453
00:26:59,800 --> 00:27:04,560
it does in other tasks, um, and it is a pretty complicated thing, right?

454
00:27:04,560 --> 00:27:11,560
So maybe, maybe that makes sense, or, um, I've had friends that I talk to, I'd hate that

455
00:27:11,560 --> 00:27:16,600
I, a lot of my references are friends, but there's, like, the field moves so fast, right?

456
00:27:16,600 --> 00:27:20,000
That, like, sometimes even archive can't keep up, which is, I think, super awesome for

457
00:27:20,000 --> 00:27:26,480
being in it, where, and, anyway, they chat about sometimes how resnets, um, oftentimes

458
00:27:26,480 --> 00:27:29,400
don't work for their computer vision architectures, right?

459
00:27:29,400 --> 00:27:35,480
Or one of the best, um, practitioners of using Contnets, um, a friend of mine, Sander,

460
00:27:35,480 --> 00:27:40,840
Dielamann, he works at DeepMind, he has not been able to find BatchNorm to work for him,

461
00:27:40,840 --> 00:27:44,160
and I find that to be really interesting, like, is it because all of his other parameters

462
00:27:44,160 --> 00:27:45,160
are tuned to BatchNorm?

463
00:27:45,160 --> 00:27:49,840
Is there something that he solves, that BatchNorm solves also, that is not necessary?

464
00:27:49,840 --> 00:27:52,440
Is, is, is he just wrong?

465
00:27:52,440 --> 00:27:57,440
Um, honestly, I don't know, but I think that there's a bunch of cool stuff there that,

466
00:27:57,440 --> 00:28:00,440
um, maybe we can figure out, right?

467
00:28:00,440 --> 00:28:07,440
And is this inherent issue inherent to deep learning, or is it just the approach we've

468
00:28:07,440 --> 00:28:08,440
taken?

469
00:28:08,440 --> 00:28:17,440
Ooh, um, I mean, I would argue that it's not even an issue in deep learning, it's actually,

470
00:28:17,440 --> 00:28:20,400
like, maybe we can look at the bright side of this, I was like, it's a miracle it even

471
00:28:20,400 --> 00:28:21,400
works.

472
00:28:21,400 --> 00:28:26,480
Um, so, um, going to the understanding topic, right?

473
00:28:26,480 --> 00:28:31,720
There's, as far as I know, no practical theory in deep learning, like, there's nothing

474
00:28:31,720 --> 00:28:36,280
that can actually, like, guide us to understandings, like, there's, what I call stories, like,

475
00:28:36,280 --> 00:28:40,240
every paper has, like, a high level story of, this is why I think it works, and if you,

476
00:28:40,240 --> 00:28:44,120
like, really try to vet the story really well, you can, like, very easily, like, disprove

477
00:28:44,120 --> 00:28:49,520
it, and I know of no story that's, like, 100% bulletproof, um, so I'm willing to make

478
00:28:49,520 --> 00:28:55,120
that claim, and so we have these stories, and, like, they, they guide people, but they,

479
00:28:55,120 --> 00:29:00,920
they rarely work out as useful tools, unfortunately, so what we have instead is empirical results.

480
00:29:00,920 --> 00:29:06,600
What we do is, we want generalization, generalization is kind of like a lofty concept, and we,

481
00:29:06,600 --> 00:29:10,880
we don't really know, like, it's not, like, you can, in, like, traditional statistics,

482
00:29:10,880 --> 00:29:15,000
you can kind of do that, um, but, like, deep learning is as much harder because you have

483
00:29:15,000 --> 00:29:19,160
so many parameters, like, you can't really measure, well, you can measure the VC dimension,

484
00:29:19,160 --> 00:29:23,680
but it's really, it's so big that it doesn't matter, um, there's a lot of things that,

485
00:29:23,680 --> 00:29:28,320
what's the VC dimension? It's, I probably would screw this up, but I'll give you, like,

486
00:29:28,320 --> 00:29:35,320
my best, like, first of all, your approximation of what it is. It's roughly how, um, powerful

487
00:29:35,320 --> 00:29:41,040
your model is, so it shows, it kind of corresponds to, like, how much data you need in order to

488
00:29:41,040 --> 00:29:46,040
get generalization. So, like, very curvy, powerful models have, like, a very high VC dimension,

489
00:29:46,040 --> 00:29:49,200
which means that you need a lot of data. VC doesn't send for very curvy, does it?

490
00:29:49,200 --> 00:29:56,160
No, it stands for, I know the VC stands for Vapnik, um, and the C stands for another person's

491
00:29:56,160 --> 00:30:04,640
name. Okay. Um, sorry. Um, so generalization, like, in a, you know, like, in the very old school

492
00:30:04,640 --> 00:30:09,840
machine learning sense, the sense that I don't think we'll come back to personally. Um, like,

493
00:30:09,840 --> 00:30:14,080
you could have bounds on, like, how much data you need in order to get, like, this epsilon

494
00:30:14,080 --> 00:30:18,160
difference between training tests and stuff like that. And that's just not something that's

495
00:30:18,160 --> 00:30:20,960
going to happen in deep learning, as long as we keep using deep learning, we're probably

496
00:30:20,960 --> 00:30:24,240
not going to get that. Right. So what we have is empirical results. And with these empirical

497
00:30:24,240 --> 00:30:28,160
results, we just have a bunch of experiments and a bunch of data sets. And we show, like,

498
00:30:28,160 --> 00:30:33,120
it seems to work on the data sets we've tried, um, hopefully it works in everything. And

499
00:30:33,840 --> 00:30:38,160
so, like, this is where you might see it as a pro, but I, sorry, as a con, but I see this as a

500
00:30:38,160 --> 00:30:41,520
huge positive of deep learning, right? Like, it's actually super cool that it generalizes,

501
00:30:41,520 --> 00:30:45,520
right? Like, you can get a new computer vision to ask. Um, I use computer vision,

502
00:30:45,520 --> 00:30:50,400
because like, that's one of the easier, um, domains and you kind of a ton of data. And you can just

503
00:30:50,400 --> 00:30:54,640
generalize, you know, you can use it to generalize. You can use image net features to generalize

504
00:30:54,640 --> 00:30:59,520
in that that that's just not something that makes sense, right? Um, I mean, like, if you look at it

505
00:30:59,520 --> 00:31:04,720
from like a really strict perspective of like, there's no guarantee that this should work, but it tends

506
00:31:04,720 --> 00:31:11,840
to work. And that's really interesting. All right. And I think that there's something, uh,

507
00:31:11,840 --> 00:31:16,000
about deep learning that allows it to generalize so well, you know, you can even generalize to

508
00:31:16,000 --> 00:31:19,600
domains that you've not even trained on. I think that there's been some work on

509
00:31:20,160 --> 00:31:25,040
generalizing image net models to cartoons. And like, even like cartoon drawings of the things

510
00:31:25,040 --> 00:31:30,480
that they were classifying, sometimes activate or there's something related to that. Yeah. So, yeah,

511
00:31:30,480 --> 00:31:36,240
it's a wonder of deep learning. I actually, there are some experimental results that try to

512
00:31:36,240 --> 00:31:43,120
explain after the fact why things work, but without being falsifiable, it's questionable how useful

513
00:31:43,120 --> 00:31:49,680
it is. Um, so perhaps maybe deep learning is exploiting some of these kinds of explanations. There

514
00:31:49,680 --> 00:31:57,920
was a recent one on physics. Okay. That, um, that deep learning is the, like, deep learning, the

515
00:31:57,920 --> 00:32:02,480
kind, the class of things that deep learning is very good at fitting are a very like a very natural

516
00:32:02,480 --> 00:32:07,680
class of functions. Therefore, since deep, deep learning models only can fit like a,

517
00:32:07,680 --> 00:32:11,040
efficiently fit a small subset of the function space, but that happens to be like very common,

518
00:32:11,680 --> 00:32:16,720
um, like based on physics, um, kinds of functions that would occur. Okay.

519
00:32:19,040 --> 00:32:24,560
So you started out talking about data and that overfitting problem and then, uh, tools,

520
00:32:24,560 --> 00:32:28,480
was that the network software software software? I, there's two more things in data, though,

521
00:32:28,480 --> 00:32:32,960
which is that data we have, which is problematic. There's a data that we, so data we have when we use,

522
00:32:32,960 --> 00:32:36,960
like data sets, this data we have that we don't use, and there's like tons and tons of data that

523
00:32:36,960 --> 00:32:41,360
we have that we don't use, that I think that we just don't know how to use well, um,

524
00:32:41,360 --> 00:32:45,120
unsupervised learning, multitask learning, transfer learning, we kind of use, but we don't

525
00:32:45,840 --> 00:32:51,360
do very smart things, I think, um, and even like, there's implicit stuff like the trajectories

526
00:32:51,360 --> 00:32:54,320
of the networks that you've passed through. Maybe there's some interesting information there.

527
00:32:54,320 --> 00:33:00,880
And the last kind was the data that we don't have that we need. Like for example, measuring

528
00:33:01,360 --> 00:33:06,240
these things that we really care about, that we are just missing right now. Like we have,

529
00:33:06,960 --> 00:33:10,560
we have no way of measuring long-term dependency, like how well networks capture longer

530
00:33:10,560 --> 00:33:16,080
dependencies. We don't have like a general RNN benchmark. We don't have a good benchmark for

531
00:33:16,080 --> 00:33:20,320
visual attention. Um, we don't have a good benchmark for hierarchical learning. Like how do we

532
00:33:20,320 --> 00:33:24,560
even know we're learning hierarchical stuff, right? Do we want to learn hierarchical stuff? Um,

533
00:33:24,560 --> 00:33:29,680
I don't know, but like if I would think that if we want to learn something, having a benchmark

534
00:33:29,680 --> 00:33:36,880
for it would be really good, right? So that was roughly it for data. Um, from a software perspective,

535
00:33:37,760 --> 00:33:43,120
it was more about like how the tools we use nowadays really limit what we can do in like every

536
00:33:43,120 --> 00:33:48,560
tools flawed in some ways, because it hits home for me personally because I'm a software engineer.

537
00:33:48,560 --> 00:33:53,760
Okay. Um, and I want to use really good tools. You mean TensorFlow doesn't solve every problem in

538
00:33:53,760 --> 00:34:01,840
the universe? Uh, no, not yet. I think they introduced some really good ideas. Um, they definitely

539
00:34:01,840 --> 00:34:11,680
brought something to the table. Um, but it, it alone isn't enough. Um, it might like the,

540
00:34:11,680 --> 00:34:16,320
the like I think better things could be built on top of it. I don't think that it's the low-level

541
00:34:16,320 --> 00:34:19,760
components that are a problem. And I actually don't think like hardware is that big of an issue.

542
00:34:19,760 --> 00:34:28,480
Like it's big of an issue that people, um, make it out to be. Um, in, in theory, in practice,

543
00:34:28,480 --> 00:34:32,320
if you really want to do the art results and things sometimes that's needed, but there's like

544
00:34:32,320 --> 00:34:36,800
higher level problems that you can solve without hardware. So the idea with behind software is that

545
00:34:37,440 --> 00:34:44,400
you can like very like easily see situations where, um, like the software we have actually

546
00:34:44,400 --> 00:34:51,520
prevents us from doing what we want to do. So I, I think I have like two examples that really

547
00:34:51,520 --> 00:35:01,520
resonated with me where that, um, an example of bad software is when, um, it's easier to explain

548
00:35:01,520 --> 00:35:07,040
in words the technique than it is with code because ideally you want to like express idea,

549
00:35:07,040 --> 00:35:11,920
you want like the flow from ideas to code to be really easy and the flow from ideas to words

550
00:35:11,920 --> 00:35:15,760
is generally pretty good. And that just means to give a bottleneck and like words to code. And

551
00:35:15,760 --> 00:35:20,400
maybe it's a reality of life that it'll never be that simple. Did you provide a specific example?

552
00:35:20,960 --> 00:35:27,360
Um, yes. I had like a list of like many examples of like different kinds of, um,

553
00:35:28,560 --> 00:35:33,280
tricks that are hard to do in various frameworks. So depending on the framework you do some things

554
00:35:33,280 --> 00:35:41,280
can be kind of difficult. So like for, what is it? For, um, so when you say tricks and frameworks,

555
00:35:41,280 --> 00:35:49,920
the basic idea being, you know, kind of the, um, at, you know, the research, I did see that you

556
00:35:49,920 --> 00:35:54,000
put a lot of paper, you were just showing a lot of papers, which is great documenting kind of

557
00:35:54,000 --> 00:36:00,160
where the ideas came from. Uh, so in the research, you know, we're introducing all these various

558
00:36:00,160 --> 00:36:06,480
tricks to improve solvability of the, of the deep learning networks. And it's not what I'm hearing is

559
00:36:06,480 --> 00:36:12,320
the tools are, you know, on the one hand, you know, great. They're, they're raising the level

560
00:36:12,320 --> 00:36:18,320
of abstraction and making this stuff, you know, more easily adoptable. But, you know, that also

561
00:36:18,320 --> 00:36:22,960
prevents us from implementing some of these tricks, which have to be plugged in at lower levels.

562
00:36:22,960 --> 00:36:28,880
Yeah, exactly. So, um, when I mentioned trick, I used that as a general term of like this,

563
00:36:28,880 --> 00:36:34,960
like one unit of thing that you do to a neural network. Like, um, you can think of layers as

564
00:36:34,960 --> 00:36:39,600
these tricks, but tricks being more than just layers. Like, for example, an additional regularizer

565
00:36:39,600 --> 00:36:44,400
might be a trick, um, or doing, like, they could be pretty complicated, I think, like doing

566
00:36:44,400 --> 00:36:49,440
unsupervised pre-training might be a trick. And the argument that I would have is that no framework

567
00:36:49,440 --> 00:36:56,480
makes everything really easy. And easy in this sense is that I would, I would ideally like it such

568
00:36:56,480 --> 00:37:02,480
that, um, everything just gets solved for me. Like, I would be able to like, like, this is probably

569
00:37:02,480 --> 00:37:07,360
not going to happen, but we can get closer, right? Like, I would like to express, like, very

570
00:37:07,360 --> 00:37:11,440
declaratively, like, what I want this neural network to be. Like, literally, like, take this

571
00:37:11,440 --> 00:37:17,600
neural network in this database, apply this transformation, um, run this transformation, um,

572
00:37:17,600 --> 00:37:23,840
do it on a, like, train on this training set. Like, I want it to be that simple. And I,

573
00:37:24,800 --> 00:37:29,680
like, I don't think it can be, but like, striving towards that, I think, is good. Sure. And, like,

574
00:37:29,680 --> 00:37:35,280
a lot of the frameworks, like, TensorFlow, um, doesn't support a bunch of the thing, like, it makes

575
00:37:35,280 --> 00:37:41,440
it a large number of lines of code in order to do something rather than few. So what should be an

576
00:37:41,440 --> 00:37:46,880
example, like, batch normalization is like a pretty simple thing, right? So, or sorry, it's a,

577
00:37:48,000 --> 00:37:52,560
it's actually not a very simple thing in terms of implementation. But like, many frameworks can do

578
00:37:52,560 --> 00:37:57,200
batch normalization very, very well. Like, torch can do batch normalization amazingly because like,

579
00:37:57,200 --> 00:38:02,160
they can just implicitly keep it state. And in torch, like, each of the nodes applies its

580
00:38:02,160 --> 00:38:06,000
updates on its own, like, when flowing through the grad and like, applying the updates.

581
00:38:06,800 --> 00:38:16,080
Um, so that's very good. Um, but, um, TensorFlow, for example, like, in order to apply a batch normalization

582
00:38:16,080 --> 00:38:19,680
after, it has to do quite a few things, right? Like, you need to create, like, some state for,

583
00:38:19,680 --> 00:38:22,720
if you're doing the rolling mean approximation, you need to create some state for the mean,

584
00:38:22,720 --> 00:38:27,280
some state for the variance. You need to make sure to, like, apply the updates to this thing. You

585
00:38:27,280 --> 00:38:32,480
need to only apply the updates at training time. And then it becomes, like, much more complicated

586
00:38:32,480 --> 00:38:38,240
than just, like, calling a layer on something, right? Um, depending on how you wrap it, of course.

587
00:38:38,240 --> 00:38:42,800
But it, like, this, this kind of thing is just a layer in torch, right? And like, every framework has

588
00:38:42,800 --> 00:38:47,920
its trade-offs, but I just don't think that we are at, like, the efficient frontier yet of, like,

589
00:38:47,920 --> 00:38:54,000
this is like, like, I think we can get benefits for free, basically. And I actually have written

590
00:38:54,000 --> 00:39:00,640
a few libraries that, um, that try to get these benefits for free. And I think they've been

591
00:39:00,640 --> 00:39:06,240
pretty successful. Um, I'm still experimenting with them because I think there's so much to do there.

592
00:39:06,800 --> 00:39:13,520
But it's, uh, it's an open problem. And are these libraries, uh, these stand-alone frameworks,

593
00:39:13,520 --> 00:39:19,680
or libraries that plug into other existing frameworks? Um, mostly they go on top of

594
00:39:19,680 --> 00:39:25,040
fiannoir tensorflow. Okay. Because I think that they're actually, or both. Um, I think that they

595
00:39:25,040 --> 00:39:29,600
are both, like, very good baseline. So I'm a big fan of the computational graph. Um, I think the

596
00:39:29,600 --> 00:39:33,760
design of theanos actually, like, quite excellent. I'm a huge fan of theano and its developer is,

597
00:39:34,640 --> 00:39:40,160
it has the downside of distributed computing. Um, but I think that its abstraction level is actually

598
00:39:40,160 --> 00:39:45,200
quite good. Like, it can capture that abstraction level very well. Its optimizations are like things

599
00:39:45,200 --> 00:39:51,840
that I probably wouldn't do by hand anyway. So you get them for free. Um, it's, it's a, it's a,

600
00:39:51,840 --> 00:39:56,400
it's a very, I am more focusing on theano tensorflow similar, but kind of as a mix of abstraction

601
00:39:56,400 --> 00:40:01,360
levels. So, um, I'm focusing on the low level aspect. I think those low level aspects are actually,

602
00:40:01,360 --> 00:40:07,360
like, quite good. Like, they might actually be on an efficient frontier of trade-offs, you know,

603
00:40:07,360 --> 00:40:13,840
like trading off like usability versus, um, usability versus like, um,

604
00:40:15,120 --> 00:40:19,360
flexibility, yeah, yeah, flexibility or performance. And I think that that's like, there's,

605
00:40:19,360 --> 00:40:23,600
that's just one view, right? Like, use a, you know, have computational graph, have like,

606
00:40:23,600 --> 00:40:27,520
all of the basic operations and they are, um, optionally use an optimizer in order to do that.

607
00:40:27,520 --> 00:40:32,800
Like, another view would be like the torch it or cafe-ish view where you bundle up the pieces

608
00:40:32,800 --> 00:40:37,200
of functionality that have a lot of, like, the, the highly optimized pieces, right? And like,

609
00:40:37,200 --> 00:40:42,080
that's the view you go for next performance. I think it's also very different philosophically,

610
00:40:42,080 --> 00:40:45,280
but there's nothing wrong with either of these views. So I'm, I'm fine building on top of that.

611
00:40:45,280 --> 00:40:49,600
This is not what you're using. It's more of, yeah, it's more of the level and how you construct

612
00:40:49,600 --> 00:40:54,480
the computational graph, which I think should be independent of theano or tensorflow. Like,

613
00:40:54,480 --> 00:40:59,280
these are just different levels, right? Like, you could have like a really nice low level thing,

614
00:40:59,280 --> 00:41:04,160
but change the high level thing on top of it and it should be fine, which is why I'm not the biggest

615
00:41:04,160 --> 00:41:09,920
fan of tensorflow's like many different abstraction levels. And I think most of, well, all of the best

616
00:41:09,920 --> 00:41:15,280
people I've talked to who use TensorFlow, um, they kind of only use a little bit of it. And they think

617
00:41:15,280 --> 00:41:20,800
that a bunch of it is like, um, it's not the greatest, but I, I don't care, I'm not using it.

618
00:41:20,800 --> 00:41:26,560
Okay. And like, it's, it's at those high levels that I think is very interesting. And like, that's

619
00:41:26,560 --> 00:41:29,760
also where the user interacts with it, right? Like, if you're having code interact with code, it

620
00:41:29,760 --> 00:41:33,680
doesn't matter. You can have like the ugliest interface in the world, like your compiler can just,

621
00:41:33,680 --> 00:41:39,200
you know, switch things around and all of that stuff. Okay. So data, software, what was the third

622
00:41:39,200 --> 00:41:45,040
piece optimization? So I touched a little bit into it with local learning. Yeah. And Andre Carpathi

623
00:41:45,040 --> 00:41:49,760
had a great quote, which I can't remember off the top of my head, but it roughly goes along the lines

624
00:41:49,760 --> 00:41:59,280
of that neural networks only do memorization. They don't do thinking. And this is problematic,

625
00:41:59,280 --> 00:42:03,280
because this is already not as good, but this is problematic because we'd ideally like them to

626
00:42:03,280 --> 00:42:07,920
think. We want them to do like cool, complicated things that like blow our minds in their coolness,

627
00:42:07,920 --> 00:42:13,200
right? And they do blow our minds already. But perhaps those things were simpler than we thought.

628
00:42:13,200 --> 00:42:17,120
Yeah. And what's going to happen when you want to do something pretty darn complicated, right?

629
00:42:17,120 --> 00:42:22,400
Like we'll see, right? Like there's some tasks that we think that would require some pretty

630
00:42:22,400 --> 00:42:26,560
complicated levels of thinking in order to do. Perhaps playing Starcraft, you need to like think

631
00:42:26,560 --> 00:42:31,520
many moves ahead and imagine what the opponent's going to do in order to like take actions. And

632
00:42:31,520 --> 00:42:37,440
neural networks are not very good at imagining what to do yet. Maybe that will change, but we'll see.

633
00:42:38,800 --> 00:42:46,000
And Andrewing likes to say that as a heuristic of what neural networks can do is anything a

634
00:42:46,000 --> 00:42:52,640
human can do in less than one second. But I mean, if that's a hard limitation, then there's a lot

635
00:42:52,640 --> 00:42:57,040
of tasks that take more than one second for people to do. And will this solve generally I for us,

636
00:42:57,040 --> 00:43:04,160
maybe not like when you phrase it that way, right? So it should be possible, right? Like it's

637
00:43:04,160 --> 00:43:09,520
modular in theory. Like you can't just have architectures that give in a magic set of parameters

638
00:43:09,520 --> 00:43:15,200
would solve that task. So this question is how do we do that, right? And there's just many tricks

639
00:43:15,200 --> 00:43:22,800
on that. And I talk a little bit about the downsides of local learning, how we don't pay attention

640
00:43:22,800 --> 00:43:27,440
to exploration in supervised learning. And like mostly it's paid attention,

641
00:43:27,440 --> 00:43:33,200
enforcement learning, but we treat it as like obviously the plane, like there is some implicit

642
00:43:34,000 --> 00:43:38,000
exploration because you're, you know, you're using stochastic gradient descent. So your

643
00:43:38,720 --> 00:43:43,280
gradients noisy. But roughly if it wasn't noisy, you'd, you know, be blocked on a point and you

644
00:43:43,280 --> 00:43:49,760
just till climb down some direction and be stuck there. And like you don't even know how good of

645
00:43:49,760 --> 00:43:58,640
a solution that is, right? So that's that can be, I don't know, like that can be a very unsatisfying

646
00:43:58,640 --> 00:44:03,040
because if the answer is, I mean, this goes back to what I was talking about like in terms of

647
00:44:03,040 --> 00:44:09,040
limitations, like maybe local learning just can't solve this, right? And that would be super

648
00:44:09,040 --> 00:44:13,440
duper unsatisfying because local learning is like our most scalable learning algorithm we have,

649
00:44:13,440 --> 00:44:17,040
like using gradients is really, really good for turning lots of parameters. Like we're going to have

650
00:44:17,040 --> 00:44:20,560
to have to make like a lot of plant, like a lot of different plans we want generally with

651
00:44:20,560 --> 00:44:25,600
our gradient descent. So yeah, we're going to have to figure it out. So we're going to have to

652
00:44:25,600 --> 00:44:30,800
figure out tricks and how to do this better. Maybe tricks for more principled exploration. And maybe

653
00:44:30,800 --> 00:44:35,440
this will make it such that these won't be problems anymore. At least our will find much harder

654
00:44:35,440 --> 00:44:40,240
problems, right? Though hopefully always be problems. And that would, that's what keeps the field

655
00:44:40,240 --> 00:44:45,040
going, right? Yeah. Yeah. But hopefully they're not intrinsic to the way we do optimization.

656
00:44:45,040 --> 00:44:50,320
And people are making better optimizers. Yeah. You know, it's quite slow, the progress.

657
00:44:51,120 --> 00:44:57,600
Right. So data software optimization and understanding, and we talked a little bit about that earlier.

658
00:44:58,320 --> 00:45:00,800
Are there, are you going to post your slides up somewhere?

659
00:45:00,800 --> 00:45:07,840
Um, probably. I think that, well, the, I think I've, I think that rarely people put the slides

660
00:45:07,840 --> 00:45:11,280
up somewhere. Okay. But they haven't asked me for the slides yet. I think they're supposed to do

661
00:45:11,280 --> 00:45:16,240
that after the presentation. Okay. Which is probably good since there was like last minute editing going

662
00:45:16,240 --> 00:45:23,840
on. Um, but it'll almost certainly be up somewhere. Okay. And how can folks, if folks want to learn

663
00:45:23,840 --> 00:45:30,320
more about what you're up to or find you do you have a GitHub or Twitter or. I had do have a GitHub.

664
00:45:30,320 --> 00:45:33,680
It's, even though that's probably not a great way to contact someone. What's it? What's it?

665
00:45:34,560 --> 00:45:44,080
I'm not. I'm not. Right. GitHub.com slash Diego. Diogio. 149. Okay. And, uh, probably email would be

666
00:45:44,080 --> 00:45:49,760
the best way. This is something that I love chatting about. It would be Diogio at. Oh, God, my

667
00:45:49,760 --> 00:45:58,640
company name's hard to spell. Um, analytic, which is E N L I T I C dot com. Okay. Great.

668
00:45:58,640 --> 00:46:00,720
Cool. Thank you. Awesome. Hey, thanks so much.

669
00:46:04,800 --> 00:46:10,080
All right, everyone. That's it for today's interview. Please leave a comment on the show notes page

670
00:46:10,080 --> 00:46:19,360
at twimlai.com slash talk slash eight or tweet to me at at Sam Charrington or at twimlai to discuss

671
00:46:19,360 --> 00:46:32,800
this show or let me know how you liked it. Thanks so much for listening and catch you next time.

