All right, everyone. I am here with Georgia Gyoksadi. Georgia is a research scientist with Facebook AI Research, aka Fair.
Georgia, welcome to the Twomol AI podcast.
Thank you. Thank you for having me.
I'm really looking forward to our chat. In particular, this interview is a direct result of a listener request.
Who listener wrote into us suggested that we get in touch with you to learn more about PyTorch 3D and what you're up to there.
But before we dive into that topic, I'd love for you to share a little bit about your background and how you came to work in computer vision.
Sure, absolutely. I think it all started maybe 15 years ago. I was a college student in Greece and Athens in electrical engineering and computer science.
And I took this class on computer vision, which was an undergrad class, but it was covering a lot of breadth and a lot of topics in computer vision.
And that's how I got really excited about it. And after I finished up, I said, you know, I think I should apply for grad school.
I really didn't know what that meant at the time. But it said, let's do it. And so I took a map. I was like, where do I want to go?
And I want to go to California. So apply to schools in California for PhD.
And a few months later, she tendra malic emailed me saying that I was accepted and that you'd love to have a chat with me or talk about, you know, if I have any questions.
That email went to my junk because I never had received any email from that address.
But I found it. I don't know how. I was like, probably obsessing with my email inbox and junk trying to get all of the response from the schools.
So, yeah, so a couple months after that, I was in a plane, started my PhD with jitendra malic on computer vision topics that I started before the deep learning era.
When we were still doing very complicated stuff and it was actually much harder to publish at the time.
And I finished my PhD in 2016 and then joined fair versus a postdoc. And then now is a full time research scientist.
Awesome. Awesome. Did you realize at the time when you got that email that this thing that you were going to go study machine learning may have gotten in the way of you going to study machine learning?
Right. Not really.
What an awesome story.
Tell us about your work at fair. What kinds of things you focus on?
So, actually, even before fair, when I was doing my PhD, I really enjoyed recognition.
This means object recognition, scene recognition.
And what that means in general is trying to teach computer systems how to translate images or videos into semantic concepts.
You know, an image is nothing else than just a collection of random values.
So, there's a lot of work in order to go from that into understanding objects or scenes of properties and any other semantic knowledge that we wish to extract from these from that input.
So, I've been working on that for now almost 10 years. First with different approaches on deep learning and deep learning came and now we're using neural networks to do a lot of this stuff.
And that fair. I'm sort of continuing in the same direction, really interested in enhancing recognition and building systems that can understand better and better.
So now we've talked about this story of kind of pre 2012 computer vision post 2012 computer vision quite a bit on the show.
But is there anything that strikes out for you in particular in your work and experience of, you know, this transition to deep learning?
Oh, my God, everything changed like everything. Yeah, it was so, you know, before 2012.
The predominate way of doing object recognition was with models that were that we call DPMs, deformable cart models, I think. Yes. Oh, my God, I forgot I read.
And, you know, it was really a struggle to sort of build on top of that. It was not as easy to enhance them to improve them. It was, it was a very difficult pipeline altogether to get anything really impressive or even a delta actually in performance.
And then after 2012 it was so everything changed things become a lot more modular and a lot more.
You really didn't have to build a big pipeline. It was all sort of end to end nicely working together.
And of course, we also had, we were fortunate enough to have really nice open source tools and libraries that allowed us to, you know, you work on a project, you release it and now I can take it and really easily build on top of that.
So that led to just an amazing progress in the field.
Even before there were things like open CV, did those play in the kind of work you were doing? They did. And they were, they were open source libraries for sure, but they were extremely complex.
And in order for you to make any change, you had to understand it very well, which made it really difficult to develop new ideas.
I mean, that was my experience. I used to have the audience that will disagree, but I remember like reading very complex papers on graph and friends and all these things that really didn't end up doing a lot of like we didn't contribute to our improvements, but we were trying, we were trying our best.
Yeah, yeah, awesome. And so tell us a little bit about PyTorch 3D. Why? Why?
Yeah, of course. Oh, yeah, that's all I'm here for. So as I said, I work on recognition, which means understanding, trying to understand objects, people, actions, so nouns and verbs, everything from an image.
And if you really look into what recognition has done in the past few years is it has really, and it's, it's tremendous, the progress that we've seen, but our models make predictions on a 2D plane.
So they treat as if the whole world is projected on 2D. And now we can mark with a 2D bonding box. Oh, there's a person, there's a person here, there's a dog here, or this person is running, but it's all in the 2D plane.
And also this really nicely fits with convolutional neural networks and grid based libraries that like PyTorch and and test for most of the operations that convolutions are assuming that there is a grid.
But if you want to move away from that, if you want to actually make predictions in the 3D space.
So no longer one understand an object bounce finding box, but you might want to understand an object by by predicting its 3D shape. Now all of a sudden, you need to do very different things than what you were doing with a CNN.
For example, 3D data is actually, they are graphs. So you have your points that those points live in 3D space. You have edges that connect these points. And now if you want to do any sort of computation on top of them, now you need very different tools.
And you want these tools to be efficient, the same way PyTorch is efficient, you want 3D operators to be efficient. So we started looking into how we can recognize objects in 3D. We started having papers like mesh or CNN, which tried to move in this direction.
And we found really quickly that we didn't have a lot of tools and a lot of efficient libraries for this. And this is how PyTorch 3D came to life. We decided that we need to enhance PyTorch and other deep learning libraries with the ability to perform efficient computations on 3D data.
And that's not just it. Then you also want to be able to go back from 3D to 2D in a differentiable way. So now this is graphics people have been working a lot on rendering.
But now that we have gradients and we need to optimize, you need these rendering pipelines and rendering operations to be able to have gradients.
Which means that you need to rethink a little bit about how you have defined these traditional graphics functions.
You might want to change them and you also need to make them very efficient. And all that is sort of what PyTorch 3D is all about.
Awesome. Awesome. I've had several conversations on the podcast around trying to apply deep learning and machine learning broadly and kind of non-uclidean 3D domains like there's the gauge, equivariant stuff.
And there's another, I'm trying to remember the model where it kind of thinks of everything as a cardio and kind of something shaped roughly, you know, heart like.
And there's a Python toolkit around that. I'm wondering is this does PyTorch 3D bar from any of those kinds of ideas or is it kind of going in a different direction?
Yeah, so, um, PyTorch 3D basically can't, it can actually take us data, any data that you that is beyond the, the, the 2D grid.
So, for example, a lot of people are working with volumes for, for medical imaging or from any sort of other sources of, of input signals.
And this is where sort of PyTorch 3D is coming into play and we actually have a lot of issues and on GitHub where people are trying to get a variety of data that we've never thought about before.
So, I think that it does offer, it has allowed people to actually do a lot more things. I think we also have to do a lot of work to keep building it and keep supporting even more signal and inputs.
But yeah, I think it definitely ties to a lot of these problems.
So, are the, the data sets that folks are using, are they, do people tend to come at it with kind of these native 3D data sets, or are they data sets that, you know, and it might mean something like, you know, something coming out of, you know, a 3D sensor of some sort, or, you know, point cloud or something like that.
Are, you know, is there, are the use cases where folks, you know, are starting with 2D and kind of annotating into 3D space. I'm not sure if that even makes sense.
No, it does make a lot of sense and it's actually both.
So, I think one thing that we wanted to focus on with our library is that one is the ability to be able to use these, the data that comes from sensors like what you said point clouds or scans of, you know,
there is companies like Matterport that scan your house if you wish to. So, this is the type of data that we are supporting in Python 3D.
And this is sort of more the straightforward use of trying to process the data with our library.
But another use case is if you want to inject 3D data in the middle of your network.
So, if you want to reason in a latent 3D space instead of an explicit 3D space, that is also a possibility with this with our library and it's a possibility because all of our operations, our operators, I'm sorry, are differentiable.
So, you can actually put them in the start and the middle and anywhere you wish and you can and you can do your network. So, the things that you can do are sort of endless.
So, what's an example of what you were just describing where you might want to have the latent 3D space as opposed to the explicit 3D space in the middle of your network.
So, there is a lot of applications that are that require this and we actually had a paper and this recent CDPR 2020, which was joint work with our intern at the time, Olivia Wildes and Rick Zalisky and Justin Johnson, where what we want, the problem that we wanted to tackle is no overview synthesis.
I give you an input image and you want to hallucinate or think of actually the computer also, what the scene will look like if you turn around or if you move straight up.
So, which means that you have to generate new images that are geometrically and semantically consistent with your initial view.
So, walls need to be vertical, the lines need to be straight. So, objects might get unaccluded, so you actually need to complete them. So, it's a very difficult task.
And there we found that actually trying to do this, just in an image to image approach like picks to picks and just putting again there to help the system hallucinate was not enough.
Actually, to capture that geometry and to make the network geometry aware, we had to inject this 3D latent space like in reasoning in that space in order to make that happen.
Exactly what we did. So, we tried to reason about a 3D point cloud, the response of the scene and then manipulate that so transform it rotated in order to make it consistent to make the no view consistent with a transformation that we wish to apply.
And then we used rendering and got to that output.
Got it. Got it. I did an interview with Josh Tobin at NURP's last year on geometry aware neural rendering. Sounds like the same problem.
I think that it's a space that is growing. There is a lot of excitement in the field about the potential. There is some recent work. Also, I don't know if you've seen NURP.
They're able to sort of generate beautiful, continuous scenes from just 2D images. And again, you need to use a 3D to reason and 3D and render back differentially. So it's, I think it's one of the exciting, exciting fields to be working on right now in computer vision and AI.
I'm biased. So, are we at the point where we're starting to see commercial use cases of this or consumer facing use cases of this kind of thing or it's Facebook using it like how mature is it or how far are we from, you know, seeing those kinds of uses.
I want to say yes, but it will be wrong. Yes. I think that I think that there is demand. Yeah, definitely they want to be using it. Are we there yet? I would say not yet.
Because when you talk about a product and making it this work, you need to make sure that it actually works. So this is like, this is one part of like my experience is being a research scientist at Facebook. Even though I don't work on products, I've realized that putting technology into products means it really has to work.
It's a whole different ball game. Exactly. And I'm very confident that there's demand. So there will be progress and I think we're going to get that.
Is the audience then for PyTorch 3D? Is it primarily used by researchers or is it used by developers, people who are building things?
I think currently it's actually being used by both. There is definitely some, some, some aspects of usage of PyTorch 3D that can make it and that are probably making into products.
But I don't think that, you know, have we solved the ARBR or can we build ARBR experiences that are, you know, the best ever. I think this is sort of for me, this is the goal or the bar. And I think this is where we're not there yet. But I think there's definitely space for a smaller impact in in product for usage, but definitely also for researchers and developers.
Can you maybe take a step back and talk about how this fits in more broadly with kind of the drive so you give computers, you know, better means of exception.
Yeah, definitely. So I think we over like since, since the deep learning sort of exploded.
We've seen a lot of works and a lot of progress in making recognition work in 2D.
Like image classification, we had Alex, that VGG, resnets, like all this has been on a steady, increasing path in terms of performance. And the same thing with recognition of objects, you know, mask our CNN, and then all the other following works that have made recognize and objects in 2D.
But in order to make these systems work, we really, what we are doing currently is that we're annotating images. So we're asking annotators to mark what is the object and what is the object type, what's its mask, and then we're forcing networks to imitate this, imitate sort of the annotators basically.
So 3D might be a way out of this. So if we can manage, because if we can manage to reason of 3D, then we can actually understand objects more naturally.
I try not to bring a lot of parallelism to how humans learn, because I don't think that what we do currently is very similar to that. But if how did you learn what an object is. So you were grabbing objects a lot, and you were rotating them, and you were looking at from different viewpoints.
And that allowed you to have an understanding of consistency and boundaries, and also semantics at the same time. So if I show you an object that you've never seen before, you will probably still recognize what it is or what the mask is, even though you might not be able to classify it.
This is something that we can do today, but maybe through 3D, which is a more natural space to reason, we can we can expand our recognition ability. I don't know if that makes sense. It's a big bet.
But it's sort of the hope.
We're hearing that as, you know, we've made a lot of progress in 2D, but 2D is kind of fundamentally, you've tossed away a lot of information, and it's not just a lot of information in terms of, you know, bits, it's a lot of information in terms of the semantics of the objects themselves.
And you're theorizing that by using that information or building systems that can take advantage of that information, those systems will be, I think what you're saying is not just maybe more accurate, but more closer to what we might think of as intelligent.
They're making, they're solving tasks based on something more fundamental. Exactly. I think, yeah, you said it very well.
I don't have, it's a hope and that we don't have proofing what's the case.
But, you know, even if we are able to learn with fewer labeled examples, by reading this space, that will be a huge win.
Is that primary, is that the primary win that would, if you, if you demonstrated that that was true, would the primary win be greater sample efficiency, or would there be other, how, how else might you expect that to express itself?
Yeah, definitely. So trying to learn with fewer labeled, fewer, fewer annotations, try to, to learn and generalize better, generalization, something that we currently can do very well.
So there is a lot of axes that you can show this.
Hopefully, we'll show that in all, we'll see. Yeah, yeah. Can you maybe walk us through the, the user experience of PyTorch 3D, and maybe starting from the, you know, when you look at the way folks have been using it, what problems are they typically trying to solve?
And, are they, is it kind of a drop-in replacement for PyTorch, you know, in 2D, are they doing the same things, or do they need to think differently fundamentally about the way they're doing stuff?
That's a very question. They, it's complementary. So it's an addition to PyTorch. And so it, it, in terms of the user experience. So, first of all, I want to say that this is work led by our awesome engineer, Nikila, and she has put up a lot of videos online on tutorials, how to get started.
I want to put it out there for people that are interested in using it. In terms of philosophy and how to use it in a broader sense, it is, it's complementary to PyTorch.
Our operators, we wanted them to be have the same modularity that PyTorch has, where it's like little operations that you stack one off to the other. It, it, it blends very low with PyTorch, so you can have a union of PyTorch operators and PyTorch 3D operators in order to do what you want.
So, and then you can put this all together. You can, you can implement a training loop and you can train this all into it. So it blends very well. And this is actually what we do. This is what we do in, in all of our papers that use it is, it's a combination of many lot PyTorch and many libraries that build on PyTorch and PyTorch 3D.
Okay. Go ahead. No, no, no, that's not you. No, just wanted to say that in terms of what, what people, how people have been using it.
We all, we launched this in January, and so my biggest, the best way for us to know how people use it in the community is through our engaging with them through GitHub.
We see that they're using it in all sorts of ways. They're using it for robotics. They're using it for graphics applications. Some are using it for medical, I don't know, there was issues about some people wanted to reconstruct teeth and wanted to actually understand better, how they can capture information there. So it's like the range of wild.
Do you have a favorite paper that uses it?
Yeah, I think one of my favorite papers is our recent CPR 2020 paper with Olivia. I thought it was a very nice way of blending in all this stuff that people have been doing in the GAN world of trying to create beautiful images, but making it harder by explicitly demanding geometry aware outputs.
Got it.
And so this is, it's a tool that's used by folks that are building stuff as well as doing research.
But is there, were there research challenges in building the tool or is the tool kind of an implementation of stuff that you already knew how to do?
No, it was, and this is why it is a library on its own is because in order to make a lot of this stuff efficient in order to make a lot of the stuff work and train and back prop in a reasonable time using reasonable memory.
We had to hack in there and implement custom CUDA kernels. And so it wasn't just building on top of five for which are in top of what Peter provides. We really have to do customized implementations to support all of this.
And so the, that the CUDA stuff is very, that's low level, very low level work to get all this to work.
I don't know if you can, I think assembly is a step below that.
And did you, are there, is there a, to get example of this like a 3D equivalent of drop out or something that you had to figure out to make this kind of stuff work or all of your 2D things, you know, generally apply, you just have to do this low level stuff to make them work in reasonable time.
So the, they're both of them were example cases. So and some there were, it was things that you could already know how to do with other implementations like was using PyTorch for example, but then it would take just an incredible amount of time.
So for example, in our measure, CNN paper, I'll calculate it that without PyTorch 3D, a network that I trained in 3 hours will probably take me 2 weeks.
So this is sort of the scale that we're talking about. And when you talk about 2 weeks, you can do research like that.
Yeah, I mean, that's just impossible.
And that example are we talking about, what was the before case, was it a problem that's natively 3D that was kind of forced into 2D and done inefficiently and now you're able to do it natively 3D, was it treated it natively 3D before, but using inefficient stuff and the Python domain and you speed it up by going down to kuda.
What allowed you, what gives you the advantage in this particular case you're thinking of?
So I think that that was sort of again, both.
But yeah, so there was, it was just a fact that a lot of the existing implementations were not meant to be for the type of data that we had.
So they immediately become inefficient. So for graphs, for example, it's not a very, it's not easy to apply existing operators.
And it was also the fact that the scale that we were talking about. So our graphs are very big.
So you really need to make sure that you have very optimized implementations for that. And then I also want to mention that there was for our differential for render, which is also in 3D in Python 3D, there it was not a matter of sort of that stuff that you couldn't do with naively.
To actually write your own custom stuff in order to, to make the rendering pipeline differentiable.
Got it.
We talked a bit about graph here as being fundamental to working in 3D, are there broader graphical uses and implications of pie towards 3D that aren't necessarily, you know, 3D in the way we think of it visual data.
Yeah, so basically you can think of a graph, not necessarily as it being 3D, you can attach any vector to your university. So it can be a graph in this high dimensional space.
So all that falls under the definition of a graph. And so this is sort of, so when I mentioned graphs, I mean that you have the ability to work operate on data structures like that.
You have are many of the folks that are using it using it in this again non fundamentally, you know, visual 3D sense using it as just a generic graph tool.
Yeah, and I think this is a grand exciting part. And when we talk about implicit representations and latent reasoning, this is what we mean where you have a point cloud or a graph that doesn't have values that can be interpreted by like the human brain like RGB colors, for example.
But it can be any sort of feature vector. And so now this is what the interesting applications come in and the interesting stuff that people do.
Awesome, awesome.
You are also kind of going back to your background and your work more broadly, you are also a program co chair for CVPR 2021 next year. And we were talking a little bit about this before we started recording.
Actually, something that you've been doing since well before this year, CVPR. Share a little bit about that process. What does it mean to be a program co chair at a conference like this in terms of, you know, what, what, what you're thinking of and, you know, how, how is it?
So correct, that was about a TV program co chair for CVPR 2021, which is next June. And as you said, very correctly work here, but starts two years before the conference actually happens.
And I've been told that it and the work ends the year after.
I'm going to show like stressing myself out here. So, yeah, so, so what it, what it means to be a program co chair. So, we are, so the program, the program chairs are responsible for putting the program of the conference together.
That means that we are in charge of the papers that will be presented and what that means is that we are in charge of everything that needs to happen so that authors can submit their papers.
And these papers need to be reviewed and a decision needs to be made.
What that means is that we need to make sure that we have the right infrastructure to do this. We have, we need, we need to make sure that we have the right and big enough review or pool of good reviewers that can offer reviews.
And we also need to have a pool of area chairs. These are sort of the people that take a batch of papers. And they are in charge of making sure reviewers do their job and that a decision is made after discussion and after discussing with other areas and with reviewers as well.
Anything, anything new in terms of the reviewing process. I mean, this is something that's evolving in academic communities, open reviews and things like that.
And whenever, you know, review season comes around for conference, actually see a lot of conversation on Twitter about the process. What's your take on all that. Right. So I think this is where the challenges come in.
And a lot of the conferences in the past have build up rules that because they were trying to target a relatively smaller audience, not only the audience for the conference, but also the number of submissions.
Now that we have the scale that we have, which is we've gone from like a couple hundred submissions, like 10 years ago, to 10,000 submissions this year. That's what we're sort of expecting.
It's not just about scaling. It's not just about scaling all of this to make the numbers, you know, make sense. That doesn't work.
So we need to rethink a lot about our procedures and what we do.
And we need to make sure that while we harvest all the positive from a growing community that we're also cognizant of the negatives.
And there's a lot of negatives.
So we need to be able to foresee and have rules in place in order to deal with those issues.
But this is this is something that we've seen larger conferences like nerves, for example, have to grapple with over the past couple of years, including, you know, changing the name of the conference.
Correct.
I mean, I think this is for me the biggest challenge is how to make it into other than making this all the process be a success is how can we ensure that this is everybody feels good being here.
And like one of the senior computer vision scientists says, you know, all you need is one, one person misbehaving that can ruin the experience for everyone.
And when you have 7,000 people in a conference, chances of that happening are much, much bigger.
So this is, this is a big challenge.
Are you planning for it to be an in person conference?
I think that at this point, we're aware that in the best case in my bi hybrid, but probably it's going to be a virtual conference in June 21.
Wow.
Yeah.
Awesome.
Yeah.
Cool.
What it's been wonderful speaking with you.
Any other parting thoughts on PyTorch or CVPR or any of the other things you've been working on?
Not really. I think, as I said, I think it's an exciting feel to be, we're in this exciting direction, the direction of 3D, to be working on right now.
So I, I'm hoping that, you know, the bet that we just talked about, like, can 3D help us recognize that are like, I hope to see more results in this direction.
And, you know, if somebody is thinking out there, what should I work on? Like, what are the good problems to work on? I think this is a good problem to work on.
And in terms of community and us, you know, existing together peacefully. I hope that we all can figure this out.
It's going to be a collective effort. So, yeah.
In terms of that, that previous point, you made it prior problems to work on, are there more granular or specific directions that you might suggest someone who, hey, I like computer graphics and 3D and machine learning.
Specifically, needs to be addressed that you think are kind of ripe opportunities for folks to jump into.
I think that what is, in my view, one, one of the unsolved problems right now is doing this at a large scale and from in the wild data. So, you know, take the millions of images that we have.
And how can you make, make 3D either predicting in 3D or reasoning in 3D space be contribute there in whatever downstream task that you wish.
So, I think this is one of the biggest challenge making this work in the wild and making this work large scale the same way that we've seen 2 2D problems benefit from this large scale training.
Awesome. Well, Georgia, thanks so much for chatting with us about what you're up to.
No, thank you for having me and for listening to me.
Thank you.
