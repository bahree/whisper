WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:34.480
I'm your host Sam Charrington. Today we're joined by Larry Mathees, senior research scientist

00:34.480 --> 00:40.280
and head of computer vision in the mobility and robotics division at JPL.

00:40.280 --> 00:45.600
Larry joins us on the heels of two presentations at this year's CVPR conference, the first

00:45.600 --> 00:52.080
on onboard stereo vision for drone pursuit or sense and avoid and another on vision systems

00:52.080 --> 00:57.960
for planetary landers. In our conversation we touch on both of these talks, his work on

00:57.960 --> 01:04.120
vision systems for the first iteration of Mars Rovers in 2004 and the future of planetary

01:04.120 --> 01:07.160
landing projects. Enjoy.

01:07.160 --> 01:12.560
Alright everyone, I am on the line with Larry Mathees. Larry is a senior research scientist

01:12.560 --> 01:17.920
at JPL and head of the computer vision group there within the mobility and robotic system

01:17.920 --> 01:23.080
section, as well as an adjunct professor in computer science at the University of Southern

01:23.080 --> 01:26.760
California. Larry, welcome to this weekend machine learning in AI.

01:26.760 --> 01:28.800
Thank you. It's a pleasure to be here.

01:28.800 --> 01:31.680
Let's get started by having you tell us a little bit about your background and how you

01:31.680 --> 01:36.120
got started working on vision for intelligent systems.

01:36.120 --> 01:43.320
Sure, I actually grew up in Canada. I studied computer graphics for a master's degree and

01:43.320 --> 01:50.520
then I went to Carnegie Mellon for a PhD. I was interested in artificial intelligence.

01:50.520 --> 01:59.480
Back in 1981 I spent a year so as a student of some of the AI faculty there and I felt

01:59.480 --> 02:07.080
that in those days the theoretical underpinnings weren't quite what I was satisfied by so I

02:07.080 --> 02:16.040
ended up working on computer vision for robotics which to me had a nice combination of solid

02:16.040 --> 02:24.840
underpinnings in math and physics and some of the appeal of AI and the kind of the

02:24.840 --> 02:30.440
visual gratification of computer graphics. It was a nice hybrid.

02:30.440 --> 02:36.680
How long have you been at JPL? Since 1989 so I think that's almost 29 years.

02:36.680 --> 02:41.360
Wow. What's your research focus there at the lab?

02:41.360 --> 02:46.480
I've been working on computer vision for autonomous navigation of unmanned vehicles. That's

02:46.480 --> 02:52.760
been our main focus the whole time. Specific activities have shifted. I started working

02:52.760 --> 02:58.040
on autonomous navigation of ground vehicles and pretty much the whole time I've been here.

02:58.040 --> 03:04.920
I've spent roughly half time working on NASA related projects and half time working

03:04.920 --> 03:11.200
on non-NASA projects. Most of our funding here comes from government sources so the non-NASA

03:11.200 --> 03:17.440
work has been mostly places like DARPA and the Army Research Lab. I worked on ground

03:17.440 --> 03:25.720
vehicles. Then I started trying to address landers and orbiters on the NASA side. I always

03:25.720 --> 03:31.400
tried to compliment what we do on the NASA side with related things on the non-NASA side.

03:31.400 --> 03:37.880
In the early 2000s I started working on vision for drones along the way we've done work

03:37.880 --> 03:42.400
on vision for autonomous navigation. Those have been my main activities.

03:42.400 --> 03:50.280
You recently returned from the CVPR conference in Salt Lake City where you presented as part

03:50.280 --> 03:57.480
of a workshop on visual odometry and computer vision based on location clues. That was the

03:57.480 --> 04:03.560
title of the workshop. Can you parse that out for us and what is visual odometry and

04:03.560 --> 04:07.080
what's the role of the location queues and all that?

04:07.080 --> 04:14.960
Visual odometry is a technique used to estimate the motion of a camera. Usually that camera

04:14.960 --> 04:20.040
is attached to something else you care about. In my business that's usually a camera on

04:20.040 --> 04:26.560
the robot. You're using the camera to track nearby features in the environment to estimate

04:26.560 --> 04:31.960
the incremental motion of the vehicle. It's called visual odometry as an analogy to

04:31.960 --> 04:37.800
more traditional forms of odometry based on integrating fuel encoders. It's a visual

04:37.800 --> 04:46.040
dead reckoning. The part about location clues. Visual odometry integrates forward from

04:46.040 --> 04:53.000
some starting point and it drifts in robotics and related applications you often want to

04:53.000 --> 04:58.200
have an estimate of absolute position and where the system is. Visual odometry doesn't

04:58.200 --> 05:07.640
do that. You need to have some other absolute reference and location queues could come from

05:07.640 --> 05:15.160
basically maps to give you estimates of absolute position. In this field there's a buzzword

05:15.160 --> 05:20.440
simultaneous localization and mapping which is all about maintaining the history of where

05:20.440 --> 05:27.680
you've been and updating that whole history. If you should double back and see places

05:27.680 --> 05:33.600
you've been before, that's called loop closure and then you can improve your estimate of

05:33.600 --> 05:38.680
the whole history. Although that loop closure doesn't necessarily tell you where you are

05:38.680 --> 05:44.040
absolutely in the world. It tells you that you're back to some place you've been before

05:44.040 --> 05:53.680
which is often just about as important. How do visual odometry and slam relate to or

05:53.680 --> 05:59.840
visual odometry in particular I guess? How does that relate to pose estimation?

05:59.840 --> 06:07.760
Pose estimation refers to the six degree of freedom, position and orientation of a system

06:07.760 --> 06:16.480
so the camera. Visual odometry is estimating the pose of the camera relative to some previous

06:16.480 --> 06:23.480
reference frame every time you take a picture. Pose estimation is basically part of visual

06:23.480 --> 06:33.880
odometry and so your talk was on vision systems for planetary landers. Can you walk us through

06:33.880 --> 06:42.640
the focus of your talk? Sure. In planetary landers so by planetary we're referring to a lot

06:42.640 --> 06:48.640
of our work at JPL has been focused on landing on Mars. We did some work in the last decade

06:48.640 --> 06:57.920
on techniques for precision landing and landing hazard avoidance on the moon and now we're

06:57.920 --> 07:05.240
working to extend these techniques to be applicable to other places like Jupiter's Moon, Europa,

07:05.240 --> 07:14.800
Saturn's Moon, Titan, Comments and Asteroids. In landing you want to, kind of a progression

07:14.800 --> 07:19.960
of capabilities that we worked on. So you want to have a good estimate of the terrain

07:19.960 --> 07:25.160
relative velocity. So in particular you know the vertical velocity and then the horizontal

07:25.160 --> 07:31.120
velocity those have to be within strict limits that touch down or you have a failed landing.

07:31.120 --> 07:36.000
That doesn't tell you where you are. It tells you what your velocity is. Next step is

07:36.000 --> 07:42.560
you'd like to know where you are. So that's the precision landing part. For that you

07:42.560 --> 07:46.520
need, you know, we're saying earlier visual geometry doesn't tell you in absolute terms

07:46.520 --> 07:51.640
where you are. For precision landing on planetary bodies we want to know in absolute terms

07:51.640 --> 08:00.200
where we are. So we have to have some external reference what has been the most approachable.

08:00.200 --> 08:06.400
Often we have orbiters that take high resolution imagery from orbit of places that we want

08:06.400 --> 08:14.160
to land. And so we've been developing algorithms that we use a downlooking camera on the spacecraft

08:14.160 --> 08:18.320
as it gets close to the surface to take pictures and then we register those pictures with

08:18.320 --> 08:23.400
the orbital imagery to get position updates. And we can use visual geometry as part of

08:23.400 --> 08:30.800
that to track features with this camera during descent to improve the velocity knowledge.

08:30.800 --> 08:37.520
So that complements the map registration to give us position knowledge. And then for places

08:37.520 --> 08:43.440
like Mars where we've got really, really good orbital imagery. So resolution of pixels

08:43.440 --> 08:48.440
on the ground of 25 to 30 centimeters per pixel which is good in this business. We can

08:48.440 --> 08:54.320
see almost all of the landing hazards from orbit before the lander ever gets there.

08:54.320 --> 08:59.320
So we don't really have to do onboard landing hazard detection for Mars. At least not

08:59.320 --> 09:03.800
anything we need you to do so far because we can map the hazards before we get there.

09:03.800 --> 09:09.000
And then we just as long as we know where we are during descent, we can plan a trajectory

09:09.000 --> 09:13.800
to a landing site that we already know is fairly safe. For other places that we want to

09:13.800 --> 09:19.360
go that aren't as well mapped, we will need onboard landing hazard detection. So this

09:19.360 --> 09:26.920
was germane to the focus of the workshop because localization visual geometry are important

09:26.920 --> 09:30.640
in planetary landers and that was the theme of the workshop.

09:30.640 --> 09:37.600
How long have we been using vision-based systems in as part of our approach to trying to

09:37.600 --> 09:40.400
land things on other planets?

09:40.400 --> 09:48.080
So the first use of vision in real time in landing was in the Mars exploration rover mission

09:48.080 --> 09:55.080
so that put the rover two rovers on Mars in basically January of 2004. Those rovers

09:55.080 --> 10:01.680
were called Spirit and Opportunity. So we had a system on that. So those rovers landed

10:01.680 --> 10:08.160
with airbags which were deployed after a parachute phase. So we put a system on that mission

10:08.160 --> 10:14.120
that estimated horizontal velocity in the last two kilometers of descent to the surface

10:14.120 --> 10:19.320
so that that velocity knowledge could be used as part of Redfer Rocket firing logic to

10:19.320 --> 10:24.000
reduce the horizontal velocity to make sure that we get inverse the airbags.

10:24.000 --> 10:28.960
We used airbags for the Mars Pathfinder mission in 1997, very successfully without these

10:28.960 --> 10:36.040
techniques. But the rovers by 2003, 2004 were quite a bit heavier and we learned some

10:36.040 --> 10:41.280
things about the winds on Mars that we didn't know before that gave us some concerns.

10:41.280 --> 10:47.840
So we added this technique to add, given added measure of safety. So that was the first

10:47.840 --> 10:53.120
time vision was used in a planetary landing system in real time onboard.

10:53.120 --> 10:59.040
Tell us a little bit about the general approach that you take to solving these types of problems.

10:59.040 --> 11:04.440
From our conversation before getting started, it sounds like you tend to focus on model

11:04.440 --> 11:11.160
based approaches as opposed to deep learning or data driven approaches.

11:11.160 --> 11:18.880
Well, so in doing computer vision for robotics, which is what this is, and especially what

11:18.880 --> 11:28.680
we do at JPL is enable missions. And so it's applied research. And you've always, so it's

11:28.680 --> 11:34.640
applied research in contrast with basic research at universities. So we've always got a mission

11:34.640 --> 11:41.160
in mind. We've all with those missions always have schedule and budget constraints and

11:41.160 --> 11:46.600
the people who build those missions tend to be a little risk averse.

11:46.600 --> 11:53.320
So what we always need to do is take advantage of anything that can give us advantage.

11:53.320 --> 11:59.880
And so that inherently leads us to multi-sensor approaches. So there's been a tendency, at

11:59.880 --> 12:05.800
least there was for a number of years in in academic computer vision research to study

12:05.800 --> 12:11.960
how much can you do with only a camera. But in our business, you know, there's always

12:11.960 --> 12:18.200
an inertial measurement unit that gives you estimates of velocities and accelerations

12:18.200 --> 12:24.080
in angular rates. So you want to use that. And if you have prior knowledge, you want

12:24.080 --> 12:30.600
to use that. So we take advantage of all of those things. And then in the space business,

12:30.600 --> 12:37.560
space-based computing is much, much less powerful than what we can do on earth. So we're talking

12:37.560 --> 12:43.040
three to four orders of magnitude. So you have to squeeze a lot of capability in a very

12:43.040 --> 12:48.000
small amount of compute. And we're going to places that we've either never been before

12:48.000 --> 12:52.800
or we don't have much, we don't have much, we have some, but not much prior data on.

12:52.800 --> 12:57.280
So it can be hard to do learning-based approaches because you just don't have data to learn from

12:57.280 --> 13:03.560
unless you get into transfer learning and things that are probably a little bit too cutting

13:03.560 --> 13:11.680
edge to put in a space vision yet. So we've tended to rely on a lot of geometry because

13:11.680 --> 13:16.880
that, you know, we've got the solid modeling foundation to do that. You know, what we need

13:16.880 --> 13:23.640
for precision landing and landing hazard avoidance is geometry anyway. We need the geometry

13:23.640 --> 13:28.400
of the terrain, we need the position of the spacecraft, the velocity of the spacecraft.

13:28.400 --> 13:34.160
So that's not necessarily strictly geometry, but it's related. It's all with sensors that

13:34.160 --> 13:41.680
are noisy. So this is fundamentally navigation. You know, the whole history of navigation

13:41.680 --> 13:47.320
research and development has always paid attention to modeling the noise and the sensors

13:47.320 --> 13:54.880
and propagating that into uncertainty in your state estimates using that uncertainty

13:54.880 --> 14:02.320
when the state estimates in planning and control. So we levered all of those techniques.

14:02.320 --> 14:06.640
And you know, where we can and where it makes sense, we're trying to use state-of-the-art

14:06.640 --> 14:12.200
learning methods. So those in particular applied to rovers. Once we're on the ground, we want

14:12.200 --> 14:17.640
to do terrain classification. And when we've had rovers driving in areas that we've seen

14:17.640 --> 14:24.040
from orbit and the rovers, you know, by now on Mars, we've accumulated, you know, several

14:24.040 --> 14:28.640
tens of kilometers of traverse. So we've got a lot of images. So we can now start to

14:28.640 --> 14:35.040
train classifiers. So if the rovers understand whether they're on bedrock or on sand, how

14:35.040 --> 14:41.880
much they might slip, how much they might sink. So we've kind of progressed from, you know,

14:41.880 --> 14:48.480
model based techniques that exploit multi-sensor fusion with very limited computation and very

14:48.480 --> 14:53.840
limited prior data to scenarios where we do have more prior data. And we can see in the

14:53.840 --> 14:58.680
future there are things in the works that we hope would give us much more computing power

14:58.680 --> 15:03.920
in the next decade and be able to use more sophisticated techniques on board.

15:03.920 --> 15:09.600
Can you talk a little bit about the more traditional approach where you're incorporating this

15:09.600 --> 15:15.440
multi-sensor fusion? What are the key research challenges there? What has been the, how would

15:15.440 --> 15:24.360
you characterize the progression of research in that field over the past few years? Ah,

15:24.360 --> 15:32.360
well, you know, in all of computer vision and robotics, we've been helped tremendously

15:32.360 --> 15:38.720
by things that basically came into the field from outside. So, you know, it's cliche to

15:38.720 --> 15:45.080
say that advances in computing power and miniaturization and, you know, more compute for less mass

15:45.080 --> 15:50.440
power volume and cost has been hugely enabling, but it's true. Same thing goes for inertial

15:50.440 --> 15:57.240
sensors. Same thing goes for cameras. So the whole, you know, the invention of CMOS

15:57.240 --> 16:07.760
imagers, which was a big step in miniaturizing cameras and power for cameras. But the,

16:07.760 --> 16:13.800
you know, revolutionary progress in mobile electronics for consumer applications has spun

16:13.800 --> 16:21.360
off effectively, tremendously valuable sensors, processors, and communication hardware that

16:21.360 --> 16:28.320
we can use in robotics. So that's one thing. Within our field, you know, the field

16:28.320 --> 16:34.000
ensured a lot in the time that I've been in it. So, last 30 years, 40 years actually

16:34.000 --> 16:42.880
including grad school, in using, you know, initially common filters and incremental batch

16:42.880 --> 16:51.280
estimation techniques, incremental bundle adjustments. So the whole simultaneous localization

16:51.280 --> 17:00.880
at mapping or SLAM, a literature has developed over the last 35 years or so. 3D perception

17:00.880 --> 17:12.640
has gone from, you know, being hardly possible in the early 80s to being, you know, routine

17:12.640 --> 17:19.320
now. So progress and algorithms for stereo vision that actually work and that are affordable

17:19.320 --> 17:27.680
progress in other 3D sensors, LIDAR, in particular, being small and compact and affordable.

17:27.680 --> 17:33.440
That's made a huge difference. So in what I do for planetary exploration, there's not

17:33.440 --> 17:40.960
a lot of object recognition per se, but that's really where one of the most important research

17:40.960 --> 17:47.760
thrusts these days is in applications on Earth. And so that's an area where people tried

17:47.760 --> 17:55.080
more model-based methods for years and that's all becoming very learning-based. So the deep

17:55.080 --> 17:59.560
learning revolution is having a big impact and how people approach those problems.

18:00.360 --> 18:09.720
You mentioned stereo perception and the use of, I guess, image-based approaches to stereo

18:09.720 --> 18:15.720
perception as opposed to point clouds like LIDAR. Can you talk a little bit about that process

18:15.720 --> 18:20.200
and how that works? I haven't thought too much about that or come across and kind of curious

18:20.200 --> 18:27.480
about that. Okay, so I was referring to, let me call it 3D perception. So 3D perception gives

18:27.480 --> 18:34.920
you a point cloud. LIDARs do that. There's multiple LIDAR technology, but think of it as

18:34.920 --> 18:41.960
kind of light-ranging. So you emit a short pulse of laser light and it reflects back to a detector,

18:41.960 --> 18:47.240
you measure that time of light that gives you range. stereo vision gives you a point cloud

18:47.240 --> 18:52.200
by using two cameras. And you know the relative positions of those two cameras. And if you

18:52.200 --> 18:57.960
can find the same, the image of the same object in both of those cameras, then you can triangulate

18:57.960 --> 19:03.560
where that object is in 3D dimension. So it's just like surveyors. Where surveyors, you know,

19:03.560 --> 19:08.200
look at the same point in the world from different locations and then they can triangulate where

19:08.200 --> 19:15.080
that point is. You do that in stereo vision by having two cameras and software that for every pixel

19:15.080 --> 19:20.760
in, let's say, the left image you find in the right image, the pixel that has the projection

19:20.760 --> 19:24.760
of the same object in the world. And so if you do that at every pixel, then you build up what we

19:24.760 --> 19:31.880
call a depth map and you triangulate. And that gives you a point cloud that is essentially

19:31.880 --> 19:39.240
the same kind of data you get from LIDAR. It's got different noise characteristics. There are

19:39.880 --> 19:44.520
strengths and weaknesses for each approach, but they're both methods to give you point clouds.

19:44.520 --> 19:52.200
Okay, okay. LIDAR is much more expensive than cameras are and yet it's being used on a lot

19:52.200 --> 19:59.800
of autonomous vehicles. Can you maybe talk about some of the relative weaknesses of the stereo

19:59.800 --> 20:08.520
vision based approach? Well, so stereo vision, to do this image match, you need to have some

20:08.520 --> 20:15.320
visual texture because that's what helps you find the same point in both cameras. So if you're in

20:15.320 --> 20:22.600
an environment where you've got a lot of surfaces with very little texture, it's hard to do stereo

20:22.600 --> 20:30.920
vision based range measurement. So think of indoor walls that are painted with very little texture

20:30.920 --> 20:39.800
or outdoors if you're driving on a road that has nice, clean smooth asphalt or concrete that's

20:39.800 --> 20:45.080
not very textured. The only way you can get a range measurement to the middle of those surfaces

20:45.080 --> 20:50.920
with stereo vision is by assuming there's a continuous surface and you basically interpolate

20:50.920 --> 20:58.920
into there. Whereas with the LIDAR, you're admitting LIDAR directly, you need the surface to have

20:58.920 --> 21:06.280
enough reflectivity to bounce that light back to the sensor. What you don't need any texture

21:06.280 --> 21:12.120
per se on the surface and because the LIDAR is admitting its own energy, it actually works better

21:12.120 --> 21:18.920
in the dark than it does during daylight. A stereo vision with typical low cost visible

21:18.920 --> 21:25.560
spectrum cameras, it needs an illuminator to work at night, that illuminator has limited range,

21:25.560 --> 21:32.360
so the new stereo vision system has limited range. The error as a function of distance,

21:33.000 --> 21:39.080
the error in stereo vision based range estimates grows as the square of the distance, whereas in LIDAR,

21:40.920 --> 21:46.600
there's tends to be an analogous error characteristic, but you can design a system to have

21:46.600 --> 21:51.160
typically greater range than a stereo system. On the other hand, as you noted,

21:51.160 --> 21:58.920
the stereo cameras can be quite small and cheap, and what they do for you is they give you,

21:58.920 --> 22:05.160
you get all of your pixels at the same point in time, and you can have a field of view that's

22:05.160 --> 22:10.680
quite large in a horizontal and vertical axis. So you can have a large field of regard,

22:10.680 --> 22:14.920
you can get all of your data simultaneously, which matters if you're moving, because if you're

22:14.920 --> 22:21.000
moving and each pixel comes in at a different point in time, that adds complexity of how do

22:21.000 --> 22:25.800
you relate that all to, if you're trying to build a map of the world and your data is coming in,

22:25.800 --> 22:30.520
it's like in a different point in time, that's a harder problem. Most LIDARs have been scanning

22:30.520 --> 22:37.320
LIDARs, and so they have that issue of motion registration of the pixels. Their LIDARs

22:37.320 --> 22:42.840
called flash LIDARs that give you all of your pixels in the point cloud at the same point in time,

22:42.840 --> 22:49.240
but they typically have narrower fields of regard. So those are some of the trade-offs between the two

22:49.240 --> 22:58.040
sensors. And you also mentioned SLAM a couple of times. How does that tend to work? Well, if you mean

22:58.040 --> 23:03.080
does it work well or poorly, these days it works well, if you need to have to talk with the work

23:04.120 --> 23:14.520
of the latter. Okay, these are all fundamentally least squares problems. So you're setting up

23:14.520 --> 23:22.120
an optimization where you've got measurements, so you've got a set of unknowns. Your unknowns

23:22.120 --> 23:29.800
are the poses for the camera from each picture and the 3D coordinates of all of your landmarks.

23:30.520 --> 23:38.360
So this whole technique has some roots in the field of photogrammetry, which is older than

23:38.360 --> 23:45.960
computer vision. And that was used for aerial surveying for long before computer vision had become

23:45.960 --> 23:53.080
big. And in that community, they called this technique of setting up a large least squares

23:53.080 --> 23:58.680
optimization for all the camera poses and all of the landmarks. They called that bundle adjustment

23:58.680 --> 24:05.560
because they thought of it as a bundle of rays from the camera out to all of the landmarks. And so

24:05.560 --> 24:11.160
your optimization is defined, you know, the positions of all these things that adjust this

24:11.160 --> 24:21.400
bundle of rays to minimize some error measure. So SLAM is basically another term for that kind of

24:21.400 --> 24:28.040
technique. And there's been a lot of progress by, you know, a number of people around the world

24:28.040 --> 24:36.280
in finding ways to do this very computationally, efficiently, incrementally. So historically,

24:36.280 --> 24:42.040
in photogrammetry, people would basically do this offline. They would take all of their measurements,

24:42.280 --> 24:47.880
process them offline, all at once, and give you your map. But a lot of what we want to do. So

24:47.880 --> 24:53.800
in photogrammetry, they came up with this term that I think they called real-time bundle adjustment

24:53.800 --> 24:59.800
where, you know, suppose you take one new picture. Can you add that to your optimization and update

24:59.800 --> 25:05.640
it efficiently? So you get a new map without having to redo all of that computation. So the progress

25:05.640 --> 25:13.480
in SLAM has been finding basically linear algebra tricks to do that more and more efficiently over time.

25:13.480 --> 25:21.160
So that we can, you know, in real-time, take new data and update this network of camera poses and

25:21.160 --> 25:27.320
landmark positions. And it's, you know, it's used more and more sophisticated basically linear

25:27.320 --> 25:33.960
algebra tricks to make it efficient to do this incrementally. So maybe going back to your talk

25:33.960 --> 25:42.120
on the vision systems, your goal was to provide a review of the progress that's been made there,

25:42.120 --> 25:50.920
and some of the challenges that the field has run into and perhaps remain. How far did we

25:50.920 --> 25:56.200
get in this conversation in terms of what you presented there? Are there other pieces that you

25:56.200 --> 26:02.040
can share with us or did we cover the bulk of your talk there? I can add a few tidbits.

26:03.080 --> 26:09.400
So, you know, we talked about how the first vision system, real-time vision system in a planetary

26:09.400 --> 26:17.880
lander was in the Mars exploration rover mission that landed in 2004. We're working on another

26:17.880 --> 26:25.800
rover mission to Mars now that we expect will launch in 2020. And the plan is to have a

26:25.800 --> 26:31.480
precision landing capability in that mission where we do what I was describing, where we have a

26:31.480 --> 26:36.920
downward-looking camera that takes pictures during the sand and onboard and real-time and registers

26:37.560 --> 26:43.480
templates from those pictures to orbital maps. So we're planning to use that in the 2020

26:43.480 --> 26:49.640
rover mission to Mars, so that'll be the first time that's been done. And then looking beyond that,

26:51.640 --> 26:59.800
we are about NASA's interested, very interested in two-pairs moving Europa these days. So we're

26:59.800 --> 27:05.960
studying possibilities for how we land on Europa. So I need to be careful to say there isn't

27:05.960 --> 27:12.600
a fund of mission to do this, but we're studying it and the reason to go there is, you know,

27:12.600 --> 27:19.640
profound scientific questions involved. One of them is, you know, is their life elsewhere

27:19.640 --> 27:26.200
than on Earth? And if there isn't life, are there chemical precursors, you know, kind of on a path

27:26.200 --> 27:31.960
toward the chemistry of life? And we know that Jupiter has a liquid water ocean underneath

27:31.960 --> 27:44.600
a water ice crust, and that kept liquid by gravitational flexing in the graph of the Europa

27:44.600 --> 27:50.280
between the gravity field of Jupiter and some of the other larger moons further away from Jupiter

27:50.280 --> 27:57.080
than Europa. And we can see the effects of that in the surface of Europa, which is all broken up,

27:57.080 --> 28:04.360
basically like ice flows. So the question is, you know, in that liquid water ocean inside

28:04.360 --> 28:12.280
of Europa, could there be chemical processes that are related to the chemical processes of life,

28:12.280 --> 28:20.280
or maybe even microbial life? And that's all very speculative, but we know that one of Saturn's

28:20.280 --> 28:27.480
moons and celadus, it's a fairly small moon, I think it's about 500 kilometers in diameter.

28:29.560 --> 28:36.200
We directly detected water vapor coming out of cracks in the surface of celadus,

28:36.200 --> 28:43.640
and we have pictures from the Hubble Space Telescope of Europa where it looks like there are

28:43.640 --> 28:49.400
water vapor queens coming out of Europa. So that's why we want to go there. The surface of Europa

28:49.400 --> 28:55.400
is extremely rough, much rougher than any place we've tried to land on Mars. The orbital

28:55.400 --> 29:01.560
reconnaissance imagery that we're going to have of Europa is lower resolution than we have for Mars.

29:01.560 --> 29:06.280
So, you know, if we're ever going to land there, we need an intelligent landing system, and so we're

29:06.280 --> 29:12.360
working to extend the techniques that were developed for Mars so that someday they could be applicable

29:12.360 --> 29:19.480
to Europa. You know, elsewhere in the outer solar system, we know there are other places that

29:19.480 --> 29:28.360
have liquid water oceans inside moons, Saturn's moon, Titan is one of those, Titan is unique

29:28.360 --> 29:36.040
in the solar system outside of Earth, in that there's a lot of organic molecules on the surface

29:36.040 --> 29:42.360
of Titan. The surface of Titan is 94 Kelvin, so it's really, really cold, and there are lakes and

29:42.360 --> 29:49.160
seas of liquid methane. So, between those organic molecules on the surface and the liquid water

29:49.160 --> 29:56.200
ocean inside, if that liquid water ever came in contact with all the organics is a lot of

29:56.200 --> 30:01.160
potential interesting chemistry that could happen, that could build up more and more complex

30:01.160 --> 30:08.680
organic molecules, and that might teach us some things ultimately about biology. So, we're

30:08.680 --> 30:13.080
interested in landing there, and that's in in in some respects that's even more challenging.

30:13.800 --> 30:18.520
The terrain is not that bad, but the remote sensing data is even lower resolution than we would

30:18.520 --> 30:24.600
have at Europa, so we have to develop techniques that can cope with that. And then someday, you know,

30:24.600 --> 30:32.360
we had also interested in Venus, you know, there have been landers on Venus, the Soviets put landers

30:32.360 --> 30:38.840
on Venus, but they went to some of the most benign terrain there, so to go to more challenging areas

30:38.840 --> 30:43.960
on Venus. It's going to be hard, you know, Venus has the densest atmosphere in the solar system

30:43.960 --> 30:49.640
by far, and the hottest temperature is on the surface by far, and it's got a very opaque cloud layer,

30:49.640 --> 30:55.640
so a very hard navigation problem on Venus, and we don't know how to solve that yet.

30:55.640 --> 31:01.160
From a computer vision perspective, clearly, you know, when you're looking at these different

31:01.160 --> 31:11.000
missions, there are huge system engineering types of challenges adapting to the constraints of

31:11.000 --> 31:18.280
a given mission. How do the algorithmic requirements change the way you approach each individual

31:18.280 --> 31:22.760
mission, or rather, how does each individual mission change the way you approach the algorithms,

31:22.760 --> 31:30.920
I guess? Yeah, so like I said at the start, we're mission oriented, and the way NASA works,

31:30.920 --> 31:37.400
at least the planetary science, really I think all of all of NASA's science, is we started with

31:37.400 --> 31:42.120
what are the scientific questions you want to answer, and then you work backwards from that,

31:42.120 --> 31:49.880
so what technology do you need, and that's always guided by what can we afford, and what has an

31:49.880 --> 31:57.000
acceptable level of risk. So it's kind of case by case, we look at the science we want to do at

31:57.000 --> 32:04.040
a particular planet or moon, and then how do we do that science, and to, you know, minimize the

32:04.040 --> 32:09.880
cost and the risk, we ask, what can we leverage that we've built before, so it minimizes the

32:09.880 --> 32:15.560
cost of new development and minimizes the risk that it's going to work, so you have to find a

32:15.560 --> 32:22.600
happy medium where you find an engineering solution that gives us the science within affordable cost

32:22.600 --> 32:29.960
and acceptable risk, so this is all motherhood, but that is the process. You also did another

32:29.960 --> 32:38.040
presentation at CVPR on onboard stereo vision for drone pursuit. Can you talk a little bit about

32:38.040 --> 32:45.640
that one? Yeah, so, you know, as I was saying, we try to have complimentary work going on where we

32:45.640 --> 32:54.280
have some long-term objectives for capabilities to develop for NASA, and our charter includes

32:54.280 --> 33:01.640
applying unique expertise to problems in other domains that are complimentary, so, you know,

33:01.640 --> 33:06.440
autonomous navigation of drones is one of those areas that's complimentary and can put an

33:06.440 --> 33:14.280
plug where we're developing. The first ever rotograph for Mars was recently approved to be a

33:14.280 --> 33:21.880
technology demonstration on that 2020 Mars rover mission, so, you know, the work that we were

33:21.880 --> 33:28.840
doing in drone pursuit or sense in the void is complimentary to those things, so, and that was

33:28.840 --> 33:36.840
funded by the Army Research Lab. So, we were inspired by, you know, if you want to have a team

33:36.840 --> 33:41.800
of cooperating drones doing anything, you don't want them to collide with each other,

33:43.000 --> 33:49.720
and in some applications, you might want to minimize communication, so you don't necessarily want

33:49.720 --> 33:56.920
the drones exchanging a lot of information. You may not have access to good external localization

33:56.920 --> 34:05.000
sensors like GPS, so you needed to all be on board, and so we thought, you know, long-term,

34:05.000 --> 34:11.320
you'd really like the drones to be able to detect each other with their own onboard sensors,

34:11.320 --> 34:17.560
so they know that they're present, they can estimate their position and velocity of other nearby

34:17.560 --> 34:24.440
drones, and then, you know, do their own path planning and control with awareness of other

34:24.440 --> 34:32.200
nearby aircraft. So, our initial motivation was cooperative teams, but these techniques are

34:32.200 --> 34:37.720
equally applicable to situations where it's not cooperative, where you might want to be pursuing

34:37.720 --> 34:46.120
another drone, and that's relevant to, you know, what people call counter UAS, or counter

34:46.120 --> 34:53.000
unmanned air systems, so there's a lot of concern about small drones being used for hostile purposes.

34:53.000 --> 35:00.520
So, drone pursuit, when we're doing our research just logistically, it turned out to be easier

35:00.520 --> 35:06.840
to do experiments in the pursuit scenario than in the cooperative scenario, so the paper ended up

35:08.200 --> 35:12.840
describing techniques that were relevant to both the cooperative team scenario and the pursuit

35:12.840 --> 35:18.520
scenario, but all of our testing was in the pursuit scenario, and then the stereo vision aspect

35:18.520 --> 35:24.360
comes back to our earlier discussion about the relative pros and cons of stereo vision versus

35:24.360 --> 35:32.440
LiDAR, so here you've got very limited payload, weight, and power capacity on small drones,

35:32.440 --> 35:38.600
so if we can do something with cameras, especially if those cameras can give us a very wide,

35:38.600 --> 35:44.360
instantaneous field of regard in horizontal and vertical axes, you just can't match that with

35:44.360 --> 35:51.800
the LiDAR these days, and these are both techniques that direct with that measure 3D range,

35:51.800 --> 35:57.080
and through that you can estimate velocity of the other aircraft, so it's quite handy if you actually

35:57.080 --> 36:02.920
know the 3D position and velocity of the other aircraft, so we started with techniques,

36:02.920 --> 36:08.360
in particular stereo vision, that would give us that, those do have range limits, so we can only see

36:08.360 --> 36:14.840
other aircraft, you know, some limited distance away, 10, 15, maybe 20 meters at the outside with

36:14.840 --> 36:23.880
our current cameras, so this was the first paper to show that this was possible and a fully

36:23.880 --> 36:31.960
integrated fashion outdoors without the aid of external position sensing sensors, you know,

36:31.960 --> 36:38.920
there needs to be extensions to be able to see other drones further away, you know, stereo

36:38.920 --> 36:43.240
vision is probably not going to cut it for that, so that probably requires techniques that can

36:43.240 --> 36:50.600
use a single camera, those require more work to develop and test than was within the scope of our

36:50.600 --> 37:00.760
paper. And from the algorithmic perspective is the system used here, primarily a composition of

37:00.760 --> 37:07.400
some of the things that we've already talked about, or are there other techniques that are specific

37:07.400 --> 37:14.520
to this system that might be interesting to explore? Well, the work in this paper was most of

37:14.520 --> 37:20.520
the competition of techniques we've already talked about, so, you know, in some respects,

37:20.520 --> 37:26.040
it's analogous through the constraints we have in space, because onboard computation is very

37:26.040 --> 37:31.320
limited, you know, it's a lot more than we have in space, but it's still pretty limited, so we

37:31.320 --> 37:39.480
were using simple techniques to do reliable 3D perception as shore rings that let us do a first

37:39.480 --> 37:47.320
demonstration of this capability. One of the questions I was asked was, could you use deep learning

37:47.320 --> 37:53.720
to detect the other aircraft? It's a very good question. We had not yet tried that for lack of

37:53.720 --> 38:03.160
training data, but that's certainly conceivable. Another thing that could be used is basically

38:03.160 --> 38:09.800
difference imaging, so, you know, a standard technique in surveillance with stationary cameras

38:09.800 --> 38:15.160
is just to take, you know, pictures over time and subtract them and look at what's changed,

38:15.160 --> 38:21.480
and that lets you pick up things that are moving around the camera, but that's harder to do when

38:21.480 --> 38:27.240
the camera itself is moving, and so that was one of the complexities that was out of scope of

38:27.240 --> 38:36.040
this work, but another approach to detects moving drones from a drone that itself is moving is

38:36.040 --> 38:42.120
background subtraction, but then you have to do motion compensation for the motion of your own drone.

38:42.120 --> 38:48.360
That's something to look at in the future. In the paper itself, you showed standard black and

38:48.360 --> 38:55.080
white images, but then also these multicolored images look like some kind of spectral type of

38:55.080 --> 39:01.480
image or something. What are those? I'm thinking you're probably referring to what we call depth maps,

39:02.120 --> 39:11.000
so yeah, that's a false color illustration of the range at each pixel. So when we do stereo

39:11.000 --> 39:15.960
vision, we get an estimate of range at each pixel. You know, a simple way to visualize that is

39:15.960 --> 39:22.680
just a color code, the range, and then show that as a pixel image. Got it. And so this work that you

39:22.680 --> 39:28.600
did, this was a full end-to-end system, not just the, it wasn't a simulation, you actually put

39:28.600 --> 39:32.920
this on a drone and put it out in the wild, is that right? That is right. How did it do?

39:32.920 --> 39:44.360
Yeah. Well, it did quite well. We tested it on the campus of JPL and on the campus of one of our sponsors.

39:47.240 --> 39:52.280
This problem is a lot easier if the other drone you're looking at is silhouette against the sky.

39:52.280 --> 39:57.160
Then it's actually a very easy problem. So it's much harder if the other drone you're looking at

39:57.160 --> 40:03.960
is seeing against what we call background clutter. So, you know, if it's at, if it's below the horizon

40:03.960 --> 40:09.720
essentially. So if you're seeing it against trees or buildings or ground and that changes over time,

40:10.520 --> 40:15.720
it becomes much harder if you look toward the sun because those, you know, sun causes all sorts

40:15.720 --> 40:22.280
of glare artifacts in the image. And we found that a key advantage of our technique is that it's

40:22.280 --> 40:28.840
pretty robust to those issues because the fact that we're using 3D perception means as long as

40:28.840 --> 40:34.200
the drone is within range, we're not fooled by background clutter because we can segment that 3D

40:34.200 --> 40:39.560
object from the background using the point cloud there. And when we look toward the sun and there

40:39.560 --> 40:46.280
are artifacts in the image, you know, unless things get really, really bad, we can often still

40:46.280 --> 40:52.600
do 3D perception that is adequate to track the drone. And since we've, you know,

40:52.600 --> 40:58.280
let's say before you turn directly at the sun, you had a 3D model, you had a model of the position

40:58.280 --> 41:03.800
and velocity of the other drone from previous images. So you can do prediction for a short amount

41:03.800 --> 41:09.880
of time, you know, if you're blinded by the sun and then probably recover track because you've

41:09.880 --> 41:16.440
got that knowledge, model based knowledge that you can use for prediction. So it didn't work pretty

41:16.440 --> 41:23.640
well. And how do you measure the performance of the system? Do you kind of go through after the

41:23.640 --> 41:28.760
fact and measure maybe distance from where the drone thought the thing it was supposed to be

41:28.760 --> 41:35.240
following was and, you know, compare that to some error function or is there some automated way

41:35.240 --> 41:41.720
that you're able to measure performance? So, you know, that's an excellent question. You always

41:41.720 --> 41:49.880
need some kind of ground truth to measure performance. In our case, we manually label the images.

41:49.880 --> 41:55.880
So we, you know, we marked what bounding boxes are on the drone. Another way to do it would be if

41:55.880 --> 42:02.680
we had, you know, an external sensor that told us where everything is. So if we had GPS, for example,

42:02.680 --> 42:10.680
on both aircraft, you could use that to give position knowledge. And in this case, I think we relied

42:10.680 --> 42:17.000
on manual labeling. We could have the sensor infrastructure finished to do that all with GPS.

42:18.680 --> 42:23.720
But that would help automate things. Well, great. Larry, this was a really interesting,

42:24.920 --> 42:29.240
really interesting conversation. I appreciate you taking the time to share with us what you're

42:29.240 --> 42:33.640
working on. Well, thank you for the time to talk about it. I appreciate that. Thanks.

42:36.680 --> 42:42.520
All right, everyone. That's our show for today. For more information on Larry or any of the topics

42:42.520 --> 42:50.200
covered in this episode, head over to twimmelai.com slash talk slash 170. If you're a fan of the pod,

42:50.200 --> 42:55.480
we'd like to encourage you to pop open your Apple or Google podcast app and leave us a five-star

42:55.480 --> 43:00.280
rating and review. Your reviews go a long way in helping new listeners find the show.

43:00.280 --> 43:30.120
As always, thanks so much for listening and catch you next time.

