WEBVTT

00:00.000 --> 00:13.400
Welcome to the Tumel AI Podcast.

00:13.400 --> 00:16.400
I'm your host Sam Charrington.

00:16.400 --> 00:24.480
Hey, what's up everyone?

00:24.480 --> 00:29.920
I recently attended the AWS Reinvent Conference in Las Vegas, and I'm excited to share

00:29.920 --> 00:38.960
a few of my many interesting conversations from that event here on the podcast this week.

00:38.960 --> 00:44.160
Before we dive in, I'd like to thank our friends at Capital One for sponsoring our Reinvent

00:44.160 --> 00:45.760
series.

00:45.760 --> 00:50.120
Capital One has been a huge friend and supporter of this podcast for some time now, and

00:50.120 --> 00:54.600
I'm looking forward to sharing my interview with Dave Castillo, Capital One's managing

00:54.600 --> 00:58.640
VP of machine learning with you on Thursday.

00:58.640 --> 01:02.400
Dave and I discussed the unique approach being taken at the company's center for machine

01:02.400 --> 01:07.480
learning, as well as some of the interesting AI use cases being developed at the bank,

01:07.480 --> 01:11.880
and the platform they're building to support their ML and AI efforts.

01:11.880 --> 01:17.640
To learn more about Capital One's machine learning and AI efforts and research, visit capitalone.com

01:17.640 --> 01:20.960
slash tech slash explore.

01:20.960 --> 01:25.760
And now on to the show.

01:25.760 --> 01:28.880
All right, everyone.

01:28.880 --> 01:34.560
We are here at Reinvent in Las Vegas, and I am with Dave Castillo.

01:34.560 --> 01:39.360
Dave is the managing vice president for machine learning at Capital One.

01:39.360 --> 01:41.640
Dave, welcome to the Twimal AI podcast.

01:41.640 --> 01:42.640
Oh, thanks, Sam.

01:42.640 --> 01:44.040
Happy to be here.

01:44.040 --> 01:47.400
Let's just jump right in and talk a little bit about your background.

01:47.400 --> 01:51.240
Tell us about your work in the machine learning field.

01:51.240 --> 01:52.240
Absolutely.

01:52.240 --> 01:54.160
So I'm actually very fortunate.

01:54.160 --> 01:59.200
I got to start machine learning way back when it was still handcrafted knowledge, and

01:59.200 --> 02:01.400
we were really focused on knowledge representation.

02:01.400 --> 02:02.800
So I started out in the NASA days.

02:02.800 --> 02:10.440
I did a lot of work for NASA, and it was about solving problems that were very theoretical,

02:10.440 --> 02:14.240
but never really saw the light of day in a production setting.

02:14.240 --> 02:16.040
That wasn't NASA?

02:16.040 --> 02:20.520
Actually, I worked with all the NASA bureaus, but I was stationed actually in Florida,

02:20.520 --> 02:27.520
and I kept the Kennedy Space Center, Johnson Space Center, and JPL, Jet Propulsion Lab.

02:27.520 --> 02:30.640
So I worked with all those three, and it was a lot of fun.

02:30.640 --> 02:35.760
We did robotic vision systems, which are obviously a whole different technology set back

02:35.760 --> 02:37.480
then, than convolutional known.

02:37.480 --> 02:44.120
That sort of annihilated all of that work, and put a lot of people out of work in the

02:44.120 --> 02:45.120
process.

02:45.120 --> 02:51.680
I started with the start of there, and really watched the transition go from knowledge representation

02:51.680 --> 02:57.680
into statistical learning, into machine learning, into deep learning, and then explainability

02:57.680 --> 03:05.760
on top of that, and then more recently, just really looking into how do we do large-scale

03:05.760 --> 03:09.240
graph related machine learning with graph machine learning.

03:09.240 --> 03:11.600
So I've had this very diverse career.

03:11.600 --> 03:17.520
graph had two startups, wind up in the media industry, as a CTO of an advertising company

03:17.520 --> 03:24.640
in New York City for several years, and I left that in 2015 and joined banking with a company

03:24.640 --> 03:30.200
called Early Warning, most popular for Zell, the master payment system, and I ran machine

03:30.200 --> 03:34.000
learning there, the innovation labs, and their data transformation.

03:34.000 --> 03:37.720
That company's owned by the seven largest banks in the U.S., including Capital One.

03:37.720 --> 03:40.400
That's how I got familiar with Capital One.

03:40.400 --> 03:46.240
I started here last June in 2019, 2018, sorry.

03:46.240 --> 03:47.240
Right, right.

03:47.240 --> 03:49.000
So what's your role at Capital One?

03:49.000 --> 03:56.400
I run what's called the Center for Machine Learning, and it's a fairly large organization.

03:56.400 --> 04:01.280
It's 200 plus people focused on a center of excellence, and I can get into a little bit

04:01.280 --> 04:03.480
more of what that means later if you like.

04:03.480 --> 04:04.480
Yeah.

04:04.480 --> 04:09.360
It's folks to Adam Wenzhel on the podcast about a year and a half ago, and he went off

04:09.360 --> 04:13.440
to do a startup, I think, now, is that the same, are you in the same role?

04:13.440 --> 04:15.520
I'm in the same role, absolutely.

04:15.520 --> 04:23.320
I actually met Adam, and I thank Adam for the incredible job he did in amassing a great

04:23.320 --> 04:28.360
talent to sort of incubate the Center for Machine Learning, and Adam had to start in

04:28.360 --> 04:34.120
a place where he was really the skill set, and the expertise around machine learning,

04:34.120 --> 04:42.160
so the machine learning opportunities and solutions all came in and fanned into Adam's organization.

04:42.160 --> 04:43.560
And that's no longer the case now.

04:43.560 --> 04:48.320
We've been able to transform that where we can scale it, and we federated a lot of

04:48.320 --> 04:49.760
capability out.

04:49.760 --> 04:54.880
So our model is more of a model of enablement now, where we actually join teams.

04:54.880 --> 05:02.040
We provide skill sets, technology, platforms, tools, education, boot camps, basically just

05:02.040 --> 05:05.960
trying to empower the other teams to be self-sufficient, and we stay with them for a

05:05.960 --> 05:08.000
period of time, and then we extract ourselves.

05:08.000 --> 05:14.040
Yeah, one of the things that I'm seeing all over the enterprise landscape, it's happening

05:14.040 --> 05:20.600
right now is this kind of transition from doing machine learning in kind of a lab type

05:20.600 --> 05:27.280
of environment, doing a lot of experimentation to, and alongside that, you know, selling

05:27.280 --> 05:31.000
it across the enterprise, and now it seems like everyone's bought in, everyone wants to

05:31.000 --> 05:36.560
figure out how to do it, how to build models, get their pet model into production, and

05:36.560 --> 05:41.840
it's causing shifts in the way organizations are organizing it to support that.

05:41.840 --> 05:44.760
It sounds like you're going through a similar transition.

05:44.760 --> 05:50.640
Yeah, so 18 months ago, we had a lot of sighted desks, bespoke initiatives, where people

05:50.640 --> 05:56.360
were creating their own solutions, they were very narrow, solve very narrow use cases,

05:56.360 --> 06:01.200
and they had to go through all the pain associated with getting something from a proof of concept

06:01.200 --> 06:07.080
into whether it be a challenger model or ultimately into production, and what's happened since

06:07.080 --> 06:11.920
then is that we've become highly collaborative because we understand the challenges associated

06:11.920 --> 06:18.600
with getting a machine learning into production in a well-managed, compliant way, and that's

06:18.600 --> 06:23.560
not an easy task, especially when you have a material model, a model that's under regulatory

06:23.560 --> 06:29.840
scrutiny, and even our immaterial models, which are not so heavily scrutinized, but still

06:29.840 --> 06:33.680
require a well-managed discipline about them.

06:33.680 --> 06:38.440
Anyway, there was a sentiment about how do we actually take all these learnings and come

06:38.440 --> 06:44.320
together as an enterprise and leverage what each other has done, and throw away some

06:44.320 --> 06:49.280
of these sighted desks solutions that don't scale, and really look toward more of an enterprise

06:49.280 --> 06:50.440
offering.

06:50.440 --> 06:55.520
And we've done a lot of work in that area, so now we have collaboration, Center for Machine

06:55.520 --> 07:00.440
Learning Collaborates with all the lines of business, we're driving an enterprise, what

07:00.440 --> 07:05.880
we call the enterprise machine learning ecosystem now, it's driven by myself and another guy

07:05.880 --> 07:10.800
named Annie Gupta, who's in our card business, and underneath that initiative, we have our

07:10.800 --> 07:17.000
feature platform, our machine learning platform, and our monitoring platform, all really

07:17.000 --> 07:24.280
looking at how do you go from inception to deployment, and then keep the 360 moving through

07:24.280 --> 07:28.320
monitoring and refit and retraining.

07:28.320 --> 07:33.080
So there's been a huge sort of awakening that has happened in Capital One, it's really

07:33.080 --> 07:39.120
exciting to see everyone coming together, it's not so territorial anymore, it's really,

07:39.120 --> 07:41.480
how do we help each other actually get to the finish line?

07:41.480 --> 07:42.680
Awesome, awesome.

07:42.680 --> 07:46.760
Before we dig into a little bit of the platform that you've built, can we talk a little

07:46.760 --> 07:49.120
bit about the use cases?

07:49.120 --> 07:57.880
I have spoken to folks at Capital One that are doing things like Fraud and AML, and there

07:57.880 --> 08:03.320
are a bunch of things that are kind of obvious ones for a financial services company, maybe

08:03.320 --> 08:07.680
we can talk about the broad landscape, but I'm also curious if you can share any ones

08:07.680 --> 08:14.600
that are maybe less obvious or more interesting or that you're personally excited about.

08:14.600 --> 08:19.320
We've been Capital One, we are looking at applying machine learning across every facet

08:19.320 --> 08:24.080
of our business, and we've done some very cool and interesting things that you wouldn't

08:24.080 --> 08:28.720
think it would be associated with a bank, so for example, when you join Capital One,

08:28.720 --> 08:33.160
you get assigned access privileges to data and to systems, that's all done by machine

08:33.160 --> 08:34.640
learning today.

08:34.640 --> 08:40.040
And many as an employee, as an employee, when you join, and that's something that we built.

08:40.040 --> 08:46.120
So that's in production, and it's actually compressed the time for associates to get

08:46.120 --> 08:50.240
up to speed and actually be productive because they've had to wait weeks and weeks and weeks

08:50.240 --> 08:56.400
to get access to certain data, to certain systems, and now it's all assigned based on these

08:56.400 --> 08:57.560
algorithms.

08:57.560 --> 08:58.920
That's kind of cool.

08:58.920 --> 09:03.440
We have space allocation that happens when we look at how badges are navigating through

09:03.440 --> 09:09.560
our buildings, and we can look at how to plan space for people, and that's an interesting

09:09.560 --> 09:11.400
application of machine learning.

09:11.400 --> 09:18.480
We did one last year where we're assigning our first year associates, they go on a rotation.

09:18.480 --> 09:24.200
So the next job is an assignment that's actually a recommendation engine that's actually

09:24.200 --> 09:27.320
made by a machine learning algorithm for that job assignment.

09:27.320 --> 09:31.920
That was all done manually on a huge spreadsheet, it was very painful for people to do that

09:31.920 --> 09:32.920
across the board.

09:32.920 --> 09:35.040
That's all automated.

09:35.040 --> 09:42.080
When we're doing, what I used to do in the ad tech industry, we're doing adward bidding

09:42.080 --> 09:48.920
with multi-arm bandits, we're doing creative and personalization, aspiring toward getting

09:48.920 --> 09:55.160
granular and personalization with multi-arm bandits, a tiny bit of reinforcement learning.

09:55.160 --> 10:02.840
So those are not typical of mapping, but I think it's a testimony of that we're serious

10:02.840 --> 10:06.960
about where we want to apply this technology.

10:06.960 --> 10:14.080
There's a plethora of other examples I could give you on reading documents from our vendors

10:14.080 --> 10:18.880
and extracting salient information out of those documents, etc.

10:18.880 --> 10:23.680
The thing that jumped out to me about a couple of those examples, the space planning and

10:23.680 --> 10:31.600
the employee example, the onboarding example, is that the functions that own those processes

10:31.600 --> 10:40.200
are HR and facilities, probably not necessarily the most technology forward or AI-enabled organizations.

10:40.200 --> 10:46.200
So did they come to you with those opportunities and ideas, or is this part of the COE process

10:46.200 --> 10:48.280
where you're already embedded with them?

10:48.280 --> 10:52.040
How does the ideation process work there?

10:52.040 --> 10:55.320
Yeah, it was really an outward reach on our part.

10:55.320 --> 10:59.880
And then once we started to educate as to what was possible, then dialogue started happening

10:59.880 --> 11:05.680
within these non-technical organizations, and then something was more and out of that.

11:05.680 --> 11:07.240
So yeah, it was an outward reach.

11:07.240 --> 11:13.040
We started up a year ago, we started up what we call advanced ML within the Center for

11:13.040 --> 11:14.360
Machine Learning.

11:14.360 --> 11:18.160
This is a new group that is focused on three things.

11:18.160 --> 11:22.920
One is we had to get better about managing our university research partnerships, and

11:22.920 --> 11:25.000
we can talk about that a little bit later.

11:25.000 --> 11:30.440
The second item was we wanted to initiate an advocacy function where we could talk to

11:30.440 --> 11:36.920
our lines of businesses about what we were doing and how we might look at doing their

11:36.920 --> 11:40.920
jobs a little bit differently with machine learning, not necessarily automating the tasks

11:40.920 --> 11:44.640
that they're doing, but thinking about their jobs differently.

11:44.640 --> 11:53.560
The third component of that was really focused on getting a collaboration across Capital

11:53.560 --> 12:02.400
One on what are the challenges that we're all facing when it comes to machine learning,

12:02.400 --> 12:08.840
whether it is at the beginning of the machine learning lifecycle or at the end, let's identify

12:08.840 --> 12:13.560
what those challenges are, and then let's look at trying to come together as an organization

12:13.560 --> 12:17.360
rather than as individual victims to solve them.

12:17.360 --> 12:22.840
Yeah, it strikes me that when thinking about the types of examples that you gave, there

12:22.840 --> 12:26.480
have to be so many examples like that.

12:26.480 --> 12:35.720
But there are also these very high value, cord of financial services, types of use cases

12:35.720 --> 12:41.760
that I guess I'm trying to formulate a question around how do you prioritize when you've

12:41.760 --> 12:49.560
got this broad portfolio of opportunities, some of which are cord of the business and

12:49.560 --> 12:58.120
drive revenue and profitability and others are supporting organizations that aren't necessarily

12:58.120 --> 13:03.280
central to that conversation or aren't not often central to that conversation.

13:03.280 --> 13:06.480
Again, the HR facilities, that kind of thing.

13:06.480 --> 13:11.920
This group that I mentioned earlier, the advocacy group, is really was meant to bring all

13:11.920 --> 13:20.560
these people together, and then what that group also does is it opines on what are the

13:20.560 --> 13:24.400
higher priority areas that we should be focused on.

13:24.400 --> 13:29.400
There's a lot of discussion, the group got very large, it got to like 35 people across

13:29.400 --> 13:34.400
the company, and so we actually split it up into sort of a hierarchy of groups.

13:34.400 --> 13:39.640
But the idea is that there are high priority opportunities for us that we definitely have

13:39.640 --> 13:46.360
to be plugged into, but there is also enough bandwidth with this new enablement model that

13:46.360 --> 13:52.440
we can actually address some of these non-high priority use cases, just to really allow us

13:52.440 --> 13:57.080
to get some bedrock out there in terms of proliferating and democratizing ML.

13:57.080 --> 14:02.520
But certainly the high-price items get discussed and they are triaged, and it's no longer

14:02.520 --> 14:06.560
someone in the center for machine learning saying, I think this is the highest priority,

14:06.560 --> 14:08.760
this is coming from the business now.

14:08.760 --> 14:13.640
That's a flip on how we operated a year and a half ago.

14:13.640 --> 14:18.320
So it sounds like kind of putting some of the pieces together, transitioning the models

14:18.320 --> 14:25.560
of one where you don't necessarily own the development of the model and getting it into production,

14:25.560 --> 14:30.080
but you're supporting the business, as well as the platform and infrastructure pieces

14:30.080 --> 14:36.680
that you mentioned have created a velocity or acceleration or efficiency to being able

14:36.680 --> 14:41.320
to work on these projects that allows you to get more done in more areas.

14:41.320 --> 14:47.880
So most of the models are owned by the owners of the use cases, so the lines of business

14:47.880 --> 14:51.960
if you will, or even our tech partners, because we often partner with our tech partners

14:51.960 --> 14:54.800
for anomaly detection, let's say.

14:54.800 --> 14:59.000
So they own those use cases and they will typically own those models.

14:59.000 --> 15:05.840
There are some models that we own as well, and that we will start and take into production.

15:05.840 --> 15:12.480
But what we're trying to do really is to empower others to be self-sufficient and to be able

15:12.480 --> 15:19.640
to leverage these best practices, leverage the tools and tech that either are emerging

15:19.640 --> 15:26.000
or have been built so that we can do things in a very consistent, well-managed and again,

15:26.000 --> 15:27.600
compliant way.

15:27.600 --> 15:36.800
Can you give us a sense of scope for how many data scientists exist within Capital One

15:36.800 --> 15:42.560
relative to the number within your COE?

15:42.560 --> 15:49.840
Just trying to understand the leverage that you're trying to create with the COE, and

15:49.840 --> 15:53.160
maybe to just tack another question onto that.

15:53.160 --> 16:01.160
One of the things that I see a lot of in industry is the shifting away from thinking of the

16:01.160 --> 16:08.040
data scientists as this monolithic unicorn that can do everything from statistical analysis

16:08.040 --> 16:14.320
to production-ready code, and you are starting to see a lot more of machine learning engineers

16:14.320 --> 16:20.520
as a separate and distinct role that works with the data scientists as part of the team

16:20.520 --> 16:27.160
and brings more of a software engineering perspective and wondering how that plays out

16:27.160 --> 16:28.360
at Capital One.

16:28.360 --> 16:35.520
There are data scientists within the lines of business that have come up through the traditional

16:35.520 --> 16:41.080
ranks of statistical analysis and data science.

16:41.080 --> 16:48.560
There have been transformations in those skill sets to more toward understanding how to

16:48.560 --> 16:53.400
address this life cycle for machine learning, so the machine learning engineering part

16:53.400 --> 16:55.160
if you will.

16:55.160 --> 17:02.480
We have worked very closely with our tech college organization to do a couple of things.

17:02.480 --> 17:10.600
One is to provide curriculum on how to round out a data scientist to understand what that

17:10.600 --> 17:16.760
whole spectrum looks like, and also a rescilling program that we have not officially launched

17:16.760 --> 17:23.560
yet, we developed all the content for, but to take data engineers or software engineers

17:23.560 --> 17:30.440
and to get them converted into machine learning engineers, this rescilling program is all

17:30.440 --> 17:32.200
about that.

17:32.200 --> 17:38.360
Within the Center for Machine Learning, we do focus on bringing in machine learning engineers

17:38.360 --> 17:42.680
so that they will have a little software engineering, they will have data science, they will have

17:42.680 --> 17:52.800
DevOps, data engineering, and so they tend to cover all the entire life cycle of machine

17:52.800 --> 17:55.640
learning if you will.

17:55.640 --> 18:04.080
But as you might expect, there isn't enough supply of those types of people, but we are

18:04.080 --> 18:09.480
seeing the lines of business actually hiring this type of talent and then it becomes sort

18:09.480 --> 18:16.600
of viral, it's contagious and they are able to leverage that skill set and teach others

18:16.600 --> 18:21.880
and get some critical mass around the MLE.

18:21.880 --> 18:27.400
We still call our machine learning engineers software engineers from an organizational

18:27.400 --> 18:35.000
perspective, it's just kind of a legacy thing, but what I would say is that the concentration

18:35.000 --> 18:40.600
of machine learning engineers within the Center for machine learning is probably at

18:40.600 --> 18:46.360
par with some of the largest lines of business that we have.

18:46.360 --> 18:52.520
Now they certainly have many more business analysts, many more data scientists than we have

18:52.520 --> 18:54.680
because they have to.

18:54.680 --> 19:02.040
And as we begin to automate a lot of these processes involved in the machine learning life cycle,

19:02.040 --> 19:08.040
these data scientists are going to be a lot more capable of doing much more than they

19:08.040 --> 19:14.920
have before because they don't know how to do a deployment, it's going to be push button.

19:14.920 --> 19:19.920
If they don't know how to access a feature set, that's going to be a library call from

19:19.920 --> 19:23.680
a Python notebook, it's going to be fairly easy for them.

19:23.680 --> 19:32.880
And so part of our strategy for our platform or our MLE ecosystem is to really take a

19:32.880 --> 19:40.240
giant step, a step function, if you will, to make these data scientists a lot more capable.

19:40.240 --> 19:46.920
While we're also teaching them the rest of the, all the other parts that encompass what

19:46.920 --> 19:49.760
you need to know for machine learning engineering.

19:49.760 --> 19:57.320
Because your organization end up owning the MLE ecosystem, the platform and responsibility

19:57.320 --> 20:02.920
for its maintenance and upkeep and all that, or you more defining the requirements and

20:02.920 --> 20:09.760
partnering with an IT or other technology organization to get it out and keep it up.

20:09.760 --> 20:13.640
Now we're an enterprise function, so that's part of the new identity we've taken on is

20:13.640 --> 20:19.000
that we are going to own and support production systems, production machine learning systems.

20:19.000 --> 20:26.320
That is a new responsibility that has come into our organization and we embrace that and

20:26.320 --> 20:29.240
we look forward to dealing with that.

20:29.240 --> 20:35.080
Yeah, going back to the platform you mentioned, feature store feature component.

20:35.080 --> 20:39.160
And I guess this is fresh on my mind because of a conversation I had yesterday.

20:39.160 --> 20:49.480
But when a data scientist or an MLE engineer builds a feature and contributes it to a feature

20:49.480 --> 20:54.920
store type of environment, there's a certain level of responsibility now for that feature

20:54.920 --> 21:01.560
it becomes kind of a mini product in a sense and it limits them in the way they evolve it

21:01.560 --> 21:02.560
potentially.

21:02.560 --> 21:06.400
They are maybe an implied support requirement.

21:06.400 --> 21:10.000
Anything necessarily specific to features but anytime you're trying to take advantage

21:10.000 --> 21:15.880
of reuse, there's this implied contract that comes from kind of contributing your code

21:15.880 --> 21:17.480
or your feature.

21:17.480 --> 21:23.280
Is that an issue that you grapple with and how does it play out at Capital One?

21:23.280 --> 21:31.080
Yeah, that's actually a very good point and it's like careful what you wish for, because

21:31.080 --> 21:34.640
there's always the flip side of something that you have to deal with.

21:34.640 --> 21:41.200
One who contributes a feature to the feature platform has to understand the responsibility

21:41.200 --> 21:44.680
that goes with that and there is a responsibility.

21:44.680 --> 21:49.440
There's a responsibility to be able to field questions on that feature.

21:49.440 --> 21:53.240
There's a responsibility stand behind the engine that calculates that feature.

21:53.240 --> 21:56.960
There's a responsibility to have gone through the governance associated with that feature

21:56.960 --> 21:59.120
and the use cases that it serves.

21:59.120 --> 22:01.080
And so it's a much bigger deal.

22:01.080 --> 22:05.800
We can't just casually commit something back, you have to understand exactly what you're

22:05.800 --> 22:10.000
doing and where that accountability lies.

22:10.000 --> 22:11.400
The same thing is true with the platform.

22:11.400 --> 22:16.080
When we took that on, we had to understand that there may be calls at two o'clock in the

22:16.080 --> 22:18.800
morning associated with this platform.

22:18.800 --> 22:21.480
The Center for Machine Learning has never had that before.

22:21.480 --> 22:30.000
And when we added talent to the Center for Machine Learning, we brought people in who

22:30.000 --> 22:36.800
had production experience, enterprise, platform experience, because a lot of people there were

22:36.800 --> 22:39.840
machine learning engineers, some of them were fresh out of school, some of them were fresh

22:39.840 --> 22:41.760
out of their PhD programs.

22:41.760 --> 22:46.240
Not many really had that level of experience, so we had to retool and we brought in a very

22:46.240 --> 22:50.920
large group to integrate into the Center for Machine Learning.

22:50.920 --> 22:56.480
But the mindset is that there is an operational accountability that we have and that anyone

22:56.480 --> 23:02.320
is going to have who contributes features into the feature platform.

23:02.320 --> 23:06.160
Now if you think about our data transformation, the same thing is true there.

23:06.160 --> 23:11.920
If you manufacture data, you're almost in the same position where you have to be able

23:11.920 --> 23:16.360
to, you know, you're going to put data into an ecosystem and make it available for someone

23:16.360 --> 23:21.000
else and there has to be some level of accountability that goes with that.

23:21.000 --> 23:24.840
It just can't be something that you casually contribute.

23:24.840 --> 23:30.640
So this is all part of our well-managed initiative and I think, you know, to scale machine learning

23:30.640 --> 23:35.560
across our organization to really transform banking, we have, we understand that we're

23:35.560 --> 23:38.880
going to have to be more disciplined about how we conduct ourselves.

23:38.880 --> 23:41.240
You mentioned something interesting in there.

23:41.240 --> 23:46.440
It sounded like you brought in an organization that had existing platform capability.

23:46.440 --> 23:49.920
Was this kind of an internal transfer of a team?

23:49.920 --> 23:55.280
It was, that's exactly what it was, there was a platform team that was in another group.

23:55.280 --> 24:00.960
And when I started to have these conversations with our executive management, we knew that

24:00.960 --> 24:07.000
we had to seed our talent with this other type of talent.

24:07.000 --> 24:11.840
And so we went in and started to look at organizational adjustments and we wound up integrating

24:11.840 --> 24:14.680
an entire group and we actually did it in two ways.

24:14.680 --> 24:18.200
We brought in, brought in half the group and then we brought in the second half of the

24:18.200 --> 24:22.360
group after we had built up some critical mass.

24:22.360 --> 24:28.400
And so now what we've done is we actually have a platform's organization within the

24:28.400 --> 24:33.560
center for machine learning that is all about enterprise level, enterprise platform.

24:33.560 --> 24:38.560
So we're talking about resilience, scalability, logging, you know, all the things that you'd

24:38.560 --> 24:41.080
expect to see with the platform.

24:41.080 --> 24:45.520
This team has the experience in actually doing all those things.

24:45.520 --> 24:51.000
Yeah, it's a really great example of something that I talked to folks about all the time,

24:51.000 --> 24:55.920
you know, whereas machine learning and building out machine learning platforms might be new

24:55.920 --> 25:03.920
to an organization, the core kind of requirements and engineering practices probably aren't

25:03.920 --> 25:04.920
new.

25:04.920 --> 25:09.520
Capital One, for example, has, you know, I've known Capital One for many years to be investing

25:09.520 --> 25:15.400
heavily in platform as a service and container orchestration and all kinds of, not to mention

25:15.400 --> 25:21.680
we're here at Reinvent Cloud and building, you know, platforms to support traditional enterprise

25:21.680 --> 25:22.680
applications.

25:22.680 --> 25:29.960
A lot of the principles that go into supporting, you know, enterprise apps, web apps are similar

25:29.960 --> 25:34.640
to, you know, some of the things you need to be thinking about when you're supporting machine

25:34.640 --> 25:38.720
learning and AI-based systems, is that your experience as well?

25:38.720 --> 25:39.720
Absolutely.

25:39.720 --> 25:40.720
Yeah.

25:40.720 --> 25:41.720
A spot on.

25:41.720 --> 25:42.960
It's definitely my experience.

25:42.960 --> 25:49.760
One of the things that you've mentioned is kind of a common thread throughout the discussion

25:49.760 --> 25:52.960
is the interface with risk and governance.

25:52.960 --> 25:56.960
Can you talk a little bit about more about that process?

25:56.960 --> 25:59.160
Yeah, absolutely.

25:59.160 --> 26:05.080
I believe that we are actually in a very unique and interesting position at Capital One when

26:05.080 --> 26:10.280
it comes to the governance around machine learning.

26:10.280 --> 26:17.000
We have been living in a regulated world for many, many years, obviously, and with varying

26:17.000 --> 26:22.360
degrees of regulatory oversight, as I mentioned before, material models and non- and immaterial

26:22.360 --> 26:29.080
models, the ones that don't have to go through such high levels of regulatory scrutiny, but

26:29.080 --> 26:36.840
we understand the process and we understand all these little sort of phases and artifacts

26:36.840 --> 26:45.600
and responsibilities and the laws of making sure that at the end of the day, we are compliant

26:45.600 --> 26:50.760
with the regulatory frameworks that we have to navigate within today manually.

26:50.760 --> 26:53.280
So let me give you an example.

26:53.280 --> 26:58.640
When we create a machine learning model, we have to go through a process to validate

26:58.640 --> 27:03.400
the outcomes and how this model has actually performed.

27:03.400 --> 27:07.480
And one way one of the approaches that we might take is to build what are called these

27:07.480 --> 27:09.920
surrogate validation models.

27:09.920 --> 27:16.240
So we go in and handcraft these other models and these other models either support the

27:16.240 --> 27:21.160
outcomes of the model that you are trying to get through governance or they might refute

27:21.160 --> 27:22.160
it.

27:22.160 --> 27:24.320
They might show something that is a little bit different.

27:24.320 --> 27:27.440
And then you have to write all this stuff up in a document and you have to get that document

27:27.440 --> 27:30.640
through and you have to defend it and so on and so forth.

27:30.640 --> 27:35.760
Is the surrogate validation model, is that like a test suite or is it?

27:35.760 --> 27:36.760
You can think of it.

27:36.760 --> 27:40.240
You can think of it as kind of model or yeah, you can think of it as that, right?

27:40.240 --> 27:45.000
But like for example, if you're doing a gradient boosted machine, you might have surrogate

27:45.000 --> 27:51.080
models that are logistic regression models that are something that are types of models

27:51.080 --> 27:55.840
that we've been able to linear models that we've been able to deal with for many, many

27:55.840 --> 28:02.840
years and we can see how these two models behave with, you know, again, certain types

28:02.840 --> 28:05.760
of data sets that they're scoring.

28:05.760 --> 28:11.160
The opportunity for us is because we know this process so well, we can actually use ML

28:11.160 --> 28:14.000
for governing ML.

28:14.000 --> 28:19.880
We can use natural language processing for auto-document generation.

28:19.880 --> 28:25.800
We can use auto ML to generate the surrogate models automatically and then run them.

28:25.800 --> 28:30.880
You know, and we can interpret those documents on the other side.

28:30.880 --> 28:39.760
We can double click into the validation models and surface the, either the supporting argument

28:39.760 --> 28:42.600
or for those models.

28:42.600 --> 28:46.080
If you think about it even beyond that, when you talk to these model risk officers, their

28:46.080 --> 28:48.760
minds are, I mean, the wheels are turning in their heads.

28:48.760 --> 28:53.320
They see so many opportunities for doing some really cool things like one of one model

28:53.320 --> 28:59.480
risk officers said to me, I'd like to have this sort of surrogate recommendation engine

28:59.480 --> 29:05.400
that's saying, hey, I just picked up this bias in this data set and your feature set.

29:05.400 --> 29:07.640
Here are the artifacts that I picked up.

29:07.640 --> 29:11.560
This thing's running kind of alongside you, you know, and it's picking this kind of

29:11.560 --> 29:18.960
bias up and recommend you make these changes or you take a look at these particular features.

29:18.960 --> 29:20.360
That's cool stuff.

29:20.360 --> 29:23.640
Then you can actually do, if you could actually do something like that.

29:23.640 --> 29:30.200
So we're looking into piloting and doing some POCs around a lot of these kind of interesting

29:30.200 --> 29:33.680
model governance kinds of tasks.

29:33.680 --> 29:38.240
Now I don't think we'll ever take the human out of the loop, especially for those highly

29:38.240 --> 29:43.680
regulated, regulated use cases, but I think we're going to make their workload significantly

29:43.680 --> 29:45.240
less laborious.

29:45.240 --> 29:50.800
If you think about the proliferation of machine learning models at Capital One, we don't

29:50.800 --> 29:55.560
want to just stack them up on the model risk officers back door and we're in the same

29:55.560 --> 29:56.560
boat we're in today.

29:56.560 --> 29:59.960
We're just, they just have a huge backlog that they have to work through.

29:59.960 --> 30:04.080
We want to give them the tools to actually get through this and we want to be able to

30:04.080 --> 30:06.120
build that right into our platform.

30:06.120 --> 30:11.080
I think that's where we are thinking about the ideas that we're thinking about are far

30:11.080 --> 30:16.320
beyond what others are, as they're looking at machine learning, just based on the conversations

30:16.320 --> 30:23.120
that I've had with them and that I'm seeing in the trade racks, is that the regulated,

30:23.120 --> 30:27.640
highly regulated models are certainly going to undergo tremendous scrutiny.

30:27.640 --> 30:28.880
We know that.

30:28.880 --> 30:31.840
But there's this other class of models that they're starting to look at as well and they're

30:31.840 --> 30:38.400
saying even though the regulatory frameworks don't cover those models, they need to be

30:38.400 --> 30:43.760
explained and interpreted and bias detected just like these other models and I think there's

30:43.760 --> 30:48.360
going to be, the sentiment is going to be that all models have to go through this sort

30:48.360 --> 30:52.560
of scrutiny and I think that tech companies are going to be facing the same thing.

30:52.560 --> 30:58.520
So what kind of advice would you give them, companies that don't have the history of

30:58.520 --> 31:05.800
operating in a regulatory framework but want to either see the writing on the wall or

31:05.800 --> 31:11.680
they are internally motivated to use machine learning in a responsible way.

31:11.680 --> 31:18.240
How do they kind of bootstrap the internal frameworks to do so?

31:18.240 --> 31:22.800
There are some, I mentioned earlier about the rate of innovation in the industry.

31:22.800 --> 31:29.080
There are some emerging companies that are providing some libraries for some of this,

31:29.080 --> 31:31.000
some types of bias detection.

31:31.000 --> 31:37.920
So there's some basic functions that you can do today with libraries and Python and then

31:37.920 --> 31:44.280
there's companies that are springing up weekly, it seems like that are addressing explainable

31:44.280 --> 31:46.480
and interpretable AI.

31:46.480 --> 31:53.240
So I would say that to those companies that don't have the expertise to just get, at

31:53.240 --> 31:59.720
least get savvy about what's happening and there's so much literature out there about

31:59.720 --> 32:00.720
this.

32:00.720 --> 32:06.720
So if you're in the cloud with the AWS, AWS does have some basic capability with Shapley

32:06.720 --> 32:14.000
values that you can use to help explain or interpret your models.

32:14.000 --> 32:20.720
So there's some fundamental pieces there that I think people should at least become aware

32:20.720 --> 32:21.720
of.

32:21.720 --> 32:25.840
But it's certainly something you should not ignore because I believe that this is going

32:25.840 --> 32:30.120
to be, in five years from now, this is going to be something that is expected of everything

32:30.120 --> 32:31.880
that we do with machine learning.

32:31.880 --> 32:34.560
We're going to have to be able to explain that model and I think it's going to be all

32:34.560 --> 32:35.560
be built in.

32:35.560 --> 32:40.600
Well, Dave, thanks so much for taking the time to share what you're up to.

32:40.600 --> 32:46.440
It sounds like it continues to be an interesting run and you're doing a lot of interesting things

32:46.440 --> 32:47.440
there at Capital One.

32:47.440 --> 32:49.880
I appreciate the opportunity to learn about it.

32:49.880 --> 32:51.160
Thanks for having me, Sam.

32:51.160 --> 32:52.160
Appreciate it.

32:52.160 --> 32:53.160
Thank you.

32:53.160 --> 33:01.920
All right, everyone, that's our show for today to follow along with our reinvent series.

33:01.920 --> 33:07.360
Visit twimmelai.com slash reinvent 2019.

33:07.360 --> 33:11.480
Thanks once again to Capital One for their sponsorship of this series.

33:11.480 --> 33:17.320
Be sure to check out capital one calm slash tech slash explore to learn more about their

33:17.320 --> 33:20.040
ML and AI research.

33:20.040 --> 33:22.960
Thanks so much for listening and catch you next time.

