Welcome to the Tumel AI Podcast. I'm your host, Sam Charrington.
All right, everyone. I am here in Vancouver, continuing my coverage and
conversations at the NURPS 2019 conference. And I've got the pleasure of being with
you again, Schmidt Huber. You again is a co-founder and chief scientist of
Nacense, the scientific director at the SWIST AI Lab Insia, as well as a professor of AI at USI
and SUPSI in Switzerland. You again and I had the pleasure of speaking just over a couple of years
ago now, where he hosted me at his lab in Logano, Switzerland, a beautiful setting and a wonderful
conversation that I encourage you to take a listen to. That was Tumel Talk number 44, almost 300
episodes ago. You again is, you know, regarded by many as the father of AI. He is his lab created
the LSTM. That was a big focus of our conversation. The LSTM is a model that, you know, we all use
all the time on our smartphones. And I'm excited to have an opportunity to catch up with you.
You again, welcome to the Tumel AI podcast. It's my pleasure being here. Thank you so much.
So, you know, I really wanted to start this conversation with a bit of an update. What are some
of the things that you've been doing in your research lab and at Nacense? What's been interesting
and exciting for you? Let me give you a couple of examples of what Nacense is doing. That's a B2B
company and now we're entering this age of active AI where you don't just have passive patent
recognition on your smartphone, but where you have machines that learn to become better machines
by translating the data that is coming in through the cameras and other sensors into actions
that manipulate the industrial processes that they are controlling. For example, we have a project
with Festo which is one of the leading robot makers and they have these hands which are being
controlled by air muscles. So you have a compressor which is generating the pressure that you need
to move these fingers around and nobody knows how to control that. And so Nacense is building the
brains that learn to control these soft robot hands that we have there. Or EOS that is leader
in industrial 3D printing where you don't just do little plastic models of something,
but where you really print engines with metal and they have all the patterns for melting the powder
and then letting it incrementally through additive manufacturing turn into engines of airplanes
and stuff like that. And one of these printers costs at least half a million or something.
So they are much more expensive than the standard cheap printers that you can buy in the supermarket.
It already seems clear that you can further improve these printing processes by understanding
better what exactly is the impact of the laser light on the melting process and on the cooling
process and how can you control the thing in a way that improves the quality of the engines that
are coming out. Then other projects with Salt Sashmeet for example they have little drones and
they are inspecting the wind turbines and then how can you automatize that and how can you learn,
make these machines learn to become better quality inspectors. With Audi a while ago we had a
demo project where a little toy Audi is maybe only one eighth-author size of a real Audi.
They learn to park not by imitating a human teacher but just by reinforcement learning just by
maximizing the pleasure signals that come in and minimizing the pain signals that come in.
Whenever one of these little cars bumps against an obstacle then it gets negative reward.
Whenever it reaches the parking lot then it gets a positive reward and as always in reinforcement
learning it's trying to minimize pain and maximize pleasure so it has to translate the incoming
video and the other data from the radar sensors and lead sensors into actions that lead to successful
parking strategies and that was the first time something like that was done in the real world.
These little Audi's they aren't as big as the as the real thing however they have all these
sensors that you have and the big Audi's the radar the cameras they go up to 120 kilometers
an hour so they are pretty fast too and sounds like a nice toy. It's a very nice toy one has
certainly that and a whole bunch of other projects are I'm happy that now that nasons isn't the
second round of financing we don't only have these these investors from the platform companies
on the Pacific Rim but now also old industry. Old industry is investing in us for example shot
is a leading maker of glass and you probably have them on your smartphone because they are owned
by the size foundation and the glass that they make isn't the lenses are the cameras on
hundreds of millions of them okay as smartphones many people don't know that making glass is
difficult and it's about controlling an industrial pipeline where you have
molten glass and it's 1,200 degrees Celsius and you want to remove the bubbles and at the right
moment you want to inject certain substances and you have turbulent mixing that you want to
control all kinds of knobs that you can adjust there and this has been traditionally done by
humans. Shot is a company which is more than a hundred years old and over the decades they have
learned how to do that well however there is still a lot of room for improvement and for making
even better glass through machines that learn to do that so suddenly investors are not just the
platform companies from the Pacific Rim but old industry which is realizing that in this coming
age of active AI suddenly you can do things that go far beyond passive pattern recognition on
this smartphone and they sense this in the middle of that. That's very exciting very exciting you
mentioned a few interesting use cases with the first one that you mentioned the robotic hand calls
to mind the recent news that you may have come across with open AI and their Rubik's Cube and they've
used reinforcement learning to kind of train the model that controls the you know this highly
dimensional robot essentially. Are you also using reinforcement learning as part of that application
or what are the kind of the main techniques that you're applying to that? In fact the open AI
robot hand is driven by the architectures that we have developed in our labs and Munich and
Switzerland over the decades. What is this open AI reinforcement learner doing? It is basically
a big LSTM network which is surrounded by a couple of smaller pre-processing networks, pre-processes
our inputs and then through lots of simulations there's a pretty good simulation of this hand
through reinforcement learning the LSTM learns to control the hand such that it can do interesting
things such as solving the Rubik's Cube in a certain fraction of the times. So yeah of course we are
using the techniques that are now popular in reinforcement learning which is train and LSTM
through reward maximization to do that. However the big difference between what what this
example of open AI is about and what you have in the real world is you don't have a good model
or see real world. You somehow have to deal with this situation that you have very few interactions
examples with the real world from which you have to build a model of the world and then you have
to do mental experiments in this model which is imperfect in many ways but can learn to improve
itself through further data that is coming in through the actions of the robot that is trying to
learn something new. So are you speaking to the relationship between simulation and having the
model operate in the real world and kind of correlating or integrating rather those two such that
you've got some kind of end-to-end training process that integrates both SIM and real.
So at the moment AI works well in the virtual realm for video games like Dota or Starcraft that's
what deep-minded Dota was done by OpenAI. Again you have an LSTM network surrounded by other
networks which is learning through policy gradient methods reward optimization methods to make
some of these weight strong and some of them weaker such that the whole system over time becomes a
better video game player and there you train the system through exposing it through billions and
trillions of video games. This is something that you can do in the virtual realm and the same
principle was applied to the robot hand of OpenAI where they also had lots of simulations
because they did have a pretty good simulation of the hand and then through certain
tricks it was possible to transfer the behavior from the virtual setting to the real world setting
and then one essential ingredient was this excellent simulation of the hand and normally you don't
have that. In almost all industrial processes you don't have that. In the real world you have all
kinds of slippage and it's not well-modeled in any simulation and you have so many things that are
not modeled. How does a baby learn to deal with that? Well the baby is receiving data as
consequence of the actions that it is generating as it is playing with its toys and from this data
it learns to build a model so it doesn't have a given model of the world. No it learns to build this
model and how does it do that? It learns to predict what's going to happen next what is the next
input given what I have done so far and some of the inputs are predictable others are not predictable
and the baby even does more than just predicting the next thing somehow it learns high level abstractions
and it learns to plan ahead in a way that apparently is not the millisecond by millisecond planning
that you would do in a detailed simulation of the world where you will try to come up with a
sequence of actions that leads to a lot of predicted reward in that simulation where you really
have a simulation that millisecond by millisecond simulates the entire world and such a simulation
is totally dependent on being a good simulation and no errors are allowed there while the models that
humans carry around they are more sophisticated in the sense that they acknowledge that certain
things are not known and other things are rather predictable and they don't do this millisecond by
millisecond planning no they do high level planning like for example when you are going to Beijing
you you make a plan for that and you don't plan the trajectories of all your muscle movements
millisecond by millisecond no you say first I have to find a taxi and then once I'm in the taxi
I'm going to the airport and then I'm entering the plane and for nine hours nothing is going to
happen and then I accident so this high level planning is something that humans are really good at
and probably also some of the animals and our robots are not good at that yet however this is
changing and we are making great progress exactly in that realm generally speaking current AI
works well in the virtual realm video games or robots where you have a really good simulation of
the robot and it doesn't work well yet in the physical real world we are so there is an implication
of this that the model that you developed with the hand was not based on simulation it was all based
on training through experimentation on the physical hand itself that's what we generally do
so we try to learn like a little baby a model of the industrial process that we are controlling
and because in most cases there is no good model of this process we have to learn it from experience
so suddenly we need a system that learns to generate experiments that lead to data that can be
used to build a better model when I think about for example you know maybe this doesn't apply to
hands to the extent that applies to like robotic arms but in with robotic arms they're you know I've
often kind of talked to folks or talked about kind of this tension between you know folks that
want to tackle that problem with end deep learning which any model versus folks that want to take
advantage of kind of traditional control systems and you know robot kinematics that you know if
you're able to integrate those traditional models you know that we do kind of know how to get a
robot an actuator from one position you know in a 3D space to another position in a 3D space
because we have kind of models that do that it sounds like your approaches aren't kind of model based
in that perspective you're learning everything from the ground up using reinforcement learning without
the benefit of simulation is that yeah what are the leaps that I'm making there you know tune that
up for me so in industrial applications we use everything that is already there okay so if there's
a reasonable model of the process then of course we are going to use that it's just that the most
exciting applications are those where you don't have that model hmm sure if you have a factory
full of industrial robots then you know exactly where each of these robots is positioned you know
exactly where it has to grip the chair and how to move it exactly at the right position in the car
where it's going to end up so there you have very precise industrial robots and they they are
extremely good at doing what they are doing because everything is totally on a control there's
a perfect model of where everything is located where the tools are where the cars are coming in
where the chairs are they they can do that without cameras even because everything is already
modeled extremely well right but that's not the future of manufacturing the future of manufacturing
is all the messy things all the messy stuff like when you are assembling a smartphone for example
there are all these little parts of this smartphone and at the moment humans have to do that also
there is no robot that is able to do that at least not in the way that humans can do it and what
you really want is a robot that is confronted with a new task and quickly learns through a few
interactions maybe a few demonstrations of a human but also a few self-generated experiments
how this thing works and how to screw in that screw and how to avoid that the screw driver
slipping and all these super complicated important things that are easy for humans but not yet
for current robots and it wants to to learn through a combination of curiosity and imitation of
what the human teachers are showing to it it wants to learn to execute this new behavior quickly
at some point as well as the teacher who is just through a visual demonstration and through
speech and language trying to explain to the robot what it wants to what it should do and then
once it is able to do that to a certain extent you want to have a robot through self-experimentation
to speed up the process and make it more efficient so that it can do it much faster than any human
and with less energy and then once it's really good you want to switch off the learning algorithm
and make a million copies and license it or sell it yeah as we talked about on this show many times
RL is particularly deep RL is historically extremely simple and efficient right and so a lot of
folks will turn the simulation because if they just try to train in the real world they'll destroy
their robots before they converge to any type of model is the work that you're doing they're
focused on the the simple efficiency side of the equation it is the essential thing which we need
for dealing with the real world because there you can have only so many training examples
and self-invented experiments and that's it you can't have a trillion experiments like in video
games right right so that's the big difference and it's really the single most important issue
the the big gap that exists between the learning abilities of humans and our best reinforcement
learning algorithms is really that so from very few training examples you want to pick up all the
things that are necessary to control that process that you're trying to control and it's what
current reinforcement learning algorithms are not so good at what are the specific things that
or specific research or papers that you're working on in your lab that try to get there is it
you know how would you even did they fall under kind of the banner of one shot few shot learning
meta learning curriculum learning imitation learning like are there are they you know one or
combinations of these things or is it an altogether different approach so since we are interested
in universal learning machines it's all of that okay but let me give you a one concrete little
example of something that we published just a couple of days ago okay in an active paper
something which is called upside down reinforcement learning upside down reinforcement learning okay
upside down already intrigued it's called upside down reinforcement learning because it turns
traditionally reinforcement learning on its head traditionally reinforcement learning works like
that you have a neural network or some other adaptive machine which takes in data from the
environment and produces actions which change the environment which means new inputs are coming in
from the environment and there is something you want to maximize which is the cumulative expected
reward so when the when the robot is acting in a good way then it gets reward and it wants to maximize
that by making some of these connections in the network stronger and others weaker such that
actions come out in response to the inputs that lead to a lot of reward success yes exactly
and then the way it's usually done is there is a second network which is a prediction machine which
sees the actions of the first network and the inputs that are coming in from the environment and
it's trying to predict given this policy that is implemented by the first network what is the
expected future reward so some of all the rewards that is going to come in until the end of the
contrial for example and then through this prediction which depends on the actions and the
information from the environment you can find actions that lead to more reward than what you
have so far and then there are simple ways of adjusting the first network such that the actions
that are being generated by it are more likely to lead to a lot of reward so what is very essential
there is this prediction of future expected reward and upside down reinforcement learning turns
all of that around it doesn't have the rewards being predicted by the outputs of such a network
no the rewards are becoming the inputs to a network which produces the output actions and which
also sees the standard inputs coming from the environment and these incoming rewards are
interpreted as commands so there's this extra input field of this control network which is
not the cumulative reward it's the reward that's a result of the previous action no it's
in fact the cumulative the cumulative reward that you would like to get between this time and
this time in the in the simplest case there are two inputs one is a time input which says between
this time and this time I want to get so much reward that's a command that's a command so now
the network sees this command and tries to come up with actions that between this time and this
time lead to so much reward it just tries to obey the command okay so now it's actually unique
its actions and it turns out that the incoming reward is less than what the command
requested so then we have a training example where the network can learn something from that and
it can learn in the following way let's adjust the command retrospectively let's take the
real reward that came in between these two time steps and use that as a command and let's run
the whole thing again and let's train it to generate this action sequence that it already has
generated in response to these modified commands which are now consistent with what the network did
okay so now it learns to generate an action sequence like what it just had generated in response
to these commands and what you really want to achieve is that you generate action sequences
that lead to more reward so you say let's give a new command to the system let's say we now want
to have twice as much reward as last time so new command everything is the same except that
now we've won twice as much reward as a command input so the network is producing outputs
in response to that and maybe the result is that the reward that is really received from the
environment is indeed twice as much because it's somehow was able to internalize then we are happy
then we say on let's try the same thing with four times as much reward and it sounds like a
binary search of the reward space or something like that maybe not binary but you I'm imagining
that you would start with you know an unachievable reward maybe infinite and you the network
achieves much less than that and then you you know it says okay maybe I'll you know go to half
of infinity or whatever half of the large number that I would love to have but the point is from
each of its failures to to satisfy the command it learns something so it learns each time something
new about how to map commands to action sequences and over time it will learn the structure of
this space of parameters that you need to translate these commands into desired behavior and then
you can do very directed exploration because you just as you said you give new commands that you
never have given before and you hope that the network will know enough about the space of these
parameters and about functions mapping reward commands to output actions such that it can
generalize and yield even more desirable behavior and so whenever these expectations fail it still has
a new bunch of training examples from which it can learn to become a better translator of commands
into action sequences so it's not some random exploration though it is a way of using pure
supervised learning to achieve this reinforcement learning objective which is get a lot of reward
and very little time and it's just learning through supervised learning to map these combinations
of cumulative rewards within certain time intervals to action sequences and supervision comes
from it's kind of a binary supervision achieved or failed to achieve the command based on some
sequence of actions yeah and so there is we know for each behavior for each trial of the agent
we know exactly how much reward did we get we know exactly which action sequence was there
and in retrospect we can adjust the reward and time command such that it is compatible with what
really was achieved if you know power play algorithms or hindsight experience replay algorithms
or if you know a rather algorithm of set poor writer's group then you already know a lot about
the principles of what upside down reinforcement learning is about in the paper how did you compare
the performance of this approach to other approaches and maybe before that is do you envision
this approach being applicable to the same or any you know reinforcing any of the kind of
traditional RL problems or it does this approach with the you know the time and the reward as the
inputs is is it particularly you know interesting or useful applied to a specific formulation of the
RL problem yeah so Hu Pech Kumar Srivasava my coworker at Naysans and a former PhD student
of mine and other colleagues at Naysans and also Editsia they came up with variants of what I just
explained and showed that although we at the moment have just little pilot versions of this
simple new type of reinforcement learning it already is able to beat certain traditional
baselines on interesting problems the thing about this upside down reinforcement learning is
that its limitations are exactly the limitations of supervised learning because basically we are
translating reinforcement learning into supervised learning in a way that depends on these deep
networks that have to learn this complicated mapping from reward commands to action sequences
that lead to the rewards that has been requested within two time steps that I also
given through the command and then this is a complex mapping and these many layers because
you change the reward command just a little bit and this might lead to quite different action
sequence that should come out and so on so we know how to train these deep networks and we have
millions of tricks and supervised learning to do that in a good way and RL Pech and the others
use that and use some of these tricks to make that really work but there is so much that we
haven't even exploited yet so many ways of improving supervised learning which can all be applied
to this in principle a very simple type of new reinforcement learning so it sounds like the
when you you mentioned that you included comparisons on traditional benchmarks you know
what categories of traditional benchmarks like the Atari games and that kind of thing or
yeah in the current experiments the result is it works especially well when there are sparse
rewards so when you don't get rewards all the time like in certain video games but only at the
end of the trial which is actually the most challenging situation because if you don't get a lot
of rewards during all your trials then there's not so much from what you can learn something which
means you have this attribution problem of what was it that you did that resulted in the rewards
that she did get the credit assigned problem exactly and some ways the most challenging type of
reward maximization problem however at the moment we have just these pilot experiments and let's
see where that goes at the moment it's also just an active tech report and it's it's useful already
we see that but who knows how much better it's going to get once we use all the tricks of
supervised learning to to improve further the current pilot implementations. And so is that being
presented here in a workshop or something like that at NURBS? Rupesh Kumar Srivasava is actually
going to present that I think tomorrow. Okay awesome awesome well we will definitely look that up
and link to the archive paper in the show notes and I'm looking forward to digging a little bit
deeper and seeing what that's all about sounds really interesting. All right well you again thanks so
much for taking a few minutes to chat with us about you know what's new what you're up to looking
forward to connecting once again in the future. Thank you so much for having me and it was a pleasure
I'm being here. Absolutely thank you. All right everyone that's our show for today. For more
information on today's show visit twomolai.com slash shows. As always thanks so much for listening
and catch you next time.
