1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:25,720
I'm your host Sam Charrington, a bit about the show you're about to hear.

4
00:00:25,720 --> 00:00:29,840
This show is part of a series that I'm really excited about in part because I've been

5
00:00:29,840 --> 00:00:32,880
working to bring it to you for quite a while now.

6
00:00:32,880 --> 00:00:37,040
The focus of this series is a sampling of the really interesting work being done over

7
00:00:37,040 --> 00:00:44,520
at OpenAI, the independent AI research lab founded by Elon Musk, Sam Altman, and others.

8
00:00:44,520 --> 00:00:48,000
A few quick announcements before we dive into the show.

9
00:00:48,000 --> 00:00:53,040
In a few weeks we'll be holding our last Twimble online meetup of the year.

10
00:00:53,040 --> 00:00:58,080
On Wednesday, December 13th, please join us and bring your thoughts on the top machine

11
00:00:58,080 --> 00:01:02,640
learning and AI stories of 2017 for our discussion segment.

12
00:01:02,640 --> 00:01:08,360
For our main presentation, formal Twimble Talk guest, Bruno Gensalves, we'll be discussing

13
00:01:08,360 --> 00:01:13,880
the paper, understanding deep learning requires rethinking generalization, by Shi Juan

14
00:01:13,880 --> 00:01:17,320
Zhang from MIT and Google Brain and others.

15
00:01:17,320 --> 00:01:22,120
You can find more details and register at twimlai.com slash meetup.

16
00:01:22,120 --> 00:01:27,480
Also, we need to build out our 2018 presentation schedule for the meetup.

17
00:01:27,480 --> 00:01:32,040
So if you'd like to present your own work or your favorite third-party paper, please

18
00:01:32,040 --> 00:01:40,080
reach out to us via email at teamattwimlai.com or ping us on social media and let us know.

19
00:01:40,080 --> 00:01:44,240
If you receive my newsletter, you already know this, but Twimble is growing and we're

20
00:01:44,240 --> 00:01:49,280
looking for an energetic and passionate community manager to help managing grow programs

21
00:01:49,280 --> 00:01:55,200
like the podcast and meetup and some other exciting things we've got in store for 2018.

22
00:01:55,200 --> 00:01:57,800
This is a full-time role that can be done remotely.

23
00:01:57,800 --> 00:02:02,400
If you're interested in learning more, reach out to me for additional details.

24
00:02:02,400 --> 00:02:06,520
I should mention that if you don't already get my newsletter, you are really missing

25
00:02:06,520 --> 00:02:11,480
out and you should visit twimlai.com slash newsletter to sign up.

26
00:02:11,480 --> 00:02:16,560
In this episode, I'm joined by Greg Brockman, OpenAI co-founder and CTO.

27
00:02:16,560 --> 00:02:19,200
Greg and I touch on a bunch of topics in this show.

28
00:02:19,200 --> 00:02:24,000
We start with the founding and goals of OpenAI before diving into a discussion on artificial

29
00:02:24,000 --> 00:02:29,600
general intelligence, what it means to achieve it, and how we go about doing so safely and

30
00:02:29,600 --> 00:02:31,480
without bias.

31
00:02:31,480 --> 00:02:36,080
We also touch on how to massively scale neural networks in their training and the evolution

32
00:02:36,080 --> 00:02:38,960
of computational frameworks for AI.

33
00:02:38,960 --> 00:02:43,440
This conversation is not only informative and nerd alert-worthy, but we cover some very

34
00:02:43,440 --> 00:02:48,800
important topics, so please take it all in, enjoy, and send along your feedback.

35
00:02:48,800 --> 00:02:53,840
A quick note before we jump in, support for this OpenAI series is brought to you by our

36
00:02:53,840 --> 00:02:59,520
friends at NVIDIA, a company which is also a supporter of OpenAI itself.

37
00:02:59,520 --> 00:03:03,440
If you're listening to this podcast, you already know about NVIDIA and all the great things

38
00:03:03,440 --> 00:03:07,360
they're doing to support advancements in AI research and practice.

39
00:03:07,360 --> 00:03:11,920
What you may not know is that the company has a significant presence at the NIPPS conference

40
00:03:11,920 --> 00:03:17,400
going on next week in Long Beach, California, including four accepted papers.

41
00:03:17,400 --> 00:03:23,560
To learn more about the NVIDIA presence at NIPPS, head on over to twimlai.com slash NVIDIA

42
00:03:23,560 --> 00:03:26,160
and be sure to visit them at the conference.

43
00:03:26,160 --> 00:03:30,640
Of course, I'll be at NIPPS as well, and I'd love to meet you if you'll be there, so

44
00:03:30,640 --> 00:03:33,120
please reach out if you will.

45
00:03:33,120 --> 00:03:36,120
And now on to the show.

46
00:03:36,120 --> 00:03:45,680
Alright everyone, I am on the line with Greg Brockman.

47
00:03:45,680 --> 00:03:49,640
Greg is a co-founder and CTO of OpenAI.

48
00:03:49,640 --> 00:03:52,640
Greg, welcome to this week in Machine Learning and AI.

49
00:03:52,640 --> 00:03:54,120
Thank you for having me.

50
00:03:54,120 --> 00:03:55,120
Awesome.

51
00:03:55,120 --> 00:03:59,800
Hey, why don't we get started as is the tradition here on the podcast with you telling us a little

52
00:03:59,800 --> 00:04:03,720
bit about your background and how you got involved in AI?

53
00:04:03,720 --> 00:04:04,720
Sure thing.

54
00:04:04,720 --> 00:04:09,480
So, I got into programming relatively late, so after high school, I took a year off and

55
00:04:09,480 --> 00:04:14,040
went abroad, was working on the chemistry textbook, and I sent it off to one of my friends

56
00:04:14,040 --> 00:04:17,760
who had done something similar in math, and you were back saying there's only one problem

57
00:04:17,760 --> 00:04:20,800
with this, which is that you don't have a PhD, so no one's going to publish it, so you

58
00:04:20,800 --> 00:04:25,720
can either self publish, or you can make a website, try to promote things that way.

59
00:04:25,720 --> 00:04:29,680
I was like, well, I guess I'll figure out how to make a website, and so I went online

60
00:04:29,680 --> 00:04:33,400
and taught myself how to code and build a little sample table sorting widget, and thought

61
00:04:33,400 --> 00:04:35,920
that was cool, and I built something bigger and bigger and never really looked back

62
00:04:35,920 --> 00:04:37,600
at the chemistry.

63
00:04:37,600 --> 00:04:43,160
And one of the really things that really captivated me was the idea of being able to write code

64
00:04:43,160 --> 00:04:48,480
that could understand things that I could not, because the way that you write code that

65
00:04:48,480 --> 00:04:52,600
you build systems, you think hard about a problem, you understand it, you write it down

66
00:04:52,600 --> 00:04:59,600
in this very obscure way that we call a program, and suddenly anyone can get the benefit of

67
00:04:59,600 --> 00:05:05,480
what you just did, and if there was a way to amplify that and to have programs that could

68
00:05:05,480 --> 00:05:09,200
do things that I didn't have to even understand myself, then suddenly the set of problems

69
00:05:09,200 --> 00:05:11,400
you could solve, you're so much broader.

70
00:05:11,400 --> 00:05:14,760
And how'd you get from building a website to that?

71
00:05:14,760 --> 00:05:15,760
Yeah, yeah.

72
00:05:15,760 --> 00:05:21,760
So, I read Turing's 1950 paper, computer machinery and intelligence, and when you read it and

73
00:05:21,760 --> 00:05:27,080
it's talking about the Turing test, and that he has this picture of, by the year 2000, you'll

74
00:05:27,080 --> 00:05:31,800
be able to build this child machine that will learn just like a human child, and he will

75
00:05:31,800 --> 00:05:33,440
get full intelligence.

76
00:05:33,440 --> 00:05:38,880
And this was in 2009 that I was reading this paper, and it's like, where is that machine?

77
00:05:38,880 --> 00:05:40,680
Why is it anyone built it?

78
00:05:40,680 --> 00:05:41,680
Nice.

79
00:05:41,680 --> 00:05:46,040
And so, throughout college, so I ended up doing a bunch of different startups, and ended

80
00:05:46,040 --> 00:05:49,840
up transferring, so I started out Harvard and I was there for a year and a half for going

81
00:05:49,840 --> 00:05:50,840
to MIT.

82
00:05:50,840 --> 00:05:54,240
I was there for a semester and a half before leaving to go work on Stripe, where I was

83
00:05:54,240 --> 00:05:59,280
the CTO for five years, and built that from four people to 250 employees, it's now around

84
00:05:59,280 --> 00:06:00,760
a thousand or so.

85
00:06:00,760 --> 00:06:04,600
And kind of, for me, the goal has always been to work on AI.

86
00:06:04,600 --> 00:06:07,800
It was just a question of when and the right way of doing it.

87
00:06:07,800 --> 00:06:12,800
And while I was working in the startup world, if you read hacker news and you look at what's

88
00:06:12,800 --> 00:06:17,680
what's happening, you see all these articles and deep learning does X, deep learning for

89
00:06:17,680 --> 00:06:23,440
Y, and the thing that was very unclear to me from the outside was substance or hype.

90
00:06:23,440 --> 00:06:28,240
And I'd actually done some similar investigation on Bitcoin, from the outside, Bitcoin similarly

91
00:06:28,240 --> 00:06:32,600
is something where there's a lot of people talking about it as substance or hype and had

92
00:06:32,600 --> 00:06:37,520
really done a deep dive and kind of concluded that this was 2014 vid, that it was kind of

93
00:06:37,520 --> 00:06:40,800
people weren't really focused on the right things, that if people were focused on just

94
00:06:40,800 --> 00:06:44,600
kind of speculation, not about building products, delivering value, you know, still might

95
00:06:44,600 --> 00:06:50,240
be the case that Bitcoin will succeed, but I had a very different observation when

96
00:06:50,240 --> 00:06:53,000
it came to deep learning and AI and what was happening.

97
00:06:53,000 --> 00:06:56,880
And I realized that a lot of my smartest friends from college were now in the field and that

98
00:06:56,880 --> 00:07:01,760
things were starting to work in a very real way and solve tasks that you just couldn't

99
00:07:01,760 --> 00:07:07,760
have solved another way. So the fact that things are actually happening, you can actually

100
00:07:07,760 --> 00:07:12,720
build systems, the kind of real world application and that it's also still very much at the

101
00:07:12,720 --> 00:07:19,760
very beginning of the S curve. For me, it was very clear that the moment is now and I was

102
00:07:19,760 --> 00:07:24,040
talking to a bunch of people in the field, I was talking to Sam Altman and he put together

103
00:07:24,040 --> 00:07:29,720
this dinner with Elon Musk and Ilya Setskiver and some others and the focus of this dinner

104
00:07:29,720 --> 00:07:35,320
was clear that things are happening, things are moving very quickly. How can we best have

105
00:07:35,320 --> 00:07:39,720
a positive impact? How can we help ensure that this plays out in the best possible way?

106
00:07:39,720 --> 00:07:45,000
Because AI is just going to be the most transformative technology that humans ever create and just

107
00:07:45,000 --> 00:07:49,160
having some, you know, any kind of contribution to making that play out better is the most

108
00:07:49,160 --> 00:07:55,560
worthwhile thing that I can imagine. And the conclusion was that it seems like it's not

109
00:07:55,560 --> 00:08:00,720
too late. It's not impossible to build a lab with a lot of the strongest researchers in

110
00:08:00,720 --> 00:08:06,520
the field, especially if you focus it on this goal of this technology. It's not enough

111
00:08:06,520 --> 00:08:09,800
just to build it. You also need to think about how do you make sure it actually benefits

112
00:08:09,800 --> 00:08:16,680
everyone? And so we had that as our hypothesis and you know, I at the time had left striped

113
00:08:16,680 --> 00:08:21,320
a couple months earlier and said, well, I'm going to go full time on trying to make this happen.

114
00:08:21,320 --> 00:08:28,320
So we put together a team and in December of 2015, launch at NIPS announced that we existed

115
00:08:28,320 --> 00:08:35,720
and since then have been working on going from zero to pushing the envelope of what is

116
00:08:35,720 --> 00:08:42,600
possible in this field. Yeah, yeah. And for those who aren't familiar with OpenAI as an

117
00:08:42,600 --> 00:08:46,840
organization, you know, what does it look like today in terms of the number of researchers

118
00:08:46,840 --> 00:08:54,640
and what's the model? Like you see folks that are affiliated with OpenAI that are affiliated

119
00:08:54,640 --> 00:08:59,120
at other places as well? How things played out, you know, since then?

120
00:08:59,120 --> 00:09:04,320
Yeah, so we tend to think of ourselves as cherry picking the best parts of academia and

121
00:09:04,320 --> 00:09:09,520
the best parts of industry towards a very focused goal. And OpenAI at the end of the day

122
00:09:09,520 --> 00:09:13,960
is a serious play to build general intelligence and make sure that it plays out well for society

123
00:09:13,960 --> 00:09:19,200
plays out in a safe way. And to do that, we don't know how to build something like general

124
00:09:19,200 --> 00:09:23,480
intelligence today. So you need to do fundamental research, you need to push the elements of

125
00:09:23,480 --> 00:09:27,280
what's possible, but it's also the case that field has really transitioned from being

126
00:09:27,280 --> 00:09:33,520
an individual sport to being a team sport. Really just this year, and maybe in 2016, you

127
00:09:33,520 --> 00:09:40,520
were able to start using large clusters of machines much more productively in 2012. Google

128
00:09:40,520 --> 00:09:45,320
had the Katner on, which is 16,000 cores, but we're able to be surpassed by two grad

129
00:09:45,320 --> 00:09:50,720
students on the GPU and a 2GPUs. And today, that's a very different story where you have

130
00:09:50,720 --> 00:09:55,920
people training images of 10 minutes on 1024 GPUs. And so when you look at something like

131
00:09:55,920 --> 00:10:00,240
our Dota project, which we'll talk about in a bit, which really require this team of

132
00:10:00,240 --> 00:10:04,000
people with engineering backgrounds, with research backgrounds, all coming together towards

133
00:10:04,000 --> 00:10:08,520
a shared goal. And so the way that we structure ourselves is that we have a few different

134
00:10:08,520 --> 00:10:11,800
teams internally. So we have a robotics team, we have a Dota team, we have a few other

135
00:10:11,800 --> 00:10:16,400
teams. And people, we have the full mix of skill sets that are required to accomplish

136
00:10:16,400 --> 00:10:21,720
a goal within those teams. We also have infrastructure team that is more of a horizontal team that

137
00:10:21,720 --> 00:10:26,120
supports the work of all these different teams and accelerates the work of those teams.

138
00:10:26,120 --> 00:10:29,680
And one thing that we're increasingly seeing is collaborations amongst the teams, which

139
00:10:29,680 --> 00:10:34,720
ends up being a really powerful thing, where we can take code from Dota and use it for

140
00:10:34,720 --> 00:10:39,520
robotics. And it really accelerates what's possible there.

141
00:10:39,520 --> 00:10:45,840
And can you speak a little bit to the, you know, the open aspect of open AI? It's, you

142
00:10:45,840 --> 00:10:51,480
know, clearly something that was important to you in setting out on this journey. But

143
00:10:51,480 --> 00:10:57,160
at the same time, there's been some critique of the level of openness at open AI. And there's

144
00:10:57,160 --> 00:11:02,040
still some things that, you know, I think you're, you know, clearly publishing a lot of research,

145
00:11:02,040 --> 00:11:06,040
but folks have asked, you know, are you publishing, could you be doing more in terms of publishing

146
00:11:06,040 --> 00:11:09,320
data sets and things like that? You know, how do you think of that?

147
00:11:09,320 --> 00:11:13,160
I think that's a great question. And I think that that's one misconception that people

148
00:11:13,160 --> 00:11:17,960
have had since the beginning of open AI is I think people have put kind of a narrative

149
00:11:17,960 --> 00:11:21,680
that's very different from what we're trying to do. And I think it's, it's a really good

150
00:11:21,680 --> 00:11:29,680
thing to ask about. So the goal of open AI is to ensure that the world post general intelligence

151
00:11:29,680 --> 00:11:36,000
is good for humans. And along the way, it's really important as, as a organization that,

152
00:11:36,000 --> 00:11:43,560
that we both, like one thing that we think about a lot is what we can do to both accelerate

153
00:11:43,560 --> 00:11:48,320
our organization, but then also things that we can do to help accelerate the field and

154
00:11:48,320 --> 00:11:53,080
things that we can do to deliver value to the world generally. And the, the last one ends

155
00:11:53,080 --> 00:11:57,640
up playing out on multiple time scales. So there's very short time scale work that you

156
00:11:57,640 --> 00:12:02,120
can do. Like, for example, today we, we published, we released a few more algorithms in our

157
00:12:02,120 --> 00:12:07,520
baselines project where we have, we've done high quality implementations of all the standard

158
00:12:07,520 --> 00:12:11,480
reinforcement learning algorithms. So now, rather than people having to, and basically

159
00:12:11,480 --> 00:12:15,440
end up with a bad baseline, that then they say, oh, my new method is better than this one,

160
00:12:15,440 --> 00:12:20,560
you can just take the work that we've done to, to well tune all of these baselines and

161
00:12:20,560 --> 00:12:25,560
use that. And so that's a short timeline delivering a value. The thing that we really

162
00:12:25,560 --> 00:12:30,160
are trying to do is a much longer timeline, right? It's really about building an organization

163
00:12:30,160 --> 00:12:34,640
that can be at the forefront of this research and to actually be able to steer how it plays

164
00:12:34,640 --> 00:12:41,480
on society. And so to do that, it's not as simple as just take the work that you do in

165
00:12:41,480 --> 00:12:45,440
real time and toss it over the fence or you know, put it up on GitHub. I think it requires

166
00:12:45,440 --> 00:12:49,600
something much more thoughtful. And so I guess a lot of how we think about it is that

167
00:12:49,600 --> 00:12:55,640
opening I will be a success if you fast forward five, 10 years or two, whenever you're, whenever

168
00:12:55,640 --> 00:12:59,600
the, the appropriate checkpoint is, and you look back and you say that the amount of

169
00:12:59,600 --> 00:13:06,080
value that we delivered over the long timeframe was the max that we could have. And some

170
00:13:06,080 --> 00:13:10,200
of that I think is in the short run, it means that you don't necessarily publish or release

171
00:13:10,200 --> 00:13:15,160
code to the maximum extent that we possibly could. That is always made with the choice

172
00:13:15,160 --> 00:13:19,300
of because we think that we're going to be able to better deliver value over the long

173
00:13:19,300 --> 00:13:26,320
run. And specifically, meaning, you know, they're clearly publishing code has a cost

174
00:13:26,320 --> 00:13:31,160
to it, has resources that are associated with it, but even more so, you know, once you

175
00:13:31,160 --> 00:13:36,120
publish it, there's some, you know, rightly or wrongly expectation of maintaining that

176
00:13:36,120 --> 00:13:41,760
over time that also has a cost and all of that, those accumulated costs, you know, potentially

177
00:13:41,760 --> 00:13:47,200
slow the organization down or kind of, you know, require ongoing resources that specifically

178
00:13:47,200 --> 00:13:52,080
the, the thinking or is that just part of it? So that is, that is a real one for sure,

179
00:13:52,080 --> 00:13:56,520
right? And again, like one weird thing about this field is that the technologies that are

180
00:13:56,520 --> 00:14:00,800
being developed are ones that are very desired by big companies, right? Yeah. Look at the

181
00:14:00,800 --> 00:14:04,600
Googles and the Facebooks and therefore, massive amounts of resources in this. And so

182
00:14:04,600 --> 00:14:10,240
they actually have an impact as a nonprofit, which, you know, we're resourced, but it's

183
00:14:10,240 --> 00:14:15,040
very different from the level of resourcing that you would see at one of those companies

184
00:14:15,040 --> 00:14:19,040
that you really have to answer the question, what is it that I'm going to do that is the

185
00:14:19,040 --> 00:14:23,480
differential impact? How can I make the most, get the most bang for my buck, make sure

186
00:14:23,480 --> 00:14:28,880
that sort of the differential impact of this organization existing is as large as possible?

187
00:14:28,880 --> 00:14:33,480
And so that means that having a large open source project that lots of people are using

188
00:14:33,480 --> 00:14:39,040
is very, very valuable, but there are lots of other people who would do the same thing

189
00:14:39,040 --> 00:14:43,080
and where it's sort of, it's much more, you look at TensorFlow, right, that that's something

190
00:14:43,080 --> 00:14:46,240
that's great for Google because lots of people are using their tools, means that when

191
00:14:46,240 --> 00:14:50,760
they hire people that they're using the same platform. And so the incentive exists for

192
00:14:50,760 --> 00:14:55,400
the big companies to do that. And I think the thing that that we view as unique to us is

193
00:14:55,400 --> 00:15:00,800
really thinking about this AGI problem and thinking about how do you make sure that when

194
00:15:00,800 --> 00:15:04,840
it comes to, you know, there's kind of two problems that are really core to AGI. The

195
00:15:04,840 --> 00:15:08,120
first is, well, you're going to be building this really powerful system, right? You should

196
00:15:08,120 --> 00:15:12,680
imagine you basically are going to train like the goal, you know, general intelligence,

197
00:15:12,680 --> 00:15:16,320
like how should you even think about that? What even is it? It's going to be a system,

198
00:15:16,320 --> 00:15:19,280
I think, for the purposes of this conversation, we should define it as a system which can

199
00:15:19,280 --> 00:15:24,600
perform any economically valuable task as well as a human. And so if you can build that,

200
00:15:24,600 --> 00:15:28,480
first of all, to train it is going to require a lot more compute than to actually run it.

201
00:15:28,480 --> 00:15:31,800
You know, let's say that you're going to need a bunch of agents that each one of those

202
00:15:31,800 --> 00:15:35,600
agents is going to run much faster than real time in order to train. And so you should

203
00:15:35,600 --> 00:15:38,880
kind of think of whatever system you're going to build, well, there's going to be this massive

204
00:15:38,880 --> 00:15:42,760
data center. It's just going to be sitting idle while you're running your single AGI. And

205
00:15:42,760 --> 00:15:46,880
so really, you're going to have the ability to run lots of them from day one. And so you

206
00:15:46,880 --> 00:15:51,840
should kind of think of the thing you're going to build as this organization of the most

207
00:15:51,840 --> 00:15:55,320
competent person you've ever met that are all working together in concert towards a shared

208
00:15:55,320 --> 00:15:59,600
goal with no ego. And so, you know, it's going to be a pretty powerful system. And, you

209
00:15:59,600 --> 00:16:04,640
know, today we have lots of, we have lots of companies that are organizations of people

210
00:16:04,640 --> 00:16:08,880
and are able to accomplish pretty, pretty wild things. And I think if you can do build

211
00:16:08,880 --> 00:16:13,160
the kind of system, I just described that it's really hard to see what the limits of

212
00:16:13,160 --> 00:16:17,000
those of that's going to be. And so the first thing you have to ask is, is it going to

213
00:16:17,000 --> 00:16:22,360
do what we want at all? And this is what is referred to as, you know, technical safety

214
00:16:22,360 --> 00:16:28,080
and a problem that that we work on. So one one step towards the solving the technical

215
00:16:28,080 --> 00:16:32,800
safety side that we've done is this human future feedback project and collaboration

216
00:16:32,800 --> 00:16:37,680
of the mind. And the idea is that the core problem on technical safety is that you need

217
00:16:37,680 --> 00:16:42,800
to be able to specify goals. The AI somehow needs to listen to you. It needs to reflect

218
00:16:42,800 --> 00:16:47,840
human values. It needs to, you know, sort of do what humans want in some pretty deep way.

219
00:16:47,840 --> 00:16:52,240
So there needs to be humans in the training process somewhere. And the human feedback project

220
00:16:52,240 --> 00:16:56,680
that we worked on is this first step in this direction where a human labeler is shown

221
00:16:56,680 --> 00:17:01,600
two videos of a behavior. And they just click on which behavior is more like the one

222
00:17:01,600 --> 00:17:05,720
that they want. And we're able to show that with 500 bits of feedback that you're able

223
00:17:05,720 --> 00:17:10,560
to train an AI to view some back flips. So that's step one. This kind of work is something

224
00:17:10,560 --> 00:17:13,920
we think is really important. And it's a little bit taboo to talk about the field, right?

225
00:17:13,920 --> 00:17:18,040
The idea that, yeah, you could build these systems. They're going to do anything crazy.

226
00:17:18,040 --> 00:17:22,160
And I totally understand why, right? I think that there's several motivations for that.

227
00:17:22,160 --> 00:17:25,480
One is that the field has gone through these these hype cycles of booms and busts and

228
00:17:25,480 --> 00:17:29,840
winters and that to really think through the, well, what if this all succeeds? What

229
00:17:29,840 --> 00:17:35,280
if it works is something where if you're just going to go through another cycle of that,

230
00:17:35,280 --> 00:17:38,480
then there's really no point. And there's a second thing, which is that I think people

231
00:17:38,480 --> 00:17:43,680
really reason from what the computers that they can do, right? You take your Pascal GPU

232
00:17:43,680 --> 00:17:48,480
and sure you can train a great image classifier, but are you going to be able to train anything

233
00:17:48,480 --> 00:17:52,680
better than that? And the answer is, well, not really. But there's really two things

234
00:17:52,680 --> 00:17:57,720
that are changing that I think people don't see that are going to really accelerate the

235
00:17:57,720 --> 00:18:01,280
kinds of models that we can run. The first one of these I alluded to earlier, which

236
00:18:01,280 --> 00:18:06,080
is the fact that you can now use data center scale in order to get better performance.

237
00:18:06,080 --> 00:18:12,640
And again, this is 2012, basically two GPUs. You get severely diminishing returns beyond

238
00:18:12,640 --> 00:18:17,960
that. That's say, to the art 2014, you could do a GPU training and that seemed great.

239
00:18:17,960 --> 00:18:24,120
And just this year, Facebook did 256 GPUs, ImageNet one hour, and someone else just did 10,

240
00:18:24,120 --> 00:18:29,200
24 GPUs, ImageNet 15 minutes. And so you get this massive scaling of the kinds of models

241
00:18:29,200 --> 00:18:35,040
that we can run, the kinds of systems we can train. The second one, which is the acceleration

242
00:18:35,040 --> 00:18:40,360
of the neural net hardware. And if you look at everything up to 2016, it's actually very

243
00:18:40,360 --> 00:18:44,200
smoothly Moore's Law, even though it's not driven by the same factors as Moore's Law,

244
00:18:44,200 --> 00:18:48,560
which is pretty remarkable. So if you look, you can actually look in, there's this diagram

245
00:18:48,560 --> 00:18:53,240
from Ray Kurzweil's book where the data cuts off at 1998. And he just fits a double exponential

246
00:18:53,240 --> 00:19:00,400
to it and predicts exactly, basically, exactly correctly, the 2016 Pascal GPU is right on

247
00:19:00,400 --> 00:19:04,760
the curve that's predicted from this data just cutting off at 1998. And so very smooth

248
00:19:04,760 --> 00:19:09,640
Moore's Law all the way through. But this year's something weird happened. This year,

249
00:19:09,640 --> 00:19:14,240
we ended up with an order of magnitude increase in the number of flaps available for running

250
00:19:14,240 --> 00:19:18,440
neural nets. You know, these numbers are all public. So there's the Volta up at my head.

251
00:19:18,440 --> 00:19:24,600
So 2016 Pascal GPU is like 20 Teraflops. The Volta came out this year is more like 90

252
00:19:24,600 --> 00:19:29,720
Teraflops. And Google TPU 2.0 is more like 180 Teraflops. And so there's really this

253
00:19:29,720 --> 00:19:36,600
explosion this year. And the thing is, we really expect this kind of acceleration of the

254
00:19:36,600 --> 00:19:41,400
compute available for neural nets in a small compact package to continue to accelerate

255
00:19:41,400 --> 00:19:46,480
much faster than Moore's Law. And it's a very simple reason. The reason is that you

256
00:19:46,480 --> 00:19:52,960
can, the neural networks are very, very parallel. And just like the brain is this big network

257
00:19:52,960 --> 00:19:56,480
of, you know, just a bunch of tiny little cores, they're all talking to each other. And

258
00:19:56,480 --> 00:20:01,920
there's some learning rule and there's some propagation rule. And the way that we've

259
00:20:01,920 --> 00:20:06,200
always designed hardware is much more for serial execution. No one's really had this incentive

260
00:20:06,200 --> 00:20:10,040
to design this massively parallel hardware before. And so there's a lot of low hanging

261
00:20:10,040 --> 00:20:14,320
free. You don't need to invent any novel technology. You can just use, you don't have to rely

262
00:20:14,320 --> 00:20:18,320
on transistors getting smaller in order to get these speedups. And so if you combine these

263
00:20:18,320 --> 00:20:22,680
two things, data center scale with faster neural network accelerators, suddenly the compute

264
00:20:22,680 --> 00:20:27,160
available for any of our models is going to really skyrocket. And so that's kind of

265
00:20:27,160 --> 00:20:32,840
the perspective that we have is that things are going to be different as a result of the

266
00:20:32,840 --> 00:20:37,800
hardware coming online. Timelines are always tricky to predict exactly. But I think that

267
00:20:37,800 --> 00:20:43,040
we're going to see even just next year, I think if you just look at what we could do

268
00:20:43,040 --> 00:20:47,440
a year ago and what you can look at right now with respect to image generation with respect

269
00:20:47,440 --> 00:20:52,600
to voice generation, I think in 2018, as long as we continue to see the compute coming

270
00:20:52,600 --> 00:20:57,840
online and the way that we expect that we should be able to have perfect video generation,

271
00:20:57,840 --> 00:21:02,600
we should be able to have basically perfect speech synthesis as well. And this kind of

272
00:21:02,600 --> 00:21:07,120
this kind of acceleration in terms of the capabilities is going to be really tied to, well, we have

273
00:21:07,120 --> 00:21:12,240
all these ideas, these models, but we need the compute in order to run them. And as long

274
00:21:12,240 --> 00:21:16,480
as we get that compute that I expect to continue to see the capabilities increase and lock

275
00:21:16,480 --> 00:21:21,080
stuff, that's thing number one that's really important. Thing number two that is extremely

276
00:21:21,080 --> 00:21:27,000
important is the question of, okay, so let's say you build an AGI, it does what humans want

277
00:21:27,000 --> 00:21:32,600
to reflect humans values. So who's values, right? And who are the people who get to specify

278
00:21:32,600 --> 00:21:36,920
what this AI should want? And that's a much harder problem, right? The first one is a technical

279
00:21:36,920 --> 00:21:40,920
problem. That sounds like a thing where, you know, if it, you know, as these systems

280
00:21:40,920 --> 00:21:44,720
play out, we're very good at solving technical problems. If you can actually build this kind

281
00:21:44,720 --> 00:21:48,360
of very powerful system, like there's good reason to believe that we put in the effort

282
00:21:48,360 --> 00:21:52,520
you should also be able to make it safe and solve that technical side. It's not easy,

283
00:21:52,520 --> 00:21:56,680
but you have to really want it. You have to really try to solve that problem, but it seems

284
00:21:56,680 --> 00:22:00,840
like a thing that is solvable. The thing that's much harder is this non-technical problem

285
00:22:00,840 --> 00:22:05,680
of who owns it and who specifies the goals. And that that is something that is also very

286
00:22:05,680 --> 00:22:10,480
core to open AI and how we think about the value that we're delivering. And that we really

287
00:22:10,480 --> 00:22:16,720
want this technology to be something that is not just benefiting one corporation, one person,

288
00:22:16,720 --> 00:22:21,280
even one small subset of people. We really want this to be something that is benefiting

289
00:22:21,280 --> 00:22:26,960
the world. And we have ideas around the right way for that to play out. But I guess when

290
00:22:26,960 --> 00:22:31,120
it comes to how do we think about open, that's exactly how we think is solving those two

291
00:22:31,120 --> 00:22:36,200
problems. If we can do that, then that is the most important thing any of us could imagine

292
00:22:36,200 --> 00:22:43,200
doing. On that latter point in terms of whose values are those things that you have ideas

293
00:22:43,200 --> 00:22:48,240
about but haven't turned into projects that are you doing? Are there public projects that

294
00:22:48,240 --> 00:22:51,760
you've been working on that speak to that second item?

295
00:22:51,760 --> 00:22:55,320
Yeah, so it's something we spend a lot of time thinking about. I think that we haven't

296
00:22:55,320 --> 00:23:00,240
yet done public speaking about our thoughts there, but a lot of what we've been spending

297
00:23:00,240 --> 00:23:04,480
time doing has been building relationships with a lot of people in the field, a lot of

298
00:23:04,480 --> 00:23:09,080
people in governments, a lot of people just in various positions who I think will end

299
00:23:09,080 --> 00:23:14,080
up being influential with respect to how this technology plays out. And this kind of,

300
00:23:14,080 --> 00:23:19,560
I think it really is trying to lay the groundwork for where we hope things to go.

301
00:23:19,560 --> 00:23:23,920
One of the themes that kept coming up as you were speaking was this notion of like a timeline

302
00:23:23,920 --> 00:23:32,840
or time frame for AGI. Do you have one that you kind of manage to or is there general

303
00:23:32,840 --> 00:23:39,160
agreement within open AI and the community as to, you know, when we think AGI is going

304
00:23:39,160 --> 00:23:44,360
to happen or even the time scale and maybe has some context for this. I don't think I've

305
00:23:44,360 --> 00:23:49,160
told this story on the podcast before maybe I have, but relatively recently I was with

306
00:23:49,160 --> 00:23:53,880
some fellow entrepreneurs talking about we're just kind of catching up and someone pushed

307
00:23:53,880 --> 00:23:59,360
me on, hey, so, you know, on this AI safety issue, they didn't use those words, but like

308
00:23:59,360 --> 00:24:04,680
are you a Mark Zuckerberg or are you an Elon Musk? You know, and I tend to answer that

309
00:24:04,680 --> 00:24:08,440
question like, well, you know, it's kind of in the middle, I think, you know, there's

310
00:24:08,440 --> 00:24:12,680
a lot of sensationalism, but he kept pressing, pressing, pressing for me to answer. And

311
00:24:12,680 --> 00:24:16,960
one of the things that occurred for me in thinking about this was that, you know, if I think

312
00:24:16,960 --> 00:24:23,360
about who Elon Musk is, his time frame is probably way longer than mine, right? You know,

313
00:24:23,360 --> 00:24:28,680
the guys like building rocket ships, he's thinking long term, right? And I tend to answer

314
00:24:28,680 --> 00:24:35,560
that question in terms of, you know, I think people really over, over blow what, you know,

315
00:24:35,560 --> 00:24:41,160
is likely to happen in 10 years, right? And so I wonder, you know, with that as context,

316
00:24:41,160 --> 00:24:47,680
like how do you think about the world, you Greg and, and open AI more generally in terms

317
00:24:47,680 --> 00:24:51,280
of the time frame for worrying about and thinking about these kinds of issues?

318
00:24:51,280 --> 00:24:56,120
Yep. Timeline is a really interesting and hard question. It is the hardest question.

319
00:24:56,120 --> 00:25:02,040
I think this is true for any technology. If you look at the invention of flight, people,

320
00:25:02,040 --> 00:25:05,440
all the experts in the field right up until flight was created, we're saying flight is

321
00:25:05,440 --> 00:25:09,120
for the birds, the Newton had proved that everything air flight would never happen. And then

322
00:25:09,120 --> 00:25:12,640
you have the right brothers during their flight just a few months later. If you look at

323
00:25:12,640 --> 00:25:17,920
kind of any transformative technology, it is really the case that it's hard to distinguish

324
00:25:17,920 --> 00:25:21,480
exactly when it will happen. I think that there's something very inherent to this because

325
00:25:21,480 --> 00:25:26,280
if people knew, oh, okay, here's a timeline to it, then you would just focus more, work

326
00:25:26,280 --> 00:25:30,240
harder, and accelerate that timeline. And so they would turn out to be inaccurate. And

327
00:25:30,240 --> 00:25:35,480
I think this, yeah, the way that we really think about it. So I think, I think Elias Dukowski

328
00:25:35,480 --> 00:25:40,040
had a good blog post where he talked about something that he did and he was listening

329
00:25:40,040 --> 00:25:44,280
to a bunch of AI experts saying, AGI is very, very far away. And he went up and he asked

330
00:25:44,280 --> 00:25:48,320
people, okay, tell me, what is the least impressive accomplishment that you're very confident

331
00:25:48,320 --> 00:25:52,840
is not going to happen in the next two years. And people really didn't have a good answer.

332
00:25:52,840 --> 00:25:57,240
And how can it be that you both have thought very, very deeply about, okay, like it's going

333
00:25:57,240 --> 00:26:00,120
to take this long, it's going to take exactly this long. Here's where we're going to deliver

334
00:26:00,120 --> 00:26:04,680
it. And also don't have a, okay, here's something that I'm willing to bet. This is the least

335
00:26:04,680 --> 00:26:08,400
impressive thing that just we're not going to do this timeline. And I think what's really

336
00:26:08,400 --> 00:26:13,400
going on, I agree with the conclusion that he has there, is that people don't really have

337
00:26:13,400 --> 00:26:18,560
a good concept of it, right? People don't really have that in general, people end up picking

338
00:26:18,560 --> 00:26:24,400
with their gut rather than through having like really rational, here's exactly the factors

339
00:26:24,400 --> 00:26:26,920
that are going to enable it. And here's why we're not going to be able to do it in the

340
00:26:26,920 --> 00:26:31,600
near term. Besides the fact that, well, I look at what my, you know, I look at my, my

341
00:26:31,600 --> 00:26:35,920
dumb AI agent where I'm trying to get this thing to even be able to tell a cat from a dog

342
00:26:35,920 --> 00:26:40,080
and can you imagine trying to build something as smart as me, like, you know, it's just

343
00:26:40,080 --> 00:26:45,520
this disconnect. And so the way that we think about it is that we certainly know that

344
00:26:45,520 --> 00:26:48,760
some things are going to be changing. And I think that specifically the hardware is

345
00:26:48,760 --> 00:26:53,080
going to change in a way that people are not expecting right now. And it is faster

346
00:26:53,080 --> 00:26:56,680
than more so is not something that people price into there. They're internal sense of

347
00:26:56,680 --> 00:27:01,720
what's happening. And there's a question of how far does that take you on what timeline

348
00:27:01,720 --> 00:27:05,160
does it take you there? And the thing that's important to us as an organization is that

349
00:27:05,160 --> 00:27:09,840
regardless of what the timeline ends up being, that we are able to have the influence that

350
00:27:09,840 --> 00:27:13,720
we want, that we're able to ensure that this thing plays ends up playing out well. And

351
00:27:13,720 --> 00:27:18,440
the second part to that is that, well, you can also say, so you don't know when the super

352
00:27:18,440 --> 00:27:22,160
transformative stuff is going to happen. But you can say something about what is going

353
00:27:22,160 --> 00:27:26,000
to happen in the near term, what is going to happen over the next five years. And again,

354
00:27:26,000 --> 00:27:31,280
it's very clear we're going to be able to do synthesis of perfect videos. You know,

355
00:27:31,280 --> 00:27:35,120
you look at what's the 2020 presidential campaign going to look like when you're able

356
00:27:35,120 --> 00:27:40,800
to generate the kinds of videos that we already see we can, we're very, very close to being

357
00:27:40,800 --> 00:27:47,640
able to do. There are a bunch of technologies. Robotics is a perfect example where today

358
00:27:47,640 --> 00:27:53,560
there's been some, you know, there's been results of learning on robots. But that none of

359
00:27:53,560 --> 00:27:57,680
the roboticists are impressed because all of the tasks that people can accomplish are

360
00:27:57,680 --> 00:28:01,640
worse than what the roboticists can already do. And so if you talk to roboticists, they

361
00:28:01,640 --> 00:28:06,040
say, okay, you know, come back calmly, we need to do something I couldn't do in the 70s.

362
00:28:06,040 --> 00:28:10,280
And that's going to change. And the moment that you change that, the moment that you have

363
00:28:10,280 --> 00:28:15,400
your first learning based result that blows away what was possible without learning, I think

364
00:28:15,400 --> 00:28:19,400
there will be a sea change. Like we've seen this in the number of other disciplines we

365
00:28:19,400 --> 00:28:24,360
saw it with vision pre 2012. You go to the big vision conference and there's like one

366
00:28:24,360 --> 00:28:29,120
neural net paper for lucky. And now there's like, basically everything is neural net's

367
00:28:29,120 --> 00:28:33,480
vision. Like I don't think anyone even remembers that it was, it was different. And I think

368
00:28:33,480 --> 00:28:37,680
that on robotics that it's pretty clear that, you know, humans aren't any smarter. We're

369
00:28:37,680 --> 00:28:41,960
not getting any better at, I think, through these problems and being able to program all

370
00:28:41,960 --> 00:28:46,360
of the rules for exactly whatever about you do and how to react. And I would say that

371
00:28:46,360 --> 00:28:50,840
fact that you're going to have learning come in and really change what is capable, what

372
00:28:50,840 --> 00:28:56,040
robots are capable of. I think it's going to be massively impactful. And so that's how

373
00:28:56,040 --> 00:29:03,160
we think about it is that the big goal of AGI is something where you can't know, you certainly

374
00:29:03,160 --> 00:29:07,360
can't know that it's close, but I also don't know that you can know that it's super far.

375
00:29:07,360 --> 00:29:10,880
And that we also know that there's going to be transformative applications in the

376
00:29:10,880 --> 00:29:15,640
mere term. And so for us, the mandate for us, the way that we operate is stay on the

377
00:29:15,640 --> 00:29:19,280
cutting edge, make sure that we're pushing forward and always be asking, how can we ensure

378
00:29:19,280 --> 00:29:24,960
that our integral over time of value delivered is as large as possible?

379
00:29:24,960 --> 00:29:31,320
That was a non, I'm not giving a timeline answer. Very, very well stated. But so you

380
00:29:31,320 --> 00:29:38,800
did give two examples of applications where you think we'll see transformative short-term

381
00:29:38,800 --> 00:29:44,120
things happening. One is audio and video generation and the other is robotics. Are there specific

382
00:29:44,120 --> 00:29:52,320
examples of kind of leading indicators or examples that are leading indicators that kind

383
00:29:52,320 --> 00:29:56,920
of give you the confidence that those two specific things, for example, will change pretty

384
00:29:56,920 --> 00:29:58,520
dramatically, pretty quickly?

385
00:29:58,520 --> 00:30:05,200
Yeah, so I guess on the robotics front, so we worked on robotics. And this is our goal

386
00:30:05,200 --> 00:30:12,560
is to be able to change and to really unlock robotics through learning methods. And

387
00:30:12,560 --> 00:30:15,480
it's actually interesting because the way that we think about it, the way that we work

388
00:30:15,480 --> 00:30:24,280
on robotics is that we are geared towards trying to build general technologies rather than

389
00:30:24,280 --> 00:30:29,760
trying to maximize robotic capabilities and that that steers the set of projects that

390
00:30:29,760 --> 00:30:32,560
we're going to work on at once or are going to pick. But I think one thing that we're

391
00:30:32,560 --> 00:30:39,000
really excited about is if we succeed, we stay focused on AGI, but can also enable

392
00:30:39,000 --> 00:30:41,800
robotics to really kick off.

393
00:30:41,800 --> 00:30:45,840
What's an example of those two things in opposition to one another?

394
00:30:45,840 --> 00:30:51,000
One perfect example is that I think that there are so many really positive applications

395
00:30:51,000 --> 00:30:55,320
that we're going to see in robotics over upcoming years. Like, elderly care robots are

396
00:30:55,320 --> 00:30:59,600
perfect example, right? That's something that I think is going to deliver value to a lot

397
00:30:59,600 --> 00:31:03,120
of people. It's going to really be transformative to a number of people's lives, but it's also

398
00:31:03,120 --> 00:31:07,080
not necessarily something that we're going to work on ourselves. The kind of thing that

399
00:31:07,080 --> 00:31:13,320
we want to do is to build the underlying technology that will allow that application to happen,

400
00:31:13,320 --> 00:31:18,200
but stay focused on pushing forward on new applications rather than productizing.

401
00:31:18,200 --> 00:31:24,240
Got it. So is that different than where basic research as opposed to applied research

402
00:31:24,240 --> 00:31:26,880
shows that is there another nuance to that?

403
00:31:26,880 --> 00:31:32,960
Yeah, so I'd say that we're halfway in between because when I think basic research,

404
00:31:32,960 --> 00:31:38,640
and I guess it might depend per field, but when I hear basic research, I think of the

405
00:31:38,640 --> 00:31:43,120
individual sport type research, right, of people kind of off on their own, thinking

406
00:31:43,120 --> 00:31:46,720
deep thoughts and coming back when they have something that seems cool.

407
00:31:46,720 --> 00:31:53,640
And that for us, that we really try to take results and push them to the limits of scale.

408
00:31:53,640 --> 00:31:58,800
And with our Dota system, that's exactly what we did where rather than just show that,

409
00:31:58,800 --> 00:32:03,280
hey, here's some system that can kind of work on, you know, in some toy way, actually

410
00:32:03,280 --> 00:32:07,680
work on a really hard task. And that I think the distinguishes it from applied research

411
00:32:07,680 --> 00:32:13,360
is that we focus, so solving Dota is clearly not going to be something that is going to

412
00:32:13,360 --> 00:32:18,080
be transformative for many people's lives and transform it to a subset of people, but

413
00:32:18,080 --> 00:32:24,120
not in the kind of direct impact that one would have in a more applied setting.

414
00:32:24,120 --> 00:32:29,240
Yeah, one of the things that I saw recently that, you know, is it a bit of an example of

415
00:32:29,240 --> 00:32:35,640
what you're suggesting will happen to videos. The Nvidia recently published some work

416
00:32:35,640 --> 00:32:40,520
using GANs to generate these synthetic celebrity faces. Did you see that one?

417
00:32:40,520 --> 00:32:43,640
Yeah, absolutely. That was incredible. That was incredible. Yeah, I was going to bring

418
00:32:43,640 --> 00:32:47,080
that up as another leading indicator of this kind of thing.

419
00:32:47,880 --> 00:32:52,280
Yeah, and so and we've already seen like, there's some other research. I forget if it was related

420
00:32:52,280 --> 00:32:59,240
to GANs or another approach where you're able to, you know, given kind of static photographs,

421
00:32:59,240 --> 00:33:04,040
you're able to kind of create three-dimensional and, you know, change the expression on the

422
00:33:04,040 --> 00:33:10,280
photographs. All of these, like the pieces are all in place, or near in place to create these

423
00:33:10,280 --> 00:33:15,000
perfect synthetic videos, although the full end-to-end thing isn't quite there yet.

424
00:33:15,000 --> 00:33:18,120
Yeah, and you know, you need to get the full end-to-end thing in place.

425
00:33:18,120 --> 00:33:24,920
What's that? Compute. And that's it. Is that your, it's just compute?

426
00:33:24,920 --> 00:33:29,240
For that particular problem, I think that our ideas are really proving out,

427
00:33:29,240 --> 00:33:32,600
and if we were able to run at larger scale, that we'd have a really good time,

428
00:33:32,600 --> 00:33:38,520
and I think that it is, I think, really important to also drill into this story around compute,

429
00:33:38,520 --> 00:33:43,320
because it's not as simple as just you take the code that someone already wrote,

430
00:33:43,320 --> 00:33:47,400
and you just run it on more GPUs, and it's magically going to solve the problem.

431
00:33:47,400 --> 00:33:54,680
But it's much more that it's like compute in this field is just like particle accelerators in physics.

432
00:33:54,680 --> 00:33:59,240
If you don't have the particle accelerator, you're just not going to discover the secrets of the

433
00:33:59,240 --> 00:34:03,000
universe, right? You're not going to have the breakthrough. If you have the particle accelerator,

434
00:34:03,000 --> 00:34:07,480
it's not just that you just, you know, somehow like the physicist is not useful, and just,

435
00:34:07,480 --> 00:34:13,160
you know, just translating ideas into experiments. It's that you now have this tool that

436
00:34:13,160 --> 00:34:19,160
fundamentally allows you to achieve the result that you were looking for, and that's really where

437
00:34:19,160 --> 00:34:24,680
we are on video generation, is that we have ideas that are clearly like in the right space,

438
00:34:24,680 --> 00:34:28,600
and maybe we need some additional tricks, maybe we need to do some additional tuning,

439
00:34:28,600 --> 00:34:33,160
but if we're able to run at much larger scale than we are right now, then we can actually try

440
00:34:33,160 --> 00:34:37,240
out these ideas that we have. And I think that the converse is also true, that is that if,

441
00:34:37,800 --> 00:34:42,200
for whatever reason, we were to freeze the level of compute that is available for run these models,

442
00:34:42,200 --> 00:34:50,040
that progress would really slow down. Yeah, your earlier point about you kind of hinted at this

443
00:34:50,040 --> 00:34:54,680
a couple of times in the conversation, but you know, I think one of the things that contributes

444
00:34:54,680 --> 00:35:02,440
to our ability to, you know, predict, well, a couple of things, I think, contribute to our ability

445
00:35:02,440 --> 00:35:08,600
to, or the difficulty we have predicting when AGI happens is, I don't know that we've like

446
00:35:08,600 --> 00:35:15,240
clearly, maybe I should phrase this as a question, like how well defined do you think it even means

447
00:35:15,240 --> 00:35:23,320
to have achieved AGI? Like, is it absolute, or is there like a minimum viable AGI product

448
00:35:23,320 --> 00:35:28,680
that would suffice? Yeah, I think this is a good question. I kind of think of whatever I think of

449
00:35:28,680 --> 00:35:33,240
the question of how do you define AGI? What does AGI? What will AGI look like? I always think

450
00:35:33,240 --> 00:35:38,840
a little bit of, are you, have you heard of bike shedding as a term? Yep, absolutely. Yeah, so it

451
00:35:38,840 --> 00:35:42,360
always, you should explain it though. You should explain bike shedding. So the idea behind bike

452
00:35:42,360 --> 00:35:46,200
shedding is, so let's say that you're designing a nuclear reactor. So what you'll do is you'll

453
00:35:46,200 --> 00:35:50,440
bring in these, you know, experts and kind of, the experts will tell you things and honestly, like,

454
00:35:50,440 --> 00:35:53,800
if they tell you, like, you got to do it this way and like, this is really important,

455
00:35:53,800 --> 00:35:57,480
you'll probably trust them and say, okay, you do it, like, you've got a lot of experience in this,

456
00:35:57,480 --> 00:36:01,400
this is great, like, go off and run with it. So when it comes to, okay, we're also going to have

457
00:36:01,400 --> 00:36:05,720
this bike shed outside and what color should it be? Everyone's going to have an opinion, right?

458
00:36:05,720 --> 00:36:10,520
Everyone feels like they are an equally qualified expert to talk about bike shed colors. And I think

459
00:36:10,520 --> 00:36:16,600
that with intelligence, there's something similar here where we all have our own conception of

460
00:36:16,600 --> 00:36:21,640
intelligence, what it's like, what's hard, what it is that we do, what's going on in our heads.

461
00:36:22,600 --> 00:36:28,920
And so I think that the question of, okay, well, this system that you've built does this,

462
00:36:28,920 --> 00:36:33,240
but it doesn't do that. What is AGI going to look like? How hard is it? When is it going to arrive?

463
00:36:33,240 --> 00:36:38,600
I think these things end up being approached, kind of like the bike shed where everyone has their

464
00:36:38,600 --> 00:36:44,360
daily experience and kind of fit that to, you know, I think in one thing that is true is that

465
00:36:44,360 --> 00:36:51,080
no one is truly an expert in AGI, right? We haven't built it yet. And so anyone who is claiming that

466
00:36:51,080 --> 00:36:56,200
I've got this special knowledge, it's a little hard to take that at face value, right? You can't

467
00:36:56,200 --> 00:37:03,240
go to university and say like, well, I got my AGI undergraduate degree. And built five of them

468
00:37:03,240 --> 00:37:07,720
in the course of getting it. That's right. That's right. That's right. And so I think that, you know,

469
00:37:07,720 --> 00:37:11,160
the bike shedding term is like, I think kind of a negative one usually, but I view it in this

470
00:37:11,160 --> 00:37:17,400
almost positive way where we have such like intelligence is just so fundamental to us and who we are

471
00:37:17,400 --> 00:37:21,320
in this notion of what it even means to be human, that everyone has thought about this, you know,

472
00:37:21,320 --> 00:37:27,160
thousands of years ago people were speculating about what it would be to build a mind and what goes

473
00:37:27,160 --> 00:37:31,960
on inside of our own heads. And so I think it's actually kind of this marvelous thing that people

474
00:37:31,960 --> 00:37:37,240
care so much, but the flip side ends up being that you almost have this philosophical debate that

475
00:37:37,240 --> 00:37:42,200
becomes very irreducible. And so the way that I think about it is that to the extent we're just

476
00:37:42,200 --> 00:37:46,440
going to try to resolve philosophical questions that have been standing for 2,000 years,

477
00:37:46,440 --> 00:37:51,880
we are probably out of luck except to the extent that our technical progress informs us. And so,

478
00:37:52,440 --> 00:37:58,200
for example, we now have a much better sense of what it's going to be to build a mind than Aristotle

479
00:37:58,200 --> 00:38:03,720
would have. So it's not going to be some big rule-based system. It's not going to be most of the

480
00:38:03,720 --> 00:38:07,480
things that you might have expected. It's going to be a big statistical system. It's going to run

481
00:38:07,480 --> 00:38:12,120
this massive parallel fashion on a bunch of cores and, you know, you can kind of describe things

482
00:38:12,120 --> 00:38:19,320
like that. Is it going to be majors multiplies and taking gradients? Well, that's a different question,

483
00:38:19,320 --> 00:38:25,240
right? And we certainly have not resolved that yet. But I think that the question then of,

484
00:38:25,240 --> 00:38:30,440
okay, so what is an AGI? A lot of how I like to frame that conversation is to kind of sidestep

485
00:38:30,440 --> 00:38:33,640
the deep philosophical questions of do you need to have something that's conscious? Do you need to

486
00:38:33,640 --> 00:38:37,960
have something that kind of fulfills other notions of intelligence and really just focus on

487
00:38:37,960 --> 00:38:45,000
what can it do? Can you build a system that is able to accomplish any economically valuable

488
00:38:45,000 --> 00:38:49,800
tasks that you put in front of it? I think that is something where you can tell, right? I think

489
00:38:49,800 --> 00:38:54,200
that you can tell if another way of reason about this is if you took a human and you wanted to

490
00:38:54,200 --> 00:38:58,920
figure out is this person, again, real intelligence? Is that something that you think you could test?

491
00:38:58,920 --> 00:39:03,400
And this is, you know, we certainly spend a lot of time trying to assess various people's

492
00:39:03,400 --> 00:39:07,880
skills and capabilities on various different axes and, you know, you can almost think of it as

493
00:39:07,880 --> 00:39:12,200
deciding if you built an AGI as giving it a bunch of different job interviews and seeing if you

494
00:39:12,200 --> 00:39:18,760
want to hire it. And I think that there's that this kind of framing of there are deep philosophical

495
00:39:18,760 --> 00:39:24,680
questions. But at the end of the day, you can think about it instead in terms of

496
00:39:25,640 --> 00:39:30,600
very functional, what is the system capable of? And the latter is something we're able to do,

497
00:39:30,600 --> 00:39:36,920
the former is something that is fundamentally very hard. I also think that this framing really

498
00:39:36,920 --> 00:39:43,480
raises a second point, which is, well, is this, you know, it's a very utilitarian kind of view

499
00:39:43,480 --> 00:39:47,240
of the kind of system that we're talking about, the kind of things that we might might want to build.

500
00:39:47,240 --> 00:39:53,080
And why should we want to build something like that at all? You know, if like the really

501
00:39:53,080 --> 00:39:56,680
opens this Pandora's box of what does it mean to be human and what is the value of humans,

502
00:39:56,680 --> 00:40:01,400
how do we make sure that humans have have meaning and really a place in the resulting world?

503
00:40:01,400 --> 00:40:06,920
And that is, I think, the hardest problem. And that is something that is, you know, something

504
00:40:06,920 --> 00:40:12,440
that's very core to open AI and how we think about this technology is that it's pretty clear

505
00:40:12,440 --> 00:40:17,080
that, like I think, indisputable, that in a short term, that companies are pouring in tons and

506
00:40:17,080 --> 00:40:22,920
tons of resources in order to make advances in AI, which is different from AGI. I think that the

507
00:40:22,920 --> 00:40:30,200
amount of resources going to that is smaller, is more focused. But I think that as it feels closer

508
00:40:30,200 --> 00:40:36,360
to people, as people feel that while I look at all this progress in AI, it feels like, you know,

509
00:40:36,360 --> 00:40:40,920
kind of my internal neural net is telling me that this could actually happen. And then you start

510
00:40:40,920 --> 00:40:44,760
thinking through the economic value that would be delivered by that system and how important it

511
00:40:44,760 --> 00:40:49,720
could be for the next company or why company, I think that that will change. And then I think that

512
00:40:49,720 --> 00:40:55,720
the question is not so much about accelerating the timeline to AI, but it's really about ensuring

513
00:40:55,720 --> 00:41:02,520
that this technology plays out in a way that isn't just one company gets on the spoils. But

514
00:41:02,520 --> 00:41:08,040
it's really about humanity is ultimately the winner. Right. You know, it may turn out, we may

515
00:41:08,040 --> 00:41:14,840
get thrown a curveball here and it may turn out that the technologies and techniques that allow

516
00:41:14,840 --> 00:41:21,720
us to create AGI are totally orthogonal to the ones that, you know, we've created in the process

517
00:41:21,720 --> 00:41:27,480
of trying to create AI. But, you know, from where we sit now, it certainly seems like, you know,

518
00:41:27,480 --> 00:41:32,680
to the point of all the pieces that we discussed that go into creating these videos, like they're all

519
00:41:32,680 --> 00:41:37,720
kind of right in line with the kinds of problems we would expect to have to solve in order to get to

520
00:41:37,720 --> 00:41:44,360
an AGI. So all of that huge investment that is, you know, profit driven, if we can say, you know,

521
00:41:44,360 --> 00:41:49,160
on the part of the many of the companies, most of the companies that are investing in those technologies

522
00:41:49,160 --> 00:41:55,640
are, you know, maybe accidentally pushed us closer to this AGI. Yep. Yeah. And it's actually

523
00:41:55,640 --> 00:42:00,920
pretty interesting that there's this classic mantra in the field that as soon as you're able to do

524
00:42:00,920 --> 00:42:06,840
it, it isn't AI anymore. And people said this about chess. Right. The chess is the most important

525
00:42:06,840 --> 00:42:11,240
thing. And that only, you know, some people are solving the thing. Exactly. And it turned out,

526
00:42:11,240 --> 00:42:15,720
all of that stuff, once it happened, people are like, ah, that's not AI. I think that this is dead.

527
00:42:15,720 --> 00:42:21,720
I think that this way of people reacting to things we're able to do is now different. And you will

528
00:42:21,720 --> 00:42:27,960
get, AlphaGo, you will get Dota. And for these systems, there really is something going on in them

529
00:42:27,960 --> 00:42:36,600
that is very akin to intuition is much deeper than simply performing some big search. And being

530
00:42:36,600 --> 00:42:43,960
very dumb and making up for that dumbness with just having your massive brain. And you think about

531
00:42:43,960 --> 00:42:49,640
the image generation that came from NVIDIA. And that's something where humans can't even sit down

532
00:42:49,640 --> 00:42:55,000
and start to think about how you could write the rules for it. And so I think this is a very

533
00:42:55,000 --> 00:42:59,320
encouraging thing. And I think that there is a, there's, there's kind of this, this piece to it,

534
00:42:59,320 --> 00:43:05,880
which is what's really going on right now is that if you look at the problem of trying to recognize

535
00:43:05,880 --> 00:43:11,000
a cat or dog in an image, trying to recognize objects and images, that the space of image is

536
00:43:11,000 --> 00:43:17,240
the super complicated, very high dimensional space. This is a high dimensional manifold. It's,

537
00:43:18,040 --> 00:43:22,760
there's this fundamental complexity in that domain. And so for the human to write down all the

538
00:43:22,760 --> 00:43:28,520
rules for that would be a pretty massive undertaking. And so what we've built is that we have these

539
00:43:28,520 --> 00:43:33,400
systems which are able to absorb the complexity of the domain and able to kind of figure themselves

540
00:43:33,400 --> 00:43:37,400
around and that you've got this neural network that's got these millions of parameters. And that's

541
00:43:37,400 --> 00:43:41,080
just not something that exists in the natural world. It's not something that we're used to.

542
00:43:41,080 --> 00:43:47,000
And it's able to reconfigure itself and to, to really absorb all of the inherent,

543
00:43:47,000 --> 00:43:55,960
that inherent complexity. And I think that the, a building to do that is what really distinguishes

544
00:43:55,960 --> 00:44:02,600
this learning revolution from AI previously. And now it might turn out that there are limits to

545
00:44:02,600 --> 00:44:06,680
what we can do with our learning algorithm. But it's also kind of crazy that the learning algorithm

546
00:44:06,680 --> 00:44:14,840
we use backpropagation is developed in 1986. How can it be that this algorithm and really

547
00:44:14,840 --> 00:44:20,120
neural nets and some ways date back to even maybe the 60s, maybe the 40s depending on how you count

548
00:44:20,120 --> 00:44:25,000
that these very simple, very obvious ideas that you couldn't run on your, you know, your particle

549
00:44:25,000 --> 00:44:28,840
accelerators if you, if you will, you didn't have the parts of accelerators to run the experiments.

550
00:44:28,840 --> 00:44:33,000
But these simple ideas turned out to be so powerful. And I think there's something really

551
00:44:33,000 --> 00:44:38,920
fundamental there that I can't decide between two different explanations. One is that intelligence

552
00:44:38,920 --> 00:44:44,280
is fundamentally simple that there's a, you know, I can, I can kind of back explain some explanation

553
00:44:44,280 --> 00:44:49,880
of well, if you had something that was complicated, then it would have a very large prior. And so

554
00:44:49,880 --> 00:44:53,880
you're kind of making this prior. And so yeah, you shouldn't expect it to be very general. The more

555
00:44:53,880 --> 00:44:58,280
depressing version of this is that well, maybe we're just really bad at making anything complicated work.

556
00:44:58,280 --> 00:45:05,080
But if it's the first, and I think there's a lot of evidence that really indicates that it is the

557
00:45:05,080 --> 00:45:10,280
first, then I think that's very encouraging that the simple ideas, if you implement them correctly,

558
00:45:10,280 --> 00:45:14,520
the mathematics, if the mathematics works, right, if the math kind of points in the right direction,

559
00:45:14,520 --> 00:45:18,520
if you implement it correctly, you scale it up massively, then you're going to be able to get

560
00:45:18,520 --> 00:45:23,800
things that, these will happen that you weren't expecting. And one thing that's really weird to

561
00:45:23,800 --> 00:45:30,520
me about the kind of progress that I see is that I've seen on repeated occasions algorithms that

562
00:45:30,520 --> 00:45:38,280
work better at large scale than their designers expected, right? And so we've seen this with algorithms

563
00:45:38,280 --> 00:45:42,840
here where talk to the person who invented it and they say, oh, no, that's not going to work for

564
00:45:42,840 --> 00:45:48,840
XYZ reason. And then we scaled up really large and actually works really well. And I think that this

565
00:45:48,840 --> 00:45:53,880
is going to, this is, again, for me as an engineer is totally contrary to experience. For me as an

566
00:45:53,880 --> 00:45:58,920
engineer, it's, you really only get, if you're lucky, the kind of performance that the person was

567
00:45:58,920 --> 00:46:04,360
intending and as soon as your 10x scale, 100x scale, good luck. Everything starts to break, right?

568
00:46:04,360 --> 00:46:11,640
Totally. Totally broken. Is there more to that than just more data and more data, you know, fixing

569
00:46:11,640 --> 00:46:18,520
more problems in terms of, or more data basically covering for our lack of sophistication in the

570
00:46:18,520 --> 00:46:22,680
algorithms themselves? Yeah, so it's something like that. Though I would phrase it a little

571
00:46:22,680 --> 00:46:28,760
differently, which is that like I think that the algorithms that we have are fundamentally capable

572
00:46:28,760 --> 00:46:35,240
of absorbing all the compute and data you can throw at them. And the data question is also an

573
00:46:35,240 --> 00:46:39,400
interesting one because the thing that people are used to is supervised learning where you have this

574
00:46:39,400 --> 00:46:44,520
big static data set that encapsulates your world knowledge. But where things are really shifting

575
00:46:44,520 --> 00:46:48,280
is towards more of the reinforcement learning paradigm. And if you think about it, that's where you

576
00:46:48,280 --> 00:46:52,280
want to be, right? You want to have an environment that you're interacting with that you're able to

577
00:46:52,280 --> 00:46:58,120
change. You have this dynamic feedback loop going on. And there you suddenly have upgraded your

578
00:46:58,120 --> 00:47:03,240
environment. Like you can think of your big set of images as just a static environment. And now

579
00:47:03,240 --> 00:47:08,600
you've upgraded to this very dynamic world. And there suddenly you're, you sort of are able to get

580
00:47:08,600 --> 00:47:12,920
infinite data, or at least you can, you can spend a lot of compute to get a lot of data.

581
00:47:12,920 --> 00:47:19,240
If it's a video game like Dota, you can run this on many, many cores. If it's a robotic simulator,

582
00:47:19,240 --> 00:47:24,760
you again can spend a bunch of compute there. If it's a real world, you're in for a little bit

583
00:47:24,760 --> 00:47:29,080
of a harder time. And so maybe you do something like Google did with having a big arm farm.

584
00:47:29,720 --> 00:47:34,520
Maybe you do something else. And I think we really want to end up is that we want to end up

585
00:47:34,520 --> 00:47:39,960
in a place where the limiting factor is the amount of compute that we can throw at these models.

586
00:47:39,960 --> 00:47:44,200
And where we can have a massive generative models that have absorbed a lot of world knowledge

587
00:47:44,200 --> 00:47:49,160
that you're able to do things inside of that. And we can't run those models yet today. We're at the

588
00:47:49,160 --> 00:47:54,680
very, you know, sort of, we're at the very nascent edge of what I expect. We're going to be able to

589
00:47:54,680 --> 00:48:00,040
do with generative models and with this kind of approach. Model-based RL is kind of the

590
00:48:00,040 --> 00:48:04,280
the term, the term of art that a lot of people use. But I think that in upcoming years we will be

591
00:48:04,280 --> 00:48:09,960
able to see lots of progress based on these ideas of scale up, use algorithms that can absorb all

592
00:48:09,960 --> 00:48:15,000
the compute and that that can make up for lack of data, that can make up for lack of everything else.

593
00:48:16,840 --> 00:48:21,560
And what specifically does model-based RL refer to relative to just RL?

594
00:48:21,560 --> 00:48:27,240
Yeah. So the idea with model-based RL is that if you have a, it may be learned or maybe not

595
00:48:27,240 --> 00:48:32,440
learned in some way, model of the environment that you query and you can kind of explore with

596
00:48:32,440 --> 00:48:36,680
them. It's kind of like you as a human if you, you know, picture your house and picture walking

597
00:48:36,680 --> 00:48:40,360
around your house and you can kind of plan things out. You can see like, oh, if I, you know,

598
00:48:40,360 --> 00:48:45,320
do this, this thing will happen and then you don't actually have to go and spend the very expensive

599
00:48:45,320 --> 00:48:49,960
time of walking through your house. And that kind of thing, you can see it's very powerful to have

600
00:48:49,960 --> 00:48:55,240
this, this ability to plan and explore an imagination rather than the real environment. But again,

601
00:48:55,240 --> 00:48:59,400
it's all very nascent. It doesn't really work right now. And I think that it really cannot work

602
00:48:59,400 --> 00:49:05,880
until we have the faster computers online. One of the things you said at the very beginning of the

603
00:49:05,880 --> 00:49:12,680
interviews kind of stuck with me is, is interesting. And that is this idea that ultimately to train

604
00:49:12,680 --> 00:49:19,960
an AGI, it's going to require massive amounts of compute. But then once we train it, like the actual,

605
00:49:19,960 --> 00:49:25,000
you know, inference, letting that AGI, you know, be generally intelligent is going to require

606
00:49:25,000 --> 00:49:29,880
much less compute. And, you know, it strikes me that there are some interesting questions there.

607
00:49:29,880 --> 00:49:34,120
Like, what do we do with all that compute? You know, you address some of it in terms of,

608
00:49:34,760 --> 00:49:40,200
well, you kind of phrase it as, you know, maybe the thing that we are doing is we are running

609
00:49:40,200 --> 00:49:47,240
multiple instances of this AGI thing in parallel, right? So we're taking advantage of all that

610
00:49:47,240 --> 00:49:52,920
compute that we had to create to train it by, you know, running a bunch of these things in parallel.

611
00:49:52,920 --> 00:50:00,360
But it also kind of makes me wonder if maybe the AGI doesn't need to be all that general,

612
00:50:00,360 --> 00:50:08,200
if we're, you know, ultimately segmenting, you know, the problem space up in the end anyway.

613
00:50:08,200 --> 00:50:11,800
Does that question make sense? What is that? Do you see where I'm going with that?

614
00:50:11,800 --> 00:50:17,960
Not entirely. I guess there are two questions here. I guess one, you know, are there other

615
00:50:17,960 --> 00:50:25,720
implications of of this idea that you propose that, you know, we're going to have, we're going to

616
00:50:25,720 --> 00:50:33,480
have to build up this massive compute capability to train the AGI. And then, you know, once we've

617
00:50:33,480 --> 00:50:37,880
trained it, we need that compute capability less. Like, what are all the implications of that?

618
00:50:37,880 --> 00:50:43,640
That's one question. And then question number two is, you know, if ultimately, you know,

619
00:50:43,640 --> 00:50:50,600
what we end up doing is running a bunch of parallel intelligences, you know, do they all need

620
00:50:50,600 --> 00:50:55,720
to be general anyway? Can we have a bunch of a cluster of intelligences that, you know,

621
00:50:55,720 --> 00:51:00,520
are really good at thing X, a cluster of intelligence that are good at thing Y, you know, scale that out.

622
00:51:00,520 --> 00:51:05,960
And that is what we, that is what ultimately we start to think of as general intelligence.

623
00:51:05,960 --> 00:51:09,560
We just have a bunch of these less general intelligences.

624
00:51:09,560 --> 00:51:14,680
Yeah, it makes a lot of sense. So I said on the first one, well, so one, one thing that I think

625
00:51:14,680 --> 00:51:21,320
is worth thinking about is when you actually build a computer system that is autonomously generating

626
00:51:21,320 --> 00:51:26,440
huge amounts of revenue or value, there suddenly becomes this big incentive to make more such

627
00:51:26,440 --> 00:51:31,800
computer systems. Like today, if you have a big pile of money, you want to turn it into more money,

628
00:51:31,800 --> 00:51:36,840
well, you start a company or you invest in a company and you hire a bunch of people and those

629
00:51:36,840 --> 00:51:41,960
people produce economic value towards some goal and that it kind of continues the cycle. Whereas

630
00:51:42,520 --> 00:51:48,120
if you have a computer that is just as good as a human worker, well, then you have a big pile of

631
00:51:48,120 --> 00:51:52,280
money, you should build a big data center and there's going to be this big incentive to kind of

632
00:51:52,280 --> 00:51:56,680
dot the world with with data center. So I think that's one perspective on what happens on a

633
00:51:56,680 --> 00:52:03,640
computer front. I think it is possibly the case that you can take your big training data center

634
00:52:03,640 --> 00:52:09,080
and use all of that compute to run a single AI much faster. And so rather than imagine if you

635
00:52:09,080 --> 00:52:16,520
had a Einstein in Silicon that you're now able to run a thousand X real time or a million X real time,

636
00:52:16,520 --> 00:52:20,280
I mean, pretty good, right? You know, this person sitting around thinking about physics and

637
00:52:20,280 --> 00:52:24,360
thinking about you got someone in there thinking about medicine and how to cure diseases, you got

638
00:52:24,360 --> 00:52:28,200
someone in thinking about how should we build rockets to go to the stars and all sorts of things

639
00:52:28,200 --> 00:52:32,680
like that. Like that would be a pretty valuable, pretty good. It's not guaranteed that we'll be

640
00:52:32,680 --> 00:52:39,800
able to use all of that compute usefully in a single AI, but I think that at the very least being

641
00:52:39,800 --> 00:52:44,280
able to run parallel copies of these of these AI is something that we should expect. And then there's

642
00:52:44,280 --> 00:52:49,640
a question of, well, what would that be good for? And I guess when I think about these, I always

643
00:52:49,640 --> 00:52:55,480
try to make analogies to things that are in our experience today. And so in our experience today,

644
00:52:55,480 --> 00:53:00,360
why do we ever want to have a group of more than one human doing something? You know, it's like

645
00:53:00,360 --> 00:53:04,360
building companies and the tasks are hard and that you have different people that specialize in

646
00:53:04,360 --> 00:53:09,000
different skills and all of those things are things that we should expect would transfer to the

647
00:53:09,000 --> 00:53:13,800
systems of the future. And so I think it'll be very valuable. By the way, so the idea of

648
00:53:13,800 --> 00:53:18,360
a computer system that autonomously produces value where all the interesting stuff is done by

649
00:53:18,360 --> 00:53:21,880
the computers and the humans just kind of stick around and clean out the fans is something that

650
00:53:21,880 --> 00:53:27,800
exists today. Sounds pretty dystopic, but if you look at Bitcoin mines, that is exactly what they

651
00:53:27,800 --> 00:53:32,360
are. And there's a good article recently with a bunch of pictures from Chinese Bitcoin mines,

652
00:53:32,360 --> 00:53:36,440
which I recommend looking at if you want to think about kind of the more cyberpunk, this topic

653
00:53:36,440 --> 00:53:42,200
version of this stuff. And so again, there are a lot of hazards here with the technology that we're

654
00:53:42,200 --> 00:53:48,040
talking about building. And again, the weirdest thing for me is the fact that it's so that people

655
00:53:48,040 --> 00:53:52,680
don't talk about this in a serious way and that I think that the, for most technologies, when

656
00:53:52,680 --> 00:53:57,400
you're building them, you think about what happens if we really succeed. And I think that for

657
00:53:57,400 --> 00:54:02,920
partially historic reasons, partially for this, this reason that we all feel our own sense of

658
00:54:02,920 --> 00:54:07,240
how far off the AGI is and how hard it's going to be and how impossible this team and imagine

659
00:54:07,240 --> 00:54:13,480
building it that really seriously think me through what happens if it works is something that

660
00:54:13,480 --> 00:54:19,880
is a bit to do. So that's thing one. And then question two, can you remember your question two was?

661
00:54:19,880 --> 00:54:27,880
I think question two was, you know, ultimately, do we need AGI at all if the deployment model,

662
00:54:27,880 --> 00:54:35,080
if you will, ends up being to, or the scalability model ends up being to segment our workload into

663
00:54:35,080 --> 00:54:42,440
a bunch of separate things. You know, does a collection of, you know, more specialized intelligences,

664
00:54:42,440 --> 00:54:48,280
you know, become the thing that we initially come to see as a general intelligence?

665
00:54:48,280 --> 00:54:53,960
Yeah. So I think that's an open question or that's a possibility. The way that I think about it

666
00:54:53,960 --> 00:54:59,480
is, I guess, again, back to the idea of we have organizations of humans that can accomplish goals

667
00:54:59,480 --> 00:55:05,320
that humans individually cannot. And so it might well be that even though you want to put specialization,

668
00:55:05,320 --> 00:55:11,240
I would certainly expect that you'll end up with specialization towards specific tasks. I think

669
00:55:11,240 --> 00:55:17,960
that I would expect that a general AI would also have these very hyper-trained narrow AI modules

670
00:55:17,960 --> 00:55:23,640
within it. And you absolutely should do that. And you know, like one thing I think is kind of

671
00:55:23,640 --> 00:55:28,040
interesting about today's AI systems is if you look at something like the Neural Turing machine,

672
00:55:28,040 --> 00:55:31,240
you know, you basically spend, it was a big model, you spent a lot of compute, a lot of data,

673
00:55:31,240 --> 00:55:35,800
a lot of training time in order to learn how to do, how to do various tasks. For example,

674
00:55:35,800 --> 00:55:41,720
one of the tasks from the original paper is to learn to sort. And pretty cool, right? This system

675
00:55:41,720 --> 00:55:46,760
learns how to sort in, you know, it's kind of learned this program. But when you really think about it,

676
00:55:46,760 --> 00:55:52,280
it's like, I could do the same thing at Python on my core in like two seconds. And so at the end

677
00:55:52,280 --> 00:55:57,160
of the day, if you have a specific task you're trying to solve, you can hyper-optimize for that

678
00:55:57,160 --> 00:56:02,280
and do a lot better from a phase efficiency standpoint than this very general thing. I think

679
00:56:02,280 --> 00:56:08,280
something similar happens with humans where we have, like when you have to sit and think about

680
00:56:08,280 --> 00:56:11,480
something when you're not a master of it, and you're trying to really reason how it works

681
00:56:11,480 --> 00:56:16,520
versus when you've practiced a bunch and it's in your muscle memory, right? It's kind of like,

682
00:56:16,520 --> 00:56:24,200
this has gone to the much more efficient hot path. And I think that we'll certainly see analogs

683
00:56:24,200 --> 00:56:31,000
to this sort of behavior. Fair enough. We're at the top of the hour, we're beyond the top of the

684
00:56:31,000 --> 00:56:36,120
hour actually. And we haven't touched on the thing that I expected us to spend a bunch of time

685
00:56:36,120 --> 00:56:42,600
on, which is the Dota 2 project, but we covered a lot of really interesting ground in terms of

686
00:56:42,600 --> 00:56:47,560
AGI and what that means and what we should be thinking about. You know what I'm thinking we should

687
00:56:47,560 --> 00:56:55,720
do is maybe, you know, call this a part one and find some time to get together again to do part

688
00:56:55,720 --> 00:57:01,080
two where we dive into the work that you've done on Dota. Sounds good. All right. Perfect.

689
00:57:01,080 --> 00:57:08,520
Now this was a lot of fun. Really appreciate it. Yeah, same here. Greg, thank you so much.

690
00:57:11,160 --> 00:57:17,640
All right, everyone. That's our show for today. Thanks so much for listening and for your continued

691
00:57:17,640 --> 00:57:23,800
feedback and support. For more information on Greg or any of the topics covered in this episode,

692
00:57:23,800 --> 00:57:31,800
head on over to twimlai.com slash talk slash 74. To follow along with our open AI series,

693
00:57:31,800 --> 00:57:38,760
visit twimlai.com slash open AI. Of course, you can send along your feedback or questions via

694
00:57:38,760 --> 00:57:45,320
Twitter to add Twimlai or at Sam Charrington or leave a comment right on the show notes page.

695
00:57:46,440 --> 00:57:51,640
Thanks once again to Nvidia for their support of this series. To learn more about what they're

696
00:57:51,640 --> 00:58:00,120
doing at nips, visit twimlai.com slash Nvidia. And of course, thanks once again to you for listening

697
00:58:00,120 --> 00:58:30,040
and catch you next time.

