Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we're joined by Vladimir Bichovsky, engineering manager at Facebook to discuss Spiral,
a system they've developed for self-tuning high-performance infrastructure services at scale,
using real-time machine learning.
In our conversation, we explore the ins and outs of Spiral, including how the system works,
how it was developed, and how infrastructure teams at Facebook can use it to replace
hand-tuned parameters set using heuristics with services that automatically optimize themselves
in minutes rather than weeks.
We also discussed the challenges of implementing these kinds of systems, how to overcome
user skepticism, and how to achieve an appropriate level of explainability.
Now on to the show.
All right, everyone.
I am on the line with Vlad Bichovsky.
Vlad is an engineering manager with Facebook based out of the Boston office.
Vlad, welcome to this week in machine learning and AI.
Thank you.
Thank you very much.
It's very exciting to be here.
So today we're going to be talking about a project that you have worked on called Spiral,
but before we do that, I'd like to explore a little bit of your background.
You did your graduate work on Image Enhancement.
Tell us a little bit about that.
That's correct.
Actually, before that, I did a little bit of systems work as well, so I worked in wireless
networking and then transitioned into computational photography primarily because I really enjoyed
photography, and I wanted to learn more about it.
So for my thesis, I worked on automatic image enhancement, which is the magic button that
people have on their phones, or actually, the algorithm I developed is now in Adobe Photoshop.
If you dig deep into the menus, you could automatically set curves, and that's the algorithm
that I built during my thesis.
So yeah, and then after I finished my PhD, I joined Facebook.
Nice.
And what group are you in in Facebook?
The group is called Machine Learning Experience, and it's the goal of our group is to basically
deliver the benefits of machine learning for everybody.
Generally, machine learning is kind of thought of as kind of this elite field for only people
with incredibly strong backgrounds can contribute or even use, whereas what we're trying to do
is kind of democratize machine learning so that every engineer can Facebook can benefit
from it.
Okay, and the specific project that you're working on is one called spiral that your group
disclosed recently.
Tell us a little bit about that project.
Sure.
Yeah, this is actually, I think it's a very exciting project.
I'm glad to be part of it.
So I think I thought it for a while about how to describe what we do, just the people who
are not necessarily deep in computer science, and I think the best analogy is the following.
So if you think about, for example, like coffee drinking habits, so we all want to sleep
at night at the same time people do enjoy their coffee.
And if you have to make a decision, do I drink coffee at 3 p.m. or not, right?
You can do a bunch of experience where you drink coffee at 3 and then see if you can
fall asleep at night, right?
And based on the result, you kind of adjust what was the right call, like for example,
I drink my coffee, then I can't fall asleep, okay, I record the outcome.
And then the same thing next day, maybe I don't drink coffee after 3 p.m. and I fall asleep
fine, right?
And so if you kind of build up the data set and you kind of define a policy for yourself,
you learn a policy which is I shouldn't be drinking coffee after 3, right?
So the same kind of thing can be done for machines where you can literally in sort of
in this environment where everything around you is changing all the time, you could kind
of see the results of your actions.
And you could see in retrospect whether or not the actions the machine has taken were
good or bad and informed future decisions, does that make sense?
So that is a really, really good explanation and example.
And I think you may have just derailed the interview because the thing you just described,
I really think it needs to exist.
I've wanted the quantified self movement, I don't know if people even talk about this
anymore, quantified self, but I've wanted this thing that just like tracks your, you
can set up some metrics and track data points and then it applies machine learning to figure
out the correlation between your actions and these other experiences you have, whether
it's health or happiness or what have you.
It automatically uses this data that you collect to make predictions as opposed to what
quantified self really ever amounted to is providing like pretty pictures of the past,
like the rear view mirror as opposed to the dashboard.
And I can tell you more about why it's so important for Facebook or generally I think
any company of that scale or any company wants to scale if you're interested.
Well, so it sounds like you're not going to let me derail the interview by talking about
applying this to quantified self, which is good, which is good, but it sounds like a set
of problems that you can maybe loosely or roughly refer to as, I don't think there
is low level as infrastructure management, but you're kind of using them to manage the
way you configure.
You have not low level infrastructure like high level software and kind of the deployment
architecture of actually, I don't think deployment architecture is the right term either,
but the configuration of software components that are driving various Facebook applications.
Yeah, yeah, that's right.
So another way, with the way I call it internally, it's more formally, it's automatic, adaptive
policy, right?
So it's automatic policy learning where instead of usually people define policies like coffee
drinking policy or cash admission policy in terms of some sort of heuristic, right?
They sort of look at like, oh, well, let's look at arrival times, let's look at something
else and let's have a bunch of if else statements where we define the behavior.
And that's generally, it generally works pretty well.
There's nothing wrong with that approach in normal circumstances.
I think the biggest challenge for the list comes up when you actually start going sort
of at a much higher speed, specifically think about Facebook and this was published on
their engineering blog, while back, Facebook is releasing a new version of web source code
every hour.
So literally, the version of Facebook you see is different from an hour to hour.
So the code that runs it is changing and any system that depends on the code and makes
assumptions about this code can potentially be outdated within hours.
So if you think about sort of dub, dub, dub, or the web of Facebook being a dependency,
it's something you kind of think it should behave a certain way if you if your service depends
on it.
And then every hour, it's actually changing.
So whatever assumptions you made about how behaves may be completely invalid and not
just invalid at sort of some sort of slow rate, literally every hour, it could be different.
And so in that environment, it's very, very hard to sort of keep pace and update your code,
update your logic of your service to stay optimal.
So it's literally, this is not like some mechanism that we're just implemented because
it's cool and we wanted to do it.
It's literally a necessity to be able to run something at this scale and run it efficiently.
So I'll give you an example of over cash, for example.
If you have cash that's caching images, it could be movies, it could be anything else.
And let's say that in the past, people have uploaded mostly PNG files and we cashed them
and for PNG files, we may choose a certain limit.
If the file is PNG file and is less than certain size, let's cash it.
If it's too big, let's not cash it, vice versa, et cetera, they could sort of find a policy
by hand with their statements.
And then a little bit later, users are not uploading PGs anymore, they're mostly uploading
JPEGs.
Right?
The policy you have written is completely useless because you're not caching any JPEG files
and your system performs very poorly, right?
So somebody has to go in and manually recode this.
Somebody has to notice that metrics are out of auto-wack and has to recode it by hand,
which is not sustainable.
Right?
Giving her things, how fast things are changing.
And so what we do instead, something that spiraling can actually learn the optimal policy
on the fly as the changes are occurring, as the changes in load are happening.
So you're talking about learning the policy on the fly and that makes me think about techniques
like active learning.
Is that formally something that you've employed here or is it an adjacent area?
It's related.
If you say active learning is something we can do, it's something that we do in some
other contexts, currently we don't do active learning inside spiral, that's not really
necessary.
So active learning is more about choosing examples to train on, right?
So to be more specific, imagine if you're doing medical experiments, right?
And you're doing some sort of experiments using new drugs or something like that.
If you, and you're trying to figure out which combination of drugs works better, right?
And each experiment costs you a lot of money to run, like it's literally not free, right?
In this case, you can just say, okay, let me take a bunch of combinations and see which
one works out, what you want to do is you want to take all previous experiments into account
and figure out which combination to try next that gives you the most information, right?
To kind of gains you the most information.
In our case, it's, that's not the setup we have, but this applies in some other situations
such as automatic configuration of services.
So this is kind of looking a little bit forward, but something we're looking into sort of in
the same vein of self maintaining services or self optimizing services is self tuning fleets.
So if you think about a number of parameters that a given service has, it's actually very
large and it's actually often not clear how to set them, especially if the environment
is changing.
It could be numbers such as number threads or sizes of queues, et cetera, right?
And what is the optimal setting today, maybe different from the optimal setting tomorrow?
And so what you want to do is you could use some of the machines in your fleet for experimentation,
figure out which of them results in a better QPS, like the number of requests per second,
right?
And pick those parameters.
And ideally you do this continuously and so this is exactly where active learning is
very useful because you don't want to use lots of machines, sort of waste resources.
You want to focus on the next combination of parameters that's more likely to be better
than the current one.
Does that make sense?
Now, that does make sense and there's definitely this kind of sense or definition of active
learning that's focused on trying to identify the training data, for example, that increase
information gain so that you can reduce your overall costs of training.
But there's also this sense, or I could be just, you know, I could be confusing ideas,
but I also get the sense of active learning or think of it in the context of like incremental
learning, meaning you get a piece of, you've got a trained model.
And then you get a piece of data, you know, labeled data incrementally without going
through an entire retraining, you're using that to enhance your model.
Was there a better name for that than active learning?
Yeah, the better name for that is online learning.
Okay.
So it's kind of like streaming.
So if you think of streaming databases, so online learning is the form of kind of learning
that's streaming, right?
And that's actually exactly what we use for our system.
And the main benefit of it in practice is feedback loops.
So it's something actually machine learning researchers rarely think about, but as an engineer
trying to just let's say enable adaptive policy in my service, I don't really want to spend
time setting up a pipeline and finding out to more than I made some sort of mistake,
right?
Because it takes a while to do the training and then to shift the data and to get me back
the model.
It's, it's a very difficult feedback loop.
So if I'm debugging something, it's, it's just not really acceptable, right?
So if I'm training a huge neural network somewhere on the back end, it's, it's just difficult.
So literally, imagine if you were trying to write a program and every time you compile,
you have to wait a day, right?
Realistically, we would not get anywhere.
So, right.
So the online model is actually really helping this case because with, with our system,
one of the modes of operation is that when it's fully embedded and online, which means
you literally plug it in, there's, it's actually two call sites and you plug in the data
and you literally run your service.
And as soon as your service starts running and providing feedback, you can start running
and start seeing the results.
You can start seeing the results improve, but you could see, oh, maybe I'm missing an
important feature.
Maybe it's, I'm misclassifying something, immediately change it, run it again and you're
good to go.
You can see the results immediately.
So that's, that is exactly where online learning is very, very beneficial in practice,
right?
So in theory, people often talk about, oh, well, maybe you can't do quite as well online
as you can do offline.
Well, when you have all the data, something we're doing is we're kind of combining the
best of both worlds, online learning is used to enable really, really tight feedback loops
because it's, to me, this is probably the most important thing in engineering.
If you're building something, you want to know how well it's doing, right?
You want to know how well it's doing right now, not, not three days from now because
you may lose context, you'll forget what you're doing, what you're trying to do, et cetera,
et cetera.
And this is something our system enables.
When you're in the steady state, when you really figured out your features, you debunked
your system, everything is good, now it's more about, okay, and then get a better, higher
accuracy for my predictions.
Then you can switch the offline learning mode and sort of, because the system was already
debunked.
And then you just improve your accuracy by using potentially larger model and more models
and more complicated models.
Does that make sense?
The curse of me that I tend to think of the, that transition is being the opposite direction,
meaning that you start with an offline train model and you use online learning to keep
that model updated.
But what you've done is you've kind of flipped that and you're using offline or rather
online learning as a way to bootstrap very quickly and without requiring the model creators
to understand the environment as well, collect the data, do all the feature engineering
that they can do when they have that data and understand the environment.
And then use the information from the online learning models that you've created to create
incrementally better models by doing batch training.
Yeah, that's exactly right, yeah.
And so the challenge is always, so unlike traditional data sets, when you actually know what
your features are or have complete data set, when we're talking about system generalists
trying to optimize the system using machine learning, they don't necessarily know the features
ahead of time, right?
There's no data set.
They're still trying to figure out, okay, is file size important for my caching policy?
Maybe not, is the name of the data center from which this data comes in from?
Is that important?
Should I add that?
Should I not add that?
And in some respect, there is no, there's no, there's no data set before you start collecting
it, right?
It sort of assumes you say, oh, I should probably add file size, I should probably add
something.
Then then you create the data set, but it's really up to the engineer to plug it in, right?
It doesn't come prepackaged as a set of benchmark of like, okay, here's a bunch of images
and here's a bunch of labels, here's a cat and here's a dog, right?
It's sort of part of the problem is that people are trying to understand their problem and
trying to define it as they're solving it, right?
And this is actually one of the probably biggest challenges for us in sort of something I find
really, really exciting about this field is that we get to change how people think, literally.
So when engineers, traditional engineers come to us with problems, they sort of, they
always sort of think in terms of those fixed policies or sort of heuristics and picking
some sort of thresholds for things and when we start working with them, their mind opens
up and they realize that they can apply this whole slew of statistical methods to a problem
which they considered very, very rigid.
And so to me, that's just like one of those seeing those aha moments is really exciting.
And oftentimes what happens next is that people realize that there's a whole bunch of new
features that they could enable by switching to using a statistical approach from sort of
a rigid heuristic.
So there's kind of a bunch of really cool side benefits that come from learning more about
how to apply systems like that.
And so practically speaking, if I'm developing a system, say a caching system, one of the
first things I might do in my system is pull a bunch of configuration information from,
you know, environment variables or configuration files or what have you instead using spiral
I'm calling an API to give me the kind of current best version of those configuration
parameters.
Is that correct?
Yeah, almost.
So in some respect, there's two ways to think about it.
So if you think of a complete clean slate where your system puts up for the first time
and we've never, spiral never has seen any data, so something that, and then you ask
spiral for predictions for a given, for a given item that comes in, like do I cache
this, do I not cache this, the spiral will just return the default value that you assigned
because it has no information at this point.
But as soon as your system starts providing feedback, as soon as you say, oh, I should
have cache this item, oh, I should not have cache this item, and as soon as you start
putting this information in, the next prediction that you ask will be better.
And so all you need to do is just pull the information at the time when you know what
decision should have been made, right?
So when you're sort of on, for cache, it may be at eviction time when you're evicting
an item and nobody has ever requested their item, you could say, well, I probably shouldn't
have cache this, right?
If it's like it wasn't necessary, like nobody asked for it, and vice versa, if you see
an item that's been hit many, many times, you're like, okay, definitely next time I see something
like this, I should cache it.
Yeah.
I think you just pointed out an important distinction, and that is spiral is not predicting
for me the underlying configuration parameters that I might use to make a decision.
It's predicting the decision, meaning it's not predicting, I'm not using it to tell
me, you know, what queue length or, you know, how much JVM memory I might need or something
like that, it's operating at a higher level and it is managing all those parameters under
the covers.
Is that correct?
Indirectly, yes, it's managing the parameters of your policy, right?
So for example, if you had a threshold before, if I should not cache images that are larger
than 100 kilobytes, right?
So you kind of had this 100 kilobyte parameter, and then you're like, well, maybe it should
change it to 70 or 120, right?
So and then you look at the quality of decisions, say, oh, yeah, 120 is way better, looks
like I get a better cache hit rate or something, right?
So that's taken away.
So if you were trying to configure a policy, you no longer have to do that.
You just provide examples of correct decisions, and spiral, spiral learns the rest of it.
The tuning of parameters of a service is sort of a different project we're doing, which
is kind of moving, looking forward at the things we're interested in.
So we could tune the number of threads, but the setup is a little bit different than spiral.
So in that setup, what you would do is you would provide statistics of how well the services
operating as a whole with a set of parameters, and we would look at those statistics and
let's say the number of requests per second that the service is providing with a given
configuration.
And we would compare it to other experience we have ran, and then using active learning
would pick the next experiment.
Okay, sounds like the queue size of 100 looks better than 110, let's try 90, right?
And that all happens behind the covers, so under the covers and the engineer never has
to see it.
In the end, what they get is, oh, it looks like currently you get the highest number of
requests per second if you use the queue length of 35 and the number of threads 15, right?
And now you just set the two service and all of a sudden your utilization drops.
You consume much less energy and you're serving many more requests.
Got it.
Got it.
But that's a separate future project.
That's correct.
Yes.
So then with spiral, imagining that, and you alluded to this, that engineers might have
previously envisioned very simple policies for different aspects of their systems, for
example, you know, cash based on file size or cash based on file type.
And now by being able to specify these policies more declaratively, they, there's also a tendency
to make the policies richer.
Is that the case?
Yes, it's sort of a very easy way.
It's formalizing the problem, right?
So you use exactly the right language.
So it's, you kind of do more declarative programming than imperative programming.
You declare what it is that you want.
One interesting thing that happens when you start working with different engineer teams
that want to try sort of using automatic policies is that they realize that their problem
is not well defined, right?
They say, oh, we really want to just optimize this metric.
So this is our, that's what we want.
And then we, when we try to formalize the problem because they have to code up that declarative
solution to, okay, this is the right answer, this is what the right answer looks like,
they realize, oh, that, that conflicts with, with this other objective, no, that's not
what we want to do.
And so that's change in mindset that happens.
It's also beneficial in sense that people figure out problems that they've always had,
but previously just had some sort of hard-quoted solution that made some sort of implicit
trade-off, right?
That's sort of the trade-off was hidden and fixed and they just lived with it.
And switching to this higher level programming model effectively forces them to be more explicit
about the trade-offs they're making, understand the trade-offs they're making the system.
So again, I think this is one of the super exciting aspects of working in this field.
Can you walk through an example that illustrates that both the specific changes that they
need to make to implement these policies and to integrate with Spiral, but also where
these types of trade-offs have come into play?
So, I mean, it's difficult to do sort of an abstract sort of an example that would be
easy to discuss on sort of an already without a Blackboard.
But if you think about, again, something like cash admission policy, right?
You sort of want, as soon as you previously could just say, oh, well, I think we should
cash this type of files and if they're not bigger than this, right?
And just roll with it.
And then you look at some metric and see, oh, well, the cash rate, the cash hit rate looks
good.
All right, let's find, let's keep it as this, right?
So you don't know what the opportunity cost is, right?
So maybe you could have had a much better cash hit rate or maybe you're burning through
flash, right?
Let's say it's a bigger cash, not only use memory, but those use flash, right?
And so now you start looking at the rates of how much flash you're running through.
And you're like, oh, no, well, it looks like we're writing over flash a lot.
Let me go back and maybe tune my policy or do something about it, right?
And then that has an effect on your cash hit rate, right?
So if you're storing your, maybe saving few items, you have lower cash hit rate.
So and then maybe different engineers are working the two problems, right?
So you have this issue where one team is working to improve the cash hit rate and the
other team is trying to improve the rate of burnout of flash, right?
And sort of you have people who are adjusting different parts of the system, which indirectly
impact the other team without sort of realizing that explicit dependency.
I mean, this is a really simple example and most people would probably figure out that
like, okay, if I were changing this, we're going to affect the cash hit rate.
But this is, this should give you a flavor of the types of trade-offs.
Now if you have a single policy controlling the two, then, then you can set recall on
your classifier and sort of make the trade-off very explicit, okay, I'll get this much cash
rate, cash hit rate at this burnout rate, right?
And sort of then you can say, okay, well, this is the classifier I have.
And it looks like I can't have both, right?
I can't have really high cash hit rate with the current policy and low burn rate.
So then what you look at is, okay, maybe we can add additional features and sort of improve
both at the same time.
Does that make sense?
Is it sort of using adaptive policy tools like Spiral allows you to make this trade-offs
explicit and then sort of see them all in one place?
And so you started talking about starting with these policies and then looking at optimization
metrics and performance and features and feature engineering, that kind of thing.
Can you talk a little bit about the data science element of this or the modeling element
of this and specifically, is that abstracted away from the presumably, it's abstracted
away to some extent from the engineers that are using Spiral or are they involved in developing
statistical models for the specific policies that they want to implement?
So that's a great question in a sense that this is something we're still trying to figure
out kind of the best way to do.
So initially when we started building Spiral, our view was that we have a lot of domain
expertise, meaning that if people who are building caches and have built in caches for many
years decide to use adaptive policy, they actually already have all the domain expertise
we need, right?
So in some respect, they really know what matters or what doesn't.
In other words, if I have written a heuristic manually by hand that checks file sizes and
file types and compression types as something that's important to my policy, that is my
feature set, right?
And sort of I already have experience and I already know what features matter.
And that's partially what enables us to do what we do because the methods, as I said,
we use our online methods and sort of their meant to be high performance, right?
Because the cache is making lots and lots of decisions per second and you can't really
have very heavy models to make those decisions.
And so currently we partner with teams and help them do the data science.
But moving forward, we're trying to template those solutions and sort of every team can
do their own data science fairly easily with the tools that we built for them.
And then they would know how well their system would perform if they were to plug Spiral
in.
Does that make sense?
So in other words, we're transitioning from doing a lot of the data science ourselves
and sort of jointly with the team with a sort of high-touch environment into more of a
self-service environment where teams have an easy way to sort of plug in their data and
get the insights and make a decision how to use that data.
And this is sort of done through education, through documentation and through potentially
shifting some of the work we do to data scientists on those teams or affiliated data
scientists.
So that's our path forward to sort of scaling this, right?
Because we're not a very large team.
We don't have hundreds of people working on this, right?
So it's been noted to have impact on Facebook scale.
It's if we just were to meet with every potential customer, we would burn out of hours
in the day.
Sure.
Sure.
Can you talk through the data science process as it exists today and in particular elements
of the modeling that are unique to the way you've formulated this problem in system?
Sure.
So the most difficult data science, in this case, ends up being the part where we are
formally defining the problem, right?
So effectively, what we do is it's structured learning, right?
So it's really you need features and you need labels.
And part of the process to work with what teams to figure out, what are the features,
which is something they usually know, and then what are the labels?
And this is something I referred to earlier, which is people don't necessarily agree
on what the labels should be and which labels are right.
And that's sort of where we bring up some sort of contradictions in the objectives
they're trying to achieve.
So a lot of it is just talking through the problem and taking the main specific problems
sort of a systems problem and bring it into machine learning terms and sort of to sort
of figure out what is our data set, what are the right answers, what are the right labels?
Can one of the big requirements of spiralist ability to generate the labels automatically,
right?
Because in order for system to stay adaptive, you need to be able to continuously
feedback the new labels, right?
To sort of continue to adopt the environment, you need to fit in the new data set, the
fresh, the fresh correct answers.
So a lot of it is number one, figure out if that can be done.
And if it can be done, then the spiralist is not the right system for a particular use
case.
And if it can be done, then figure out what are those labels taking them and then just
applying the basic machine learning methods to see if we can achieve desired accuracy,
desired quality.
And are there specific methods or techniques that are used particular to the online learning
aspect of this?
So one of the very old and very kind of battle-tested methods is something we also use, it's
called multinomial-nave-base.
It comes from a family of nave-base methods and it's sort of very simple that effectively
is counting.
So if you have, let's say one place where this is also used is spam detection, right?
So that's actually what makes spam detection practical on desktop when you get an email
and it says it has words like Viagra and something else, something else, okay, that's spam,
right?
Because your other email, your good email probably doesn't have words like Viagra in
it, right?
That's probably not something you can verse about.
So if you count the number of times Viagra occurs in your email, it's very low in sort
of a good email and if you count the number of times Viagra occurs in bad email, that's
very high.
So that's effectively what multinomial-nave-base and generate nave-base methods do.
And the big benefit of those methods is that they're incredibly fast, right?
Because you literally just do counting.
There is no grading descent, there's none of that needs to be done and as soon as you
updated your counters, your model has updated and you're now ready to make predictions with
higher certainty or not a larger data set.
You say as soon as you've updated your counters, what are those counters represent?
Are those counters features from the input data or are they kind of internal state that's
keeping track of thresholds or things like that?
It's the internal state that keeps track of thresholds.
Literally, like one counter is sort of the prior counter, empirical prior, which is how
many good emails have you gotten versus bad emails, right?
So if you just count that and if 99% of your emails are good and if you ask me to predict
that for a new email, is that likely to be good or bad?
Based on that single counter, I'll say, well, it's most like the good because really most
of the emails you get are good without even looking at the content, right?
And on top of that, I can look at word occurrences in the email and which words occur in that.
And if that's, if that email says hello Sam and it says sort of how are you, let's get
together this weekend.
That's one set of words that are being hit right and like weekend and hello and everything
else.
Those are, all those counts are usually high for good emails.
And then if it says, you know, Pills, Viagra, something, something, something else, those
counts are very low when you're regular email, right?
So does that, does that make sense?
You're basically conditionals in your feature space.
That's exactly right.
So you're keeping kind of class conditional probabilities, you're keeping those scores
and you're applying them to as soon as soon as you updated the counters, you have new conditional
probabilities and you could use them to classify the next item.
We've talked about the caching example multiple times.
You've mentioned spam a couple of times.
Is this used for content filtering types of applications at Facebook or is that more
an example?
That's definitely an example.
Just anytime you see multi-nevenly based, like if you go into Wikipedia, they'll literally
give you a spam as an example of how this method works.
Are there other areas beyond the cache example that this is being used at?
Yeah.
So one other area.
I mean, this literally can be used in any area where the examples can be generated automatically.
Caches just one of them in a sense, like you, you know and retrospect which items you
should or should not have cashed.
Anytime you can retrospectively check a decision, the system applies and one other specific
example is something like retribility.
So imagine that you were many, so many companies run batch jobs, right?
So Facebook has a lot of large system that runs batch jobs.
And batch jobs often can fail for various reasons, right?
Sometimes it could be there's a syntax error that I checked in bad code because I'm a
irresponsible developer, right?
And there's syntax error in it and after four hours of computing something, it hits my
incorrect statement and breaks, right?
Literally the job fails.
The results have not been delivered.
Or there may be instructions failure.
Let's say there's network connectivity issues with a particular data center and the
job is trying to get the data and it fails because it couldn't get the data, right?
And maybe those does this be very quickly.
So in the first case, you actually don't want to retry the job, right?
But particularly because it could be very expensive to rerun it, right?
So you sort of you run for a bunch of phases, you run it for hours, spend on hundreds of
computers and then you fail because there is a syntax error.
Well, if I and you're going to run it again the next time.
Exactly.
Exactly.
So I'm going to fail again next time.
I can retry it three times or four times or five times.
I will still fail and I will waste resources.
On the other hand, if the job is, if the failure was intermittent, you actually do want to
retry, right?
And this is exactly the same setup as with the cache because retroactively, you know what
the right answer answer should have been, right?
And certain types of jobs, no matter how many times you retry them, they fail.
Other jobs succeed on retries.
So learning to classify which, what to retry and what not to retry is again a type of scenario
where we use spiral.
So features here might be job, return codes and infrastructure metrics and things like
that.
Yeah, this feature features in this example that are literally logs, right?
So to return codes, press the log that this job produced.
So it's kind of just literally the text that was out, the last, at last, say, 100 lines
of text that were produced and that's fed into the classifier, along with the kind of
number of retries that happened, meaning, okay, this message was produced and we retried
it three times and nothing good happened, right?
We've never succeeded.
So next time we get the similar message, well, that means we probably shouldn't retry it.
So you've got these, these logs, I'm imagining a typical log is, you know, has some significant
depth to it.
It's not kind of abstracted down to a particular, you know, word or error code or something,
but it's in there.
How are you narrowing in on the, you know, the specific signal within a big log file?
Well, then it's down through the magic of machine learning.
I guess in the context of, of this, this online learning where you're, you're basically
doing counting, right, you've, you've got to tell it what to count, right?
You're not, yeah, I'm not, I'm not hearing that you're doing, you know, deep learning
or something like that and you're training a model to figure out what to count.
Well, we are, we are trying to, so there's, I mean, it's fundamental to deep learning
or any kind of other learning, they're all similar, right?
They're all, there is a, there is a theorem called no free lunch theorem.
You can look it up on Wikipedia, but it, it's fairly, it's fairly complicated, but in
a nutshell, it says that no classifier is superior to another classifier.
In other words, if, if I can, with the right set of transformations on the input data,
I can get results that are as good with a simple classifier as I would on raw data with
a more sophisticated classifier, does that make sense?
Sure.
Yeah.
Effectively, if I pre-process the logs using some sort of vectorization, using something
else and then plug them into a very simple classifier, I'll get the results that are almost
as good or, or as good as sort of a sophisticated neural, natural language processing method
with, you know, recurring neural networks or something like that.
Right.
Well, that's what I was asking about.
Even processing the logs and vectorizing them or creating an embedding or something like
that.
That's correct.
Exactly.
Yeah, that's what I was getting at.
Okay.
Cool.
Interesting.
Interesting.
And so, you know, one of my initial thoughts and I alluded to this earlier in the discussion
was, you know, kind of broad implications of this kind of technique in infrastructure
management, for example, Google has kind of famously talked about some of their results
in applying machine learning to managing, you know, HVAC systems within the data center.
It seems like this kind of approach could have similar applicability.
Correct.
Yes.
And that's what we're trying to do as well.
I mean, if you weren't going to a large company, so a company that with as much infrastructure
as Facebook or Google, at some point you want to take people out of the loop, right?
And that's exactly what we're doing with our methods.
We're trying to make sure people aren't doing kind of routine maintenance or mundane
work and are actually focused on creative tasks by automating the routine work and sort
of maintenance work.
And so, have you done anything to try to apply this to kind of management of physical
infrastructure?
Our team in particular hasn't, but I don't know if other teams have.
There's many other teams that are doing very interesting things.
So we haven't, but I can't speak for all of Facebook.
What were some of the main things that your team has learned in building the system and
getting it in the hands of some users?
I think the main thing that we've learned is that it's, you do have to change the mindset
of people before they're comfortable using our system because initially when they start
using it, the sort of just see it as magic, it's like, oh, this magical thing just makes
decisions.
It's like, why did it make that decision?
And sort of, there's a price for an activity.
There's a price for optimality, right?
In this case, it's maybe somewhat reduced transparency.
So if you think about the example I gave earlier, which is a sort of an EFELS tree of statements
for you, where your policies encoded, you could look at any decision and you can trace
it back through that tree and say, well, that image was not cashed because it was just
over the file size limit, right?
As soon as you start using statistical methods, that observability goes away, right?
It's not, there were changes.
It requires you to think in different terms, right?
It would, it requires you to think in terms of larger data set and sort of statistical
averages, not EFELS statements.
I think that's the biggest challenge for us.
It's sort of getting people to let go of control and trusting a machine learning system.
And it's very much on us to prove to them that it's actually better than what they had
before.
We've been lucky in a sense that we've enabled our customers to do things they didn't
think were possible.
And so they, they're kind of embraced us and they said, this is great.
This really saves me many hours in a day.
And now we can do this thing that I didn't think we could.
So that helps.
But generally, when you start talking to people, they're a little bit skeptical like, well,
is this going to do the right thing?
How am I going to debug it?
Right.
So that's kind of our next challenge in terms of scaling this to sort of literally every
engineer at Facebook and potentially after that, you know, every engineer in the world
who wants to benefit from this.
Have you started exploring approaches to creating some degree of observability, explainability,
that kind of thing?
Yes.
And so that's a kind of an ongoing process.
Traditionally, so this is something I actually learned during my, during the time of work
to my dissertation, methods such as nearest neighbor methods tend to be easier to explain
because whatever person says, why was this decision made, you could provide examples and
say, well, because this example looks very similar to those two examples, which you said
were positive.
That's why this example is positive.
Does that make sense?
It's because nearest neighbors, family methods, they work by literally through examples.
Like, here's a prototype examples.
Now, tell me what this new, never seen before example.
What is the label for this example?
We can literally use that as an interface and say, well, it's because you provided us
with this data.
And so that's the direction we're kind of thinking about and sort of that's those methods we
want to try to see if we can exploit and sort of run effectively online.
Awesome.
Well, Vlad, thanks so much for taking the time to chat with us about this.
This is a really interesting project.
Thank you very much.
I'm, thank you for having me on the show.
Awesome.
Take care.
Bye-bye.
All right, everyone.
That's our show for today for more information on Vlad or any of the topics covered in the
show, visit twomla.com slash talk slash 221.
As always, thanks so much for listening and catch you next time.
