WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.840
I'm your host Sam Charrington.

00:32.840 --> 00:39.000
I'd like to start out by thanking Siemens for sponsoring today's show.

00:39.000 --> 00:43.720
You probably know Siemens as a global leader in the area of industrial electrification,

00:43.720 --> 00:49.920
automation and digitalization, or as a supplier of systems for power generation and transmission,

00:49.920 --> 00:52.520
as well as medical diagnosis.

00:52.520 --> 00:56.720
I recently had an opportunity to attend the company's spotlight on innovation event down

00:56.720 --> 01:00.460
in Orlando, Florida and learn about some of the interesting work happening within the

01:00.460 --> 01:02.560
R&D branch of the company.

01:02.560 --> 01:06.520
I'm happy to be able to share a bit of that with you via today's conversation with Batu,

01:06.520 --> 01:09.320
a computer-visant research manager there.

01:09.320 --> 01:15.320
To find out more about what Siemens is up to in areas like AI, digital twin, cybersecurity,

01:15.320 --> 01:25.320
smart city infrastructure and more, visit twimbleai.com slash Siemens.

01:25.320 --> 01:28.440
Alright everyone, I've got Batu, our soy on the show.

01:28.440 --> 01:34.280
Batu is a research manager for vision technologies and solutions at Siemens Corporate Technology.

01:34.280 --> 01:37.440
Batu, welcome to this week of machine learning and AI.

01:37.440 --> 01:39.720
Sure, it's great to be here.

01:39.720 --> 01:44.440
It's great to have you on the show and I'm looking forward to this conversation.

01:44.440 --> 01:49.360
How do we start with having you share a little bit about your background and how you came

01:49.360 --> 01:53.280
to work in computer vision and machine learning?

01:53.280 --> 01:54.280
Sure.

01:54.280 --> 02:00.560
So, actually I started my education and career in the field of electrical and computer

02:00.560 --> 02:05.480
engineering with the focus on computer vision topics in Turkey.

02:05.480 --> 02:12.360
And after finishing up my Bachelor of Science there, I joined Siemens and started working

02:12.360 --> 02:17.520
on compensation geometric problems such as 3D reconstruction from point class and geometrical

02:17.520 --> 02:18.920
shape matching.

02:18.920 --> 02:25.320
And at that point deep learning was taking off every day and at that point we were mostly

02:25.320 --> 02:30.120
using the traditional machine learning algorithms and that's how my interest started.

02:30.120 --> 02:34.880
At that time we're always using okay, can we extract some hand carved features in order

02:34.880 --> 02:39.200
to represent the geometries, but then we realized that there might be a better way of doing

02:39.200 --> 02:41.920
this and that's how I started this machine learning journey.

02:41.920 --> 02:47.560
So, after I received my PhD in computational geometry at CMU, I joined Siemens corporate

02:47.560 --> 02:52.520
technology and I started integrating artificial intelligence and computational geometry together.

02:52.520 --> 02:56.720
And right now I'm technology manager for vision, technologies and solutions team in Siemens

02:56.720 --> 03:02.520
corporate technology which is actually R&D hub for Siemens and we are providing high quality

03:02.520 --> 03:07.400
research development and consulting services for our business units within the Siemens.

03:07.400 --> 03:14.000
Okay, you mentioned that a big part of your focus is kind of merging, did I interpret

03:14.000 --> 03:19.120
this right kind of merging the traditional computational geometry approaches and deep

03:19.120 --> 03:20.120
learning?

03:20.120 --> 03:23.480
Is that how you're approaching the various problems you're working on?

03:23.480 --> 03:29.920
That's correct, that's one facade of our approach, but basically our team is known

03:29.920 --> 03:33.360
as the computer vision with limited data problem.

03:33.360 --> 03:39.680
Okay, so our team is working on how to solve computer vision problems by leveraging some

03:39.680 --> 03:46.200
simulation methods to generate synthetic data or by developing intelligent AI algorithms

03:46.200 --> 03:50.960
to eliminate the need of large amount of training data set collection.

03:50.960 --> 03:55.640
Because you might be seeing in most of the domains that this is one of the obstacles

03:55.640 --> 03:58.760
to actually deploy AI systems in industry.

03:58.760 --> 04:04.480
We have to collect a large amount of training data sets for a specific custom task for

04:04.480 --> 04:09.520
your interest, but at the same time after collecting this data, you also have to annotate

04:09.520 --> 04:10.840
this data.

04:10.840 --> 04:15.840
So we are working closely with academia and researchers within Siemens to tackle this

04:15.840 --> 04:21.680
problem from multiple angles, how to solve learning or computer vision with limited

04:21.680 --> 04:22.680
data problem.

04:22.680 --> 04:29.720
Is your use of simulation primarily focused on synthetic data or are you using simulation

04:29.720 --> 04:31.760
in other ways as well?

04:31.760 --> 04:34.800
So we use simulation in two different ways.

04:34.800 --> 04:40.400
The first way of we use the simulation is actually to generate synthetic data.

04:40.400 --> 04:46.640
And one great example use case that we had all have developed in the past is about spare

04:46.640 --> 04:48.440
part recognition.

04:48.440 --> 04:53.400
This is a problem if you have a mechanical object that you deployed in the field and

04:53.400 --> 04:59.120
you need to perform maintenance and service operations on this mechanical functional object

04:59.120 --> 05:00.120
over time.

05:00.120 --> 05:06.400
In order to solve this problem, what we are working on is using simulation to synthetically

05:06.400 --> 05:13.880
generate a training data set for object recognition for large amount of entities.

05:13.880 --> 05:19.840
In other words, we synthetically generate images as if these images are collected in real

05:19.840 --> 05:23.800
word from an expert and they are annotated from an expert.

05:23.800 --> 05:28.240
And this actually comes for free using the simulation.

05:28.240 --> 05:33.600
We have developed a system together with our Siemens colleagues where we deployed this

05:33.600 --> 05:37.440
for the maintenance applications of the trains.

05:37.440 --> 05:41.400
And the main goal is a service engineer goes to the field.

05:41.400 --> 05:46.320
He takes his tablet, he takes a picture, then he draws a rectangle box.

05:46.320 --> 05:50.480
And the system automatically identifies what is the object of interest that the service

05:50.480 --> 05:52.640
engineer would like to replace.

05:52.640 --> 05:57.520
And in order to make the system reliable, we have to take into consideration different

05:57.520 --> 06:04.600
lightning conditions, texture, colors, or whatever these parts can look like in a real world

06:04.600 --> 06:05.600
environment.

06:05.600 --> 06:13.600
Okay, so you've got some ground truth data, which is the parts itself, you know, from,

06:13.600 --> 06:18.960
you know, these could be imagining you have them both from a, like, maybe catalog image

06:18.960 --> 06:24.640
as perspective, but for some of them, you probably have down to a CAD description of these

06:24.640 --> 06:25.640
parts.

06:25.640 --> 06:32.320
And you want the service engineer to be able to take a picture of the parts in place,

06:32.320 --> 06:36.000
not having, you know, taking the part out and putting it on a white background or something

06:36.000 --> 06:40.760
like that, but just in place and then draw a bounding box of some sort around it.

06:40.760 --> 06:46.200
And your system should be able to identify what the part in question is.

06:46.200 --> 06:47.360
That's exactly the case.

06:47.360 --> 06:49.760
So we are interested for the built-in case.

06:49.760 --> 06:55.040
So you like to eliminate the need of this assembling and mechanical object in order to identify

06:55.040 --> 06:59.960
the object of interest, because sometimes people might have already engraved some QR codes

06:59.960 --> 07:00.960
on these parts.

07:00.960 --> 07:06.800
However, we are targeting the application scenario where this object will take so much time

07:06.800 --> 07:13.440
to disassemble or the QR code is not visible or due to some customer preferences, we cannot

07:13.440 --> 07:16.960
print any QR code and ID tags on this part.

07:16.960 --> 07:21.600
Our system is designed to be able to work on such scenarios.

07:21.600 --> 07:29.360
I can see how generating synthetic data, particularly your kind of typical data augmentation

07:29.360 --> 07:35.920
types of synthetic data where you're doing shifts and changing the lighting conditions

07:35.920 --> 07:39.960
and things like that would help a model solve this problem.

07:39.960 --> 07:48.240
But you also have, I would imagine the issue of kind of occlusion of your part is a big

07:48.240 --> 07:50.040
challenge to overcome.

07:50.040 --> 07:55.080
Is that dealt with via synthetic data or other methods?

07:55.080 --> 07:56.920
So that's a great question actually.

07:56.920 --> 08:01.720
Yes, so that was actually the first step that we took was data augmentation methods to

08:01.720 --> 08:07.480
be able to actually use some previous catalog images or CAD models to generate synthetic

08:07.480 --> 08:08.480
images.

08:08.480 --> 08:13.120
However, the real challenge is modeling also the sensor structure.

08:13.120 --> 08:17.960
So what are these sensor noise that you are going to get when you are in the field?

08:17.960 --> 08:22.720
Because if you synthetically generate images, you don't consider the noise that comes from

08:22.720 --> 08:26.880
your acquisition device or you don't consider the distortion that comes from your acquisition

08:26.880 --> 08:30.560
device or some other electromagnetic field effect.

08:30.560 --> 08:37.120
And this is where we published a paper in 2016 that's called depth synth and where we

08:37.120 --> 08:44.200
modeled with very high detail how a depth sensor works, such that are simulator generates

08:44.200 --> 08:48.680
very high detailed depth images from the CAD model renderings.

08:48.680 --> 08:51.240
So that's what it makes it really successful.

08:51.240 --> 08:57.280
We are not only generating or augmenting data by creating different scales, different

08:57.280 --> 09:03.600
rotations, but even we take into account the parameters that affects the image acquisition.

09:03.600 --> 09:09.640
And that way we were able to achieve success to deploy such a system in real world applications.

09:09.640 --> 09:17.480
And so does the depth characteristic address the occlusion of your part by other parts

09:17.480 --> 09:18.480
in assembly?

09:18.480 --> 09:23.280
Or how do those two factors tie together?

09:23.280 --> 09:24.280
Sure.

09:24.280 --> 09:25.280
Sure.

09:25.280 --> 09:30.920
Actually, when we synthetically render these images, we render the entire assembly so that

09:30.920 --> 09:37.920
even though we see the objects itself, we synthetically simulate as if there are occlusions

09:37.920 --> 09:39.720
from the neighboring parts.

09:39.720 --> 09:43.600
So our synthetic database does not only consist of clean images.

09:43.600 --> 09:49.160
We also take into account all these cases where you synthetically simulate this corner

09:49.160 --> 09:50.480
cases as well.

09:50.480 --> 09:54.800
So this is all baked into the simulation platform that we have.

09:54.800 --> 10:00.960
I'm curious with depth synth paper, what the general approach was.

10:00.960 --> 10:07.080
How did you incorporate the characteristics of the sensor into the training of the models

10:07.080 --> 10:09.080
to get better results?

10:09.080 --> 10:10.080
Sure.

10:10.080 --> 10:16.280
For example, in our paper, we talk about the representation of the depth synth pipeline

10:16.280 --> 10:22.040
which consists of the projector modeling, camera modeling, object modeling, and reconstruction

10:22.040 --> 10:24.080
and post-processing models.

10:24.080 --> 10:29.800
And each of these models takes as input a cat model and synthetically generates a rendering

10:29.800 --> 10:33.120
of a depth image corresponding to this cat model.

10:33.120 --> 10:38.480
And during the projector modeling, for example, we simulate the patterns motion between exposures,

10:38.480 --> 10:40.120
vector and lens effects.

10:40.120 --> 10:46.760
In the camera modeling, we take into account distortion, motion blur and other noises that

10:46.760 --> 10:48.320
are coming from the environment.

10:48.320 --> 10:52.360
In the object modeling, we take into account motion control, illumination, material properties

10:52.360 --> 10:56.360
and also even the surface microgeometer modeling properties.

10:56.360 --> 11:00.880
And at the end of the day, we even apply how this geometry or how this image will look

11:00.880 --> 11:06.600
like if our device is applying some post-processing steps like the smoothing or hole filling.

11:06.600 --> 11:10.480
And that's how we actually get depth images that looks really realistic.

11:10.480 --> 11:18.400
And in our paper, we show the comparison our simulation results with the real data acquisition.

11:18.400 --> 11:24.360
And we even simulate what if somebody is using a hand-held device and he walks within

11:24.360 --> 11:29.920
the building, we take into account the vibration that comes from walking within the environment

11:29.920 --> 11:33.120
or the motion between the exposures as well.

11:33.120 --> 11:36.040
And that's make it really strong.

11:36.040 --> 11:42.920
And so are the modules that you described, are these modules steps in a pipeline, or are

11:42.920 --> 11:48.400
they more like layers in a deep learning model?

11:48.400 --> 11:53.320
So for this one, this is not based on the deep learning model.

11:53.320 --> 11:57.200
But right now, this is still into a pipeline.

11:57.200 --> 12:03.320
But during that time, we actually modeled this mathematically, these behaviors.

12:03.320 --> 12:04.320
Cool.

12:04.320 --> 12:08.840
We dove right into a pretty detailed use case.

12:08.840 --> 12:15.400
But maybe before we talk about more use cases, I can have you take a step back and talk

12:15.400 --> 12:20.600
a little bit more broadly about what your group at Siemens does.

12:20.600 --> 12:21.600
Sure.

12:21.600 --> 12:22.600
Sure.

12:22.600 --> 12:28.920
In our group, actually, every day, we have three different type of interactions or activities.

12:28.920 --> 12:33.640
And in our group, as I mentioned before, we are targeting the computer vision with limited

12:33.640 --> 12:34.640
data problem.

12:34.640 --> 12:40.400
However, what are the different venues where we actually execute this work?

12:40.400 --> 12:45.160
And this can be grouped in under the three different project categories or activities.

12:45.160 --> 12:50.720
Our first activity involves day-to-day interaction with our business units, Siemens business units

12:50.720 --> 12:56.240
to understand their needs and try to identify which computer vision research topics can make

12:56.240 --> 12:58.960
the largest business impact for them.

12:58.960 --> 13:05.120
After identifying such opportunities, we formulate proof-of-concept projects with them to improve

13:05.120 --> 13:06.560
their competitive advantage.

13:06.560 --> 13:11.520
But this is one type of the interaction that we have within the company for our scientists.

13:11.520 --> 13:16.960
The second type of interaction that also excites many of our scientists here is our interaction

13:16.960 --> 13:19.360
for more fundamental research topics.

13:19.360 --> 13:25.000
We are very active and engaged with subsidies and grants in USA with screen potential

13:25.000 --> 13:30.520
call for proposal issued by U.S. government entities that align with our long-term vision.

13:30.520 --> 13:34.280
And if there is anything relevant for us, we collaborate with multiple universities and

13:34.280 --> 13:37.720
Dinosaur partners to drive wide-papers and full proposals.

13:37.720 --> 13:43.400
And a great example of such activity was, right now, we are one of the performance within

13:43.400 --> 13:48.680
DARPA Physics of AI Project, together with Cornell University.

13:48.680 --> 13:55.560
And we are also part of the DARPA Automatic Scientology Extraction Project program as well.

13:55.560 --> 13:58.760
This actually concludes our second type of activities.

13:58.760 --> 14:04.800
And the third type of activity is more about how can our researchers and scientists work

14:04.800 --> 14:10.840
on the topics that are going to create a business impact in the next two, three years for Siemens?

14:10.840 --> 14:15.120
For these topics, our teams are collaborating with universities and other industrial partners

14:15.120 --> 14:19.000
to innovate and publish research results in top conferences.

14:19.000 --> 14:24.800
For example, the last year our team had three CVPR-1 ECCV publication and presentation.

14:24.800 --> 14:30.040
And this year, we had one CVPR publication and we also organized a computer vision with

14:30.040 --> 14:36.080
by a state of workshop led by our principal scientist Dr. Jan Ernst and with our team member

14:36.080 --> 14:40.880
Dr. Kondron Pank in Long Beach, California last week.

14:40.880 --> 14:41.880
Okay.

14:41.880 --> 14:45.720
What was your CVPR paper this time?

14:45.720 --> 14:46.720
Sure.

14:46.720 --> 14:51.160
This time, it was actually on a very, very interesting topic and what we called learning

14:51.160 --> 14:53.160
without memorizing.

14:53.160 --> 15:00.440
So this project is also an important, incremental learning is an important task aimed at

15:00.440 --> 15:06.680
increasing the capability of a trained model in terms of the number of classes recognizable

15:06.680 --> 15:08.160
by the model.

15:08.160 --> 15:13.760
So what it means is the key problem is the requirement of storing the training data associated

15:13.760 --> 15:19.360
with existing classes while teaching the classifier to learn new classes.

15:19.360 --> 15:24.440
And this is a work jointly done by Rajat Vikram Singh and Kondron Pank.

15:24.440 --> 15:31.560
And what they actually invented is they find that they find that a way of emulating the

15:31.560 --> 15:37.440
relationship between a student and the teacher, how it works in real world.

15:37.440 --> 15:40.080
And they apply that to artificial intelligence.

15:40.080 --> 15:47.040
So the way the system works is there is one teacher model which tries to explain what it

15:47.040 --> 15:52.400
learned so far, but it doesn't want the student to just learn everything by heart.

15:52.400 --> 15:57.600
It wants the student to understand why he makes this decisions.

15:57.600 --> 16:03.520
And that way, if you actually keep able of teaching the neural network, how a neural network

16:03.520 --> 16:08.880
makes decision, then you can actually pass the inherited information from the previous

16:08.880 --> 16:14.120
experiences we do not require access to the previous data sets.

16:14.120 --> 16:19.760
Are there maybe correlations between the student teacher type of an approach and something

16:19.760 --> 16:28.480
like an auto encoder where you're kind of trying to map what the primary model learns

16:28.480 --> 16:34.800
into some lower dimensional space and then kind of build that back out?

16:34.800 --> 16:41.360
Yes, it's similar because actually this work is built upon one of our previous publications

16:41.360 --> 16:47.080
which was trying to do attention modeling and attention modeling is actually trying to

16:47.080 --> 16:53.960
understand where does our neural network pay most of the attention when it makes a decision.

16:53.960 --> 16:59.880
And we are capable of highlighting, oh, okay, these are the set of the pixels that contributes

16:59.880 --> 17:01.480
to the most of the decision.

17:01.480 --> 17:06.920
And this is the paper that we published last year, it's called the Tanme Ver to look.

17:06.920 --> 17:15.360
And in this paper, by leveraging this powerful explanation mechanism, we are able to encapsulate

17:15.360 --> 17:21.840
knowledge that's very unique from the training data set in a lower dimensional representation

17:21.840 --> 17:23.640
step.

17:23.640 --> 17:27.080
And that's how it works actually, that's why it's so relevant to the auto encoder what

17:27.080 --> 17:32.040
we are using another mechanism to actually encapsulate the knowledge coming from the training

17:32.040 --> 17:33.040
data sets.

17:33.040 --> 17:34.040
Okay.

17:34.040 --> 17:44.200
And so how does that mechanism work in the learning without memorizing paper?

17:44.200 --> 17:45.200
Sure.

17:45.200 --> 17:51.560
The way this work works is actually, first of all, what's the real use case?

17:51.560 --> 17:57.440
The real use case is, you might be familiar with all these edge devices where we are capable

17:57.440 --> 18:01.280
of performing real-time computer vision tasks on them.

18:01.280 --> 18:07.800
And if you'd like to improve your model, for example, you'd like to train or teach this

18:07.800 --> 18:13.480
model classification or recognition of the new objects, you don't want to retrain your

18:13.480 --> 18:20.000
system using the previous training data set first, it takes too much time again.

18:20.000 --> 18:25.400
And you might be not have a connectivity or you might not have access to this data.

18:25.400 --> 18:30.320
And there's also memory limitations on your edge device, which prevents you to actually

18:30.320 --> 18:35.080
store the data on your device for further training purposes.

18:35.080 --> 18:42.040
So the main idea is, can we encapsulate the knowledge that was already inherited from

18:42.040 --> 18:47.040
a previous training stage and pass it to the next stage of the training?

18:47.040 --> 18:53.240
And the way the algorithm works is, it tries to memorize, actually we call it learning

18:53.240 --> 19:00.960
without memorization, but it tries to always consistently pay attention to the same regions

19:00.960 --> 19:04.280
of the image pixels to make the decision.

19:04.280 --> 19:11.360
So it doesn't allow its attention mechanism to divert too much by looking at the new observed

19:11.360 --> 19:12.360
data.

19:12.360 --> 19:18.880
So that way by keeping our attention consistent, we are preventing our attention to be heard

19:18.880 --> 19:20.480
or disturbed.

19:20.480 --> 19:26.120
And by preventing that, we are capable of actually maintaining the data as much as possible.

19:26.120 --> 19:31.760
Of course, this is something at research stage and we are working on it further for more

19:31.760 --> 19:34.360
industrial use cases.

19:34.360 --> 19:42.320
Going back to the, tell me where to look paper, what's the key innovation there?

19:42.320 --> 19:47.560
So the key innovation there was, it was a very interesting paper because, I don't know

19:47.560 --> 19:51.440
if you are familiar with the attention modeling frameworks, but what they are trying to do

19:51.440 --> 19:57.240
is, the famous example is you'd like to recognize the ship.

19:57.240 --> 20:01.160
And in your training dataset, you always have the ships that are on the sea.

20:01.160 --> 20:05.360
Then you'd like to see, okay, let me show me the, what are the pixels that my neural network

20:05.360 --> 20:10.000
is paying attention and you realize that all my neural network is paying attention to

20:10.000 --> 20:14.200
the sea because it's actually learning the patterns of the sea instead of the geometrical

20:14.200 --> 20:15.840
features of a ship.

20:15.840 --> 20:23.480
So existing algorithms until our development, the attention models were not extracting

20:23.480 --> 20:30.760
the boundaries of the object of interest, they were more noisy compared to our recent

20:30.760 --> 20:39.200
development, but we did that work was we demonstrated that we can actually mask or hide some

20:39.200 --> 20:43.720
pixels of the objects or some pixels of the image.

20:43.720 --> 20:48.160
And then we sent back this information, this mask or altered image back to the class

20:48.160 --> 20:53.160
fire until it's not capable of recognizing it any of the correct classes.

20:53.160 --> 20:58.120
That way in an iterative fashion, we remember more pixels from the images until our class

20:58.120 --> 21:01.840
fire starts failing to making the right decision.

21:01.840 --> 21:07.640
And at that moment said, okay, we are successfully recovered all the set of pixels, which impacts

21:07.640 --> 21:11.880
the decision of making, this is a cat, this is a ship, or etc.

21:11.880 --> 21:14.840
This is the one branch of our rock.

21:14.840 --> 21:21.240
The second branch of the box, okay, the masking is done as part of the process, it's

21:21.240 --> 21:26.600
not like a labeling type of a step, is that correct?

21:26.600 --> 21:27.600
That's correct.

21:27.600 --> 21:35.960
It was completely trained and where the system optimizes jointly trying to identify where

21:35.960 --> 21:38.240
to pay attention pixels.

21:38.240 --> 21:43.520
And at the same time, I removed some pixels to see if it needs to grow these pixels over

21:43.520 --> 21:44.520
time.

21:44.520 --> 21:48.880
And this way, it actually corrects and improves its attention mechanism, and we realized

21:48.880 --> 21:52.880
that we were able to get much more accurate boundaries of the attention.

21:52.880 --> 21:58.000
And they were much more clustered together, such that they are more compact representation

21:58.000 --> 21:59.000
of the knowledge.

21:59.000 --> 22:02.000
And it also provides explainability to aspect.

22:02.000 --> 22:10.360
And I think at least some folks in our audience will be familiar with the line paper, which

22:10.360 --> 22:15.600
is Carlos Gastron and his folks from the University of Washington.

22:15.600 --> 22:21.320
And they do something similar where they kind of perturb aspects of the inputs to determine

22:21.320 --> 22:28.120
which are the pixels that are most relevant to the classifier decision.

22:28.120 --> 22:33.120
And you relate this work to that?

22:33.120 --> 22:34.120
Sure.

22:34.120 --> 22:39.960
From a higher level, we are trying to solve a similar problem.

22:39.960 --> 22:44.240
But I don't know the exact details of that paper, so that's why I won't be comfortable

22:44.240 --> 22:46.720
to directly compare them to here.

22:46.720 --> 22:47.720
Cool.

22:47.720 --> 22:54.160
And then you mentioned a computer vision with bias data workshop that you did at CVPR this

22:54.160 --> 22:55.160
year.

22:55.160 --> 22:57.120
Can you talk a little bit about that?

22:57.120 --> 22:58.120
Yes.

22:58.120 --> 23:05.120
And actually, this was the second series of this workshop organized by our principal scientist

23:05.120 --> 23:06.120
Jan Ernst.

23:06.120 --> 23:12.120
And actually, the main objective of this workshop was bringing together people from industry

23:12.120 --> 23:13.120
academia.

23:13.120 --> 23:21.720
And also, students as well to identify what are the big problems right now in the computer

23:21.720 --> 23:24.400
vision with limited and biased data.

23:24.400 --> 23:30.920
And one famous example is like, if you have a training database, you always have a class

23:30.920 --> 23:31.920
imbalance problem.

23:31.920 --> 23:36.560
You always have so many data points of the nice results.

23:36.560 --> 23:42.080
But you don't have enough number of samples of the negative samples or the problematic cases

23:42.080 --> 23:46.560
or out of distribution cases or the unique rare cases.

23:46.560 --> 23:50.120
So we had the list of speakers.

23:50.120 --> 23:58.840
You can also find it online, the list of speakers from the website, CVPR 2019 workshop website.

23:58.840 --> 24:05.080
And we discussed together what are the some common problems that the academia and industry

24:05.080 --> 24:06.640
is facing right now.

24:06.640 --> 24:13.840
And what are the some approaches that academia and industry tried so far to tackle this problem?

24:13.840 --> 24:16.480
That was the main objective of the workshop.

24:16.480 --> 24:21.680
Now these kind of class imbalance problems that you've described.

24:21.680 --> 24:27.800
They come up all over the place, but they are particularly present in what I imagine are

24:27.800 --> 24:34.000
pretty common use cases for you at Siemens, like industrial inspection where you're trying

24:34.000 --> 24:41.440
to identify damaged parts or, you know, damaged products coming off of an inspection line

24:41.440 --> 24:49.680
or maybe looking for damage on wind turbine rotors, things like that, like you've got

24:49.680 --> 24:56.360
so many pictures of the, you know, the good parts and very few pictures of the different

24:56.360 --> 25:01.400
types of damaged parts.

25:01.400 --> 25:06.000
What are some of the key ways that you're able to deal with this?

25:06.000 --> 25:11.760
Sure, actually, some people call this problem out of distribution learning.

25:11.760 --> 25:15.840
Some people call it novel to detection or animal detection as well.

25:15.840 --> 25:20.480
And yes, you're right, because it's always easy to say, okay, this is something normal

25:20.480 --> 25:26.040
looking versus this is something defective, but there might be a hundred different ways

25:26.040 --> 25:28.120
this defect might look like, right?

25:28.120 --> 25:34.440
So and if you'd like to try to annotate all this together, it might take too much time

25:34.440 --> 25:39.360
and you might not even able to recover entire problem space.

25:39.360 --> 25:46.840
So for this one, we are working some animal detection algorithms in order to learn representations

25:46.840 --> 25:52.720
similar approaches, what you mentioned previously using auto encoders, such that you learn

25:52.720 --> 25:58.000
visual trends and then being able to identify, okay, if this is how the normal scene looks

25:58.000 --> 26:04.840
like, how can we identify this sample does not fit to this distribution and the data that

26:04.840 --> 26:07.960
we learned from and that's labeled as the normal.

26:07.960 --> 26:14.880
And this is a very, very powerful technology if you can scale it different use cases as

26:14.880 --> 26:15.880
well.

26:15.880 --> 26:20.040
So what are some of the most interesting, you know,

26:20.040 --> 26:25.640
use cases or case studies that you've had a chance to work on at Siemens, you know,

26:25.640 --> 26:30.720
what's most exciting of the various applications of computer vision there?

26:30.720 --> 26:31.720
Sure.

26:31.720 --> 26:38.880
And actually, this is a very hot area and the computer vision and we always try to do research

26:38.880 --> 26:43.480
that's grounded to the industrial needs and business needs.

26:43.480 --> 26:50.080
And we usually take approaches together with government as well and we are one of the

26:50.080 --> 26:57.080
performance in one of the programs by Office of Naval Research and this is called the

26:57.080 --> 27:06.320
Activity Recognition Project where we are trying to develop a system, the user will interact

27:06.320 --> 27:12.640
with an AI system where the user will tell, okay, I'm looking for an object and the user

27:12.640 --> 27:17.760
will be able to define this object, okay, it's going to have red short, it's going to

27:17.760 --> 27:23.920
have blonde hair or it's going to say short, tall, these attributes or I'm looking for

27:23.920 --> 27:26.120
a white car or etc.

27:26.120 --> 27:32.080
Then the system automatically filters our or distals, millions of video images or millions

27:32.080 --> 27:38.880
of the videos in order to identify and locate your object of interest that you defined.

27:38.880 --> 27:43.800
But the nice thing, an interesting thing is this is a problem that's actually two way

27:43.800 --> 27:49.640
where we believe and this is very exciting, we call this human in the loop idea.

27:49.640 --> 27:57.000
So what we believe is you or the user can provide all this information to the system but

27:57.000 --> 28:00.480
we don't want AI system to just return, okay, these are the results.

28:00.480 --> 28:06.760
We want AI system to come back and counter back saying that did you mean by this and such

28:06.760 --> 28:11.040
that they are going to have a conversation between the user and the AI system until they

28:11.040 --> 28:16.000
actually filter out and locate the object of interest because sometimes we also observe

28:16.000 --> 28:20.680
that people are not good at it by describing what they are looking for and the system might

28:20.680 --> 28:25.760
be able to correct it saying that did you mean by that test and we are trying to actually

28:25.760 --> 28:32.160
map human input to the visual search queries and having a conversation between these two

28:32.160 --> 28:33.160
agents.

28:33.160 --> 28:39.960
Yeah, I'm envisioning a lot of moving parts here between the natural language elements

28:39.960 --> 28:46.240
of it and the visual querying and choosing the right examples to maximize information

28:46.240 --> 28:49.440
transfer between the humans and the computer.

28:49.440 --> 28:52.760
What are the different elements of solving a problem like that?

28:52.760 --> 28:58.720
Sure, and actually that's what makes CMAS really unique because CMAS corporate technology

28:58.720 --> 29:04.400
is a research R&D hub for CMAS and we are actually investing in research and development

29:04.400 --> 29:08.160
in the domain of artificial intelligence and there are multiple teams contributing all

29:08.160 --> 29:13.240
over the world with diverse backgrounds and as you have seen here, this problem requires

29:13.240 --> 29:15.280
a natural language processing background.

29:15.280 --> 29:17.240
This requires computer vision background.

29:17.240 --> 29:25.480
This problem requires also the person identification or this kind of application backgrounds.

29:25.480 --> 29:32.920
So that's why the main team consists of multiple subgroups and one of the team is focusing

29:32.920 --> 29:40.920
on identifying attributes from multi-camera streams so that actually you need to still train

29:40.920 --> 29:46.920
these systems on some training datasets such that you can learn what does mean really

29:46.920 --> 29:50.840
having a red car, what does mean really having a blue car.

29:50.840 --> 29:56.880
So the first part of the computer vision element is being able to identify the objects

29:56.880 --> 30:01.240
and then being able to identify the features of these objects.

30:01.240 --> 30:07.120
Other words, after you identify this and you are capable of actually now processing the

30:07.120 --> 30:12.680
human input where the user says I'm looking for, after you see that sentence, you look

30:12.680 --> 30:18.680
for the verbs and the nouns and our natural language processing experts are working on

30:18.680 --> 30:25.120
how to relate the user queries to the mathematical queries in order to actually find the objects

30:25.120 --> 30:29.640
of interest that are found by this computer vision modules.

30:29.640 --> 30:33.200
Other words, of course, there is another team which is the user interaction team which

30:33.200 --> 30:39.280
is focusing on, okay, what is the information exchange efficiency?

30:39.280 --> 30:44.800
So are we actually going to be able to find a solution if you go down to this route or

30:44.800 --> 30:50.520
should the system come back and ask the user, did you mean by this or such that it actually

30:50.520 --> 30:53.720
improve the quality of the search results?

30:53.720 --> 31:00.880
And along the lines of government projects, you mentioned earlier DARPA Physics of AI

31:00.880 --> 31:05.280
project and I wanted to ask you a little bit about that.

31:05.280 --> 31:07.720
What's that project about?

31:07.720 --> 31:15.880
So DARPA Physics of AI is a program where multiple university partners are contributing

31:15.880 --> 31:22.960
and we have our team member, Dr. Tichun Chang is the PI for that project and this project

31:22.960 --> 31:29.080
is actually trying to incorporate physics into the artificial intelligence systems in order

31:29.080 --> 31:32.880
to eliminate the deed of large training datasets.

31:32.880 --> 31:38.800
And as I explained at the beginning of my discussion, so this is really what excites us learning

31:38.800 --> 31:43.640
with limited data and whenever there is such a program, we try our best to be part of

31:43.640 --> 31:50.280
it and here we are working with the Cornell University and actually this project is trying

31:50.280 --> 31:59.680
to reconstruct 3D images from a sub sampled resolution sensor data where they know some

31:59.680 --> 32:04.120
physics constraints but they don't know they don't have the full visibility to the data

32:04.120 --> 32:09.920
for reconstruction and they are focusing on how to incorporate the physics in order to

32:09.920 --> 32:15.920
be able to reconstruct 3D by knowing the relationship within the elements in this data

32:15.920 --> 32:16.920
and etc.

32:16.920 --> 32:27.000
Okay, I find this general direction of research to be really interesting in that we were

32:27.000 --> 32:36.680
kind of coming from a history of building systems based on very robust models of the physical

32:36.680 --> 32:43.680
world and then we, the pendulum kind of swung to let's throw away all the physics and

32:43.680 --> 32:50.040
just use statistical models, you know, from, you know, just building up from data.

32:50.040 --> 32:54.560
And I find that there's a lot of interesting work happening in the middle now kind of

32:54.560 --> 32:58.760
the pendulum is kind of swung into the middle where we're trying to integrate these two

32:58.760 --> 33:06.800
and attach the physical knowledge of the world or the way whatever system that we're modeling

33:06.800 --> 33:12.520
works with the statistical information or statistical modeling techniques.

33:12.520 --> 33:15.320
There are a bunch of really interesting applications there.

33:15.320 --> 33:23.560
Yes, and actually one everyday example that I use is like, you don't need to observe

33:23.560 --> 33:29.200
1000 times a car is driving on the road in order to understand a regular car does not

33:29.200 --> 33:30.200
fly.

33:30.200 --> 33:34.520
So if you can integrate some, if you can integrate some knowledge about, okay, there's

33:34.520 --> 33:38.720
gravity, there are roads and etc.

33:38.720 --> 33:44.480
You won't need that much data, that's something that excites us and we definitely interested

33:44.480 --> 33:46.320
in this time of topics.

33:46.320 --> 33:54.480
You know, I was at the Siemens spotlight on innovation event not too long ago and one

33:54.480 --> 34:01.360
of the use cases that our application areas that I saw Siemens talking quite a bit about

34:01.360 --> 34:10.320
was smart cities and a lot of kind of a broad much broader array of applications and I usually

34:10.320 --> 34:13.120
see when smart cities comes up.

34:13.120 --> 34:17.560
Is that an area that your group works in?

34:17.560 --> 34:22.920
Not at the moment, but we believe the technology that we develop can be applied there as well.

34:22.920 --> 34:27.480
You also mentioned a project automating scientific knowledge extraction.

34:27.480 --> 34:29.040
What's that one about?

34:29.040 --> 34:37.240
Sure, that's a project that we are collaborating with our sister group here in Siemens CT and

34:37.240 --> 34:44.840
actually, that's the PR is the Dr Janis and actually what we are trying to in that project

34:44.840 --> 34:52.000
is another DARPA effort and the objective is if you have list of papers, how can you

34:52.000 --> 34:56.960
make sure that the knowledge that extracted from this papers corresponds to the code that

34:56.960 --> 35:03.480
you actually get delivered at the end of the day or how can you make sure that you can

35:03.480 --> 35:10.800
automatically understand what is actually explained in this paper by looking at the images,

35:10.800 --> 35:15.680
the text and making its relationship to the software.

35:15.680 --> 35:21.640
And our team is contributing to the recognition of and the composition of the neural network

35:21.640 --> 35:24.400
images on these papers.

35:24.400 --> 35:31.200
And these papers are mostly scientific papers where you have the multiple layers and actually

35:31.200 --> 35:36.240
we are working on developing a system that's capable of taking as input such a scientific

35:36.240 --> 35:43.480
paper and then process the images on these papers and tell you okay, this paper is leveraging

35:43.480 --> 35:49.200
and neural network architecture X with this many layers and they are using this dropout

35:49.200 --> 35:52.000
and this pooling layer and etc.

35:52.000 --> 35:57.680
So we are working on that component to be able to automatically screen such scientific

35:57.680 --> 36:02.360
knowledge papers and try to summarize the content and knowledge in them.

36:02.360 --> 36:07.480
My sense is that a lot of people who are listening to this podcast will probably be really

36:07.480 --> 36:15.320
interested in a system like that, particularly with all of the papers that come up in our

36:15.320 --> 36:23.440
space and that are published to archive, it's super difficult to keep up with even a subset

36:23.440 --> 36:24.840
of those papers.

36:24.840 --> 36:31.880
Yes, that's true, but of course now we are at the resource stage of this algorithm.

36:31.880 --> 36:39.240
In the future we are going to see hopefully more and more examples of such work because

36:39.240 --> 36:46.920
right now if I remember correctly in CVPR there were 9,300 attendees and over 1000 different

36:46.920 --> 36:52.200
papers and right now if you spend three or four days it really takes too much time to

36:52.200 --> 36:56.160
even figure out what are the papers that you like to attend and visit.

36:56.160 --> 37:02.560
So such a solution would be even good if it can actually screen what are the titles and

37:02.560 --> 37:06.320
then does really fit my interest and then I can go there.

37:06.320 --> 37:11.680
It is kind of towards the basic research side of the spectrum, you know at least relative

37:11.680 --> 37:14.720
to what some other corporate research arms are doing.

37:14.720 --> 37:22.120
What is the, how do you ensure that this stuff is tied into and aligned with the actual

37:22.120 --> 37:29.640
stuff that Siemens is doing and for that matter how important you've mentioned that that's

37:29.640 --> 37:36.720
important to you but you know how do you strike a balance between contributing in these broader

37:36.720 --> 37:42.760
efforts and collaborations and doing things that ultimately has an impact on the kind of

37:42.760 --> 37:45.200
stuff that Siemens cares about.

37:45.200 --> 37:52.000
Sure, and actually with government and also other research projects we are more tackling

37:52.000 --> 37:57.280
the lower tier out projects which means the lower technology readiness levels where we

37:57.280 --> 38:01.240
develop some proof of concept and we publish some papers.

38:01.240 --> 38:06.520
Of course we secure the IP and that's the one of the important metrics for us.

38:06.520 --> 38:14.920
How many new ideas or IPs can be actually contributed to our Siemens business units to increase

38:14.920 --> 38:17.360
their competitive advantages.

38:17.360 --> 38:23.840
From a grounding perspective any topic that we invest we always first think about does

38:23.840 --> 38:30.360
it really relate to the problem that we have in-house or does it really have some potential

38:30.360 --> 38:35.440
to be converted into a product in the future and that's always our grounding mechanism.

38:35.440 --> 38:40.960
Our team is working all these papers that we publish even though they sound on their

38:40.960 --> 38:48.120
cool research projects they all get into the real world as one component of a product

38:48.120 --> 38:50.640
that you experience in the world.

38:50.640 --> 38:56.000
And actually the spare part recognition algorithm that I discussed at the beginning of our

38:56.000 --> 39:00.240
interview that's actually called is spare idea.

39:00.240 --> 39:04.720
If you go online and Google it you can see that's actually a product that our business

39:04.720 --> 39:09.320
units right now actually leveraging and providing the service.

39:09.320 --> 39:12.640
So that's how the technology transition happens first.

39:12.640 --> 39:19.800
We actually perform the research internally and in collaboration with the academicians.

39:19.800 --> 39:25.160
And then our job is transferring this research to relevant products but we always keep

39:25.160 --> 39:30.600
in mind that whatever we do needs to provide some impact hopefully in the next two years

39:30.600 --> 39:32.600
for Siemens overall.

39:32.600 --> 39:38.400
When you're taking an idea like this at the you know later stages of this process like

39:38.400 --> 39:45.360
the Easy Spares are there unique challenges that you run into kind of in those final steps

39:45.360 --> 39:48.960
of commercialization always.

39:48.960 --> 39:55.040
And that's why we always have support from other experts from our business units who

39:55.040 --> 39:59.800
are really know how to productize something.

39:59.800 --> 40:06.320
And we definitely experienced some challenges like one of the challenges is with the neural

40:06.320 --> 40:10.440
networks, the memory or the processing times.

40:10.440 --> 40:16.200
So we experienced such challenges during the end of the proof of concept phase before

40:16.200 --> 40:20.040
demonstrating that to our business units.

40:20.040 --> 40:24.880
Well I want to thank you so much for taking the time to share a bit of what you're up

40:24.880 --> 40:25.880
to.

40:25.880 --> 40:30.680
Sounds like a lot of really interesting stuff and I'm imagining some folks will be interested

40:30.680 --> 40:36.640
in digging into some of the recent CVPR papers that you'll be sharing with us.

40:36.640 --> 40:40.280
Yes, thank you very much for your time as well.

40:40.280 --> 40:45.320
And I would like to also point out that in case if there are any other additional questions

40:45.320 --> 40:50.480
your audience is free to reach me out and I think you can also post my email your website

40:50.480 --> 40:53.640
so that I would be happy to connect with them afterwards.

40:53.640 --> 40:56.160
We'll include it in the show notes.

40:56.160 --> 40:57.160
Thanks about to.

40:57.160 --> 40:58.160
Thank you.

40:58.160 --> 41:07.360
Alright everyone that's our show for today.

41:07.360 --> 41:13.600
For more information on today's guests visit twomolai.com slash shows.

41:13.600 --> 41:16.880
Thanks again to Siemens for sponsoring today's episode.

41:16.880 --> 41:21.640
Be sure to check them out at twomolai.com slash Siemens.

41:21.640 --> 41:50.160
As always thanks so much for listening and catch you next time.

