WEBVTT

00:00.000 --> 00:15.320
Welcome to the Tumel AI Podcast, I'm your host Sam Charrington.

00:15.320 --> 00:23.120
Hey, what's up everyone?

00:23.120 --> 00:26.720
Before we get to the show, I'd like to send a huge thanks to everyone who participated

00:26.720 --> 00:32.720
in last week's first ever interactive podcast listening session with Kwak Le.

00:32.720 --> 00:36.360
Kwak and I had a great time answering your questions and discussing his perspective

00:36.360 --> 00:41.000
on the state of deep learning research and all the cool things he's up to at Google.

00:41.000 --> 00:44.680
If you missed the session, we'll have a transcript of the Q&A linked from the show notes

00:44.680 --> 00:50.520
page at twumelai.com slash talk slash 366.

00:50.520 --> 00:55.840
That said, I am pleased to announce our next live program, responsible data science in

00:55.840 --> 01:01.160
the fight against COVID-19, which will be live streamed this Wednesday, April 22nd at

01:01.160 --> 01:03.080
noon pacific time.

01:03.080 --> 01:07.240
Since the beginning of the coronavirus pandemic, we've seen an outpouring of interest on the

01:07.240 --> 01:12.200
part of data scientists and AI practitioners wanting to make a contribution.

01:12.200 --> 01:16.240
At the same time, some of the resulting efforts have been criticized for promoting the spread

01:16.240 --> 01:20.800
of misinformation or being disconnected from the applicable domain knowledge.

01:20.800 --> 01:25.560
In this discussion, we explore the question of how data scientists and MLNI practitioners

01:25.560 --> 01:30.520
can contribute responsibly to the fight against coronavirus and COVID-19.

01:30.520 --> 01:34.760
This discussion will be streamed live on YouTube and other social media channels.

01:34.760 --> 01:40.160
For more information, visit twumelai.com slash RDSCOVID.

01:40.160 --> 01:42.320
And now on to the show.

01:42.320 --> 01:43.800
Alright, everyone.

01:43.800 --> 01:49.080
I am here in Vancouver at NURBS 2019, and I've got the pleasure of being seated with Ristu

01:49.080 --> 01:50.080
Mikkuleinen.

01:50.080 --> 01:56.000
Ristu is a former guest of the podcast, as well as an associate VP of Evolutionary AI at

01:56.000 --> 02:01.480
Cognizant and Professor of Computer Science at the University of Texas at Austin.

02:01.480 --> 02:04.640
Ristu, welcome back to the Twimal AI podcast.

02:04.640 --> 02:05.640
Thank you.

02:05.640 --> 02:06.640
Pleasure to be here.

02:06.640 --> 02:11.840
So it's been a bit since we last spoke at least a year, because that's how long you've

02:11.840 --> 02:13.400
been at Cognizant.

02:13.400 --> 02:18.880
In addition to being at UT Austin, you were working on a startup called Sentient.

02:18.880 --> 02:21.160
Tell us a little bit about the story and transition there.

02:21.160 --> 02:22.160
Yeah, that's right.

02:22.160 --> 02:27.440
So Sentient was a startup in AI, Evolutionary AI in particular.

02:27.440 --> 02:34.280
And we had at Sentient, made a couple of products, one in stock trading, another one in Evolutionary

02:34.280 --> 02:37.440
Optimization of Webpages.

02:37.440 --> 02:41.600
And at one point, those were spun off at their own units.

02:41.600 --> 02:47.600
And the research team now moved to Cognizant, where we continue as an Evolutionary AI research

02:47.600 --> 02:52.040
team and powering applications, trying to take Evolutionary AI to the real world in multiple

02:52.040 --> 02:53.040
applications.

02:53.040 --> 02:56.760
Cognizant is primarily a consulting company?

02:56.760 --> 03:05.040
Yes, it has done a lot of outsourcing and now a lot of consulting in AI and data modernization,

03:05.040 --> 03:10.640
digitization, and AI is becoming a much bigger part of the company because that's a natural

03:10.640 --> 03:14.600
extension of digitizing and starting to work with data.

03:14.600 --> 03:22.440
And so before we started rolling, you mentioned that a big part of what this transition does

03:22.440 --> 03:27.440
for you and the team gives you more exposure to real world scenarios and maybe exposes

03:27.440 --> 03:32.000
some of the things that you need to be thinking about when you're trying to apply Evolutionary

03:32.000 --> 03:33.600
AI in the real world.

03:33.600 --> 03:35.080
Talk a little bit more about that.

03:35.080 --> 03:42.840
Yes, that's been a lot of fun doing consulting and doing modernization in many different industries.

03:42.840 --> 03:48.520
It turns out that AI is not just building something like self-driving cars that never existed

03:48.520 --> 03:53.160
before, but there's a lot of opportunity in just about everywhere to take advantage

03:53.160 --> 03:54.160
of AI.

03:54.160 --> 03:55.160
AI.

03:55.160 --> 04:00.320
And that's because almost all industries now have data.

04:00.320 --> 04:05.080
They collect data about the customers, about their products, how they work, how they sell,

04:05.080 --> 04:07.680
and also feedback on what works what doesn't.

04:07.680 --> 04:11.840
And that data provides a great opportunity to bring in AI, to learn from that data and

04:11.840 --> 04:13.680
then improve it in the future.

04:13.680 --> 04:16.440
And it turns out that it doesn't really matter what industry you are in.

04:16.440 --> 04:21.840
You might be in retail, healthcare, or oil exploration, or manufacturing, or many, many

04:21.840 --> 04:23.640
different industries.

04:23.640 --> 04:27.520
All of those are now on the verge of being able to take advantage of AI.

04:27.520 --> 04:29.040
And that's what we're trying to do.

04:29.040 --> 04:34.080
Take the technology there and make it actually work and add value.

04:34.080 --> 04:41.560
And so how is that change in perspective is kind of exposure to different use cases changed

04:41.560 --> 04:45.760
how you think about your particular work with evolutionary AI.

04:45.760 --> 04:50.160
And maybe we can start with having you do a little refresher or primer on evolutionary

04:50.160 --> 04:54.040
AI for folks who haven't yet heard your, you know, our first conversation.

04:54.040 --> 04:55.040
Sure.

04:55.040 --> 04:59.840
And it is different from what you might imagine what AI is today, because today, most

04:59.840 --> 05:04.960
of AI is taking that data and then building a model that predicts what will happen in

05:04.960 --> 05:05.960
the future.

05:05.960 --> 05:10.960
And that is, for instance, image recognition, object recognition, language understanding,

05:10.960 --> 05:15.560
speech recognition, all of those are trying to get AI to do what we already know how to

05:15.560 --> 05:16.560
do.

05:16.560 --> 05:22.760
Now, with evolutionary AI, we're trying to take the next step and that's making AI creative.

05:22.760 --> 05:28.000
Having AI actually come up with new solutions or designs.

05:28.000 --> 05:30.200
Discover something that does not already exist.

05:30.200 --> 05:33.200
And that might be, for instance, a design for a web page.

05:33.200 --> 05:38.240
Humans can design web pages, turns out that AI, evolutionary AI can do it better.

05:38.240 --> 05:43.600
Come up with solutions or designs that take advantage of interactions that humans have

05:43.600 --> 05:45.080
hard time understanding.

05:45.080 --> 05:47.800
Humans can only keep a couple of variables in mind.

05:47.800 --> 05:51.800
With evolution, we can keep hundreds, so hundreds of thousands of variables in mind and

05:51.800 --> 05:54.280
optimize in a much larger scale.

05:54.280 --> 05:55.680
And that's what evolutionary AI is.

05:55.680 --> 05:57.520
It's making AI creative.

05:57.520 --> 06:02.320
And there's a whole new opportunity here in applying it to different places.

06:02.320 --> 06:06.240
Not just predict, but also prescribe and create.

06:06.240 --> 06:10.920
And this web page design use case is one that you spent quite a bit of time working

06:10.920 --> 06:11.920
on at Centian.

06:11.920 --> 06:17.680
Is that still something that you're in the context of cognizant going out and working

06:17.680 --> 06:22.280
with people on or if you brought ends to other use cases?

06:22.280 --> 06:26.840
Yes, that was one of the first we started with and it's now its own company called

06:26.840 --> 06:28.080
Evolve.

06:28.080 --> 06:34.200
And they are marketing and they are building and developing that application, automated

06:34.200 --> 06:37.560
web site optimization to maximize conversions.

06:37.560 --> 06:38.560
We are doing other things.

06:38.560 --> 06:45.640
We're really expanding out the opportunity to use AI to optimize decision making.

06:45.640 --> 06:50.840
And that means that in companies, there's a lot of data about situations where decisions

06:50.840 --> 06:55.560
had to be made in order to achieve certain objectives like maximize performance while

06:55.560 --> 06:57.320
minimizing cost.

06:57.320 --> 07:03.000
It turns out that data is absolute gold mine because we can learn from it what happens.

07:03.000 --> 07:07.720
If you decide these actions in these situations, are you going to achieve your objectives?

07:07.720 --> 07:10.240
So we can learn from it and we can optimize it.

07:10.240 --> 07:15.040
And that's not something that is done today, but it can be done as soon as we get those

07:15.040 --> 07:22.680
data sources together so we can learn from history, we can start optimizing for the future.

07:22.680 --> 07:31.000
When you summarize the technical foundations of evolutionary AI or in essence how it works

07:31.000 --> 07:34.520
what it does, how do you capture that quickly for folks?

07:34.520 --> 07:35.520
Yes.

07:35.520 --> 07:39.520
So Evolve or AI is population-based search.

07:39.520 --> 07:41.240
So you are trying to discover something new.

07:41.240 --> 07:45.440
You're trying to develop a new design or maybe a decision strategy.

07:45.440 --> 07:48.000
You create a population of potential solutions.

07:48.000 --> 07:53.000
That might be 100, 200 different possible solutions.

07:53.000 --> 07:56.120
And then you evaluate how well each one of those work.

07:56.120 --> 07:58.880
And then you discover that these are the good ones, these are the bad ones, we throw away

07:58.880 --> 07:59.880
the bad ones.

07:59.880 --> 08:05.240
You keep the good ones and then form recommendations and mutations of those good ones.

08:05.240 --> 08:07.480
It's like it's after biological evolution.

08:07.480 --> 08:10.640
This is how search takes place.

08:10.640 --> 08:15.040
In computational evolution you can throw away a lot of solutions that don't work.

08:15.040 --> 08:20.280
You're looking for some where the building blocks coincide so that you get a better solution

08:20.280 --> 08:22.280
than your previous ones.

08:22.280 --> 08:25.960
And this way you can find solutions in huge search spaces.

08:25.960 --> 08:31.120
Two to the two to the 70th is one example we came up with designing a multiplexer for

08:31.120 --> 08:33.360
70 bits, multiplexer problem.

08:33.360 --> 08:37.880
Two to the two to the 70th is so large that if you print it out on a piece of paper it

08:37.880 --> 08:42.960
takes light 95 years to go from the beginning of the number to the end of that number.

08:42.960 --> 08:46.040
That's how many states there are where you're trying to find a solution.

08:46.040 --> 08:49.440
So that's the power of evolution because it's population-based search.

08:49.440 --> 08:53.920
You find building blocks, recombine them and gradually find the solutions.

08:53.920 --> 08:56.160
And it turns out there are many problems like that in the world.

08:56.160 --> 09:01.160
We haven't really thought of them as machine learning problems because they are so huge.

09:01.160 --> 09:02.160
But now we can.

09:02.160 --> 09:05.560
And part of that is that we have the compute and we have the technology.

09:05.560 --> 09:12.600
And so is the idea to identify the problems that naturally lend themselves or are naturally

09:12.600 --> 09:19.640
kind of inherently population-based or evolution-based or is this a technique that can be applied

09:19.640 --> 09:22.400
broadly wherever we're trying to do machine learning?

09:22.400 --> 09:25.600
Yeah, it is very broad.

09:25.600 --> 09:30.160
It applies to problems where we need to find solutions we don't already know.

09:30.160 --> 09:35.240
So it's not object detection, object recognition because we already know what those objects

09:35.240 --> 09:36.240
are.

09:36.240 --> 09:37.240
We have the power status.

09:37.240 --> 09:40.120
It's problems where we don't know what the right solutions are.

09:40.120 --> 09:43.320
What is the optimal design of a web page, for instance?

09:43.320 --> 09:44.320
We don't know that.

09:44.320 --> 09:48.840
We have to explore, try out alternatives and learn what works and what doesn't.

09:48.840 --> 09:54.640
Now the population-based search is unique in that it allows you to explore a lot more.

09:54.640 --> 09:59.360
You could do some of this with reinforcement learning and it'd be successful.

09:59.360 --> 10:04.000
But it's based on improving an individual solution.

10:04.000 --> 10:07.960
And if you have to refine it, you can with that technique.

10:07.960 --> 10:10.400
With population-based search, you can hedge your bets.

10:10.400 --> 10:13.520
You can have a population that's very broadly distributed.

10:13.520 --> 10:18.320
You try things that you don't know anything about and you find stepping stones.

10:18.320 --> 10:22.000
But then you can inspire in order to find something that's surprising.

10:22.000 --> 10:24.840
And that's the crux of it.

10:24.840 --> 10:28.280
Evolution allows you to explore and find surprises.

10:28.280 --> 10:31.480
Those that human designers might not think of.

10:31.480 --> 10:37.520
You mentioned reinforcement learning and I was going to ask about this, the relationship

10:37.520 --> 10:41.840
between the two as you describe the two.

10:41.840 --> 10:48.480
It almost sounds like in a way that evolution or these population-based approaches is like

10:48.480 --> 10:52.200
a breadth-first search, whereas reinforcement learning is a depth-first search.

10:52.200 --> 10:56.760
Yeah, that's kind of an interesting way of looking at it, I think it's accurate in a

10:56.760 --> 11:01.200
sense that it's breadth-first search, but you don't try to find every possible solution.

11:01.200 --> 11:06.160
You are casting your white net in that sense, have a lot of breadth.

11:06.160 --> 11:11.440
But when you find something useful, partial solutions, then you do a recombination of

11:11.440 --> 11:12.440
those.

11:12.440 --> 11:15.800
That's the recombination, that's the interesting secret.

11:15.800 --> 11:19.720
So you don't have to systematically do breadth-first search, you find something that works and

11:19.720 --> 11:23.520
you recombine and focus where you go next.

11:23.520 --> 11:28.280
That's the key of evolution and the theory of evolution is evolution, the computation is

11:28.280 --> 11:29.280
based on that.

11:29.280 --> 11:35.080
Building blocks, hypothesis and schema theorem, there's some theory that suggests that when

11:35.080 --> 11:39.520
you find good partial solutions, they will become more prevalent in the population and that's

11:39.520 --> 11:42.280
how you find those solutions that solve the problem.

11:42.280 --> 11:47.480
So kind of going back to that analogy in deep reinforcement learning, there's work that's

11:47.480 --> 11:52.720
happening to try to maximize what we learn as we're making, taking actions as opposed

11:52.720 --> 11:59.120
to just throwing them out, you know, once we've gotten to an end state, that's kind of

11:59.120 --> 12:02.480
analogous to what we're doing with evolution.

12:02.480 --> 12:06.880
We're trying to take, you know, what we're learning from kind of this breadth-first search

12:06.880 --> 12:10.400
if you will and combine the best of these elements together.

12:10.400 --> 12:12.760
I think I may be pushing this analogy too far.

12:12.760 --> 12:18.040
They really are different in that in reinforcement learning, you're trying to improve in a single

12:18.040 --> 12:19.040
solution.

12:19.040 --> 12:24.760
You get a little closer when you're thinking of off-policy learning where you're trying

12:24.760 --> 12:32.120
to combine knowledge from multiple hypothetical universes, but fundamentally, population search,

12:32.120 --> 12:39.160
space search, distribute your potential solutions, and it's based on statistics, obviously, it's

12:39.160 --> 12:44.600
a stochastic search method, but stochastic is represented by the population.

12:44.600 --> 12:48.360
So what's there in the population represents what you know about the domain.

12:48.360 --> 12:53.560
So you gradually focus your individuals where the most likely solutions are.

12:53.560 --> 12:56.480
But you always want to, there's another part of it.

12:56.480 --> 12:58.480
You always want to maintain diversity.

12:58.480 --> 13:04.480
You always want to explore areas that you don't know about, and that's unique to evolution.

13:04.480 --> 13:07.840
That allows you to do that without much cost.

13:07.840 --> 13:11.320
I mean, you can afford it because you have multiple solutions if some don't turn out.

13:11.320 --> 13:12.320
That's no big deal.

13:12.320 --> 13:13.600
You try something else.

13:13.600 --> 13:18.000
And you eventually find a component that's been missing and then you recombine it with

13:18.000 --> 13:21.680
other solutions to make big leaps in performance.

13:21.680 --> 13:23.440
It's not a gradual search method that way.

13:23.440 --> 13:28.280
You make these recombinations that actually allow you to make large changes and large improvements.

13:28.280 --> 13:33.520
One of the areas that this has been applied is in like neural architecture search.

13:33.520 --> 13:34.520
Yes.

13:34.520 --> 13:35.520
Is that an area that you work in?

13:35.520 --> 13:36.520
Yes, absolutely.

13:36.520 --> 13:39.520
That's something I continue to work on.

13:39.520 --> 13:43.600
And this has also been happening for a long time.

13:43.600 --> 13:45.480
Architectural matter in neural networks.

13:45.480 --> 13:51.560
And currently it is only a few people who have expertise to construct these architects.

13:51.560 --> 13:53.120
So they're very different.

13:53.120 --> 13:55.800
Now we can automate that by doing architects of search.

13:55.800 --> 13:59.920
And again, population based search is great in that it allows you to hedge your bets.

13:59.920 --> 14:01.240
Try out different architectures.

14:01.240 --> 14:04.840
If you find an innovation, you can combine it with other kinds of architectures.

14:04.840 --> 14:06.640
And therefore, come up with better ones.

14:06.640 --> 14:09.320
And this is something that's been going on for decades.

14:09.320 --> 14:15.000
We used to do it so that evolution optimizes the entire network, including its weights.

14:15.000 --> 14:20.040
And now more recently, we optimized the architecture, the topology of the network.

14:20.040 --> 14:23.680
And then use stochastic gradient to train them.

14:23.680 --> 14:26.800
So they're interesting synergies between different approaches.

14:26.800 --> 14:35.160
But again, the evolution based architecture search is the most, perhaps, bold or exploratory.

14:35.160 --> 14:39.360
Because in architecture search, you can get quite a good performance by just tuning hyper

14:39.360 --> 14:42.720
parameters, tuning some small modifications.

14:42.720 --> 14:45.800
But with evolution, you can search a lot larger space.

14:45.800 --> 14:51.920
We can look at the topologies modules, as well as hyper parameters, and find more surprising

14:51.920 --> 14:53.640
architectures that way.

14:53.640 --> 14:56.920
So this is, of course, very compute-intensive.

14:56.920 --> 14:57.920
But that's what you want.

14:57.920 --> 14:59.720
Because compute is still coming out.

14:59.720 --> 15:02.000
We have access to more compute than ever.

15:02.000 --> 15:05.800
And we can actually utilize it to do this more broad search and find more surprises.

15:05.800 --> 15:11.280
And so from a research perspective, where is the research frontier around evolutionary

15:11.280 --> 15:15.840
search relative to a year plus ago when we spoke like, how is it?

15:15.840 --> 15:19.840
I'm assuming it like everything else in this space is evolving quickly.

15:19.840 --> 15:22.640
What are the contemporary topics that folks are talking about?

15:22.640 --> 15:30.600
Yeah, there's one area is novelty, trying to encourage indeed search to discover things

15:30.600 --> 15:34.400
that are surprising, how to do that right.

15:34.400 --> 15:37.280
We are starting to get a handle on that.

15:37.280 --> 15:42.000
It's not just that you reward novelty, but you do it in a systematic way so that you guarantee

15:42.000 --> 15:44.080
a certain level of quality, as well.

15:44.080 --> 15:49.600
So there's a synergy between optimizing the performance and encouraging surprises and

15:49.600 --> 15:51.440
novelty to be discovered.

15:51.440 --> 15:57.960
What I think is still a biggest leap that we need to make in evolution is even though we

15:57.960 --> 16:03.280
have computational methods that have demonstrated the discovery of new things, we're still not

16:03.280 --> 16:06.520
quite at the level of natural biological evolution.

16:06.520 --> 16:10.360
For instance, we are missing major transitions.

16:10.360 --> 16:16.040
How do you search and change your own representations so that now you can search in a different level?

16:16.040 --> 16:19.680
Like transition from single cell to multicell organisms.

16:19.680 --> 16:24.360
Creating evolution, discover representations that allow you to build a larger structure

16:24.360 --> 16:25.360
systematically.

16:25.360 --> 16:30.440
What's an example of that particular challenge applied to a real use case?

16:30.440 --> 16:32.880
Well, it could be, for instance, in behavior.

16:32.880 --> 16:37.440
In AI, we see a lot of agents in games, for instance, so in that domain, it might be

16:37.440 --> 16:43.920
that you have individuals now that perform very well in Atari games, for instance, maybe

16:43.920 --> 16:44.920
Starcraft.

16:44.920 --> 16:51.240
But the next level there might be that they start building roads, they start building buildings

16:51.240 --> 16:59.120
or vehicles and then discover strategies that utilize those structures that they constructed.

16:59.120 --> 17:04.520
That would be a concrete major transition from one level where we operate at the level

17:04.520 --> 17:08.560
of whatever objects they are, to constructing new objects that then you use.

17:08.560 --> 17:12.240
Yes, that strikes me as a big leap, to get its creativity.

17:12.240 --> 17:22.320
In some ways, I don't want to necessarily invoke AGI, but it's a creative essence that

17:22.320 --> 17:26.080
has been very difficult for us to achieve with AI.

17:26.080 --> 17:30.680
Presumably, you're scaling that back to small leaps.

17:30.680 --> 17:31.680
What exactly does that mean?

17:31.680 --> 17:34.400
How do you formulate that problem from a research perspective?

17:34.400 --> 17:36.400
That's exactly right.

17:36.400 --> 17:38.280
That's why it's called major transitions.

17:38.280 --> 17:40.360
We've seen it in biology a few times.

17:40.360 --> 17:44.680
It's not something that happens every day in biology, ain't it?

17:44.680 --> 17:48.360
But we haven't really seen it in evolution, computational evolution yet.

17:48.360 --> 17:49.360
But we are getting there.

17:49.360 --> 17:51.280
It's done to understand how that might work.

17:51.280 --> 17:54.000
Indeed, they happen in small steps.

17:54.000 --> 17:58.200
Open and open and discover is another term that's used there.

17:58.200 --> 18:01.960
Something that doesn't really run out of steam after you solve one problem, but it creates

18:01.960 --> 18:04.080
another level of problem that you then start.

18:04.080 --> 18:06.240
Typically, those are done in games.

18:06.240 --> 18:10.680
So you perhaps evolve the game environment together with the solution.

18:10.680 --> 18:14.680
So you start with navigation, a simple environment, and then you have more objects there that

18:14.680 --> 18:19.760
you have to get around on maybe other opponents that become more sophisticated.

18:19.760 --> 18:21.680
So that's how you approach it.

18:21.680 --> 18:27.160
Sounds a little bit like curriculum learning in a sense that you're kind of gradient-graduating.

18:27.160 --> 18:32.320
You're making your environment or your problem more complex than trying to...

18:32.320 --> 18:33.320
Yes.

18:33.320 --> 18:38.480
It's much like curriculum learning with one exception, the curriculum is designed automatically.

18:38.480 --> 18:41.960
It's a co-evolutionary system.

18:41.960 --> 18:45.520
So you have solutions and problems that evolve at the same time.

18:45.520 --> 18:48.800
And that's how you might get open-endedness in the end, and that's the goal.

18:48.800 --> 18:55.120
We're not there completely yet, but there are some indications that we might get there one day.

18:55.120 --> 18:57.600
So that's one big area.

18:57.600 --> 19:03.960
There are also, I think, an interesting opportunity in looking at what's been learned in biology

19:03.960 --> 19:09.280
in the last 50 years or so, because most of the techniques are based on our understanding

19:09.280 --> 19:14.840
of how genetic algorithms work based on population biology.

19:14.840 --> 19:17.560
But we know a lot more now than we did in 50 years ago.

19:17.560 --> 19:22.400
So looking at what current biology thinks is happening in evolution and discovery might

19:22.400 --> 19:23.800
give us better computation.

19:23.800 --> 19:30.480
So one of them is that it's not just selection, mutation and recombination, but there's also

19:30.480 --> 19:31.480
drift.

19:31.480 --> 19:36.840
There's also large populations with weak selection that allows you to come up with solutions

19:36.840 --> 19:38.960
that don't necessarily help.

19:38.960 --> 19:43.480
They are weakly deleterious, but in the long run, serve a stepping stone to something

19:43.480 --> 19:44.480
bigger.

19:44.480 --> 19:48.280
And our computational methods don't actually do that today, and we're starting to look

19:48.280 --> 19:50.080
into how we can do that.

19:50.080 --> 19:54.600
And a third area might be indirect representations.

19:54.600 --> 20:00.400
So we are currently in evolutionary computation focusing on genetic representation that maps

20:00.400 --> 20:04.000
directly to the solution, one to one.

20:04.000 --> 20:07.440
In biology, there's a developmental process.

20:07.440 --> 20:09.280
There's interaction with environment.

20:09.280 --> 20:12.040
There's even interactions between genes.

20:12.040 --> 20:16.000
And then the final product, like a human being, is not determined by genes.

20:16.000 --> 20:19.400
It's determined by genes plus environmental interactions.

20:19.400 --> 20:20.960
How do we take advantage of that?

20:20.960 --> 20:25.720
How do we create a learning process that takes into account that there's a mechanism of

20:25.720 --> 20:27.840
interaction that makes you what you are?

20:27.840 --> 20:32.640
So that's a third area where we're going today and in the future.

20:32.640 --> 20:39.160
I'm going back to the analogy of, or the use case of architecture search and trying to

20:39.160 --> 20:44.960
think through the way that this problem of creativity expresses itself.

20:44.960 --> 20:54.600
And when you're trying to apply evolutionary AI to something like an architecture search,

20:54.600 --> 21:01.920
are you starting with, presumably you're starting with some known architectures and evolving

21:01.920 --> 21:07.800
based on those, and at some point you want to get to something new, right?

21:07.800 --> 21:14.920
Kind of refine that, you know, how do you represent the knowns in a given problem generally

21:14.920 --> 21:19.680
and then, you know, what does it mean to, you know, recombine things?

21:19.680 --> 21:21.520
It depends how ambitious you want to be.

21:21.520 --> 21:25.880
I mean, you could start with an architecture that already exists and mostly refine it.

21:25.880 --> 21:32.400
You take dense net, you take rest net, you take something like that, modify some of the topology

21:32.400 --> 21:33.400
of that.

21:33.400 --> 21:37.200
So you've got layers and connections and kind of these existing, you know, modules and

21:37.200 --> 21:40.560
things like that and you're reorganizing these things that you already know.

21:40.560 --> 21:41.560
And that already helps.

21:41.560 --> 21:46.280
And you're taking, you're taking the components and principles and then defining a search

21:46.280 --> 21:51.320
space around it and finding if there's a better solution that utilizes those components.

21:51.320 --> 21:56.520
And that works quite well now, but you could try to make it more ambitious.

21:56.520 --> 22:02.720
And that means that you are defining a larger space, maybe, so you take one principle,

22:02.720 --> 22:06.400
but might be that architectures are build on modules.

22:06.400 --> 22:09.800
And then you repeat those modules many time in some kind of organization.

22:09.800 --> 22:14.640
So now you start by evolving a module, which is a combination of different layer types.

22:14.640 --> 22:19.360
You program those up first, define a module and then you define how you combine it at

22:19.360 --> 22:20.360
the second level.

22:20.360 --> 22:22.680
So that's another good approach.

22:22.680 --> 22:26.400
It's a little bit broader search space now because you have a search of modules as well

22:26.400 --> 22:28.240
as the whole architecture.

22:28.240 --> 22:35.520
But you have to, if you have to have a hunch or an idea where the solutions are likely

22:35.520 --> 22:40.520
to be, so that the search will be, you know, finished in our lifetime.

22:40.520 --> 22:41.520
Yeah.

22:41.520 --> 22:43.200
That's kind of the thing that I was poking at.

22:43.200 --> 22:46.280
It's like, you know, how do you represent the thing that you have no idea what it is?

22:46.280 --> 22:52.280
You have to, it has to be at least directed and, you know, not totally open ended or

22:52.280 --> 22:53.280
else.

22:53.280 --> 22:54.280
Yeah.

22:54.280 --> 22:59.240
Well, this is a really a crucial research question because you scientifically, well, you

22:59.240 --> 23:00.920
want it to be as open and as possible.

23:00.920 --> 23:04.680
You want the system to be able to discover things you don't already know.

23:04.680 --> 23:05.680
Right.

23:05.680 --> 23:09.000
But the more knowledge you put in, the more likely you are to find some solutions.

23:09.000 --> 23:10.000
Right.

23:10.000 --> 23:15.120
So it comes back to the major transitions that we really need a system that can evolve

23:15.120 --> 23:20.360
its own representations so that when it discovers something useful, it can use that as a new

23:20.360 --> 23:23.000
representation and build upon it.

23:23.000 --> 23:24.640
And we're not quite there yet.

23:24.640 --> 23:25.640
Yeah.

23:25.640 --> 23:29.280
I mean, we have, live an idea how to put together modules and then structures based on

23:29.280 --> 23:34.480
those modules, but eventually it bottoms out on certain layer types that you program

23:34.480 --> 23:37.360
up, certain ways of doing the connectivity.

23:37.360 --> 23:41.920
And that is based on what we know of what kind of architectures people have built and

23:41.920 --> 23:42.920
how they work.

23:42.920 --> 23:47.640
So you try to abstract those principles and expand a little bit, but not too much.

23:47.640 --> 23:52.920
So it's not a needle in a, well, it's needle in a haystack, but it's still guided towards

23:52.920 --> 23:53.920
architectures.

23:53.920 --> 23:55.200
We believe I like it to work.

23:55.200 --> 23:59.800
And so is this space gotten more accessible over the past year in change or they're like

23:59.800 --> 24:03.840
easily accessible toolkits that someone can, you know, download and play around with

24:03.840 --> 24:04.840
those implementations.

24:04.840 --> 24:05.840
Yes.

24:05.840 --> 24:06.840
Yeah.

24:06.840 --> 24:07.840
There's a lot more now.

24:07.840 --> 24:09.800
They look more approaches as well.

24:09.800 --> 24:16.920
And most of those approaches are clever ideas on how to make do with less, make more

24:16.920 --> 24:17.920
with less.

24:17.920 --> 24:22.280
So, for instance, you evolve a smaller architecture and then there's a mechanism of expanding

24:22.280 --> 24:29.080
it, like just copying and making more, adding more depth and so on, which adds more power.

24:29.080 --> 24:35.840
So you're trying to discover a principle and then mechanically expand it to get more performance.

24:35.840 --> 24:41.040
But there's another direction, I think, that's really interesting in the future also.

24:41.040 --> 24:47.400
And it's not just to try, not just try to come up with better performing architectures,

24:47.400 --> 24:51.520
but optimize something else about that architecture as well, like its size.

24:51.520 --> 24:57.200
You're trying to come up with a small architecture that does the job to a certain specification.

24:57.200 --> 25:05.760
So maybe architectures that are more robust against adversarial attacks or other aspects

25:05.760 --> 25:11.760
like that, use less energy, people are becoming quite aware that these are really not sustainable

25:11.760 --> 25:12.760
architectures.

25:12.760 --> 25:14.680
You have hundreds of millions of parameters.

25:14.680 --> 25:20.000
You can't really use it on a car or a phone or a doll or something like that.

25:20.000 --> 25:21.920
They have to be manageable.

25:21.920 --> 25:26.640
So optimizing the architectures, not just for performance, but other metrics as well.

25:26.640 --> 25:30.760
I think it's an interesting future direction and practical one.

25:30.760 --> 25:36.960
Sounds analogous to multi-task learning approach, which is shown to have some great

25:36.960 --> 25:39.000
advantages and other applications.

25:39.000 --> 25:44.440
Yeah, and that carries over the idea that we shouldn't necessarily start from scratch

25:44.440 --> 25:45.440
every time.

25:45.440 --> 25:52.600
Because we already have some good models, use them and build on them and use other tasks

25:52.600 --> 25:56.400
to bring them together, you don't have to start from scratch, you already have representations

25:56.400 --> 25:59.200
that support multiple different tasks.

25:59.200 --> 26:01.520
They might support a new task much more easily.

26:01.520 --> 26:05.760
It might be possible to build something that supports a new task more easily.

26:05.760 --> 26:14.960
And similarly, it has to do with data who has 100 million examples of their own application.

26:14.960 --> 26:19.040
Those 100 million examples exist for ImageNet and maybe a couple of other benchmark tasks.

26:19.040 --> 26:27.200
And if you actually try to solve your problem, maybe lung disease classification or something,

26:27.200 --> 26:29.400
there aren't that many cases.

26:29.400 --> 26:35.160
So you actually have to use other datasets in order to support learning of your own task.

26:35.160 --> 26:40.240
And that's a multi-task domain and there also architecture search makes a big difference.

26:40.240 --> 26:41.240
So that's interesting.

26:41.240 --> 26:43.480
We asked what has happened since we last talked.

26:43.480 --> 26:49.240
That has happened that we used to focus kind of one single track mine on performance, but

26:49.240 --> 26:52.160
now we realize that there are many other things that need to be optimized in order to make

26:52.160 --> 26:53.160
these things practical.

26:53.160 --> 27:00.440
Switching topics briefly, you recently co-authored a position paper on kind of the historical evolution

27:00.440 --> 27:04.320
of AI and what that says about where things are going or need to go.

27:04.320 --> 27:05.920
Can you give us a quick summary of that?

27:05.920 --> 27:07.320
Yeah, I'd love to.

27:07.320 --> 27:09.120
How many hours do you have?

27:09.120 --> 27:12.760
It's why I qualified that.

27:12.760 --> 27:22.120
Well, there's a lot of discussion about AI and its role in society and good and bad.

27:22.120 --> 27:29.440
And there are some reactions that suggest that AI is going to be dangerous and it's going

27:29.440 --> 27:34.680
to increase inequality and various other challenges like that.

27:34.680 --> 27:39.720
But what we took a look at is some of the other technologies that have been in a similar

27:39.720 --> 27:41.280
role in the past.

27:41.280 --> 27:45.120
And what happened there and what is happening now and discovered that there's actually an

27:45.120 --> 27:49.720
analogy of what AI can do and what the pitfalls are as well.

27:49.720 --> 27:54.520
So those other technologies were computing in general and the world by web.

27:54.520 --> 28:01.560
So if you look at computing, it was initially just for a few government or industry research

28:01.560 --> 28:08.680
labs, then became out-can PCs and Macs and now other people can use them, graphical interfaces

28:08.680 --> 28:09.680
to them.

28:09.680 --> 28:15.560
And cell phones that you have now computing in your pocket and it is now right now computing

28:15.560 --> 28:18.880
has become like plumbing or like electricity.

28:18.880 --> 28:23.200
So you don't even know where it happens, you don't have to, it's available.

28:23.200 --> 28:31.040
So we call those stages standardization like PCs, usability, like user interfaces, consumerization

28:31.040 --> 28:32.440
like cell phones.

28:32.440 --> 28:37.680
And then last phase is making it part of the infrastructure.

28:37.680 --> 28:44.360
And we see that in world by web as well, HTML, style seats, web 2.0 and what's happening

28:44.360 --> 28:50.760
now is that everything is based on web interfaces or commerce and social media.

28:50.760 --> 28:56.640
So in the world by web, the same kind of stages have happened and in both computing and

28:56.640 --> 29:01.880
world by web, beyond the last stage, it's becoming the fabric of society.

29:01.880 --> 29:05.720
Computing happens like plumbing, world by web is everywhere.

29:05.720 --> 29:10.160
And you have your personal as well as business interactions in it.

29:10.160 --> 29:15.840
So if you look at that and look at AI, you can see that there should be similar stages

29:15.840 --> 29:16.920
in the future.

29:16.920 --> 29:18.960
We are at the very beginning right now.

29:18.960 --> 29:26.240
AI is done by a few experts, well in New York, there's 13,000 such experts, but at least

29:26.240 --> 29:27.520
people are learning from it.

29:27.520 --> 29:33.640
But it's still quite difficult to understand and apply and see the opportunities.

29:33.640 --> 29:39.680
So that's before any of these four stages, but you can think of standardization.

29:39.680 --> 29:41.440
So that's what does that even mean?

29:41.440 --> 29:45.560
I'm trying to wrap my head around what would standardization mean for such a broad topic

29:45.560 --> 29:46.560
like AI.

29:46.560 --> 29:51.000
And would that is that I can't imagine that same question could have been asked about

29:51.000 --> 29:53.440
computing before it was standardized.

29:53.440 --> 29:59.760
Well, the way I look at it, it's, it's, it's, it means that the different AI's can

29:59.760 --> 30:01.120
talk to each other.

30:01.120 --> 30:05.520
So we have standard interfaces you can have a visual recognition system that can talk

30:05.520 --> 30:07.640
to a natural language processing system.

30:07.640 --> 30:11.440
And these are developed by different people, different companies, different research labs,

30:11.440 --> 30:12.800
and they can talk to each other.

30:12.800 --> 30:17.800
So we have standardized interfaces so that the AI's can build upon each other and talk

30:17.800 --> 30:18.800
to each other.

30:18.800 --> 30:22.880
But it's still a machine starting to machines, it's program starting, that's a standardization.

30:22.880 --> 30:26.240
It becomes usability when people can talk to them.

30:26.240 --> 30:28.320
And we are not there yet.

30:28.320 --> 30:31.280
We don't really have an easy interface to talk to an AI.

30:31.280 --> 30:34.040
It's very difficult to take advantage of those.

30:34.040 --> 30:39.960
We have some initial attempts to that programming language is perhaps an interface that allow

30:39.960 --> 30:44.000
you to direct the other certain solutions.

30:44.000 --> 30:46.120
But it's not, by and large, not yet happening.

30:46.120 --> 30:48.120
That's a usability part.

30:48.120 --> 30:53.600
Consumeration means that anyone can take blocks away AI and put together a solution.

30:53.600 --> 30:57.960
They can manage their finances, their health, they can, I don't know, design their garden,

30:57.960 --> 31:03.360
the home, they use AI to do it, and they parameterize it, they run it, and they interact

31:03.360 --> 31:04.360
with it.

31:04.360 --> 31:09.440
So making it a consumer product, something that everybody can use for their advantage.

31:09.440 --> 31:11.040
That's a consumerization.

31:11.040 --> 31:20.360
Karl Semain, one of the popular visions of AI, that of everyone's going to have this

31:20.360 --> 31:24.280
potentially a cadre of intelligent agents that are out acting in the world on their

31:24.280 --> 31:30.120
behalf, and that requires the ability for these agents to talk to other agents and to

31:30.120 --> 31:32.560
talk to other systems and things like that.

31:32.560 --> 31:37.480
Yes, it requires that, that was a standardization, but also talk to people, the usability part.

31:37.480 --> 31:42.720
But then the fact that you are actually in control, you consumer picks what you want

31:42.720 --> 31:46.560
to do and what components to use and how to put them together.

31:46.560 --> 31:52.320
So it becomes such a second nature for humans that you can actually solve problems with it.

31:52.320 --> 31:58.080
So it becomes a point where you think of say furnishing a room, you call an AI agent to

31:58.080 --> 32:02.240
try out different alternatives, make suggestions, you are totally running it, you're in control,

32:02.240 --> 32:05.840
you calling it, it's not controlling you, that's an important part of it.

32:05.840 --> 32:12.440
It becomes consumer goods just like, you know, cell phones or running errands on the

32:12.440 --> 32:14.560
web, through the web.

32:14.560 --> 32:20.480
And the last step I think is really quite interesting and in the future, that it could become

32:20.480 --> 32:22.880
AI could become a fabric of society.

32:22.880 --> 32:29.680
And what that means is that we as a society decide what we want.

32:29.680 --> 32:35.520
We want to maximize perhaps productivity and growth, but we might also say that we want

32:35.520 --> 32:43.160
to minimize impact on the environment or maximize equality and access.

32:43.160 --> 32:49.880
And then we can use AI to design policies and execute those policies so that those goals

32:49.880 --> 32:50.880
are met.

32:50.880 --> 32:52.360
I mean, that's what AI does.

32:52.360 --> 32:57.840
It's really good in the end on optimization and discovery of how to achieve certain goals.

32:57.840 --> 33:03.600
But when it becomes institutionalized in the sense and in society, it means it gives

33:03.600 --> 33:09.440
humans the power to come up and to achieve the goals that we decide.

33:09.440 --> 33:12.280
And that's remarkable because it's not happening today.

33:12.280 --> 33:13.360
It's never happened.

33:13.360 --> 33:16.320
There's always been individual personal agendas.

33:16.320 --> 33:22.000
There's always been dishonesty graph, various things that get in the way and create conflict.

33:22.000 --> 33:31.280
But if we let AI do the optimization, all we have to do is agree on what we want.

33:31.280 --> 33:38.080
And that's not a small challenge, but it separates all the challenges that get in the way today

33:38.080 --> 33:43.160
from the really what we should agree upon is what we want.

33:43.160 --> 33:45.200
That's the ultimate goal.

33:45.200 --> 33:49.880
When getting there, I think there's a stage where it becomes irresponsible to try to make

33:49.880 --> 33:53.880
decisions and create policies without the use of AI.

33:53.880 --> 33:54.880
AI is objective.

33:54.880 --> 33:55.880
It's based on data.

33:55.880 --> 34:00.920
It allows you to optimize and currently it's done by humans and not so well.

34:00.920 --> 34:05.760
And it becomes irresponsible to try to make these decisions as humans when AI can do

34:05.760 --> 34:06.760
it better.

34:06.760 --> 34:10.840
But we have to still take the responsibility of setting those goals so that we can use

34:10.840 --> 34:13.960
those AI in a responsible and productive manner.

34:13.960 --> 34:15.520
So that's the last stage for AI.

34:15.520 --> 34:18.280
And that's why I think it's exciting.

34:18.280 --> 34:21.200
And it's also important that we are now at the crossroads.

34:21.200 --> 34:29.760
We can actually adopt that vision and make AI such that it helps us achieve what we want.

34:29.760 --> 34:32.000
And we have to make the right decisions in a way.

34:32.000 --> 34:36.440
We cannot let the mistakes of the computing enrolled by web get in a way.

34:36.440 --> 34:38.240
There were mistakes along the way.

34:38.240 --> 34:42.840
And our monopolies as well as over regulation, they can get in a way.

34:42.840 --> 34:47.960
And we have to recognize that we need to build these capabilities over time.

34:47.960 --> 34:49.360
We need to adjust to them.

34:49.360 --> 34:55.960
So is the main thrust of the paper and the model, is it that is it to kind of locate us

34:55.960 --> 34:56.960
in time?

34:56.960 --> 34:59.160
Hey, we are here in the next stage of standardization.

34:59.160 --> 35:04.800
Is it to reassure us that everything's going to be okay because there's this kind of bright

35:04.800 --> 35:10.320
future for AI driven policy making like what is the main thing that you're trying to convey?

35:10.320 --> 35:16.640
Yeah, that's, it's definitely trying to raise awareness that we have a great opportunity

35:16.640 --> 35:21.000
to make AI work for us and improve the world that way.

35:21.000 --> 35:22.240
But we have to recognize it.

35:22.240 --> 35:23.240
We have to recognize it.

35:23.240 --> 35:26.920
We have to build it most likely through these stages because we've seen two examples of

35:26.920 --> 35:27.920
them.

35:27.920 --> 35:32.720
And at the time when we were working on computing a world where we made some mistakes,

35:32.720 --> 35:36.640
we could learn from those and do better this time.

35:36.640 --> 35:41.640
And it's possible, for instance, that AI is over-regulated.

35:41.640 --> 35:43.040
There's no access to data.

35:43.040 --> 35:47.080
There's no decision making done by AI because people are afraid of it and they don't really

35:47.080 --> 35:53.880
see the potential benefit and it will never develop or it will, development will be delayed.

35:53.880 --> 35:58.160
But recognizing the potential, recognizing the pitfalls allows us to see what needs to

35:58.160 --> 36:03.200
be done and hopefully nurture the field so that it will, will get there and it will happen

36:03.200 --> 36:09.280
in smaller steps. We can't replace everything a decision-making with AI today.

36:09.280 --> 36:10.680
We'll have to develop the technology.

36:10.680 --> 36:11.880
We have to develop the datasets.

36:11.880 --> 36:17.680
We have to develop the, it's a humans' understanding how to use it.

36:17.680 --> 36:24.120
And there are some challenges like you pointed out that humans are not willing to give control

36:24.120 --> 36:26.120
very easily to machines.

36:26.120 --> 36:30.640
But there's also, I've seen, as a professor, I've seen a change in the last couple of years

36:30.640 --> 36:38.000
and decades that people are much more accepting of it now because they can see the benefits.

36:38.000 --> 36:41.640
And that's happened in, say, just one example, self-driving cars.

36:41.640 --> 36:47.720
2003, we were working with manufacturers developing self-driving systems and they said that they

36:47.720 --> 36:52.560
will never happen because people will not let machines take over their car.

36:52.560 --> 36:55.520
They can only warn you while you're driving.

36:55.520 --> 36:57.600
But somehow the attitudes changed.

36:57.600 --> 37:01.640
People saw what the opportunities are and now we have self-driving cars almost ready

37:01.640 --> 37:03.240
to hit the road.

37:03.240 --> 37:09.960
That kind of change of attitudes has to happen and people have to be educated and learn

37:09.960 --> 37:15.280
of what's possible and learn to understand what the limitations are, so to avoid them.

37:15.280 --> 37:19.400
But the sooner we understand, both of those dimensions, the opportunities as well as

37:19.400 --> 37:20.400
the challenges.

37:20.400 --> 37:24.120
I think better off we are and we can make this future happen sooner and another day.

37:24.120 --> 37:27.440
Unfortunately, we're going to have to leave that there, put a pin in it, we probably should

37:27.440 --> 37:32.440
have started the conversation there and we could have gotten into it for the whole time.

37:32.440 --> 37:37.520
But interesting perspective for sure and I am very appreciative of the update on Evolutionary

37:37.520 --> 37:42.640
AI, so he still thanks so much for taking the time to chat with us.

37:42.640 --> 37:43.640
My pleasure.

37:43.640 --> 37:44.640
Thank you.

37:44.640 --> 37:45.640
Awesome.

37:45.640 --> 37:46.640
Thank you.

37:46.640 --> 37:47.640
All right, everyone.

37:47.640 --> 37:50.280
That's our show for today.

37:50.280 --> 37:56.200
To learn more about today's guest or the topics mentioned in this interview, visit twimmelai.com.

37:56.200 --> 38:01.200
Of course, if you like what you hear on the podcast, please subscribe, rate and review

38:01.200 --> 38:03.960
the show on your favorite pod catcher.

38:03.960 --> 38:29.840
Thanks so much for listening and catch you next time.

