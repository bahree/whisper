1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,440
I'm your host Sam Charrington.

4
00:00:32,440 --> 00:00:37,800
In this episode I speak with Rob Walker, vice president of decision management and analytics

5
00:00:37,800 --> 00:00:44,600
at Pegasystems, a leading provider of software for customer engagement and operational excellence.

6
00:00:44,600 --> 00:00:49,160
Rob and I discussed what's required for enterprises to fully realize the vision of providing

7
00:00:49,160 --> 00:00:54,880
a hyper personalized customer experience and how machine learning and AI can be used

8
00:00:54,880 --> 00:01:00,600
to determine the next best action an organization should take to optimize sales, service, retention

9
00:01:00,600 --> 00:01:04,800
and risk at every step in the customer relationship.

10
00:01:04,800 --> 00:01:10,480
Along the way we dig into a couple of key areas, specifically some of the techniques his organization

11
00:01:10,480 --> 00:01:16,000
uses to allow customers to manage the trade-off between model performance and transparency,

12
00:01:16,000 --> 00:01:21,360
particularly in light of new laws like GDPR, and how all of this ties to an enterprise's

13
00:01:21,360 --> 00:01:26,160
ability to manage bias and ethical issues when deploying machine learning.

14
00:01:26,160 --> 00:01:30,200
We cover a lot of ground in this one and I think you'll find Rob's perspective really

15
00:01:30,200 --> 00:01:32,560
interesting.

16
00:01:32,560 --> 00:01:38,560
If you're looking for more enterprise perspectives on the latest in AI, robotics, customer engagement

17
00:01:38,560 --> 00:01:43,920
and digital process automation technology, you should check out Pegasystems upcoming

18
00:01:43,920 --> 00:01:52,160
conference Pegaworld, which will be held June 3rd to 6th at the MGM Grand in Las Vegas, Nevada.

19
00:01:52,160 --> 00:01:57,960
I'll be there, Rob will be there, and so will IT and experience leaders from organizations

20
00:01:57,960 --> 00:02:03,800
like Afflack, Anthem, Genworth Financial, JP Morgan Chase, and more.

21
00:02:03,800 --> 00:02:13,880
To learn more, visit Pegaworld, and as a Twymilistner, be sure to use the discount code PW18TWIML

22
00:02:13,880 --> 00:02:16,800
for $150 off of your registration.

23
00:02:16,800 --> 00:02:26,360
Thanks to Pegaworld for sponsoring this episode of the podcast, and now on to the show.

24
00:02:26,360 --> 00:02:29,600
All right everyone, I am on the line with Rob Walker.

25
00:02:29,600 --> 00:02:34,480
Rob is Vice President of Decision Management and Analytics with Pegasystems.

26
00:02:34,480 --> 00:02:36,600
Rob, welcome to the podcast.

27
00:02:36,600 --> 00:02:37,960
Thank you.

28
00:02:37,960 --> 00:02:43,040
It is great to have you on, as is the tradition here on the podcast, why don't we start out

29
00:02:43,040 --> 00:02:48,360
by having you tell us a little bit about your background and how you got involved in

30
00:02:48,360 --> 00:02:49,760
Analytics and AI?

31
00:02:49,760 --> 00:02:50,960
Yeah, thank you.

32
00:02:50,960 --> 00:02:51,960
Happy to do so.

33
00:02:51,960 --> 00:02:56,520
Yeah, I was really attracted to AI very early on.

34
00:02:56,520 --> 00:03:03,600
I mean, this is like, so in the 90s, when I was at the university, and I was doing my computer

35
00:03:03,600 --> 00:03:11,080
science studies, the specialization I chose was AI, which obviously is not quite the AI

36
00:03:11,080 --> 00:03:16,600
we have right now, but on the other hand, you know, much has actually remained the same,

37
00:03:16,600 --> 00:03:19,800
just a day die I think has changed a lot.

38
00:03:19,800 --> 00:03:27,000
But that was a really fun sort of specialization, and later I got my PhD in AI as well, specifically.

39
00:03:27,000 --> 00:03:31,040
And after that, I thought this was like, we're now talking like 2000 to approximately.

40
00:03:31,040 --> 00:03:35,640
I was really, you know, getting ready to do something in business.

41
00:03:35,640 --> 00:03:42,000
So I co-founded a company, company called KIQ, which was really specializing in what

42
00:03:42,000 --> 00:03:45,680
I would say is Applied AI, so that was early days.

43
00:03:45,680 --> 00:03:50,480
I was really like, apply predictive analytics, but at a very high scale around, you know, customer

44
00:03:50,480 --> 00:03:55,680
engagement specifically, but predicting things like credit, risk, or customer behavior

45
00:03:55,680 --> 00:04:01,480
in general, but at a very large scale, and then applying it in real time.

46
00:04:01,480 --> 00:04:06,840
And some facts about that was that we got the venture capital for that company, like moments

47
00:04:06,840 --> 00:04:09,920
before the 2000, you know, crash.

48
00:04:09,920 --> 00:04:13,760
So we just got the money in to start, so that was a really, really good timing.

49
00:04:13,760 --> 00:04:17,240
And then four years later, yeah, that was lucky.

50
00:04:17,240 --> 00:04:22,280
And AI wasn't as hot then as it is now, so that was double lucky.

51
00:04:22,280 --> 00:04:27,920
We got that going, sold it into some pretty large companies, and we're then acquired

52
00:04:27,920 --> 00:04:34,200
four years later by a company called Cordient, which is a Cupertino company next door

53
00:04:34,200 --> 00:04:40,000
neighbors with Apple, who really used that technology and AI around customer experience

54
00:04:40,000 --> 00:04:41,840
and personalization.

55
00:04:41,840 --> 00:04:45,760
And then Pegasystems acquired Cordient for that particular thing as well.

56
00:04:45,760 --> 00:04:49,880
So it's a long-winded thing, a long-winded way of saying that I've been in this space

57
00:04:49,880 --> 00:04:56,600
for a very long time, and it hasn't let many things distract me from using AI in a very

58
00:04:56,600 --> 00:05:00,800
practical way with sort of, you know, pretty large companies.

59
00:05:00,800 --> 00:05:02,800
And what's your focus at Pegas now?

60
00:05:02,800 --> 00:05:08,040
So at Pegas, it's really like, we have this thing called, you know, the customer brain

61
00:05:08,040 --> 00:05:13,200
or the customer decision hub, but it's a little bit more technical, and it's basically

62
00:05:13,200 --> 00:05:18,320
one of the core elements of everything we do, especially around customer engagement.

63
00:05:18,320 --> 00:05:25,880
So it's helping us hyper-personalize interactions in lots of different channels, very high volume

64
00:05:25,880 --> 00:05:31,000
kind of things, and so things digital presence on the website or mobile, but at the same

65
00:05:31,000 --> 00:05:38,600
time, you know, decide on next best actions for outbound communications, or if this is

66
00:05:38,600 --> 00:05:47,120
a bank in ATM, every single interaction we're trying to optimize, and that's not just AI,

67
00:05:47,120 --> 00:05:50,640
but AI is a very critical component of that.

68
00:05:50,640 --> 00:05:57,080
And are you optimizing it primarily from what perspective, from a financial perspective,

69
00:05:57,080 --> 00:06:03,560
or you mentioned customer experience earlier from a broader customer experience perspective?

70
00:06:03,560 --> 00:06:04,840
How do you think about that?

71
00:06:04,840 --> 00:06:09,200
Yeah, so the optimization function is basically critical.

72
00:06:09,200 --> 00:06:13,960
So I spent a lot of time, you know, talking to, especially the business people, but also

73
00:06:13,960 --> 00:06:19,920
with the data scientists in these large companies, so to decide on what their metric is, right?

74
00:06:19,920 --> 00:06:27,520
So we do next best action, and best, typically, is a combination of, like, future value, right?

75
00:06:27,520 --> 00:06:32,520
So it's not, we're, as best practice, we don't try to, for instance, if this is about,

76
00:06:32,520 --> 00:06:37,480
you know, a web interaction, it's not about, okay, what should we be selling you, or what

77
00:06:37,480 --> 00:06:38,480
should you be doing?

78
00:06:38,480 --> 00:06:44,640
It's really trying to optimize future value of the relationship, and that means that, yeah,

79
00:06:44,640 --> 00:06:49,720
there are products, maybe, that you're selling, but you can also try to nurture the relationship

80
00:06:49,720 --> 00:06:53,800
you can look at MPS scores that you're trying to optimize.

81
00:06:53,800 --> 00:07:00,640
So we're not specifically prescribing what the best metric is, but it's typically a proxy

82
00:07:00,640 --> 00:07:06,680
for future value of that relationship, that then every action is trying to optimize.

83
00:07:06,680 --> 00:07:10,640
Can you maybe walk us through a more concrete example of that?

84
00:07:10,640 --> 00:07:16,320
It doesn't have to be a specific customer, but if you've got a specific one in mind, just

85
00:07:16,320 --> 00:07:21,320
to put that kind of little process that you outlined for us into context?

86
00:07:21,320 --> 00:07:22,320
Sure.

87
00:07:22,320 --> 00:07:28,640
Yeah, let's take a large bank, for instance, and they've been on record, like Royal Bank

88
00:07:28,640 --> 00:07:35,240
of Scotland, or PNC Bank in the US, both of them, there are many other banks that are

89
00:07:35,240 --> 00:07:40,760
using this kind of stuff, but what they are doing is basically make sure that at every

90
00:07:40,760 --> 00:07:45,200
opportunity they have, when they engage in with a customer in any channel, so it can

91
00:07:45,200 --> 00:07:51,440
be sort of an old-fashioned email, or even more old-fashioned printed email, but typically

92
00:07:51,440 --> 00:07:58,280
obviously now the digital channels on the web, in the mobile, in the app everywhere, every

93
00:07:58,280 --> 00:08:06,040
single interaction, they're basically pinging the AI, which is then predicting all sorts

94
00:08:06,040 --> 00:08:15,080
of behaviors, like likelihood to turn, likelihood to, if this is a bank of getting into financial

95
00:08:15,080 --> 00:08:21,280
trouble, and maybe not repaying alone, obviously propensities around the whole host or product

96
00:08:21,280 --> 00:08:27,400
or product combinations that the bank would be selling them, and then a bunch of optimization

97
00:08:27,400 --> 00:08:34,240
algorithms, but also rules put in by humans to decide on, okay, if that's the case, then

98
00:08:34,240 --> 00:08:39,320
contextually, right now, for this interaction, and that maybe one interaction out of billions

99
00:08:39,320 --> 00:08:45,680
in a single day, this is what we should be doing, and that's sort of the process we're

100
00:08:45,680 --> 00:08:50,240
going through, and this could be, this is an offer, this is what we show on the website,

101
00:08:50,240 --> 00:08:56,080
this is a notification on your mobile phone, this is a prompt on the ATM terminal, any

102
00:08:56,080 --> 00:08:57,360
of those kind of things.

103
00:08:57,360 --> 00:09:07,720
I'm wondering what your take is on kind of where we are, I guess, as an industry in this

104
00:09:07,720 --> 00:09:14,200
process of applying AI and analytics to the customer experience, right?

105
00:09:14,200 --> 00:09:20,440
So calling it AI is new, but a lot of what you're describing, folks have tried to use predictive

106
00:09:20,440 --> 00:09:29,120
analytics for quite some time to optimize the next offer or the next step in managing

107
00:09:29,120 --> 00:09:30,120
a relationship.

108
00:09:30,120 --> 00:09:39,800
And personally, when I think about my relationships with large enterprises like banks, like telecom

109
00:09:39,800 --> 00:09:45,640
companies, I don't really get the sense that they're optimizing anything about that relationship.

110
00:09:45,640 --> 00:09:55,280
It feels very static, and I wonder what the barriers are to them realizing this kind of

111
00:09:55,280 --> 00:10:00,720
vision of a more personalized kind of relationship with me as their customer.

112
00:10:00,720 --> 00:10:06,680
Yeah, I think that, and that's obviously many, many do not, and there are, it's amazing

113
00:10:06,680 --> 00:10:13,160
given the benefits that I think we, but also other companies show, all of this is data

114
00:10:13,160 --> 00:10:14,160
driven.

115
00:10:14,160 --> 00:10:18,080
So it's not particularly hard to show sort of the delta on some of these, on some of

116
00:10:18,080 --> 00:10:19,080
these things.

117
00:10:19,080 --> 00:10:23,560
And the reason it's, and I have to say it's going fast, so I mean, we certainly don't complain

118
00:10:23,560 --> 00:10:28,720
about traction, but it's not going as fast as you would expect, because it's not how

119
00:10:28,720 --> 00:10:33,960
these large organizations are organized, and this is nothing to do with the technology,

120
00:10:33,960 --> 00:10:38,720
although I'll touch on that in a second as well, but it's also just culturally, you know,

121
00:10:38,720 --> 00:10:44,800
people in these big banks or big communications companies or other companies are incentivized

122
00:10:44,800 --> 00:10:46,720
on a specific part of the business, right?

123
00:10:46,720 --> 00:10:51,320
So if you're talking to a bank, in your example, you know, the people that want to sell

124
00:10:51,320 --> 00:10:56,400
you credit cards do their thing, typically pretty old-fashioned as well.

125
00:10:56,400 --> 00:10:59,800
And then the people that want to sell you mortgages do their thing.

126
00:10:59,800 --> 00:11:02,960
And then there are, you know, ten other lines of businesses that do their thing, and it

127
00:11:02,960 --> 00:11:07,920
all comes together, you know, at the point of contact, but it's not a concerted effort.

128
00:11:07,920 --> 00:11:11,120
It's not optimized for the bank or for you.

129
00:11:11,120 --> 00:11:18,600
It's basically trying to make sense of all sort of suboptimal, you know, goals.

130
00:11:18,600 --> 00:11:24,840
That's why it feels incoherent and inconsistent, and typically then all the different channels

131
00:11:24,840 --> 00:11:28,400
are not connected to this one brain in the middle either, right?

132
00:11:28,400 --> 00:11:35,360
So anyway, there's a lot of ways to make mistakes or to, you know, get suboptimal results

133
00:11:35,360 --> 00:11:37,120
as you are experiencing.

134
00:11:37,120 --> 00:11:44,880
So what are you seeing as the main ways that companies are trying to dismantle these silos

135
00:11:44,880 --> 00:11:47,920
and create better experiences?

136
00:11:47,920 --> 00:11:51,960
Yeah, well, obviously they're not, you know, these big companies are not charity.

137
00:11:51,960 --> 00:11:55,600
So they, I think the big move is, well, there's twofold.

138
00:11:55,600 --> 00:11:59,680
One is they actually have a lot of data, right?

139
00:11:59,680 --> 00:12:03,760
I mean, obviously there's a whole big data discussion, but they have a lot of it now.

140
00:12:03,760 --> 00:12:05,120
And it's in much better quality.

141
00:12:05,120 --> 00:12:09,160
So if I rewind even like ten years, you know, when everybody was building, you know,

142
00:12:09,160 --> 00:12:14,360
data warehouses and things like that, we're in an incomparably better state on data.

143
00:12:14,360 --> 00:12:20,280
And also the awareness of that, you know, that these big companies have that data is,

144
00:12:20,280 --> 00:12:23,240
it has grown, has grown tremendously.

145
00:12:23,240 --> 00:12:30,200
And then the question is, can they basically, based on what AI can, can do to optimize,

146
00:12:30,200 --> 00:12:32,560
you know, these, these interactions?

147
00:12:32,560 --> 00:12:39,120
Do they believe that, you know, there's a tangible result and we're talking about a lot

148
00:12:39,120 --> 00:12:40,120
of money, right?

149
00:12:40,120 --> 00:12:44,480
And that, I think, is the biggest reason for a lot of these big companies pivoting towards

150
00:12:44,480 --> 00:12:49,920
this because it's so easy to show once you get it going, what the benefits are, right?

151
00:12:49,920 --> 00:12:56,560
Both in customer satisfaction and in, you know, bottom line benefits that it's, it becomes

152
00:12:56,560 --> 00:13:01,640
pretty hard to, you know, neglect that kind of progress.

153
00:13:01,640 --> 00:13:07,320
Can you talk a little bit about the process that company might take to identify, you know,

154
00:13:07,320 --> 00:13:10,120
where it's biggest opportunities lie?

155
00:13:10,120 --> 00:13:16,520
I imagine for a lot of these companies, there are tons of ways that they could apply analytics

156
00:13:16,520 --> 00:13:24,520
and AI to create better outcomes and prioritizing them becomes a bit of a challenge in and

157
00:13:24,520 --> 00:13:25,520
of itself.

158
00:13:25,520 --> 00:13:26,520
Yeah.

159
00:13:26,520 --> 00:13:27,520
Not that that's true.

160
00:13:27,520 --> 00:13:29,200
And that becomes a huge, huge challenge.

161
00:13:29,200 --> 00:13:35,600
What we typically do is to, and this is much easier than it was, like not that low

162
00:13:35,600 --> 00:13:40,800
ago because of all the references and these tangible results.

163
00:13:40,800 --> 00:13:45,120
So I have a lot easier conversations than I had before.

164
00:13:45,120 --> 00:13:50,200
But one of the things that I think radically changes, but also really brings the need of

165
00:13:50,200 --> 00:13:56,200
AI to the forefront is that if you become, like, really customer centric and you have

166
00:13:56,200 --> 00:14:00,160
these one-to-one relationships, the whole model is changing, right?

167
00:14:00,160 --> 00:14:05,320
I mean, a traditional bank or large company would have marketing campaigns where obviously

168
00:14:05,320 --> 00:14:10,880
data scientists would create their predictive models and do all their magic and create

169
00:14:10,880 --> 00:14:13,560
segments and do targeting.

170
00:14:13,560 --> 00:14:19,360
But that's all based on sending out a message for a particular product or offer, right?

171
00:14:19,360 --> 00:14:25,840
If you actually put the customer at the center and now you have AI figuring out all of their

172
00:14:25,840 --> 00:14:30,720
behaviors that are relevant, you know, from a risk perspective, retention perspective,

173
00:14:30,720 --> 00:14:38,800
surface perspective, as well as sales, now you have maybe a thousand different predictive

174
00:14:38,800 --> 00:14:44,080
models and you need to optimize with all of those outputs as inputs.

175
00:14:44,080 --> 00:14:49,920
Now, that's the optimization challenge that, on the one hand, gives you a lot better

176
00:14:49,920 --> 00:14:55,200
customer satisfaction because it's, the relevance will go through the roof and so will

177
00:14:55,200 --> 00:14:57,000
bottom line benefits.

178
00:14:57,000 --> 00:15:01,520
But it means that a lot of companies or a lot of people in these companies now need to

179
00:15:01,520 --> 00:15:05,120
collaborate where they didn't have to before, right?

180
00:15:05,120 --> 00:15:09,160
Because the markets people would do one thing, the cars people would do a different thing.

181
00:15:09,160 --> 00:15:15,040
Now it's all about the customer and AI has now a big vote in what should be done or very

182
00:15:15,040 --> 00:15:21,080
strong opinions about, you know, recommendations that it makes about what the next best action

183
00:15:21,080 --> 00:15:22,080
would be.

184
00:15:22,080 --> 00:15:28,440
So it sounds like the first step is really at least for the companies that you're working

185
00:15:28,440 --> 00:15:32,200
with and the approach that you're taking with them.

186
00:15:32,200 --> 00:15:41,400
The first step is really centralizing, centralizing is maybe not the right word, but kind of establishing

187
00:15:41,400 --> 00:15:46,840
this customer-centric perspective, which is kind of strange for me to hear in some sense

188
00:15:46,840 --> 00:15:51,640
because like, you know, one-to-one marketing, 360 degree marketing, these kinds of, these

189
00:15:51,640 --> 00:15:57,560
are 20 plus year old terms and we just have tools and approaches that we can apply to

190
00:15:57,560 --> 00:15:58,720
them now.

191
00:15:58,720 --> 00:16:03,280
But it's a little surprising that it's still so hard to get companies to buy into this

192
00:16:03,280 --> 00:16:04,680
as an idea.

193
00:16:04,680 --> 00:16:05,680
Yeah.

194
00:16:05,680 --> 00:16:12,080
Well, partly it's still cultural, but partly also, honestly, that is because you know, one-to-one

195
00:16:12,080 --> 00:16:19,160
marketing or one-to-one interactions may be, you know, not the latest idea, I'm sure that,

196
00:16:19,160 --> 00:16:23,080
you know, we heard that maybe in the 90s or something like that.

197
00:16:23,080 --> 00:16:27,600
But to actually make it real, you do need AI.

198
00:16:27,600 --> 00:16:32,560
So I think that is really the breakthrough of the last 10 years, right, that you can have

199
00:16:32,560 --> 00:16:38,960
the self-learning models, adaptive models, all sorts of AI algorithms that these big companies

200
00:16:38,960 --> 00:16:45,000
can trust to turn through their data, come up with propensities, learn on the fly, execute

201
00:16:45,000 --> 00:16:48,960
contextually, and make them a lot of money.

202
00:16:48,960 --> 00:16:55,520
I mean, that's not a given, right, and one approach even now is to, for instance, say,

203
00:16:55,520 --> 00:17:01,240
let's ring fans just 1% of your customers, right, and let's see, you know, the difference

204
00:17:01,240 --> 00:17:06,520
of this new approach and contrast it with the control group of 99%, just to make sure

205
00:17:06,520 --> 00:17:12,080
that you can trust it, and you know, that's when you see the benefits, and they're not

206
00:17:12,080 --> 00:17:17,800
like marginal benefits, so you know, that sort of propels the project forward.

207
00:17:17,800 --> 00:17:25,280
So one of the other topics that I hear come up a lot in the context of enterprise applications

208
00:17:25,280 --> 00:17:31,080
of AI as the whole idea of explainability or transparency.

209
00:17:31,080 --> 00:17:39,200
This is coming to the fore, you know, a bit more of a pointed fashion in Europe in the context

210
00:17:39,200 --> 00:17:44,920
of GDPR, and I'm wondering if this is something that you run into with the companies that

211
00:17:44,920 --> 00:17:46,720
you're talking to?

212
00:17:46,720 --> 00:17:50,360
Yes, and that's very important.

213
00:17:50,360 --> 00:17:55,920
So we have GDPR sort of on the European side, which is many things, but one of the things

214
00:17:55,920 --> 00:18:03,280
is that, you know, you need to be able to explain any kind of AI or algorithm that is making

215
00:18:03,280 --> 00:18:07,840
meaningful decisions, so that's obviously a lot of wiggle space around what is meaningful,

216
00:18:07,840 --> 00:18:16,120
but clearly, you know, you can imagine that this is about, you know, alone or some big

217
00:18:16,120 --> 00:18:20,680
financial stake or risk calculations, that is a meaningful decision.

218
00:18:20,680 --> 00:18:28,760
So that's GDPR, that's one thing, but equally outside Europe as well, using AI as a black

219
00:18:28,760 --> 00:18:33,200
box for these, especially for the larger companies, these large enterprise companies is just

220
00:18:33,200 --> 00:18:34,200
too high a risk, right?

221
00:18:34,200 --> 00:18:42,400
So they have a whole compliance process around AI and explainability is very important.

222
00:18:42,400 --> 00:18:47,800
And for instance, one approach that we're taking really at the core of the technology

223
00:18:47,800 --> 00:18:51,680
is to give controls that are not naive about this, right?

224
00:18:51,680 --> 00:18:57,560
You can obviously say, let's insist on transparent AI, which obviously, you know, as everybody

225
00:18:57,560 --> 00:19:00,680
will understand clearly, will come at the cost, right?

226
00:19:00,680 --> 00:19:04,960
I mean, transparency because it's a cost.

227
00:19:04,960 --> 00:19:09,680
It means that the algorithm cannot be as effective as it could otherwise be, so you can't

228
00:19:09,680 --> 00:19:16,160
just switch it off and say, oh, you can only do regression models or whatever it is.

229
00:19:16,160 --> 00:19:22,040
So we believe that at the core of these AI systems, there needs to be a control that allows

230
00:19:22,040 --> 00:19:25,680
much more granular control around transparency.

231
00:19:25,680 --> 00:19:34,120
Say, for instance, for risk or more regulatory areas, you may really insist on transparent

232
00:19:34,120 --> 00:19:36,640
models and we'll define that in a moment.

233
00:19:36,640 --> 00:19:42,840
Whereas maybe in marketing or in image recognition, you are, you know, you can go all out with

234
00:19:42,840 --> 00:19:48,360
your, you know, opaque algorithms like deep learning or, you know, XG boost models or

235
00:19:48,360 --> 00:19:53,680
assemble models, all of these things that are, you know, become much more intractable.

236
00:19:53,680 --> 00:20:00,600
So that's a discussion we have a lot around AI and I think the trust in AI is really important

237
00:20:00,600 --> 00:20:03,080
to make this whole thing fly.

238
00:20:03,080 --> 00:20:05,840
And you mentioned that you would define transparency.

239
00:20:05,840 --> 00:20:10,160
How do you articulate that to customers?

240
00:20:10,160 --> 00:20:14,880
Yeah, so there are multiple ways, obviously, the naive way of this is basically tagging

241
00:20:14,880 --> 00:20:15,880
the algorithm.

242
00:20:15,880 --> 00:20:23,520
I mean, they're obviously, you know, like deep learning algorithms are notoriously opaque.

243
00:20:23,520 --> 00:20:30,240
But more technically, what we do is basically look at the model output and then reverse engineering

244
00:20:30,240 --> 00:20:35,800
it with transparent methods like, for instance, decision trees and then the complexity of

245
00:20:35,800 --> 00:20:41,640
a decision tree that is trying to reverse engineer the model then becomes the size of that

246
00:20:41,640 --> 00:20:48,200
decision tree then becomes a proxy for transparency or opacity really.

247
00:20:48,200 --> 00:20:50,000
Can you elaborate on that a bit?

248
00:20:50,000 --> 00:20:59,000
So you're training arbitrary models independent of kind of this transparency versus opacity

249
00:20:59,000 --> 00:21:04,200
metric and then you're like fitting a decision tree to them and depending on the complexity

250
00:21:04,200 --> 00:21:09,200
of the resulting decision tree, that tells you whether the models should be classified

251
00:21:09,200 --> 00:21:11,200
as transparent versus opaque.

252
00:21:11,200 --> 00:21:12,200
Did I get that right?

253
00:21:12,200 --> 00:21:14,600
Yeah, that's basically, that's basically correct.

254
00:21:14,600 --> 00:21:20,080
So obviously we don't apply to any, you know, area, but especially around customer engagement.

255
00:21:20,080 --> 00:21:25,000
So what we're doing, we're getting all of these predictions of behavior from neural

256
00:21:25,000 --> 00:21:30,640
mass or genetic algorithms or, you know, more transparent algorithms like decision trees

257
00:21:30,640 --> 00:21:36,800
or regression models, whatever the case may be, some may be built by data scientists,

258
00:21:36,800 --> 00:21:44,760
some may be built automatically by the AI, but basically once we get all of these scores

259
00:21:44,760 --> 00:21:50,960
or all of these propensities, we are done going to fit it, as you said, with a transparent

260
00:21:50,960 --> 00:21:58,040
algorithm and that in itself will give us a good view of, you know, how opaque it

261
00:21:58,040 --> 00:21:59,040
is.

262
00:21:59,040 --> 00:22:07,800
So it sounds like underneath or kind of embedded in this process is some, I don't know

263
00:22:07,800 --> 00:22:14,200
if you met an optimizer or some model, some auto ML, I guess, is one way to describe

264
00:22:14,200 --> 00:22:18,120
it that is, you've got to, you've got a problem.

265
00:22:18,120 --> 00:22:24,840
You're testing a bunch of different models or training a bunch of different models on

266
00:22:24,840 --> 00:22:30,000
this data and you coming up with parameterized models.

267
00:22:30,000 --> 00:22:35,160
And each of those, you know, on the one hand, you know, you've got some model that's the

268
00:22:35,160 --> 00:22:36,160
best model.

269
00:22:36,160 --> 00:22:40,920
It may be a model that's not natively transparent.

270
00:22:40,920 --> 00:22:47,000
And if that's the case, then you walk back from that model using this process of training

271
00:22:47,000 --> 00:22:53,280
a decision tree on it to get like the, you know, the closest transparent model to that

272
00:22:53,280 --> 00:22:54,280
model.

273
00:22:54,280 --> 00:22:55,280
Yeah.

274
00:22:55,280 --> 00:23:00,880
That's one way of, there are actually multiple approaches.

275
00:23:00,880 --> 00:23:02,320
So that's one way of doing it.

276
00:23:02,320 --> 00:23:05,600
And basically, you say, this is the closest transparent model.

277
00:23:05,600 --> 00:23:12,320
The other way of doing it is saying, well, whenever these two models agree on a particular

278
00:23:12,320 --> 00:23:16,560
prediction or whatever classification or whatever it is, if they agree, we'll just take

279
00:23:16,560 --> 00:23:22,960
the transparent model and then we only worry about the 5% of cases where they disagree.

280
00:23:22,960 --> 00:23:26,160
And then we make a choice based on the area we're in.

281
00:23:26,160 --> 00:23:30,720
So if we're in a regulatory area where we have to explain this, maybe GDPR, maybe it's

282
00:23:30,720 --> 00:23:37,960
our own compliance office as in these large companies will then actually go either all

283
00:23:37,960 --> 00:23:43,560
the way with the transparent model or that's where we switch to, you know, a more opaque

284
00:23:43,560 --> 00:23:50,040
model and take the risk that we don't completely understand how it works, but only for a small

285
00:23:50,040 --> 00:23:52,520
percentage of the, of the data points.

286
00:23:52,520 --> 00:23:58,120
There are a lot of activities, both on a research side as well as in companies that are using

287
00:23:58,120 --> 00:24:08,040
AI to try to create explainability out of otherwise opaque algorithms through the, kind

288
00:24:08,040 --> 00:24:11,760
of the architecture of those algorithms, are you involved in that as well?

289
00:24:11,760 --> 00:24:12,760
Yes.

290
00:24:12,760 --> 00:24:17,120
So we're trying to, and honestly, we're also going to see sort of where the market goes,

291
00:24:17,120 --> 00:24:18,120
right?

292
00:24:18,120 --> 00:24:24,280
But to our own technology in this case, so we currently look at like, you know, reverse

293
00:24:24,280 --> 00:24:33,680
engineering, these basically these scoring bins with decision tree algorithm as a good proxy.

294
00:24:33,680 --> 00:24:40,040
But and maybe that's good enough, but there are other algorithms that, you know, specifically

295
00:24:40,040 --> 00:24:45,640
are trying to reverse engineer these models and if they work better, we'll use those.

296
00:24:45,640 --> 00:24:52,600
So I think the point for us as sort of an enterprise AI vendor for around customer engagement

297
00:24:52,600 --> 00:24:56,560
is that we need to give these companies the control over this.

298
00:24:56,560 --> 00:25:00,960
Otherwise, they can't use it to the, to the largest extent.

299
00:25:00,960 --> 00:25:04,800
And that would be, you know, not a good thing for their customers and obviously for these

300
00:25:04,800 --> 00:25:05,800
organizations.

301
00:25:05,800 --> 00:25:06,800
Yeah.

302
00:25:06,800 --> 00:25:10,520
I guess that's an interesting topic in and of itself, kind of the, there's a, seems

303
00:25:10,520 --> 00:25:18,760
to be a broader debate around even asking this question, like even, you know, insisting

304
00:25:18,760 --> 00:25:26,520
on transparency or explainability for some of these algorithms, there seems to be at

305
00:25:26,520 --> 00:25:36,760
least in the Twitter echo chamber, a vocal group of folks that object to the idea of requiring

306
00:25:36,760 --> 00:25:45,120
transparency for algorithms, you know, in, in some sense, because, you know, I guess the

307
00:25:45,120 --> 00:25:52,360
argument is that it limits innovation, it provides unreasonable constraints, the example

308
00:25:52,360 --> 00:25:55,840
that you hear a lot is, hey, we don't know how the brain works, but we make decisions

309
00:25:55,840 --> 00:25:57,720
all the time and we're comfortable with that.

310
00:25:57,720 --> 00:26:01,280
You know, I'm wondering what your perspective is on kind of that broader argument.

311
00:26:01,280 --> 00:26:02,280
Yeah.

312
00:26:02,280 --> 00:26:07,800
So I think that's a fascinating debate and suddenly the scientist in me is, is definitely

313
00:26:07,800 --> 00:26:09,480
with that school.

314
00:26:09,480 --> 00:26:16,040
It's like, it's an, it's an, it's a constraint on AI that, you know, as a rule, if that would

315
00:26:16,040 --> 00:26:20,920
actually be, if we would legislate that away, you know, the opaque algorithms, I think

316
00:26:20,920 --> 00:26:26,160
that would be a bad thing, also not sustainable, by the way, because I think it's not just

317
00:26:26,160 --> 00:26:31,400
about business benefits and, and, and, and if we go, you know, maybe zoom in on this

318
00:26:31,400 --> 00:26:33,640
a little bit because I do think this is fascinating.

319
00:26:33,640 --> 00:26:40,120
I think a lot of people don't seem to realize or don't want to realize that even opaque

320
00:26:40,120 --> 00:26:45,760
AI, like the human brain perhaps, you know, if it makes better decisions, that's not always

321
00:26:45,760 --> 00:26:47,720
a good thing just for companies, right?

322
00:26:47,720 --> 00:26:52,240
It's also probably a good thing for, for humans, right?

323
00:26:52,240 --> 00:26:56,880
It may be that you are not getting the loan that you should otherwise get or it may be

324
00:26:56,880 --> 00:27:02,760
that you get diagnosed with something before a human can, and those are good things.

325
00:27:02,760 --> 00:27:10,320
So I'm also off to school that we should not, you know, rule out, opaque algorithms.

326
00:27:10,320 --> 00:27:14,920
I do, however, believe that certainly in the face where we're now, where, you know,

327
00:27:14,920 --> 00:27:21,240
the greater population needs to gain trust in these kind of things that we need to control

328
00:27:21,240 --> 00:27:26,760
mechanism that allows you to allow opaque algorithm where you are comfortable, and even then

329
00:27:26,760 --> 00:27:31,960
you need all sorts of bias tests and ethical tests at the end, but also I'll be able to

330
00:27:31,960 --> 00:27:38,400
restrict it with the same control algorithm or control mechanism in areas that are very

331
00:27:38,400 --> 00:27:39,400
heavily regulated.

332
00:27:39,400 --> 00:27:44,040
Otherwise, we would never see AI there.

333
00:27:44,040 --> 00:27:47,080
You mentioned bias and ethical test.

334
00:27:47,080 --> 00:27:51,000
This is something that I've been writing about a bit in my newsletter of late.

335
00:27:51,000 --> 00:27:55,080
I'm wondering what work you've been doing in that area.

336
00:27:55,080 --> 00:27:56,080
Yeah.

337
00:27:56,080 --> 00:28:00,720
So we have been, and I think you can tell from the emphasis on this, you know, on this

338
00:28:00,720 --> 00:28:06,200
called control mechanism, we really believe that for AI to be trusted and accepted and

339
00:28:06,200 --> 00:28:11,280
have, you know, large scale adoption, we do need these kind of control mechanisms.

340
00:28:11,280 --> 00:28:17,480
At the same time, even with these control mechanisms, we have the thing, so we call it revision

341
00:28:17,480 --> 00:28:23,240
management, which is the overall sort of police on any of these customer strategies that

342
00:28:23,240 --> 00:28:24,840
you put in place, right?

343
00:28:24,840 --> 00:28:30,160
It's not just about AI, it's also about performance, it's about all sorts of things.

344
00:28:30,160 --> 00:28:38,040
Because in our case, the decision layer is used at such a scale interacting with, you

345
00:28:38,040 --> 00:28:42,160
know, as I said, billions of interactions that you really need to get it right.

346
00:28:42,160 --> 00:28:47,760
So we have this sort of revision manager that makes sure that can, you know, that all of

347
00:28:47,760 --> 00:28:51,080
them QA is done.

348
00:28:51,080 --> 00:28:57,960
And at that level, we believe the ethical test and bias tests need to be formally incorporated.

349
00:28:57,960 --> 00:29:02,840
So you don't have just a business officer signing off on the business benefits and an IT

350
00:29:02,840 --> 00:29:07,000
manager signing off on the SLAs and all these kind of things.

351
00:29:07,000 --> 00:29:12,120
You should probably also have an ethical officer saying, listen, this is showing a bias

352
00:29:12,120 --> 00:29:17,640
that it goes beyond what we as a brand think is acceptable.

353
00:29:17,640 --> 00:29:23,320
And, you know, I think you should do that across the board, but especially if you allow

354
00:29:23,320 --> 00:29:28,240
some of the more opaque algorithms, which I think you should in some areas, I think without

355
00:29:28,240 --> 00:29:34,640
these tests, and not just as a kind of an afterthought, but part of your whole QA process,

356
00:29:34,640 --> 00:29:38,400
I think is critical for large companies.

357
00:29:38,400 --> 00:29:44,000
Because some of these new technologies and approaches get popularized, one of the things

358
00:29:44,000 --> 00:29:52,920
you start to see is new job titles emerge that kind of tell you where enterprises are going.

359
00:29:52,920 --> 00:29:58,120
So I remember when chief security officer and chief data officer came about, and now

360
00:29:58,120 --> 00:30:01,720
people are talking about, you know, whether there should be a chief AI officer, do you

361
00:30:01,720 --> 00:30:07,960
see many enterprises now that have a chief ethics officer or do you see that as something

362
00:30:07,960 --> 00:30:12,160
that's emerging how far away from something like that, or where does it have been lived

363
00:30:12,160 --> 00:30:14,920
within, you know, today's enterprise?

364
00:30:14,920 --> 00:30:20,200
Yeah, I think I haven't seen it tightly yet, but I think it's probably now with the

365
00:30:20,200 --> 00:30:25,480
compliance officer or maybe the risk officer, but probably the compliance officer.

366
00:30:25,480 --> 00:30:32,240
But I think we will definitely see that, especially when customers are interacting with AI

367
00:30:32,240 --> 00:30:33,640
more directly, right?

368
00:30:33,640 --> 00:30:38,920
And a lot of what I'm talking about, they don't necessarily see, right?

369
00:30:38,920 --> 00:30:43,160
They may see a personalized page, but you don't know the magic behind that, right?

370
00:30:43,160 --> 00:30:48,440
But when we see people talking to, you know, cognitive agents and chatbots, and it's

371
00:30:48,440 --> 00:30:53,840
very obvious that they are AI, I think people will become much more sensitive to that.

372
00:30:53,840 --> 00:31:01,240
And I can imagine, yeah, a chief AI officer or ethics officer is something we will see.

373
00:31:01,240 --> 00:31:09,720
But definitely when we do these large implementations in this revision process, an ethical sign-off

374
00:31:09,720 --> 00:31:11,600
is part of the process.

375
00:31:11,600 --> 00:31:16,760
And I can ignore it, but you know, that's what you should be putting in.

376
00:31:16,760 --> 00:31:26,240
And my sense is that the kind of science around making that ethical sign-off, you know,

377
00:31:26,240 --> 00:31:32,760
isn't very well developed at this point, how are companies managing that?

378
00:31:32,760 --> 00:31:35,200
How do they know when they should be signing off?

379
00:31:35,200 --> 00:31:43,440
What kind of process are they taking to determine whether an algorithm is, you know, ethically

380
00:31:43,440 --> 00:31:44,440
acceptable?

381
00:31:44,440 --> 00:31:48,840
Yeah, so I think that's currently, first of all, that's in its infancy, right?

382
00:31:48,840 --> 00:31:56,920
And what we are prescribing is basically looking at, like, you know, distributions of outcomes

383
00:31:56,920 --> 00:32:03,120
and then comparing it with, you know, the process, the more transparent process or the

384
00:32:03,120 --> 00:32:05,400
human process.

385
00:32:05,400 --> 00:32:10,680
But that only works, for instance, for instance, let's assume that your AI is used by a big

386
00:32:10,680 --> 00:32:12,920
bank and it's turning racist, right?

387
00:32:12,920 --> 00:32:14,520
Can easily happen.

388
00:32:14,520 --> 00:32:19,840
And obviously, and this is a discussion, I think, where we're now having a lot, with all

389
00:32:19,840 --> 00:32:25,080
the big data and also the efficiency of the AI, obviously race, you know, doesn't have

390
00:32:25,080 --> 00:32:31,120
to be, you know, in your data or gender or age for it, obviously, to, you know, to use

391
00:32:31,120 --> 00:32:32,120
these concepts.

392
00:32:32,120 --> 00:32:34,160
So anyway, that's, I think that's a given for this audience.

393
00:32:34,160 --> 00:32:40,040
But just to be precise, is that if you have an opaque algorithm and you have big data,

394
00:32:40,040 --> 00:32:45,760
there is no way that you can tell from the, you know, the algorithm that it is, you know,

395
00:32:45,760 --> 00:32:49,320
it is racist or has some other, you know, weirdness.

396
00:32:49,320 --> 00:32:54,800
The thing is, you can only test that if you have a test for it.

397
00:32:54,800 --> 00:33:00,080
And that may mean if you really don't have some, and I'm just making a brace as an example.

398
00:33:00,080 --> 00:33:05,040
If you don't actually carry that in your database, you cannot, you know, test the distribution,

399
00:33:05,040 --> 00:33:12,480
get a loan, see if it's materially or statistically significantly different from your desired policy,

400
00:33:12,480 --> 00:33:16,120
which should be, obviously, completely, politically correct.

401
00:33:16,120 --> 00:33:23,000
So that presents a little bit of a catch 22 and that I imagine organizations have strived,

402
00:33:23,000 --> 00:33:28,320
have strived to keep those criteria out of their databases or is that not the case?

403
00:33:28,320 --> 00:33:29,320
No.

404
00:33:29,320 --> 00:33:30,320
So yes.

405
00:33:30,320 --> 00:33:34,000
So for some of these things like, like, you know, age and gender, they probably have

406
00:33:34,000 --> 00:33:37,160
it for other purposes, so you can do it for other things.

407
00:33:37,160 --> 00:33:41,200
And this could be sexual orientation or political affiliation, all of these things.

408
00:33:41,200 --> 00:33:48,360
You probably will need a panel, you know, so you actually have a specific group of representative

409
00:33:48,360 --> 00:33:51,480
group of customers that you do ask for these attributes.

410
00:33:51,480 --> 00:33:55,560
So you can check your bias and your algorithm.

411
00:33:55,560 --> 00:34:00,480
But I haven't, I haven't seen that as part of a formal process yet.

412
00:34:00,480 --> 00:34:01,480
Okay.

413
00:34:01,480 --> 00:34:02,480
Okay.

414
00:34:02,480 --> 00:34:12,840
So you mentioned, I forgot the terms you use, but you in describing the process for kind

415
00:34:12,840 --> 00:34:16,800
of putting the final stamp of approval on a model.

416
00:34:16,800 --> 00:34:21,200
You talked about some of the testing steps and, you know, all this brings to mind the

417
00:34:21,200 --> 00:34:28,520
broader context of scaling and operationalizing AI within the enterprise.

418
00:34:28,520 --> 00:34:36,080
What are some of the things that you're seeing with regards to putting these models into

419
00:34:36,080 --> 00:34:42,040
production and how do they correlate to, are we learning things from, you know, other,

420
00:34:42,040 --> 00:34:47,480
you know, scaling and operationalizing processes like DevOps on the application development

421
00:34:47,480 --> 00:34:52,520
side or, you know, how how evolved and mature is this process now?

422
00:34:52,520 --> 00:34:53,520
Yeah.

423
00:34:53,520 --> 00:34:58,280
I think this is sort of more the area of like decision management, right?

424
00:34:58,280 --> 00:35:05,440
So it's this model execution layer that I think is getting is getting pretty mature, right?

425
00:35:05,440 --> 00:35:06,760
But I think that it's an important thing.

426
00:35:06,760 --> 00:35:12,720
If I look at the breakthroughs in AI, I'm not even thinking so much about the algorithmic

427
00:35:12,720 --> 00:35:13,720
sort of progression.

428
00:35:13,720 --> 00:35:18,720
And obviously we have seen, you know, very cool new algorithms, but, you know, a lot of

429
00:35:18,720 --> 00:35:20,360
it is still is still the same.

430
00:35:20,360 --> 00:35:25,000
I think the two things that have changed the most are the data that's available, both

431
00:35:25,000 --> 00:35:30,320
to learn, but also to apply the AI to in production and the speed, right?

432
00:35:30,320 --> 00:35:35,760
So we're not talking about taking like one predictive model into production, even if

433
00:35:35,760 --> 00:35:40,960
it's a complex predictive model, I'm talking about thousands of them, right?

434
00:35:40,960 --> 00:35:47,600
Thousands of them combined with all sorts of financial rules and cutoffs and thresholds

435
00:35:47,600 --> 00:35:54,720
and inventory and constraints, all of these things together are used multiple times in

436
00:35:54,720 --> 00:36:00,800
a single interaction to continuously predict or decide on this next best action.

437
00:36:00,800 --> 00:36:02,920
That's that's a big scale.

438
00:36:02,920 --> 00:36:09,360
If you're a company, the size of sprint or one of the one of the big banks, like Citibank

439
00:36:09,360 --> 00:36:16,520
or Royal Bank of Scotland, that's like such a volume of interaction to apply a thousand

440
00:36:16,520 --> 00:36:21,400
predictive models, thousands of rules, and then every single time something changes in

441
00:36:21,400 --> 00:36:26,320
the context, like a customer says, yes or no, or maybe or this is too expensive or clicks

442
00:36:26,320 --> 00:36:29,680
or doesn't click, and then you recalculate it.

443
00:36:29,680 --> 00:36:36,200
That was just completely impossible just maybe five, six years ago.

444
00:36:36,200 --> 00:36:43,840
And that's I think also makes AI and control over AI so important right now.

445
00:36:43,840 --> 00:36:51,080
And so are you are you seeing companies do specific things to try to wrangle, you know,

446
00:36:51,080 --> 00:36:54,920
all of these models that they have in production, is that something that your tools are doing

447
00:36:54,920 --> 00:36:59,040
or you're seeing them using open source tools, what do you think the landscape looks

448
00:36:59,040 --> 00:37:00,040
like?

449
00:37:00,040 --> 00:37:04,680
Now, well, what I'm seeing is like I think the open source tools still specifically tend

450
00:37:04,680 --> 00:37:09,560
to, you know, the mostly I think the data scientist community, right?

451
00:37:09,560 --> 00:37:14,880
So they're obviously getting very good at, you know, getting you all sorts of algorithms.

452
00:37:14,880 --> 00:37:19,600
And I think this honestly, especially on the big cloud platforms will become much more

453
00:37:19,600 --> 00:37:24,160
of a commodity, right, where they will just have, you know, APIs around predictive modeling

454
00:37:24,160 --> 00:37:26,160
and all sorts of things.

455
00:37:26,160 --> 00:37:34,360
I think where it gets a lot more complex and challenging is to have a company in which

456
00:37:34,360 --> 00:37:40,120
the business people themselves are trained enough and confident enough to build these big

457
00:37:40,120 --> 00:37:42,440
customer strategies looking at evidence, right?

458
00:37:42,440 --> 00:37:44,480
And this is a very big pivot.

459
00:37:44,480 --> 00:37:47,720
So this brings us all the way back to this one to one, right?

460
00:37:47,720 --> 00:37:52,880
If you figure out for one person and you have to execute a thousand propensity models

461
00:37:52,880 --> 00:37:56,440
and then all of your rules and constraints and all these kind of things to defigure

462
00:37:56,440 --> 00:37:58,560
out what the next best action is.

463
00:37:58,560 --> 00:38:03,000
And then the customer says, yeah, but I think it should be a hundred dollars less.

464
00:38:03,000 --> 00:38:04,440
I'm just making this up.

465
00:38:04,440 --> 00:38:09,520
And we recalculate if that's acceptable at that scale, when you have like 50 million

466
00:38:09,520 --> 00:38:15,520
customers using all of these different channels, that I think is the challenge.

467
00:38:15,520 --> 00:38:20,800
And I think there's no, I haven't seen any open source thing dealing with that.

468
00:38:20,800 --> 00:38:28,960
I think where we're looking at, for instance, is basically abstracting from whatever tool

469
00:38:28,960 --> 00:38:32,760
or set of tools are giving us the predictive insight because we think that will

470
00:38:32,760 --> 00:38:35,040
become a commodity, right?

471
00:38:35,040 --> 00:38:39,480
We complement it with lots of very high-scale self-learning stuff, but I think in the end

472
00:38:39,480 --> 00:38:44,760
that becomes a commodity and it's building decisions out of that and make that transparent

473
00:38:44,760 --> 00:38:49,640
and do that at the scale so that these are huge companies can use it in every single

474
00:38:49,640 --> 00:38:51,160
interaction channel.

475
00:38:51,160 --> 00:38:54,080
I think that's not easy.

476
00:38:54,080 --> 00:38:57,840
And I don't think open source will get us there quickly.

477
00:38:57,840 --> 00:39:02,720
So just to make sure I understand your perspective on that, the part that you're abstracting

478
00:39:02,720 --> 00:39:09,680
away at the level that you're thinking about it is the underlying, kind of modeling,

479
00:39:09,680 --> 00:39:13,960
model building and perhaps even the models themselves.

480
00:39:13,960 --> 00:39:22,560
And then the part where you mentioned the part that you're focused on is the decisioning

481
00:39:22,560 --> 00:39:23,560
piece.

482
00:39:23,560 --> 00:39:29,280
But there, I guess it strikes me that there is a middle piece which is kind of the scaling

483
00:39:29,280 --> 00:39:31,320
and operationalization.

484
00:39:31,320 --> 00:39:35,160
Does that exist today, is that something that we're still as an industry building and

485
00:39:35,160 --> 00:39:40,240
figuring out in your perspective or where do you see that evolving?

486
00:39:40,240 --> 00:39:42,360
No, I think we're there.

487
00:39:42,360 --> 00:39:47,280
Certainly we are doing that for the large companies, but we are consuming and this is one of

488
00:39:47,280 --> 00:39:48,280
the challenges, right?

489
00:39:48,280 --> 00:39:56,400
We are consuming these predictive models if somebody loves R or one of the modeling tools.

490
00:39:56,400 --> 00:39:57,400
We would consume that.

491
00:39:57,400 --> 00:40:02,920
There's an exchange format called PMML that you can use to sort of help standardize that.

492
00:40:02,920 --> 00:40:07,640
The thing is, if you want to have AI sort of at the forefront of customer engagement,

493
00:40:07,640 --> 00:40:12,000
you need to be able to execute these models contextually, contextually.

494
00:40:12,000 --> 00:40:20,160
So we're no longer, you know, in an area or a time where we would have big, bad jobs

495
00:40:20,160 --> 00:40:25,080
and all of these models would put scores in databases and then we'd retrieve the scores.

496
00:40:25,080 --> 00:40:30,480
So now we know the likelihood you're going to buy this is 0.8 and the risk is 0.2 because

497
00:40:30,480 --> 00:40:35,440
we need to execute them contextually like we do in a really human conversation.

498
00:40:35,440 --> 00:40:41,080
So the models need to be executed contextually and that, I think, is much more of the technical

499
00:40:41,080 --> 00:40:46,720
challenge than getting the models in the first place, although at the moment we still feel

500
00:40:46,720 --> 00:40:51,600
like there's a lot of model laboratories out there, right?

501
00:40:51,600 --> 00:40:55,480
These tools where data scientists can have 100 different algorithms to build the better

502
00:40:55,480 --> 00:40:58,400
they want, which is cool and we will consume that.

503
00:40:58,400 --> 00:41:03,520
But a model factory to actually give the business, you know, the throughputs and the volume

504
00:41:03,520 --> 00:41:08,640
of models that they need, probably not there yet, but I think that will become a commodity.

505
00:41:08,640 --> 00:41:09,640
Awesome.

506
00:41:09,640 --> 00:41:10,640
Awesome.

507
00:41:10,640 --> 00:41:16,040
Well Rob, to wrap up, do you have any final thoughts that you'd like to share with the audience?

508
00:41:16,040 --> 00:41:18,280
I think we touched on a lot.

509
00:41:18,280 --> 00:41:22,240
We've got a lot of ground.

510
00:41:22,240 --> 00:41:23,640
All right.

511
00:41:23,640 --> 00:41:24,640
Awesome.

512
00:41:24,640 --> 00:41:27,040
Well, it's been great getting to chat with you about this stuff.

513
00:41:27,040 --> 00:41:32,600
There is certainly a lot of interesting things happening as enterprises wrap their arms

514
00:41:32,600 --> 00:41:38,600
around AI, but also there's, you know, they've, you know, while AI is kind of the hot new

515
00:41:38,600 --> 00:41:42,960
term, there's, you know, they've certainly been working on analytics, decision management,

516
00:41:42,960 --> 00:41:47,280
predictive stuff for quite a while and it's interesting to see how these things co-evolve

517
00:41:47,280 --> 00:41:48,280
together.

518
00:41:48,280 --> 00:41:51,640
So thank you for taking some time to share your perspective with us.

519
00:41:51,640 --> 00:41:52,640
You're very welcome.

520
00:41:52,640 --> 00:42:17,000
Thank you.

521
00:48:52,640 --> 00:49:09,640
Thanks again to Rob and Pegasystems for sponsoring this show.

522
00:49:09,640 --> 00:49:14,760
And of course, make sure you head on over to pegaworld.com to learn more about the conference

523
00:49:14,760 --> 00:49:21,400
and be sure to use the code PW1820 for 150 off of registration.

524
00:49:21,400 --> 00:49:24,200
Thanks so much for listening and catch you next time.

