WEBVTT

00:00.000 --> 00:11.720
All right, everyone. Welcome to another episode of the Twomel AI podcast. I am, of course,

00:11.720 --> 00:18.640
your host, Sam Charington. And today I'm joined by Shayan Mahanti, CEO of Watchful. Before

00:18.640 --> 00:22.160
we get going, be sure to take a moment to hit that subscribe button wherever you're

00:22.160 --> 00:27.960
listening to today's show. Shayan, we are going to spend some time talking about a topic

00:27.960 --> 00:34.920
that we both enjoy chatting about. So this should be a fun one. Data-centric AI in particular.

00:34.920 --> 00:38.400
But before we do, I'd love to have you share a little bit about your background.

00:38.400 --> 00:43.640
Awesome. Thanks so much for having me, Sam. My name is Shayan. I'm the CEO and co-founder

00:43.640 --> 00:48.320
of a company called Watchful. And we're building the platform for machine teaching, which

00:48.320 --> 00:53.520
is very much like a data-centric AI approach. And I'll talk about that a little bit later,

00:53.520 --> 01:01.920
I assume. But prior to this, I built systems at Facebook. So I was a tech lead for a stream

01:01.920 --> 01:07.400
processing team that processed all the ads-metrics data for all Facebook products. And I led

01:07.400 --> 01:12.480
some machine learning teams there as well. I'm also a guest scientist at Los Alamos National

01:12.480 --> 01:18.560
Labs, where I've given talks ranging from Atomada Theory to machine learning. So really, really

01:18.560 --> 01:24.120
pleased to be here. Yeah, before we get too far into the conversation, give us a quick

01:24.120 --> 01:29.280
overview of Watchful. Yeah. So as I mentioned before, we're building what we call a machine

01:29.280 --> 01:36.320
teaching platform. Really, our goal is to build tools to help people largely automate the

01:36.320 --> 01:42.120
process of labeling data for machine learning. We kind of see that as the biggest bottleneck

01:42.120 --> 01:48.320
to getting machine learning into most organizations, most companies. And we're really trying

01:48.320 --> 01:53.880
to sort of close that gap and make it so that we can increase the number of machine teachers

01:53.880 --> 01:58.920
in the world, so to speak, or allow companies and organizations to solve their hardest problems

01:58.920 --> 02:03.960
using machine learning. When you mentioned that you've seen labeling

02:03.960 --> 02:09.360
be the biggest gap, but I want to ask you to elaborate on that. But at the same time,

02:09.360 --> 02:12.920
it's like we've been talking about that for a long time. It shouldn't be that much of

02:12.920 --> 02:19.200
a surprise. And yet, you know, here we are with Andrew and others having to name this

02:19.200 --> 02:25.040
challenge and talk about data centric AI. You know, share a little bit about your experience

02:25.040 --> 02:29.840
with, you know, how labeling is actually done in organizations and the kind of challenge

02:29.840 --> 02:35.160
that it represents. So I would say that they're kind of like two main ways that labeling

02:35.160 --> 02:41.600
is done today. The first is by crowd. So that's where you get mostly managed vendors. That's

02:41.600 --> 02:46.400
where you used to have mechanical Turk. And now it's, you know, largely upended by the

02:46.400 --> 02:53.600
scales and appens of the world and so on. You can think of that shape of process as you

02:53.600 --> 02:57.800
have some data, you hand your data to an army of humans, and that army of humans labels

02:57.800 --> 03:04.640
it for you and hands it back. That works really well for tasks that require almost no context.

03:04.640 --> 03:10.240
So you want to do things like box stop signs or box pedestrians or you want to say, you

03:10.240 --> 03:16.920
know, get some sentiment on some tweets or something like that. That works really well.

03:16.920 --> 03:22.040
The second primary way that labeling is done today is by bringing it in house. So that's

03:22.040 --> 03:28.960
where you have vendors that provide interfaces for managing your own army of humans or that's

03:28.960 --> 03:34.160
kind of where we play, which is kind of like in the auto labeling space where the goal

03:34.160 --> 03:40.320
is instead of having an army of humans to begin with, you could have the one or two experts

03:40.320 --> 03:47.120
who are really like the de facto domain sort of like knowledge owners of a particular

03:47.120 --> 03:52.440
task or particular set of problems and have them label the data. But instead of requiring

03:52.440 --> 03:57.560
30 of them to spend a month in a room just labeling data, instead ideally they should

03:57.560 --> 04:01.960
only have to spend maybe four hours across two people and output the same quantity and

04:01.960 --> 04:07.200
the same quality of data, but several orders of magnitude faster. So that's kind of the

04:07.200 --> 04:14.440
space that we play. Now we're seeing more of a shift to kind of like this ladder way

04:14.440 --> 04:20.280
of labeling. What we found is that a lot of the time the things that you'd otherwise want

04:20.280 --> 04:25.760
to use crowd labeling for actually for the most part has been solved by way of these

04:25.760 --> 04:30.840
like foundation models. So now you've got open AI building things like GPT-3 and so

04:30.840 --> 04:36.080
on where you can largely just reuse those models for a lot of like the common tasks that

04:36.080 --> 04:40.600
you'd otherwise be using an army of humans for. Now obviously like in computer vision

04:40.600 --> 04:45.000
there's just like a longer tail on a lot of those tasks so that might not be the case.

04:45.000 --> 04:49.720
But especially in language, you can for the most part these days use a foundation model

04:49.720 --> 04:57.280
for a lot of these common tasks and not have to worry too much about army of human labeling.

04:57.280 --> 05:03.040
What gets really interesting is when you want to fine tune that foundation model on your

05:03.040 --> 05:10.040
task and that then requires bringing that labeling task in house and really trying to formulate

05:10.040 --> 05:14.280
what exactly the data set needs to look like in order to get good performance out of your

05:14.280 --> 05:18.440
system. And that's where data centric AI really can help.

05:18.440 --> 05:22.120
It strikes me that you're making a bit of a leap there that I'd love for you to elaborate

05:22.120 --> 05:27.840
on. Why is it that fine-tuning one of these foundation models requires you bringing

05:27.840 --> 05:31.760
the labeling in house. I suspect it has something to do with this idea of context that you

05:31.760 --> 05:32.760
refer to.

05:32.760 --> 05:38.760
Yeah, exactly. And again like what I'm not going to say is that every use case requires

05:38.760 --> 05:42.440
in house labeling. Like there are a fair number of them that you know you could you could

05:42.440 --> 05:45.920
just farm out to as many people as you'd like because it requires very little context.

05:45.920 --> 05:50.680
But the moment it requires context that is specific to your organization that's when

05:50.680 --> 05:55.200
you must bring it in house or you have to find some way to produce that context externally

05:55.200 --> 06:00.680
for your organization which frankly is like a philosophically hard problem. So that's

06:00.680 --> 06:06.000
where sort of like having as I mentioned you know your one or two experts who really

06:06.000 --> 06:11.080
are like the the de facto standard for how a particular problem is solved inside of

06:11.080 --> 06:12.080
your organization.

06:12.080 --> 06:16.240
If you could sit them down and have them label a very small amount of data and use that

06:16.240 --> 06:22.080
in the fine-tuning process that should yield a better system than if you were to try and

06:22.080 --> 06:26.240
capture that context or knowledge by way of like a set of instructions that you then

06:26.240 --> 06:29.480
hand to a bunch of people who have never done this task before.

06:29.480 --> 06:34.240
That's sort of the idea but then you get into all sorts of sort of interesting pitfalls

06:34.240 --> 06:41.240
where how much data is the right amount of data to get a good system you know like what

06:41.240 --> 06:46.120
parts of my data should I be focused on how much time should I be spending and in what's

06:46.120 --> 06:51.800
more is that these experts generally speaking have other things to do then sit there and

06:51.800 --> 06:56.520
label data all day right you know doctors have doctor things to do lawyers have lawyers

06:56.520 --> 07:02.320
things to do so like what you want to do is make sure that you're using their time as

07:02.320 --> 07:07.040
efficiently as possible and data centric AI is really just like a framework for how to

07:07.040 --> 07:08.720
think about all these things.

07:08.720 --> 07:13.440
And when you think about data centric AI in terms of a framework like how do you parse

07:13.440 --> 07:15.720
out the pieces of it?

07:15.720 --> 07:21.080
So it's kind of like a hard question I can tell you the way I think about it right which

07:21.080 --> 07:27.280
is they're kind of like two primary like there's one big decision you have to make which

07:27.280 --> 07:35.680
is do I want big data or do I want small data that's kind of like again a philosophically

07:35.680 --> 07:39.720
hard question answer but let me let me try and like unpack this.

07:39.720 --> 07:43.880
Not all data is made equal when we're talking about machine learning training a lot of

07:43.880 --> 07:47.640
the time when you're training on several millions of rows or several several million

07:47.640 --> 07:52.600
data points the vast majority of them are actually not moving the needle that much for

07:52.600 --> 07:53.960
a model.

07:53.960 --> 07:58.480
So that begs the question okay if I could just filter out the vast majority of my data

07:58.480 --> 08:02.320
especially so if I can interrupt especially so if you're starting with a pre-trained

08:02.320 --> 08:07.200
model totally that's already been trained on the general exactly right exactly so you

08:07.200 --> 08:12.480
want to like narrow in on on on the specific right but now the question is okay if I have

08:12.480 --> 08:17.520
a small data set like the reason why you might want to train on a larger data set is because

08:17.520 --> 08:21.760
it includes more diversity in that data set or there's more of a chance that it includes

08:21.760 --> 08:25.320
more diversity there's obviously not that guarantee it really depends on the distribution

08:25.320 --> 08:31.240
of your data but there's a higher likelihood that a much larger sample will yield the right

08:31.240 --> 08:35.840
diverse sort of like properties that you'd want in your data set for your model to generalize

08:35.840 --> 08:39.960
well on the other hand if you artificially constrain the size of the data set you have

08:39.960 --> 08:47.920
to be very careful about not introducing unintended bias or lack of diversity in the data because

08:47.920 --> 08:52.040
again you're artificially limiting it so that's sort of like the first question you have

08:52.040 --> 08:56.800
to ask do you have the right make up of data the right make up of techniques to be able

08:56.800 --> 09:02.600
to select just a small portion of the data set such that you still get the same properties

09:02.600 --> 09:06.840
of diverse who you're looking for for generalization of your model and you're not risking

09:06.840 --> 09:13.160
very much on the other hand if your data set or your task requires large amounts of diversity

09:13.160 --> 09:18.280
and you're not confident that you can sub select you know a tiny portion that is both

09:18.280 --> 09:23.120
needle moving and highly diverse you might actually be better off training on a larger data

09:23.120 --> 09:29.600
set but then you need ways to label that larger data set so depending on which direction

09:29.600 --> 09:35.840
you go with that fork you might end up with different techniques you know I'll throw

09:35.840 --> 09:42.120
two examples out there like active learning is a very good technique for the sub selection

09:42.120 --> 09:47.920
and sort of like iteration on very small data sets while something like weak supervision

09:47.920 --> 09:57.320
may be better at larger scale data labeling sort of tasks so depending on which side of

09:57.320 --> 10:02.760
that equation you err on you start going down a different path that then takes you through

10:02.760 --> 10:06.880
several different techniques that you can kind of like pull together like Lego blocks the

10:06.880 --> 10:11.800
yield a very good labeled data set but the properties of that will actually depend on both

10:11.800 --> 10:16.800
your underlying data set type of model you're trying to train and you know kind of the task

10:16.800 --> 10:23.240
at hand now for completeness I'd love to have you explain active learning and weak supervision

10:23.240 --> 10:29.520
but I think before we do that it's interesting and it strikes me as interesting and important

10:29.520 --> 10:36.960
to really emphasize that data centric AI isn't a technique it's like a theme or an idea

10:36.960 --> 10:44.720
and and that is an umbrella for a number of techniques that could help an organization

10:44.720 --> 10:50.720
be more data centric which I guess we haven't defined but yeah yeah and your end calls it

10:50.720 --> 10:57.920
a movement yeah which is pretty much what it is I mean like frankly a lot of these techniques

10:57.920 --> 11:05.080
a lot of these like um processes are not new you know like they've been done for decades

11:05.080 --> 11:10.920
already at several companies the reason why it's an interesting movement is because while

11:10.920 --> 11:17.040
a select few have been able to sort of intuit a lot of this motion just by nature of working

11:17.040 --> 11:23.160
with their data and working with their models many haven't yet and it's a valuable process

11:23.160 --> 11:29.720
to sort of like capture that knowledge and bring it to bear on the rest of the world by

11:29.720 --> 11:35.000
wave like a movement or a framework or set of frameworks so that's I think what makes

11:35.000 --> 11:40.280
data centric AI so interesting it's the fact that it's not a technique it's not a selection

11:40.280 --> 11:45.400
of techniques in fact it's it's really just like a shift in mentality which then means

11:45.400 --> 11:51.320
that like data scientists don't necessarily have to learn brand new types of machine learning

11:51.320 --> 11:54.840
you know you don't have to learn a brand new skill set in order to apply this you really

11:54.840 --> 12:00.440
just have like shift your mentality a little bit and that's what makes it so interesting

12:00.440 --> 12:05.320
so explain active learning for us so active learning takes a lot of different shapes and forms

12:05.320 --> 12:12.200
I think one of the most common shapes is sort of this idea of uncertainty sampling so mechanically

12:12.200 --> 12:19.960
what you do is you have let's say a large data set and maybe some of it is really well

12:19.960 --> 12:25.000
labeled other parts or not what you do is you train a model on the parts that are already labeled

12:25.640 --> 12:31.480
and then you inference against the parts that are not and you measure how uncertain the model is

12:31.480 --> 12:36.600
on certain data points there and the ideas that the parts that are most uncertain are the parts

12:36.600 --> 12:40.840
where a human should probably look at it and evaluate and adjust the label as necessary so

12:41.400 --> 12:45.720
those are theoretically the data points that will move the needle the most for the model when

12:45.720 --> 12:50.120
it's retrained on them and you kind of keep doing this until eventually the model is actually doing

12:50.120 --> 12:55.560
a pretty good job of predicting across the entire data set so that's sort of active learning and

12:55.560 --> 13:01.880
again it's very very good for these like you know small data problems so to speak and again I'm

13:01.880 --> 13:05.720
painting with a very broad brush here because active learning can also be applied to very large data

13:05.720 --> 13:11.640
problems but it's specifically very very good at these small data problems where you have a very

13:11.640 --> 13:17.320
very large set of data and you want to know what part of it is likely the most interesting for you

13:17.320 --> 13:22.520
to sort of spend your time looking at as a human that's where active learning can be like uniquely

13:22.520 --> 13:28.680
valuable on the other hand weak supervision is a technique that's been gaining popularity in the

13:28.680 --> 13:36.120
last couple of years where the idea is that perhaps you don't have any like highly trustworthy labels

13:36.120 --> 13:44.200
so as an example your data might be so huge or so varied or just require an immense amount of

13:44.200 --> 13:49.480
subject matter expertise that it's very difficult to get any amount of like direct supervision

13:50.280 --> 13:56.680
in any meaningful way so what you might do is create labeling functions or these like weak

13:56.680 --> 14:02.760
learners or or you know sources of weak supervision and it's weak supervision because you can kind of

14:02.760 --> 14:08.040
think of these as like functions you know a very simple function could be like a regax like if I

14:08.040 --> 14:14.280
see the word credit card somewhere in this text that I know that this customer support ticket is

14:14.280 --> 14:19.720
very likely payment related right and then you rinse and repeat with several other patterns like

14:19.720 --> 14:25.560
if I see bank or invoice or if I see interest rate or whatever then then you know it has something

14:25.560 --> 14:33.480
to do with money so in a way these are kind of like heuristics you know they're rules of thumb

14:33.480 --> 14:37.160
they're they're no better than just saying like you know this thing is going to be noisy some

14:37.160 --> 14:42.600
percentage the time but note that you're not going row by row anymore you're not saying okay I

14:42.600 --> 14:47.400
have to sit down and say yes or no whether this is you know money related or not or payment related

14:47.400 --> 14:52.280
or not or whatever you're saying generally speaking if I see these words and maybe

14:52.280 --> 14:57.800
approximately this order then it is very likely payment related so by doing this you create a

14:57.800 --> 15:03.720
collection of these functions that you can then pass into a model that learns a label based on

15:03.720 --> 15:09.880
the presence or lack thereof certain labeling functions and kind of like the combinations and

15:09.880 --> 15:14.840
the contradictions between them and then it doesn't really matter how much data you're passing

15:14.840 --> 15:20.120
into the system you know you could be passing 2000 rows which would be easy or 20,000 rows or

15:20.120 --> 15:25.640
200,000 rows or two million rows it doesn't really matter because the idea is that as long as

15:25.640 --> 15:31.720
you've sampled the data in a way where it's representative of the larger whole then the functions

15:31.720 --> 15:37.000
you've created are still representative of the larger whole you know you can you can apply them

15:37.000 --> 15:42.120
to a larger sample as long as it's sampled in the same way and get the same or predictable results

15:42.120 --> 15:51.560
um but you can imagine that like weak supervision as a side effect is very very good at labeling

15:51.560 --> 15:57.000
for like the head of a distribution where there might be a lot of commonality you know you can create

15:57.000 --> 16:02.280
labeling functions that cover large quantities of data but as you get further and further into

16:02.280 --> 16:07.880
long tail it becomes harder and harder you know all of a sudden you're creating labeling functions

16:07.880 --> 16:13.880
that cover maybe one row at a time and that's like strictly no better than having hand labeled them

16:13.880 --> 16:17.800
you know it might even be worse because you have to like sit there and articulate a rule for it

16:19.160 --> 16:23.880
active learning on the other hand can can help direct you to where that long tail might be or at

16:23.880 --> 16:28.840
least the parts of the long tail that are most needle moving for your model yeah well it strikes me

16:28.840 --> 16:35.240
is super interesting about the about weak supervision uh and you know tell me if this this resonates

16:35.240 --> 16:42.600
in a lot of ways is kind of what you might do absent machine learning like you create a bunch of

16:42.600 --> 16:50.760
heuristics and you you know we probably wouldn't have called it label something but like you assign a

16:50.760 --> 16:58.120
tag or you categorize a transaction and you know so we know how to do that you know but the problem

16:58.120 --> 17:04.360
is always that you know using a rejects isn't going to be exact or if you need to make it exact or

17:04.360 --> 17:10.520
account for kind of noise then your system gets a whole lot more complex if you're trying to do it

17:11.400 --> 17:17.800
declaratively or you know imperatively but machine learning helps with that because now all of

17:17.800 --> 17:25.240
these you know imputed labels are kind of probabilistic and we're assuming noise and so the two

17:26.040 --> 17:31.240
the heuristic and the machine learning kind of marry at well in this idea of of weak supervision

17:31.240 --> 17:36.760
yeah that that's exactly right I think like the theme of this conversation I think is things that

17:36.760 --> 17:43.000
have been done for decades but haven't really had like labels put on them uh like we we're talking

17:43.000 --> 17:49.960
about expert systems and rule systems and and so on like you know there's a very classic motion

17:49.960 --> 17:53.960
in just like software engineering in general where it's like there's a very real business problem

17:54.600 --> 17:58.840
the way you solve it first is you throw humans at it right you like have a bunch of people

17:58.840 --> 18:04.200
just like go and manually do the thing eventually you build simple software to solve that the very

18:04.200 --> 18:09.400
first iteration of simple software will be things like rules you know but then very quickly you discover

18:09.400 --> 18:16.600
that your rules are brittle and like as as business logic changes and as your problem space changes

18:16.600 --> 18:22.760
and so on it becomes very difficult to manage just large quantities of rules that they overlap or

18:22.760 --> 18:30.040
conflict in various ways um so there's this like non-linear jump they end up having to make

18:30.040 --> 18:35.880
from an expert system or rule system to machine learning and that to me is one of the most

18:35.880 --> 18:40.360
interesting jumps that an organization can can make and that's sort of why weak supervision is so

18:40.360 --> 18:45.800
interesting it's because as you rightfully pointed out you can repurpose a lot of those rules

18:46.760 --> 18:52.440
as heuristics you know it's it's a simple mentality shift which then yields an implementation change

18:52.440 --> 18:59.000
but still it's like instead of talking about them as rules treat them as rules of thumb right just

18:59.640 --> 19:03.720
assume that they're heuristic in nature assume that they're going to be noisy in some unknowable

19:03.720 --> 19:09.640
capacity and build robustness around your system that way and then output some label data that

19:09.640 --> 19:15.000
you can then train a model on that will hopefully then learn an even more robust and generalized

19:15.000 --> 19:20.440
form of the relationship between the input and the output and that's where you get some like really

19:20.440 --> 19:26.440
meaningful gains and and I think that gap is one of the most interesting to traverse because

19:27.640 --> 19:35.160
prior to the formalization like weak supervision there was no linearization of the process going

19:35.160 --> 19:39.400
from like a rule system to machine learning like you would have to stop what you're doing and then

19:39.400 --> 19:43.480
like go hand your data to a crowd and then like you know you have to kind of go all in on machine

19:43.480 --> 19:48.760
learning at the moment you do that here there's like all of a sudden a nice incremental process

19:48.760 --> 19:53.400
that you can you can adhere to which makes weak supervision a very interesting solution

19:53.400 --> 19:58.760
for lots of organizations that find themselves in that sort of like chasm between I know I need to

19:58.760 --> 20:05.160
get to AI but I still have a bunch of like legacy rule systems or expert systems that I don't

20:05.160 --> 20:08.680
want to like lose because you're providing value how do I sort of bridge that gap yeah I think the

20:08.680 --> 20:13.640
other part of my intuition and tell me if this is your experience is that we think of machine learning

20:13.640 --> 20:19.800
as you know more complex than kind of the traditional way of doing things in some ways right in

20:19.800 --> 20:25.560
other ways know but I guess I'm trying to get it like building a really robust rule based system

20:25.560 --> 20:32.920
that can accommodate all of the corner cases and uncertainty in a noisy system it is hard and it's

20:32.920 --> 20:40.040
like bespoke and I you know I don't have any data to this but my sense is that a lot of organizations

20:40.040 --> 20:47.480
have plot a lot of time and money trying to handcraft this over and over again and this idea of

20:47.480 --> 20:55.240
you know weekly system for weak supervision provides a you know a kind of an elegant you know

20:55.240 --> 21:02.600
framework for a side stepping needing to make the rules piece bullet proof and still getting to

21:02.600 --> 21:09.240
the ultimate outcome that you're trying to to get to your intuition is absolutely correct I mean

21:09.240 --> 21:15.400
we've seen this time and time again where an organization will attempt to build the world's best

21:15.400 --> 21:23.720
set of rules and it always turns out that it's not the world's best set of rules like it's never good

21:23.720 --> 21:30.360
you know it's it's maybe good enough but like we as humans are like notoriously bad at being able

21:30.360 --> 21:35.880
to articulate edge cases like it's it's if we can articulate it very easily it's no longer an

21:35.880 --> 21:41.720
edge case you know it's something that we can write a rule for but that's where highly generalizable

21:41.720 --> 21:47.320
like deep learning models come into play that's the whole reason why you would train a model out

21:47.320 --> 21:52.840
of a weak supervision like process as opposed to just using the weak supervision process itself

21:52.840 --> 21:59.320
as the model it's because of that added level of generalization generalizability you would train

21:59.320 --> 22:06.360
a larger like you know deep learning model that has no concept of the rules of the heuristics that

22:06.360 --> 22:13.880
went into creating the labels all it sees are the label and the input data and ideally then it learns

22:13.880 --> 22:19.480
a very rich relationship between the input and and the projected output such that it you know

22:20.360 --> 22:24.120
it's not going to reverse engineer the rules because it has no concept of how many rules it

22:24.120 --> 22:28.920
took to create those labels it's going to learn something else and we hope that the thing it learns

22:28.920 --> 22:34.120
is actually more generalizable than the human input rules that went into that to begin with.

22:34.120 --> 22:43.400
Is there a an in-between kind of traditional rules and you know something more sophisticated

22:43.400 --> 22:49.480
that's like rule functions you know functions or that you were thinking of when you made that

22:49.480 --> 22:58.920
comment like how how far can you you know have you seen folks go before introducing the machine

22:58.920 --> 23:04.680
learning model part beyond just kind of rules engine maybe that's a way to get at it.

23:04.680 --> 23:10.440
Interestingly we actually have customers that are using watchful as their model in production

23:11.080 --> 23:16.920
despite us telling them hey like you know you should probably consider training a model

23:16.920 --> 23:22.680
and then using that model in production here here kind of like the properties to think about right

23:22.680 --> 23:28.600
it really depends on your use case and this is sort of you know my my co-founder hates it when

23:28.600 --> 23:35.960
I say this but it holds true it's like every data science problem the answer is it depends like

23:35.960 --> 23:39.960
it doesn't matter what the problem is the answer is always it depends so the answer here is it

23:39.960 --> 23:49.480
depends so if your data changes frequently you know from from time to time or if you find that

23:49.480 --> 23:54.360
the variability that you captured in the samples not truly representative the variability that you

23:54.360 --> 24:02.840
see in the total population of your data or if you need you know sort of like an additional layer

24:02.840 --> 24:08.840
of nuance that you find very difficult to capture by way of heuristics you know where perhaps you

24:08.840 --> 24:13.960
actually do want something that's pre-trained that you're then fine tuning and so on that's

24:13.960 --> 24:20.280
we're having a deep learning model trained after the fact is so helpful you know it it provides

24:20.280 --> 24:25.480
that additional level of granularity that additional level of generalizability that would be

24:25.480 --> 24:31.080
notoriously difficult for a human to articulate in heuristics again these are these are functions

24:31.080 --> 24:36.760
right these are functions that a human is writing in some capacity now there's obviously a lot of

24:36.760 --> 24:40.520
like cleverness that you can add on top of this to make sure that these functions are expressive

24:40.520 --> 24:49.320
and and very powerful and so on but at some point you start almost converging on like model

24:49.320 --> 24:55.720
activations you know a sufficiently complex function is just like you know almost no different

24:55.720 --> 25:00.520
from just like modeling a single activation in a model or something like that you know so

25:00.520 --> 25:08.280
there's also a trade-off between explainability and power as we've seen in in AI in general but

25:08.280 --> 25:14.280
you know obviously the same goes for weekly supervised systems it's as your functions become more

25:14.280 --> 25:20.360
and more powerful as they become more and more general they also lose some degree of explainability

25:21.480 --> 25:29.240
so again the answer is always it that it depends but here especially it depends because

25:29.240 --> 25:34.760
in some use cases you can get away with just creating a couple of heuristics that are really

25:34.760 --> 25:40.120
really good at labeling your data and all you really needed was that like additional layer of

25:40.120 --> 25:47.000
fuzziness almost that that embrace of noise in these heuristics as opposed to rules right and

25:47.000 --> 25:53.400
and that that might be good enough in other cases you might definitely rely on having some like

25:53.400 --> 25:59.240
you know pre-trained quote-unquote common sense in your model that is then used to sort of like

25:59.240 --> 26:06.280
boost overall like system performance so it kind of depends on your use case so we've had

26:06.280 --> 26:13.080
customers go the entire you know 10 miles that they had to go with just pure watchful we've

26:13.080 --> 26:17.800
had other customers trained very sophisticated models after the fact and get you know really really

26:17.800 --> 26:25.960
good results so yeah again it depends you mentioned previously this idea machine teaching

26:26.920 --> 26:31.800
can you elaborate on that is that kind of a you know an umbrella terminology that you're

26:31.800 --> 26:37.320
using for all of the things that we've discussed from active learning to week supervision or

26:38.520 --> 26:44.440
is it is it more or something different so machine teaching was actually a term coined by

26:44.440 --> 26:52.040
Microsoft Microsoft research specifically and so the best way to frame it relative to data center

26:52.040 --> 26:58.680
AI is that data center AI is very much a movement machine teaching is sort of a mentality coupled

26:58.680 --> 27:05.960
with like a set of strategies or proposed strategies so it becomes slightly more concrete machine

27:05.960 --> 27:15.720
teaching is a part of the data center AI movement it's just one layer of granularity deeper so the

27:15.720 --> 27:21.640
idea behind machine teaching is kind of simple it's that classical machine learning research

27:22.200 --> 27:29.160
has largely focused on building bigger and better machine learning models right it's like

27:29.160 --> 27:34.440
focusing on the student if you want to call it that and the idea is that you want to create a

27:34.440 --> 27:41.720
student that requires very little data to learn how to do the thing the problem is that is in

27:41.720 --> 27:47.800
large part very difficult to predict you know machine learning invention true like machine learning

27:47.800 --> 27:53.320
architecture sort of like innovation happens in almost like step wise motions so it's very

27:53.320 --> 27:59.080
difficult to predict when the next innovation is going to happen on the other hand

27:59.080 --> 28:05.400
the whole idea of machine teaching is if you could flip the coin and instead of focusing so much on

28:05.400 --> 28:11.800
the student focus more on the teacher and make it so that the person training this model or teaching

28:11.800 --> 28:17.960
it how to do the thing is several orders of magnitude more effective at doing that what does that

28:17.960 --> 28:21.800
world look like you know where it doesn't matter what student you're training you know it could be

28:21.800 --> 28:27.400
a very sophisticated deep learning model or it could be you know something more classical it shouldn't

28:27.400 --> 28:33.000
really matter because the same tool sets should be applicable in both cases where we're focusing a

28:33.000 --> 28:39.960
lot on making the teacher much more effective so techniques like weak supervision and active learning

28:39.960 --> 28:45.240
go into this as well as things like you know transfer learning and and even like

28:47.240 --> 28:53.080
like simulated data approaches and you know synthetic data rather like all of these things kind of

28:53.080 --> 29:01.640
belong under that umbrella where it's a very strong focus on taking what's in the head of an

29:01.640 --> 29:07.560
expert and applying that programmatically as much as possible to data which is then used to train

29:07.560 --> 29:13.400
models and not requiring you know 40 doctors to sit in a room for a month labeling data

29:14.200 --> 29:21.400
that's that's sort of the idea yeah I really appreciate the what I think I was the broadening

29:21.400 --> 29:29.160
of the term machine teaching I know very well the folks said bonsai which was a company that was

29:29.160 --> 29:35.480
acquired by Microsoft and they were kind of the spark of this machine teaching idea applied to

29:37.640 --> 29:43.160
reinforcement learning and industrial AI we collaborated on an industrial AI ebook and

29:43.160 --> 29:48.040
they were a long time client of mine and it's great to see that term resurface here in the context

29:48.040 --> 29:55.080
of data and data center AI yeah we think it's criminally underused we like we we stumbled upon

29:55.080 --> 29:59.800
the terminology in one of the research papers out of the Microsoft research lab after they got

29:59.800 --> 30:05.000
acquired and it just really resonated with us like that is exactly what we're building that's

30:05.000 --> 30:12.440
exactly what needs to be built for AI to make its way into most organizations so yeah absolutely

30:12.440 --> 30:16.680
I mean we're we're really excited about all the work that's happening in this space and it's

30:16.680 --> 30:20.840
it's becoming more like the concept of machine teaching is becoming more and more popular so

30:20.840 --> 30:29.080
I'm glad to see it's it's you know slow vital resurgence how do you see this fitting into the overall

30:29.960 --> 30:35.160
model development workflow when an organization you know has a problem you know they think machine

30:35.160 --> 30:41.720
learning is the solution to this you know do they start with rules do they start with

30:41.720 --> 30:47.720
do they need to think totally differently about the the way that they approach building models today

30:47.720 --> 30:53.960
yeah as I mentioned before it's it's more of like a mentality shift so it's less about like what

30:53.960 --> 31:00.120
technique they start about they start with it's more about matching the data to the architecture

31:00.120 --> 31:08.440
and and here's here's what I mean by that especially these days like machine learning coding

31:08.440 --> 31:12.840
for the most part is like kind of a solve problem you know I'm going to hand wave like a lot here

31:12.840 --> 31:19.160
so bear with me uh it's like you know with with with the very frameworks that exist there's Keras

31:19.160 --> 31:25.160
there's you know there's torch and and so I'm like uh there's several just like very nice

31:25.160 --> 31:30.520
commoditized ways to build almost any neural architecture you can imagine and even more than that

31:30.520 --> 31:35.720
they're like very nice architectures that you can just get off the shelf that will work for most

31:35.720 --> 31:42.520
problems so a lot of the time organizations don't really have to innovate on machine learning code

31:42.520 --> 31:49.880
itself uh so for all intents and purposes like a data centric AI perspective you can kind of hold

31:49.880 --> 31:57.640
the code static and you can iterate on the data to get better yield and that's where the shift

31:57.640 --> 32:04.680
and mentality comes you know it's yes I could have like an army of humans on standby and just give

32:04.680 --> 32:10.280
them data and like if if the data I get back is not good I tweak the rules or tweak the description

32:10.280 --> 32:16.040
just slightly to get you know perhaps better yield and so on but it's very much like again stepwise

32:16.040 --> 32:21.560
motion and each time you do that you might have a lag of like two weeks before you get your data back

32:21.560 --> 32:28.440
it's not a very efficient process so data centric AI is about not only focusing very carefully

32:28.440 --> 32:33.480
on what data and how you're getting it labeled and that sort of thing but it's also about

32:33.480 --> 32:39.080
the user experience or at least the experience around acquiring that labeled data how do I shorten

32:39.080 --> 32:47.320
the period of time between hey I need data to okay I've got good data and that process could

32:47.320 --> 32:52.120
you know utilize things like weak supervision depending on what your data looks like and that sort

32:52.120 --> 32:56.680
of thing it could use things like active learning it could use things like synthetic data it really

32:56.680 --> 33:02.760
depends on what you have at your disposal both from like an infrastructure and like machine learning

33:02.760 --> 33:08.440
ops perspective as well as from like a data distribution perspective like does your data lend itself

33:08.440 --> 33:12.520
to active learning particularly well does it lend itself to weak supervision particularly well does

33:12.520 --> 33:20.440
it lend itself perhaps better to synthetic data so really this is just about like talking about the

33:20.440 --> 33:27.320
data specifically as opposed to the model because good data can be used to train almost any model

33:27.320 --> 33:32.520
and again lots of hand waving here depending on the model architecture you might need more data

33:32.520 --> 33:38.200
you might need less data you know it depends on so many variables but the idea is that for the

33:38.200 --> 33:44.600
most part the hard thing about machine learning and the enterprise today is no longer the actual

33:44.600 --> 33:49.480
modeling component and it's actually on the data and it's just about thinking holistically about

33:49.480 --> 33:56.840
that now and do you find that that's the case uh you know hand waving notwithstanding kind of

33:56.840 --> 34:04.680
across the the most valuable of an organization's problems you know I've you know talked about this

34:04.680 --> 34:11.000
idea of model driven enterprise and like it's the anti commoditization of machine learning like yeah

34:11.000 --> 34:16.280
you can you know use machine learning in your organization and get some advantage if it's

34:16.280 --> 34:20.920
you know just built into all the stuff that you use but it's not really your models you know

34:20.920 --> 34:27.240
solving your problems for your data that's where the real values going to be uh is is that in line

34:27.240 --> 34:32.360
with what you're saying or yeah it's it's absolutely in line I don't know that this is a hot

34:32.360 --> 34:38.040
take anymore but I I firmly believe that in the next 10 years every company will have some sort

34:38.040 --> 34:43.640
of AI functionality internally um it's just an inevitability and the question ask is like

34:44.840 --> 34:50.040
why hasn't that happened yet you know like modeling techniques have again become largely

34:50.040 --> 34:55.640
commoditized already like why haven't we been able to bring AI into the fold in a meaningful way

34:55.640 --> 35:02.120
across all enterprises so that's sort of where our conviction is around the data piece it's that

35:02.120 --> 35:07.800
the hard part is not actually on the model architecture or the code aspect of this if you think

35:07.800 --> 35:15.240
about AI is just like code plus data it's exactly as you said it's like my hardest problems are around

35:15.240 --> 35:21.720
like is one part of the business where I got like one subject matter expert who knows how to perform

35:21.720 --> 35:27.400
this particular task no one else knows how to perform it except for this person I want to build a

35:27.400 --> 35:33.080
model to augment that person but very quickly I end up in a catch 22 where I would need that person

35:33.080 --> 35:38.600
to be in the process of labeling that data for that model but they're already a critical bottleneck

35:38.600 --> 35:44.280
bottleneck to my business so I can't have them sit in a room for like a month labeling data I

35:44.280 --> 35:49.640
really do need them to be productive on top of labeling data and that it just becomes very very

35:49.640 --> 35:54.360
hard to build a system that will actually help there so that's why a lot of the time if you do

35:54.360 --> 36:02.200
see enterprises kind of like trying AI they're usually trying it for use cases that are not the most

36:02.200 --> 36:07.800
needle moving you know that's why like the typical hello world for machine learning is like a

36:07.800 --> 36:13.080
Twitter sentiment analysis bot it's because it's like the most accessible data and it's not that

36:13.080 --> 36:17.720
the architecture is particularly simple a lot of the time the sentiment analysis bots like

36:17.720 --> 36:22.840
they use off-the-shelf architectures that are fine you know a lot of it is just around like the

36:22.840 --> 36:26.840
mechanics of acquiring the data cleaning it like going through process of training the model

36:26.840 --> 36:32.600
iterating on it and so on it's just availability of data so our conviction is that if we can make it

36:33.320 --> 36:40.040
dead simple for your expert to label your data in your environment as quickly as humanly

36:40.040 --> 36:46.280
possible without requiring them to like take time away from the things that they're already a bottleneck

36:46.280 --> 36:52.200
day-to-day for then we can price organizations in to actually using AI for solving their hardest

36:52.200 --> 36:57.720
problems that's that's kind of like the laser focus we have you know maybe an insight you know

36:57.720 --> 37:03.640
out of this question in your responses that model is kind of an overloaded term and

37:03.640 --> 37:15.720
you can use an off-the-shelf model architecture off-the-shelf code but the thing that makes you know

37:15.720 --> 37:24.280
after you apply your kind of proprietary data to those things you have a proprietary high-value

37:24.280 --> 37:31.640
model at the you know coming out of the process that may be based on a commodity architecture

37:31.640 --> 37:37.560
yeah that that's exactly right generally speaking again like I'm I'm very excited about like

37:37.560 --> 37:43.080
these massive foundation models that are being built and I still think that we need more of these

37:43.080 --> 37:50.040
foundation models across other data modalities and other tasks so I am certainly not pushing against

37:50.040 --> 37:55.240
the idea of these foundation models that use lots of generic label data to produce you know

37:55.240 --> 38:05.080
generally available good insight the hard part is marrying that to specific use cases and a lot

38:05.080 --> 38:12.040
of the time you can use a GPT-3 for like very interesting use cases but in order to match that

38:12.040 --> 38:18.520
with enterprise value you have to marry it to data or a use case that is unique to that enterprise

38:18.520 --> 38:23.480
most of the time and that's where like that whole data centric movement becomes very interesting

38:23.480 --> 38:28.280
because then you can marry the foundation model with very specific very curated data that you

38:28.280 --> 38:33.240
can then combine to fine tune a model that again becomes proprietary it could be based on this

38:33.240 --> 38:37.800
like massive architecture that you kind of like repurpose from somewhere else but the piece that

38:37.800 --> 38:42.680
makes it yours is the fact that it was trained on your data for your task by your people what what

38:42.680 --> 38:53.080
do organizations need to be successful in making this mindset shift I think like the big thing is

38:53.080 --> 38:57.720
just like this is kind of again going to be hand wave but just like being open to it that's like

38:58.440 --> 39:04.520
like the biggest hurdle it's like a lot of the time folks get entrenched in the way things have

39:04.520 --> 39:09.400
been done for a very long time and it's very difficult to break out of it one of the things that we

39:09.400 --> 39:17.800
find like very common is the idea that hand labeled data is ground truth um and and this is sort of

39:17.800 --> 39:24.680
like an interesting idea where like it's it's just not the case in the real world you know a lot

39:24.680 --> 39:31.640
of the time people have uh subjectivity that comes to play or or they have biases that manifest in

39:31.640 --> 39:38.840
their labeled data and and try as hard as you might it's impossible to get a 100% accurate and

39:38.840 --> 39:46.280
fair data set labeled by an army of humans uh so that's like one of the biggest hurdles that

39:46.280 --> 39:51.240
we've had to face in like communicating what we do to the rest of the market it's getting people

39:51.240 --> 39:57.080
to acknowledge that the way they've gone about doing things so far has been leveraging

39:58.120 --> 40:03.000
words that they don't actually mean do you go as far as you know trying to convince people to stop

40:03.000 --> 40:09.720
using the term ground truth when yes yeah it's not absolutely ground truth it's just their labeled

40:09.720 --> 40:14.040
data yeah that's exactly right I mean like we we we try and educate them on life hey you're like

40:14.040 --> 40:18.520
hey this actually means something and it's not what you're what you're thinking yeah words matter

40:18.520 --> 40:23.800
you know like like you can't go around saying that your data is ground truth when it's like

40:23.800 --> 40:28.840
clearly riddled with bias and like incorrect labels and assumptions like we've seen this time and

40:28.840 --> 40:35.400
time again so first we educate them that like look hand labels themselves are noisy in some

40:35.400 --> 40:44.360
unknowable capacity they are by nature weak you know perhaps not as weak as functions but they

40:44.360 --> 40:51.800
are still weak in some way and now the question is how weak right how much can we trust the hand

40:51.800 --> 40:56.520
labels that you already have like how do we leverage them in in a more robust system how do we

40:56.520 --> 41:02.360
combine them with other sources of signal to then boost or reduce the signal introduced overall

41:02.360 --> 41:08.840
those become very very interesting questions and part of our like overall conviction is that

41:09.480 --> 41:16.760
you know as we move towards a more mature nl ops workflow overall you know data centric AI

41:16.760 --> 41:21.160
notwithstanding right if we're talking about just like the maturity of machine learning as a whole

41:21.160 --> 41:27.320
from the enterprise explainable AI has been a very very important thing for several years now

41:28.280 --> 41:34.440
but the interesting thing to us is that a lot of the techniques there are all around explaining

41:35.320 --> 41:39.720
your model and like why your model is making certain decisions and how they could play out

41:39.720 --> 41:44.120
but the reason why your model is making a bias decision is because it was trained on bias data

41:44.120 --> 41:49.000
right like right especially if we're holding yeah potential exceptions to that naturally you know

41:49.000 --> 41:56.680
I saw a good one where someone was using distilbert to predict sentiment on certain phrases so it

41:56.680 --> 42:04.440
was like if a movie was like the term was like this movie was produced in India that would have

42:04.440 --> 42:10.520
a very high sentiment then there's another where it was like this movie was produced in Germany

42:10.520 --> 42:16.280
and that had a very low sentiment and the reason why that happened there's several reasons

42:16.280 --> 42:20.680
obviously distilbert was trained on like internet data there's obviously imbalance and in that

42:20.680 --> 42:25.000
data because it's probably overfitting on like this pre-trained notion of like World War 2 and

42:25.000 --> 42:30.920
you know several mentions of Germany that are like not very favorable but even more interestingly

42:30.920 --> 42:36.120
the model architecture is such that like on the sentiment analysis side there's no notion of

42:36.120 --> 42:41.240
neutrality it's either positive or negative so the model architecture actually ended up biasing

42:41.240 --> 42:46.920
the entire outcome because there's like those are inherently neutral statements but it was forced

42:46.920 --> 42:51.960
to either make a positive or negative assumption that's such a clear example of this thing that

42:51.960 --> 42:57.560
we've been fighting about on Twitter for years like that bias is only in the data and not in the

42:57.560 --> 43:03.400
models no it's everywhere it's everywhere you can't escape it it's it's it's literally everywhere

43:03.400 --> 43:09.240
and but you just have to be smart about like where you're limiting the possible sources of bias

43:09.240 --> 43:14.440
you know there's human-generated bias both in the way of data as well as in the way of model

43:14.440 --> 43:20.200
architectures there's bias in the way labels or predictions are used you know downstream there's

43:20.200 --> 43:28.040
so many different ways that you could sort of like capture bias unintentionally in a system but

43:28.760 --> 43:34.840
as part of that like we should be building systems to find where this bias might be being introduced

43:34.840 --> 43:39.000
you know we should we should be building more robust systems exactly all the way up and all the way

43:39.000 --> 43:44.360
down the stack so it's part of this like because we touch the data a lot our notion is that you

43:44.360 --> 43:47.800
should be able to explain that data you should be able to explain the data that goes into your

43:47.800 --> 43:52.920
models otherwise how are you going to reason about it so like again the point I'm trying to make

43:52.920 --> 43:56.440
is that you just have to be like open to a lot of these things you have to be willing to shift

43:56.440 --> 44:01.160
your mentality and willing to shift again as a side effect your techniques in producing some of

44:01.160 --> 44:06.440
these models and then naturally it also helps if you've taken the first couple of steps in your

44:06.440 --> 44:10.760
machine learning journey if you know what status quo looks like if you know why this is different

44:10.760 --> 44:16.120
if you know how this plays into the rest of your anal ops stack that's helpful but not required

44:16.120 --> 44:21.080
I think the biggest requirement is very much just like a willingness to have your mind changed

44:21.080 --> 44:28.840
you talked about the noisiness of hand label data and you know to some degree we've

44:28.840 --> 44:35.240
recognized that and sophisticated organizations have you know employed a number of strategies

44:35.240 --> 44:42.760
to try to mitigate that you know quorum among laborers and managing labor labor labor

44:42.760 --> 44:48.280
quality and all this kind of stuff is it data centric AI or some of the things that we've

44:48.280 --> 44:56.200
been talking about under that banner week supervision etc is that a substitute for all that do you

44:56.200 --> 45:03.800
need kind of these you know advanced labeling strategies less if you're employing some of the

45:03.800 --> 45:09.480
things that we've been talking about our argument is yes however that's not necessarily the argument

45:09.480 --> 45:16.040
shared across the board yeah it depends exactly so so here's a deal like you've got you've got

45:16.040 --> 45:21.480
things like cap of values right so like measuring how good of a label or someone is and

45:21.480 --> 45:25.960
and that sort of thing you've got inter annotator disagreement as a side effect of that you've got

45:25.960 --> 45:30.840
as you mentioned quorum between hand labelers you've got all these different things that play in

45:30.840 --> 45:36.440
I think they're ways to do that correctly I just haven't seen them yet so for instance quorum

45:36.440 --> 45:41.960
amongst hand labelers like let's say you have like seven out of 10 people say that this thing is

45:41.960 --> 45:47.960
toxic and three people say that it's not toxic like it's a tweet or something like that like

45:47.960 --> 45:54.920
my conviction is that every single one of those people was correct in some capacity right there's

45:54.920 --> 45:59.640
some amount of subjectivity right like what what exactly is toxicity toxicity is a thing that

45:59.640 --> 46:04.120
is very difficult to actually capture the nuance of especially if you're trying to write up a

46:04.120 --> 46:11.400
description so you have to acknowledge that there's like cascading systems that go into these labels

46:11.400 --> 46:18.680
like you have to have the world's most perfect rules that describe how to label a thing in order to

46:18.680 --> 46:24.440
get inherent consistency in the labels that come out the other end and we're human you know we're

46:24.440 --> 46:28.440
not going to write the world's most perfect descriptions it's just not going to be possible we're

46:28.440 --> 46:33.160
not going to write the world's most perfect criteria for labeling it's just not possible

46:33.160 --> 46:41.720
so this kind of speaks to you know we talked about bias and data and how there's also bias and

46:42.520 --> 46:48.120
models you know here we're pointing specific to there's also kind of inherent bias in

46:48.840 --> 46:54.280
labeling systems and labeling tools and the way you're you know labeling instructions

46:54.280 --> 46:59.960
exactly I mean it's it's all over the place right like bias can be introduced in a multitude of

46:59.960 --> 47:07.480
places so you have to be like very cognizant about that and I think that like trying to articulate

47:07.480 --> 47:13.000
whether someone is a good or bad labeler is fundamentally the wrong approach the reason for

47:13.000 --> 47:20.120
this is because you might be trying to categorize them overall as a bad labeler but they might be

47:20.120 --> 47:24.200
very very good at labeling certain parts of the data because of their context because of their

47:24.200 --> 47:30.520
expertise but very bad at labeling other parts of the data and I think it's more accurate to

47:30.520 --> 47:34.920
indicate whether the expertise they're bringing to the table in particular segments of the data

47:35.560 --> 47:39.960
is actually valuable or not so that's sort of like point number one point number two is I believe

47:39.960 --> 47:47.720
that these labels should capture the rich context behind what you know sort of the one or zero

47:47.720 --> 47:52.760
output is so for instance in the earlier case if seven out of ten people said this thing is

47:52.760 --> 47:58.440
toxic I'd want to know that I'd want my model to acknowledge the nuance that went into this so that

47:58.440 --> 48:03.320
it can actually learn a richer relationship between the input space and the output value so

48:04.520 --> 48:08.200
that is actually very very important and again that's why I'm saying that I think that there

48:08.200 --> 48:13.800
there are ways that you could build a system that does a lot of this stuff but I haven't seen it yet

48:14.520 --> 48:19.640
and so like we take kind of like just a different approach to it our system is not a hand labeling

48:19.640 --> 48:25.320
system you know we we have hand labeling as part of our process but a we acknowledge that there's

48:25.320 --> 48:30.920
no such thing as ground truth and b all of our labels are inherently probabilistic by nature

48:31.560 --> 48:36.920
so you can have probabilities that are 100% or 0% so you can have extreme confidence on either side

48:36.920 --> 48:41.640
but you can also have confidence in the middle you know like you can have that 70% likelihood

48:41.640 --> 48:49.480
probability or or or something else right but that allows for 101 degrees of freedom in your

48:49.480 --> 48:54.920
in your label space which then hopefully yields a model that will learn a richer relationship

48:54.920 --> 48:59.560
between the input and the output so it's a long way of saying yeah I think you could you know

48:59.560 --> 49:03.720
build better hand labeling systems I just haven't seen it yet you referred to

49:03.720 --> 49:12.040
data centric AI and and the the kind of stuff we're talking about is part of the ml ops stack

49:12.040 --> 49:18.280
historically and in a lot of ways there's been kind of a separation between all of the data stuff

49:18.280 --> 49:25.720
the data stack and even you know data ops and data prep and all the stuff and kind of ml ops

49:25.720 --> 49:31.400
you kind of it does data centric AI kind of bring those together yeah it does again the whole

49:31.400 --> 49:38.600
concept here is that in today's world as it relates to AI you can broadly consider the code

49:39.320 --> 49:44.920
to be like held static and again AI is sort of like code plus data and if you're holding the

49:44.920 --> 49:49.160
code static then the thing you're iterating on is data which means that by nature if you want

49:49.160 --> 49:54.920
that to go back through an ml ops system it has to be connected in some way so the argument here

49:54.920 --> 50:01.160
is data should be a part of the stack in some meaningful way perhaps not the entire like process

50:01.160 --> 50:05.160
of procuring the data and so on like you know maybe some of that is a little bit more

50:05.160 --> 50:14.200
bespoke or you know a little less framework but really the idea is that AI is code plus data

50:14.200 --> 50:19.400
and we have very very robust ways of managing that code due to decades of innovation on the

50:19.400 --> 50:26.520
software engineering side and we haven't seen quite the same thing in an ml concept or context

50:26.520 --> 50:32.840
on the data side and that's I think the part that data centric AI really aims to help

50:32.840 --> 50:38.200
sort of like push it's the idea that data really should be a part of the ml ops stack

50:38.200 --> 50:47.880
and do you find that these ideas apply equally well across media type modality text audio

50:47.880 --> 50:53.640
video yeah yeah absolutely um the obviously the only real place where it starts breaking down

50:53.640 --> 50:58.120
is the moment you start leaving like supervised machine learning because then it you might not

50:58.120 --> 51:03.240
need data right which is totally reasonable but but broadly speaking when we're talking about

51:03.240 --> 51:08.440
machine learning the enterprise we are generally talking about some sort of supervised system

51:08.440 --> 51:15.400
and some capacity and yeah we have seen that this applies like roughly the same across every

51:15.400 --> 51:23.160
modality across every task now devil's always in the details right like managing things like

51:24.360 --> 51:29.800
like 3d point cloud data is very different to managing things like you know just single

51:29.800 --> 51:35.640
dimensional text or something like that like um the way you go about interfacing with this data

51:35.640 --> 51:40.760
and the way you go about applying your subject matter expertise to it will differ like quite a lot

51:40.760 --> 51:49.240
from from modality modality um now like a whole purpose to be for for my company is finding the

51:49.240 --> 51:55.240
common threads across these modalities and across these these tasks such that you can reuse the

51:55.240 --> 51:59.800
same skill set the same workflow no matter what data you're bringing to the table and no matter

51:59.800 --> 52:04.440
what task you're bringing to the table not all things will be similar not all things will be the

52:04.440 --> 52:10.040
same but a lot of them will at least the core will and that's kind of like where we focus almost

52:10.040 --> 52:14.840
all of our effort it's it's finding that common ground well cyan has been wonderful having you on

52:14.840 --> 52:21.960
the show and chatting about your take on data centric AI lots of great stuff in here um thank you

52:21.960 --> 52:49.080
thank you so much Sam this is a lot of fun I really appreciate it

