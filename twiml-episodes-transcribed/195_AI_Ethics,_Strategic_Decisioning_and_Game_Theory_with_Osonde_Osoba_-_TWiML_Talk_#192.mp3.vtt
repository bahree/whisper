WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:30.960
I'm your host Sam Charrington.

00:30.960 --> 00:36.920
In this episode of our deep learning endoba series, we're joined by Oshonde Osaba, engineer

00:36.920 --> 00:41.720
at Rand Corporation and professor at the party Rand Graduate School.

00:41.720 --> 00:47.280
Oshonde and I spoke on the heels of the endoba where he presented on AI ethics and policy.

00:47.280 --> 00:52.480
We discuss framework-based approach for evaluating ethical issues such as applying the ethical

00:52.480 --> 00:57.360
principles laid out in the Velmont report and how to build an intuition for where ethical

00:57.360 --> 01:01.000
flashpoints may exist in these discussions.

01:01.000 --> 01:05.640
We then shift gears to Oshonde's own model development research and end up in a really

01:05.640 --> 01:10.040
interesting discussion about the application of machine learning to strategic decisions

01:10.040 --> 01:15.800
and game theory, including the use of fuzzy cognitive map models.

01:15.800 --> 01:20.040
Before we jump in, I'd like to send a big shout out to our friends at Google AI, which

01:20.040 --> 01:25.400
recently opened up applications for its 2019 residency program.

01:25.400 --> 01:29.880
The Google AI Residency is a one-year machine learning research training program with

01:29.880 --> 01:35.000
the goal of helping individuals from all over the world and with a diverse set of educational

01:35.000 --> 01:40.160
and professional backgrounds become successful machine learning researchers.

01:40.160 --> 01:44.960
Find out more about the program at g.co slash AI residency.

01:44.960 --> 01:47.200
And now on to the show.

01:47.200 --> 01:50.720
All right, everyone.

01:50.720 --> 01:53.280
I am on the line with Oshonde Osaba.

01:53.280 --> 01:59.360
Oshonde is an engineer at the RAN Corporation and a professor at the Party RAN Graduate

01:59.360 --> 02:00.360
School.

02:00.360 --> 02:03.000
Oshonde, welcome to this weekend machine learning and AI.

02:03.000 --> 02:04.600
Glad to be here.

02:04.600 --> 02:05.600
Awesome.

02:05.600 --> 02:12.360
So, you did your PhD in machine learning and now you spend quite a bit of your time working

02:12.360 --> 02:19.320
on the policy implications of machine learning and AI in particular with regard to ethics

02:19.320 --> 02:23.640
among actually rolling up your sleeves and doing some model development.

02:23.640 --> 02:25.840
Tell us a little bit about your background.

02:25.840 --> 02:26.840
Oh, yeah.

02:26.840 --> 02:32.320
So, I did my engineering degree at USC, the University of Southern California, across town

02:32.320 --> 02:33.320
from where I am.

02:33.320 --> 02:38.280
I mean, sunny Southern California, and Southern Monica by the beach.

02:38.280 --> 02:44.480
Yeah, so at USC, I was focused on more theoretical aspects of learning algorithms.

02:44.480 --> 02:49.680
So, I was trying to figure out how to improve machine learning algorithms, statistical learning

02:49.680 --> 02:53.880
algorithms using the care for the injection of noise.

02:53.880 --> 02:56.360
And it was a very theoretical type of thing.

02:56.360 --> 02:57.360
I was finished.

02:57.360 --> 03:00.000
I wasn't quite sure what to do with myself.

03:00.000 --> 03:03.880
So, you know, as far as this is where you could either go up to see the come valley and

03:03.880 --> 03:08.960
do some work for a commercial entity or figure out something to do in academia.

03:08.960 --> 03:11.160
I sort of found a halfway point.

03:11.160 --> 03:14.400
At Rand, Rand is a public policy think tank.

03:14.400 --> 03:16.320
Oh, they don't like the word think tank.

03:16.320 --> 03:20.160
We do a lot of thinking about policy problems.

03:20.160 --> 03:24.960
A mandate that Rand is to improve decision-making through objective analysis.

03:24.960 --> 03:30.160
And so, I had worked there over a summer while I was in graduate school looking, well,

03:30.160 --> 03:31.160
I need money.

03:31.160 --> 03:32.560
I worked there for the summer.

03:32.560 --> 03:35.240
And I figured it was a pretty good fit.

03:35.240 --> 03:42.200
So, I went back there and it was just around the time when lots of agencies, lots of people

03:42.200 --> 03:45.680
across the country are starting to think about machine learning and the implications.

03:45.680 --> 03:50.240
And so, I hit the ground running at Rand, just deploying models, deploying machine learning

03:50.240 --> 03:53.480
models to answer that response questions.

03:53.480 --> 04:00.360
And my growth at Rand has basically been me recognizing using my technical skills, what

04:00.360 --> 04:05.440
I would recognize in some of the normative implications, I'm trying to address them.

04:05.440 --> 04:10.200
The normative implications are using AI and machine learning in public policy spaces.

04:10.200 --> 04:12.600
So, I tried to balance both now.

04:12.600 --> 04:15.240
It's been an interesting ride, I guess.

04:15.240 --> 04:18.240
And you recently returned from the deep learning and daba.

04:18.240 --> 04:19.680
Tell us what did you present on?

04:19.680 --> 04:28.640
Yes, I also asked me because I had written some stuff on the issue of bias in algorithmic

04:28.640 --> 04:29.640
systems.

04:29.640 --> 04:37.240
I was asked to talk about more broadly, if you think about it, the question of bias or

04:37.240 --> 04:42.880
we call bias or equity in these algorithms, it's an ethical question.

04:42.880 --> 04:48.920
Because the ethical norms, they retry to say, okay, do our systems adhair or do it by

04:48.920 --> 04:50.640
a little ethical norms.

04:50.640 --> 04:56.160
And so, in the past year, I've been trying to broaden that discussion quite a bit.

04:56.160 --> 05:01.480
Not just on questions of ethics or bias, but also questions of ethics more broadly.

05:01.480 --> 05:07.160
And so, the question becomes, when a commercial entity, when a public entity decides to use

05:07.160 --> 05:13.200
machine learning for a particular decision problem, what are the ethical constraints, they

05:13.200 --> 05:18.840
should be looking into, what are the frameworks by which they can judge themselves to make

05:18.840 --> 05:22.200
sure that they're not violating x, y, z, ethical constraints.

05:22.200 --> 05:29.320
So, my talk at the end of I was trying to make that more formal, as opposed to just a touch

05:29.320 --> 05:34.600
of feeling discussion of ethics that often goes on, try to give formal, at least concrete

05:34.600 --> 05:37.520
frameworks by which we can think about these things.

05:37.520 --> 05:39.360
I like the sound of that.

05:39.360 --> 05:43.800
It sounds like there were a number of frameworks that you presented.

05:43.800 --> 05:46.520
Was there a specific number of them?

05:46.520 --> 05:49.960
Have you categorized the framework space, if you will?

05:49.960 --> 05:54.280
No, I have not, if I've done that, I think I would be very happy.

05:54.280 --> 05:58.920
But I mean, that's like, no, that's a huge, a huge step.

05:58.920 --> 06:04.720
But one of the things I relied on in discussing this was some of the work by Shannon Barlow-Altor

06:04.720 --> 06:07.640
of Santa Clara University.

06:07.640 --> 06:15.760
And the idea here is that if you try to impact in polls, normative, prescriptive ethics

06:15.760 --> 06:19.160
on the system, that this not always, it doesn't always work.

06:19.160 --> 06:24.760
Like, coming up with the universal ethical law for guidance, tech practice, tech development

06:24.760 --> 06:28.960
is not usually an easy or feasible task.

06:28.960 --> 06:32.680
So we went through this process where we went, okay, instead of trying to say this is

06:32.680 --> 06:39.680
how this should be, instead of imposing norms on just unilaterally, we tried to elicit

06:39.680 --> 06:41.520
the norms from case studies.

06:41.520 --> 06:47.960
So the first part of the conversation at the end of that was, okay, here are a couple

06:47.960 --> 06:55.640
of cases where people were thinking about using algorithms in a particular decision process

06:55.640 --> 07:01.880
how people build their intuition on where the ethical flashpoints might be.

07:01.880 --> 07:04.920
And then towards the end, we started coming up with frameworks.

07:04.920 --> 07:11.760
The one framework I liked that I sort of pushed a little bit was the development principle

07:11.760 --> 07:12.760
idea.

07:12.760 --> 07:17.920
So if you think about the development principles, it's really the set of principles.

07:17.920 --> 07:23.760
And I suppose that came out of a Belmont report, like I think in the 70s, trying to think

07:23.760 --> 07:29.800
through what are, how do you think through the implications of human experiments, of experiments

07:29.800 --> 07:33.760
that affect humans, so human subjects protection and stuff.

07:33.760 --> 07:37.800
And so part of the, no, no, yes, EEL, M-O-M-T.

07:37.800 --> 07:38.800
Okay.

07:38.800 --> 07:45.360
So part of the, on the center, I was trying to pull into the spaces every time you develop

07:45.360 --> 07:49.560
an algorithm to answer a problem that affects lots of people.

07:49.560 --> 07:54.000
At this sense, a social experiment, it's like, it's a social experiment that affects

07:54.000 --> 07:55.560
lots of human subjects.

07:55.560 --> 08:02.080
So it's not, it's not, it doesn't seem too far of a step to say that we should at least

08:02.080 --> 08:06.640
try to judge those new applications according to the development principles.

08:06.640 --> 08:11.040
And so we have things like, uh, download principles are, uh, it's basically your, your

08:11.040 --> 08:16.200
people critical in taking it a little bit further, so the basic one is do know harm, uh, there's

08:16.200 --> 08:21.080
the great, then the other one is, um, make sure your application is just, and then the

08:21.080 --> 08:23.240
third one is respect autonomy.

08:23.240 --> 08:29.000
Now, those sound very abstract and high-fifileting, and so one of the things, one of the tasks I

08:29.000 --> 08:35.040
gave myself was to try to, um, root those, make those more concrete in the case that

08:35.040 --> 08:37.880
is we're talking through, I thought the deep learning in Hidabah.

08:37.880 --> 08:43.240
And so the first one, you know how, actually interesting because, but the most part, we

08:43.240 --> 08:45.640
all think we are doing the best we can.

08:45.640 --> 08:51.760
So the, I think we decided that we, we limited not just to do know harm, but do know foreseeable

08:51.760 --> 08:57.240
harm, understand that we don't probably, we are always able to, to understand the impact

08:57.240 --> 09:01.960
and the piece of technology, any piece of model, any model has a particular space.

09:01.960 --> 09:08.760
So those are the types of conversations, case studies, to, to prime intuitions, um, building,

09:08.760 --> 09:12.880
building up, um, suggesting frameworks like the bell and press, at least the bell and

09:12.880 --> 09:19.400
press full frame, to help us, um, frame new, new app, thinking of new applications, and

09:19.400 --> 09:23.480
just trying to get general takeaways, general, understandable flashpoints, and I think about

09:23.480 --> 09:25.240
ethics and technology practice.

09:25.240 --> 09:28.040
Well, what were the case studies that you went into?

09:28.040 --> 09:36.400
So, yeah, this is like two or three weeks ago, the first one was, I guess, we're trying

09:36.400 --> 09:44.760
to do, so this is a currently famous, currently popular in famous is the right word, should

09:44.760 --> 09:51.120
people use facial recognition technologies for filing at boarders, now it's the first

09:51.120 --> 09:52.120
one.

09:52.120 --> 09:56.560
And we're, one of the things I was trying to do there, because it's an, I work mostly

09:56.560 --> 10:02.600
in the United States, the deep learning in Daba is, is a more, um, international community.

10:02.600 --> 10:09.800
I, I sent out a survey to try to get, to try to get people's, people's perspectives

10:09.800 --> 10:12.200
on these types of questions earlier on.

10:12.200 --> 10:13.200
Okay.

10:13.200 --> 10:18.160
And I, I made it, I made it such that the survey was, was tag based on where, what part

10:18.160 --> 10:23.480
of the world these people, um, the survey respondents identified with, um, so we had people

10:23.480 --> 10:24.480
like, yeah.

10:24.480 --> 10:28.800
And the, the idea is that we're increasingly, we are having this issue of, um, our value

10:28.800 --> 10:30.080
pluralism.

10:30.080 --> 10:33.000
So let's, let's walk through the, the chain there.

10:33.000 --> 10:37.920
The first part of the chain is, well, questions of equity, questions of biases are normative,

10:37.920 --> 10:39.360
ethical questions.

10:39.360 --> 10:43.400
The second part of the chain is that, well, different parts of the world have different

10:43.400 --> 10:47.560
ethical norms that I wish to leave, but it's a very cultural thing.

10:47.560 --> 10:53.440
And so part of the, the point of the survey was trying to elicit this, any disconnect,

10:53.440 --> 11:00.440
any discontinuities in how, say, somebody in Europe thinks about privacy versus say, somebody

11:00.440 --> 11:02.440
in Africa thinks about privacy.

11:02.440 --> 11:09.960
So I, I, there was, in the survey was, they didn't really elicit those sharp disconnects,

11:09.960 --> 11:14.720
but in the conversation at the end of that, there was this sense that some people thought

11:14.720 --> 11:19.920
it was perfectly fine and some people thought it wasn't, it was, there were such privacy

11:19.920 --> 11:26.080
constraints on these official recognition providing that the security benefits or any potential

11:26.080 --> 11:32.160
security, security benefits of official recognition technologies were, were overwhelmingly

11:32.160 --> 11:34.880
positive compared to the potential downfalls.

11:34.880 --> 11:41.080
And so there is not the conversation in that case that he illustrated that there was no,

11:41.080 --> 11:46.720
there was no unified perspective on the official recognition technology.

11:46.720 --> 11:52.320
And so when you, you have a situation where there's no unified perspective that seems

11:52.320 --> 11:58.840
like a perfect opportunity to try to apply some framework, how did you apply the framework,

11:58.840 --> 12:03.320
the Belmont frame to the, to this particular case study?

12:03.320 --> 12:11.480
So the, I think it's hard to take an issue, which there is polarization or significant

12:11.480 --> 12:18.560
diversity of opinion and try to marshal it into some preferred, whatever that preferred

12:18.560 --> 12:20.160
position is.

12:20.160 --> 12:29.400
So the, the, the procedure or the, the way forward was to highlight that, okay, you want,

12:29.400 --> 12:34.280
so one person brought up the idea that, well, governments, that technically more trustworthy

12:34.280 --> 12:39.320
than say commercial companies at the India use of official recognition technologies.

12:39.320 --> 12:45.200
And that is, that is a question that's terribly dependent on what your experience has been

12:45.200 --> 12:48.440
with government and what part of the world you're from.

12:48.440 --> 12:52.680
And so it's not like you cannot apply the framework and come up with a single answer that

12:52.680 --> 12:54.080
should control.

12:54.080 --> 13:01.160
So my, my job was essentially point out that, okay, if that's your perspective, recognize

13:01.160 --> 13:06.720
that the respectful personal autonomy, that, that factor, that, that's the third principle

13:06.720 --> 13:08.160
in the Belmont principle.

13:08.160 --> 13:13.600
That is not, you know, that's not uniformly guaranteed across the world with different

13:13.600 --> 13:14.600
governments.

13:14.600 --> 13:18.160
And so every time you're making decisions, it's not a, it's not a question, or you should

13:18.160 --> 13:20.400
always use facial recognition technology.

13:20.400 --> 13:24.480
It's more a question of when you're thinking about it, ask these questions, these types

13:24.480 --> 13:25.480
of questions.

13:25.480 --> 13:32.000
And that might help you make more contextual, context sensitive answers to those, to, to

13:32.000 --> 13:35.280
whether you should use the technology or not.

13:35.280 --> 13:44.200
Is that the same kind of process that you would walk one of your, a ran client through?

13:44.200 --> 13:51.680
Do you find that they, that they want more concrete answers or do they want, kind of these

13:51.680 --> 13:56.200
experiences that help them understand the, the scope of the issues?

13:56.200 --> 14:05.560
I think in general, when you're, when you're answering questions on, on behalf of somebody

14:05.560 --> 14:08.200
else, there is a balance that needs to be struck.

14:08.200 --> 14:15.760
You want to relay strong useful insights, strong useful frameworks for proceeding, when

14:15.760 --> 14:20.240
you leave the room, but at the same time, there is a, there is an expectation that you

14:20.240 --> 14:25.560
give them concrete answers, finally somewhat concrete answers to the issues that they are facing.

14:25.560 --> 14:28.640
So I try to strike a balance between the two.

14:28.640 --> 14:35.240
So I find somebody asks me, usually when I'm working on, so the, the presentation was a

14:35.240 --> 14:40.240
little bit, the deep learning in that position was, was at a higher level of abstraction than

14:40.240 --> 14:46.640
I would normally work at, because at, at that point, I'm trying to present as much insight,

14:46.640 --> 14:50.080
as much abstract insight as possible.

14:50.080 --> 14:56.480
And actually talking to clients, naturally dealing with governmental clients, I would, I would

14:56.480 --> 15:03.160
usually start with a concrete demonstration of, okay, this is, this is a facial recognition

15:03.160 --> 15:07.880
technology trying to, you, you're, you're suggesting that you might want to use, well,

15:07.880 --> 15:15.520
these technologies have X, Y and Z, X, Y and Z characteristics, the most important in

15:15.520 --> 15:20.400
the recent days, the recent months being that there is a differential error rate that has

15:20.400 --> 15:26.480
implications for, for justice, you can treat everybody the same, you can treat everybody

15:26.480 --> 15:29.840
equitably, whatever equitably needs life context.

15:29.840 --> 15:36.040
And how does that affect your procedures, your operations, and what do you intend to do

15:36.040 --> 15:41.560
to deal to address those, those are equity constraints, those equity constraints.

15:41.560 --> 15:46.200
So usually I'm working, at least I'm talking to clients at that level, at the more concrete

15:46.200 --> 15:52.640
level of, of detail, but the goal is not just to give them, if you can point out answers,

15:52.640 --> 15:58.200
the goal is to have useful abstractions that help make sense of the world, so that when

15:58.200 --> 16:02.960
somebody else comes with a similar type of problem, you can give them interesting answers,

16:02.960 --> 16:04.560
informed answers.

16:04.560 --> 16:08.560
What types of clients are these generally, and what types of questions are these generally,

16:08.560 --> 16:15.440
and who's generally the actual client, what is their role?

16:15.440 --> 16:21.080
People with experience at Rand Differ, it's one of those places where you make your own

16:21.080 --> 16:30.280
way, my projects, most of my projects tend to be focused on questions I think are important

16:30.280 --> 16:37.000
as opposed to client driven, but in general Rand as a whole has clients are biggest clients

16:37.000 --> 16:44.280
are in the house, about just under how our work comes from what we call the federally

16:44.280 --> 16:49.680
funded research and development centers, we have three of them, or I think we have four

16:49.680 --> 16:55.640
now, we have one for the US Army, one for the US Air Force, one for the Office of the Secretary

16:55.640 --> 17:04.920
of Defense, and one for DHS, and so we would generally take a series of portfolio of strategic

17:04.920 --> 17:11.920
and tactical research problems for those four, those four clients, or subsection, sub

17:11.920 --> 17:13.680
divisions of those four clients.

17:13.680 --> 17:19.280
On the other hand, we have a health division, we have a justative restructure and energy

17:19.280 --> 17:24.920
division, we have, we need to have a labor population division, but in general those

17:24.920 --> 17:32.520
will, those will house our non-defense questions, and there you'd have people getting grant

17:32.520 --> 17:38.680
money from NIH, NSF, or foundations like the Robert Wood Johnson Foundation, or the Gates

17:38.680 --> 17:42.880
Foundation, to answer some policy relevant questions.

17:42.880 --> 17:49.720
There isn't as much detail, I've hesitant to go into a lot more detail about the nature

17:49.720 --> 17:54.400
and of the clients and the types of questions I go into because that might become detestive.

17:54.400 --> 18:02.400
Sure, sure, no, that helps me understand the broad process by which folks are coming

18:02.400 --> 18:08.120
to you and the types of questions that they might be looking for.

18:08.120 --> 18:16.280
At the NW presented this one framework, are there other frameworks that you've relied

18:16.280 --> 18:21.480
on to address similar types of ethical issues?

18:21.480 --> 18:27.360
Also, there is one that I tend to, it's not so much a framework at all, I guess it's

18:27.360 --> 18:29.280
a framework in a sense.

18:29.280 --> 18:36.280
So oftentimes when people in the fairness, accountable, transparent, machine learning,

18:36.280 --> 18:41.600
generally the AI policy space, think about machine learning models, they generally think

18:41.600 --> 18:49.120
about it as single monolithic algorithm, single monolithic models that are solving single

18:49.120 --> 18:51.480
decision questions.

18:51.480 --> 18:58.160
Increasingly, there is this issue of systems level thinking, like even if every algorithm

18:58.160 --> 19:04.920
in a particular system is unbiased, is transparent and all that stuff.

19:04.920 --> 19:08.800
It doesn't mean that the outcome is on from the entire system and it's really going

19:08.800 --> 19:15.120
to be fair unbiased.

19:15.120 --> 19:18.720
Are there specific examples of that phenomenon that come to mind?

19:18.720 --> 19:23.360
It is easier to pay attention to say the criminal justice system where you're always trying

19:23.360 --> 19:29.840
to steal algorithms, but then we always try to steal algorithms at different parts of

19:29.840 --> 19:30.840
the system.

19:30.840 --> 19:35.920
For example, there's the risk criminal risk, risk racism estimation.

19:35.920 --> 19:42.880
There's a risk estimation algorithm that has been popular, has been part of the conversation

19:42.880 --> 19:48.840
for the past two years or so, based on public has worked, and the idea that has always

19:48.840 --> 19:50.760
been all over, we just need to fix that.

19:50.760 --> 19:55.480
Then there was an interesting conversation that has been happening over the past year,

19:55.480 --> 20:00.520
where even if you take this issue up, essentially, if falls on the umbrella,

20:00.520 --> 20:04.680
run away, feedback, run away, feedback loops in systems,

20:04.680 --> 20:10.680
where, because even if you have an algorithm that's performing safe and optimal way,

20:10.680 --> 20:20.200
historic inequities can cause the algorithm, can cause a system to diverge in outcomes

20:20.200 --> 20:25.800
for different groups. So, this is sort of like perturbations of the initial conditions

20:25.800 --> 20:31.480
for a system, for the criminal justice system, is causing this vast,

20:31.480 --> 20:36.440
outsized differences in outcomes. And this happens even if the algorithms

20:36.440 --> 20:40.440
themselves, without paying attention to the historical data, the algorithms themselves

20:40.440 --> 20:46.280
are trying to behave optimally. So, that's the one, of course, the example I have seen,

20:46.280 --> 20:52.360
but this behavior, same as one of the things I've been trying to do, is pay attention to,

20:52.360 --> 20:57.800
essentially, the systems level feedback effect of what's just focusing on individual algorithms

20:57.800 --> 21:04.600
and systems. So, we've been doing this for one of my projects' works on this equity concern,

21:04.600 --> 21:08.360
for the criminal justice system, for insurance systems, onto insurance systems,

21:08.360 --> 21:14.280
and for employment systems. And we emphasize the systemic aspect of what's

21:14.280 --> 21:19.800
to this in individual decision point aspect. And it's usually a lot more, I don't know,

21:19.800 --> 21:23.960
I find it a lot more enlightening than focusing on a single algorithm.

21:23.960 --> 21:32.520
Does looking at it from a system's perspective make these problems more or less actionable

21:32.520 --> 21:39.160
for the clients that are struggling to deal with them? I think it makes the analysis and the

21:39.160 --> 21:45.480
discussion a little bit more complex, but the world we live in is complex, social systems,

21:45.480 --> 21:50.760
which is algorithms that operate in the other complex. But one of the things that one of the

21:50.760 --> 21:57.400
extra levels, extra dimensions, degrees of freedom, that this type of perspective gives you is,

21:57.400 --> 22:02.840
well, okay, now algorithmism working out so well, you could do one of the two things,

22:02.840 --> 22:11.720
you could modify the model directly, or you could pay attention to the outcomes and do post-hoc

22:11.720 --> 22:20.040
modifications, post-hoc corrections, so the outcomes to try to correct any inequity in the model.

22:20.040 --> 22:26.040
And one of the, so an example of where this has gone wrong is, so in child welfare systems,

22:26.040 --> 22:31.960
I think there was a paper last year or the year before looking into the child welfare

22:31.960 --> 22:38.680
system, they tried to use an algorithm to better estimate the risk of adverse outcomes for

22:38.680 --> 22:45.240
children through that system. They had a situation where the algorithm was, was doing in terms of

22:45.240 --> 22:51.400
predictive powers doing pretty well, but in terms of implementation, so we had these social workers

22:51.400 --> 22:56.360
taking a look at the algorithm, but because either because of, they don't understand what the algorithm

22:56.360 --> 23:01.560
is doing or they don't trust that the algorithm is taking everything into account, they decide

23:01.560 --> 23:06.760
to circumvent whatever recommendations the algorithm gives them, so that's like a post-hoc

23:06.760 --> 23:11.880
modification to the behavior of an algorithm. If you're focused on the algorithm alone,

23:11.880 --> 23:15.960
then you're not going to, you're not going to pay attention to those types of,

23:15.960 --> 23:19.960
those types of secondary effects that actually quite important to the real world.

23:19.960 --> 23:24.520
And some of the work we're seeing in the criminal justice system is exactly this.

23:24.520 --> 23:32.600
The algorithm is doing an analysis for, does not matter what. So the algorithm was fine,

23:32.600 --> 23:39.880
but the post-hoc implementation was causing, was amplifying, very, very tiny difference in ways

23:39.880 --> 23:44.600
that were of well and sustainable. That is important going forward as we start thinking about

23:44.600 --> 23:50.280
where algorithms fit in systems. I guess it's a long way of saying yes, it's more complex,

23:50.280 --> 23:55.320
but it gives you more degrees of freedom to play, to try to intervene to correct mistakes.

23:56.040 --> 24:01.080
Right, right, right. So you're taking a step back and dealing with these kinds of problems,

24:01.720 --> 24:08.520
you've mentioned two tools that you use. One is applying frameworks like Belmont. Another is

24:08.520 --> 24:15.240
taking a systems thinking approach to looking at the problem. Are there other tools that you

24:15.240 --> 24:22.520
use in this way? I mean, I'm still developing my thinking. Could I say that there are the tools?

24:23.320 --> 24:31.880
Not so much. There is another space in which I think people that I've been trying to think of

24:31.880 --> 24:37.800
frameworks. And this is the part question of how do you regulate the free use of algorithms?

24:37.800 --> 24:42.120
Should you regulate the use of algorithms? And even if you wanted to, how would that work?

24:42.120 --> 24:48.600
And so the standard, the standard approach to regulation has always been in polls laws.

24:50.040 --> 24:54.280
If it was laws, either you can do this or you can't do that.

24:55.640 --> 25:01.960
Increasingly for spaces where compliance is hard to, to assure, there is the idea of

25:01.960 --> 25:07.400
where compliance is hard to assure, where norms are actually more important than hard regulations.

25:07.400 --> 25:12.040
The idea of soft governance approaches becomes more, is beginning to get more attention.

25:12.920 --> 25:18.600
I think the regulatory scheme, the frameworks for regulating the proper,

25:18.600 --> 25:26.200
or the beneficial use of algorithms, that's still there a lot there, but there's a lot of

25:26.200 --> 25:32.280
thinking going on there, but not much. There isn't as much hard and I guess there aren't as much

25:32.280 --> 25:38.920
proven frameworks that I can point to. If you kind of take that thought exercise around

25:38.920 --> 25:48.360
soft regulation and establishing norms and try to take on a broad issue like algorithmic

25:48.360 --> 25:55.000
transparency, you end up in likely a very different place than something like a GDPR.

25:55.960 --> 26:01.320
How would you compare and contrast the solution that you might end up with a

26:01.320 --> 26:07.240
soft regulatory approach and something like GDPR? That's an interesting question.

26:07.960 --> 26:16.920
I imagine that this is me giving my best educator guess and this is really the correct

26:16.920 --> 26:23.160
answer to this. I imagine that the level of soft regulations that you're trying to build norms

26:23.160 --> 26:29.320
and trying to build consensus that a broad coalition of people can agree to, a broad coalition

26:29.320 --> 26:36.040
of players can agree to. The general thing about broad coalition is decision-making that

26:36.040 --> 26:41.640
that record based on broad coalition building is it's going to be a weaker regulatory framework,

26:42.280 --> 26:47.880
but if you're working in a highly polarized, highly fractionated space, maybe that's the best

26:47.880 --> 26:53.480
you can do. GDPR is interesting because they come at it from a strong normative frame,

26:53.480 --> 27:02.360
the idea that the consumer is key, that privacy is not an absolute right, but it's a highly

27:02.360 --> 27:09.800
prioritized right, and that the people are central as opposed to the companies. That controls,

27:09.800 --> 27:14.920
and it's an interesting and important perspective. It's a perspective that's been missing from

27:14.920 --> 27:21.480
the history of American technology innovation, specifically when it comes to data driven,

27:21.480 --> 27:27.400
data driven innovation, but I don't think a sub-governance framework would have come up with that.

27:30.600 --> 27:41.000
For the yet word, here's the driving follow-up question. There are spaces where trying to assert

27:41.000 --> 27:48.600
a primary normative frame is the best solution to getting compliance. So one of the things about

27:48.600 --> 27:54.680
GDPR is there is a question of whether it's going to drive fragmentation and regulation.

27:54.680 --> 28:01.160
So you have United States, California is following GDPR's footsteps, but the United States has a

28:01.160 --> 28:06.840
federal system doesn't seem to be able to or want to or have the want to use a strong term to apply

28:06.840 --> 28:12.600
to a system. It doesn't seem to be moving in that general direction. China is certainly not moving

28:12.600 --> 28:24.040
in the same direction as GDPR. If you try to impose global laws concerning data privacy,

28:24.040 --> 28:29.240
and you come at it with that strong normative foundation like what the GDPR does,

28:29.960 --> 28:35.720
it's an interesting exercise to see if that was going to be a viable global

28:35.720 --> 28:43.400
up switch to regulation. Right. Particularly in light of the global nature of the technologies

28:43.400 --> 28:49.160
that they're aiming to govern. Interesting. So you spend roughly half your time on

28:49.160 --> 28:57.640
Kennedy's broad ethical and policy issues, and another half of your time at Rand on model

28:57.640 --> 29:05.240
development and kind of rolling up your sleeves and building systems or proofs of concepts for clients.

29:06.760 --> 29:15.160
Is that work also in this domain? Are you exploring explainability models and transparency?

29:15.160 --> 29:20.360
That kind of thing. I am a bit more exploratory tree in my technical work,

29:20.360 --> 29:28.280
largely because for me, the technical development keeps me honest. When I try to talk about

29:28.280 --> 29:33.320
the policy, regulatory and ethical framing, if I don't have a technical understanding,

29:33.320 --> 29:40.920
I think I am generally off base if I don't keep that understanding. But in terms of the actual

29:40.920 --> 29:49.960
types of technical models I tend to pursue. So there's stuff on the explanation. There isn't

29:49.960 --> 29:57.320
as much. So DARPA has a program on this, a program unexplainably. I talk to them a couple of

29:57.320 --> 30:06.200
times. Everybody talks to them to see what they're thinking. But I tend to, my perspective on it

30:06.200 --> 30:10.840
is a little bit, I think the models I tried to build so I addressed that a little bit more

30:11.640 --> 30:18.360
exploratory tree than what is probably the raining stuff. In general, I think where I've

30:18.360 --> 30:24.040
spent most of my thinking, most of my technical thinking, I've been on two things,

30:25.000 --> 30:30.760
older models. So this issue of explainability in machine learning models,

30:31.560 --> 30:40.600
it's sort of, I work with people who grew up in the heyday of the ATV AI, AI boom. It's sort of

30:40.600 --> 30:45.640
throwing back to that phase. So back then I had this system, like the systems that were super

30:45.640 --> 30:49.960
explainable, they could do forward inference, like most of our machine learning models can do.

30:49.960 --> 30:54.600
But they could also do backward inference in the sense that you give it a state of an output state

30:54.600 --> 30:59.800
and it can give you a set of likely input states that have caused, which is one form of explanation.

30:59.800 --> 31:04.680
So in the sense, those older systems were better at it than the current systems. And so one of

31:04.680 --> 31:11.960
the things I have tried to be explored in the space of explorability is this hybrid of older

31:11.960 --> 31:19.080
Xper system style models with current statistical machine learning models. This is not like

31:19.960 --> 31:25.880
heresy in the sense that you have people like Jerry, let's go back at Stanford doing this thing

31:25.880 --> 31:32.360
where he is using decision tree models to try to create a meta model summary of a black box model,

31:32.360 --> 31:40.120
however that black box is. And what my approach is to refer more to what they call further systems.

31:40.120 --> 31:46.200
So for the systems where these old experts, the expert systems based model, they were more,

31:46.200 --> 31:53.240
they were used often for linguistic mapping between even then parts of sets of systems.

31:53.720 --> 31:58.680
And so the idea there is to take a black box model, create a gray box model that is a bit more

31:58.680 --> 32:04.600
expletable that sort of summarizes that black box model. And sort of see if you can get better

32:04.600 --> 32:11.080
explanations, isn't that a gray box model. I think the couple of, so I mentioned Leskovich as

32:11.080 --> 32:17.320
I approached to that, there was also Lipton's work on born-again networks, I think trying to address

32:17.320 --> 32:23.960
it in a similar way. But these are all like a very tree type of model to see what's possible,

32:23.960 --> 32:29.720
what's feasible. But the other, the one I actually care about, well, care about is in my care about

32:29.720 --> 32:38.440
all these models. The one I actually spent a lot of time thinking about is the use of machine learning

32:39.400 --> 32:47.400
models or at least AI-stamp models to try to answer strategic questions in complex adaptive systems.

32:47.400 --> 32:54.920
So you have the situation where you have a stick, let me try to give it a useful example.

32:54.920 --> 33:01.400
So half, I went to paper a while ago, like a year or two ago, where you had a bunch of experts

33:01.400 --> 33:07.320
across the world who have thought very carefully about what does it take for local population to

33:07.320 --> 33:12.840
begin to support terrorism. So the common public support for industry and terrorism model.

33:12.840 --> 33:19.080
So you have all these experts pulling their insights to come up with interesting factors

33:19.080 --> 33:24.280
and interesting dynamics for how this might work. But that's all linguistic, that's all

33:24.280 --> 33:28.120
unstructured data. So one of the things I've been trying to do, and if you think about what

33:28.120 --> 33:33.800
RAND does, RAND is very focused on trying to improve decision-making, even in those types of

33:33.800 --> 33:39.320
complex domains where expertise is rare and is always debilistly defined, debilist in the sense

33:39.320 --> 33:45.080
that it's not, it's not crisply defined in terms of data. So I'm trying to come up with models,

33:45.080 --> 33:53.400
AI models that take those types of expertise and create positive models that allow you to explore

33:53.400 --> 33:59.640
and make that a strategy decision. And there's an old model, an old model from... Actually,

33:59.640 --> 34:05.080
it's not that old, it's coming back, it's being used in medical diagnosis. It's called a fuzzy

34:05.080 --> 34:13.320
cognitive maps. So it allows you to apply... Fuzzy cognitive maps? Okay. It allows you to formalize

34:13.320 --> 34:20.200
a complex decision space and allows you to explore what are the possible outcomes for different types of

34:20.200 --> 34:28.200
input. Actually, it's very interesting. It says that, yeah, I also did it for... So I'm

34:28.200 --> 34:34.200
interested in this because it allows me to take my quantitative skills and apply to all these

34:34.200 --> 34:41.560
areas of research that have formerly... That usually has a quantitative analysis. So I did it for

34:41.560 --> 34:49.160
terrorism stuff, like trying to understand the dynamics of the support for terrorism. I recently

34:49.160 --> 34:58.120
did it for a model of great amount of sense to see this trap. I don't know if I pronounced that

34:58.120 --> 35:02.760
properly because I always write it up. I never actually talk about it. It's that to keep it in this

35:02.760 --> 35:09.560
trap. So the idea that I only have two powers, one rising and one previously dominant,

35:09.560 --> 35:18.040
that there is a tendency towards that interaction engine in war. So that's how the

35:18.040 --> 35:23.800
historians discuss it. But it'd be interesting to see if there is data to support that. So you

35:23.800 --> 35:30.600
have to create a model that represents that dynamics and explorers using our cross-term based methods

35:30.600 --> 35:35.960
to see whether it is actually the case that if dynamics are such as the historians describe,

35:35.960 --> 35:40.280
you get an increased likelihood for war or not. And both of those examples that you

35:40.280 --> 35:47.320
decided, you know, particularly pre-having previously mentioned, you know, unstructured data,

35:47.320 --> 35:56.600
natural language. The spaces are so incredibly broad. How do you begin to constrain them such

35:56.600 --> 36:06.200
that you can, you know, start to model? So this is where it's really valuable to work in a

36:06.200 --> 36:14.760
building full of non-engineers, experts and PhDs who are non-engineers and anthropologists.

36:14.760 --> 36:22.600
Because what happens is I walk into the building, somebody walks up to me, either political

36:22.600 --> 36:26.840
scientists, usually political scientists or anthropologists, they walk up to me and say, oh,

36:26.840 --> 36:31.640
we have this. This problem will be exploring probably over lunch. And we talk through it

36:31.640 --> 36:37.880
and I start thinking because I'm an engineer, but I by temperament and by character. I start

36:37.880 --> 36:42.520
thinking, okay, what types of models, what types of quantitative models can I use to try to say

36:42.520 --> 36:50.760
something interesting about this problem they are dealing with? That's how that usually starts.

36:50.760 --> 36:56.600
And growing back and forth, taking my intuition, taking my modeling with their expertise,

36:56.600 --> 37:03.960
is usually some sort of solution, some sort of an approach begins to emerge. And because

37:03.960 --> 37:09.880
it's, because this type of interaction is actually quite novel, at least it's, it's,

37:09.880 --> 37:15.640
it's something people tend to avoid because it's interdisciplinary work is sort of annoying,

37:15.640 --> 37:20.600
the language is different. It really is annoying, I guess. I'm not joking on this.

37:20.600 --> 37:28.040
So it's much of what I end up doing is really low-hanging through it because if I claim to solve

37:28.040 --> 37:34.920
the really hard problems in those in the other spaces that require the level expertise,

37:34.920 --> 37:40.360
I can't rightly, I can't rightly claim to, even if I'm talking to a lot of experts.

37:40.360 --> 37:46.200
Okay, okay. Those are my, those are those those two explanation and positive

37:46.200 --> 37:51.000
explanations. Those are sort of my, my pet project. I spend a lot of time thinking and developing

37:51.000 --> 37:58.440
models on that. For actual work, we do things like recently we had some work on use of

37:59.560 --> 38:04.040
reinforcement learning and, again, and generative research networks to try to develop

38:05.160 --> 38:11.720
planning solutions in a, in an agent-based simulation model. So is that type of stuff that

38:11.720 --> 38:20.200
keeps me sharp and keeps me abreast, but it's not, I don't need to produce like a production

38:20.200 --> 38:31.400
ready code on that. The whole space of applying machine learning to the strategic decision-making

38:31.400 --> 38:39.720
is quite a fascinating one. Can you maybe take us into a little bit more detail for either

38:39.720 --> 38:45.240
the examples that you gave? You know, what does the data look like that you ended up using?

38:46.200 --> 38:53.880
You know, what's kind of the process? Okay, so, so the details, so what happens here is that

38:53.880 --> 39:00.600
you're, you're necessarily doing a high-grade type of model. Like you're trying to create a model

39:00.600 --> 39:08.120
that takes either equal parts, expert, expert, elicitated, elicitated structure and data

39:08.120 --> 39:13.480
in form structure and trying to merge it into one. The last people I wrote on this,

39:14.680 --> 39:23.000
we had, we had, for example, we demonstrated using, like, our explicitization to create a map,

39:23.000 --> 39:27.800
a quantitative map that we could then use for forecasting and prediction, but then we used

39:28.840 --> 39:35.960
Google Trans Data to estimate, to essentially time series data, to estimate this transfer

39:35.960 --> 39:41.400
of certain types of connections. So that would be some typo, Bob. The technical term is

39:41.400 --> 39:45.080
differential, helping in learning, but in reality, it's just some form of advanced,

39:46.040 --> 39:51.560
advanced, as it's not the right word. It's just an involved, also, device learning technique.

39:52.520 --> 39:57.560
In general, there's that we have, and half the time this provision has to, some of this provision

39:57.560 --> 40:03.400
has to confirm expert, expert, expert, elicitation. So it's sort of a semi-supervised in the sense that

40:03.400 --> 40:10.040
there is expert guidance for, say, the directionality of the edges, and so the maps I'm talking about,

40:10.040 --> 40:16.280
they're essentially director graphs. So there's expert guidance on the direction of the edges,

40:16.280 --> 40:23.240
in that director graph, and then there is data, lots of providers learning using data,

40:23.240 --> 40:31.960
to try to tune the strength of those connections. There are other ways to do it. This is the simplest

40:31.960 --> 40:42.120
way I've found. It begins to push into the renal of causal inference using Bayesian belief networks.

40:42.120 --> 40:47.000
So they are trying to, I guess it's causal knowledge to discover that's technically what you

40:47.000 --> 40:52.840
need to have to try to do. You're trying to take a bunch, if you had no expert whatsoever,

40:52.840 --> 40:57.960
you're trying to take a bunch of data, a bunch of time series data, and properly,

40:57.960 --> 41:06.600
properly, in fare using algorithms like on junction tree algorithms or expectation

41:06.600 --> 41:13.800
propagation. They're trying to correctly fare both, mostly the strength of the connections,

41:13.800 --> 41:22.520
or if you're doing an actual knowledge discovery, the actual graph, the fantasy relationships

41:22.520 --> 41:28.840
between nodes and between our variables. But because I'm working at a different dynamic,

41:28.840 --> 41:33.800
it's just the same algorithms to use the same algorithms to treat them differently.

41:33.800 --> 41:39.080
So I guess the best summary there is, it's a form of causal knowledge to discover

41:39.080 --> 41:43.480
causal inference using a different type of, or different type of model.

41:43.480 --> 41:47.800
I'm so curious, though, about the data, for example, in the case of this,

41:47.800 --> 41:53.720
trying to even forget the way you frame the problem with the terrorism.

41:53.720 --> 42:01.320
Oh, probably so. So that first paper, that I will not, that was entirely expert driven,

42:01.320 --> 42:06.760
expertise driven, so we had a lot of data driven. Oh, it's a different type of data,

42:06.760 --> 42:11.400
in this end. Meaning, or rather not statistical. Not statistical, exactly.

42:11.400 --> 42:18.200
So it's a form of, the form of, we had a corpus, a language corpus. The way technically you'd

42:18.200 --> 42:25.800
want to do this in a fully, in a fully, in a full project would be stick at natural language corpus

42:25.800 --> 42:31.960
that's relevant to the topic of interest in this terrorism, create, identify the relevant

42:31.960 --> 42:38.920
keywords. So in this case, there are things like keywords related to how the local population

42:38.920 --> 42:44.760
feels, the group is how legitimate the field of group is, how acceptable the risks are.

42:44.760 --> 42:52.280
So you create filters, language filters for the types of, for the types of identified factors.

42:52.280 --> 42:58.760
And then you just basically look for core currents of those, of those, of those filter terms

42:58.760 --> 43:06.440
with other terms. And core currents will agree, the more, the more to, to types of terms,

43:06.440 --> 43:10.840
core curve, the more you put, you increase the edge of the strength of the, of the connection

43:10.840 --> 43:19.320
between those two things. So in some sense, it's, it's using, it kind of almost a graphical

43:19.320 --> 43:27.320
approach to sentiment analysis across this corpus. Do you have any recommendations for folks that

43:27.320 --> 43:36.680
are interested or intrigued about this, you know, these fuzzy models or applications of machine

43:36.680 --> 43:42.520
learning to strategic decisions or game theoretical applications? Where should folks start looking?

43:44.280 --> 43:50.920
So for the fuzzy models, for the fuzzy cognitive maps, I think the best, the best discussion

43:50.920 --> 43:58.840
is still, yeah, there is a summary book, I think by El Penicchi, Papadro, Drew, in Greece,

43:59.560 --> 44:05.880
it's out there. I can, I can send a link later on. It's just a survey of fuzzy cognitive maps,

44:05.880 --> 44:13.560
theories and applications, recent applications. And on the, the issue of, on the issue of causal

44:13.560 --> 44:21.400
inference, I would, I would say the books like, I'm just name, corals, books on causality,

44:23.640 --> 44:29.160
in a different, in a different tradition, I would say maybe, Rubin, it's not Rubin's books,

44:29.160 --> 44:33.880
there are a couple of books on propensity score, score methods that are relevant.

44:34.600 --> 44:38.920
So I have a ton of books, but I don't remember, I don't always remember the name,

44:38.920 --> 44:45.720
I never tried, I didn't remember them in the spot. On the issue of strategic, strategic decision

44:45.720 --> 44:52.120
making, using machine learning, I think, so that's definitely an interesting area,

44:52.120 --> 45:00.600
largely because I feel like that underserved in the literature. So I refer to, I refer

45:00.600 --> 45:06.440
increasingly, at least while I was working on that particular project, to a really old book by

45:06.440 --> 45:12.760
Axe-Rod, I believe it's David Axe-Rod, the structure of decision making. And for that style of

45:12.760 --> 45:19.400
graphical, graphical, coalesce of expert knowledge, I haven't seen as much work, but they're

45:19.400 --> 45:25.560
paper, the fuzzy cognitive map papers do that. So we had the first paper that caused a cognitive

45:25.560 --> 45:32.440
map of how, how the economics and appetite in South Africa interrelated. So most of the

45:32.440 --> 45:38.920
interesting strategic uses of fuzzy cognitive maps are still in papers, I think. There is another

45:38.920 --> 45:48.040
area of, so this algorithmic game theory idea, the idea of mechanism design to try to solve

45:48.040 --> 45:54.680
some of the incentive problems you're seeing. I think Tim Rothgarten has a book called

45:54.680 --> 46:04.040
algorithmic interaction. I think that's going to be an increasingly important area when we're

46:04.040 --> 46:10.920
thinking about feedback systems and decision making. You're going to need some type of mechanism

46:10.920 --> 46:17.880
design process to try to incentivize people to act in specific ways, and there you get to,

46:17.880 --> 46:23.320
with the use of algorithmic game theory, you get to throw in all your quantitative skills to try

46:23.320 --> 46:29.320
to improve decision making of a game theory to set it. I think Rothgarten's book is the

46:29.320 --> 46:36.120
best in that space. Well, Oshonday, thank you so much for taking the time to chat with me about

46:36.120 --> 46:44.840
this stuff, really interesting stuff. Let's talk about it. All right, everyone, that's our show

46:44.840 --> 46:50.760
for today. For more information on Oshonday or any of the topics covered in this episode,

46:50.760 --> 46:58.120
head over to twimlai.com slash top slash 182. For more information on the entire deep learning

46:58.120 --> 47:05.640
and daba podcast series, visit twimlai.com slash endaba 2018. Thanks again to Google for their

47:05.640 --> 47:12.600
sponsorship of this series. Be sure to check out the 2019 AI residency program at g.co slash AI

47:12.600 --> 47:24.040
residency. As always, thanks so much for listening and catch you next time.

