Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In this episode of our deep learning endoba series, we're joined by Oshonde Osaba, engineer
at Rand Corporation and professor at the party Rand Graduate School.
Oshonde and I spoke on the heels of the endoba where he presented on AI ethics and policy.
We discuss framework-based approach for evaluating ethical issues such as applying the ethical
principles laid out in the Velmont report and how to build an intuition for where ethical
flashpoints may exist in these discussions.
We then shift gears to Oshonde's own model development research and end up in a really
interesting discussion about the application of machine learning to strategic decisions
and game theory, including the use of fuzzy cognitive map models.
Before we jump in, I'd like to send a big shout out to our friends at Google AI, which
recently opened up applications for its 2019 residency program.
The Google AI Residency is a one-year machine learning research training program with
the goal of helping individuals from all over the world and with a diverse set of educational
and professional backgrounds become successful machine learning researchers.
Find out more about the program at g.co slash AI residency.
And now on to the show.
All right, everyone.
I am on the line with Oshonde Osaba.
Oshonde is an engineer at the RAN Corporation and a professor at the Party RAN Graduate
School.
Oshonde, welcome to this weekend machine learning and AI.
Glad to be here.
Awesome.
So, you did your PhD in machine learning and now you spend quite a bit of your time working
on the policy implications of machine learning and AI in particular with regard to ethics
among actually rolling up your sleeves and doing some model development.
Tell us a little bit about your background.
Oh, yeah.
So, I did my engineering degree at USC, the University of Southern California, across town
from where I am.
I mean, sunny Southern California, and Southern Monica by the beach.
Yeah, so at USC, I was focused on more theoretical aspects of learning algorithms.
So, I was trying to figure out how to improve machine learning algorithms, statistical learning
algorithms using the care for the injection of noise.
And it was a very theoretical type of thing.
I was finished.
I wasn't quite sure what to do with myself.
So, you know, as far as this is where you could either go up to see the come valley and
do some work for a commercial entity or figure out something to do in academia.
I sort of found a halfway point.
At Rand, Rand is a public policy think tank.
Oh, they don't like the word think tank.
We do a lot of thinking about policy problems.
A mandate that Rand is to improve decision-making through objective analysis.
And so, I had worked there over a summer while I was in graduate school looking, well,
I need money.
I worked there for the summer.
And I figured it was a pretty good fit.
So, I went back there and it was just around the time when lots of agencies, lots of people
across the country are starting to think about machine learning and the implications.
And so, I hit the ground running at Rand, just deploying models, deploying machine learning
models to answer that response questions.
And my growth at Rand has basically been me recognizing using my technical skills, what
I would recognize in some of the normative implications, I'm trying to address them.
The normative implications are using AI and machine learning in public policy spaces.
So, I tried to balance both now.
It's been an interesting ride, I guess.
And you recently returned from the deep learning and daba.
Tell us what did you present on?
Yes, I also asked me because I had written some stuff on the issue of bias in algorithmic
systems.
I was asked to talk about more broadly, if you think about it, the question of bias or
we call bias or equity in these algorithms, it's an ethical question.
Because the ethical norms, they retry to say, okay, do our systems adhair or do it by
a little ethical norms.
And so, in the past year, I've been trying to broaden that discussion quite a bit.
Not just on questions of ethics or bias, but also questions of ethics more broadly.
And so, the question becomes, when a commercial entity, when a public entity decides to use
machine learning for a particular decision problem, what are the ethical constraints, they
should be looking into, what are the frameworks by which they can judge themselves to make
sure that they're not violating x, y, z, ethical constraints.
So, my talk at the end of I was trying to make that more formal, as opposed to just a touch
of feeling discussion of ethics that often goes on, try to give formal, at least concrete
frameworks by which we can think about these things.
I like the sound of that.
It sounds like there were a number of frameworks that you presented.
Was there a specific number of them?
Have you categorized the framework space, if you will?
No, I have not, if I've done that, I think I would be very happy.
But I mean, that's like, no, that's a huge, a huge step.
But one of the things I relied on in discussing this was some of the work by Shannon Barlow-Altor
of Santa Clara University.
And the idea here is that if you try to impact in polls, normative, prescriptive ethics
on the system, that this not always, it doesn't always work.
Like, coming up with the universal ethical law for guidance, tech practice, tech development
is not usually an easy or feasible task.
So we went through this process where we went, okay, instead of trying to say this is
how this should be, instead of imposing norms on just unilaterally, we tried to elicit
the norms from case studies.
So the first part of the conversation at the end of that was, okay, here are a couple
of cases where people were thinking about using algorithms in a particular decision process
how people build their intuition on where the ethical flashpoints might be.
And then towards the end, we started coming up with frameworks.
The one framework I liked that I sort of pushed a little bit was the development principle
idea.
So if you think about the development principles, it's really the set of principles.
And I suppose that came out of a Belmont report, like I think in the 70s, trying to think
through what are, how do you think through the implications of human experiments, of experiments
that affect humans, so human subjects protection and stuff.
And so part of the, no, no, yes, EEL, M-O-M-T.
Okay.
So part of the, on the center, I was trying to pull into the spaces every time you develop
an algorithm to answer a problem that affects lots of people.
At this sense, a social experiment, it's like, it's a social experiment that affects
lots of human subjects.
So it's not, it's not, it doesn't seem too far of a step to say that we should at least
try to judge those new applications according to the development principles.
And so we have things like, uh, download principles are, uh, it's basically your, your
people critical in taking it a little bit further, so the basic one is do know harm, uh, there's
the great, then the other one is, um, make sure your application is just, and then the
third one is respect autonomy.
Now, those sound very abstract and high-fifileting, and so one of the things, one of the tasks I
gave myself was to try to, um, root those, make those more concrete in the case that
is we're talking through, I thought the deep learning in Hidabah.
And so the first one, you know how, actually interesting because, but the most part, we
all think we are doing the best we can.
So the, I think we decided that we, we limited not just to do know harm, but do know foreseeable
harm, understand that we don't probably, we are always able to, to understand the impact
and the piece of technology, any piece of model, any model has a particular space.
So those are the types of conversations, case studies, to, to prime intuitions, um, building,
building up, um, suggesting frameworks like the bell and press, at least the bell and
press full frame, to help us, um, frame new, new app, thinking of new applications, and
just trying to get general takeaways, general, understandable flashpoints, and I think about
ethics and technology practice.
Well, what were the case studies that you went into?
So, yeah, this is like two or three weeks ago, the first one was, I guess, we're trying
to do, so this is a currently famous, currently popular in famous is the right word, should
people use facial recognition technologies for filing at boarders, now it's the first
one.
And we're, one of the things I was trying to do there, because it's an, I work mostly
in the United States, the deep learning in Daba is, is a more, um, international community.
I, I sent out a survey to try to get, to try to get people's, people's perspectives
on these types of questions earlier on.
Okay.
And I, I made it, I made it such that the survey was, was tag based on where, what part
of the world these people, um, the survey respondents identified with, um, so we had people
like, yeah.
And the, the idea is that we're increasingly, we are having this issue of, um, our value
pluralism.
So let's, let's walk through the, the chain there.
The first part of the chain is, well, questions of equity, questions of biases are normative,
ethical questions.
The second part of the chain is that, well, different parts of the world have different
ethical norms that I wish to leave, but it's a very cultural thing.
And so part of the, the point of the survey was trying to elicit this, any disconnect,
any discontinuities in how, say, somebody in Europe thinks about privacy versus say, somebody
in Africa thinks about privacy.
So I, I, there was, in the survey was, they didn't really elicit those sharp disconnects,
but in the conversation at the end of that, there was this sense that some people thought
it was perfectly fine and some people thought it wasn't, it was, there were such privacy
constraints on these official recognition providing that the security benefits or any potential
security, security benefits of official recognition technologies were, were overwhelmingly
positive compared to the potential downfalls.
And so there is not the conversation in that case that he illustrated that there was no,
there was no unified perspective on the official recognition technology.
And so when you, you have a situation where there's no unified perspective that seems
like a perfect opportunity to try to apply some framework, how did you apply the framework,
the Belmont frame to the, to this particular case study?
So the, I think it's hard to take an issue, which there is polarization or significant
diversity of opinion and try to marshal it into some preferred, whatever that preferred
position is.
So the, the, the procedure or the, the way forward was to highlight that, okay, you want,
so one person brought up the idea that, well, governments, that technically more trustworthy
than say commercial companies at the India use of official recognition technologies.
And that is, that is a question that's terribly dependent on what your experience has been
with government and what part of the world you're from.
And so it's not like you cannot apply the framework and come up with a single answer that
should control.
So my, my job was essentially point out that, okay, if that's your perspective, recognize
that the respectful personal autonomy, that, that factor, that, that's the third principle
in the Belmont principle.
That is not, you know, that's not uniformly guaranteed across the world with different
governments.
And so every time you're making decisions, it's not a, it's not a question, or you should
always use facial recognition technology.
It's more a question of when you're thinking about it, ask these questions, these types
of questions.
And that might help you make more contextual, context sensitive answers to those, to, to
whether you should use the technology or not.
Is that the same kind of process that you would walk one of your, a ran client through?
Do you find that they, that they want more concrete answers or do they want, kind of these
experiences that help them understand the, the scope of the issues?
I think in general, when you're, when you're answering questions on, on behalf of somebody
else, there is a balance that needs to be struck.
You want to relay strong useful insights, strong useful frameworks for proceeding, when
you leave the room, but at the same time, there is a, there is an expectation that you
give them concrete answers, finally somewhat concrete answers to the issues that they are facing.
So I try to strike a balance between the two.
So I find somebody asks me, usually when I'm working on, so the, the presentation was a
little bit, the deep learning in that position was, was at a higher level of abstraction than
I would normally work at, because at, at that point, I'm trying to present as much insight,
as much abstract insight as possible.
And actually talking to clients, naturally dealing with governmental clients, I would, I would
usually start with a concrete demonstration of, okay, this is, this is a facial recognition
technology trying to, you, you're, you're suggesting that you might want to use, well,
these technologies have X, Y and Z, X, Y and Z characteristics, the most important in
the recent days, the recent months being that there is a differential error rate that has
implications for, for justice, you can treat everybody the same, you can treat everybody
equitably, whatever equitably needs life context.
And how does that affect your procedures, your operations, and what do you intend to do
to deal to address those, those are equity constraints, those equity constraints.
So usually I'm working, at least I'm talking to clients at that level, at the more concrete
level of, of detail, but the goal is not just to give them, if you can point out answers,
the goal is to have useful abstractions that help make sense of the world, so that when
somebody else comes with a similar type of problem, you can give them interesting answers,
informed answers.
What types of clients are these generally, and what types of questions are these generally,
and who's generally the actual client, what is their role?
People with experience at Rand Differ, it's one of those places where you make your own
way, my projects, most of my projects tend to be focused on questions I think are important
as opposed to client driven, but in general Rand as a whole has clients are biggest clients
are in the house, about just under how our work comes from what we call the federally
funded research and development centers, we have three of them, or I think we have four
now, we have one for the US Army, one for the US Air Force, one for the Office of the Secretary
of Defense, and one for DHS, and so we would generally take a series of portfolio of strategic
and tactical research problems for those four, those four clients, or subsection, sub
divisions of those four clients.
On the other hand, we have a health division, we have a justative restructure and energy
division, we have, we need to have a labor population division, but in general those
will, those will house our non-defense questions, and there you'd have people getting grant
money from NIH, NSF, or foundations like the Robert Wood Johnson Foundation, or the Gates
Foundation, to answer some policy relevant questions.
There isn't as much detail, I've hesitant to go into a lot more detail about the nature
and of the clients and the types of questions I go into because that might become detestive.
Sure, sure, no, that helps me understand the broad process by which folks are coming
to you and the types of questions that they might be looking for.
At the NW presented this one framework, are there other frameworks that you've relied
on to address similar types of ethical issues?
Also, there is one that I tend to, it's not so much a framework at all, I guess it's
a framework in a sense.
So oftentimes when people in the fairness, accountable, transparent, machine learning,
generally the AI policy space, think about machine learning models, they generally think
about it as single monolithic algorithm, single monolithic models that are solving single
decision questions.
Increasingly, there is this issue of systems level thinking, like even if every algorithm
in a particular system is unbiased, is transparent and all that stuff.
It doesn't mean that the outcome is on from the entire system and it's really going
to be fair unbiased.
Are there specific examples of that phenomenon that come to mind?
It is easier to pay attention to say the criminal justice system where you're always trying
to steal algorithms, but then we always try to steal algorithms at different parts of
the system.
For example, there's the risk criminal risk, risk racism estimation.
There's a risk estimation algorithm that has been popular, has been part of the conversation
for the past two years or so, based on public has worked, and the idea that has always
been all over, we just need to fix that.
Then there was an interesting conversation that has been happening over the past year,
where even if you take this issue up, essentially, if falls on the umbrella,
run away, feedback, run away, feedback loops in systems,
where, because even if you have an algorithm that's performing safe and optimal way,
historic inequities can cause the algorithm, can cause a system to diverge in outcomes
for different groups. So, this is sort of like perturbations of the initial conditions
for a system, for the criminal justice system, is causing this vast,
outsized differences in outcomes. And this happens even if the algorithms
themselves, without paying attention to the historical data, the algorithms themselves
are trying to behave optimally. So, that's the one, of course, the example I have seen,
but this behavior, same as one of the things I've been trying to do, is pay attention to,
essentially, the systems level feedback effect of what's just focusing on individual algorithms
and systems. So, we've been doing this for one of my projects' works on this equity concern,
for the criminal justice system, for insurance systems, onto insurance systems,
and for employment systems. And we emphasize the systemic aspect of what's
to this in individual decision point aspect. And it's usually a lot more, I don't know,
I find it a lot more enlightening than focusing on a single algorithm.
Does looking at it from a system's perspective make these problems more or less actionable
for the clients that are struggling to deal with them? I think it makes the analysis and the
discussion a little bit more complex, but the world we live in is complex, social systems,
which is algorithms that operate in the other complex. But one of the things that one of the
extra levels, extra dimensions, degrees of freedom, that this type of perspective gives you is,
well, okay, now algorithmism working out so well, you could do one of the two things,
you could modify the model directly, or you could pay attention to the outcomes and do post-hoc
modifications, post-hoc corrections, so the outcomes to try to correct any inequity in the model.
And one of the, so an example of where this has gone wrong is, so in child welfare systems,
I think there was a paper last year or the year before looking into the child welfare
system, they tried to use an algorithm to better estimate the risk of adverse outcomes for
children through that system. They had a situation where the algorithm was, was doing in terms of
predictive powers doing pretty well, but in terms of implementation, so we had these social workers
taking a look at the algorithm, but because either because of, they don't understand what the algorithm
is doing or they don't trust that the algorithm is taking everything into account, they decide
to circumvent whatever recommendations the algorithm gives them, so that's like a post-hoc
modification to the behavior of an algorithm. If you're focused on the algorithm alone,
then you're not going to, you're not going to pay attention to those types of,
those types of secondary effects that actually quite important to the real world.
And some of the work we're seeing in the criminal justice system is exactly this.
The algorithm is doing an analysis for, does not matter what. So the algorithm was fine,
but the post-hoc implementation was causing, was amplifying, very, very tiny difference in ways
that were of well and sustainable. That is important going forward as we start thinking about
where algorithms fit in systems. I guess it's a long way of saying yes, it's more complex,
but it gives you more degrees of freedom to play, to try to intervene to correct mistakes.
Right, right, right. So you're taking a step back and dealing with these kinds of problems,
you've mentioned two tools that you use. One is applying frameworks like Belmont. Another is
taking a systems thinking approach to looking at the problem. Are there other tools that you
use in this way? I mean, I'm still developing my thinking. Could I say that there are the tools?
Not so much. There is another space in which I think people that I've been trying to think of
frameworks. And this is the part question of how do you regulate the free use of algorithms?
Should you regulate the use of algorithms? And even if you wanted to, how would that work?
And so the standard, the standard approach to regulation has always been in polls laws.
If it was laws, either you can do this or you can't do that.
Increasingly for spaces where compliance is hard to, to assure, there is the idea of
where compliance is hard to assure, where norms are actually more important than hard regulations.
The idea of soft governance approaches becomes more, is beginning to get more attention.
I think the regulatory scheme, the frameworks for regulating the proper,
or the beneficial use of algorithms, that's still there a lot there, but there's a lot of
thinking going on there, but not much. There isn't as much hard and I guess there aren't as much
proven frameworks that I can point to. If you kind of take that thought exercise around
soft regulation and establishing norms and try to take on a broad issue like algorithmic
transparency, you end up in likely a very different place than something like a GDPR.
How would you compare and contrast the solution that you might end up with a
soft regulatory approach and something like GDPR? That's an interesting question.
I imagine that this is me giving my best educator guess and this is really the correct
answer to this. I imagine that the level of soft regulations that you're trying to build norms
and trying to build consensus that a broad coalition of people can agree to, a broad coalition
of players can agree to. The general thing about broad coalition is decision-making that
that record based on broad coalition building is it's going to be a weaker regulatory framework,
but if you're working in a highly polarized, highly fractionated space, maybe that's the best
you can do. GDPR is interesting because they come at it from a strong normative frame,
the idea that the consumer is key, that privacy is not an absolute right, but it's a highly
prioritized right, and that the people are central as opposed to the companies. That controls,
and it's an interesting and important perspective. It's a perspective that's been missing from
the history of American technology innovation, specifically when it comes to data driven,
data driven innovation, but I don't think a sub-governance framework would have come up with that.
For the yet word, here's the driving follow-up question. There are spaces where trying to assert
a primary normative frame is the best solution to getting compliance. So one of the things about
GDPR is there is a question of whether it's going to drive fragmentation and regulation.
So you have United States, California is following GDPR's footsteps, but the United States has a
federal system doesn't seem to be able to or want to or have the want to use a strong term to apply
to a system. It doesn't seem to be moving in that general direction. China is certainly not moving
in the same direction as GDPR. If you try to impose global laws concerning data privacy,
and you come at it with that strong normative foundation like what the GDPR does,
it's an interesting exercise to see if that was going to be a viable global
up switch to regulation. Right. Particularly in light of the global nature of the technologies
that they're aiming to govern. Interesting. So you spend roughly half your time on
Kennedy's broad ethical and policy issues, and another half of your time at Rand on model
development and kind of rolling up your sleeves and building systems or proofs of concepts for clients.
Is that work also in this domain? Are you exploring explainability models and transparency?
That kind of thing. I am a bit more exploratory tree in my technical work,
largely because for me, the technical development keeps me honest. When I try to talk about
the policy, regulatory and ethical framing, if I don't have a technical understanding,
I think I am generally off base if I don't keep that understanding. But in terms of the actual
types of technical models I tend to pursue. So there's stuff on the explanation. There isn't
as much. So DARPA has a program on this, a program unexplainably. I talk to them a couple of
times. Everybody talks to them to see what they're thinking. But I tend to, my perspective on it
is a little bit, I think the models I tried to build so I addressed that a little bit more
exploratory tree than what is probably the raining stuff. In general, I think where I've
spent most of my thinking, most of my technical thinking, I've been on two things,
older models. So this issue of explainability in machine learning models,
it's sort of, I work with people who grew up in the heyday of the ATV AI, AI boom. It's sort of
throwing back to that phase. So back then I had this system, like the systems that were super
explainable, they could do forward inference, like most of our machine learning models can do.
But they could also do backward inference in the sense that you give it a state of an output state
and it can give you a set of likely input states that have caused, which is one form of explanation.
So in the sense, those older systems were better at it than the current systems. And so one of
the things I have tried to be explored in the space of explorability is this hybrid of older
Xper system style models with current statistical machine learning models. This is not like
heresy in the sense that you have people like Jerry, let's go back at Stanford doing this thing
where he is using decision tree models to try to create a meta model summary of a black box model,
however that black box is. And what my approach is to refer more to what they call further systems.
So for the systems where these old experts, the expert systems based model, they were more,
they were used often for linguistic mapping between even then parts of sets of systems.
And so the idea there is to take a black box model, create a gray box model that is a bit more
expletable that sort of summarizes that black box model. And sort of see if you can get better
explanations, isn't that a gray box model. I think the couple of, so I mentioned Leskovich as
I approached to that, there was also Lipton's work on born-again networks, I think trying to address
it in a similar way. But these are all like a very tree type of model to see what's possible,
what's feasible. But the other, the one I actually care about, well, care about is in my care about
all these models. The one I actually spent a lot of time thinking about is the use of machine learning
models or at least AI-stamp models to try to answer strategic questions in complex adaptive systems.
So you have the situation where you have a stick, let me try to give it a useful example.
So half, I went to paper a while ago, like a year or two ago, where you had a bunch of experts
across the world who have thought very carefully about what does it take for local population to
begin to support terrorism. So the common public support for industry and terrorism model.
So you have all these experts pulling their insights to come up with interesting factors
and interesting dynamics for how this might work. But that's all linguistic, that's all
unstructured data. So one of the things I've been trying to do, and if you think about what
RAND does, RAND is very focused on trying to improve decision-making, even in those types of
complex domains where expertise is rare and is always debilistly defined, debilist in the sense
that it's not, it's not crisply defined in terms of data. So I'm trying to come up with models,
AI models that take those types of expertise and create positive models that allow you to explore
and make that a strategy decision. And there's an old model, an old model from... Actually,
it's not that old, it's coming back, it's being used in medical diagnosis. It's called a fuzzy
cognitive maps. So it allows you to apply... Fuzzy cognitive maps? Okay. It allows you to formalize
a complex decision space and allows you to explore what are the possible outcomes for different types of
input. Actually, it's very interesting. It says that, yeah, I also did it for... So I'm
interested in this because it allows me to take my quantitative skills and apply to all these
areas of research that have formerly... That usually has a quantitative analysis. So I did it for
terrorism stuff, like trying to understand the dynamics of the support for terrorism. I recently
did it for a model of great amount of sense to see this trap. I don't know if I pronounced that
properly because I always write it up. I never actually talk about it. It's that to keep it in this
trap. So the idea that I only have two powers, one rising and one previously dominant,
that there is a tendency towards that interaction engine in war. So that's how the
historians discuss it. But it'd be interesting to see if there is data to support that. So you
have to create a model that represents that dynamics and explorers using our cross-term based methods
to see whether it is actually the case that if dynamics are such as the historians describe,
you get an increased likelihood for war or not. And both of those examples that you
decided, you know, particularly pre-having previously mentioned, you know, unstructured data,
natural language. The spaces are so incredibly broad. How do you begin to constrain them such
that you can, you know, start to model? So this is where it's really valuable to work in a
building full of non-engineers, experts and PhDs who are non-engineers and anthropologists.
Because what happens is I walk into the building, somebody walks up to me, either political
scientists, usually political scientists or anthropologists, they walk up to me and say, oh,
we have this. This problem will be exploring probably over lunch. And we talk through it
and I start thinking because I'm an engineer, but I by temperament and by character. I start
thinking, okay, what types of models, what types of quantitative models can I use to try to say
something interesting about this problem they are dealing with? That's how that usually starts.
And growing back and forth, taking my intuition, taking my modeling with their expertise,
is usually some sort of solution, some sort of an approach begins to emerge. And because
it's, because this type of interaction is actually quite novel, at least it's, it's,
it's something people tend to avoid because it's interdisciplinary work is sort of annoying,
the language is different. It really is annoying, I guess. I'm not joking on this.
So it's much of what I end up doing is really low-hanging through it because if I claim to solve
the really hard problems in those in the other spaces that require the level expertise,
I can't rightly, I can't rightly claim to, even if I'm talking to a lot of experts.
Okay, okay. Those are my, those are those those two explanation and positive
explanations. Those are sort of my, my pet project. I spend a lot of time thinking and developing
models on that. For actual work, we do things like recently we had some work on use of
reinforcement learning and, again, and generative research networks to try to develop
planning solutions in a, in an agent-based simulation model. So is that type of stuff that
keeps me sharp and keeps me abreast, but it's not, I don't need to produce like a production
ready code on that. The whole space of applying machine learning to the strategic decision-making
is quite a fascinating one. Can you maybe take us into a little bit more detail for either
the examples that you gave? You know, what does the data look like that you ended up using?
You know, what's kind of the process? Okay, so, so the details, so what happens here is that
you're, you're necessarily doing a high-grade type of model. Like you're trying to create a model
that takes either equal parts, expert, expert, elicitated, elicitated structure and data
in form structure and trying to merge it into one. The last people I wrote on this,
we had, we had, for example, we demonstrated using, like, our explicitization to create a map,
a quantitative map that we could then use for forecasting and prediction, but then we used
Google Trans Data to estimate, to essentially time series data, to estimate this transfer
of certain types of connections. So that would be some typo, Bob. The technical term is
differential, helping in learning, but in reality, it's just some form of advanced,
advanced, as it's not the right word. It's just an involved, also, device learning technique.
In general, there's that we have, and half the time this provision has to, some of this provision
has to confirm expert, expert, expert, elicitation. So it's sort of a semi-supervised in the sense that
there is expert guidance for, say, the directionality of the edges, and so the maps I'm talking about,
they're essentially director graphs. So there's expert guidance on the direction of the edges,
in that director graph, and then there is data, lots of providers learning using data,
to try to tune the strength of those connections. There are other ways to do it. This is the simplest
way I've found. It begins to push into the renal of causal inference using Bayesian belief networks.
So they are trying to, I guess it's causal knowledge to discover that's technically what you
need to have to try to do. You're trying to take a bunch, if you had no expert whatsoever,
you're trying to take a bunch of data, a bunch of time series data, and properly,
properly, in fare using algorithms like on junction tree algorithms or expectation
propagation. They're trying to correctly fare both, mostly the strength of the connections,
or if you're doing an actual knowledge discovery, the actual graph, the fantasy relationships
between nodes and between our variables. But because I'm working at a different dynamic,
it's just the same algorithms to use the same algorithms to treat them differently.
So I guess the best summary there is, it's a form of causal knowledge to discover
causal inference using a different type of, or different type of model.
I'm so curious, though, about the data, for example, in the case of this,
trying to even forget the way you frame the problem with the terrorism.
Oh, probably so. So that first paper, that I will not, that was entirely expert driven,
expertise driven, so we had a lot of data driven. Oh, it's a different type of data,
in this end. Meaning, or rather not statistical. Not statistical, exactly.
So it's a form of, the form of, we had a corpus, a language corpus. The way technically you'd
want to do this in a fully, in a fully, in a full project would be stick at natural language corpus
that's relevant to the topic of interest in this terrorism, create, identify the relevant
keywords. So in this case, there are things like keywords related to how the local population
feels, the group is how legitimate the field of group is, how acceptable the risks are.
So you create filters, language filters for the types of, for the types of identified factors.
And then you just basically look for core currents of those, of those, of those filter terms
with other terms. And core currents will agree, the more, the more to, to types of terms,
core curve, the more you put, you increase the edge of the strength of the, of the connection
between those two things. So in some sense, it's, it's using, it kind of almost a graphical
approach to sentiment analysis across this corpus. Do you have any recommendations for folks that
are interested or intrigued about this, you know, these fuzzy models or applications of machine
learning to strategic decisions or game theoretical applications? Where should folks start looking?
So for the fuzzy models, for the fuzzy cognitive maps, I think the best, the best discussion
is still, yeah, there is a summary book, I think by El Penicchi, Papadro, Drew, in Greece,
it's out there. I can, I can send a link later on. It's just a survey of fuzzy cognitive maps,
theories and applications, recent applications. And on the, the issue of, on the issue of causal
inference, I would, I would say the books like, I'm just name, corals, books on causality,
in a different, in a different tradition, I would say maybe, Rubin, it's not Rubin's books,
there are a couple of books on propensity score, score methods that are relevant.
So I have a ton of books, but I don't remember, I don't always remember the name,
I never tried, I didn't remember them in the spot. On the issue of strategic, strategic decision
making, using machine learning, I think, so that's definitely an interesting area,
largely because I feel like that underserved in the literature. So I refer to, I refer
increasingly, at least while I was working on that particular project, to a really old book by
Axe-Rod, I believe it's David Axe-Rod, the structure of decision making. And for that style of
graphical, graphical, coalesce of expert knowledge, I haven't seen as much work, but they're
paper, the fuzzy cognitive map papers do that. So we had the first paper that caused a cognitive
map of how, how the economics and appetite in South Africa interrelated. So most of the
interesting strategic uses of fuzzy cognitive maps are still in papers, I think. There is another
area of, so this algorithmic game theory idea, the idea of mechanism design to try to solve
some of the incentive problems you're seeing. I think Tim Rothgarten has a book called
algorithmic interaction. I think that's going to be an increasingly important area when we're
thinking about feedback systems and decision making. You're going to need some type of mechanism
design process to try to incentivize people to act in specific ways, and there you get to,
with the use of algorithmic game theory, you get to throw in all your quantitative skills to try
to improve decision making of a game theory to set it. I think Rothgarten's book is the
best in that space. Well, Oshonday, thank you so much for taking the time to chat with me about
this stuff, really interesting stuff. Let's talk about it. All right, everyone, that's our show
for today. For more information on Oshonday or any of the topics covered in this episode,
head over to twimlai.com slash top slash 182. For more information on the entire deep learning
and daba podcast series, visit twimlai.com slash endaba 2018. Thanks again to Google for their
sponsorship of this series. Be sure to check out the 2019 AI residency program at g.co slash AI
residency. As always, thanks so much for listening and catch you next time.
