All right, everyone. I'm here with Parvez Ahmed. Parvez is head of data science applied research at LinkedIn. Parvez, welcome to the Twoma AI podcast.
Hi, thanks for having me. It's very nice to be here. I'm really looking forward to digging into our conversation. We had a really interesting pre-call. I love the way you described your kind of organizing principle for the group as well as the analogy that you make for how you decide what you work on and we're going to dig into those areas.
But before we do, I'd love to have you share a little bit about your background and how you came to work in the field.
Sure. It's probably a little bit of a non-linear trajectory. My PhD is from Berkeley. I specialize in computer vision, machine learning.
So my major is electrical engineering computer sciences. After that, I actually wanted to spend some time working on problems more in the biology site. So my postdoc was with Gene Myers working on hythropod, you know, bioinformatics problems.
I worked in a place called Horde Huesment Clean Institute, Genelia Farm Research Center. Now it's called Genelia Research Center.
After some time there, I had a very small lab where we worked on computational models and tried to intersect them with some experimentation models set of experimentation that we were doing.
In 2014, mainly because of some personal constraints, family related aspects, we decided to move to industry.
Since then, I chose to mostly focus on building products end to end. So I've been in both tech and biotech sites of business, but mostly in startup site.
I had a chance to build at least three products end to end. Over time, also transition more on to the management side have been building teams. I came to LinkedIn in 2019. And I took over this team that was actually.
Yeah, the idea is the head of data science at LinkedIn about how to set up a horizontal team that focuses on, you know, more applied research problems. So I've been here about two years.
And your team kind of sits at the intersection between research and products. How do you think about where the team sets.
Another question. So, I mean, it might help you to have a broader sense of how the data science is organized at LinkedIn.
There is a data org, which is a key pillar in engineering organization at LinkedIn and data art has three, you know, three or four key pillar organizations under it.
One of them in data science classically at LinkedIn, data science operated in what we call a vertical fashion where, you know, there is a line of business.
Let's say LinkedIn marketing solutions or LinkedIn talent solutions are even like flagship, which is essentially the central blue app for LinkedIn.
There is a team that's very closely associated with this work, a particular line of business and focusing on problems that surface there.
I try to solve those problems in a very iterative action work very closely with the PM associated with that line of business.
I think one of the things that is relatively new couple of years ago.
We started these horizontal teams that at least couple of them, one of them is called data science applied research, which is my team.
And there is a sister team that we have called data science productivity and experimentation.
So, this team, my sister team is more tooling focus. So, it's a lot of tools.
Our focus is more on methodology, algorithms, platformization.
So, there was a bit of an open space in terms of how we set up this team, what are the right things to focus on, how do we choose where we spend our time sort of idea.
What we did is actually to have this conversation democratically with everybody in the team, we were like 10-ish people when I started.
We are close to like 20 plus people today.
So, we had a series of conversations in team meetings about, you know, what are the considerations we need to take into account.
And in summary, what I see it as as interfacing between really having an impact.
So, applied is a very important part of how we position ourselves.
We actually do care about solving problems. So, we are very problem oriented.
But we want to do so by going deep into hard technical problems and really solving them.
And there are two investment pillars that I mentally use.
One is how do we pick problems that essentially if we solve them very well, help multiple lines of businesses.
Take for example, something like doing a really good job on experimentation helps multiple lines of businesses evaluate how their products are working, help them, you know, iteratively ship the products better.
Something like forecasting, if you do a really good job, it also allows multiple businesses to have an ability to set their goals and actually measure how things are going and recognize when things are not going well.
So, the utility across multiple lines of business is a key pillar for how we think about it.
And the second important pillar is what is the strategic scope or an impact.
So, on this pillar, it doesn't need to be actually a native part of the products today.
But something that we believe is going to be really important for us to invest in.
Some of this insight comes from executive leadership who is thinking ahead about where the company is going in the next two years, five years.
Some of it comes from us thinking really deeply about where, you know, LinkedIn is moving.
What does LinkedIn in three to five years look like?
Examples of this kind would be how do we create products that actually are privacy preserving?
What kinds of technologies do we need to invent or operationalize?
Other aspects could be that, you know, LinkedIn has done a fantastic job of making experimentation a native part of developing products.
But most of the experimentation, you know, traditionally sets up a certain set of metrics.
We watch these metrics and we decide based on these metrics, but what about other things that are moving?
Are there unintended consequences that are happening?
How do we think about them?
So, this notion of, you know, trying to understand unintended consequences of the way we develop products was something that we wanted to invest in.
And that's a, that's also became a focus area in our team.
So, to summarize, I think we pick problems that actually either have utility across multiple lines of businesses and going deep into it helps, you know, across the world.
Or we pick problems that actually have really strong strategic utility where knowing whether to invest further or not has value at a very strategic level.
So we essentially go look into that.
Right now, so each of these have two focus areas or tracks that actually operationalize where we put resources into.
That's where we are.
I think in terms of focus areas, that's the thinking, but there was also a couple of important elements.
We explicitly wanted to be problem oriented, so the applied part of our name is actually a significant modifier.
We do care about solving problems. We want to ground ourselves in really solving hard, large scale problems.
LinkedIn offers a lot of opportunities of this kind.
There are also certain questions that we actually asked ourselves to kind of motivate where we put our time and energy.
I may not be able to go into that, but so there was a lot of thought into it.
One important factor from people point of view is.
The types of problems that we want to go into, sometimes you may need to spend significant amount of work, like maybe multiple quarters of somebody investing their time and going deep into it.
How do we orchestrate ourselves so that we stay relevant are actually solving the problems, but don't send people into rabbit holes where they go find out an answer.
If it doesn't work, we want to make sure that the team is organized in such a way that it doesn't hurt their career trajectory.
If you're building a product and you're a machine learning engineer working for the product, then your fate or the career trajectory is pretty tight to the fate of this product implicitly.
When you operate horizontally, we needed to think about how do we think about doing good to the people in my team while at the same time having an impact across the board.
And what does that mean concretely in terms of how you help folks ensure that the time that they invest on these long term projects is going to be a creative to their careers and how you manage the overall portfolio.
Yeah, I think so you brought up a term that I will actually double click on and go into it.
I see our team as essentially a portfolio of investment PCs.
You think of it like a white combinator of applied research tracks, and that was the analogy that I was alluding to earlier that I thought was pretty interesting when we first discussed it, so please elaborate on that.
Yeah, I mean, I came from startups, so I'm using something that I'm a little familiar with.
So partially, I think it's my own personal bias, but it helps me give a nice framework in terms of thinking where we are and like in terms of trajectory.
In the process of building end to end products in my startup experience, what I see is different phases. You can go from an idea all the way to something that is generally available, shipping, signing contracts, making money.
There are different phases that this idea goes through.
You know, you have to build sometimes a proof of concept, then you have to build an MVP, then MVP needs to get a little bit more shaped to become an alpha beta product, and then you need to scale it, you need to drive adoption.
Different aspects of these in if you translate them to LinkedIn's ecosystem, require different types of partnerships.
It's hard for any single team to do this all alone, because LinkedIn is very functionally organized.
So what we do is essentially, you know, first recognize whether we have interesting idea and it's worth investing in.
And do we have actually an understanding that if done well, this is going to have a significant impact, then we try to actually find people that we feel like have the core competency to take it from A to B.
This is also very important, if you have a really good idea, but you don't have the right set of people and the right mindset, it's not going to go well.
So in that sense, also the Y combination analogy is very, very appropriate.
People do absolutely matter.
So what we have done is to basically say, okay, if it is very researchy idea, let's say, you know, how do you think about every testing for fairness, which we have been working on for multiple years, or how do we think about building actually usable products using differential privacy and roll them out at scale, then you have to start very early.
Try to identify, is it even doable? What kinds of gaps do exist? Is there actually an algorithmic gap? Is there a methods gap? Then once we feel like we have the story together, try to have a partnership with the product and a real problem where we can demonstrate this through a proof of concept and start slowly going through this kind of.
Each day series, series B, as I call it in my head, it's not that necessarily, this is how it's operationalized, but it has given me a mental framework to think about it in a more holistic way.
And we always decide, do we want to go further or stop here? Do we want to go further or stop here?
What that does is that if let's say there is an idea and we have gone to a decent depth and it turns out that either that's enough and actually this is the extent that we need to go or there is nowhere to go and it's a dead end.
We can stop and we can actually stop risking an individual that's actually spending their time because it's important for their career.
But in the cases where we have gone somewhere and turned out that this is a dead end, it saves company costs of other people going into that extent, right?
Because when we come back, we would have a very comprehensive idea about, hey, these are all the kinds of things you can do and it doesn't work.
But we have several positive examples where we invest a little bit and it starts to grow and we see utility and organically starts to grow.
So we have examples of both good and bad kind, but we want to be able to start new things or drop things that aren't working and be much more driven by market fit within the LinkedIn ecosystem.
One thing that I'm finding interesting in this part of the conversation is we often talk about the need to establish your success criteria early on at the very beginning of the project.
So you know when it's good enough, right? So you're not chasing this incremental percent that's not really adding value. And there's, you know, in some ways, attention or contrast at least between, you know, that's upfront establishment of parameters versus what you're saying is maybe more incremental or at least reminiscent of kind of a stage gate type of approach to.
You know, we're going to have some set of milestones or some, you know, frequency with which we reevaluate.
And I think the, maybe the connection between those two is that the first is focused on success criteria and the second is focused on failure criteria.
Is that the right way to think about it?
Let me see if I can speak to this. I think having success criteria or an understanding of what is what does success mean is important, but it's easier to phrase when you can see line of sight where you're going.
And let's say you're working on, let's say you're working on developing deep learning app in a computer, imagine, and you're 20 years ago, how are you going to know that this is going to work in a very particular way you don't know.
Some of the problems that we're investing in, I don't have a line of sight yet. So what I've tried to do is to phase it out in the way we operate and be fairly agile about checking in. I think it keeps us honest.
But it, so what we do is to have a horizon saying that, okay, let's spend, you know, let's say, let's send two really competent people three or four really competent people along this line and give ourselves this much time and say we see where we are and really kind of take a very experimentation type approach in the way we approach these things.
And you do have to have your strong hunches. So if you are operating in these spaces without really understanding how research work, it obviously is hard to make this judgment calls.
But you need to be open to the fact that you might find a very interesting insight that you didn't know before and you need to be able to pivot on that.
For example, you know, there are some examples that I'm thinking about as I tried to illustrate, we started this project to do explainability in my team.
When I came here, it was like one of these fledgling early stage projects, I needed to decide whether to keep it or not.
I have a worked on explainability before in the context of like the product that we built in biotech. So I also saw a way to actually operationalize and we had a really good person. So we start working on it.
Fortunately, we were working on it just in the sense of hey, how do we solve it for go to market and we did something for a particular product and it turned out to be actually go to market, meaning what?
Go to market means, you know, Lincoln has multiple products that is trying to sell. It's a cluster of SaaS offerings.
If go to market implies, you know, if somebody who is the right customer to sell to, how do you take this product to the market?
Because we have our own sales force, our own marketing force, how do we actually help them do their job better empowering them to actually go to the market with our products.
So an internal use case for enabling your sales and marketing teams.
Exactly. It's a well-fraised use case. So we can evaluate it. Okay. Is it working or not?
And also once you sell your product, sometimes there are adjacencies. So you can also always do like cross sales upsells and so on.
So we started there. We tried to see, okay, can we solve this problem for a couple of the problems?
It turns out that hey, we can do it in a generalizable way and we make a call. Do we keep going or we stop here and we decide to keep going on that one and essentially start to expand how it's done.
Sometimes we have to come back and redo. You know, our stack so that it actually scales better. For example, we had written a product stack.
Then we learned that it's actually better to translate it to an internal auto ML like framework called pro ML that's very large inside LinkedIn.
So we actually translated our learnings into a pro ML component and just evolve from there to come back to your original point.
We do have success criteria. We have a sense of what that success mean, but it's faced. And we are open to the fact that between phases, these things can evolve and be open to it.
Yeah, I think one of the things that I was that just kind of peak my interest is that, you know, when you're invested in a project, either, you know, as a team or as an individual, you often get in the scenario where success seems like kind of, you know, just one short win away.
And it's very easy to keep, you know, to continue to double down, double down, double down. And it's, you know, it's hard to know when to, you know, one to pivot, one to cut bait, you know, Seth go didn't wrote a book, the dip, which is like, you know, to set go to book.
It's 60 pages, but it's about this idea applied to careers and the like, like, how do you know when you should change direction.
It's a tough set of problems. Yes, it is. I think it's a problem at multiple levels, but what I really want to solve it for. And I think we have done a good job with LinkedIn is to solve it at the individual level because at an individual level, this problem exists.
And I am really invested in making sure that people that are in my team have a good experience and that look back at the time that they have spent in the team and they think that we did some amazing things.
You know, balanced good time. What is at least in my experience, I've been a researcher, right, I, you know, I've been principal engineer, I have been through this.
So some of it is colored by how I learned to balance these things.
What I have come to realize is that I think as a researcher or as a machine learning practitioner, you should also have a portfolio of projects.
Sometimes, you know, which way one of these or a few of these are going, sometimes you absolutely don't.
But we have a notion of 20% projects, something that we borrowed from an idea that Google popularized sometimes ago.
But what I try to do as a manager is to make sure that people are not indexed on something fully that they have a portfolio at the individual level and the portfolio actually ladders up very nicely to the broader applied research portfolio.
Then what's happening is that sometimes some things we would like for them to be table stakes.
Something that this person with their capability can do and keep going so they can actually justify their job and being funded.
Some things probably is focused on what they truly believe in and maybe they have a really good intuition.
They want to go there and if it doesn't work, that's fine because we are learning something about the way it's not working.
A key component of my team being very, very successful is to create this space where people can go and actually fail and still come out with a reasonably looking average results for their career saying that,
I tried there are two things that was terrible. I tried and nothing worked and there are two or three things that I tried where actually you know things work.
So what it does is maybe it allows the passion projects to have the individual bigger which passion projects need and then also contribute to more team oriented projects where maybe you are doing your part and you're essentially moving the larger project along and.
You know it's probably a little bit more clear what to do and bills have portfolio at your level and it also allows me to have balance portfolio at the air level.
Because I want people to take risks, but I also want people to not get burned out by the fear of you know failing and like have the ability to fail and stop.
Yeah, I thought it may be useful to dig a little bit into a couple of these use cases to both you know because I'm curious about how you approach them, but maybe you know they might also provide interesting examples of the portfolio management aspects that we've talked about and the team and culture aspects that we've talked about.
In particular, LinkedIn's approach to experimentation and then you've got this more strategic aspect of that or extension which is trying to quantify unintended consequences and experimentation.
Can we dig into those areas a little bit and have you share a little bit about what you're doing now.
So let me anchor my comments on what I said earlier, which is that we have these two pillars through which I think about how we are spending our resources on the cross lines of business utility type of an idea.
The right now experimentation is in everything that we do. I mean, most of the features that essentially get rolled out are experimented upon we understand how the impact is happening.
I should maybe interrupt to note that in this context, we're talking about online experimentation.
We're talking about those two kind of experimentation during model development, I kind of think that's right, that's right.
I mean, the scale of experimentation, I think driven by a bunch of people, yeah, I was the one of the architects of this, yeah, has been on your show before is impressive.
It is very impressive just the ability to have an understanding of iterative roll out and understanding how certain ideas of market fit is really amazing.
I haven't seen that before in my experience.
So that it has impact on multiple lines of businesses. So you can at least park it as hey, this is actually something that we have done an IPO like it's well understood has been scaled.
Where do we go from there? It's a different set of challenges that that particular area has.
For something like forecasting, which also has multiple lines of business utility.
We needed to basically start from an idea saying can we just use open source tools to solve this problem.
So we spend some time to essentially do a survey and say, okay, how are things behaving?
The specific problem in that case is essentially trying to deliver something like forecasting as a service that serves multiple organizations within LinkedIn.
That's right. I think the problem is that if different businesses or different groups have their own forecasting done using their own tools, it's hard to roll them up to provide a cohesive view at the business level.
And one of the things platforms offer is that if you have a consistent use and it's trustworthy, then everybody is using the same framework and it allows you to ladder up.
Different businesses, different components and have a like a much more holistic understanding, right?
So it at an exact level, this makes things easier. It also easier to understand.
In the COVID context, obviously, this was very much in demand because there was a lot of volatility. So there's a lot of interest in this kind of work.
The first thing was to essentially look around, look at the tooling that's available.
See if we can just reuse it or maybe build rappers to scale it up.
Then we start running into situations where internal customer, let's say, marketing services has certain forecasting problem and the external tools are not solving it.
Then we try to understand, hey, what is the aspect that we can do better to satisfy the requirements of this customer?
Maybe for example, they need a iterative fast RCA, they probably need much higher customized building and tuning that maybe the external tools are not offering.
So that gives us a very clear picture of what to build, where to innovate.
Then we go a little bit deeper and start building out.
The forecasting work at this point is continues to grow. We have 12 different use cases that are already onboarded and we also had to make design decisions about do we purely build it as a service where everybody has to adopt or actually we also build it as a service and a library where maybe somebody does not want to use their service but wants to incorporate the algorithm library into their service internally.
But engineering decisions we have to contend with and be mindful about how we go.
We open sourced great guide, I think last week, very recently, seems to be well received so far, but with any of these kinds of effort, I feel like we know as much as we do and we do as well as we can.
So if there are cases where it's not working and we have chances to improve, we continually invest in it. It's been a really wonderful journey.
One of the things as a manager that I really enjoy is that when you're building things of this kind, people come together, they organically start to form teams sometimes.
It's like watching Beatles come together where there is a couple of really strong engineers, the different set of people form and they're creating something together and that process for me.
People learn how to work together, people actually understand each other's strengths and weaknesses and produce something that goes out and is very well received.
That's beautiful and I mean probably one of the most fun aspects of me running my team.
On the strategy side, but before we do that, since you mentioned it, I'd love to double click on, I can't believe I said that I hate that as a tech aphorism double clicking on things, but let's maybe dig into, I don't know if that's any better.
And Silverkite a little bit and what you're looking to do there, what you produced and open source and even it was open source, was that a goal originally? How did that come about?
I mean, what we are fundamentally trying to solve is to build something that can solve multiple forecasting use cases across LinkedIn. We want to platformize. That's our primary goal.
LinkedIn has a culture of sharing what we build with the external community, something also I find appealing.
A lot of the people in my team are very much subscribed to this philosophy.
So when we had something that was working and we had an ability to allow other algorithms to be plugged into a unified interface, we thought, hey, we should put it out there.
So if you know somebody wants to use it, they can use it.
So it's also good for folks in my team to have this visibility. I think it's great for their career, but we also genuinely want to encourage people to co-develop.
There have been things that have gotten open source from LinkedIn like Kafka, you know, that have become bigger.
I don't compare ourselves to that. I think there are much more stellar successes, but we build something we are proud of it.
And we think that it's useful. So we just want to share it with the general community.
And the hope is that it will bring like-minded people and it will also help us understand what are the things that we didn't do well, maybe give us a chance to get better.
But our primary focus is to serve our internal customers. We have a lot of them and we actually want to provide them a state of art solution that can solve their forecasting needs.
There, I think we are continuing to prioritize it. We have a wonderful partner in our sister team in the data org that's much more engineering capability type of a team.
So we have a strong partnership to try to prioritize it internally and it's ongoing.
One of the nice side effects of getting the forecasting part right has been that we now are starting to look at a broader spectrum of times here is problems.
So forecasting is a sister problem to anomaly detection.
Most people that come to you and ask for forecasting or anomaly detection also want root cause analysis.
So there are a lot of statistical problems that are very adjacent.
So initially we were very focused on one and then we got it right.
We feel like we have a piece of your answer. We are trying to slowly build something that's more holistic.
What we want to do aspirationally is to try to get to the level that experimentation got at LinkedIn.
To basically inform the performance management and like change the culture of performance management.
There is still a lot of people trying to do week over week year over year.
It's nothing wrong, but it assumes certain steady state to the business that we don't see.
So we are actually trying to provide tools where forecasting and become a maximum natural component of how people do metrics performance management.
You were elaborating on the strategic side.
Yeah, so I can go to that and the strategic side.
It requires thinking a little bit ahead. So the way I think about it is, you know, LinkedIn is where we are today.
So can you imagine what LinkedIn's problems would be and what its place would be in the Microsoft ecosystem.
Let's say three years out, five years out.
The main two investment areas for us on the strategic side has been one is called computational social science.
Predominantly, the work that has come out of there is the every testing aspect of fairness that's actually been open source.
It's.
So there is a blog post from Guillaume and LinkedIn is ranked blog.
And we continue to actually work on how to understand our products, how to understand unintended consequences of our products.
We also look at other elements like, you know, what is the correct way to or maybe better way to build a professional network.
There was a blog post from Indian a few weeks ago on this topic on the separately.
We feel like differential privacy as one of the elements of privacy engineering is something we felt like, you know, very strongly about investing in.
The reasons are the following, you have to remember, I before LinkedIn, I was in farmer or biotech space.
And in biotech space, sometimes even if you're trying to do the right thing, let's say you're trying to develop drugs that actually help people with mental health, which was essentially the mission of the previous company I worked at.
You need data in order to build data products, you need data and certain companies have certain data and it becomes really hard to build good algorithms with very small biased data.
But if you try to collect data across multiple companies, it's extremely hard for some sometimes for good reasons because you're trying to have protections.
But this brings into the question the problem where you are trying to join datasets from different entities.
In the case of LinkedIn, let's say, LinkedIn has some interesting data and let's imagine that Microsoft has certain interesting data.
Can you build a data product that synthesizes data between them, but in a way that does not compromise the commitment we are making to the members.
We have made certain commitments about member privacy, we actually really care about respecting them.
LinkedIn is very much about members first.
Can we build technologies that essentially guarantee that the member privacy is ensured that the re-identification kinds of risks are minimized while still providing utility.
So I find different privacy really interesting from this context.
Now, when we started, this is a new area for me.
So just in a complete spirit of honesty, I had to learn a lot, but I was very fortunate that I started working with a couple of engineers in my team that are fantastic.
And they know the problem really, really well. Ryan has been on your podcast before at the time when I think he's a Newrips paper on TalkK, D.B. Algorithm came out, fabulous guy.
But we started working on it. I started to understand what the technology is capable of, where the gaps could be in the context of our problems.
Then we started to invest today. I feel like my view is that there are two large classes of differential privacy results from my partially educated lens that I see.
Local differential privacy algorithms are focused on enabling privacy at the point of measurement. Let's say you have an iPhone and you are doing an emoji and like how are these emojis being collected at the company level.
You could do a differential privacy. So there is a coin flip at the point of collecting the data.
There are other companies. Let's say LinkedIn could be next door, could be like Facebook that collect data.
They have terms of service that allow you to collect certain types of data from the member and they have a large database of these members.
How do you actually then build analytics products from these large kind of member protected are.
Data set, right? So there I feel like global privacy differential privacy approaches are much more relevant. So we focused a lot on global D.B. work, at least in the last 18 months or so.
We have several examples of this. The one I really enjoyed was Microsoft wanted to do skills initiative announcement with LinkedIn.
So they wanted to actually showcase this result where we wanted to take data from economic graph, which is this giant graph consisting of people, companies, schools and how these things are related and provide insights to policy think tanks, garments about which are the top jobs, which are the top industries that are hiring in different geolocations very relevant.
I think if the time of like, you know, middle of last year because the pandemic had affected the economy and people were interested in, you know, how the labor market is responding where the openings are so on support.
So we wanted to enable this view of economic graph. And we wanted it to be visible, but can we do it while actually making sure that the member data is privatized or actually we can provide guarantees on privacy.
So the technology that we developed on the global D.P. was extremely helpful in this context and it's continued to grow.
One thing that I really enjoy about LinkedIn is that it's such a complicated large ecosystem that when we think that we got it right in one context, something comes up and we realize, oh, we didn't solve for this again.
For example, we thought, oh, we understand how to build labor market insights type products very well.
Then another product came that actually had the constraint that the data was getting updated pretty rapidly.
I mean, think the comparison between sensors, which also is using D.P. maybe gets collected every 10 years to data in the LinkedIn feed that probably updates like every second.
People are doing something, liking something, clicking on something, making new connections.
So if you're building analytics products for the latter, it's almost like streaming data and thinking about how to do differential privacy for streaming data, we realize it was not solved.
Similar to how to think, think about differential privacy for top K before we realize it wasn't solved. So we came up with algorithms.
And not solved from an algorithmic and research perspective or from an engineering perspective or both.
And how does your, where does your team fit on that spectrum?
So in the top K and the streaming difference privacy side, both.
So the strength of the team still is very much in the applied research side. So most of our people, we have very strong engineers, but the core of the team is no strong set of applied researchers.
So when we identify that, hey, certain algorithmic methodological opening exists, we spend some time try to actually see if we can come up with algorithms to solve this problem.
You know that we have that right answer. Typically, we actually put the papers out. If you look at archive, we have a steady stream of papers coming out from our group.
Then we start talking to our partners in the engineering and try to protect us this into a system.
We have really, really good partnerships with engineering teams. And this has been an anchor in thinking really well about, okay, I have this cool algorithm. Can I actually make it work?
What are the constraints that I need to think about? For differential privacy, the partnership with this team called Pino, which is actually an open source all up.
Has been really useful because there are engineers that understand the constraints of high throughput analytics really well.
And they can look at ideas that we come up with and say, okay, it will work well, but maybe you need to think about it differently and so on.
So to come back to your original question, we invest still in the two lens context of like cross business utility or strategy. And for strategy, the fairness and privacy were obvious places to go, at least when I looked at the thesis in the middle of 2019 and it still continues to be pretty relevant.
What other things would become the new players, I don't know, and I'm very open. So this is a conversation we have every six months and we say, hey, what is it that we don't know and should we think about it?
I was actually going to ask about that. We talked about this why commonator analogy from the perspective of an individual project and the stages that it goes through, but there's also the invest don't invest decision and how do you pick the winners identify the bets to make?
Does that analogy apply in this context as well, or are there aspects of, you know, thinking about it as a why commonator type of a process that informs the way you manage the portfolio in that way?
I think it does, but let me just be open and say it's I'm learning to, I don't claim to say that I have this down.
These are judgment calls. What I have really benefited from is by having a point of view and then talking to my boss and talking to their boss and essentially just making sure that the executive see the same thing that I do and this alignment that we have in Lincoln has been a really strong factor.
Because then you're seeing this from multiple points of view and some people see it farther because they have a perspective. Let's say you're talking to a super engineering or CEO, they can see things that we don't.
And just formulating this point of view and saying, hey, this is what we think and like floating it out and having this alignment has been really useful.
Maybe this is similar to going talking to an investor and telling them telling you, I don't believe you kind of an idea.
And I mean, so in that sense, I think it's a sort of a strategic A B test and it has been useful.
Sometimes there are also situations where they may be wrong and you may have to push for it, but it does give you a way to evaluate, are you along the right path or not?
Are there transferable insights that might be useful to someone who's listening, you know, you see everybody doing this wrong and picking projects or you've learned that you need to look for these three things like anything like that that you could share.
Okay, really good question. I would lean on something that, you know, one of my previous bosses that I really love told me success in the industry, he used to say is down to three things and it applies in this context.
So let me repeat at least what he used to say is ability desire and opportunity three things and if they meet, you know magic happens.
Ability and desire is down to finding the right people.
Somebody that's very competent, but does not really care about this particular problem, isn't going to very go very far.
Somebody that's very passionate, but doesn't have the technical ability to carry it out, isn't going to go too far either.
And the opportunity is something where if you solve this problem, does it actually help the company and help our members?
I find personally that grounding what we do in the context of what is the vision and mission of the company extremely useful because it's a way to say, no, we don't really care about investing in certain area because I don't see how it maps.
Or at least it forces a conversation where somebody has a cool idea.
For example, there was a threat about, hey, we should why aren't we thinking about quantum computing? I'm like, okay, how does it help members get better jobs?
If you can tell me the answer, or at least help me think through this, I'm willing to talk.
So it gives a very nice grounding so that we know where we are.
But what is transferable is great technology is made by great people.
If you are building something that can be done by one person and you have the ability and you are passionate about it, then you can identify right opportunity.
Maybe you can do it all.
A lot of the great technology actually requires teams.
So what you should look for is complementarity in finding these three elements, right?
Somebody may be really good at identifying opportunities, somebody might be really good at skills and technical ability and somebody might be extremely passionate.
And if you put people like this together, amazing things happen. Greg Ed is such an example.
Nice.
Well, Parvaz, thanks so much for taking the time to chat with us and share a bit about what you're working on and how you're organizing your work and teams at LinkedIn.
Thank you. I really appreciate the time and it's nice to be here again.
Same here. Thank you.
Thank you.
