Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Before we get going, I'd like to send a huge thanks to our friends at HPE for sponsoring
this week's series of shows from the O'Reilly AI Conference in New York City.
At the conference, HPE presented on InfoSight, which is the company's cloud-based AI ops
solution for helping IT organizations better manage and ensure the health of their IT infrastructure
using AI.
I've previously written about AI ops and it's definitely an interesting use case for machine
learning.
To check out what HPE InfoSight is up to in this space, visit Twimbleai.com slash HPE.
Alright everyone, I am here in New York City for the O'Reilly AI Conference and I'm with
Joanna Bryson.
Joanna is a reader at the University of Bath.
Apparently, a reader is something like a super associate professor.
And Joanna's here to speak on maintaining human control of artificial intelligence.
We've been interacting on Twitter for quite some time and it is a pleasure to finally meet
you in person.
I am so bad with these things.
I totally forgot you were that guy on Twitter too.
That is awesome.
Yeah, I'm super happy to be here.
We've said super twice in the same 10 minutes.
I hope that's okay.
It is not a problem at all.
It is not a problem at all, not a problem at all.
Before we got started, we shared quite a bit about Chicago.
We both spent about 10 years or so on Chicago.
Yeah.
And we went to the competing universities there, right?
North Western for me.
And University of Chicago for me as an undergraduate.
Uh-huh.
Uh-huh.
So starting from your undergrad in Chicago, how did you make your way to working in AI?
Oh, yeah.
Well, that is absolutely critical to understand who I am that I did do that undergraduate
degree, the liberal arts degree at Chicago.
So my first degree was basically behavioral sciences, which at that time meant nonclinical
psychology.
They've annoyingly rebranded.
So now it means a business degree, I did not do a business degree.
Anyway, so I actually am interested in the fundamentally what makes, you know, what
is intelligence?
How does it work?
Why does some species use it more than others?
Why do some people use it more than others?
Why do individuals use it more sometimes than other times?
I mean, I am really interested in the blue sky, basic nuts and bolts of intelligence.
And I got my start at Chicago with the liberal arts degree.
And so then when I worked for five years in industry and I went into the master's degree
at Edinburgh, which was at that time, one of the very few places that you could do just
a degree in artificial intelligence.
In fact, their AI department was older than their computer science department there.
So it was no more computer science than it was linguistics or philosophy or music.
They actually had a big music component.
And of course, psychology and neuroscience.
So that was a really great opportunity, too, to really get at what intelligence was.
And I think Europe is still more inclined to be focused on that.
Although there are some great programs in America like Indiana and Colorado, where AI sort
of falls under psychology and not under computer science.
Anyway, I already knew that I kind of wanted to go to MIT because that was the best thing.
I thought.
So I managed to get the letters of reference.
I needed more than the grades and got into MIT for my PhD, which was super interesting
and opened a lot of doors, but was also some kind of a nightmare in many ways.
And especially for someone coming in with a psych degree into the computer science department.
But I did learn some really, again, some really important things about computer science.
Like for example, that there's something that is computational tractability.
There are limits to what we can know.
And there are physical limits.
They take time, space, and energy to do computation.
So then also while I was at MIT, I noticed the people projected.
They over-identified with AI, and they had really weird beliefs about it.
But only in some context, only if it was like, for example, I was working on a robot shaped
like a human.
Right?
There were other robots everywhere that were actually better.
This robot, it turned out, wasn't grounded properly.
So it's like you're going to have to figure out that it wasn't grounded.
We couldn't figure out why the different process, it was supposed to be like a parallel brain.
We couldn't figure out why the different processors didn't talk to each other.
So anyway, the person that figured that out was the one that they hired when they fired
me.
At some point, they cut all the sort of liberal arts people off the project because they weren't
getting the funding they were hoping for.
And so anyway, while I was on that, as I said, I noticed people, well, I couldn't
help but notice they would interrupt me.
I was sitting there and they'd be saying, hey, it'd be unethical to unplug that robot.
And I'd be like, well, it's not plugged in.
So that was how I got into AI ethics.
So I wrote my first paper with a philosopher friend I had named Phil time.
And so that was the way I've got my first AI ethics paper.
And then I was already a mature student because as we also discussed that, that worked in
Chicago in the financial industry for a few years and paid off my undergraduate debts.
So I didn't get many calls when I was looking to work as a professor in America.
So I applied to like two places in the UK and I got offered five interviews.
So I was like, oh, okay, I guess these guys are still into this stuff.
So I went back there and again, the British, like it's not the tenure system.
So I could do whatever I wanted.
They have a really high, or they did at the time have a very high toleration of eccentricity.
And so I was able to pursue the blue sky stuff I was interested in as long as I was really
good at teaching programming and I got the occasionally AI paper out.
So then in things kind of checked the line and then in 2007, the last time we had a president
we were terrified of, George Bush was talking, his administration gave like tens of millions
to Ron Arkin to make ethical AI robots warriors.
Now I thought, I didn't know that Ron Arkin had been trying to get that money for ages.
I thought that this was Bush trying to do like, I would grade with robots because as I
said, people over identified with them.
So I started cranking up the publishing in the AI ethics area.
And so then I started getting like called to the table by governments.
The EU is really good at this stuff.
The British are very good at it.
And they would be asking questions that like nobody knew the answers to like, how is
AI changing society?
But because of the other work I've been doing, all that stuff about like, what is intelligence
for and how does it change the species and what do you use it for?
I was able to more answer those questions than a lot of other people.
So I've found up doing a huge amount of sort of consultations into really interesting
obviously companies, but also more NGOs like OECD and the Red Cross, Chatham House, you
know, people that I was, I didn't think I would have anything to offer, but what's cool
is they bring in like these round tables and then I would be one of the two or three
AI experts and then I'd learn about the other stuff.
So right now what I think is actually the most important stuff I'm working on in terms
of research is that I started working on political economy trying to understand things
like why polarization is correlated with wealth, wealth inequality, not wealth, I mean,
wealth is fine, right?
But when society gets to, when the amount of income is too different and we don't think
it's probably the income that really is doing it, we think it's like how comfortable
you feel, then that tends, although that always, to lead to a high political polarization.
So that again goes back to the original stuff I was interested in about how do we cooperate,
why do we have these conversations, why am I here, you're not paying me, right?
But I think it's a worthwhile thing to do is if somebody wants my opinion, I think it's
the obligation of academics, I mean, that's basically what we are, we're getting paid
by taxpayer money to be receptacles of information and also to do some research, which people
tend to focus on more, but really we were the ones who were good at answering questions.
So then when people ask questions, we should answer, that was a long answer, I'm sorry,
I like that.
Do you want, is that okay?
That is totally fine, that's totally fine.
So one of the things that you touched on and it's a big focus of your, your research is
understanding natural intelligence.
And do you think of that as a prerequisite to understanding artificial intelligence or
talking about artificial intelligence?
No, I mean, obviously not because there's a lot of people who are leaders in AI that
know nothing about natural intelligence and government, unfortunately.
So I mean, one of the other things about Chicago is that politics is like a sport there.
It's like, I watched the Cubs and I read about politics and it was, a lot of people just
show a striking amount of ignorance about that, but there are great, great people in AI,
they're making real contributions to society.
But we need now at this point to realize that once you get to the point where you're really
altering, even if you aren't making that much money like Twitter.
But certainly if you're making a lot of money and also you're altering that landscape
of what it is to be human, you've got to talk with people, you've got to go in and participate
with government.
I mean, really participate, not just try to block them from doing things to you.
I mean, actually think about the problems you're causing in society, you know, the great
power equals great responsibility thing, you know, that we need to get those guys talking.
So no, no, unfortunately, it's not at all true that you have to understand this stuff.
How can we best use our understanding of natural intelligence or even taking a step back?
What do we understand about natural intelligence and how can we apply that to AI?
Is that even a narrow enough question to start to answer?
I mean, I think what academics are is we look at things in nature that are the world
or anything that we can't understand and then we try to say, especially if we feel like
we ought to be able to understand that, I almost understand it by don't quite.
So one of the big questions that motivated me a lot as a PhD student was why are the different
regions of the brain?
Why do they have different architectures, right?
Because why wouldn't nature just find one best architecture, right?
Because it's had billions of years to do that, right?
Why wouldn't it do that?
And again, this was like, I wasn't taking to heart that part about computation I told
you about before.
Now I understand and that was something and it was interesting because I mean, I literally
like really smart people like, you know, Sandy Pentland, some of these people when I was
finishing my PhD, they were the ones who helped me see what I had found, which was that
this explained why the modular approaches to AI were working well, right?
It's that basically you have a lot of learning problems to solve and learning again, it's
about tractability and it's about making it likely that you'll learn it in time, right?
So you can prove sometimes, for example, we don't need in some deeply theoretical sense,
we don't need deep learning, we don't need multiple layers to neurons in theory, like,
you know, a single hidden layer is sufficient to solve any problem.
But in practice, it's incredibly unlikely that you're going to get there with that single
middle layer.
And when you add lots of other layers, you're adding information in that can accelerate
the learning.
Well, of course, if you're accelerating some kinds of learning, you're decelerating other
kinds.
And that's the whole point is that you want to make something that's likely to learn
the problem you're setting it.
And so basically the different parts of the brain are sitting there solving different
kinds of problems, like the problems that I have and the problems that ears have and
the problems of planning and of choosing of all the options before you and of meeting
the goals you have.
I mean, these are just different kinds of problems.
And so they have different architectures that are best able to facilitate you learning
how and also, of course, controlling doing that.
So yeah, I mean, I remember I was just laughing when, you know, the deep mind got sold
for 400 million pounds.
And then they're going, oh, we're providing artificial general intelligence.
It's like, there's never going to be an algorithm that gives you a missing, you know,
there's never going to be a single solution.
And you guys, it's like 14 of them.
And the reason they were getting 400 million pounds was they were really good at tweaking
the parameters and nobody else was as good as they were at it.
That was like, if there was a GI, they would have been worth anything, right?
They were worth a lot of money because they had specialist capacity, some of which were
intuitive, but some of which were obviously that they're smart people and they ran to good
strategies.
Well, maybe let's transition a little bit to the talk that you'll be giving tomorrow
on the topic of maintaining human control of artificial intelligence.
I think, you know, I think of that as a headline or a topic.
In a lot of ways, it's obvious that we need to maintain control of artificial intelligence.
But when you think about that, like, what does that mean to you and why is that important?
Well, it's really interesting.
There's two, there's two big differences.
One is that some people really, really, really want to replace themselves.
Like, it's part of their own drive for immortality is to have these AI offspring.
Yeah.
So there's a surprising number of people are still talking about.
They think like AI is these aliens we've discovered and we need to make friends with
them, you know?
And I wish we would make friends with other people, you know, like the amount of devotion
to that idea.
I mean, strong, emotional, and also some, I mean, some of them are very well informed
philosophers and things like that, you know, this is immoral necessity.
And just like, no, it's like, so I tried for a long time to communicate that, look,
look, we designed something.
So if we build something that required our obligation, we would be being mean to it.
But I think I, in a way, gave the whole idea of more credit than it deserved.
And I should have focused more on the fact that you, I was just doing this logical thing
and saying, look, you know, we shouldn't do that for logical reasons.
But actually pragmatically, we're never going to build something that experiences the world
as much like us as a fruit fly does.
And people kill fruit flights left right in the center, you know, that just the, the,
the phenomenological experience is dependent on all these sorts of, the sensors we have,
the subset of all the computation we can do that we've evolved into, the space that we're
in.
So, but that's not the most important part.
And I think I've just spent too much time on it probably again.
But even that makes me, makes me wonder, granted, we can't create something that has the
same sensory experience as we do because it won't have the same sensors.
But does that also imply that you don't think that we can create something that has a degree
of self-awareness?
Oh.
Yeah.
See, that's again, there's a lot of points.
Am I falling into some trap that everybody falls into?
No, exactly.
Everybody, like, so there's so many terms that, that we, we use to mean human, right?
Actually, the best one, I think the one you're really interested in is moral agent and
moral patient.
That's two terms.
moral agent is something that if it does something that's responsible, we are society
considers it responsible.
And that varies by society.
No, there isn't, there isn't universal agreement about how old you have to be before you're
an adult, right?
I mean, some cultures, you're an adult when your dad dies, and obviously you're only an
adult if you're male.
Right.
So, so this is, on the other hand, the moral patient is the thing you have to take care of.
That's something that, that the society realizes it has, it has obligations towards.
And so that can include things like the environment, you know, things that don't, or babies, right?
Things that are not moral agents themselves.
But, but that we, we think we realize we need to take care of.
Although a lot of philosophers think the reason we have to take care of those things is
because it's in our own interests.
So that basically only moral agents have the, the, the, the foundation of moral patient
agency, but that as we realize the, the sort of interconnected world we live in, then we
extend our needs through these other organisms that, that we have identity with.
And incidentally, that's the best explanation for why we would ever take care of AI is that
if we don't take care of things that we feel empathy for, then we might, then we, we
harden our hearts and we, and we learn to treat things we do.
We should empathize with like other people badly.
So that's like what the British have come up with, with response to that is say, okay,
so don't make AI the reminds you of people, you know, there's two sides of that, right?
So yes, of course we have, I would say we, you know, if, if you think that consciousness
is self, self-awareness, then AI and robots have more of that than we do, right?
They have RAM.
I mean, any computer science has a RAM.
It can know exactly what all the parameters are, right?
Part of what it is to be human is not to have access to all that stuff, actually, because
a lot of it will slow you down and clutter your way forward.
So we, learning for us is about consolidating all that knowledge into the stuff that's
actually going to be most likely to be useful in the future.
So that's not, it's not quite the same thing.
But anyway, let me skip on to the thing that's actually really important, which is I tend
to talk about AI as necessarily extending from humans, like the A is artifact.
So it's something you've made for some purpose.
And so the other side of maintaining human control is actually basically not making something
that's going to be a big dumpster fire, right?
And I was really expecting a technical term there.
So it's like, it almost got, it almost went south from that, but I was good at the last
minute.
But anyway, so the, I am particularly concerned about organizations that deliberately try
to evade responsibility for what they do.
So like when a government comes in and decides that they want to cut taxes for some of their
friends and that the way they'll do that is by stopping services for people that didn't
vote for them.
And then they hide that within like some complicated software that they sort of outsource
and try to obfuscate, right?
So there's been cases of this.
And if you don't have proper rules about accountability, well, if you have proper accountability,
and now just to cut to the boring chase of this, all you need is decent DevOps, right?
You just need to be able to go in and see, you know, what was the rationale to writing
the software, right?
And so you can see what was intended if people documented and logged that properly, right?
Or you can even guess that I mean, they may not say, you know, we want to, you know, do
bad things to the people on the wrong side of the town, but you can still see like, you
know, save money in this neighborhood or something, you know?
So the, the, but for quite a lot of what's happened is that people aren't doing the kind
of standard standard stuff you do in software engineering in AI, again, because it's over
identification, maybe because too many, you know, psychologists or whatever are dropping
in and haven't had their computer science courses.
But maybe, maybe because people just think, oh, you don't have to do that with AI, it's
going to teach itself.
Like it's the, the machine is supposed to responsible for itself.
And we can't do that.
We cannot hold machines responsible.
There's no way, you know, the penalties of law that we've invented dissuade humans.
In fact, they would dissuade sheep if the sheep could understand, right?
So a lot of it is based on, you don't want to lose social status.
You don't want to lose, you know, freedom, you don't want to lose time, right?
Yeah, you're not going to build AI that is sincerely going to care about those kinds
of things in the, in the pervasive way that we do, you know, it's just systemic for us.
When you isolate someone, it is a form of torture, I mean, for any length of time, right?
So the, and yeah, you're not going to make AI that, I mean, if you could, it would be unethical,
but you're not going to make AI that, that, that, that not AI that you can maintain safely,
right?
Again, this is me being, um, probably overly generous.
I don't think there's any way you can do a whole brain uploading, but if you could,
then that would be more like a clone and then that would, and then all the stuff I'm
saying wouldn't apply to that, but you wouldn't be able to maintain an extended, like,
you can suffer.
So it's, it wouldn't be a very good product, right?
It would be, I mean, obviously some people would buy it because as I said, they want
to have perpetual, I mean, so stupid.
Why, why do people think that like AI, their AI child is going to live longer than they
are, right?
Because like, what is it?
Like the meantime to failure of a software system, like five years and humans last like
80 years, right?
You know, that, that's just like, it reminds me of everyone being so excited about an Apple
car.
It's like, have you used a computer, an Apple phone, do you really want an Apple car?
Well, yeah, I'm kind of excited about that, but yeah, but no, I assume that they would
get together with some people who knew how to handle those things, you know, but they,
like, yeah, no, that if you want to do something that's going to last longer than you're going
to live, then you should get, you know, like carve some stuff in stone or something.
It's not about building an eye, you know, or, you know, build a big cathedral thing.
People seem to care about those.
Right.
I cared.
I was really upset.
I lost two hours.
Really, really about that, Twitter.
One of the things that struck me in your right up for your talk and just now, I don't
know that I've ever heard an academic say the word DevOps.
Well, that's, you know, okay, well, there's two parts of that.
First of all, I did spend that five years doing software engineering.
Uh-huh.
Secondly, you know, part of the reasons I mentioned before that was when I was at MIT,
I was a psychologist and I was having a little bit of a struggle.
Actually, the very first course I took every other week was like doing a proof, every
other week was doing like pseudo code.
And I was getting like seas on the proofs and aids.
I was like one of the leading in the class, my first, my, you know, like the, MIT doesn't
program, they build computers from scratch or like out of free or transforms or something.
Right.
So, anyway, but, you know, so, but there were certain struggles I was having.
But I just realized that the whole thing about systems engineering of AI was wide open.
No one was paying attention to it.
So I used to say I did, I did systems AI and, and unfortunately, I guess I wasn't influential
enough then.
Maybe I should try again now to make that a thing.
Not good to talk.
Yeah.
Well, seriously.
But anyway, it's the systems engineering of AI and I kind of want people to be able
to Google that and find out the systems engineering predates even computers.
You know, because there was like these people claiming there's only four people working
on AI safety.
It's such rubbish.
I mean, everybody, there's AI is pervasive in software now.
Everybody's using AI.
Everybody's using machine learning, which is, you know, it's a huge change.
But that's why there's all these guys from like, you know, Accenture and whatever here.
Right.
You know, everybody is using this stuff now.
And everyone who's been developing a system and trying to make sure it stands up is obviously
doing all kinds of AI safety, they're doing all kinds of security.
And they're trying to figure this stuff out.
And so there's thousands of people working in this area and there's probably hundreds
who've published on it.
But they just didn't go out and say, oh, yes, I'm saving the world, you know, I, I, there
are some real problems because there are certain billionaires that only fund existential
threats.
And then there's certain academics that want to bring in millions of dollars or pounds
or something.
And, and so they say, oh, yeah, AI is an existential threat.
It's like, you know, that's not helpful.
But what is an existential life, it's an existential threat, but something that really
screws up a lot of people's lives and can cause war and things like that is when we lose
control of our governments or our corporations, you know, we, we need to figure out how to
plug things together when we change the landscape so much.
There are, you know, serious, serious problems, but it's not the AI itself.
It's, it's, I actually think that the changes are more about the changes that are making
it harder that we need to reinvent governance and that we need to really think about how
to plug the software industry into regulation, have more to do with the digital, you know,
the fact that we can move perfect replicas of things across the planet in an instant.
That's what's really changing things.
And it doesn't matter whether the intelligence is something that's coming from a computer
or the intelligence is coming from eight billion connected humans, right?
Either way, you've just got a totally new situation.
And I think that's where a lot of the regulatory challenges are coming from.
So then when you have a talk called maintaining human control of artificial intelligence,
is it fair to say it's not really about human control of artificial intelligence at all,
but rather about human control of human institutions in a world of artificial intelligence?
I'm going to finish with that stuff, but I will focus on the DevOps.
And what I'm going to, what I do when I'm talking to people that are mostly industry is I try
to reach out with this idea that look, we can do this responsibly.
We know how and we need to help people.
I mean, the main thing is that we want to help the government enforce so that when we do
good practice, other people doing bad practice don't suddenly get lots of money in all the
VC or whatever, you know?
So for the vast majority of actors, it's in all of our interests to do things that make
the software stable, right?
So yeah, so I've learned DevOps just by going to these kinds of meetings, right?
That's not what we call it in the East, but that's okay, you know?
But the focus on the problem and the solution, it's just unbelievable that people are sitting
here.
I mean, I've talked to the head of, I'm trying to say this, I will go ahead and say this.
There's this thing that European Union, I have unbelievable respect for them.
They are leading an AI in regulation, I'm about to say something bad, you can see it's
because I'm going to say out the caveats first, they're about great, they are, right?
So you know, it's 550 million people, I don't want to like run it down.
I think it's like one of the, they're leading the free world right now and they're certainly
leading an AI ethics and regulation, but they put together this high level experts group
and there's like 52 people in it, which it was supposed to be small, but somehow, you
know, things happened.
Those are super smart people.
There's some of my friends and colleagues that I work with, I have lots of respect for,
you know?
But I wound up at this European Union meeting with the guy that was the chair of that group
and he had never heard about logging, you know, about revision control.
He had no idea how easy it is to demonstrate that you follow good practice on AI and he
had never thought about AI as a product being developed so that you have the same kinds
of due diligence responsibilities as any other product.
Now this stuff is already happening in the automotive industry, for example.
That's why every time there's an accident or incident happening with a driverless car,
it's on the front page.
What went wrong?
Why it went wrong?
You know, what the car perceived?
How it was developed?
Why that thing happened?
Why?
Because that's a decently regulated space and AI doesn't change the fact that automotive
is decently regulated.
So what's not regulated because it's new and nobody's written the rules of stuff like
social media and things like that.
But there's nothing about the AI that's being used there that makes it any harder for
us to know about what's going on in those companies and whether they were paying attention
to the right things, then it does an automotive company.
It's just that there hasn't been the history and so the people coming in, you know, the
jot out of college when they were, you know, first years or whatever, just didn't realize
what they needed to do.
And the government hasn't, it's possible that governments need some kind of specialists
like they have for environmental enforcement, right, you know, or medical that they need
to bring together people that can go through and check, you know, the admin logs.
But you know, we've just gotten incredibly sloppy and you can read one of my colleagues
say to Gears has written some amazing thing about the agile turn that talking about how
everybody's throwing their software together so fast they have no idea where their software
libraries are coming from.
And of course, famously, Frank Pasquale has done the same thing about data, just showing
that data is coming from all over incredibly unethically the way it's passing between
hands.
We need to be able to demonstrate the provenance of our, the libraries we link to and the
data we train our systems from because there are bad actors out there, you know, but it's
just, it is basic DevOps.
It's really not, it's not, it's not rocket science at all.
It really is something that it's basic administration, it's basic bookkeeping and that's how we
need, you know.
And so they got super, I got to say these guys, it wasn't just, it was like several of
the people that were looking at this, that there was, again, apparently a dumpster fire
that was this, this high-level expert group.
And they're just like, nobody told us this, I'm like, I don't know what's going on with
the other 52 people, right, but, but maybe, I mean, I think this is an important thing.
We were talking about our education and our bank and everybody's a little different.
One of my colleagues at Princeton, our veterinarianin, has this wonderful paper showing, well, it's
both terrifying and wonderful, that you can uniquely identify someone if you have like
15 of their T.co links from their browser history.
So T.co means it's Twitter.
And so you can tell which tweets those links came from because they're compressed, you
know, you are else, right?
And then from seeing what links you've got it's about to click, you can, you can, for
most, like for 95% of people, you can uniquely identify them because we all follow different
people on Twitter, right?
And so you can just, and that's the mother of public, right?
So in the one hand, it's terrifying from a privacy perspective.
But what's cool about it is it just shows we're all different.
We all have different combinations and sets of knowledge.
And yeah, so I'm the one person that happens to be in AI ethics because of the paper I wrote
on as a PhD student about people over identifying with my robot and someone who knows what DevOps
is, right?
You know, like, and so, and so that's the, you know, that's something useful.
It does strike me that the DevOps and specifically the disciplines that we're talking about, understanding
logging and versioning and all of these things, they're necessary, but not sufficient for
providing the kind of control that you're talking about.
Right.
But what that stuff is for is for providing accountability.
If the companies want to defend themselves, if something goes horribly wrong, then you
have to say who should pay for it, basically, you know, the taxpayers paying for it or,
you know, are the companies paying for it?
And in particular, when you're looking at things where you're saying that, you know, did
some third country interfere with your election or whatever, you know, there, it really matters
that you get to the nuts and bolts about why was things written the way they were written?
And so if people don't follow DevOps, right?
So first of all, you're right, you need somebody who would go through and read that stuff,
right?
And secondly, of course, they're still going to be unanticipated things.
They will happen like the, you know, like flash crashes and stuff, like we just, we get
better and better at handling those things, right?
So I, you know, people freak out about flash crashes.
I think the fact that they're only flash crashes, you know, like, okay, some people lost
money.
I understand that couldn't devastate some lives, but compared to like the 1929 when people
were starving in the streets, which did not happen at least in America in 2008, incidentally,
as bad as 2008 was, it wasn't like what 1929 was, right?
So we are getting better and better at recovering from this stuff and we can dwell too much
on the negative.
But no, it is one part, but it's a core part and it's part, part of why it's important
is about the demystification.
Part of it is because then we can make sure that corporations absorb their fair share
of the, of the costs when we're cleaning up from messes, right?
And also, hopefully by making them do that, we make it less likely messes happen, right?
But part of it is just to make it clear that you can do that stuff, right?
And to help people understand, all right, oh, yeah, it's just, it's an ambient technology
now.
You know, just like we're going to look back and before there was newspapers, you know,
or whatever, we can say what changed.
You know, this, I had no idea about this, that polarization increased when people started
getting their newspapers.
Do you have an economic polarization?
Yeah, no, political polarization.
Political polarization.
That would actually help people to get more information.
But unfortunately, what it happened was they then started focusing at the national level
and using and getting these national identity things and not, and so it actually, yeah,
that was like, apparently that was part of what, getting mail delivered or something,
that they, they, they, you know, it's like, I can't even keep all this stuff on my head.
I get it from other people.
But it was amazing.
And that's only 100 years ago, the, the rate at which were changing society in terms
of information is outstanding.
And so again, projecting this onto AI and saying something's going to happen in 60 years
or 30 years is not helpful.
What we need to do is to see what we've already done and to recognize how much things are
already changing and who's explaining that, you know, although one of the scary things,
of course, is that every time you do discover something like this, you're also handing the
levers to the bad actors, you know, as we saw with the, that PNAS article that came out,
what was it?
It was like 2012 or something.
It showed that the one that you showed that you, you need like, I don't know, 80 Facebook
likes and you could predict who someone was going to vote for better than their partner
could.
Right?
You know, I remember seeing that and I thought, oh, but I actually thought a bigger deal
was a paper that came out of Microsoft and like NYU about how you could tell how people
are going to vote from their connect, their connect to sitting on top of their television
set.
And you can also tell when people are going to get divorced and you can tell, you know,
as you could just see whether they're looking at each other, whether they're looking at
the TV, when the commercials come on for the different presidential candidates, you
know, you have all this information there and like no one is talking about this, right?
And then in the end, that was, I thought I wrote a paper in 2014 that came out in 2015
that predicted the 2016 might be altered by AI, but unfortunately I said by that Microsoft
connect stuff, right?
So I was near miss, but you know, obviously if you have this information, people are going
to use it.
And so apparently this is another crazy thing, autocrats usually really do their populists,
but they usually really do make make the poor better off.
They usually wind up giving their money.
So it's possible, and again, we haven't seen this yet, but maybe the Trump is the first
one who doesn't do that.
And that might be because he has the social, the political science that says, oh, you know,
decreasing polar, decreasing inequality actually decreases polarization.
So you'd probably be less likely to get reelected if you actually helped the people that voted
you for you.
I'm wondering, do you have kind of three key takeaways for that you're planning to add
it to us?
I'm sorry.
People usually come up with cool edits that at the end, but I realize it's a lot of work.
So Joanna, you're giving this talk tomorrow, human control of artificial intelligence is
a necessity.
What are the three key takeaways that you're hoping folks will walk away with?
Well, I think, I think there's actually only two.
I mean, no, me, okay, I'll make it three.
One is fun.
That is possible.
Right.
So the most important thing is just realizing that it is an artifact.
It's not only something we can do but it's something we should do.
Secondly, that all maintaining responsibility involves is keeping track of what humans do.
We don't need to know what every weight in a deep neural network does.
We need to know who trained it when they thought they were done.
What tests did they use, what libraries did they use?
And that's about it, right?
And maybe we also want to be able to go back and do some forensics.
If it comes up with a result we don't like, and then we say, okay, let's test to make
sure that it wasn't picking up on some defended characteristics, as we say in the UK.
And try changing a few things in the resume and see if we can get the result we expected.
Right.
But those are things that we can do, even if we have a completely black box around the
AI from the outside, just as long as we have government inspectors can go through and check
the logs and make sure people file a good practice, right?
So that's the main, the key thing.
And then the third thing is that, or the original second thing, the first one, the third thing
is that it isn't that AI is threatening us.
That we are finding new ways to express power over each other.
And it's really in the interest of everybody, all the big companies, all the little companies,
all the medium sized companies, the ordinary people to get involved and the users, I fight
for the users.
I'm sorry, I digress.
Right.
It's in everybody's interest to live in a society where there is a stable and foreseeable
economy and politics, so it makes sense to pay our taxes and to participate in governments.
Awesome.
Well, Joanna, thanks so much for taking the time.
Again, I made something hard to add to that.
Sorry.
No, well, yeah, thanks for doing this.
Absolutely.
Absolutely.
It's kind of fun.
Thank you.
And nice to meet you too.
All right, everyone.
That's our show for today.
If you like what you've heard here, please do us a huge favor and tell your friends about
the show.
And if you haven't already hit that subscribe button yourself, make sure you do so you
don't miss any of the great episodes we've gotten in store for you.
For more information on any of the shows in our AI conference series, visit twimolei.com
slash AINY19.
Thanks again to HPE for sponsoring the series.
Make sure to check them out at twimolei.com slash HPE.
As always, thanks so much for listening and catch you next time.
