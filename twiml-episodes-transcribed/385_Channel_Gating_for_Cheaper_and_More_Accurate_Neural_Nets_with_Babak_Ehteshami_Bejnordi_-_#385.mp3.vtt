WEBVTT

00:00.000 --> 00:13.120
Welcome to the Tumel AI Podcast.

00:13.120 --> 00:16.240
I'm your host Sam Charrington.

00:16.240 --> 00:24.840
Hey, what's up everyone?

00:24.840 --> 00:29.180
Over the next few weeks, we'll be exploring just a bit of the great research that was

00:29.180 --> 00:33.700
showcased during last week's CVPR conference.

00:33.700 --> 00:38.180
Before we get to today's episode, though, I'd like to send a huge thank you to our friends

00:38.180 --> 00:44.540
at Qualcomm for their support of the podcast and their sponsorship of this series.

00:44.540 --> 00:50.940
Qualcomm AI research is dedicated to advancing AI to make its core capabilities, perception,

00:50.940 --> 00:55.700
reasoning, and action ubiquitous across devices.

00:55.700 --> 01:00.340
The work makes it possible for billions of users around the world to have AI enhanced

01:00.340 --> 01:04.980
experiences on Qualcomm technology's powered devices.

01:04.980 --> 01:11.620
To learn more about what Qualcomm is up to on the research front, visit twimmelai.com-qualcomm

01:11.620 --> 01:15.460
QAL-COM.

01:15.460 --> 01:18.100
Next up, a quick community update.

01:18.100 --> 01:22.300
If you're interested in the topic of causal modeling and machine learning, but missed

01:22.300 --> 01:27.980
out on the initial cohort of the course we hosted with Robert Osa-Zooness, I'm happy

01:27.980 --> 01:33.460
to announce the second cohort of this course and study group will be starting soon.

01:33.460 --> 01:40.140
For more information, join us this Thursday, June 25th for a live webinar that Robert

01:40.140 --> 01:45.780
and I will be hosting to introduce causality and review all of the details of the course,

01:45.780 --> 01:48.980
including the many enhancements he's made this time around.

01:48.980 --> 01:58.900
For more information, visit twimmelai.com-causal, and now on to the show.

01:58.900 --> 02:03.060
Alright everyone, I am on the line with Bobak at Tashami Bishnordi.

02:03.060 --> 02:07.540
Bobak is a research scientist at Qualcomm AI Research.

02:07.540 --> 02:10.060
Bobak, welcome to the Twimmelai.com podcast.

02:10.060 --> 02:12.140
And thank you for having me.

02:12.140 --> 02:15.500
It's great to have an opportunity to chat with you and I'm looking forward to learning

02:15.500 --> 02:20.260
a bit more about your research and what you're up to there at Qualcomm.

02:20.260 --> 02:23.860
But to get us started, why don't you share a little bit about your background and how

02:23.860 --> 02:25.900
you came to work in AI?

02:25.900 --> 02:26.900
Sure.

02:26.900 --> 02:32.100
So I did my master's in electrical engineering at Chonmer's University of Technology in

02:32.100 --> 02:33.340
Sweden.

02:33.340 --> 02:38.140
And in that program we had a bunch of courses like machine learning and patent recognition

02:38.140 --> 02:43.300
as well as some image analysis courses, which kind of drew me to this field and I decided

02:43.300 --> 02:48.820
to do my master thesis on cervical cancer diagnosis using ML.

02:48.820 --> 02:53.260
So kind of in the medical imaging domain and I absolutely enjoyed it and decided to do

02:53.260 --> 02:56.780
my PhD in the similar field.

02:56.780 --> 03:04.220
So I came to the Netherlands in Rodwald University and for my PhD I was developing machine learning

03:04.220 --> 03:08.940
models for breast cancer diagnosis in histopathological images.

03:08.940 --> 03:14.380
histopathological images are microscopic images of tissue.

03:14.380 --> 03:20.700
And kind of in the early mid part of my PhD this deep learning revolution happened.

03:20.700 --> 03:26.620
And me and a lot of my colleagues quickly switched to use deep learnings and get familiar

03:26.620 --> 03:34.420
with retraining it ourselves and kind of moved to that to using that in my entire project.

03:34.420 --> 03:38.340
And during my PhD I also organized the chameleon challenge.

03:38.340 --> 03:46.180
It was a challenge on finding cancer metastases on breast to more patients.

03:46.180 --> 03:49.140
And it turned out to be a very successful challenge.

03:49.140 --> 03:56.420
And was one of the first examples in which using AI the top leading algorithms were actually

03:56.420 --> 03:59.420
outperforming human experts.

03:59.420 --> 04:06.460
We compared the top two algorithms in the challenge with a panel of 11 pathologies and all they

04:06.460 --> 04:10.540
were actually beating all the 11 pathologies without exception.

04:10.540 --> 04:17.700
I also did a visiting research at Harvard at bedclap and also towards the end of my PhD

04:17.700 --> 04:23.700
I decided to join Qualcomm where I'm here for a bit more than two years now and I'm mainly

04:23.700 --> 04:25.940
working on conditional computation.

04:25.940 --> 04:29.700
What is conditional computation tell us a little bit more about that?

04:29.700 --> 04:35.220
So conditional computation in the context of neural networks refers to a class of algorithms

04:35.220 --> 04:42.380
that can selectively activate its units conditioned on the input it receives.

04:42.380 --> 04:45.340
Such that on average we have lower computation costs.

04:45.340 --> 04:51.260
And when I'm speaking of units it could be layers or individual filters for example.

04:51.260 --> 04:58.300
And the thing is in our feed forward neural networks we usually have this prior that no

04:58.300 --> 05:03.340
matter what input is is receiving we always run all the layers, all the filters no matter

05:03.340 --> 05:04.340
what.

05:04.340 --> 05:09.060
And reality some examples might be simple and some harder and maybe for simple examples

05:09.060 --> 05:14.020
we could exit earlier from that from the network and finish the classification or sometimes

05:14.020 --> 05:20.420
we were for a classification task classifying the image of a cat and sometimes classifying

05:20.420 --> 05:22.540
the image of a vehicle for example.

05:22.540 --> 05:26.340
But now even in the middle of a network that we are kind of certain that we are dealing

05:26.340 --> 05:32.500
with a picture of a cat we are still applying all those vehicle detection features or filters

05:32.500 --> 05:38.420
in our to our feature map which is superfluous and also from a generalization perspective

05:38.420 --> 05:40.580
that is bad for our network.

05:40.580 --> 05:45.660
So conditional computation aims to selectively activate parts of a network.

05:45.660 --> 05:50.260
It could be different layers, it could be channels, it could be actually a whole subnetwork

05:50.260 --> 05:54.220
in your main network which you could completely deactivate.

05:54.220 --> 06:00.580
In fact if we want to look back the first examples were maybe by Hinton, Jaffa Hinton

06:00.580 --> 06:06.740
and Robert Jacobs from 1991 they had a paper adaptive mixture of local experts and the

06:06.740 --> 06:14.740
idea there was they trained a giant network with many kind of small subnetworks which were

06:14.740 --> 06:20.180
experts for a specific subset of a data and then there was a gating network which would

06:20.180 --> 06:24.900
get the input as well and based on the input it decides which of these subnetworks should

06:24.900 --> 06:26.300
be selected.

06:26.300 --> 06:31.700
And in that way they were kind of encouraging the network to be expert, the subnetworks

06:31.700 --> 06:35.740
to be expert on their own specific data.

06:35.740 --> 06:40.740
And also in recent years in the era of deep learning we see more of the more approaches

06:40.740 --> 06:43.140
are using conditional computation.

06:43.140 --> 06:50.100
Maybe one of the early examples was branching it which adds auxiliary classifiers in your

06:50.100 --> 06:54.620
network, let's say some early in the network, some in the middle of your network.

06:54.620 --> 06:58.660
And the aim is that easy examples should exit early years.

06:58.660 --> 07:04.740
Another work which was an inspiring work was convolutional networks with adaptive inference

07:04.740 --> 07:13.540
graphs which basically decides to gate basically activate or deactivate individual layers

07:13.540 --> 07:16.140
in a red net block, conditional input.

07:16.140 --> 07:21.900
They hypothesize that maybe some layers are important for specific classes but not for

07:21.900 --> 07:28.060
the others and they could learn how to turn on and off individual basically layers in

07:28.060 --> 07:29.060
a network.

07:29.060 --> 07:34.100
And they were actually inspired by their own previous works which showed that if you

07:34.100 --> 07:39.700
actually delete individual layers in a resonant model at test time it's accuracy will not

07:39.700 --> 07:40.700
drop.

07:40.700 --> 07:42.020
That was very interesting.

07:42.020 --> 07:48.380
Of course on the layers which you have a stride too so a down sampling you may get

07:48.380 --> 07:52.780
a little bit of performance drop but on the rest of the layers you didn't get so they decided

07:52.780 --> 07:55.900
to learn then to drop.

07:55.900 --> 08:01.220
And this is also in short contracts to other types of complex like like VGG network you cannot

08:01.220 --> 08:06.220
actually do that because if you drop a layer that their whole sequential part will break

08:06.220 --> 08:09.100
but in resonant it was perfectly doable.

08:09.100 --> 08:17.060
And that actually constituted the basic idea for what we want to do as well which was starting

08:17.060 --> 08:20.020
to gate channels instead of layers.

08:20.020 --> 08:27.740
Before we get into the details of your specific research are the conditional computation techniques

08:27.740 --> 08:35.100
that you're applying exclusively focused on inference or conceptually it seems like

08:35.100 --> 08:37.340
you could apply it to both.

08:37.340 --> 08:38.340
All right.

08:38.340 --> 08:39.340
Yes, exactly.

08:39.340 --> 08:42.180
You could definitely apply it at training time as well.

08:42.180 --> 08:48.460
So there's like an if and if else argument inside your training for example is saying

08:48.460 --> 08:54.460
the activate you could choose not to send the gradients to that part of a network and

08:54.460 --> 09:02.260
you could basically gain speed or an inference training time and also inference time as well.

09:02.260 --> 09:08.500
And the specific techniques that you focus on are those more concerned with inference

09:08.500 --> 09:09.500
time?

09:09.500 --> 09:18.460
Yes, well, our main focus has been on inference time we try to, so it is definitely possible

09:18.460 --> 09:24.860
to do that for training time as well but we mainly focus on inference time as well.

09:24.860 --> 09:34.100
With the over-arching implication that you're trying to reduce the power and cost of doing

09:34.100 --> 09:35.780
inference on a device.

09:35.780 --> 09:36.780
Right, exactly.

09:36.780 --> 09:40.620
So maybe that is actually not the only motivation.

09:40.620 --> 09:41.620
Okay.

09:41.620 --> 09:47.180
Interestingly, if you train such networks these networks tend to perform much better than

09:47.180 --> 09:52.220
counterpart like a fixed neural network with the same computation cost.

09:52.220 --> 09:59.140
And I will describe that in a bit that the reason is that these models tend to you can

09:59.140 --> 10:05.740
start training a very giant model but then at inference time select only very limited

10:05.740 --> 10:11.340
subsets of the resources available, condition on the input, you say, okay, this network is

10:11.340 --> 10:14.700
huge but you should just pick the relevant ones.

10:14.700 --> 10:22.300
It induces the network to have expert subnetworks inside the network as well because by gating

10:22.300 --> 10:27.980
it understands, okay, these subset of filters should be activated for cat detection and the

10:27.980 --> 10:31.060
rest maybe for another for vehicle detection.

10:31.060 --> 10:36.140
So it actually interestingly improves the performance as well which is a good thing.

10:36.140 --> 10:41.540
Well, why don't I take a step back and have you kind of describe at a high level the

10:41.540 --> 10:47.180
different types of approaches you're taking in these papers and then we can dig into more

10:47.180 --> 10:48.180
specifics.

10:48.180 --> 10:49.340
Sure, sure.

10:49.340 --> 10:54.260
So in the first paper which was batch shaping for learning conditional channel gated and

10:54.260 --> 11:01.140
networks we were basically thinking instead of gating individual layers in the network,

11:01.140 --> 11:06.460
let's make it more fine-grained and gate individual filters.

11:06.460 --> 11:13.500
That would give us more, basically more flexibility and more power representation power.

11:13.500 --> 11:19.420
It would allow us actually to gain more interpretability as well to understand which filters

11:19.420 --> 11:22.060
are firing for what classes.

11:22.060 --> 11:25.780
So that was the main focus of the first paper.

11:25.780 --> 11:31.220
The second paper which was on continual learning actually before starting the second paper,

11:31.220 --> 11:37.940
we already knew that this idea of channel gating would be very useful for multitask learning

11:37.940 --> 11:43.740
and continual learning because for example in multitask learning one problem they have.

11:43.740 --> 11:49.700
So we know that if you want to learn a couple of tasks together, it generally tasks might

11:49.700 --> 11:55.660
help each other so that the performance gets a little bit improved in general because

11:55.660 --> 11:57.980
they can share the features among them.

11:57.980 --> 12:01.940
But as a number of these tasks, in principle, suddenly you see the performance actually

12:01.940 --> 12:07.980
starts dropping and that is because of feature interference.

12:07.980 --> 12:14.420
By forcing a specific task to use a feature which is not relevant for the task, it actually

12:14.420 --> 12:19.300
starts degrading performance and we thought that if we could use our gated networks we

12:19.300 --> 12:25.700
could decide when to activate or deactivate features and not allow feature sharing if

12:25.700 --> 12:28.100
a feature is not relevant.

12:28.100 --> 12:33.700
And in continual learning these gates could actually serve as memory which would allow

12:33.700 --> 12:40.140
us that if a feature is particularly important for a specific task, maybe we have to preserve

12:40.140 --> 12:44.540
it and not allow other tasks to update it too much.

12:44.540 --> 12:51.700
And in the last paper, if we mainly focused on video long range activity detection videos

12:51.700 --> 12:57.500
for classification and we thought we might get as input a video which is like 10 minutes

12:57.500 --> 13:03.140
but we know that there is huge amount of redundancy and relevant things happening which are not

13:03.140 --> 13:09.420
really important for the actual classification task and we did dynamic gating of individual

13:09.420 --> 13:16.740
snippets in the video to only focus on the important parts and actually saved a huge amount

13:16.740 --> 13:18.780
of computation based on that.

13:18.780 --> 13:19.780
Okay.

13:19.780 --> 13:24.220
Well let's jump into the first paper because I have some questions there.

13:24.220 --> 13:30.140
You mentioned that this paper is kind of shifting from thinking about gating layers to

13:30.140 --> 13:32.100
gating individual filters.

13:32.100 --> 13:35.900
What are the specific types of filters you're gating here?

13:35.900 --> 13:42.420
So the design is something like this, assume we have a resonant block, we are augmenting

13:42.420 --> 13:47.420
this while we are adding to this resonant block a gating module, this gating module gets

13:47.420 --> 13:51.060
the same representation that goes to the resonant as input.

13:51.060 --> 13:57.300
And its output is a bunch of gates and the number of these gates is equal to the number

13:57.300 --> 14:03.820
of filters let's say in a first convolutional layer of the resonant block.

14:03.820 --> 14:09.900
So one gate couple to each filter and the filter wants to activate or deactivate the use

14:09.900 --> 14:14.820
of that filter and we had a bunch of criteria for example we wanted the gating module to be

14:14.820 --> 14:21.380
very light because we didn't want to have a lot of additional overhead.

14:21.380 --> 14:26.460
We wanted the gates to be input dependent so that they don't make just arbitrary decisions

14:26.460 --> 14:31.220
but they see the input and condition and the input make a decision.

14:31.220 --> 14:36.580
And last we wanted it to make a binary decision.

14:36.580 --> 14:39.820
We didn't want it to be like an attention because attention would not keep saving you

14:39.820 --> 14:40.820
any compute.

14:40.820 --> 14:45.460
We wanted to be zero or one which means it would have been a discrete optimization problem

14:45.460 --> 14:49.900
and we used gumball softmax for training this network.

14:49.900 --> 14:55.380
But it was very interesting problem because we started training this model and from the

14:55.380 --> 14:59.580
very beginning if you have let's say it's a classification problem let's say and you

14:59.580 --> 15:01.980
have the cross entropy loss.

15:01.980 --> 15:06.940
The gate automatically want to learn the most trivial solution which is just be on all

15:06.940 --> 15:07.940
the time.

15:07.940 --> 15:12.540
And by being on all the time it satisfy the objective of the cross entropy loss and it's

15:12.540 --> 15:18.620
as if you're training a boring regular network it's just a standard regular network.

15:18.620 --> 15:23.220
While we wanted the gates to behave conditionally to be sometimes off for certain types of

15:23.220 --> 15:25.740
input and sometimes on.

15:25.740 --> 15:26.740
And for that.

15:26.740 --> 15:32.700
So training this network it kind of all at once you're not separately training the gate

15:32.700 --> 15:38.020
layer and the network itself right we're training into and all at once exactly.

15:38.020 --> 15:43.780
So then we started saying okay we want to specify these gates we don't want it to be always

15:43.780 --> 15:44.780
on.

15:44.780 --> 15:49.540
And by doing that we found out actually if you for example apply L0 loss a lot of gates

15:49.540 --> 15:55.500
turn out to be permanently off which is actually not interesting either because permanently

15:55.500 --> 16:00.140
off would be equivalent to these model compression techniques like pruning techniques.

16:00.140 --> 16:05.420
We want the gate to be at the same time sometimes on and sometimes off.

16:05.420 --> 16:11.340
So and interestingly when we decided to encourage entropy to to have a higher entropy let's

16:11.340 --> 16:16.980
say sometimes on sometimes off these gates could easily cheat they could generate a probability

16:16.980 --> 16:23.460
distribution center that 0.5 so assume at the output of the gate you pass it to a sigmoid

16:23.460 --> 16:27.340
and then take the arc max or or a threshold 0.5.

16:27.340 --> 16:33.900
It could put the distribution center 0.5 that when you add the gumball noise and and take

16:33.900 --> 16:39.900
the sample it is randomly on and off it's not really like yes just randomly so it it

16:39.900 --> 16:45.220
had learned to be a random dropout which was really not not something we wanted.

16:45.220 --> 16:50.780
But we wanted in contrast was a U shaped by model distribution which means for certain

16:50.780 --> 16:55.660
the type of data the output is 1 and for certain data it is 0 and we said okay we have

16:55.660 --> 17:02.100
this strong prior why don't we actually help the gate to have a distribution like this

17:02.100 --> 17:08.060
and that we that came the our other contribution which was the batch shaping loss which was

17:08.060 --> 17:14.740
inspired by the Kramer fanmeasus criterion what it does it basically takes our prior distribution

17:14.740 --> 17:20.300
which is a better like a U distribution we can see the F and then we also compute the

17:20.300 --> 17:27.020
empirical CDF of our sampled distribution from the output of a gate for batch of data

17:27.020 --> 17:32.460
and then we minimize the distance between these CDFs and the interestingly well CDF is

17:32.460 --> 17:39.180
perfectly differentiable the derivative of that of a CDF is PDF so we could get that

17:39.180 --> 17:44.540
freely from the forward pass and then the distributions perfectly matched each other

17:44.540 --> 17:49.780
which was very interesting to see and we think this batch shaping loss could be very interesting

17:49.780 --> 17:54.340
for a lot of other applications when you want to match to distributions before we go into

17:54.340 --> 18:00.500
the some of the applications which distributions were matching one another was a better distribution

18:00.500 --> 18:07.540
so we we forced the gate to have a better like distribution that's a 50% on 50% off

18:09.140 --> 18:16.500
and that helped a lot the the gate to behave more conditionally because he knew that it can only

18:16.500 --> 18:22.660
be 50% on so let's turn off turn it off for whatever irrelevant type of data and this helped the

18:22.660 --> 18:28.820
network to behave really conditionally and we could also visualize when it is on when it is off

18:28.820 --> 18:37.060
and it was really making sense and interestingly we saw very good behavior of this network we

18:37.060 --> 18:43.140
we saw that we could actually take a resident 18 model let's make this resident 18 model

18:43.140 --> 18:50.420
incredibly wide let's make it 20x wider but at an inference time before it to make it so much

18:50.420 --> 18:55.780
as small that this is the site of the size of a standard resident 18 with widths one so it has

18:55.780 --> 19:00.420
a lot of filters but you are only allowed to choose among them such that the size doesn't

19:00.420 --> 19:05.780
doesn't change that much but on average so for more complex examples you could choose more filters

19:05.780 --> 19:12.420
and easier less filters and this resident 18 at an inference time size could perform as with as

19:12.420 --> 19:20.260
a resident 50 for example it performs much better than it's it's kind of an equivalent size

19:20.260 --> 19:26.900
network which is with a fixed architecture and how would you compare the complexity of the

19:26.900 --> 19:34.180
very wide resident 18 with the resident 50 so the complexity we we measured that in terms of

19:34.180 --> 19:41.300
MAC operations so multiplication and accumulation operations so it's much much more cheaper than

19:41.300 --> 19:49.300
resident 50 of course and we of course forced it to have with disparity to not how to not use

19:49.300 --> 19:55.620
all the filters so that in front time it is actually still the size of a resident 18 when you were

19:55.620 --> 20:03.620
applying the the bad shaping is the idea that one of your parameters is how many of the gates are

20:03.620 --> 20:10.820
active is that how you how you were able to shrink down this wide resident 18 to a much more

20:10.820 --> 20:18.900
narrow one yeah so so we had a prior for these for this better distribution the prior was actually

20:18.900 --> 20:26.980
being 60% on and 40% off and we started with with a very strong coefficient at the start of

20:26.980 --> 20:33.380
training so all the gates are forced to take that shape but we as we go on we slowly reduced that

20:33.940 --> 20:40.020
that coefficient so that the model has a little bit more flexibility and then we see that some of

20:40.020 --> 20:45.540
the gates might actually just get rid of that conditionality and go and be completely on those are

20:45.540 --> 20:50.100
maybe the filters which are very important and it's like too fundamental they they need to be

20:50.100 --> 20:56.260
always executed but there's some gates all the majority of the gates keep that shape but we also

20:56.260 --> 21:03.700
added the L0 loss and L0 loss has the ability to actually throw away if a filter is completely

21:03.700 --> 21:10.500
useless so for example if due to poor initialization some of the filters turned out not to be that

21:10.500 --> 21:18.100
useful for the classification task it could actually push it down to completely off so we have

21:18.100 --> 21:24.260
a combination of completely off filters and completely on filters but the majority conditional

21:24.260 --> 21:30.340
filters and they take the conditional ones take different shapes some of them might be 80% off

21:30.340 --> 21:37.620
20% on and some of them 80% on 20% off and we using the coefficient of these two losses we

21:37.620 --> 21:45.300
controlled how much Mac we want to save basically now it sounds like you if I'm understanding this

21:45.300 --> 21:51.380
right you kind of set off with this idea of building these conditional channel gate and networks

21:51.940 --> 21:58.340
and when you saw kind of the gates you know all on or all all for kind of flipping randomly

21:58.340 --> 22:05.220
back and forth were these presumably these were your initial experiments and they were surprising

22:05.220 --> 22:12.660
like did you I'm curious as you were seeing these kind of results you know how did you

22:12.660 --> 22:19.700
note it to go to the to the batch shaping you know as opposed to I'm thinking about you know maybe

22:19.700 --> 22:28.260
this just doesn't want to work right yeah exactly so so we had several minutes

22:28.260 --> 22:34.660
meetings with Maxwell and we had a lot of brainstorming sessions very interesting one one day he came

22:34.660 --> 22:41.300
so I was plotting the histogram of the gate output a lot of them were centered in the middle

22:41.300 --> 22:46.180
and he was so sharp like as soon as he saw he said oh these are like random dropout you want

22:46.180 --> 22:52.500
these U shape right and I said oh I want that U shape but it said let's think about how how to

22:52.500 --> 22:58.660
make that U shape and we thought of a lot of different experiments for example pushing the values

22:58.660 --> 23:07.300
to be away from 0.5 so the values tended to be in the center we decided to make it pushing towards

23:07.300 --> 23:12.340
the two sides but that didn't work we we tried the entropy as a loss that didn't work because the

23:12.340 --> 23:19.540
gating much it could easily cheat and after everything in the middle and a lot of losses actually

23:19.540 --> 23:25.700
led to not conditionality so complete pruning which is like static compression methods which

23:25.700 --> 23:34.020
were we were not interested we wanted the conditional way of doing that and over time we thought

23:34.020 --> 23:41.060
that actually you cannot so a single gate you cannot cause a specific distribution without

23:41.060 --> 23:46.740
regarding the entire batch so that the whole point was that this distribution is only defined

23:46.740 --> 23:52.340
well when you take the entire batch into account so you know in this batch there is a collection

23:52.340 --> 23:58.580
of data points and you want to these data points to get to behave differently for some of them

23:58.580 --> 24:03.140
we wanted to be on and for some of them to be off and we didn't know which ones are going to be

24:03.140 --> 24:09.940
on off isn't it the network has to learn and we actually didn't even know of the so at least

24:09.940 --> 24:16.260
I didn't know of the Kramer-Famizus criterion we came up with this loss and then later stage

24:16.260 --> 24:23.540
we found out oh did this Kramer-Famizus criteria did exist and it was like proposed like 40-50

24:23.540 --> 24:29.220
years ago but it was never used in the deep learning and we made it like differentiable and

24:30.180 --> 24:36.740
and there is one key point inside that and that when you get the output of a gate you have to sort it

24:37.620 --> 24:45.220
and and then generate CDF of the sorted sorting values and then during the backprop you have to

24:45.220 --> 24:51.460
remember to un sort the operation so that the gradient goes through the right input and the

24:51.460 --> 24:58.340
right direction so there were a bunch of tricks but then we tried to basically narrow down the

24:58.340 --> 25:04.180
problem make it as simple as possible to find out what makes it work and this turned out to work

25:04.180 --> 25:08.900
nice nice and you you mentioned that this batch shaping technique has your starting to

25:08.900 --> 25:15.380
mention that this back shaping technique has other applications sure sure I'm thinking that it could

25:15.380 --> 25:21.380
potentially be used for whatever application in which you want so you want to match the distribution

25:21.380 --> 25:27.220
of a parameterized feature in your network let's say a feature map to any distribution for example

25:27.220 --> 25:31.460
in batch norm we may want it to be normally distributed we could actually enforce that with the

25:31.460 --> 25:38.820
batch shaping loss and basically in whatever application that you want to impose a prior on the

25:38.820 --> 25:46.260
distribution of your feature maps you could do that in this with this technique. So the next paper

25:46.260 --> 25:52.260
where you're looking at task where continual learning is that building on this batch norm

25:52.260 --> 25:57.620
or sorry batch shaping technique or is it a separate research in the direction of the

25:57.620 --> 26:04.340
conditional gated networks. So it is a follow up on the channel gated networks we use the same

26:04.340 --> 26:09.140
the exact same type of network but instead of applying it to ResNet because we knew it would work

26:09.140 --> 26:14.660
for regular networks as well like VGG type we we applied it both for regular types of models

26:14.660 --> 26:22.420
and and ResNet models so maybe I could briefly tell of talk about the continual learning problem

26:22.420 --> 26:30.100
well in continual learning we we have basically this setting that you want to learn a specific task

26:30.100 --> 26:36.180
let's say task a using a neural network and you train this model using data from task a and

26:36.180 --> 26:42.260
get a very good accuracy let's say then you go to task b but you assume you should consider that

26:42.260 --> 26:48.660
you will never have access to data from task a anymore and now you want to train this previously

26:48.660 --> 26:54.900
trained model again such that it performs very good on task b but doesn't forget what it has

26:54.900 --> 27:00.340
learned on task a in reality the fundamental problem with continual learning is that we have the

27:00.340 --> 27:05.220
catastrophic forgetting problem because as soon as you update the weights they completely forget

27:05.220 --> 27:10.820
what they previously learned and that is actually a fundamental problem we have in training neural

27:10.820 --> 27:16.980
networks that's why we when training we train these models using many epochs or we have to shuffle

27:16.980 --> 27:21.940
the data all the all the time we cannot just take a batch and perfectly learn it and go to the

27:21.940 --> 27:27.060
next batch to perfectly learn it because we keep forgetting and this problem also arises in

27:27.060 --> 27:34.820
continual learning so our idea was that we could use these channel gated networks

27:36.660 --> 27:43.940
to bring several benefits but but let me let me before that tell about two very fundamental

27:43.940 --> 27:52.580
approaches which people use one of them is is basically maybe a work by deep mind on elastic

27:52.580 --> 27:58.660
weight consolidation what they do they measure at the end of training on task one they measure

27:58.660 --> 28:05.380
the importance of the individual filters using the diagonals of the Fisher matrix that if a

28:05.380 --> 28:11.220
feature if a particular weight is very important let's not update it too much let's slow down the

28:11.220 --> 28:15.700
learning on those weights and the features that were less important we could actually

28:17.300 --> 28:22.500
learn them better or make them more available for the future task well obviously the problem with

28:22.500 --> 28:29.060
this method is that as you as you go on and on and learn more tasks you tend to have this trade

28:29.060 --> 28:35.860
off between learning new things and not being able to change things previously learned and this

28:35.860 --> 28:42.820
approach works very well on simple data sets like MNES for for now for let's say two or three tasks

28:42.820 --> 28:48.260
but as soon as you increase the number of tasks it fails there is another vein of approaches

28:49.060 --> 28:55.060
which are basically progressive neural networks which they say okay let's train our model on task A

28:55.060 --> 29:02.500
and then when we go to the next task list add neurons and learn task B and we have the ability to

29:02.500 --> 29:08.740
learn previously to use previously learned features but are not able to change them the problem

29:08.740 --> 29:15.460
with this approach is that as you add neurons it will become really not very scalable because your

29:15.460 --> 29:20.740
model is going to get huge and also you're forcing to share previously learned features which

29:20.740 --> 29:28.260
might be irrelevant for the current task that is coming so so our idea was was like this so we

29:28.260 --> 29:35.140
thought we let's use our channel gated networks so you have task one you you train it using our

29:35.140 --> 29:42.340
channel gated network and each of the layers have a specific gating module which is specific for

29:42.340 --> 29:48.420
that the specific task so each task has its own gating modules these gating modules are not only

29:48.420 --> 29:58.740
task dependent but also input dependent so they basically during training we basically enforce

29:58.740 --> 30:05.620
these gates to not use all the filters because we want to make a lot of filters available for future

30:05.620 --> 30:13.060
tasks so we impose a lot of sparsity let's say a specific layer has 100 filters using channel gated

30:13.060 --> 30:21.140
methods we and sparsity we enforce that it for example uses 20 of out of the 100 filters available

30:21.140 --> 30:28.420
and because filters or these gates are input dependent as well not all not all of these 20 filters

30:28.420 --> 30:34.740
will be actually used for all the examples some may use only five filters some maybe the entire

30:34.740 --> 30:43.460
filters this makes these these these channel gated models extremely efficient and very with very

30:43.460 --> 30:50.260
low computational cost you're essentially adding the task as a another input to the channel

30:50.260 --> 30:55.380
gate that you already had before yes that would then put dependent yes but then we go to the

30:55.380 --> 31:04.020
to a new task now the new task is going to have its own gating module well these gating modules

31:04.020 --> 31:09.220
are actually very important things because they if we study them how they are firing they could

31:09.220 --> 31:14.740
easily tell us how important the feature is if a filter is if a gate is firing too often it means

31:14.740 --> 31:19.700
the filter is going to be used all the time we should never change this filter so what we do we look

31:19.700 --> 31:26.660
at the the most important features and in practice we actually even because of a heavy sparsification

31:26.660 --> 31:35.220
if a if a filter is selected even only once we chose to keep it and freeze it so all the features

31:35.220 --> 31:41.220
or the filters that were important for task one we freeze all of them and all the rest of the

31:41.220 --> 31:46.180
filters which we are not used we re-initialize them and make them available for the new task

31:46.180 --> 31:53.780
and the new task can obviously learn the new filter is based on based on basically its own objective

31:53.780 --> 32:00.900
but it also has the option using its own gating modules to to choose to use previously learned

32:00.900 --> 32:06.180
features as well it is not allowed to update them but it can choose to use or not use them

32:07.140 --> 32:12.820
which would make these networks to have positive transfer of information but not forcing it

32:12.820 --> 32:19.140
so if a feature is not irrelevant it will not be shared and you can keep growing and increasing

32:19.140 --> 32:24.740
more tasks the great thing is that these gates are always input dependent and inference time

32:24.740 --> 32:31.540
they are extremely light so if you make this network extremely wide still it would be super light

32:31.540 --> 32:36.980
in at the inference time because only a very small subset of filters are going to be selected

32:36.980 --> 32:48.580
at each layer so how exactly are you allowing the the tasks to choose the filters from the previous

32:48.580 --> 32:55.380
tasks is this based on initialization or something something else so so just imagine we have a

32:55.380 --> 33:02.260
resonant block and in and in the let's say in the first convolutional layer we have 100 filters

33:02.260 --> 33:08.500
10 of them are frozen so they are not he's the gate is not or they're not updateable

33:08.500 --> 33:14.660
and 90 of them are are learnable so the gating module gets the same representation that goes to

33:14.660 --> 33:21.220
these resonant block as input and chooses okay should I use this filter number one which is

33:21.220 --> 33:26.900
not updateable or not it makes a binary decision pretty much like before but it cannot update the

33:26.900 --> 33:33.540
filter it just has to say choose to use or not but for the rest of a 90 it can update them

33:34.660 --> 33:42.340
and so we trained this model and very interestingly we saw that there were actually a lot of

33:42.340 --> 33:49.460
SOTA and very good methods already in the literature and on four data sets we were able to

33:49.460 --> 33:56.660
to achieve as good or better performance than all competing methods but then we wanted to make

33:56.660 --> 34:02.820
this a little bit more challenging there is a specific setting in which you actually don't even

34:02.820 --> 34:10.020
know which tasks you are trained you are operating on so up to now we were assuming that you know

34:10.020 --> 34:16.740
which task you are going to work on at test time and you would only activate that that branch

34:16.740 --> 34:22.260
of the network and the gating modules of that is specific task but in some cases you might not even

34:22.260 --> 34:27.860
know which task you are working on and this is a very complicated setting and there is not much

34:27.860 --> 34:36.500
working the literature and what we did was that so we we assumed from the start of at the inference

34:36.500 --> 34:41.940
time so you could train as usual but at inference time you would when you get an input you would

34:41.940 --> 34:47.140
gate them with different hypothesis this is task one this is task two this is task three the same

34:47.140 --> 34:53.540
input but at the end of the network we we added the task classifier which looks at the gating

34:53.540 --> 34:59.540
patterns of this feature map how how this feature map has been altered based on different

34:59.540 --> 35:06.580
hypothesis and based on that makes a makes a decision that which task you are you should be classifying

35:06.580 --> 35:12.180
this network but but of course this classifier at the end of the network is going to suffer from

35:12.180 --> 35:16.980
forgetting as well because you have as you increase the number of tasks you have to retrain this

35:16.980 --> 35:26.420
classifier and we chose to use a generative model to remember the data that was used for previous

35:26.420 --> 35:32.500
tasks so when we are training on task one we are also using a generative model to learn to generate

35:32.500 --> 35:38.820
data from task one and these generated are some generated samples are used for training the task

35:38.820 --> 35:45.940
classifier at the end they're not that many methods but we there was one method from cvpr 19 that

35:45.940 --> 35:51.380
we compared against and we were outperforming it on the on all the data sets but actually a very

35:51.380 --> 35:57.540
large margin but it's still a very complex task it's like it sounds like your model is starting to

35:57.540 --> 36:03.140
get very complex when you start to add a generative model on top of everything you've already done

36:03.140 --> 36:12.340
yeah definitely if this is kind of a problem which is very complex already and adding the complexity

36:12.340 --> 36:18.500
of the generative model and being able to generate very sharp and it nice examples just

36:18.500 --> 36:23.620
adds more complexity to the continual learning problem and do you think that there's something

36:24.820 --> 36:34.260
in particular that you've done with the application of conditional networks that allows the

36:34.260 --> 36:38.820
generative model to work or could you have that could you then now that you've seen some good

36:38.820 --> 36:43.940
results with the generative model take that back and apply that to other aspects of the network

36:43.940 --> 36:51.540
without the conditional gating for the unknown multitask problem that that's a very good question

36:51.540 --> 36:58.500
so first of all these gating models sorry these generative models are not going to be used at

36:58.500 --> 37:05.700
the inference time we don't need them it's just used for training the basically the task classifier

37:05.700 --> 37:12.020
and after training you can throw them away but we also actually thought about training this

37:12.020 --> 37:19.620
generative model using a unified giant generative model which can be gated as well in the same

37:19.620 --> 37:25.540
way that we are we are training this this inference model we didn't try that but I think it would be

37:25.540 --> 37:31.940
an interesting idea to use gating for for generating examples as well so creating with task one

37:31.940 --> 37:38.100
generate examples of task one things like that we've got a third paper that we wanted to make

37:38.100 --> 37:47.060
sure to cover as well and that's the time gate paper refresh us on on the setting that that

37:47.060 --> 37:56.020
paper is looking into sure so in that paper we primarily focused on recognizing long range

37:56.020 --> 38:02.340
activities in in long range videos so the problem with long range activity what what first of

38:02.340 --> 38:08.900
what do we call when do we call a video a long range activity so assume we have a 10-minute

38:08.900 --> 38:15.780
video and all that is happening inside that video is a guy skiing and the label of that video

38:15.780 --> 38:21.300
is skiing that's going to be so easy to classify because even if you look at a single frame you

38:21.300 --> 38:26.900
would be able to classify the entire video this we don't call a long range activity a long range

38:26.900 --> 38:33.220
activity could be for example making a pancake because it has so many atomic small action items

38:33.220 --> 38:39.060
which you have to recognize getting getting the egg from the from the freeze for example getting

38:39.060 --> 38:44.100
powder scrambling or mixing everything together and I'm putting on the path so it has a lot of

38:44.100 --> 38:50.340
atomic activities and in order to recognize that this action is making a pancake you have to

38:50.340 --> 38:56.660
identify all of this but all of these could happen at different time intervals inside a 20-minute

38:56.660 --> 39:03.700
video and if you want to go and classify frame by frame all these 20-minute video is gonna explode

39:03.700 --> 39:09.220
it's gonna cost you need to recognize all that or can you just recognize the round thing you know

39:09.220 --> 39:13.460
kind of with bubbles and then getting flipped at the end and then you figure out you you're making

39:13.460 --> 39:18.820
a pancake it could be that there are a lot of classes which are similar for example scrambled eggs

39:18.820 --> 39:24.260
might be making it scrambled eggs could be very similar as well and maybe there are items like

39:24.260 --> 39:30.020
picking up the flower could also be a key item here that would help you identify that it's

39:30.020 --> 39:36.100
actually making a pancake and picking out the flower by itself might not be enough because

39:36.100 --> 39:42.100
the person could be making a very puffed cake as well so you need really multiple things to be

39:42.100 --> 39:48.980
able to to recognize the entire video so for the setting you've got this long video with multiple

39:48.980 --> 39:57.940
tasks being illustrated tasks in the human behavior sense yeah and you're trying to classify

39:57.940 --> 40:07.540
all of the entire video as one thing that's happening exactly exactly so the so one of the very

40:07.540 --> 40:13.140
common way of doing that is if you divide this entire video into chunks of snippets which let's

40:13.140 --> 40:20.580
say each snippet is eight frames and then you could give it to a very heavy model like i3d or

40:20.580 --> 40:27.700
or a resident 3d model that would make a representation out of these eight frames and then you do that

40:27.700 --> 40:32.340
for like a sliding window all throughout your network then you have a classifier that looks at

40:32.340 --> 40:37.860
these features and does the classification this is going to be extremely expensive so what we saw

40:37.860 --> 40:44.820
we saw that we could maybe actually do a gating using a gating module dynamic that dynamically

40:44.820 --> 40:51.060
looks at these frames and decides which parts of the video are interesting to look at so our

40:51.060 --> 40:58.820
approach basically has made up of two steps it has a very very light classifier or basically

40:58.820 --> 41:07.940
network in the start which gets a very compact representation of the of an input frame then we have

41:07.940 --> 41:12.980
a gating module and this gating module is conditioned both on the on the segment and also looks at

41:12.980 --> 41:21.220
the context so not only the current frame but also at the other frames and using this gating

41:21.220 --> 41:28.820
module we are able to to say whether this particular frame is relevant for classifying this video

41:28.820 --> 41:36.420
or not and only the ones which are the most relevant and we could also similar to previous works

41:36.420 --> 41:42.420
impose a sparsity so that the network the light network does not pick so many frames only

41:42.420 --> 41:49.700
a limited numbers and only the most relevant ones would go to a heavy network and do the actual

41:49.700 --> 41:55.780
heavy operation which is which is common we use the same models as well and the good thing is

41:55.780 --> 42:02.180
that you could couple these lights network with any model you could actually use very super efficient

42:02.180 --> 42:07.300
models which are also already implemented and complete with this gating module it would still

42:07.300 --> 42:13.700
give you a lot of benefit and no matter what model we use as the main feature representation

42:13.700 --> 42:20.180
extractor we roughly saved half of the mac operations and even at that point we slightly got

42:20.180 --> 42:27.220
improved performance maybe because we we are able to help the network focus only on the

42:27.220 --> 42:32.900
relevant parts of the video and get over from the distractions and actually get slightly better

42:32.900 --> 42:40.420
performance even even at half the computation cost and this approach is definitely orthogonal to

42:40.420 --> 42:47.140
a lot of compression techniques because some some approaches focus on making the models compact

42:47.140 --> 42:52.740
and let's say you could you could basically do our channel gating on these models as well or

42:52.740 --> 42:59.380
you could do static compression methods and adding these gating modules to only focus on the

42:59.380 --> 43:03.700
relevant parts of the video could really bring down the computation cost overall.

43:03.700 --> 43:11.380
And so is the use of channel gating or conditional gating in this context you know how similar is

43:11.380 --> 43:16.980
it you know an application is conditional gating in the other two papers it seems like it's

43:16.980 --> 43:23.300
somewhat different. It is somewhat different actually so we use the same gumball softmax trick

43:23.300 --> 43:31.860
trick for for training these gating modules but it is basically for training this we we sought

43:31.860 --> 43:39.700
of first extracting a bunch of concept kernels concept kernels could be extracted using

43:40.660 --> 43:45.620
let's say a network which is pre-trained on the data set you want to work on and getting some

43:45.620 --> 43:51.460
representations let's say I want 100 kernels at the end and you you basically compress this data

43:51.460 --> 43:57.780
and say these are the core concepts of my data set and then this gate as inputs instead of

43:57.780 --> 44:03.620
getting or training or during training this is before the start of the training you could this

44:03.620 --> 44:11.940
is an independent concept so you could extract that anyway okay and then for these gating is so

44:11.940 --> 44:16.340
previously we had for example as input to our gates the representation coming from the

44:16.340 --> 44:22.420
resident block here is very different here the gating module would get as input the representation

44:22.420 --> 44:27.300
coming from the last layer of the light network in the start of the model so the light

44:27.300 --> 44:34.420
network first give us some representation and using that and and also we do a dot product of that

44:34.420 --> 44:39.860
with the concept kernels to see if there is anything interesting in this frame given the concept

44:39.860 --> 44:45.940
kernels that I know about this data set and this dot product would go to our gating module and

44:45.940 --> 44:51.860
this gating module well of course it uses the gumball softmax trick which which we've been using

44:51.860 --> 44:59.460
previously as well I think I'm struggling with what is actually gated in this are you gating the

45:00.020 --> 45:06.580
kind of the input flowing into the kind of the the overall network or you you're still doing

45:06.580 --> 45:11.540
something similar where you're you're gating you know resident modules or filters or something

45:11.540 --> 45:20.500
like that the former so so the light net and a heavy net and if the light net says the current

45:20.500 --> 45:29.380
frame is irrelevant the entire frame and and actually this snippet around it so we we analyze the

45:29.380 --> 45:35.220
center of the snippet let's say we have eight frames that goes to the heavy net but the light net

45:35.220 --> 45:40.660
only analyzes that the middle frame of these eight among these eight frames and if you know it's

45:40.660 --> 45:48.740
not interesting then we just completely get not analyze the entire snippet okay and so you've got

45:48.740 --> 45:54.900
these eight frames and you're analyzing the center frame are you striding or is it eight and then

45:55.460 --> 46:02.660
the the next day uniformly sampled and so uniform sample a lot of these segments let's say eight

46:02.660 --> 46:08.260
frame eight frame eight frame until the end to cover the entire video and the only the two net only

46:08.260 --> 46:14.020
analyzes the center frame among these let's say the frame number four and based on that makes

46:14.020 --> 46:21.940
a decision is these the entire chunk of frames useful for the current classification task or not

46:21.940 --> 46:29.700
are there other applications to this then video or why did why did you even go after this

46:29.700 --> 46:38.100
particular problem so we thought gating in general would benefit a lot in let's say classification

46:38.100 --> 46:44.820
problems or or we even tried that on segmentation problem as well but a dataset with the biggest

46:44.820 --> 46:52.340
amount of redundancy we think is video video analyzing video there is so much correlation in

46:52.340 --> 46:57.460
in different frames that you're basically observing the same thing over time that it makes it that

46:57.460 --> 47:05.620
the most low hanging fruit for for gating operations we are actually following up a lot of tasks

47:05.620 --> 47:11.940
using a conditional compute for video analysis as well including video segmentation classification

47:11.940 --> 47:19.300
etc apologies if you mentioned this already but the specific results of this in terms of the

47:20.340 --> 47:26.500
videos and your the computational complexity all that what did you end up finding so you found

47:26.500 --> 47:33.300
that for example no matter what heavy type architecture we we use whether it is the most complex

47:33.300 --> 47:41.060
resident 3d model let's say resident 3d 101 or it's a very efficient model we can reduce the cost

47:41.060 --> 47:46.900
by half while still getting slightly better performance than the original model which so we both

47:46.900 --> 47:53.060
have and perform better and one very interesting property of this this approach is that it makes

47:53.060 --> 47:59.220
interprecipibility very easy because you could immediately look at the chunks of video in the entire

47:59.220 --> 48:05.540
10 minute video that are selected and are they actually correlated so you could very easily analyze

48:05.540 --> 48:13.540
if the gates are working properly or not and did you see any interpretability results in other

48:13.540 --> 48:19.140
couple of papers or problems that you were looking at as a result of this yeah absolutely so in

48:19.140 --> 48:25.540
the first work for the channel gating we we targeted some of the some of the gates that would

48:25.540 --> 48:31.220
uh that the most interesting gates are the ones which are very selective in firing for example

48:31.220 --> 48:36.340
only firing five percent of the times if you look at them the examples look very similar for

48:36.340 --> 48:44.820
example only tiny insects in the middle of grasses are detected so by by this particular gate

48:44.820 --> 48:51.540
or for example all the giant creatures in in the water are detected by a particular gate

48:51.540 --> 48:56.980
and then in the continual learning it was we thought it would be harder to recognize these these

48:56.980 --> 49:05.220
gates uh to to see differences but it was uh very very interesting we we looked at a gate which was

49:05.220 --> 49:11.060
running in all different tasks let's say the task was one versus two classification for task one

49:11.700 --> 49:17.860
three versus four for task two four versus five task three et cetera and there was a gate

49:17.860 --> 49:23.860
that would activate whenever the wall in in the first like blanks i didn't recognize it but then

49:23.860 --> 49:29.700
i focused on it said okay this this gate is actually firing for all the bolt phones and all the

49:29.700 --> 49:36.500
ones which are like the tiny phones uh which is the the defter is not very bold it would not fire

49:36.500 --> 49:41.700
which which found that feature interesting but but it's in general it's a little bit more difficult

49:41.700 --> 49:47.060
uh when the number of classes is not huge to find interpretability but for internet for example

49:47.060 --> 49:52.820
it was very easy to to see uh interesting uh to get interesting insights of what these

49:52.820 --> 50:01.860
gate is doing yeah yeah this this work in conditional gated networks yeah how does this um

50:01.860 --> 50:07.620
ultimately kind of get expressed in you know products and and things like that what do you do with

50:07.620 --> 50:14.100
you know this stuff that you're you know learning and discovering uh so this is currently mostly

50:14.100 --> 50:21.700
at our at a research level of course um we we have been um so the the entire vision is that

50:21.700 --> 50:27.780
we could probably at some point have a very very giant network that could be very sparsely activated

50:27.780 --> 50:32.820
it could even work for multiple tasks completing independent tasks and then condition on the input

50:32.820 --> 50:38.740
it would find out which path in the network uh it would it would actually should should need to be

50:38.740 --> 50:44.500
activated so at the inference time it's gonna be very light but uh but of course um the first step

50:45.300 --> 50:50.900
that we took we talked to the software team and uh to see what are the requirements for

50:50.900 --> 51:00.020
for uh making use of such models and usually um our discussions were were so we we need to convince

51:00.020 --> 51:06.980
them that we need to implement a specific layer is that that would allow uh implementing this

51:06.980 --> 51:13.780
on our our particular hardware uh we are in the middle of a discussion and uh trying to use this

51:13.780 --> 51:20.260
channel gating in a in a number of works um but in general a lot of times this when the when the

51:20.260 --> 51:26.580
trick that we are using or researching is straightforward that could have very fast impact and go

51:26.580 --> 51:33.380
directly into the product for example one of my uh colleagues uh Marcus Nagel and and Martin

51:33.380 --> 51:42.580
Taiman uh have been working on a quantization paper uh which was from iccv 2019 i think uh and this

51:44.020 --> 51:49.860
paper was working very well they they talked to the software team it was immediately implemented

51:49.860 --> 51:56.820
and uh it was suddenly like in a in a matter of a couple of months event to our Snapdragon

51:56.820 --> 52:03.220
platform and people could could actually use it and then later on we got in our uh private

52:03.220 --> 52:08.260
what what's that group i got a message that this uh this particular quantization method is now

52:08.260 --> 52:14.260
employed by this very famous app that i'm not going to name it but but but but not suddenly maybe

52:14.260 --> 52:19.380
hundreds of millions of people are going to are going to make use of the uh the the technique that

52:19.380 --> 52:24.740
you developed it was completed research at some point but now it is actually in your product

52:24.740 --> 52:32.580
wow wow but something like uh like this work is it um you know to what degree is it you know

52:32.580 --> 52:40.340
broadly applicable uh to a wide variety of of applications and in particular we've talked about

52:40.340 --> 52:46.660
you know different applications that it it can be applied to but uh in each of those cases

52:46.660 --> 52:56.100
you're kind of tuning the application of um the conditional gating uh i think quite a bit

52:56.100 --> 53:03.620
you know do you do you see a uh a point in which this is um a kind of off the shelf you know

53:03.620 --> 53:09.060
thing that's automatically applied across different use cases you know what's the the path towards

53:09.060 --> 53:16.900
making it generally applicable and that's a good question i think uh so i'm really advocating

53:16.900 --> 53:23.060
the use of conditional compute uh in general i think it is biologically also very uh very

53:23.060 --> 53:30.580
uh relevant because uh a lot of uh uh the models of the brain say say that the neurons are activated

53:30.580 --> 53:35.860
in a very sparse fashion and it's not like a pruning method that some of the neurons are never

53:35.860 --> 53:41.620
activated it's it's really dynamic sometimes some elements are activated and sometimes not so i

53:41.620 --> 53:50.420
think this should definitely be um a direction in the future uh and uh i think it's kind of

53:50.420 --> 53:56.100
uh it would be weird for me if the future of AI would be that we have let's say for 100

53:56.100 --> 54:01.300
applications we have 100 independent models uh which are only trained with that data and we

54:01.300 --> 54:06.660
run them all one at uh one at the time this this is not going to be the future i think i think it

54:06.660 --> 54:14.500
would be a centralized model which would be very good in operating uh uh on multiple tasks at the same

54:14.500 --> 54:21.700
time and it could sparsely get activated um so initially even i when i talked to some hardware

54:21.700 --> 54:28.980
forks they they thought this might be a little bit uh and not uh in maybe by next year as it may not

54:28.980 --> 54:35.220
happen but they definitely see uh that this might happen at some point because they they see the

54:35.220 --> 54:42.260
vision that this uh this could be part of the future uh of AI models and this immediately says that

54:42.260 --> 54:48.260
for example to get this implemented you need an if else statement so if this happened if get

54:48.260 --> 54:55.060
gate says this operation uh you need to do this and else do that so we need uh first of to see

54:55.060 --> 55:01.780
what can be done by the software forks uh and then if if necessary we could even reach out to

55:01.780 --> 55:07.620
to the hardware forks to influence the next generation of even hardware i think well ballback

55:07.620 --> 55:12.980
thanks so much for taking the time to share with us uh what you're up to there it sounds like uh

55:12.980 --> 55:18.100
really interesting stuff that um you know i hope to to see more of in the future

55:18.100 --> 55:21.140
it was my pleasure thank you very much for having me thank you

55:25.300 --> 55:30.340
all right everyone that's our show for today for more information on today's show

55:30.340 --> 55:40.820
visit twomolai.com slash shows as always thanks so much for listening and catch you next time

