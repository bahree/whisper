Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charington.
I'd like to start off by thanking everyone who's taken the time to check out our new Facebook
and YouTube pages over the past couple of weeks.
We really enjoy hearing from listeners, whether via Facebook, YouTube, Twitter, or email,
so please, please, please continue reaching out to us.
We'd also like to say thanks to the folks over at O'Reilly, who provided us a ticket to
giveaway for next week's Strata plus a Duke World Conference.
With that being said, we do have a winner to announce.
Congratulations to Evan Wright.
Evan is a Principal Data Scientist at Security Startup Anomaly, and he'll be receiving a bronze
pass to the conference, looking forward to meeting you, Evan.
Thanks to all who participated and be on the lookout for more contests in the near future.
Speaking of events, last week I announced one that I'm organizing, and I hope you'll
check it out.
It's called the Future of Data Summit, and it'll be taking place in Las Vegas and May
at the Interop ITX Conference.
To give you a taste of the great content I'm pulling together for you at the event,
last week's podcast featured James McCaffrey of Microsoft Research, who's just one of
the 15 great speakers you're here from at the summit.
We'll be covering a bunch of really exciting topics, including of course machine learning
in AI, but also IoT and edge computing, augmented and virtual reality, blockchain, algorithmic
IT operations, data security, and more.
You can learn more about the summit at twimmolai.com slash future of data.
And now about today's show.
This week my guest is Shubo Sengupta, Research Scientist at Baidu.
I had the pleasure of meeting Shubo at the rework Deep Learning Summit earlier this year,
where he delivered a presentation on systems challenges for deep learning, and we dig into
this topic in the interview.
Shubo has tons of insights to share into how to do deep learning at scale, and even
if you're not operating at the scale of Baidu, I think you'll learn a ton from our conversation
about architecting, productionalizing, and operationalizing deep learning systems.
We also spent some time discussing the role of GPUs and hardware in building scalable
machine learning systems, and that's an area he has a lot to say about, as the author
of the KUDPP library, the KUDA Data Parallel Primitives Library, which was the first parallel
programming library for GPUs.
And now on to the show.
Hello everyone, I am on the line with Shubo Sengupta, a Research Scientist at Baidu.
Shubo and I met at the rework Deep Learning Summit in San Francisco last month.
I really enjoyed his presentation on systems challenges for deep learning, which did a great
job of talking through some of the challenges associated with deep learning at scale.
And I'm very grateful to Shubo for agreeing to join me on the podcast to discuss his presentation
at the summit, as well as some of his other work.
So welcome Shubo, and thanks so much for joining me on the podcast.
Thank you Sam, thanks for having me.
Yeah, I'm really looking forward to this conversation.
There were a bunch of really good presentations at the summit, but as a guy who found his
way to machine learning and AI, but also spends a lot of time thinking about cloud and big
data infrastructure, yours really spoke to me.
So I'm looking forward to digging into it.
Thanks, I was a little bit afraid that people won't like it, because it was a little bit
off track in some sense.
I thought it was fantastic.
Why don't we start with having you share a little bit about your current role at Baidu
and some of the things you've done previously and how you got involved in machine learning
and AI?
Sure, so as you said, I'm a research scientist at Baidu Silicon Valley AI lab, and I've
been with the lab for the past two and a half years, a little bit more than two and a
half years, almost since the inception of the lab.
I primarily work in speech and language, I would say more speech than language, over the
last two and a half years I've primarily worked on what is called ASR, which is the piece
of technology that takes voice as we speak and then converts it into text.
And we have built one of the best English ASR systems and by far the best Mandarin ASR systems
which powers most of our products in China.
And currently I'm starting to work on other things like speech synthesis, a little bit
of language modeling to keep in the same domain.
Surprisingly enough, I actually did not start in speech.
I started my career, so to speak, in computer graphics.
I was very, very interested in special effects and films and I know this sounds a little
bit like a fairy tale, but I came to do a PhD in computer science to this country because
I was so blown away by the films that Pixar was making.
So my goal was if I get to grad school, maybe I'll get to work at Pixar at some point.
And did you ever work at Pixar?
I did.
I spent six months as an intern at Pixar developing some of the early kind of GPU-assisted
what are called relighting tools.
These tools give you a fast preview of the film scenes that the lighting designers are
using to relight the scene.
Okay.
And that was an incredible experience.
I still very fondly look back on that experience.
I think it was back in 2006, so a while ago, but it still sticks in memory.
And that's kind of what I started taking these GPUs that were only used for playing games
and starting to use them for doing a general purpose compute because, you know, I thought
of graphics as just another computation domain.
So during my PhD, I came up with all the basic parallel algorithms that people used to program
GPUs.
So I was very lucky that I was very early in this field and made a lot of impact with
the author.
Yeah.
Was the author of the first parallel programming library for GPUs and did a lot of basic algorithm
work on GPUs.
So, you know, I've been with GPUs for 12 years now almost since the early days when there
was there were very, very hard to program for general purpose compute that were primarily
used for graphics.
So it's very heartening to see that, you know, so many different areas being so positive
and impacted by GPUs on that.
What was the name of the library that you worked on?
CUDPP, CUDA data parallel primitives library.
It's still around and not so much popular anymore because a lot of other libraries like thrust
et cetera, which is good because other people have, I mean, once you leave grad school,
it's very hard to keep on contributing code to that library because you have other engagements
a full-time job, et cetera.
But my advisor and his students are still putting in new algorithms into the library.
So it's still around, people still use it.
And that library itself has spawned other efforts like thrust from Nvidia and modern GPU also
from Nvidia.
Okay.
And I, yeah, and I should also give a shout out to Nvidia because Nvidia had been extremely
kind in financial terms and in resources throughout my research career and they still work
closely with Nvidia even though I don't work there.
But I work with a lot of people in Nvidia on almost a day-to-day basis.
Well, feel free to give that shout out to Nvidia because in fact, I was at a conference
last week where Speaker from Nvidia gave a shout out to this podcast.
Awesome.
Awesome.
Yeah.
I mean, Nvidia has supported my career ever since.
I mean, literally my first day of grad school.
My first day, I think my first week in this country, I was at a talk at Nvidia because
my advisor was there.
One of his students was giving a talk so it was in 2004.
So it's been a while, but the relationship is strong.
That's great.
So I, you know, what my thinking for this conversation was about, was that we would
really dive into the systems types of issues.
And given your expertise in GPUs, I have so many questions about kind of GPUs and how
they're used in ML and AI and some of the libraries that support them.
So at some point, we want to make sure to touch on that stuff as well.
Yep.
Absolutely.
So in your talk, you gave a specific example of a speech recognition that you worked
on that had some interesting challenges.
Can you talk a little bit about that project or one of the, you know, a specific type of
project that you can talk about?
Yeah, sure.
So we have primarily been working on ASR, which is essentially speech recognition for
the past two years or so.
And when we started, one interesting thing about our lab was we didn't have any speech
experts.
We still really don't have any speech experts.
So we decided that since we don't have any experts, then we'll have the data kind of guide
us.
So we knew, we kind of, I wouldn't say new, but we were confident that if we could get
a lot of data, and we knew how to train really deep networks, and we will be able to
build these systems.
So one thing that kind of was a challenge from us for us, at least in the very beginning,
was we had to do a lot of experimentation.
And we still do at scale to give you an idea, typically a publicly available data set.
I think the largest one is 3,000 hours.
And we typically do all our experimentation of multiple of tens of thousands of hours.
So it's an order of, yeah, so it's an order of magnitude difference, and this is hours
of speech data that we're talking about.
Yeah, hours of speech data, yes.
So it's millions of training examples, essentially.
So typically around 14 million utterances is about 10,000 hours if that gives you an idea.
So we have many millions of these training examples that we need to train our networks
on.
And we know that deep learning is a very empirical field.
I mean, nobody tells you that this is the network.
You have to kind of hunt around and figure out what the network actually is.
Which means running a lot of experiments, each of our experiments takes anywhere from
20 to 50 x-flops.
So that's like, you know, XI is 10 to the power 18, so it's a lot of flops for one experiment.
And typically, you would want to finish an experiment within three weeks is pretty much the
patience we have.
So early on, we had to like build the systems that is very, very high-performance.
And it still is, in my opinion, from what the numbers have seen is the fastest training
system in the industry by at least twice as fast of what the best number have seen publicly
so far.
And we kind of had to invent this thing because there is nothing when we started in late
014 around October of 014 is when we started September, October, that time frame.
So we build this thing and we are, I think we're still are ahead.
And what we are doing is slowly kind of open sourcing parts of this framework.
So we want to kind of take our ideas in some sense, infect the field or infect the community
with all these high-performance computing ideas, which I think are very, very appropriate
for deep learning.
Deep learning is this problem that needs a lot of compute.
And that's kind of what also differentiates our group is not only do we do deep learning,
but we do this deep learning through this very high-performance computing approach.
And we do all our experimentation set at big scale, which kind of separates us.
And other people are catching on.
I mean, you see now Google and Facebook and others of a lot of data have kind of come
around to this kind of high-performance centric approach.
Yeah, and one funny thing, there's an anecdote and it is true.
What I like to tell is when we first started training in Mandarin and built a very decent
Mandarin ASR system, not the best, but very decent, there was not a single Mandarin-speaking
person in the group.
Wow.
But this is the power of data.
You can look at a training curve and you know that if the training curve is going down,
you're doing well.
You don't need to know the language.
My colleague, Ryan and I started doing it over, I think the Christmas of O-14, when we
got our first big Mandarin data set, we started training and immediately we started getting
very good results.
That's incredible.
And essentially the same network architecture does both English and Mandarin, which you
know kind of validates your belief that the neural network will learn the differences
in the languages by itself.
And English and Mandarin are very different languages, I mean, they're so different.
And the fact that almost the same neural network architecture does both is still to me,
you know, I've been doing this for two and a half years, is still to me one of the
biggest surprises and kind of validates our approach that led the data guide you.
So when you're approaching one of these problems and you're trying to kind of iterate to a
neural network architecture, where do you tend to start?
Do you start from, you know, published research like Google Med or one of the, you know,
published network architectures that is kind of known good in a given class of problem
or, you know, at this point, do you have, you know, your own best practices or proprietary
network architectures?
How do you tend to approach the network architecture problem?
Yeah, that's a very good question.
And speech is a little bit different from identifying images like you said, an image
net and a Google net kind of architectures, which are stacks of convolutional networks,
because speech has a time dependency, right?
You can think of a speech as a sequence of audio samples, typically a good practice is
to choose audio samples that are 10 milliseconds each.
So every 10 milliseconds, you've got to get an audio sample.
But there is a time dependency, right?
Because what you say, you know, at time t equals something is dependent on what you said,
t minus something.
So the styles of network that we had to train was what are called recurrent nets.
And recurrent nets have been actually around for a long time.
But when we first started in 2014, I don't think anybody had used recurrent nets that
widely in speech, but we kind of had a gut feeling that since recurrent nets captured
this time dependency, that's where we should start.
So we started off doing very vanilla bidirectional recurrent nets, which actually bidirectional
means they go backward and forward in time.
So there's one layer, which goes backward, multiple layers, which goes backward in time and
other multiple layers, which was forward in time.
That's kind of where we started off with.
And Aoni Hanun, who is now back at Stanford, he had done some initial work on speech and
he was at play around with your recurrent networks a bit.
So he was the proponent of using recurrent networks.
And since then we have evolved to be used gated recurrent units, which are a little bit fancier
recurrent networks, LSTMs.
On top of that, you can use even fancier constructs like potential models, which kind of look
at capturing some sort of memory inside these networks.
So in these networks, in some sense, are capturing memory of what you've said in the past.
So yeah, you kind of look at your problem and then there are wide classes of these networks.
So convolutional networks kind of capture features in kind of an image space, in a 2D space.
And this recurrent networks kind of capture features in time dimension.
So depending on where your problem lies, that's where you would start.
For example, tomorrow, for example, I have to, you know, I'm trying to do something in video.
Where would I start?
I would probably start with a recurrent network as a first guess, because video also has
a concept of time.
So that's kind of where you start off with.
And nothing is actually proprietary.
I mean, one good thing about the communities, people are very open about sharing architectures.
Like all our architectures are public, I mean, we publish yearly almost, yearly cadence.
What networks we have been using, and they're all like standard known networks.
Just the architecture is different, like how they're connected, how many layers, what
is the cost function and so on and so forth.
So what's the relationship between the network architecture and underlying systems that
support them?
That's a good question, typically, you know, a lot of people, I think, because Google
Net and because ImageNet has become such a widely known competition that people focus
on these stacks of convolutional nets a lot, recurrent nets because they go in time, they're
a little bit difficult because say you have a really long audio transcript, which means
it will be kind of enrolled in time, you can think of it as a for loop.
So depending on how long the audio transcript is, you have to kind of unroll that loop in
time, which makes it a little bit more challenging compared to like convolutional networks.
So their memory kind of blows up because you have these long utterances that you're transcribing.
Sometimes your deployment can be a challenge because you are doing this small matrix multiplies
which are not very high performance depending on how you do the inference, inference is when
you actually deploy these models.
And you're also, and users are also very, very sensitive to latency, so which means you
have to do it.
Like when you're talking to Siri, for example, you spoke something to Siri and Siri took,
I don't know, one minute to answer, obviously you're not going to be a very happy user.
We've moved on by that point.
Yeah, exactly.
So you have to think about all these issues when you're doing inference for these kind
of recurrent networks.
And then very recently there are network architectures that are incredibly challenging things like
wave net and byte net, which came out of Google DeepMind and PixelCNN, PixelRNN.
So what these networks are doing, they're generating one sample at a time, and this sample
is at a very fine time granularity, which means they're generating one sample every 61
microsecond, which basically 61 microsecond is basically one over 16,000, like a 16 kilohertz
audio.
So these networks are used to send us to the opposite problem.
So given a text, you're synthesizing speech.
So the denure incredibly latency bound, you have to generate an audio sample every 61 microseconds.
And this entire network is like 40 layers, 50 layers, deep, something like that.
So this is actually a research frontier, like how do you make things like wave nets and
byte nets, synthesize things.
Same thing happens with the pictures, like the PixelCNN paper if you look at, I think
it was presented last year's ICML.
They're synthesizing images, one pixel at a time, and to generate each pixel, you have
to do this inference through this deep, really deep network of 40, 50 layers, which makes
this very, very challenging.
So from the system side, I think these networks are kind of at the forefront of difficulty
and challenge of deploying these kind of networks.
Now having been in the field for so long, does it still amaze you that any of this stuff
works?
Oh, on a daily basis.
And I think it's an Achilles heel, in some sense, that the interpretability of these networks
is extremely hard, especially for speech.
If you look at some of the work that has happened in the Google net kind of world where you
can kind of deconvolve a layer and kind of say, okay, this layer is detecting like a
face.
This layer is detecting like a, I don't know, the chin or something, because humans, visually,
we are very rich.
We can look at these patterns visually.
I have no idea how to visualize a recurrent network, so how do you visualize it?
Audio form, I mean, I don't know, because audio has in some sense much less information
compared to like an image, it literally is a picture of a thousand words, right?
So in our domain, it's almost impossible to gain any sort of interpretability in the
networks we train, which makes our life really hard, because we see almost every daily
basis, we see these, you know, some, something we see that is like, how did that happen?
And, you know, it's very hard to gain insight into these networks.
They're very much a black box, which makes them quite hard.
It's incredible to think that this, the neural network is looking at, you know, milliseconds
of speech and is able to do anything with it, you know, let alone create words and have
them be partially intelligible.
Yeah, and it's doing for two languages, which are completely unrelated.
I mean, we do like, when we first start in Mandarin, people were saying, oh, you need
to do explicit tone modeling, because Mandarin is a tonal language and all that stuff, right?
And you're like, you know, we're not linguists, we are not language experts or speech experts,
you're like, no, we're not going to do it and we're going to just try kind of the quote
unquote brute force method of like feeding it to the neural network and see if the neural
network can figure out the tonality of the languages.
And it does.
I mean, you know, if you ask me what layer of the neural network is learning these tones,
I have no idea.
Well, I had a, I had a guest on the show Pascal Fung, who works on, who spent quite a
bit of time in her career working on speech.
And she quoted, I forget the name of the researcher, who's one of the pioneers in statistical
modeling of speech, who said something to the effect of my speech recognition, accuracy
goes up every time I fire a linguist from my lab or something like that.
Yeah.
That's a very popular saying and we kind of jokingly say it inside of her, especially when
we first started that we had like no clue what to do with.
So your accuracy should be at the top, right?
But I think that as you get, I mean, going from say 80, when we first started, I think
we were 80% accurate, 75% accurate.
Going from that to 95, 96% is actually easier than going from 96% or 99%.
Of course.
It's that famous sigmoid core, right?
I mean, you have to 10x more work to get, I don't know, 1% more better accuracy.
Right.
Because making Androang gives us really nice example that it's very hard to build systems
that are better than humans in accuracy.
Because then you kind of lose sense of like, normally when you develop the systems, you
kind of get a sense of why the system is failing.
Maybe the system is failing because it can, because it has background noise or something
like that.
But once it's into the range where the humans will make mistakes, but then you can judge
something that is already better than you, because then you don't know what these
failure cases are.
Right.
So the last 3% or the last 2% is really hard to bridge.
And that's kind of where we are now.
And becoming increasingly hard, we're making progress, but it's not as rapid of a progress.
We are, we want to close the gap.
And I think we can close the gap, but it's going to be incredibly challenging.
And the flip side is, I don't think until you get to that 99% accuracy level, you can't
really have a technology that kind of gives you the sense of magic, right?
You know, right?
It just works.
Right.
You're not there yet with ASR.
Right.
Right.
It's amazing how close we're getting though.
Yeah.
Yeah.
You said a couple of things that I thought were pretty interesting that I'd like to go
back to, one was that you were starting to see results that were 2x better than, you
know, what you were seeing publicly.
Were those results in terms of accuracy, or I think it was training time that you were
what were those.
So those were training times, right?
Yeah.
Training so from the very outset, what we have focused on was not just absolute training
time, but one thing that I learned from NVIDIA while I was NVIDIA is what is called
the speed of light.
The speed of light is basically under ideal circumstances, what is the fastest you can
go?
And that typically is if your processor is running at full speed all the time, which obviously
doesn't happen.
So then what you should aim for is what fraction of the speed of light are you at?
And typically for any real world application, especially for applications that run for
three weeks, if you are at 50% of speed of light, which means, you know, a GPU has a speed
of light of, typically, are on six terraflops if you take a Titan X GPU.
So if you can sustain three terraflops, that's actually incredibly good.
So instead of focusing on absolute training time, we look at what fraction of speed of light
are we?
Because that's a good measure of efficiency, right?
So we target at 50% and I just saw, I think, a paper couple of days back.
I believe from Google, it's a very good paper, it's on a mixture of experts, and they're
I think getting around 25% efficiency.
So the field is in some sense moving there that people are caring about things like efficiency
efficiency.
I've been 25% of speed of light, basically.
And we have been at 50% of speed of light since at least a year and a half.
So in that sense, we have been 2x or more ahead of the field.
And this is something that we as a group have been trying to kind of evangelize that it's
not just important to tell you the absolute number, but also what fraction of speed of light
are you?
Because otherwise, it's hard to measure how efficient you are.
And that's a different kind of thinking I feel.
And I think it, I actually learned it from Nvidia when I was at Nvidia because it's a
game.
When you're developing these algorithms for games, games also have this kind of a hard requirement
that you have to generate a frame every 30th of a second, because it's typically a game
for 30th.
Yes.
Right.
So then you need to do it very, very efficiently, with a computer game, for example.
Right.
Right.
And do you, does your tea, is it reflected in your team composition as well?
Do you have more or folks from a systems background than you would typically find doing
in an AI research group or?
Absolutely.
So this was also something that is very unique to our group.
When we first started kind of formulating the lab, Adam Coates and Andrew kind of understood
that the systems would be a really important aspect of deep learning.
So we have a really deep engine system.
We don't have a lot of people, but the people we have in our group, I think combined have
like 30, 40 years experience in building very, very high performance systems.
And it's also a unique opportunity, because there's a group that people can wear different
hats, in fact, encourage to wear different hats.
So I come from a GPU background, other people come from GPU background, we learn about
speech, about deep learning, about ML, and then ML people and the deep learning to come
from the ML or no more ML deal background, they learn about how to build these systems.
I mean, you can't just write code and, you know, hope for the best.
I mean, you know, depending on what code you write and how you write it or how you architect
it, your training run might take two months in sort of three weeks.
Right.
Right.
And good luck with that.
So it's a very interesting composition of our lab and I don't think it exists anywhere
else, but I think other people like Google, Facebook, etc, they're all seeing the value
of this kind of an approach and they're all kind of ramping up, I mean, the TensorFlow
team is incredible and they're, they're also have a lot of really great systems people
and they're, they've kind of realized the value of this as well.
It's interesting.
I had an opportunity to hear Andrew speak at this conference that I was at last week.
I was at Giga OM AI conference and one of the things that he said that was really interesting
was that, you know, in Silicon Valley, we've got various established practices for, you
know, lots of things, building web apps, building mobile apps, you know, building backend infrastructures
for those kinds of systems like even as, even in spite of how fast those things are moving,
like we have these established, you know, principles and architectures and like, you know, DevOps,
we've got a team composition that kind of works and an AI, we're kind of inventing all
that stuff over again and don't have, you know, people, everyone's doing things differently,
trying to experiment and figure out what works and what works best.
Yeah, I think that's absolutely true.
I think the, there is a notion of what an AI product should be, but I don't think AI
products have had their, what I call the Photoshop moment, right?
For the Photoshop was one of those early desktop apps, so to speak, which kind of defined
what a desktop application should be or, or a, you know, a word perfect or the early days
of Microsoft Word, where, you know, these applications is defined what a desktop publishing
or a desktop content generation application should look like.
Right.
I don't think AI product has had that defining moment that this app or product or service,
people will say, oh, my God, this is how, you know, AI products and services should be
built.
And again, yeah, there's a lot of experimentation, there's a lot of experimentation and personal
assistance, bots, things like Siri and Google Voice and our, our services in China of doing
a SAR, but I don't think I, I don't get the sense that we have had that Photoshop moment
yet.
So another interesting point you made early on was talking about, you talked about some
of the deployment challenges of the neural networks that you work on.
And we often, we often overlook those, like a lot of the conversation I think is around
training and the training challenges, just because it's so time and tense, but you mentioned
some of the matrix multiplication challenges and other things, you know, what do you find
to be, how do you address those challenges and how do you, like, how do you more fully
characterize them?
Yeah, I think the deployment challenges are driven quite a lot by the, the application
themselves, like as I said, any kind of service, like speech or, for example, you're doing
image tagging, for example, in an app, for example, say an app, let's you tag pictures
of food or something, users care, like all these services are very user-facing and, you
know, our attention span or what we expect from a service has decreased over years, like
we really want everything to be very, very snappy, especially for things like speech, right?
Speech is like our interface, like when I talk to you, expect an answer, I don't expect
the silence, like you, you're not going to speak to me after a minute, after I've been
speaking.
So latency becomes really, really important.
And then this is a challenge if the, the models are deployed in the data center, because
if you're using a mobile phone, then they need to take into account the time that takes
for the request to come to your data center, and then you run the model, generate the
output, whatever you want is given, give to the user, and then you send it back.
And for speech, it also depends on like, you know, you're doing, taking a, you know,
speech segment and converting into text, like when do you start dropping up these speech
segments?
So to the user, it feels like it's a completely seamless kind of interaction.
Ideally, you would want these models to be pushed out to the edge, right?
You, I would want my speech recognition engine, the ASR engine, to be running on my phone.
It has not quite happened.
So what people do is they take kind of a split model, where a part of the model is running
on the phone, and the reason you can run it on these devices is because usually they're
so compute intensive, I mean, I can actually take our model and run it on the, on the iPhone
or Android phone or whatever, a top of the line Android phone, but I can bet you that
you're the, I'll drain the battery in like 20 minutes.
And that won't be a very happy user, right?
Right.
So people kind of split the model.
So maybe a part of the model that does hey Siri or hey Alexa or whatever is resident on
the device, and that only does that, that bit.
And then whatever you say after a Siri gets shipped up to the cloud and then transcribed
there and then brought back.
So those are the kind of, so I think more and more as the devices get more capable and
the other thing that people are also looking at is these neural networks that are very
over parameterized.
I mean, you know, typically a neural network would be say 100 million parameters.
So after you train it, it turns out that you can actually shrink it down a lot.
You can compress the models down to really small models without losing a lot of accuracy.
So people have been, and we are doing it as well, taking a network and shrinking it down
and seeing if we can deploy it to an edge device.
And going back to the matrix multiplies, in training, what you do is this thing called
mini batching rates and mini batch, you take a bunch of training examples, shove it into
like one big, big matrix because big matrices are easier to get very high performance on.
When you're doing speech recognition in the cloud for user-facing services, you can't
really take a lot of utterances and shove it into this large matrix because then you have
to wait.
So, and, you know, users don't like to wait.
So then you have this problem of then you are ending up multiplying what I call skinny
matrices having one or two examples, which are not fast.
So then you have to kind of write these special matrix, multiply, multiplication, routines
that are not optimized for fat matrices or normal matrices, but for skinny matrices.
And everybody's doing it.
I mean, Google has a team, I believe, that who does this kind of, you know, special coming
up with special libraries, which are optimized for the skinny matrix multiplication, read
all the same.
So, yeah, the training side is kind of a different world and has its own kind of challenges
that I think doesn't get talked about a whole lot.
But it's also extremely important because this is where you can, I mean, this is where
kind of the rubber meets the road, right?
This is where, you know, users start using your model to do something useful.
Right.
Right.
Right.
The architecture you were describing or at least the end goal of the architecture you
were describing starts to sound a lot to me like, you know, almost like, if you think
about Hadoopland, in one of your first slides in your presentation, there's a picture of
the Hadoop elephant, right?
In Hadoopland, there's kind of this, you know, migration from batch processing, map reduced
to Spark and streaming processing.
It almost sounds like you're describing a similar migration that many AI apps are taking
or will need to take over time.
Yeah, I think I haven't heard the characterization before, but that is exactly right.
Yeah, during, increasingly during, when you deploy, you are, you're, you're, it's some
kind of streaming and when you're streaming, you are very, very latency sensitive, especially
if there's a user that's using the service.
And typically during training, you are batching stuff up into these things called mini batches
to get much more computational efficiency.
And typically one, I think one of the first lessons I learned when I joined grad school
as my advisor said, latency problems are really hard to solve.
I mean, it's, if a problem has high latency and user scare about low latency, that those
problems typically end up taking a lot of engineering effort, it's harder to parallelize
your way out of a latency problem.
It is very hard.
You have to think about, you know, different algorithms and yeah, it's all like different
hardware.
It's, it's a mess.
Like, if your problem is bandwidth challenged or flop challenged, you can, you can try,
you can solve it relatively easily, then trying to solve something that is latency challenged.
And user scare about latency, I mean, it is, we want stuff to work instantly.
We'd be have very little patience for something that is, especially for things like ASR and
then these kind of things.
One of the techniques you described in your presentation was the graph transformation
of some of these problems.
Can you talk a little bit about the problem space there, the motivation and what you found?
Yeah.
So I think it was pretty forward looking.
In fact, after the talk, I met some people who said, you guys are five years ahead of
the field.
Nobody cares.
So, so what's really happening?
It was a good place to be, right?
So what we are seeing is you have, I mean, you can think of a neural network as an execution
graph.
I mean, execution graph has some operations that you're doing in the nodes and then you're
sending some data across the edges.
It's a very generic way or a very flexible way of saying what you're doing.
What we have found out is as we develop these very, very interesting architectures, they
are becoming pretty hard to scale.
So typically, what we would do is we come up with a really interesting architecture.
We start training in TensorFlow or any of the different frameworks because specifying
these architectures is very easy thanks to the work that all the different tool vendors
have done.
We have excellent tooling for specifying architectures.
So we get up and running.
We are training on, say, 1,000, 2,000 hours of data.
And now we need to scale to what we call our native datasets, which are tens of thousands
of hours.
This making this jump from very, doing experimentation on a couple of thousand hours of data to tens of
thousands of hours of data becomes very, very hard.
Because once you have to take these kind of graphs, which tens of locals graph dev and
various tool vendors of their own name, into a bunch of cores, I mean, you can think
of all these GPUs, this bunch of cores, and inside the GPU, there are also another bunch
of cores, this mapping gets incredibly hard.
So what we do is then a bunch of people, we get together and try to hand tune it, hand
map it down to these kind of cores.
Usually it takes us anywhere between eight months to a year where we can take the networks
that we want that we are training on, say, a small amount of data, which is a couple of
thousand hours of data to like a massive amounts of data, to a lot of hand engineering, hand
tooling, which ideally I want to shorten it, like ideally I want to shorten it to two
months, where typically we are looking at, say, a wave net or a bite net or an intentional
model, which are this very, very flexible, very interesting architectures, but also very
complicated to scale.
So that's what I was trying to get at, is like, we really don't have this kind of intermediate
layer, which takes a graph specification from a framework, be TensorFlow or MXNet or
PyTorch or Torch, and takes this graph and then kind of efficiently maps it down to your
GPUs or CPUs or what have you, and scale transparently.
I mean, today you are doing an experiment on one GPU, tomorrow you flip a switch, you're
doing an experiment on 64 GPUs, and you see no performance hit whatsoever.
We just don't have it, and now it's a very extremely painful process.
And then this really kind of gets at productivity of a group.
I mean, we do it because there's no other way, but ideally, it's kind of a wake up call
for the community, for the community to come together and build this software tooling
or the software middle layer that's absent.
And as I said, in full generality, it's an intractable problem.
So you have to figure out what sort of restrictions you want to place, so you can kind of do both
things that you can get the flexibility of looking at the networks that you want to
look at and also be able to scale these networks out to the data set that you want to scale
at.
And they're usually opposing goals.
So you have to carefully figure out what compromises you have to put in the system.
Let's talk a little bit more about the eight months and what goes into that.
Is it that it's a computationally iterative approach to solving this problem, meaning
you've got this model that after one computationally iterative cycle of training a model and coming
up with a model that you think works, you then are having to run it at scale, figure out
where it breaks, tweak it, and then are you remodeling, I guess?
Are you retraining in that process, or are there other steps taking place to get to
a model that runs at scale?
And now it's usually the same model, you just have to re-implemented, say you take an
attentional model, for example, these are pretty complicated models.
And typically what happens is you run out of memory because these models are large, there
is so much intermediate computation and so much intermediate state that you have to save.
You make the models larger, I mean when I say I make the model larger means the individual
layer sizes changes, so the connectivity stays the same, but the individual layer sizes
say go from a dimension of say 256 to say 1024, and when you increase the dimension, stuff
increases by a square, because it's a matrix, so now you're suddenly out of memory, and
then if you're suddenly out of memory, you can run on the GPU.
And then you have to look at the graph of this and figure out, typically as I said, every
edge in the graph is something that stores memory, you need to figure out, okay, I need
to collapse this graph, and collapsing this graph essentially means writing a new program,
because you can think of the nodes as essentially small programs, like a matrix multiply or something,
so then you have to start fusing these nodes in some interesting way, or think about how
do I partition this graph over two different GPUs, right now this entire graph doesn't fit
on one GPU, so now I have to partition this graph over two GPUs, so as soon as you think
of partitioning a graph over two GPUs, it's like, okay, how do I communicate?
All right, all these problems start coming up, it's extremely handcrafted in that sense.
Right, I understand, it sounds like you're starting with this, again, this model that you've
constructed using these tools that isn't necessarily designed for scale, and then you systematically
have to, if we think about this in a context of traditional compute, you systematically
are denormalizing your database, distributing out, and rewriting everything in assembly.
Yeah, pretty much, I mean, at the end of the day for some of the networks, we just have
to rewrite an assembly because they're not fast enough, otherwise, so it's a very like,
it's almost like an artisan chipping away at a problem, and in some sense, I mean,
as soon as you have an artisan chipping away at a problem, it doesn't scale, right?
It slows you down, I mean, that's why we invented compilers because people are tired of writing
assembly by hand, so yeah, so that layer doesn't exist.
It's interesting to think about, as we think about general AI and the path to getting towards
it, to having an existence, a totally generalized AI, yeah, I think we think about a lot of things,
but I'm not sure that in that conversation, at least I've not heard it explicitly called
out, in order to do that, this system is going to have to be on the cloud, it's going to
have to be massive, it's going to have to be trained on massive amounts of data, it's
probably not going to be, well, there's certainly a lot of algorithmic work that needs to
be done, and lots of other types of work that to be done, data, data management, et cetera,
but there are huge systems problems, fundamental systems problems that we're so far from solving.
Yeah, so that's why I was trying to say it's kind of trying to wake up call or whatever.
I think one of the problems also is we as a research community have done a fairly bad
job of having massive amount of publicly available data set, the only big publicly available
data set is ImageNet, and the architectures to do well on ImageNet are actually fairly simple.
We know how to run them fast, very fast, in fact, I mean, I would not be surprised if ImageNet
like networks get embedded in an ASIC and be available in your form and your camera and whatnot.
Right. But a lot of the problems that we deal with in speech generation and speech synthesis
or speech recognition in language understanding, there are no public data sets. I mean,
there are no public data sets that are big. So people do, we look at the research that happens
in network architecture exploration, if you look at the papers that come out in ICLR or ICML
and NIPs, they are all working on really small data sets, so they don't face the challenge of scale.
And I don't blame them, it's not that they're doing it purposefully, just just no data.
I mean, yeah, so that's also, I think, is a challenge that I didn't talk about in that talk
itself because it wasn't the right thing, but this I feel is a really big challenge that we don't
really have good, large, publicly available data sets for stuff that's not ImageNet.
Because if you are thinking about AGI or whatever, we need a lot of publicly available data
where we can actually scale. Unless we come up with algorithms that will work at small scale,
which people are working on that too, but we haven't made as much progress as we have in other fields.
I've heard this, someone described this problem to me as the industry having overfitted
on ImageNet and some of these other data sets. I totally tell that. I mean, I tell that to people
like whoever is willing to listen, that look, you're overfitting on ImageNet. I tell that to every
vendor we work with, they came up with like every, we work with a lot of different hardware vendors
and stuff. And they all come up with like all these results that look, we are XYZ fast on ImageNet
and like, we don't care. We just don't care, it's a solved problem. I mean, ImageNet is solved.
I mean, it just did. I mean, as I said, it's solved enough to the point that I expect to see it in A6.
Yeah. Yeah. Wow. So we are, we're getting close to the end of our hour, but I'd love to hear you
talk through, actually, I had one more question related to the separate talking about.
So all of these system things that we're describing has what's the best way to ask this question.
You have an HPC background, which is, you know, I think increasingly rare in the, you know,
the, in a world where people are coming up from kind of web-based architectures.
Yeah. Yeah. And I don't hear a lot about that in deep learning necessarily and machine learning.
Can you talk about how that kind of thinking is influenced what you've done and where you're
directly pulling from HPC? It's been a while since I've been involved in HPC community,
but does it lead you, for example, to use like Infiniband and some of the exotic HPC stuff,
or how does it influence you? Oh, yes. I mean, Infiniband is not exotic in our world.
It's, it's pretty standard. I mean, when we first started building stuff,
we started using Infiniband. So the HPC kind of thing is, as I said, as efficiency and speed
of light is extremely important to us. So, as I said, the first thing we think about is how
efficient we are in terms of the processor flops. So if the processor is, say, as I said,
six terraflops or seven terraflops, how close to that number can we get over a period of time?
I mean, it just doesn't have to be bursty. So if you say that, oh, I can reach seven terraflops,
but I can only reach it for like, I don't know, two seconds while my whole application takes two hours.
That's no good. So how much can you sustain over the entirety of an application runtime?
Right. Which in, as I said, in our example, isn't many weeks. So that's, I think, the driving kind of
HPC thought is how efficient can I get? And when you think about efficiency, you tend to build systems
that are what I call our tightly coupled, which means, you know, a loosely coupled system is kind
of a web style system, right? You have this one, you blades are connected by probably a gigabit
Ethernet or maybe 10 gig, I don't know, I think it's still gigabit. But we build like really fat
nodes, like each of our node is like eight GPUs in it because we want this computing elements.
If you think of a GPU as a computing element, we want them to be as close together as possible
because we know that, you know, there's a fundamental physical limit on how fast you can move
electrons over a wire. So if you want to move stuff fast, you want the wire to be very close to
each other, similarly with infinity band. So, um, infinity band, we have like infinity band switches
in our rack. So we don't have many racks. Um, but each of our rack is pushing like 30 kilowatt,
of, of power. And then typical, you know, data center rack in Google, um, Facebook or even us,
I mean, our webst, I mean, we have huge data centers in China for web style workloads.
Um, they're like at less than 10 kilowatts. Right. So it's a very different kind of mindset,
which is a mindset that, okay, I need to push, you know, orders of exoflops. So if I have to push
orders of exoflops, I need to be maximally efficient on, uh, on my system and I have to build
system that have very, uh, small distances to transfer data, like really short wire distances.
So you tend to build what I call fat nodes that are, we're very closely connected, uh, to each
other using, um, in usually infinity band or, you know, Ethernet is also catching up, but there's
also like, uh, also telling somebody that I would like to see more research in interconnects,
because low latency interconnect is becoming really important with deep learning. And you see,
research in that with, um, uh, not only from infinity band, from melanox, with also the
mandeling from, uh, Nvidia and then omnipad from Intel. And I think, uh, we will see more and more
of these low latency interconnects, uh, for workloads like deep learning.
Hmm. Great. Great. Uh, so you tell me, uh, based on your time, do we have time to run through?
Yeah, yeah. Ladies and GPU and, um, okay. Awesome. Yeah. So, you know, maybe we should start with, uh, you
know, the, at the highest level, the evolution that you've seen and how folks are applying GPUs to
these types of workloads, why they're important, um, you know, frameworks, uh, QDNN, I can only
assume is related to the, the kuda stuff that you were working on. Like, how does all that
stuff fit together? Um, so it, in some sense, uh, you know, deep learning is a, is a problem area that
is, is squarely in the comfort zone of the GPU, because if you look at the fundamental operation
in deep learning, uh, it is a major regular matrix matrix map. And if you look at graphics,
it is also regular matrix map. And it's smaller matrices, but, but, uh, but it's also matrix
map. So if you have dense matrix algebra, um, which is what essentially deep learning is, almost
every operation and deep learning is a dense matrix map, it's squarely in the comfort zone of the
GPU. And that is why GPUs are doing so well in this space. Um, and then toolkits like QDNN is
essentially taking, uh, things like this special convolutions. I mean, a convolution is in some
sense a dense matrix map, which is what, how kudianN does it? Um, so what kudianN is doing is taking
the special kind of convolutions that can be in four dimensions, um, that is used in deep learning
and putting it in a library, which is exactly the right thing to do. Um, because they're different
from the convolutions that's used in say traditional, uh, vision or traditional, um, graphics or so.
So these are special kind in that sense. Um, and then what kudianN is doing is also kind of moving
up the stack. For example, now kudianN has, um, a GRE implementation, for example, a batch norm
implementations. In some sense, I think kudianN is going to do a pivot and change its name because
it does, it does a lot more than just, um, convolutions and stuff that's used in image net.
There's a more and more they're putting in stuff, um, that's used in, uh, recurrent nets. And then
you have these, uh, frameworks like TensorFlow and PyTorch and, uh, MxNet and Tiano, um, and many more
who, uh, they're probably 20 odd frameworks, uh, that layer over, uh, these frameworks like kudianN,
like mkldnN that I think intel has. Um, and then what these frameworks are doing is let's you, um,
kind of very easily, uh, specify this network architectures, like an image net or a wave net or
a bite net or a intentional net or a recurrent net. Um, uh, and you can think of each one of the, um,
the nodes in these networks as something as a call into kudianN in some sense.
Okay. Um, and they're all in python because python has kind of become the defect to, um, kind of
language in the AI world or the DL world. So, so kuda itself is the API for programming the GPUs,
basically. Yeah. So kuda is both, uh, kuda is a big thing. I mean, different people think of kuda's
different thing ways. Um, kuda is also language in some sense. Okay. Um, it's got a little bit of
its own syntax. It's very much like cc++, uh, and a little bit more syntax. Um, so kuda is a whole
like you can, uh, so you use that and it also has a bunch of API calls that you call into the GPU.
So it's both like a programming framework as API and also this language itself, uh, for programming
the GPUs. Yeah. Okay. Okay. And then kudianN sits on top of that, uh, to provide higher level, uh,
primitives for programming deep neural nets. And then the frameworks, uh,
uh, tors, Deano, TensorFlow sit on top of that. Yep. That's exactly right. Okay. And kudianN is
written, could be written as in kuda, definitely written as in kuda. You can also, I mean, for, uh,
spiss reasons of speed, you can even write an assembly, which they might have done, but yeah,
that's the right picture to have in mind. It's kuda, kudianN framework. Yeah. Going from bottom to top.
Okay. And so where do you see the, what do you see things going on the hardware side? Um,
you know, Intel is trying to, uh, work on some things. Um, the Google has their tensor processing
unit. Um, how do you see how do you see all that's evolving? Yeah. That's a little bit of a hard
question to answer, but it's the future. The future is kind of unclear in some sense. I mean,
the sense that, uh, the network architectures are evolving so fast that it's kind of hard for
the processors to kind of keep up. Um, so far, what has happened is, you know, everything is
this nice matrix multiply thing that works really well on GPUs, but we do have, um, networks
that are actually pretty hard to do on GPUs. For example, WaveNet is one such network.
So I think for the, the chip meant, I mean, ideally you want both to like,
co evolve like the hardware architecture would evolve with the network architecture,
but the problem is, um, it's very easy for me to come up with a very interesting looking
network, um, using TensorFlow, but it's all, it's very hard to build hardware. Uh, building
hardware is this incredibly, um, you know, in resource intensive, money intensive, uh, process.
Um, so it's a little bit hard to see where the hardware is going to evolve. I think the way
Nvidia and Intel are approaching the problem is, okay, let's just make it fast for, uh,
matrixes of some sizes. And we have this benchmark out called deep bench, which tells what
matrix sizes it should be for some of, uh, our networks. And then Google has, uh, developed
TensorFlow, which it's not a lot of details out, but what I suspect it is is more towards inference.
And doing very low power inference, but not so much for training, uh, for some specific models.
So you could know that I was saying that as networks like Google net and things like that
mature, you'll probably see them burnt into a sick and then shipped in a phone or a camera or
wherever you're doing some kind of image or object segmentation. Um, so yeah, I think people
are kind of just hedging their bets and trying to see how the networks are going to mature. And
it kind of affects both ways because I won't make a network, I won't come up to the network that
is not efficient on any processor that I can run on. Right, right. So you have no incentive to
push to envelope, uh, for the hardware folks and the hardware folks have no incentives to push to
envelope because they don't have any networks to run on their stuff. Right. So like, uh, I wouldn't
say no incentive, but I would say they're both looking at each other kind of rarely. Right,
right. Who's going to dump the billions into this science project? Yeah. Exactly. Yeah.
It's an interesting spot to be in. Yeah. I mean, it's going to cost billions. I mean, let's not
get ourself. I mean, you know, that that's where it costs to build like fabs and yeah, all of that
stuff. Yeah. Um, and one of the issues that comes up, uh, at least in, you know, Intel talking about
uh, uh, night's bridge and their advanced processors that are trying to solve this problem or at least,
um, you know, cut off and video at the past is the issue of floating point versus in eight.
Like, can you kind of talk through what that's all about? Yeah. So reduced precision is this
really interesting thing. And this is also why I feel the industry is optimizing towards Google net.
So what? Yeah. We have been trying to do reduced precision for two and a half years.
And doing reduced precision, like, which is what Intel is instead of say float 32 for
recurrent nets like speech is incredibly hard. Uh, we have it is incredibly hard. So I thought,
yeah, just to jump in here. Um, what I had envisioned this as a simplification of the chip that
allows Intel, for example, to stuff more, uh, you know, to make a, a given silicon die able to
do more math. I hadn't really thought of it as something that's harder. You're basically float 32
is your 32 bit floating point. Uh, yeah. So what, when I say hard, I mean, um, uh, getting it.
So if I say a train a network in reduced positions in float 16 or in data in software 32, uh,
that network will not perform in terms of accuracy as well as a float network trained in float 32.
That's what I meant. Um, so if, if I have chips that, you know, can only do reduced precision,
then it doesn't help me at all because my models don't perform well. And why is that I was under
the impression that the reason why in eight was great, we're using integers was great was because
the networks themselves don't inherently take advantage of floating point. Is that, I guess that
doesn't really make sense, right? That only makes sense in the Google net world. This is why I
does. Yeah. Uh, it does not so far has made sense in, in recurrent nets. Um, we may be seeing some
light at the end of the tunnel very early days. Uh, recurrent nets for speech, I feel are some
of the hardest networks you can train. Uh, they're very, very hard to train. Uh, in float 32,
we have figured out quite well, but we have been trying for at least two and a half years to do
a reduced precision. Um, and we have not been able to do it. And when I say have not been able to do
it, what I mean is the models we train using reduced precision is not as good as the models we train
is in float 32. Uh, and if it's not as good, I'm not going to deploy. I mean, right, right.
So that's, that's, I think reduced precision is another case of the industry, um, optimizing on
Google map, um, which, which may be fine because they might have customers, uh, all they want is
object detection, say automated driving, for example, object detection, object segmentation. So
they have a market where they can sell these chips and, um, but, uh, it's not appropriate for
all neural nets, especially not recurrent neural nets yet. I mean, we might able to figure out
how to do it, but we're not there yet. And so I can make sure I understand, um, when we're talking
about training, uh, network and float 32 versus N8 is the issue or it is the issue that, uh,
between the neurons in our neural network, we've got, you know, these weights and the weights are,
you know, multiplied by states to give us outputs. Is it that the weights are integers versus
floats and or the states are integers versus floats? So all that. So, yeah. So,
that's like, I mean, you, you have touched upon a very interesting thing. Like there are many
things in the neural net that you can choose to keep in low position. Do you want to keep the weights
in low position? Do you want to keep the intermediate state that you generate in low position?
Do you want to keep both in low precision? Uh-huh. But do you want to keep only the gradients in low
precision? I mean, there are many design choices you can make. Um, and there are,
uh, all kinds of funny, interesting, unexplainable results depending on what you choose to do when
we train these networks for speech. Uh, typically, uh, at the end of the training process, you spit
out a model, which is basically the weights. Um, so typically you just want the weights to be,
in low precision. But, uh, in recurrent nets, uh, the convergence is extremely sensitive
because you're doing this serial operation through time. Right. Uh, this is, this is extremely
sensitive floated floating point error. Um, to the point that if we take an algorithm and move
from CPU to GPU, the floating point mat is now floating point addition, uh, is not associated,
which means the sequence in which you do the addition. If it's different, you'll get a different
number. Um, so say, uh, you move an algorithm from CPU to GPU, you will get a slightly different
result. Uh-huh. And sometimes that can make and break a model. And that has happened with us
a few times. Um, yeah. So that that space is, I think, it's, uh, it's, it's, it's a field of research.
Like, how do we think of floating point in this new world of, uh, training neural nets? Um,
and I wish people who deal with this kind of stuff, like, uh, the floating point committee or
whatever, um, wouldn't research this more. And in terms of the, the architectures, the GPUs
are more optimized for the lower precision, uh, but they also do. Yeah. So typically GPUs have
only done float because that's what games and games don't need anything better or more than float.
Uh-huh. And then, then GPUs started getting used in oil and gas, for example,
oil and gas needs doubles, uh, for the different partial differential equations they're solving.
And then comes along deep learning. And deep learning is saying that I can do with float and for
things like Google net, I can get away with intake or float 16. Uh, so the GPUs are now starting
to support float 16 as well natively. Um, so that, so the, so the now the GPU vendors have to kind
of target three different markets, one which uses float and other which uses double and other,
a part of the market. Wow. Okay. So this has been super interesting. Uh, before we go,
anything, um, you know, what are you excited about? Anything that you're working on or anything
that, any asks for the community, the industry? Uh, how would you like to close us out?
Um, I, I am personally interested a lot in, in speech synthesis. I think, um, you know,
you can build really natural interfaces. You need, um, speech that sounds natural. Um,
we are far from there. Like that's why you have this robotic voice that talks to you for
various devices. I think there's a lot of research to be done on very natural sounding, um,
speech from all these devices that interact with voice. And, and, and going far out, I mean,
I would really like to see like, you know, a conversation being a first class UI element with
all the devices that we use. Right. Because as I said, conversation is our UI. Um, that'll be,
uh, incredibly nice if we see it and say the next five years or so, it's challenging. It's going
to be very challenging. Um, so that's kind of my, my dream in some sense. And, and in the shorter
term, of course, the systems, uh, goals that I talked about, like this kind of a middleware or
middle ground where we take these graph transformation. And starting to happen a little bit,
TensorFlow has this thing called the XLA, uh, which is really, uh, kind of the first step in that
direction, the XLA compiler that came out with the TensorFlow one release that was last week,
I think. Yep. Um, so the, uh, yeah, so people are starting to take note. And hopefully we are,
they're not like five years ahead of it. Maybe things will get softer,
the next couple of years. Wow. Awesome. Well, Shubo, uh, how can folks find you online? How can they
follow the work that you're doing? Um, so the work that I do at Baidu, we have a website,
research at Baidu.com, where, um, let me see if I have the URL right. Um, uh, and a lot of the work
is there. Um, I'm, you know, I'm very easily findable on LinkedIn, Shubo's and Gupta,
S H U B H O and S E N G U P T A. Um, yeah, I'm happy for people to get in touch with me,
ask me questions, whatever, I'd be happy to elaborate. And thank you for your,
thank you for your time and this platform. Awesome. Well, thank you so much. I really
appreciate you taking the time to join, uh, join in the show and I really enjoyed the discussion.
Yeah, me too. Thank you. Thanks. Bye.
All right, everyone. That's our show for today. Thanks again to Shubo for an amazing conversation.
I hope you all enjoyed it. Once again, thanks so much for listening and for your continued support.
Please remember that we want to hear from you. You can comment on the show via the show notes page,
via the at Twomlai Twitter handle or my own at Sam Charrington handle via our new Facebook and
YouTube pages or just good old fashioned email to Sam at Twomlai.com. Your likes and subscriptions
really help support the show. So keep them coming. The notes for this show will be up on Twomlai.com
slash top slash 14 where you'll find links to Shubo and the various resources mentioned in the show.
Thanks so much for listening and catch you next time.
