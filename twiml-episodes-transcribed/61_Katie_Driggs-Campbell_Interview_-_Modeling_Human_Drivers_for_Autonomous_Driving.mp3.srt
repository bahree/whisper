1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,640
I'm your host Sam Charrington.

4
00:00:23,640 --> 00:00:29,480
We are back with our third show this week and the third in our Autonomous Vehicle Series.

5
00:00:29,480 --> 00:00:35,080
My guest this time is Katie Driggs Campbell, postdoc in the Intelligent Systems Lab at Stanford

6
00:00:35,080 --> 00:00:39,120
University's Department of Aeronautics and Astronautics.

7
00:00:39,120 --> 00:00:44,600
Katie joins us to discuss her research into human behavioral modeling and control systems

8
00:00:44,600 --> 00:00:47,000
for self-driving vehicles.

9
00:00:47,000 --> 00:00:51,540
Katie also gives us some insight into her process for collecting training data, how social

10
00:00:51,540 --> 00:00:55,400
nuances come into play for self-driving cars, and more.

11
00:00:55,400 --> 00:01:00,000
Our Autonomous Vehicle Series is supported by Mighty AI and I'd like to take a brief

12
00:01:00,000 --> 00:01:02,640
moment to thank them for their support.

13
00:01:02,640 --> 00:01:07,800
Mighty AI helps companies working in the Autonomous Vehicle market create training and validation

14
00:01:07,800 --> 00:01:10,360
datasets to support computer vision.

15
00:01:10,360 --> 00:01:16,240
Their platform combines guaranteed accuracy with scale and expertise and includes annotation

16
00:01:16,240 --> 00:01:21,280
software, consulting and managed services, proprietary machine learning, and a global

17
00:01:21,280 --> 00:01:24,680
community of pre-qualified annotators.

18
00:01:24,680 --> 00:01:29,400
If you haven't caught my interview with their CEO, Darren Nakuda, which was the first show

19
00:01:29,400 --> 00:01:37,000
in this series, please be sure to check it out at twimlai.com slash talk slash 57.

20
00:01:37,000 --> 00:01:44,960
And of course, be sure to visit them at www.mty.ai to learn more and follow them on Twitter

21
00:01:44,960 --> 00:01:48,200
at Mighty underscore AI.

22
00:01:48,200 --> 00:01:52,080
Before we jump in, if you're in New York City next week, we hope you'll join us at

23
00:01:52,080 --> 00:01:55,440
the NYU Future Labs AI Summit.

24
00:01:55,440 --> 00:01:59,280
As you may remember, we attended the inaugural summit back in April.

25
00:01:59,280 --> 00:02:04,040
This year's event features more great speakers, including Karina Cortez, head of research

26
00:02:04,040 --> 00:02:10,400
at Google New York, David Venturelli, Science Operations Manager at NASA Ames Quantum AI

27
00:02:10,400 --> 00:02:16,400
Lab, and Dennis Mortensen, CEO and founder of startup x.ai.

28
00:02:16,400 --> 00:02:26,320
For the event homepage, visit aiSummit2017.futurelabs.nyc, and for 25% off of all tickets, use the

29
00:02:26,320 --> 00:02:29,160
code twimla25.

30
00:02:29,160 --> 00:02:33,440
And now on to the show.

31
00:02:33,440 --> 00:02:42,120
All right, everyone, I am on the line with Katie Driggs Campbell.

32
00:02:42,120 --> 00:02:48,600
Katie is a postdoc at Stanford in the Intelligent Systems Laboratory in the Department of Aeronautics

33
00:02:48,600 --> 00:02:50,400
and Astronautics.

34
00:02:50,400 --> 00:02:53,040
Katie, welcome to this week in machine learning and AI.

35
00:02:53,040 --> 00:02:55,120
Yeah, thanks for having me.

36
00:02:55,120 --> 00:02:57,680
I am really looking forward to this conversation.

37
00:02:57,680 --> 00:03:05,080
So as you know, we are kind of in the midst of a series of podcasts on autonomous vehicles.

38
00:03:05,080 --> 00:03:11,480
And usually we end up talking to folks that are in CS or electrical engineering or other

39
00:03:11,480 --> 00:03:17,040
disciplines, but you are out of the Department of, again, Aeronautics and Astronautics.

40
00:03:17,040 --> 00:03:23,640
And so I'm really looking forward to digging into the connection between that and autonomous

41
00:03:23,640 --> 00:03:24,960
vehicles.

42
00:03:24,960 --> 00:03:29,840
And I guess a good way to start is to have you tell us a little bit about your background

43
00:03:29,840 --> 00:03:32,960
and your path to what you're doing now.

44
00:03:32,960 --> 00:03:33,960
Yeah, yeah, sure.

45
00:03:33,960 --> 00:03:37,800
So I'll tell you a little bit about, I guess, how I ultimately ended up in an aerospace

46
00:03:37,800 --> 00:03:38,800
department.

47
00:03:38,800 --> 00:03:43,160
I started out in electrical engineering, actually.

48
00:03:43,160 --> 00:03:49,000
So I did my undergrad at Arizona State University, but I was really interested in control systems

49
00:03:49,000 --> 00:03:51,000
and ultimately robotics.

50
00:03:51,000 --> 00:03:56,080
So I was really interested in how we can control and interact ultimately with people.

51
00:03:56,080 --> 00:03:59,960
So when I applied to grad school, I applied to mostly robotics programs.

52
00:03:59,960 --> 00:04:05,680
So then I ended up at UC Berkeley, where I worked with Ruzuna Baichi in robotics, and

53
00:04:05,680 --> 00:04:08,840
that was in electrical engineering and computer science.

54
00:04:08,840 --> 00:04:11,440
So I slowly shifted to computer science.

55
00:04:11,440 --> 00:04:14,320
And that's, I guess, when I first started working on intelligent vehicles.

56
00:04:14,320 --> 00:04:19,160
So when we first got started on this, this was, you know, six some years ago.

57
00:04:19,160 --> 00:04:23,720
So autonomous vehicles weren't, I guess, they hadn't really quite hit the road.

58
00:04:23,720 --> 00:04:25,720
So we were actually looking at...

59
00:04:25,720 --> 00:04:26,720
More pun intended.

60
00:04:26,720 --> 00:04:27,720
Exactly.

61
00:04:27,720 --> 00:04:28,720
Exactly.

62
00:04:28,720 --> 00:04:32,440
We were actually working on semi-autonomous vehicles, and we found that the big problem

63
00:04:32,440 --> 00:04:36,080
with semi-autonomous vehicles was how do you model the human?

64
00:04:36,080 --> 00:04:39,560
So we started to think about how we can model the human, how we can design control systems

65
00:04:39,560 --> 00:04:40,560
to keep people safe.

66
00:04:40,560 --> 00:04:45,240
So we thought a lot about texting while driving, how different environmental influences

67
00:04:45,240 --> 00:04:49,640
will affect your driving abilities, and how we can ultimately, you know, design active

68
00:04:49,640 --> 00:04:51,480
safety systems to keep you safe.

69
00:04:51,480 --> 00:04:57,000
And slowly over the years, this shifted to autonomous vehicles like everyone else.

70
00:04:57,000 --> 00:04:58,640
We're still the human had a huge impact.

71
00:04:58,640 --> 00:05:03,000
So we were still thinking about how people either interact with autonomy in the vehicle,

72
00:05:03,000 --> 00:05:07,960
or how the autonomous vehicle can interact with other people or human drivers on the road

73
00:05:07,960 --> 00:05:08,960
around them.

74
00:05:08,960 --> 00:05:11,560
That's sort of how I slowly shifted into autonomous vehicles.

75
00:05:11,560 --> 00:05:12,560
Yeah.

76
00:05:12,560 --> 00:05:16,800
So then I finished my PhD, and I started working in an aerospace lab.

77
00:05:16,800 --> 00:05:21,640
So we're the AI section of the aerospace department here.

78
00:05:21,640 --> 00:05:27,440
And so the lab I work in is famous for air flight and collision avoidance systems, actually.

79
00:05:27,440 --> 00:05:28,440
Okay.

80
00:05:28,440 --> 00:05:29,440
Yeah.

81
00:05:29,440 --> 00:05:33,920
So thinking about how we can design safety systems for planes, which isn't all that different

82
00:05:33,920 --> 00:05:34,920
from vehicles, actually.

83
00:05:34,920 --> 00:05:40,040
So the lab I'm in is actually mostly funded by vehicle companies and working on autonomous

84
00:05:40,040 --> 00:05:41,880
vehicles and decision making for vehicles.

85
00:05:41,880 --> 00:05:48,080
So it's sort of a different than maybe you're what you might expect from an aerospace

86
00:05:48,080 --> 00:05:50,840
department, but a lot of the systems are pretty similar.

87
00:05:50,840 --> 00:05:58,000
And I imagine that aerospace departments have been working on this problem of, you know,

88
00:05:58,000 --> 00:06:04,360
auto pilot, right, comes from, you know, piloting a plane and, you know, space programs and

89
00:06:04,360 --> 00:06:09,680
things like that have been trying to have autonomous or semi-autonomous remote vehicles

90
00:06:09,680 --> 00:06:13,120
on, you know, like the lunar land or things like that.

91
00:06:13,120 --> 00:06:18,480
I imagine it was that there's quite a history in aeronautics and aerospace departments

92
00:06:18,480 --> 00:06:19,480
around this kind of work.

93
00:06:19,480 --> 00:06:20,480
Is that the case?

94
00:06:20,480 --> 00:06:21,480
Yes.

95
00:06:21,480 --> 00:06:22,480
Yes, definitely.

96
00:06:22,480 --> 00:06:27,960
So I've heard aerospace described as the department that is or studies the system of systems.

97
00:06:27,960 --> 00:06:31,760
Which is exactly what autonomous vehicles are, exactly what planes are, but it's really

98
00:06:31,760 --> 00:06:36,040
thinking about how all these different components and the pilots and autonomy and all these things

99
00:06:36,040 --> 00:06:37,040
sort of fit together.

100
00:06:37,040 --> 00:06:39,200
So yeah, definitely.

101
00:06:39,200 --> 00:06:43,480
And exactly what you said, there's a long, long history of dealing with automation in

102
00:06:43,480 --> 00:06:44,480
the aerospace industry.

103
00:06:44,480 --> 00:06:49,120
So it really sort of fits nicely into a lot of the same curtain.

104
00:06:49,120 --> 00:06:50,120
Awesome.

105
00:06:50,120 --> 00:06:51,120
Awesome.

106
00:06:51,120 --> 00:06:53,520
So what's your research focus there?

107
00:06:53,520 --> 00:06:58,520
So here I am currently working on generally autonomous vehicles.

108
00:06:58,520 --> 00:07:02,200
So a lot of the decision making and control.

109
00:07:02,200 --> 00:07:06,880
So how can we use things like deep learning to come up with very human-like decisions?

110
00:07:06,880 --> 00:07:07,880
Okay.

111
00:07:07,880 --> 00:07:13,080
And how then do we use these sort of, perhaps not the most trustworthy systems like deep

112
00:07:13,080 --> 00:07:14,560
learning can be?

113
00:07:14,560 --> 00:07:18,160
How can we design that robust controllers to execute these decisions?

114
00:07:18,160 --> 00:07:22,560
So how do we still get some of the robustness from the more traditional learning control techniques

115
00:07:22,560 --> 00:07:25,800
while using these more advanced sort of AI tools?

116
00:07:25,800 --> 00:07:26,800
Hmm.

117
00:07:26,800 --> 00:07:30,520
So what do you mean when you say human-like driving?

118
00:07:30,520 --> 00:07:31,520
Right.

119
00:07:31,520 --> 00:07:35,680
So I think, well, personally, when I think about autonomous vehicles, hitting or coming

120
00:07:35,680 --> 00:07:40,280
onto the road relatively soon, sorry, no more puns.

121
00:07:40,280 --> 00:07:43,400
We need to think about how they'll integrate with the human drivers that are currently

122
00:07:43,400 --> 00:07:44,400
on the road.

123
00:07:44,400 --> 00:07:50,200
We can't expect that there's going to be homogeneous autonomous vehicles anytime soon.

124
00:07:50,200 --> 00:07:54,200
So they need to be able to interact with other humans on the road, and the other humans

125
00:07:54,200 --> 00:07:57,440
on the road need to be able to interact with that autonomous vehicle.

126
00:07:57,440 --> 00:08:01,040
So you can't have the autonomous vehicle doing things that are unexpected, even if they

127
00:08:01,040 --> 00:08:07,880
might be optimal in some sense, they need to be optimal in the social context that it

128
00:08:07,880 --> 00:08:09,520
will be driving in.

129
00:08:09,520 --> 00:08:10,520
Mm-hmm.

130
00:08:10,520 --> 00:08:15,680
And so responding to erratic St. Louis or New York drivers, those are the ones that I know

131
00:08:15,680 --> 00:08:20,160
the best, but I'm sure everyone has their, I guess, probably wherever you are.

132
00:08:20,160 --> 00:08:21,160
Yep.

133
00:08:21,160 --> 00:08:22,160
The drivers that you hate.

134
00:08:22,160 --> 00:08:23,160
Yep.

135
00:08:23,160 --> 00:08:24,160
California stops.

136
00:08:24,160 --> 00:08:25,160
Exactly.

137
00:08:25,160 --> 00:08:26,160
Exactly.

138
00:08:26,160 --> 00:08:27,160
So you mentioned that.

139
00:08:27,160 --> 00:08:34,960
And then you also mentioned an aspect of the research that is, so we're doing all these

140
00:08:34,960 --> 00:08:37,560
things to build out these deep learning models.

141
00:08:37,560 --> 00:08:42,400
And then maybe we think about the deep learning models as directly controlling things, but

142
00:08:42,400 --> 00:08:45,880
it sounds like you're suggesting that a big part of your research is, well, maybe if we

143
00:08:45,880 --> 00:08:52,880
put some other stuff between the deep learning models and the drive by wire systems, you

144
00:08:52,880 --> 00:08:54,880
know, we can get better results.

145
00:08:54,880 --> 00:08:55,880
Yes.

146
00:08:55,880 --> 00:08:56,880
Yes.

147
00:08:56,880 --> 00:09:02,080
And I think, from my perspective, it's really important, until you can really prove things

148
00:09:02,080 --> 00:09:05,880
about how the deep learning and the decision making that comes from these learning models

149
00:09:05,880 --> 00:09:08,200
or how they're going to function.

150
00:09:08,200 --> 00:09:13,240
I think it is important that you have sort of some more reliable method for actually executing

151
00:09:13,240 --> 00:09:18,440
these maneuvers and making sure that they're safe and interpretable even.

152
00:09:18,440 --> 00:09:26,560
So can you maybe walk us through some of the specifics about the research in each of

153
00:09:26,560 --> 00:09:27,560
these camps?

154
00:09:27,560 --> 00:09:34,600
I guess we can start with what are all of the research challenges associated with autonomous

155
00:09:34,600 --> 00:09:39,000
vehicles interacting with human driven vehicles?

156
00:09:39,000 --> 00:09:40,000
Yeah.

157
00:09:40,000 --> 00:09:41,000
Sure.

158
00:09:41,000 --> 00:09:49,280
I think one of the really big problems is that you basically, you can model people, but

159
00:09:49,280 --> 00:09:54,000
every model is only going to, or it's going to have places where it fails.

160
00:09:54,000 --> 00:09:59,960
So coming up with a way to balance, you know, a general model of how people typically behave,

161
00:09:59,960 --> 00:10:05,640
while still capturing sort of the crazy things that people also do, is a really hard problem.

162
00:10:05,640 --> 00:10:11,160
As you get something like, you're either not accounting for the places where you really

163
00:10:11,160 --> 00:10:12,600
need to be safe.

164
00:10:12,600 --> 00:10:16,960
So when people do something really unexpected or maybe do something kind of crazy, but

165
00:10:16,960 --> 00:10:20,440
on the flip side, you'll be over-conservative if you only model those things.

166
00:10:20,440 --> 00:10:22,480
So you still want to be able to get to your destination.

167
00:10:22,480 --> 00:10:27,880
So there's some balance between, you know, being safe and actually thriving normally.

168
00:10:27,880 --> 00:10:33,080
So figuring out how to balance that in an intelligent way is really, really difficult.

169
00:10:33,080 --> 00:10:37,680
And what's the general approach to that that you've taken in your research?

170
00:10:37,680 --> 00:10:41,880
So the general approach that I've taken is trying to do something like switching maneuvers.

171
00:10:41,880 --> 00:10:45,920
So if you have, you try and detect basically when people are starting to deviate from the

172
00:10:45,920 --> 00:10:47,920
normal sort of expected behaviors.

173
00:10:47,920 --> 00:10:52,520
If you have a typical model of how people behave, you can use that for the most part, but

174
00:10:52,520 --> 00:10:56,360
you have to always be aware and always be monitoring people for when they start, as I said,

175
00:10:56,360 --> 00:10:57,360
deviating.

176
00:10:57,360 --> 00:11:00,400
So you look for sort of like the anomaly driver.

177
00:11:00,400 --> 00:11:03,000
And then you can start saying, okay, this person's acting a little bit weird.

178
00:11:03,000 --> 00:11:07,320
So I'm going to be more conservative around this driver.

179
00:11:07,320 --> 00:11:11,080
What does it even mean to have a model of how people behave?

180
00:11:11,080 --> 00:11:18,240
Like I guess I think of, I think of the stuff that I've seen, you know, around deep learning

181
00:11:18,240 --> 00:11:25,640
is maybe like a lower level, you know, kind of lower level models or capturing lower level,

182
00:11:25,640 --> 00:11:30,760
you know, ideas about how to interact, how a vehicle might interact with the world.

183
00:11:30,760 --> 00:11:36,120
Is there also a part of the system that is, you know, trying to capture at large, like

184
00:11:36,120 --> 00:11:38,600
the behavior of people?

185
00:11:38,600 --> 00:11:39,600
Yeah.

186
00:11:39,600 --> 00:11:46,120
So at least in my work, there is a lot of my PhD work was coming up with general models

187
00:11:46,120 --> 00:11:47,280
of how people behave.

188
00:11:47,280 --> 00:11:51,440
So how do you take all these big data sets and come up with models that are useful from

189
00:11:51,440 --> 00:11:52,440
that?

190
00:11:52,440 --> 00:11:53,440
So yeah.

191
00:11:53,440 --> 00:11:59,160
So whether some examples of things that you can get a model to, that you can kind of capture

192
00:11:59,160 --> 00:12:04,480
in a model about a person's behavior with regard to driving in particular.

193
00:12:04,480 --> 00:12:08,560
So in driving in particular, we've really been thinking about lane changing behaviors.

194
00:12:08,560 --> 00:12:14,200
So it's a pretty common maneuver and it's pretty easy to find examples of.

195
00:12:14,200 --> 00:12:19,120
So in this work, we've been thinking about how people respond to merging behaviors.

196
00:12:19,120 --> 00:12:23,040
So if you try to cut someone off, how are they likely to respond?

197
00:12:23,040 --> 00:12:28,240
If you want to execute a maneuver, what sort of cues do you need to send to that person

198
00:12:28,240 --> 00:12:32,320
to make sure that they will actually let you in if the gap is not big enough?

199
00:12:32,320 --> 00:12:36,960
So how do you sort of handle these like social nuances in your motion and in your trajectory

200
00:12:36,960 --> 00:12:37,960
planning?

201
00:12:37,960 --> 00:12:38,960
Okay.

202
00:12:38,960 --> 00:12:42,680
What's the general approach you've taken to address that kind of thing?

203
00:12:42,680 --> 00:12:48,640
Like is it changes to the way you model or is it, you know, a set of heuristics that

204
00:12:48,640 --> 00:12:54,120
you kind of build around the model or are you injecting things into the system otherwise?

205
00:12:54,120 --> 00:12:55,120
Right.

206
00:12:55,120 --> 00:12:59,280
So in this original work for modeling how people behave and how people respond, we were

207
00:12:59,280 --> 00:13:02,240
really trying to think about how we can come up with a robust prediction.

208
00:13:02,240 --> 00:13:07,120
So we actually came up with a new modeling method to capture these both behaviors in

209
00:13:07,120 --> 00:13:09,040
a sort of a more general fashion.

210
00:13:09,040 --> 00:13:14,320
So instead of thinking about trying to predict a person by guessing their exact trajectory,

211
00:13:14,320 --> 00:13:18,400
we started thinking about how we can come up with basically sets that they might follow.

212
00:13:18,400 --> 00:13:21,640
So we think of an area that they might enter basically.

213
00:13:21,640 --> 00:13:25,240
So when you start thinking about things in terms of set behavior instead of just an exact

214
00:13:25,240 --> 00:13:27,840
trajectory, you get a much more robust prediction.

215
00:13:27,840 --> 00:13:31,360
So you might be off a little bit but you'll still capture the general behavior.

216
00:13:31,360 --> 00:13:36,120
And this is sort of where what I was mentioning before with balancing being over conservative

217
00:13:36,120 --> 00:13:39,120
and being quite precise kind of comes in.

218
00:13:39,120 --> 00:13:43,440
So when you start basically reducing the uncertainty in your prediction, you'll shrink

219
00:13:43,440 --> 00:13:46,400
this set down to something that's smaller and more precise.

220
00:13:46,400 --> 00:13:50,840
But if you're more uncertain or you start detecting some anomalies, you can grow this

221
00:13:50,840 --> 00:13:53,520
set out and capture more of the uncertainty.

222
00:13:53,520 --> 00:13:56,480
And that will just automatically influence how you change.

223
00:13:56,480 --> 00:14:00,440
So if you basically take these sets and incorporate them into your low level controller that

224
00:14:00,440 --> 00:14:05,120
is planning and trying to keep you in safe regions, this will sort of automatically be captured

225
00:14:05,120 --> 00:14:06,600
by that.

226
00:14:06,600 --> 00:14:13,480
And in this work, does the set represent are you evaluating the set in terms of kind

227
00:14:13,480 --> 00:14:21,920
of, you know, in or out or likelihood of in or out or are you looking at like geographic

228
00:14:21,920 --> 00:14:26,600
regions as probabilistic fields that are maybe more continuous?

229
00:14:26,600 --> 00:14:27,600
Right.

230
00:14:27,600 --> 00:14:28,600
Right.

231
00:14:28,600 --> 00:14:29,600
So a little bit of both.

232
00:14:29,600 --> 00:14:35,560
So the ultimate or the output of this model is something like confidence intervals.

233
00:14:35,560 --> 00:14:39,680
So you get these sort of strict boundaries and a probability associated with these different

234
00:14:39,680 --> 00:14:43,640
basically level sets of trajectories. So it's a little bit of both.

235
00:14:43,640 --> 00:14:48,440
So once you pick what confidence interval you would like to pick, you have a strict set

236
00:14:48,440 --> 00:14:51,840
and you can basically evaluate this by inner out.

237
00:14:51,840 --> 00:14:56,760
But you can also look at this as basically a series of confidence intervals.

238
00:14:56,760 --> 00:15:00,120
So then you get something like an empirical distribution back out.

239
00:15:00,120 --> 00:15:01,120
Okay.

240
00:15:01,120 --> 00:15:02,120
Okay.

241
00:15:02,120 --> 00:15:09,280
When you're implementing this in the lane changing context, is it, I guess I'm trying

242
00:15:09,280 --> 00:15:15,600
to to picture the, I guess, dimensionality is overloaded.

243
00:15:15,600 --> 00:15:21,520
But like if you are you thinking about it from the perspective of a car and like the lane

244
00:15:21,520 --> 00:15:27,760
ahead, you know, how many feet from the vehicle, another vehicle is likely to intrude on.

245
00:15:27,760 --> 00:15:33,920
So like a two dimensional kind of interaction or is it more a three dimensional interaction

246
00:15:33,920 --> 00:15:40,000
where you are thinking about the distance between your car and your lane and the other car

247
00:15:40,000 --> 00:15:44,680
in the other lane as well as where it might intrude into your lane?

248
00:15:44,680 --> 00:15:46,360
That's a great question.

249
00:15:46,360 --> 00:15:52,920
So in most of the work that I've been doing, I've been looking at basically a set of three

250
00:15:52,920 --> 00:15:54,160
vehicle interactions.

251
00:15:54,160 --> 00:15:58,080
So if you think about predicting the vehicle that you want to merge in front of, you can

252
00:15:58,080 --> 00:16:02,160
think about the influences of that vehicle basically being what your actions will be in

253
00:16:02,160 --> 00:16:03,760
a vehicle in front of them.

254
00:16:03,760 --> 00:16:08,200
We've been sort of looking at that network, but we can expand this out by sort of looking

255
00:16:08,200 --> 00:16:10,240
at things like clusters of behavior.

256
00:16:10,240 --> 00:16:16,800
So if you think about different vehicles, sort of inserting more influences on vehicle,

257
00:16:16,800 --> 00:16:20,440
you can think about how this sort of generally happens by looking at clusters of behavior.

258
00:16:20,440 --> 00:16:24,600
So if there's more vehicles, you can basically do a look up and get more clusters or similar

259
00:16:24,600 --> 00:16:26,600
clusters, similar situations.

260
00:16:26,600 --> 00:16:30,960
And by the similarity metric, you can expand this out to more vehicles and larger networks

261
00:16:30,960 --> 00:16:32,440
and things like that.

262
00:16:32,440 --> 00:16:33,440
Okay.

263
00:16:33,440 --> 00:16:34,440
Yeah.

264
00:16:34,440 --> 00:16:37,880
I've seen some interesting videos of how to driver behavior.

265
00:16:37,880 --> 00:16:46,040
And I think even autonomous vehicle interactions with drivers where, you know, the vehicle isn't

266
00:16:46,040 --> 00:16:48,800
given a certain level of aggressiveness.

267
00:16:48,800 --> 00:16:53,560
It will just get stuck like it cannot execute a lane change.

268
00:16:53,560 --> 00:17:00,000
And so there's, you know, there's this need that you're describing for the vehicle to,

269
00:17:00,000 --> 00:17:06,240
you know, not just be able to anticipate human behavior, but to start to emulate human behavior

270
00:17:06,240 --> 00:17:10,960
because like signaling to go into another lane isn't just, you know, putting on the blinkers.

271
00:17:10,960 --> 00:17:14,240
It's also like starting to go into the other lane.

272
00:17:14,240 --> 00:17:15,240
Exactly.

273
00:17:15,240 --> 00:17:16,240
Exactly.

274
00:17:16,240 --> 00:17:19,560
I was talking to someone in LA about this and they said, if you turn your blinker on that

275
00:17:19,560 --> 00:17:21,640
is a cue for the other driver to just speed up.

276
00:17:21,640 --> 00:17:22,640
Exactly.

277
00:17:22,640 --> 00:17:24,440
So you don't just want that.

278
00:17:24,440 --> 00:17:25,440
Right.

279
00:17:25,440 --> 00:17:31,240
So you've got, you've got this ability to kind of predict other driver's behaviors and

280
00:17:31,240 --> 00:17:40,360
when they're likely to enter your lane and you can kind of expand that to multiple vehicles.

281
00:17:40,360 --> 00:17:42,440
I, how do you, what's the next step?

282
00:17:42,440 --> 00:17:46,680
How do you kind of build on that to create a more robust system?

283
00:17:46,680 --> 00:17:47,680
Right.

284
00:17:47,680 --> 00:17:52,720
So now that we have a good model of our environment, if we believe this is a good model of our environment.

285
00:17:52,720 --> 00:17:56,960
So the next layer that we were thinking about is how can we safely execute sort of high

286
00:17:56,960 --> 00:17:57,960
level decisions.

287
00:17:57,960 --> 00:18:00,440
And this is where we started using deep learning.

288
00:18:00,440 --> 00:18:04,800
So if we have a good representation of the environment we want to think about, should

289
00:18:04,800 --> 00:18:06,240
I execute a lane change?

290
00:18:06,240 --> 00:18:08,800
Will this help me achieve my goal?

291
00:18:08,800 --> 00:18:13,400
And also there is some, we wanted to see if we could capture things like sort of implicit

292
00:18:13,400 --> 00:18:14,400
rules.

293
00:18:14,400 --> 00:18:19,360
So there are some rules that aren't necessarily captured by the letter of the law.

294
00:18:19,360 --> 00:18:22,560
So like at intersections, there's a strict ordering.

295
00:18:22,560 --> 00:18:23,560
That happens.

296
00:18:23,560 --> 00:18:27,080
So if one vehicle comes first, they get to pass through.

297
00:18:27,080 --> 00:18:29,880
There's some yielding rules, but people don't always follow these rules.

298
00:18:29,880 --> 00:18:32,760
There's so much certainty in these rules.

299
00:18:32,760 --> 00:18:37,520
So we've been using deep learning and simulation to try and capture some of these sort of nuance

300
00:18:37,520 --> 00:18:40,880
behaviors to come up with these sort of high level decisions.

301
00:18:40,880 --> 00:18:42,320
Like what high level action should I do?

302
00:18:42,320 --> 00:18:46,280
So these high level actions can be things like execute a lane change or go ahead and move

303
00:18:46,280 --> 00:18:48,200
through the intersection now.

304
00:18:48,200 --> 00:18:50,720
And so we've been doing that with, as I mentioned, deep warning.

305
00:18:50,720 --> 00:18:54,040
But deep learning in and of itself is not very trustworthy.

306
00:18:54,040 --> 00:18:58,200
So a lot of what we've been doing is trying to think about how we can develop new tools

307
00:18:58,200 --> 00:19:03,360
and new learning algorithms to try and make these systems more, or give some insight to

308
00:19:03,360 --> 00:19:05,000
the confidence of the system.

309
00:19:05,000 --> 00:19:09,840
So can we determine when our deep learning algorithm is uncertain about its decision?

310
00:19:09,840 --> 00:19:12,480
And if we know when it's uncertain, we can decide whether or not we should listen

311
00:19:12,480 --> 00:19:14,120
to it or not.

312
00:19:14,120 --> 00:19:18,120
And this example, what's the training data?

313
00:19:18,120 --> 00:19:23,440
So in this example, we've been putting a lot of effort into coming up with good simulated

314
00:19:23,440 --> 00:19:24,720
traffic models.

315
00:19:24,720 --> 00:19:28,840
So we can basically train in simulation and then transfer it to a real vehicle, which

316
00:19:28,840 --> 00:19:32,360
is a whole other problem that we're also working on.

317
00:19:32,360 --> 00:19:33,360
Okay.

318
00:19:33,360 --> 00:19:41,920
So the simulated traffic data is, I mean, I'm envisioning like the video game Frogger.

319
00:19:41,920 --> 00:19:46,800
You've just got all this traffic and you're, it's kind of moving at different speeds

320
00:19:46,800 --> 00:19:53,200
and is that how you're not literally, but is, are you just generating several lanes

321
00:19:53,200 --> 00:19:59,000
of traffic and how are you representing the vehicles in this training data set, for example?

322
00:19:59,000 --> 00:20:00,000
Right.

323
00:20:00,000 --> 00:20:04,160
So there's a lot of work in the lab that I'm in that goes into coming up with good driver

324
00:20:04,160 --> 00:20:07,160
models for generating traffic.

325
00:20:07,160 --> 00:20:11,560
So there's lots of really interesting work going on and actually using deep learning to

326
00:20:11,560 --> 00:20:16,360
mimic these driving behaviors so you can validate your autonomous vehicle.

327
00:20:16,360 --> 00:20:18,400
So how do you generate these scenarios?

328
00:20:18,400 --> 00:20:21,160
How do you generate realistic behavior?

329
00:20:21,160 --> 00:20:25,760
So then you can either train your system or just do some validation.

330
00:20:25,760 --> 00:20:30,120
So if you just need to make sure it works or you get some metric for how likely this

331
00:20:30,120 --> 00:20:34,200
should crash, you can use these validation tools to do that.

332
00:20:34,200 --> 00:20:35,200
Mm-hmm.

333
00:20:35,200 --> 00:20:38,480
I'm still trying to visualize the training data set.

334
00:20:38,480 --> 00:20:45,840
Is it like are you looking at a car as a, like a two dimensional kind of representation

335
00:20:45,840 --> 00:20:51,600
of points or space or something like that or are there some simplifying assumptions

336
00:20:51,600 --> 00:20:56,920
or is it maybe more complex than that, you know, based on camera imagery or something?

337
00:20:56,920 --> 00:21:00,880
So these ones we are just using, we have a simulated LiDAR sensor.

338
00:21:00,880 --> 00:21:05,560
So from our ego vehicle, we basically just use these detection points, which are pretty

339
00:21:05,560 --> 00:21:09,920
similar to what we can extract from sensors on a real vehicle.

340
00:21:09,920 --> 00:21:14,560
So from the ego vehicle's perspective in simulation, you basically just get something

341
00:21:14,560 --> 00:21:18,880
like a LiDAR image, but projected down to just a 2D plane.

342
00:21:18,880 --> 00:21:19,880
Okay.

343
00:21:19,880 --> 00:21:20,880
Got it.

344
00:21:20,880 --> 00:21:29,200
So is it the 2D plane from above the vehicle or the 2D plane like looking ahead from a vehicle?

345
00:21:29,200 --> 00:21:32,600
So for us, you can think of it as like an occupancy grid so you can like look down at the

346
00:21:32,600 --> 00:21:37,320
world, your vehicles in the center of it, you get sort of this distance around the vehicle

347
00:21:37,320 --> 00:21:38,320
there.

348
00:21:38,320 --> 00:21:39,320
Okay.

349
00:21:39,320 --> 00:21:40,320
Okay.

350
00:21:40,320 --> 00:21:45,280
So the question was around the objective function, like how did you, how did you construct

351
00:21:45,280 --> 00:21:48,840
the objective function for this model that you trained?

352
00:21:48,840 --> 00:21:49,840
Right.

353
00:21:49,840 --> 00:21:55,800
So for these initial models to start, what we were using was a, some tools called imitation

354
00:21:55,800 --> 00:21:56,800
learning.

355
00:21:56,800 --> 00:22:03,120
So we basically wanted to figure out how we can imitate a model or imitate some expert

356
00:22:03,120 --> 00:22:04,120
model.

357
00:22:04,120 --> 00:22:06,320
So we have some sort of expert behavior that we want to mimic.

358
00:22:06,320 --> 00:22:08,760
So this can be a human, for example.

359
00:22:08,760 --> 00:22:13,040
So if we have some examples of how the human is going to behave or some example trajectories

360
00:22:13,040 --> 00:22:18,880
of this person driving, we basically want to try it and be as similar to this driver as

361
00:22:18,880 --> 00:22:21,120
possible or similar to this human.

362
00:22:21,120 --> 00:22:26,840
So mimicking the expert in training and transferring this knowledge over to our novice or our

363
00:22:26,840 --> 00:22:28,360
deep learning algorithm.

364
00:22:28,360 --> 00:22:29,360
Okay.

365
00:22:29,360 --> 00:22:31,880
So you've got your driver behavior model.

366
00:22:31,880 --> 00:22:39,480
You've trained a deep learning model that can try to optimize an objective relative

367
00:22:39,480 --> 00:22:44,960
to, you know, some expert that it's imitating what, what's next.

368
00:22:44,960 --> 00:22:49,840
So then once we have a model that can effectively mimic these high level decisions, hopefully

369
00:22:49,840 --> 00:22:53,200
pretty well and hopefully in a safe way, that's when we started thinking about how we can

370
00:22:53,200 --> 00:22:55,520
actually implement this in a robust controller.

371
00:22:55,520 --> 00:23:01,600
So we've been using some pretty standard tools from control, like model predictive control

372
00:23:01,600 --> 00:23:07,480
and robust control so we can take these high level commands and turn them into trajectories

373
00:23:07,480 --> 00:23:10,800
that the vehicle then can follow quite smoothly.

374
00:23:10,800 --> 00:23:14,800
So basically using these commands or these high level commands from the deep learning,

375
00:23:14,800 --> 00:23:19,040
now you can sort of start thinking about different models and sort of some of the differences

376
00:23:19,040 --> 00:23:21,480
between the simulation and the real world car.

377
00:23:21,480 --> 00:23:26,760
So you can sort of extract the high level information and execute it in the real vehicle.

378
00:23:26,760 --> 00:23:28,160
So that's actually what we're testing now.

379
00:23:28,160 --> 00:23:32,320
So we're putting this on a real vehicle and testing all of that.

380
00:23:32,320 --> 00:23:33,320
Nice, nice.

381
00:23:33,320 --> 00:23:39,640
You mentioned a couple of disciplines in control systems, robust control and model, something

382
00:23:39,640 --> 00:23:40,640
control.

383
00:23:40,640 --> 00:23:41,640
Model predictive control.

384
00:23:41,640 --> 00:23:42,640
Model predictive control.

385
00:23:42,640 --> 00:23:47,120
Can you walk us through what those are and the assumptions that they're making and what

386
00:23:47,120 --> 00:23:48,440
they're trying to accomplish?

387
00:23:48,440 --> 00:23:49,440
Yeah, yeah.

388
00:23:49,440 --> 00:23:53,640
So the heart of all of what we do is model predictive control and then model predictive control.

389
00:23:53,640 --> 00:23:56,840
So basically using a model of your vehicle and the environment.

390
00:23:56,840 --> 00:24:01,480
It basically solves an optimization problem to give you an optimal trajectory over some

391
00:24:01,480 --> 00:24:03,360
finite time horizon.

392
00:24:03,360 --> 00:24:07,680
So say two seconds in the future, I'm going to plan the optimal trajectory to achieve

393
00:24:07,680 --> 00:24:08,680
my goal.

394
00:24:08,680 --> 00:24:13,520
And since this is just an optimization program, effectively you can easily put in things

395
00:24:13,520 --> 00:24:17,560
like safety constraints, you can tune your trajectory.

396
00:24:17,560 --> 00:24:23,000
So you have a smooth trajectory and basically by solving this problem, you can come up with

397
00:24:23,000 --> 00:24:24,680
your optimal trajectory.

398
00:24:24,680 --> 00:24:28,560
And the kind of cool thing about model predictive control is even though it's a finite time

399
00:24:28,560 --> 00:24:29,560
horizon.

400
00:24:29,560 --> 00:24:33,440
So it only works for about two seconds in the future, whatever your finite time is.

401
00:24:33,440 --> 00:24:35,040
And it's an open loop trajectory.

402
00:24:35,040 --> 00:24:38,440
You basically take one step in the future and then you re-solve the problem.

403
00:24:38,440 --> 00:24:42,960
So you re-solve this trajectory or for this trajectory at each time step.

404
00:24:42,960 --> 00:24:47,400
And so by doing this, receding horizon by sort of sliding along and constantly planning

405
00:24:47,400 --> 00:24:51,320
some time in the future, you actually approximate things like the infinite horizon.

406
00:24:51,320 --> 00:24:53,880
So basically the optimal control policy.

407
00:24:53,880 --> 00:24:57,680
So it's an open loop policy executed in a close up action.

408
00:24:57,680 --> 00:25:05,720
I was recently reading some reviews of some of the production driver assistance, like

409
00:25:05,720 --> 00:25:10,920
autonomous driver assistance, if you will, technologies like Cadillac has one, Mercedes

410
00:25:10,920 --> 00:25:11,920
has one.

411
00:25:11,920 --> 00:25:13,800
Obviously Tesla, I forget the other one.

412
00:25:13,800 --> 00:25:16,600
I think it was infinity that was in this article.

413
00:25:16,600 --> 00:25:23,160
And they talked about how one of the things that they noticed was that for most of these

414
00:25:23,160 --> 00:25:26,280
systems, I think Tesla was the only exception.

415
00:25:26,280 --> 00:25:30,440
Like the car would basically bounce back and forth between the lane markers.

416
00:25:30,440 --> 00:25:38,000
And it, and I'm speculating a little bit, but it sounds like what you're doing with model

417
00:25:38,000 --> 00:25:44,520
predictive control would tend to smooth out that kind of effect as opposed to, you know,

418
00:25:44,520 --> 00:25:48,760
maybe deriving a path straight out of a deep learning model.

419
00:25:48,760 --> 00:25:51,320
Like is that a reasonable kind of intuition about this?

420
00:25:51,320 --> 00:25:53,720
Or does it show up in other ways?

421
00:25:53,720 --> 00:25:55,080
No, that's exactly right.

422
00:25:55,080 --> 00:25:59,920
So by using this basically constant smoothing and optimization technique, you do come up with

423
00:25:59,920 --> 00:26:01,880
much smoother trajectories.

424
00:26:01,880 --> 00:26:06,480
It not only addresses things like the sort of jerky chattering, I guess that happens when

425
00:26:06,480 --> 00:26:11,560
you sort of oscillate between lanes, but it also helps for things like overshoot and

426
00:26:11,560 --> 00:26:14,800
things like that as well, or is jumping back and forth when you do things like turning,

427
00:26:14,800 --> 00:26:18,920
that usually comes from common planning and techniques like that.

428
00:26:18,920 --> 00:26:23,960
And now you're making me remember, you know, grad school control systems courses with

429
00:26:23,960 --> 00:26:30,440
like dampening and all these other things that you need to think about to avoid, you know,

430
00:26:30,440 --> 00:26:31,640
overshooting and oscillation.

431
00:26:31,640 --> 00:26:32,640
Yep, yep.

432
00:26:32,640 --> 00:26:33,640
Yeah.

433
00:26:33,640 --> 00:26:37,640
It's amazing how now that I'm implementing things on a real vehicle, how all of these

434
00:26:37,640 --> 00:26:41,880
original controls that are really coming back, I think I haven't thought about like 10

435
00:26:41,880 --> 00:26:42,880
years.

436
00:26:42,880 --> 00:26:43,880
Nice.

437
00:26:43,880 --> 00:26:44,880
Nice.

438
00:26:44,880 --> 00:26:45,880
Very important.

439
00:26:45,880 --> 00:26:50,560
What are you seeing as you're trying to, you know, go from these models to implementing

440
00:26:50,560 --> 00:26:52,320
them on a real vehicle?

441
00:26:52,320 --> 00:26:57,520
Are you finding that, are you surprised by anything or are there things that, you know,

442
00:26:57,520 --> 00:27:01,880
were working fine in simulation, but, you know, you needed to be tweaked as you moved

443
00:27:01,880 --> 00:27:03,320
to a real vehicle?

444
00:27:03,320 --> 00:27:04,320
Yes.

445
00:27:04,320 --> 00:27:10,920
I think one of my favorite quotes was, or is, everything is doomed to work in simulation.

446
00:27:10,920 --> 00:27:11,920
So.

447
00:27:11,920 --> 00:27:12,920
Nice.

448
00:27:12,920 --> 00:27:17,640
We have all these nice simulation tools, but, and we think we have things working really

449
00:27:17,640 --> 00:27:22,720
well, but a lot of it is still has to be hand-tuned and it's amazing how some of these simple

450
00:27:22,720 --> 00:27:26,800
things are, especially when you kind of get caught up in a lot of the really cool stuff

451
00:27:26,800 --> 00:27:30,600
that's happening in AI, you think a lot of the really cool stuff comes out of the deep

452
00:27:30,600 --> 00:27:34,680
learning, but when you go to actually test things on the vehicle, so much of it comes

453
00:27:34,680 --> 00:27:37,120
down to this low-level control, actually.

454
00:27:37,120 --> 00:27:42,400
So it is kind of amazing how much time is actually spent on the, I guess, more traditional

455
00:27:42,400 --> 00:27:43,400
things.

456
00:27:43,400 --> 00:27:46,320
So as you said, like, the dampening and making sure everything is smooth and tuning

457
00:27:46,320 --> 00:27:47,880
cost functions and things like that.

458
00:27:47,880 --> 00:27:53,760
So I think there's still a lot of work that has to go into the end to end stuff to make

459
00:27:53,760 --> 00:27:55,840
sure it works well.

460
00:27:55,840 --> 00:28:06,320
And is there kind of research into applying some of the ways we optimize machine learning

461
00:28:06,320 --> 00:28:13,200
to optimizing, you know, these control systems, like, you know, can you think of, can you

462
00:28:13,200 --> 00:28:18,640
apply hyperparameter optimization to these control systems to find the right dampening

463
00:28:18,640 --> 00:28:24,400
constants and all that kind of stuff, or is it, are the, you know, traditional techniques

464
00:28:24,400 --> 00:28:25,760
kind of good enough there?

465
00:28:25,760 --> 00:28:30,120
Yeah, so I think you can do some of those things, but again, you still have this problem

466
00:28:30,120 --> 00:28:36,080
where you ultimately have to put all this on a vehicle, you have to test it there,

467
00:28:36,080 --> 00:28:41,040
and because you're testing on a real vehicle that has had a lot of very expensive equipment

468
00:28:41,040 --> 00:28:42,040
on it.

469
00:28:42,040 --> 00:28:46,840
You have to be kind of careful with what you are willing to test and make sure you're

470
00:28:46,840 --> 00:28:47,840
very confident in this.

471
00:28:47,840 --> 00:28:52,640
So you can't do a full, you know, cross validation set on there unless you're confident

472
00:28:52,640 --> 00:28:55,120
that it's going to work really well.

473
00:28:55,120 --> 00:28:59,520
But yeah, there's some work that we've been working on in robust reinforcement learning.

474
00:28:59,520 --> 00:29:05,200
So trying to do things like take into account some of the uncertainty that we think we might

475
00:29:05,200 --> 00:29:09,720
see or some of the things like model mismatch and trying to create basically make sure

476
00:29:09,720 --> 00:29:15,000
that our control systems and our deep learning algorithms are robust to these uncertainties.

477
00:29:15,000 --> 00:29:21,040
So I think that should help it, but we'll see if that actually works or not when we actually

478
00:29:21,040 --> 00:29:23,440
go to the real vehicle without work.

479
00:29:23,440 --> 00:29:25,240
Can you elaborate on that work a little bit more?

480
00:29:25,240 --> 00:29:26,240
Yeah, sure.

481
00:29:26,240 --> 00:29:31,040
So we're building up with some tools in from what's called robust adversarial reinforcement

482
00:29:31,040 --> 00:29:32,040
learning.

483
00:29:32,040 --> 00:29:36,720
So in this framework, the basic idea is if you have some uncertainties in your model

484
00:29:36,720 --> 00:29:41,120
or uncertainties in your environment, you can actually treat this as a game between

485
00:29:41,120 --> 00:29:42,120
two players.

486
00:29:42,120 --> 00:29:47,200
So if you have your controller, which is we'll call it the protagonist and your antagonist,

487
00:29:47,200 --> 00:29:51,000
which is basically these disturbances or these uncertainties that are trying to sort

488
00:29:51,000 --> 00:29:53,600
of perturb your model in a negative way.

489
00:29:53,600 --> 00:29:57,560
So if you basically treat these both as agents that you want to train a model for, you can

490
00:29:57,560 --> 00:30:03,240
train your protagonist to achieve some goal and then you can train your antagonist to basically

491
00:30:03,240 --> 00:30:06,160
get you to not achieve that goal.

492
00:30:06,160 --> 00:30:11,480
So by training these two, basically, your antagonist and your protagonist iteratively,

493
00:30:11,480 --> 00:30:14,560
you develop something that approximates a robust controller.

494
00:30:14,560 --> 00:30:20,880
So you have your ultimate system is able to handle things like uncertainty in your model

495
00:30:20,880 --> 00:30:26,360
because you've been training it against a adversarial uncertainty or disturbance.

496
00:30:26,360 --> 00:30:30,200
So that's one way we're trying to deal with these model mismatches.

497
00:30:30,200 --> 00:30:32,000
We'll see how it works.

498
00:30:32,000 --> 00:30:33,960
I'll get back to you on that.

499
00:30:33,960 --> 00:30:34,960
Yeah.

500
00:30:34,960 --> 00:30:35,960
Yeah.

501
00:30:35,960 --> 00:30:46,120
Are you finding that there's a kind of a cultural mismatch between the folks that come

502
00:30:46,120 --> 00:30:54,560
from the traditional control systems perspective where you've got established practices

503
00:30:54,560 --> 00:31:05,600
and well-defined guarantees or at least laws of physics to kind of rely on versus the folks

504
00:31:05,600 --> 00:31:08,680
that are more of the deep learning camp where you're kind of just throwing a bunch of

505
00:31:08,680 --> 00:31:12,600
data against the wall and this thing is miraculously training itself.

506
00:31:12,600 --> 00:31:13,600
Yes.

507
00:31:13,600 --> 00:31:14,600
Yeah.

508
00:31:14,600 --> 00:31:17,280
There's a huge cultural gap there.

509
00:31:17,280 --> 00:31:20,560
So one of the things I try and do is try and sort of bridge that gap.

510
00:31:20,560 --> 00:31:25,800
But I think there's just a, it's almost like they're speaking different languages.

511
00:31:25,800 --> 00:31:29,680
These two communities tend to get a little bit territorial in what they can and can't

512
00:31:29,680 --> 00:31:30,680
do.

513
00:31:30,680 --> 00:31:31,680
So yes.

514
00:31:31,680 --> 00:31:32,680
In short.

515
00:31:32,680 --> 00:31:33,680
Awesome.

516
00:31:33,680 --> 00:31:44,480
Well, to close us out, what's next for you beyond getting your vehicle working, not

517
00:31:44,480 --> 00:31:49,120
that that's like a short task or a foregone conclusion, what are some of the other things

518
00:31:49,120 --> 00:31:51,400
on the horizon for you and your work?

519
00:31:51,400 --> 00:31:52,400
Yeah.

520
00:31:52,400 --> 00:31:55,760
So some of the other things I've been thinking about is thinking about autonomous vehicles

521
00:31:55,760 --> 00:31:58,720
at a little bit of a higher level or a broader perspective.

522
00:31:58,720 --> 00:32:03,360
So some of the recent work that I'm just starting has been on how can you use autonomous vehicles

523
00:32:03,360 --> 00:32:08,360
and some of these tools from planning to do things like assistant evacuation and disaster

524
00:32:08,360 --> 00:32:09,360
responses.

525
00:32:09,360 --> 00:32:12,160
So using some of these same tools to help there.

526
00:32:12,160 --> 00:32:17,480
I've also been thinking about how you can start applying some of these tools to fleets.

527
00:32:17,480 --> 00:32:22,960
So you can think about optimizing overall performance by making small changes on a minor level,

528
00:32:22,960 --> 00:32:28,200
have to do things when you have many, many vehicles all operating together.

529
00:32:28,200 --> 00:32:29,200
So.

530
00:32:29,200 --> 00:32:33,040
And so are these things like cooperative autonomy and swarming behaviors and the like?

531
00:32:33,040 --> 00:32:34,040
Yeah.

532
00:32:34,040 --> 00:32:35,040
Yeah.

533
00:32:35,040 --> 00:32:36,040
A little bit of that.

534
00:32:36,040 --> 00:32:39,200
And so I think the evacuation planning is a little bit more of the cooperative side for

535
00:32:39,200 --> 00:32:42,360
the fleet performance if you still have individual operators.

536
00:32:42,360 --> 00:32:45,560
So if you think of like a delivery fleet, you still have an individual there driving the

537
00:32:45,560 --> 00:32:46,880
vehicle and making decisions.

538
00:32:46,880 --> 00:32:52,760
So how can you optimize maybe their behaviors on a minor level and then have greater performance

539
00:32:52,760 --> 00:32:53,760
overall?

540
00:32:53,760 --> 00:32:54,760
Interesting.

541
00:32:54,760 --> 00:32:55,760
Interesting.

542
00:32:55,760 --> 00:33:00,360
So what's the best way for folks to learn more about your research or connect with you?

543
00:33:00,360 --> 00:33:01,360
Yeah.

544
00:33:01,360 --> 00:33:04,360
You can find me on the internet.

545
00:33:04,360 --> 00:33:06,120
So you can look at my website.

546
00:33:06,120 --> 00:33:13,200
So just stampord.edu, backstop, tilde, krdc, or you can shoot me an email, krdc at stampord.edu.

547
00:33:13,200 --> 00:33:14,200
Awesome.

548
00:33:14,200 --> 00:33:15,200
Awesome.

549
00:33:15,200 --> 00:33:21,640
Thank you so much for joining me to discuss this is really interesting research and I'm looking

550
00:33:21,640 --> 00:33:24,040
forward to keeping tabs on what you're up to.

551
00:33:24,040 --> 00:33:25,040
Thanks.

552
00:33:25,040 --> 00:33:26,040
Yeah.

553
00:33:26,040 --> 00:33:27,040
It was a great conversation.

554
00:33:27,040 --> 00:33:28,040
Thank you for having me.

555
00:33:28,040 --> 00:33:29,040
Thank you.

556
00:33:29,040 --> 00:33:32,840
All right, everyone.

557
00:33:32,840 --> 00:33:35,160
That's our show for today.

558
00:33:35,160 --> 00:33:40,320
Thanks so much for listening and for your continued feedback and support.

559
00:33:40,320 --> 00:33:45,080
For more information on Katie or any of the topics covered in this episode, head on

560
00:33:45,080 --> 00:33:49,440
over to twomolei.com slash talk slash 59.

561
00:33:49,440 --> 00:33:57,280
To follow along with the autonomous vehicle series, visit twomolei.com slash AV 2017.

562
00:33:57,280 --> 00:34:03,000
Of course, you can send along your feedback or questions via Twitter to at twomolei or

563
00:34:03,000 --> 00:34:07,960
at Sam Charrington or just leave a comment on the show notes page.

564
00:34:07,960 --> 00:34:11,560
Thanks again to my DAI for their sponsorship of this series.

565
00:34:11,560 --> 00:34:18,040
Be sure to check out my interview with their co-founder and CEO Darren Nikuda at twomolei.com slash

566
00:34:18,040 --> 00:34:26,520
talk slash 57 and take a look at what the company's up to at www.mty.ai.

567
00:34:26,520 --> 00:34:44,720
Thanks again for listening and catch you next time.

