All right, everyone. I am here with Jose Miguel Hernandez-Lobato, who is a university lecturer
in machine learning at the University of Cambridge. Miguel, welcome to the Tumbo AI podcast.
Thanks. Thanks a lot for inviting me. Hey, I'm looking forward to digging in, learning a bit
about what you're working on, but I'd love to have you start by sharing a bit about your background
with our audience. How did you come to work in machine learning? So good. That's a good question.
So I was, as an undergrad, always interested in artificial intelligence, and obviously the
exposure that I had to artificial intelligence was more the traditional AI. For example,
as an undergrad, I started coding my own test programs, and so on. And then when I finished my
undergrad, I was deciding what to do, and there was this opportunity of doing a PhD, and at the time,
the most similar thing to a PhD in AI was focused on this topic of machine learning, and I just
started working in machine learning, and actually I think I was quite lucky, because I started
at a time where machine learning was not so popular and so hot, and now recently it has
exploded in interest and opportunities. So I think I was quite lucky to get just
when it was not so popular, and I'm in an actually very good position to take advantage of that.
That's awesome. Tell us a little bit about your research interests.
Good, so yeah, so I'm faculty in Cambridge at the moment, and Cambridge is well known for having
a strong focus on Bayesian methods. My interest is at the intersection of Bayesian methods and
deep learning, and I think Bayesian methods, they bring a lot of opportunities to quantify uncertainty,
and this will help us a lot to solve many problems, where uncertainty is very important,
especially in decision-making problems. For example, in active learning, if we want to know
what data to collect next, so that we learn fast about a particular problem, how to, for example,
find the new molecules with improved properties fast, we can also make use of this uncertainty,
and even if we want to, for example, compress the parameters of neural networks,
so that they have reduced the size, for example, if we want to transfer a neural network
to a smartphone or download it, for example, or we want to implement the neural network in
hardware, and we want to store the neural network in a low memory device, you can use these
techniques, and in general, I'm very excited about the challenges of obtaining uncertainty
as to much with deep learning. It's not a straightforward, but there are many opportunities for
having a big impact by doing this. If you were to taxonomize the different approaches,
the folks are taking kind of at this intersection of Bayesian and deep learning,
where is the activity happening right now? Good, so what happens at the moment is that as I said,
it's not really easy to obtain the accurate estimates of uncertainty, and in practice,
the problem is interactable and you have to do approximations. So there are many different ways
of doing these approximations. You could use, for example, sampling based methods that
draw approximate samples of posterior distribution. You could also think of the
instead of drawing samples to come up with a deterministic approximation that
quantifies your uncertainty. This will be, for example, Gaussian distribution. We will try to
obtain a Gaussian distribution that quantifies your uncertainty about the weights of your new
network. This is a different approach, and another alternative is to have something even kind
of in between of the two, something more flexible than a Gaussian, but not as complicated as drawing
samples. And this could be, for example, implicit models that work with very flexible distributions.
So people work a bit on these different techniques to obtain uncertainty estimates.
And at present, there is not really like a very well, like one method that is really known to be
better than the others, and this sort of activity of trying to come up with methods that have
good trade-offs in terms of the computational cost that it takes to obtain these uncertainty
estimates and the quality of the uncertainty estimates. And one of the areas that you've
been applying this kind of work is around in some recent papers around molecular design.
Yeah, that's right. So that's an idea that I'm really excited about. Finding new molecules
with improved properties is really challenging. So usually you have to propose a new molecule
and maybe you have to collect data to see how good that molecule is. And based on that data,
you may update your models and propose another molecule and the whole process repeats.
And the idea is how to achieve this in a way that you find faster some molecule with good properties.
So in this, yeah. Is there a particular aspect of this problem that lends itself to
a Bayesian approach, or where these uncertainty estimates are particularly useful,
or are those kind of separate areas of research for you? So there are some
the estimates in this case are very useful because you will use your uncertainty to guide
the search for better molecules. So typically, you will want to
collect data on those molecules for which you have a chance of getting good results.
And this means that there should be some uncertainty or some probability that the value
or the properties of the molecule are actually maybe better than what you found before.
So you will use this uncertainty and actually the quality of your predictions for the properties
of the molecules to decide what molecules to find next. And the molecules that we choose are
those that have some trade-off between the, if you think about it, the point estimate of how good
the molecule is, but also some high uncertainty. And when you are able to balance these two things,
like molecules where you have uncertainty and there is the probability that you will obtain
actually something better than what you have found before, then you will be
interested in collecting data for those molecules. Got it. And now you've taken a couple of different
approaches to this problem. One paper was focused on kind of searching over the possible chemical
reactions. And then more recently, you're doing this via 3D and in 3D space. And this was a
paper that was featured or that you presented at ICLR recently. That's right. So yeah. So the whole
thing of finding the molecules is actually quite challenging because you, molecules have discrete
objects and they have a lot of structure. And it's not really a straightforward how to come up
with something that looks like a realistic molecule like the ones that you find in the real world.
So the approach that we follow to do this is to use deep generative models. And these are going
to be deep unit networks that are trained on large amounts of existing molecules. For example,
and they will be able to generate new molecules. And the question is how do you want to represent
the molecules and how do you want to train these models to generate new molecules? So obviously,
you may want to synthesize those molecules in practice. So what you could do is to have a deep
generative model that is trained on existing data about chemical reactions, how how
existing molecules are synthesized using chemical reactions. And then you will be able to generate
molecules via chemical reactions. So the model will tell you how to synthesize the generated
molecules. The other approach that is also actually quite important is based on
the actual location of the atoms. So most generative models of molecules they work using molecular graphs.
And we just generate a graph with the atoms that form the molecule and the connections
between those. But there is a lot of important structure in the 3D configuration of the atoms
that is missed by these models. And another approach is to actually generate molecules in 3D
space instead of generating these molecular graphs. That is what the approach based on chemical
reactions does. You can generate the molecules in 3D space placing one atom and then connecting
that atom with previous ones and so on. And the advantage of this is that you have actually more
information about the structure of the molecule and this can be very important for determining
the properties of the molecule. As an example you have that the molecule of water actually
is not the way the hydrogen atoms are connected to the oxygen atom is not the
for example flat. You have like a small angle between the bonds and this gives the the
molecule of water specific properties. And if you just work with graphs without taking into account
for example this angle between the bonds then you won't be able to capture those properties.
So that's why I think capturing these properties and being able to generate molecules in 3D
spaces is quite interesting. So I just want to say that just to motivate this at the moment
there is a huge interest in the application of deep learning methods to this problem because
there are many many opportunities to in particular accelerate the design of new
tracks for example this is something that is very very expensive companies are spending a huge
amounts of money into designing new tracks and deep learning now is offering a lot of possibilities
to accelerate this process. I can see how having worked with this data in kind of the two-dimensional
kind of the flat graph realm would leave you wanting to try a 3D approach. Did your
specific approach or architecture for approaching the 3D problem? Did you start over? Was that kind
of built on what you did for the 2D problem? Yeah so actually the work on 3D is one of the
first works in this area so we really were starting almost from scratch because the approach is at
the moment not as sophisticated as previous models but we are able to successfully generate
these molecules in 3D space. So some of the molecules that we are generating in 3D space they
are relatively simple much simpler than the ones generated with previous models but it's really
exciting. Just to clarify how we do this the idea is to have our reinforcement learning agent
that is going to learn how to place these atoms in a space. The agent is given an initial back
of atoms and the agent has to choose what atoms to place in a space and how to arrange those
and the only feedback that the agent gets is the final energy or the configuration the energy of
the configuration of the atoms. So the agent has to learn by trial and error how to find 3D
configurations of atoms that have the energy and they are physically possible in their work.
And so some of the traditional challenges with reinforcement learning are the sample
inefficiency and the challenges of creating the objective function. How did issues like that manifest
themselves in this paper? That's right so typically reinforcement learning methods are data hungry
and the way we address this is to use some fast numerical methods that approximate the energy
of the atoms in a space. So we are actually able to collect a relatively large amount of data
to find this. The problem with this is that obviously you may want to find molecules that are
stable and that have like this 3D configuration that is physically possible but you may also want
to find molecules with interesting properties. And the question is that then you have to combine
these two different objectives. One is the energy of the molecule that might be cheap
to obtain and then you may also have another property of the molecule that could be actually
quite expensive to obtain. You may have to do experiments in a laboratory to finally test the
properties of that molecule in the real world that are the ones that you would expect.
So in practice you would have to find a balance between how to combine the two approaches
and the one way to do this is to use for example sub-agat models. The idea is how to solve
sub-agat models. These models will, for example, make predictions about the expensive properties
based on some existing data. And that reinforcement learning agent will try to
will be working with the predictions of these sub-agat models that will be that will be
very fast to a lot of weight. And to what degree have you integrated in kind of the Bayesian
approach into this reinforcement learning oriented formulation of the problem?
In the reinforcement learning approach we haven't really done that yet but we have done that
in the case of the, for example, the models with chemical reactions. And the idea here is to do
something called the Bayesian optimization methods. Bayesian optimization methods are actually
in the world, correct? Yeah, that's in the in the 2D graph setting. The idea is that
Bayesian optimization methods work also by using these sub-agat models and the idea is that you will
you will fit, for example, a predictor of the properties of the molecule that gives you estimates
of uncertainty. And this could be, for example, a Gaussian process. These are very good models
that work quite well to provide the estimates of uncertainty. And then you can use these
uncertainties to decide what data to collect next. What is super interesting is that we are using
deep generative models to generate the molecules, but to connect these models with a Bayesian optimization
approach, what we do is that we work with the latent representation of molecules given by the
deep generative models. These deep generative models, like variation algorithm colors, they learn
a compressed representation of the data. They have some latent variables. And by using these
models, we are actually able to map molecules to a low dimensional latent space that is
actually continuous. And then we can map points from this latent space back to the original space
of molecules. So what we do is then use Bayesian optimization techniques to do molecule optimization
in this latent space. So this is like a super interesting idea. And the advantage of this is that
you have now an approach that is going to do efficient optimization because you are using this
Bayesian optimization methods that work very well in continuous spaces and relatively low dimensional.
And we are combining these methods with a deep generative model that will generate molecules
that are similar to those found in the real world. And this is going to be done in a way without
having any expertise on how the molecules are constructed. You just have a data set of molecules.
You fit your deep generative model to this data. You obtain this latent space that represents
the molecules in a continuous representation. And then you can do molecule optimization in that
space. So this is some areas where I'm currently working quite a lot with members of my group.
Got it. And does the surrogate model extension to the 3D setting allow you to do some more things?
Yeah. In principle, you could do similar things. This is something that we haven't really
explored yet. But you could have your reinforcement learning agent optimizing some combined
objective between the energy of the molecule that you are constructing and the prediction of
the property of the molecule according to the surrogate model. This is obviously something
that we haven't done yet. But it's a very exciting direction for future work.
Awesome. Awesome. So you also do some work in compression. Tell us about that.
That's right. So this is something that I'm really excited about. And this is some work that
has been done by some of my PhD students in particular, Martin Harasi and Greg Flamick.
And this is a completely radical new way of doing compression. So most compression methods,
they actually work by mapping, for example, a sequence of characters to another representation
of that sequence that has a size that is proportional to the log probability of the original
sequence. This is what typically you do in compression. And it's well known. Not that the
information that a particular value of a random variable has is determined by the log probability
of this of this random variable. So typical methods for compression, they just compress
a specific value of a random variable. They find a new representation that is more efficient.
And there are many methods for doing this like arithmetic coding is an example.
Well, this is really cool of the methods that we are working with, which we call them
relative entropy coding methods. The idea is that we are not really compressing one value of a
random variable. We are compressing random samples. So we just have distributions. We could have
like a base distribution that is shared by the center and the receiver. And what we want to
compress is a random sample from a target distribution. And we don't really care exactly
about the particular sample that we get. We just want to be able to transmit a random sample
from this target distribution to the receiver without actually telling the receiver what the
distribution we are sampling from. The receiver only has this base distribution that is shared
between the center and the receiver. And yeah, we call this method the relative entropy coding.
And what is interesting is that this technique is able to find the compressed representations
of the weights of neural networks that are currently state of the, these methods are really
state of the ad for compressing neural networks. And so how does compressing the distribution
lend itself to compress communication from the center to the receiver?
Yeah, so what you actually do is you don't really send the distribution. So you just send some
bits that will allow the receiver to reconstruct a random sample. And this random sample in general,
the center doesn't really control what sample you send. You just are able to choose randomly one
and then send some bits to the receiver. And the receiver will take those bits, use the shared
distribution, the base distribution that is shared between the center and the receiver,
and use that the base distribution to recover the sample. So in general, you don't really send
the distribution itself, but you are able to send samples from the distribution.
And that's because the distribution, the compressed distribution is, you consider as pre-shared,
it's known to both center and receiver.
Yes, that's right. So that's some information that is
agreed between center and receiver. And typically this could be a very simple distribution that
is easy to sample from, for example, a standard Gaussian distribution. And what is interesting
of these method is that you are really just sending random values of random variables without
caring much about the value of those. And this is extremely useful for compressing the weights
of neural networks. Because neural networks are very, very tolerant to perturbation of the weights.
So it might be fine if you just send some randomly corrected version of your weights,
your neural network might still be able to make accurate predictions using that corrected
version of the weights. So by doing that, by sending some small perturbation of the weights,
and not exactly caring about sending a specific value of the weights, we are able to achieve
the best existing compression rates for neural networks. Got it. But you might not want to apply
this to traditional communication. That's right. So the question is, what type of communication
you can apply these to? Yeah. You think some of that you can actually apply these to many other
types of data, not only the weights of your networks. For example, images, you may say,
I want to send some image. You may also tolerate some error in your image. And it turns out
that it might be fine if you send just some version of your image that is slightly corrected.
This is usually called the lossy compression. So you won't be able to send the original image,
but the reconstruction quality of the image could be still quite good. And obviously,
all this depends on the tradeoff that you have between how much you want to compress the image,
and you would have a potential loss, and maybe you don't want to compress the image so much,
and the loss is not so high. And then you've also been applying deep generative models to robust
prediction. Tell us about that work. That's right. So this is something that we have been really
working on very recently, and there is a lot of interest in this area. So it turns out, I mean,
this is widely known. Deep learning methods, they are not very robust, especially to
experience features, for example, that could be looking as useful to make predictions in your
data, but they're actually not like this. And the typical example of this is this problem where you
have to classify images of camels and images of cows. And you could imagine that the cows appear
typically with a background of a green field, and the camels could appear with a background of a
desert. And if you try to see that deep learning methods to classify these images, the deep learning
method is very likely to fix on the background pixels, which are not really representative of what
you want to learn. It's not really differentiating between the shape of the cow and the camel. And when
you try to make predictions, then for example, for a cow that is standing in a beach and not in a
green field, then it's going to make a horrible mistakes. So the idea is then how can you make
deep learning methods more robust so that they don't fix on these patterns in the data that are
just spurious and they are not really representative of the prediction problem that you want to solve.
So there has been a lot of interesting work in this area, especially with some methods
called the invariance risk minimization. And they actually come up with solutions to this problem,
but they are based on using linear models. And obviously linear models, they are not really going to
be very accurate in many different settings where actually the patterns in the data are not linear.
So what do we propose with invariant risk minimization?
Yeah, that's right. So this is the work around, yeah, that's what I didn't know about this work.
It's very widely known and it's having, it's had like a lot of impact.
Well, I was going to ask that you explain that as a broad concept, independent of linear versus
non-linear. Yeah, so the idea is that you want to find some, the idea here to solve this problem
is to find some predictor that is going to be invariant across different representations of the
data or environments so that, for example, you could imagine that you have these images of
cameras and cows and the type of background maybe changes slightly in across two different
versions of your dataset. And then what happens is that this correlations between the background
pixels and the target label is going to change from one version of the data and the other. Maybe in
one version of the data, the fraction of the cows with green fields is slightly higher than
in another version of the data. So if you have a predictor that focuses on these pixels,
then it's going to be changing across this environment. So in some environments,
the predictor could be more reliable and in other environment it might be less reliable.
And this allows you to identify that the patterns that the predictor is capturing are probably
not realistic and curious. So there is to find some predictor that will be invariant
and it's going to work well across different environments. And for example, this predictor that
focuses on the shape of the cow or the camel will be invariant. And the idea is to have a predictor
that, for example, could work in the non-linear case. So I can say briefly how we solve this problem
and the idea is that we use deep generative models also to solve this problem and we use something
really cool and that is really exciting at the moment. And it's a family of deep generative models
operational to encoders that are identifiable. This is well known that the existing variational
to encoder models, if you flip them to the data, the latent variables that you will be obtaining
could be different if you train different times your methods. So there are many different
transformations of your latent variables and the model could just run to achieve those transformations.
So in general, every time you learn your model, the latent variables that you could obtain and
calling the data could be different. And this makes these methods not very reliable if you want
to use the latent variables for predictions. What has been very recently developed is a family
of deep generative models for variational to encoder models that are identifiable. And this
means that every time you train these models on the data using, for example, different initializations
of your neural networks, you will always obtain the same latent variables. And actually,
this can be useful for solving this invariant risk immunization problem. The idea is that you
will find latent variables that represent the data and then you can use causal identification
methods to choose those latent variables that are actually predictive of the target property.
So in the case of the images and the of camels and cows, you could think of fitting an
identifiable variational to encoder model to this data. And then finding some latent variables
that some will be describing the grass and other variables will be describing the shape of the
cows and the camels. And using the causal identification methods, you can choose those latent
variables that are actually connected with the shape of the cows and the camels with the final
label. And the other latent variables like the green fields and that determine the green field
or the desert, they won't be captured and they won't be identified as cows are related to the
target label. Can you give us an overview of how the causal identification part of that works?
That's right. So what happens is that you have an underlying cows and models. So you have
these latent variables that you have learned with the identifiable variational to encoder model,
and you have now data for the values of the latent variables and data for the target property.
And obviously you also have data for the different environments that I mentioned before. All
these works only if you have these different environments where things change. So once you
have observations from these latent variables, you can try to identify what's the actual
causal direction in the generation of the data. For example, the latent variable points
towards the label. And that means that the label is actually generated by the latent variable
or actually the label for the image points towards the latent variable. And that means that
the actually the latent variable is caused by the label. So the causal identification methods,
they will apply either independence tests to identify what is this right direction for the
other roles that connect the different variables. For they are based on other underlying assumptions.
Like, for example, something that is used in practice is to assume that the label is obtained
as a nonlinear transformation of the latent variable plus some additiveness.
And if you have a model that works in that way, you have that the label is actually generated
as a nonlinear transformation of the latent variable plus some noise. You can actually
identify the right direction by just fitting the nonlinear model in both directions. You can try to
use a nonlinear model to predict the label from the latent variable or a nonlinear model to
predict the latent variable from the label. And you can then look at the statistical patterns
in the noise. And the right direction will actually have a specific properties that allows you
to identify that. This is some work on Cups and inference that has been really exciting.
This was done in the Bernhard circles that and this work actually that we are, I mean, we're
working on this and we plan to submit it to new ribs. This is done also in collaboration with
Bernhard circles and one of my PhD students. Got it, got it. And so how in the case of this last
work that we've been discussing, how do you evaluate the results and how well is it working?
Good. So yeah, so right now evaluating the performance of these methods is challenging.
And right now there is a benchmark problem that people are considering for the evaluation of
these methods and this is called the colored M-nist. It's a, I mean, M-nist, everyone is familiar with
M-nist, everyone that works in the planning. So you have the M-nist, this data set with digits,
and you will have that some digits are half different colors. So you color the digits with
the colors that are red or green and you will choose these colors in a way that they are
spiritually correlated with the target label. So you would have some of these fake
correlations between the colors and the target label. And obviously, for example, you could say,
I mean, you typically classify the digits in two categories, I think, from 0 to 4 is
category one and from 5 to 9 is category two. So you will say the color of the digit, most of the
times, agrees with the actual category. So you have these colors that are correlated with the
label. And what happens is that you have two different versions of your data where this
probability of agreement of the color with the label changes slightly. And this is related to
this thing that I mentioned before that you need some variation in the spurious correlations
to be able to identify them as spurious. So you can then train a deep learning methods on this
data. And what happens is that the test data, actually, the color has no association at all
with the label. So actually if you train a deep learning method on this data and then you evaluate
the predictive performance on the test data, a normal deep learning method is going to perform
extremely poorly because the colors are actually the opposite. The combination of the colors
is the opposite at test time. So you could use this benchmark to see how well you are doing.
And actually, we have extremely good results in this benchmark because we are able to both
find nonlinear representations of the data and also nonlinear predictions. We are able to do
nonlinear predictions with the methods that we have developed. We achieve some of the best
existing results in this particular benchmark. Awesome. And talk a little bit about how your
broader work around applying Bayesian methods and uncertainty estimation applies in this particular
problem. And that's right. So in this case, the idea is the variation out on color model.
So we have this latent variable model that explains how the data is generated. And obviously,
you have latent variables. So these are variables that are not really observed. So the deep
genetic model uses these latent variables to generate the data but do not observe those.
So you really need to infer those latent variables from the data. And you need to use Bayesian
methods for this. For example, you have to do something called the typical evaluation
and inference where you feed a simple approximate distribution to the posterior distribution
over the latent variables. So you could use a variation and inference to solve this problem.
However, it's much better if you use other techniques. And this is what I mentioned before
about the different trade-offs that you have in Bayesian methods. That you could have simple
methods that are maybe computationally cheap and they work relatively well. But you could have
more advanced methods that are maybe more expensive. But they can be much more accurate.
And an example in this case could be sampling based methods. You could think of instead of
using variational inference, which is what most people use when they train
every variation out on colors, you could use some sampling based method to do something
more efficient and more accurate in this case. Awesome. You've got a few other papers at
this latest iClear conference. Yeah. Why don't we quickly kind of talk through those? One of them is
Yeah, so I can talk a bit about some of those papers as well. There is also another paper,
which I'm also really excited about. And it got an oral presentation that I clear. And it's
called the clue. And it's a method for interpretability in machine learning. Right now there is a lot of
interest in trying to open the black box of deep learning methods. No, you train these methods and
you don't really know why they make some predictions. So a lot of people have been focusing on
interpretability methods for normal deep learning techniques. And they will say, oh, my deep learning
method now says that my loan should be rejected, for example. And the joint interest is knowing why
that happens. However, as I mentioned before, uncertainty is very important in many decision
making problems and in many prediction tasks. And if you care about uncertainty, you may have
that your base and deep learning method just tells you, I don't really know what's the prediction,
what's the prediction in this particular setting. For this particular data point, maybe I don't
really know if you should be given your loan or not. So I'm very uncertain about what the right
predictions should be. And the question is then, why is your base and deep learning method
and certain? You may want to actually try to understand what's maybe making the data your deep learning
method and certain. So can the general explainability is focused on the prediction and in this
clue paper, you're focused on the uncertainty estimate? That's right. We are actually focusing on
interpreting uncertainty estimates and trying to come up with interpretability of why deep learning
methods might be uncertain. And what is interesting is that we also use deep generative models and
variation and color models for this. So the idea is that it's similar techniques and obviously
there are many applications of these methods. So what we do is we say, okay, we have this
base and deep learning method and it's saying that this particular data point is highly uncertain.
So what we do is we train a deep generative model, a variation and color model on the data.
And this will map the data into some low dimensional late in the space. And now what you can do
is try to find new points in this latent space that are close to the original data point.
But where the uncertainty of the neural network when you decode those latent points back and
you make predictions, the neural network becomes more confident. So by doing this, we are now
able to say, you have this data point for which your neural network is very uncertain.
Now I have this other data point that is very close to the original one, but the neural network
is much more confident and it's much more certain. And now you can look at the two data points,
the one for which the neural network is uncertain and the one for which the neural network now is
confident. And this will give you a lot of information telling you why the neural network is uncertain.
And you can look at the differences between the two data points and you will be able to understand
why this is the case. And we have tested this on obviously N-next. We have this N-next digit
and we get some of these digits. We have for example, a four looks like a nine, but you
wouldn't really be able to say this is a four, this is a nine. And then this method precisely tells you
the pixels in the image that are actually creating the confusion in the Bayesian deep learning method.
This highlighting, for example, those pixels that make the four look like a nine or not.
And I'm really excited about this because I think it's opening now a new area for research
into interpretability because people will now think, okay, now we can apply interpretability
methods to answer this and we can try to understand better why our methods don't know what they should know.
So you're, help me understand how the approach gives helps you interpret what's happening.
So you are finding close points in the latent space and we understand kind of some geometric
properties about the relationships between points and the latent space. But we don't necessarily
understand the latent space itself and kind of what the dimensions in that space mean.
So how does knowing, how do you get from knowing the two close points, share similar values?
How do you get from there to understanding the causes or being able to interpret?
Yeah, so that's that's I would question. So why does this whole thing works? No, and how
how just operating in this latent space makes makes any sense. So the idea is that the latent space,
the way you train these models is that the latent space has some structure and it captures some
similarity between the different data points. So because the latent space has low
dimension, you will have to find some compressive representation of data points in this latent
space. And whenever two data points are close to each other in the latent space,
this should still exhibit some similar patterns or regularities. Otherwise, you wouldn't be able to
compress the data. So we know that now data points in latent space that are close to each other should
be relatively similar. So our goal is to find these different versions of the original data point
for which the neural network is more confident, the answer is less. So what we can do is now
map these points from latent space into the predictions of our model by just decoding from the latent
space, feeling that as an input to the base and neural network and then getting the value of the
uncertainty. And then you could just do gradient based optimization in the latent space
to find some new points in the latent space that decodes into something that the
neural network is highly like much more confident. And obviously, you want to stay close to the
original point because otherwise what you could get could be very different and that would be
not very informative. No, if you just say, oh, this is a data point where you are very confident
and the other one where you are very uncertain and they are completely different, you don't really
find anything, any pattern that you could explain why things happen. So this technique is actually
called counterfactual, it's a counterfactual method for interpretability. And what typically
means is that you say, I have this data point and actually my predictions are very uncertain.
And by using this counterfactual approach, you say how my data point should have been so that my
network is very confident in your predictions. That's what we are trying to do our counterfactual
by saying, okay, this is the data point that I got and I imagine that things would have been
otherwise and I got another version of this data point for which my neural network is much more
confident. And then by just sticking to something that is close in late in the space,
but we are the uncertainty actually decreases quite quickly, then we are able to
obtain this informative data points. Got it, got it. Awesome.
Well Miguel, thanks so much for joining us and sharing a bit about what you're up to. Thanks
thanks a lot. Yeah, thanks a lot for inviting me. Awesome. Thank you.
