WEBVTT

00:00.000 --> 00:16.400
Hello and welcome to another episode of Twimo Talk, the podcast why interview interesting

00:16.400 --> 00:21.560
people, doing interesting things and machine learning and artificial intelligence.

00:21.560 --> 00:25.200
I'm your host Sam Charrington.

00:25.200 --> 00:29.960
I've mentioned here on the podcast a couple of times that I'm keenly interested in industrial

00:29.960 --> 00:33.040
applications of machine learning and AI.

00:33.040 --> 00:37.360
I've come a long way in my research in this area and I'm very close to publishing a special

00:37.360 --> 00:38.960
report on the topic.

00:38.960 --> 00:42.280
If you're interested in learning more about this work when it's completed, I've put up

00:42.280 --> 00:46.560
a form at twimolayi.com slash industrial AI.

00:46.560 --> 00:51.080
Fill out that form and I'll let you know when the report is available.

00:51.080 --> 00:56.560
In conjunction with this work, we've got something really special to debut here on the podcast.

00:56.560 --> 01:01.520
For the next several weeks, I'll be interviewing some very interesting guests working on topics

01:01.520 --> 01:04.440
connected to industrial AI.

01:04.440 --> 01:08.720
We'll be discussing different application areas like manufacturing, warehouse automation

01:08.720 --> 01:14.480
and logistics and practice areas like robotics, reinforcement learning and simulation.

01:14.480 --> 01:18.360
We've been planning this for quite some time and we're very excited for you to hear what

01:18.360 --> 01:19.920
we've cooked up.

01:19.920 --> 01:25.200
Of course, as always, we love your comments, feedback and suggestions, which you can leave

01:25.200 --> 01:31.280
on the series page at twimolayi.com slash industrial AI.

01:31.280 --> 01:37.000
Our first guest in the industrial AI series is Ilya Baranov, engineering manager at Clear

01:37.000 --> 01:38.920
Path Robotics.

01:38.920 --> 01:43.000
Ilya is responsible for setting the engineering direction for all of Clear Path's research

01:43.000 --> 01:44.400
platforms.

01:44.400 --> 01:49.160
He likes to describe his role at the company as both enabling and preventing the robot

01:49.160 --> 01:50.160
revolution.

01:50.160 --> 01:55.960
He's a longtime contributor to the open source robotics community and RAS and open source

01:55.960 --> 01:58.400
robotic operating system.

01:58.400 --> 02:02.960
In our conversation, we cover a lot of ground, including what it really means to field autonomous

02:02.960 --> 02:08.280
robots, the use of autonomous robots and research and industrial environments, the different

02:08.280 --> 02:12.160
approaches and challenges to achieving autonomy and much more.

02:12.160 --> 02:16.000
This was a really fun interview and I'm excited to share it with you.

02:16.000 --> 02:21.000
Before we get started, I'd like to give a huge thank you to the team over at Banzai.

02:21.000 --> 02:26.440
Banzai, who is supporting this podcast series, as well as my forthcoming report, has been

02:26.440 --> 02:29.680
a big supporter of my research in this area.

02:29.680 --> 02:34.440
Banzai offers an AI platform that empowers enterprises to build and deploy intelligent

02:34.440 --> 02:35.440
systems.

02:35.440 --> 02:39.480
I've been following them since their initial launch just over a year ago when I'm at

02:39.480 --> 02:44.160
the founders at a conference and I've been very impressed with both the team and technology.

02:44.160 --> 02:49.440
If you're trying to build AI-powered applications, focus on optimizing and controlling the systems

02:49.440 --> 02:53.080
in your enterprise, you should take a look at what they're up to.

02:53.080 --> 02:57.600
They've got a really unique approach to building AI models that lets you use high-level code

02:57.600 --> 03:02.880
to model the real world concepts in your application, automatically generate train and evaluate

03:02.880 --> 03:07.400
low-level models for your projects, using technologies like deep learning and reinforcement

03:07.400 --> 03:13.560
learning, and easily integrate the models into your applications and systems using APIs.

03:13.560 --> 03:19.200
You can check them out at bions.ai and definitely tell them that you appreciate their support

03:19.200 --> 03:21.160
of the podcast.

03:21.160 --> 03:29.480
And now on to the show.

03:29.480 --> 03:36.000
All right, everyone, I am on the line with Ilya Baranov, who is an engineering manager

03:36.000 --> 03:41.800
with Clear Path Robotics, and I'm excited to have Ilya on to talk about the intersection

03:41.800 --> 03:45.920
between robotics and machine learning and AI.

03:45.920 --> 03:46.920
How are you doing?

03:46.920 --> 03:48.320
Oh, I'm doing great today.

03:48.320 --> 03:49.320
Thank you.

03:49.320 --> 03:50.320
Awesome, awesome.

03:50.320 --> 03:52.080
Why don't we start with a little bit of your background?

03:52.080 --> 03:56.360
Can you tell us a little bit about how you got to Clear Path?

03:56.360 --> 03:57.360
For sure.

03:57.360 --> 04:00.920
So I've always really liked robotics ever since a young age.

04:00.920 --> 04:08.440
I probably started out with Lego, as I'm sure most of us started it this way.

04:08.440 --> 04:13.880
And I really wanted to go to school at University of Waterloo due to their co-op program.

04:13.880 --> 04:18.320
So when I joined them in the first year, I joined their robotics team.

04:18.320 --> 04:23.560
And at the time, we had a pretty broad robotics team where the first years would do sort

04:23.560 --> 04:26.960
of small competitions with sumo robots and line following.

04:26.960 --> 04:30.880
And then the upper years would usually work towards their capstone project or final

04:30.880 --> 04:32.480
year design project.

04:32.480 --> 04:33.480
Okay.

04:33.480 --> 04:37.400
And so I really liked this one group that was doing a capstone project that was doing

04:37.400 --> 04:39.720
autonomous mind-sweeping.

04:39.720 --> 04:42.840
And so I asked to help them out instead of doing the first year stuff.

04:42.840 --> 04:48.120
Because by that point, I had gotten beyond the line following stuff.

04:48.120 --> 04:54.480
And so I ended up helping them out with their GPS solution to position their robot.

04:54.480 --> 04:57.480
And that team turned into Clear Path, essentially.

04:57.480 --> 04:58.480
Oh, wow.

04:58.480 --> 04:59.480
Okay.

04:59.480 --> 05:00.480
Yeah.

05:00.480 --> 05:01.480
So it's quite interesting.

05:01.480 --> 05:04.600
There's a lot of learning to do, even especially in the first year.

05:04.600 --> 05:08.040
One of the funny things that ended up happening is a little bit of the code and firmware

05:08.040 --> 05:14.200
that I wrote and Ryan Garry wrote made our robot go in circles whenever it'd get to its

05:14.200 --> 05:18.000
actual weight point that it was designated to go to.

05:18.000 --> 05:21.600
We didn't really have any good tolerance on our GPS goal.

05:21.600 --> 05:22.600
So it'd get there.

05:22.600 --> 05:26.240
And then GPS would drift a little bit and so chase it and GPS would drift a little more

05:26.240 --> 05:27.920
and it would chase it back.

05:27.920 --> 05:28.920
And so.

05:28.920 --> 05:30.680
Oh, wow.

05:30.680 --> 05:34.040
So yeah, that's a kind of funny thing that happens when you're trying to do this stuff

05:34.040 --> 05:35.040
in university.

05:35.040 --> 05:36.040
Uh-huh.

05:36.040 --> 05:42.160
Well, I think it would help folks understand a little bit more of the context of what we're

05:42.160 --> 05:43.160
talking about.

05:43.160 --> 05:47.800
If we go into a little bit of Clear Path, what is the company focused on?

05:47.800 --> 05:48.800
Yeah, for sure.

05:48.800 --> 05:53.600
So out of those kind of routes, the four founders decided that they would try to have

05:53.600 --> 05:56.800
a go at it and create robotics for research.

05:56.800 --> 06:01.720
But they actually started on the idea of can we take this mind-sweeping robot idea and

06:01.720 --> 06:03.360
actually apply it to the real world?

06:03.360 --> 06:09.040
And they were fairly surprised to find out that large defense industries didn't want

06:09.040 --> 06:14.720
to buy from four guys in a garage.

06:14.720 --> 06:18.840
So they had to kind of switch their idea and so they went with what they knew.

06:18.840 --> 06:23.040
They knew that they understood the kind of university level research application of

06:23.040 --> 06:24.040
robotics.

06:24.040 --> 06:28.240
They had talked to a lot of professors and so they decided to create platforms.

06:28.240 --> 06:32.800
So Clear Path really got started creating robotics platforms for research.

06:32.800 --> 06:33.800
Okay.

06:33.800 --> 06:38.360
So the idea being that if you're a researcher and you're trying to do some work outdoors

06:38.360 --> 06:44.320
or indoors, any kind of development for robotics, for positioning, or movement, or interaction,

06:44.320 --> 06:48.960
instead of spending the term, you and your grad student creating this robot, you would

06:48.960 --> 06:53.000
purchase one from us already integrated with the sensors that you wanted to use.

06:53.000 --> 06:54.000
Okay.

06:54.000 --> 06:55.000
Got it.

06:55.000 --> 06:56.000
Yeah.

06:56.000 --> 06:58.800
So this kind of platformed approach to research robots and especially our kind of niche

06:58.800 --> 07:03.760
that we found was the outdoor rugged market because there had been a lot of indoor ones

07:03.760 --> 07:08.640
like the pioneer from, sorry, I'm forgetting their name right now, but yeah, there

07:08.640 --> 07:13.720
been a few kind of indoor robots before but nobody had really done an outdoor robot platform

07:13.720 --> 07:14.720
at the time.

07:14.720 --> 07:15.720
Okay.

07:15.720 --> 07:22.400
When I look on the Clear Path site, there are, it's not just one or a couple of different

07:22.400 --> 07:23.400
types of robots.

07:23.400 --> 07:29.120
There are a bunch of different platforms, both kind of these rugged, ruggedized outdoor

07:29.120 --> 07:32.760
ones as well as some indoor ones.

07:32.760 --> 07:44.520
There's UAVs, there are ones for the, you know, sea, it's called USV, so sea vehicle.

07:44.520 --> 07:50.080
And there's even a video of one towing a plane, is that a real application of one of these

07:50.080 --> 07:51.080
robots?

07:51.080 --> 07:54.960
Not quite, actually, it's funny you should mention that.

07:54.960 --> 07:59.920
That was actually a fun shoot we did for the Discovery Channel, where we tried to find

07:59.920 --> 08:03.400
the limits of the towing capability of our grizzly platform.

08:03.400 --> 08:04.400
Okay.

08:04.400 --> 08:09.320
And by the time we had towed a fully fueled jet, pretty much up a little incline, you can't

08:09.320 --> 08:14.080
really see it in the video, but the actual tarmac there sloped.

08:14.080 --> 08:17.680
We couldn't really figure out what was the next big thing we could tell, so we got to

08:17.680 --> 08:18.680
stop there.

08:18.680 --> 08:19.680
Nice.

08:19.680 --> 08:20.680
Nice.

08:20.680 --> 08:21.680
I mean, it looks awesome.

08:21.680 --> 08:22.680
It's very impressive.

08:22.680 --> 08:23.680
Yeah.

08:23.680 --> 08:24.680
Yeah.

08:24.680 --> 08:25.680
It's a lot of fun.

08:25.680 --> 08:26.680
Definitely.

08:26.680 --> 08:27.680
Yeah.

08:27.680 --> 08:32.000
So the grizzly and the more modern version, the Wardhog is meant for outdoor heavy work.

08:32.000 --> 08:36.240
So agriculture and mining is really two of the places where we've seen a lot of interest

08:36.240 --> 08:37.240
there.

08:37.240 --> 08:38.240
Okay.

08:38.240 --> 08:41.680
So one, you know, it's funny, you could find these trivial sounding applications for

08:41.680 --> 08:42.680
these things.

08:42.680 --> 08:43.680
I have a real need.

08:43.680 --> 08:47.120
For example, we worked with a vineyard where they had a problem where they'd have birds

08:47.120 --> 08:49.200
come and eat the grapes constantly.

08:49.200 --> 08:51.400
And so there's two real solutions to this.

08:51.400 --> 08:54.880
One is you put a bird netting, which is crazy expensive because you have to cover vast

08:54.880 --> 08:58.040
fields with this fairly expensive mesh.

08:58.040 --> 09:01.960
You're not about to tell us about autonomous scarecrow, are you?

09:01.960 --> 09:02.960
Pretty much.

09:02.960 --> 09:03.960
Pretty much.

09:03.960 --> 09:04.960
Yeah.

09:04.960 --> 09:05.960
Yeah.

09:05.960 --> 09:08.160
So the other solution they went with is they made birdbangers.

09:08.160 --> 09:11.720
So they're basically this big propane air cannon that goes off every once in a while, makes

09:11.720 --> 09:13.200
a loud sound and scares them off.

09:13.200 --> 09:14.200
Oh, wow.

09:14.200 --> 09:15.200
Yeah.

09:15.200 --> 09:18.400
But the crow's in particular got clever and they figured out that the birdbanger was always

09:18.400 --> 09:21.080
in one place and didn't really scare them anymore.

09:21.080 --> 09:23.880
And whenever people would move around the birdbanger, the crow's would notice that people

09:23.880 --> 09:27.200
were carrying it around and they just move away from that area, but still eat the grapes

09:27.200 --> 09:28.200
elsewhere.

09:28.200 --> 09:29.200
Oh, wow.

09:29.200 --> 09:33.520
And so what we ended up doing is mounting one of these birdbangers on the grizzly and

09:33.520 --> 09:39.320
have it autonomously drive up and down the rows very quietly and then let it off and

09:39.320 --> 09:41.520
then go to the next spot quietly and let it off.

09:41.520 --> 09:46.240
And because it's electric and it's so slow, you know, or it could be made to go slowly,

09:46.240 --> 09:47.240
it worked.

09:47.240 --> 09:48.240
It scared them off.

09:48.240 --> 09:49.240
Awesome.

09:49.240 --> 09:53.240
So, you know, it sounds like a completely ridiculous use case, but it actually, you know,

09:53.240 --> 09:54.800
it was worth the time, it was worth the money.

09:54.800 --> 09:58.120
The complexity was low enough that we could actually assemble this application pretty much

09:58.120 --> 10:00.000
from building blocks.

10:00.000 --> 10:06.520
And now was this a research application or is this one of, does the company also do commercial,

10:06.520 --> 10:08.520
slash industrial solutions?

10:08.520 --> 10:09.520
Yeah, absolutely.

10:09.520 --> 10:14.200
So that one was research, but in the last few years, what we've done is we've created

10:14.200 --> 10:18.760
this new division inside of clear path called automotors and so what automotors does is

10:18.760 --> 10:25.080
we create these indoor platforms, the auto 1500 and the auto 100 to move around payload

10:25.080 --> 10:26.080
indoors.

10:26.080 --> 10:29.440
So in a factory or warehouse setting is kind of our target market right now.

10:29.440 --> 10:30.440
Okay.

10:30.440 --> 10:32.120
So we took up a big chunk of our team.

10:32.120 --> 10:37.240
We grew from roughly 20 people when I joined and now we're about 187 and most of that

10:37.240 --> 10:39.280
growth has come from the automotor side.

10:39.280 --> 10:40.280
Okay.

10:40.280 --> 10:47.720
I'll mention this because it was initially confusing to me, the clear path automotors

10:47.720 --> 10:52.640
has nothing to do with the auto autonomous truck company that Uber bought.

10:52.640 --> 10:54.400
No, but they were spelled the same.

10:54.400 --> 10:55.400
Yeah.

10:55.400 --> 10:56.400
Yeah.

10:56.400 --> 11:03.720
It kind of sort of happened that we filed that auto motor or auto trademark and branding

11:03.720 --> 11:05.360
a little bit before that.

11:05.360 --> 11:09.560
And yes, the two companies are totally unrelated, just a total coincidence.

11:09.560 --> 11:18.120
And the auto motor's platform is somewhat reminiscent of the Keeva style of autonomous

11:18.120 --> 11:21.200
warehouse materials handling robots.

11:21.200 --> 11:26.560
Do you, you know, the Keeva is kind of deployed as this entire system.

11:26.560 --> 11:32.320
Are you guys focusing just on the robots or are you also developing this entire warehouse

11:32.320 --> 11:34.640
automation system to go with them?

11:34.640 --> 11:35.640
Absolutely.

11:35.640 --> 11:37.520
We're doing the entire system all at once.

11:37.520 --> 11:42.440
And one thing I'd like to kind of point out is that Keeva in my mind is a little bit more

11:42.440 --> 11:48.040
of the old school style of robots wherein every single robot individually is actually fairly

11:48.040 --> 11:49.040
dumb.

11:49.040 --> 11:53.520
No, they'll follow QR codes on the floor and they get commands from the control system

11:53.520 --> 11:57.240
to go from point A to point B in a straight line and that's it, right?

11:57.240 --> 12:01.360
And when they do their pickup operation, it's essentially a blind pickup operation.

12:01.360 --> 12:07.400
And so the entire system is only applicable when you have a kind of lights out where

12:07.400 --> 12:12.760
there's no humans beyond a certain point, whereas the systems that we make are actually

12:12.760 --> 12:15.080
intended to work with humans.

12:15.080 --> 12:18.640
And so they're intended to take the same walkways that you'd see forklifts drive down,

12:18.640 --> 12:20.960
the same walkways that you'd see people walk around.

12:20.960 --> 12:25.600
So we think our system is a lot more flexible because you don't have to structure the entire

12:25.600 --> 12:27.520
warehouse around the robots.

12:27.520 --> 12:32.320
In fact, ideally, really nothing changes because it's infrastructure free.

12:32.320 --> 12:36.760
You could have any factory could drop in a few autos and they could get to work on day

12:36.760 --> 12:37.760
one pretty much.

12:37.760 --> 12:38.760
Oh, that's awesome.

12:38.760 --> 12:40.840
That's a great distinction to make.

12:40.840 --> 12:46.360
And it's also a good segue into one of the big questions that I've had or really a distinction

12:46.360 --> 12:51.440
that I think is worth exploring at least is, you know, I think in the robotics field we

12:51.440 --> 12:57.040
throw around the term autonomous quite a bit or I've seen it thrown around quite a bit.

12:57.040 --> 13:01.080
And I'm wondering, you know, I'd like to explore what exactly that means, right?

13:01.080 --> 13:06.640
I think, you know, just given the example you just provided in terms of the way the

13:06.640 --> 13:11.720
design principles of auto relative to Kiva, you know, Kiva will call their robots autonomous

13:11.720 --> 13:17.800
as well, but they're, you know, being controlled and directed by some central machine.

13:17.800 --> 13:24.640
So autonomous doesn't necessarily imply intelligent, let's say, you know, how do you guys think

13:24.640 --> 13:27.200
of, you know, what autonomous means?

13:27.200 --> 13:31.680
And is there any standardization around that term in the robotics industry like there

13:31.680 --> 13:36.240
are levels of standardization of, you know, what it means to be self-driving or autonomous

13:36.240 --> 13:37.760
in the automobile industry?

13:37.760 --> 13:41.960
They're starting to be and they're mostly focused around the concept of safety.

13:41.960 --> 13:42.960
Okay.

13:42.960 --> 13:47.040
So what level of safety is it to work with these robots in human areas?

13:47.040 --> 13:48.040
Okay.

13:48.040 --> 13:52.720
And so, and this goes back really to the older, well, still still current welding style robots

13:52.720 --> 13:57.480
for car plants, for example, where the arm will come down and weld the specific point

13:57.480 --> 13:59.440
completely blindly, right?

13:59.440 --> 14:03.960
It has no concept of what it's doing realistically and has no concept of what's in the way.

14:03.960 --> 14:08.640
And so it will gladly, yeah, it will gladly smash right through somebody, right?

14:08.640 --> 14:15.000
And so that I would call, now I'm unsure if there is a kind of a growing term, but around

14:15.000 --> 14:20.000
here what we say is that systems like that are automated, but they're not necessarily

14:20.000 --> 14:21.000
autonomous.

14:21.000 --> 14:22.000
Okay.

14:22.000 --> 14:26.320
So that the system is doing a thing automatically, it is repeating the same motion, but it

14:26.320 --> 14:30.840
is not actually planning its own path, it's not making its own decisions.

14:30.840 --> 14:33.240
And so that's the case here as well.

14:33.240 --> 14:37.600
And we've had, you know, there have been mobile robots inside of factories for quite a

14:37.600 --> 14:38.600
while.

14:38.600 --> 14:41.000
I mean, Toyota has been making them probably since the 80s.

14:41.000 --> 14:46.480
And yeah, and these systems will follow magnetic tape on the floor or buried markers.

14:46.480 --> 14:49.240
And they'll go from station to station, they'll pick up a load, they'll go somewhere else

14:49.240 --> 14:50.240
and drop it off.

14:50.240 --> 14:55.000
And again, I consider that system automated, but it's not really autonomous, it's only

14:55.000 --> 15:01.560
follows a very specific set of instructions and can't deviate, it can't replan.

15:01.560 --> 15:06.120
And if anybody trips its safety laser, it will just stop, it has no other option.

15:06.120 --> 15:13.200
So then going back to our mobile scarecrow, if you will, when you say that it's autonomously

15:13.200 --> 15:17.840
navigating the vineyard, to what degree is it actually doing that autonomously?

15:17.840 --> 15:18.840
Yeah, absolutely.

15:18.840 --> 15:24.880
Well, I mean, that specific one was kind of more of a test case here in Niagara region.

15:24.880 --> 15:29.560
So that one was actually just using GPS waypoints going from point to point.

15:29.560 --> 15:35.320
And if it saw something in front of it using a stereo camera, it would just stop.

15:35.320 --> 15:36.320
Okay.

15:36.320 --> 15:39.080
So that was one of those cases where it's kind of automated, but not autonomous.

15:39.080 --> 15:40.080
Okay.

15:40.080 --> 15:45.880
However, in the auto cases, what they're doing is they actually use laser scanners to build

15:45.880 --> 15:49.440
up a map of the environment and find their own path through the environment.

15:49.440 --> 15:50.440
Okay.

15:50.440 --> 15:54.800
And so they're actively each unit is making its own decisions, even though we have this

15:54.800 --> 15:59.680
overarching dispatching system that will command each unit to do certain tasks.

15:59.680 --> 16:03.120
The actual execution of that task is usually up to the robot itself.

16:03.120 --> 16:04.120
Okay.

16:04.120 --> 16:05.120
Interesting.

16:05.120 --> 16:11.880
And to what degree at auto are you getting into notions of the robots collaborating

16:11.880 --> 16:14.360
with one another or with humans?

16:14.360 --> 16:15.360
Yeah.

16:15.360 --> 16:16.360
Absolutely.

16:16.360 --> 16:19.560
So with one another, the one kind of interesting story I can tell is there's a client

16:19.560 --> 16:25.480
we have where we're working towards replacing the standard assembly line, essentially.

16:25.480 --> 16:29.520
So you can imagine vehicles or machines going by on an assembly line.

16:29.520 --> 16:34.640
And if you have a different range of products, you could have three different assembly lines

16:34.640 --> 16:37.840
for a simple, a medium and a very complex product, for example.

16:37.840 --> 16:41.720
So instead of that, what we're doing is the robot actually carries the product and then

16:41.720 --> 16:43.320
there's only one assembly line.

16:43.320 --> 16:48.200
And so as the large auto 1500 is carrying the product, smaller robots are driving up and

16:48.200 --> 16:52.760
delivering, here's a simple part for the cheap model, here's a much more complex part

16:52.760 --> 16:54.640
for the expensive model.

16:54.640 --> 16:58.280
And oh, by the way, the more complex model needs more time for assembly.

16:58.280 --> 17:02.560
And so the robot will drive slower and it'll have bigger buffers between it and the next

17:02.560 --> 17:03.560
auto 1500.

17:03.560 --> 17:08.200
So if you can imagine it, it's this totally mobile assembly line using robots as the actual

17:08.200 --> 17:09.200
carriers.

17:09.200 --> 17:10.200
Right.

17:10.200 --> 17:11.200
Oh, that's interesting.

17:11.200 --> 17:12.200
Yeah.

17:12.200 --> 17:13.200
Absolutely.

17:13.200 --> 17:15.800
And so that's a case where not only are robots collaborating with each other because you

17:15.800 --> 17:20.520
have smaller deliveries to larger deliveries and this kind of dynamic spacing.

17:20.520 --> 17:24.440
But you also have humans collaborating directly with the robots where they know that their

17:24.440 --> 17:27.440
parts are arriving just in time for them to use.

17:27.440 --> 17:33.120
And yeah, and you can imagine there's hundreds of different ways that this is more optimal

17:33.120 --> 17:34.720
than a assembly line.

17:34.720 --> 17:39.120
If something is too slow or something gets broken or something's not quite right, that can

17:39.120 --> 17:43.120
be pulled out of the assembly line without the rest of the assembly line even noticing.

17:43.120 --> 17:47.400
And since the dispatcher system is keeping track, it will automatically adjust the distance

17:47.400 --> 17:48.560
between robots.

17:48.560 --> 17:54.960
In general, when you're deploying a system like this, what's the kind of level of abstraction

17:54.960 --> 17:57.600
if you will that the customer needs to deal with?

17:57.600 --> 17:58.960
Yeah, that's a great question.

17:58.960 --> 18:02.280
So we have this entire mapping system.

18:02.280 --> 18:07.160
And if you could kind of imagine, let's take a case of a small manufacturer.

18:07.160 --> 18:11.200
So they want to use the auto 100 to move parts around the facility.

18:11.200 --> 18:15.120
So on day one, an auto 100 arrives at their facility and they drive it around.

18:15.120 --> 18:18.840
So this first step, they're physically driving it with a little joystick.

18:18.840 --> 18:23.800
The idea being that humans are especially people who work at the facility, much better

18:23.800 --> 18:27.680
know what is a dangerous area and what's a safe area than a machine could.

18:27.680 --> 18:30.600
So for the initial step, we really don't want them driving autonomously.

18:30.600 --> 18:33.280
We want somebody to kind of shepherd them around.

18:33.280 --> 18:37.240
As it does that first pass, it builds up this map of the environment that is then uploaded

18:37.240 --> 18:39.840
to the control system or the dispatcher.

18:39.840 --> 18:46.240
And after that, all the robots now know this map and the user from an abstraction perspective,

18:46.240 --> 18:50.360
they get this two dimensional floor plan that they can then draw on with just a mouse

18:50.360 --> 18:51.360
click.

18:51.360 --> 18:52.360
This is cell one.

18:52.360 --> 18:53.360
This is cell two.

18:53.360 --> 18:54.360
Don't go here.

18:54.360 --> 18:55.680
This is a one way zone.

18:55.680 --> 18:58.200
This is a slow speed zone, so on and so on.

18:58.200 --> 19:02.000
And so they're essentially painting in the map in what style that they want the robots

19:02.000 --> 19:03.000
to work in.

19:03.000 --> 19:04.000
Okay.

19:04.000 --> 19:08.520
And lastly, the robots are also able to detect what is a charging station, what is a docking

19:08.520 --> 19:11.280
station if they need higher precision.

19:11.280 --> 19:14.680
And those are automatically, those are features automatically added to the map.

19:14.680 --> 19:18.760
So at the end of this, you know, first day, the user has a map of their plant.

19:18.760 --> 19:22.400
They know where all the cells are, where the charging stations are, and where all the robots

19:22.400 --> 19:23.400
are positioned.

19:23.400 --> 19:29.000
After that, they can either do individual commands and tasks or fleet white commands.

19:29.000 --> 19:33.520
So they can tell a robot, I need you to go from here to here right now, and that's kind

19:33.520 --> 19:35.160
of a one time command.

19:35.160 --> 19:39.560
Or you could set up a chain saying, I need robots from this dock to this dock every 30

19:39.560 --> 19:41.080
seconds.

19:41.080 --> 19:42.920
And then it takes off.

19:42.920 --> 19:43.920
Okay.

19:43.920 --> 19:49.160
And then a next level would be integrating that in with some type of plant management system

19:49.160 --> 19:54.720
that is, you know, based on kind of flow of, you know, machines and materials or integrating

19:54.720 --> 20:00.360
into a CNC or something like that to, I, what is that, you know, integrating into the

20:00.360 --> 20:04.360
broader production pipeline, what is that fit in?

20:04.360 --> 20:05.360
Yeah.

20:05.360 --> 20:08.080
So that's not something we've started out quite yet.

20:08.080 --> 20:09.320
That's definitely on our roadmap.

20:09.320 --> 20:10.320
Okay.

20:10.320 --> 20:13.200
However, we have a lot of integration partners that have started doing that.

20:13.200 --> 20:20.520
So we have seen quite a few different full robotic arms and dual manipulator systems mounted

20:20.520 --> 20:22.000
on top of auto units.

20:22.000 --> 20:23.000
Oh, wow.

20:23.000 --> 20:24.000
Okay.

20:24.000 --> 20:30.360
And actually, at this year's ICRA 2017, we're going to have a UR5 arm on top of a rich

20:30.360 --> 20:32.640
back, which is a research robot.

20:32.640 --> 20:39.160
But the research robots are also now deploying this autonomy research kit for use by researchers.

20:39.160 --> 20:42.880
And one of the demonstrations we'll have there is trivially scripting the arm to pick up

20:42.880 --> 20:47.840
an object, drive somewhere and put it down in about, you know, five, ten lines of code.

20:47.840 --> 20:48.840
Okay.

20:48.840 --> 20:50.360
Oh, wow.

20:50.360 --> 20:57.040
One of the things that you mentioned that has come up in, and my research is being really

20:57.040 --> 20:58.040
important.

20:58.040 --> 21:05.640
And a lot of ways distinct from the treatment of AI and robotics in kind of the academic

21:05.640 --> 21:12.440
literature is this idea of learning from the subject matter expertise that is inherent

21:12.440 --> 21:18.320
in the customer environment or the ultimate deployment environment of the robot.

21:18.320 --> 21:23.040
So for example, you talked about, hey, let's let humans drive the robot around.

21:23.040 --> 21:29.200
So we don't have to let the robot bang around the warehouse for a week to try to figure

21:29.200 --> 21:32.040
out what's what.

21:32.040 --> 21:41.400
And then letting humans then come in and put markings on the map to identify areas that

21:41.400 --> 21:46.080
robots shouldn't go or should go or directions, things like that.

21:46.080 --> 21:55.880
I'm curious your perspective on that notion generally of capturing human expertise in

21:55.880 --> 22:05.040
the programming of the robot with AI and any challenges that it represents, other areas

22:05.040 --> 22:08.400
that you might see it come up, things like that.

22:08.400 --> 22:09.400
Yeah, for sure.

22:09.400 --> 22:15.920
So one of the early problems we had was when we were looking into getting these robots

22:15.920 --> 22:19.120
into factories, we had two different responses.

22:19.120 --> 22:21.160
One of them was, this is fantastic.

22:21.160 --> 22:22.400
The robot can go anywhere.

22:22.400 --> 22:23.800
I don't have to do any planning.

22:23.800 --> 22:28.280
It'll just figure everything out, which isn't quite right because really to get to that

22:28.280 --> 22:32.480
kind of level of information, you're talking about a general purpose AI, which we're not

22:32.480 --> 22:33.960
close to.

22:33.960 --> 22:34.960
Right.

22:34.960 --> 22:39.240
And then the other end of the spectrum, which we also got was, well, how can I depend

22:39.240 --> 22:45.720
on this thing if it's not exactly following this path that I laid on the floor for it?

22:45.720 --> 22:51.400
And so some of our early efforts involved putting virtual tape into the map.

22:51.400 --> 22:57.080
So behaviors to tell the robot that I need you to follow this exact path and don't

22:57.080 --> 22:58.080
deviate from it.

22:58.080 --> 23:01.520
If anything gets in your way, stop, otherwise keep going.

23:01.520 --> 23:06.520
And that kind of behavior really comes from the learning that a lot of the times in the

23:06.520 --> 23:11.800
academic community, getting the thing to work and work well and not crash into things is

23:11.800 --> 23:12.800
quite an achievement.

23:12.800 --> 23:13.800
It's quite difficult.

23:13.800 --> 23:15.440
That's what we spend a lot of time on.

23:15.440 --> 23:19.640
But especially when you're talking about industrial use cases, the customer is looking at it as

23:19.640 --> 23:20.640
equipment.

23:20.640 --> 23:24.120
They're not really looking at it as this self-driving vehicle.

23:24.120 --> 23:30.960
And so they don't care that it's doing these magic, creating maps and navigating.

23:30.960 --> 23:36.480
They care about what is the tag time, how efficient is it, and what is its downtime.

23:36.480 --> 23:41.800
And a lot of the time, you will find that through these people working there for years and

23:41.800 --> 23:44.160
years, they know what the most efficient route is.

23:44.160 --> 23:50.600
They know that even though this route is longer in terms of footpath or absolute distance,

23:50.600 --> 23:54.960
it will take a shorter time because it tends to be less congested, for example.

23:54.960 --> 24:01.280
So adding in that human layer knowledge of a here's an area that is more clear usually

24:01.280 --> 24:06.040
is kind of an interesting trick that we've had to fuse the best of human knowledge of

24:06.040 --> 24:11.560
the environment with this kind of superhuman level planning and optimization that computers

24:11.560 --> 24:12.560
can do.

24:12.560 --> 24:13.560
Great.

24:13.560 --> 24:14.560
Great.

24:14.560 --> 24:20.160
Can you talk a little bit about all of the various ways, maybe catalog the various ways that

24:20.160 --> 24:25.120
you guys use machine learning in AI with the auto systems?

24:25.120 --> 24:26.120
Yeah.

24:26.120 --> 24:32.600
So some of the things we're really looking into, one of them, it's sort of on the border

24:32.600 --> 24:38.840
machine learning, but one of the main kind of problems of creating these systems is getting

24:38.840 --> 24:40.840
an accurate map.

24:40.840 --> 24:47.200
And so there's a standard way to use laser data and turn it into a map just by basically

24:47.200 --> 24:48.680
it's called graph slam.

24:48.680 --> 24:55.080
And that method works well on its own, but it does have inherent problems of kind of distortions

24:55.080 --> 24:56.080
and things like that.

24:56.080 --> 25:01.480
So you do have to apply a little bit of intelligence to the way that you're creating these maps and

25:01.480 --> 25:06.320
that you know that on a factory floor, for example, it would be very unlikely for the outer

25:06.320 --> 25:07.320
wall to curve.

25:07.320 --> 25:08.320
Right.

25:08.320 --> 25:09.320
Right.

25:09.320 --> 25:13.040
So that just doesn't look right to a human and so it shouldn't look right to our algorithms

25:13.040 --> 25:14.040
either.

25:14.040 --> 25:18.920
And I mean, too, you're doing things like maybe classifying points and curve fitting and

25:18.920 --> 25:22.160
that kind of thing to try to generate the ultimate map.

25:22.160 --> 25:23.160
Generate much better maps.

25:23.160 --> 25:24.160
Yeah.

25:24.160 --> 25:28.000
And the kind of the step further than that is after we have a decent map, we can also

25:28.000 --> 25:31.440
think about can we classify obstacles in that space?

25:31.440 --> 25:37.760
So just using laser data or perhaps camera data or ultrasonic data, can we classify that

25:37.760 --> 25:40.440
I'm fairly confident this is a forklift.

25:40.440 --> 25:43.880
If this is a forklift, that means that it has these large times.

25:43.880 --> 25:48.440
If I can't see the times, then either they're out of my laser scanning range or something

25:48.440 --> 25:49.440
else is going wrong.

25:49.440 --> 25:54.800
So I'm going to give this an extra wide birth because I can't tell where the hazard is.

25:54.800 --> 25:56.440
Or otherwise, oh, I do see the times.

25:56.440 --> 25:57.440
I know exactly where it is.

25:57.440 --> 26:01.680
So I'm going to shrink my approach distance and I can be more efficient in my pathening.

26:01.680 --> 26:02.680
Okay.

26:02.680 --> 26:06.960
So obstacle recognition and being able to predict what an obstacle is going to do in terms

26:06.960 --> 26:12.240
of movement or speed or direction is definitely something that makes planning more efficient.

26:12.240 --> 26:17.640
And how about in terms of the robot, the navigation?

26:17.640 --> 26:25.080
I mean, we've talked about a little bit of the the pathing is all of the the pathing more

26:25.080 --> 26:30.200
or less deterministic, you know, outside of the fact that it has to detect objects and,

26:30.200 --> 26:39.600
you know, react to them or the is the general path that takes more or less set or will one

26:39.600 --> 26:45.480
of these robots ever deviate if it, you know, knows of an alternate path and find some obstacle

26:45.480 --> 26:48.880
that, you know, isn't moving or not moving quickly enough.

26:48.880 --> 26:49.880
Yeah.

26:49.880 --> 26:50.880
Absolutely.

26:50.880 --> 26:55.160
It's, it's making a constant projection of how long it's going to take to arrive to its

26:55.160 --> 26:56.160
end goal.

26:56.160 --> 26:57.160
Okay.

26:57.160 --> 26:58.160
And it's updating the system.

26:58.160 --> 27:02.760
So the system knows this robot needs a 30 second time from point A to point B and it's

27:02.760 --> 27:05.760
only going to make it in in 32 seconds.

27:05.760 --> 27:07.080
So something's not quite right.

27:07.080 --> 27:14.760
So this path is either not possible to do in this time or there's more detourists or obstacles

27:14.760 --> 27:18.080
in the way than there should be at this time.

27:18.080 --> 27:22.440
And so the robot's constantly making this projection of what is the fastest way to get

27:22.440 --> 27:23.440
there?

27:23.440 --> 27:24.440
What is the most efficient way to get there?

27:24.440 --> 27:29.920
And if it's on a path and it sees obstacles or even if other robots in the space have detected

27:29.920 --> 27:35.720
those obstacles and uploaded them to the supervising system, then that robot will know,

27:35.720 --> 27:40.760
okay, this is likely a less ideal path, so I'm going to take this alternative path.

27:40.760 --> 27:46.200
Can you talk a little bit about the, maybe the difference between machine learning and

27:46.200 --> 27:52.640
AI that is running on the robots versus running on the supervising system?

27:52.640 --> 27:55.040
What functions maybe sit where?

27:55.040 --> 27:58.480
Yeah, I can talk a little bit about that.

27:58.480 --> 28:04.000
Ideally speaking, we don't want to have our individual robots constantly trying to adjust

28:04.000 --> 28:07.200
all the time or constantly trying to learn all the time.

28:07.200 --> 28:11.760
Because this runs into one of the fundamental problems you have, especially applying AI

28:11.760 --> 28:17.800
and industry, is that you have to be probably safe, right, especially with these machines

28:17.800 --> 28:20.760
being very large and fast.

28:20.760 --> 28:25.640
You have to be sure that even though there's a hardware safety system on board, triggering

28:25.640 --> 28:31.000
a hardware emergency stop is not a desirable effect because you just, at the very best

28:31.000 --> 28:36.440
case, you've messed up your, your tack time, right, you've messed up your delivery window.

28:36.440 --> 28:40.200
But in the worst case, you can't stop fast enough and you actually hit something.

28:40.200 --> 28:47.320
So, we need to ensure that the robots on the robot level have as predictable behavior as

28:47.320 --> 28:48.480
possible.

28:48.480 --> 28:53.080
And this even goes up to the level of, if you'll notice, the auto 1500s and the 100s

28:53.080 --> 28:55.720
have a LED strip around the edge.

28:55.720 --> 29:00.600
And so what we do is because, especially the auto 1500, it's fundamentally, doesn't really

29:00.600 --> 29:02.840
have a front in a rear.

29:02.840 --> 29:05.400
They're almost a mirror image of each other.

29:05.400 --> 29:07.920
And so the robot doesn't care if it's going forwards or backwards.

29:07.920 --> 29:11.480
And so that leads to a lot of confusion for people because they couldn't tell what the

29:11.480 --> 29:13.200
robot was planning to do, right.

29:13.200 --> 29:17.440
And then people will naturally, you know, if you can imagine you see this machine behaving

29:17.440 --> 29:22.920
in a predictable way, you'll naturally start to stop and be wary and kind of clog up

29:22.920 --> 29:24.960
the root, right.

29:24.960 --> 29:29.280
So, having people comfortably know where the robot's planning to go means though naturally

29:29.280 --> 29:30.960
clear out of that space.

29:30.960 --> 29:35.080
And then it's kind of a reinforcement effect in that the robot will move better.

29:35.080 --> 29:38.760
And so that LED strip, what it actually does is it actually puts headlights at the front

29:38.760 --> 29:41.200
and tail lights at the back like a car.

29:41.200 --> 29:45.040
And it will actually blink yellow blinking turning signals when it's turning in one direction

29:45.040 --> 29:46.200
of the other.

29:46.200 --> 29:50.040
And people really respond naturally to that because we have this kind of built in language

29:50.040 --> 29:51.040
for roads.

29:51.040 --> 29:52.040
Right.

29:52.040 --> 29:53.640
What does it look like when cars coming to stop?

29:53.640 --> 29:56.680
Well, the back red tail lights will flare brighter.

29:56.680 --> 29:57.680
Right.

29:57.680 --> 29:58.680
What does it look like when it's trying to pass?

29:58.680 --> 30:03.680
Well, it might blink the white front lights telling you, go ahead, you know, you go first.

30:03.680 --> 30:08.000
And so all these kind of behaviors really make it much more predictable in its movement.

30:08.000 --> 30:11.520
And as a side effect, they make it more safe.

30:11.520 --> 30:22.080
What else goes into, I mean, the so you've talked about safety systems, but do customers,

30:22.080 --> 30:24.480
you mentioned provably safe.

30:24.480 --> 30:30.680
What goes into the proving of the safety and to what degree do you have to do that are

30:30.680 --> 30:37.320
they're a specific regulatory requirements that you run into that have standards for proving

30:37.320 --> 30:38.320
safety.

30:38.320 --> 30:40.800
And, you know, what are those and how do you address them?

30:40.800 --> 30:41.800
Yeah.

30:41.800 --> 30:42.800
Absolutely.

30:42.800 --> 30:47.440
So there's one of the the fields where we're seeing here is that a lot of the standards

30:47.440 --> 30:52.040
in industry for so called collaborative robots are really designed around collaborative

30:52.040 --> 30:53.040
arms.

30:53.040 --> 30:56.760
So universal robot or the backster from rethink, right?

30:56.760 --> 31:00.600
These are robots that are intended to work with humans.

31:00.600 --> 31:05.000
And so they're very, very clearly defined on maximum inertia's pinch points and things

31:05.000 --> 31:06.000
like that.

31:06.000 --> 31:11.320
But actually proving that a system that is mobile on the ground, that's a much more tricky

31:11.320 --> 31:12.320
story.

31:12.320 --> 31:13.320
Okay.

31:13.320 --> 31:14.480
So there's a large set.

31:14.480 --> 31:22.920
I think we currently follow 12 or 13 different standards for safe systems working with people.

31:22.920 --> 31:23.920
Okay.

31:23.920 --> 31:28.720
And so from the very basic level, things like the emergency stop system has to be dual

31:28.720 --> 31:29.720
redundant.

31:29.720 --> 31:35.680
So at any given time, if any one system fails, the system can still stop safely.

31:35.680 --> 31:36.680
Mm-hmm.

31:36.680 --> 31:41.920
Other things going all the way up to our planner on the robot itself should never plan

31:41.920 --> 31:45.040
into a space that it can become unsafe.

31:45.040 --> 31:50.760
So even in the worst case scenario, some horrible bug freezes the entire computer, the robot

31:50.760 --> 31:54.400
should still be able to stop itself safely.

31:54.400 --> 31:55.400
Mm-hmm.

31:55.400 --> 31:56.400
Right?

31:56.400 --> 32:02.280
Well, you have the highest level, which is the supervisory system, commanding robots and

32:02.280 --> 32:05.160
kind of hinting them a most reasonably safe path.

32:05.160 --> 32:06.160
Mm-hmm.

32:06.160 --> 32:10.280
Then you have the actual autonomy on board the robot, ensuring that it's going at a safe

32:10.280 --> 32:15.360
speed with a safe clearing distance in front of it and planning into a safe space.

32:15.360 --> 32:19.600
Then a lower level down, you have the microcontroller that's doing the low level command and real

32:19.600 --> 32:24.600
time control of brakes, motor and coder values and those kind of thing.

32:24.600 --> 32:28.480
And then even outside of all that, you have this completely separate hardware safety system

32:28.480 --> 32:31.200
that's tied into the safety lasers.

32:31.200 --> 32:35.440
So that when the lasers are set to mode, that it's expecting the robot to travel at one

32:35.440 --> 32:41.000
meter per second, we know that the robot will stop at this speed in one meter seconds.

32:41.000 --> 32:45.040
So the laser, you know, safety trip range has to be at least this far.

32:45.040 --> 32:46.040
Okay.

32:46.040 --> 32:51.400
And so a lot of the challenge to build this safe system was, can we prove and can we test

32:51.400 --> 32:56.800
out that the robot will actually stop at this speed under all of these conditions?

32:56.800 --> 32:57.800
Right.

32:57.800 --> 32:58.800
Right.

32:58.800 --> 33:02.200
And so how we did that is we actually have a motion tracking system similar to what's

33:02.200 --> 33:04.360
used in the film industry.

33:04.360 --> 33:08.400
And we put tracker dots all over our robots and we would run them at carburet obstacles

33:08.400 --> 33:13.440
over and over again for hours from different directions, different speeds, different payloads,

33:13.440 --> 33:19.000
everything and figure out exactly how long it takes to stop in all of these conditions.

33:19.000 --> 33:20.000
Okay.

33:20.000 --> 33:24.840
So the interesting things we found out is there's actually a slight omission in some of

33:24.840 --> 33:33.040
the safety standards in that the safety standard assumes if you're going around an arc or curve,

33:33.040 --> 33:37.160
if you stop the motors, so you remove power from them, the robot will naturally go in a

33:37.160 --> 33:38.160
straight line.

33:38.160 --> 33:39.160
Mm-hmm.

33:39.160 --> 33:41.040
So you can imagine, you know, you're swinging away it on a string.

33:41.040 --> 33:43.000
If you let go, it should travel in a straight line.

33:43.000 --> 33:44.000
Right.

33:44.000 --> 33:48.560
But that's not the case because you actually do have rotational momentum on your robot.

33:48.560 --> 33:52.680
So you will actually go in a, you will continue to go in a curve, not as much of a curve

33:52.680 --> 33:55.040
as you are going, but more than you would expect.

33:55.040 --> 33:56.040
Right.

33:56.040 --> 33:59.320
And so these kind of things are really hard to see unless you do these thousands of hours

33:59.320 --> 34:02.480
of testing and you track it down to the millimeter.

34:02.480 --> 34:03.480
Mm-hmm.

34:03.480 --> 34:06.840
But that really shapes the way that our laser safety field sets work.

34:06.840 --> 34:07.840
Mm-hmm.

34:07.840 --> 34:13.080
And it seems like that doesn't even take into account the, you know, the physics of whatever

34:13.080 --> 34:14.080
your payload is.

34:14.080 --> 34:19.160
You know, if you've got a bunch of palatized things stacked up, you know, 10 feet tall,

34:19.160 --> 34:24.480
you're going to have kind of a dispersion radius of things all over the, the warehouse

34:24.480 --> 34:30.160
floor that you need to kind of keep track of if you're trying to figure out, you know,

34:30.160 --> 34:34.520
what the dangerous radius around this, this robot is.

34:34.520 --> 34:35.520
Absolutely.

34:35.520 --> 34:36.520
Yeah.

34:36.520 --> 34:41.200
And so the kind of next steps that we're really looking into is, can we use our load sensors

34:41.200 --> 34:45.480
to ensure that we never tip anything over even if somebody puts something improperly on

34:45.480 --> 34:47.480
top of the robot?

34:47.480 --> 34:52.600
Can we, can we use camera systems to ensure that we have looking around corners that the

34:52.600 --> 34:54.000
robot can't see?

34:54.000 --> 34:58.320
And so to start breaking ahead of time, can we project light ahead of the robot so that

34:58.320 --> 35:01.920
people know that the robot's coming around the corner?

35:01.920 --> 35:02.920
Those kind of systems.

35:02.920 --> 35:03.920
Yeah.

35:03.920 --> 35:05.760
There's definitely, there's, there's lots and lots of different directions to go, but

35:05.760 --> 35:10.320
at the most fundamental level, it's ensuring that your systems are dual redundant and you

35:10.320 --> 35:13.920
have a completely hardware-based safety stop system.

35:13.920 --> 35:20.880
It sounds like in a lot of ways, there's a convergence between industrial robotics and

35:20.880 --> 35:29.520
consumer autonomous vehicles and the technologies that are perhaps accelerating due to the interest

35:29.520 --> 35:35.520
in self-driving cars are going to kind of find their way into the industrial realm.

35:35.520 --> 35:37.400
Are you guys seeing that?

35:37.400 --> 35:38.400
Yeah.

35:38.400 --> 35:39.400
Yeah.

35:39.400 --> 35:46.040
And undoubtedly, you know, the consumer autonomous vehicle market is a very large market, but

35:46.040 --> 35:49.280
in my opinion, it's a much more difficult market to capture.

35:49.280 --> 35:50.280
Right.

35:50.280 --> 35:55.400
The nice thing about the inside of a factory is it never rains.

35:55.400 --> 35:57.880
So you're never going to have the fire sprinklers off, right?

35:57.880 --> 35:58.880
Yeah, exactly.

35:58.880 --> 36:03.760
So you're never going to have rain, you're never going to fog, you're never going to ice.

36:03.760 --> 36:06.240
And so those, these are all things that we don't have to worry about.

36:06.240 --> 36:09.040
And so we have much more predictable systems.

36:09.040 --> 36:14.560
And so a lot of the times, it's actually kind of nice that the autonomous vehicle market

36:14.560 --> 36:18.400
is thinking about all these problems to make their lasers and cameras more robust against

36:18.400 --> 36:21.040
these effects that we don't even have to worry about.

36:21.040 --> 36:24.600
So we get this additional robustness, essentially, for free.

36:24.600 --> 36:30.400
Do you guys, to what degree do you guys use reinforcement learning?

36:30.400 --> 36:36.040
That's technology that has come up quite a bit in my research into these systems.

36:36.040 --> 36:39.360
Is that something that you guys are looking at or using?

36:39.360 --> 36:44.560
We're definitely looking at it, especially as I mentioned, with the idea of, can we classify

36:44.560 --> 36:47.960
obstacles that we see in our space?

36:47.960 --> 36:51.960
But that isn't yet released into our actual industrial offerings at the moment.

36:51.960 --> 36:52.960
Okay.

36:52.960 --> 37:00.040
And so over time, you're adding more capability in terms of like computer vision and things

37:00.040 --> 37:07.080
like that, generally, to be able to detect and differentiate between various obstacles.

37:07.080 --> 37:13.760
So that is that primarily for the safety use case or there are other things that you

37:13.760 --> 37:20.240
envision the robots doing with the ability to make those kinds of distinctions?

37:20.240 --> 37:21.240
Yeah.

37:21.240 --> 37:25.920
So the real three things are safety, efficiency, and accuracy.

37:25.920 --> 37:26.920
Okay.

37:26.920 --> 37:34.440
So with safety, having a stereoscopic camera system, such as on the auto 100, it allows

37:34.440 --> 37:38.480
you to see obstacles outside of the field of range of the lighter.

37:38.480 --> 37:39.480
Okay.

37:39.480 --> 37:43.680
So especially the auto 100, we expect it to be used more in colored environments and smaller

37:43.680 --> 37:44.680
environments.

37:44.680 --> 37:49.240
So it really needs to see that there's a desk that's up above where the lighter can see.

37:49.240 --> 37:51.240
To ensure that doesn't crash into it.

37:51.240 --> 37:52.240
Right.

37:52.240 --> 37:53.240
Right.

37:53.240 --> 37:56.720
The second idea in terms of efficiency is if we can classify obstacles, we can figure out

37:56.720 --> 37:58.960
what their behavior movement is.

37:58.960 --> 38:02.800
And so naturally as a person, if you're walking down the street and you see somebody walking

38:02.800 --> 38:06.360
towards you, I mean, and sometimes humans will make this mistake, but you'll go to the

38:06.360 --> 38:08.960
left, they'll go to the left, you'll go to the right, they'll go to the right.

38:08.960 --> 38:10.560
You know, you could do that little dance.

38:10.560 --> 38:11.560
Right.

38:11.560 --> 38:12.560
Right.

38:12.560 --> 38:16.200
So the idea here is can we kind of heuristically figure out a way to ensure that robots

38:16.200 --> 38:22.520
don't do that kind of behavior, that they actually have a almost a personality, they'll tend

38:22.520 --> 38:28.640
to stick to the wall when they see a person very visibly get out of your way or vice versa,

38:28.640 --> 38:31.920
you know, they're carrying something very heavy and very quickly, they'll, they'll, you

38:31.920 --> 38:35.200
know, sound a little alarm, even though you're not in the range, you're not in the way yet,

38:35.200 --> 38:38.080
they'll warn you ahead of time because they know you might get in the way.

38:38.080 --> 38:39.080
Right.

38:39.080 --> 38:40.080
Right.

38:40.080 --> 38:41.080
And so those kind of things.

38:41.080 --> 38:42.080
So that would be kind of the efficiency state.

38:42.080 --> 38:45.040
Well, I mean, and safety again as well, but also ensuring that the robot doesn't have

38:45.040 --> 38:47.880
to be out of the way or can plan ahead of time.

38:47.880 --> 38:48.880
Yeah.

38:48.880 --> 38:54.600
So basically in terms of actual localization, camera gives you several orders of magnitude

38:54.600 --> 38:57.760
more rich data than just a laser.

38:57.760 --> 39:04.280
And so you can start positioning yourself based on paint or markings or you name it light

39:04.280 --> 39:05.280
fixtures.

39:05.280 --> 39:09.480
And so that gives you this added level robustness and accuracy in terms of positioning yourself

39:09.480 --> 39:11.280
in three dimensional space.

39:11.280 --> 39:12.280
Okay.

39:12.280 --> 39:16.120
And all those things actually require machine learning quite to a large extent.

39:16.120 --> 39:21.480
You have to understand that even if this obstacle doesn't look like it's in the way, if

39:21.480 --> 39:25.280
you recognize it as a table, you can make certain assumptions like, okay, well, tables

39:25.280 --> 39:27.080
usually touch the floor.

39:27.080 --> 39:30.840
And so even if I can't see the legs on the other side, I know there's something on the

39:30.840 --> 39:31.840
other side.

39:31.840 --> 39:35.760
And if I know roughly the dimensions, I know roughly how big I have to avoid this obstacle

39:35.760 --> 39:38.160
by even if I can't see the other end of it.

39:38.160 --> 39:39.160
Right.

39:39.160 --> 39:40.160
Right.

39:40.160 --> 39:47.160
And the challenge is that I think you're trying to overcome with the computer vision applications

39:47.160 --> 39:53.360
is the, the lidar is generally only giving you a two-dimensional kind of lay of the land.

39:53.360 --> 39:54.360
Right.

39:54.360 --> 39:56.160
And it's typically they're mounted pretty low.

39:56.160 --> 40:02.200
So like anything that's around knee level, it sees and anything below or above that,

40:02.200 --> 40:05.640
it's kind of oblivious to is that the way yours work as well.

40:05.640 --> 40:06.640
Yeah.

40:06.640 --> 40:07.640
That's close.

40:07.640 --> 40:09.320
I mean, we try to mount ours right about ankle level.

40:09.320 --> 40:10.320
Okay.

40:10.320 --> 40:11.800
Or as low as we possibly can.

40:11.800 --> 40:12.800
Okay.

40:12.800 --> 40:19.280
Because you really want to see people's ankles or feet, ideally, when you know exactly what

40:19.280 --> 40:20.800
their contact point to the floor is.

40:20.800 --> 40:21.800
Okay.

40:21.800 --> 40:27.440
But yeah, as I mentioned earlier, kind of one of our early issues we had a lot was forklifts.

40:27.440 --> 40:30.560
So a forklift driver who didn't put his tines down.

40:30.560 --> 40:31.560
Yeah.

40:31.560 --> 40:37.080
So the two kind of big forks at the front, they would be sticking up in the air right about

40:37.080 --> 40:38.080
knee level.

40:38.080 --> 40:39.080
Okay.

40:39.080 --> 40:42.560
And it's kind of a deadly area, if you think about it, because it's just out of range

40:42.560 --> 40:43.560
of the LiDAR.

40:43.560 --> 40:44.560
Yeah.

40:44.560 --> 40:45.560
Yeah.

40:45.560 --> 40:46.560
And so that was quite tricky.

40:46.560 --> 40:49.960
And you're really playing this bouncing game, you theoretically you want to be right at

40:49.960 --> 40:51.880
ground level.

40:51.880 --> 40:56.720
But the problem with that is a, you can't really mount a laser that low because you'll

40:56.720 --> 40:59.200
just start to scrape it on the floor.

40:59.200 --> 41:02.720
And the other problem is is that floors aren't actually that flat.

41:02.720 --> 41:03.720
Right.

41:03.720 --> 41:09.720
So you do actually, there's another aspect there of kind of human guided machine learning

41:09.720 --> 41:15.000
where you want the robots to understand that this is not an obstacle, nor is it a hill.

41:15.000 --> 41:19.600
It's just the floors slightly uneven.

41:19.600 --> 41:24.320
I would have thought that, you know, the poor concrete floors would be a lot more flat

41:24.320 --> 41:28.880
and reliable than anything in my old wood floor house.

41:28.880 --> 41:30.360
You would be amazed.

41:30.360 --> 41:32.920
It's really a factor of distance, right?

41:32.920 --> 41:37.920
Your factory floor is so massive that's a 0.1 degree difference turns into, you know,

41:37.920 --> 41:38.920
a few feet.

41:38.920 --> 41:39.920
Yeah.

41:39.920 --> 41:41.520
So it's quite surprising.

41:41.520 --> 41:46.520
So tell me this, are there any particular, you know, beyond the things we've talked about

41:46.520 --> 41:54.480
in the computer vision domain, are there any trends or research or technologies that

41:54.480 --> 42:02.120
you're tracking in the machine learning, deep learning AI domain that kind of have

42:02.120 --> 42:05.840
you excited for the implications to robotics?

42:05.840 --> 42:06.840
Yeah.

42:06.840 --> 42:14.080
So what we're taking a close look at is understanding maps and understanding the behavior of mapping

42:14.080 --> 42:15.440
and positioning.

42:15.440 --> 42:20.480
So can we, for example, understand that here's a factory floor and just off the factory

42:20.480 --> 42:22.920
floor, there's a lunchroom, for example.

42:22.920 --> 42:23.920
Okay.

42:23.920 --> 42:27.600
And as a human, you would assume that somewhere between 11 o'clock in the morning and

42:27.600 --> 42:28.600
1 p.m.

42:28.600 --> 42:31.280
in the afternoon, the lunchroom is going to be very busy.

42:31.280 --> 42:35.120
So don't bother to walk through there because you know it's going to be busy.

42:35.120 --> 42:38.960
And so can we apply that kind of concept to our understanding of maps to the robots

42:38.960 --> 42:39.960
understanding maps?

42:39.960 --> 42:44.240
So as they're driving around, they collect this data, they send it up, and then our processes

42:44.240 --> 42:49.520
to system can understand that this area is very busy at this time.

42:49.520 --> 42:53.280
Even though I have no vision of it, I know from last week and the week before that this

42:53.280 --> 42:55.520
area is probably too busy for me to plan through.

42:55.520 --> 42:58.000
So I'm just going to avoid it without even trying.

42:58.000 --> 43:02.040
So that's definitely an area that we're looking into is understanding maps and kind of gaining

43:02.040 --> 43:07.920
a more fundamental level understanding of how space is changing throughout the day.

43:07.920 --> 43:08.920
Okay.

43:08.920 --> 43:11.840
And that example that I give is kind of most easy to understand.

43:11.840 --> 43:17.600
But there's also, especially small factories and small kind of cell-based manufacturing.

43:17.600 --> 43:19.080
Things change every day.

43:19.080 --> 43:21.160
This day you're making cases for a cell phone.

43:21.160 --> 43:27.480
The next day you're making, I don't know, car dashboard accessories, that kind of stuff.

43:27.480 --> 43:33.200
So people, but also their equipment is moving around on the factory floor and having the

43:33.200 --> 43:38.520
robot understand that this drill press that was here yesterday is now over here, but it's

43:38.520 --> 43:39.680
the same object.

43:39.680 --> 43:42.680
So when I ask you to go to the drill press, you don't go to the old location, you go to

43:42.680 --> 43:45.000
the new location.

43:45.000 --> 43:49.000
So again, those are things that mapping and understanding of the space is something that

43:49.000 --> 43:50.600
we're really looking into.

43:50.600 --> 43:57.640
So you're about to head off to ICRA, which is in Singapore this year.

43:57.640 --> 44:00.880
That's the International Conference on Robotics and Automation.

44:00.880 --> 44:01.880
Is that right?

44:01.880 --> 44:02.880
That's correct.

44:02.880 --> 44:04.720
What are you excited to see there?

44:04.720 --> 44:05.720
Oh.

44:05.720 --> 44:10.520
Well, all of it pretty much.

44:10.520 --> 44:14.640
We've seen quite a large growth in these two kind of fields.

44:14.640 --> 44:17.680
One is mobile manipulation.

44:17.680 --> 44:23.200
So not only having these human safe arms working in their cell, but actually moving from

44:23.200 --> 44:24.800
cell to cell.

44:24.800 --> 44:31.280
And so one of our industrial partners, what they did is they're working on very high precision,

44:31.280 --> 44:35.160
very trackable manufacturing.

44:35.160 --> 44:40.360
So they're making jet engine parts or nuclear reactor parts.

44:40.360 --> 44:46.120
And so every single part needs to have a history and needs to be checked every single step.

44:46.120 --> 44:48.920
So one of our autos is carrying around a manipulator.

44:48.920 --> 44:53.120
The manipulator will come up to a station, take a block of aluminum, put it in a CNC and

44:53.120 --> 44:58.920
see, start it, wait for it to finish, take it out, bring it to the measuring station, measure

44:58.920 --> 45:01.640
it, label it, track it in their database.

45:01.640 --> 45:04.840
And so we've been working on that for a little while, but now we're starting to see a lot

45:04.840 --> 45:07.760
of different companies take a crack at that same problem.

45:07.760 --> 45:12.680
Is can we make multi kind of flexible manipulation?

45:12.680 --> 45:15.880
And the other big trend we're seeing is survey robotics.

45:15.880 --> 45:22.000
So remote inspection of pipelines and power lines and power stations and places where it's

45:22.000 --> 45:26.360
just very far to get to and you don't really want a human there all the time.

45:26.360 --> 45:30.080
But you do want to have high quality data come back from it.

45:30.080 --> 45:35.720
And so those are two fields that we see, even large industrial partners, or just large

45:35.720 --> 45:37.840
companies in general.

45:37.840 --> 45:42.560
For example, DJI got started in the kind of consumer space, but they're really making

45:42.560 --> 45:47.480
inroads in the, can they do remote survey of data with their drones?

45:47.480 --> 45:49.400
So that should be pretty interesting as well.

45:49.400 --> 45:54.160
And I'm starting to see, there are a number of companies that are trying to tackle the

45:54.160 --> 46:00.880
indoor industrial drone problem or space.

46:00.880 --> 46:07.560
For example, flying around warehouses or supermarkets to, you know, hopefully overnight

46:07.560 --> 46:16.000
where no one's in there to take inventory, which is a super expensive process for companies.

46:16.000 --> 46:17.000
Yeah.

46:17.000 --> 46:24.160
And you guys have, are you, are you less far, I presume, I should say, that given that

46:24.160 --> 46:29.960
you have, you know, auto motors, which is kind of the commercialization and kind of industrial

46:29.960 --> 46:40.640
facing company focused on the, you know, indoor materials handling platforms, are you kind

46:40.640 --> 46:46.760
of moving in a commercial direction with the UAV and the USB, the aerial and the seaborne

46:46.760 --> 46:53.800
vehicles as well, or are those more research oriented for, you know, the foreseeable future?

46:53.800 --> 46:59.120
So definitely the unmanned aerial vehicles tend to be a little bit more research focused

46:59.120 --> 47:04.320
for now, at least on our side, simply the reason being that the main preventor from them

47:04.320 --> 47:08.360
being used for most tasks is just battery life.

47:08.360 --> 47:09.360
Yeah.

47:09.360 --> 47:12.800
So if you're doing aerial surveys and you're doing aerial surveys of data, that's great.

47:12.800 --> 47:16.400
15, 20 minutes, that's all pretty much you need to collect the data you need.

47:16.400 --> 47:17.400
Yeah.

47:17.400 --> 47:21.680
But if you're doing something like store inventory, you need to do an hour or two hours.

47:21.680 --> 47:22.680
Right.

47:22.680 --> 47:24.480
And it's just not really feasible.

47:24.480 --> 47:29.280
On the other hand, though, the unmanned surface vehicle are heron.

47:29.280 --> 47:35.120
That's an interesting case because we had actually done some work there, becoming a data

47:35.120 --> 47:43.360
provider to provide data for municipalities and companies about tailings ponds and overflow

47:43.360 --> 47:47.520
ponds in those kind of places where you need to know the depth and water quality of a small

47:47.520 --> 47:48.520
body of water.

47:48.520 --> 47:49.520
Okay.

47:49.520 --> 47:53.880
And so the idea would be that different companies would take this heron out to a site, do

47:53.880 --> 47:59.600
an automated survey, a GPS guided automated survey of the depth and quality of a pond.

47:59.600 --> 48:03.480
And then that data is uploaded to us and we provide this data as a service.

48:03.480 --> 48:05.720
Oh, super interesting.

48:05.720 --> 48:08.880
So that's something that we're looking into as well.

48:08.880 --> 48:13.920
So that's actually quite a lot closer to a real direct commercial application where the

48:13.920 --> 48:17.360
user is using it just as another piece of equipment and not really worrying about it

48:17.360 --> 48:18.360
being a robot.

48:18.360 --> 48:19.360
Oh nice.

48:19.360 --> 48:25.680
And maybe switching gears a little bit, you are kind of active in the open source robotics

48:25.680 --> 48:26.680
community.

48:26.680 --> 48:32.560
There's a robot, ROS is robotics operating system or robot operating system that you've done

48:32.560 --> 48:33.920
quite a bit of work with.

48:33.920 --> 48:39.600
And is that what's the intersection with with ClearPath is through the ClearPath platforms

48:39.600 --> 48:42.920
run some version of ROS?

48:42.920 --> 48:43.920
Yeah.

48:43.920 --> 48:44.920
Yeah.

48:44.920 --> 48:52.160
So ROS is this idea of can we apply the Linux ideal where Linux is free and open source

48:52.160 --> 48:56.680
and you can run it on anything, can we apply that to robots as well?

48:56.680 --> 49:01.760
So the idea behind ROS is it runs predominantly on top of Ubuntu, but it can run on a few

49:01.760 --> 49:03.360
other platforms.

49:03.360 --> 49:09.200
And it will talk the same language from microcontrollers all the way up to servers.

49:09.200 --> 49:10.440
Okay.

49:10.440 --> 49:15.240
And so that makes assembling a robot very, very easy because you can add on an arbitrary

49:15.240 --> 49:19.840
microcontroller, an almost an arbitrary sensor because most of them now have supported

49:19.840 --> 49:26.960
ROS drivers and assemble your robot almost as Lego bricks, which is what we do.

49:26.960 --> 49:31.680
And so we became a very early supporter of ROS and we continue to be one of the kind of

49:31.680 --> 49:36.680
the largest companies that provide all of our platforms are ROS compatible, all of our

49:36.680 --> 49:37.680
research platforms.

49:37.680 --> 49:39.440
Oh nice.

49:39.440 --> 49:48.560
And is there a standard interface between ROS and kind of higher level machine learning

49:48.560 --> 49:53.680
AI stuff or are those kind of two separate things at this point?

49:53.680 --> 49:57.920
At this point they're a little bit separate, but they're starting to get there.

49:57.920 --> 50:04.160
So one example is everybody should be familiar with the OpenCV framework for image processing.

50:04.160 --> 50:08.320
And OpenCV actually came out of Willa Garage, which also started ROS.

50:08.320 --> 50:09.320
Okay.

50:09.320 --> 50:13.800
So at the same time, Willa Garage created the kind of open source robotics movement and

50:13.800 --> 50:18.040
a lot of these early fundamental libraries for image processing, a little bit of the machine

50:18.040 --> 50:20.840
learning work and those kind of tasks.

50:20.840 --> 50:21.840
Hmm.

50:21.840 --> 50:22.840
Interesting.

50:22.840 --> 50:23.840
Interesting.

50:23.840 --> 50:24.840
Yeah.

50:24.840 --> 50:30.760
And I guess this is also an aside, but I did a, I got in on a Kickstarter for this little

50:30.760 --> 50:33.720
miniature LiDAR system called Scance.

50:33.720 --> 50:35.040
Have you ever come across that?

50:35.040 --> 50:36.040
I believe so.

50:36.040 --> 50:37.040
Yeah.

50:37.040 --> 50:41.360
I haven't done anything with it, which is the case with most of my electronic Kickstarter

50:41.360 --> 50:51.400
projects, but it looks like it's pretty cool that you can make all of the components in

50:51.400 --> 51:01.720
this ecosystem from the platforms to the UAVs to the sensors, including LiDAR, they've

51:01.720 --> 51:12.320
just become so affordable and miniature as it's become very accessible, which I think contributes

51:12.320 --> 51:18.320
to people being able to do lots of things and play with different ideas, including kind

51:18.320 --> 51:20.320
of the ML and AI angle.

51:20.320 --> 51:21.320
Yeah.

51:21.320 --> 51:22.320
Absolutely.

51:22.320 --> 51:25.600
The cost of LiDAR is coming down quite significantly.

51:25.600 --> 51:33.400
But more than that, really, the real enabler in my mind is the rapid growth of processing

51:33.400 --> 51:35.400
power for images.

51:35.400 --> 51:39.840
So not only the libraries are designed to run on CPUs, but also GPU-based processing

51:39.840 --> 51:46.040
of images, because as nice as LiDARs are, and we use them almost everywhere, fundamentally,

51:46.040 --> 51:52.040
we build the world around humans and humans mostly use vision for their navigation.

51:52.040 --> 51:55.440
And so almost everything we've structured, we've structured around this idea of you

51:55.440 --> 52:01.200
have a roughly human sized object that can see roughly human distances, and that's how

52:01.200 --> 52:04.000
we get around it.

52:04.000 --> 52:10.600
So fundamentally, I believe that in the long run, in the next 50 to 100 years, most of

52:10.600 --> 52:14.680
our systems will start to use cameras as their predominant source.

52:14.680 --> 52:22.160
And that's beneficial even today, because a relatively decent camera is maybe $20.

52:22.160 --> 52:26.880
And it's more than good enough to do cutting edge research.

52:26.880 --> 52:31.800
And you know, you can start with two cameras that are the same model.

52:31.800 --> 52:35.600
You put them on a ruler, so they're a measured distance apart.

52:35.600 --> 52:38.320
You calibrate out their small imperfections.

52:38.320 --> 52:43.440
And then you attach it to a consumer grade laptop, and you have a cutting edge stereoscopic

52:43.440 --> 52:46.160
vision platform that you can start doing research on.

52:46.160 --> 52:47.160
That's awesome.

52:47.160 --> 52:53.320
Any particular, you know, for folks that are interested in the hobbyist angle here, any

52:53.320 --> 52:59.600
particular links or pointers or places that you find are helpful for folks getting started?

52:59.600 --> 53:00.600
Yeah.

53:00.600 --> 53:06.880
Well, I'll kind of suggest that people visit the Clear Path Robotics Ross 101 series of

53:06.880 --> 53:10.800
tutorials, where we can help people get started on Ross.

53:10.800 --> 53:14.840
We have a little virtual machine, so you don't have to install Ubuntu on your home computer.

53:14.840 --> 53:17.480
You can kind of just run it virtually.

53:17.480 --> 53:21.600
And all of our robots are simulated there, along with sensors.

53:21.600 --> 53:26.520
So you could actually simulate, for example, Husky with a stereo camera, with a LiDAR,

53:26.520 --> 53:31.080
with an IMU, and move around a virtual map and start doing mapping, navigation, path

53:31.080 --> 53:34.840
planning, all of that for free on any laptop.

53:34.840 --> 53:36.560
Oh, that's amazing.

53:36.560 --> 53:38.640
I'm definitely going to have to do that.

53:38.640 --> 53:40.760
Well, you brought up simulation.

53:40.760 --> 53:44.800
That's a topic that I want to dig into also, but I fear we are bumping up against

53:44.800 --> 53:51.080
a time constraint here, so I'm going to save that for another conversation.

53:51.080 --> 53:56.560
But thank you so much for taking the time to speak with me about your work.

53:56.560 --> 54:03.200
It is really fascinating stuff that kind of speaks to the, you know, the kid geek playing

54:03.200 --> 54:06.360
with Legos and trying to make stuff automated.

54:06.360 --> 54:08.480
I really appreciate you taking the time out.

54:08.480 --> 54:09.480
Absolutely.

54:09.480 --> 54:10.480
Thank you very much.

54:10.480 --> 54:11.480
All right.

54:11.480 --> 54:19.360
All right, everyone, that's our show for today.

54:19.360 --> 54:24.520
Thanks so much for listening and for your continued support, comments, and feedback.

54:24.520 --> 54:29.000
We're excited to hear what you think about this show and the industrial AI series we've

54:29.000 --> 54:30.960
just kicked off.

54:30.960 --> 54:35.560
I'd also like to thank our sponsor, Banzai, once again, be sure to check out what they're

54:35.560 --> 54:39.120
up to at vons.ai.

54:39.120 --> 54:43.720
Speaking of Banzai, they'll also be at the O'Reilly AI conference in New York City later

54:43.720 --> 54:45.080
this month.

54:45.080 --> 54:51.880
If you'd like to attend, you can save 20% on registration by using our special code PC

54:51.880 --> 54:55.480
Twimble, PC-TW-I-M-L.

54:55.480 --> 54:59.880
We'll include the code and a link to the registration page in the show notes.

54:59.880 --> 55:02.440
I'd love to meet up with listeners at the conference.

55:02.440 --> 55:07.600
In fact, I'm planning a community meetup during the event and I'll share details as soon

55:07.600 --> 55:09.840
as they've been ironed out.

55:09.840 --> 55:16.280
As usual, the notes for this episode can be found at twimbleai.com slash talk slash

55:16.280 --> 55:18.400
27.

55:18.400 --> 55:23.440
For information on industrial AI, my report on the topic or the industrial AI podcast

55:23.440 --> 55:28.360
series, visit twimbleai.com slash industrial AI.

55:28.360 --> 55:32.560
As always, remember to post your favorite quote or takeaway from this episode and we'll

55:32.560 --> 55:34.840
send you a laptop sticker.

55:34.840 --> 55:38.920
You can post them as comments to the show notes page via Twitter, you are following us

55:38.920 --> 55:42.960
at at Twimbleai, aren't you, or via our Facebook page.

55:42.960 --> 56:10.200
Thanks again for listening and catch you next time.

