1
00:00:00,000 --> 00:00:10,520
All right, everyone. Welcome to another episode of the Twomo AI podcast. I am your host, Sam

2
00:00:10,520 --> 00:00:16,560
Charrington. And today, I'm joined by Sebastian Rashka, an assistant professor of statistics

3
00:00:16,560 --> 00:00:23,440
at the University of Wisconsin-Madison, as well as the lead AI educator at Grid AI. Sebastian,

4
00:00:23,440 --> 00:00:28,000
I'm really looking forward to digging into our chat. We'll touch on your research, as

5
00:00:28,000 --> 00:00:33,120
well as your work in ML education. Welcome to the podcast. Yeah, thank you, Sam, for the kind

6
00:00:33,120 --> 00:00:39,120
invitation to be on your podcast. I'm super excited to chat about research, education, or whatever.

7
00:00:39,120 --> 00:00:44,640
Yeah, you feel like chatting about today. Fantastic. Well, let's start by having you share a little

8
00:00:44,640 --> 00:00:50,720
bit about your background and introducing yourself to our audience. How do you end up in machine learning?

9
00:00:50,720 --> 00:00:55,680
Yeah, that is a good question. It goes back many years now, but when I was a PhD student,

10
00:00:55,680 --> 00:01:02,400
I was studying computational biology. And yeah, my professor back then, she recommended me

11
00:01:02,400 --> 00:01:08,880
taking this introduction to statistical pattern recognition class, which was a grad level class

12
00:01:08,880 --> 00:01:13,680
in the computer science department, and really that got me hooked. And so that was mostly about

13
00:01:13,680 --> 00:01:19,760
like Bayesian statistics and yeah, Bayesian methods for pattern recognition. But yeah, somehow it

14
00:01:19,760 --> 00:01:25,040
clicked with me and I found it super fascinating that you can kind of like teach computers how to

15
00:01:25,040 --> 00:01:30,800
recognize patterns in data. And from there on, I was just, yeah, super hooked and studying machine

16
00:01:30,800 --> 00:01:38,000
learning. And yeah, then I ended up writing a book back then in 2015, Python machine learning,

17
00:01:38,000 --> 00:01:43,280
joined the University of Wisconsin Medicine in the statistics department. And yeah, now at

18
00:01:43,280 --> 00:01:51,120
Grid AI as a educator. So yeah, it has been quite a journey. But yeah, I'm still as much excited as

19
00:01:51,120 --> 00:01:56,320
I was back then about machine learning. So there's so much cool stuff coming out too. So it's

20
00:01:56,320 --> 00:02:03,520
an exciting field. Yeah, absolutely. And your work in particular around education has been

21
00:02:03,520 --> 00:02:08,560
inspirational to a lot of people. I mentioned that you are a great follow on Twitter. We'll link to

22
00:02:08,560 --> 00:02:19,360
your Twitter handle profile in the show notes. And when I think about your take, it kind of reminds me

23
00:02:19,360 --> 00:02:26,080
of a little bit of the top down approach that like a Jeremy Howard advocates, but also with

24
00:02:26,720 --> 00:02:31,840
that academic grounding of a bottom up approach. You sooner do a really good job of fusing those

25
00:02:31,840 --> 00:02:38,960
together. Tell, tell us a little bit about, you know, what got you excited about the teaching aspect

26
00:02:38,960 --> 00:02:46,480
of machine learning and your philosophy around that. Yeah, so yeah, it's kind of like related to what

27
00:02:46,480 --> 00:02:52,720
you just mentioned, like fusing the academic approaches, let's say the mathematical details.

28
00:02:52,720 --> 00:02:59,840
And then also the practical aspect. So personally, I must say I'm really I enjoy coding. I really like

29
00:02:59,840 --> 00:03:05,120
programming. It's something I do for fun on the weekends and like also contributing to open

30
00:03:05,120 --> 00:03:11,120
source software. But then I'm also sometimes curious about certain things how they work. So it's

31
00:03:11,120 --> 00:03:17,280
like maybe coming from academia, you are like taught to investigate, to make sure you understand

32
00:03:17,280 --> 00:03:22,400
everything you're using. And yeah, I'm trying to kind of bridge the gap between the two that it's

33
00:03:22,400 --> 00:03:28,320
fun where you get to code things and these types of things. But at the same time, you also develop an

34
00:03:28,320 --> 00:03:33,280
understanding of how these things work because they make you, let's say, more powerful in terms of,

35
00:03:33,280 --> 00:03:38,080
you know, what to change and what to expect. It's kind of like a deeper level of understanding. And

36
00:03:38,080 --> 00:03:42,240
I think it's also very satisfying to know what's going on when you change certain things or when

37
00:03:42,240 --> 00:03:48,640
you code something up. And yeah, for teaching, that's a good point. I try also to combine the two.

38
00:03:49,920 --> 00:03:54,480
It's, I mean, everyone has different preferences. But when I was taking classes, I enjoy

39
00:03:55,440 --> 00:04:01,120
sometimes the mathematical details. But it's something I also, I need to learn on my own time.

40
00:04:01,120 --> 00:04:06,240
It's for me very hard to sit in a, let's say, lecture and just see mathematical equations.

41
00:04:06,240 --> 00:04:11,840
It's often too fast for me, even. So in that case, I try to balance it that I don't, let's say,

42
00:04:11,840 --> 00:04:17,680
have slides full of equations that I kind of change it up with big picture concepts. And also,

43
00:04:17,680 --> 00:04:23,360
yeah, code examples, because I think code examples are at least helping me a lot like in terms of

44
00:04:23,360 --> 00:04:29,200
solidifying things I learned. And yeah, when I'm teaching at the university, I also noticed that

45
00:04:29,200 --> 00:04:35,360
students like this a lot. So yeah, it is, it is a fun part. I think it's the reward. You learn

46
00:04:35,360 --> 00:04:39,680
something. And then you apply it and see, well, this is actually so cool. I want to do more of it.

47
00:04:39,680 --> 00:04:45,040
And then I think if you do it like this way, where you show cool examples and empower people to

48
00:04:45,040 --> 00:04:50,800
develop cool applications, people will automatically be motivated to learn more details. But if you

49
00:04:50,800 --> 00:04:55,200
start the other way around, if you teach people like the nitty gritty details, I think it's easy to

50
00:04:55,200 --> 00:05:00,480
lose track and get bored. That happened to me like with mathematics when I was like in high school

51
00:05:00,480 --> 00:05:05,680
taking math classes. I wasn't really excited about that because it was like a bunch of numbers.

52
00:05:05,680 --> 00:05:11,280
I can move them around cool. But why is that useful? Why is that cool? And yeah, with coding,

53
00:05:11,280 --> 00:05:15,840
you can immediately see what you can do with it if you develop like your machine learning applications.

54
00:05:16,400 --> 00:05:22,640
And yet in my class, I make sure that there's a decent portion of that. So I also have these

55
00:05:22,640 --> 00:05:28,480
class projects I like my students to work on, where at the, yeah, I would say middle of the semester,

56
00:05:28,480 --> 00:05:34,800
the students submitted proposal, project proposal. It could be anything they are interested in.

57
00:05:34,800 --> 00:05:39,760
It just has to be related to machine learning or deep learning. And then they get to work on it

58
00:05:39,760 --> 00:05:44,400
for the rest of the semester, while of course taking the lectures. But I think also based on

59
00:05:45,440 --> 00:05:50,240
hearing from students, that was really the fun part because at the end, they work in teams of

60
00:05:50,240 --> 00:05:55,440
three. At the end, they have like the teamwork experience. They develop something. They have

61
00:05:55,440 --> 00:05:59,520
something to show for us. So we also have presentations and class where the students get to talk

62
00:05:59,520 --> 00:06:04,320
about their project in front of other students. And then in the end, they have, I mean, I make this

63
00:06:04,320 --> 00:06:08,720
voluntary because I don't want to force anyone, but they can share the project that's there on GitHub

64
00:06:09,200 --> 00:06:13,440
publicly. And a lot of students like that because then when they apply for jobs, they can actually show

65
00:06:14,800 --> 00:06:18,800
where they apply that can show, okay, I've actually done something cool here with machine learning.

66
00:06:18,800 --> 00:06:23,600
I know how it works. I mean, of course, you can't expect in a semester to build

67
00:06:23,600 --> 00:06:28,400
like this huge AI system, but it is at least something cool. I mean, there were so many cool

68
00:06:28,400 --> 00:06:31,760
projects in the last couple of years that I've seen. I was really impressed. And I think

69
00:06:32,400 --> 00:06:36,720
it's fun for me to see all of that, but also fun for the students to, yeah, um,

70
00:06:36,720 --> 00:06:41,200
practically work on that and develop their skills. So yeah. So that's my approach to teaching

71
00:06:41,200 --> 00:06:45,840
basically. I make sure, well, I want to make sure that students also get this practical experience

72
00:06:45,840 --> 00:06:51,920
because I think it's also very important and motivating. Do you think about it from a pedagogical

73
00:06:51,920 --> 00:06:56,640
perspective or is this just kind of how it all naturally came together for you? I'm not that good

74
00:06:56,640 --> 00:07:02,400
at, let's say, reading literature in education and figuring out what is the best approach to teaching.

75
00:07:02,400 --> 00:07:08,960
For me, I mostly go with gut feeling. Honestly, how I do these things is really, I'm thinking about

76
00:07:08,960 --> 00:07:14,320
what do I like? How do I like, let's say, consuming information? Is this too much math? Is this

77
00:07:14,320 --> 00:07:18,960
too much code? Is is that the right balance? Would I be excited about this? And it's basically just

78
00:07:18,960 --> 00:07:24,800
that like being myself motivated and excited and then just going with it. That's awesome. That's

79
00:07:24,800 --> 00:07:32,560
awesome. You did a workshop or participated in a workshop teaching machine learning at ECML

80
00:07:33,280 --> 00:07:40,560
last year and did a session deeper learning by doing integrating hands-on research projects into

81
00:07:40,560 --> 00:07:45,440
a machine learning course. And it sounds like that in some ways, you're sharing all the things that

82
00:07:45,440 --> 00:07:52,960
we just touched on your experiences teaching. Are there for folks that are putting maybe putting

83
00:07:52,960 --> 00:07:59,520
together courses? Are there interesting things that you've learned about how to integrate projects

84
00:07:59,520 --> 00:08:06,800
in? Are there things that don't or is it more about be sure to focus on and include practical

85
00:08:06,800 --> 00:08:13,360
hands-on projects versus not? I think this is one approach of doing it, but it's maybe not the

86
00:08:13,360 --> 00:08:21,600
perfect approach. But one, I would say, one downside of a project is that if you imagine students

87
00:08:21,600 --> 00:08:26,080
taking the first machine learning class or deep learning class, they don't know what you can do

88
00:08:26,080 --> 00:08:32,960
with machine learning. So the one challenge I found is, yeah, you ideally want to have more time

89
00:08:32,960 --> 00:08:38,240
so that students can work on the project more. I mean, let's say stretching it all out over the

90
00:08:38,240 --> 00:08:43,920
whole semester, but then the problem becomes that if people have not encountered machine learning

91
00:08:43,920 --> 00:08:48,080
or have learned about machine learning, it's hard for them to propose a project centered around

92
00:08:48,080 --> 00:08:53,120
a machine learning because at that point they may not know what's feasible or not. So in this

93
00:08:53,120 --> 00:08:59,520
proposal that I make them submit at this middle of the semester, I also give them feedback. But yeah,

94
00:08:59,520 --> 00:09:05,280
it is the middle of the semester where it's kind of like a little bit late. And then also one downside

95
00:09:05,280 --> 00:09:10,000
is that they haven't seen all the methods yet that we get to cover in class because the class

96
00:09:10,000 --> 00:09:14,400
still goes on. But despite that, I mean, these are little downsides. I think there are lots of

97
00:09:14,400 --> 00:09:19,520
upsides where people really get to, yeah, get to use the methods that you learn about. And

98
00:09:20,320 --> 00:09:27,200
here I find also it's helpful to have homework examples where students get like also

99
00:09:28,160 --> 00:09:34,080
understand the details where in the beginning, it might be harder to really develop a machine learning

100
00:09:34,080 --> 00:09:39,280
that's a pipeline from scratch because if you're new to it, there are so many things to think about.

101
00:09:39,280 --> 00:09:45,360
So what I found works well is providing template code and then leaving out let's say core parts

102
00:09:45,360 --> 00:09:53,280
of the code. Like, for example, here we have the framework of a model at another, let's say,

103
00:09:53,280 --> 00:09:58,640
activation function or something like that, something little small. And the students will think about

104
00:09:58,640 --> 00:10:03,280
these things like how to do that, but then also have these templates that they can reuse and

105
00:10:03,280 --> 00:10:09,440
let's say their class projects as a framework. So I think this is helpful to provide students with

106
00:10:10,160 --> 00:10:16,000
ample examples like frameworks, frameworks in terms of code that is self-contained, it works.

107
00:10:16,000 --> 00:10:21,360
Of course, they will adopt that for their project, but at least something to help them so that

108
00:10:21,360 --> 00:10:26,240
they don't have to do everything from scratch. Because I think this is one intimidating thing about

109
00:10:26,240 --> 00:10:29,920
machine learning. There are so many tools and so many frameworks, especially for deep learning,

110
00:10:29,920 --> 00:10:35,440
that it's a lot of code and it can sometimes be a little bit overwhelming. And then kind of like

111
00:10:35,440 --> 00:10:40,080
managing that by showing a lot of examples and yet providing templates, I think, helps.

112
00:10:40,080 --> 00:10:45,040
What do you find is most challenging for folks just getting started with machine learning?

113
00:10:45,040 --> 00:10:50,880
For people who want to use machine learning, one challenge is how do I get my data into this

114
00:10:50,880 --> 00:10:55,360
machine learning model? I mean, there are, of course, there's machine learning and let's say the

115
00:10:55,360 --> 00:11:01,120
non-neuronetwork deep learning part, where these are also two different cans of worms, but

116
00:11:01,120 --> 00:11:05,840
the second learning on the surface looks very simple, very simple to use. It's like a few lines

117
00:11:05,840 --> 00:11:11,600
of code where you can fit your classifier, but I think the main challenge is really preparing your data.

118
00:11:12,800 --> 00:11:17,680
Especially if you don't want to do something like iris floor classification. I think the

119
00:11:17,680 --> 00:11:21,920
biggest challenge is students have so many cool ideas for their class projects, but then it's

120
00:11:21,920 --> 00:11:27,360
always about how do we make sure this data is the right format for machine learning classifier.

121
00:11:27,360 --> 00:11:32,480
For example, if we are using second learn, this is usually a tabular data set, so we make sure

122
00:11:32,480 --> 00:11:37,680
we have class labels and then also the columns, the feature columns. And for deep learning, okay,

123
00:11:37,680 --> 00:11:44,960
it's a little bit more, I would say raw where you can work with image data directly or text data,

124
00:11:44,960 --> 00:11:50,880
but still thinking about how to organize your data that is one challenge, I think.

125
00:11:50,880 --> 00:11:56,960
In chatting earlier, you emphasize, you really take a lot of care to make sure you walk through

126
00:11:56,960 --> 00:12:02,160
kind of classical machine learning before diving into deep learning. Talk a little bit more about

127
00:12:02,160 --> 00:12:09,840
that and your experience is there. Yeah, that is a good point. So in my book, it's a pretty long book,

128
00:12:09,840 --> 00:12:18,320
so it could have been two books almost. So it is a thick book. Yeah, we'll keep you busy for a while,

129
00:12:18,320 --> 00:12:24,320
but it is essentially, you can think of it as two books, a machine learning book where you get to

130
00:12:24,320 --> 00:12:29,760
learn about the basics of machine learning, model evaluation, hyperparameter tuning, all the basics

131
00:12:29,760 --> 00:12:36,240
in the context of psychic learn and tabular data sets. And then the second half, I think chapter

132
00:12:36,240 --> 00:12:41,680
11 or 12 is the turning point where I explain how to implement a neural network from scratch using

133
00:12:41,680 --> 00:12:50,000
NumPy and then from that, it starts with the deep learning parts. And yeah, so I feel like, okay,

134
00:12:50,000 --> 00:12:55,680
this could have been two books, but nowadays, I think this deep learning is so powerful and so

135
00:12:55,680 --> 00:13:00,720
let's say exciting that a lot of people sometimes forget that there is also non-deep learning

136
00:13:00,720 --> 00:13:05,920
machine learning where I think there's still a place for that if you especially look on Kaggle,

137
00:13:05,920 --> 00:13:12,080
a lot of competitions are still won by XGBoost, which is a grade in boosting classifier for tabular data.

138
00:13:12,080 --> 00:13:18,800
And honestly, I would rather say it should be kind of like, I mean, machine learning is still

139
00:13:18,800 --> 00:13:23,440
like the non-deep learning part a good baseline, but also depending really on your data set,

140
00:13:23,440 --> 00:13:29,040
how much data do you have and what format your data is, you want to use one approach over the other.

141
00:13:29,040 --> 00:13:33,680
For example, for smaller data sets or tabular data sets, machine learning might be, let's say,

142
00:13:33,680 --> 00:13:38,080
the way to go, whereas for a large image data set, you may want to go with deep learning,

143
00:13:38,080 --> 00:13:43,760
but still you could apply a machine learning classifier as a baseline, like logistic regression

144
00:13:43,760 --> 00:13:48,240
or something like that. So in that case, with that book, having both machine learning and deep

145
00:13:48,240 --> 00:13:53,600
learning in there, I think it is kind of like a reminder to people, hey, it's not always about

146
00:13:53,600 --> 00:13:58,240
deep learning, also consider, let's say, sometimes machine learning make, making sure that people who

147
00:13:58,240 --> 00:14:03,120
say have their first encounter with machine learning and deep learning, that they know the big picture

148
00:14:03,120 --> 00:14:10,560
basically, that there is more than just new networks also. You mentioned earlier just the proliferation

149
00:14:10,560 --> 00:14:18,400
of frameworks in ML and DL. Do you, how do you advise folks when they're thinking about projects,

150
00:14:18,400 --> 00:14:22,720
you know, where to start, what frameworks and tools choose that kind of thing? Oh, yeah, there is

151
00:14:22,720 --> 00:14:28,960
also a big question. I think, I mean, again, there is no right or wrong, I would say there are

152
00:14:28,960 --> 00:14:35,840
just different tools for different tasks. Personally, I started with, actually, I implemented a lot of

153
00:14:35,840 --> 00:14:40,800
things from scratch because this is with learning experience, but in the real world, I advise people

154
00:14:40,800 --> 00:14:47,120
to use something that is well supported, well developed because it's not only about, let's say,

155
00:14:47,120 --> 00:14:52,800
having it back free or error free, but also about efficiency. So, and there are also, if you use

156
00:14:52,800 --> 00:14:57,760
something that is out there, there's also usually a wider ecosystem and a community that you can

157
00:14:57,760 --> 00:15:03,520
ask for questions and help, which I think is also very important because nowadays, there are

158
00:15:03,520 --> 00:15:09,200
millions of algorithms and it's hard to find sometimes which one is, let's say, the right one,

159
00:15:09,200 --> 00:15:16,000
and sometimes just asking people, chatting about it is helpful. And yeah, for regular machine learning,

160
00:15:16,000 --> 00:15:20,880
that is not deep learning, I would advise scikit-learn still. I think it's the, maybe most mature

161
00:15:20,880 --> 00:15:26,480
library out there. People in my department may disagree because my department is small, let's say,

162
00:15:26,480 --> 00:15:34,320
R based the language R, but I personally, I think going with Python, if you want to do machine learning

163
00:15:34,320 --> 00:15:38,400
or deep learning is the way to go. So, one consideration is, yeah, which programming language,

164
00:15:38,400 --> 00:15:45,280
I think nowadays, 95% also is done in Python. And then, scikit-learn for machine learning,

165
00:15:45,280 --> 00:15:50,960
and deep learning becomes a little bit trickier because we have a lot of more frameworks out there.

166
00:15:50,960 --> 00:15:57,920
So, back then, I remember my first book, I covered Ciano, which I think is still around the people

167
00:15:57,920 --> 00:16:04,160
at PMC3. I think they adopted it and still maintain it. But I don't think people use it for

168
00:16:04,160 --> 00:16:10,640
deep learning anymore. So, I think it was like 2000, might be 15, 16, where TensorFlow came around,

169
00:16:10,640 --> 00:16:16,400
which is maybe the first big breakthrough library for deep learning, where it became really,

170
00:16:16,400 --> 00:16:21,200
really popular. And then, yeah, there were other libraries like MXNet, Chainer, and so forth,

171
00:16:21,200 --> 00:16:26,720
and then eventually PyTorch in 2017. And I think nowadays, when I look at the trends on

172
00:16:26,720 --> 00:16:32,240
papers with code and just reading research papers and looking out there, I feel like I would say,

173
00:16:32,240 --> 00:16:39,440
like 80% now is a PyTorch. And I would say with PyTorch, it's a two-edged sort. It's a little bit

174
00:16:39,440 --> 00:16:45,280
more verbose, let's say, then carous. But at the same time, for me, it makes me more productive

175
00:16:45,280 --> 00:16:49,600
in my research. I wouldn't say it's necessary better for everything, but I feel like personally,

176
00:16:49,600 --> 00:16:55,600
it strikes a nice balance between giving you all the tools you need, but giving you also some

177
00:16:55,600 --> 00:17:00,160
control over what you're doing in case you need to modify something. Because, yeah, in my research

178
00:17:00,160 --> 00:17:06,240
projects, unless it's just an application, I sometimes want to develop my own custom layer,

179
00:17:06,240 --> 00:17:12,160
like a custom output layer or custom loss function. And PyTorch is very fast for that. And it is

180
00:17:12,160 --> 00:17:18,240
a little bit, I would say, more low level. So there are also other APIs on top of that,

181
00:17:18,240 --> 00:17:22,240
and there are a lot of APIs on top of that. But in general, I would say for deep learning,

182
00:17:22,240 --> 00:17:25,280
I would personally recommend a PyTorch. I don't know this.

183
00:17:25,280 --> 00:17:31,920
Yeah, it's interesting that you mentioned our, I don't think I've covered our a lot on the podcast,

184
00:17:31,920 --> 00:17:38,960
but we did have this really awesome panel that we did that was exploring what's the best

185
00:17:38,960 --> 00:17:43,840
programming language for machine learning. And we had representatives from, like, I think,

186
00:17:43,840 --> 00:17:51,200
eight, it was like closure and JavaScript and Swift and Scala and Julia. There's certainly a ton

187
00:17:51,200 --> 00:17:56,400
of different, what you can do, you can do machine learning and a lot of different languages.

188
00:17:56,400 --> 00:18:03,120
And starting with what you know is always a good place or not a bad place at least, but the

189
00:18:03,120 --> 00:18:09,280
Python ecosystem is certainly very strong. If I don't, if you don't mind, I can tell you two

190
00:18:09,280 --> 00:18:16,480
anecdotes. So personally, so the first thing is when I was a student back then, I also started

191
00:18:16,480 --> 00:18:23,840
with R before I started Python. And I was doing most of my statistics stuff and plotting in R.

192
00:18:23,840 --> 00:18:30,560
And then I ended up in the computational biology context. I ended up having to process some custom

193
00:18:30,560 --> 00:18:36,080
data. And that's where I learned Python just for the data wrangling. I learned a little, a

194
00:18:36,080 --> 00:18:41,840
lot of, I learned about Pearl, but Pearl was a little bit unwieldy for me. So I heard Python is

195
00:18:41,840 --> 00:18:48,320
the hot new thing. So I learned Python. And I was writing these really weird scripts. I had

196
00:18:48,320 --> 00:18:54,240
like a batch script that was calling Python for processing the data and then ingesting it into R

197
00:18:54,240 --> 00:18:59,280
to make the plots. So for a long time, I just used R for making plots, but the rest wasn't Python

198
00:18:59,280 --> 00:19:04,720
and then eventually I switched. But one thing you mentioned is about also deep learning and machine

199
00:19:04,720 --> 00:19:11,680
learning in R. I was recently at a seminar at our university where there was a talk on, it was

200
00:19:11,680 --> 00:19:18,560
in general about like a machine learning industry. And the person also presented, I don't want to say,

201
00:19:18,560 --> 00:19:23,280
like names or anything, but it was kind of funny. It was like more like a fun thing. The person

202
00:19:23,280 --> 00:19:30,240
mentioned that they were training model in TensorFlow. And they had presented the results in a

203
00:19:30,240 --> 00:19:36,000
conference in an R related conference. And I was like, how does, wait, you trained this model

204
00:19:36,000 --> 00:19:41,120
in TensorFlow, but you presented in the conference that is related to R. How did you do that? And then

205
00:19:41,120 --> 00:19:47,360
the person said, basically, don't tell anyone, but I actually, I used just the API. I did it in

206
00:19:47,360 --> 00:19:52,080
Python and just use the R API or something like that so that it can be submitted to the conference.

207
00:19:52,080 --> 00:19:58,480
But it was another hood basically a TensorFlow and Python and stuff like that. Now I see why you

208
00:19:58,480 --> 00:20:06,000
were naming names. It was kind of funny though. It's like, I mean, R is still, I mean, I think it

209
00:20:06,000 --> 00:20:11,200
has its place for sure. I think it's very user friendly, especially for statistics. And as soon as

210
00:20:11,200 --> 00:20:15,840
you need to do some, I don't know, just hypothesis testing and statistics, there is of course stats

211
00:20:15,840 --> 00:20:21,520
models, but it is just so many extra steps. And I feel like R comes with a lot of batteries included

212
00:20:21,520 --> 00:20:25,680
when it comes to statistical modeling. So I think it definitely has its place. And of course,

213
00:20:25,680 --> 00:20:31,760
like you mentioned, Julia, I also have a few colleagues who work in Julia and my colleagues

214
00:20:31,760 --> 00:20:37,280
who use Julia, they really love Julia. It's like, I think that's, it's a really nice language.

215
00:20:37,280 --> 00:20:41,840
It's just for deep learning. I think it hasn't caught on yet. And I think it's maybe like a

216
00:20:41,840 --> 00:20:46,960
chicken egg problem where you don't have the community in Julia for deep learning. And without

217
00:20:46,960 --> 00:20:51,600
the community, you don't have, let's say, the libraries that are working on frameworks and

218
00:20:51,600 --> 00:20:56,880
yeah, I think I mean, it's possible for sure. And another name I don't want to mention, but I was

219
00:20:56,880 --> 00:21:03,440
on a committee, a PhD committee, and the student did some research on deep learning, like developing

220
00:21:03,440 --> 00:21:08,240
some custom methods and was using Julia for that. And in the end, the student mentioned to me, I

221
00:21:08,240 --> 00:21:14,000
wish I had used PyTorch because it made certain things more elegant if you wanted to do things

222
00:21:14,000 --> 00:21:18,960
from scratch because you have this array type in Julia and so forth. But overall, just the leg of

223
00:21:18,960 --> 00:21:23,440
the framework, I think they have a library. I forgot the name. It's not blocks. I forgot the name.

224
00:21:23,440 --> 00:21:30,080
But it is not as mature, let's say. And then once you want to compare your methods to other

225
00:21:30,080 --> 00:21:36,400
methods, you bump into problems because yeah, you can either compare across languages or you have

226
00:21:36,400 --> 00:21:41,040
to re-implement everything. And I think that is also one important consideration you want to maybe

227
00:21:41,040 --> 00:21:46,240
look at what other people are using, not because you want to copy them, but you want to maybe also

228
00:21:46,240 --> 00:21:51,760
compare your methods, especially if you want to develop a new research method. It's good to have

229
00:21:51,760 --> 00:21:57,520
something out there that is also useful to other people. So if let's say other people you are

230
00:21:57,520 --> 00:22:02,000
using your framework, it's a higher benefit for the community if you contribute it in that

231
00:22:02,720 --> 00:22:06,800
language. But then also if you want to make sure you make progress in research, it's easier to

232
00:22:06,800 --> 00:22:12,080
compare. If you make a small change to the loss function, it is easier to compare that to the

233
00:22:12,080 --> 00:22:16,000
reference in the same framework compared to let's say another framework because then you don't know,

234
00:22:16,000 --> 00:22:21,440
is this because of numerical approximation? Is it due to something else's improvement? So yeah,

235
00:22:21,440 --> 00:22:25,520
I think there's a lot of benefits going with what everyone else is using, although

236
00:22:26,240 --> 00:22:31,520
I usually try to do something different just because it's exciting, but there are definitely

237
00:22:31,520 --> 00:22:37,680
benefits, yeah. Maybe a little bit more about the book, have you, did you find a way to incorporate

238
00:22:37,680 --> 00:22:46,320
this idea of projects into the book? That is a good question. Unfortunately, no, there are only like

239
00:22:46,320 --> 00:22:53,440
more like toy projects. The problem is, I think it's really hard to do. I mean, there have been some

240
00:22:53,440 --> 00:23:00,560
great, not necessarily machine learning books that I'm thinking of, but just computer programming

241
00:23:00,560 --> 00:23:05,920
books that the ones that I like are the ones that I like the most are the ones that kind of take this

242
00:23:05,920 --> 00:23:11,760
longitudinal project approach and it kind of flows from the beginning to the end and adds,

243
00:23:11,760 --> 00:23:17,440
you know, each chapter will add on to the project, but I've got to imagine that that's really,

244
00:23:17,440 --> 00:23:22,560
really difficult to do. Yeah, yeah. Now that you mentioned it, I remember, I was last year reading a

245
00:23:22,560 --> 00:23:27,520
book, Introduction to PyTorch, I think, or Introduction to Deep Learning with PyTorch by E. Life

246
00:23:27,520 --> 00:23:33,040
Stevens, Luca and Tiga, and Thomas Feren. And they had something I could describe. The first

247
00:23:33,040 --> 00:23:37,840
portion was an introduction to PyTorch, and then the second part of the book was a very long

248
00:23:37,840 --> 00:23:42,640
example explaining, I think it was an MRI example, like object detection and these types of things,

249
00:23:42,640 --> 00:23:46,960
but it was like this huge project that they walked through. And I think this is very valuable because

250
00:23:48,080 --> 00:23:54,720
there's not much stuff like that out there. But one thing you mentioned about involving people

251
00:23:54,720 --> 00:24:01,040
in the project, the problem here is really how you scale that up, because in my class, I had

252
00:24:01,040 --> 00:24:06,960
usually 70 students, and I was just at the limit of my capacity. When the students submitted their

253
00:24:06,960 --> 00:24:12,320
proposals, I spent multiple weeks reviewing them. There were only two pages each, but if you have

254
00:24:12,880 --> 00:24:20,080
70 students, groups of three, you have like 24 groups, approximately 23, 24 groups, and yeah,

255
00:24:20,080 --> 00:24:26,320
reading 23, like short papers, and then thinking about them, giving feedback, that is a lot of time,

256
00:24:26,320 --> 00:24:30,720
and then the same thing for the final project, which was in the format of the eight page paper,

257
00:24:30,720 --> 00:24:36,320
a conference paper. Yeah, this keeps you busy, especially if you want to also

258
00:24:36,320 --> 00:24:40,560
during the semester provide feedback. So with a book, I can see the challenge is,

259
00:24:41,280 --> 00:24:45,760
yeah, you can describe a project, but it is hard to give feedback if people are,

260
00:24:45,760 --> 00:24:50,800
it's if you give an open ended exercise or something like that. But I think maybe that's

261
00:24:51,360 --> 00:24:55,520
that's what Kaggle competitions are almost for, where you have also this community around it,

262
00:24:55,520 --> 00:24:59,920
where people work on a similar project, and then they can help each other with feedback and

263
00:24:59,920 --> 00:25:06,560
so forth. But yeah, I think projects are very powerful, but there's still like the issue of how to

264
00:25:07,760 --> 00:25:13,520
how to help people with that, like in terms of having the time and capacity for that.

265
00:25:13,520 --> 00:25:17,600
You mentioned PyTorch Lightning earlier. Can you talk a little bit about that, and

266
00:25:18,560 --> 00:25:24,480
you know, what it is, how you use it? Yeah, so PyTorch Lightning, I must say, I just started using

267
00:25:24,480 --> 00:25:31,120
it recently earlier this year. It is essentially, I wouldn't call it a framework. It's more like a

268
00:25:31,120 --> 00:25:37,760
platform. So it is in a way an API that organizes your code, but it is more than just a framework.

269
00:25:37,760 --> 00:25:42,880
It's like, I would call it more like a platform because it helps you integrate other technologies as

270
00:25:42,880 --> 00:25:48,800
well. So it kind of brings together multiple things because when we do like deep learning now,

271
00:25:48,800 --> 00:25:53,840
what I usually tend to do in my research projects even, I try to write everything from scratch.

272
00:25:53,840 --> 00:25:59,200
I was, I mean, using PyTorch, but then I had my training loop. I had like a function that iterates

273
00:25:59,200 --> 00:26:04,720
over the data set to compute the test accuracy because you can't load the whole data into memory

274
00:26:04,720 --> 00:26:09,920
because it's too large. And then also logging. So I had my own logger where I was writing to a

275
00:26:09,920 --> 00:26:15,120
CSV file, for example, or sometimes using weights and biases or even just a tensor board.

276
00:26:15,680 --> 00:26:21,200
And yeah, I was just putting everything together myself. And I think this works if you work alone,

277
00:26:21,200 --> 00:26:26,160
but then also I noticed like three months later coming back to the project. I had like 20 helper files

278
00:26:26,160 --> 00:26:31,120
that I was importing from. I wanted to change something. It was like a mess. So and also when I was

279
00:26:31,120 --> 00:26:35,920
collaborating with students, if you have your custom code, it makes sense to you, but it doesn't

280
00:26:35,920 --> 00:26:43,760
make sense to anyone else. So PyTorch Lightning, yeah, it's basically, it's not much in a way that

281
00:26:44,960 --> 00:26:49,600
it's not much different, let's say, than PyTorch. It's more like on top of it, but it helps you

282
00:26:49,600 --> 00:26:55,760
integrate different other tools without having to, let's say reinvent the wheel. So what you do

283
00:26:55,760 --> 00:26:59,600
is essentially you can still have your regular PyTorch model. So you don't change anything about

284
00:26:59,600 --> 00:27:04,640
that model. It has your regular forward method and so forth. And then you have a Lightning module.

285
00:27:05,120 --> 00:27:10,000
And this is like a class that wraps around the PyTorch module. But when you do that, you

286
00:27:10,000 --> 00:27:15,120
you have still full control. You define, okay, how does my forward step look like? So the forward

287
00:27:15,120 --> 00:27:21,200
step for training or testing. And you define an optimizer. And then you have a trainer class.

288
00:27:21,520 --> 00:27:26,320
And this is it. And what's nice about it is in a trainer class, you can specify what type of

289
00:27:26,320 --> 00:27:31,280
logger you want, like weights and biases, tensor board, simple CSV logger. And what's really

290
00:27:31,280 --> 00:27:36,640
powerful is you can specify how many GPUs you want. So I mean, you can do this in PyTorch, but it's

291
00:27:37,280 --> 00:27:43,200
much more extra work. And I never really got it to work myself. So right now, for my research

292
00:27:43,200 --> 00:27:47,680
projects, I was just using the same code I had was wrapping it around a Lightning module. And I

293
00:27:47,680 --> 00:27:52,640
was just training it on multiple GPUs. So in that case, I mean, it's also fully open source. And

294
00:27:52,640 --> 00:27:57,040
if you want, you can still access the original PyTorch model. It's really just a wrapper around

295
00:27:57,040 --> 00:28:01,520
it that that gives you certain things for free, which is nice. I'm curious what your sense for

296
00:28:01,520 --> 00:28:12,080
for PyTorch usage beyond. Is it still primarily research oriented? Do you have a sense or visibility

297
00:28:12,080 --> 00:28:17,840
into whether it's seeing broader adoption in industry and commercial context? I still see a lot

298
00:28:17,840 --> 00:28:24,640
of tensor flow-out in that context. Yeah, that is an excellent question. I honestly, because I'm

299
00:28:24,640 --> 00:28:30,960
coming more like from an academia background, I must say I haven't really deployed anything

300
00:28:30,960 --> 00:28:38,880
myself yet. But talking to colleagues, I think it really depends on the company you work on.

301
00:28:38,880 --> 00:28:44,240
I mean, some people similar prefer one cloud provider over the other. It's one framework over

302
00:28:44,240 --> 00:28:50,240
the other. But there is no, I think big limitation of using PyTorch anymore for, let's say, deployment.

303
00:28:50,240 --> 00:28:56,480
So I'm not super familiar with how things work under the hood. Sometimes I look at the source code

304
00:28:56,480 --> 00:29:03,280
and it's really scary for me, like seeing all the files and stuff. But so how I understand it now

305
00:29:03,280 --> 00:29:11,040
is that you have Python where you run PyTorch in primarily. And the bottleneck of using Python

306
00:29:11,040 --> 00:29:16,800
is just like 10%. If you would remove Python, just run the code without Python, it would be maybe

307
00:29:16,800 --> 00:29:22,880
10% faster. That's not much of a difference. And they have two different ways you can go from that

308
00:29:22,880 --> 00:29:29,040
to C code. One is, I forgot the name is actually one is essentially tracing your code where it's

309
00:29:29,040 --> 00:29:34,880
really a static graph from that where you, if you have a followup, it gets unrolled is this

310
00:29:34,880 --> 00:29:39,200
static one. Like, and then they have the other approach, which is, I think it's called Torch

311
00:29:39,200 --> 00:29:46,480
script, where you go from this Python API to a, I think it's called lip torch, which is like

312
00:29:46,480 --> 00:29:52,080
the C++ API. And that one can be just used anywhere. I mean, I think they have a lot of tools

313
00:29:52,080 --> 00:29:57,120
in the recent versions, also for mobile deployment and stuff like that. So I think to be honest,

314
00:29:57,120 --> 00:30:04,160
there is no really bottleneck, no big bottleneck anymore, like using it for serious applications.

315
00:30:04,160 --> 00:30:08,480
And then also you have all the quantization things to make it faster.

316
00:30:09,680 --> 00:30:15,680
Yeah, so I think this is all really like what they focused on last year to make it more

317
00:30:15,680 --> 00:30:22,880
deployment friendly. One more thing just comes to mind is Onix, like the ONNX format, where

318
00:30:22,880 --> 00:30:29,520
you can also export PyTouch models to ONNX. And I think you can then also use it in like the Apple

319
00:30:29,520 --> 00:30:36,000
framework, I think metal and core ML. But this is something to be honest, beyond my comfort zone,

320
00:30:36,000 --> 00:30:41,120
I'm more like let's say research education. I am not really someone who is deploying applications.

321
00:30:41,120 --> 00:30:45,920
So yeah, let's, let's maybe switch gears and talk a little bit about your research.

322
00:30:45,920 --> 00:30:54,880
You're most recently you've been focused on ordinal regression among other topics. Can you

323
00:30:54,880 --> 00:31:01,280
share a little bit about that field and why you find it interesting and kind of what the research

324
00:31:01,280 --> 00:31:07,600
frontier is there? Yeah, that is a good point. So odd another regression, it's maybe an abstract

325
00:31:07,600 --> 00:31:13,520
term, but how we can think about it is how do we use methods when in a supervised learning context,

326
00:31:13,520 --> 00:31:19,360
when the class tables have a natural order. So usually when we teach or use machine learning,

327
00:31:19,360 --> 00:31:23,840
we have like these two different scenarios where one scenario is classification, let's say

328
00:31:23,840 --> 00:31:30,400
Iris flower classification, we have Satosa, Versicolor and Reginica, but we can't really say one is,

329
00:31:30,400 --> 00:31:34,480
let's say, Versicolor is bigger than Reginica, there's no order, it's just independent class

330
00:31:34,480 --> 00:31:40,560
tables. And then the other type is regression, where we have for example, I don't know house prices

331
00:31:40,560 --> 00:31:47,280
or something like that, where you have numeric or continuous target. And ordinal regression

332
00:31:47,280 --> 00:31:52,640
sits somewhere in between where we have something that looks on the surface like classification

333
00:31:52,640 --> 00:31:57,200
problem, but it has an order. And so the class tables have an order, for example,

334
00:31:57,200 --> 00:32:02,560
on Amazon customer ratings, where we have one, two, three, four, five stars. And I mean, it could

335
00:32:02,560 --> 00:32:06,800
also be kind of like a regression problem, but the difference is really we don't know

336
00:32:06,800 --> 00:32:12,960
where we can't quantify the distance between things. And we can say, okay, one in two stars and

337
00:32:12,960 --> 00:32:19,920
four and five stars, it is one star difference, but yeah, it is a little bit more tricky than that,

338
00:32:19,920 --> 00:32:24,160
because it's hard to compare a one to a two star review to a four and a five star review. It's

339
00:32:24,160 --> 00:32:28,400
it's hard to quantify this distance. And the same thing is true like for let's say other things

340
00:32:28,400 --> 00:32:36,240
like disease, if you have a scale between no disease, medium or mild disease and severe disease,

341
00:32:36,240 --> 00:32:42,320
it's hard to put a label on it how different these two distances are. And this is really where

342
00:32:42,320 --> 00:32:47,920
you don't want, so ordinal regression is where you don't want to or can't quantify the

343
00:32:49,200 --> 00:32:53,920
distance between categories, but at the same time, you have ordering. You can say

344
00:32:55,520 --> 00:33:01,920
that, for example, no disease is less than moderate than less than severe. That's like an ordering.

345
00:33:01,920 --> 00:33:07,120
Would you specifically use it? I'm thinking about it in contrast to like trying to

346
00:33:07,760 --> 00:33:16,720
attack a regression problem where you're concerned about integer values would or no regression

347
00:33:16,720 --> 00:33:23,840
be used when you've got a much smaller set of labels or can use it if your set of labels is

348
00:33:23,840 --> 00:33:28,480
relatively unconstrained and you're just really trying to focus on integers. Yeah, that is a good

349
00:33:28,480 --> 00:33:35,920
point. Actually, there is no limit to the number of classes. So in our first paper, we focused on

350
00:33:35,920 --> 00:33:44,160
age classification where we had 70 different age labels like from one to 70 years. And also,

351
00:33:44,160 --> 00:33:48,160
we thought, okay, age, I mean, this could be modeled with a regression model, but it's a little

352
00:33:48,160 --> 00:33:53,360
bit trickier than that because if you think about a person who is, let's say, 10 years old and the

353
00:33:53,360 --> 00:34:00,000
person who is 15 years old, there's a lot of change that takes place when a person becomes older

354
00:34:00,000 --> 00:34:06,720
in that five year time frame compared to, let's say, a person who is 80 and 85, where in this

355
00:34:06,720 --> 00:34:12,560
age frame, maybe the texture of the skin changes mostly, whereas for a younger person, it's maybe

356
00:34:12,560 --> 00:34:20,400
more like the growth, like the bones change and so forth. So in that case, you can use it with any

357
00:34:20,400 --> 00:34:26,720
type of labels, but yeah, you're really flexible with that. So, but what I feel like was

358
00:34:27,440 --> 00:34:33,600
people, or let's say, when I look for tutorials or anything like that, there has not been much

359
00:34:33,600 --> 00:34:41,280
attention, or people were not really providing, let's say, help or tutorials or methods for how to

360
00:34:41,280 --> 00:34:45,440
do that with deep learning. There is the classical statistics literature where we have

361
00:34:45,440 --> 00:34:50,320
ordinary regression models, but nothing really for, let's say, deep neural networks. So if you have

362
00:34:50,320 --> 00:34:55,600
an image data set and you, let's say, want to assess the damage to a building, you can't really say,

363
00:34:56,240 --> 00:35:00,640
so if you have like a collapsed roof versus like a scratch or something like that, it's a very

364
00:35:00,640 --> 00:35:06,240
type, a different type of difference compared to, or it is hard, let's say, to quantify things

365
00:35:06,240 --> 00:35:11,280
sometimes. You can try to put numbers on it, but really there are a lot of problems where there are

366
00:35:11,280 --> 00:35:16,880
no numbers that you can put on it, but you still want to, let's say, try in a classifier to recognize

367
00:35:16,880 --> 00:35:22,320
more severe damage compared to moderate damage or no damage, let's say, in terms of insurances or

368
00:35:22,320 --> 00:35:29,120
buildings and so forth. And yeah, so we have been focused on developing new networks for that,

369
00:35:29,120 --> 00:35:33,600
but then also the challenge is you don't want to, let's say, develop a completely new type of

370
00:35:33,600 --> 00:35:38,800
neural network, because then it's really hard for people to use that and compare to other methods.

371
00:35:38,800 --> 00:35:45,520
So the focus was essentially what are like small changes that we can make to modify an existing

372
00:35:45,520 --> 00:35:51,840
classifier such that it becomes an ordinary regression model. I think this is really cool because

373
00:35:51,840 --> 00:35:56,880
that allows people ready to take something they already trained and then just change a few lines

374
00:35:56,880 --> 00:36:02,080
of code and see if it becomes better. If it doesn't become better, okay, maybe I spent five minutes

375
00:36:02,080 --> 00:36:06,560
making that change, no big loss, but maybe it makes things better and then I think it's a huge

376
00:36:06,560 --> 00:36:11,680
win when people can just improve their model without having to spend a lot of time developing

377
00:36:11,680 --> 00:36:16,240
something completely new. Yeah, and so there are, I'm not sure if you're interested, there are a

378
00:36:16,240 --> 00:36:25,040
couple of methods for that I could talk about. Sure, yes, okay. One method was from 2016.

379
00:36:25,040 --> 00:36:32,320
It's not by our group, but it was published in CBPR. We call it just the order regression

380
00:36:32,320 --> 00:36:38,400
network by new at L. Yeah, and so how they tackled that problem was by, it's something called

381
00:36:38,400 --> 00:36:46,160
extended binary classification. So they take a class label, let's say you have five different classes

382
00:36:46,720 --> 00:36:52,320
and you have the classable three. So what they would do is they would extend this integer number

383
00:36:52,320 --> 00:37:00,880
into five or four zeros and ones. So you turn this multi-category classification problem

384
00:37:00,880 --> 00:37:06,320
into multiple binary classification problems and then you are predicting, is my label greater than

385
00:37:06,320 --> 00:37:11,200
one? Is my label greater than two? Is my label greater than three? So you have a lot of, or you have,

386
00:37:11,200 --> 00:37:15,920
let's say, in this case, four different yes and no questions and then you can just,

387
00:37:17,200 --> 00:37:22,720
and so if the classable is on three, you answer, classable is greater than one, yes, greater than two,

388
00:37:22,720 --> 00:37:28,880
yes, greater than three, no, and then you can, based on that, sum up the ones, add in one to it and

389
00:37:28,880 --> 00:37:34,640
end up with the label. And each problem is then modeled as a binary classification task,

390
00:37:34,640 --> 00:37:39,840
so we can use something we are familiar with like the logistic loss function on the binary

391
00:37:39,840 --> 00:37:45,360
cross entropy loss and then sum up these binary cross entropy losses. And so this worked really

392
00:37:45,360 --> 00:37:49,920
well in that paper when they have that. Sounds a bit like an extension to one hard encoding for

393
00:37:49,920 --> 00:37:56,240
categorical variables. Yeah, yeah, it is kind of like that, right? So except you have, you can have

394
00:37:56,240 --> 00:38:00,400
multiple ones basically instead of just one one, right? Yeah, but it is kind of like an encoding,

395
00:38:00,400 --> 00:38:05,120
right? Right, right. You turn this problem into a multiple binary classification task.

396
00:38:05,120 --> 00:38:11,440
The one little problem with that was when you do that, you can have like rank inconsistencies.

397
00:38:11,440 --> 00:38:17,120
So what happens, let's say in an age, a prediction problem, you can predict that the person is older

398
00:38:17,120 --> 00:38:25,120
than 41, not older than 42, but older than 43, which is like conflict. How can a person be not older

399
00:38:25,120 --> 00:38:31,360
than 42, but then older than 43? So yeah, in our work, we just, yeah, it was like a little bit

400
00:38:31,360 --> 00:38:37,280
of math. We did there. And then we had like a small tweak to prevent this rank inconsistency.

401
00:38:37,280 --> 00:38:43,360
And we found that this also, yeah, improves prediction performance by a lot. And it's a really

402
00:38:43,360 --> 00:38:49,360
small change. And all together with this method, we call that a coral, C-O-R-A-L. And

403
00:38:49,360 --> 00:38:57,280
a sense for consistent rank logits. So with that, you only have to do two small changes.

404
00:38:57,760 --> 00:39:03,120
So you have to change the last layer. There's like a constraint. We have in the last layer a

405
00:39:03,120 --> 00:39:08,960
weight sharing constraint. But we, I have like a PyTouch package, you can just import the layer

406
00:39:08,960 --> 00:39:13,360
and just use that. And then the other one is the loss function. And this is really it. So there

407
00:39:13,360 --> 00:39:18,640
are like two little changes to the code. Maybe the binary extension could be also considered as a

408
00:39:18,640 --> 00:39:23,840
change to the class table. But it's really something that you can do in five minutes. And then you

409
00:39:23,840 --> 00:39:30,560
have instead of a classifier, an auto regression model. And recently, we have another method called

410
00:39:30,560 --> 00:39:37,120
corn, C-O-R-N, which is taking this to another level. It's a bit more flexible. It has

411
00:39:37,120 --> 00:39:41,360
better performance than coral. It is a little bit more, you have to be more careful because

412
00:39:41,360 --> 00:39:45,040
with more, let's say, power comes more responsibility. So it's easier to overfit.

413
00:39:45,040 --> 00:39:51,200
But yeah, so these two methods, what I like about them is really they are very easy to implement.

414
00:39:51,200 --> 00:39:56,240
And everyone can use them. If you're using a classifier, you can just change a few lines of code.

415
00:39:56,240 --> 00:40:02,720
And yeah, you have an auto regression classifier. Maybe to close things out, we'll return a

416
00:40:02,720 --> 00:40:11,120
little bit back to education and talk about your recent role at Grid AI and kind of what you

417
00:40:11,120 --> 00:40:17,760
have planned there. Oh, yeah, that is a big question. A small short question at first glance,

418
00:40:17,760 --> 00:40:25,200
but there is a lot of things behind it. So yeah, I have been recently joined a Grid AI,

419
00:40:25,200 --> 00:40:31,440
which is focused on deep learning at scale. My role there is, though, lead AI educator,

420
00:40:31,440 --> 00:40:38,000
where I'm developing educational materials. So I'm essentially just doing what I love doing. I'm

421
00:40:38,000 --> 00:40:43,760
developing material to explain machine learning and deep learning to people. And in a sense,

422
00:40:44,400 --> 00:40:51,040
what I felt like also is I like teaching at the university, but also, let's say, going the next step,

423
00:40:51,040 --> 00:40:56,960
maybe having a more like online base or like a course that is accessible to everyone,

424
00:40:57,600 --> 00:41:03,680
my plan is to develop a free course that people can take. There's no restriction, nothing,

425
00:41:03,680 --> 00:41:09,280
it's a totally free course. And also, let's say, nicely produced with where I get to focus on,

426
00:41:09,280 --> 00:41:14,000
let's say, making this really nice and also involving the community with feedback. So yeah,

427
00:41:14,000 --> 00:41:19,600
that is what I'm currently working on. It will be, firstly, yeah, my first goal is to have

428
00:41:19,600 --> 00:41:25,600
something out maybe later this year focused on PyTorch. Maybe also PyTorch lightning, so

429
00:41:25,600 --> 00:41:29,360
something around that. So I'm currently working on that. And yeah, I'm really excited because

430
00:41:29,360 --> 00:41:36,880
I think that's like my passion. I wrote a book recently, but I also, now that I have a book,

431
00:41:36,880 --> 00:41:41,520
let's say, going back to the course development, developing courses. But I really like about that,

432
00:41:41,520 --> 00:41:48,560
it's also thinking it through. It's like, it is something where you get creative and you think

433
00:41:48,560 --> 00:41:55,040
about, okay, how should I cover what? And yeah, in the past, that was always like, I think you

434
00:41:55,040 --> 00:42:01,280
have to do it in order to see what is like the pro and kind of introducing a topic in a certain order.

435
00:42:01,280 --> 00:42:07,600
And this now offers me another attempt doing that, like seeing how I can structure a course and how

436
00:42:07,600 --> 00:42:12,000
I can develop a course. And of course, yeah, tinkering with code. This will be also very

437
00:42:12,880 --> 00:42:18,000
code-focused course. And hope I can also develop good exercises because I think it's also very

438
00:42:18,000 --> 00:42:22,640
important. There's a lot of material out there. But three, learn things. It's important to apply

439
00:42:22,640 --> 00:42:27,680
these things and also checking your understanding with having good exercises. So I'm currently,

440
00:42:27,680 --> 00:42:32,240
yeah, working on developing all of that. And hopefully I will have something by the end of the year

441
00:42:32,240 --> 00:42:36,720
that I can share with you and the community. And yeah, you can tell me what you think.

442
00:42:36,720 --> 00:42:43,280
We'll be keeping an eye out for it. In the meantime, thanks so much for joining and sharing a

443
00:42:43,280 --> 00:42:47,840
bit about what you've been up to. It's been great finally meeting you. Thank you. That was a

444
00:42:47,840 --> 00:42:53,920
total fun episode here. And I think, yeah, I could go on forever, but yeah, it was nice

445
00:42:53,920 --> 00:42:59,360
chatting with you. And I enjoyed it. And yeah, whenever you feel like it, I'm always open to

446
00:42:59,360 --> 00:43:27,520
talk more.

