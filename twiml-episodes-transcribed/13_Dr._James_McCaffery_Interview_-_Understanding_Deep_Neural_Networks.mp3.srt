1
00:00:00,000 --> 00:00:20,480
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:20,480 --> 00:00:25,560
people doing interesting things in machine learning and artificial intelligence.

3
00:00:25,560 --> 00:00:27,840
I'm your host Sam Charrington.

4
00:00:27,840 --> 00:00:31,760
If you listened to last week's show you know that today Friday May 3rd is the last day

5
00:00:31,760 --> 00:00:36,760
to register for our O'Reilly Strata Hadoop World Conference giveaway.

6
00:00:36,760 --> 00:00:41,760
You've got until midnight Pacific time to register, and you can do that via our new Facebook

7
00:00:41,760 --> 00:00:42,840
page.

8
00:00:42,840 --> 00:00:46,960
Whether or not you're interested in attending Strata Hadoop, we'd really appreciate you

9
00:00:46,960 --> 00:00:52,560
taking a moment to like our Facebook page, as well as subscribe to our new YouTube channel.

10
00:00:52,560 --> 00:00:55,840
We'll link to both of these in the show notes.

11
00:00:55,840 --> 00:01:00,240
Last week I mentioned that I'm working on an event called the Future of Data Summit,

12
00:01:00,240 --> 00:01:04,080
and I'm excited to share some of the details of that event with you now.

13
00:01:04,080 --> 00:01:09,480
The event is part of a larger IT industry conference called Interop ITX, and I've worked

14
00:01:09,480 --> 00:01:14,680
with the team at UBM that organizes the event for several years now.

15
00:01:14,680 --> 00:01:18,920
At last year's conference, I presented a workshop called the IT Leaders Guide to Machine

16
00:01:18,920 --> 00:01:23,520
Learning, and based on the strong response to that session, they asked me to work with

17
00:01:23,520 --> 00:01:26,360
them to do something bigger this time around.

18
00:01:26,360 --> 00:01:32,400
The result is a two-day future of data summit that will bring together noted experts and

19
00:01:32,400 --> 00:01:39,360
practitioners to discuss the future of enterprise data from a variety of technology perspectives.

20
00:01:39,360 --> 00:01:44,360
We'll be exploring the innovation and opportunity being offered in areas such as, of course,

21
00:01:44,360 --> 00:01:50,080
machine learning in AI and cognitive services, but also IoT and edge computing, augmented

22
00:01:50,080 --> 00:01:57,080
and virtual reality, blockchain, algorithmic IT operations, data security and privacy,

23
00:01:57,080 --> 00:01:58,080
and more.

24
00:01:58,080 --> 00:02:02,640
I've handpicked the speakers to both inspire summit attendees with a view into what's

25
00:02:02,640 --> 00:02:07,840
possible, as well as to provide practical insights into how to get there.

26
00:02:07,840 --> 00:02:13,040
To give you a taste of what I've got planned, here are just three of the 16 great speakers

27
00:02:13,040 --> 00:02:14,920
on our agenda for the summit.

28
00:02:14,920 --> 00:02:19,840
Well, first off, you remember Josh Bloom, a former guest on the podcast who start up

29
00:02:19,840 --> 00:02:23,760
wise.io, was recently acquired by GE.

30
00:02:23,760 --> 00:02:30,440
Well, Josh will be joining us to speak about building AI products from idea to production.

31
00:02:30,440 --> 00:02:35,880
Intel's Asaf Araki will give us a view into the next five plus years of compute, storage

32
00:02:35,880 --> 00:02:40,800
and network innovation in his talk titled, How the Future of Hardware Enables the Future

33
00:02:40,800 --> 00:02:42,680
of Data.

34
00:02:42,680 --> 00:02:48,300
And Diana Kelly, global executive security advisor at IBM, will be discussing the future

35
00:02:48,300 --> 00:02:54,200
of threat landscape and how to protect cloud, IoT and big data systems.

36
00:02:54,200 --> 00:02:59,160
I've got more information about the event, as well as a preliminary agenda posted at

37
00:02:59,160 --> 00:03:02,700
twimmolai.com slash future of data.

38
00:03:02,700 --> 00:03:06,760
On that page, you'll also find details for registering for the conference and a code

39
00:03:06,760 --> 00:03:10,000
offering a special discount for twimmolisteners.

40
00:03:10,000 --> 00:03:13,760
To give you a bit of a sample of the type of content you'll get at the event, our guest

41
00:03:13,760 --> 00:03:19,560
on the show today is James McCaffrey, who's a research engineer at Microsoft Research.

42
00:03:19,560 --> 00:03:24,200
James will be speaking at the summit on understanding deep neural networks, and that's the focus

43
00:03:24,200 --> 00:03:27,240
of our conversation on the podcast as well.

44
00:03:27,240 --> 00:03:31,280
We had a good time with this conversation, and even if you know your way around a DNN,

45
00:03:31,280 --> 00:03:34,200
I think you'll pick up some interesting tidbits.

46
00:03:34,200 --> 00:03:39,880
Enjoy the show and check out the event page at twimmolai.com slash future of data, or

47
00:03:39,880 --> 00:03:46,600
the show notes page at twimmolai.com slash talk slash 13, for more information on James

48
00:03:46,600 --> 00:03:48,440
or the summit.

49
00:03:48,440 --> 00:03:51,080
And now on to the show.

50
00:03:51,080 --> 00:03:56,720
Hey, everyone, I am here with James McCaffrey.

51
00:03:56,720 --> 00:04:02,760
James is with Microsoft Research, and we've got an exciting show for you this time, and

52
00:04:02,760 --> 00:04:07,040
we're going to be spending some time digging into deep neural nets.

53
00:04:07,040 --> 00:04:10,120
James, why don't you introduce yourself?

54
00:04:10,120 --> 00:04:11,120
Hi, Sam.

55
00:04:11,120 --> 00:04:13,440
Thanks for having me today.

56
00:04:13,440 --> 00:04:14,600
My name is James McCaffrey.

57
00:04:14,600 --> 00:04:17,560
I work at Microsoft Research.

58
00:04:17,560 --> 00:04:21,960
Before working at research in the research division of Microsoft, I worked in the product

59
00:04:21,960 --> 00:04:27,200
groups, so I have some experience sort of on the pragmatic side of things.

60
00:04:27,200 --> 00:04:34,080
And before joining Microsoft, I was a university professor in mathematics and computer science.

61
00:04:34,080 --> 00:04:39,760
So I made that transition from academia to industry.

62
00:04:39,760 --> 00:04:48,800
At Microsoft, my area of expertise is machine learning, and in particular, neural networks.

63
00:04:48,800 --> 00:04:54,400
One of the things I do here at Microsoft Researcher, my role, is somewhat of a hybrid.

64
00:04:54,400 --> 00:05:01,120
At Microsoft Research, we have, I'm going to guess maybe in the neighborhood of 300 serious

65
00:05:01,120 --> 00:05:08,960
researchers, world class guys, that have very specific domain knowledge.

66
00:05:08,960 --> 00:05:18,040
And my role is because I have enough mathematical knowledge to understand these guys.

67
00:05:18,040 --> 00:05:23,960
And also my software engineering background, I act as a interface between the engineering

68
00:05:23,960 --> 00:05:24,960
groups here.

69
00:05:24,960 --> 00:05:25,960
Microsoft and the research groups.

70
00:05:25,960 --> 00:05:29,120
And I do, of course, some research on my own.

71
00:05:29,120 --> 00:05:30,120
Interesting.

72
00:05:30,120 --> 00:05:36,640
So the way we got connected was, in fact, you're going to be speaking at an event that I'm

73
00:05:36,640 --> 00:05:44,400
organizing as part of the NROP ITX conference in May, and that event is called the Future

74
00:05:44,400 --> 00:05:46,040
of Data.

75
00:05:46,040 --> 00:05:52,880
And you're going to be speaking there about understanding deep neural nets.

76
00:05:52,880 --> 00:05:56,920
That's a topic that you've been spending quite a bit of time on of late, isn't it?

77
00:05:56,920 --> 00:06:01,080
But sort of interesting, yes and no.

78
00:06:01,080 --> 00:06:07,520
I'd say I've been looking at neural networks for many, many, many years.

79
00:06:07,520 --> 00:06:13,640
But we're sort of in this area of the third wave of artificial intelligence, and in particular

80
00:06:13,640 --> 00:06:15,840
deep neural networks.

81
00:06:15,840 --> 00:06:25,720
And there's no clear consensus on exactly why deep neural networks, which is a sort of

82
00:06:25,720 --> 00:06:31,120
I'd call it a subset of artificial intelligence or a tool that enables artificial intelligence.

83
00:06:31,120 --> 00:06:34,200
Why they're making this giant come back again?

84
00:06:34,200 --> 00:06:40,520
Maybe some of your listeners can remember back in the 80s, the first wave of neural networks

85
00:06:40,520 --> 00:06:47,920
that held great promise, at least theoretically, but they tended to over-promise and under-deliver.

86
00:06:47,920 --> 00:06:53,520
And then for a long time, artificial intelligence, the phrase, wasn't really used because it

87
00:06:53,520 --> 00:06:57,760
had sort of gotten a stigma attached to it.

88
00:06:57,760 --> 00:07:02,320
But then here's the analogy I always use for people.

89
00:07:02,320 --> 00:07:08,720
I think many of your listeners might remember some of the speech recognition software that

90
00:07:08,720 --> 00:07:14,160
was popular in the 90s, Dragon was very well known.

91
00:07:14,160 --> 00:07:16,920
In fact, there still really are well known too.

92
00:07:16,920 --> 00:07:23,480
But then all of a sudden, about two years ago, two and a half years ago, seemingly

93
00:07:23,480 --> 00:07:35,560
out of nowhere, we had Siri and Cortana and Alexa from Apple, Microsoft and Amazon of course.

94
00:07:35,560 --> 00:07:42,120
And the speech recognition just seemed to take this gigantic quantum jump in improvement.

95
00:07:42,120 --> 00:07:48,160
And I'm fortunate to be working directly with those guys that created that quantum jump.

96
00:07:48,160 --> 00:07:49,960
In fact, it really was.

97
00:07:49,960 --> 00:07:53,280
And it was all due to deep neural networks.

98
00:07:53,280 --> 00:08:00,560
And now, you mentioned speech recognition that's clearly one of the big application areas.

99
00:08:00,560 --> 00:08:06,760
There was some news recently, I think, within the past three months or so about a group

100
00:08:06,760 --> 00:08:15,600
that I guess just hit a new kind of past, a new bar in terms of speech recognition accuracy.

101
00:08:15,600 --> 00:08:20,840
I think it was 95 or high 90s percent.

102
00:08:20,840 --> 00:08:22,840
Was that at Microsoft?

103
00:08:22,840 --> 00:08:28,000
It's interesting because there's several different benchmarks.

104
00:08:28,000 --> 00:08:33,160
And right now, there's tremendous amounts of excitement.

105
00:08:33,160 --> 00:08:36,120
I'm sort of beating around the bushes.

106
00:08:36,120 --> 00:08:41,000
My bottom line answer in a second will be, I'm not sure it could well have been.

107
00:08:41,000 --> 00:08:44,320
But there's literally breakthroughs.

108
00:08:44,320 --> 00:08:50,960
I was sitting in a talk just a few weeks ago where there was sort of like an arms race

109
00:08:50,960 --> 00:08:54,160
on some of these benchmark problems.

110
00:08:54,160 --> 00:09:03,800
And therefore speech recognition, text recognition, basically any kind of input type things.

111
00:09:03,800 --> 00:09:12,360
And literally, one research group after another is improving and jumping over the others

112
00:09:12,360 --> 00:09:16,240
on a week-by-week basis on it.

113
00:09:16,240 --> 00:09:23,920
Or reminds me of the early days of jet aircraft in the 1950s when there seemed to be a new

114
00:09:23,920 --> 00:09:28,360
speed record set every few months or a few weeks or months.

115
00:09:28,360 --> 00:09:34,520
Well, we're sort of in that same area of frantic activity.

116
00:09:34,520 --> 00:09:35,520
That doesn't sound quite right.

117
00:09:35,520 --> 00:09:43,720
It's not so much frantic activity, but significant advances are happening weekly now in several

118
00:09:43,720 --> 00:09:50,680
areas of AI, and most of them are directly related to deep neural networks.

119
00:09:50,680 --> 00:09:53,360
So maybe let's take a step back.

120
00:09:53,360 --> 00:10:01,880
I'm curious, how do you define and describe deep neural nets to people?

121
00:10:01,880 --> 00:10:03,880
This is very interesting.

122
00:10:03,880 --> 00:10:12,200
I have a way, I get asked this question so much, so excuse me, even among my colleagues

123
00:10:12,200 --> 00:10:15,280
who have PhDs in all different kinds of fields.

124
00:10:15,280 --> 00:10:22,000
And my peer engineers, who are some of the best engineers in the world, it's very, very

125
00:10:22,000 --> 00:10:28,080
difficult to explain what deep neural networks are without a picture of some sort.

126
00:10:28,080 --> 00:10:36,440
I found that the only way I can describe completely to the satisfaction of anyone is to use

127
00:10:36,440 --> 00:10:42,760
a diagram, and that's what I do in many of my talks, including the one I'll be doing

128
00:10:42,760 --> 00:10:44,080
for you.

129
00:10:44,080 --> 00:10:50,160
So using vocabulary, it's not, but the main differences are, I'll try to express as best

130
00:10:50,160 --> 00:10:51,400
I can.

131
00:10:51,400 --> 00:10:56,000
Whenever I try to explain what a deep neural network is, I start and say, and it kind of

132
00:10:56,000 --> 00:11:00,960
makes sense, you have to have an absolute solid understanding of what a so-called regular

133
00:11:00,960 --> 00:11:08,680
neural network is, and because the distinction until recently, when you said neural network,

134
00:11:08,680 --> 00:11:14,360
you meant what is now called a single hidden layer neural network.

135
00:11:14,360 --> 00:11:17,280
They're the simplest forms of neural network.

136
00:11:17,280 --> 00:11:21,720
And deep neural networks can actually have several different meanings.

137
00:11:21,720 --> 00:11:27,360
At the basic level, a deep neural network is simply, I mean, it's really simple.

138
00:11:27,360 --> 00:11:34,120
It's just a more complicated basic neural network with multiple hidden layers.

139
00:11:34,120 --> 00:11:39,800
If I can interrupt you and go back to this single hidden layer neural network, we're talking

140
00:11:39,800 --> 00:11:48,400
about a neural network that will have an input layer and then this hidden layer and an output

141
00:11:48,400 --> 00:11:49,400
layer.

142
00:11:49,400 --> 00:11:58,040
Basically, each of those layers has a set of weights assigned to them and using some math

143
00:11:58,040 --> 00:12:05,920
and algorithms, backpropagation, for example, you're able to, based on throwing a bunch

144
00:12:05,920 --> 00:12:11,120
of training data at these neural networks, come up with a, quote unquote, optimal set of

145
00:12:11,120 --> 00:12:14,600
weights, which really is what defines the neural network.

146
00:12:14,600 --> 00:12:18,680
Is that like, is that a good way to describe what this single...

147
00:12:18,680 --> 00:12:21,280
That's absolutely correct.

148
00:12:21,280 --> 00:12:22,960
Put another way.

149
00:12:22,960 --> 00:12:28,600
In the end, a neural network is just a very complex mathematical equation that can be used

150
00:12:28,600 --> 00:12:30,160
to make predictions.

151
00:12:30,160 --> 00:12:34,200
The number of inputs is determined by your data.

152
00:12:34,200 --> 00:12:39,920
Suppose you're trying to predict the political party affiliation of a person and that could

153
00:12:39,920 --> 00:12:44,040
be democratic, republican, or other.

154
00:12:44,040 --> 00:12:49,640
So that's what you're trying to predict, and your features, which means the variables

155
00:12:49,640 --> 00:12:54,160
you use to make the prediction, suppose that could be four things.

156
00:12:54,160 --> 00:13:00,560
The person's age, their annual income, their level of education, and some other metrics.

157
00:13:00,560 --> 00:13:01,560
So four.

158
00:13:01,560 --> 00:13:06,400
Therefore, your neural network would have four input nodes and it would have three output

159
00:13:06,400 --> 00:13:07,400
nodes.

160
00:13:07,400 --> 00:13:13,120
But the hidden layer processing nodes, this hidden layer is where the processing, most

161
00:13:13,120 --> 00:13:14,960
of the processing is done.

162
00:13:14,960 --> 00:13:18,920
The number of those nodes has to be determined by trial and error.

163
00:13:18,920 --> 00:13:21,760
But in a regular neural network, there is one such layer.

164
00:13:21,760 --> 00:13:25,080
So it might be maybe ten hidden nodes.

165
00:13:25,080 --> 00:13:27,920
But with a deep neural network, you just add multiple layers.

166
00:13:27,920 --> 00:13:34,800
You might have three hidden layers of ten processing nodes, twenty processing nodes,

167
00:13:34,800 --> 00:13:37,080
and then ten processing nodes.

168
00:13:37,080 --> 00:13:42,120
And in fact, neural networks have been getting much, much deeper than three layers of

169
00:13:42,120 --> 00:13:43,120
late.

170
00:13:43,120 --> 00:13:44,120
Is that right?

171
00:13:44,120 --> 00:13:45,120
Quite right.

172
00:13:45,120 --> 00:13:52,120
Until, relatively recently, there's been two things have been occurring that have led

173
00:13:52,120 --> 00:13:58,720
to these dramatic increases across multiple areas of artificial intelligence.

174
00:13:58,720 --> 00:14:05,760
One of them is that we're just getting more raw horsepower to process these things.

175
00:14:05,760 --> 00:14:13,520
It turns out that they get exponentially more complex and so it turns out that we're

176
00:14:13,520 --> 00:14:16,520
just getting more and more processing power.

177
00:14:16,520 --> 00:14:23,000
But the second thing is combined at the same time is that we're getting very clever with

178
00:14:23,000 --> 00:14:24,280
architecture.

179
00:14:24,280 --> 00:14:30,320
And that is combining these different hidden layers in very clever ways instead of doing

180
00:14:30,320 --> 00:14:32,440
it naively.

181
00:14:32,440 --> 00:14:38,880
The analogy in this case reminds me of the advances in computer chess programs where computer

182
00:14:38,880 --> 00:14:45,680
chess programs all of a sudden got very, very good, better than any human being, someone

183
00:14:45,680 --> 00:14:47,160
unexpectedly.

184
00:14:47,160 --> 00:14:53,600
And it was it was not due merely to more processing power and it wasn't due simply to better

185
00:14:53,600 --> 00:14:54,600
algorithms.

186
00:14:54,600 --> 00:14:56,720
It was a combination of the two.

187
00:14:56,720 --> 00:15:01,520
So we're getting quickly to an area that I find really interesting.

188
00:15:01,520 --> 00:15:07,560
And that is the architecture of deep neural nets.

189
00:15:07,560 --> 00:15:14,560
I have a ton of questions about this so I'm excited that we get a chance to chat about

190
00:15:14,560 --> 00:15:15,560
it.

191
00:15:15,560 --> 00:15:16,560
I know.

192
00:15:16,560 --> 00:15:25,520
I guess as a as a as a preface to this, I know that Microsoft research has been one of

193
00:15:25,520 --> 00:15:31,520
many research organizations that's been kind of pushing the front to here.

194
00:15:31,520 --> 00:15:39,640
And in fact, in 2015, they authored a paper on what's called deep residual learning that

195
00:15:39,640 --> 00:15:46,000
won the image net competition that year.

196
00:15:46,000 --> 00:15:52,080
And so, you know, I guess what I want to talk about is like what is deep neural net architecture

197
00:15:52,080 --> 00:15:59,120
and, you know, what is deep residual learning and what are convolutional layers like, you

198
00:15:59,120 --> 00:16:06,800
know, so take us from this description of deep neural net and layers through how those

199
00:16:06,800 --> 00:16:12,040
the architecture of those networks has evolved and, you know, what are what are how do we

200
00:16:12,040 --> 00:16:13,520
think about all that right now?

201
00:16:13,520 --> 00:16:16,920
OK, I'll do my best to describe these again.

202
00:16:16,920 --> 00:16:21,000
When I do describe these, I almost always have to use a diagram because I'm going to

203
00:16:21,000 --> 00:16:26,680
talk about architecture, it's sort of going to be a bunch of nodes and how they're connected.

204
00:16:26,680 --> 00:16:35,120
So let me sort of talk about all of these things that you've mentioned are closely related.

205
00:16:35,120 --> 00:16:37,840
They're somewhat cousins to each other.

206
00:16:37,840 --> 00:16:44,400
Let's let's take the residual neural network that you just described.

207
00:16:44,400 --> 00:16:55,040
Now this is more of an exotic variety and in my mind, at least, the residual neural network

208
00:16:55,040 --> 00:17:01,160
is very, very close to a close cousin to a type of neural network called a recurrent

209
00:17:01,160 --> 00:17:02,760
neural network.

210
00:17:02,760 --> 00:17:04,880
They're usually abbreviated RNNs.

211
00:17:04,880 --> 00:17:05,880
Right.

212
00:17:05,880 --> 00:17:12,240
Now, what makes it recurrent neural network special and by the way, there's a ton of research

213
00:17:12,240 --> 00:17:16,200
activity on all of these things that we're talking about now.

214
00:17:16,200 --> 00:17:22,440
But a standard neural network does not maintain state.

215
00:17:22,440 --> 00:17:26,640
You feed it some inputs and it produces some outputs.

216
00:17:26,640 --> 00:17:29,880
Then, the next set of inputs come along.

217
00:17:29,880 --> 00:17:31,720
The neural network is essentially wiped clean.

218
00:17:31,720 --> 00:17:36,680
It doesn't maintain state from that previous set of inputs and outputs.

219
00:17:36,680 --> 00:17:44,880
A recurrent neural network has memory and internal memory, so to speak, and that manifests itself

220
00:17:44,880 --> 00:17:47,320
with just some extra nodes.

221
00:17:47,320 --> 00:17:54,160
If you can imagine a regular neural network with a hidden layer of nodes, say, 10 nodes

222
00:17:54,160 --> 00:18:01,680
in there, there's going to be a recurrent neural network has a second group of 10 nodes

223
00:18:01,680 --> 00:18:06,520
that maintain the memory of the previous input.

224
00:18:06,520 --> 00:18:12,440
This allows this, just intuitively, you can tell that this makes the neural network much

225
00:18:12,440 --> 00:18:20,680
more powerful and smart because it has, in an English word, it has context.

226
00:18:20,680 --> 00:18:28,040
This means, for instance, suppose you're trying to predict, you're coming along and your

227
00:18:28,040 --> 00:18:36,160
inputs are words in a sentence and you're trying to predict what the next word might be.

228
00:18:36,160 --> 00:18:41,280
This is something that you might see on like a smartphone when you're typing a message

229
00:18:41,280 --> 00:18:47,040
and it tries to predict what your next word might be, although there's a pretty rudimentary

230
00:18:47,040 --> 00:18:48,040
right now.

231
00:18:48,040 --> 00:18:55,920
If you just used a regular neural network to do that, input is separate and you wouldn't

232
00:18:55,920 --> 00:19:01,520
have any context, but a recurrent neural network would have a shadow of the memory of the

233
00:19:01,520 --> 00:19:07,440
previous inputs and it would be able to make a better guess at what the next word is because

234
00:19:07,440 --> 00:19:13,680
the next word in a sentence is clearly going to depend on what the first words were.

235
00:19:13,680 --> 00:19:20,040
These recurrent neural networks sometimes are categorized into short termy recurrent

236
00:19:20,040 --> 00:19:27,160
neural networks where they only have a limited ability to remember quite recent inputs or

237
00:19:27,160 --> 00:19:32,760
they can be one of the most exciting areas of research right now is these long term recurrent

238
00:19:32,760 --> 00:19:38,720
neural networks and they just maintain more memory and these things have the potential

239
00:19:38,720 --> 00:19:45,320
to be super powerful and to get to sort of close the circle here.

240
00:19:45,320 --> 00:19:55,240
In my mind, I view the Microsoft residual neural networks as one of these long term recurrent

241
00:19:55,240 --> 00:20:01,920
neural networks with some special architecture features thrown in sort of customer.

242
00:20:01,920 --> 00:20:10,160
So you mentioned long-term memory and short-term memory and in fact on this show, I've

243
00:20:10,160 --> 00:20:18,880
talked quite a bit about applications using LSTM RNN which is long short-term memory.

244
00:20:18,880 --> 00:20:23,760
How does that relate to long-term and short-term?

245
00:20:23,760 --> 00:20:33,720
Well, to tell you the truth, the vocabulary is not very standardized and they all from

246
00:20:33,720 --> 00:20:34,720
a...

247
00:20:34,720 --> 00:20:38,560
So maybe these long short-term memories are what you're referring to as long-term and

248
00:20:38,560 --> 00:20:39,560
that's also...

249
00:20:39,560 --> 00:20:40,560
Yeah, okay.

250
00:20:40,560 --> 00:20:41,560
Thank you.

251
00:20:41,560 --> 00:20:42,560
Okay.

252
00:20:42,560 --> 00:20:43,560
You precisely said what I was trying to say.

253
00:20:43,560 --> 00:20:44,560
Got it.

254
00:20:44,560 --> 00:20:45,560
Got it.

255
00:20:45,560 --> 00:20:46,560
Okay.

256
00:20:46,560 --> 00:20:47,560
So, yeah.

257
00:20:47,560 --> 00:20:55,400
We have been hearing tons about different applications of these LSTM networks, you know, often

258
00:20:55,400 --> 00:21:02,560
relating to the example that you use which is you're predicting...

259
00:21:02,560 --> 00:21:07,920
Trying to predict words or things like that from a sentence.

260
00:21:07,920 --> 00:21:15,360
Which kind of brings us to maybe the difference between predictive networks and generative

261
00:21:15,360 --> 00:21:16,360
networks?

262
00:21:16,360 --> 00:21:17,360
Oh.

263
00:21:17,360 --> 00:21:18,360
Okay.

264
00:21:18,360 --> 00:21:19,360
Very good.

265
00:21:19,360 --> 00:21:20,360
This is...

266
00:21:20,360 --> 00:21:26,860
If I had to pick one area where there's more excitement, intellectual excitement in the

267
00:21:26,860 --> 00:21:32,600
research community than any other, it's exactly said these generative neural networks.

268
00:21:32,600 --> 00:21:34,880
They're called GAN.

269
00:21:34,880 --> 00:21:41,560
One of the most popular forms of these is called GAN, a generative adversarial network.

270
00:21:41,560 --> 00:21:48,400
Sure, it's really conceptually a little bit difficult to grasp, and here's how I think

271
00:21:48,400 --> 00:21:50,440
about it.

272
00:21:50,440 --> 00:21:57,920
A generative neural network does just what you might expect from its description is it

273
00:21:57,920 --> 00:22:01,600
doesn't try to make a prediction based on input.

274
00:22:01,600 --> 00:22:09,040
It more or less tries to create new inputs in some sense, which is a little bit hard

275
00:22:09,040 --> 00:22:10,040
to grasp.

276
00:22:10,040 --> 00:22:15,120
Now, I'll be the first to say that I don't fully understand these things.

277
00:22:15,120 --> 00:22:19,400
Like everybody else, they've only been around...

278
00:22:19,400 --> 00:22:24,800
Really, the biggest name is a guy named Ian Goodfellow, who is the best-known name in

279
00:22:24,800 --> 00:22:25,800
this area.

280
00:22:25,800 --> 00:22:29,680
And these things have only really been around for a matter of months now.

281
00:22:29,680 --> 00:22:33,680
By that, I mean, maybe a year and a half to two years or so.

282
00:22:33,680 --> 00:22:36,640
So a lot of us are still trying to figure it out.

283
00:22:36,640 --> 00:22:45,040
The classic example, at least, that I used on my blog post, is that you can feed a neural

284
00:22:45,040 --> 00:22:53,040
network a bunch of Van Gogh paintings, and then that generative neural network will be

285
00:22:53,040 --> 00:23:00,920
able to generate and create paintings based on the style of Van Gogh.

286
00:23:00,920 --> 00:23:04,520
In short, what it's doing is it's sort of separating out.

287
00:23:04,520 --> 00:23:07,360
It's learning to separate style from content.

288
00:23:07,360 --> 00:23:12,600
Well, this is all very difficult for me to get my head around.

289
00:23:12,600 --> 00:23:18,920
And I'll say that people who are much, much smarter than me figure that this is something

290
00:23:18,920 --> 00:23:24,000
that could lead to tremendous breakthroughs in the future.

291
00:23:24,000 --> 00:23:32,480
And for folks that want to dig into that last use case, I believe the paper is called

292
00:23:32,480 --> 00:23:35,280
Neural Artistic Style Transfer.

293
00:23:35,280 --> 00:23:41,080
Or at the very least, if you Google that or Bing that, you'll be able to find lots of

294
00:23:41,080 --> 00:23:44,200
information about that application.

295
00:23:44,200 --> 00:23:45,200
Right.

296
00:23:45,200 --> 00:23:46,200
Exactly right.

297
00:23:46,200 --> 00:23:50,040
But yeah, so there's generative networks and GANs.

298
00:23:50,040 --> 00:23:56,000
In fact, I just had an opportunity to hear in Goodfellow talk about this last week.

299
00:23:56,000 --> 00:24:05,600
I was at an event, a deep learning summit, rework deep learning summit in San Francisco.

300
00:24:05,600 --> 00:24:13,680
And the basic idea there, as I understand it, is you've got, as you mentioned, a network

301
00:24:13,680 --> 00:24:20,880
that is kind of trained to produce or approximate inputs.

302
00:24:20,880 --> 00:24:27,200
And then you feed the stuff that it spits out to another network that is, I think, called

303
00:24:27,200 --> 00:24:32,400
a discriminator network that's trained to basically measure how close those inputs are

304
00:24:32,400 --> 00:24:37,840
to the real life thing, the thing that you're trying to approximate.

305
00:24:37,840 --> 00:24:41,640
And then you basically have a feedback loop between these two.

306
00:24:41,640 --> 00:24:43,960
That's correct.

307
00:24:43,960 --> 00:24:49,280
They're called adversarial because really under the covers, there's two neural networks

308
00:24:49,280 --> 00:24:50,280
going on.

309
00:24:50,280 --> 00:24:59,200
Number one is trying to generate information and fake out the other neural networks.

310
00:24:59,200 --> 00:25:00,200
So they're adversarial.

311
00:25:00,200 --> 00:25:02,040
They're working against each other.

312
00:25:02,040 --> 00:25:09,880
And this relates more to the architecture, the engineering architecture.

313
00:25:09,880 --> 00:25:13,440
But as you said, the real goal is to generate information.

314
00:25:13,440 --> 00:25:20,680
And the idea being there, that if a neural network is smart enough to generate information,

315
00:25:20,680 --> 00:25:28,400
then it's also smart enough to understand and discriminate information.

316
00:25:28,400 --> 00:25:32,600
So we talked about RNNs.

317
00:25:32,600 --> 00:25:35,640
What about convolutional neural nets?

318
00:25:35,640 --> 00:25:41,600
How are those different from RNNs and other types of deep neural nets?

319
00:25:41,600 --> 00:25:49,600
It's funny that talking about convolutional networks, they're usually abbreviated CNNs,

320
00:25:49,600 --> 00:25:55,680
that now they seem like they're just sort of old news.

321
00:25:55,680 --> 00:26:00,800
But in fact, they're quite new still.

322
00:26:00,800 --> 00:26:07,880
The main problem with deep neural networks, as I described, a basic deep neural network,

323
00:26:07,880 --> 00:26:14,880
which is a simple architecture, but with just lots of nodes in multiple layers.

324
00:26:14,880 --> 00:26:17,520
The problem there is the training.

325
00:26:17,520 --> 00:26:23,640
The number of weights and biases that you have to compute, or using your optimization

326
00:26:23,640 --> 00:26:41,240
algorithm, just becomes intractable.

327
00:26:41,240 --> 00:26:49,920
The number of weights you have to do is 5 times 6, plus 6 times 3, plus 6, plus 3.

328
00:26:49,920 --> 00:26:58,720
As you expand the number of nodes, in English, it increases exponentially, that's not mathematically

329
00:26:58,720 --> 00:26:59,720
correct.

330
00:26:59,720 --> 00:27:03,720
Let's just say it gets really big, really fast, it gets intractable.

331
00:27:03,720 --> 00:27:09,960
It doesn't drive you crazy when people do say that, and it's not actually exponential.

332
00:27:09,960 --> 00:27:15,160
It depends on how much I've been drinking.

333
00:27:15,160 --> 00:27:22,160
You know, I try to see, because here at Microsoft, I speak to different audiences, I'll speak

334
00:27:22,160 --> 00:27:27,720
to business leaders, I'll speak to engineers, and I'll speak to mathematicians.

335
00:27:27,720 --> 00:27:33,640
When you're speaking to anybody but the mathematicians, if you try to phrase yourself too carefully

336
00:27:33,640 --> 00:27:40,800
and be correct, you mess up your argument, but when I say, for instance, that the output

337
00:27:40,800 --> 00:27:47,160
of a neural network, a classifier, are probabilities, oh, my math colleagues will go nuts and go,

338
00:27:47,160 --> 00:27:48,160
no, they're not.

339
00:27:48,160 --> 00:27:49,160
No, they're not.

340
00:27:49,160 --> 00:27:50,440
I go, okay, yeah, I know they're not.

341
00:27:50,440 --> 00:27:59,480
But we're back to convolutional neural networks, because a straightforward approach just

342
00:27:59,480 --> 00:28:02,560
isn't intractable computationally.

343
00:28:02,560 --> 00:28:14,160
The idea, and let's see, this was, I always have trouble, remember, is Yan Lee-Kun?

344
00:28:14,160 --> 00:28:15,680
Is the big name here?

345
00:28:15,680 --> 00:28:22,520
He created an architecture where the main idea of this architecture was to make these

346
00:28:22,520 --> 00:28:24,480
things tractable.

347
00:28:24,480 --> 00:28:30,040
CNNs are used almost exclusively for image processing.

348
00:28:30,040 --> 00:28:36,920
This is an area that I'm not too familiar with, I mean, I'm from Earth, math of it all.

349
00:28:36,920 --> 00:28:43,680
But imagine you have an image, or a set of images, and you want to classify them.

350
00:28:43,680 --> 00:28:49,640
The classic example is called the M-ness database, where there's a data set of umpteen thousand

351
00:28:49,640 --> 00:28:59,200
handwritten digit characters that were called from IRS tax returns and digitized.

352
00:28:59,200 --> 00:29:02,880
And so suppose you want to classify, you know, what is this?

353
00:29:02,880 --> 00:29:03,880
Is it a digit one?

354
00:29:03,880 --> 00:29:05,640
Is it a digit two or so forth?

355
00:29:05,640 --> 00:29:11,080
Well, even a very small image is going to have thousands of pixels, and each pixel is

356
00:29:11,080 --> 00:29:13,320
going to be one input.

357
00:29:13,320 --> 00:29:20,080
Now if you get, go up to like a seriously large picture, or even something that a smartphone

358
00:29:20,080 --> 00:29:23,920
can take, you've got millions of inputs.

359
00:29:23,920 --> 00:29:29,960
And millions of inputs, you just can't deal with that in a basic way.

360
00:29:29,960 --> 00:29:37,040
So the, the, the brilliancy of convolutional neural networks is to simplify.

361
00:29:37,040 --> 00:29:42,680
It still uses the same basic ideas of neural networks, but it uses them in very clever

362
00:29:42,680 --> 00:29:49,720
ways by slicing and dicing the image up and sharing weights instead of having to calculate

363
00:29:49,720 --> 00:29:53,520
a million times a million, which is whatever that is, weights.

364
00:29:53,520 --> 00:30:00,640
You can break it up, and there's a part of the secret sauce is shared weights where weights

365
00:30:00,640 --> 00:30:06,200
in a particular area of inputs, meaning a particular area of the image are shared.

366
00:30:06,200 --> 00:30:08,600
And there's a lot more to it than that.

367
00:30:08,600 --> 00:30:14,320
Convolutional neural networks are really a remarkable achievement of architectural design,

368
00:30:14,320 --> 00:30:17,960
and they're now considered more or less standard.

369
00:30:17,960 --> 00:30:25,400
Many of the tools that you can find in particular Google's tool, whose name I can never remember

370
00:30:25,400 --> 00:30:31,280
because I don't use it, it runs strictly on Linux, do you know which one I'm talking

371
00:30:31,280 --> 00:30:32,280
about?

372
00:30:32,280 --> 00:30:33,280
Say I'm with it.

373
00:30:33,280 --> 00:30:34,280
Google's TensorFlow.

374
00:30:34,280 --> 00:30:35,280
TensorFlow.

375
00:30:35,280 --> 00:30:36,280
Oh, thank you, thank you.

376
00:30:36,280 --> 00:30:37,280
Yes, sir.

377
00:30:37,280 --> 00:30:45,120
Anyway, so Google's TensorFlow can do CNNs, and Microsoft has a recently released, basically

378
00:30:45,120 --> 00:30:53,440
a same idea called CNTK, not a real, it doesn't slide off the tongue really easily there.

379
00:30:53,440 --> 00:31:00,280
But these things are now well known, but I always like to point out that it took a lot

380
00:31:00,280 --> 00:31:02,360
of researchers a lot of years.

381
00:31:02,360 --> 00:31:09,760
In fact, the CNN version that's in common use now is called CNN version five or something

382
00:31:09,760 --> 00:31:14,960
like that, which means there were many major iterations and tons of work that went

383
00:31:14,960 --> 00:31:15,960
on.

384
00:31:15,960 --> 00:31:20,040
So, in short, to summarize, you know, these CNNs to the best of my knowledge are used almost

385
00:31:20,040 --> 00:31:24,480
exclusively for image processing, but they are the state of their art.

386
00:31:24,480 --> 00:31:32,280
However, they have some really interesting problems that a lot of, there's a lot of thought

387
00:31:32,280 --> 00:31:35,440
about some of the limitations of CNNs.

388
00:31:35,440 --> 00:31:37,440
Can you speak a bit to those?

389
00:31:37,440 --> 00:31:43,600
Yeah, I sure there was a very interesting, fascinating paper that came out of Google

390
00:31:43,600 --> 00:31:46,760
Research, what was it called?

391
00:31:46,760 --> 00:31:54,360
It was called the intriguing properties of neural networks, something like this.

392
00:31:54,360 --> 00:32:02,240
And the key takeaway is, and I like to use this example of which I don't think was in

393
00:32:02,240 --> 00:32:04,720
the paper, but other people followed up on it.

394
00:32:04,720 --> 00:32:13,560
You can, suppose you train a CNN to recognize images, you can feed it a picture of a school

395
00:32:13,560 --> 00:32:19,200
bus, and it's clearly a school bus, and the CNN will recognize it.

396
00:32:19,200 --> 00:32:28,840
But by cleverly messing up just a few of the pixels, the image is completely unchanged

397
00:32:28,840 --> 00:32:30,120
to the human eye.

398
00:32:30,120 --> 00:32:36,440
However, this exact same classifier now sees the school bus as an ostrich, so it's the

399
00:32:36,440 --> 00:32:38,160
bus to ostrich effect.

400
00:32:38,160 --> 00:32:42,280
Well, this is very troubling in a lot of ways.

401
00:32:42,280 --> 00:32:47,520
It raises, by the way, you can't just throw, you can't just randomly mess up the picture,

402
00:32:47,520 --> 00:32:52,040
you have to do it in a very clever way, but it raises some important issues.

403
00:32:52,040 --> 00:32:56,000
One of them is, at least the whole question of comprehension.

404
00:32:56,000 --> 00:33:02,040
Does a CNN really understand things if you can hoax it this way?

405
00:33:02,040 --> 00:33:08,160
It also raises questions of, if people are going to, and they are, using these CNNs for

406
00:33:08,160 --> 00:33:17,600
things, which have security implications, or imagine medical imaging, where it has implications

407
00:33:17,600 --> 00:33:19,200
for health and safety.

408
00:33:19,200 --> 00:33:22,120
Are law enforcement exactly?

409
00:33:22,120 --> 00:33:29,080
If these things have this inherent weakness, maybe there's something wrong with CNNs.

410
00:33:29,080 --> 00:33:34,320
This is all just the speculation that's going to, and no one really knows, but at least

411
00:33:34,320 --> 00:33:40,360
just some very interesting questions, and the research goes on at just giving more

412
00:33:40,360 --> 00:33:47,280
interest in research, in particular, some of my colleagues are working on trying to go

413
00:33:47,280 --> 00:33:55,920
back to the very, very early days, where instead of just using raw math and raw processing,

414
00:33:55,920 --> 00:34:01,240
we're going to try to do some symbolic and some sort of a deeper level of understanding.

415
00:34:01,240 --> 00:34:04,840
Can you elaborate on that?

416
00:34:04,840 --> 00:34:11,920
What does that mean in this context, and how will we apply symbolic, symbolic here?

417
00:34:11,920 --> 00:34:22,960
Because we just hired the person who's considered the leading guy in this area, and he only started

418
00:34:22,960 --> 00:34:32,480
here, here is Paul, and I'll spell his last name, S-M-O-L-E-N-S-K-Y, Paul Smolensky.

419
00:34:32,480 --> 00:34:39,520
We just hired him out of Johns Hopkins University, and he's been, what many people,

420
00:34:39,520 --> 00:34:43,520
claiming he considered the leading researcher in this area of symbolic reasoning and machine

421
00:34:43,520 --> 00:34:44,520
learning.

422
00:34:44,520 --> 00:34:52,360
I got his book, and I'm a fairly bright guy, I have a PhD, but this was a complicated

423
00:34:52,360 --> 00:34:53,360
book.

424
00:34:53,360 --> 00:34:59,440
He's thinking at a different level, and he's trying to, I had an interesting chat with

425
00:34:59,440 --> 00:35:02,400
him in the hallway the other day.

426
00:35:02,400 --> 00:35:11,280
He sits right behind me, and the analogy goes like this, when I was an undergraduate, my

427
00:35:11,280 --> 00:35:17,600
very first degree was in cognitive psychology, which, through various things, that led to

428
00:35:17,600 --> 00:35:19,120
math and that led to computer.

429
00:35:19,120 --> 00:35:25,240
Anyway, when I was in my cognitive psychology days, I worked with a brilliant researcher

430
00:35:25,240 --> 00:35:35,000
Art Duncan-Luce, and his goal was to create a complete mathematical framework and description

431
00:35:35,000 --> 00:35:38,120
of certain areas of psychology in the human mind.

432
00:35:38,120 --> 00:35:40,640
In other words, try to map cognition.

433
00:35:40,640 --> 00:35:42,880
How do people think?

434
00:35:42,880 --> 00:35:46,440
Because still, we still don't know how people think.

435
00:35:46,440 --> 00:35:55,600
To map that call is attempting, in some ways, to create a meta framework for symbolic

436
00:35:55,600 --> 00:35:58,880
reasoning and logic.

437
00:35:58,880 --> 00:36:06,080
This is, right now, deep neural networks have been remarkably effective in doing what

438
00:36:06,080 --> 00:36:12,160
I call the sort of sensory aspect of artificial intelligence.

439
00:36:12,160 --> 00:36:15,520
Imagine the five senses that we have.

440
00:36:15,520 --> 00:36:20,520
Even vision and pattern and image recognition, they're really good at speech recognition.

441
00:36:20,520 --> 00:36:24,960
They're really good at, even the robotics manipulation, they're really good at, but the

442
00:36:24,960 --> 00:36:33,440
one thing that they just were not even close right now is the reasoning aspects of it.

443
00:36:33,440 --> 00:36:40,120
That's what the symbolic type of process is designed to do, or one area.

444
00:36:40,120 --> 00:36:43,440
It's one attack on this.

445
00:36:43,440 --> 00:36:48,800
I know that was a little bit vague and fishy, but maybe you can get Paul in a future one

446
00:36:48,800 --> 00:36:50,560
of your podcasts to talk about.

447
00:36:50,560 --> 00:36:52,760
I'd love to hear what he has to say.

448
00:36:52,760 --> 00:36:53,760
Okay.

449
00:36:53,760 --> 00:36:54,760
Awesome.

450
00:36:54,760 --> 00:36:55,760
Yeah.

451
00:36:55,760 --> 00:36:57,080
That would be great.

452
00:36:57,080 --> 00:37:11,440
We've got CNNs, RNNs, and I still want to probe around the idea of network architecture

453
00:37:11,440 --> 00:37:14,640
and residual learning.

454
00:37:14,640 --> 00:37:29,160
There was a blog post by a guy named Steven Merity who's at Salesforce now.

455
00:37:29,160 --> 00:37:34,240
He came in via one of their recent acquisitions.

456
00:37:34,240 --> 00:37:38,040
If I remember correctly, he wrote this blog post and the title was something along the lines

457
00:37:38,040 --> 00:37:44,120
of network architecture is the new feature engineering, meaning in traditional machine

458
00:37:44,120 --> 00:37:55,200
learning, a big part of the job was trying to figure out how to massage your data and

459
00:37:55,200 --> 00:38:08,000
how to create whether natural or man-made features that express the underlying properties

460
00:38:08,000 --> 00:38:12,360
of your data in a way that your machine learning algorithms can easily train on those

461
00:38:12,360 --> 00:38:15,040
and produce accurate results.

462
00:38:15,040 --> 00:38:24,440
This new world, defining the network architecture of your deep neural nets is the moral equivalent

463
00:38:24,440 --> 00:38:25,440
if you will.

464
00:38:25,440 --> 00:38:32,840
It's the new thing that we need to do to massage our data and our solutions to produce accurate

465
00:38:32,840 --> 00:38:33,840
results.

466
00:38:33,840 --> 00:38:41,400
I'm trying to, I'm wondering if you can help us wrap our heads around what that process

467
00:38:41,400 --> 00:38:51,000
looks like and what are the things that researchers or engineers are thinking about as they start

468
00:38:51,000 --> 00:38:56,120
with a problem and say, I've got this data set and I think deep neural net is the way

469
00:38:56,120 --> 00:38:59,920
to solve this problem.

470
00:38:59,920 --> 00:39:05,880
How do they then get to, oh, well, the optimal answer is something that I'm going to call

471
00:39:05,880 --> 00:39:13,920
the deep residual network that has 150 layers and these convolutional layers and every fifth

472
00:39:13,920 --> 00:39:18,200
layer is a residual layer and that whole process.

473
00:39:18,200 --> 00:39:21,200
Is that something you can speak to?

474
00:39:21,200 --> 00:39:27,880
Well, yeah, I'll talk about this because sadly the bottom line is there's no good answer

475
00:39:27,880 --> 00:39:31,880
to this.

476
00:39:31,880 --> 00:39:37,840
If sort of the phrase that everybody has heard a million times is that machine learning

477
00:39:37,840 --> 00:39:45,960
and AI and deep learning and all this is still as much art as it is science and that has

478
00:39:45,960 --> 00:39:54,400
been true and it still is true, there are some incredibly bright people who work in this

479
00:39:54,400 --> 00:39:55,400
field.

480
00:39:55,400 --> 00:40:00,620
I'm fortunate enough to work with some of the greatest minds, I mean, they're world

481
00:40:00,620 --> 00:40:02,560
famous and leaders.

482
00:40:02,560 --> 00:40:09,120
But when we sit around drinking coffee and chatting about this, there's so much unknown.

483
00:40:09,120 --> 00:40:18,760
And the brightest guys in the world are learning daily and new stuff and for instance, another

484
00:40:18,760 --> 00:40:26,760
related thing here is that another hot area is reinforcement learning, which is how

485
00:40:26,760 --> 00:40:28,520
does that fit in?

486
00:40:28,520 --> 00:40:35,960
Even among my colleagues, we're talking about, you know, we're knowledge junkies.

487
00:40:35,960 --> 00:40:41,640
We're just constantly trying to soak this information up, but things are happening so fast

488
00:40:41,640 --> 00:40:45,080
and there's so much unknown.

489
00:40:45,080 --> 00:40:53,000
The area you're talking about, network architecture, that's one way, I mean, that would be a good

490
00:40:53,000 --> 00:40:56,040
surrogate term for exactly what's going on in all of research.

491
00:40:56,040 --> 00:41:01,520
Now, it's almost all related directly or indirectly to the architecture.

492
00:41:01,520 --> 00:41:08,600
Now, I'm a pretty, you know, I believe in simplicity and for me, network architecture,

493
00:41:08,600 --> 00:41:16,200
deep architecture is really simple in the one hand where it's just how you combine your

494
00:41:16,200 --> 00:41:22,280
processing nodes and not so much input output in different ways.

495
00:41:22,280 --> 00:41:27,040
And it boils down to, think about the human brain, it's been some interesting work done

496
00:41:27,040 --> 00:41:34,120
by all things DARPA, the defense agency in conjunction with IBM, where one of the

497
00:41:34,120 --> 00:41:41,320
projects they have and Microsoft has a similar project that I don't think I can talk about

498
00:41:41,320 --> 00:41:42,320
now.

499
00:41:42,320 --> 00:41:46,680
It's name, it's still under wraps, but I can give you a rough idea of what we're doing

500
00:41:46,680 --> 00:41:52,960
by talking about the IBM and the department defense thing, where the idea here is that

501
00:41:52,960 --> 00:41:59,720
instead of, it's almost too simple, instead of using the approach we're using right now,

502
00:41:59,720 --> 00:42:05,720
which is to get very clever with very specific types of architecture, very, you know, just

503
00:42:05,720 --> 00:42:07,520
think of a blueprint.

504
00:42:07,520 --> 00:42:16,200
Instead, take the approach that the human brain may have and that is just make your architecture

505
00:42:16,200 --> 00:42:23,040
a bunch of, a bunch of nodes totally connected, in other words, like the human brain.

506
00:42:23,040 --> 00:42:29,600
And then instead of using supervised learning, where you have to have labeled data, you

507
00:42:29,600 --> 00:42:36,360
have to have known correct outputs with your, you know, inputs, use unsupervised learning.

508
00:42:36,360 --> 00:42:42,680
And I'm sort of tossing out a schmorker's board of terms here, but unsupervised learning

509
00:42:42,680 --> 00:42:49,600
is another incredibly hot area of research right now, where we realize that methods that

510
00:42:49,600 --> 00:42:56,000
require labeled training data, which is just the way to say data, where you tag what the

511
00:42:56,000 --> 00:42:58,920
correct output is, that can only take you so far.

512
00:42:58,920 --> 00:43:04,600
It's just not going to scale to the, you know, the kinds of things that we want to do.

513
00:43:04,600 --> 00:43:09,920
Anyway, back to the DARPA IBM thing, they're creating this thing, where their goal is

514
00:43:09,920 --> 00:43:17,040
to create a processor in hard work, because, you know, IBM is known for that, that kind

515
00:43:17,040 --> 00:43:22,080
of work, that is, you know, skills to biological levels.

516
00:43:22,080 --> 00:43:27,640
And as far as I can recall from last time I read that, an article on it, they believe

517
00:43:27,640 --> 00:43:37,000
that they have successfully created in-hardware a neural network, and they're not calling

518
00:43:37,000 --> 00:43:42,840
it that, that roughly simulates the complexity of the brain of a honeybee.

519
00:43:42,840 --> 00:43:47,120
And then, okay, the question here is not, okay, so how does it learn?

520
00:43:47,120 --> 00:43:49,800
And you know, that wraps around back to symbolic thing.

521
00:43:49,800 --> 00:43:54,680
So, I'm sorry, that was kind of a rambling answer here, but I agree that you're right,

522
00:43:54,680 --> 00:44:02,320
that it's right now, if you wanted to summarize all of the, or most of the areas, including

523
00:44:02,320 --> 00:44:08,680
these generative adversarial networks, the long short-term memory networks, the residual

524
00:44:08,680 --> 00:44:13,400
networks, it's all about the architecture.

525
00:44:13,400 --> 00:44:21,680
So to, to maybe further summarize, I guess the way, the way I kind of take away, what

526
00:44:21,680 --> 00:44:27,800
I would take away from what you are saying is, you know, maybe on the one hand, you know,

527
00:44:27,800 --> 00:44:37,360
to ask, you know, how do we create new network architectures for a given problem?

528
00:44:37,360 --> 00:44:44,640
We're just too early to, to, right now, we're in the stage where the fact that we come

529
00:44:44,640 --> 00:44:50,120
up with a new architecture for a problem that is, that works and is useful, like that's

530
00:44:50,120 --> 00:44:55,880
a big deal, and, and we're going to have to do a lot of that before we can say, oh, this

531
00:44:55,880 --> 00:45:01,320
is the process for creating new architectures for our given problems.

532
00:45:01,320 --> 00:45:07,560
Let me interrupt by saying, sorry for interrupting you, but you just recalled to my mind a very

533
00:45:07,560 --> 00:45:13,240
well-known paper, it's actually not even a paper, it's basically a blog post, but it's

534
00:45:13,240 --> 00:45:19,240
extremely well-known in the, you know, in our field, it's called the unreasonable effectiveness

535
00:45:19,240 --> 00:45:26,480
of recurrent neural networks. With the idea being that, you know, there's no obvious

536
00:45:26,480 --> 00:45:30,520
connection, you know, for the person who, I tried to look up, I could not find too much

537
00:45:30,520 --> 00:45:35,120
history on recurrent neural networks. There was some indication, but it's not exactly

538
00:45:35,120 --> 00:45:41,640
clear who thought of them first, but it's not at all obvious, you know, you have these recurrent

539
00:45:41,640 --> 00:45:48,640
architecture that has worked unexpectedly well, so unreasonably well. So the point is,

540
00:45:48,640 --> 00:45:52,760
I know the stuff is obvious, and we're still in the very, very early stages of figuring

541
00:45:52,760 --> 00:45:55,480
all this out exactly what you said.

542
00:45:55,480 --> 00:46:04,040
And then, I guess along those lines, if I am a listener and I'm building, I want to,

543
00:46:04,040 --> 00:46:10,920
I want to create a solution, you know, does it stand a reason that, you know, what, where

544
00:46:10,920 --> 00:46:17,960
would you start if you were building something? Like, would you even, would you even try to

545
00:46:17,960 --> 00:46:22,600
build your own deep neural net, or would you use some off-the-shelf implementation? Would

546
00:46:22,600 --> 00:46:27,880
you, you know, use a service? Would you, if you thought that you, like, how would you

547
00:46:27,880 --> 00:46:31,880
know if you needed to build your own thing?

548
00:46:31,880 --> 00:46:39,440
That's an interesting question, and there's a, there's, I want to say controversy, but

549
00:46:39,440 --> 00:46:49,960
there are differences of opinion here. My personal opinion is that whenever I'm going to tackle

550
00:46:49,960 --> 00:46:55,280
a problem, for instance, one of the problems that I'm fascinated by, and I've worked on

551
00:46:55,280 --> 00:47:03,400
for many years, is predicting America and National NFL football scores. And I like that as

552
00:47:03,400 --> 00:47:09,000
an interesting problem, because it's concrete, it's practical, and you can determine your,

553
00:47:09,000 --> 00:47:15,800
you know, how good you are right away. And I originally started using sort of standard

554
00:47:15,800 --> 00:47:21,160
canned approaches. I started with sort of regression techniques, and then I started

555
00:47:21,160 --> 00:47:28,680
using regular sort of neural networks from a tool, like WCA, like TensorFlow, and things

556
00:47:28,680 --> 00:47:35,480
like that. And I got up to a certain level of accuracy or goodness, and I just couldn't

557
00:47:35,480 --> 00:47:41,640
get better. No matter what I did, I couldn't get better. Until I threw it all away and created

558
00:47:41,640 --> 00:47:49,000
my own neural architecture from scratch, where it was custom designed for this problem.

559
00:47:49,000 --> 00:47:56,640
In much the same way that convolutional networks are absolutely custom designed for image

560
00:47:56,640 --> 00:48:06,120
recognition with, you know, they're optimized because an image has pixel values and the RGB

561
00:48:06,120 --> 00:48:14,280
or the red, green, blue values. Anyway, to cut to the chaser to reiterate, I totally believe

562
00:48:14,280 --> 00:48:23,560
that at least now with the tools that we have, you get the best results by far by creating

563
00:48:23,560 --> 00:48:31,360
your own custom version. And I do. Now, the problem here is that I write code every day.

564
00:48:31,360 --> 00:48:36,960
And these things are not easy, right, even for extremely advanced developers. And it's

565
00:48:36,960 --> 00:48:42,080
very time consuming and very difficult. So there aren't, you know, I'm not trying to

566
00:48:42,080 --> 00:48:47,080
sound boastful, but there aren't many people like me who can spin up a custom design neural

567
00:48:47,080 --> 00:48:56,280
network in, you know, two days or a week. So I think it's going to boil down to eventually

568
00:48:56,280 --> 00:49:02,120
boil down to problems. One of the things that we talk about a lot at Microsoft is the

569
00:49:02,120 --> 00:49:13,280
democratization of AI or machine learning. And the analogy here is maybe you and some

570
00:49:13,280 --> 00:49:18,640
of your listeners can remember the days, the very early days when spreadsheets, Lotus

571
00:49:18,640 --> 00:49:25,800
1, 2, 3, we're just becoming popular. And a lot of people were saying, what, why, why

572
00:49:25,800 --> 00:49:31,880
are companies, including Microsoft, making these spreadsheets? Why would people, why would

573
00:49:31,880 --> 00:49:37,720
normal people ever want to use a spreadsheet? This is just something for accounts. So,

574
00:49:37,720 --> 00:49:44,360
but then Lotus 1, 2, 3, and later Excel and the others democratized numeric processing

575
00:49:44,360 --> 00:49:49,400
with spreadsheets. And then all of a sudden, all kinds of interesting good things happened

576
00:49:49,400 --> 00:49:57,600
from that. In much the same way, the goal to democratize machine learning is the idea

577
00:49:57,600 --> 00:50:07,280
that if you give some basic machine learning tools and knowledge to millions of people,

578
00:50:07,280 --> 00:50:10,680
they're going to find interesting ways to use it and solve problems that we haven't

579
00:50:10,680 --> 00:50:16,640
even thought of. That said, though, I still believe that just like you can only do so

580
00:50:16,640 --> 00:50:23,480
much with Excel and numeric processing, you'll only ever be able to do so much with a

581
00:50:23,480 --> 00:50:27,840
canned program, or no matter how powerful the tool is, and that there's always going

582
00:50:27,840 --> 00:50:34,840
to be the need for machine learning artisans. I don't know if I said that word, right?

583
00:50:34,840 --> 00:50:42,400
Go in and create custom models and custom prediction models for particular problems.

584
00:50:42,400 --> 00:50:48,960
Your description made me prompting me to ask myself, what is the VBA for deep neural

585
00:50:48,960 --> 00:50:51,400
nets? Exactly.

586
00:50:51,400 --> 00:51:04,800
And let's skip that right over a second. Before we leave, I want to talk to you about applications,

587
00:51:04,800 --> 00:51:13,120
including the stuff that you do around NFL scores. But before we leave that, there are

588
00:51:13,120 --> 00:51:19,360
a couple of areas that I wanted to dig into around. And these are all things that I noted

589
00:51:19,360 --> 00:51:26,160
that you wrote blog posts around. One of them is around, I made this comment about network

590
00:51:26,160 --> 00:51:32,040
architecture being the new feature engineering. But in fact, it sounds like there is some of

591
00:51:32,040 --> 00:51:38,520
the old feature engineering that's still important and that needs to be done around data encoding

592
00:51:38,520 --> 00:51:43,840
and normalization when dealing with neural nets and deep neural nets. And I was wondering

593
00:51:43,840 --> 00:51:50,840
if you could speak to that. And then I wanted to ask you about drop out and cross entropy

594
00:51:50,840 --> 00:51:52,840
error as well.

595
00:51:52,840 --> 00:51:59,160
Okay, so I didn't quite follow the first part of what you're asking, exactly.

596
00:51:59,160 --> 00:52:08,360
Oh, you wrote this blog post about data encoding and normalization. And I didn't dig into

597
00:52:08,360 --> 00:52:18,000
that post in a lot of detail, but I was wondering if there are specific techniques in those areas

598
00:52:18,000 --> 00:52:24,760
related to neural nets and deep neural nets beyond the kind of things that you do in traditional

599
00:52:24,760 --> 00:52:25,760
machine learning.

600
00:52:25,760 --> 00:52:31,400
Very, very interesting. This is something that I'll answer in direct as usual. It seems

601
00:52:31,400 --> 00:52:40,880
like I always do. The bottom line is, okay, neural networks, no matter how you slice and

602
00:52:40,880 --> 00:52:47,320
dice it currently, they only understand numbers. They're number crunchers, now very, very

603
00:52:47,320 --> 00:52:53,040
interesting complex number crunchers. So all of your input data eventually has to be,

604
00:52:53,040 --> 00:52:57,640
or not eventually has to be right away turned into some kind of numeric form, so it can

605
00:52:57,640 --> 00:53:04,240
be understood by the neural network. And people who are new to the field, this is often

606
00:53:04,240 --> 00:53:12,960
one of the most discouraging parts of learning machine learning is that it seems that there's

607
00:53:12,960 --> 00:53:20,840
an endless number of data transformation techniques and just all this data massaging before

608
00:53:20,840 --> 00:53:25,360
you can ever get to the really interesting part. And it can get quite depressing for

609
00:53:25,360 --> 00:53:30,520
new people, but I always tell my audiences when I'm doing training and things like that.

610
00:53:30,520 --> 00:53:35,880
That fortunately, there's only a discrete number of these things. You have to learn,

611
00:53:35,880 --> 00:53:43,440
for instance, there are four real ways, I mean, four major ways to normalize your data,

612
00:53:43,440 --> 00:53:48,280
so it's all scaled to roughly in the same range or so. Now, once you know those four,

613
00:53:48,280 --> 00:53:52,040
and once you understand when they're used and when they're not used to have a few examples,

614
00:53:52,040 --> 00:53:55,080
then you got it. But at first, you know, when you're first trying to learn it, it seems

615
00:53:55,080 --> 00:53:58,520
like, hopeless, oh, man, I've got to worry about data normalization. I've got to worry

616
00:53:58,520 --> 00:54:09,600
about data encoding in the same way that there is only a few ways for data encoding. Now,

617
00:54:09,600 --> 00:54:16,760
so the answer is that, and then, and none of those things have changed with deep neural

618
00:54:16,760 --> 00:54:23,360
networks. Got it. However, I'm always cautious to say because that's sort of the accepted,

619
00:54:23,360 --> 00:54:29,360
generally accepted truth. But I love to, you know, take, whenever I hear something like

620
00:54:29,360 --> 00:54:33,080
that, in fact, I hadn't really thought about it until you asked this question. I always

621
00:54:33,080 --> 00:54:39,720
like to go back and look and go, you know what? Is this really true? Just because everyone

622
00:54:39,720 --> 00:54:44,880
says it's true, it just sort of like creates like this viral thing. Here's an example

623
00:54:44,880 --> 00:54:50,440
where I argue all the time with my colleagues. It's something very basic. Suppose you're

624
00:54:50,440 --> 00:54:55,080
trying to use a neural network, this is going to be a little bit technical, but you're supposed

625
00:54:55,080 --> 00:54:59,320
you're trying to use a neural network to predict something that can only take one of two

626
00:54:59,320 --> 00:55:05,760
values. For instance, you're trying to predict whether a person is male or female based

627
00:55:05,760 --> 00:55:10,880
on a voting behavior, based on age, based on, based on all these other things. So another,

628
00:55:10,880 --> 00:55:18,120
it's a binary classification problem. Now, these standard and totally accepted by everybody,

629
00:55:18,120 --> 00:55:26,200
except me, technique is to create a neural network that has only a single output node.

630
00:55:26,200 --> 00:55:32,200
And that single output node is going to be a number between zero and one where values

631
00:55:32,200 --> 00:55:40,280
less than 0.5 are going to indicate one of the two outcomes, male, say, and values greater

632
00:55:40,280 --> 00:55:48,720
than 0.5 are going to indicate the other female. So, and that is mathematically efficient

633
00:55:48,720 --> 00:55:56,920
as opposed to the alternative of having a neural network that has two output nodes explicitly,

634
00:55:56,920 --> 00:56:05,320
where they sum to one. So, you still get the same result. In other words, if your listeners

635
00:56:05,320 --> 00:56:10,760
know about a multi-class classifier, you just use the exact same architecture, but with

636
00:56:10,760 --> 00:56:14,560
two output nodes. In other words, when you're doing classification, you will never ever

637
00:56:14,560 --> 00:56:20,040
see a two output node, neural network classifier, because the idea being that if you're trying

638
00:56:20,040 --> 00:56:24,560
to predict one of two things, just make it a single node. Well, I tell everybody, I go,

639
00:56:24,560 --> 00:56:29,520
you know, okay, yeah, it makes sense, but I haven't, you know, explain that to me, you

640
00:56:29,520 --> 00:56:36,760
know, prove that to me that one node is exactly equivalent to two nodes. And so, but anyway,

641
00:56:36,760 --> 00:56:44,840
my point, I'm getting fired up because I'm, like I'm, I'm passionate about questioning

642
00:56:44,840 --> 00:56:49,520
common knowledge. So, back to your thing. So, it's common knowledge now that the data

643
00:56:49,520 --> 00:56:55,920
encoding and normalization techniques, that were commonly used and are being used for

644
00:56:55,920 --> 00:56:59,800
standard neural networks. We don't need anything new for deep neural networks. I'm not so

645
00:56:59,800 --> 00:57:00,800
sure.

646
00:57:00,800 --> 00:57:10,520
Well, before we leave the specific example, are you, are you excited about questioning the

647
00:57:10,520 --> 00:57:19,880
fact that a two node network in this example is inferior to a one node network, or have

648
00:57:19,880 --> 00:57:26,560
you demonstrated that there are some cases that a two node network is superior to a single

649
00:57:26,560 --> 00:57:33,320
node network, or for some external reasons, by external reasons, I mean, like, you know,

650
00:57:33,320 --> 00:57:37,720
maybe they're the same in terms of accuracy, but implementation wise, one is better than

651
00:57:37,720 --> 00:57:44,480
any other. What's the source of your excitement around this question?

652
00:57:44,480 --> 00:57:50,520
There's two, I say a couple, or at least two reasons. Okay, one, and primarily, I think

653
00:57:50,520 --> 00:57:54,200
it's hard to sometimes to be, you know, self-evaluate. I think it's probably psychological

654
00:57:54,200 --> 00:58:02,600
in my part, where in my world, knowledge is power. Knowing more than someone else is

655
00:58:02,600 --> 00:58:09,160
considered, you know, our mark of success. Yeah, it take, I work with a lot of guys who

656
00:58:09,160 --> 00:58:16,680
work in some form of sales. And for them, you know, I mean, everybody's competitive,

657
00:58:16,680 --> 00:58:21,640
but for them, a measure of success for them is how much money they make, because that's

658
00:58:21,640 --> 00:58:29,440
the external kind of manifestation of their goodness in some way. So in, you know, research

659
00:58:29,440 --> 00:58:35,440
and stuff, your measure is no one something, or coming, understanding something, publishing

660
00:58:35,440 --> 00:58:39,280
something first, they're the people that, so I think that there's a psychology there,

661
00:58:39,280 --> 00:58:48,800
where, if most people like me, you know, were competitive in some sense of the definition,

662
00:58:48,800 --> 00:58:53,600
where if everybody is saying this, and I'm somehow able to prove, everybody else was

663
00:58:53,600 --> 00:58:58,600
wrong, I'd get great satisfaction out of that. So that does, I mean, it sounds kind

664
00:58:58,600 --> 00:59:04,640
of terrible, but I think that's part of it. Now, the other part is, from an implementation

665
00:59:04,640 --> 00:59:11,040
point of view, I know that working on the code end of things, every implementation that

666
00:59:11,040 --> 00:59:20,240
I've seen has a completely has sort of two different code bases for neural network classifiers,

667
00:59:20,240 --> 00:59:26,680
one for binary classification, and one for all other cases. But if, when you're classifying,

668
00:59:26,680 --> 00:59:31,720
doing a binary classification, and you have two output nodes, then you only have one

669
00:59:31,720 --> 00:59:35,680
code base. In other words, the neural network is the neural network, where the number of

670
00:59:35,680 --> 00:59:41,480
output nodes is the number of classes that you're trying to predict. So from that engineering

671
00:59:41,480 --> 00:59:44,640
point of view, it's very appealing.

672
00:59:44,640 --> 00:59:51,760
There's an elegance to have the same solutions at the same code base for independent of

673
00:59:51,760 --> 00:59:57,640
the specifics of the problem, or to not have the exception of the single class or the

674
00:59:57,640 --> 00:59:59,480
two class prediction.

675
00:59:59,480 --> 01:00:11,320
And then regarding training deep neural nets, what's the state of the art there? And I

676
01:00:11,320 --> 01:00:17,520
think my sense is that that training techniques, or tell me if this is true or not, that training

677
01:00:17,520 --> 01:00:23,360
techniques are tied very closely to architecture. At this point, meaning the research papers

678
01:00:23,360 --> 01:00:30,280
that talk about new architectures are also talking about specific training techniques for

679
01:00:30,280 --> 01:00:37,080
those architectures, or is that not the case, and then talk about dropout, which is, I

680
01:00:37,080 --> 01:00:40,280
think that was Jeff Hinton's group in 2014.

681
01:00:40,280 --> 01:00:41,280
Go ahead.

682
01:00:41,280 --> 01:00:43,280
If that's enough to get going with.

683
01:00:43,280 --> 01:00:48,640
Well, first of all, I agree with you for the first part of your question, and that in

684
01:00:48,640 --> 01:00:55,960
general, there are a few exceptions, but in general, if you create a custom network architecture,

685
01:00:55,960 --> 01:01:03,320
then you'll have to use a custom training algorithm optimization. Now, there's, I'll make

686
01:01:03,320 --> 01:01:10,840
a parenthetical remark that an area that I believe has great promise. And again, I'm

687
01:01:10,840 --> 01:01:18,160
in a very much of a minority view here, is that there are certain optimization algorithms

688
01:01:18,160 --> 01:01:24,600
and techniques that can be applied to any network architecture. And in general, they're called

689
01:01:24,600 --> 01:01:30,320
swarm intelligence optimization algorithms, particle swarm optimization, and so forth.

690
01:01:30,320 --> 01:01:37,160
There's some others. And basically, they just use absolute brute force, whereas most

691
01:01:37,160 --> 01:01:42,560
opt, they're not based, the swarm techniques are not based on calculus and gradients and

692
01:01:42,560 --> 01:01:49,000
things. So that's, you know, most optimization algorithms are based on calculus, and you

693
01:01:49,000 --> 01:01:54,360
have to calculate derivatives, and the derivatives depend on the architecture. So that's why you

694
01:01:54,360 --> 01:02:00,520
got to basically, in most cases, create a custom training algorithm, if you have a custom

695
01:02:00,520 --> 01:02:07,440
training custom neural architecture. And then, as I mentioned, I'm intrigued by the

696
01:02:07,440 --> 01:02:12,400
idea of applying the swarm optimization to these things. I've made a few stabs at it,

697
01:02:12,400 --> 01:02:17,760
but like anything else, there's just not enough time. Going back, I remember, in the

698
01:02:17,760 --> 01:02:22,120
previous discussion we were talking about, you know, my two node versus one node binary

699
01:02:22,120 --> 01:02:26,880
cluster, I just haven't had time to look at it. It would take, you know, I'd have to

700
01:02:26,880 --> 01:02:33,600
dedicate a week or two to that. And it's like all of us, you know, I mean, I've got more

701
01:02:33,600 --> 01:02:40,640
things that I have to do, than things, than time to do. So in short, I agree with you

702
01:02:40,640 --> 01:02:49,320
that custom training algorithms are needed with the possible exception of swarm optimization,

703
01:02:49,320 --> 01:02:53,000
which in my few stabs, I haven't been entirely successful, but I'm not ready to give up

704
01:02:53,000 --> 01:02:59,480
on them. Now, with regards to drop out as interest, there's a whole bunch of, not a whole

705
01:02:59,480 --> 01:03:05,680
bunch of techniques, but quite a few techniques and drop out is one. Now, I remember drop out

706
01:03:05,680 --> 01:03:14,160
training, which is closely related to jittering input jittering and so on, are all designed,

707
01:03:14,160 --> 01:03:19,840
are mostly designed to prevent overfitting during training. That's sort of their motivation,

708
01:03:19,840 --> 01:03:27,120
in most cases. And drop out training was everywhere, I'd say two to three years ago. It was a very

709
01:03:27,120 --> 01:03:35,040
hot area of research, a lot of excitement around it, and that sort of faded out for reasons

710
01:03:35,040 --> 01:03:44,480
which aren't clear to me. I have this in nagging suspicion a lot of times that trends in research,

711
01:03:44,480 --> 01:03:51,760
and you know, very high end mathematical research, are subject to trends and fashions, just

712
01:03:51,760 --> 01:03:56,720
like a lot of things are, and sometimes things fall out of favor for no apparent reason.

713
01:03:56,720 --> 01:04:03,640
An example of this that I like to point out is that there's a neural network training

714
01:04:03,640 --> 01:04:12,400
algorithm called resilient back propagation. It's a form of obviously a variation of back

715
01:04:12,400 --> 01:04:19,840
propagation. I did some experimentation on it where I generated artificial data sets, very

716
01:04:19,840 --> 01:04:28,320
large artificial data sets, and the resilient back propagation algorithm, I mean clearly outperformed

717
01:04:29,840 --> 01:04:36,320
normal back propagation. Now, I have to say that with an asterisk. The problem with, it's almost

718
01:04:36,320 --> 01:04:42,160
impossible to compare training algorithms because they all have so many hyperparameters

719
01:04:42,160 --> 01:04:49,760
typically the learning rate, momentum rate, regularization, you know, L1 regularity. There's

720
01:04:49,760 --> 01:04:56,800
just too many parameters. You're not completely comparing apples to oranges, but you're comparing

721
01:04:56,800 --> 01:05:02,400
two different kinds of apples, perhaps. So it's very difficult to tell. So anyway, drop out

722
01:05:02,400 --> 01:05:12,640
training is something that just seems to not be in fashion, but is there and I'm a believer

723
01:05:12,640 --> 01:05:18,400
in dropout training, but you know, it's kind of funny and now that you asked this question,

724
01:05:18,400 --> 01:05:26,480
I'm thinking back to recent neural networks that I've done. And I haven't been using dropout to

725
01:05:26,480 --> 01:05:35,280
tell you the truth because it is, it's in my world, you know, I spin up custom on neural networks

726
01:05:35,280 --> 01:05:43,680
myself and they're quite difficult to implement. It creates a lot of extra work. And so I take the

727
01:05:43,680 --> 01:05:52,080
often take the easy way out. And what is that easy way out? Is it, I mean besides I'm not using dropout,

728
01:05:52,080 --> 01:05:57,760
are there other things that you're doing with your data or there are other algorithms that have

729
01:05:57,760 --> 01:06:05,280
the same effect of avoiding overfitting? Or is it, you know, your standard, you know, data

730
01:06:05,280 --> 01:06:10,880
segmenting validation sets, that kind of thing? Yeah, you know, I got to be honest with you,

731
01:06:10,880 --> 01:06:16,160
I don't really have a good answer to that question, you know, I'm not sure to be perfectly honest.

732
01:06:16,160 --> 01:06:27,920
For one, you know, here, I was talking to the chief architect of Microsoft's CNTK tool,

733
01:06:27,920 --> 01:06:32,400
which is our, or it's released to the public, you can find it on GitHub. It's our version of

734
01:06:32,400 --> 01:06:39,600
TensorFlow. Deep neural networks, including convolutional neural networks and recurrent neural networks

735
01:06:39,600 --> 01:06:46,400
and stuff. And I was talking to him one time because he's not only a, he's a great, the main architect,

736
01:06:46,400 --> 01:06:55,680
very bright guy and named Frank. And we were talking and I asked him a question. I saw some really

737
01:06:55,680 --> 01:07:01,440
weird behavior that I didn't understand. I don't remember what the weird behavior was. And so I saw him

738
01:07:02,320 --> 01:07:07,600
in the hallway and I said, hey, Frank, you know, and then I described the phenomenon. And you go,

739
01:07:07,600 --> 01:07:14,800
I said, can you think of anything that would cause that to happen? By the way, later turned out,

740
01:07:14,800 --> 01:07:21,520
it was just a weird, it was just weird random randomness. But Frank thought about it before I

741
01:07:21,520 --> 01:07:25,360
goes, you know, the only thing I can think of is that you've got a bug in your code.

742
01:07:26,560 --> 01:07:32,960
And that was, I mean, he was, and I tell you, you know, when I saw that, when I saw the behavior

743
01:07:32,960 --> 01:07:37,520
that I've described, that's what, that was my first question. Oh, man, I must have a serious bug

744
01:07:37,520 --> 01:07:43,040
in my code somewhere. Well, it turns out that it wasn't a bug at all. I was just sort of bizarre

745
01:07:43,040 --> 01:07:48,800
behavior. And, but the conversation led us to talk about. And then, you know, we're a conversation

746
01:07:48,800 --> 01:07:55,680
sort of meandered. And I said, yeah, I remember. I told him the story how I spun up a neural network

747
01:07:55,680 --> 01:08:02,240
as a few years ago. And I was using it for, you know, work. It was actually using it. It was

748
01:08:02,240 --> 01:08:08,480
performing very well until one day I was looking at the code, dusted off the code, and realized,

749
01:08:08,480 --> 01:08:17,680
I'd missed a, I'd completely missed updating one of the bias values. In other words,

750
01:08:18,480 --> 01:08:25,440
I completely was ignoring one of the constants in the equation. And yet, the neural network was

751
01:08:25,440 --> 01:08:30,480
performing well. So the moral of the story. And so I mentioned that to Frank, he goes, yeah, I've

752
01:08:30,480 --> 01:08:36,320
done this many times myself. So what's happening here is when you create a neural network, it's

753
01:08:36,320 --> 01:08:42,320
really, really hard to tell it if it's good or not, because you can get, you can get good results

754
01:08:42,320 --> 01:08:48,720
and have a seriously flawed implementation. So in the same way by coming back to this dropout

755
01:08:48,720 --> 01:08:55,600
thing where adding dropout or moving dropout, you'd think it'd be a relatively easy to tell

756
01:08:55,600 --> 01:09:02,640
is this helping me or hurting me? It's not at all easy to determine. And in total, there are

757
01:09:02,640 --> 01:09:07,520
a few things that have been really interesting for me about this conversation. But one of the most

758
01:09:08,400 --> 01:09:18,560
is the, like the hard definitive stand you took on the need to craft your own networks. And

759
01:09:18,560 --> 01:09:28,880
I think how that relates to here is, I don't know, I guess the idea that deep neural nets are,

760
01:09:30,080 --> 01:09:36,640
you know, they're kind of magic black boxes, right? And they're particularly magic,

761
01:09:36,640 --> 01:09:41,600
they're magic black boxes, even if you built them from scratch. And they're particularly,

762
01:09:41,600 --> 01:09:45,520
they're going to be even worse if you are using something out of the box that you don't fully

763
01:09:45,520 --> 01:09:54,800
understand. Absolutely. And I think this also relates to, you know, there's always this question

764
01:09:54,800 --> 01:10:01,920
around, you know, using these out of the box tools. And, you know, for many types of problems,

765
01:10:01,920 --> 01:10:07,040
you're trying to get from zero to 80 percent. And, you know, the researchers are trying to get

766
01:10:07,040 --> 01:10:14,640
from 95.2 percent to 95.7 percent. And so that's kind of an argument for, well, you know, just use the tool.

767
01:10:16,880 --> 01:10:21,520
You know, but if you, yeah, even if you're just trying to get to 80 percent, if you really need to

768
01:10:21,520 --> 01:10:29,120
understand what's happening, or you need to be able to understand what's happening in the case where

769
01:10:30,320 --> 01:10:35,840
it generally works great, but for whatever some variant in your input data produces

770
01:10:35,840 --> 01:10:41,760
outlandishly wrong results, like you have to know what's going on under the covers.

771
01:10:42,560 --> 01:10:45,920
Quite, I mean, I think you, I think you phrased that really, really well.

772
01:10:47,600 --> 01:10:53,120
I guess, how are you doing on time? Are you still, you have, I have a meeting that started

773
01:10:53,600 --> 01:10:59,600
right now. Okay. So we'll have to wrap this up. I'm afraid. Okay. So we'll wrap this up.

774
01:10:59,600 --> 01:11:06,000
Maybe I can just ask you to quickly tell us about you, you mentioned you do your own research, you've

775
01:11:06,000 --> 01:11:11,920
done some projects like NFL scores, like what's your, what's the project that you're most excited

776
01:11:11,920 --> 01:11:16,480
about? And maybe give us a quick overview of that. And if it's something that's public where we can

777
01:11:16,480 --> 01:11:23,840
learn more. Well, the project I'm working on right now, interestingly enough, is that Microsoft

778
01:11:23,840 --> 01:11:31,360
recently launched what's called the AI school. Okay. This is a big deal. Microsoft is a large

779
01:11:31,360 --> 01:11:40,080
organization and we create products and services. Microsoft has made a massive investment, both

780
01:11:40,080 --> 01:11:50,880
money wise and sort of culture wise, where our senior leadership believes that putting intelligence,

781
01:11:50,880 --> 01:11:57,120
real intelligence into every product and service that we do is critically important.

782
01:11:59,120 --> 01:12:06,480
So some of our senior leaders I've seen say something where they believe that this wave of

783
01:12:06,480 --> 01:12:11,760
adding artificial intelligence and machine learning intelligence into our products is every

784
01:12:11,760 --> 01:12:19,840
bit as important as, you know, the internet came to pass. So towards that, Microsoft created what's

785
01:12:19,840 --> 01:12:30,800
called the AI school. And I was hired from my previous to help run the AI school because I had

786
01:12:30,800 --> 01:12:36,640
a background in education. And I'd say that I'm pretty relative to most of my peers. I have a

787
01:12:36,640 --> 01:12:43,440
pretty broad knowledge of many areas of machine learning AI. I'm not nearly as deep as they are,

788
01:12:43,440 --> 01:12:49,920
of course. So that's what I'm working on right now. I'm trying to spin up, trying to determine how

789
01:12:49,920 --> 01:12:56,480
to transfer knowledge of all these things that we just talked about and place that knowledge into

790
01:12:56,480 --> 01:13:03,280
the hands of the software developers that we have, the project managers that we have, the business

791
01:13:03,280 --> 01:13:08,800
decision makers that we have, the salespeople who sell our products and generate the revenue that

792
01:13:08,800 --> 01:13:15,360
you know, keeps me employed because everybody I was surprised we we sent out an announcement to

793
01:13:15,360 --> 01:13:22,240
this like over creating the AI school. And we thought we'd get a, you know, maybe a couple hundred

794
01:13:22,800 --> 01:13:31,520
messages of interest exclusively from engineers and developers. But we got thousands and thousands

795
01:13:31,520 --> 01:13:37,200
literally we're I mean, we were overwhelmed by the response and not only just from engineers.

796
01:13:37,200 --> 01:13:46,800
I think engineers see pretty clearly that machine learning and artificial intelligent skills are

797
01:13:46,800 --> 01:13:53,840
quickly becoming must have skills for them. In other words, they're going to have to know how to

798
01:13:53,840 --> 01:13:57,520
put logistic regression in or they're going to have to know the difference between this kind of

799
01:13:57,520 --> 01:14:02,400
classifier and that kind of classifier. But so that made sense, but we were surprised by the number

800
01:14:02,400 --> 01:14:10,320
of people, designers, UI people, literally across the organization people, exactly. So that's

801
01:14:10,320 --> 01:14:14,880
what I'm working on now. And I'm very, you know, passionate about this and very interested in it.

802
01:14:14,880 --> 01:14:21,200
And trying to deliver this knowledge while at the same time, in fact, I remember the the

803
01:14:21,200 --> 01:14:28,320
researcher hired me to run this and he's actually in charge of this one of the most famous names

804
01:14:28,320 --> 01:14:36,080
in speech recognition. He basically created the technology behind Cortana, which is the same as

805
01:14:37,040 --> 01:14:42,480
technology behind here. I mean, extremely famous guy, but and he told me when I was interviewing

806
01:14:42,480 --> 01:14:48,400
for this position from my my old position just upstairs, by the way, he told me, you know, how

807
01:14:48,400 --> 01:14:56,720
you going to manage or how are you going to balance doing, you know, your job of creating training

808
01:14:56,720 --> 01:15:04,160
classes and delivering classes and doing videos and stuff with the need to stay up to date because

809
01:15:04,160 --> 01:15:09,760
things are rolling out on a, you know, weekly, monthly basis. Right. And I said, well, you know,

810
01:15:09,760 --> 01:15:14,000
that's that's the challenge. So, you know, I'll conclude by saying, you know, I'm really excited

811
01:15:14,000 --> 01:15:19,520
about working on the Microsoft AI school, but also really excited about all the all the things that

812
01:15:19,520 --> 01:15:26,400
are going on, generative adaptive neural networks and and all these other things. And now you've got,

813
01:15:26,400 --> 01:15:30,240
you certainly got me excited about this AI school and probably a lot of listeners as well as

814
01:15:30,240 --> 01:15:36,880
this primarily an internal resource or it will it be a public resource that my Microsoft is

815
01:15:36,880 --> 01:15:41,280
promoting. Yeah, it's a good good good. Something that we've talked about and we're really not quite

816
01:15:41,280 --> 01:15:47,920
sure. You know, our our mandate, of course, initially at least is to provide this internally.

817
01:15:47,920 --> 01:15:53,520
No, it's not a secret or anything, but we don't have any externally facing kind of information

818
01:15:53,520 --> 01:15:59,920
very much. But on the other hand, a lot of people are saying, Hey, you know, I mean, we want to the

819
01:15:59,920 --> 01:16:05,680
the content that we develop could be useful to everybody. Right. The deal here is that there's

820
01:16:05,680 --> 01:16:11,440
a lot of content out there already. What we're trying to do is find our sweet spot where

821
01:16:12,000 --> 01:16:17,920
how can we use our particular areas of expertise? We don't want to just rehash and redo,

822
01:16:17,920 --> 01:16:24,480
say, for instance, most of your listeners probably know about Andrew Eng out of Stanford, his

823
01:16:24,480 --> 01:16:29,920
excellent online course at Switch, which I think are probably pretty much state of the art.

824
01:16:29,920 --> 01:16:33,600
We don't want to just try to replicate that for a couple of reasons. We'd be wasting our time

825
01:16:33,600 --> 01:16:40,080
and we probably wouldn't do as good a job. So we're trying to find areas where we have our internal

826
01:16:40,080 --> 01:16:46,480
expertise and certainly not only the knowledge, but the method of delivery. And once we figure that

827
01:16:46,480 --> 01:16:51,680
out, then I fully believe that we'll be able to share that with everybody. Great. Great. With that

828
01:16:51,680 --> 01:16:58,000
James, you've been very gracious with your time. Thank you so much. And look forward to

829
01:16:58,000 --> 01:17:02,400
keeping in touch and to our, you know, when we meet in person at the the future of data summit.

830
01:17:03,280 --> 01:17:05,920
Thanks, Sam. It was a pleasure chatting with you and thanks for your time.

831
01:17:09,920 --> 01:17:15,280
All right, everyone. That's our show for today. Once again, thank you so much for listening and

832
01:17:15,280 --> 01:17:21,440
for your continued support. Please remember that we want to hear from you. You can comment on

833
01:17:21,440 --> 01:17:28,160
the show via the show notes page via the at Twomo AI Twitter handle or my own at Sam Charrington

834
01:17:28,160 --> 01:17:36,400
handle via our new Facebook and YouTube pages or just via good old fashioned email to Sam at Twomo AI.com.

835
01:17:37,600 --> 01:17:42,240
Please do show some love to our new Facebook and YouTube pages though. Your likes and

836
01:17:42,240 --> 01:17:47,280
subscribers there will really help support the show. And remember, if you're catching this

837
01:17:47,280 --> 01:17:53,200
podcast on Friday, you've still got time to register for our Strata Hadoop giveaway. The winner

838
01:17:53,200 --> 01:17:58,720
will be announced on next week's show. The notes for this show and all the links I've mentioned

839
01:17:58,720 --> 01:18:14,640
will be posted at Twomo AI.com slash talk slash 13. Thanks again for listening and catch you next time.

