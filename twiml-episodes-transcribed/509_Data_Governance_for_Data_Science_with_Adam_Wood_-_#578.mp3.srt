1
00:00:00,000 --> 00:00:11,600
All right, everyone. Welcome to another episode of the Twemal AI podcast. I am your host,

2
00:00:11,600 --> 00:00:17,680
Sam Charrington. Today, I'm joined by Adam Wood, Director of Data Governance and Data Quality

3
00:00:17,680 --> 00:00:23,600
at MasterCard. Of course, before we get going, be sure to take a moment to hit that subscribe

4
00:00:23,600 --> 00:00:28,560
button wherever you're listening to today's show. I had the pleasure of meeting Adam at a

5
00:00:28,560 --> 00:00:33,360
recent event I hosted with Claudeira, a data leader's round table back in February.

6
00:00:34,240 --> 00:00:39,840
And it is a pleasure to welcome him to the show to talk a little bit about data governance

7
00:00:39,840 --> 00:00:45,040
for machine learning. Adam, welcome to the podcast. Thank you for having me, Sam. It's good to see you

8
00:00:45,040 --> 00:00:52,080
again. It's great to see you and looking forward to digging into our chat. Of course, I love to have

9
00:00:52,080 --> 00:00:57,920
you introduce yourself to our audience and share a bit about how you how you came to work in data.

10
00:00:58,880 --> 00:01:03,840
Yeah, absolutely. So a lot of people don't know this by by education. I've got a doctorate

11
00:01:03,840 --> 00:01:10,720
in biochemistry and through several twists and turns of career moved into IT at Monsanto and

12
00:01:10,720 --> 00:01:16,960
didn't want to do anything else. So I've spent roughly 12 years, the last 12 years of my career

13
00:01:16,960 --> 00:01:23,920
in the data governance and data strategy space and that has spanned Monsanto healthcare companies,

14
00:01:23,920 --> 00:01:30,960
the financial industry and picked up a lot of knowledge. How data science has grown,

15
00:01:30,960 --> 00:01:35,760
how governance has been applied within those different industries, how data is treated

16
00:01:35,760 --> 00:01:40,080
within those different industries and the different sensitive information and regulations that

17
00:01:40,080 --> 00:01:45,200
get applied across the board. So with my current job, I've tried to bring all of that industry

18
00:01:45,200 --> 00:01:49,440
knowledge forward and see how it can help MasterCard progress in their initiatives.

19
00:01:50,160 --> 00:01:58,560
Awesome, awesome. You know, let's maybe start by talking about the various use cases

20
00:01:59,360 --> 00:02:07,040
for the data and data platforms that you're working with at MasterCard. Both machine learning

21
00:02:07,040 --> 00:02:12,880
and beyond. How's data being used there? Yeah, so MasterCard obviously being a global company,

22
00:02:12,880 --> 00:02:18,000
we bring in a lot of information and you can guess what that information is related to. We

23
00:02:18,000 --> 00:02:22,160
process a lot of credit card information for banks in different merchants globally.

24
00:02:23,280 --> 00:02:28,080
The governance needs there have really grown with the volume of data that we process.

25
00:02:28,640 --> 00:02:35,520
And even, you know, the growth of data science as a function is driven more of the need

26
00:02:36,320 --> 00:02:40,720
for governance across the board. So is data privacy regulations have grown?

27
00:02:40,720 --> 00:02:49,280
As the volume of information has grown, the need for us to regulate and do more with it from

28
00:02:49,280 --> 00:02:55,600
a governance standpoint has definitely grown as well. But downside is governance as a classical

29
00:02:55,600 --> 00:03:02,960
initiative tended to get in the way. And so our data needs are for more open use of the information

30
00:03:02,960 --> 00:03:09,200
and putting guardrails on the information rather than being this blocker to data access and use.

31
00:03:09,200 --> 00:03:16,960
So the majority of what we work on right now are ways to automatically detect and catalog sensitive

32
00:03:16,960 --> 00:03:22,080
information across the company and across the borders, the different countries that we do business

33
00:03:22,080 --> 00:03:28,160
in and to make sure every single security and privacy regulation is being followed down to the

34
00:03:28,160 --> 00:03:33,680
letter. And doing so, what we've been able to do is bring solutions forward that enable the

35
00:03:33,680 --> 00:03:40,240
data science community to understand where information lives, what it means, how to access it and

36
00:03:40,240 --> 00:03:46,640
how to do so responsibly using privacy, privacy management and consent management to make sure

37
00:03:46,640 --> 00:03:51,280
anything that we're using the information for is always in line with regulations that we're

38
00:03:51,280 --> 00:03:58,400
facing up to. Now, governance used to be very inflexible. The regulations are changing more and

39
00:03:58,400 --> 00:04:04,000
more. New ones are being added to the table all the time and the data science world has become

40
00:04:04,000 --> 00:04:09,600
incredibly flexible and needs to be moving fast. So everything that we do from a data science

41
00:04:09,600 --> 00:04:15,760
standpoint at MasterCard and actually at several other companies has had two flex with cloud

42
00:04:15,760 --> 00:04:21,920
adoption with machine learning and AI, the scale of data that's needed, the performance needs that

43
00:04:21,920 --> 00:04:27,840
are available. And I would tell you the number one thing that we're focused on right now is heavy

44
00:04:27,840 --> 00:04:33,440
reuse, helping people understand that there is information out there that other data science teams

45
00:04:33,440 --> 00:04:39,200
have curated and put together. And the quality rules that have been applied don't have to be the

46
00:04:39,200 --> 00:04:44,800
spoke. They can be shared across teams. So helping connect data science teams through our governance

47
00:04:44,800 --> 00:04:51,120
platforms and helping them move faster through shared components and reuse has been one of the

48
00:04:51,120 --> 00:04:58,080
largest undertakings that we brought forward. When we think about governance, well, you mentioned

49
00:04:58,080 --> 00:05:03,760
one of the things that is often thought about is this barrier, maybe internal bureaucracy that you

50
00:05:03,760 --> 00:05:09,440
have to overcome to use your data. But more broadly, I guess you can kind of think of it as they're

51
00:05:09,440 --> 00:05:16,880
being like carrots and sticks in terms of why you might invest in data governance and build out

52
00:05:16,880 --> 00:05:24,400
those kinds of processes. And before we started recording, you shared an example of a significant

53
00:05:24,400 --> 00:05:29,600
stick that was passed down by a regulatory body. Can you talk a little bit about this example

54
00:05:29,600 --> 00:05:36,800
that you shared and what it means for folks that are using data on a global scale?

55
00:05:36,800 --> 00:05:43,520
Yeah, absolutely. So the stick that I was referring to has to do with GDPR and the newest

56
00:05:43,520 --> 00:05:50,960
regulations around Shrems. So Amazon was slapped with an $898 million fine. And it was because they

57
00:05:50,960 --> 00:05:57,920
were using data for some of their data science projects that users had consented to them collecting

58
00:05:57,920 --> 00:06:03,360
but they had not consented to the way it was being used. And so privacy regulations have gone

59
00:06:03,360 --> 00:06:08,800
well beyond. I need to know where my data is, how you're tracking it and making sure it's encrypted

60
00:06:08,800 --> 00:06:15,280
and that you're being responsible as you store it. Now, it's if you're going to take my information,

61
00:06:15,280 --> 00:06:21,680
you can only use it in the way that I stated you could. So consent management alongside

62
00:06:21,680 --> 00:06:27,920
the information for people to consume has to be available. People have to have the understanding

63
00:06:27,920 --> 00:06:32,560
that this information that we've collected in this particular place can only be used for these

64
00:06:32,560 --> 00:06:38,800
purposes. And that's where a lot of governance tooling has moved to. So, you know, regulations

65
00:06:38,800 --> 00:06:44,240
and the growth of data science have driven a lot of the governance discipline. You're right,

66
00:06:44,240 --> 00:06:50,560
it used to be a massive blocker. It used to be very prohibitive. Now, it's become much more agile,

67
00:06:50,560 --> 00:06:56,960
a lot more automated and light touch so that we are bringing as many tools forward as possible to

68
00:06:56,960 --> 00:07:04,240
track metadata, policies, consent, and map that side by side with the information on what the

69
00:07:04,240 --> 00:07:09,280
data is and where it's stored. So that is any data science consumer comes forward.

70
00:07:10,000 --> 00:07:16,480
Once to use it, they know the guardrails that they have to to follow. Yeah, I was very surprised to

71
00:07:16,480 --> 00:07:25,200
hear about this, this Amazon fine. For many years, we've kind of operated in this gray area where,

72
00:07:25,200 --> 00:07:31,680
you know, there are all kinds of open questions with regard to, you know, what happens if you take

73
00:07:31,680 --> 00:07:37,520
data and incorporate it into a model, you know, privacy, you know, it wasn't really, you know,

74
00:07:37,520 --> 00:07:42,240
it was indeterminate, you know, there were legal and IP issues that were kind of indeterminate.

75
00:07:43,440 --> 00:07:48,640
Sounds like, you know, some of these things are getting clearer and at least in the case that you

76
00:07:48,640 --> 00:07:56,000
mentioned, the, you know, it's falling back on, you know, if explicit consent wasn't given for

77
00:07:56,000 --> 00:08:02,080
that use, then it can't be used there. And I think, you know, I wonder how many organizations are

78
00:08:02,080 --> 00:08:07,200
operating, you know, with that kind of constraint in place. You know, it has gotten clearer,

79
00:08:07,200 --> 00:08:12,320
but it's also gotten more difficult to manage because the variety of data that people are bringing

80
00:08:12,320 --> 00:08:19,280
in, it gets scattered very quickly. So making sure that you have governance processes highly

81
00:08:19,280 --> 00:08:25,280
automated and scalable processes to make sure that consent and privacy standards are being

82
00:08:25,280 --> 00:08:29,680
adhered to, making sure that data is encrypted at rest and the different types of things that

83
00:08:29,680 --> 00:08:36,000
regulators are asking for. That's all built into these data cataloging tools, which have taken,

84
00:08:36,000 --> 00:08:42,640
taken huge indicators from the data science community as a whole. So the management of privacy

85
00:08:42,640 --> 00:08:49,680
and consent has been incredibly difficult to start because the, the traditional BI, if I go back

86
00:08:49,680 --> 00:08:56,720
10 years, there was no such thing as serving up raw information to an end consumer, right? It

87
00:08:56,720 --> 00:09:01,040
was all going through something that had already been curated, something that had already been

88
00:09:01,040 --> 00:09:09,440
deemed safe. Exactly. It was only exposed through a handful of BI tools. Or there was an army of

89
00:09:09,440 --> 00:09:15,120
data architects and engineers that handled a handful of databases. So they knew what all the

90
00:09:15,120 --> 00:09:22,560
information represented and how to be careful with it. So now data science needs access to the raw.

91
00:09:22,560 --> 00:09:26,880
They don't need the curated layer because they want to be able to build features off of the

92
00:09:26,880 --> 00:09:32,560
raw information and they want to be exposed to all of the variables there to be able to create a

93
00:09:32,560 --> 00:09:37,200
true model of the raw information that they'll encounter instead of trying to build an AI model

94
00:09:37,200 --> 00:09:43,360
on top of a business intelligence layer. It's kind of a no-brainer. So but looking at rather than

95
00:09:43,360 --> 00:09:48,480
going through these highly curated views where, where consent and policy assignment or

96
00:09:48,480 --> 00:09:53,600
everything else would be so much easier, the difficulty has, has become going to all of the

97
00:09:53,600 --> 00:10:00,560
underlying product databases and operational databases and figuring out how to merge consent

98
00:10:00,560 --> 00:10:06,000
and privacy with the underlying data stores. And I'll keep this entire conversation. I'll keep

99
00:10:06,000 --> 00:10:12,400
coming back to data catalogs that are really providing that opportunity. And even you'll see

100
00:10:12,400 --> 00:10:16,960
some of the services being stood up in the public cloud providers right now that are helping

101
00:10:16,960 --> 00:10:21,520
facilitate this. And Amazon just published, I want to say it was a month and half the two months

102
00:10:21,520 --> 00:10:28,080
ago that they've released crawlers for their cloud environments to help identify sensitive

103
00:10:28,080 --> 00:10:32,560
and personal information in all of their cloud environments that can also be fed into their data

104
00:10:32,560 --> 00:10:37,520
catalog. So even the public cloud providers are starting to release these capabilities that

105
00:10:37,520 --> 00:10:42,960
enable governance but do it in a very scalable and automated way. One of the other things that

106
00:10:42,960 --> 00:10:51,280
came up that is just a level of complexity that never really occurred to me. This was something

107
00:10:51,280 --> 00:10:58,240
we discussed that the data leaders roundtable was, you know, you have kind of introduced this

108
00:10:58,240 --> 00:11:09,040
issue of governance and, you know, the requirement to track user consent. But then when you

109
00:11:10,240 --> 00:11:18,720
talk about it at kind of a global scale, you know, that global scale implies not just, you know,

110
00:11:18,720 --> 00:11:30,160
more, more records, more, you know, access. But also this requirement to federate in a sense

111
00:11:30,160 --> 00:11:34,560
because it's not just one regulatory regime. You have all these different regulatory regimes.

112
00:11:34,560 --> 00:11:39,920
You've got requirements that data, you know, stay local to a particular

113
00:11:42,160 --> 00:11:47,280
locale. You know, of course, you can articulate this better than me. You kind of walk through some

114
00:11:47,280 --> 00:11:51,520
of the complexity that you run into there. Yeah. So some of the newest standards that have come

115
00:11:51,520 --> 00:11:57,840
out of, you know, India, China, Germany, South Africa. It's this concept of nationalism.

116
00:11:58,800 --> 00:12:03,200
And each one of these countries brings forth a different set of rules to where

117
00:12:03,920 --> 00:12:09,040
where data is created. And if it carries certain attributes, it can't leave that country.

118
00:12:09,840 --> 00:12:14,960
And when you look at a lot of global companies, their information is processed centrally,

119
00:12:14,960 --> 00:12:21,040
data warehouses are built centrally. And so a lot of AI models and a lot of machine learning

120
00:12:21,040 --> 00:12:28,480
models are grabbing training sets centrally. But what happens now when you can't centralize

121
00:12:28,480 --> 00:12:34,640
that information and everything has to be federated? I know this is certainly, certainly impacted

122
00:12:34,640 --> 00:12:40,640
governance on the data science side as well. I think a lot of companies are struggling with how to

123
00:12:40,640 --> 00:12:47,280
train locally and then build the model centrally. But I can tell you one of the things that's

124
00:12:47,280 --> 00:12:52,400
that's another area of governance that's gaining a lot of momentum and has been for the last three

125
00:12:52,400 --> 00:12:59,520
to four years is the concept of data lineage, which helps us understand where all of the same

126
00:12:59,520 --> 00:13:05,840
data sets are occurring from even in a federated model and how they can be matched and merged and

127
00:13:05,840 --> 00:13:12,240
flow together. This is incredibly beneficial too as we're exchanging information across

128
00:13:12,240 --> 00:13:17,280
different boundaries between different countries to make sure that we stand compliance with

129
00:13:17,280 --> 00:13:25,760
regulations. But at the same time, data scientists as long as I've known any mature data scientists

130
00:13:25,760 --> 00:13:31,120
initiatives, they want to know where their information is sourced from, how it was constructed,

131
00:13:31,120 --> 00:13:38,400
and now is there anything that's limiting its use regionally or consent or privacy related?

132
00:13:39,040 --> 00:13:45,280
The nationalism is bringing in a new area of complexity where you can only use information

133
00:13:45,280 --> 00:13:52,880
or a certain location in that location. So data lineage is really helping us uncover exactly

134
00:13:52,880 --> 00:13:58,480
where that information is being sourced from so that data scientists can make sure that they're

135
00:13:58,480 --> 00:14:03,280
only using it for a specific purpose in that region now. So it's given us a different layer of

136
00:14:03,280 --> 00:14:11,920
granularity to try and protect people from using information in a way it wasn't intended or

137
00:14:11,920 --> 00:14:18,720
moving outside of the boundaries of a country with it. Now I know a kind of fast moving data science

138
00:14:18,720 --> 00:14:23,600
world in which you've got folks, you know, building all kinds of different pipelines and doing

139
00:14:23,600 --> 00:14:29,040
different kinds of transformations, pulling this information into models, sometimes hierarchical,

140
00:14:30,800 --> 00:14:35,520
you know, models, how are folks keeping track of lineage?

141
00:14:35,520 --> 00:14:41,120
Yeah, so some of the newest initiatives that I've been part of, there's some brilliant people

142
00:14:41,120 --> 00:14:44,800
in the data science community of MasterCard that really have a good handle on this.

143
00:14:45,440 --> 00:14:52,320
But as we're building feature stores, we can build in as new feature sets are being created,

144
00:14:52,320 --> 00:14:59,840
we can build in the metadata around how, what types of, how is each feature constructed,

145
00:14:59,840 --> 00:15:06,320
what types of cleansing logic was used, and all of that information is going to be published

146
00:15:06,320 --> 00:15:11,120
back to our data catalog so that the data science community not only understands that these

147
00:15:11,120 --> 00:15:15,680
feature sets are out there and available for use, they understand how they were built.

148
00:15:15,680 --> 00:15:21,520
And so at the intention here, you're right, data science is moving faster and faster.

149
00:15:22,000 --> 00:15:25,840
One of the things that we want to do to enable people is make sure that they have a shared

150
00:15:25,840 --> 00:15:32,160
set of components to use. Rather than 30 or 40 people going after the exact same data set and

151
00:15:32,160 --> 00:15:37,920
taking the time to create their own feature set from it, now we can use data lineage.

152
00:15:37,920 --> 00:15:43,280
We can use the metadata that's been gathered around other curated feature sets that have been

153
00:15:43,280 --> 00:15:48,240
built. Combine that with the information from the feature store to help people locate that

154
00:15:48,240 --> 00:15:54,160
information and move towards it and start using it very quickly. So the agility of data science,

155
00:15:55,280 --> 00:16:00,080
they're going to keep coming up with new use cases, but as people build new feature sets,

156
00:16:00,880 --> 00:16:05,280
we want to make sure that that's immediately understood and available for other people to consume.

157
00:16:05,840 --> 00:16:10,320
As chances are, if there's some new wonderful initiative, a new data science program coming

158
00:16:10,320 --> 00:16:15,680
forward, a lot of people are going to be going after the same data. And whoever reaches the end

159
00:16:15,680 --> 00:16:20,000
goal first and build something that's really valuable, we want other people to start consuming that

160
00:16:20,000 --> 00:16:25,200
right away. With regard to the feature store and metadata management in particular,

161
00:16:27,360 --> 00:16:32,800
talk a little bit more about how you've attacked that problem, what you've built,

162
00:16:34,320 --> 00:16:39,200
and kind of where that's going. So this all started off of the ability to just

163
00:16:39,200 --> 00:16:47,120
curate data. Not even moving towards a feature site yet. It was just how do we build the data sets

164
00:16:47,120 --> 00:16:52,880
from the raw information? How do we aggregate from multiple databases or denormalize something

165
00:16:52,880 --> 00:16:58,160
from multiple data warehouse sources to build something that we could use to train a particular model?

166
00:16:59,360 --> 00:17:05,040
So all of our metadata management, our data catalog was already consuming that information,

167
00:17:05,040 --> 00:17:11,840
meaning we had tracked every single database and every table and every column and every shred of

168
00:17:11,840 --> 00:17:17,280
business metadata that we could get for that information. So that as data scientists were starting

169
00:17:17,280 --> 00:17:23,280
to build curated data sets, they understood what they were bringing in. And then as they built

170
00:17:23,280 --> 00:17:28,880
their data sets, we've got some very, very incredible partners that are publishing back metadata

171
00:17:28,880 --> 00:17:35,280
for what they've done to construct these sets. So we now know that in these tables,

172
00:17:35,280 --> 00:17:40,640
this is information that was sourced from x, y, and z. We're intending to use it for

173
00:17:41,200 --> 00:17:45,040
these additional problems. It's a training set for this set of models.

174
00:17:46,480 --> 00:17:52,800
The discipline that I would love to see going forward is that within any sort of data science

175
00:17:52,800 --> 00:17:59,760
platform, they make sure to relate back to how that particular feature set was constructed to

176
00:17:59,760 --> 00:18:06,240
make sure all of these components are fully understood. We do see gaps here and there. So I look

177
00:18:06,240 --> 00:18:11,280
at it as let's say I cooked a wonderful dinner and I didn't write down any of the recipe.

178
00:18:11,920 --> 00:18:16,080
And now I've got to go do it again. I've got to start from scratch. We don't want people to have

179
00:18:16,080 --> 00:18:22,480
to do that. So what we do with the feature store is very similar to that. So every set that's

180
00:18:22,480 --> 00:18:30,240
being created, you know, it's how was this feature normalized, you know, how was it clandes,

181
00:18:30,240 --> 00:18:36,960
how was any sort of back fills done to make sure that this was a complete feature set for someone

182
00:18:36,960 --> 00:18:43,120
to be able to consume and use and for what purposes were people consuming it. So again, if I'm

183
00:18:43,120 --> 00:18:49,520
working under a particular set of consent assumptions and I'm able to see other data sets or

184
00:18:49,520 --> 00:18:55,200
feature sets that were built from that same set of assumptions and that same consent. I'm off to

185
00:18:55,200 --> 00:19:00,400
the races. I don't have to spend any time trying to construct it myself. I may have some tweaks

186
00:19:00,400 --> 00:19:05,120
that I want to make to it, but I've got a huge leg up. Historically, when we've talked about feature

187
00:19:05,120 --> 00:19:13,600
stores, you know, one aspect of the feature store is kind of making the features available and

188
00:19:13,600 --> 00:19:21,920
kind of ensuring consistency of features and things like that. And another aspect of it is more

189
00:19:21,920 --> 00:19:27,360
kind of the store part of this, not as in storage, but as in a place that you, you know, a marketplace

190
00:19:27,360 --> 00:19:36,880
almost or really focusing on feature reuse and enabling, yeah, teams of void kind of reinventing

191
00:19:36,880 --> 00:19:45,600
the wheel. And there's been this historical kind of, you know, I guess it depends a lot on the

192
00:19:45,600 --> 00:19:49,600
organization. Sometimes you find folks push back on that because they don't want to, you know,

193
00:19:49,600 --> 00:19:54,320
create something that they need for their project and then have to, you know, own and support it,

194
00:19:54,320 --> 00:20:01,200
you know, and have it be used in ways that it wasn't intended. You mentioned reuse earlier.

195
00:20:01,200 --> 00:20:09,360
I'd love for you to share a bit about how you see reuse playing out in org as big as mastercard.

196
00:20:09,360 --> 00:20:14,080
Yeah, so I can tell you some other companies in the way they've addressed it and some of the things

197
00:20:14,080 --> 00:20:20,240
that we're thinking about as well. If you look at some of the companies that have really published a

198
00:20:20,240 --> 00:20:27,840
lot of their governance process and their data catalogs, Lyft, Uber, Spotify, they have all gone and

199
00:20:27,840 --> 00:20:34,640
built their own and they've built their own with data science in mind. So how do you get to reuse?

200
00:20:34,640 --> 00:20:40,720
Whenever you search for particular information, it actually brings back sample queries that have

201
00:20:40,720 --> 00:20:46,640
been run, the people that that have been most frequently accessing that information samples of the

202
00:20:46,640 --> 00:20:53,840
data sets. So, you know, it drives reuse by helping you build a community of people that are

203
00:20:53,840 --> 00:21:00,880
after the exact same information. The way we've approached it is we work with the data science

204
00:21:00,880 --> 00:21:07,120
community who is the larger teams that are building these data sets, bring them into our data

205
00:21:07,120 --> 00:21:15,840
catalog and then there's a way to certify them as top quality assets. And so we also add additional

206
00:21:15,840 --> 00:21:21,760
metadata so that any data science coming in can look for these are the certified assets that are

207
00:21:21,760 --> 00:21:26,880
available for the data science community, see them immediately and then be able to access them.

208
00:21:27,920 --> 00:21:33,840
We do also take some approaches running analytics on our data science environments to look at

209
00:21:33,840 --> 00:21:39,600
query behaviors. Very similar to what these other companies were doing, but looking at behaviors of

210
00:21:39,600 --> 00:21:46,560
individual users within individual teams to see what data they're going after and can we connect

211
00:21:46,560 --> 00:21:52,560
that to a data set that's already been curated and is available to use and then we can actually

212
00:21:52,560 --> 00:22:00,320
drive those users towards those those curated feature sets. So right now we can't guarantee reuse,

213
00:22:01,520 --> 00:22:08,080
but we can do we can strongly promote reusable feature sets that are out there and by including

214
00:22:08,080 --> 00:22:13,600
the logic that was used to build them, we make sure that we can iterate on top of those and

215
00:22:13,600 --> 00:22:18,880
continue to build new ones from a base component rather than trying to, like I said, reinvent the

216
00:22:18,880 --> 00:22:24,160
wheel every single time. In the broader context of ensuring the responsible use of machine learning

217
00:22:24,160 --> 00:22:31,120
and dealing with issues like bias, the ideas of like data cards and data sheets for data sets

218
00:22:32,160 --> 00:22:39,840
have been proposed and are increasingly popular. Are you doing anything like that to kind of

219
00:22:39,840 --> 00:22:46,800
profile the data and the the way it's been sourced and the various biases so that folks

220
00:22:47,920 --> 00:22:54,240
that are trying to use it have more visibility into that? So I would say I'm not as familiar with

221
00:22:54,240 --> 00:23:01,840
any bias that's gone into the data. We do source a lot of external information as many companies do.

222
00:23:02,960 --> 00:23:07,920
We also have several third-party acquisitions that we've made. So whenever we don't have the

223
00:23:07,920 --> 00:23:14,160
information or the capability internally, master card's been making a lot of acquisitions and the

224
00:23:14,160 --> 00:23:21,760
open banking space and fraud detection and identity management. And so we do and every single one of

225
00:23:21,760 --> 00:23:26,960
those instances when we start looking at the information that those companies bring forward, we're

226
00:23:26,960 --> 00:23:33,840
always thinking about new analytics that can be built. And the first the first issue that's always

227
00:23:33,840 --> 00:23:41,280
tackled is what is underrepresented? What do they have that's new to us? What do we have that's

228
00:23:41,280 --> 00:23:46,800
probably new to them? And are there any gaps remaining? Is there anything if we're going to build a

229
00:23:46,800 --> 00:23:53,920
comprehensive model over this information? What type of information do we need to make to or bring

230
00:23:53,920 --> 00:24:00,240
in so that the end result is not skewed? Trying to eliminate as much bias as possible because a lot

231
00:24:00,240 --> 00:24:05,760
of these smaller acquisitions, they cater to one class of user, one class of customer.

232
00:24:06,720 --> 00:24:10,640
So when we're bringing in acquisitions, of course, there's going to be some level of bias,

233
00:24:10,640 --> 00:24:15,360
but master card has the advantage of being a very large global company with an absolutely

234
00:24:15,360 --> 00:24:21,600
massive customer base. And so those types of evaluations help us understand bias pretty quickly.

235
00:24:22,320 --> 00:24:27,680
Now in terms of data profiling, this is also a space that we're spending considerable efforts

236
00:24:27,680 --> 00:24:35,920
and considerable time in going into really understanding across the board, value distributions,

237
00:24:36,720 --> 00:24:41,680
what is the use of the information? Is it reference data? Is it sensitive information?

238
00:24:42,240 --> 00:24:46,880
And we're doing all of this to make sure that we manage encryption. We're making sure that we

239
00:24:46,880 --> 00:24:53,680
manage lineage and flow of sensitive information appropriately. And it's also tied to consent

240
00:24:53,680 --> 00:25:00,400
management. I would say that particular information though, we're still working on ways to really tie

241
00:25:00,400 --> 00:25:05,840
it to the data science community and make it usable. I think the first foray into that is really

242
00:25:05,840 --> 00:25:11,200
going to be around the consent and privacy management, but then figuring out ways to make that give

243
00:25:11,200 --> 00:25:16,080
additional value to data science teams from our data profiling results is something we're exploring

244
00:25:16,080 --> 00:25:22,560
right now. How does data quality plan into all of this? Yeah, so it's always been a really special

245
00:25:22,560 --> 00:25:30,080
relationship between data quality and data science. Mostly because most of the data science teams

246
00:25:30,080 --> 00:25:35,600
that I've worked with, the first words out of their mouth are don't touch the data. Don't try to

247
00:25:35,600 --> 00:25:41,440
sanitize anything before it hits my hands because I want to have influence over the way that it's

248
00:25:41,440 --> 00:25:48,400
cleansed. So some of the things that we really try to do, we know when there is something that's

249
00:25:48,400 --> 00:25:53,120
that's very error prone. For instance, credit card numbers that come in with special characters

250
00:25:53,120 --> 00:25:58,960
and letters in them. Okay, there might be something in that transaction that is valuable to a data

251
00:25:58,960 --> 00:26:04,800
science team, but chances are it's going to have other errors in it and we don't want people to

252
00:26:04,800 --> 00:26:11,680
start to use that type of information. So data quality is really used more from a product development

253
00:26:11,680 --> 00:26:19,680
standpoint. Is it recommended that this data is fit for use or fit for purpose? But in terms of

254
00:26:19,680 --> 00:26:25,040
data quality for data science, a lot of those measures are actually coming from the data science

255
00:26:25,040 --> 00:26:32,160
teams themselves rather than these dedicated data quality organizations that exist and almost every

256
00:26:32,160 --> 00:26:38,560
enterprise organization right now. So data quality for data science has really taken on a life of

257
00:26:38,560 --> 00:26:44,080
its own. It's become a very interesting space in terms of cleansing and future creation.

258
00:26:44,880 --> 00:26:50,960
And so the big level of encouragement that I give to these data science groups is great.

259
00:26:50,960 --> 00:26:56,640
You know, the tools that we have for regular operational data quality, they track what we're

260
00:26:56,640 --> 00:27:01,600
running. We know exactly what we're running it against. If you're building a brand new pipeline,

261
00:27:01,600 --> 00:27:08,880
you have to do the same thing. And you also, my big recommendation is take the data quality

262
00:27:08,880 --> 00:27:15,680
measures and and cleansing measures that you're building and make them some form of reusable component.

263
00:27:15,680 --> 00:27:20,560
If you're going through the effort of doing it, almost like a feature store, there needs to be

264
00:27:20,560 --> 00:27:27,120
some form of feature cleansing store. And that way when you're providing metadata around how you've

265
00:27:27,120 --> 00:27:33,440
created everything, you can cleanly reference the processes that you used to get there or at least

266
00:27:33,440 --> 00:27:39,680
some logic around what's been done from your personal data quality standpoint to let other people

267
00:27:39,680 --> 00:27:45,600
understand how things were created. But data quality is an interesting space in data science.

268
00:27:45,600 --> 00:27:51,760
It's taking shape a lot more than it was. You know, it was sort of go at it alone, cleanse the

269
00:27:51,760 --> 00:27:58,000
information, however you see fit. And you know, the normal data quality measures that organizations

270
00:27:58,000 --> 00:28:03,760
would run don't touch my data. Stay out of it. Let me do what I need to do. Those two are going

271
00:28:03,760 --> 00:28:08,800
to have to meet in the middle somewhere. I think we're on the way to doing that. Just because the

272
00:28:08,800 --> 00:28:15,440
the level of effort it takes to keep reinventing feature sets and features and everyone applying

273
00:28:15,440 --> 00:28:20,560
the same logic over and over with no way to reference it centrally. I think that's about to

274
00:28:20,560 --> 00:28:27,120
change very soon. When we spoke earlier, you also reference data domain discovery. How does that

275
00:28:27,120 --> 00:28:33,840
play into all of this? Yeah. So data domain discovery is a term that's used specifically by

276
00:28:33,840 --> 00:28:41,200
Informatica. Informatica is governance suite. But the domain discovery relates down to what is the

277
00:28:41,200 --> 00:28:48,320
logic that I can run as part of data profiling that lets me understand the type of information that's

278
00:28:48,320 --> 00:28:53,360
in each particular column or what each particular key in a document represents.

279
00:28:54,320 --> 00:29:01,600
And so what companies are starting to do with that information is as you have a series of what are

280
00:29:01,600 --> 00:29:09,600
just loosely called tags or data domains or some form of label for each document, each table,

281
00:29:09,600 --> 00:29:14,880
each column, each key, you can build additional logic on top of that to figure out what governance

282
00:29:14,880 --> 00:29:21,360
policies you have to assign. And so data domains are another one of these ways that can be fully

283
00:29:21,360 --> 00:29:28,000
automated, that can be fully scalable and is one of the newer governance processes that's really

284
00:29:28,000 --> 00:29:35,920
taken hold. There is no manual intervention in these processes. It's something to where you can

285
00:29:35,920 --> 00:29:43,120
scale across as many databases, hybrid cloud environment on premise, fully cloud environment,

286
00:29:43,120 --> 00:29:50,160
and come away with your data sets being labeled with what they contain. And that type of logic

287
00:29:50,160 --> 00:29:55,040
can immediately help you understand if there are different types of security needs, if there are

288
00:29:55,040 --> 00:30:01,040
different types of policy and privacy implications. So is the general idea here that

289
00:30:02,320 --> 00:30:06,880
you know we've got some new database maybe it comes in for being an acquisition or a third party

290
00:30:06,880 --> 00:30:14,640
source and it hasn't been previously curated and this these data discovery data domain discovery

291
00:30:14,640 --> 00:30:23,920
tools can identify columns that represent PII like addresses or phone numbers names. Yeah that's

292
00:30:23,920 --> 00:30:29,360
exactly the point. So the scale of information that every company is bringing in is absolutely

293
00:30:29,360 --> 00:30:35,520
gotten massive logarithmic growth. And not only that data brokers have started to become I mean

294
00:30:35,520 --> 00:30:40,560
those are in the news all the time people selling information whether whether that's appreciated

295
00:30:40,560 --> 00:30:47,360
or not a lot of companies are using information that they didn't generate that someone else did

296
00:30:47,360 --> 00:30:54,000
and now they have to take ownership of it. And so what data domain discovery or data tagging does

297
00:30:54,000 --> 00:30:59,760
is it looks at that information and tries to determine for many patterns in it is it an email address

298
00:30:59,760 --> 00:31:06,560
phone number IP address anything that could be used to reverse engineer your identity or the

299
00:31:06,560 --> 00:31:14,880
identity of your financial information. So obviously fraud has become a very very big deal in

300
00:31:14,880 --> 00:31:21,120
recent years the sophistication of identity theft and people that are stealing credit cards and

301
00:31:21,120 --> 00:31:28,320
making transactions in your name matter of fact two years ago someone filed my my IRS tax return

302
00:31:28,320 --> 00:31:35,280
in my name. So people are getting really creative and so making sure that we understand all

303
00:31:35,280 --> 00:31:41,600
information that we're taking in and processing making sure that any sensitive information around

304
00:31:41,600 --> 00:31:47,360
a customer or bank or any sort of financial transaction is well understood and protected.

305
00:31:48,000 --> 00:31:53,120
This is where the entire governance industry is headed. So these are definitely initiatives

306
00:31:53,120 --> 00:32:01,680
that we've taken on and it also helps us in terms of R&D or POC methods to know which

307
00:32:01,680 --> 00:32:08,240
information needs to be anonymized encrypted we apply that same thing to our operations making

308
00:32:08,240 --> 00:32:13,280
sure that we understand which information needs to be encrypted or even hashed make sure it's

309
00:32:13,280 --> 00:32:19,040
secured at all times. And so this is this is akin to data profiling it's an additional step that

310
00:32:19,040 --> 00:32:24,000
results in labeling of each individual element in a database but that makes sure in an automated

311
00:32:24,000 --> 00:32:30,400
way across the board globally we have the right controls in place to understand what our data

312
00:32:30,400 --> 00:32:37,440
represents and to make sure our customers stay safe. So we've talked a lot about the tooling

313
00:32:37,440 --> 00:32:47,120
around data management data catalog even feature stores a lot of organizations are also investing

314
00:32:47,120 --> 00:32:54,640
in building out platforms for the the data science parts of the workflow is that something that

315
00:32:54,640 --> 00:33:00,160
master card is doing as well. Yeah absolutely so within within our data warehousing environments

316
00:33:00,160 --> 00:33:07,040
we're big users of cladera we have roughly a 20 petabyte Hadoop environment right now very

317
00:33:07,040 --> 00:33:12,560
large contains a ton of information and one of the native components there is the data science

318
00:33:12,560 --> 00:33:20,560
workbench. And so along with that the majority of our data science workflows the data pulls a

319
00:33:20,560 --> 00:33:26,400
lot of the aggregations a lot of the the testing file and training file storage is all happened

320
00:33:26,400 --> 00:33:32,080
with happening within that singular platform. That's also where a lot of our governance activities

321
00:33:32,080 --> 00:33:39,280
have focused to understand we're we're using cladera manager to be able to pull logs how people

322
00:33:39,280 --> 00:33:44,640
are accessing the information how they're interrogating the information and being able to build

323
00:33:44,640 --> 00:33:49,840
different types of analytics to help connect different members of our user community.

324
00:33:50,960 --> 00:33:58,080
So it's it's been a decentralized platform for data science at master cards so far and we're

325
00:33:58,080 --> 00:34:04,480
exploring other ones as well but but Hadoop and cladera have played a very large part for us and

326
00:34:04,480 --> 00:34:11,920
even within the newest releases Apache Atlas as a data catalog component of the cladera platform

327
00:34:11,920 --> 00:34:15,840
has come forward and we're looking forward to building out some additional integrations with

328
00:34:15,840 --> 00:34:22,400
that to strengthen our global governance footprint. Got it and historically one of their pitches

329
00:34:22,400 --> 00:34:30,000
for the data science workbench is kind of this integration between the governance capability

330
00:34:30,000 --> 00:34:36,000
and their like historical data management tooling and the data science workflows and now

331
00:34:37,280 --> 00:34:41,600
you know even emphasizing the ability to deploy on different clad environments which you

332
00:34:41,600 --> 00:34:47,040
mentioned earlier. So even some of the newer things that we've done within data cataloging and

333
00:34:47,040 --> 00:34:52,960
this is happening everywhere it's not a standalone platform. We're building a bunch of API

334
00:34:52,960 --> 00:34:58,480
services that can be accessed and called directly from within data science workbench to be able to

335
00:34:58,480 --> 00:35:05,840
both pull out metadata and push back metadata. So I was referencing you know how to share

336
00:35:05,840 --> 00:35:11,120
how you've built different feature sets or different trainings sets. We want people to be able to

337
00:35:11,120 --> 00:35:15,920
do that natively from within the platform that they're working in so they can search our catalog

338
00:35:15,920 --> 00:35:20,960
natively from within data science workbench and then turn around and at the end of the day when

339
00:35:20,960 --> 00:35:27,120
they when they've built their data set publish that metadata back in so that we'd get a record of

340
00:35:27,120 --> 00:35:32,160
not only how it was built but also where it's stored now so that other people can have access to it.

341
00:35:32,160 --> 00:35:37,360
So Adam a lot of what we've talked about thus far has been you know from your perspective as the

342
00:35:37,360 --> 00:35:42,960
the governance and data management person informed by the work you've done with data scientists

343
00:35:42,960 --> 00:35:50,240
is there a set of things that you wish data scientists knew about the space beyond what we've

344
00:35:50,240 --> 00:35:55,840
talked about or things that you wish that they would do that would make them more effective in

345
00:35:55,840 --> 00:36:02,400
thinking about working with governance. Absolutely so I mentioned I've been in this space for

346
00:36:02,400 --> 00:36:10,720
10 to 12 years right nobody cared about governance when I started it was pure overhead nobody wanted

347
00:36:10,720 --> 00:36:17,920
to do it but then data science became a discipline and it had to get more flexible and now what you'll

348
00:36:17,920 --> 00:36:23,120
find is a lot of these governance platforms take their opportunities from data science.

349
00:36:23,120 --> 00:36:29,120
I think we've all made peace with this is where innovation comes from this is where profitability

350
00:36:29,120 --> 00:36:34,000
is going to come from for a lot of companies and our products have to be able to support that

351
00:36:34,960 --> 00:36:40,240
governance used to get in the way it can't do that anymore when you're chopping at the revenue

352
00:36:40,240 --> 00:36:46,640
stream of your company you've got to make changes so my my biggest advice to data scientists

353
00:36:46,640 --> 00:36:51,840
first and foremost get acquainted with what the platform does get acquainted with how to

354
00:36:51,840 --> 00:36:56,560
contribute information to and consume information from a data catalog because I guarantee your

355
00:36:56,560 --> 00:37:04,720
company has one a lot of people just don't know of its existence in most cases. Beyond that figure

356
00:37:04,720 --> 00:37:10,720
out how you become a contributor and I don't just mean a contributor to the data catalog or your

357
00:37:10,720 --> 00:37:18,480
governance program what I mean is if you're building feature sets figure out what metadata you can

358
00:37:18,480 --> 00:37:25,040
do to improve the work of everyone else around you if you're building data quality logic if you're

359
00:37:25,040 --> 00:37:30,560
building cleansing logic if you're building up feature sets make sure that you're tracking how

360
00:37:30,560 --> 00:37:36,880
that information was constructed and that you're putting it into a central repository so that people

361
00:37:36,880 --> 00:37:43,840
can access it at a later date when you do that when it hits that central repository that's where data

362
00:37:43,840 --> 00:37:50,000
tagging and data profiling is happening so you get the added benefit of letting us do the

363
00:37:50,000 --> 00:37:56,240
automated processes to protect your information for you. We can now let you and everybody else know

364
00:37:56,960 --> 00:38:01,920
if there's sensitive information in it is a data set that's open for use and can be used by anybody

365
00:38:01,920 --> 00:38:10,640
internally. Now a lot of the processes that we run are enablers we try incredibly hard not to

366
00:38:10,640 --> 00:38:16,160
get in the way anymore. What we want to do is make sure data science gets served the information

367
00:38:16,160 --> 00:38:20,960
that they need to be more effective and like you were talking about earlier we are here to promote

368
00:38:20,960 --> 00:38:28,880
reuse and direct people to the right information and how to use it responsibly and so the final

369
00:38:28,880 --> 00:38:36,800
thing that I'll say is I want every data scientist to focus on reuse. Heavy reuse I mean feature

370
00:38:36,800 --> 00:38:41,680
stores came about because the industry was saying we can't keep going at this alone.

371
00:38:43,680 --> 00:38:50,240
Well the data catalog is helping you not have to but in order to do that you have to focus on the

372
00:38:50,240 --> 00:38:55,200
ways that you're constructing data sets and the way that you're tracking that information. You can

373
00:38:55,200 --> 00:39:00,640
contain some of that in your feature store. I'd ask from a governance standpoint that you've pushed

374
00:39:00,640 --> 00:39:07,520
the rest of that metadata into the data catalog so that privacy compliance cyber security everyone

375
00:39:07,520 --> 00:39:13,840
else has the privilege to see your work and make sure that is protected for you. We're not here to

376
00:39:13,840 --> 00:39:19,680
cause roadblocks we're here to be an uplift and help you get to your angle faster but we want to

377
00:39:19,680 --> 00:39:26,320
make sure that we have you set for success through making sure you're adhering to privacy consent

378
00:39:26,320 --> 00:39:32,560
all these other risks that are out there so that you can move very quickly in a de-risked environment.

379
00:39:32,560 --> 00:39:39,200
Awesome awesome well Adam thanks so much for joining us and sharing a bit about your experience

380
00:39:39,840 --> 00:39:45,280
with data governance as great to chat once again. Yeah absolutely thank you so much Sam.

381
00:39:45,280 --> 00:39:55,280
Thanks Adam.

