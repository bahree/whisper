WEBVTT

00:00.000 --> 00:16.400
All right, everyone. I am super excited to be here with Peter Abil. Peter is a professor at UC

00:16.400 --> 00:21.680
Berkeley, where he's co-director of the Berkeley Artificial Intelligence Research Lab,

00:21.680 --> 00:29.040
or Bear, as well as co-founder, president, and chief scientist at CoVarians. Peter, welcome back

00:29.040 --> 00:34.160
to the Twimal AI podcast. Sam, really good to see you again. Thanks for having me.

00:35.120 --> 00:42.160
It is really hard to believe that it's been just under four years since you were on this show.

00:42.800 --> 00:48.800
You were guest, your interview was number 28, and we're approaching 500 now.

00:50.560 --> 00:56.960
What a journey. What a journey. You've been up to a lot of cool stuff, including starting your

00:56.960 --> 01:03.680
own podcast. We'll talk a little bit about that. But since it's been so long and you've been up

01:03.680 --> 01:09.760
to so much, why don't you take a few minutes and share with our audience a little bit of background

01:09.760 --> 01:17.520
and what your primary research interests are. Sure, yeah. So four years ago, long time.

01:18.400 --> 01:25.120
So a lot has happened since. The things I'm mostly thinking about these days are kind of too

01:25.120 --> 01:36.240
fold. One is with my covariant hat on as co-founder. At CoVarians, we're trying to bring

01:37.280 --> 01:44.160
AI robotics into the real world. So take it out of the lab, out of simulation, and make robots

01:44.160 --> 01:50.800
do things for us in the real world. So law happening there and looking forward to talking more about

01:50.800 --> 01:57.280
that with you, Sam. And then my other had at Berkeley, we're doing academic research in AI and

01:57.280 --> 02:04.240
robotics. And I would say some of the things we're most excited about these days are work at the

02:04.240 --> 02:11.360
junction of unsupervised learning and reinforcement learning, where of course unsupervised learning

02:11.360 --> 02:17.280
is where the learning happens without any annotation of the data, which is nice because it can be

02:17.280 --> 02:21.440
a lot more efficient that way, no annotation needed. But then reinforcement learning, the trial,

02:21.440 --> 02:26.960
and error learning that can allow robot to acquire skills from its own experience, its own practice.

02:27.520 --> 02:32.000
And so the combination of those two is we're spending a lot of time right now at Berkeley.

02:32.880 --> 02:40.720
Awesome. And we'll dig into both of those areas in detail. But before we do, for those who

02:40.720 --> 02:47.040
didn't catch your podcast four years ago or don't know your story, how'd you get into robotics?

02:47.840 --> 02:54.880
Sure. Yeah. So for me, in some ways, it didn't start in robotics per se. It started with artificial

02:54.880 --> 03:02.560
intelligence, the kind of brain behind the robots. For me, it was, well, when I was in underground

03:02.560 --> 03:07.760
studying all kinds of things, it just seemed everything was interesting. But it also seemed that

03:07.760 --> 03:13.680
to do well, he got to pick something to specialize into. And so the question was, okay, what is

03:13.680 --> 03:19.760
going to be the most interesting thing to spend my time on? And for me, it became pretty clear that

03:20.400 --> 03:26.320
it's so intriguing that we as humans can think. It's kind of what sets us apart, I would say,

03:26.320 --> 03:33.840
mostly from other animals is that we can think deeper, harder than that most animals. And so

03:33.840 --> 03:39.680
that to me was so fascinating. And of course, the first thing you might think is, okay, then study

03:39.680 --> 03:44.960
neuroscience, because that's studying how the brain works. But that discipline just, I mean,

03:44.960 --> 03:49.360
fascinating, but it just seems so hard to make progress on to, you know, dissect the brain and

03:49.360 --> 03:54.960
understand how it works. It's so difficult. So I figured try to engineer something that thinks,

03:55.920 --> 04:02.240
seems more natural as a way to make some progress and get closer to what we're doing as humans when

04:02.240 --> 04:07.440
we're thinking. Do you follow neuroscience research at all? Does that inspire you?

04:08.960 --> 04:12.640
Oh, there's a lot of things in neuroscience that have inspired me. I think it's,

04:13.680 --> 04:18.400
it's really interesting. The thing that maybe inspires me most these days is these findings about

04:20.000 --> 04:24.800
how general the human brain is. So there are these findings studies from several years back now,

04:25.440 --> 04:30.400
where it was shown that you probably heard us before, Sam, but it was shown that you can, for

04:30.400 --> 04:37.360
example, put the electrode pattern on a person's tongue. And this person can be blindfolded,

04:37.360 --> 04:44.160
it can be, can be blind. And if the electrical pattern on your tongue is active in the same kind of

04:44.160 --> 04:49.600
pattern as an image that you would see with your eyes, your brain can learn to process that and see

04:49.600 --> 04:55.760
and they had his demonstration where somebody was effectively rock climbing while seeing through their

04:55.760 --> 05:02.080
tongue. And so this kind of thing to me blows my mind down. The brain is so general what you use

05:02.080 --> 05:08.400
for taste input can be used to see. And so yeah, I think it's really inspiring to go that direction

05:08.400 --> 05:17.600
of generality. Absolutely, absolutely. Let's maybe start with talking about what you're up to

05:17.600 --> 05:26.240
at covariant. I think the last time we spoke, if I remember correctly, you were a year or so out

05:26.240 --> 05:34.640
from founding covariant. Yeah, so last time we were a little, yeah, still about a year before we

05:34.640 --> 05:43.920
started covariant. And what happened is we started seeing this trend that it seemed a lot of progress

05:43.920 --> 05:52.720
had been made by awesome and by many others on building more intelligent robots. And that always

05:52.720 --> 05:57.440
seemed a missing piece. If you look at robots out in the world that are doing useful things for us,

05:57.440 --> 06:03.840
you go to a car factory and you look around, it's amazing. So many robots, right? And they're building

06:03.840 --> 06:09.600
cars and that's, that's amazing. But if you look at the details, these robots are doing the same

06:09.600 --> 06:16.240
motion effectively over and over and over, very diligent. But also, it requires a lot of structure.

06:16.240 --> 06:24.000
Most problems cannot be solved by just repeated motion. And so it seemed three, four years ago,

06:24.000 --> 06:29.520
it seemed that maybe time had come where we can make robots smarter. Enough research

06:29.520 --> 06:35.040
progress had been made to start taking on bringing this to the real world, to give robots

06:35.040 --> 06:41.360
the ability to think and see and think and react to their environment, rather than just following

06:41.360 --> 06:46.480
pre-purgant motions. And that seemed just like it would open up so many more opportunities

06:46.480 --> 06:51.360
than what was possible until then. And so that was for us really to trigger this notion. Okay,

06:51.360 --> 06:57.920
we think we can start building robots that can see, react, think, and do many more things

06:57.920 --> 07:05.600
down pre-purgant robots can do. Now, the opportunity for robots that can think is obviously quite

07:05.600 --> 07:11.920
broad under specific tasks or problems that you're going after, is this the classical

07:11.920 --> 07:18.480
pick and place kind of robotics problem? Or are you beyond that? How do you think about the

07:18.480 --> 07:24.480
problem domain? It's so interesting. You bring that up because you say pick and place, right? And

07:24.480 --> 07:30.640
it's exactly what we're looking at, but it's not how we started out. So we started out and we said,

07:30.640 --> 07:36.800
let's do a full investigation of what are the most pressing problems when we talk with

07:37.760 --> 07:43.200
manufacturing companies, with warehousing companies with agricultural firms and so forth

07:43.200 --> 07:49.120
construction. And we spent the first half year of Covaran talking with about 200 different

07:49.120 --> 07:55.680
companies and getting a sense for, especially if you could have a really smart robot that

07:57.760 --> 08:02.320
given its physical foreign factor, you know, a standard industrial robot,

08:02.320 --> 08:09.760
but it's more smarter, what would it be able to do for you? And many, many things people want

08:09.760 --> 08:16.560
wanted robots to do, but it became also very clear that in logistics, slash warehousing,

08:16.560 --> 08:23.280
distribution centers, e-commerce fulfillment, that's where everybody was just

08:24.400 --> 08:30.960
hurting for help. They wanted robots to come help. They had already automated.

08:32.000 --> 08:36.160
What effect will is done with legs? They're running around the warehouse. A lot of people know about

08:36.160 --> 08:41.360
the Kiva robots at Amazon that can go under shelves and bring shelves to people. There's other

08:41.360 --> 08:46.720
systems too, but the same idea that the leg work has been automated, but that's a structured

08:46.720 --> 08:51.280
automation problem. It's not an AI problem in the same way that we're looking at AI these days.

08:52.560 --> 08:57.520
But to hand work, what people do with their hands, it just, there was no automation for it,

08:57.520 --> 09:02.480
and they wanted. They want to complete that automation process of these warehouses.

09:03.200 --> 09:08.880
And after six months, it became so clear to us that's the problem. We need to focus on,

09:08.880 --> 09:13.920
at least first, hopefully we can generalize from there, and we think we will, but that's how

09:13.920 --> 09:21.440
our initial focus is effectively pick and place type solutions for warehousing logistics,

09:21.440 --> 09:29.200
fulfillment, and so forth. I'm imagining that a lot of the progress in that domain to date has been

09:29.920 --> 09:37.600
adapting the robots to the very specific form factors of the problem that someone is trying to solve.

09:37.600 --> 09:42.640
We've got, you know, these boxes, we're going to align these boxes in this way, and so that the

09:42.640 --> 09:48.240
robot can play some smaller part. And maybe a part of what you're trying to do is loosen those

09:48.240 --> 09:54.960
constraints by making the robots smarter and more agile in their ability to manipulate in the

09:54.960 --> 10:01.680
real world. That's exactly right. So when you go into a warehouse, there is already a lot of

10:01.680 --> 10:07.840
conveyors and mobile robots and so forth that bring things where they generally to be, but they don't

10:07.840 --> 10:14.640
bring individual items. They tend to bring shelves or boxes. And then the next step is for

10:16.080 --> 10:24.800
either a worker or a robot to look into that box and pick out the one item that's needed

10:24.800 --> 10:33.920
to fulfill the next order. And to do that, you actually need a very general kind of capability. And

10:33.920 --> 10:38.000
I think that's probably a bit surprising to many people who are not familiar with warehouse,

10:38.000 --> 10:44.160
but many of these warehouses have millions of different skews, which are stockkeeping units. And

10:45.760 --> 10:50.800
these skews also turn over very quickly to have different packaging and so forth. So you cannot,

10:50.800 --> 10:56.320
it's not enough to say, oh, these are the skews. I'm just going to set up a system that's ready

10:56.320 --> 11:03.360
for these specific skews. You actually need to somehow build a AI system that understands the

11:03.360 --> 11:09.360
general notion of object that can look at a box, items in the box, especially the box itself

11:09.360 --> 11:15.520
is easy, but the items in the box and understand, oh, there are items there. And there's an item over

11:15.520 --> 11:19.920
there, another one over there, there's overlapping items there. And that's the one I want to pick.

11:19.920 --> 11:23.600
And here's how I'm going to pick it where I'm going to place my suction cup or my gripper.

11:23.600 --> 11:27.840
And this is how I'm going to maneuver it out of that box without flinging anything else

11:28.640 --> 11:37.440
out of the box too. And for us, as humans, these things are very simple to do. You can do them

11:37.440 --> 11:42.480
without thinking you can be, you know, maybe listening to a podcast or something while you're

11:42.480 --> 11:48.720
doing this. I think you about was being talked about there. But to get a robot to do that

11:48.720 --> 11:55.680
is actually really, really hard. It's one of those things where, sure, it's not too hard to get,

11:55.680 --> 12:01.280
you know, 80% success, but to get to the levels of autonomy that you need to be

12:01.920 --> 12:09.520
trustworthy in a commercial setting, several nines of reliability, 99.9% and above,

12:10.240 --> 12:14.960
all of a sudden becomes really, really hard because this system now needs to understand

12:14.960 --> 12:20.560
pretty much any situation that encounters. And while most people proud and not so familiar with

12:20.560 --> 12:27.360
warehouses, it's very similar to self-driving cars. We've had demos of self-driving cars for

12:27.360 --> 12:33.520
10 years or longer. And the demos today don't necessarily look all that different if you watch a

12:33.520 --> 12:39.040
one minute, two minute demo of a self-driving car, but they've become more reliable. But it remains

12:39.040 --> 12:43.840
very hard to get to perfect or near-perfect reliability, of course. That's the challenge at

12:43.840 --> 12:50.080
long tail of always new things you can encounter, either as a self-driving car and that demand or,

12:50.080 --> 12:55.760
as a warehouse robot, where you're always seeing new items, new packaging, and somehow you need to

12:55.760 --> 13:05.360
make sense of that and reliably pick in place. It's maybe an interesting analogy. I'm thinking of

13:05.360 --> 13:14.480
many folks in the field. Maybe Andrew Ng is one of the most well-known who has kind of talked

13:14.480 --> 13:20.240
about this view that, you know, self-driving cars will first happen when we constrain their

13:20.240 --> 13:26.960
environment, you know, maybe the autonomous bus lane and an airport as opposed to general self-driving

13:26.960 --> 13:34.880
ability. We've already done a lot of that work in the industrial realm. You've talked about the

13:34.880 --> 13:40.800
conveyor robots and how we've, you know, we've really constrained that environment. Is there,

13:40.800 --> 13:47.360
you know, does that say that maybe the, you know, that's not going to be enough even for

13:48.240 --> 13:51.680
self-driving cars or is that, you know, pushing the analogy too far?

13:51.680 --> 13:58.080
Yeah, I think, I think it's a really good question. The self-driving car side of things,

13:59.520 --> 14:06.800
is it possible to create real value within constrained environments? For example, the natural one

14:06.800 --> 14:12.080
would be lanes dedicated to self-driving cars. They might make it much easier, probably,

14:12.080 --> 14:16.880
would be much easier, but at the same time, I mean, where are those lanes?

14:16.880 --> 14:22.800
Do the marked them? Trains are like that, of course, and I mean, trains, you know, not to step

14:22.800 --> 14:27.040
on the railroad tracks, because that's dangerous. That's not where you're supposed to be, right?

14:28.000 --> 14:31.920
And so definitely I could see a version of if you go that far and really delineate,

14:31.920 --> 14:37.680
this is self-driving car territory only. I suspect today's technology would probably create

14:37.680 --> 14:43.040
a lot of value, but at the cost of a lot of real estate, being taken up by those dedicated

14:43.040 --> 14:49.760
tracks and so forth. Proud that what's happening there, but of course, I'm not the one pushing

14:49.760 --> 14:55.200
self-driving cars personally, but have a lot of friends doing that. It seems highways,

14:57.040 --> 15:01.520
is sufficiently constrained, because typically, people aren't walking around there, and, you know,

15:01.520 --> 15:06.080
everything is cars and motorcycles. So there are different environment there.

15:06.080 --> 15:14.240
I think what I'm wondering is, and this is maybe a bit of a tangent, but are there things that

15:14.240 --> 15:22.640
you've learned in your experience, in factories, that, you know, inform how you might approach

15:22.640 --> 15:28.480
the self-driving car problem, and this whole constraining the environment, if you were to,

15:28.480 --> 15:30.640
you know, decide to go try to solve that problem tomorrow.

15:30.640 --> 15:37.600
No direct lessons in that way, but I think there's a very strong analogy in a different way,

15:37.600 --> 15:46.640
which is the chasing the long tail of events, whether it's items or configurations of items,

15:46.640 --> 15:56.080
or it's traffic situations, the kind of getting 80% coverage isn't too hard, that that's very

15:56.080 --> 16:02.160
feasible with, you know, a relatively fast effort. But getting to the reliability that you need

16:02.960 --> 16:08.160
to actually function, well, there's this long tail of things that you barely ever see.

16:08.160 --> 16:13.760
Each one of them, you barely ever see, but when you add it up together, there's a lot of it.

16:14.400 --> 16:22.560
And so you need to somehow observe all these rare events that together add up to non-rare

16:22.560 --> 16:26.640
probability mass. So that's why it's so important. There was just one rare event that happens once

16:26.640 --> 16:30.480
in, you know, a million years, who cares, but it was a million of those events that happened

16:30.480 --> 16:33.840
once in a million years, all of a sudden, every year you have an event, right? And that's kind

16:33.840 --> 16:40.240
of what's happening is there is this long tail of very rare things that you somehow need to train

16:40.240 --> 16:44.320
your neural networks. In most cases, I imagine, self-driving people train your neural networks too,

16:44.320 --> 16:50.160
but definitely for us at Coverin, train your neural networks to have a more general understanding

16:50.160 --> 16:57.360
of objects, their shapes, their graspability, their interactions, when they're in a bin,

16:57.360 --> 17:02.320
when you try to pick one out, or when you place how that interacts with each other,

17:04.720 --> 17:09.680
don't understand that for every scenario you might encounter without having the ability to train

17:09.680 --> 17:14.240
on every scenario, you need to train something that generalizes. And I think that that is kind of

17:14.240 --> 17:20.080
the case in both situations, whether it's self-driving or warehouse picking, you need that general

17:20.080 --> 17:31.920
realization ability. I remember from our last conversation, you were very staunchly for end-to-end

17:32.800 --> 17:39.920
deep learning. And I wonder if your experience solving problems in the real world and industrial

17:39.920 --> 17:47.440
domain has changed that at all. You know, we talked about end-to-end versus, you know, ensembles

17:47.440 --> 17:53.920
and incorporating physical knowledge of the world, that kind of thing. Do you still feel

17:53.920 --> 18:01.840
strongly about single end-to-end deep learning model? So I think, yeah, you're absolutely

18:01.840 --> 18:06.560
right with your observation about what we chat about last time. And maybe I've adjusted things a

18:06.560 --> 18:14.080
little bit, but I'm still very close to where I was last time. So first of all, I would say there are two

18:14.080 --> 18:21.440
different things to think about. So one thing to think about is research, academic research,

18:21.440 --> 18:29.600
when you write papers, where do you think most of the novelty surprises, new things are going to be?

18:30.560 --> 18:36.160
And my sense is that while in the other hand, when we put things in the real world, often

18:36.160 --> 18:43.920
complementing it with the chronology you have from physics and so forth can give you a stronger system,

18:44.960 --> 18:51.920
from a purely research academic point of view, my sense is that those more classical approaches

18:51.920 --> 18:58.400
have been researched so well already that there is less room to push the boundary in terms of

18:58.880 --> 19:03.840
novelty and surprises and things that, you know, oh, wow, then expect this to be possible in

19:03.840 --> 19:11.600
now a computer AI robot can do this. And so in the academic realm, I'm really big on

19:12.400 --> 19:17.840
going as purely on learning as possible because there is the most novelty there.

19:18.640 --> 19:25.200
On the more practical thing, which you're, of course, it's stupid to throw away anything that you

19:25.200 --> 19:30.240
know has, you know, guarantees and is guaranteed to work. If you know that, you know, you have

19:30.240 --> 19:35.760
maybe a 3D understanding of the environment, well, the notion that the environment is 3D and then

19:35.760 --> 19:42.160
you can build a 3D map of it or at least a depth map or whatever you want to build, it seems

19:42.160 --> 19:48.000
stupid to not not use that kind of notion and just say, hey, we'll never tell the neural net

19:48.000 --> 19:54.000
explicitly that, you know, 3D matters. It seems a waste of it.

19:54.000 --> 19:55.520
We've been sounds crazy saying it.

19:55.520 --> 20:03.840
Yeah, that said, here's one thing that we found that I think that ties into this, which is

20:05.440 --> 20:13.840
the tricky thing with coding in prior knowledge is that it's very hard to be super precise

20:14.560 --> 20:18.320
about prior knowledge. And so whenever you think you're encoding prior knowledge,

20:19.280 --> 20:23.280
you might be encoding something that's actually slightly untrue, slightly incorrect in some

20:23.280 --> 20:32.640
situations. Can you give an example? So let's say, here's the simple example. Let's say, say,

20:32.640 --> 20:40.960
hey, a good way to pick up an object is go to the middle of a flat region with a suction cup,

20:40.960 --> 20:45.280
go to the middle of a flat region. That might seem like a good heuristic and say, okay,

20:45.280 --> 20:50.320
that's where you should go. And obviously, if you, if you code that in from day one, it'll work

20:50.320 --> 20:55.600
better than if you say, hey, just learn where to pick something because now it has to figure out

20:55.600 --> 21:01.360
that the middle might be better than an edge and so forth. But the downside of hard coding in

21:01.360 --> 21:05.360
that the middle is a good spot is now all of a sudden maybe have overlapping objects.

21:06.880 --> 21:13.200
Maybe the middle is not a great spot to reach or you need to kind of slide things out another way

21:13.200 --> 21:17.040
or maybe it's an object that has a center of gravity that's not even close to the middle.

21:17.040 --> 21:23.680
Or maybe there is a way to place multiple suction cups off center in a way that is actually more

21:23.680 --> 21:30.560
robust than placing things right in the middle. And so what then happens if you hard code and you say,

21:30.560 --> 21:36.000
well, we're just going to hard code that once we understand the scene, you go for the middle.

21:36.880 --> 21:42.560
Well, all of a sudden, how do you now special case around that? And you're kind of stuck.

21:42.560 --> 21:48.320
And so the general philosophy I like is this notion that if you have prior knowledge, don't hard code

21:48.320 --> 21:57.520
it into your system. Instead, use it for effectively data generation. It can any prior knowledge can

21:57.520 --> 22:03.760
usually be turned into a data augmentation or a data generation scheme. And so you might be able to

22:03.760 --> 22:09.520
generate a bunch of example grasps that way for thinking about grasping. And they're good training

22:09.520 --> 22:13.440
data because they're good. They're not necessarily perfect, but they're good and they'll help you.

22:14.160 --> 22:19.760
But I think that's ultimately going to let you keep improving as you encounter more corner cases

22:19.760 --> 22:24.480
where a data driven approach will give you an exception to get an exception exception without

22:24.480 --> 22:30.320
if then else, if then else is just the more data speaks for itself. And the neural net will absorb

22:30.320 --> 22:36.320
that. And so I think that that's really the way I tend to see it in the long term is use all that

22:36.320 --> 22:42.000
prior knowledge to help you train the neural network because that prior knowledge can help you generate

22:42.000 --> 22:47.680
so much data for your neural network. That's a really interesting way to unify the two ideas.

22:49.280 --> 22:54.640
When you approach the types of problems you're approaching,

22:56.240 --> 23:04.400
imagining that ultimately you're trying to build a product or a platform that is as general as

23:04.400 --> 23:15.440
possible, but there's a lot of kind of custom consulting-ish customization for each individual

23:15.440 --> 23:20.720
problem. I'm curious if you could speak to that and how you've approached it.

23:22.240 --> 23:29.360
Yeah, I think that is exactly the trap to avoid. If you try to build a sustainable company,

23:29.360 --> 23:37.600
if you get into a ton of one-off consulting efforts, you're just like, you're not building a product,

23:37.600 --> 23:42.640
you're not building something that you can repeatedly deliver. But at the same time,

23:43.840 --> 23:49.120
whenever you're asked to deliver something to a customer, it has to be what they need. You can

23:49.120 --> 23:54.480
just say, hey, we built not what you want, not what you need, because then they'll just be like,

23:54.480 --> 23:59.600
yeah, well, then we don't want it. If there's going to be exactly what you need, but it's going

23:59.600 --> 24:06.800
to be not exactly what 10 customers need, too. Exactly. There's a natural tension there,

24:06.800 --> 24:12.880
and the way we've been thinking about this is, first of all, first of all,

24:12.880 --> 24:18.800
doing a lot of market research. As we did the market research, we saw, effectively,

24:18.800 --> 24:26.240
big picture wise, we saw a recurring theme of more intelligence for robots. If we can build

24:26.240 --> 24:30.880
something that can make robots understand their environment, react to it, that's going to be able

24:30.880 --> 24:37.440
to power a lot of different solutions. Then zooming in on the beach heads of the

24:37.440 --> 24:43.600
say of what we're delivering our technology right now, we saw that in warehousing, there are

24:43.600 --> 24:52.240
really three main applications that we decided to cater to. The first one is order picking.

24:52.240 --> 24:58.000
It's where a robot is presented with items that were in storage and needs to pick one at a time

24:58.000 --> 25:04.960
to put it into a shipping container, and then things get chipped off. That's order picking. Then

25:04.960 --> 25:15.360
a second one is put-walling, or sometimes put-to-light. In the put-wall, what happens is many orders

25:15.360 --> 25:22.320
come into a warehouse, and somebody or a robot has gone around the warehouse and collected the items

25:22.320 --> 25:29.760
for all these orders, but it might be for 100 orders or even more. It's all collected, let's say,

25:29.760 --> 25:34.880
in a shopping cart or in a bin. It's not sorted by orders. They collected everything needed

25:34.880 --> 25:41.680
for fulfilling those maybe 100 orders. Now that shows up at a put-wall, and now the robot's job

25:41.680 --> 25:50.320
is to take one item at a time out of that shopping cart effectively, and then scan it and place it

25:50.320 --> 25:57.440
into a temporary storage that is associated with that specific order until that order has been

25:57.440 --> 26:03.440
completely filled from that shopping cart, and then somebody behind that put-wall will take it

26:03.440 --> 26:10.640
and pack it up and ship it off. That's put-walling. The third one is parcel certification,

26:10.640 --> 26:15.440
induction sorting, where there's a lot of parcels going around these facilities, and they often

26:15.440 --> 26:19.920
need to be simulated, because there's a bulk of parcels coming in. Can you reliably place

26:19.920 --> 26:25.760
them one at a time onto a conveyor so that it can go through scanners and so forth to be sorted

26:25.760 --> 26:32.240
to the right outbound truck, let's say? These three are the ones that were focused on at first,

26:32.240 --> 26:35.920
where we can build something that's very channel, so anybody who needs a put-wall, anybody who needs

26:35.920 --> 26:44.720
order picking, anybody who needs induction sorting, we can deliver to that without having to do

26:46.080 --> 26:54.080
one-off consulting for each specific deployment. And are those problem definitions

26:54.080 --> 27:05.040
constrained enough that you can put-walling, for example, you can cover all or the vast majority

27:05.040 --> 27:16.080
of the use cases, or are they further constrained by, for example, the individual skews have to be

27:16.080 --> 27:25.040
in boxes as opposed to loose bolts or things like that. How granular does the strike zone need to be?

27:25.920 --> 27:29.680
Yeah, it's really interesting to ask that, and I'm glad you're asking because I should have

27:29.680 --> 27:36.560
mentioned that anyway. So what's so interesting about how we're building this is that, at least

27:36.560 --> 27:43.920
one, I'm so excited about is that we're building a single system behind all three application

27:43.920 --> 27:53.040
domains. So the same neural networks are trained for put-walling and for order picking and for

27:53.040 --> 27:59.440
induction sorting. And you might think, well, why? I mean, those are somewhat different problems,

27:59.440 --> 28:06.880
but the core is still the same. The core is you look at a bunch of items. Sure, parcels versus

28:06.880 --> 28:12.880
for put-wall often small items versus order picking a range of sizes of items, but you're still

28:12.880 --> 28:20.080
at the fundamental level looking at a bunch of items and making sure you're picking reliably one

28:20.080 --> 28:28.400
at a time, possibly scanning and then placing. And so it generalizes across all three of those,

28:28.400 --> 28:33.280
and then within them, as you said, some facilities might be focused on cosmetics. Others might be

28:33.280 --> 28:39.280
focused on pharmaceuticals, others on apparel, yet others could be focused on maybe electrical

28:39.280 --> 28:48.720
supplies and so forth. And again, it's all the same neural network for all those domains.

28:48.720 --> 28:53.680
And I think that's actually the best way to do it because even within a single domain,

28:55.280 --> 29:01.760
you cannot cover everything. There's so much turnover on all these skews that and packaging

29:01.760 --> 29:06.160
of the skews that you cannot cover. You can now say, these are the 100 items we need to do. Let's

29:06.160 --> 29:11.280
have something specific for that. You need to have some that really generalizes. And by building it

29:11.280 --> 29:17.760
in an even more general way, then let's say just cosmetics, you actually build in more expertise

29:17.760 --> 29:22.640
into the neural networks about what it tends to see, what makes for an individual object,

29:22.640 --> 29:28.240
what is the 3D of the situation it's looking at, what's the best way to approach, to grasp,

29:28.240 --> 29:33.280
to remove, and then what's the fastest trajectory to bring from point A to point B and so forth.

29:33.280 --> 29:42.720
And to be clear, are your models primarily focused on the manipulation task or whatever the

29:42.720 --> 29:47.520
generalization of that? I'm assuming a given robot might have some set of tools that needs to

29:47.520 --> 29:56.000
select and you need to identify the best contact point and a variety of factors associated with

29:56.000 --> 30:03.600
let's call it manipulation. And then, you know, I imagine that beyond manipulation, there are other

30:03.600 --> 30:09.920
parts of the problem that I'm thinking a more like traditional optimization and not necessarily

30:10.880 --> 30:17.280
places where you require neural networks. But I guess that's what I'm asking you to correct for me.

30:17.280 --> 30:21.920
What's the scope within these three problem domains of the models that you're building?

30:21.920 --> 30:28.480
Yeah, so there's a couple of interesting things that you're touching upon there. One is that

30:29.840 --> 30:35.120
indeed, not every item can be picked with the same end-of-factor. And so there are things

30:35.120 --> 30:40.320
called tool changers where robot can change the end-of-factor on the fly and say, oh, actually,

30:40.320 --> 30:45.760
I should use gripper now, I should use two suction cup or maybe six suction cup end-of-factor.

30:45.760 --> 30:54.800
And so you can switch that as needed, though it turns out surprise, somewhat surprisingly,

30:54.800 --> 31:02.160
initially not surprising anymore now, suction cups go very, very far in most warehouse facilities.

31:02.880 --> 31:09.760
Now, what's also interesting is that the neural network is the same when it's thinking about how

31:09.760 --> 31:14.640
to grasp them and then play these objects, whether it's suction cup or gripper and independent

31:14.640 --> 31:20.160
the number of suction cups it's using. And this is going a little bit deeper, but the simplest way

31:20.160 --> 31:25.280
to think of is imagine you have a single neural network, the main body of the network that thinks

31:25.280 --> 31:32.720
about grasping. You don't just give it the scene that it's looking at to grasp in, but you also

31:32.720 --> 31:37.600
give it the configuration of the end-of-factor. And based on the combination of scene and end-of-factor,

31:38.160 --> 31:43.840
it decides what to do. And that way, again, you can generalize beyond what's possible otherwise,

31:43.840 --> 31:47.760
which is really cool, so you don't have one-off training for new end-of-factors. No, it's all shared

31:47.760 --> 31:54.640
learnings across every deployment, every type of end-of-factor. Just punching it with a question

31:54.640 --> 32:01.920
here, are you also giving it a tabular style data associated with the product, like skewed

32:01.920 --> 32:07.280
dimensions, that kind of thing, or are you primarily focused on visual or sensor inputs?

32:07.280 --> 32:18.160
So, that data can often be made available, but not always. We prefer not to rely on it,

32:19.600 --> 32:24.960
just because it can be a pretty strong assumption to rely upon that.

32:27.040 --> 32:31.200
But I mean, there are places where it can be made available, but we prefer not to rely on it.

32:31.200 --> 32:41.040
Okay. Yeah. And the other thing you brought up is kind of optimization. Yes, optimization

32:41.040 --> 32:48.880
matters a lot. It turns out speed is, I mean, reliability is key, but then once it's reliable,

32:48.880 --> 32:55.680
speed also matters. If you're about, you know, works at half the speed is creating, half the value,

32:55.680 --> 33:01.440
and in a facility like, you know, warehouse or factories and so forth, if you're not keeping the

33:01.440 --> 33:08.480
pace, you might be bottlenecking the entire process. So, keeping the required pace is really

33:08.480 --> 33:15.360
important. And so, there's many methods to optimize robot trajectories that can help do things,

33:15.360 --> 33:20.480
but actually things get really interesting once you're holding objects down, you know,

33:20.480 --> 33:26.000
might be flinging because they're heavy and they're floppy and so forth. And all of a sudden,

33:26.000 --> 33:31.280
you know, you can go about it as analytically anymore, as you would do when you just think

33:31.280 --> 33:35.760
about the robot itself. And all of a sudden learning, which you thought maybe it doesn't matter

33:35.760 --> 33:40.960
for motion, all of a sudden, oh, actually learning can matter again to do better than you can without

33:40.960 --> 33:50.160
learning. Interesting. Kind of continuing that example of objects flinging, you know, there's a

33:50.160 --> 33:59.120
suction cup. Every once in a while, something's going to drop. Do you consider that kind of

33:59.120 --> 34:04.880
out of scope and, you know, every night a human goes around and picks up all the drop things and

34:04.880 --> 34:13.360
rebends them? Or are you also trying to build intelligence into the robots to remediate that,

34:13.360 --> 34:20.880
pick up the items that fall or some other, some other scenario? Yeah. So what you're getting at there,

34:20.880 --> 34:31.520
I think, is at its core, why? Warehouse robotics is such a great place to be in that you need to

34:31.520 --> 34:37.040
be very reliable. But if just every now and then something needs a human intervention,

34:37.040 --> 34:44.960
that's okay. If a person needs to spend 10 minutes, let's say at the end of the day, to clean up

34:44.960 --> 34:49.200
a couple of things that the robot did, it's no problem. I mean, obviously you don't want to

34:49.200 --> 34:54.080
rub it to break anything, but if it just puts something next to a bin, it drops it in, in trajectory.

34:54.720 --> 34:59.360
Of course, we don't want it to happen. We want it to be always 100% reliable. But the beauty here is

34:59.360 --> 35:06.720
that you can create a lot of value once you are, let's say, 99.9% reliable, which is a really

35:06.720 --> 35:14.720
high bar and is very hard to meet. But 99.9% is not 100%. And 99.9% maybe wouldn't cut it for

35:14.720 --> 35:21.360
self-driving. You might not want a 99.9% self-driving car on the road, because I mean, it means, you know,

35:21.360 --> 35:26.640
one in a thousand times, let's say crosses an intersection, if that's how we count, it gets into

35:26.640 --> 35:32.640
an accident. That would be 99.9% reliability on crossing intersections. That's not good enough.

35:32.640 --> 35:38.160
You would never deploy that. But if you think about, and that's really where commercialization

35:39.760 --> 35:45.120
is what we think about in great detail is, when does the robot start creating value? And so the

35:45.120 --> 35:49.840
value creation in our perspective, everything we've seen with end customer starts roughly at that

35:49.840 --> 35:58.240
99.9% mark, because the robot would typically do anywhere from 500 to 2000 operations per hour,

35:58.240 --> 36:01.760
depending on the type of station. Some stations have to be fast or some stations have more

36:01.760 --> 36:08.720
involvement, like extra scanning and so forth, they're a bit slower. But even at a slow station, a 500

36:09.520 --> 36:18.240
per hour station, at 99.9%, it means, well, you are just making a mistake as once every two hours.

36:18.240 --> 36:22.720
And that means nobody has to constantly manage, babysit this robot. It's, you know, once every two

36:22.720 --> 36:28.640
hours, sure, let's quickly fix something. And that's creating real value. And that in my mind is

36:28.640 --> 36:34.080
kind of the benchmark is 99.9 is, of course, we want we're looking to go further than that.

36:34.080 --> 36:40.160
Don't get me wrong. But that's where it becomes just, yeah, robot is helpful as opposed to

36:40.640 --> 36:50.560
robot needs a babysitter. Yeah. So I could continue on talking about industrial AI and robotics for

36:51.280 --> 36:56.320
the entire episode, the entire interview I've written a bit about industrial AI and I find it

36:56.320 --> 37:03.280
a fascinating topic. But I also want to touch on some of the research interests that you mentioned

37:03.280 --> 37:09.520
earlier. In particular, you talked about this junction of unsupervised and reinforcement learning

37:09.520 --> 37:16.800
and some of the work you're doing there unsupervised learning is something that has been a goal

37:17.440 --> 37:24.800
from a research perspective for a while for its data efficiency. But it's really played out

37:24.800 --> 37:33.440
in big ways in natural language processing recently. Talk a little bit about, you know, that junction

37:33.440 --> 37:41.360
and where you see it being interesting. Yeah. So as you know, Sam, I've been working on

37:41.840 --> 37:45.760
reinforcement learning for a very long time, right? And reinforcement learning is,

37:47.120 --> 37:52.000
well, it's trial and error learning and it's something we're very familiar with as humans when

37:52.000 --> 37:57.760
when we see a child learn to crawl, learn to run, that that's reinforcement learning and when

37:57.760 --> 38:03.680
we train a dog to, you know, maybe sit, that's reinforcement learning when we say sit and it

38:03.680 --> 38:09.200
sits, we'll give it a treat and if it doesn't sit, we might yell at it and from that feedback,

38:09.200 --> 38:14.640
it figures out what it means to sit and it requires that skill, right? Listening and executing.

38:15.360 --> 38:21.040
And so that's reinforcement learning and a lot of the big successes in reinforcement learning that

38:21.040 --> 38:28.560
we've seen so far have been in simulation, right? So arguably that the most famous success

38:28.560 --> 38:36.240
alpha go, right? The best computer go player and beating the best human players out of deep

38:36.240 --> 38:40.880
mind, reinforcement learning played a very big role in that because it was playing against itself

38:40.880 --> 38:46.240
over and over and over to become better. But it's all in simulation, same with many video games,

38:46.240 --> 38:52.000
a lot of simulated robotics results. But the question is when when you try to transition from

38:52.000 --> 38:56.480
simulation to real world, there are a few things you can do, of course, you can say, hey, maybe

38:56.480 --> 39:00.080
that my simulator can be perfectly matched with the real world and that can help you, but

39:01.200 --> 39:07.840
ultimately you want to be more data efficient. You want to not just learn from reward, you want

39:07.840 --> 39:12.640
to learn in other ways, you want to not just, you know, do something for a minute and at the end,

39:12.640 --> 39:18.080
nobody will ride or wrong. No, you want to have feedback. That's that's much denser, much,

39:18.080 --> 39:26.160
much more informative. And naturally reinforcement learning doesn't really do that in it's kind of

39:26.160 --> 39:32.240
vanilla form. But we've seen, as you said, in natural language processing, we've seen unsupervised

39:32.240 --> 39:39.600
learning, which is learning from just vast amounts of text. And so to make the analogy there,

39:39.600 --> 39:44.960
maybe natural language processing, you want to classify the sentiment is this a positive or a

39:44.960 --> 39:52.880
negative review, or is this a, I don't know, a correct English sentence or a broken English sentence,

39:52.880 --> 39:58.960
things like that. You want to learn to classify that. And instead of just generating examples of

39:58.960 --> 40:06.640
positive and negative articles, you would instead first, because that would take a lot of annotated

40:06.640 --> 40:10.960
training that you'd first train on just predicting the next word in a sentence on all the text you

40:10.960 --> 40:14.800
can find on the internet. Once you're really good at that, you must have internalized something

40:14.800 --> 40:21.040
inside the neural network about language and its meaning, such that now from a few examples,

40:21.040 --> 40:26.240
you understand positive or negative sentiment classification. And so the question is, can we do

40:26.240 --> 40:33.680
the same thing in reinforcement learning? Can we have this auxiliary unsupervised task that has

40:33.680 --> 40:38.560
a lot of signal that's not directly the reinforcement signal, but still signal to learn from for

40:38.560 --> 40:45.600
the neural network to then quickly acquire a new skill. And so we've been working on that quite a

40:45.600 --> 40:52.080
bit in the last year, year and a half. And so what we've seen, and it's not just us also some

40:52.080 --> 41:02.400
some people at NYU Montreal and so forth, we've actually been able to bridge the gap between

41:02.400 --> 41:09.440
what happens when you learn, let's say a robot has to learn some skill, let's say running or

41:09.440 --> 41:15.600
or maybe crawling and so forth, when the robot has to learn it, we're having access, full access

41:15.600 --> 41:20.720
to its entire configuration at all times, all its joint angles, its position, its orientation,

41:21.680 --> 41:27.040
that's full access, full state access. That always learned pretty fast, but it would learn

41:27.040 --> 41:34.000
slow if all I guess you see is a video stream of itself, because that's maybe a hundred by a

41:34.000 --> 41:39.520
hundred image, so 10,000 pixels, it's much higher dimensional than the succinct state description

41:39.520 --> 41:46.080
of the robot. And so what we saw is that massive gap, the learning efficiency, when learning from

41:46.080 --> 41:52.000
access to state, which is quite efficient, compared to learning with access to image input only,

41:52.000 --> 41:59.200
was just this massive gap. And so the question is how do we bridge this, and we did it with unsupervised

41:59.200 --> 42:07.040
learning effectively. So as the robot is doing its trial and error, instead of only paying attention

42:07.040 --> 42:15.040
to rewards and trying to become better based on what's as good or a bad run so far, it is also

42:15.680 --> 42:20.960
doing unsupervised learning on the images it's seeing. And what it means specifically in our

42:20.960 --> 42:26.400
case is we did something called contrastive learning. So in contrastive learning, what you do is you have,

42:28.000 --> 42:32.480
you want to learn what's in an image, but how do you learn it if nobody tells you what's in an

42:32.480 --> 42:39.440
image? Well, here's the idea, imagine you just download two images, and you don't know what's in

42:39.440 --> 42:48.320
them, you have no idea what's in them. But now for one of the images, you make a duplicate,

42:48.320 --> 42:53.520
but a modification as you duplicate it. So maybe you have an image and you crop it in two different

42:53.520 --> 42:58.720
ways. So now I have two different crops of the same image. And then there's the other one.

42:59.440 --> 43:03.120
Now the two different crops of the same image, even though you don't know what's in it,

43:03.120 --> 43:08.800
you know they have the same thing in it. And it's different from what's in the other image.

43:08.800 --> 43:12.080
Most likely, if you randomly download the image, most likely, the other image has something else.

43:12.720 --> 43:16.880
And that's really where the signal comes from. Now you train your neural network to understand that

43:16.880 --> 43:24.320
these two crops of that same first image, neural networks should know that's the same,

43:24.320 --> 43:28.560
doesn't know what it is, but it should embedded somewhere in a space where it puts those two

43:28.560 --> 43:34.960
close together. And the other image should be far away from it. That idea turns out really,

43:34.960 --> 43:40.000
really powerful. It's been shown to work really well by Jeff and then collaborators, as well as

43:40.000 --> 43:46.160
Camminghan collaborators on doing that on images followed by image recognition training. And it's

43:46.160 --> 43:52.800
very, very efficient. We can do the same thing in reinforcement learning from images. So you apply

43:52.800 --> 43:58.240
the same idea. What the robot is seeing now, first of what it's seeing in a different time,

43:58.800 --> 44:04.960
well, thing is seeing now you take two different crops. And that is still the same thing. And the

44:04.960 --> 44:09.360
neural network learns that there's two views of the same thing and different from the other thing.

44:09.360 --> 44:16.080
Very simple idea conceptually, but very, very powerful. You train that way all of a sudden,

44:16.800 --> 44:23.040
you can train almost as efficiently from image inputs, ask with direct access to state.

44:23.920 --> 44:29.600
This one we evaluated this on the kind of standard deep mind control suite simulated robotics

44:29.600 --> 44:39.040
environments. In the, there's a whole area research around

44:39.040 --> 44:45.360
multitask learning that seems to suggest, although, you know, there's certainly arguments against this

44:45.360 --> 44:54.000
that networks generalize better when they have more to do essentially. I'm wondering if

44:55.600 --> 45:00.640
part of what you're seeing is the effect of just giving the network something else to learn,

45:00.640 --> 45:10.000
as opposed to the unsupervised task, contributing to kind of the core reinforcement learning

45:10.000 --> 45:20.720
thing that it's supposed to learn. So you're absolutely right in that multitask tends to help

45:20.720 --> 45:25.760
a means why when, you know, for example, at Coverand, we train the same neural network across

45:25.760 --> 45:31.280
many application domains because it'll help to train across all those domains use the same network

45:31.280 --> 45:38.400
rather than specialized to each one of them. Same in the research here when, when we train a

45:38.400 --> 45:44.640
neural network for multiple tasks, often it can do better. But the beauty about bringing the unsupervised

45:44.640 --> 45:51.600
losses that the other, you can think of the unsupervised loss as multitask where the multitask

45:51.600 --> 45:57.200
additional thing doesn't require you to do any annotation, doesn't require you to give any

45:57.200 --> 46:03.120
reward signals. So it's it's like the cheapest way to achieve multitask. There's a few

46:03.120 --> 46:08.480
men out for cheapest ways. Still a lot of compute required just just to be clear. Still a lot of

46:08.480 --> 46:17.840
compute involved. Yeah, I think the the analogy that I was coming from is like in, you know,

46:17.840 --> 46:23.040
an LP when you're training birth or something and you you have this unsupervised formulation of

46:23.040 --> 46:35.120
the problem. The problem that you're trying to solve in a lot of ways is directly related to the

46:37.360 --> 46:41.440
you're in building the language model, you know, blanking out the words is like that's, you know,

46:41.440 --> 46:50.960
teaching you language and I'm wondering if there's a distinction between the second task, you know,

46:50.960 --> 46:59.120
is the second task it just happens to be an unsupervised task. And the relationship between that

46:59.120 --> 47:06.000
and the core reinforcement learning task that you're trying to solve. I'm not sure that I'm

47:06.000 --> 47:15.200
completely articulating the question in a clear way, but the the idea is is the unsupervised

47:15.200 --> 47:22.320
task kind of core to does it inform the core reinforcement learning process and that loss or is

47:22.320 --> 47:30.880
it just an ancillary other thing and it's the the multitask aspect or the other task that is what

47:30.880 --> 47:38.560
you know, creates value or causes you to increase performance. Right. So you're actually

47:39.600 --> 47:44.960
getting at something that we're working on right now. So let me say a bit more about that. So

47:46.320 --> 47:53.440
to very directly answer your question, I think when we use the contrastive loss on a given image

47:53.440 --> 47:59.840
compared to an image at a different time, we're not tying it very directly to the

47:59.840 --> 48:07.840
locomotion task versus crawling versus maybe sit or maybe, you know, push an object. It's not

48:07.840 --> 48:12.480
very closely tied to that. I would think I'm inclined to think of it as it's like learning

48:12.480 --> 48:18.640
division system of the robot. It's it's learning to see and understand, you know, that it's

48:18.640 --> 48:23.200
saying the same thing in these two situations versus seeing something else there. But it's not

48:23.200 --> 48:27.920
learning about how the world works and that's kind of what you're getting at and reinforcement

48:27.920 --> 48:33.200
you're trying to achieve goals. Your robot's supposed to get from point A to point B or

48:33.200 --> 48:36.640
achieve something with the objects in front of it or it could be in a game you're supposed to

48:36.640 --> 48:43.120
get a high score in the game and so forth. And so this whole notion of achieving goals and how

48:43.120 --> 48:51.440
the world works, I think is a natural complementary aspect. So imagine you in the unsurprised sense,

48:51.440 --> 48:58.240
you've been able to complement your reinforcement to get a bit of a vision system trained. Now what

48:58.240 --> 49:04.400
you also want is effectively a how does the world work system such that when I'm asked to do something

49:04.400 --> 49:09.920
in this world as a robot, I already kind of know how the world works. I don't have to flail my arms

49:09.920 --> 49:15.520
a gazillion different ways and no, I don't have to learn that if I don't touch an object it's not

49:15.520 --> 49:22.720
good move kind of thing. I know that that the world works with contact forces and so forth. So

49:22.720 --> 49:28.000
if I want block A and block B, well block B that's on on the bottom has to be on the table first

49:28.000 --> 49:33.200
and then those things are things that don't come from what I just described the contrast of learning

49:33.200 --> 49:38.160
I just described. You need a temporal aspect to it. And so that I think is one of the important

49:38.160 --> 49:42.800
next steps. It requires more compute and that's probably why it's coming a bit later in the progression

49:42.800 --> 49:52.240
of research overall. But the notion there would be can you understand what are natural video sequences.

49:52.240 --> 49:58.320
So if you download a lot of video, what does it tend to look like? And that's a very hard problem

49:58.320 --> 50:01.520
because video is very high-dimensional. I mean that training or neural networks for video

50:01.520 --> 50:06.560
prediction or understanding which videos are more related or less related to each other is

50:06.560 --> 50:12.320
computational and intense problem. So it's the kind of problem that I mean I'm sure you're well aware.

50:12.320 --> 50:16.720
Jan LaKoon has been talking about for a very long time when he gives the cake analogy of

50:16.720 --> 50:21.920
reinforcement learning is the cherry on the cake is the reward signal but the unsupervised is

50:21.920 --> 50:27.360
the most of the cake right and the icing is the supervised learning unsupervised is most of it

50:27.360 --> 50:32.400
what he thinks of his video in the context of robotics at least he think of video robots have

50:32.400 --> 50:37.760
watched so many videos that they know how the world works a lot of research still has to be done

50:37.760 --> 50:43.680
there but I think that will be really important. The third part so there's the understanding what

50:43.680 --> 50:50.560
you're looking at is part one we made a lot of progress on that then video understanding behavior

50:50.560 --> 50:55.840
understanding in that sense how the world works the third part is for the robot to have its own skills

50:57.280 --> 51:01.280
and that's where it gets much closer to what you're talking about is something we're actively

51:01.280 --> 51:06.560
working on which is can you let the robot just spend time on its own just the robots just on its

51:06.560 --> 51:14.240
own in a room or wherever it is and can you make it spend its time meaningfully effectively play

51:14.240 --> 51:20.480
what what for children we would call play you say oh kid is just playing but actually yeah kid is

51:20.480 --> 51:24.000
just playing you might say well as if it did some chores or whatever but actually the kid is

51:24.000 --> 51:31.040
just playing it's actually learning about the world it's the kid now understands especially young kids

51:31.040 --> 51:35.680
they understand how now objects interact understand how the world works from interacting with it

51:35.680 --> 51:41.600
and that kind of play is another important component if I think about the long term future of

51:41.600 --> 51:47.280
robotics if you want to get to less supervision less need to train everything into the robots just

51:47.280 --> 51:55.360
let them try things on their own why would why would that kind of play be different from the

51:55.360 --> 52:05.040
goal-directed exploration that we do today in RL so there is a good amount of work that's

52:05.040 --> 52:10.080
already happening in RL and essentially getting this kind of play to surface so goal-directed

52:10.080 --> 52:16.240
exploration is a great example there's other work that you know kind of similar but it's called

52:16.240 --> 52:23.280
then curiosity or something like that yeah essentially you give rewards for experiencing something

52:23.280 --> 52:33.920
that you have an experience before and I think that's worked really really well and environments that

52:33.920 --> 52:41.120
are I would say somewhat closed meaning if you think about let's say an Atari game which is a

52:41.120 --> 52:46.560
popular reinforced learning benchmark environment usually there's a right way to play the game and

52:46.560 --> 52:51.840
there's not too many other things you can do or you die and that's what I mean with a kind of

52:51.840 --> 52:56.160
closed environment you there was and if you so if you're curious and you try to experience new

52:56.160 --> 53:00.960
things all you you've died many times in the game that's not new anymore so the the new things are

53:00.960 --> 53:05.440
the things that take you to the next level in the game and so there's a very natural alignment

53:06.320 --> 53:13.280
between novelty and the actual task that we care about and that's why curiosity,

53:13.280 --> 53:17.040
goal-oriented exploration so forth have had a great amount of success

53:17.040 --> 53:22.560
but that breaks down in the real world where there are so many things you could do

53:22.560 --> 53:28.800
or even in some games like Minecraft which is becoming a new benchmark that people get excited

53:28.800 --> 53:34.480
about for this exact reason in Minecraft you can build so many different things and so

53:34.480 --> 53:43.040
just experiencing something novel you can keep doing that forever and never learn something

53:43.040 --> 53:47.920
interesting and I think that's kind of the next step there and that's maybe where I think of

53:48.560 --> 53:51.840
things like children's play as being a bit different because it seems like somehow

53:52.560 --> 53:58.640
they have a bit more intuition than our current AI systems about what's interesting play

53:58.640 --> 54:05.680
as opposed to just what's what's novel but actually not not that interesting interesting interesting

54:05.680 --> 54:12.960
I want to switch gears to a paper that you recently posted up on archive pretrained transformers

54:12.960 --> 54:19.600
as universal computation engines tell us a little bit about that work and what you're looking to

54:19.600 --> 54:29.760
achieve there yeah so this paper for me was one of the most surprising things we've done usually

54:29.760 --> 54:34.320
I feel like when we write a paper we have a pretty clear intuition that we know ahead of time

54:34.320 --> 54:39.600
okay this intuition should be leveraged this way in the algorithm and as a consequence it should

54:39.600 --> 54:44.160
work and we should be able to take things to to the next level that that's a fairly typical research

54:44.160 --> 54:48.400
kind of progression and sure there's iterations because your intuition might be wrong but then

54:48.400 --> 54:52.640
you refine your intuition and you improve the algorithm but this is kind of surprising to me because

54:52.640 --> 55:00.560
here we didn't really we didn't really come up with a new algorithm it was really an investigation

55:00.560 --> 55:07.920
right so what we did is we looked at pretrained language models so what's very popular these days

55:07.920 --> 55:14.160
is to on massive amount of text train a transformer model which is specific architecture of of neural

55:14.160 --> 55:22.880
network to do next token prediction in a in a document right and if you do this it turns out

55:22.880 --> 55:28.000
it works really well on other language tasks and that had been known and opening up Google

55:28.000 --> 55:36.160
Facebook and so many many results around this but the thing we were wondering is well if it's so

55:36.160 --> 55:42.640
good at doing all these tasks it wasn't really directly trained for could there be something more

55:42.640 --> 55:48.400
general it has learned when it's trained to predict the next token on so much text on the internet

55:49.040 --> 55:55.680
by seeing the previous text predict what comes next might there be a more general reasoning mechanism

55:55.680 --> 56:02.880
that has been internalized inside this kind of neural network beyond language and so the way we

56:02.880 --> 56:09.280
tested this we said okay let's see if we train we take a model train on language only not train on

56:09.280 --> 56:18.160
anything else and then let's put it to use to now classify images or do a prediction about a

56:18.160 --> 56:25.840
protein sequence binding protein sequence binding sites or a simple math kind of problem like compute

56:25.840 --> 56:32.320
the x-war of a sequence of bits and none of these are language tasks obviously you have to do a

56:32.320 --> 56:35.760
you can't just put the language model in front of it because the language model only takes in language

56:35.760 --> 56:42.160
you can't give it an image doesn't know what to do but the so they're also not they're also not

56:42.160 --> 56:48.480
generative tasks like we've seen gpt3 apply to lots of different areas generating web pages from

56:48.480 --> 56:57.280
text but they all share this common generative you know text-based property exactly they tend to be

56:57.280 --> 57:04.320
generate something like the things you have seen before right text right so this model is all

57:04.320 --> 57:09.920
train them text we want but we we believed that there was a chance that it actually reasons in a

57:09.920 --> 57:15.760
more general way that there's some kind of thing in the neural network that makes it reason about

57:15.760 --> 57:21.280
objects that are on its input and then draw conclusions on its output and that maybe

57:22.000 --> 57:26.720
there are general reasoning patterns in there that if we use that reasoning engine and put it

57:26.720 --> 57:31.360
in front of an image it'll also reason about what's in the image and how these things might interact

57:31.360 --> 57:35.680
and so forth of course need to do a little bit of impedance matching so we take the image

57:35.680 --> 57:43.200
we have to do an embedding so just we just do a linear embedding layer which is one layer can

57:43.200 --> 57:48.320
do almost no work for neural networks right that's the whole idea that we want that pre-trained

57:48.320 --> 57:53.600
language model to do all the work so just a linear embedding layer pre-trained language model

57:53.600 --> 57:57.920
and then a linear output layer again because we don't want to output words this time we want to

57:57.920 --> 58:03.840
output a decision what's in an image or is this x or a zero or a one or is this will there be a

58:03.840 --> 58:12.240
bond here for the protein or not so just changing the output and input with just

58:13.440 --> 58:19.920
linear single layer and then one other thing we had to do we had to do the normalization that happens

58:19.920 --> 58:28.640
inside the transformer network so there's layer normalization we had to retrain that to make

58:28.640 --> 58:33.680
sure it's on the right scale for the data that comes through now just doing that was enough

58:33.680 --> 58:40.080
to get surprisingly good performance on these other tasks and so to me that was very surprising

58:40.080 --> 58:48.240
because it confirmed that when you train this neural network on language it's actually not that

58:48.240 --> 58:53.920
specialized for language at all if you train up enough language it's actually internalizing

58:53.920 --> 58:57.920
some more general reasoning pattern of course we don't fully understand this yet but we have this

58:57.920 --> 59:04.160
observation here that it hasn't turned on something that's very generally reusable of course we

59:04.160 --> 59:12.720
tested we said what if we just put a random transformer in the middle same as the language model

59:12.720 --> 59:18.320
but randomly initialized so not trained on land same architecture right randomly initialized

59:19.600 --> 59:24.240
and actually does something which is kind of surprising that it's also kind of capable of

59:24.240 --> 59:29.360
doing something but not as well as the pre-trained language model so there is it seem like there's

59:29.360 --> 59:35.360
both some power in the architecture being surprisingly powerful in general and then there's

59:35.360 --> 59:41.040
additional power in what you learn on language transfer is over through these other domains which

59:41.760 --> 59:45.920
goes back to something that I mentioned at the very beginning we talk about you know what am I

59:45.920 --> 59:54.640
excited about and I mentioned this this notion of well the human brain is so general and it seems

59:54.640 --> 59:59.600
like some part of the brain that's normally used for one kind of reasoning could be used for other

59:59.600 --> 01:00:06.240
things processing of signal from your tongue can apparently do visual processing people have seen

01:00:06.240 --> 01:00:11.440
isn't and blind people also that they're what normally part of brain used for visual processing

01:00:11.440 --> 01:00:16.560
for blind people can be used for audio processing part of it and so this kind of reusability

01:00:16.560 --> 01:00:22.080
of generality we're kind of seeing nothing like human brain just to be clear human brain is

01:00:22.080 --> 01:00:26.960
completely not understood and and way more advanced than anything we're working on here but

01:00:28.960 --> 01:00:34.880
it's it you know try to make progress in that direction of something that's more general reasoning

01:00:34.880 --> 01:00:42.480
not special purpose to a specific domain when you talk about the random model and the pre-trained

01:00:42.480 --> 01:00:49.360
model kind of sandwich between these linear layers are you you freezing the transformers and then

01:00:50.400 --> 01:00:56.000
fine-tuning or training the linear layers for in a supervised manner for the specific problem

01:00:56.000 --> 01:01:00.560
that the idea correct so once you once you have the supervised task you

01:01:00.560 --> 01:01:09.040
freeze the transformer except for the layer norm parameters and the linear input and linear output

01:01:09.040 --> 01:01:14.880
layer so those get retrained so I think it's about like 0.1 percent or something of the overall

01:01:14.880 --> 01:01:21.680
parameter set being trained that way and then when you say surprisingly good performance does that

01:01:21.680 --> 01:01:28.960
mean state of the art on some task or we're surprised that it worked at all but it's not particularly

01:01:28.960 --> 01:01:35.920
useful or state of the art it wasn't state of the art I mean it was state of it was state of the art

01:01:35.920 --> 01:01:40.320
in terms of this kind of research I mean beyond state of the art in terms of this kind of research

01:01:40.320 --> 01:01:44.640
where you're not allowed to train on the task you care about or not much only those linear layers

01:01:44.640 --> 01:01:49.840
in that sense yes absolutely state of the art but in terms of if you say I want the best

01:01:49.840 --> 01:01:56.320
in world best in the world image classifier right right am I going to first train on language

01:01:56.320 --> 01:02:03.680
and then only have a linear layer in input and the output to work with no that's not not yet

01:02:03.680 --> 01:02:09.920
or maybe we'll never I don't know give us the best in world image classifier I just want to make

01:02:09.920 --> 01:02:15.840
sure I was not making assumptions on what surprisingly good meant yeah what a man was surprisingly

01:02:15.840 --> 01:02:20.960
good is surprising that it even you know that it doesn't much much better than chance yeah

01:02:20.960 --> 01:02:29.920
yeah awesome awesome and where do you see this particular line of research going what's what are

01:02:29.920 --> 01:02:42.800
the next steps so I think there is a lot of opportunity in research on multimodal data

01:02:44.160 --> 01:02:49.120
and of course the work I mentioned here is one work another work that stands out that I'm sure

01:02:49.120 --> 01:02:56.400
you've seen is the the clip work by open AI where they specifically trained at the same time

01:02:56.400 --> 01:03:01.920
on language and images so it was trained anything on language and images not just train on one

01:03:01.920 --> 01:03:08.880
and it also works on the other but that's often the case often you have access to multiple

01:03:08.880 --> 01:03:14.240
data modalities that are in some sense not perfectly aligned necessarily but you know that are

01:03:14.240 --> 01:03:19.120
clearly related and you can coach train them yeah um I think there's a lot of opportunity there I

01:03:19.120 --> 01:03:26.240
think there was a lot of I mean even I mean this could be audio and video that could be trained on

01:03:26.240 --> 01:03:33.600
at the same time this could be text and images this could maybe and and you know if you think

01:03:33.600 --> 01:03:41.680
about robotics the same things it could be could be audio video um but it could maybe be able to

01:03:41.680 --> 01:03:49.600
be sent someday if we have a better holy factory um artificial sensors um combine that with other

01:03:49.600 --> 01:03:55.600
percepts I think there's a lot of opportunity to um learn unified representations that could

01:03:55.600 --> 01:04:02.960
probably go further than when we train on each separately great great well Peter it's been wonderful

01:04:02.960 --> 01:04:07.920
catching out with you appreciate you being so generous with your time and sharing a bit about

01:04:07.920 --> 01:04:15.200
what you've been up to uh very cool stuff well Sam thank you so much this was uh a lot of fun

01:04:15.920 --> 01:04:21.360
and and actually I guess before we close out I should I mentioned this upfront but you're you've

01:04:21.360 --> 01:04:29.920
joined the the podcaster brother hiding sisterhood you know um let's we won't talk too much about

01:04:29.920 --> 01:04:34.400
it but you know what's your podcast what are you trying to do and where should folks go to find it

01:04:34.400 --> 01:04:43.840
yeah so um just a few weeks ago I started um my own podcast called the robot brains and um you can

01:04:43.840 --> 01:04:53.120
find it on Spotify Apple and so forth just the robot brains um and having a lot of fun meeting

01:04:54.320 --> 01:05:00.720
what we're really focused on is is guests who tried to bridge the gap between research

01:05:00.720 --> 01:05:08.000
AI research and bringing AI into the real world that's kind of the general theme um I think it's

01:05:08.000 --> 01:05:14.640
a really exciting time to see AI transition in many many places into real world and so a lot of

01:05:14.640 --> 01:05:20.160
the discussions are centered around that but you know it it also goes from there to other topics

01:05:20.160 --> 01:05:27.520
generally AI research robotics research and applications very cool I'm assuming it can be found on

01:05:27.520 --> 01:05:34.720
Spotify Apple Google all the usual places right just look for the robot brains awesome we'll link

01:05:34.720 --> 01:05:39.760
to it in the show notes thanks once again Peter thank you Sam thank you so much for having me

01:05:39.760 --> 01:06:06.480
this was a lot of fun thanks

