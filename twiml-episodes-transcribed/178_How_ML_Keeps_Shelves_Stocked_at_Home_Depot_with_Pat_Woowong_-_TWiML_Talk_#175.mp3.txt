Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington and this episode we're joined by Pat Ruong, principal engineer
and the applied machine intelligence group at the Home Depot.
We discuss a project that Pat recently presented at the Google Cloud Next Conference which
used machine learning to predict shelf-out scenarios within stores.
We dig into the motivation for this system and how the team went about building it, including
what types of models ended up working best, how they collected their data, their use of
Kubernetes to support future growth in the platform and much more.
Enjoy.
All right everyone, I am on the line with Pat Ruong.
Pat is a principal engineer in the applied machine intelligence group at the Home Depot.
Pat, welcome to this week in machine learning and AI.
Thank you for having me Sam, it's a pleasure being here.
So why don't we get started by having you tell us a little bit about the applied machine
intelligence group and what your focus is there?
Sure, of course.
So the applied machine intelligence group at Home Depot is kind of like a newly formed
team.
We've been around for about a year with the current team members that are on the team and
our primary focus is to operationalize machine learning.
So what that means is for, I'm sure like all the people that are listening to this
probably know what that means, but for those of you that don't, it's basically taking
machine learning models that don't just generate reports, reporting type things.
It's things that interact with the real world, things that you can actually have an effect
on in real life.
And it's taking stuff from conceptualization all the way to production.
So we do everything from the discovery work to building the models, data pipe binding,
even app development and putting that into production for business teams.
And do you have a established platform for productionalizing these types of projects
or do you tend to build out infrastructure on a case by case basis, depending on what
a specific application needs?
That's a good question.
So typically I think a lot of productionalization of ML models follows the same kind of pattern.
It's data pipe binding, step one, and then feeding the pipeline or the tables that you have
or whatever data you have into the model, step two, and then three doing something with
that information afterwards.
And then potentially feeding that information back into the model for it to learn off of
if you have like an active learning model or something like that.
And what we did with this very first project that we presented at Google, we basically built
out a bunch of infrastructure that didn't exist.
We built it all in Google Cloud.
So if you, for those of you that don't know about Home Depot's transition to Google Cloud,
we recently have begun a huge, I wouldn't say recently, maybe like two years ago, we started
this massive journey to move all of our enterprise data warehousing into Google Cloud.
And it has been a huge success for us as a company because we are now able to do all of these
things and experimentation in the Cloud without having to buy more servers to put them in
the data center, stand up hardware, we can just request new, we can request resources
on demand and we can try things out without having to spend a lot of man hours to get that
infrastructure stood up in our own data center.
So the specific project that you presented on was one focused on minimizing shelf-outs
at Home Depot stores.
Can you talk about the context and origin of that particular project?
Yeah, it's really interesting how this project came about actually one of the data scientists
that's on my team.
He was part of the supply chain organization within Home Depot and he had this idea
to, and his name is Sashi Gandhavarpu, and he had this idea to use signals from within
the store to kind of predict whether stuff would be on the shelf.
So the reason this is, it was kind of like a really important idea is that within any
type of retail space, unless you have cameras or sensors or some kind of thing within
the store to track products, once it enters the store, you almost have no idea where it
is until it leaves this, leaves the store either through the register or shrink or something
like that.
So we had this, he had this idea to use data from, from the sales or there, we can get
into like the features in a second, but essentially using data to drive the shelf-out prediction.
So what a shelf-out is, is something is in the store, but not on the shelf.
And that's not the same thing as it being out of stock, which is obviously if it's out
of stock, then our supply chain will react to the out of stock, the out of stock levels
and it'll be triggered through supply chain and it'll obviously come back through to the
store for someone to put back on the shelf, but this is for stuff that gets either someone
picks it up off the shelf and you don't know where it goes, like maybe they try to buy
it, but they will like, okay, well, I don't want this anymore and they get to the checkout
line and they don't want any more or maybe someone takes the item or steals it or, or maybe
it's just so on top of the shelves and those reacts.
We probably all had this experience where we go online, it says there are 20 in the store,
you go to the shelf and there is just a big hole and you got to find someone and they climb
up on the big ladder and sort through those boxes.
Yeah, and I actually mentioned that in the talk and the Google talk is about the Home
Depot is kind of like a working warehouse, so there's no back room.
So if it says there's four in the store, they're probably in the store somewhere, but we
probably don't know where they are and it usually takes quite a long time.
If it's not on the shelf and it's not directly in the overhead right above that item, then
it'll take hours to find it.
One of the things that I appreciated about the presentation you did or the way you
approached this project is that you didn't just assume that this would be a good idea.
You actually manually brute-forced it, as you said, by having people manually stock the
shelves and then you measure the revenue lift from actually having the products on the
shelves.
That's something that I think Home Depot is very â€“ that we do quite often with a lot
of these tests.
So before we're allowed to go past like a POC stage or even before we go to POC stage,
we have to determine the ROI for the project that we're about to do to convince people to
give us money to do the project.
So in order to do that, we said, okay, well, if we had a system that could continuously
keep stuff in stock and on the shelf, what would that look like and how much money would
that actually generate?
So you add labor into the store.
Obviously, it's not scalable to the entire company to add labor that just does pack downs
the entire time, what pack downs like moving stuff into the shelves.
But it essentially simulates what our project ended up doing.
And from that, we were able to determine the sales lift from doing some like that and
the ROI for developing this kind of technology.
In fact, you saw that just by increasing shelf availability by a single percent, you were
able to justify kind of continuing on with this project.
How did you approach the next step, which is kind of the modeling step?
Yeah, so what we did was we built some ETL pipelines to pull data in from the various
different sources that we were going to use, supply chain, sales, space planning.
There's so many different data sources.
I can't even list all of them.
I think some of the features that we generated were in the deck somewhere.
But anyway, so what we...
Can you give us some examples of those?
Sure, of course.
So this is an interesting one.
So Sashi actually likes using this example and it's a very good example of a really interesting
feature is when something comes into receiving in Home Depot, sometimes there's this notion
of something called shelf capacity.
And shelf capacity is how many things you can put on the shelf at one given time.
Sometimes what ends up happening is you receive a pack of something and the ratio of that
pack to how many things you can actually put on the shelf is not an even number.
What I mean by that is like there's a remainder, obviously.
So let's say you put seven in the pack and then four on the shelf.
So you end up with three back in the overhead.
And to an associate, they're going to say, okay, well, is there more things to be put back
on the shelf or is that shelf actually out of stock?
And that thing for some...
Or that trigger for someone to go look for those other three things doesn't happen all
the time.
So sometimes things get lost in the overhead until someone actually goes and loads and finds
and says, oh, there's three of these things, I better go put it on.
But when people change shifts, they don't know exactly where the other person who did
the stocking before put that thing.
So it gets lost.
And that leads to shelf outs.
So that ratio of pack size to shelf capacity is an interesting feature that comes from
different things.
So it comes from the planogrammed size of the actual thing that you're trying to put
on there.
So how much shelf capacity is.
Also, when was the last time it was received from the pack size from supply chain?
And that can vary actually between stores.
So sometimes some stores get more in a single pack size and sometimes...
Yeah, so that's an interesting thing.
Obviously sales is a huge indicator of a shelf out.
The forecast, what we expect to sell, what its price was, the on hands that we have.
So let's say we had three of them yesterday and then four of them the day before and
then eight of them the day before, three days before.
So that slope of something decreasing over time is also another thing that we use as
a feature.
Interesting.
And you mentioned that the data that ultimately comes to feed the model came from a bunch
of different sources, is there a way to characterize the number of sources or the level of effort
and just building out that data pipeline?
Yeah, it was a pretty monumental effort, I think, because it was a very cross-functional
effort to get all of this information.
And Home Depot is such a big company and there's different teams that are responsible for
different data sources.
So getting all of the teams to help with their...
We use a lot of them as advisory roles as well because we weren't able to think of every
single feature that went into this model.
They came up with some ideas for us.
So we met with a lot of these teams over the course of several months and they kind of
helped build out this feature set.
Some of them, including obviously space planning, finance, supply chain, inventory planning
and replenishment, store operations, there's a lot.
You kind of talked about this data pipeline and all this data that you need to receive from
different places.
How much of that came before the modeling process, like in the exploratory phase and how
much of that effort was, you know, one you're trying to productionalize.
Like did you...
I'm imagining in the modeling phase, you're doing samples, but even those samples might
have had to have been fairly large.
Like how did you approach that?
Yeah.
So some of the features that, or some of the data sources that we used weren't even in
the cloud when we started building this model.
So we actually had to develop the ETL pipelines that would put this data into the cloud from
the operational databases that took a little while.
There were several of those that we had to do.
So when we were gathering all of this data, it took, I would say, like, six months to
get everything in a stable state before we could actually, I mean, I guess the modeling
happened simultaneously, but as we were trying to, like, so we had samples of this as we were
going along.
Like you said, and we were able to build these models off of, like, old data, but in
order to get it to a state where we could run the training process on a weekly basis,
took quite a long time.
And like we're trying to remember, like, all the details of how, of, like, all the things
that we did, and there's just, there's just so many of them.
And we had help along the way, right?
It wasn't just our team, like, there were other teams that are cross functional teams
that were telling us where stuff was, because it was very difficult to find database.
You can imagine, like, how many databases Home Depot has for every team that exists there.
And various different teams within Home Depot are in different stages of their cloud journey
too.
So sometimes we'd work with some teams that would know how Google Cloud worked.
And the Lingo and all the different types of technologies that are available in there,
and that some had no idea, because they hadn't really started that journey yet.
So it was definitely an interesting challenge doing that.
What did you end up with in terms of a model, either the details or some sense of the complexity
there?
Yeah, that model was actually pretty simple.
So the original model that we built was a random forest model.
I think we used Scikit for the initial one, and then we switched to XG booths, so gradient
booths to trees.
And it actually performed really, really well for our purposes.
And we felt like it was good enough, and we didn't need to explore anything any further
than that.
But we do, so when we also engaged Google professional services about halfway through our journey, and
they helped us build a TensorFlow model using Dataflow or Apache Beam.
And it ended up, I think, the performance was about the same as our XG booths model.
So we went with a more simple model just for trying to get it out there.
So we were kind of like on the clock to deliver some results.
So we wanted to show our business partners like, hey, this thing is going to work.
And there was a lot of, I don't want to say skepticism, but there was a lot of like
I would say, you know, you kind of like have to convince people how ML is going to add
value.
And we really wanted to show that it was going to add value because we saw the results
through our paper validations, right?
So in the, after the initial model was built, went to the stores and validated, did some
like paper validations, they're like, okay, wow, this is actually going to work.
So let's get this to a production status fast so we could possibly can, and as fast as
we possibly could wasn't really, in my, I don't think it was fast enough.
We were kind of, we, we had a lot of technical challenges to get it there.
But obviously, this was the first one that all of us did together.
So we learned a lot along the way.
Well, I want to dive into some of those technical challenges further.
But before we do that, I'm curious, what would you say accounted for most of the skepticism
on the part of your business partners?
You showed them that if you increased shelf availability, you can generate a ton of incremental
revenue.
So I imagine they bought into that.
Was it specifically, they didn't think you'd be able to predict shelf-outs given the data
that you had available in the stores?
I think there's just kind of like a misconception about what machine learning does and how it
can add value because I think a lot of people currently think it's like a, it's like one
of those hype technologies, right?
Like everybody thinks that it needs to, that they need it for whatever problem.
And I don't think like it was their focus at the time to adopt this into their stack yet.
Because you know, we were a new team.
And like the way that our team is positioned within the company isn't that we're not like
part of the team that we're building this for.
So in essence, we were kind of like a consultants within Home Depot, if you will, and we kind
of like build these things out.
I guess we were more seen of like as a POC team versus like getting something all the
way somewhere or developing something all the way to its production-wise state even
that we could.
And we were kind of like trying to prove our worth if you know what I mean.
Sure. So you started building out these data pipelines in parallel with the modeling step.
At some point in time, you had the data pipelines and the models in place.
What was next?
And if I skipped something and kind of setting that up, Devon, no, no, no, no, that's perfect.
So yeah, after we had the model, we knew it was producing valid results, actionable results.
We had to build some service layers to be able to send the model predictions to the stores.
So that took a little bit of time.
So we ended up integrating with an existing application that's already in the Home Depot
that associates are they're familiar with using.
And this was at this few level that we wanted to send predictions at.
So it ended up being like a perfect app for us to use.
We used this app and we made like a few one slight modification, which enabled us to get
solicit feedback from the users.
The thing that we added was a shelf out yes or no on this on the screen where they're
actually going to check these items.
So they can see if the so we can see if the things that we're sending are actually accurate
versus just blindly sending them out.
So then we that was like the next biggest thing.
And that kind of we probably I think I think we should have probably done that a little
bit sooner.
That development step really took a lot of time and it was basically like we were working
on that the entire time before being able to get any further.
So it was kind of like a roadblock for us at the time.
So specifically this was hooking into the existing app.
And I think it was an app that is on these little mobile devices at the store associates.
And are you are you hooking into an existing notification process that is targeted to specific
associates or does it go into a queue, but it's basically like a pick list or checklist
of items that you need to check on their stock out status and or restock if necessary.
Oh, not stock, but shelf out.
Yeah, that's exactly what it is.
So that application is it's called a smart list and what we what it was traditionally
used for was to have associates go and check the stock levels of things that that we thought
were not at the right levels.
So we were like, okay, well, this is like the perfect app to be able to stick this into
because instead of them having to go and check the stock levels, we can check if they're
on the shelf too.
So we added this button to allow that to happen.
And now we're getting data from the stores that were live in about the stock levels
and if they're shelfed out or not when they go and check them for the purposes of the
development with this app, you weren't asking them to count the number of items on the
shelf like a shelf inventory.
It was just on the shelf or not.
No, actually, they they still do that.
So we we tried to make it as we tried to make it as minimal in terms of having to retrain
people as possible so we didn't want to introduce like a new workflow to them at the time.
We we kept the app exactly the same pretty much and we didn't do anything except for adding
this one button that they had to do during their normal inventory checking process.
So it made it so that it's it's kind of like more intuitive so that no one really had
to know what was going on behind the scenes like to them, the the tasks were identical.
No one really knows like if the tasks are coming from the original task generation system
or from us. And so if I could ask the rude slash ignorant question, when you look at
the picture of this button, it's like on shelf, yes, no.
But yet you said it was really complicated and took a long time.
Where is the hitting complexity in that button that seemed so easy?
It's yeah, I'm without saying like anything like like too bad.
It's like the framework that was used on it, I wasn't very familiar with it.
So I had to learn how the deployment process was set up.
So this is this is where all the complexity came in.
I had to learn how all of that was set up, how their deployments work, how to deploy
their code to the stores, which takes a long time.
So home people obviously we're very we don't like deploying, breaking potentially breaking
changes to the store environment.
So there's a lot of checks in place to make sure that what you're actually deploying
is not going to break anything and all of those processes that are there are the things
that make it complicated.
So the actual app changing the app actually wasn't that hard once I was able to figure
out what to change and then building the services that were on prem that hooked into that.
So home depot stores are kind of like they they have their own kind of data centers inside
of them and they don't they don't feed off of a centralized location per say.
So we you have to there's a really really long complicated deployment process.
So that's where a lot of the complexity comes in makes sense makes sense.
So you've got this capability now deployed out into the stores via this app.
How do you tie that all together with the model?
Perfect so the model runs every day at six o'clock in the morning.
The predict the predictions part and we we generate all of the the tasks that so that
are supposed to get sent to the store for that day and we're not sending.
I think we send them over like a certain confidence thresholds like I think it fits.
If we think it's I don't remember exactly the percentage that it that it's set at.
But what if they're over a certain confidence level we send the tests to the stores and we
only do.
I think we send maybe like 150 or something like that per day to each store for to have them
go fix which isn't really that much in terms of how many there are.
So there's like 33,000 skews in in a in any particular home depot store but we're only
really trying to fix about 150 of them per day which ends up adding over time.
So hopefully like those 150 you fix it one day will be a that it'll kind of like have
a cascading effect over time and you sort it by confidence or you sorting by profitability
or some business metric.
So there we haven't actually started doing the profitability the business metrics yet.
So we exclude some of them obviously there's departments like lumber where it's very unlikely
that if the even if there was a shelf out that you could fix it because if the lover is
not there then you can't get any more yeah.
And there's there's other categories that are like that.
So as we were as we were pushing this up in the in our pilot stage we found a lot of interesting
things that we didn't really know about the store landscape and the like the the overall
shelf out rate of it individual department.
So obviously like your high your high your high selling things like your like power tools
and stuff they there's a lot of people buying those all the time but stuff like the blind
section wasn't as as so like we were sending a shelf out prediction to them that weren't
accurate in the blind section.
So we ended up just not sending those ones at all either there's a couple other there's
a couple other categories that were like that those ones stand out the most but it was
interesting seeing that that data come back that obviously like we didn't know before
that those those things would happen.
Another interesting thing that that happened as we were deploying this out was because
we have that six o'clock I guess our SLA for sending these things out is eight o'clock
in the morning but we start running the pipeline at six with up to eight o'clock being
the last the the last time that these tasks can go out the reason being is we want to make
sure that the tasks that are going out to the stores have not been affected by people
doing things in the stores because the data that we have right now it's not coming in
in an hourly cadence so we can't really do predictions at on an hourly level yet because
of our data sources.
So we need to make sure that when we're sending these tasks out to the stores that the
tasks get to the phone before anyone has done anything to change the store but that's
why we that's why we set our SLA to eight o'clock in the morning and if it if it doesn't
meet that SLA then we don't send anything out because it would it would affect our accuracy.
It sounds like we've kind of closed the loop on this process and how the predictions
are being made at least you run your pipeline daily are you also doing periodic retrains?
Yes we are so we're retraining weekly at the moment so that's actually another interesting
that we found is that the model would become stale after about after like two weeks our
accuracy started dropping and we were like okay well is that mean that it's not good
anymore or what's what was going on this is when we were only alive in one store and
we hadn't set up automatic retraining it so we were trained the model off of the latest
data that we were getting back from the shelf out data and we saw the accuracy jump back
up and we're like okay well there's got to be some kind of thing that something that's
going on that causes that to happen and I think it's a it's a combination of the things
that are being sold in the store are very seasonal so Home Depot obviously and it gets a
lot of other retailers and maybe Home Depot specifically has a lot of seasonality so
like things that are being sold in the spring usually don't get sold in the summer especially
things that don't get sold in the winter don't get sold in the summer like snow blowers
and things like that's a good example grills get sold a lot in the summer but yeah so
like incorporating that last week of data into the training really improve the accuracy
of it and we we tried to we we assumed or maybe not assume but we we hypothesized that training
on a year's worth of data would capture that seasonality but it didn't so we still have
a little bit of digging to figure out what that what's going on there really maybe getting
back to the kind of the infrastructure elements of this system on the you know starting out
with the ETL pipelines how did you implement those so yeah we did all of the ETL pipelines
in BigQuery it was relatively easy to get all of those data sources in there you know I really
feel like it wouldn't have been possible to get all of these different cross functional
teams data in an easily consumable way if we hadn't done a cloud migration yet so that was
kind of like a key key component or key thing that had to happen for us to be able to do a project
like this and I think I said this when I was at Google or doing the next presentations that
but that yeah this this is a huge initiative that allowed projects like this to even happen
because we couldn't have even imagined tackling a project like this before without having that
that amount of data in one place for us to to get it so yeah the ETL pipelines they're all done
in in BigQuery we feed that into Google Cloud store or we extract that into Google Cloud storage
and then we feed that into our model that our model writes something back into BigQuery and then we
have a service that picks those inferences out of BigQuery and pushes them to the phone can you
elaborate on the this loop BigQuery to storage to back to BigQuery sure so the home Home Depot has a
and this is something that a previous team that I was on bill we have a thing that automates all
of our ETL process our extraction from on prem to the to Google Cloud and we use that same tool
to help us automate a lot of our running processes so our model it runs on a Kubernetes cluster
so we send the model code off to the cluster to run but in order to chain all of these things
together we have a bunch of SQL that runs in the beginning and then SQL being BigQuery
SQL and then after the SQL data prep steps I think there's like there's probably like 15
data data aggregation steps before maybe maybe even more there's probably like 20 if you count
all of the ingest pieces so that I'd say about like 15 to 20 SQL steps and then after that the
resulting features table is exported to Google Cloud storage for the model to consume and then
the model reads it all into itself because it's an XGBoost library that we're using and XGBoost is
fed in with pandas so we just read read it in with pandas put it into XGBoost and then let it do
it let it do its thing and then afterwards the the model then writes all of its results back into
BigQuery so that's kind of like the circle of how that works and then there's yeah and then there's
a little bit um there's some there's some other steps that do the sending of the task right so we
have to obviously send this out every day at six or by 8 a.m so we have another process that picks
all of those tasks up and sends them to the individual store servers and then obviously when whenever
the associate's come in and they work the tasks they hit the yes or no button on the app and then
we get our feedback and we can find out how well we're doing and then that goes into another cycle
we although I will say one caveat we have not started retraining our model off of
off of the data that's been coming back from smart list yet the reason being is that we
didn't have enough of a sample to be able to do that with yet so the the the model is our project
is live in around 50 stores at the moment but it's not a large enough sample size of that's like
representative of the entire company to be able to build a model off of it yet so we're waiting
until we get past our 50 stage or 50 store um current stage and then once we get to like maybe
I don't know I think probably we'll probably start building out that pretty soon but
I'm not really sure when at what level we're going to consider putting that back into our new model
when you talk about that feature are you thinking about that in the sense of like an active learning
where you're dynamically updating your model or just using that information for additional feature?
um I think we're probably going to go with an active learning type of situation where it will
it might update the the model as it is I mean ultimately what we really want to do
is have this uh work on an hourly basis so it can react very very quickly to these types of
situations versus having to wait an entire day um I think we're like maybe we're pretty far away
from being able to do that but I think the that's like the that's the dream is being able to get
it to that level um and I think active learning would definitely be the thing that we would
want to go to so you mentioned you've got a Kubernetes cluster is that where the model's running?
yeah you've got containerized scikit-learn or python uh somewhere that is you're kicking off
these containers and they're just as part of their launch they're pulling data and running the
model yeah that's correct how many instances or pods or containers what's the best way to to
measure that the kind of scale of yeah it's um it's not really that big so I think we run on
I think we're using like CPU training right now actually which is um it's it actually serves
our purposes for now because we haven't had to change anything but it's like a I think it's like
15 V CPUs and 50 gigabytes of RAM um per pod um and we have a uh um I mean the nice thing about
Kubernetes obviously is like when you're not using it it scales down to zero so um it uh so
whenever we launch jobs uh I guess the that's for like the training part of it um
um is uh we use like 50 gigabytes of RAM and um 15 CPUs uh the data size is around 66
million skews so you just take the number of skews per store times the number of stores that we
have so it's like six around 66 million per store so that's around the data volume that we're
doing it's not anything like super huge in terms of what you would use uh I take that back sorry
that's like the maximum amount that we use for for uh in our inference pipeline so that's that's
like the the biggest that it'll ever get really for our inference pipeline for our training dataset
before any of the uh data preparation happens um like the our sales table is massive every transaction
that's happened at home depot for the past seven years so there's yeah it's it's pretty big it's
like several terabytes um the supply chain data is even is it's crazy or it's uh
though that's like you know you can imagine like a supply chain right so it's every time something
gets touched in a supply chain has essentially has a row in in a table um and it and it has all
of its history maintained too so that one's even that one's even bigger but the I think the
training set that we end up end up with is um trying trying to think I think it's it's definitely
less than 50 gigabytes it's not very big okay and so these these 15 uh vcps with Kubernetes
is there how many actual nodes is that is that 15 or using uh it's a yeah it's just a single node
with um with the with that turned up for now single node single worker or multiple workers uh
a single node single worker okay got it um and so you've got uh this single node single worker set
up and even in that kind of scenario you're still taking on the overhead of Kubernetes because of
what just kind of ease of deploy and uh replicate containers and stuff like that yeah that's a good
question so initially when we're building this out we wanted to have something where we could run
other projects in so we didn't want to be limited by um we wanted to be able to submit jobs to this
cluster um on a uh for when we wanted to start other projects essentially being able to do this
thing and take on more projects take on more work and do more types of exploratory work throughout
the company so that was our intent of building this thing out the way that we did and it sounds
like you're running it on uh GCE as opposed to GKE uh we're running on GKE uh okay yeah I guess
I was thinking you don't think about the VCPUs and GKE yeah yeah I mean yeah you you can you can
set the um set the node size or in the node pool settings um on GKE if you or I guess you can
the way that we have our setup is we have one cluster that does our training and predictions
so um they're different node pools and and each node pool has different size of um size of nodes
that that it can pull from and then when we request the CPU from it from the um from the job
configuration like the training job obviously it has its affinity set to the training uh node pool
and then vice versa for the predict if I remember correctly at next google announce a bunch of
extensions to BigQuery I BigQuery ML I think was what they called the machine learning extension
is that something that you see as playing a role in this type of system uh I have yeah I think
BigQuery ML has a really is it's actually really really cool tools so I've played with it a little
bit um not like not a lot but I think from a trying to like do some discovery work and seeing
what kinds of things are there it's an amazing tool because you don't have to build anything
to try it out um it's it's literally just using whatever the semantics that they put in there
to to be able to run a model on top of it and I I'm sure they're coming out with tons of other
models that you can run inside of it and um just based off that alone like it definitely
be worth us trying to see how well it does compared to our current system we just haven't had time
to do it yet so maybe to kind of wrap things up where do you see this progressing from here both
in terms of the shelf out project we've talked a little bit about future directions there but also
additional projects kind of in this vein at Home Depot yeah so um that's that's something that
our team is kind of like trying to do is uh kind of you know democratized ML throughout the
company make it easy for other teams to do stuff like this um with with shelf out specifically
we want to we obviously want to see it deployed to all the all the stores which we're pretty sure
is going to happen eventually um we're not sure when but it'll happen um and then and then also
implementing the active learning piece so using the data that we're getting back from the first
phone from this uh from smart list to be able to do our next wave of training and I think I didn't
mention this earlier but if um you haven't seen the presentation there's a part about where we
gather training data so at the moment the training data that we're being gathered is coming from
that we're gathering is coming from another initiative in Home Depot that that um this team called
the Met team is actually scanning outs within a store they're not doing it across the entire
company but they're doing it across a subset of them and um that's where we're building our
training data off of right now so we want to switch the training data to use the data from smart
list versus using that and obviously it's like additional labor that's in the stores and whatnot
so um ultimately it would be better if you didn't have to do that because all they're doing is
scanning stuff that's not on the shelf it's kind of boring so um another thing that we want to
see happen I guess like all a lot of the infrastructure that we built out again is for taking on new
projects so um hopefully there will be some other interesting things that we get that are coming down
the pipe and um we're really looking forward to all the other teams that we're going to get to work
within the coming years a quick question on the this platform idea you know it's is clear how
in the case of the training framework building it on Kubernetes you're building a little bit ahead
of the requirement for this specific project so that you can easily have infrastructure for future
projects does that same principle apply on the data pipeline side are there specific abstractions
that you built to or generalize tools that you built out that you are looking to being able to
replicate across different projects so we have a um I guess we have a beam pipeline that we're
not currently using that could be used for the streaming use case so like if data eventually gets
streamed in we would we would adopt like a data data flow or batchy beam type of model for
ingesting that data um that's a component that we're not currently using that we could reuse
later um another thing is that we have a um people don't really think about running I guess you could
say production type workloads for feeding applications off of an analytics type of database
so people don't really use think of BigQuery or any type of like enterprise data warehouse as uh
as a database that they would use to feed an app so a lot of the data sources they obviously
they come from other teams yada yada yada and they they come at different time so we built a system
to wait for all of these different jobs to finish um and for tables to update properly so that
as soon as everything is completely done our job kicks off automatically so kind those kinds of
tooling things that we built are definitely going to be useful for us in the future kind of like a
dependency checker for data resources yeah exactly that's what that's pretty much what it is
and it's actually been working very very well for us I mean um our our rollout to the 50 stores
that we're at so before we went to next we were in five stores and the day that next started
we rolled out to 50 stores and we weren't even at um even out the home base to do it we just hit a
button and it worked sort of awesome yeah yeah I mean it was major uh yeah yeah yeah exactly it was
very it was very very minimal to to get it up and running from remotely without without
all hands on deck kind of thing so it was it was nice seeing that happen all right everyone
that's our show for today for more information on pet or any of the topics covered in this show
visit twimmel ai.com slash talk slash 175 if you're a fan of the podcast please pop open your apple
or google podcast app and leave us a five star rating and review your reviews are a great way
to help new listeners find the show as always thanks so much for listening and catch you next time
