1
00:00:00,000 --> 00:00:04,960
All right, everyone. Welcome to another episode of the Twimble AI podcast. I am your host,

2
00:00:04,960 --> 00:00:10,400
Sam Charrington. And today, I'm joined by Heather Nollis, a principal machine learning engineer

3
00:00:10,400 --> 00:00:15,760
at T-Mobile. Before we get going, be sure to take a moment to hit that subscribe button wherever

4
00:00:15,760 --> 00:00:21,040
you're listening to today's show. Heather, welcome to the podcast. Thanks for having me, Sam. I'm

5
00:00:21,040 --> 00:00:25,360
excited to be here. I'm excited to chat with you. It's going to be a fun conversation

6
00:00:25,360 --> 00:00:31,200
about your journey with machine learning at T-Mobile. But before we jump into that,

7
00:00:31,200 --> 00:00:35,600
I'd love to have you share a little bit about your journey in machine learning more broadly.

8
00:00:35,600 --> 00:00:42,800
How do you get into the field? So my undergraduate degree is actually a neuroscience. And when I was

9
00:00:42,800 --> 00:00:50,160
studying neuroscience, I had this study where I had to keep rats alive for a year and measure

10
00:00:50,160 --> 00:00:55,360
their blood pressure. And at the end of my study, I had this notebook full of data where I very

11
00:00:55,360 --> 00:01:00,160
diligently had written every single thing about all of these rats. And I was so excited about

12
00:01:00,160 --> 00:01:04,560
my results. And I gave them to the PI of my lab. And I was like, here we go. And she was like,

13
00:01:04,560 --> 00:01:09,040
great, now you can pack this past this off to our analytics team who will analyze your results.

14
00:01:09,040 --> 00:01:14,880
And being like a micro-managing person who likes to know everything, I almost blew a gasket.

15
00:01:14,880 --> 00:01:20,800
They're like, what? My data. I'm perfectly qualified. And she's like, are you? And I was like,

16
00:01:20,800 --> 00:01:27,120
I will become qualified. Watch. And so I went, I took my first Python courses. I took some

17
00:01:27,120 --> 00:01:32,080
bioinformatics courses. And at that point, I was like, what I really want to do. Well, I kind of

18
00:01:32,080 --> 00:01:36,960
fell in love with computer science at that moment. And imagined I would end up with a PhD

19
00:01:36,960 --> 00:01:43,440
at molecular neuropharmacology doing big data that way. So I went to get a master's in computer

20
00:01:43,440 --> 00:01:48,240
science. And while I was getting that master's, I started working at team mobile. And while I was

21
00:01:48,240 --> 00:01:53,760
there, the team that I was on said, Oh, we're thinking about doing some AI proof of concepts.

22
00:01:53,760 --> 00:01:58,000
And my hand just got straight up in the air. It was like big data. It's the whole reason I started

23
00:01:58,000 --> 00:02:03,520
programming. It's like all of my side projects are in LP. Like, please pick me. And so that's kind

24
00:02:03,520 --> 00:02:07,920
of the story of how I got into doing this professionally. Awesome. I think we're going to have to call

25
00:02:07,920 --> 00:02:13,520
this episode machine learning comma cooler than rats. It is. It is though. Yeah.

26
00:02:16,560 --> 00:02:22,960
And so I guess we can start from the beginning then. You were there. Was that was this proof of

27
00:02:22,960 --> 00:02:30,320
concept the beginning of all ML at all of team mobile or in your particular quarter of team

28
00:02:30,320 --> 00:02:35,440
mobile? Yeah. So what had been happening is I think the same thing that many large companies had

29
00:02:35,440 --> 00:02:40,320
where you had huge data warehouses, lots of analytics, people doing decision science to put

30
00:02:40,320 --> 00:02:45,520
numbers into power points to help executives make really smart decisions. But we didn't have

31
00:02:45,520 --> 00:02:50,320
any real time models. And we weren't doing anything that I thought was super cutting edge. So at

32
00:02:50,320 --> 00:02:55,120
this is five years ago, so at the time, it was deep learning at all. And so my team's goal in

33
00:02:55,120 --> 00:03:00,720
this proof of concept was to put team models first real time deep learning model into production.

34
00:03:00,720 --> 00:03:06,000
And that's kind of where we've we've staked our home. It's like we do things real time. We focus

35
00:03:06,000 --> 00:03:12,560
on on more cutting edge style solutions. And we don't do a lot of that like batch analytics. We

36
00:03:12,560 --> 00:03:18,720
are the real time AI team got it. And how big was the team at the beginning? So the initial proof

37
00:03:18,720 --> 00:03:23,600
of concept, there was me and one other engineer and we were like, we need a we need a very strong

38
00:03:23,600 --> 00:03:27,760
data leader in the mix. So we we brought in my wife Jacqueline Nolis who has been on your podcast

39
00:03:27,760 --> 00:03:32,880
before. And so she helped us with that original proof of concept. And so it was the three of us

40
00:03:32,880 --> 00:03:40,240
that took that first API into production. And then wow. So how that happens kind of a cool story,

41
00:03:40,240 --> 00:03:46,400
if I can digress. How we even got to do this proof of concept was that team mobile has like an

42
00:03:46,400 --> 00:03:52,160
internal shark tank innovation round. So we pitched all these ideas and then they gave us $100,000

43
00:03:52,160 --> 00:03:59,360
in three months to pull something off. Okay. And so that's when we developed our first our first

44
00:03:59,360 --> 00:04:04,160
NLP deep learning model. We got it released into production. And then at the end of the innovation

45
00:04:04,160 --> 00:04:08,480
round, you pitched to executives trying to get them to buy your product forever. And we were

46
00:04:08,480 --> 00:04:12,960
bought. So after that, the we said, okay, now we have to build a full scale product around this

47
00:04:12,960 --> 00:04:18,240
and and get it in front of our front line to actually be used in contact centers and scale it.

48
00:04:18,240 --> 00:04:24,400
And so maybe we're going further down the the rat hole coming back to rats again. But like,

49
00:04:24,400 --> 00:04:30,400
how was that $100,000 spent? Was that like salaries or were there other hard costs associated with

50
00:04:30,400 --> 00:04:36,880
this POC? It was mostly salaries. Some of it was also training because we had the question of should

51
00:04:36,880 --> 00:04:43,120
we be building the stuff ourselves? Of course, I think that like, but we wanted to make sure that

52
00:04:43,120 --> 00:04:47,760
we weren't doing to mobile a disservice by not considering a lot of these like big vendors that

53
00:04:47,760 --> 00:04:52,960
claim to offer intent modeling as well. So some of it was in in trials with those sorts of things

54
00:04:52,960 --> 00:05:00,080
and then a lot of the rest was just staffing and research. And so talk about the this thing that

55
00:05:00,080 --> 00:05:06,720
you pitched the the the products. What what what was it seeking to solve? What was the use case?

56
00:05:06,720 --> 00:05:13,040
So so the team that I was on was focused on building all of the software plumbing that connects

57
00:05:13,040 --> 00:05:18,800
the experts in contact centers, solving team mobile problems to the customers that are trying to

58
00:05:18,800 --> 00:05:25,120
talk to us via digital means. So this is mostly Facebook, Twitter, our app, however, they're typing

59
00:05:25,120 --> 00:05:30,480
to us. We built the plumbing for all of that. And so when it came time to say, well, team mobile wants

60
00:05:30,480 --> 00:05:35,760
to do some AI stuff. What should we do? Of course, I say we are sitting on top of tons of conversation

61
00:05:35,760 --> 00:05:40,640
transcripts. We have all of this data of our customers telling us exactly what their problem is.

62
00:05:40,640 --> 00:05:48,400
And so our very first like just proof that we could even build a deep learning model was just a

63
00:05:48,400 --> 00:05:53,600
simple intent model. So we had 88 different intents that we identified or topics really throughout

64
00:05:53,600 --> 00:05:57,920
team mobile that people are really calling in about. And so it was developing and deploying that

65
00:05:58,480 --> 00:06:04,560
first model. And the product that we stood up around it was one messaging expert who's responding

66
00:06:04,560 --> 00:06:11,120
to Twitter, Facebook, app messages, web messages might have 10 different windows open at one time

67
00:06:11,120 --> 00:06:17,360
because these chats are asynchronous. And context switching is really difficult. So actually showing

68
00:06:17,360 --> 00:06:21,920
the topic of the conversation in whatever quick facts we can pull up about this person's account

69
00:06:21,920 --> 00:06:26,960
related to that topic is super, super useful because then they don't have to prep themselves.

70
00:06:26,960 --> 00:06:31,040
As soon as a customer comes in, we've already got their first message. We've calculated what

71
00:06:31,040 --> 00:06:35,600
their intent is. They're coming in. They're looking to talk about upgrading their phone. We've

72
00:06:35,600 --> 00:06:38,640
actually went out, pulled the data to say what phone they have now. And then we've

73
00:06:39,200 --> 00:06:43,600
surfaced links that say in case you don't know how to upgrade someone's phone. Here's actually how

74
00:06:43,600 --> 00:06:48,640
you do it. And so that's our first product that we came out with. And we called it expert assist.

75
00:06:48,640 --> 00:06:54,400
But now blank assist is industry standard terminology. But I will confidently say we coined that one.

76
00:06:54,400 --> 00:07:06,400
And so what were your you had a bunch of data? Was your data already I presume it wasn't already

77
00:07:06,400 --> 00:07:13,440
labeled for something or? No, what was it? Yeah, I'm so glad you asked about that because when we

78
00:07:13,440 --> 00:07:18,560
talk about where did that first $100,000 go so much of it went to the most expensive labeling

79
00:07:18,560 --> 00:07:24,160
job of all time. T-Mobile's very into customer privacy. So no way were they going to let us have

80
00:07:24,160 --> 00:07:28,080
an external vendor label our data. And at the time, they were like, we don't even want you to

81
00:07:28,080 --> 00:07:32,160
bring on anyone new because they might not have the business knowledge to accurately label data.

82
00:07:32,160 --> 00:07:38,960
So we paid data scientists to label our first 20,000 conversations manually, which yeah, most

83
00:07:38,960 --> 00:07:44,480
expensive labeling of all time. We now have an internal labeling team of five data annotators.

84
00:07:44,480 --> 00:07:51,120
So much different story now. And so what were some of your first steps in kind of building this out?

85
00:07:51,120 --> 00:07:55,920
The first thing that we said is we wanted to try unsupervised learning, right? We didn't want to

86
00:07:55,920 --> 00:08:00,720
have to take a supervised deep learning approach if we could do something with unsupervised learning.

87
00:08:00,720 --> 00:08:05,840
But using LDA and almost any unsupervised approach that we could, it really came up with that.

88
00:08:05,840 --> 00:08:11,440
There are two major types of conversations that happen in T-Mobile. About 80% of them are about

89
00:08:11,440 --> 00:08:19,360
the topic T-Mobile and about 20% are about the topic phones. So that's where we said, okay,

90
00:08:19,360 --> 00:08:23,840
I don't know about this. And I actually, I found a status. Even if you set your number of classes

91
00:08:23,840 --> 00:08:28,640
to be much bigger than that, like they still, it's just different ways to say phone and different

92
00:08:28,640 --> 00:08:33,440
ways to say T-Mobile. Yes, or different types of phone. And that's not necessarily business actionable,

93
00:08:33,440 --> 00:08:38,160
right? Like we weren't seeing any business actionable classes shake out. And I talked to another

94
00:08:38,160 --> 00:08:44,080
team who has managed to do this sort of modeling in an unsupervised way. But it involved like a

95
00:08:44,080 --> 00:08:48,720
Rube Goldberg machine of data pipelines to get somewhere. And so we said, we don't have time to

96
00:08:48,720 --> 00:08:52,800
think about that. Let's at this point, let's label 20,000 conversations and see what we can do.

97
00:08:52,800 --> 00:08:57,200
And you are ultimately hoping for your intent to these topics to pop out of this

98
00:08:58,000 --> 00:09:04,720
unstructured, unsupervised process. Yeah. And the thing that was really important is because we

99
00:09:04,720 --> 00:09:09,920
wanted to do real-time stuff, topics of like phone T-Mobile, even the type of phone someone's

100
00:09:09,920 --> 00:09:15,440
talking about, not useful real-time to an expert, because you have a human being sitting there

101
00:09:15,440 --> 00:09:20,080
also listening to the conversation. Like they very well know that it's still about iPhone.

102
00:09:20,080 --> 00:09:25,600
You know, so any of the classes that we create, we want to make sure there's actually business

103
00:09:25,600 --> 00:09:30,720
actions we can take there. And so the unsupervised thing didn't pan out. And so what'd you move to?

104
00:09:30,720 --> 00:09:35,520
So after that, that's when we decided to do supervised learning. But then we came to the problem

105
00:09:35,520 --> 00:09:41,760
that everybody has, which is what is our taxonomy going to be. Like every legacy company, we had

106
00:09:41,760 --> 00:09:47,920
taxonomies that existed. I believe that each taxonomy should be created for the specific problem

107
00:09:47,920 --> 00:09:54,240
that it's looking to solve instead of shoe horning as many as many meanings as you can to classes

108
00:09:54,240 --> 00:10:00,960
that are already created. And so we had to spend a lot of time advocating against some taxonomies

109
00:10:00,960 --> 00:10:07,760
used for post-call analytics. So I had to prove that the topics that we need real-time

110
00:10:07,760 --> 00:10:14,080
are not the same things that are useful after the call to say what people are generally talking

111
00:10:14,080 --> 00:10:20,400
about. And that took a very long time and was very difficult. So proving that we needed to actually

112
00:10:20,400 --> 00:10:25,680
develop our own taxonomy for this problem, very difficult. Because from a business perspective,

113
00:10:25,680 --> 00:10:29,760
it's introducing confusion. We have one dashboard. We know what all these things mean. Please don't

114
00:10:29,760 --> 00:10:36,160
let us make us learn another one. But after we won that argument, we asked our product and business

115
00:10:36,160 --> 00:10:40,960
people to go basically lock themselves in a room and come out with some sort of hierarchical

116
00:10:40,960 --> 00:10:46,480
taxonomy for us. And then we refined that taxonomy during our labeling process. So since we had

117
00:10:46,480 --> 00:10:51,600
data scientists having the label stuff, we were able to point out when classes were going to be

118
00:10:51,600 --> 00:10:56,960
easily confused based on the language being used or whenever they called out a class that was like

119
00:10:56,960 --> 00:11:03,280
nobody ever talks about that. So an example is they had five high-level categories for what

120
00:11:03,280 --> 00:11:08,320
different topics would be about in T-Mobile. And one of the categories was network. And

121
00:11:09,200 --> 00:11:13,280
customers don't have a lot of nuanced language to speak about the network. They really just say,

122
00:11:13,280 --> 00:11:18,320
hey, I have a question about my network or my signal. So they had spent time creating like 16

123
00:11:18,320 --> 00:11:22,480
subclasses of network conversations for us to have to come back and say you wasted your time.

124
00:11:22,480 --> 00:11:26,640
People really only, our customers only vaguely speak about network because they're not engineers.

125
00:11:28,880 --> 00:11:35,760
So going back to this idea of building the case for your own taxonomy,

126
00:11:36,400 --> 00:11:41,040
in retrospect, was there a silver bullet? How did you win that argument?

127
00:11:41,040 --> 00:11:44,400
Well, for someone else who's maybe embroiled in that now, like what?

128
00:11:44,400 --> 00:11:49,440
Yeah, well, the first thing I will say is if you're going to have this argument one time,

129
00:11:49,440 --> 00:11:53,840
you're probably going to have to have it a hundred times. So for me, like, I won this argument once,

130
00:11:53,840 --> 00:12:01,520
and then I've had to win it every time since. And what I keep going back to is I think that

131
00:12:02,160 --> 00:12:07,360
business users often forget that modeling is kind of the easy part of data science.

132
00:12:07,360 --> 00:12:11,680
So when we talk about creating new models, new taxonomies, they get very nervous. But for me,

133
00:12:11,680 --> 00:12:16,960
what takes a long time on our project is figuring out the business case. Can we actually bring value

134
00:12:16,960 --> 00:12:21,760
if I build this model? I can build shiny models for forever that sound really cool that aren't

135
00:12:21,760 --> 00:12:27,040
making anyone money. And so that's what takes a long time. So one of the things that's really

136
00:12:27,040 --> 00:12:32,160
helped me is being able to document that and saying, you're scared of us changing the taxonomy

137
00:12:32,160 --> 00:12:36,240
because you're afraid it will take time. That's not the bulk of the time. So you don't need to be

138
00:12:36,240 --> 00:12:42,400
scared about that. And then the second thing is just trying to drive home a culture of small models

139
00:12:42,400 --> 00:12:48,640
for small problems, build things specific for your use case to answer it exactly. Otherwise,

140
00:12:48,640 --> 00:12:54,720
you will get a deteriorated product. And so there's that. We also took all of the enterprise tax

141
00:12:54,720 --> 00:13:00,080
onomies and lined them up against each other. And I was able to show places where I needed

142
00:13:00,720 --> 00:13:03,920
a piece of information that the old taxonomies did not have inside of it.

143
00:13:03,920 --> 00:13:11,680
So a good example there might be, sometimes when people can't pay their bill, they set up a

144
00:13:11,680 --> 00:13:15,360
payment arrangement. And sometimes they can't pay their payment arrangement. And for me,

145
00:13:15,360 --> 00:13:21,680
that can't pay payment arrangement is very important. And I had that as a separate class. And in

146
00:13:21,680 --> 00:13:26,720
the other thing, it was all just billing. And it's like, I actually, like, billing might be useful

147
00:13:26,720 --> 00:13:31,120
for a PowerPoint when you need to know how much time call center experts are spending on billing,

148
00:13:31,120 --> 00:13:35,280
but it's not useful for helping an expert solve a call about billing while it's happening.

149
00:13:35,280 --> 00:13:41,280
And that kind of, like, light bulb moment. But I've had to, every time we go to build a new model,

150
00:13:41,280 --> 00:13:45,680
I do have to have this fight again. And so I have like a pre-planned deck that just kind of lines

151
00:13:45,680 --> 00:13:55,040
it all out. Nice. Talk more about this idea of small models versus big Uber models.

152
00:13:55,040 --> 00:14:02,880
Yeah. So our very first model that we released, I mentioned it, it had 88 different classes

153
00:14:02,880 --> 00:14:08,720
and was kind of a disaster. It still exists. It's still running. Like, like, it's still running.

154
00:14:08,720 --> 00:14:15,680
We are doing the giant refactor on it right now. But I learned a lot in doing that. So the first

155
00:14:15,680 --> 00:14:21,760
thing was we were really dedicated to doing a representative sample of the data. So we did not

156
00:14:21,760 --> 00:14:26,560
subsample anything. It was like a truly representative sample of conversations we labeled.

157
00:14:26,560 --> 00:14:32,320
And so we labeled this, but it turns out that many business actionable things are small and rare.

158
00:14:33,040 --> 00:14:39,200
So great that I picked the 88 most frequent topics, but they actually need when somebody is

159
00:14:39,200 --> 00:14:45,520
requesting a password update. And that's nowhere in my 88 topics. And so that's when we started saying,

160
00:14:45,520 --> 00:14:50,240
well, if we need password questions, let's build specific models and then talk about how we can

161
00:14:50,240 --> 00:14:57,040
write route to those specific submodels, if necessary. But just because it's kind of difficult,

162
00:14:57,040 --> 00:15:01,200
I'll give you an example of my favorite small model that we have that the natural language model.

163
00:15:02,640 --> 00:15:06,880
And I like it because it's an example of a time that we removed a chatbot from people by being

164
00:15:06,880 --> 00:15:12,800
smart. So when you messaged us, we used to have, well, thank you for those of us who didn't want to

165
00:15:12,800 --> 00:15:18,560
use that chatbot. Right. Yeah. That's why I like it. I built chatbots before. I think that there's

166
00:15:18,560 --> 00:15:24,480
a place for them. But sometimes when you can get away without it, don't. But so it was whenever

167
00:15:24,480 --> 00:15:28,560
you would message us for the first time, like if you were normally someone who called T-Mobile,

168
00:15:28,560 --> 00:15:33,600
but this time you're texting us, we would send you a picker that said, are you a customer select,

169
00:15:33,600 --> 00:15:39,520
yes or no? And the reason why we had to do that is some, some, you would think T-Mobile knows your

170
00:15:39,520 --> 00:15:43,360
phone number. You should know if we're a customer or not. But we actually don't in many situations,

171
00:15:43,360 --> 00:15:49,760
depending on which app you're coming to us from. And what we quickly realized is like asking people,

172
00:15:49,760 --> 00:15:53,840
yes or no, there is very silly because we can really tell from their first message whether

173
00:15:53,840 --> 00:15:57,680
they're a customer or not. If they messaged us, they say, I need help with my bill. Nobody

174
00:15:57,680 --> 00:16:02,160
is just not a customer needs help with their bill. But if they say, I want to switch to T-Mobile,

175
00:16:02,160 --> 00:16:07,760
nobody who is a customer says that. And so we were able to just take the first messages and the

176
00:16:07,760 --> 00:16:12,880
picker responses that were already selected and build a really quick, like, shallow neural network

177
00:16:12,880 --> 00:16:17,360
between the two and eliminate that picker for 80% of customers to chat with us.

178
00:16:18,560 --> 00:16:23,920
Going back to the previous example, you had this 88-class model and you're kind of

179
00:16:24,720 --> 00:16:29,200
in the process of refactoring it out to smaller models kind of along this idea.

180
00:16:30,720 --> 00:16:35,920
What do you hope to replace it with? You would think the router is still a class, an 88-class

181
00:16:35,920 --> 00:16:41,920
classifier. If it's a learn router, are you looking at heuristic approaches as opposed to

182
00:16:41,920 --> 00:16:47,680
the learn approaches? For us, one of the major things was that when we created this

183
00:16:47,680 --> 00:16:51,760
out of this representative sample of data, we aren't able to get those classes that

184
00:16:51,760 --> 00:16:55,920
business users are asking us for. So if they're asking us for something that occurs 0.001

185
00:16:55,920 --> 00:16:59,840
percent of the time, and we keep pulling this representative sample of data, we will have to

186
00:16:59,840 --> 00:17:04,720
label millions of conversations before we have enough to get the enough data to even get what

187
00:17:04,720 --> 00:17:10,720
they want to show up. So one of the major things in our refactor that we're focused on is reducing

188
00:17:10,720 --> 00:17:16,080
the number of pieces of label data. We need to create these new intensor topics.

189
00:17:16,080 --> 00:17:22,240
And so we are using, we still have the same, well, we've done some taxonomy refactoring since then,

190
00:17:22,240 --> 00:17:27,760
but we are shifting from our own in-house neural network to using distilbert from hugging phase

191
00:17:27,760 --> 00:17:32,640
as a baseline and then retraining on that. And then we're able to reduce the number

192
00:17:33,440 --> 00:17:39,040
of label data pieces we need per class dramatically before it would be 5 to 10,000. And now we're

193
00:17:39,040 --> 00:17:45,040
at 1,000, which makes us be faster for our business. So that's our major thing that we're focused on.

194
00:17:45,760 --> 00:17:53,760
The other thing that's kind of changed since we released is we originally released our models

195
00:17:53,760 --> 00:18:03,600
in R. So it's just R in a Docker container as an API. And they ran pretty okay. Like we were doing

196
00:18:03,600 --> 00:18:09,920
two million returns a day, but we are this kind of is where we switched to voice. So we were

197
00:18:09,920 --> 00:18:16,400
building this all in messaging. We had to have 20 containers of our model, but it was serving two

198
00:18:16,400 --> 00:18:21,360
million responses a day. And the team of all the business, they came to us and they said,

199
00:18:21,360 --> 00:18:25,440
it's really cute what you're doing here for all of the customers contacting us in messaging,

200
00:18:25,440 --> 00:18:32,000
but 90% of care traffic is in voice. So we say, okay. So we want you to build this where it will work

201
00:18:32,000 --> 00:18:40,560
for people who call in as well. Say, okay. Well, we can't have over 200 pods running of our topic

202
00:18:40,560 --> 00:18:44,960
model when this goes through production. What do you mean just sprinkle some Kubernetes on there?

203
00:18:44,960 --> 00:18:51,520
Like it's the most expensive of all time. Well, and the thing is is that messaging conversations

204
00:18:51,520 --> 00:18:55,600
tend to be pretty succinct. If you're chatting with somebody, you're direct on a voice conversation.

205
00:18:55,600 --> 00:19:00,800
You're going to talk about the weather and your kids. And so even 200 is not a good example,

206
00:19:00,800 --> 00:19:05,520
because it almost just depends on how chatty people are. And so that's when we said, okay,

207
00:19:05,520 --> 00:19:10,480
we need to do something smarter with how we're serving these models. And what we ended up doing is

208
00:19:10,480 --> 00:19:17,840
they are now deployed as Java Spring Boot services that run Python as a sidecar. And they are,

209
00:19:17,840 --> 00:19:23,600
they have both API endpoints, but then they're also Kafka consumer producers. So we use Kafka

210
00:19:23,600 --> 00:19:28,080
streaming architecture because we have so many predictions that we're making at this point.

211
00:19:28,080 --> 00:19:33,040
Was Java is Java just an engineering standard there that folks were comfortable with running in

212
00:19:33,040 --> 00:19:40,560
broad or yes, like we have people who like Java, but we did try in Python first because like we're

213
00:19:40,560 --> 00:19:45,440
like our data scientists know Python. Let's do this in Python. At the time when we were doing it,

214
00:19:45,440 --> 00:19:53,360
the Python streaming libraries for Kafka were not mature enough to do the joins that we need to do

215
00:19:53,360 --> 00:20:00,640
to get all of our data to be like filtered correctly. And so we ended up switching to Java because

216
00:20:00,640 --> 00:20:04,640
of that, but it's something that we do look at every once in a while to see how mature those

217
00:20:04,640 --> 00:20:09,440
Kafka streaming libraries for Python have become. Because once they can do very great streaming

218
00:20:09,440 --> 00:20:14,320
joins, then we would love to get Java out of our stack. I don't feel the need to keep languages

219
00:20:14,320 --> 00:20:19,040
around just because they're they look good enterprise wide, but yeah, there's a whole other

220
00:20:19,040 --> 00:20:25,040
rat hole maybe around streaming joins. Where where the complexity come in there?

221
00:20:25,040 --> 00:20:31,440
Well, at the time we were trying to build a dashboard for the business operations center that helped

222
00:20:31,440 --> 00:20:38,560
them with staffing. So I'll I'll kind of set it up where we have all sorts of bells and whistles

223
00:20:38,560 --> 00:20:43,520
on networks that tell you whatever those towers are going down, but no matter how fast we make

224
00:20:43,520 --> 00:20:47,360
those, we will never be faster than our customers who are going to tell us the second there is

225
00:20:47,360 --> 00:20:51,760
any problem. And when cell phone towers go down, people fled to messaging.

226
00:20:51,760 --> 00:20:56,080
You want to do like some kind of event correlation across all these things to try to figure out what

227
00:20:56,080 --> 00:21:02,000
is the actual thing happening? Even that or even being able to tell the operations center before

228
00:21:02,000 --> 00:21:06,480
the network engineering team has figured out what is going on, right? So network engineering team,

229
00:21:06,480 --> 00:21:11,200
they might get a blip. There's an issue. They're still investigating, but we can read in customers

230
00:21:11,200 --> 00:21:16,080
words. And we we know where that customer usually is using their phone because they have a primary

231
00:21:16,080 --> 00:21:22,240
place of use address. And so we can make some assumptions there. They also wanted us to look at

232
00:21:22,240 --> 00:21:28,720
the topics that people were coming in on and say, are there any anomalies? Because we had like if

233
00:21:28,720 --> 00:21:33,280
there's a billing systems issue, it could take a while for us to figure that out, but our customers

234
00:21:33,280 --> 00:21:37,920
might be telling us bright this second. And if we just look at the at how often customers are

235
00:21:37,920 --> 00:21:42,640
coming in with billing issues, maybe we can predict some of those downstream issues. We are also

236
00:21:42,640 --> 00:21:46,720
looking at it for staffing. So if we can predict how many conversations we're going to get in what

237
00:21:46,720 --> 00:21:51,440
sort of topics we're going to have, how do we make sure that our contact centers are staffed

238
00:21:51,440 --> 00:21:56,480
appropriately? And it was in doing that. They had a bunch of different filtering. They wanted

239
00:21:56,480 --> 00:22:00,720
to do on that dashboard, like click a state and then see all the topics. They also wanted to do

240
00:22:00,720 --> 00:22:05,840
trending topics for those states. So unsupervised utterances we were pulling out of there. And it was

241
00:22:05,840 --> 00:22:12,960
in all of that clicking that like the Python was not performing well on the joins to do that type

242
00:22:12,960 --> 00:22:20,800
of filtering. Even taking a step back, going from text to speech, it sounds like yeah, there's a

243
00:22:20,800 --> 00:22:28,640
ton of complexity that I'm imagining is introduced and going from going from one to the other.

244
00:22:28,640 --> 00:22:32,960
Can you talk a little bit about how you dealt with that? Yeah, so I'll tell a personal story first

245
00:22:32,960 --> 00:22:38,240
because I think it's really fun. I had worked in NLP but only on text and we had one other

246
00:22:38,240 --> 00:22:42,800
machine learning engineer who was our speech specialist. When we finally got green light to

247
00:22:42,800 --> 00:22:47,600
start working on speech, she was like, Heather, here's an initial reading. I'm going to be out for

248
00:22:47,600 --> 00:22:53,360
two weeks. You study speech to text and when I come back, we'll have a conversation. And she

249
00:22:53,360 --> 00:22:58,240
she came back and I sat down and I was like, so I've been thinking a lot about tropical

250
00:22:58,240 --> 00:23:04,640
semi-rings because you can use them to optimize a lot of speech to text calculations and she just

251
00:23:04,640 --> 00:23:10,960
looked at me and said, you've gone on the wrong path. She's like, you will never need any of that

252
00:23:10,960 --> 00:23:17,040
math. And I was like, oh, whoops. So that was my first lesson and like everything here is different.

253
00:23:17,680 --> 00:23:23,440
There's a lot that you have to think about, well, first from the technical perspective,

254
00:23:23,440 --> 00:23:30,640
which text, a computer gets text, how does your computer get a phone call? I didn't know the answer.

255
00:23:30,640 --> 00:23:38,000
I do now, but you, like the signals that phones are sending are like, I think RTP and STP

256
00:23:38,000 --> 00:23:43,520
streams and those are not web sockets or anything that a computer can consume. So you actually

257
00:23:43,520 --> 00:23:50,400
have to take those phone streams and route them through something like Zoom that can take

258
00:23:50,400 --> 00:23:56,320
computer calls to make them into phone calls and vice versa, use pieces of that to actually

259
00:23:56,320 --> 00:24:01,840
convert the audio streams into web sockets so that way we can even get the audio to start. And so

260
00:24:01,840 --> 00:24:07,600
at that point, it was kind of mind-blowing. And then in the speech space at all, you have to

261
00:24:07,600 --> 00:24:12,480
start thinking about not only the language that customers are saying to you, but the acoustic

262
00:24:12,480 --> 00:24:16,240
environment that they are in and how that's impacting the transcription. And for us,

263
00:24:16,240 --> 00:24:23,840
most speech detect modeling that's done is for, well, it's either done by, it's done on data

264
00:24:23,840 --> 00:24:30,080
that is men reading audiobooks slowly, how it's trained, and then it's normally trained for

265
00:24:30,080 --> 00:24:36,480
people speaking slowly in their acoustically nice living room. But that's not our customers,

266
00:24:36,480 --> 00:24:40,960
like our customers are in line at Starbucks and they are trying to do this on their lunch break,

267
00:24:40,960 --> 00:24:45,920
like they are yelling and their dogs are barking in the background. And so figuring all of that

268
00:24:45,920 --> 00:24:50,960
out and learning about that and about the way that different accents work and how we can try and

269
00:24:50,960 --> 00:24:57,600
make this technology we're building work equitably for all of our customers has been like,

270
00:24:57,600 --> 00:25:01,200
there's nothing like it in the text world whatsoever. You don't have the features.

271
00:25:01,760 --> 00:25:08,720
You started talking about the the compute environment pods and all that stuff.

272
00:25:08,720 --> 00:25:16,320
How are you running out? Is this the GPU heavy workload? Where is it running?

273
00:25:16,320 --> 00:25:23,280
Yeah. So our transcription stuff is incredibly GPU heavy. Our topic modeling

274
00:25:23,920 --> 00:25:29,360
less so, but the transcription is. And so we started out with a like an on-prem

275
00:25:30,240 --> 00:25:37,600
provider specific for GPU hardware because at the time we were getting in like ridiculously

276
00:25:37,600 --> 00:25:41,200
good latency because that that's one thing that's really important is a lot of time speech

277
00:25:41,200 --> 00:25:46,320
detects. It doesn't matter if it's slow because you might be transcribing stuff for post calling

278
00:25:46,320 --> 00:25:52,560
analytics, but for us, if we're trying to build experiences that pop up to help an expert solve a

279
00:25:52,560 --> 00:25:58,000
problem, we have to be faster than the human being hearing this sentence. So if someone's like I

280
00:25:58,000 --> 00:26:03,040
need to set up, I would love to pay my bill and I say great, I'm here to help you pay your bill.

281
00:26:03,040 --> 00:26:08,720
Let me dig around and then I click the app, I start to pay their bill and then a pop up comes up

282
00:26:08,720 --> 00:26:12,800
and says, would you like to pay their bill because our transcription is slow? I've created.

283
00:26:12,800 --> 00:26:19,760
That sounds like clippy. Yeah, exactly. And so so latency is like super important to us to build

284
00:26:19,760 --> 00:26:26,560
trust. And so we originally had an on-prem provider because we were getting good latency and we took

285
00:26:26,560 --> 00:26:32,800
it to AWS and we were like, hey, like can what can you guys do here at the time their latency

286
00:26:32,800 --> 00:26:37,440
was not very good. But since then we actually have been able to switch to a cloud provider. So

287
00:26:37,440 --> 00:26:45,600
we do use AWS to do this. But we have our own internal hosted Kubernetes that is then hosted on AWS

288
00:26:45,600 --> 00:26:51,280
if that makes up. Okay. But we do. It is a GPU heavy workload. So we do have them running on GPU

289
00:26:51,280 --> 00:26:56,640
instances for our topic models. Right now we're in the process of converting those to AWS

290
00:26:56,640 --> 00:27:02,960
inferringia chips, which we're still we're still waiting to see what the actual improvement will be from using

291
00:27:02,960 --> 00:27:12,480
the inferringia chips. This may be going back a bit, but you you mentioned that with the current version

292
00:27:13,120 --> 00:27:21,840
of this, I think as part of the the text to speech-based system transition, you went from

293
00:27:21,840 --> 00:27:28,640
something to distilbert like pre-trained distilbert and fine-tuning that. What was the the thing before?

294
00:27:29,200 --> 00:27:36,400
It was a hand a handcrafted network using Keras that was a like a shallow convolutional neural network.

295
00:27:37,040 --> 00:27:45,840
Okay. So very bespoke. Right. Exactly. And so it takes a lot to say we we're we're not going to do

296
00:27:45,840 --> 00:27:50,320
that anymore. But we still do like bespoke neural networks for other problems, but for our giant

297
00:27:50,320 --> 00:27:57,120
topic model distilbert's good. We in our first our first experiment with distilbert, we just like

298
00:27:57,120 --> 00:28:01,920
pulled it off the shelf and took our already labeled data and trained against it. And after one

299
00:28:01,920 --> 00:28:05,520
epic, we saw very good results. So we're like, okay, this is probably the direction that we're going.

300
00:28:05,520 --> 00:28:13,520
You can get this in just one epic. And have you had to do anything special to to get it to deal

301
00:28:13,520 --> 00:28:20,000
with the number of classes that you're working with? Not so far, but as I kind of mentioned, we're

302
00:28:20,000 --> 00:28:24,800
in the process of maybe breaking apart the models into a bunch of different submodels and having

303
00:28:24,800 --> 00:28:31,040
having a better router at the top. So we might end up doing that. We have zero interest in introducing

304
00:28:31,040 --> 00:28:36,800
new classes into this model because it's it's so old and giant. We one of our data scientists

305
00:28:36,800 --> 00:28:41,360
calls it the Toyota Corolla models. Like it just it's it's reliable. It does what it does, but no

306
00:28:41,360 --> 00:28:48,400
and super impressed by it. So we want to kind of let it continue do what it's doing. But any of

307
00:28:48,400 --> 00:28:53,040
the new type of topic modeling that we're looking to do, we are we are building other models to do

308
00:28:53,040 --> 00:28:59,200
it. We will never have an 88 class model again because maintaining it is awful. And I don't know if

309
00:28:59,200 --> 00:29:05,520
you answered the question about what what you're anticipating to use for that router. Oh yeah,

310
00:29:05,520 --> 00:29:11,600
so we don't know yet. It really depends on the type. Well, it depends on the type of experiences

311
00:29:11,600 --> 00:29:20,240
that that are our care stakeholders want. So I think a writ like for the first go that we have,

312
00:29:20,240 --> 00:29:26,080
we have two different experiences that we are rolling out. One is about network and one is

313
00:29:26,080 --> 00:29:31,040
about customers who are dissatisfied. And we think it's just going to be a logistic regression.

314
00:29:31,040 --> 00:29:35,280
Right? Like just which one do they go to dissatisfied or network or otherwise maybe

315
00:29:36,080 --> 00:29:41,040
which we can get in that. But we're not sure how that's going to scale. It really depends

316
00:29:41,040 --> 00:29:47,680
on their roadmap. So we've got to be in tight collaboration. So the idea then is that as opposed to

317
00:29:47,680 --> 00:29:53,040
kind of this flat 88 classes, you're going to have a kind of a hierarchical taxonomy. And you just

318
00:29:53,040 --> 00:29:57,680
have to figure out which of the which of the handful of branches they need to be routed to. And then

319
00:29:57,680 --> 00:30:03,360
you'll do the kind of fine grain classification lower down. Yes. Yeah. So it will be a hierarchical

320
00:30:03,360 --> 00:30:09,360
taxonomy. We will pass it off. And then within each of those general experiences that we're trying

321
00:30:09,360 --> 00:30:14,960
to roll out, we want to have like a proper like state engine where we have intent being recognized

322
00:30:14,960 --> 00:30:19,360
and then we're kind of deciding what to pop out for the for the expert next using an LSTM

323
00:30:19,360 --> 00:30:24,080
of some sort. So that's kind of the future vision that we're building where we're able to like

324
00:30:24,080 --> 00:30:31,040
walk experts through these flows smartly using the NLP that we're doing to check off any checks

325
00:30:31,040 --> 00:30:36,800
or fill in any boxes for them as they go through these workflows. And now there are tons of these

326
00:30:36,800 --> 00:30:45,360
off-the-shelf kind of intent engines and chatbot engines and conversational AI tools. How do you

327
00:30:45,360 --> 00:30:53,200
think about kind of the build versus buy decision? So what I tell my stakeholders always is if there's

328
00:30:53,200 --> 00:30:56,880
something out there that honestly works better than what I would build, I would love to buy it.

329
00:30:57,520 --> 00:31:01,840
But I'm very hesitant to put something that works less well than what we have in front of our

330
00:31:01,840 --> 00:31:08,800
experts. And so I can give a very easy example here which is recently we had a vendor solution

331
00:31:08,800 --> 00:31:16,480
that we were looking at that identified promises and commitments inside of conversations. And

332
00:31:17,200 --> 00:31:21,840
that's very exciting because we always want to know what our experts are promising our customers

333
00:31:21,840 --> 00:31:25,680
but we're committing to and if they call back in later and they say you promised me we want to be

334
00:31:25,680 --> 00:31:31,520
able to say yes we did promise you you're right I'm sorry. But when we dug into it the out of the

335
00:31:31,520 --> 00:31:38,400
box accuracy was like 53% so he said okay let's dig into some more of these and they didn't make any

336
00:31:38,400 --> 00:31:43,600
sense to us they weren't what we would consider promises or commitments from a business perspective

337
00:31:43,600 --> 00:31:48,320
whereas we put one of our data scientists on the task for one week and we can come up with something

338
00:31:48,320 --> 00:31:54,960
with about 80% accuracy because we know what we're looking for and that's the very hard story that

339
00:31:54,960 --> 00:32:02,240
we've been telling our stakeholders over and over again is that yes general language models work

340
00:32:02,240 --> 00:32:07,440
very well for general English but we don't really speak English at T-Mobile like we speak T-Mobile

341
00:32:07,440 --> 00:32:12,240
leagues there are so many words in the English language that will never appear and there are so

342
00:32:12,240 --> 00:32:16,000
many things that we are going to talk about that are completely different so an example I like to

343
00:32:16,000 --> 00:32:20,480
use is the word jump has a literal different meaning at T-Mobile because jump is the name of our

344
00:32:20,480 --> 00:32:26,720
insurance plan so like it's an argument for custom embeddings and so for me I think it's totally

345
00:32:26,720 --> 00:32:32,480
appropriate to take an off the shelf solution to prove a concept out and see if there's any value

346
00:32:32,480 --> 00:32:38,080
but then I always like to ask can you do something cheaper and better and so I can kind of speak

347
00:32:38,080 --> 00:32:42,640
about our speech detects here where we originally did roll out with vendor partners so we did a

348
00:32:42,640 --> 00:32:50,800
huge RFP every major speech detects writer in the world that exists I have reviewed them and we

349
00:32:50,800 --> 00:32:57,120
launched our original proof of concept with AWS transcribe and so we did use a vendor but

350
00:32:57,120 --> 00:33:02,080
immediately once we had the audio data we started looking at open source solutions and saying

351
00:33:02,080 --> 00:33:09,680
what can we do on our specific data and the word error rate on our data that we have right now

352
00:33:09,680 --> 00:33:14,400
is nine state of the arts like seven to five like state of the art for like standard slow

353
00:33:14,400 --> 00:33:20,240
English is seven to five and our word error is nine anytime we test a vendor product against it it

354
00:33:20,240 --> 00:33:29,520
is 18 human like 18 ish and you lose measurable meaning after 20 and so I just kind of all the time

355
00:33:29,520 --> 00:33:35,040
even when vendors say we have we have state of the art speech detects when you run it against our

356
00:33:35,040 --> 00:33:40,560
calls it's not because our acoustic environments are very strange the language that we use is

357
00:33:40,560 --> 00:33:47,120
different we have a lot of strange factors in our business and so just trying to reiterate constantly

358
00:33:47,120 --> 00:33:52,720
like like yes the numbers that you're seeing in the media are great if we were Wikipedia you know

359
00:33:52,720 --> 00:34:02,480
like if this but we're not our conversations are not Wikipedia so you mentioned dealing with

360
00:34:02,480 --> 00:34:08,880
accents for example and you know that representative of being representative of other biases that

361
00:34:08,880 --> 00:34:18,240
you might encounter how do you approach that whole spectrum of factors yeah so so for me when I

362
00:34:18,240 --> 00:34:23,680
think about it if I'm building efficiency tooling for the frontline I'm trying to make them

363
00:34:23,680 --> 00:34:30,080
smarter better at their jobs most contact centers compensate people by how happy the customers are

364
00:34:30,080 --> 00:34:35,200
afterward how quickly they saw calls how quickly they go through them and so then I'm thinking

365
00:34:36,080 --> 00:34:42,640
it is so incredibly important for me personally to build models that serve everyone equally

366
00:34:42,640 --> 00:34:47,440
because if not if I'm building models that help some people that's literally going to increase

367
00:34:47,440 --> 00:34:51,200
their paycheck and if I'm building models that don't help other people that's going to literally

368
00:34:51,200 --> 00:34:55,200
decrease their paycheck and so we talk about that all the time on our team like it is

369
00:34:55,200 --> 00:35:02,560
it is the most important thing to us and so we actually we have a full-time AI ethics specialist

370
00:35:02,560 --> 00:35:07,920
who she's an engineer and she's focused mostly on our voice models and so there's a data set

371
00:35:07,920 --> 00:35:13,680
called the Mozilla Common Voice Data Set which is anybody can go you can speak and transcribe

372
00:35:13,680 --> 00:35:18,800
your audio, tag it with your demographic data so we started with that testing our models against

373
00:35:18,800 --> 00:35:24,640
Mozilla Common Voice Data Set we found it to be pretty insufficient in some striking ways so an

374
00:35:24,640 --> 00:35:31,600
example is they have multiple different Asian accents listed in the data set for Africa they

375
00:35:31,600 --> 00:35:38,960
just have African as if there are yeah as if there aren't multiple African accents and so we

376
00:35:38,960 --> 00:35:45,360
use them originally to to kind of get a benchmark there but what we're working on really now is

377
00:35:45,360 --> 00:35:50,640
building our own team mobile version of that like what is the data set that accurately represents

378
00:35:50,640 --> 00:35:55,200
all of our customers and all of our employees that we can use to measure different word error

379
00:35:55,200 --> 00:36:00,160
rates against and see how we can improve and the one of the reasons why this is super important to

380
00:36:00,160 --> 00:36:05,760
us is our speech to text solution is very hard to scale like by the time we are at full scale we

381
00:36:05,760 --> 00:36:10,560
will be the largest speech to text for contact centers in the world or at least that was true a

382
00:36:10,560 --> 00:36:16,960
year ago I haven't checked both but but so it's very hard to scale and so we are right now live with

383
00:36:16,960 --> 00:36:24,240
7,000 agents but we plan to be live to everybody in the US by the end of 2022 and everybody globally

384
00:36:24,240 --> 00:36:31,040
by the end of 2023 and so for me what's very important is before we roll out to global partners

385
00:36:31,040 --> 00:36:36,560
so anybody who is answering the phone who is from any other country for two mobile I need to make

386
00:36:36,560 --> 00:36:42,640
sure that my models work well for them and so we have a huge focus on those specific areas

387
00:36:42,640 --> 00:36:48,480
in collecting that data to make sure that we will not release models that do not perform within

388
00:36:48,480 --> 00:36:54,480
an acceptable window of standard in this situation and so that's really kind of how we've done it

389
00:36:54,480 --> 00:36:59,600
but there's not there's not a good open source data set that has all the data that we need to

390
00:36:59,600 --> 00:37:04,320
accurately bias test it so we're just having to curate our own. You've been talking about speech

391
00:37:04,320 --> 00:37:11,520
to text speech to text is the transcript an actual product that you need or is the transcript an

392
00:37:11,520 --> 00:37:17,680
input to downstream things and that's kind of a you know the next question is you know do you

393
00:37:17,680 --> 00:37:23,600
think about like some uber you know deep network that goes straight from speech to intense and

394
00:37:23,600 --> 00:37:28,560
skip the whole text thing. I'm glad that you asked that because that was my dream originally

395
00:37:28,560 --> 00:37:32,320
when we said speech to tech I was like and we're going to build a tim modeling directly on audio

396
00:37:32,320 --> 00:37:38,640
signals and that's when my speech scientist was like Heather no okay she's like the computation

397
00:37:38,640 --> 00:37:42,960
for that would be so ridiculous but so what our speech to text mostly does right now is it's

398
00:37:42,960 --> 00:37:48,160
it's pattern it's powering a product we call auto memo after the end of every call our experts

399
00:37:48,160 --> 00:37:52,720
have to spend three to five minutes typing out everything that it was about and we said actually

400
00:37:52,720 --> 00:37:58,880
we think we can summarize some of this. Doing that took a very long time it turns out that

401
00:37:59,520 --> 00:38:04,000
human speech is not very good to even do an extractive summary from so like not only

402
00:38:04,000 --> 00:38:09,520
organization is still hard right well and especially when people speak we're not very succinct

403
00:38:09,520 --> 00:38:14,160
so that took a very long time to figure out what that product will look like and it is a combination

404
00:38:14,160 --> 00:38:19,680
of we add the top three what we call called drivers like the things that we think made them actually

405
00:38:19,680 --> 00:38:25,680
call in to the memo and then we also have it where experts can click and view the entire transcript

406
00:38:25,680 --> 00:38:32,000
if they need to dig in for some reason. I don't love showing the full transcript because I'm

407
00:38:32,000 --> 00:38:36,080
like we're going to make mistakes and people are going to give us feedback and it will be embarrassing

408
00:38:36,080 --> 00:38:40,880
but so far so good the experts seem to really like that and we're we're even toying around with

409
00:38:40,880 --> 00:38:46,640
the concept of if they can see the transcript in real time while they're talking could they flag

410
00:38:46,640 --> 00:38:52,720
things for correction so for us to go in and correct the transcripts later but so but the

411
00:38:52,720 --> 00:38:56,880
ultimate dream is that the speech to text is powering like all of these Jarvis style pop-ups

412
00:38:56,880 --> 00:39:01,040
just automatically drive and do all the silly things for the expert while they can just focus

413
00:39:01,040 --> 00:39:05,840
on having the human conversation the thing my robots will never be good at probably but

414
00:39:06,800 --> 00:39:12,400
at what are you using for summarization that's why I say it's an it's another internal model that

415
00:39:12,400 --> 00:39:17,920
just pulls out these three call drivers but we did over six months on an extractive summary thing

416
00:39:19,040 --> 00:39:22,640
the thing is is that it's very hard to get it to be business actionable it would come up with

417
00:39:22,640 --> 00:39:30,240
stuff like I'd be like I really like my phone plan I want a new phone you know like and that's not

418
00:39:30,240 --> 00:39:35,120
a good summary because a good summary for that is upgrade call but yeah one thing this kind of

419
00:39:35,120 --> 00:39:42,560
interesting is a lot of maybe it's not interesting maybe it's just expected but you know you haven't

420
00:39:42,560 --> 00:39:50,560
mentioned the terms but a lot of what you're experiencing connects to a theme that we've

421
00:39:50,560 --> 00:39:57,440
been exploring on the podcast over the past few weeks a couple months data center AI like you

422
00:39:57,440 --> 00:40:02,240
started with this heavy focus on models but a lot of the things that you ran into require that you

423
00:40:02,240 --> 00:40:09,280
refine the data refine the data and is that a does that term resonate for you or is that something

424
00:40:09,280 --> 00:40:14,640
that you've you've looked at at all yeah yeah I I would say that I know that there's like a

425
00:40:14,640 --> 00:40:21,120
official like data driven AI movement and I would say I think for largely an alignment right like

426
00:40:21,120 --> 00:40:27,760
like don't don't go creating AI for the sake of doing a stunt and and make sure that what you're

427
00:40:27,760 --> 00:40:35,520
doing actually delivers value and so yeah I do think it resonates can you can you talk a little

428
00:40:35,520 --> 00:40:44,160
bit more about the the ML ops aspect of keeping all these models up and running it in production

429
00:40:44,160 --> 00:40:50,320
like what kind of platforms and tooling have you built out to support all this yeah so our team

430
00:40:50,320 --> 00:40:56,240
is like so focused on product development that we honestly have not done our due diligence in

431
00:40:56,240 --> 00:41:02,400
many ways for model maintenance and so right now we have like we do have model audits that will happen

432
00:41:02,400 --> 00:41:11,520
so we have some lamb does that automatically put a percentage of conversations into AWS ground truth

433
00:41:11,520 --> 00:41:16,320
it for our data annotation team to actually check on and then we have some reporting that can

434
00:41:16,320 --> 00:41:22,640
be done around that but for the most part we haven't seen significant enough data drift to

435
00:41:22,640 --> 00:41:28,400
to touch our models except for when a team mobile and sprint merge together because all of a sudden

436
00:41:28,400 --> 00:41:32,160
sprint is no longer a competitor and people are talking about sprint they're talking about us

437
00:41:32,160 --> 00:41:35,760
and so that required a that was significant enough data drift for us to retrain everything

438
00:41:37,200 --> 00:41:41,120
but for the most part we are just like it works until somebody tells us otherwise

439
00:41:41,120 --> 00:41:53,040
which which is not the best strategy and otherwise it sounds like you're fairly invested in AWS's

440
00:41:53,040 --> 00:41:59,680
various offerings for from a tech perspective although you did say you you kind of built your own

441
00:41:59,680 --> 00:42:04,880
world drone Kubernetes cluster that you just happen to be hosting on AWS yeah yeah and so I would

442
00:42:04,880 --> 00:42:11,520
say we are interested in AWS's offerings only in their bare components so we are rarely a consumer

443
00:42:11,520 --> 00:42:17,360
of like a cognitive services service like we our team doesn't really buy those we mostly build

444
00:42:17,360 --> 00:42:23,840
our own models and we have them end up deployed on AWS but though we recently moved to Google and

445
00:42:23,840 --> 00:42:30,000
in my spare time when I'm doing side projects I use GCP for most of my staff and so hopefully we

446
00:42:30,000 --> 00:42:34,240
will have a tight-knur partnership with Google in the future and maybe I'll be talking about TPUs

447
00:42:34,240 --> 00:42:44,720
instead of infrared chips but we'll see and do you have opinions on either of their like data

448
00:42:44,720 --> 00:42:53,760
science workbench you know environments Sage maker vertex so Sage maker I feel like it works

449
00:42:53,760 --> 00:43:00,080
like it's fine I feel like I what I really like about working in GCP is they separate

450
00:43:00,080 --> 00:43:06,080
compute and storage completely all the time and so you always have to really think about your

451
00:43:06,080 --> 00:43:11,760
compute and your storage I also like that it's like a push architecture so you're always waiting

452
00:43:11,760 --> 00:43:18,240
for pushes instead of other things so that's why I like doing software but as far as like the

453
00:43:18,240 --> 00:43:23,120
different platforms for individual development I feel like it's all mostly fine like it's all

454
00:43:23,120 --> 00:43:27,520
it's all kind of apples to apples we end up having to build so many custom components on top of

455
00:43:27,520 --> 00:43:34,240
whatever we're doing it ends up the same we've done trials with Azure Databricks too so like

456
00:43:34,240 --> 00:43:39,280
Databricks through Microsoft Azure and we're like we already have a AWS fill so I guess we'll

457
00:43:39,280 --> 00:43:48,080
just keep doing that one for now but awesome awesome any thoughts on kind of you know future directions

458
00:43:48,080 --> 00:43:57,200
what's next on your roadmap yeah yeah so there's definitely the rolling out speech to everybody

459
00:43:57,200 --> 00:44:02,960
making sure that it works great for all of our customers we also have some interesting stuff

460
00:44:02,960 --> 00:44:08,240
potentially coming up with legal you know on the phone people always redo terms and conditions

461
00:44:08,240 --> 00:44:15,280
and you have to accept when when you are audited to show to the auditors that that happened you have

462
00:44:15,280 --> 00:44:21,360
to like pull conversations and show them like audio recordings the thing why can't we just build a

463
00:44:21,360 --> 00:44:27,200
dashboard that does a cosine similarity between terms and conditions and what was actually said I

464
00:44:27,200 --> 00:44:31,280
guess they read them and then they agreed so that's one of our really quick wins that's coming up

465
00:44:31,280 --> 00:44:36,080
that I'm I'm pretty excited about but for the most part everything that we do is trying to move

466
00:44:36,080 --> 00:44:42,400
to this dream of the autonomous desktop so like I said so our care experts can sit back chat

467
00:44:42,400 --> 00:44:47,440
have the human interaction while we are taking notes for them opening the right apps at the right

468
00:44:47,440 --> 00:44:54,240
time and so what I'm hoping is we are standing up a very robust click tracking infrastructure for

469
00:44:54,240 --> 00:44:59,840
our care desktop and I'm excited to predict clicks right like to open windows at the right time

470
00:44:59,840 --> 00:45:03,920
do things of that nature that's outside of the natural language intent stuff that we've been

471
00:45:03,920 --> 00:45:09,840
focused on we also are doing a bunch of stuff in what we call the virtual retail space lots of

472
00:45:09,840 --> 00:45:15,200
people went to buy phones and don't want to go into a store so we're like how can we use AI to help

473
00:45:15,200 --> 00:45:22,000
sales people sell and that's like helping create positioning statements helping them write fit

474
00:45:22,000 --> 00:45:27,360
accessories to a phone that this particular customer is trying to buy so that that recommendation

475
00:45:27,360 --> 00:45:32,080
space we've also just began to dip our toes into that's very exciting and then measuring the

476
00:45:32,080 --> 00:45:35,840
impact of recommendation systems is a whole separate bag that I'm excited to unpack

477
00:45:37,040 --> 00:45:43,280
nice nice well Heather thanks so much for taking the time to share a bit about what you're up to

478
00:45:43,280 --> 00:45:50,880
very cool very cool anecdotes and and lots of interesting lessons learned in there yeah thank you

479
00:45:50,880 --> 00:46:20,480
it was a nice chat

