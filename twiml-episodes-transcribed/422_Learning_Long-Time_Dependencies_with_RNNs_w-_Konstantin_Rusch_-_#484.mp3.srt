1
00:00:00,000 --> 00:00:16,960
All right, everyone. I am here with Constantine Rush. Constantine is a PhD student at ETH Zurich.

2
00:00:16,960 --> 00:00:20,640
Constantine, welcome to the podcast. Thanks for having me here, Sam.

3
00:00:21,440 --> 00:00:27,040
Hey, I'm really looking forward to digging into our conversation, which will focus on a couple of

4
00:00:27,040 --> 00:00:37,920
your recent papers, both of them focused on RNNs. But before we jump into that, I'd love to have

5
00:00:37,920 --> 00:00:42,560
you take a few minutes and kind of share your background. How did you get started working in

6
00:00:42,560 --> 00:00:48,720
machine learning? Yes, absolutely. So actually, my background is mathematics. So I did my

7
00:00:48,720 --> 00:00:54,080
bachelor's in pure mathematics in Germany at the University of Bonn, which is a kind of nice place

8
00:00:54,080 --> 00:00:59,120
for mathematics in Germany. Then I did applied mathematics for my masters in the UK.

9
00:01:00,000 --> 00:01:05,280
And now I'm at ETH. I'm actually affiliated with the mathematics department here.

10
00:01:05,280 --> 00:01:11,200
And I'm doing my PhD in applied mathematics. And I'm focusing mostly on machine learning,

11
00:01:11,200 --> 00:01:16,640
of course. And also not only on classical machine learning, but also on machine learning for

12
00:01:16,640 --> 00:01:23,920
scientific computing, you know, to solve physical systems and so on. And yeah, so how did I get

13
00:01:23,920 --> 00:01:28,640
interested in machine learning? I think actually two years ago, I didn't even really know what

14
00:01:28,640 --> 00:01:35,280
machine learning was. So I really was interested, you know, in this classical applied mathematics,

15
00:01:35,280 --> 00:01:39,360
like doing numerics, solving partial differential equation, equations,

16
00:01:39,360 --> 00:01:44,560
domestic differential equations, and so on. And I even worked for almost three years for the

17
00:01:44,560 --> 00:01:50,800
German aerospace center. And there I focused basically as a student researcher, and there I basically

18
00:01:50,800 --> 00:01:58,000
focused on this classical scientific computing problems, you know, as I said, solving PDE,

19
00:01:58,000 --> 00:02:03,280
and so on, and geometric problems. And then during my time in the UK, when I did my masters in

20
00:02:03,280 --> 00:02:08,160
applied mathematics, I did basically two really nice courses. The first one was about dynamical

21
00:02:08,160 --> 00:02:15,920
systems, so nonlinear dynamics and chaos. And the second one was computational cognitive neuroscience,

22
00:02:15,920 --> 00:02:21,680
which was super awesome. And there was actually some kind of connection because in this computational

23
00:02:21,680 --> 00:02:29,600
cognitive neuroscience course, we took a look at, you know, mathematical models from neurobiology.

24
00:02:29,600 --> 00:02:34,960
So for instance, at the Fitsunagumo model, maybe you heard of it, probably your audience had.

25
00:02:34,960 --> 00:02:43,200
So it's basically the mathematical model of the firing of the action potential of a single

26
00:02:43,200 --> 00:02:49,520
biological neuron. And you know, you can describe it more or less as a so-called relaxation oscillator,

27
00:02:49,520 --> 00:02:55,520
and you can do all this kind of mathematical theory around that. And it was just super interesting.

28
00:02:55,520 --> 00:03:00,800
And so far, it had nothing to do with classical machine learning, you know, like classifying

29
00:03:00,800 --> 00:03:07,920
endless digits and so on. But then I started a project and I was searching for some literature

30
00:03:07,920 --> 00:03:13,760
about this connection between neurobiology and dynamical systems. And then I found a lot of

31
00:03:13,760 --> 00:03:21,040
very recent papers about dynamical systems theory in actual classical machine learning,

32
00:03:21,040 --> 00:03:27,600
meaning, for instance, using dynamical systems to construct architectures, deep learning architectures,

33
00:03:27,600 --> 00:03:33,280
RNNs or feed forward neural networks and so on. And I was directly hooked and I really liked

34
00:03:33,280 --> 00:03:39,680
these papers. And then yeah, I just started to read more, read more. I had my own ideas. I tried a

35
00:03:39,680 --> 00:03:47,600
bit and here. So that's basically my background. Awesome. Awesome. We should maybe kind of establish

36
00:03:47,600 --> 00:03:52,800
the definitions. When you say dynamical systems, what exactly does that entail?

37
00:03:52,800 --> 00:04:02,720
Yeah. So it's basically systems, which are time dependent. So basically you have two kind of

38
00:04:02,720 --> 00:04:10,720
systems, a continuous time systems and discrete time systems. And mostly in physics and biology,

39
00:04:10,720 --> 00:04:17,600
we use continuous time systems where you know, your time parameter is actually an element of some

40
00:04:17,600 --> 00:04:25,120
interval 0 to capital T, for instance. And you look at so-called ordinary differential equations

41
00:04:25,120 --> 00:04:33,440
or systems of ordinary differential equations. And I think they're quite famous. I mean,

42
00:04:33,440 --> 00:04:40,640
they're also used already directly machine learning like this neural ODE stuff and so on.

43
00:04:40,640 --> 00:04:48,960
And then there's discrete time dynamical systems where your time is this is more like an

44
00:04:48,960 --> 00:04:58,080
iterator. It's not continuous anymore. And so you don't have an ordinary differential equation

45
00:04:58,080 --> 00:05:05,200
anymore, but you just have some recurrent update of your so-called hidden state. So of the one

46
00:05:05,200 --> 00:05:15,280
you're propagating forward and time basically. Cool. So you took these classes when you were

47
00:05:15,280 --> 00:05:22,400
working on your masters in the UK and your current work is focused on the same field. Did you

48
00:05:23,040 --> 00:05:31,360
come to the PhD with these ideas, with ideas for applying dynamical systems and oscillators to

49
00:05:31,360 --> 00:05:36,960
RNNs or is that something that you developed more recently? No, actually I already had this

50
00:05:36,960 --> 00:05:44,240
kind of ideas before I came to ETH. But I started more or less, I had to pause them for a bit

51
00:05:44,240 --> 00:05:54,480
because I came here and I first focused on some more mathematics based papers and problems. So we

52
00:05:54,480 --> 00:06:03,600
did some sampling related kind of problems which are only applied in scientific computing and not

53
00:06:03,600 --> 00:06:10,000
in classical machine learning. But it was all the time curious and I tried so many things in my

54
00:06:10,000 --> 00:06:15,760
free time on the weekend and I had really good results. And so I talked to my supervisor,

55
00:06:15,760 --> 00:06:25,520
C.D. Mischra, and we decided to go a bit deeper and really try to figure out what is the reason

56
00:06:25,520 --> 00:06:35,440
for these good results and what is it modeling at all and so on. Great. So let's start talking

57
00:06:35,440 --> 00:06:42,800
about the motivation for the paper. What's the broad problem that you are trying to solve here?

58
00:06:42,800 --> 00:06:50,400
Okay, here. So the motivation was that for recurrent neural networks, so all we do is we

59
00:06:51,120 --> 00:06:58,560
suggest a new RNN architecture, recurrent neural network architecture, and the big problem for

60
00:06:58,560 --> 00:07:06,880
recurrent models RNNs is that it's quite hard to learn very long time dependencies and that means

61
00:07:06,880 --> 00:07:15,360
okay, for RNNs, if you apply them to data, it's always sequential data. So there's some kind

62
00:07:15,360 --> 00:07:23,120
of sequential internal dependencies in your underlying data. And for instance, if you have

63
00:07:23,840 --> 00:07:29,360
internal dependencies for long range, so for instance at the beginning of your sequence,

64
00:07:29,360 --> 00:07:36,080
you have important information which is needed at the very end in order to classify the sequence

65
00:07:36,080 --> 00:07:43,440
for instance correctly, that we would call a long time dependency. And now it also depends on how

66
00:07:43,440 --> 00:07:50,160
long this dependency is of course, right? So RNNs are typically very, very good for sequence length

67
00:07:50,160 --> 00:07:57,360
of 100, 200. If you go to a thousand, it's already super hard and not mentioning doing 10,000

68
00:07:57,360 --> 00:08:03,040
or whatever. And the problem behind that is basically the way you train your recurrent neural

69
00:08:03,040 --> 00:08:09,520
network. So what you do is you use backpropagation through time where you basically unfold your RNN

70
00:08:09,520 --> 00:08:19,520
in time. And while you backpropagate the errors back in time more or less. And if you would write

71
00:08:19,520 --> 00:08:26,800
down the full gradient you're interested in computing, then you would see that there is one term,

72
00:08:26,800 --> 00:08:34,800
which is basically just the influence of a hidden state from a previous time, let's say at time k,

73
00:08:35,680 --> 00:08:42,160
on some hidden state at a later time, let's say a hidden state n. And so it's basically partial

74
00:08:42,160 --> 00:08:49,760
derivative of your hidden state with respect to this hidden state of the previous time. And now if

75
00:08:49,760 --> 00:08:57,280
you look at this term and you basically apply the chain will all over again, you will have a long

76
00:08:57,280 --> 00:09:05,200
product. And this product has basically n minus k plus one factors in it. And so if you imagine

77
00:09:05,200 --> 00:09:11,760
if on average these these factors are a bit less than one, let's say they're 0.9, then it's

78
00:09:11,760 --> 00:09:17,280
basically just 0.9 to the power of n minus k plus one. And if n minus k is very long, very big,

79
00:09:17,280 --> 00:09:23,840
then you just go to 0 exponentially fast or you go to infinity if it's a bit bigger than one

80
00:09:23,840 --> 00:09:29,040
on average, right? If it's like one dot one on average, then you explode. And so this is called

81
00:09:29,040 --> 00:09:37,920
the Wenishing and exploding gradient problem because exactly of that. And it may sound, well,

82
00:09:37,920 --> 00:09:44,560
okay, then just stabilize this gradient in some way. But it's actually not that easy. So of course,

83
00:09:44,560 --> 00:09:51,440
if for instance, if you consider your recurrent model as just the identity, right, then your

84
00:09:51,440 --> 00:09:57,920
gradient will just be the identity matrix. Great. Now we're super stable, right? But we learn

85
00:09:57,920 --> 00:10:02,720
nothing basically. And so that's the point. So basically it's twofolded. On the one hand side,

86
00:10:02,720 --> 00:10:08,240
you want to have gradient stability. So the gradient doesn't blow up or Wenishing. But at the same

87
00:10:08,240 --> 00:10:15,120
time, you also want to be expressive that you can learn actual interesting data, which the identity

88
00:10:15,120 --> 00:10:22,240
map won't be helpful for, right? So we have some success with tech architectures like LSTM's

89
00:10:22,240 --> 00:10:28,960
and GRUs and addressing this. Absolutely. Absolutely. So LSTMs, what they do, they use some kind of

90
00:10:28,960 --> 00:10:34,960
gating mechanism. And if you carefully write down the gradient there, you will see that you

91
00:10:34,960 --> 00:10:41,040
actually have a chance to mitigate the Wenishing gradient problem. However, the exploding

92
00:10:42,000 --> 00:10:48,160
gradient will might still happen. So the gradient can still explode for LSTMs and for GRUs.

93
00:10:48,160 --> 00:10:54,400
And there's a lot of papers where they have used LSTMs for long-range long-term dependencies.

94
00:10:54,400 --> 00:11:01,200
And they kind of start to fail at a few hundred like 1,000. So if the sequence has like a length

95
00:11:01,200 --> 00:11:07,440
of 1,000, it's kind of tricky for the LSTM to learn that. But LSTMs are super expressive,

96
00:11:07,440 --> 00:11:13,680
so that I can say already from my experience. And I think that's one of the reasons why LSTMs

97
00:11:13,680 --> 00:11:20,800
are still the most used are in an architecture. Maybe, I don't know, maybe the most used architecture

98
00:11:20,800 --> 00:11:31,840
at all, I don't know. They are quite popular. Absolutely. And so, so you've got this, this exploding

99
00:11:31,840 --> 00:11:38,000
vanishing gradient problem with long-term sequences. You know, there are things like LSTMs,

100
00:11:38,000 --> 00:11:48,240
but they're not perfect. And so your approach is inspired in some ways by the year, you know,

101
00:11:48,240 --> 00:11:52,720
what you learned about the neurons and the oscillators and the neurons. Tell us, you know,

102
00:11:52,720 --> 00:11:59,600
how it connects to what you observe there. Yeah, absolutely. So our approach was really

103
00:11:59,600 --> 00:12:04,720
inspired by neurobiology. As I said before, if you take a look at the dynamical systems,

104
00:12:04,720 --> 00:12:09,840
which model this kind of firing of the action potential, you have this kind of oscillatory

105
00:12:09,840 --> 00:12:14,480
behavior, right? It's actually a relaxation oscillator where you basically accumulate your

106
00:12:14,480 --> 00:12:20,080
stimulus. And then if you surpass a certain threshold, you just fire and then you relax.

107
00:12:20,080 --> 00:12:25,200
And so you have this periodically, this oscillatory behavior more or less. And you see also this kind

108
00:12:25,200 --> 00:12:32,240
of oscillatory behavior. If you, for instance, consider full parts of the brain, for instance,

109
00:12:32,240 --> 00:12:38,720
the hippocampus, they're people call it, I mean, I'm not a neurobiologist, I can just tell you

110
00:12:38,720 --> 00:12:44,640
what I read. And they call it also hippocampus oscillations. And you can, they're actually very

111
00:12:44,640 --> 00:12:50,480
beautiful papers where they measure these kind of oscillations in vivo and in vitro. And yeah,

112
00:12:50,480 --> 00:12:57,920
that was basically the motivation. So something which makes kind of sense in neurobiology,

113
00:12:57,920 --> 00:13:04,960
but we don't try to exactly mimic this kind of behavior, right? So that's just the abstract

114
00:13:04,960 --> 00:13:11,680
essence. And now we just forget about all this complicated underlying biological behavior.

115
00:13:11,680 --> 00:13:16,480
And we just take the message, okay, using oscillators might be a good idea.

116
00:13:16,480 --> 00:13:23,520
Yeah. Yeah. And that's what we did. And I think if you think about oscillators, I mean,

117
00:13:23,520 --> 00:13:29,520
this kind of harmonic oscillators where you have some linear combination of sine and cosine

118
00:13:29,520 --> 00:13:36,320
as a solution, it's super stable, right? If your amplitude of the oscillations doesn't blow up

119
00:13:36,320 --> 00:13:41,200
or vanish, it's it's perfectly stable. And also, I mean, if you take the gradient, you again

120
00:13:41,200 --> 00:13:45,840
have some kind of, if you take the credit of sine, you have cosine and vice versa, right?

121
00:13:45,840 --> 00:13:51,680
And so even the gradient is very nice behaved or you would expect that it's very nice behaved.

122
00:13:52,400 --> 00:13:58,080
And coming from this neurobiological background, we were also kind of hoping that, you know,

123
00:13:58,080 --> 00:14:03,920
it's the kind of expressive because if that's something you can find more or less in nature

124
00:14:03,920 --> 00:14:11,120
and in neurobiology, then why not try it, right? And so we came up with this coupled oscillatory

125
00:14:11,120 --> 00:14:19,200
RNN architecture, which is basically a system of ordinary of second order ordinary differential

126
00:14:19,200 --> 00:14:27,040
equations. So second order ODE's modeling these kind of coupled oscillators, where we also have

127
00:14:27,040 --> 00:14:34,080
two to additional controlling terms in there. So as I said before, if your amplitude goes to infinity

128
00:14:34,080 --> 00:14:41,120
or goes to zero, then we basically got nothing, right? So we have to make sure that they are also

129
00:14:41,120 --> 00:14:47,040
kind of nice behaved. And we can do that actually by adding some some controlling terms in there,

130
00:14:47,040 --> 00:14:53,280
controlling the damping. So that means like over time, how much do you, do you damp your oscillations?

131
00:14:53,280 --> 00:15:01,440
And and a controlling parameter for the frequency of the system. And yeah, in the end, it's it's

132
00:15:01,440 --> 00:15:11,520
really just just a damped controlled and coupled oscillator. And we discretize that with a so-called

133
00:15:11,520 --> 00:15:16,880
implicit explicit discretization scheme and IMAX scheme, just to make sure because we are not

134
00:15:16,880 --> 00:15:22,240
working with the continuous formulation of the system, but with the discrete time formulation of

135
00:15:22,240 --> 00:15:28,400
the system. And to make sure that we have all these structure preserving behavior in our discrete

136
00:15:28,400 --> 00:15:36,160
time system now, we use some kind of well-behaved numerical discretization scheme for ODE's,

137
00:15:36,160 --> 00:15:41,680
which is just this IMAX scheme. And then we come up with this discretized ODE, which we interpret

138
00:15:41,680 --> 00:15:49,680
as an RNN. And yeah. When you describe the oscillators as coupled, what exactly does that mean?

139
00:15:49,680 --> 00:15:56,800
Okay. Where are they coupled? Yeah. So the system has many dimensions, right? So basically,

140
00:15:56,800 --> 00:16:06,640
each dimension corresponds to one neuron in the recurrent titan state of the RNN. And so typically,

141
00:16:06,640 --> 00:16:13,200
for for normal problems, you use like 128 dimensions or 256 dimensions, which would mean you have in

142
00:16:13,200 --> 00:16:20,640
your, we are only talking for one layer RNNs right now, right? And so you typically have 128 hidden

143
00:16:20,640 --> 00:16:30,320
neurons or 256 hidden neurons. And coupled means that the information from one dimension can go

144
00:16:30,320 --> 00:16:38,000
also in the other dimension. So that means basically the hidden weight matrix we have is not

145
00:16:38,000 --> 00:16:46,240
sparse or has any any weird structure. It's it's it's a simple dance matrix visit.

146
00:16:48,480 --> 00:17:01,280
And you say you you have this system of of coupled oscillators. You write those as ODE's. And then you

147
00:17:01,280 --> 00:17:08,880
express those as an RNN. How does that last stage work? Is that a kind of straightforward

148
00:17:10,000 --> 00:17:16,240
expression of the ODE as RNN or is there, you know, are there work or tricks that you need to do to

149
00:17:16,240 --> 00:17:22,000
to be able to do that? Yeah, that's actually a really good question. So no, it's it's not

150
00:17:23,040 --> 00:17:28,720
so at least to me, it was not directly totally clear how to interpret that as an RNN. And there are

151
00:17:28,720 --> 00:17:34,400
different choices actually to do because for instance, you have something like a time step for

152
00:17:34,400 --> 00:17:40,320
your discretization scheme, right, some some kind of DT, which is just a small, small parameter.

153
00:17:40,960 --> 00:17:47,120
And the question is how to treat this parameter, for instance, is it a hyperparameter of your

154
00:17:47,120 --> 00:17:52,960
RNN architecture? Do you want to train it? You if you want to train it, you have to constrain it in

155
00:17:52,960 --> 00:17:57,680
some way because it shouldn't be too big and it shouldn't be negative because you don't have negative

156
00:17:57,680 --> 00:18:02,240
time, right? You want to be have something bigger than zero, but you also don't want to be too big.

157
00:18:02,240 --> 00:18:08,800
So there are actually many, many questions. And well, we went for the easiest first answer,

158
00:18:08,800 --> 00:18:13,440
I guess, we just treat them as a hyperparameter. And also our two controlling parameters I just

159
00:18:13,440 --> 00:18:19,680
mentioned are two hyperparameters. So in the end, we have an RNN with three hyperparameters,

160
00:18:21,440 --> 00:18:26,960
which you do not really need. So you can actually treat them as trainable parameters too,

161
00:18:26,960 --> 00:18:31,600
but you have to constrain them in certain ranges. As I said before, for instance, by using

162
00:18:31,600 --> 00:18:36,640
sigmodel activation function, or really you, because also this controlling parameters should be

163
00:18:36,640 --> 00:18:40,320
just non-negative or positive. Let's say they should be positive.

164
00:18:43,520 --> 00:18:53,680
So you have your system expresses an RNN and then what next? It sounds like maybe the next step

165
00:18:53,680 --> 00:18:59,120
was doing some benchmarking against existing approaches for RNNs.

166
00:18:59,680 --> 00:19:09,280
Yeah, not directly though. So first, all I said so far was some kind of intuition that it might

167
00:19:09,280 --> 00:19:16,000
be stable. Also, the credience might be well-behaved, but the key feature of our, or the key goal of

168
00:19:16,000 --> 00:19:22,480
our paper is actually to show, to mathematically prove now that this gradient is actually stable.

169
00:19:22,480 --> 00:19:28,160
So we have two to main propositions. You can also call them theorems if you want to,

170
00:19:29,680 --> 00:19:35,520
which we actually fully prove. And the first one is basically showing that the gradient can't

171
00:19:35,520 --> 00:19:42,480
explode, giving some assumptions. And now many people, I guess in the audience, will be like,

172
00:19:42,480 --> 00:19:48,640
okay, assumptions. Now it's not going to be met. And you know, everything was garbage. But in this

173
00:19:48,640 --> 00:19:53,360
case, we can actually check these assumptions that they are very mild. So we actually, we check

174
00:19:53,360 --> 00:19:58,480
them for each experiment we did. And they were always satisfied during the whole training procedure.

175
00:19:58,480 --> 00:20:06,000
So it's nothing crazy. It just means that your weights of your RNN are not too big. I mean,

176
00:20:06,000 --> 00:20:14,080
in the end, if they go to infinity, then what's the point of using this RNN? So that's a reasonable

177
00:20:14,080 --> 00:20:20,320
assumption. And with this assumption, we can actually show that the gradient is bounded from above.

178
00:20:21,200 --> 00:20:26,560
That's the first theorem. And the second theorem was maybe a bit harder even to show.

179
00:20:26,560 --> 00:20:36,480
It's some kind of asymptotic expansion of this gradient, which shows that we have some

180
00:20:36,480 --> 00:20:43,760
terms, some sort of asymptotic expansion means that we have some expansion in some orders.

181
00:20:43,760 --> 00:20:49,760
And we choose, in this case, to use the time step dt, which is quite small. And we had some

182
00:20:51,120 --> 00:20:57,440
expansion of orders in this small time step dt. And we can see that the leading order of this

183
00:20:57,440 --> 00:21:02,320
expansion is of order, I think time step to the power of three over two, something like this.

184
00:21:02,320 --> 00:21:09,440
And this is actually independent of your sequence length. So it doesn't matter anymore if you have

185
00:21:09,440 --> 00:21:16,080
a sequence of length 10 or of length 10,000. This leading order term will always be there and

186
00:21:16,080 --> 00:21:20,000
will always be of the same order. So that means that the wenshin gradient problem is actually

187
00:21:20,000 --> 00:21:26,480
mitigated and for all sequence lengths, basically. And so to recap that part,

188
00:21:26,480 --> 00:21:35,440
essentially, because you've got this specific form of RNN based on the coupled oscillators,

189
00:21:35,440 --> 00:21:47,120
you're then able to produce some closed form bounds on the, is it the convergence of the gradient?

190
00:21:47,120 --> 00:21:57,360
And that requires some assumptions that you make. And these are assumptions, they're not constraints,

191
00:21:57,360 --> 00:22:04,000
they're just assumptions. And by observation, you've not seen them being violated in kind of normal

192
00:22:04,000 --> 00:22:16,000
use of the RNN. Exactly. Yeah, absolutely. Yeah. And maybe one more point because it was actually

193
00:22:16,000 --> 00:22:23,760
quite interesting. So these two propositions, we proved in the full discrete case. So for the real

194
00:22:23,760 --> 00:22:32,800
RNN, because if you go discrete, some wild things can happen. So it's always a bit tougher to prove

195
00:22:32,800 --> 00:22:38,960
the discrete tastes and not only the continuous case. And so that's actually the case of the RNN.

196
00:22:38,960 --> 00:22:46,080
And interestingly, we also did the continuous case. And for that, we only can

197
00:22:47,120 --> 00:22:52,800
bound the gradient from above. So we can mitigate the exploding gradient problem. But because we

198
00:22:52,800 --> 00:22:56,800
had this expansion in this time step, right? And in the continuous case, you go with your time step

199
00:22:56,800 --> 00:23:03,600
to zero, because you want to be continuous in some way. We don't have this kind of feature anymore.

200
00:23:03,600 --> 00:23:09,440
So that's really the mitigation of the Wenzhen gradient problem is really a feature of the

201
00:23:09,440 --> 00:23:17,280
discretization and not of the oscillatory dynamics anymore, which I thought was quite interesting.

202
00:23:21,120 --> 00:23:28,960
And so I'm so very curious how this, what kind of result you saw and how it compares from a

203
00:23:28,960 --> 00:23:34,560
performance perspective? Yeah, that's yeah, that's even more interesting. That's true.

204
00:23:35,520 --> 00:23:42,480
Because if you have a good theory, okay, that's nice, but it also should work in real life, right?

205
00:23:42,480 --> 00:23:46,800
We have things like LSTMs that we know how to use and they work pretty well.

206
00:23:46,800 --> 00:23:57,600
And so the question is, we've got, like you said, you've got theories on the bounds of the gradients here,

207
00:23:57,600 --> 00:24:04,960
but is it useful? Yeah, absolutely. So we applied it to many long-term dependency benchmarks.

208
00:24:04,960 --> 00:24:11,120
So the typical benchmarks, for instance, the first one, which I really liked was, and it's already

209
00:24:11,120 --> 00:24:17,280
quite old, so it's the adding problem. And it was first proposed in the original LSTM paper, actually,

210
00:24:17,280 --> 00:24:27,200
and it's just a synthetic sequential, no, it's actually regression. And the idea here is that you

211
00:24:27,200 --> 00:24:34,000
have sequences of arbitrary length, and they're two-dimensional, and the first dimension are just

212
00:24:34,000 --> 00:24:40,480
uniform random numbers. And the second dimension are all zeros, except for two positions,

213
00:24:40,480 --> 00:24:47,280
they're set to one, and these positions are chosen randomly, also uniform randomly,

214
00:24:47,280 --> 00:24:55,200
and they are chosen in both half of the sequence. And now the goal of the RNN,

215
00:24:55,200 --> 00:25:03,440
especially to the output should be the two random uniform numbers of the first dimensions

216
00:25:03,440 --> 00:25:08,800
at the position indicated by the ones in the second dimension. And so it's just adding them

217
00:25:08,800 --> 00:25:14,960
together. So they call it adding, and the nice part here is that we can, you know, use sequence

218
00:25:14,960 --> 00:25:22,080
length of 100, we can use 1,000, and we can make it harder, harder, harder. And actually, we tried

219
00:25:22,080 --> 00:25:29,600
until a sequence length of 5,000. I think, actually, I've not seen that before in another paper,

220
00:25:29,600 --> 00:25:36,080
so normally what you do is you try 50, 100, 200, 500, something like this. Some people go up to 1,000,

221
00:25:36,080 --> 00:25:42,800
and you can see that, for instance, LSTMs, they already fail at 500, 750, so they are not

222
00:25:42,800 --> 00:25:50,080
able to converge anymore, so they can't effectively learn this kind of problem. And for corn,

223
00:25:50,080 --> 00:25:54,800
we actually had that, even in the case for 5,000, you get more or less direct convergence,

224
00:25:54,800 --> 00:26:03,440
which is really nice, yeah. Yeah, I really like it. Any interesting observations in terms of

225
00:26:03,440 --> 00:26:16,480
computational complexity with this approach? Yes. Yeah, so efficiency, yeah, the question is

226
00:26:16,480 --> 00:26:23,600
in this task, actually, how long do you try, so how long do you let the RNN model learn, right?

227
00:26:23,600 --> 00:26:29,280
So if I'm saying LSTM fails for 500, it doesn't mean if you do maybe a million iterations,

228
00:26:29,280 --> 00:26:35,440
learning steps, it might converge, who knows, right? And the interesting part really was here

229
00:26:35,440 --> 00:26:43,600
that you need maybe, I don't know, in terms of hundreds, we needed maybe 10 of them, and we tried

230
00:26:43,600 --> 00:26:50,240
LSTMs at least for 600 hundreds, so for 60,000 learning steps, and it did not converge at all,

231
00:26:50,240 --> 00:26:56,240
and also the other important architectures, like this exponential RNN, which is basically some

232
00:26:56,240 --> 00:27:02,240
kind of unitary RNN, which comes from the idea that you want to constrain your hidden to hidden

233
00:27:02,880 --> 00:27:10,240
structure, and all these kind of famous architectures, they did not work so well, especially in the

234
00:27:10,240 --> 00:27:16,560
very, very long case, so that was actually quite nice. But of course, it's just a synthetic

235
00:27:17,600 --> 00:27:22,800
test, right? It's just a synthetic benchmark, so maybe no one cares about this in the end,

236
00:27:22,800 --> 00:27:28,480
so you have to try more real world data, of course, and maybe a second one, which was quite interesting,

237
00:27:28,480 --> 00:27:34,640
was something like, and that's not a long-term dependency task anymore, because what I said

238
00:27:34,640 --> 00:27:38,320
at the very first beginning, that okay, you want to have credit instability, you want to learn

239
00:27:38,320 --> 00:27:42,480
long-term dependencies, but you also want to be expressive, right? You want to learn complicated

240
00:27:42,480 --> 00:27:49,920
problems, and so we tried this task from, it's basically human action recognition, it's

241
00:27:49,920 --> 00:27:57,760
basically on a smartphone with the sensors measured six different activities, like standing up,

242
00:27:57,760 --> 00:28:05,280
walking, running, something like this, and the idea is that you have some time series measured

243
00:28:05,280 --> 00:28:12,160
from your sensors over time on your Samsung S3, I guess, was it? And so in the end, you want to

244
00:28:12,160 --> 00:28:20,080
classify, based on these action, action time series, what the people have done, and even there,

245
00:28:20,080 --> 00:28:26,720
we got extremely good results. I mean, it's not a huge benchmark, so it's hard to say it was

246
00:28:26,720 --> 00:28:31,360
outperforming everything, what is out there, but it was at least outperforming everything we were

247
00:28:31,360 --> 00:28:36,800
aware of, so all the papers that have done that, we were outperforming them, which was quite nice.

248
00:28:36,800 --> 00:28:43,680
Awesome, awesome. You're working on a follow-on paper to the corn paper. Can you tell us a little

249
00:28:43,680 --> 00:28:50,880
bit about that one? Yeah, so the follow-up is called Unicorn, but with a double N in the end,

250
00:28:50,880 --> 00:28:56,960
so it's really like a neural network, and it stands for undamped independent controlled

251
00:28:56,960 --> 00:29:04,720
oscillatory RNNs. It was kind of chosen, such that we have this kind of nice Unicorn name,

252
00:29:04,720 --> 00:29:12,960
you know, but that's, so the idea here is that instead of using coupled oscillators,

253
00:29:12,960 --> 00:29:19,840
we first uncoupled them. And so basically, if you have heard of this independent RNN,

254
00:29:20,480 --> 00:29:25,840
where they instead having a hidden to hidden weight matrix, which is just the full,

255
00:29:26,880 --> 00:29:31,520
hidden dimensions, time-hidden dimensions, dense matrix, you just have a vector,

256
00:29:31,520 --> 00:29:41,760
and you just multiply it. So then you don't have any interconnection between the different

257
00:29:41,760 --> 00:29:46,080
neurons anymore, between the different hidden neurons, or in terms of dynamical systems,

258
00:29:46,080 --> 00:29:51,680
you don't have any interconnections between the different dimensions. So in the end,

259
00:29:51,680 --> 00:29:57,760
what you do is you just solve for each dimension, you solve an independent system, you don't even

260
00:29:57,760 --> 00:30:03,200
have to do it on the same computer. So that was one of the points, so you can solve one dimension

261
00:30:03,200 --> 00:30:08,320
on this computer and another dimension on the next computer. And because of this kind of

262
00:30:08,320 --> 00:30:15,280
independency, and we are undamped, so we don't have some damping term in there anymore,

263
00:30:15,280 --> 00:30:23,120
we can show that this system is actually a Hamiltonian system. And if you discretize these

264
00:30:23,120 --> 00:30:29,440
kind of systems with some symplectic integrators from Hamiltonian mechanics, also people in

265
00:30:30,480 --> 00:30:35,040
molecular simulations are very interested in that because they model basically everything

266
00:30:35,040 --> 00:30:44,000
with Hamiltonian systems. And so the numerics to use here is so-called symplectic integrators,

267
00:30:44,000 --> 00:30:48,400
and you can show that you more or less end up if you integrate them with the discrete time

268
00:30:48,400 --> 00:30:53,040
Hamiltonian, which is quite close to your actually continuous time Hamiltonian. And the nice feature

269
00:30:53,040 --> 00:31:00,800
of Hamiltonian systems is that they are invertible in time. And that means now that, for instance,

270
00:31:01,760 --> 00:31:07,600
if you train it with backpropagation through time, as I said, right at first you propagate

271
00:31:07,600 --> 00:31:13,360
forward in time your input, and then you have at the end some output, and then for backpropagating,

272
00:31:13,360 --> 00:31:19,840
you propagate this error back in time. And for that, you have to store each hidden state at every

273
00:31:19,840 --> 00:31:26,960
time. And so you can backpropagate, but in this case, because it's invertible in time, you can

274
00:31:26,960 --> 00:31:32,640
just store the last hidden state, and reconstruct these hidden states based on the last hidden state,

275
00:31:32,640 --> 00:31:38,560
because it's perfectly invertible in time. And so you know, you kind of have the same memory

276
00:31:38,560 --> 00:31:43,920
efficiency, like in neural ODEs, with the nice feature that you don't have to go continuous,

277
00:31:43,920 --> 00:31:50,640
because for standard neural ODEs, it's only true if you're going with your time step to zero,

278
00:31:50,640 --> 00:31:56,000
which can be very expensive. And I mean, on a digital computer, you can't go basically to zero,

279
00:31:56,000 --> 00:32:01,680
but you always have some rounding errors and so on. And here we don't have any problem anymore.

280
00:32:01,680 --> 00:32:07,680
We can even use the time step of 0.1, and it's still perfectly invertible. And so yeah,

281
00:32:07,680 --> 00:32:13,280
it scales just perfectly. It's very memory efficient. And on top, because we have this kind of

282
00:32:13,280 --> 00:32:19,600
independencies, as I said before, we don't have to train it on the same computer. And that makes a

283
00:32:19,600 --> 00:32:25,360
lot of sense if you use it on GPUs directly, where you basically have an independent thread,

284
00:32:25,360 --> 00:32:29,920
or you don't even need a thread, so it can be even threads of different blocks, which cannot

285
00:32:29,920 --> 00:32:36,480
communicate with each other. And you can just train each independent dimension on an independent

286
00:32:36,480 --> 00:32:41,600
thread, and just in the end, add everything together. And this is extremely fast. So

287
00:32:42,880 --> 00:32:47,680
I actually did an experiment, and we have some some really efficient code on GitHub already.

288
00:32:47,680 --> 00:32:53,760
You can check it out if you're interested in it. It's directly implemented in CUDA. So it's,

289
00:32:53,760 --> 00:33:00,080
of course, it's using PyTorch, but it's some CUDA extension, where we really do this kind of

290
00:33:00,080 --> 00:33:07,760
independent threading. And maybe it's interesting to hear, but on my local GPU, I had a speed up of

291
00:33:08,800 --> 00:33:15,200
one epoch on sequential amnesty took me like half a day on the GPU using PyTorch for a

292
00:33:15,200 --> 00:33:21,360
three layer unicorn, for instance. And with this CUDA extension, by really using this kind of

293
00:33:21,360 --> 00:33:30,880
independency, I got speed up from half a day to half a minute. And so how is the

294
00:33:30,880 --> 00:33:41,440
expressivity of the RNN impacted by the independence? Yeah, good question. We did some benchmarks,

295
00:33:41,440 --> 00:33:46,880
so one of them, the problem for for expressivity is that you know, for credit and stability,

296
00:33:46,880 --> 00:33:51,840
you can formulate it mathematically. You can say, okay, it should be in this in this range,

297
00:33:51,840 --> 00:33:55,760
and it shouldn't blow up or go to zero. You can write it down. There's no problem with that,

298
00:33:55,760 --> 00:34:02,240
but for expressivity, you know, what's the mathematical formulation of expressivity? So I don't

299
00:34:02,240 --> 00:34:07,840
know any, I think what's maybe the closest to it is some kind of universality, right? If you can

300
00:34:07,840 --> 00:34:14,000
prove universality, that's at least something you would need. It's absolutely not sufficient,

301
00:34:14,000 --> 00:34:18,720
but it's at least necessary. So if you can show some kind of universality, that's that's nice.

302
00:34:19,840 --> 00:34:26,240
But besides this, I'm not aware of any expressivity mathematical results. And so what you do is you

303
00:34:26,240 --> 00:34:33,840
classify it more or less by empirical data. So you use some some benchmark, which people think

304
00:34:33,840 --> 00:34:39,280
requires high expressivity. And if it performs well on that, you say, okay, it has high

305
00:34:39,280 --> 00:34:45,440
expressivity. At least that was my impression. I'm, you know, happy to learn, maybe I was wrong,

306
00:34:45,440 --> 00:34:50,960
and there's a nice mathematical formula for that, but I can't come up with one and I'm not aware

307
00:34:50,960 --> 00:34:57,680
of any. And so we tried it on one benchmark, which is called IMDB, which is basically just

308
00:34:57,680 --> 00:35:03,840
classifying its sentiment analysis. So you classify movie reviews from this IMDB database,

309
00:35:03,840 --> 00:35:09,520
exactly. And that can be long, and that can also have long-term dependencies, right? I mean,

310
00:35:09,520 --> 00:35:16,400
the front of the reviews would be, I don't know, some kind of irony thing. You know, I really love

311
00:35:16,400 --> 00:35:21,840
this film. And then in the end, or this movie and in the end, you say, I love it because it sucks.

312
00:35:22,480 --> 00:35:27,680
So you have some kind of long-term dependencies, but you also have high expressivity because

313
00:35:27,680 --> 00:35:34,160
you do natural language processing. I mean, they're way better benchmarks for that, I guess, but it's

314
00:35:34,160 --> 00:35:39,440
at least a start. And on that, we got quite good results. So it outperformed corn actually,

315
00:35:39,440 --> 00:35:44,640
but we have to say we are using two layers for that. And for corn, we're only using one layer.

316
00:35:44,640 --> 00:35:49,040
So it could also be a layer thing. But for this independent unicorn, you can't use one layer,

317
00:35:49,040 --> 00:35:55,600
because then you really have no interdependencies. And you also struggle a lot. So this interdependencies,

318
00:35:55,600 --> 00:36:01,600
you still have them, but they come from the input to hidden matrix. And so you basically

319
00:36:02,240 --> 00:36:09,760
delay them by one layer. Do you know more or less what I mean? So you need at least two layers to

320
00:36:09,760 --> 00:36:19,040
have some kind of interdependencies between the hidden neurons. Okay, awesome. Where do you see this

321
00:36:19,040 --> 00:36:30,080
research going next? Yeah, so my personal big goal is to, you know, so far what we did was we have

322
00:36:30,080 --> 00:36:35,760
some kind of great instability, which is great. We can learn long-term dependencies, but there's

323
00:36:35,760 --> 00:36:44,320
a reason why people use LSTMs all the time, because they are just so expressive. And for instance,

324
00:36:44,320 --> 00:36:52,080
if you if you try something like this PTB pantry bank language modeling task, where for instance,

325
00:36:52,080 --> 00:37:00,640
you have some some Wall Street articles, I guess. And the the chart level would be that you read

326
00:37:00,640 --> 00:37:07,920
in the sequence of the words of the characters. And you want to predict the next character,

327
00:37:07,920 --> 00:37:13,520
basically, in the sequence. I mean, as humans, we are quite good in that, I guess. I mean, if I would

328
00:37:13,520 --> 00:37:20,320
say to see the and then would say, okay, next comes an S to see this, for instance. So for us,

329
00:37:20,320 --> 00:37:26,240
it's it's quite quite easy to do maybe, but for RNNs, it's quite hard, especially if they're not

330
00:37:26,240 --> 00:37:33,840
expressive. And if you're honest and you try, you know, all these more or less famous long-term

331
00:37:33,840 --> 00:37:42,000
dependency RNNs, they are not performing very well. So if you if you take a look at the metric they use,

332
00:37:44,400 --> 00:37:51,040
then it's all the time way worse than LSTMs. So X, RNNs are worse, our corners worse or

333
00:37:51,040 --> 00:37:58,240
unique corners worse. All these kind of nice nice RNNs, they suffer from limited expressivity. I

334
00:37:58,240 --> 00:38:03,840
just want to be so honest, right? So I'm not saying that corn is like the best thing out there.

335
00:38:03,840 --> 00:38:08,400
It's a silver bullet, use it all the time. No, there's a real use case for this. This is long-term

336
00:38:08,400 --> 00:38:15,440
dependencies, but if you do like high crazy expressivity tasks where you don't require long-term

337
00:38:15,440 --> 00:38:22,640
dependencies, then don't use it basically. And so the idea is that LSTMs are very strong in that,

338
00:38:22,640 --> 00:38:28,880
but they suffer from not being very good at learning long-term dependencies to have something,

339
00:38:28,880 --> 00:38:33,280
you know, which kind of bridges these two things, which kind of connects these two things. So

340
00:38:33,280 --> 00:38:40,640
something which has good can learn long-term dependencies very efficiently, but at the same time

341
00:38:40,640 --> 00:38:45,920
has high expressivity. And actually we are working on something like this. We are working with

342
00:38:45,920 --> 00:38:56,560
the group in Berkeley right now, and maybe in a few months, you know, we will publish. Yeah,

343
00:38:56,560 --> 00:39:02,160
yeah, then there will be some good news. I'm really motivated. I'm really thrilled about this

344
00:39:02,160 --> 00:39:09,600
project. It's working really good. And yeah, that would be my ultimate goal in this kind of RNN

345
00:39:09,600 --> 00:39:18,400
community. Oh, that's great. That's great. Well, Constantine, thanks so much for taking the time to

346
00:39:18,400 --> 00:39:23,680
share a bit about what you're working on. Yeah, no worries. It was a pleasure being here. My pleasure.

347
00:39:23,680 --> 00:39:39,840
Thank you. Bye-bye.

