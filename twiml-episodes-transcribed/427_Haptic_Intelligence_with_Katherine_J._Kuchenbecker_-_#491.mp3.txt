All right, everyone. I am here with Katherine Krukenbecker.
Katherine is a director at the Max Plank Institute for Intelligent Systems.
Katherine, welcome to the Twoma AI podcast.
Thanks so much. I'm really happy to be here.
I'm really looking forward to digging into our conversation and your work, which is at the
intersection of robotics and machine learning, and I'd love to have you start us off with
a little bit of background. How did you come to work in the field?
Yeah, so I grew up in California. Actually, although my name looks German and I live in Germany,
I'm actually American, and I was raised by a research psychologist. My mom was a now retired
professor and a surgeon, and I was always fascinated by how things work, and I wanted to create
technology that helped people. I also did a lot of art, and I like writing, so I had many,
many different interests, and I was an athlete, and it was that athletics that led me to study at
Stanford University, which was also close enough to home, but far enough away, and I studied
mechanical engineering. I really enjoyed understanding both physics, but then how I had to also design
and build things that produced functionality in the world, and I was always drawn more towards
smart systems with sensors and actuators and programming. I actually delayed taking my first
programming class because I'd heard it was so difficult, all the other athletes, I was a
viable player. They all said, oh, you know, the programming class, so hard, and I loved it beyond
words. And then I took more computer science, I took more electrical engineering, and I decided
to stay for a master's degree, and I worked actually as a teaching assistant in a machine shop
for two years, helping like design, helping students learn design and manufacturing, like welding
and casting bronze, and milling aluminum and making parts, staying in the shop late at night,
helping, and I really fell in love with working with younger people, helping them design and create
things. And I also took this amazing mechatronics class there, and like realized I wanted to
begin my professor, and I needed a PhD, and I needed a PhD advisor, so I looked around and found
a new professor, I was his first PhD student, his name is Kuntan Niemeyer, and he was one of the
first engineers at Intuitive Surgical, a robotics surgery company that most of you probably know,
they make the DaVinci robot. And we got along super well, he was also a volleyball player, and I think,
yeah, the mechanical engineering computer science electrical intersection is really robotics,
and then turning it to do something useful, whether that's in health or in consumer products,
or I mean, we work on so many different things now. Yeah, so I got my PhD there, and
moved to the East Coast after graduating. I did a brief postdoc at Johns Hopkins University
with Alessandro Camaro, who's now a professor at Stanford, and then I started my faculty career
at the University of Pennsylvania in the grass lab, which is a great robotics group, and I was super
lucky to have colleagues in several different academic departments, and was there for nine and a half
years, and we did, I mostly do haptics research, robotics, and while I was at Penn, I started doing
more research in autonomous robots, giving autonomous robots a sense of touch, sometimes through
machine learning, and then in 2017, January, I had the chance to move here to Germany to become
a director at the Max Plank Institute for Intelligent Systems. We've been around for about 10
years now. It's a wonderful place to do interdisciplinary research in robotics and AI.
Awesome, awesome. Tell us a little bit about your research interest. You've given us a taste of
them, but what are some of the various areas that you're focused on? Yeah, so a lot of our work
comes back to the sense of touch. So haptics are physical interaction with the world, and I know many
researchers study vision, whether it's computer vision or natural vision, or hearing auditory
perception for humans and animals or artificial, we know far less about the sense of touch.
And so from a researcher's perspective, that makes it really interesting, because it's still
like relatively early days, for how do we create artificial systems that can perceive their world
through touch, and then we also work on the dual problem, which is actually where I started,
which is creating devices and programming them to present high quality haptics sensations to
users. And so a lot of the work that we do is inspired by some fundamental insights that like
the sense of touch is really important to people and to artificial agents. It's often under-appreciated,
and it's not yet understood that well. And when I talk about touch, I'm kind of talking about
two things. And one is your skin, like distributed all over your body. You have cells, receptors,
that can process different kinds of physical stimuli. So whether something is touching you,
or how big of an area it's covering, if it's sliding across, if it's shaking back and forth,
vibration, if it's stretching, if it's warm or cool, taking heat from you, and that's distributed
all over your body. So it's like a few tiniest sense. And I think you probably already know robots
don't usually have any skin or almost any skin. They're really deficient in that. And then on
the other side, there's also your movement through the world, like your joints and the muscles
forces. And your brain is constantly combining all that streaming data from all over your skin,
and the movements you're making with your body to help you understand your own physical,
how you're moving through the world, if you're contacting something that's expected or unexpected,
and processing that. And the final thing, cool thing about the sense of touch is it's very interactive.
So vision is active, isn't it? You move around and you get different perceptions of a world,
maybe see something from different angles, but touch is even more so. Because there's a really
tight loop between when I touch this glass and I start moving my finger down, I can make movements
that are almost not visible and really, really feel something very different or start moving in a
different way. And so yeah, we're fast, I'm fast and you can probably tell I'm talking too long
about this by touch-based interactions. And we start from this foundation and work on kind of,
maybe a few different areas like social physical human robot interaction. So giving robots a sense
of touch whether that's actually making sensors for them or how they should process the data
and often to interact with people. Our more specifically just tactile sensing for robots and
for manipulation. We also work on systems where a human is controlling a robot from a distance.
So like the DaVinci robot where a surgeon remotely operates these little tools that are deep inside
the person and we work on giving you a sense of touch and maybe using that to provide demonstrations
for autonomous robots. And then last, we also work in like virtual reality devices and algorithms
to let you feel simulations like maybe dragging across well on a cell phone. You probably felt
some have to feedback but feeling able to feel textures of surfaces on a cell phone or in virtual
reality that would make and break in contact with things and to make it feel more realistic.
Nice, nice. I noticed the title of your recent presentation at the Eichler conference was
haptic intelligence and I kind of had the sense that that was maybe a double-on-tondra like
the sense of touch or haptic informs our intelligence but there's also intelligence required to
process it digitally. And I thank you in kind of articulating your research portfolio.
You spoke to the way that sense that that that sense kind of informs the way that we or robots
move through the world. Yeah, I think the sense of touch is a little under-recognized and it's
sort of coming up in importance in robotics. Of course, first if a robot's standing there it needs
to be able to perceive its environment, to be able to move around and not hit anything. Actually,
the goal is often don't touch anything, don't break anything, don't hurt the person. But then if we
want to do something, if we want to manipulate, move things around, pack a box, clean up a room,
help a person, robots are going to need to reach out and touch something. And then like many other
researchers we often turn to humans and animals for some inspiration and are curious about
understanding I could scientifically what is known about the sense of touch. Often as a researcher
you might have some intuition about how you would design a system. And sometimes that works well.
And sometimes it doesn't because there are aspects that you maybe don't properly anticipate. And so
a common example is like often a roboticist we think I need to put a force sensor on my robot.
And that means you have to cut the wrist and put a force sensor here and then between the hand
and the wrist the robot can feel the load as it like picks something up. But then that force
sensor here can't feel but things that happen higher up. And it also doesn't give you that much
information about where contact is happening. And so skin is distributed and has like it's quite rich
and a little bit different from what I would think I would need a force sensor. But maybe
also another example is our skin is especially sensitive to transients, to changes in stimuli.
And a lot of times as an engineer you might look at the levels and think oh it's really important
that it's like 2.3 newtons or the illumination what level that is. But these sometimes the
illumination in a room radically changes over the day and your vision system is adapting or your
touch system is also kind of has these is able to focus in on changes which are often really relevant
and important for behavior accomplishing what you want. So nice. You talked about some of your
kind of early you know your early days and just liking to build and make stuff. And I just had
this wonderful memory of when I was a kid I was into electronics and one of the things that I was
playing with at some point was a force sensor. It was like some piezoelectric foam or something like
that and you put it between two like printed circuit board elements and you can I guess the
resistance varies based on the pressure that's applied. Actually we're working on something similar
to that. We're actually working on something similar to that. We have a paper under review
and a human computer interaction conference on something very similar but to give rope to like put
touch sensors all over the body of a now robot. You need something cost effective and simple
so it'll keep working but please go ahead. I'm glad you had a chance to make a force sensor.
I hadn't thought about that in who knows how long but just kind of clearly came back to
me so thanks for that. I'm glad I I hope that lots of young kids have a chance to test out
engineering and computer science as you were telling your story I was flashback on I was lucky
that for my 13th birthday and this was a long time ago this was in the 1990s my parents gave me
a Dell 386 computer and it still had like a DOS 5.0 command line and windows 3.1 that you like
ran from the command line and I like tried absolutely everything possible in windows and learned
all about it and then learned some weird visual programming language and I was thought oh my gosh
this programming thing is really cool at the time I had had a knee injury and I had to do physical
therapy and it required me to like tense my muscles for 10 seconds and then pause for 5 seconds
and tense and you always had to sit in water clock and I found that very cumbersome and I wanted
something that would keep that time and I did three sets of 10 repetitions and I wanted to be
able to read while I did my exercises so I used this visual programming language that I'd learned
on my new old school windows 3.1 computer to make a program that would beep to tell me when to
start and stop and so I could read and I didn't realize until I was later in college and I'd
heard all these rumors how hard programming was amazing it's so powerful and let you out of
me things and it also when I later realized it forces you to think much more clearly it forces
you to think like a computer and be extremely logical so I think that's maybe why I fell in love
with it as a bachelor student as an undergrad that's awesome that's awesome so tell us a little bit
more about kind of where machine learning comes into the picture when you're thinking about
haptics yeah so I mean machine learning is a wonderful tool especially for understanding patterns
that are too complex for us to see with the eye I come from more of a mechanical engineering
background we believe in physics we believe in Newton's second law and lots of things we can write
a some we can write out equations of motion and then try to predict how things will behave but
I think we have to make a lot of assumptions like linearity or yeah we have to sometimes you have
to simplify things so much to make these equations work and like all models all of these like even
no matter how beautiful they are they're all going to be wrong at some point in the real world
is dirty and messy and complicated and not linear and not even not very well paved and so then if
we want to have a robot or a system that's going through the world and physically interacting with
things for example we work on designing and creating tactile sensors that incorporate machine
learning in the flow we can do much more if we can free ourselves of some of these really rigid
physical assumptions and open things up and then then we need a tool to really understand those
complicated mappings and nonlinear transformations and yeah so the very very first project I did
that involved machine learning was a DARPA funded project when I was an assistant professor at
the University of Pennsylvania it was a collaboration with Trevor Daryl who's a computer science
professor at Berkeley Peter Biel and Tom Griffith and some other people were also on the bigger
project but I worked closely with Trevor on it was a grounded language acquisition for robotics
so can we enable robots to associate words with their physical sensations and there were other
cool parts of the project where they were working on verbs like seeing actions and nouns but we
worked on adjectives like how you would describe how things feel through the sense of touch
and we installed at the time state-of-the-art tactile sensors in a PR2 which is a humanoid robot
and had it touch a set of about 60 objects that had many different physical properties and it
touched them in different ways and we also had humans come and touch the exact same 60 objects
and describe them in words and rate them with words so then we could pick out a particular object
from this data set this set of objects it's called the pen haptic adjective corpus I think
have pen haptic adjective corpus and we would know what words people describe and also the frequency
with which different people describe the words and then we could have the robot touch the same object
in different ways and we trained simple machine learning so we did at the time manual feature
extraction and included support vector machines to have the robot be able to have all these
clusters that could touch a new thing and like say is this slippery is this nice is this scratchy
is this soft and that kind of opened my eyes to how much we could do beyond like me just looking at
the signals or trying to write how can I write a formula for nice or scratchy and would I even want
to sit there and try to come up with formulas it was painful enough to have to come up with features
and now we can automatically figure out what features are and maybe go just end to end but
as a time even that much was really powerful that we could then bring new objects to the robot and
have it touch it and it would give labels it would tell us what it what would it how would it describe
this object and it's actually pretty good and I thought this this is fun we should do more stuff
like this and so that's where it began that's awesome so much of the emphasis and you alluded to
this earlier in machine learning and robotics is around vision and the application of computer
vision to all different aspects of robotics from you know local motion to grasping to manipulation
are there things that you're working on that are you know to what degree have we kind of integrated
vision and haptics is that an area of active research in the field it definitely is I hope that
robot I think robots are now and there will be increasingly multi-sensory the section with the
workshop that I was in that I clear was about with some multimodal embodied robot multi-modal
embodied intelligence or learning learning I think different senses that a robot or a human has
provide different kinds of information from the world and I'm convinced not everyone is convinced
that that having some touch sensing and probably audio also will greatly benefit agents robotic
systems in the future they have to have vision there are a few different places where there is
crossover or synergy and one I briefly alluded to is we're using we have created a tactile sensor
it's under review now so I can't talk too much about it I showed a little video just with some
preliminary results during my talk at ike there and here we're actually using a camera like many
others have been to doing a vision-based tactile sensor and we're basically then we have a soft
structure this is joint work with gare of martes and his PhD student Juan Boson at this to
tubing inside of our institute mexican super intelligent systems and so we built a soft structure
that has a camera that's looking inside and can see the deformations of where soft sensors
being touched and many others have made beautiful maybe thin or other shaped sensors and we
tried to make one that was basically the shape of your human thumb so three-dimensional and can
feel on all sides and here we use a camera to capture simultaneously the deformation feel but
not even the deformation feel just the pixel values all over what's seen inside with some structured
lighting and then we Juan Boson Gare created a beautiful deep learning architecture and we have
a little robot that goes around a pokes on the finger and we get all this data to know exactly
where it's been touched when and it can then generalize and we can touch it on the outside and it
can show us a map of all the forces it's feeling and not just normal direction indented but
sheer also so if you push in and you slide along it it can feel you're pushing up or down
it can tell if you're touching it in a large area or small area if it's touching something in that
way and so here's an example where we're taking great stock commercial off-the-shelf good camera
and then adapting deep learning like vision-based deep learning but the thing we're learning is
something haptic and it actually works pretty well and I think some other researchers around the
world have done similar things and then certainly like I was already saying before robot should
combine what they see in the world with what they feel I'm not working in that so much right now
but accepting rather primitive ways but I think that's beneficial for sure and can you speak to
the with that previous project you mentioned how that data that or that model might be used so
you've got the the robot kind of grasping this thumb-shaped thing and you're using computer vision to
um I maybe didn't explain it well so there's the the sensor eventually will be mounted on a robot
but in the beginning it's just sitting on like on a bench and then another robot like a
sort of 3d printer a gantry comes around and pokes it and we hope to mount we plan to I have a
new code-wise PhD student with Garen who will both be working to miniaturize the sensor and adapt it
maybe bring in other modalities and then also to put it on a robot so that the robot could move
through the world and touch things and and learn so uh and be able to physically interact in a
controlled way with things in the world and eventually maybe we imagine robots whole parts of the
robot could be made in this way maybe the distal extremities the fingers are are the most natural
components because you don't need that space so much for like joints and and wires to be running
I think we'll have a combination of different technologies um deployed you don't need the same
precision of and the same richness of tactile sensation like on your back or on your elbow as you
do on your fingertips this is more like a fingertip finger hand um I didn't realize that you were
developing the device that would then be the extremity um and so the idea is that the device
that's ultimately deployed on a robot would continue to have a vision sensor in it and
you're building a model that would um allow uh I'm ready your face here yes or no I think I'm
not explaining myself well something maybe let me try one more time uh it's actually near the end
of the day here in Germany so I may be uh not functioning on all cylinders we're basically trying
to make a robot finger let's imagine we made two of them and the robot would have these in the hand
there's a camera inside the robot's hand looking through its own fingers and seeing and yeah
okay you get it the camera is inside the hand that is deforming the thumb or the camera's inside
the thumb camera's inside the thumb right camera is definitely looking inside the robot's own body
looking at seeing the skin from the inside yep and I got that and and that is what you I think the
distinction that I was trying to draw is is the camera like a you know train time thing and then
you've got something else or the camera is always going to be there and you're building a model
that basically allows the computer to you know turn this video signal into something that's more
useful from a a grasping perspective the second the camera is the transducer so we have the mechanical
structure the concept mechanical structure that takes the physical contact and it's just passively
deforming and then the camera sees that and then the deep learning and it's capturing frames
over time and then each frame gets passed through this deep neural net which is based on a resident
and then it outputs the force field so other people and also in one part of our paper we worked on
just estimating where am I being touched like I'll put XY coordinates or in the magnitude of the
force or something or how many like there that framework I personally it doesn't leave very far
because what if I get touched one two three four five infinitely many I have a funny patch
is thinking about discrete outputs it didn't really work and so we actually started this project
in 2017 shortly after I came here sometimes important things or hard things take a long time and I
think that you should help to have some patience and persistence and have many projects going in
parallel so that some things will bear fruit sooner actually here you see behind me it always seems
impossible until it's done this is one of my models and then yeah we output this map of basically
little points each one has a force vector a 3d force vector so the robot can reconstruct
the agent with it has this thumb this finger can reconstruct where it's being touched
and the direction the force is happening against it so kind of like a vectorized point cloud
yeah like a set of points and each one has a little vector that tells it is it being touched
and in which direction is it being pushed got it yeah and it was trained so the trainee day
that comes from a little thing poking it at all these different locations and it moves around and
then we have all every frame with it new okay a little four millimeter diameter indenture
touch me here or here and here and it presses down it pushes down at different force levels and
also laterally the shear is really important and it vastly expands like the data you have to collect
it's not just where did I get touched and how hard was it pushing but also it was translating
laterally against my skin so okay and so that you have that external robot that's essentially
generating the training data for your ResNet it's known ground truth of what the forces that
it's imparting into this this new sensor that is the camera and the inside out camera that
ultimately we want to use to create the point cloud exactly yeah and there's a force sensor
on the robot that's probing our sensor which is called insight I should have called I sort of
said that I shouldn't give it a name we're probing it from the outside and we know how the
force vector and where it's being touched and yeah it's been a really fun project actually I really
enjoy collaborating across disciplines so like I know a little about machine learning actually
really like math I really like programming I really like making plots and drawing diagrams
and understanding things and learning new things but it's even more satisfying that we're
trying to fumble through alone to partner with someone really smart and a few really smart people
and learn from them and yeah that's I think one of the most rewarding things about being a researcher
like an academic researcher awesome awesome you mentioned the importance of having multiple projects
going on in parallel so that you can wait out the the insights so to speak no pun intended
what are how long it takes for an insight to arrive or yeah go ahead what are some other projects
that you're working on in the lab oh my gosh I can't even count I have 15 PhD students eight postdocs
three research scientists like four or five people who recently graduated or recently finished and
have faculty positions of their own okay I'll tell you just a few one we are making a hugging robot
it's called huggy back when my PhD student Alexis Block she was a master student with me at Penn
she made huggy about one point oh the first one which was a lot of fun actually got a lot of press
at the time and then for her PhD which is through our Center for Learning Systems which is joint
with ETH she is designed and built from scratch a new hugging robot hug about 2.0 it can see users
it adapts to you so it's soft and warm which is very important and then it embraces you at adapts
to your body size and your position and it lets you go when you're ready to go this was our we had
a paper ed HRI the human robot interaction conference this year on this and then we have a paper
under review where we did some machine learning to give this robot a better sense of touch and be
able to know what the user's doing so when you hug someone you don't just stand there and then
like back away you might pat them on the back or squeeze them and the robot has a pneumatic
torso so an inflated like a beach toy inflated torso that makes it soft and we heat it up
but then it has a microphone and a pressure sensor inside the torso so we collected a bunch of
training data where people come in and enter into hug with a robot and then we told them to pat
the robot or squeeze it or rub its back or do nothing and we have data I don't know from 20
something people doing these things in various states and we also had the robot respond like pat
you back or squeeze you and got a mapping of like what people prefer people really like being
squeezed by a robot and then we have what we call hug you about 3.0 where it does this in real time
so it's feeling it's this microphone and pressure data in real time and making judgments like
what did the user do and then deciding how to respond and it's pretty fun to hug so that's honestly
I know people you'll be surprised hugging robots we have another study that we just finished
that I can't talk too much about we're still analyzing the data but it looks like people might even
sometimes prefer hugging a robot to hugging a person because there's no social pressure if you
hug a robot you don't want to worry it's going to like judge you anyway I'm imagining the abstract
of the paper saying something like hug you about 3.0 achieve superhuman performing and hug
detachment priming or something oh yeah we're working on hugging robots it has been surprising
a fun and also I mean Alexis Black and I have had so much fun she's going to graduate soon
and she'll start a postdoc in the US and hopes for a faculty crew of her own but this is one of
the most rewarding things is getting a young scientist she started working with me actually when
she was an undergraduate back at Penn and then yeah now about to graduate and when we first came
up with the idea of hugging robots we thought oh no that's too crazy and then we were like no we
got to do it and it's been real fun we have collaborators too at ETH her co-advisors Rogéic
Gasser's and Ultramar Hiligas who've also been helping with like the computer vision for the robot
to see and the evaluation so stay tuned for more stuff about hugging bat there's a little machine
learning in there it has to be real time that's that's important for robot perception things have
to be able to execute in real time perhaps it's gathering the training data what can be hard you
have to bring real people in and have them really hug your robot and not hurt them and
then get a big diversity people are really diverse and behave in strange ways sometimes
let's see and then we do a bunch of other human robot interaction like robots for exercise or
something like tactile sensing for robots we do some things on surface haptics like on a cell phone
like understanding the physical contact between human finger and the screen I mean I can't even
think about some augmented reality for robotic surgery have to be back in robotic surgery
I don't even know like a lot of stuff and so the machine learning primarily come into play
when you're integrating other senses or doing multimodal work or is there a a degree to which
machine learning has become kind of part of the you know classical or part of the the standard
uh processing you know work floor or tool chain for haptic data if there's enough standardization
to even say something like that I mean machine learning plays a role in many of our projects not
in all but I mean we have some projects that are really hardware focused like design and creation
but I think it plays a role in many of our projects and sometimes it's not even rewrote the machine
learning so for example we have a paper at Ikra the International Conference on Robotics
in our nation this year about the robot interaction studio which we call a platform for unsupervised
HRI but not unsupervised like a machine learning person would think we want unsupervised like no
supervisor no experiment or no coach in the room just you had people come in and play with our
it's a Baxter robot so it's a humanoid robot with two arms and um we used a commercial marker
and less motion capture system called capturing um it's wonderful. Baxter are on the person
is playing with the Baxter. You don't have to wear anything it's in the room so there we put 10
cameras in the room. Oh okay. And we basically function like Baxter's eyes and it's constantly
so there's you walk in the room you don't have to put anything on as a user and it fits a skeleton
to you in like 20 seconds and then Baxter knows where you are in the room and the pose of your body
and we programmed only for the paper at Ikra um we programmed only that the robot we had Baxter
always track you in the room so that you know it knows where you are and it's just always facing
you people respond it has a smiley face and uh and then we had it do a sequence of like commands
like it was a coach like it would point around the room and it lift its arms up and it would like
put its hand up like it wanted you to come to it and then we just looked at did people do those
kinds of things and then looked at statistics of did they walk around the room did they mimic the robot
and we could make heat maps and like where they put their hands and where they went in the room
and actually people are cues which were designed just with intuition were um we're actually
reasonably successful and people were remarkably interested in the robot and had a fun time playing
with it and um then in data that hasn't been published yet we had the robot also notice what the
person is doing and try to correct them or guide them and give them feedback on like and that
became even more interesting because then there's this loop the robot I can see what you're doing
and can like come back try to say oh yeah you're you're I'm doing this but you're doing something
else and it's like no lift your arms up like we're supposed to be doing the sexual sacrifice
and we did this with like no speech no instructions to the users um but here's an example where we're
using this commercial marketless motion capture system that has deep learning in in it uh to
deliver really great perceptual real-time perceptual capabilities to a robot robotic system
and then using that to create new experiences um another example i mentioned briefly um
augmented reality in um robotic surgery yeah we also are doing some computer vision on like what the
so someone else trained uh the neural network but we can do computer vision on what the surgeon
is seeing to figure out where the tools are and then use that to create interactive functionality so
those are places where we're really grateful that other wonderful researchers or experts in
machine learning and vision have delivered capabilities that we can then integrate and then um into
interactive systems and then other times the machine learning is more at the core of what we're
doing and not typically it is more on when we're designing and creating tactile sensors for robots
or interpreting information from tactile sensors that robots already have um yeah
nice nice um is part of your research or to what degree i guess i should ask is part of your
research focused on the kind of the softer side of human robot interaction uh maybe for context
i've had really interesting conversations on related topics to this with uh iana Howard for
example talking about this um you know what i think of as the this deference relationship that
she's observed between humans and robots also Kate darling um uh studies this as well uh is that uh
is that a explicit focus in your research or something that you observe while doing other things
well just as a side note it's great you've talked to iana and Kate i i know iana and greatly admire
her and have read an article recently about Kate and i would love to get to meet her someday
and it was really inspired by her advice uh that we should treat robots and we should have
our paradigm more that they're like animals than like people we do hri it's like i actually let
the topics that we study in my group mostly be guided by my students interests and i'm relatively
flexible like there's various things i'm interested in i can get excited about a lot of stuff i have
ideas not a lot of things and i have had over the last few years especially since i moved from pen
more students really interested in human robot interaction but and yeah we do human subject studies
we're curious but maybe something that's different um we don't usually use like a wizard of
os paradigm where there's a human controlling the robot i'm usually trying to make actual technical
systems that are functioning in real time because those constraints need to be overcome and
i want to operate kind of within what would what could work in the next five years not what
it's going to take some superhuman human level perception is beyond what robots are delivered right
now and so we're often challenging i'm often challenging my students and we're trying to deliver
interactive systems that can function in real time and i'm part of i'm certainly a roboticist
when i was a PhD student there was kind of a joke that in robotics you could get away with proof by
video like if you made a video of your robot doing something once and like submitted a nico paper
in that one time it was impressive like no one might ask does it always do that how many times did
you test it and it's a bit time in cheek but being able to deliver the same quality of performance
over and over and especially robust to different things that might change and the environment is
really hard and in haptics where we're creating sensations for people to feel if they if you read a
paper and it's just the technical development and algorithms or the description of the system and
there was never an experiment of people the reader the reviewers are skeptical they don't believe
you your papers are not going to get accepted and not only that do you have to like bring real
naive people who aren't members of the research team in and have them do stuff and look at their
both their behavioral their performance differences and their opinions in a very unbiased way
it's very scientific you also need to bring your demo to the conference and let people experts
from around the world try your system and see if they can break it or there's this like pride of
i brought a real system hti they do this also uh bring real interactive systems and other people
try them and that's how you can gain real credibility as a researcher by showing your systems live
and you also learn a lot by demonstrating your systems whether it's to school kids or visiting
researchers or at a conference i strongly encourage demonstrating or work live if you can it's
terrifying but exhilarating awesome awesome uh you also mentioned as we're getting started uh
uh and throughout the the interview can be your passion for mentoring and uh the
personal side of your work and the importance of diversity and i thought maybe to wrap things up
you can share a little bit of you know how you think about those topics yeah thanks um you might
have noticed i'm a woman and there aren't that many women in robotics there aren't that many
women in machine learning i'm super lucky that i had great mentors and that uh people gave me
opportunities i worked my butt off um to do well and to get into a good university to learn what i
could i i was brought up to really my parents had like to those who are given much much is expected
and like what are you going to do with your life kid like where are you going to make a contribution
they're really wonderful people this is probably the best thing they gave me wonderful access to
education and made it clear that they were interested to see what i was going to do with those
opportunities and i and they also did encourage me even though i was interested in topics that they
they're both pretty technical and so maybe i was got my mom with like fix the sprinkler system
she's a PhD in psychology in the 1970s she was programming for trend on punch cards to to
analyze experimental data that she came she she had collected so maybe i didn't drift that far
away from my my parents in terms of what they do but in the field that i'm in mechanical engineering
computer science electrical engineering there aren't that many women so i'm grateful that i
was given opportunities and i believe that young people of all all types uh deserve a chance and
to fall in love with this field and to bring their insights and um ideas and to have yeah to have
a chance to contribute and so that's been something that's been important to me throughout
have a really diverse team here at Penn also at Penn and here at max plank my team is more than
50 percent women or about 50 50 and from all over the world and also many different majors like
many different fields of study and i i see every day the benefits of those different perspectives
and the cooperation there can be challenges there are more cultural like people from
vastly different cultures i remember a time i met a PhD student uh who like what had never had a
female professor before much and his policy was he didn't touch women and but we still managed
to have a good conversation and he did a super good job on his quals um and came to we came to really
have a good relationship um so i i think it's a good personal growth to work in a diverse group
and uh i am very passionate about supporting young people and often that's simply by a few things
like asking them how they are how's it going where they want to go in life and how can i help you
how can i help you get there where what do you what are you aiming for and what are you excited by
and have you thought about this or where can we go with that so i don't know if that's a good answer
to your question but at the end of the day i think the people i work with the people i help train
the people i learn from are much more important to me than the actual work that we do i'm a total
nerd i love the research that i do i'm very proud of our papers and our findings and our
videos and i do love it but at the end of the day i'm even more proud of the the people and what
they've taught me what i've learned from them and the relationships that we build together
that's awesome that's awesome okay then thanks so much for sharing a bit about what you and your
team are up to it's been great chatting it has been great sam thanks so much for having me thank you
