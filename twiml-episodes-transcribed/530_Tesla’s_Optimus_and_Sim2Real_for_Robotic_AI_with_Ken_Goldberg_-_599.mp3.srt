1
00:00:00,000 --> 00:00:05,040
All right, everyone. Welcome to another episode of the Twemal AI podcast. I am your host,

2
00:00:05,040 --> 00:00:10,640
Sam Charrington. And today I'm joined by Ken Goldberg, the professor of industrial engineering

3
00:00:10,640 --> 00:00:16,880
and operations research, and the William S. Floyd Jr. Distinguished Chair in Engineering at UC

4
00:00:16,880 --> 00:00:23,280
Berkeley. Ken is also the chief scientist at MB Robotics. Before we get going, be sure to take

5
00:00:23,280 --> 00:00:28,880
a moment to hit that subscribe button wherever you're listening to today's show. Ken, it has been a

6
00:00:28,880 --> 00:00:33,600
bit. Welcome back to the podcast. Thank you, Sam. It's a pleasure. I've been enjoying listening to

7
00:00:33,600 --> 00:00:39,200
your shows over the last couple of years. Awesome. Yeah, it is hard to believe that it has been two

8
00:00:39,200 --> 00:00:46,560
and a half years since the last time we spoke. Of course, a ton has been happening in Robotics.

9
00:00:46,560 --> 00:00:52,160
And I guess we're going to use this next next little bit for you to catch us up on everything.

10
00:00:52,160 --> 00:00:58,720
Well, it's been actually an amazing time. I mean, with the pandemic, obviously, we were in the

11
00:00:58,720 --> 00:01:04,560
middle of it when we last talked. And so much has happened. I would say that it's actually been a

12
00:01:04,560 --> 00:01:10,160
very productive time for Robotics. That people have made enormous amounts of progress. There's a

13
00:01:10,160 --> 00:01:16,320
lot of publications, a lot of research that's been going on. And also the world has changed in

14
00:01:16,320 --> 00:01:23,360
interesting ways that I think is as favorable to Robotics as a field. Absolutely. Absolutely.

15
00:01:23,360 --> 00:01:31,280
Now, I'll refer folks back to our conversation that was episode number 359 back in March of 2020,

16
00:01:31,280 --> 00:01:38,400
the third wave of robotic learning for a great conversation in your full background. But for those

17
00:01:38,400 --> 00:01:44,240
who, you know, haven't caught that, why don't you share a little bit about how you came into

18
00:01:44,240 --> 00:01:52,320
Robotics? Well, I was, I've been interested in robots since I was a kid. And back in the old days

19
00:01:52,320 --> 00:02:04,800
of Star Trek and Waston Space. And I've just continued that for, you know, 50 years. And what I

20
00:02:04,800 --> 00:02:15,600
have to say is I rediscovered Robotics as an undergrad. And found that it was just absolutely a

21
00:02:15,600 --> 00:02:20,640
fascinating set of questions. I was lucky to have a great advisor, Regina Bicci. It was at

22
00:02:20,640 --> 00:02:27,040
UPEN. At the time, she took me under a wing, really mentored me. And she's still a good friend.

23
00:02:27,040 --> 00:02:32,640
She was at Berkeley for many years now back at Penn. And then I went to Carnegie Mellon where I

24
00:02:32,640 --> 00:02:42,720
worked with both Matt Mason and also for a little bit with Mark Rayburt. And so I had just the

25
00:02:42,720 --> 00:02:48,800
opportunity to work right at the heart of where robotics has been, had been start growing. And I

26
00:02:48,800 --> 00:02:54,640
took an interest in one particular problem, which was grasping. And I, I've been working on the

27
00:02:54,640 --> 00:03:02,560
same problem for 35 years. Which says a lot about how hard that problem is. Exactly. Exactly.

28
00:03:02,560 --> 00:03:06,480
You know, it's interesting because most, you know, I think everyone on this, who's listening to

29
00:03:06,480 --> 00:03:11,920
this knows that it's a, it's a hard problem for robots. But it's still interesting for the public

30
00:03:11,920 --> 00:03:17,680
that most people, you know, we humans do this effortlessly. It's very easy to pick up almost

31
00:03:17,680 --> 00:03:23,600
anything, right? That we, you're handed. In fact, my dog can pick up almost anything with a parallel

32
00:03:23,600 --> 00:03:27,680
jog ripper, right? It's, it's a, it's a, it's a, it's a, and it quickens out, you know, you can pull

33
00:03:27,680 --> 00:03:31,680
out some very weird shaped object and it'll quickly figure out where to grasp it.

34
00:03:31,680 --> 00:03:37,760
That is a fascinating skill. But if you do that in front of a robot, it is, we're very far from

35
00:03:37,760 --> 00:03:44,480
being able to, to do that reliably. Yeah. I think one of the things, um, and kind of looking over

36
00:03:44,480 --> 00:03:51,600
some of your recent work that most jumped out at me as indicative of the kind of progress we've made

37
00:03:51,600 --> 00:04:00,640
is a paper that you worked on with your team autonomously untangling long cables. And I guess that

38
00:04:00,640 --> 00:04:07,600
struck me because I remembered back to, I don't, it must have been maybe four or five years ago.

39
00:04:07,600 --> 00:04:12,320
I don't think it was, it was your paper. I think it was Peter Abiel had this paper of like,

40
00:04:13,280 --> 00:04:20,640
trying to untie a knot on like a gigantic rope, you know, a single knot. And if the color of

41
00:04:20,640 --> 00:04:26,800
the background change didn't work, if the color of the rope changed, it didn't work. And here you are

42
00:04:26,800 --> 00:04:34,000
with this paper like untangling a hornet's nest of cables. Well, it's not quite the hornet's

43
00:04:34,000 --> 00:04:37,920
nest yet, but, um, but you're, you're, you're, you're right in it. That's, that's a very good

44
00:04:37,920 --> 00:04:45,600
perspective. We were, um, Peter was working with a thick cable. Um, and it was, I believe that was

45
00:04:45,600 --> 00:04:52,480
tying a knot. Okay. To get to create a knot. A few years ago, we started looking at untangling.

46
00:04:52,480 --> 00:05:00,320
And we started with very simple, very small sections of cable. So on the order of eight inches.

47
00:05:00,960 --> 00:05:06,480
And we actually used our surgical robot as an implementation of that. So we had, um, the

48
00:05:06,480 --> 00:05:14,240
Da Vinci, um, trying to untangle these, uh, very small, uh, segments. And they were very simple knots.

49
00:05:14,240 --> 00:05:20,720
They were just overhand knots. And that, that was a, that's a hard problem because it's, you're in

50
00:05:20,720 --> 00:05:27,280
the realm of, of deformable objects. And so, you know, and there's an infinite state space

51
00:05:27,920 --> 00:05:35,360
for those. There's also self-occlusion. And, and, and in fact, though, we minimize the self-occlusion

52
00:05:35,360 --> 00:05:39,600
because of the, the size of the knot, the size of the cable, right? It was fairly small. So you

53
00:05:39,600 --> 00:05:45,840
didn't have all this overlap and slack, if you will. So the key was to identify where the

54
00:05:45,840 --> 00:05:50,960
knots were, which we, we learned with a deep network, lots of examples. And then it would just

55
00:05:50,960 --> 00:05:56,720
start pulling at these knots and then, and then be able to open them. What was, what was exciting

56
00:05:56,720 --> 00:06:03,120
was the, the, we started thinking about longer cables and really expanding this into the macro scale

57
00:06:03,120 --> 00:06:09,840
with a, a full-scale robot, a dual-armed, um, um, UMI robot. And really starting to think about

58
00:06:09,840 --> 00:06:15,760
all the complexities now of, of, of truly trying to untangle this thing, pull it apart and manage

59
00:06:15,760 --> 00:06:22,080
the workspace for a dual-armed robot, which is, there's got a lot of complexities in its own

60
00:06:22,080 --> 00:06:27,040
right, because you have to avoid self-collisions with the two arms. They have to work around each

61
00:06:27,040 --> 00:06:34,080
other. And there's challenges and perception. And also, the workspace of the, of the robot is

62
00:06:34,080 --> 00:06:40,000
surprisingly small. It's usually about the size of a dinner plate, really. And the cable, in this

63
00:06:40,000 --> 00:06:44,960
case, was about three meters. So that greatly exceeds the size of the workspace. So you have to think

64
00:06:44,960 --> 00:06:52,240
about how to move things in and out of the, of the workspace and resolve ambiguities. But that,

65
00:06:52,240 --> 00:06:56,480
that has been a very fun project, I have to say. The team has done a great job. It was actually

66
00:06:56,480 --> 00:07:04,960
led by two, uh, two undergrads who became master students now, uh, Vynavy and Koshek. And they, um,

67
00:07:05,760 --> 00:07:12,480
and, and with, uh, Justin Kerr, a PhD student, they presented this at RSS this year. And they did

68
00:07:12,480 --> 00:07:19,040
such a fantastic job. They won the best systems paper award there. Yeah, congrats to, to you and

69
00:07:19,040 --> 00:07:26,160
them on the best systems paper award. The award calls out the, the systems and nature of this. Uh,

70
00:07:26,160 --> 00:07:30,640
can you talk a little bit about about this as a systems challenge? Definitely. And thanks for asking

71
00:07:30,640 --> 00:07:37,200
about that. In fact, it is a systems paper because you have the, the system of the perception system,

72
00:07:37,200 --> 00:07:42,880
the, the planning system, the actuation system. One of the key things that made this new work possible

73
00:07:42,880 --> 00:07:50,400
was a very, very clever hardware design that Justin came up with, which was to add a little foot

74
00:07:50,400 --> 00:07:57,600
onto the parallel jaw grippers. And if you think of it as just a little L shape, but a very small,

75
00:07:57,600 --> 00:08:03,360
I, almost a toe, if you will, what that allows it to do is that the, the grippers now can be fully

76
00:08:03,360 --> 00:08:09,280
closed and that holds the cable tightly. But if you open them by a little more than the diameter

77
00:08:09,280 --> 00:08:16,000
of the cable, then those two toes basically are interlock and they, um, prevent the cable from

78
00:08:16,000 --> 00:08:23,520
escaping. So we call that caging in, in robotics. Caging is where you have the object contained,

79
00:08:23,520 --> 00:08:30,080
can't escape, but it may not be held immobilized. So it might bounce around. So caging like you put

80
00:08:30,080 --> 00:08:37,040
your hand or, you know, our bird in a bird cage, it can move, but it can't escape. So caging is,

81
00:08:37,040 --> 00:08:43,040
um, is an interesting geometric problem in its own right. But here we, we think about caging,

82
00:08:43,040 --> 00:08:49,040
which is a way of allowing the, the, the gripper to enclose the cable and slide along the cable,

83
00:08:49,040 --> 00:08:57,680
which tends to pull through knots and tangles. So that turned out to be a big, very, very critical

84
00:08:57,680 --> 00:09:03,840
part of the, of the system, that piece of hardware that wasn't allowed us to introduce new primitives.

85
00:09:04,800 --> 00:09:11,440
So what we call cage pinch dilation, which is where we pinch with one jaw, cage with the other

86
00:09:11,440 --> 00:09:19,760
jaw and then pull dilate. And that allows the, uh, the, the system to untangle individual knots.

87
00:09:19,760 --> 00:09:27,840
And then we systematically pull through the slack and there's one other challenges, the depth,

88
00:09:27,840 --> 00:09:34,000
finding the where to grasp a cable is difficult because the cable is not lying perfectly on the plane.

89
00:09:34,000 --> 00:09:41,840
So it tends to loop and sometimes those loops can lift three or four inches off the plane. So if you

90
00:09:41,840 --> 00:09:48,800
go to grasp them, you really do need to have some sense of depth. And so we use a depth camera,

91
00:09:48,800 --> 00:09:57,360
but those are very slow because of the scanning process. So what we are moving to right now,

92
00:09:57,360 --> 00:10:03,840
and this is continuing, is to, is, is, is trying to avoid the depth camera. Is the depth, depth camera

93
00:10:03,840 --> 00:10:11,040
that you've been using? Is it, uh, like a vision-based 3D stereo thing or more like a point cloud,

94
00:10:11,040 --> 00:10:16,880
connect type of camera? More like a point cloud connect. So it's using a laser scanner and, uh,

95
00:10:16,880 --> 00:10:23,440
and, and, and it's, so it's a structure of light, but what, um, that scan is slow. It's about,

96
00:10:23,440 --> 00:10:29,920
it's about one frame per second at best. Now, the, um, and it also has a lot of specularities,

97
00:10:29,920 --> 00:10:36,080
problems with the, just getting that accurate is, is challenging. So here's what we've been thinking

98
00:10:36,080 --> 00:10:44,000
about, Sam, is, um, trying to imagine that we can't know exactly the height of the, of the cable,

99
00:10:44,000 --> 00:10:48,880
but what we can do is we can use geometry and physics. So what we do is place the,

100
00:10:48,880 --> 00:10:54,560
trying to estimate a, a position for the gripper above the cable in such a way that the cable

101
00:10:54,560 --> 00:10:58,960
is somewhere in here. We don't know where, but what we're doing is now lowering the cable down

102
00:10:58,960 --> 00:11:05,120
in such a way that it will again cage the, um, the cable as it moves to the, to the work surface,

103
00:11:05,120 --> 00:11:09,600
and then it closes. And so we're guaranteed to get the cable even though we don't know it's

104
00:11:09,600 --> 00:11:14,960
exact depth. So this is the kind of thing where we're, we're, we're interested in is how to use

105
00:11:14,960 --> 00:11:24,720
geometry and mechanics to perform things where the censoring may not be sufficient. And another example

106
00:11:24,720 --> 00:11:31,440
is that if we, um, if, what we, we, here's something we're very interested in right now,

107
00:11:31,440 --> 00:11:39,040
which is starting to use the subtle clues where I want to, um, determine where, let's say,

108
00:11:39,040 --> 00:11:43,360
do what really care about where the, the cable is. It's, it's sort of moved up and sitting on

109
00:11:43,360 --> 00:11:49,920
over the table. What I can do is move down with a jaws and carefully monitor what's going on.

110
00:11:49,920 --> 00:11:55,040
And when the, when the moment when there's a movement of the cable will be an indication that I've

111
00:11:55,040 --> 00:12:03,760
made contact. So the vision can actually provide a lot more information than we tend to think. We

112
00:12:03,760 --> 00:12:09,520
don't need, we don't need a contact sensor here. We have a vision because the cable will imperceptibly

113
00:12:09,520 --> 00:12:14,480
move just a few pixels. But knowing that is a trigger to say, wait, that's the height. And we know

114
00:12:14,480 --> 00:12:19,520
the height of the robot arm because of the kinematics. So I can determine that. Well, therefore that's

115
00:12:19,520 --> 00:12:27,680
where the, where the cable is. Interesting. Interesting. Uh, so hardware played a, a big role in, uh,

116
00:12:27,680 --> 00:12:35,200
allowing you to do this. Can you talk a little bit about, um, the software or MLAI side of things.

117
00:12:35,200 --> 00:12:40,400
And, uh, with a particular emphasis on, you know, things that have changed over the past,

118
00:12:41,120 --> 00:12:47,120
a couple of years that have really helped to tackle a problem like this. Sure. And the, the,

119
00:12:47,120 --> 00:12:51,520
because we've been, this problem has been evolving. And it's one thing I'm actually just

120
00:12:51,520 --> 00:12:55,680
want to mention, I really believe in. So I came to the students last week and we were saying,

121
00:12:55,680 --> 00:13:00,800
you know, I'm really proud of the, the, the projects that tend to continue over multiple years.

122
00:13:01,600 --> 00:13:07,760
Uh, you know, DexNet, as you, as you know, was our grasping work and started with DexNet 1.0.

123
00:13:07,760 --> 00:13:15,200
And when all the way up to 4.0. And we're not working on 5.0 actually. Because what I like is that

124
00:13:15,200 --> 00:13:21,920
we take a problem and we really try to, to, to, to dig deeper and deeper into it. And in this case,

125
00:13:21,920 --> 00:13:27,840
it's, um, it's really looking at the failure modes, really trying to not be satisfied with the

126
00:13:27,840 --> 00:13:34,160
performance that, you know, and, and, and, and wanting to understand how can this be made better?

127
00:13:34,160 --> 00:13:39,200
How can we really push the envelope? How can we really reduce the failures? In this line of work,

128
00:13:39,200 --> 00:13:46,480
we, we systematically characterize the failures. So, you know, just as an aside, I see

129
00:13:46,480 --> 00:13:53,680
today so many papers that, you know, say, we've bit, we've beat the state of the art. And, uh, you

130
00:13:53,680 --> 00:14:00,480
know, thank you. This is, this is superior and end of story. Um, and it's very frustrating because

131
00:14:00,480 --> 00:14:04,400
I always want to know, well, where, how did you beat the state of the art? What, you know, just

132
00:14:04,400 --> 00:14:10,320
giving me a, a mean success rate doesn't tell me a lot. I want to know, did you, did you succeed on

133
00:14:10,320 --> 00:14:17,120
a number of cases where the prior out, you know, the baselines failed? Or what, what, what did you,

134
00:14:17,120 --> 00:14:23,040
where did you succeed? And where did you fail? And, and that failure, you know, it's interesting

135
00:14:23,040 --> 00:14:29,600
because we have a, I think an instinctive desire not to want to look at that. But it's actually

136
00:14:29,600 --> 00:14:35,440
where the most interesting aspects of the problem are. And so you really want to study those

137
00:14:35,440 --> 00:14:41,120
failure modes. And so we do, we, we, we, we, we characterize them into different categories. Every

138
00:14:41,120 --> 00:14:46,240
single case, we really determine what was the failure mode. Where did that one go wrong? And

139
00:14:46,240 --> 00:14:53,600
then we try and look at those and say, okay, how can we address that? So the, the, the, the ideas

140
00:14:53,600 --> 00:14:59,600
that in each case, we have to, to really develop new primitives and new primitives both for

141
00:14:59,600 --> 00:15:06,240
perception and for action. And then we try and think about how do we sequence between those

142
00:15:06,240 --> 00:15:13,040
primitives? So the, so these are the software aspects. One of the, one of the algorithmic aspect is

143
00:15:13,040 --> 00:15:20,320
learning the, to recognize just where knots are in images. And so we, we did that in real by

144
00:15:20,320 --> 00:15:27,120
sampling lots and lots of examples. We just have the, the system watching the cable and we do

145
00:15:27,120 --> 00:15:32,080
a certain amount of self supervised movement. So the cable, basically tie a knot in it. And then

146
00:15:32,080 --> 00:15:38,160
we allow the system to move itself, move the cable around, taking images over and over again.

147
00:15:38,160 --> 00:15:44,480
And then we, we either manually or what we hope to do is the more and more is to have a self

148
00:15:44,480 --> 00:15:49,520
supervised method that provides ground truth, labels, images, and then we can train a network.

149
00:15:49,520 --> 00:15:54,080
So that's been a core part of it. And that's, you know, that's a huge thing where deep learning

150
00:15:54,080 --> 00:16:01,440
has changed the equation over the last decade that we have that perceptual ability that, that,

151
00:16:01,440 --> 00:16:06,480
that really can solve some of the very, very complex perceptual problems where it's very hard

152
00:16:06,480 --> 00:16:11,040
to analytically determine what you're looking for. Right. And this is to determine what is a knot.

153
00:16:11,040 --> 00:16:16,080
It's actually very hard. One thing that also is interesting is there's a whole body of theory

154
00:16:16,080 --> 00:16:22,320
called knot theory. And mathematicians have been working on this for centuries. It's, it's,

155
00:16:22,320 --> 00:16:27,360
it's very interesting. It's, uh, it's where they, they begin by turning it into a, a graph.

156
00:16:28,560 --> 00:16:35,600
So they abstract away from all the geometry and, and the, the physics. But I'm very interested in,

157
00:16:35,600 --> 00:16:40,560
in combining these. And there's one technique that comes from, from, um, from knot theory,

158
00:16:40,560 --> 00:16:44,880
called a rate of mice to move that is actually analogous to what we do when we pull through

159
00:16:45,520 --> 00:16:49,440
the, uh, the cable from end to end. So we've been applying that. We've been applying new,

160
00:16:49,440 --> 00:16:55,760
new forms of, uh, of, of learning in particular. We're right now, again, trying to remove the,

161
00:16:55,760 --> 00:17:01,120
the reliance on the depth sensor and learn primitives that can determine, they can compensate for

162
00:17:01,120 --> 00:17:06,960
not having depth. And I'm also very interested in having human in the loop. And by that, I mean,

163
00:17:07,760 --> 00:17:16,240
very sporadically, um, um, when the system is stuck where it's not making progress or it determines

164
00:17:16,240 --> 00:17:21,440
that it's in a sufficiently uncertain state, it can call in a human for help.

165
00:17:22,880 --> 00:17:28,240
And this, this is, this is, this is interesting in its own right, in more general sense of,

166
00:17:28,800 --> 00:17:33,520
of when, how do you do this to move? You want to minimize the burden on the supervisor of the human.

167
00:17:34,960 --> 00:17:41,760
And by the way, this is, this is a, this is a, a widespread issue. For example, as you,

168
00:17:41,760 --> 00:17:47,520
as you know, Google is testing an automated, uh, car service, taxi service in the Bay Area.

169
00:17:48,320 --> 00:17:55,360
And my understanding is that they have humans, um, networked in and are standing by.

170
00:17:56,480 --> 00:18:02,240
Now you have one human, probably controlling multiple taxis. So now you have a question, how,

171
00:18:02,240 --> 00:18:08,320
when, when do you call that human in? Right. And you, you want to minimize that because,

172
00:18:08,320 --> 00:18:13,600
ideally, you can have one human supervising 50 taxis, right? So you don't need, you don't need

173
00:18:13,600 --> 00:18:21,200
the person that often. But in, in any robotics case, it can be very tedious to be constantly bothered,

174
00:18:21,200 --> 00:18:28,880
right? So there's a nice problem in when, when do you call human? And also when the human comes in,

175
00:18:28,880 --> 00:18:34,480
when do you transfer control back to the robot? So you, you have these nice dual problems,

176
00:18:34,480 --> 00:18:39,200
and they both have to do it in a sense, a model of confidence. So we've been looking at that. And

177
00:18:39,200 --> 00:18:44,080
I think that that applies to many of the kind of tasks we're interested in, where there are these

178
00:18:44,080 --> 00:18:49,920
failure modes that you're really unrecoverable, at least currently. And so that's where there's no

179
00:18:49,920 --> 00:18:54,960
harm. And, you know, maybe once an hour or so, you want to have a human come over just to just

180
00:18:54,960 --> 00:19:00,080
something and then, um, then, you know, go back to whatever they were doing. Yeah. When I, when I

181
00:19:00,080 --> 00:19:06,560
introduce you, I referenced that you're in the School of IE and OR, and it strikes me in your

182
00:19:06,560 --> 00:19:11,440
description of the, you know, this problem, there's also some interesting kind of classical OR

183
00:19:11,440 --> 00:19:17,200
queuing theory types of problems in there, you know, despite the, the degree to which I enjoyed

184
00:19:17,200 --> 00:19:22,240
working on that kind of stuff in grad school, I have not looked a whole lot into what's happening

185
00:19:22,240 --> 00:19:26,960
to marry machine learning and queuing theory. Do you know of anything interesting out there?

186
00:19:26,960 --> 00:19:32,880
Oh, well, it's quite a bit. I mean, queuing is a, is also a beautiful model. There's, it's,

187
00:19:32,880 --> 00:19:38,560
it's always used a Poisson distribution assumption. And a lot of nice theorems you can prove on that,

188
00:19:38,560 --> 00:19:45,280
but the reality is not, it doesn't behave that way. So how can you generalize that to real empirical

189
00:19:45,280 --> 00:19:51,280
objects or real empirical distributions? And I'm glad you mentioned, um, that you, you know, this

190
00:19:51,280 --> 00:19:58,080
connection with, with queuing, you know, in, in OR, you know, my colleagues say, well, we've

191
00:19:58,080 --> 00:20:04,720
been doing, you know, we've been doing AI for, uh, with the century now. I mean, what, because,

192
00:20:04,720 --> 00:20:11,360
because in some sense, you know, mark up decision problems, um, these have been the core of operations

193
00:20:11,360 --> 00:20:19,440
research for a very long time. And so those are early forms of AI and still, and being rediscovered

194
00:20:19,440 --> 00:20:25,280
in a way, especially where it comes to optimization, which is at the core of deep learning. And so many

195
00:20:25,280 --> 00:20:31,760
of the models that we're using now. So it, it's very natural to have connection between, uh, OR

196
00:20:31,760 --> 00:20:37,200
and AI. And the industrial engineering side of it comes with the other aspect, which is how do you

197
00:20:37,200 --> 00:20:43,680
make these systems practical? And that's where factors like what we're just talking about, the human

198
00:20:43,680 --> 00:20:50,640
interface come into play. You know, there's a, there's a distinction between robotics and automation.

199
00:20:51,760 --> 00:21:00,960
And robotics is obviously much more popular and, uh, enticing and the press labs and robots,

200
00:21:00,960 --> 00:21:06,960
et cetera. And so I've always been amused by the fact that, you know, if you start talking about

201
00:21:06,960 --> 00:21:12,240
automation, it's tends to be, you know, just, uh, that sounds like something, you know, in a,

202
00:21:12,240 --> 00:21:17,200
in a factory, I don't want to really talk about that. But if it's, is robotics, it's really exciting

203
00:21:17,200 --> 00:21:24,080
and energizing and, um, you know, it feels like, you know, science fiction. What's been happening,

204
00:21:24,080 --> 00:21:29,200
I think, in the last few years is that there's a trend toward automation because there's a

205
00:21:29,200 --> 00:21:34,480
recognition that we want to start putting things into practice. And that's where you have to worry

206
00:21:34,480 --> 00:21:41,840
about robustness. You have to put guarantees on performance. You want to worry about

207
00:21:41,840 --> 00:21:49,200
cost, reliability, all those, those factors that are, um, you know, often overlooked when you're

208
00:21:49,200 --> 00:21:53,200
just doing something in a lab. Yeah. Interesting. I thought you were going to go a totally

209
00:21:53,200 --> 00:22:02,080
different, uh, direction with that last, last comment. Um, people often will talk about software

210
00:22:02,080 --> 00:22:11,760
robots. Uh, and I, I'll ask you what, what your take is on that. But to me, like a robot, part of

211
00:22:12,880 --> 00:22:20,400
what fundamentally defines a robot is this bridging of the digital and the physical realms and

212
00:22:20,400 --> 00:22:24,960
something that, you know, purely exists in the digital realm. Unless we're talking about

213
00:22:24,960 --> 00:22:30,160
a simulation of something that exists in the physical realm, uh, I don't like calling those

214
00:22:30,160 --> 00:22:35,920
things robots. The software or it's, you know, some, you know, it's automation, uh, it's supposed to,

215
00:22:37,280 --> 00:22:41,760
you know, software robot. Oh, no, I agree. I can't, I couldn't agree more. I mean, I think that's

216
00:22:41,760 --> 00:22:47,920
actually, it's a misdomer. People often say bots, right? Oh, it's the, you know, bots took down

217
00:22:47,920 --> 00:22:55,200
this website, right? Because it was automated, um, some automated modules that would be able to

218
00:22:55,200 --> 00:23:02,560
do something, but they're just software, right? And they're, I think that is definitely a confusion.

219
00:23:02,560 --> 00:23:09,840
And I, I mean, to my mind, the robot has to have a physical component. In fact, this brings up

220
00:23:09,840 --> 00:23:17,120
another aspect, which is a lot of research has been done just in simulators and then demonstrated

221
00:23:17,120 --> 00:23:27,600
with simulation. And I think there's a danger there that if you, you can, you, you can almost have

222
00:23:27,600 --> 00:23:32,160
a self-fulfilling prophecy. You build the system, you build the simulator, you're retuned,

223
00:23:32,160 --> 00:23:37,120
you work with a simulator, you tune off of the simulator. It's very nice because it gives you

224
00:23:37,120 --> 00:23:44,080
the ability to do, to collect lots of huge amounts of data and you can do resets in the simulation.

225
00:23:44,080 --> 00:23:50,560
But if your simulation is even slightly deviates from reality, when you now take that policy and

226
00:23:50,560 --> 00:23:58,800
put it into practice, you have a performance can, can degrade, you know, dramatically. And so

227
00:23:58,800 --> 00:24:06,400
a lot of the early mijoco demonstrations of walking machines, etc. looked great and they

228
00:24:06,400 --> 00:24:14,000
look beautiful and just surprising how fast they would learn. But then they would not easily transfer

229
00:24:14,000 --> 00:24:23,040
into real machines. So this is the, you know, the, the, the sim to real gap that I think is so

230
00:24:23,040 --> 00:24:29,280
interesting right now. And it is really important to recognize. And you've been doing a bunch of work

231
00:24:29,280 --> 00:24:35,120
in that area as well, you and your, your live feed talk a little bit more about kind of how you

232
00:24:35,120 --> 00:24:40,640
characterize that sim and real gap as a set of research problems and some of the specifics that

233
00:24:40,640 --> 00:24:46,000
you've been working on. Sure. One of the things that I've always been interested in is this,

234
00:24:46,880 --> 00:24:56,960
is the, is the limitations of simulation and in grasping. And I talk about the, the very real problem

235
00:24:56,960 --> 00:25:06,320
of, of the, being able to, there's essentially indeterminacies in physics that are due to friction.

236
00:25:06,320 --> 00:25:12,400
And the example I always like to point out is just pushing as a pencil across your, your,

237
00:25:12,400 --> 00:25:18,320
your desk. And if you, if you do that with just put your index finger and you do, you do, you

238
00:25:18,320 --> 00:25:25,600
do that repeatedly, the position of the pencil will be very different. And so in, it's a chaotic

239
00:25:25,600 --> 00:25:33,360
system. It's basically based on the, the, this complex surface physics, the surface topography.

240
00:25:34,000 --> 00:25:40,560
And that is very difficult. It's not, it changes every single time you, you perform this. So

241
00:25:40,560 --> 00:25:46,880
in a sense, it's impossible to predict how that pencil is going to, with the final state of

242
00:25:46,880 --> 00:25:52,480
the pencil. It's, it's, it's, it's, it's, it's undecidable. I feel like, I think we talked about

243
00:25:52,480 --> 00:25:57,200
this in a fair amount of detail last time. All right. You have a good memory. I, I, I know,

244
00:25:57,200 --> 00:26:02,000
I don't want to repeat myself, but I wasn't saying that because you're repeating yourself. I'm,

245
00:26:02,000 --> 00:26:09,680
I was more saying that because there, there's a part of me that wants to, you know, get into a

246
00:26:09,680 --> 00:26:15,040
philosophical argument about it. And I'm wondering if we, if I got us into that philosophical argument

247
00:26:15,040 --> 00:26:21,520
at last time, you know, the basic question being, is it kind of practically chaotic because we,

248
00:26:21,520 --> 00:26:28,720
we don't have the resolution to incorporate the fluctuations in the surface and the dust particles

249
00:26:28,720 --> 00:26:36,240
and all these things? Or is it, you know, if we could do, if we could capture the microscopic

250
00:26:36,240 --> 00:26:43,360
physics, would we then have a deterministic system? Or are the, whether, whether always be

251
00:26:44,400 --> 00:26:48,720
some element that we can capture, you know, humidity, temperature, what have you?

252
00:26:48,720 --> 00:26:54,480
Mm-hmm. Mm-hmm. No, I, I, I, I love that. We could probably talk for an hour just on that.

253
00:26:54,480 --> 00:27:00,080
I've been using the, the term, thinking about the terms epistemic and aleatory uncertainty.

254
00:27:00,080 --> 00:27:05,200
And this is exactly what you're talking about. So the epistemic is that we just need to model

255
00:27:05,200 --> 00:27:09,600
this better. We have those aspects we don't, we, we don't currently know, but aleatory is what's

256
00:27:09,600 --> 00:27:15,920
inherently uncertain. And that's, you know, it's the same for throwing a dice, right? You, you,

257
00:27:15,920 --> 00:27:20,080
if you're having better and better models, you're still not going to know how, you know,

258
00:27:20,080 --> 00:27:25,760
being able to predict that with certainty is, you know, inherently uncertain. Now, at some point,

259
00:27:25,760 --> 00:27:30,560
you get down to the, uh, the subatomic level and you back to the Einstein and God does not play

260
00:27:30,560 --> 00:27:35,840
dice at the universe, right? So, but, but, but, but any, but any practical sense, you're never going

261
00:27:35,840 --> 00:27:41,120
to be able to predict the position of that pencil. And so you have this, the, the reason I say this

262
00:27:41,120 --> 00:27:46,560
is it's not, doesn't mean that you can't do it. People do it all the time. We pick up pencils.

263
00:27:46,560 --> 00:27:51,760
So what's going on? What's missing? And I think that robots and simulators have a problem because

264
00:27:51,760 --> 00:27:58,000
they're very, they're deterministic. They, the, the simulation has one outcome. And if you perform

265
00:27:58,000 --> 00:28:03,120
the same thing over over, it's doing the same thing. So you tend to, to, to have a system of policy

266
00:28:03,120 --> 00:28:08,800
that's trained on that particular outcome, but it's not trained to be robust to those variations.

267
00:28:08,800 --> 00:28:15,680
Now, the, the trick to doing that is in this idea of domain randomization, right? Which is where

268
00:28:15,680 --> 00:28:21,360
you randomize the outputs in the simulator, but you want to do that very, very methodically.

269
00:28:22,320 --> 00:28:29,680
So the, the, the, the trick there is to have the simulator have ranges of outputs that are

270
00:28:29,680 --> 00:28:35,360
consistent with what you would see and simulate in reality. So this is why what we've been talking

271
00:28:35,360 --> 00:28:43,280
about recently is, is, is real to sim. And we learned this by, through a, a project that we were

272
00:28:43,280 --> 00:28:49,840
doing on, it was very, very, also working with cables, but here the problem is to what we call

273
00:28:49,840 --> 00:28:56,800
cleaner robot casting. And so here you have a robot with a, a weight on the end of a cable.

274
00:28:57,520 --> 00:29:03,520
And the robot is holding the cable above the surface and it basically casts the, the cable out,

275
00:29:03,520 --> 00:29:09,440
like you would a fishing rod, fishing lure. And then you pick a target somewhere on the surface

276
00:29:09,440 --> 00:29:14,400
above where the cable has landed. And then you want the robot to do a motion that will cause the

277
00:29:14,400 --> 00:29:19,440
cable to wind up, but the endpoint to wind up at a particular target point, meaning when it's pulling

278
00:29:19,440 --> 00:29:24,640
it back or when it's casting it out. No, when it's pulling it back. So you want to sort of, you want

279
00:29:24,640 --> 00:29:30,400
to sort of motion like this that will cause a dynamic motion towards a land in, in this particular

280
00:29:30,400 --> 00:29:35,840
point. The, the nice thing is you can do a lot of supervised data collection, self-superbite data

281
00:29:35,840 --> 00:29:40,480
collection. So the system, we have a camera overhead, we have this cable set up and this system

282
00:29:40,480 --> 00:29:48,400
just basically does this, you know, all day long. So we have a, a data on the input control that

283
00:29:48,400 --> 00:29:54,080
we give to the robot and where did the actual cable land. And one thing is there's, there's this

284
00:29:54,080 --> 00:29:58,160
aleatory uncertainty and you can measure it because you give it exactly the same cable motion

285
00:29:58,160 --> 00:30:03,280
and the, the, the endpoint lands in different places, right? And so we can actually draw

286
00:30:03,280 --> 00:30:08,480
an ellipse around that and say, this is the uncertainty that's inherent. Even though this can run

287
00:30:08,480 --> 00:30:14,720
all day long, it's still only capable of generating a few thousand examples. You really need

288
00:30:14,720 --> 00:30:22,400
many more to train a reliable policy. So we wanted to use a simulator. Now, what we found was

289
00:30:22,400 --> 00:30:27,120
that the simulators, they're the number of simulation packages that can do this. Mojoko, Isaac,

290
00:30:27,120 --> 00:30:32,800
Sim and others. And they all look good. They all kind of, when you look at them, they look very

291
00:30:32,800 --> 00:30:36,720
similar to what we're seeing in the physical space. But then we try to actually give it the true

292
00:30:36,720 --> 00:30:43,200
parameters of what we're measuring. And there's a, there's a deviation, right? So this is where

293
00:30:43,200 --> 00:30:52,800
the question is, how do we tune that simulator to closely match reality? So that's the real

294
00:30:52,800 --> 00:30:57,600
system. So you see the difference because if you just use Mojoko and you try to have a walking

295
00:30:57,600 --> 00:31:02,160
machine and you just start with the simulator, right? You, you get this thing and it trains and

296
00:31:02,160 --> 00:31:06,320
it runs and then you say, okay, now I've got a policy. Let me go pull it and try it on a real robot

297
00:31:06,320 --> 00:31:10,880
and it doesn't work. But if you start out by saying, I have a real robot and now I want to make a

298
00:31:10,880 --> 00:31:18,720
simulator that really mirrors what's going on with that, um, that, that robot. And this is

299
00:31:18,720 --> 00:31:24,640
closely related. Maybe it's, it's very similar to the idea of the digital twin. That's very popular

300
00:31:24,640 --> 00:31:33,520
now. And the key is how do you actually do this tuning systematically? And people call that system

301
00:31:33,520 --> 00:31:39,440
identification, right? That's a very common term. But there's a lot of misconceptions about that.

302
00:31:39,440 --> 00:31:44,880
System ID is, uh, is fairly well understood if you have, if you know the structure of the system,

303
00:31:44,880 --> 00:31:48,880
you know, if you have equations that decide the system like a pendulum and you want to identify

304
00:31:48,880 --> 00:31:54,240
what is the mass at the tip of that pendulum, then system ID is very good for telling you that.

305
00:31:55,040 --> 00:32:00,960
But if you have a system like this, this, uh, piece of cable and, uh, and the frictional

306
00:32:00,960 --> 00:32:06,640
interactions of something sliding across the surface, then you don't have a, you don't have a structural

307
00:32:06,640 --> 00:32:11,760
mark. And so it's very hard to figure out what should the, what should the parameters be in the

308
00:32:11,760 --> 00:32:16,400
simulation? By the way, simulation has a lot of parameters. There's, there's, um, things like

309
00:32:16,400 --> 00:32:21,360
torsion of the cable, there's friction of everything. There's inner, you know, when the cable

310
00:32:21,360 --> 00:32:25,040
rubs against itself, there's another frictional property, another frictional parameter for that.

311
00:32:25,760 --> 00:32:31,440
So there's a dozen or more. And now you have a nice optimization problem because you have a

312
00:32:31,440 --> 00:32:35,120
bunch of data that you've collected in real and you want to tune the simulator to match that.

313
00:32:36,400 --> 00:32:40,960
So, um, so we've been exploring that in this context that we actually turned out that in,

314
00:32:40,960 --> 00:32:45,840
we have a paper real to sim to real. We start out with real, we tune the simulator, then we can

315
00:32:45,840 --> 00:32:51,200
generate lots of examples. We combine that with the, the limited number of samples we got in real

316
00:32:51,200 --> 00:32:56,720
and then train a policy and then bring that back into real. And so, and that seems to perform

317
00:32:57,440 --> 00:33:04,000
better, much better than if we just use a very limited amount of real data, which is all we can get,

318
00:33:04,000 --> 00:33:10,560
or if we just use all the simulated data that wasn't tuned to the real system.

319
00:33:10,560 --> 00:33:15,360
So, I've been excited about this because I think it applies to so many problems that we're

320
00:33:15,360 --> 00:33:22,400
looking at in robotics, where we look at a, we have these, these systems and we really want

321
00:33:22,400 --> 00:33:28,240
to have a simulator that's, that's very physical accurate. And what's also been very interesting,

322
00:33:28,240 --> 00:33:35,600
as you, as you know, is that, um, Nvidia has made a major push in simulation. So they've got a huge

323
00:33:35,600 --> 00:33:42,400
team of various student researchers developing Isaac Sim and Variant and also thinking about how to

324
00:33:42,400 --> 00:33:51,760
make those run very fast. And in parallel, um, deep mind acquired Mujoko last year. And that was,

325
00:33:51,760 --> 00:33:59,840
it was almost exactly a year ago, that was a major, major milestone because Mujoko, um, was,

326
00:33:59,840 --> 00:34:04,320
was a, was a very good system, but it was run by a fairly small team. Now it went into deep mind,

327
00:34:04,320 --> 00:34:13,120
which has, you know, much more resources. And they've assembled a fantastic team of physicists

328
00:34:13,120 --> 00:34:20,560
and researchers to basically take Mujoko to an entirely new level of realism. So it's been fantastic

329
00:34:20,560 --> 00:34:25,520
to have these two projects coming along, where they're both getting the simulators better and better.

330
00:34:26,480 --> 00:34:30,960
And that is, I think, is going to lead to major breakthroughs in the field.

331
00:34:30,960 --> 00:34:38,160
You know, along these lines, I wonder if you have been involved in exploring the

332
00:34:40,000 --> 00:34:45,920
possible impact of causal based models here. I'm thinking about the ellipse that you're

333
00:34:45,920 --> 00:34:51,440
describing around kind of some ideal point, you know, where you're casting back to. And you've got

334
00:34:52,160 --> 00:34:57,680
a bunch of different sources of possible, you know, call it noise. You know, you've got

335
00:34:57,680 --> 00:35:03,760
measurement noise, you've got control noise, various other things. You know, is there some kind of

336
00:35:04,960 --> 00:35:11,120
discot that are folks looking at causality as a way to understand that, you know, how these

337
00:35:11,120 --> 00:35:16,640
inputs combine to create uncertainty and to create better models in the robotic realm?

338
00:35:17,200 --> 00:35:24,080
Well, I would have to say one, one answer to that is by trying to figure out how to optimize

339
00:35:24,080 --> 00:35:32,240
the tuning process. And that is that there is a causality inherent in controllable in the sense that

340
00:35:32,240 --> 00:35:39,040
that the robot is able to change its parameters. And you want to be able to do that systematically.

341
00:35:39,600 --> 00:35:45,440
So one way to do it, let's just take the planer robot casting is you pretty much randomly generate

342
00:35:45,440 --> 00:35:52,000
a lot of control inputs, trajectories for the arm. And then you just observe where the end point

343
00:35:52,000 --> 00:35:59,040
of this cable winds up on the surface. Now, you can just generate a big data set and then throw it

344
00:35:59,040 --> 00:36:06,080
in and then try and analyze that to come up with a model. A better way is to do that systematically

345
00:36:06,080 --> 00:36:14,240
where you start doing some random examples, but then you start testing basically values of those

346
00:36:14,240 --> 00:36:21,360
parameters and going back out into the real system and fine tuning it so that also regions of

347
00:36:21,360 --> 00:36:28,080
the state space that you hadn't explored earlier, you can or you haven't explored sufficiently,

348
00:36:28,080 --> 00:36:33,200
you can you can reevaluate and put more, do more experiments in that area.

349
00:36:34,080 --> 00:36:39,680
So that I think is really interesting where the experiments in real are costly, if you will.

350
00:36:39,680 --> 00:36:46,640
They require time and an offer and it's very difficult to reset the system to obtain

351
00:36:46,640 --> 00:36:55,360
the same input to run it again, right? But it's very, so you want to be thoughtful and systematic

352
00:36:55,360 --> 00:37:00,640
and this is related to the theory of the design of experiments. And typically you're trying to

353
00:37:00,640 --> 00:37:06,400
maximize some kind of mutual information gain that you will, by doing this experiments,

354
00:37:06,400 --> 00:37:10,560
you want to choose the experiment that's going to gain you the most information. But it turns out

355
00:37:10,560 --> 00:37:18,560
computing that solving that is a very difficult problem. So there's so many interesting open

356
00:37:18,560 --> 00:37:27,440
problems here and it's very exciting to see how the field is is maturing. I think robotics is

357
00:37:28,480 --> 00:37:35,600
is moving at a remarkable pace, but it's still from the public perception very far from what

358
00:37:35,600 --> 00:37:43,600
what people commonly, you know, commonly think should happen. And people are still thinking, well,

359
00:37:43,600 --> 00:37:48,080
the robot, you know, why don't we have our robot drivers? Why don't we have our robot,

360
00:37:49,200 --> 00:37:55,600
you know, robots in the kitchen and robots at home taking care of us? And these things are

361
00:37:55,600 --> 00:38:03,040
still very far off, unfortunately. Maybe digging into that a little bit where maybe three weeks beyond

362
00:38:03,040 --> 00:38:13,040
Tesla's Optimus Robot announcement, which is, you know, causing people to ask the question again,

363
00:38:13,040 --> 00:38:21,680
oh, hey, are we close to our robot in the home? Elon says we are. What's your take on Optimus? What

364
00:38:21,680 --> 00:38:28,320
was really demonstrated there, you know, the extent to which it demonstrates that we're close to,

365
00:38:28,320 --> 00:38:35,600
you know, practical everyday robotics? Okay, so I do have a take on this. I was very interested.

366
00:38:35,600 --> 00:38:41,840
I watched it right as it came out and it was it was very interesting. I mean, I do have to hand it

367
00:38:41,840 --> 00:38:53,840
to Elon Musk. He's a great entertainer. He has a real knack for doing things and doing stunts

368
00:38:53,840 --> 00:39:00,880
and basically having ideas that are really, you know, out there. But he's also, you know,

369
00:39:00,880 --> 00:39:07,360
has been a visionary. He has actually has succeeded in certain categories. So he's done with

370
00:39:08,400 --> 00:39:16,560
electric cars and with Tesla, with in terms of batteries, in terms of space and landing

371
00:39:16,560 --> 00:39:24,320
an aircraft or a rocket back on the earth, that's remarkable. And those are, you have to put

372
00:39:24,320 --> 00:39:29,120
those into context of everything else that he's doing. And so I have to say, my first reaction is

373
00:39:29,920 --> 00:39:39,120
was, look, you know, what's going on here? He is, he's probably keenly aware that the price

374
00:39:39,120 --> 00:39:45,520
earnings ratio of an auto company is, you know, kind of at one level, but the price earnings ratio

375
00:39:45,520 --> 00:39:52,560
of a robotics company is much higher. So if he transforms Tesla into a robotics company,

376
00:39:52,560 --> 00:40:00,160
there's a very clear benefit for that. Right. So that could be one, one part of what he's thinking.

377
00:40:00,160 --> 00:40:06,320
But I think what's also going on is that he's saying that he really is putting his, you know,

378
00:40:06,320 --> 00:40:14,400
put it putting out there, you know, a bold new idea. And he's not afraid to take risks like that.

379
00:40:14,400 --> 00:40:21,040
Now, what I was happy to see was that, you know, that robot was, was substantial progress from

380
00:40:22,080 --> 00:40:27,680
the past year when they had first announced it. And I remember when he very first announced it,

381
00:40:27,680 --> 00:40:34,080
I thought, what are you talking about a humanoid? No, that's not going to happen. But he has,

382
00:40:34,080 --> 00:40:41,680
he's really put real research behind it. Now, it was, it fell short of anyone's expectation,

383
00:40:41,680 --> 00:40:48,160
if you know about what's going on with agility, robotics, and, and, and, and Boston Dynamics.

384
00:40:48,880 --> 00:40:57,760
But it was, it is a start. I think that the one thing I'm very excited about is that he understands

385
00:40:57,760 --> 00:41:02,960
the this aspect of automation in the sense that it has to make something that's going to run

386
00:41:03,440 --> 00:41:09,280
reliably and cost-effective. So when anybody has been building humanoid over the past

387
00:41:09,280 --> 00:41:15,440
three decades, nobody's really talked about the cost effectiveness of that, right? But he was,

388
00:41:15,440 --> 00:41:20,720
he went out and said $20,000, right? Okay. Well, what does that mean? That means he's going to have

389
00:41:20,720 --> 00:41:25,520
to develop some new motors that are, because he's got a lot of motors in the system. What would you

390
00:41:25,520 --> 00:41:35,680
say the, the list price is on the analog? Oh, the Alex? Oh, it's, I, I, I haven't, I haven't seen,

391
00:41:35,680 --> 00:41:41,440
but it's, it's probably over 200,000. Order of magnitude, a couple order of magnitude, maybe?

392
00:41:41,440 --> 00:41:47,840
Yes. And the, the thing is that these are, you know, you have to amortize all the research

393
00:41:47,840 --> 00:41:52,400
over, over the, over the volume. So if you have, if you're arguably able to produce in volume,

394
00:41:52,400 --> 00:42:00,000
you can do this. The, the challenges in designing new motors, gear trains, sensors that, these

395
00:42:00,000 --> 00:42:05,200
are all things we actually need in robotics. You know, arms, there's a number of arms out there,

396
00:42:05,200 --> 00:42:14,080
but they're really, they're all still, um, fairly expensive and either imprecise or, um, or,

397
00:42:14,080 --> 00:42:20,400
or dangerous, right? So I think there's, I would, I would love to see, and I think Tesla is in a

398
00:42:20,400 --> 00:42:24,960
perfect position to do this, is that they would come out with a new line of motors, and those

399
00:42:24,960 --> 00:42:29,360
could be used for robots. They might even come out, here's something I'd be saying. I think they

400
00:42:29,360 --> 00:42:35,600
could come out with an industrial robot arm, that, um, that Tesla would come out with a new arm,

401
00:42:35,600 --> 00:42:42,480
that would be, that would actually be very high precision, low cost, low mass, and, and we need that.

402
00:42:42,480 --> 00:42:47,680
So particularly if it's one that's based on their own needs and experience as a manufacturer.

403
00:42:47,680 --> 00:42:52,480
Exactly. Exactly. Because that, that's right. So he has a use case right there, right? And there's

404
00:42:52,480 --> 00:42:57,680
all kinds of aspects of the, and they tried to automate with, with existing robots and had

405
00:42:57,680 --> 00:43:02,640
number of challenges. If they build their own robots now, that's very, that really changes

406
00:43:02,640 --> 00:43:07,920
the equation. No, very, no one else, no other company has that big of a use case and production

407
00:43:07,920 --> 00:43:13,600
capability. So they could do it. And the other is sensors. You know, we're just talking about

408
00:43:13,600 --> 00:43:18,880
the connect and all the lines of 3D sensors. They could really put their muscle behind a really

409
00:43:18,880 --> 00:43:27,280
nice compact, uh, uh, sensor that could be, they could give us 3D or very fast 2D sensing.

410
00:43:27,280 --> 00:43:33,280
And that would be very bad. Tactile sensing. By the way, that's also heated up since the last

411
00:43:33,280 --> 00:43:42,400
time we talked that Facebook has really developed a partnership with, um, with Jelsight and come out

412
00:43:42,400 --> 00:43:50,480
with the, um, digit, um, tactile sensor. And that is very interesting. By the way, that's a big

413
00:43:50,480 --> 00:43:57,040
breakthrough. Um, and we've been using that. And now Jelsight just announced a new version of

414
00:43:57,040 --> 00:44:04,480
their sensor, higher resolution. And they're faster. Um, we're very interested in this is like a new

415
00:44:04,480 --> 00:44:10,320
generation of tactile sensing that I think we're going to see lots of applications. Okay. That's

416
00:44:10,320 --> 00:44:14,960
super interesting. I, we may have talked about this last time. I remember as a kid taking, uh,

417
00:44:15,920 --> 00:44:20,480
Paisio electric foam or something like that and slapping it between a couple of circuit boards

418
00:44:20,480 --> 00:44:27,520
printed on one side and using that as like a touch-ish pressure sensor. Exactly. Exactly. We,

419
00:44:27,520 --> 00:44:31,920
you know, good memory. I, I, we did talk about that because that was where I got started as an

420
00:44:31,920 --> 00:44:37,040
undergrad doing trying to build touch sensors. And it's, it's that, you know, you don't have that

421
00:44:37,040 --> 00:44:42,320
yet. And the Jelsight is, is kind of another breakthrough just as connect was that starts to make

422
00:44:42,320 --> 00:44:48,240
that more, more feasible. And because it's using optics, it's kind of riding the, the curve of,

423
00:44:48,240 --> 00:44:57,680
of advances in, in cameras. So it sounds like, uh, kind of the, the, the, to net out your take on

424
00:44:57,680 --> 00:45:03,600
optimists. There's some interesting things that you hope grow out of it. Um, but you didn't

425
00:45:03,600 --> 00:45:10,480
necessarily see anything that, uh, you know, if you were Boston dynamics would make you fear for

426
00:45:10,480 --> 00:45:16,160
the future of your, your own company. No, but I would have to say, I think, well, I think it's good

427
00:45:16,160 --> 00:45:23,360
for the field. So when you have, you know, you have someone with that level of attention

428
00:45:23,920 --> 00:45:29,840
and, and, and mind share, coming out and saying robotics is where we're going to make major

429
00:45:29,840 --> 00:45:36,880
advances. That is good for the whole field. I think it, it talks to young engineers who, you know,

430
00:45:36,880 --> 00:45:42,880
want to take a robotics class or want to maybe go into the field. It also, it speaks to investors

431
00:45:42,880 --> 00:45:47,200
who are, you know, look at his track record and say, hey, maybe he's going to pull us off.

432
00:45:47,920 --> 00:45:52,720
And I think, I think it would be, I don't, I think a bit very unwise to bet against him.

433
00:45:53,360 --> 00:45:57,360
In other words, I'm not saying he's going to come out with a, with a practical humanoid.

434
00:45:57,360 --> 00:46:01,440
I don't think that's, I think he's going to quickly discover how complicated that is.

435
00:46:01,440 --> 00:46:06,000
Yeah, I think that's what, uh, what I'm kind of getting at or trying to get your take on it.

436
00:46:06,000 --> 00:46:11,120
Well, he, you know, I was, I was joking that, you know, uh, it's true, rocket, robotics isn't

437
00:46:11,120 --> 00:46:17,840
rocket science. It's, it's actually, it's much harder. Um, and, and by that, you know, because

438
00:46:17,840 --> 00:46:22,640
this comes back to the things we were talking about, you know, um, basically doing a landing

439
00:46:22,640 --> 00:46:28,560
of a, of a rocket, um, back on, you know, that stabilizing that is a beautiful control problem.

440
00:46:28,560 --> 00:46:34,000
But there's, the, there's only contact at the end of that. In relation, you have continuous

441
00:46:34,000 --> 00:46:40,240
contacts. And those are very difficult and non deterministic for all the reasons we talked

442
00:46:40,240 --> 00:46:46,480
about. So that, that problem is technically harder. And so getting that right is going to require

443
00:46:46,480 --> 00:46:51,600
the next generation. I mean, we're, that's what I'm excited about. So yeah, because I do feel like

444
00:46:51,600 --> 00:46:56,320
we're at the point where the lot of vectors are lining up that we're going, we're going to see

445
00:46:56,320 --> 00:47:03,360
progress and having someone like Elon and his, you know, army of supporters is, uh, is a great

446
00:47:03,360 --> 00:47:09,440
thing for the field. Mm hmm. Uh, I guess one more topic I want to take on before we wrap up,

447
00:47:09,440 --> 00:47:15,600
you're the chief scientist at Ambi Robotics. Uh, what is Ambi up to and, and how is it pushing

448
00:47:15,600 --> 00:47:22,560
the field forward? Well, I've been very impressed with the team, um, at Ambi. The, since we started,

449
00:47:24,080 --> 00:47:30,240
three years ago, the, the team has just been absolutely fantastic, very, very laser focused,

450
00:47:30,240 --> 00:47:39,040
Jeff Mueller as the, as the, the, um, the chief technology officer and really the mastermind

451
00:47:39,040 --> 00:47:45,520
behind Dexnet, he has been leading the technical team and, um, on the software side. And then Steve

452
00:47:45,520 --> 00:47:53,680
McKinley and David Geely have been working on the hardware side. So it's a blend of, of, of very

453
00:47:53,680 --> 00:48:01,600
elegant new software and hardware that are coming together in these systems. And, um, the, the,

454
00:48:01,600 --> 00:48:10,000
the CEO who Jim Leifer has this incredible background in the, in logistics. So this actually

455
00:48:10,000 --> 00:48:16,720
also has happened since you and I talked last. Jim has been, you know, a history working in Wal-Mart

456
00:48:16,720 --> 00:48:22,800
and working with, um, with a number of, uh, of logistics companies. So he really understands

457
00:48:22,800 --> 00:48:28,800
the real problems. And then they, we've been working with the company, uh, named Pitney Bose,

458
00:48:29,840 --> 00:48:40,320
who is a, you know them? Uh, yep. I, uh, I worked at Pitney Bose, uh, as an undergrad for a co-op.

459
00:48:40,320 --> 00:48:48,880
I was doing, um, I was doing, uh, I forget the term, like essentially laying out, uh, custom,

460
00:48:48,880 --> 00:48:55,120
um, custom chips. Really? Oh my God. So you know Pitney Bose. Pitney is very interesting. They're,

461
00:48:55,120 --> 00:48:59,920
they're an old company. They've been a old-school company. Postal, yeah. Postal leaders,

462
00:48:59,920 --> 00:49:04,720
1920. So they just celebrated their 100-year anniversary. And what's been interesting is,

463
00:49:04,720 --> 00:49:08,480
when you look at them, they've, they've always been looking at technology for postage in various

464
00:49:08,480 --> 00:49:14,640
ways. And they, um, they, they, but they, what they do is they, they're sort of behind the scenes

465
00:49:14,640 --> 00:49:19,360
of a lot of the postal sorting systems around the country. And so they installed them for the US

466
00:49:19,360 --> 00:49:26,320
Postal Service, for UPS, for, for FedEx and others. So they really know this technology. And it's been

467
00:49:26,320 --> 00:49:32,000
a pleasure to work with them because they, they're really true engineers. They really try to solve

468
00:49:32,000 --> 00:49:38,480
problems. And so, um, partnering with them has been terrific because we're, we work side-by-side,

469
00:49:38,480 --> 00:49:45,600
where they've, they've essentially, um, you know, become our, our biggest customer. And we,

470
00:49:45,600 --> 00:49:53,040
you know, we're installed 60 of our systems over the summer all across America. And they just

471
00:49:53,040 --> 00:50:00,160
invested in us. So what are the system? Is it, uh, is it a, a hardware system? Yes. The system

472
00:50:00,160 --> 00:50:06,800
is hardware software. It's called ambiSort. And what it does is it takes bins of packages and

473
00:50:06,800 --> 00:50:13,120
sort them into smaller bins according to zip code. So sorting is a little different than just grasping.

474
00:50:13,120 --> 00:50:18,560
You have to grasp the object and then scan it, determine what the code it is, put it on another

475
00:50:18,560 --> 00:50:24,320
a gantry robot that then puts it out to, uh, and drops it into a bin. So there's a quite a bit of

476
00:50:24,320 --> 00:50:29,040
hardware. In fact, it's, it's sort of the size of a living room. Um, it actually fills up at 18

477
00:50:29,040 --> 00:50:36,720
wheel truck. That's, that's the, and what's the form factor of the robot? Is it, uh, an arm type of form

478
00:50:36,720 --> 00:50:42,640
factor or? Yes. So there's a, the standard six-degree of freedom industrial arm at the front of it.

479
00:50:42,640 --> 00:50:48,480
It's picking things up. And then there's a gantry type robot. Like I think of it x, y, um, system that,

480
00:50:48,480 --> 00:50:53,920
that has a pivot that drops the package into the appropriate bin. So that whole system is called

481
00:50:53,920 --> 00:51:02,400
ambi sort and involves lots of cameras, lots of safety features, lots of, of, of, of, of, failsafe

482
00:51:02,400 --> 00:51:08,720
features. It's a, it's a big operation. It's got thousands of parts. But that is, those are the

483
00:51:08,720 --> 00:51:15,200
systems we're talking about. So each one of those can, can, can basically sort through hundreds of

484
00:51:15,200 --> 00:51:22,480
parcels per hour. And that is a big, um, it's an interesting challenge because it's very hard. This

485
00:51:22,480 --> 00:51:30,240
is a, can be back breaking work. Humans are, are prone to making a lot of mistakes. Um, and people,

486
00:51:30,240 --> 00:51:36,400
the turnover in these warehouses is enormous. Uh, you know, all the companies, now Amazon is very

487
00:51:36,400 --> 00:51:43,120
big on this. They're starting to, to try to, um, find ways to automate this. And this is really our

488
00:51:43,120 --> 00:51:53,040
focus. Awesome. Awesome. Um, and the, uh, you know, for that, for, for packages, you often see,

489
00:51:53,760 --> 00:52:00,240
you know, as opposed to hand, uh, you know, gripper types of actuators like suction actuators and

490
00:52:00,240 --> 00:52:06,800
other things. You mentioned Dexnet. So I'm assuming, uh, you're doing kind of more of a gripper type of

491
00:52:06,800 --> 00:52:14,000
actuator. Well, a great question. So Dexnet 3.0 was, uh, suction, where we, we took the same,

492
00:52:14,000 --> 00:52:19,200
same idea and applied it to the suction model. In, in a sense, you have a gripper is a two-point

493
00:52:19,200 --> 00:52:24,240
contact. You have to find two points on the object of pair. In suction, you have to find one.

494
00:52:25,120 --> 00:52:31,760
And so it's just one point contact. But the, but the physics are very different. And so the

495
00:52:31,760 --> 00:52:38,400
resistance to shear forces, for example, are much lower for a suction cup than a gripper. So in

496
00:52:38,400 --> 00:52:45,520
the, the suction cup can be, is, is actually the, the workhorse for this kind of work in industry.

497
00:52:45,520 --> 00:52:53,040
And we can extend Dexnet, um, in a number of ways to make it work in this context. And that's

498
00:52:53,040 --> 00:52:59,280
really been where the team has been pushing the envelope. And we also collect data from every one

499
00:52:59,280 --> 00:53:04,800
of these systems. So it's, it's a, it's a wonderful problem from a machine learning point of view.

500
00:53:04,800 --> 00:53:08,960
Because we have data sets, we have images, we have sensor values, we have all this, we can

501
00:53:09,520 --> 00:53:16,640
characterize every single failure. And analyze it. And then try back testing different algorithms

502
00:53:16,640 --> 00:53:22,080
to be able to reduce those. And that's, that's where there's a, there's a huge opportunity. Because

503
00:53:22,720 --> 00:53:28,240
really, there's this gap. Can we start closing it to really increase the, the throughput,

504
00:53:28,240 --> 00:53:36,560
the pigs per hour? Awesome. Awesome. Uh, well, Ken, I think, uh, we covered a ton of ground,

505
00:53:37,120 --> 00:53:42,800
but also demonstrated that it's really hard to stick to in a half years of robotics,

506
00:53:43,440 --> 00:53:49,200
advancement and innovation in an hour. Uh, so I think that just means we'll have to be sure to

507
00:53:49,200 --> 00:53:53,600
catch up more frequently in the future. Ah, I would love that. Thanks Sam. I have to say I've been

508
00:53:53,600 --> 00:54:00,160
such a pleasure because I listened to your podcast on my bike rides, um, mountain biking. And so

509
00:54:00,160 --> 00:54:07,200
I've just enjoyed them so, so many good hours on a bike, uh, with you. And, um, last thing I want

510
00:54:07,200 --> 00:54:12,000
to say, I don't know if that, when this will air, but the conference on robot learning is going to be

511
00:54:12,000 --> 00:54:20,480
in New Zealand this, in, in December, 14 through the 18th. And it is, um, so I'm, I'm chairing the

512
00:54:20,480 --> 00:54:27,200
conference and we've been, it's been a real pleasure. We have 500 papers submitted a top-notch group

513
00:54:27,200 --> 00:54:34,320
of about 200 papers will be presented there. And you can register as an online, um, online to watch

514
00:54:34,320 --> 00:54:40,240
all the talks and everything else, uh, for, I think it's like close to $200, not too expensive.

515
00:54:40,240 --> 00:54:45,600
And then I will tell you your audience that we're also going to make all this available offline

516
00:54:45,600 --> 00:54:50,720
after the conference. Awesome. Uh, thank you so much, Sam. I really appreciate your great

517
00:54:50,720 --> 00:55:17,680
heartbeat. Thanks so much, Ken. All right. Take care.

