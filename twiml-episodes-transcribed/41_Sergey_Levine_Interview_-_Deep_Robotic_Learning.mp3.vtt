WEBVTT

00:00.000 --> 00:15.880
Hello and welcome to another episode of Twimal Talk, the podcast where I interview interesting

00:15.880 --> 00:20.840
people, doing interesting things in machine learning and artificial intelligence.

00:20.840 --> 00:23.240
I'm your host Sam Charrington.

00:23.240 --> 00:29.000
This week we continue our industrial AI series with Sergei Levine, an assistant professor

00:29.000 --> 00:34.040
at UC Berkeley, whose research focus is deep robotic learning.

00:34.040 --> 00:38.800
Sergei is part of the same research team as a couple of our previous guests in the series,

00:38.800 --> 00:44.720
Chelsea Finn and Peter Rebel, and if the response we've seen to those shows is any indication,

00:44.720 --> 00:47.480
you're going to love this episode.

00:47.480 --> 00:52.480
Sergei's research interests and our discussion focus in on how robotic learning techniques

00:52.480 --> 00:58.760
can be used to allow machines to autonomously acquire complex behavioral skills.

00:58.760 --> 01:03.160
We really dig into some of the details of how this is done, and I found that my conversation

01:03.160 --> 01:08.040
with Sergei filled in a lot of gaps for me from the interviews with Peter and Chelsea.

01:08.040 --> 01:11.800
By the way, this is definitely a nerd alert episode.

01:11.800 --> 01:15.680
Before we jump into the show, I'd like to thank everyone who's taken the time to enter

01:15.680 --> 01:18.400
our AI conference giveaway.

01:18.400 --> 01:23.360
You all know that one of my favorite things to do is to give away free stuff to listeners,

01:23.360 --> 01:28.320
and we've been fortunate to be able to give away tickets to the O'Reilly AI conference,

01:28.320 --> 01:33.200
to lucky Twomo listeners since the very first event in the series last year.

01:33.200 --> 01:38.200
Well, we've got a couple of exciting updates for those of you who want in on this opportunity.

01:38.200 --> 01:44.000
First, we're making it even easier to enter our ticket giveaway for the San Francisco event,

01:44.000 --> 01:48.720
and second, we're giving away two tickets now, not just one.

01:48.720 --> 01:55.560
To enter the contest in 30 seconds or less, just hit pause right now, and visit TwomoAI.com

01:55.560 --> 01:59.200
slash AISF right from your phone.

01:59.200 --> 02:05.640
Finally, a quick thank you to our sponsors for the Industrial AI series, Banzai, and

02:05.640 --> 02:08.440
Wise.io at GE Digital.

02:08.440 --> 02:10.800
By now, you know a bit about Banzai, right?

02:10.800 --> 02:15.640
You've heard me mention their AI platform, which lets enterprises build and deploy intelligent

02:15.640 --> 02:16.640
systems.

02:16.640 --> 02:21.040
Well I actually spent some time in the Banzai offices in Berkeley last week learning more

02:21.040 --> 02:26.480
about that platform and recording an interview with their co-founder and CEO Mark Hammond.

02:26.480 --> 02:30.200
It was a great conversation and I'm really looking forward to getting it up on the

02:30.200 --> 02:32.800
podcast in a few weeks.

02:32.800 --> 02:37.720
In the meantime, I'll reiterate that if you're trying to build AI-powered applications

02:37.720 --> 02:43.920
focused on optimizing and controlling the physical systems in your enterprise, whether robots,

02:43.920 --> 02:48.920
or HVAC systems, or supply chains, you should take a look at what they're up to.

02:48.920 --> 02:53.000
They've got a unique approach to building AI models that lets you model the real world

02:53.000 --> 02:57.880
concepts in your application, automatically generate, train, and evaluate low level

02:57.880 --> 03:03.040
models for your project using technologies like reinforcement learning, and easily integrate

03:03.040 --> 03:07.240
those models into your applications and systems using APIs.

03:07.240 --> 03:13.800
You can check them out at Banz.ai slash TwomoAI, and definitely let them know you appreciate

03:13.800 --> 03:17.400
their support of the podcast and this series.

03:17.400 --> 03:22.760
Last week, I announced wise.io at GE Digital as a sponsor for this series as well.

03:22.760 --> 03:27.360
Wise.io was among the first companies I began following in what I call the machine learning

03:27.360 --> 03:31.000
platform space back in 2012-2013.

03:31.000 --> 03:36.360
I've since interviewed co-founder Josh Bloom here on the show and mentioned the company's

03:36.360 --> 03:39.760
subsequent acquisition by GE Digital.

03:39.760 --> 03:45.120
At GE Digital, the wise.io team is focused on creating technology and solutions that enable

03:45.120 --> 03:51.680
advanced capabilities for the industrial internet of things, making infrastructure more intelligent

03:51.680 --> 03:55.520
and advancing the industry's critical to the world we live in.

03:55.520 --> 03:59.920
I want to give a hearty thanks and shout out to the team at wise.io at GE Digital for

03:59.920 --> 04:03.960
supporting my industrial AI research and this podcast series.

04:03.960 --> 04:08.520
Of course, you can check them out at wise.io.

04:08.520 --> 04:10.520
And now onto the show.

04:10.520 --> 04:21.960
Hey everyone, I am on the line with Sergei Levine.

04:21.960 --> 04:29.080
Sergei is an assistant professor at UC Berkeley in the EECS department and I'm super excited

04:29.080 --> 04:30.520
to have him on the show.

04:30.520 --> 04:31.520
Hi Sergei.

04:31.520 --> 04:32.520
Hello.

04:32.520 --> 04:33.520
How are you doing?

04:33.520 --> 04:35.520
I'm doing well.

04:35.520 --> 04:36.520
Wonderful.

04:36.520 --> 04:41.960
How about we start by having you introduce yourself and talk a little bit about your background

04:41.960 --> 04:48.160
and how you got interested in your current area of research and what that is?

04:48.160 --> 04:49.160
Sure.

04:49.160 --> 04:54.240
So I actually started off in graduate school working on computer graphics and particularly

04:54.240 --> 04:55.240
in computer graphics.

04:55.240 --> 04:59.920
I was really interested in simulating virtual humans, simulating virtual characters.

04:59.920 --> 05:03.600
And the trouble is that if you want to simulate very realistic virtual humans, one of the

05:03.600 --> 05:06.640
things you have to do is you have to simulate intelligence because humans are intelligent

05:06.640 --> 05:08.960
and machines by default aren't.

05:08.960 --> 05:13.960
So a lot of my work turned out to be essentially artificial intelligence work in computer graphics

05:13.960 --> 05:17.400
to get these virtual characters to behave in ways that look plausible.

05:17.400 --> 05:21.720
So from there, I decided that, well, if I have some methods that work reasonably well

05:21.720 --> 05:25.720
in computer graphics, I can create some plausibly realistic virtual humans, perhaps those are methods

05:25.720 --> 05:28.240
that are also applicable, for example, to robotics.

05:28.240 --> 05:32.520
So I did a postdoc after that in robotics, turns out that a lot of the stuff works well

05:32.520 --> 05:34.000
for robots as well.

05:34.000 --> 05:38.800
And a lot of that led to my current work in reinforcement learning and deep learning.

05:38.800 --> 05:39.800
Fantastic.

05:39.800 --> 05:46.280
I noticed on your website that you've got a paper, except that you're speaking at a computer

05:46.280 --> 05:51.720
animation conference, are you still fairly active in the video domain?

05:51.720 --> 05:52.800
Not as much in recent years.

05:52.800 --> 05:57.040
So I think my last paper there was in 2012.

05:57.040 --> 06:00.640
I am giving a guess lecture this summer actually at SCA, that's a symposium on computer

06:00.640 --> 06:04.640
animation to talk about some of the recent progress in deep reinforcement learning.

06:04.640 --> 06:09.000
So actually, since I moved to robotics, actually, a lot of this technology has made actually

06:09.000 --> 06:13.280
a big impact in graphics, and that's really right about now and this past year that's

06:13.280 --> 06:14.600
been registering a lot.

06:14.600 --> 06:19.960
So they invited me to come give a talk to them about how some of this stuff is going.

06:19.960 --> 06:20.960
Fantastic.

06:20.960 --> 06:21.960
Fantastic.

06:21.960 --> 06:27.080
So as you know, we recently had on the show Peter Rebel and Chelsea Finn, who are your

06:27.080 --> 06:34.240
colleagues there at Berkeley, and the conversations I had with those guys were really, really interesting.

06:34.240 --> 06:39.720
And let's maybe take a minute to talk about the research that you're doing in a little

06:39.720 --> 06:43.000
bit more detail and we can dive in deeper.

06:43.000 --> 06:44.000
Sure.

06:44.000 --> 06:48.920
So the area that I work in can be broadly categorized as robotic learning.

06:48.920 --> 06:53.640
So I'm interested in developing algorithms and models that can allow robots to autonomous

06:53.640 --> 06:58.520
who learn very large and complex repertoire of behaviors so that they can take on more

06:58.520 --> 07:02.320
and more of the functionality that we associate with intelligent human beings so that they

07:02.320 --> 07:06.080
can do all the things that are dangerous and pleasant or for other reasons undesirable

07:06.080 --> 07:07.840
for people to do themselves.

07:07.840 --> 07:12.960
And to me, this problem is not just a problem that has a lot of interesting practical implications,

07:12.960 --> 07:16.880
it's also something that I think can serve as a really valuable lens and artificial

07:16.880 --> 07:22.240
intelligence because in the end, we have only one proof of existence of true intelligence

07:22.240 --> 07:24.960
of human beings and human beings are embodied.

07:24.960 --> 07:30.000
So we don't just exist sort of in the ether thinking abstract thoughts, we actually have

07:30.000 --> 07:34.120
a body we interact with the world and the nature of that interaction is very central to

07:34.120 --> 07:37.240
shaping who we are and how and how we reason about things.

07:37.240 --> 07:41.680
So I think that dealing with systems that are embodied systems like robots gives us a very

07:41.680 --> 07:47.480
valuable perspective in understanding how we might be able to construct artificial intelligence.

07:47.480 --> 07:55.880
So more so than some of the non-physical applications of machine learning in AI, including other

07:55.880 --> 07:58.560
deep learning applications like gameplay.

07:58.560 --> 08:05.400
Well, so the thing about other applications of AI is that oftentimes, especially in

08:05.400 --> 08:10.200
things like computer vision, speech recognition, so on, we work with just the perception

08:10.200 --> 08:11.200
half the equation.

08:11.200 --> 08:15.200
So we think about how we can take in data and produce a particular answer.

08:15.200 --> 08:17.760
But the nature of intelligence is much more complex than that.

08:17.760 --> 08:21.880
It's about taking in information, reasoning about it, making decisions, thinking about

08:21.880 --> 08:25.000
the outcomes of those decisions and so on and so on.

08:25.000 --> 08:28.240
Now you mentioned game playing, which has some elements of this.

08:28.240 --> 08:33.080
But one thing that game playing won't let you do is it won't let you tackle the full

08:33.080 --> 08:37.560
complexity and diversity of the real world because the real world is characterized not

08:37.560 --> 08:42.520
just by sequential nature, but also by its diversity, by the sheer number of unexpected

08:42.520 --> 08:47.240
things that might happen in a natural interaction, which computer vision has dealt with for decades,

08:47.240 --> 08:50.240
but without handling the decision making and the game playing handles the decision making

08:50.240 --> 08:53.840
but without handling so much of diversity.

08:53.840 --> 09:00.760
So to what degree is your research and robotic learning kind of integrative across all

09:00.760 --> 09:02.000
these different fields?

09:02.000 --> 09:10.920
Are you specifically focused on pulling together some of the state of the art research from

09:10.920 --> 09:18.160
these various fields or is your domain within robotic learning kind of established and you're

09:18.160 --> 09:19.560
heading down a path that way?

09:19.560 --> 09:22.680
I don't know if that question makes any sense, but if you kind of get a sense from where

09:22.680 --> 09:23.680
I'm going.

09:23.680 --> 09:24.680
I think I see where you're going.

09:24.680 --> 09:28.920
This is actually a very good question and something that for robotics has been sort of

09:28.920 --> 09:34.720
one of these big tensions over the years is that it's often been very tempting for researchers

09:34.720 --> 09:39.760
to think of robotics as fundamentally a systems or integration exercise.

09:39.760 --> 09:43.440
So if you have, let's say, a very effective computer vision system and you have a very

09:43.440 --> 09:48.080
effective, let's say, planning system, well, maybe building an intelligent robot is just

09:48.080 --> 09:52.400
a matter of welding those species together, connecting up the wires and seeing it work.

09:52.400 --> 09:56.320
And a lot of people have hoped for exactly this that by making progress independently in

09:56.320 --> 10:00.400
different domains, we'll get closer and closer to intelligent robots.

10:00.400 --> 10:03.320
Unfortunately, reality hasn't quite panned out that way.

10:03.320 --> 10:07.160
And a lot of robotics will actually lament that if they take sort of the latest image

10:07.160 --> 10:10.640
net train model and put it on their robot and try to use it for object detection in the

10:10.640 --> 10:14.440
wild, it'll actually do a pretty terrible job because the biases that are present in

10:14.440 --> 10:18.000
the kind of data sets that those models are trained on don't really reflect what a robot

10:18.000 --> 10:21.360
will see from its cameras in natural environments.

10:21.360 --> 10:25.560
So I actually think that in order to really get this right, we need to draw on the lessons

10:25.560 --> 10:30.000
in the state of the art models in, you know, game playing, vision and so on.

10:30.000 --> 10:32.680
But at some point, we have to kind of do a lot of that ourselves.

10:32.680 --> 10:37.520
We have to take the lessons, but not necessarily the technical components themselves.

10:37.520 --> 10:40.640
And for that reason, I've actually been a really big advocate of end to end training for

10:40.640 --> 10:45.360
robotic learning where we set up models that include both perception and control and

10:45.360 --> 10:49.680
act to train together to perform the particular task the robot needs to handle instead of relying

10:49.680 --> 10:53.520
on integration of existing components.

10:53.520 --> 10:58.680
In taking a look at your research, I came across a really interesting example of the

10:58.680 --> 11:03.960
effect you're describing, the particular research was where you were training a robot arm.

11:03.960 --> 11:09.920
I think it was a backster robot to tie knots in a rope.

11:09.920 --> 11:15.880
And some of the comments associated with the research on the, I think there was a

11:15.880 --> 11:22.600
get-hud page about it was that, hey, we trained this system on a, I think it was a red

11:22.600 --> 11:23.600
rope.

11:23.600 --> 11:27.720
And, you know, we're working hard to make it work with a white rope also that's a little

11:27.720 --> 11:28.720
bit stiffer.

11:28.720 --> 11:33.800
And we trained it on a background that was a green background and, you know, that doesn't,

11:33.800 --> 11:37.760
we found that that doesn't generalize to other backgrounds.

11:37.760 --> 11:43.760
This is a conversation point that came up with Peter as well, this notion of mastery versus

11:43.760 --> 11:44.760
generalization.

11:44.760 --> 11:48.880
Can you talk a little bit about that and how your research is taking that issue on?

11:48.880 --> 11:49.880
Yeah, absolutely.

11:49.880 --> 11:55.360
So, the backster paper that you're referring to there, what we did is we actually had a robot

11:55.360 --> 11:59.520
practice tying knots, but of course, it was one robot and it was practicing tying knots

11:59.520 --> 12:00.720
in one particular rope.

12:00.720 --> 12:04.480
So the resulting system could do really well at tying knots in that rope, it could kind

12:04.480 --> 12:08.280
of tie knots in ropes that looked a little similar and it pretty much broke down if you

12:08.280 --> 12:13.000
gave it something, you know, a rope that was too thick or too thin or something like that.

12:13.000 --> 12:19.440
But here's the thing that in robotics, there's like oftentimes when we run experiments,

12:19.440 --> 12:22.360
the experiment is the entirety of the data collection process.

12:22.360 --> 12:26.320
So if you imagine an experiment in computer vision, you take all of ImageNet, you train

12:26.320 --> 12:29.040
your model on it and you show its performance.

12:29.040 --> 12:33.040
In robotics, an experiment basically amounts to generating an entire new data set, training

12:33.040 --> 12:35.160
your model on it and then observing its performance.

12:35.160 --> 12:39.240
So of course, if you're generating an entire data set every time, if you have one robot,

12:39.240 --> 12:42.720
just a little bit of time, it's not going to generalize very far.

12:42.720 --> 12:47.120
We did actually try to study at one point what would happen is if we scaled up the style

12:47.120 --> 12:52.280
of technique, we did this actually in partnership with Google, which has quite a

12:52.280 --> 12:55.920
bit more resources as far as deploying large numbers of robots.

12:55.920 --> 12:59.880
And we tried to see actually, like if we run data collection at the scale of something

12:59.880 --> 13:05.600
like ImageNet, can we actually get robotic skills that generalize effectively?

13:05.600 --> 13:11.000
So what we did there is we set up, we called this the ARM farm by analogy to server farm.

13:11.000 --> 13:15.880
We set up a cluster of about 14 robots and we had them basically working day and night

13:15.880 --> 13:18.000
to practice grasping objects.

13:18.000 --> 13:21.840
So we chose grasping because it's something they can do to pretty much any object and

13:21.840 --> 13:25.520
it's also very important for a lot of other robotic manipulation tasks.

13:25.520 --> 13:29.600
And we had them running day and night like this and they collected about 800,000 grasps,

13:29.600 --> 13:31.880
each grasped had maybe five to ten images.

13:31.880 --> 13:35.480
So the total size of the data set was about on the same order of magnitude as ImageNet.

13:35.480 --> 13:39.760
And there we did find that actually the resulting networks that you train on that really large

13:39.760 --> 13:43.200
data set, they do actually generalize effectively to new objects that are completely different

13:43.200 --> 13:44.840
than what they've seen before.

13:44.840 --> 13:49.560
In fact, when you do learning at this larger scale, you can observe some really interesting

13:49.560 --> 13:51.560
or emergent behavior.

13:51.560 --> 13:55.400
One of the things that we were thinking as we did this work as well, grasping is a very

13:55.400 --> 13:56.400
geometric behavior.

13:56.400 --> 14:01.000
So probably the first thing that these systems will learn about is the geometry of objects

14:01.000 --> 14:04.680
in the world still learn that you need to put the finger on one side, put the finger on

14:04.680 --> 14:06.720
the other side and so on.

14:06.720 --> 14:11.240
What we saw, which surprised us a little bit, is that in the earlier stages of training,

14:11.240 --> 14:16.440
when you have maybe 100,000 grasps before we collected the full data set of a million,

14:16.440 --> 14:21.680
in the early stages of training, the network actually didn't pay as much attention to geometry,

14:21.680 --> 14:24.760
but what it did do is it paid a lot of attention to material properties.

14:24.760 --> 14:28.720
It recognized right away that if something was really soft, then it could pinch it and

14:28.720 --> 14:30.040
pick it up really easily.

14:30.040 --> 14:32.560
But if something was rigid, then it couldn't do that.

14:32.560 --> 14:36.640
And this is completely different from how conventional, manually designed grasping systems

14:36.640 --> 14:40.320
tend to work because when you manually design a grasping system, you're going to use

14:40.320 --> 14:44.480
some sort of geometric motion planning and you're going to completely ignore the material

14:44.480 --> 14:45.480
properties.

14:45.480 --> 14:48.800
It's really interesting to us and that's sort of underscored, I think, the value that

14:48.800 --> 14:52.440
you get from using learning through trial and error because you actually learn about

14:52.440 --> 14:55.720
the patterns that are really present in the world rather than the ones that your analytic

14:55.720 --> 14:59.240
model thinks are important.

14:59.240 --> 15:04.320
Are there any other emergent behaviors that you observed in that set of experiments?

15:04.320 --> 15:05.320
Let me see.

15:05.320 --> 15:08.600
So that was the only one that we could pin down in the sense that we could actually measure

15:08.600 --> 15:12.240
it, like we could actually put different objects in front of it and quantify that, yes,

15:12.240 --> 15:14.680
it was really employing the strategy.

15:14.680 --> 15:18.920
And formally, there were a few things that it did tend to do pretty consistently that

15:18.920 --> 15:20.400
I can kind of speculate a little bit about.

15:20.400 --> 15:22.520
I just don't have the hard numbers for it.

15:22.520 --> 15:26.360
Intended to figure out, for example, that if you have something like a brush that you

15:26.360 --> 15:31.000
should pick up the brush by the stiff part rather than the flexible bristles, which is

15:31.000 --> 15:35.440
nice, it tended to figure out that center of masses of objects really matter, especially

15:35.440 --> 15:37.640
for awkwardly shaped objects.

15:37.640 --> 15:40.000
So those were some of the things that it picked up on.

15:40.000 --> 15:42.680
There were also a few mistakes that actually made that we're kind of amusing.

15:42.680 --> 15:48.520
So it just so happened that a lot of the soft things in our training objects were brightly

15:48.520 --> 15:52.840
colored because we bought, you know, we wanted to buy small items of clothing and small items

15:52.840 --> 15:54.680
of clothing, our children's clothing.

15:54.680 --> 15:55.680
And children's clothing.

15:55.680 --> 15:57.000
So we brightly colored.

15:57.000 --> 16:01.480
So it had this association of things that were brightly colored were soft.

16:01.480 --> 16:03.880
And in our test set of objects, we had a pink stapler.

16:03.880 --> 16:07.320
And that pink stapler was just impossible for it to pick up because it was just convinced

16:07.320 --> 16:10.800
that this pink stapler was a soft fuzzy thing and it could just pinch it.

16:10.800 --> 16:16.160
So that's a good example, actually, of the kind of funny data set biases that you can

16:16.160 --> 16:21.080
get that will actually affect you even in real world tasks like this.

16:21.080 --> 16:22.080
Interesting.

16:22.080 --> 16:23.080
Interesting.

16:23.080 --> 16:31.560
When I hear you describe the examples, an example like the pink stapler, it makes me wonder,

16:31.560 --> 16:37.840
you know, to what extent is it possible to layer the traditional object recognition types

16:37.840 --> 16:43.360
of technologies into a model like this, like, should it be able to recognize the stapler

16:43.360 --> 16:50.640
first and then have some higher level abstraction that we're also training on in addition to

16:50.640 --> 16:52.040
just the raw pixels?

16:52.040 --> 16:53.360
Is that something you look at?

16:53.360 --> 16:54.760
Yeah, that's a very good question.

16:54.760 --> 16:57.400
That's actually something that we've thought about a lot here.

16:57.400 --> 17:03.200
So one of the big things that you get out of traditional approaches to dog detection,

17:03.200 --> 17:04.680
it's not actually the models themselves.

17:04.680 --> 17:05.680
It's the data.

17:05.680 --> 17:12.000
There's very large and extremely diverse data sets, label data sets of objects with bounding

17:12.000 --> 17:14.840
boxes, segmentation, and so on.

17:14.840 --> 17:16.920
And it would be really nice to try to use that.

17:16.920 --> 17:20.400
But at the same time, you want to avoid losing the benefit of end-to-end training.

17:20.400 --> 17:24.560
So if you simply run a bounding box detector on what the robot is seeing and then ask

17:24.560 --> 17:27.960
it to pick things up, well, it's not just the bounding box that matters for the grasp,

17:27.960 --> 17:30.800
it has to also understand something about what's in that bounding box.

17:30.800 --> 17:33.920
So you don't want to lose the benefit of the end-to-end training, but at the same time,

17:33.920 --> 17:38.200
you want to somehow get more out of all these auxiliary sources of information.

17:38.200 --> 17:41.600
One of the things that we've been working on a little bit, and this isn't out yet,

17:41.600 --> 17:46.400
but this will be released in probably a couple of weeks, is some work on semi-supervised

17:46.400 --> 17:51.560
learning of robotic skills, where we combine experience from the robot's point of view that

17:51.560 --> 17:56.160
includes the actions that it took and the observations that it saw with kind of a weekly

17:56.160 --> 17:57.960
labeled image data set.

17:57.960 --> 18:02.160
And weekly labeled in the sense that that data set just tells you, does the image contain

18:02.160 --> 18:03.800
the object that the robot needs to use?

18:03.800 --> 18:07.920
If the robot is learning, for example, how to put a cap on a bottle, the weekly labels

18:07.920 --> 18:11.080
might say, does this image contain a bottle or not?

18:11.080 --> 18:14.080
And the idea is that the robot itself, when it's interacting with the world, maybe it

18:14.080 --> 18:18.280
only gets to interact with a few instances of those objects.

18:18.280 --> 18:22.040
So it can use those few instances to understand the physics of the behavior, but it's not

18:22.040 --> 18:26.080
really enough for it to really generalize to understand what the entire class of objects

18:26.080 --> 18:27.400
of this type looks like.

18:27.400 --> 18:32.880
So the weekly labeled data is there to basically show it, what can this skill be applied to?

18:32.880 --> 18:37.160
And the important thing when incorporating this weekly label data is not to lose the benefit

18:37.160 --> 18:38.160
of the intent training.

18:38.160 --> 18:42.760
So in this technique that we develop, we're actually including the weekly label data and

18:42.760 --> 18:46.520
the robot's own experience at the same time in a joint training procedure, rather than

18:46.520 --> 18:50.600
actually splitting things up into components and then trying to wire them up together as

18:50.600 --> 18:52.440
in the more kind of conventional systems approach.

18:52.440 --> 18:54.280
And that turns out to work very well.

18:54.280 --> 18:57.760
Under the hood, the method has kind of an intentional flavor to it, so it basically

18:57.760 --> 19:01.560
learns what kind of objects to pay attention to from the weekly label data and then uses

19:01.560 --> 19:06.520
that attentional mechanism to perform the task at test time.

19:06.520 --> 19:09.400
How do you express weakness in this model?

19:09.400 --> 19:14.160
Well, when I say weekly labeled, I just mean that the images have a label that only tells

19:14.160 --> 19:16.840
you whether the object you care about is present or not.

19:16.840 --> 19:22.680
So you can think of this as a person telling the robot, here are the things that you can

19:22.680 --> 19:24.920
execute this skill on.

19:24.920 --> 19:29.320
So here, lots of pictures of the thing that you can do this task too, and here are all

19:29.320 --> 19:32.440
the pictures of things that you cannot do this task too.

19:32.440 --> 19:33.440
Right.

19:33.440 --> 19:34.440
Right.

19:34.440 --> 19:42.400
And is there a general approach to incorporating in kind of higher level abstractions, higher

19:42.400 --> 19:50.360
level abstractions into models like this, meaning, you know, in the case of a, and going

19:50.360 --> 19:55.800
back to the stapler example, you know, we could do the object detection and determine that,

19:55.800 --> 20:00.600
hey, this is a stapler, but there's also, you know, there are other neural nets that or

20:00.600 --> 20:06.920
other examples that can do geometry detection and things like that and orientation detection.

20:06.920 --> 20:13.280
And I guess the question that I'm trying to get at is it sounds like the general approach

20:13.280 --> 20:18.800
to applying deep learning in this model is, you know, let's just collect a bunch of data

20:18.800 --> 20:23.000
and, you know, throw it at and train on a bunch of data.

20:23.000 --> 20:27.600
And if there are important features, you know, the model will figure it out, the network

20:27.600 --> 20:28.600
will figure it out.

20:28.600 --> 20:34.560
And what I'm curious about is, is that do you, hey, I guess what are the, you know, what's

20:34.560 --> 20:38.520
the, is there an analytical foundation to that assertion?

20:38.520 --> 20:44.200
And if not, are there other ways that folks are looking at incorporating in abstractions

20:44.200 --> 20:53.440
or features into, you know, these models to help them, you know, both generalize and train faster?

20:53.440 --> 20:56.160
So I think there's perhaps a little more to it than that.

20:56.160 --> 21:01.680
So it used to be that when we thought about kind of the, the previous generation, generational

21:01.680 --> 21:06.680
machine learning models, the way that we would imagine using them is exactly when you describe

21:06.680 --> 21:10.080
that we say, okay, we have some edge detector, we have a pose detector, we have some kind

21:10.080 --> 21:14.840
of thing that will analyze local geometry, we'll plug that into the downstream module and

21:14.840 --> 21:16.640
so on and so on.

21:16.640 --> 21:22.520
The thing about deep learning is that the model itself, you know, it's good for making

21:22.520 --> 21:26.080
predictions, but there's nothing kind of unique or special about it.

21:26.080 --> 21:30.360
You can actually have the same model perform multiple tasks.

21:30.360 --> 21:35.080
And that's often not actually that much harder than stapling together two models that each

21:35.080 --> 21:40.760
perform those tasks. So if you want a model that can, you know, segment an image and detect

21:40.760 --> 21:44.240
poses of objects, you could train two separate models and then combine their outputs or

21:44.240 --> 21:46.760
it can just train one model that does both of those tasks.

21:46.760 --> 21:51.360
And the latter is often not actually that much harder, but it has a substantial benefit

21:51.360 --> 21:55.000
which is when you train a single model to perform multiple tasks, it can actually learn

21:55.000 --> 22:00.400
internal representations that share the knowledge that's contained in those two tasks.

22:00.400 --> 22:06.760
So if you were to ask me how I would consider combining, let's say, a object pose detector

22:06.760 --> 22:11.240
and a grasping system, I would much rather train a single model that predicts both pose

22:11.240 --> 22:16.480
and grasp than to take a pose predictor and feed its output into a grasp predictor.

22:16.480 --> 22:19.440
And the reason for that is that the data already has all the information.

22:19.440 --> 22:22.240
There's nothing, you know, magical that's contained in the model that's not already contained

22:22.240 --> 22:24.840
in the data and it's possible to train these joint models.

22:24.840 --> 22:30.040
So I might as well take both data sets and train one model that'll benefit from the shared

22:30.040 --> 22:33.520
structure in both of those tasks, then train two completely destroyed models and then

22:33.520 --> 22:36.000
try to stable them together afterwards.

22:36.000 --> 22:37.000
Right.

22:37.000 --> 22:38.000
Right.

22:38.000 --> 22:47.160
Have you run into situations where there's there are pre-existing models trained on inaccessible

22:47.160 --> 22:48.160
data?

22:48.160 --> 22:52.800
I guess I'm maybe I'm kind of chasing the chasing the tail of the scenario a little bit,

22:52.800 --> 22:56.880
but it sounds like, you know, there may be some corner case where it makes sense to do

22:56.880 --> 23:00.920
that if you don't have access to the data, but you do have access to the model.

23:00.920 --> 23:06.720
But I get the point that in general, the data is the data and if you can train one model

23:06.720 --> 23:12.280
that can build these internal representations, it's much more efficient than trying to

23:12.280 --> 23:17.840
engineer one model that can solve part of the problem and another model that uses that

23:17.840 --> 23:20.680
to solve the thing that you're actually trying to do.

23:20.680 --> 23:21.680
Yeah.

23:21.680 --> 23:25.200
Basically, it's a lot easier for us to compose data sets than it is to compose models.

23:25.200 --> 23:27.200
Right.

23:27.200 --> 23:33.280
So, one of the challenges that comes up that you've spent some time looking at is the

23:33.280 --> 23:37.400
efficiency of training these deep learning models.

23:37.400 --> 23:40.680
Sample efficiency in particular is one of the ways you talk about that.

23:40.680 --> 23:43.880
Can you talk a little bit about that problem and the things you've done there?

23:43.880 --> 23:44.880
Right.

23:44.880 --> 23:48.080
So, I assume you're referring specifically to sample efficiency for deep reinforcement

23:48.080 --> 23:49.880
learning algorithms.

23:49.880 --> 23:50.880
That's correct.

23:50.880 --> 23:55.760
So, deep reinforcement learning algorithms are kind of a funny creature.

23:55.760 --> 24:00.520
Deep learning, like standard deep learning with gradient descent, it's a common perception

24:00.520 --> 24:02.640
that it's inefficient.

24:02.640 --> 24:07.360
And in some sense, it is like we can build very good object detectors, but we need maybe

24:07.360 --> 24:11.520
millions of images to train them, which might seem like a lot, but if you consider what

24:11.520 --> 24:15.280
that model is really doing, it's reasoning about pixels, edges, everything from those pixels

24:15.280 --> 24:20.360
and edges all the way to complex higher level concepts, that's actually pretty sophisticated.

24:20.360 --> 24:23.480
With deep reinforcement learning, though, things get a lot worse.

24:23.480 --> 24:28.880
So, if you look at the kind of sample complexity for learning to play, let's say, a simple

24:28.880 --> 24:33.680
video game like Pong, and there you're going to be looking at millions or even tens of

24:33.680 --> 24:38.680
millions of images for a task with visual diversity that's nowhere near where we see

24:38.680 --> 24:41.440
in conventional, let's say, computer vision data sets.

24:41.440 --> 24:47.440
So, visually it's very simple, physically it's very simple, but you need a lot of samples

24:47.440 --> 24:52.240
to learn that task, and those samples involve actively interacting with an environment.

24:52.240 --> 24:55.440
How it happens to be a simulated environment, so you can run it much faster than real

24:55.440 --> 24:59.760
time on a server, but still something here seems a little out of whack.

24:59.760 --> 25:05.240
Something here is a lot worse than perhaps it should be.

25:05.240 --> 25:08.120
And what's the intuition for why that is the case?

25:08.120 --> 25:09.800
There are a couple of reasons for it.

25:09.800 --> 25:13.440
The short version is that we don't fully understand, but the long version is that there are

25:13.440 --> 25:16.920
a few things that are being done that could perhaps be done differently.

25:16.920 --> 25:21.480
Now, if I knew exactly the answer to this, then of course, I would have a much more efficient

25:21.480 --> 25:26.560
algorithm to give you, but it's possible to guess a few things here.

25:26.560 --> 25:31.800
One of the things is that reinforcement learning provides a much weaker signal than supervised

25:31.800 --> 25:32.800
learning.

25:32.800 --> 25:37.200
So in reinforcement learning, even though it's gradient-based optimization, you don't

25:37.200 --> 25:40.000
really have gradients of the thing that you really care about.

25:40.000 --> 25:44.960
You're sort of estimating them in this very peculiar way, depending on the reinforcement

25:44.960 --> 25:49.040
learning algorithm that you use, so you essentially get a lot less information from every gradient

25:49.040 --> 25:50.040
step.

25:50.040 --> 25:54.400
A lot of reinforcement learning algorithms also tightly couple the collection of data in

25:54.400 --> 25:58.200
the environment and the updating of the model, which is very different from supervised

25:58.200 --> 25:59.200
learning.

25:59.200 --> 26:01.880
So in supervised learning, you first collect a large data set, and then you take many, many

26:01.880 --> 26:04.200
gradient steps on that large data set.

26:04.200 --> 26:09.160
In reinforcement learning, you often interleave collection of data and updating the model

26:09.160 --> 26:12.080
because you need to collect data that agrees with your model.

26:12.080 --> 26:15.480
So if you're learning a policy, you'd like to collect the kind of experience that that

26:15.480 --> 26:18.760
policy will actually see, and you want to do this iteratively.

26:18.760 --> 26:22.000
So that means that you're often throwing out lots of data from old policies that you

26:22.000 --> 26:25.760
can no longer use because your policy has changed, and that prevents you from reusing

26:25.760 --> 26:26.760
old data.

26:26.760 --> 26:28.920
So that can be very harmful for sample efficiency.

26:28.920 --> 26:33.080
In fact, some of the most inefficient methods, methods like policy gradient, that are very

26:33.080 --> 26:36.840
convenient to use in simulation, they're often the most inefficient in the real world

26:36.840 --> 26:39.160
because they can't reuse data.

26:39.160 --> 26:42.800
So we need to look at methods that can reuse old data, these are sometimes called off-policy

26:42.800 --> 26:44.320
algorithms.

26:44.320 --> 26:48.640
Before we go there, can you elaborate on the throwing out of the data?

26:48.640 --> 26:54.240
Is this something that the algorithm is doing as part of the way it's constructed, or

26:54.240 --> 26:58.440
is this something that we're doing manually to tell us a little bit more about what we

26:58.440 --> 26:59.440
mean by that?

26:59.440 --> 27:03.680
Oh, so that's just how a lot of on-policy, policy gradient algorithms work.

27:03.680 --> 27:09.160
So these algorithms will operate as following, they will collect experience from the current

27:09.160 --> 27:14.480
policy, they will compute a gradient descent direction on that experience, they will take

27:14.480 --> 27:19.000
that gradient step, update the policy, and now they need more data from the latest policy,

27:19.000 --> 27:20.000
which has not been updated.

27:20.000 --> 27:24.040
So they have to throw out all the old data and collect a new batch of data.

27:24.040 --> 27:29.280
So if you want a mental picture of what this looks like, if you have a robot that lets

27:29.280 --> 27:33.440
say it's learning to walk, it'll try to walk a couple of times, update its behavior, try

27:33.440 --> 27:36.520
to walk a couple more times, and so on and so on, and that's the reinforcement learning

27:36.520 --> 27:37.520
process.

27:37.520 --> 27:41.120
But you have to remember that each time it changes the behavior like that, it has to

27:41.120 --> 27:44.320
basically collect new experience because you need to understand how well its current

27:44.320 --> 27:45.600
policy is really doing.

27:45.600 --> 27:46.600
Right.

27:46.600 --> 27:50.240
So that can get really, really expensive in terms of the amount of time it needs to spend

27:50.240 --> 27:52.280
collecting experience.

27:52.280 --> 27:56.520
So if you're running stuff in a simulator on a server farm somewhere, then it's okay

27:56.520 --> 27:59.680
you can paralyze all that and everything is reasonable.

27:59.680 --> 28:03.880
But if that's a real physical system that's actually executing those trials, that can

28:03.880 --> 28:06.040
get extremely tight consuming.

28:06.040 --> 28:07.040
Mm-hmm.

28:07.040 --> 28:12.680
Okay, and you're about to talk about some of the ways we can get beyond this.

28:12.680 --> 28:13.680
Right.

28:13.680 --> 28:16.200
So one of the things we can do is we can look at off-policy algorithms.

28:16.200 --> 28:22.280
So these are algorithms that can supplement their training with data from other policies.

28:22.280 --> 28:24.760
So what can you learn from other policies?

28:24.760 --> 28:28.920
Well, intuitively, one of the things that you can learn is you can learn about predicting

28:28.920 --> 28:35.440
future events because the rules of physics and so on, they will hold true regardless

28:35.440 --> 28:38.080
of which policy you're executing.

28:38.080 --> 28:43.000
And the kind of future events that you can predict can range all the way from very detailed

28:43.000 --> 28:46.400
where you're actually predicting, let's say, the entirety of your future observations.

28:46.400 --> 28:49.960
And this is sometimes called model-based reinforcement learning.

28:49.960 --> 28:54.560
Or all the way to something fairly abstract, like the future rewards that you will see.

28:54.560 --> 28:58.880
And this is actually a type of model-free reinforcement learning that's sometimes referred

28:58.880 --> 29:03.080
to as value function estimation or Q-learning that also falls up this category.

29:03.080 --> 29:05.360
But they're all kind of prediction-style methods.

29:05.360 --> 29:09.880
So on the one extreme, you're predicting the entirety of your future sensory observations

29:09.880 --> 29:13.360
and on the other extreme, you're predicting something very abstract, like rewards that

29:13.360 --> 29:15.200
you will see in the future.

29:15.200 --> 29:18.840
And that tends to be more efficient because that allows you to incorporate data from other

29:18.840 --> 29:22.040
policies, including your own past policies.

29:22.040 --> 29:28.200
Hmm. And so, can you talk a little bit about those policies and how they differ from

29:28.200 --> 29:29.200
one another?

29:29.200 --> 29:30.200
Yeah.

29:30.200 --> 29:33.640
So I can talk a little bit about the model-based reinforcement learning because I feel

29:33.640 --> 29:37.480
like this is something that perhaps hasn't gotten quite as much attention in the research

29:37.480 --> 29:41.240
community in recent years because there's been a lot of excitement about model-free reinforcement

29:41.240 --> 29:42.240
learning.

29:42.240 --> 29:46.160
The model-based reinforcement learning, it's perhaps not as far along because the prediction

29:46.160 --> 29:50.680
problem that is trying to solve is a lot harder, but it has a lot of problems for dramatically

29:50.680 --> 29:53.960
improving sample efficiency for two reasons.

29:53.960 --> 29:58.440
The first reason is the one I mentioned that you can use data from other policies.

29:58.440 --> 30:02.320
But the second reason, which is perhaps a little more subtle, is that a model-based reinforcement

30:02.320 --> 30:06.640
learning, every sample has a lot more bits of supervision.

30:06.640 --> 30:10.880
So if you imagine what you're doing when you're, let's say, predicting a value function,

30:10.880 --> 30:16.480
you're predicting one scalar value that's a function of your current observation or state.

30:16.480 --> 30:20.040
When you're predicting everything that will happen in the future, maybe you're predicting

30:20.040 --> 30:24.640
future images that you will see, there are many more bits of supervision in that prediction

30:24.640 --> 30:25.640
problem.

30:25.640 --> 30:29.480
So every single sample actually carries a lot more bits of supervision, and that means

30:29.480 --> 30:32.640
that your model can learn a lot more from each of those samples.

30:32.640 --> 30:35.840
Now the flip side of the coin is that your model is now trying to solve a much harder problem.

30:35.840 --> 30:39.640
It doesn't have to predict just a single scalar value, just a predict an entire image.

30:39.640 --> 30:44.240
So it's sort of a little bit unclear how that shakes out, but potentially the benefit

30:44.240 --> 30:47.360
in sample complexity can actually be quite substantial there.

30:47.360 --> 30:51.880
We've done a little bit of work on model-based reinforcement learning for vision-based tasks,

30:51.880 --> 30:55.920
section on real physical robots, and this is some work that we did that also involved

30:55.920 --> 31:00.120
actually paralyzing data collection across multiple robots, but at a much smaller scale.

31:00.120 --> 31:04.960
So with the grasping, I mentioned that we needed about 800,000 grasped attempts.

31:04.960 --> 31:08.560
For the model-based reinforcement learning, we actually trained a video prediction model

31:08.560 --> 31:13.000
for pushing objects around on a table with about 50,000 pushes.

31:13.000 --> 31:16.920
And that was actually effective for generalizing to new objects and pushing them in new directions

31:16.920 --> 31:17.920
and so on.

31:17.920 --> 31:21.720
Simply by predicting what the robot will see in the future, and then taking the actions

31:21.720 --> 31:25.120
for which that model predicts the kind of outcomes that you want.

31:25.120 --> 31:28.920
So that was already a lot more efficient and it ran on real physical systems.

31:28.920 --> 31:32.640
Now the downside is that because the prediction problem there is so hard, the predictions

31:32.640 --> 31:34.040
were very short range.

31:34.040 --> 31:38.560
So the robot could only execute behaviors maybe with a horizon of two to three seconds.

31:38.560 --> 31:40.680
So these weren't complex behaviors.

31:40.680 --> 31:43.400
And that's because the prediction problem is so hard, but hopefully as we get better

31:43.400 --> 31:46.720
and better video prediction models, which is a very active area of research right now,

31:46.720 --> 31:49.960
these methods will get better and better.

31:49.960 --> 31:52.880
Is that the inverse reinforcement learning problem?

31:52.880 --> 31:56.520
No, this is the model-based reinforcement learning problem.

31:56.520 --> 32:02.320
So when I looked at the, again going back to the backster robot video, it talked a little

32:02.320 --> 32:07.840
bit about this inverse RL where you are, it sounded like you're doing the same thing.

32:07.840 --> 32:12.440
You've got your rope in one state, you have the human move it to another state, and then

32:12.440 --> 32:16.840
you're looking at the action, the robot action that it would take to get it from one state

32:16.840 --> 32:19.760
to another and producing the inverse of that.

32:19.760 --> 32:28.040
Or that becomes the action that the robot takes to move the rope into the rope to a position

32:28.040 --> 32:32.280
that's required to imitate what the human did.

32:32.280 --> 32:37.120
So that's the inverse RL, how are what you just described sounded very similar to that.

32:37.120 --> 32:39.120
So I think what you mean is actually inverse dynamics.

32:39.120 --> 32:40.120
Inverse dynamics.

32:40.120 --> 32:41.120
Okay.

32:41.120 --> 32:44.480
So when you have a model-based reinforcement learning problem, there's actually different

32:44.480 --> 32:46.800
ways that you can represent your predictive model.

32:46.800 --> 32:50.280
The most common way is to build what's called a forward dynamics model.

32:50.280 --> 32:54.480
So forward dynamics means that you're predicting from the present to the future.

32:54.480 --> 32:57.520
So you're looking at your current observation, your current action, and you're predicting

32:57.520 --> 32:59.960
what the next observation will look like.

32:59.960 --> 33:05.040
Inverse dynamics means you're predicting from the future to the action.

33:05.040 --> 33:09.520
So that means that you're looking at your current observation, your future observation,

33:09.520 --> 33:12.200
and you're predicting what action will get you from one to the other.

33:12.200 --> 33:13.200
Right.

33:13.200 --> 33:17.000
So I've got the rope in position A, I've got the rope in position B, what's the action

33:17.000 --> 33:19.200
that's required to get it from A to B?

33:19.200 --> 33:20.200
Exactly.

33:20.200 --> 33:24.440
So it's just another kind of predictive model and they have different pros and cons.

33:24.440 --> 33:27.960
So with a forward model, you can run it forward many steps because you can basically

33:27.960 --> 33:32.400
recursively apply it to its own predictions, but you have to work a little harder to get

33:32.400 --> 33:33.400
the action.

33:33.400 --> 33:37.320
With the inverse model, the action comes right out of the model, but it's difficult to

33:37.320 --> 33:40.800
chain it together because you don't know what the following observation will be because

33:40.800 --> 33:43.440
the model has some predictive observations and predictive actions.

33:43.440 --> 33:47.040
So inverse models are perhaps a little easier to use, they're a little easier to train,

33:47.040 --> 33:50.680
but they're a little harder to use for longer term planning.

33:50.680 --> 33:51.680
Okay.

33:51.680 --> 33:52.680
Okay.

33:52.680 --> 33:57.640
So in the discussion about sample efficiency, one of the things that I came across was

33:57.640 --> 34:01.360
mirror descent, guided policy search, can you talk a little bit about that and where

34:01.360 --> 34:02.360
that fits in?

34:02.360 --> 34:03.560
Sure.

34:03.560 --> 34:11.560
So mirror descent, guided policy search is a technique for optimizing a very complex policies

34:11.560 --> 34:16.040
like deep neural network policies by only using supervised learning to train the policy

34:16.040 --> 34:17.040
itself.

34:17.040 --> 34:19.680
And that sounds a little bit funny because if we're doing reinforcement learning, well,

34:19.680 --> 34:21.680
that's not supervised learning.

34:21.680 --> 34:25.360
So mirror descent, guided policy search sort of plays this game where it tries to figure

34:25.360 --> 34:29.960
out what is the supervision that I can give to a supervised learning algorithm such that

34:29.960 --> 34:35.600
when it trains some complex policy, that policy will do the right thing.

34:35.600 --> 34:39.720
So it's like if you know that you're that only supervised learning is ever allowed to

34:39.720 --> 34:43.240
touch the neural net, what can you give to the supervised learning algorithm so that it

34:43.240 --> 34:47.280
does the right thing for solving a reinforcement learning problem?

34:47.280 --> 34:51.840
And the way that the algorithm works is something like this that you're going to basically

34:51.840 --> 34:58.120
have a model based teacher that's going to generate training data for your supervised

34:58.120 --> 35:03.040
learning algorithm, so that model based teacher is, it's a kind of model based URL method,

35:03.040 --> 35:04.680
but it's not a deep model based URL method.

35:04.680 --> 35:08.360
It's just a, you can think of it almost like a, like a non-parametric algorithm.

35:08.360 --> 35:11.520
So it'll look at a few different trajectories that you took, figure out how to improve each

35:11.520 --> 35:15.680
of those individual trajectories by themselves without reasoning about any policies.

35:15.680 --> 35:19.120
And then that will generate training data so that your neural network can be trained

35:19.120 --> 35:21.320
with supervised learning to do better.

35:21.320 --> 35:24.480
So instead of reinforcement learning, which looks at you at the parameters of your model

35:24.480 --> 35:28.120
and says, how do I change these parameters to be better, in this mirror to send guided

35:28.120 --> 35:32.320
policy search, it actually looks at the trajectories that you executed, fit some model figures

35:32.320 --> 35:36.080
out how those trajectories should be improved, and then as those improvements as training

35:36.080 --> 35:38.520
data for regular supervised learning.

35:38.520 --> 35:42.420
That way the neural net is only ever trained with supervised learning and standard back

35:42.420 --> 35:43.420
problem.

35:43.420 --> 35:44.420
Okay.

35:44.420 --> 35:47.360
But at a very high level, the reason that this procedure is efficient has a lot to do with

35:47.360 --> 35:51.440
why model based our algorithms are efficient, because it really is a kind of model based

35:51.440 --> 35:55.200
on real algorithms, it's just one that, under the hood, uses standard supervised learning

35:55.200 --> 35:58.560
to train the policy neural network.

35:58.560 --> 36:04.720
So there's another interesting paper I came across, and that was the one on policy sketches.

36:04.720 --> 36:09.160
Can you talk a little bit about that work and what the goals are and what the results

36:09.160 --> 36:10.160
were?

36:10.160 --> 36:11.160
Yeah, I'd be happy to talk about that.

36:11.160 --> 36:14.280
So that was worked by a student named Jacob Andreas, together with Professor Dan Klein,

36:14.280 --> 36:18.620
who's another professor here at UC Berkeley, Jacob and Dan, they both worked on natural

36:18.620 --> 36:20.400
language processing.

36:20.400 --> 36:26.800
So the premise in that paper is that we'd like to see how symbolic descriptions of tasks,

36:26.800 --> 36:30.800
you can think of these as very, very simplified natural language, how symbolic descriptions

36:30.800 --> 36:33.880
of tasks can be used to improve learning.

36:33.880 --> 36:38.600
And the key ingredient there is that we'd like to basically see how symbolic descriptions

36:38.600 --> 36:42.400
can improve learning without assuming that those symbolic descriptions are grounded.

36:42.400 --> 36:46.120
So without assuming that the agent already understands what the symbols mean.

36:46.120 --> 36:49.400
So if you, let's say go to a foreign country, let's say you don't speak French and you go

36:49.400 --> 36:54.720
to France, and someone tells you, in French, how to, let's say, make a piece of furniture

36:54.720 --> 36:55.960
out of wood.

36:55.960 --> 36:58.480
And then they tell her how to make another piece of furniture out of wood, and then they

36:58.480 --> 37:00.560
tell you how to make a bench out of wood.

37:00.560 --> 37:03.400
Well, listening to those descriptions, you'll probably notice some common patterns.

37:03.400 --> 37:07.200
You'll probably notice that some words repeat, and if you hear enough of these descriptions

37:07.200 --> 37:11.440
and you actually perform those tasks and you kind of understand physically what it means,

37:11.440 --> 37:15.120
you'll find those patterns, even if you don't actually speak the language, and eventually

37:15.120 --> 37:18.880
when you hear a new phrase describing new items that you can construct out of wood, for

37:18.880 --> 37:23.200
example, you might be able to put the pieces together and figure that out more quickly.

37:23.200 --> 37:24.840
So that was kind of the idea that we were working with.

37:24.840 --> 37:29.960
So what Jacob did is he constructed this sort of simplified version of a Minecraft video

37:29.960 --> 37:30.960
game.

37:30.960 --> 37:32.240
So it's like a little crafting video game.

37:32.240 --> 37:34.880
It was simplified because we didn't want to deal with vision, we just wanted to deal

37:34.880 --> 37:38.240
with kind of simple kind of top-down navigation problems.

37:38.240 --> 37:41.680
And it had these tasks that were like, you know, pick up the wood, or chop it on the

37:41.680 --> 37:46.680
tree, pick up the wood, make the chest, for example, chop it on the tree, get the wood,

37:46.680 --> 37:52.120
make a boat, or, you know, grab the coal, put it in the oven, and so on.

37:52.120 --> 37:55.480
And there was a long list of these different tasks that the agent could perform that were

37:55.480 --> 37:59.200
constructed out of these symbolic verbs, essentially.

37:59.200 --> 38:02.840
And the agent would be given a set of these tasks, it would learn them.

38:02.840 --> 38:05.560
And the symbolic descriptions would just be given as an additional input.

38:05.560 --> 38:08.600
So they would result in some decomposition of the neural net, but there's actually

38:08.600 --> 38:09.920
different ways you could do that.

38:09.920 --> 38:12.800
But essentially they would be provided as an input to the agent without telling it what

38:12.800 --> 38:13.960
those symbols really mean.

38:13.960 --> 38:17.240
And just by learning the different tasks with the different symbolic descriptions, it

38:17.240 --> 38:21.200
could actually figure out how to then use new symbolic descriptions to solve new tasks

38:21.200 --> 38:23.040
more quickly.

38:23.040 --> 38:24.040
Interesting.

38:24.040 --> 38:31.520
It's funny when we talk about learning objects, object detection, and images, you know,

38:31.520 --> 38:37.800
the amount of data that is required to train a neural network to figure out what an object

38:37.800 --> 38:43.680
represents seems so large compared to our ability as humans to do it.

38:43.680 --> 38:48.520
This is an example where I would need at least a million examples of the French sentence.

38:48.520 --> 38:53.360
So not knowing if I didn't know French, you know, I can imagine needing a ton of examples

38:53.360 --> 38:58.360
of training examples for myself to be able to figure out the language and then how to

38:58.360 --> 39:00.920
put that together to make some furniture.

39:00.920 --> 39:04.520
But you know, if you spoke Spanish, you'd probably figure it out much more quickly.

39:04.520 --> 39:05.520
And that's true.

39:05.520 --> 39:06.520
Ah, this is true.

39:06.520 --> 39:10.360
And I think that actually there's something to that as far as how the learning-based systems

39:10.360 --> 39:11.960
can work better.

39:11.960 --> 39:14.720
I talked before about multitask learning.

39:14.720 --> 39:19.680
And one of the things that distinguishes humans from these learned models that humans

39:19.680 --> 39:23.760
are actually always doing multitask learning, we're always doing multiple things at once.

39:23.760 --> 39:27.600
We're looking for things in our environment, doing something, we're worrying about we're

39:27.600 --> 39:31.160
going to have for dinner, we're worrying about some other stuff, we're observing some

39:31.160 --> 39:35.360
interesting, you know, car that we see on the road over there, we're always doing many,

39:35.360 --> 39:36.840
many things.

39:36.840 --> 39:41.360
And perhaps a lot of our efficiencies actually down to this fact that we're never learning

39:41.360 --> 39:43.480
anything truly from scratch.

39:43.480 --> 39:47.520
Because we're learning so many things all at once, any new thing that we have to do,

39:47.520 --> 39:52.240
we get a broad basis of knowledge in which to draw to figure out that new thing.

39:52.240 --> 39:57.160
So in a sense, perhaps what we're doing is we're actually extremely broad kind of multitask

39:57.160 --> 39:58.160
learners.

39:58.160 --> 40:01.800
And maybe that's a big part of how we get that efficiency.

40:01.800 --> 40:05.320
And what's the relationship between multitask and transfer learning?

40:05.320 --> 40:08.840
Well, monthly task learning is one of the ways to get transfer learning.

40:08.840 --> 40:13.600
Right, so in multitask learning, we're learning multiple things in parallel.

40:13.600 --> 40:20.240
And in transfer learning, we are transfer learning is a broader idea that includes taking

40:20.240 --> 40:23.560
pre-trained models and using them, applying them to other things.

40:23.560 --> 40:29.280
I guess the direction that, you know, the curiosity that has been peaked is like, how do

40:29.280 --> 40:36.160
we combine all of these things to, you know, make our ability to train these models even

40:36.160 --> 40:37.160
faster?

40:37.160 --> 40:42.120
Right, well, so one of the things we've been looking at quite a bit actually is how we

40:42.120 --> 40:47.080
can use past experience to accelerate future learning so that we've worked on this in the

40:47.080 --> 40:50.760
context of reinforcement learning, supervised learning, and so on.

40:50.760 --> 40:54.680
There are a number of ways you can approach that problem, but they all sort of boil down

40:54.680 --> 40:59.800
to some version of looking at your past experience, breaking it up into, you know, little

40:59.800 --> 41:03.240
pieces of training data, little pieces of validation data, trying to build your model

41:03.240 --> 41:06.960
such that when it sees that little training data, it'll do well in that little validation

41:06.960 --> 41:10.840
data and do this many, many, many times so that you get a model that's basically good at

41:10.840 --> 41:13.760
quickly adapting to small training sets.

41:13.760 --> 41:17.000
There are different ways that you can construct these types of models that, you know, many

41:17.000 --> 41:21.920
other groups and end us have looked at, but that's sort of the big picture setup.

41:21.920 --> 41:24.200
These are sometimes called metal learning algorithms.

41:24.200 --> 41:28.920
I think that's actually an extremely promising direction for the future to really take deep

41:28.920 --> 41:33.200
learning methods beyond this regime of always relying on really gigantic data sets.

41:33.200 --> 41:37.840
And I think it goes hand in hand with multitask learning that basically that the way that we

41:37.840 --> 41:43.320
can get to the kind of efficiency that we've seen humans is by solving many tasks, solving

41:43.320 --> 41:47.840
those tasks in a metal learning context so that we're using the our past experience of

41:47.840 --> 41:51.360
solving old tasks to accelerate the solving of new tasks.

41:51.360 --> 41:54.800
And then when we encounter new tasks that we hadn't seen before, we'll generalize and

41:54.800 --> 41:58.240
quickly adapt to them.

41:58.240 --> 42:05.400
And does multitask learning necessarily imply a single network across all of the tasks or

42:05.400 --> 42:07.680
are there variations there?

42:07.680 --> 42:08.680
There are definitely variations.

42:08.680 --> 42:13.160
So one of the things that we've studied actually as well as several other groups is how we

42:13.160 --> 42:15.080
can construct actually modular networks.

42:15.080 --> 42:19.080
So networks that will have some components that are shared and some components that are

42:19.080 --> 42:21.200
distinct between tasks.

42:21.200 --> 42:24.680
And the nice thing when you build modular networks, actually the policy sketches paper you

42:24.680 --> 42:27.640
mentioned is an instance of this that also had modular networks.

42:27.640 --> 42:28.960
And you have modular networks.

42:28.960 --> 42:34.160
One of the things that you can observe is that there will actually be kind of interfaces

42:34.160 --> 42:36.840
that emerge naturally between different modules.

42:36.840 --> 42:41.880
So in a robotic context, let's say you might have a module for perception and maybe you

42:41.880 --> 42:46.480
have one module for a color camera and a different module for LiDAR.

42:46.480 --> 42:51.880
And then you have a module for actuation for a robot with four links and a different module

42:51.880 --> 42:54.080
for actuation for robot with three links.

42:54.080 --> 42:55.760
And you can mix and match any combination of these.

42:55.760 --> 42:58.560
You can say, OK, here's a LiDAR robot with four links.

42:58.560 --> 43:01.280
Here's a RGB camera robot with three links.

43:01.280 --> 43:03.480
Train different combinations of these modules.

43:03.480 --> 43:07.080
And then you can actually find that you might get generalization to new combinations of

43:07.080 --> 43:08.680
sensors and robots.

43:08.680 --> 43:13.160
And you can figure out that bottleneck between the two modules actually constitutes a kind

43:13.160 --> 43:15.160
of a learned interface.

43:15.160 --> 43:18.640
Because different modules, they have to basically adopt the common interface because they don't

43:18.640 --> 43:22.960
know who's going to be downstream from them or who's going to be upstream.

43:22.960 --> 43:26.080
And at the systems level, what are the implications of that?

43:26.080 --> 43:32.320
Is it then easy to take one of these modules and drop it into another system or does it

43:32.320 --> 43:33.480
not quite work like that?

43:33.480 --> 43:35.400
Well, I think that's part of the hope.

43:35.400 --> 43:36.840
So I think we haven't seen that yet.

43:36.840 --> 43:40.720
But in the long run, that's, I think, one of the really interesting things about modular

43:40.720 --> 43:47.080
network designs is that perhaps it could actually be possible to use this as a way to combine

43:47.080 --> 43:51.720
the benefit of intent learning with the benefit of modularity to be able to actually train

43:51.720 --> 43:55.320
up some component, let's say, if you're doing autonomous driving, you train up a particular

43:55.320 --> 43:59.160
vision component on one car, maybe supplement it with image net data, and then you just drop

43:59.160 --> 44:00.440
it into a different car.

44:00.440 --> 44:05.600
But then that different car has its own modules for, let's say, controlling the acceleration

44:05.600 --> 44:07.000
or something like that.

44:07.000 --> 44:09.720
So I think that that's part of the hope we're not quite there yet.

44:09.720 --> 44:14.200
This work is still in fairly early stages, but I think that's definitely a really exciting

44:14.200 --> 44:16.720
place that this kind of stuff could go.

44:16.720 --> 44:25.040
The scenarios you just described were all end-to-end trained, at least in the initial system,

44:25.040 --> 44:29.640
they're end-to-end trained as opposed to training module at a time, is that, right?

44:29.640 --> 44:30.640
Right.

44:30.640 --> 44:34.360
So that's actually the, that's the nice thing about modular neural networks as opposed

44:34.360 --> 44:38.520
to modular anything else is that neural networks can be composed.

44:38.520 --> 44:42.720
So if you have a modular neural network, you can still train the whole thing, a combination

44:42.720 --> 44:44.440
of multiple modules end-to-end.

44:44.440 --> 44:48.760
Now when I say end-to-end, there could be different ends, so end-to-end could mean that your

44:48.760 --> 44:53.640
vision system is simultaneously trained on image net recognition and feeding the right

44:53.640 --> 44:57.640
visual representation to a downstream control module to perform some task.

44:57.640 --> 44:59.160
Yeah, it's interesting.

44:59.160 --> 45:05.480
So the general question that I want to get out here is, I think the basis that you've

45:05.480 --> 45:11.240
laid out for end-to-end robotic learning makes a ton of sense.

45:11.240 --> 45:16.080
At the same time, when I talk to folks in industry about how they're using neural networks

45:16.080 --> 45:20.480
in deep learning, and I present this vision of, hey, we're just going to have this one

45:20.480 --> 45:26.040
uber neural network that can figure everything out, invariably, I get back some reaction

45:26.040 --> 45:28.680
that's like, no, no, no, we don't do it like that at all.

45:28.680 --> 45:29.680
It doesn't work.

45:29.680 --> 45:30.680
It's too hard.

45:30.680 --> 45:32.960
How do you account for the gap there?

45:32.960 --> 45:37.320
Do you see similar things, I guess, for one, and how do you account for that gap?

45:37.320 --> 45:42.720
Well, I think one way to look at it is it's a little bit like the difference between gasoline

45:42.720 --> 45:44.840
cars and electric cars.

45:44.840 --> 45:50.240
So it's very difficult to, right now, or maybe even like five years ago, to make the case

45:50.240 --> 45:54.200
that, well, everybody should have electric cars because gasoline cars are really good.

45:54.200 --> 45:57.800
We've been designing them and improving them for almost a century.

45:57.800 --> 46:04.160
So of course, you build the first electric car that's not going to be as nice as a gasoline

46:04.160 --> 46:08.080
car that's benefiting from all those decades of engineering, but I think the technology

46:08.080 --> 46:09.080
is progressing.

46:09.080 --> 46:12.280
So I think the reason that you hear, especially, you know, it's certainly in robotics that

46:12.280 --> 46:17.000
we get this a lot, that you'd like to be able to use something like a manually designed

46:17.000 --> 46:21.160
motion planner on top of your learned computer vision system because that motion planner is

46:21.160 --> 46:25.600
really good, like it's benefited from decades of development.

46:25.600 --> 46:27.880
So that's, I think, maybe the explanation.

46:27.880 --> 46:30.400
Now, the solution, I'm not sure.

46:30.400 --> 46:32.720
I think there are a couple of possible solutions.

46:32.720 --> 46:36.920
One solution is that, well, maybe we should see what makes that really nice, manually designed

46:36.920 --> 46:38.600
component work so well.

46:38.600 --> 46:43.000
Can we incorporate that into a learning system or can we use it as part of an intent system?

46:43.000 --> 46:46.920
So can we use that manually designed, let's say, controller, differentiate through it,

46:46.920 --> 46:50.320
compute gradients and use those to improve our vision system?

46:50.320 --> 46:54.000
Or maybe we just need to make a little more progress in reinforcement learning to the

46:54.000 --> 46:58.760
point where we can replace that component without a loss of capability.

46:58.760 --> 46:59.760
Right.

46:59.760 --> 47:00.760
Right.

47:00.760 --> 47:01.760
Great.

47:01.760 --> 47:03.800
Any other things that you wanted to cover?

47:03.800 --> 47:07.120
No, I think that's everything on my hand.

47:07.120 --> 47:08.120
Fantastic.

47:08.120 --> 47:09.120
Fantastic.

47:09.120 --> 47:13.880
Well, what's the best way for folks to catch up with you to learn more about what you're

47:13.880 --> 47:14.880
up to?

47:14.880 --> 47:19.200
I know you've got a webpage on the Berkeley site that will include a link to in the show

47:19.200 --> 47:20.200
notes.

47:20.200 --> 47:23.840
Are there any other ways for folks that you'd like folks to get in touch with you?

47:23.840 --> 47:25.880
I think my website is a good place to start.

47:25.880 --> 47:30.400
Also for anybody who is interested in learning about deep reinforcement learning, we do have

47:30.400 --> 47:35.360
a course that I and Chelsea and our colleague John Showman taught at UC Berkeley, and that

47:35.360 --> 47:36.680
all that material is online.

47:36.680 --> 47:40.320
So if you search for Berkeley deep reinforcement learning course, you can find all those

47:40.320 --> 47:41.320
lectures.

47:41.320 --> 47:42.320
That can also be a good resource.

47:42.320 --> 47:45.720
But yeah, for getting in touch with me, definitely my website will be a place to start.

47:45.720 --> 47:46.720
Fantastic.

47:46.720 --> 47:51.040
Well, I'll make sure I include the link to the course in the show notes as well.

47:51.040 --> 47:53.360
And thank you so much for being on the show.

47:53.360 --> 47:54.360
Thank you.

47:54.360 --> 47:57.360
Thanks, sir.

47:57.360 --> 48:00.560
All right, everyone.

48:00.560 --> 48:02.880
That's our show for today.

48:02.880 --> 48:07.960
Thanks so much for listening and for your continued feedback and support.

48:07.960 --> 48:12.720
For the notes for this episode, to ask any questions or to let us know how you like the

48:12.720 --> 48:20.480
show, leave a comment on the show notes page at twomolai.com slash talk slash 37.

48:20.480 --> 48:26.720
For more information on industrial AI, my report on the topic or the industrial AI podcast

48:26.720 --> 48:31.480
series, visit twomolai.com slash industrial AI.

48:31.480 --> 48:34.280
The report is complete and it's beautiful.

48:34.280 --> 48:37.360
And I'll be notifying folks who sign up at that page.

48:37.360 --> 48:40.040
How they can receive a copy of it shortly.

48:40.040 --> 48:46.920
Once you're done with this show, take 30 seconds to head over to twomolai.com slash AISF

48:46.920 --> 48:52.840
to enter our giveaway for a free ticket to the AI conference in San Francisco in September.

48:52.840 --> 48:55.920
You could be one of two lucky winners.

48:55.920 --> 48:58.160
Thanks again for listening and catch you next time.

