All right, everyone. I am super excited to be here with Peter Abil. Peter is a professor at UC
Berkeley, where he's co-director of the Berkeley Artificial Intelligence Research Lab,
or Bear, as well as co-founder, president, and chief scientist at CoVarians. Peter, welcome back
to the Twimal AI podcast. Sam, really good to see you again. Thanks for having me.
It is really hard to believe that it's been just under four years since you were on this show.
You were guest, your interview was number 28, and we're approaching 500 now.
What a journey. What a journey. You've been up to a lot of cool stuff, including starting your
own podcast. We'll talk a little bit about that. But since it's been so long and you've been up
to so much, why don't you take a few minutes and share with our audience a little bit of background
and what your primary research interests are. Sure, yeah. So four years ago, long time.
So a lot has happened since. The things I'm mostly thinking about these days are kind of too
fold. One is with my covariant hat on as co-founder. At CoVarians, we're trying to bring
AI robotics into the real world. So take it out of the lab, out of simulation, and make robots
do things for us in the real world. So law happening there and looking forward to talking more about
that with you, Sam. And then my other had at Berkeley, we're doing academic research in AI and
robotics. And I would say some of the things we're most excited about these days are work at the
junction of unsupervised learning and reinforcement learning, where of course unsupervised learning
is where the learning happens without any annotation of the data, which is nice because it can be
a lot more efficient that way, no annotation needed. But then reinforcement learning, the trial,
and error learning that can allow robot to acquire skills from its own experience, its own practice.
And so the combination of those two is we're spending a lot of time right now at Berkeley.
Awesome. And we'll dig into both of those areas in detail. But before we do, for those who
didn't catch your podcast four years ago or don't know your story, how'd you get into robotics?
Sure. Yeah. So for me, in some ways, it didn't start in robotics per se. It started with artificial
intelligence, the kind of brain behind the robots. For me, it was, well, when I was in underground
studying all kinds of things, it just seemed everything was interesting. But it also seemed that
to do well, he got to pick something to specialize into. And so the question was, okay, what is
going to be the most interesting thing to spend my time on? And for me, it became pretty clear that
it's so intriguing that we as humans can think. It's kind of what sets us apart, I would say,
mostly from other animals is that we can think deeper, harder than that most animals. And so
that to me was so fascinating. And of course, the first thing you might think is, okay, then study
neuroscience, because that's studying how the brain works. But that discipline just, I mean,
fascinating, but it just seems so hard to make progress on to, you know, dissect the brain and
understand how it works. It's so difficult. So I figured try to engineer something that thinks,
seems more natural as a way to make some progress and get closer to what we're doing as humans when
we're thinking. Do you follow neuroscience research at all? Does that inspire you?
Oh, there's a lot of things in neuroscience that have inspired me. I think it's,
it's really interesting. The thing that maybe inspires me most these days is these findings about
how general the human brain is. So there are these findings studies from several years back now,
where it was shown that you probably heard us before, Sam, but it was shown that you can, for
example, put the electrode pattern on a person's tongue. And this person can be blindfolded,
it can be, can be blind. And if the electrical pattern on your tongue is active in the same kind of
pattern as an image that you would see with your eyes, your brain can learn to process that and see
and they had his demonstration where somebody was effectively rock climbing while seeing through their
tongue. And so this kind of thing to me blows my mind down. The brain is so general what you use
for taste input can be used to see. And so yeah, I think it's really inspiring to go that direction
of generality. Absolutely, absolutely. Let's maybe start with talking about what you're up to
at covariant. I think the last time we spoke, if I remember correctly, you were a year or so out
from founding covariant. Yeah, so last time we were a little, yeah, still about a year before we
started covariant. And what happened is we started seeing this trend that it seemed a lot of progress
had been made by awesome and by many others on building more intelligent robots. And that always
seemed a missing piece. If you look at robots out in the world that are doing useful things for us,
you go to a car factory and you look around, it's amazing. So many robots, right? And they're building
cars and that's, that's amazing. But if you look at the details, these robots are doing the same
motion effectively over and over and over, very diligent. But also, it requires a lot of structure.
Most problems cannot be solved by just repeated motion. And so it seemed three, four years ago,
it seemed that maybe time had come where we can make robots smarter. Enough research
progress had been made to start taking on bringing this to the real world, to give robots
the ability to think and see and think and react to their environment, rather than just following
pre-purgant motions. And that seemed just like it would open up so many more opportunities
than what was possible until then. And so that was for us really to trigger this notion. Okay,
we think we can start building robots that can see, react, think, and do many more things
down pre-purgant robots can do. Now, the opportunity for robots that can think is obviously quite
broad under specific tasks or problems that you're going after, is this the classical
pick and place kind of robotics problem? Or are you beyond that? How do you think about the
problem domain? It's so interesting. You bring that up because you say pick and place, right? And
it's exactly what we're looking at, but it's not how we started out. So we started out and we said,
let's do a full investigation of what are the most pressing problems when we talk with
manufacturing companies, with warehousing companies with agricultural firms and so forth
construction. And we spent the first half year of Covaran talking with about 200 different
companies and getting a sense for, especially if you could have a really smart robot that
given its physical foreign factor, you know, a standard industrial robot,
but it's more smarter, what would it be able to do for you? And many, many things people want
wanted robots to do, but it became also very clear that in logistics, slash warehousing,
distribution centers, e-commerce fulfillment, that's where everybody was just
hurting for help. They wanted robots to come help. They had already automated.
What effect will is done with legs? They're running around the warehouse. A lot of people know about
the Kiva robots at Amazon that can go under shelves and bring shelves to people. There's other
systems too, but the same idea that the leg work has been automated, but that's a structured
automation problem. It's not an AI problem in the same way that we're looking at AI these days.
But to hand work, what people do with their hands, it just, there was no automation for it,
and they wanted. They want to complete that automation process of these warehouses.
And after six months, it became so clear to us that's the problem. We need to focus on,
at least first, hopefully we can generalize from there, and we think we will, but that's how
our initial focus is effectively pick and place type solutions for warehousing logistics,
fulfillment, and so forth. I'm imagining that a lot of the progress in that domain to date has been
adapting the robots to the very specific form factors of the problem that someone is trying to solve.
We've got, you know, these boxes, we're going to align these boxes in this way, and so that the
robot can play some smaller part. And maybe a part of what you're trying to do is loosen those
constraints by making the robots smarter and more agile in their ability to manipulate in the
real world. That's exactly right. So when you go into a warehouse, there is already a lot of
conveyors and mobile robots and so forth that bring things where they generally to be, but they don't
bring individual items. They tend to bring shelves or boxes. And then the next step is for
either a worker or a robot to look into that box and pick out the one item that's needed
to fulfill the next order. And to do that, you actually need a very general kind of capability. And
I think that's probably a bit surprising to many people who are not familiar with warehouse,
but many of these warehouses have millions of different skews, which are stockkeeping units. And
these skews also turn over very quickly to have different packaging and so forth. So you cannot,
it's not enough to say, oh, these are the skews. I'm just going to set up a system that's ready
for these specific skews. You actually need to somehow build a AI system that understands the
general notion of object that can look at a box, items in the box, especially the box itself
is easy, but the items in the box and understand, oh, there are items there. And there's an item over
there, another one over there, there's overlapping items there. And that's the one I want to pick.
And here's how I'm going to pick it where I'm going to place my suction cup or my gripper.
And this is how I'm going to maneuver it out of that box without flinging anything else
out of the box too. And for us, as humans, these things are very simple to do. You can do them
without thinking you can be, you know, maybe listening to a podcast or something while you're
doing this. I think you about was being talked about there. But to get a robot to do that
is actually really, really hard. It's one of those things where, sure, it's not too hard to get,
you know, 80% success, but to get to the levels of autonomy that you need to be
trustworthy in a commercial setting, several nines of reliability, 99.9% and above,
all of a sudden becomes really, really hard because this system now needs to understand
pretty much any situation that encounters. And while most people proud and not so familiar with
warehouses, it's very similar to self-driving cars. We've had demos of self-driving cars for
10 years or longer. And the demos today don't necessarily look all that different if you watch a
one minute, two minute demo of a self-driving car, but they've become more reliable. But it remains
very hard to get to perfect or near-perfect reliability, of course. That's the challenge at
long tail of always new things you can encounter, either as a self-driving car and that demand or,
as a warehouse robot, where you're always seeing new items, new packaging, and somehow you need to
make sense of that and reliably pick in place. It's maybe an interesting analogy. I'm thinking of
many folks in the field. Maybe Andrew Ng is one of the most well-known who has kind of talked
about this view that, you know, self-driving cars will first happen when we constrain their
environment, you know, maybe the autonomous bus lane and an airport as opposed to general self-driving
ability. We've already done a lot of that work in the industrial realm. You've talked about the
conveyor robots and how we've, you know, we've really constrained that environment. Is there,
you know, does that say that maybe the, you know, that's not going to be enough even for
self-driving cars or is that, you know, pushing the analogy too far?
Yeah, I think, I think it's a really good question. The self-driving car side of things,
is it possible to create real value within constrained environments? For example, the natural one
would be lanes dedicated to self-driving cars. They might make it much easier, probably,
would be much easier, but at the same time, I mean, where are those lanes?
Do the marked them? Trains are like that, of course, and I mean, trains, you know, not to step
on the railroad tracks, because that's dangerous. That's not where you're supposed to be, right?
And so definitely I could see a version of if you go that far and really delineate,
this is self-driving car territory only. I suspect today's technology would probably create
a lot of value, but at the cost of a lot of real estate, being taken up by those dedicated
tracks and so forth. Proud that what's happening there, but of course, I'm not the one pushing
self-driving cars personally, but have a lot of friends doing that. It seems highways,
is sufficiently constrained, because typically, people aren't walking around there, and, you know,
everything is cars and motorcycles. So there are different environment there.
I think what I'm wondering is, and this is maybe a bit of a tangent, but are there things that
you've learned in your experience, in factories, that, you know, inform how you might approach
the self-driving car problem, and this whole constraining the environment, if you were to,
you know, decide to go try to solve that problem tomorrow.
No direct lessons in that way, but I think there's a very strong analogy in a different way,
which is the chasing the long tail of events, whether it's items or configurations of items,
or it's traffic situations, the kind of getting 80% coverage isn't too hard, that that's very
feasible with, you know, a relatively fast effort. But getting to the reliability that you need
to actually function, well, there's this long tail of things that you barely ever see.
Each one of them, you barely ever see, but when you add it up together, there's a lot of it.
And so you need to somehow observe all these rare events that together add up to non-rare
probability mass. So that's why it's so important. There was just one rare event that happens once
in, you know, a million years, who cares, but it was a million of those events that happened
once in a million years, all of a sudden, every year you have an event, right? And that's kind
of what's happening is there is this long tail of very rare things that you somehow need to train
your neural networks. In most cases, I imagine, self-driving people train your neural networks too,
but definitely for us at Coverin, train your neural networks to have a more general understanding
of objects, their shapes, their graspability, their interactions, when they're in a bin,
when you try to pick one out, or when you place how that interacts with each other,
don't understand that for every scenario you might encounter without having the ability to train
on every scenario, you need to train something that generalizes. And I think that that is kind of
the case in both situations, whether it's self-driving or warehouse picking, you need that general
realization ability. I remember from our last conversation, you were very staunchly for end-to-end
deep learning. And I wonder if your experience solving problems in the real world and industrial
domain has changed that at all. You know, we talked about end-to-end versus, you know, ensembles
and incorporating physical knowledge of the world, that kind of thing. Do you still feel
strongly about single end-to-end deep learning model? So I think, yeah, you're absolutely
right with your observation about what we chat about last time. And maybe I've adjusted things a
little bit, but I'm still very close to where I was last time. So first of all, I would say there are two
different things to think about. So one thing to think about is research, academic research,
when you write papers, where do you think most of the novelty surprises, new things are going to be?
And my sense is that while in the other hand, when we put things in the real world, often
complementing it with the chronology you have from physics and so forth can give you a stronger system,
from a purely research academic point of view, my sense is that those more classical approaches
have been researched so well already that there is less room to push the boundary in terms of
novelty and surprises and things that, you know, oh, wow, then expect this to be possible in
now a computer AI robot can do this. And so in the academic realm, I'm really big on
going as purely on learning as possible because there is the most novelty there.
On the more practical thing, which you're, of course, it's stupid to throw away anything that you
know has, you know, guarantees and is guaranteed to work. If you know that, you know, you have
maybe a 3D understanding of the environment, well, the notion that the environment is 3D and then
you can build a 3D map of it or at least a depth map or whatever you want to build, it seems
stupid to not not use that kind of notion and just say, hey, we'll never tell the neural net
explicitly that, you know, 3D matters. It seems a waste of it.
We've been sounds crazy saying it.
Yeah, that said, here's one thing that we found that I think that ties into this, which is
the tricky thing with coding in prior knowledge is that it's very hard to be super precise
about prior knowledge. And so whenever you think you're encoding prior knowledge,
you might be encoding something that's actually slightly untrue, slightly incorrect in some
situations. Can you give an example? So let's say, here's the simple example. Let's say, say,
hey, a good way to pick up an object is go to the middle of a flat region with a suction cup,
go to the middle of a flat region. That might seem like a good heuristic and say, okay,
that's where you should go. And obviously, if you, if you code that in from day one, it'll work
better than if you say, hey, just learn where to pick something because now it has to figure out
that the middle might be better than an edge and so forth. But the downside of hard coding in
that the middle is a good spot is now all of a sudden maybe have overlapping objects.
Maybe the middle is not a great spot to reach or you need to kind of slide things out another way
or maybe it's an object that has a center of gravity that's not even close to the middle.
Or maybe there is a way to place multiple suction cups off center in a way that is actually more
robust than placing things right in the middle. And so what then happens if you hard code and you say,
well, we're just going to hard code that once we understand the scene, you go for the middle.
Well, all of a sudden, how do you now special case around that? And you're kind of stuck.
And so the general philosophy I like is this notion that if you have prior knowledge, don't hard code
it into your system. Instead, use it for effectively data generation. It can any prior knowledge can
usually be turned into a data augmentation or a data generation scheme. And so you might be able to
generate a bunch of example grasps that way for thinking about grasping. And they're good training
data because they're good. They're not necessarily perfect, but they're good and they'll help you.
But I think that's ultimately going to let you keep improving as you encounter more corner cases
where a data driven approach will give you an exception to get an exception exception without
if then else, if then else is just the more data speaks for itself. And the neural net will absorb
that. And so I think that that's really the way I tend to see it in the long term is use all that
prior knowledge to help you train the neural network because that prior knowledge can help you generate
so much data for your neural network. That's a really interesting way to unify the two ideas.
When you approach the types of problems you're approaching,
imagining that ultimately you're trying to build a product or a platform that is as general as
possible, but there's a lot of kind of custom consulting-ish customization for each individual
problem. I'm curious if you could speak to that and how you've approached it.
Yeah, I think that is exactly the trap to avoid. If you try to build a sustainable company,
if you get into a ton of one-off consulting efforts, you're just like, you're not building a product,
you're not building something that you can repeatedly deliver. But at the same time,
whenever you're asked to deliver something to a customer, it has to be what they need. You can
just say, hey, we built not what you want, not what you need, because then they'll just be like,
yeah, well, then we don't want it. If there's going to be exactly what you need, but it's going
to be not exactly what 10 customers need, too. Exactly. There's a natural tension there,
and the way we've been thinking about this is, first of all, first of all,
doing a lot of market research. As we did the market research, we saw, effectively,
big picture wise, we saw a recurring theme of more intelligence for robots. If we can build
something that can make robots understand their environment, react to it, that's going to be able
to power a lot of different solutions. Then zooming in on the beach heads of the
say of what we're delivering our technology right now, we saw that in warehousing, there are
really three main applications that we decided to cater to. The first one is order picking.
It's where a robot is presented with items that were in storage and needs to pick one at a time
to put it into a shipping container, and then things get chipped off. That's order picking. Then
a second one is put-walling, or sometimes put-to-light. In the put-wall, what happens is many orders
come into a warehouse, and somebody or a robot has gone around the warehouse and collected the items
for all these orders, but it might be for 100 orders or even more. It's all collected, let's say,
in a shopping cart or in a bin. It's not sorted by orders. They collected everything needed
for fulfilling those maybe 100 orders. Now that shows up at a put-wall, and now the robot's job
is to take one item at a time out of that shopping cart effectively, and then scan it and place it
into a temporary storage that is associated with that specific order until that order has been
completely filled from that shopping cart, and then somebody behind that put-wall will take it
and pack it up and ship it off. That's put-walling. The third one is parcel certification,
induction sorting, where there's a lot of parcels going around these facilities, and they often
need to be simulated, because there's a bulk of parcels coming in. Can you reliably place
them one at a time onto a conveyor so that it can go through scanners and so forth to be sorted
to the right outbound truck, let's say? These three are the ones that were focused on at first,
where we can build something that's very channel, so anybody who needs a put-wall, anybody who needs
order picking, anybody who needs induction sorting, we can deliver to that without having to do
one-off consulting for each specific deployment. And are those problem definitions
constrained enough that you can put-walling, for example, you can cover all or the vast majority
of the use cases, or are they further constrained by, for example, the individual skews have to be
in boxes as opposed to loose bolts or things like that. How granular does the strike zone need to be?
Yeah, it's really interesting to ask that, and I'm glad you're asking because I should have
mentioned that anyway. So what's so interesting about how we're building this is that, at least
one, I'm so excited about is that we're building a single system behind all three application
domains. So the same neural networks are trained for put-walling and for order picking and for
induction sorting. And you might think, well, why? I mean, those are somewhat different problems,
but the core is still the same. The core is you look at a bunch of items. Sure, parcels versus
for put-wall often small items versus order picking a range of sizes of items, but you're still
at the fundamental level looking at a bunch of items and making sure you're picking reliably one
at a time, possibly scanning and then placing. And so it generalizes across all three of those,
and then within them, as you said, some facilities might be focused on cosmetics. Others might be
focused on pharmaceuticals, others on apparel, yet others could be focused on maybe electrical
supplies and so forth. And again, it's all the same neural network for all those domains.
And I think that's actually the best way to do it because even within a single domain,
you cannot cover everything. There's so much turnover on all these skews that and packaging
of the skews that you cannot cover. You can now say, these are the 100 items we need to do. Let's
have something specific for that. You need to have some that really generalizes. And by building it
in an even more general way, then let's say just cosmetics, you actually build in more expertise
into the neural networks about what it tends to see, what makes for an individual object,
what is the 3D of the situation it's looking at, what's the best way to approach, to grasp,
to remove, and then what's the fastest trajectory to bring from point A to point B and so forth.
And to be clear, are your models primarily focused on the manipulation task or whatever the
generalization of that? I'm assuming a given robot might have some set of tools that needs to
select and you need to identify the best contact point and a variety of factors associated with
let's call it manipulation. And then, you know, I imagine that beyond manipulation, there are other
parts of the problem that I'm thinking a more like traditional optimization and not necessarily
places where you require neural networks. But I guess that's what I'm asking you to correct for me.
What's the scope within these three problem domains of the models that you're building?
Yeah, so there's a couple of interesting things that you're touching upon there. One is that
indeed, not every item can be picked with the same end-of-factor. And so there are things
called tool changers where robot can change the end-of-factor on the fly and say, oh, actually,
I should use gripper now, I should use two suction cup or maybe six suction cup end-of-factor.
And so you can switch that as needed, though it turns out surprise, somewhat surprisingly,
initially not surprising anymore now, suction cups go very, very far in most warehouse facilities.
Now, what's also interesting is that the neural network is the same when it's thinking about how
to grasp them and then play these objects, whether it's suction cup or gripper and independent
the number of suction cups it's using. And this is going a little bit deeper, but the simplest way
to think of is imagine you have a single neural network, the main body of the network that thinks
about grasping. You don't just give it the scene that it's looking at to grasp in, but you also
give it the configuration of the end-of-factor. And based on the combination of scene and end-of-factor,
it decides what to do. And that way, again, you can generalize beyond what's possible otherwise,
which is really cool, so you don't have one-off training for new end-of-factors. No, it's all shared
learnings across every deployment, every type of end-of-factor. Just punching it with a question
here, are you also giving it a tabular style data associated with the product, like skewed
dimensions, that kind of thing, or are you primarily focused on visual or sensor inputs?
So, that data can often be made available, but not always. We prefer not to rely on it,
just because it can be a pretty strong assumption to rely upon that.
But I mean, there are places where it can be made available, but we prefer not to rely on it.
Okay. Yeah. And the other thing you brought up is kind of optimization. Yes, optimization
matters a lot. It turns out speed is, I mean, reliability is key, but then once it's reliable,
speed also matters. If you're about, you know, works at half the speed is creating, half the value,
and in a facility like, you know, warehouse or factories and so forth, if you're not keeping the
pace, you might be bottlenecking the entire process. So, keeping the required pace is really
important. And so, there's many methods to optimize robot trajectories that can help do things,
but actually things get really interesting once you're holding objects down, you know,
might be flinging because they're heavy and they're floppy and so forth. And all of a sudden,
you know, you can go about it as analytically anymore, as you would do when you just think
about the robot itself. And all of a sudden learning, which you thought maybe it doesn't matter
for motion, all of a sudden, oh, actually learning can matter again to do better than you can without
learning. Interesting. Kind of continuing that example of objects flinging, you know, there's a
suction cup. Every once in a while, something's going to drop. Do you consider that kind of
out of scope and, you know, every night a human goes around and picks up all the drop things and
rebends them? Or are you also trying to build intelligence into the robots to remediate that,
pick up the items that fall or some other, some other scenario? Yeah. So what you're getting at there,
I think, is at its core, why? Warehouse robotics is such a great place to be in that you need to
be very reliable. But if just every now and then something needs a human intervention,
that's okay. If a person needs to spend 10 minutes, let's say at the end of the day, to clean up
a couple of things that the robot did, it's no problem. I mean, obviously you don't want to
rub it to break anything, but if it just puts something next to a bin, it drops it in, in trajectory.
Of course, we don't want it to happen. We want it to be always 100% reliable. But the beauty here is
that you can create a lot of value once you are, let's say, 99.9% reliable, which is a really
high bar and is very hard to meet. But 99.9% is not 100%. And 99.9% maybe wouldn't cut it for
self-driving. You might not want a 99.9% self-driving car on the road, because I mean, it means, you know,
one in a thousand times, let's say crosses an intersection, if that's how we count, it gets into
an accident. That would be 99.9% reliability on crossing intersections. That's not good enough.
You would never deploy that. But if you think about, and that's really where commercialization
is what we think about in great detail is, when does the robot start creating value? And so the
value creation in our perspective, everything we've seen with end customer starts roughly at that
99.9% mark, because the robot would typically do anywhere from 500 to 2000 operations per hour,
depending on the type of station. Some stations have to be fast or some stations have more
involvement, like extra scanning and so forth, they're a bit slower. But even at a slow station, a 500
per hour station, at 99.9%, it means, well, you are just making a mistake as once every two hours.
And that means nobody has to constantly manage, babysit this robot. It's, you know, once every two
hours, sure, let's quickly fix something. And that's creating real value. And that in my mind is
kind of the benchmark is 99.9 is, of course, we want we're looking to go further than that.
Don't get me wrong. But that's where it becomes just, yeah, robot is helpful as opposed to
robot needs a babysitter. Yeah. So I could continue on talking about industrial AI and robotics for
the entire episode, the entire interview I've written a bit about industrial AI and I find it
a fascinating topic. But I also want to touch on some of the research interests that you mentioned
earlier. In particular, you talked about this junction of unsupervised and reinforcement learning
and some of the work you're doing there unsupervised learning is something that has been a goal
from a research perspective for a while for its data efficiency. But it's really played out
in big ways in natural language processing recently. Talk a little bit about, you know, that junction
and where you see it being interesting. Yeah. So as you know, Sam, I've been working on
reinforcement learning for a very long time, right? And reinforcement learning is,
well, it's trial and error learning and it's something we're very familiar with as humans when
when we see a child learn to crawl, learn to run, that that's reinforcement learning and when
we train a dog to, you know, maybe sit, that's reinforcement learning when we say sit and it
sits, we'll give it a treat and if it doesn't sit, we might yell at it and from that feedback,
it figures out what it means to sit and it requires that skill, right? Listening and executing.
And so that's reinforcement learning and a lot of the big successes in reinforcement learning that
we've seen so far have been in simulation, right? So arguably that the most famous success
alpha go, right? The best computer go player and beating the best human players out of deep
mind, reinforcement learning played a very big role in that because it was playing against itself
over and over and over to become better. But it's all in simulation, same with many video games,
a lot of simulated robotics results. But the question is when when you try to transition from
simulation to real world, there are a few things you can do, of course, you can say, hey, maybe
that my simulator can be perfectly matched with the real world and that can help you, but
ultimately you want to be more data efficient. You want to not just learn from reward, you want
to learn in other ways, you want to not just, you know, do something for a minute and at the end,
nobody will ride or wrong. No, you want to have feedback. That's that's much denser, much,
much more informative. And naturally reinforcement learning doesn't really do that in it's kind of
vanilla form. But we've seen, as you said, in natural language processing, we've seen unsupervised
learning, which is learning from just vast amounts of text. And so to make the analogy there,
maybe natural language processing, you want to classify the sentiment is this a positive or a
negative review, or is this a, I don't know, a correct English sentence or a broken English sentence,
things like that. You want to learn to classify that. And instead of just generating examples of
positive and negative articles, you would instead first, because that would take a lot of annotated
training that you'd first train on just predicting the next word in a sentence on all the text you
can find on the internet. Once you're really good at that, you must have internalized something
inside the neural network about language and its meaning, such that now from a few examples,
you understand positive or negative sentiment classification. And so the question is, can we do
the same thing in reinforcement learning? Can we have this auxiliary unsupervised task that has
a lot of signal that's not directly the reinforcement signal, but still signal to learn from for
the neural network to then quickly acquire a new skill. And so we've been working on that quite a
bit in the last year, year and a half. And so what we've seen, and it's not just us also some
some people at NYU Montreal and so forth, we've actually been able to bridge the gap between
what happens when you learn, let's say a robot has to learn some skill, let's say running or
or maybe crawling and so forth, when the robot has to learn it, we're having access, full access
to its entire configuration at all times, all its joint angles, its position, its orientation,
that's full access, full state access. That always learned pretty fast, but it would learn
slow if all I guess you see is a video stream of itself, because that's maybe a hundred by a
hundred image, so 10,000 pixels, it's much higher dimensional than the succinct state description
of the robot. And so what we saw is that massive gap, the learning efficiency, when learning from
access to state, which is quite efficient, compared to learning with access to image input only,
was just this massive gap. And so the question is how do we bridge this, and we did it with unsupervised
learning effectively. So as the robot is doing its trial and error, instead of only paying attention
to rewards and trying to become better based on what's as good or a bad run so far, it is also
doing unsupervised learning on the images it's seeing. And what it means specifically in our
case is we did something called contrastive learning. So in contrastive learning, what you do is you have,
you want to learn what's in an image, but how do you learn it if nobody tells you what's in an
image? Well, here's the idea, imagine you just download two images, and you don't know what's in
them, you have no idea what's in them. But now for one of the images, you make a duplicate,
but a modification as you duplicate it. So maybe you have an image and you crop it in two different
ways. So now I have two different crops of the same image. And then there's the other one.
Now the two different crops of the same image, even though you don't know what's in it,
you know they have the same thing in it. And it's different from what's in the other image.
Most likely, if you randomly download the image, most likely, the other image has something else.
And that's really where the signal comes from. Now you train your neural network to understand that
these two crops of that same first image, neural networks should know that's the same,
doesn't know what it is, but it should embedded somewhere in a space where it puts those two
close together. And the other image should be far away from it. That idea turns out really,
really powerful. It's been shown to work really well by Jeff and then collaborators, as well as
Camminghan collaborators on doing that on images followed by image recognition training. And it's
very, very efficient. We can do the same thing in reinforcement learning from images. So you apply
the same idea. What the robot is seeing now, first of what it's seeing in a different time,
well, thing is seeing now you take two different crops. And that is still the same thing. And the
neural network learns that there's two views of the same thing and different from the other thing.
Very simple idea conceptually, but very, very powerful. You train that way all of a sudden,
you can train almost as efficiently from image inputs, ask with direct access to state.
This one we evaluated this on the kind of standard deep mind control suite simulated robotics
environments. In the, there's a whole area research around
multitask learning that seems to suggest, although, you know, there's certainly arguments against this
that networks generalize better when they have more to do essentially. I'm wondering if
part of what you're seeing is the effect of just giving the network something else to learn,
as opposed to the unsupervised task, contributing to kind of the core reinforcement learning
thing that it's supposed to learn. So you're absolutely right in that multitask tends to help
a means why when, you know, for example, at Coverand, we train the same neural network across
many application domains because it'll help to train across all those domains use the same network
rather than specialized to each one of them. Same in the research here when, when we train a
neural network for multiple tasks, often it can do better. But the beauty about bringing the unsupervised
losses that the other, you can think of the unsupervised loss as multitask where the multitask
additional thing doesn't require you to do any annotation, doesn't require you to give any
reward signals. So it's it's like the cheapest way to achieve multitask. There's a few
men out for cheapest ways. Still a lot of compute required just just to be clear. Still a lot of
compute involved. Yeah, I think the the analogy that I was coming from is like in, you know,
an LP when you're training birth or something and you you have this unsupervised formulation of
the problem. The problem that you're trying to solve in a lot of ways is directly related to the
you're in building the language model, you know, blanking out the words is like that's, you know,
teaching you language and I'm wondering if there's a distinction between the second task, you know,
is the second task it just happens to be an unsupervised task. And the relationship between that
and the core reinforcement learning task that you're trying to solve. I'm not sure that I'm
completely articulating the question in a clear way, but the the idea is is the unsupervised
task kind of core to does it inform the core reinforcement learning process and that loss or is
it just an ancillary other thing and it's the the multitask aspect or the other task that is what
you know, creates value or causes you to increase performance. Right. So you're actually
getting at something that we're working on right now. So let me say a bit more about that. So
to very directly answer your question, I think when we use the contrastive loss on a given image
compared to an image at a different time, we're not tying it very directly to the
locomotion task versus crawling versus maybe sit or maybe, you know, push an object. It's not
very closely tied to that. I would think I'm inclined to think of it as it's like learning
division system of the robot. It's it's learning to see and understand, you know, that it's
saying the same thing in these two situations versus seeing something else there. But it's not
learning about how the world works and that's kind of what you're getting at and reinforcement
you're trying to achieve goals. Your robot's supposed to get from point A to point B or
achieve something with the objects in front of it or it could be in a game you're supposed to
get a high score in the game and so forth. And so this whole notion of achieving goals and how
the world works, I think is a natural complementary aspect. So imagine you in the unsurprised sense,
you've been able to complement your reinforcement to get a bit of a vision system trained. Now what
you also want is effectively a how does the world work system such that when I'm asked to do something
in this world as a robot, I already kind of know how the world works. I don't have to flail my arms
a gazillion different ways and no, I don't have to learn that if I don't touch an object it's not
good move kind of thing. I know that that the world works with contact forces and so forth. So
if I want block A and block B, well block B that's on on the bottom has to be on the table first
and then those things are things that don't come from what I just described the contrast of learning
I just described. You need a temporal aspect to it. And so that I think is one of the important
next steps. It requires more compute and that's probably why it's coming a bit later in the progression
of research overall. But the notion there would be can you understand what are natural video sequences.
So if you download a lot of video, what does it tend to look like? And that's a very hard problem
because video is very high-dimensional. I mean that training or neural networks for video
prediction or understanding which videos are more related or less related to each other is
computational and intense problem. So it's the kind of problem that I mean I'm sure you're well aware.
Jan LaKoon has been talking about for a very long time when he gives the cake analogy of
reinforcement learning is the cherry on the cake is the reward signal but the unsupervised is
the most of the cake right and the icing is the supervised learning unsupervised is most of it
what he thinks of his video in the context of robotics at least he think of video robots have
watched so many videos that they know how the world works a lot of research still has to be done
there but I think that will be really important. The third part so there's the understanding what
you're looking at is part one we made a lot of progress on that then video understanding behavior
understanding in that sense how the world works the third part is for the robot to have its own skills
and that's where it gets much closer to what you're talking about is something we're actively
working on which is can you let the robot just spend time on its own just the robots just on its
own in a room or wherever it is and can you make it spend its time meaningfully effectively play
what what for children we would call play you say oh kid is just playing but actually yeah kid is
just playing you might say well as if it did some chores or whatever but actually the kid is
just playing it's actually learning about the world it's the kid now understands especially young kids
they understand how now objects interact understand how the world works from interacting with it
and that kind of play is another important component if I think about the long term future of
robotics if you want to get to less supervision less need to train everything into the robots just
let them try things on their own why would why would that kind of play be different from the
goal-directed exploration that we do today in RL so there is a good amount of work that's
already happening in RL and essentially getting this kind of play to surface so goal-directed
exploration is a great example there's other work that you know kind of similar but it's called
then curiosity or something like that yeah essentially you give rewards for experiencing something
that you have an experience before and I think that's worked really really well and environments that
are I would say somewhat closed meaning if you think about let's say an Atari game which is a
popular reinforced learning benchmark environment usually there's a right way to play the game and
there's not too many other things you can do or you die and that's what I mean with a kind of
closed environment you there was and if you so if you're curious and you try to experience new
things all you you've died many times in the game that's not new anymore so the the new things are
the things that take you to the next level in the game and so there's a very natural alignment
between novelty and the actual task that we care about and that's why curiosity,
goal-oriented exploration so forth have had a great amount of success
but that breaks down in the real world where there are so many things you could do
or even in some games like Minecraft which is becoming a new benchmark that people get excited
about for this exact reason in Minecraft you can build so many different things and so
just experiencing something novel you can keep doing that forever and never learn something
interesting and I think that's kind of the next step there and that's maybe where I think of
things like children's play as being a bit different because it seems like somehow
they have a bit more intuition than our current AI systems about what's interesting play
as opposed to just what's what's novel but actually not not that interesting interesting interesting
I want to switch gears to a paper that you recently posted up on archive pretrained transformers
as universal computation engines tell us a little bit about that work and what you're looking to
achieve there yeah so this paper for me was one of the most surprising things we've done usually
I feel like when we write a paper we have a pretty clear intuition that we know ahead of time
okay this intuition should be leveraged this way in the algorithm and as a consequence it should
work and we should be able to take things to to the next level that that's a fairly typical research
kind of progression and sure there's iterations because your intuition might be wrong but then
you refine your intuition and you improve the algorithm but this is kind of surprising to me because
here we didn't really we didn't really come up with a new algorithm it was really an investigation
right so what we did is we looked at pretrained language models so what's very popular these days
is to on massive amount of text train a transformer model which is specific architecture of of neural
network to do next token prediction in a in a document right and if you do this it turns out
it works really well on other language tasks and that had been known and opening up Google
Facebook and so many many results around this but the thing we were wondering is well if it's so
good at doing all these tasks it wasn't really directly trained for could there be something more
general it has learned when it's trained to predict the next token on so much text on the internet
by seeing the previous text predict what comes next might there be a more general reasoning mechanism
that has been internalized inside this kind of neural network beyond language and so the way we
tested this we said okay let's see if we train we take a model train on language only not train on
anything else and then let's put it to use to now classify images or do a prediction about a
protein sequence binding protein sequence binding sites or a simple math kind of problem like compute
the x-war of a sequence of bits and none of these are language tasks obviously you have to do a
you can't just put the language model in front of it because the language model only takes in language
you can't give it an image doesn't know what to do but the so they're also not they're also not
generative tasks like we've seen gpt3 apply to lots of different areas generating web pages from
text but they all share this common generative you know text-based property exactly they tend to be
generate something like the things you have seen before right text right so this model is all
train them text we want but we we believed that there was a chance that it actually reasons in a
more general way that there's some kind of thing in the neural network that makes it reason about
objects that are on its input and then draw conclusions on its output and that maybe
there are general reasoning patterns in there that if we use that reasoning engine and put it
in front of an image it'll also reason about what's in the image and how these things might interact
and so forth of course need to do a little bit of impedance matching so we take the image
we have to do an embedding so just we just do a linear embedding layer which is one layer can
do almost no work for neural networks right that's the whole idea that we want that pre-trained
language model to do all the work so just a linear embedding layer pre-trained language model
and then a linear output layer again because we don't want to output words this time we want to
output a decision what's in an image or is this x or a zero or a one or is this will there be a
bond here for the protein or not so just changing the output and input with just
linear single layer and then one other thing we had to do we had to do the normalization that happens
inside the transformer network so there's layer normalization we had to retrain that to make
sure it's on the right scale for the data that comes through now just doing that was enough
to get surprisingly good performance on these other tasks and so to me that was very surprising
because it confirmed that when you train this neural network on language it's actually not that
specialized for language at all if you train up enough language it's actually internalizing
some more general reasoning pattern of course we don't fully understand this yet but we have this
observation here that it hasn't turned on something that's very generally reusable of course we
tested we said what if we just put a random transformer in the middle same as the language model
but randomly initialized so not trained on land same architecture right randomly initialized
and actually does something which is kind of surprising that it's also kind of capable of
doing something but not as well as the pre-trained language model so there is it seem like there's
both some power in the architecture being surprisingly powerful in general and then there's
additional power in what you learn on language transfer is over through these other domains which
goes back to something that I mentioned at the very beginning we talk about you know what am I
excited about and I mentioned this this notion of well the human brain is so general and it seems
like some part of the brain that's normally used for one kind of reasoning could be used for other
things processing of signal from your tongue can apparently do visual processing people have seen
isn't and blind people also that they're what normally part of brain used for visual processing
for blind people can be used for audio processing part of it and so this kind of reusability
of generality we're kind of seeing nothing like human brain just to be clear human brain is
completely not understood and and way more advanced than anything we're working on here but
it's it you know try to make progress in that direction of something that's more general reasoning
not special purpose to a specific domain when you talk about the random model and the pre-trained
model kind of sandwich between these linear layers are you you freezing the transformers and then
fine-tuning or training the linear layers for in a supervised manner for the specific problem
that the idea correct so once you once you have the supervised task you
freeze the transformer except for the layer norm parameters and the linear input and linear output
layer so those get retrained so I think it's about like 0.1 percent or something of the overall
parameter set being trained that way and then when you say surprisingly good performance does that
mean state of the art on some task or we're surprised that it worked at all but it's not particularly
useful or state of the art it wasn't state of the art I mean it was state of it was state of the art
in terms of this kind of research I mean beyond state of the art in terms of this kind of research
where you're not allowed to train on the task you care about or not much only those linear layers
in that sense yes absolutely state of the art but in terms of if you say I want the best
in world best in the world image classifier right right am I going to first train on language
and then only have a linear layer in input and the output to work with no that's not not yet
or maybe we'll never I don't know give us the best in world image classifier I just want to make
sure I was not making assumptions on what surprisingly good meant yeah what a man was surprisingly
good is surprising that it even you know that it doesn't much much better than chance yeah
yeah awesome awesome and where do you see this particular line of research going what's what are
the next steps so I think there is a lot of opportunity in research on multimodal data
and of course the work I mentioned here is one work another work that stands out that I'm sure
you've seen is the the clip work by open AI where they specifically trained at the same time
on language and images so it was trained anything on language and images not just train on one
and it also works on the other but that's often the case often you have access to multiple
data modalities that are in some sense not perfectly aligned necessarily but you know that are
clearly related and you can coach train them yeah um I think there's a lot of opportunity there I
think there was a lot of I mean even I mean this could be audio and video that could be trained on
at the same time this could be text and images this could maybe and and you know if you think
about robotics the same things it could be could be audio video um but it could maybe be able to
be sent someday if we have a better holy factory um artificial sensors um combine that with other
percepts I think there's a lot of opportunity to um learn unified representations that could
probably go further than when we train on each separately great great well Peter it's been wonderful
catching out with you appreciate you being so generous with your time and sharing a bit about
what you've been up to uh very cool stuff well Sam thank you so much this was uh a lot of fun
and and actually I guess before we close out I should I mentioned this upfront but you're you've
joined the the podcaster brother hiding sisterhood you know um let's we won't talk too much about
it but you know what's your podcast what are you trying to do and where should folks go to find it
yeah so um just a few weeks ago I started um my own podcast called the robot brains and um you can
find it on Spotify Apple and so forth just the robot brains um and having a lot of fun meeting
what we're really focused on is is guests who tried to bridge the gap between research
AI research and bringing AI into the real world that's kind of the general theme um I think it's
a really exciting time to see AI transition in many many places into real world and so a lot of
the discussions are centered around that but you know it it also goes from there to other topics
generally AI research robotics research and applications very cool I'm assuming it can be found on
Spotify Apple Google all the usual places right just look for the robot brains awesome we'll link
to it in the show notes thanks once again Peter thank you Sam thank you so much for having me
this was a lot of fun thanks
