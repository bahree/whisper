WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.200
I'm your host Sam Charrington.

00:31.200 --> 00:36.160
Alright Twimble listeners, this is your last chance to join the conversation and share

00:36.160 --> 00:39.400
your take on home and personal AI.

00:39.400 --> 00:47.600
Entries for our My AI Contest close on Sunday, February 25th at 1159 PM Eastern.

00:47.600 --> 00:54.800
So go ahead, hit pause right now, jump on over to twimbleai.com slash My AI, check out some

00:54.800 --> 00:58.240
of the existing entries and submit yours.

00:58.240 --> 01:03.360
Before we move on, I want to give a quick shout out and thanks to everyone who has submitted

01:03.360 --> 01:12.200
a video so far this week, so cheers to you Matt, Bradley, Lawrence and Mohit.

01:12.200 --> 01:17.300
In this episode, I speak with Shreverm Notarajan, Associate Professor in the Department of

01:17.300 --> 01:20.220
Computer Science at UT Dallas.

01:20.220 --> 01:25.060
While at nips a few months back, Shreverm and I sat down to discuss his work on statistical

01:25.060 --> 01:28.500
relational artificial intelligence.

01:28.500 --> 01:33.260
Star AI is the combination of probabilistic and statistical machine learning techniques

01:33.260 --> 01:35.860
with relational databases.

01:35.860 --> 01:40.740
We cover systems learning on top of relational databases and making predictions with relational

01:40.740 --> 01:44.900
data with quite a few examples from the healthcare field.

01:44.900 --> 01:50.420
Shreverm and his collaborators have also developed Boost SRL, a gradient boosting based

01:50.420 --> 01:55.020
approach to learning different types of statistical relational models.

01:55.020 --> 01:59.220
We briefly touched on this along with other implementation approaches.

01:59.220 --> 02:04.380
Alright, let's do it.

02:04.380 --> 02:08.500
Alright everyone, I am here in Long Beach, California.

02:08.500 --> 02:13.220
I'd say sunny Long Beach, California, but it's actually fairly late in the evening.

02:13.220 --> 02:18.820
My first interview here at nips, in fact, the first time I've attended the nips conference,

02:18.820 --> 02:22.620
and I've got the pleasure of being seated with Shreverm Notarajan.

02:22.620 --> 02:28.500
Shreverm is an Associate Professor of Computer Science at the University of Texas at Dallas.

02:28.500 --> 02:31.180
Shreverm, welcome to this week in Machine Learning and AI.

02:31.180 --> 02:33.140
Hey, great to be here.

02:33.140 --> 02:36.340
Why don't we get started by having you tell us a little bit about your background and

02:36.340 --> 02:40.860
how you got interested in machine learning and artificial intelligence?

02:40.860 --> 02:46.260
So my interests are in machine learning, artificial intelligence, primarily in this field called

02:46.260 --> 02:51.780
Statistical Relational Artificial Intelligence, about which we are giving a tutorial tomorrow,

02:51.780 --> 02:54.060
which is why I'm here at nips.

02:54.060 --> 03:00.220
And the application and adaptation of these algorithms to many real problems, focusing

03:00.220 --> 03:07.580
mainly on healthcare type problems, but also on natural language, understanding, finance

03:07.580 --> 03:10.820
types of problems, but mainly focusing on healthcare problems.

03:10.820 --> 03:11.820
How did I get interested?

03:11.820 --> 03:14.780
I think it started from a grad school.

03:14.780 --> 03:20.260
When I came here for masters at Oregon State, my interest was primarily just computer science

03:20.260 --> 03:22.100
in the broad area of computer science.

03:22.100 --> 03:27.700
But I took artificial intelligence courses under Professor Prasad Thali Polly, who went

03:27.700 --> 03:30.860
on to later become my PhD advisor.

03:30.860 --> 03:33.340
And I know I just liked the style of teaching.

03:33.340 --> 03:38.980
I liked the fact that the artificial intelligence and machine learning techniques combined two

03:38.980 --> 03:43.340
of my favorite topics, math and statistics, with computer science.

03:43.340 --> 03:49.420
So the combination of math and computer science kind of looted me into AI.

03:49.420 --> 03:54.020
So my thesis was primarily in artificial intelligence and machine learning, didn't have too much

03:54.020 --> 03:55.020
healthcare problems.

03:55.020 --> 04:00.300
When I did a post-doc at University of Wisconsin-Madison, Professor David Page and Professor Jude

04:00.300 --> 04:06.420
Shavelic introduced me to the possibility of using machine learning and artificial intelligence

04:06.420 --> 04:08.420
to solve healthcare problems.

04:08.420 --> 04:14.180
We are looking at electronic health data, electronic health record data from Marshfield, Wisconsin.

04:14.180 --> 04:20.580
That got me interested into kind of like personalized medicine, now it's called precision health.

04:20.580 --> 04:25.820
So I got interested in adapting our algorithms for these problems.

04:25.820 --> 04:28.420
So I think that's how my journey has happened so far.

04:28.420 --> 04:29.420
Okay.

04:29.420 --> 04:30.420
Great.

04:30.420 --> 04:35.340
And so you mentioned that you're doing a tutorial tomorrow, right?

04:35.340 --> 04:44.180
It's on statistical artificial intelligence or star AI, which is a great acronym.

04:44.180 --> 04:45.180
Thanks.

04:45.180 --> 04:46.180
What is star AI?

04:46.180 --> 04:53.820
So star AI is the combination of probabilistic or classical statistical machine learning with

04:53.820 --> 04:57.740
more logical or relational artificial intelligence.

04:57.740 --> 05:05.220
The idea being that so the traditional artificial intelligence techniques and machine learning

05:05.220 --> 05:09.220
techniques make a lot of assumptions on the data.

05:09.220 --> 05:12.460
They need a lot of preprocessing, they need a lot of engineering.

05:12.460 --> 05:20.060
Whereas what the star AI feels, star AI methods try and do is kind of look at the data in

05:20.060 --> 05:24.380
a kind of a more holistic manner, look at the data in the natural form, in the relational

05:24.380 --> 05:25.380
form.

05:25.380 --> 05:26.380
Okay.

05:26.380 --> 05:29.660
A classic example is most of these machine learning algorithms do what is called as an

05:29.660 --> 05:37.260
IID assumption, independent and identically distributed, which means each of us is described

05:37.260 --> 05:41.140
by the same features or same set of attributes.

05:41.140 --> 05:44.060
And each of us is drawn independent of each other.

05:44.060 --> 05:47.620
And then you learn a classifier, you learn a predictor on top of us.

05:47.620 --> 05:50.020
Whereas if you look at real data, it's not true.

05:50.020 --> 05:54.900
The probability of you having diabetes or me having diabetes depends on your parents.

05:54.900 --> 05:58.660
And in my case, my parents and my parents and so on.

05:58.660 --> 06:00.180
So the family history matters.

06:00.180 --> 06:01.180
Sure.

06:01.180 --> 06:06.100
I can see that's particularly not true in the case of diabetes or disease.

06:06.100 --> 06:09.740
Are there other examples outside of the healthcare realm or that assumption?

06:09.740 --> 06:11.740
Oh, even social behavior, right?

06:11.740 --> 06:17.180
How some, even some small thing, the probability of somebody smoking depends on the fact that

06:17.180 --> 06:20.300
their social network friend smokes are not.

06:20.300 --> 06:26.180
How popular a particular person is, let's say in a scientific field, depends on who

06:26.180 --> 06:31.660
is or her co-authors, his collaborators are, so how the popularity levels of the collaborators.

06:31.660 --> 06:36.100
So if you take classical machine learning methods, they kind of project these multi-relational

06:36.100 --> 06:41.500
data into a single fixed flat form and they operate on that.

06:41.500 --> 06:44.140
Statistical relational learning methods on the other hand, allow the data to be in a

06:44.140 --> 06:45.540
structural form.

06:45.540 --> 06:50.420
And we try to learn using the power of first-order logic and relational logic.

06:50.420 --> 06:55.420
So think of learning in a relational database using statistical methods.

06:55.420 --> 06:56.420
Okay.

06:56.420 --> 07:00.580
So that's why it's called statistical relational artificial intelligence, because it takes

07:00.580 --> 07:05.580
the classical machine learning methods but kind of upgrade them to learn on relational

07:05.580 --> 07:07.700
data, allowing data to be in the natural form.

07:07.700 --> 07:11.580
So you take any data, electronic health records or classic examples, but you look at

07:11.580 --> 07:17.380
social networks or you look at author citations, you look at movie databases, everything

07:17.380 --> 07:18.380
right?

07:18.380 --> 07:23.580
Most real data, now Google stores, everybody stores the data in a relational database.

07:23.580 --> 07:27.620
But somehow when we are learning it, they are transformed into a different format.

07:27.620 --> 07:33.420
They are aggregated, they are somehow shortened, and they are all compressed into one form,

07:33.420 --> 07:36.860
and then the algorithms run on those compressed form.

07:36.860 --> 07:41.500
In our case, what we are saying is, well, the data be in its pure form, which is relational.

07:41.500 --> 07:47.060
Somehow, the models be learned at a relational level, which is why we sometimes call these

07:47.060 --> 07:52.460
models as lifted models, because they are lifted from a flat representation to a much

07:52.460 --> 07:55.540
more rational representation.

07:55.540 --> 07:59.340
Now when you first started going through this, the first thought that jumped out in my

07:59.340 --> 08:01.340
mind was relational database.

08:01.340 --> 08:08.540
But then as you described it from the context of different distributions and kind of moving

08:08.540 --> 08:12.980
beyond the independent, identically distributed assumption, I thought he's not talking about

08:12.980 --> 08:14.500
relational database stuff.

08:14.500 --> 08:17.340
But then you come back full circle to relational database.

08:17.340 --> 08:24.740
Can you elaborate on the relationship between statistical relational AI and relational databases?

08:24.740 --> 08:27.060
So relational databases are representations.

08:27.060 --> 08:31.820
So relational databases are how the data is actually stored on whatever you're storing them

08:31.820 --> 08:32.820
on.

08:32.820 --> 08:37.020
The statistical relational AI says, I'll take the structure of the relational database and

08:37.020 --> 08:39.300
learn a model upon the structure.

08:39.300 --> 08:46.620
So what happens is, so let's just take a simple example in an IID framework in the classical

08:46.620 --> 08:53.100
machine learning framework, each of us, you and I, your father, my father, we all become

08:53.100 --> 08:56.540
individual examples for a classifier.

08:56.540 --> 09:04.100
In these relational models, however, your family becomes one mega example, my family becomes

09:04.100 --> 09:05.100
one mega example.

09:05.100 --> 09:10.380
We may be connected, I don't know, when we go like 20 levels up.

09:10.380 --> 09:15.620
But for whatever data we have, we could be different sets of mega examples.

09:15.620 --> 09:21.180
And we are, so what these statistical relational learning does is learn at these mega example

09:21.180 --> 09:23.940
levels, then at individual example levels.

09:23.940 --> 09:26.300
So how are they related to databases?

09:26.300 --> 09:30.900
If you look at a database and I start, let's say, with you, or let's say we start from

09:30.900 --> 09:36.060
Shreya Ram, that's easier to explain in an academic context, then you go to my advisor,

09:36.060 --> 09:38.500
my advisor's collaborators, my collaborators.

09:38.500 --> 09:43.620
You can form a small network of people around me and basically use that information to predict

09:43.620 --> 09:47.300
something about me, whether I'll be successful, whether I'll, I'm going to have a paper next

09:47.300 --> 09:49.460
year or whether I'm going to have a grant next year.

09:49.460 --> 09:53.660
You may be able to predict that by looking at whom I'm working with, what topics I'm working

09:53.660 --> 09:56.700
on, how popular it is and so on and so forth.

09:56.700 --> 10:00.300
When it comes to relational data, you're basically looking at all the relations that are connected

10:00.300 --> 10:04.660
to me and figuring out how you can make some predictions about me.

10:04.660 --> 10:12.340
This is true, not just in citations, it's true in many of these health care problems,

10:12.340 --> 10:16.900
it's true in natural language problems, it's true in any real problem that you look

10:16.900 --> 10:17.900
at.

10:17.900 --> 10:22.900
You have objects, people are objects and they are related and you have objects everywhere,

10:22.900 --> 10:27.740
people are objects, things are objects and emails are objects and projects are objects

10:27.740 --> 10:32.820
and then we are all related to emails and projects and papers and so on and so forth.

10:32.820 --> 10:35.940
Then you start talking about relations, how are these relations?

10:35.940 --> 10:39.340
Maybe two people work on a project, right?

10:39.340 --> 10:44.380
Two people work for another person and that person reports to an organization and the

10:44.380 --> 10:48.540
organization is managed by somebody so there's always these situations that you can tease

10:48.540 --> 10:49.540
out from a database.

10:49.540 --> 10:54.020
So it's very related to a database and you can think of what we do as learning on top

10:54.020 --> 10:56.820
of a database.

10:56.820 --> 11:03.340
So I'm envisioning then, well I guess there's two directions that I want to go with

11:03.340 --> 11:12.180
as one is the extent to which what you're specifically talking about is from an implementation

11:12.180 --> 11:19.940
related to learning on databases, meaning taking advantage of schemas and primary keys

11:19.940 --> 11:24.500
and things like that, that's one possibility.

11:24.500 --> 11:32.740
But then I'm also hearing something that sounds to me like whereas we might have in traditional

11:32.740 --> 11:41.500
machine learning a set of independent examples in statistical relational AI you've got this

11:41.500 --> 11:48.300
set of examples and kind of overlaid by some connectivity graph and it sounds like

11:48.300 --> 11:55.020
what you're doing in a sense is maybe finding different ways to kind of featureize that

11:55.020 --> 11:58.220
graph and use it in the training process.

11:58.220 --> 12:04.340
That's actually one one's reasonably simple way of understanding what we're doing.

12:04.340 --> 12:08.220
Reasonably simple is a great place to start, that's true.

12:08.220 --> 12:11.580
The reason I'm saying reasonably simple is because we really do not construct too many

12:11.580 --> 12:12.580
features.

12:12.580 --> 12:17.380
We let the features be defined based on the relationships themselves.

12:17.380 --> 12:23.740
So for instance let's go back and take a classic example of movies, I want to predict

12:23.740 --> 12:27.620
that the next Marvel movie, how much money is it going to make and then you want to

12:27.620 --> 12:31.180
look at, well typically people look at history, how things change and so on.

12:31.180 --> 12:34.620
But you could also look at the actors and you could look at the different actors that

12:34.620 --> 12:38.860
are involved in a movie and you could say well these lead actors have typically tend

12:38.860 --> 12:43.500
to get a lot of box office collection and so on and so forth.

12:43.500 --> 12:48.180
So what we are trying to do is we are looking at the fact that one Marvel movie might

12:48.180 --> 12:56.660
have five stars, five big entertainment stars, like big actors and then another movie

12:56.660 --> 13:01.660
on the other hand might be carried solo by a single, you know, Iron Man, is Iron Man

13:01.660 --> 13:03.340
even Marvel or are they decent?

13:03.340 --> 13:04.340
That's Marvel.

13:04.340 --> 13:05.340
Okay, I got that right.

13:05.340 --> 13:11.980
So Robert Downey himself carries one movie with him, but let's say the Avengers are

13:11.980 --> 13:14.540
some other one is carried by five, six people.

13:14.540 --> 13:16.780
Does that necessarily mean that it's better than one?

13:16.780 --> 13:18.140
We don't know that, right?

13:18.140 --> 13:22.580
And what typical machine learning methods tend to do is kind of collapse them into one group.

13:22.580 --> 13:26.500
We on the other hand say well, how about we let the data speak for itself?

13:26.500 --> 13:30.900
We each of them combine some way and I look at the previous Avengers, look at who's directing

13:30.900 --> 13:33.060
who's imparting.

13:33.060 --> 13:36.780
So from that perspective, I'm not doing too much feature engineering, but I'm figuring

13:36.780 --> 13:43.340
out how can I use this graph of people and make that use that to make a prediction that

13:43.340 --> 13:48.020
I cannot with a normal machine learning algorithm.

13:48.020 --> 13:51.340
The other problem is here is the problem, okay?

13:51.340 --> 13:55.380
So let's say you in classical machine learning, you make an assumption.

13:55.380 --> 13:58.100
Well, I'm going to look at the top five actors.

13:58.100 --> 14:02.660
But let's say suddenly Marvel decides to make this mega Avengers movie with 20 actors,

14:02.660 --> 14:03.660
right?

14:03.660 --> 14:07.900
Okay, so you've got to go back to your model and say, ooh, man, we went from five to 20.

14:07.900 --> 14:08.900
Right.

14:08.900 --> 14:09.900
And he's just your model.

14:09.900 --> 14:10.900
Exactly.

14:10.900 --> 14:13.020
You might want to, you cannot generalize that easily.

14:13.020 --> 14:14.020
Right.

14:14.020 --> 14:17.380
We on the other hand make no assumption on the number of individuals, number of objects.

14:17.380 --> 14:20.540
We can say as long as, you know, they're actors in a movie, we're going to use their

14:20.540 --> 14:22.260
information to make predictions.

14:22.260 --> 14:26.980
So that's where the power of relational representations come in.

14:26.980 --> 14:27.980
Okay.

14:27.980 --> 14:28.980
And they're kind of tied.

14:28.980 --> 14:29.980
Actually, you ask me two questions.

14:29.980 --> 14:33.780
One is on the implementation aspect, the one on the, and I don't think they are too different.

14:33.780 --> 14:34.780
Okay.

14:34.780 --> 14:37.860
Because when you think about how these are ultimately implemented, they are in some

14:37.860 --> 14:42.580
sense implemented based on either a logical representation, the power of first-order logic,

14:42.580 --> 14:47.180
as people know for a long time in computer science, or from a relational representation as

14:47.180 --> 14:48.180
a database.

14:48.180 --> 14:53.020
So it's I, so which is why in US, this is called statistical relational learning.

14:53.020 --> 14:56.180
Because most of the work is done on top of relational databases.

14:56.180 --> 14:59.460
In Europe, this is called probabilistic logic learning.

14:59.460 --> 15:03.660
Because most of the machinery underlying is a logical representation.

15:03.660 --> 15:06.860
And both of them do minimal feature construction.

15:06.860 --> 15:09.780
They kind of learn at the level of the databases.

15:09.780 --> 15:13.500
The cool thing is they are statistical and probabilistic because you can all of the data

15:13.500 --> 15:15.020
to be noisy.

15:15.020 --> 15:16.060
So that's what happens.

15:16.060 --> 15:18.220
So our data can be noisy.

15:18.220 --> 15:19.900
Our models are robust to noise.

15:19.900 --> 15:24.700
Our models are robust to changing number of individuals, number of parameters, number

15:24.700 --> 15:27.060
of objects in the world.

15:27.060 --> 15:33.700
And so we learn at the level of objects and relations, not at the level of specific people.

15:33.700 --> 15:34.700
Okay.

15:34.700 --> 15:38.140
And I think that's why these models are powerful.

15:38.140 --> 15:39.140
Okay.

15:39.140 --> 15:41.140
And so how does it work?

15:41.140 --> 15:45.940
How does learning work or how does kind of what's the, you know, the method that you

15:45.940 --> 15:46.940
applied?

15:46.940 --> 15:50.300
So what we do, for instance, I'm going to talk about one specific method that we do,

15:50.300 --> 15:54.300
which is called relational functional gradient boosting.

15:54.300 --> 15:57.980
There is this famous gradient boosting technique inside machine learning.

15:57.980 --> 16:02.540
What we are doing is we are elevating it to the relational setting.

16:02.540 --> 16:06.500
So it's called lifted gradian boosting.

16:06.500 --> 16:07.660
And the idea is very simple.

16:07.660 --> 16:10.980
So let's say I'm interested, let's just take a simple example.

16:10.980 --> 16:13.940
I'm interested in predicting if somebody has a heart attack.

16:13.940 --> 16:14.940
Okay.

16:14.940 --> 16:18.100
Let's look at the database, let's say I'm a positive example in that I have a heart

16:18.100 --> 16:19.100
attack.

16:19.100 --> 16:21.500
Sam, you're a negative example, you look much fitter than I am.

16:21.500 --> 16:24.180
So you don't have a heart attack.

16:24.180 --> 16:29.180
And what happens is my model uses my attributes and it starts to learn.

16:29.180 --> 16:33.940
And then it says, who is the father of this person diabetic, turns out my father is.

16:33.940 --> 16:34.940
Okay.

16:34.940 --> 16:38.060
And then is the, is the cholesterol level of this person pretty high?

16:38.060 --> 16:43.300
If so, then the probability of a heart attack is, let's say, my model comes up with me having

16:43.300 --> 16:44.300
a probability of 0.7.

16:44.300 --> 16:45.860
It's a heart attack.

16:45.860 --> 16:49.020
The same model comes with you to have a probability of 0.13.

16:49.020 --> 16:50.020
Okay.

16:50.020 --> 16:54.540
Now, my, because I'm a positive example, it should say that I have a probability of

16:54.540 --> 17:01.060
1 to have a heart attack, but I said 0.7, which means 1 minus 0.7, 0.3 is the mistake

17:01.060 --> 17:03.180
that the model makes on me.

17:03.180 --> 17:07.740
Your probability should have been 0, because that's, you don't have a heart attack according

17:07.740 --> 17:08.740
to the data.

17:08.740 --> 17:11.700
But my model said 0.13 as your probability.

17:11.700 --> 17:16.780
So, your weight, so to say, is minus 0.13, that is the mistake.

17:16.780 --> 17:22.340
So, every positive example will have a weight of greater than or equal to 0.

17:22.340 --> 17:26.020
Every negative example, like you, will have a weight of less than or equal to 0.

17:26.020 --> 17:30.380
So, what we do is, we learn a small model, regression model, it would be a class which

17:30.380 --> 17:35.820
says, if the father is, father of this person is diabetic, and if this person's cholesterol

17:35.820 --> 17:41.740
level in the last three months is greater than, I don't know, let's say the H.D.L.

17:41.740 --> 17:46.820
cholesterol is less than 30, then the probability of heart attack is this.

17:46.820 --> 17:51.420
You could go do other things, for instance, that says, the relational power comes from

17:51.420 --> 17:56.540
the fact that your class could say something like, if any of his close family members, and

17:56.540 --> 18:02.660
this close family members could mean, my dad, my dad's dad, my dad's brother, my mom's

18:02.660 --> 18:06.700
brothers and so on and so forth, and you will have completely different number of uncles

18:06.700 --> 18:13.820
and aunts as I do, so you could say something like, if a close family member has a predisposition

18:13.820 --> 18:17.540
for heart attack, then the probability of heart attack for me is so much.

18:17.540 --> 18:22.100
And you could say that, because of the fact that I am not defining, I can define close

18:22.100 --> 18:28.660
family member to be either an uncle or an aunt or a first cousin or my grandparents

18:28.660 --> 18:33.060
on either side and so on and so forth, and we are not restricting the number.

18:33.060 --> 18:35.100
You just look into the data and you can learn it.

18:35.100 --> 18:41.980
So what the model does is, it tries to find out these factors that matter, put them together

18:41.980 --> 18:46.620
and make a prediction, and then it finds which mistake, where it makes a mistake, learn

18:46.620 --> 18:49.580
something else to fix that mistake.

18:49.580 --> 18:56.500
So for people who don't have low H.D.L., and whose father don't have, let's say, diabetes,

18:56.500 --> 19:01.260
it could be that they smoke a lot and drink a lot, and for them there is a different issue.

19:01.260 --> 19:05.500
So what the model does is, oh, okay, I got freedom mostly covered, there's only a small

19:05.500 --> 19:09.380
mistake on freedom, but there is this other person who has a heart attack, whom I am not

19:09.380 --> 19:10.380
covered.

19:10.380 --> 19:11.380
So let me focus on that person.

19:11.380 --> 19:16.900
So it kind of fixes these mistakes repeatedly and learns one robust model in the end.

19:16.900 --> 19:21.340
And it scales up pretty well, we have, the code is available online, so people can look

19:21.340 --> 19:25.220
at that from my webpage as well, okay.

19:25.220 --> 19:26.220
Simple things.

19:26.220 --> 19:31.740
So one, you talked about heart attacks, smoking, cholesterol levels, these are all things

19:31.740 --> 19:36.460
that I would associate as being features in a traditional model, but you said that you

19:36.460 --> 19:39.460
don't really get into features as much with this.

19:39.460 --> 19:42.060
How do you, how do you recognize all that?

19:42.060 --> 19:43.900
So I have to make it clearer.

19:43.900 --> 19:47.700
In the sense that these are all features, so everything is a feature.

19:47.700 --> 19:50.700
What I'm saying is we don't really do a lot of feature engineering.

19:50.700 --> 19:55.580
We don't say, oh, for instance, a beautiful question, right?

19:55.580 --> 19:59.220
So let's say you go to the hospital, again, you're looking much fitter than I am, you'll

19:59.220 --> 20:02.340
go to the hospital maybe once a year to do your annual checkup.

20:02.340 --> 20:05.380
I go to the hospital three or four times a year, okay.

20:05.380 --> 20:09.580
Now think about how a classical machine learning algorithm will do.

20:09.580 --> 20:14.780
You will have one measurement of cholesterol, I will have four measurements of cholesterol.

20:14.780 --> 20:21.140
I might have my A1Cs recorded every, actually four, it's good for a year, so four A1Cs

20:21.140 --> 20:25.220
which is every three months, I'm going to record my blood sugar level.

20:25.220 --> 20:28.860
And so I have multiple measurements and you have only one measurement.

20:28.860 --> 20:30.180
How do we do this?

20:30.180 --> 20:33.180
Well, classical methods say min, max, average.

20:33.180 --> 20:37.660
So it takes an average of my cholesterol level, it takes a minimum maximum and puts them

20:37.660 --> 20:38.660
as features.

20:38.660 --> 20:41.260
This is what we call as feature engineering, okay?

20:41.260 --> 20:42.580
We don't do that.

20:42.580 --> 20:47.380
We say, if the cholesterol level in the last one year has ever been greater than this.

20:47.380 --> 20:49.260
And that can be written by logic.

20:49.260 --> 20:54.900
So logic statement says there exists a cholesterol level, this is called quantification in logic.

20:54.900 --> 20:58.620
And so they kind of, so we don't even do that min, max, etc.

20:58.620 --> 20:59.620
They can be done.

20:59.620 --> 21:01.300
If needed, we can do it.

21:01.300 --> 21:03.540
But doing that is a subset of what we can do.

21:03.540 --> 21:07.940
We can just let the data as it is, which says that anytime in the last five years, Srinams

21:07.940 --> 21:14.020
cholesterol level has been this, anytime, after the age of 55, this person's A1C has been

21:14.020 --> 21:15.020
this.

21:15.020 --> 21:16.460
And so we can tease that out.

21:16.460 --> 21:20.700
So what I mean by feature engineering is we don't construct specific features for specific

21:20.700 --> 21:21.700
problems.

21:21.700 --> 21:23.300
You just let the data be in its natural form.

21:23.300 --> 21:24.620
It still sounds like features.

21:24.620 --> 21:29.900
It sounds like the if statement is like a one-hard encoding on whether their cholesterol

21:29.900 --> 21:30.900
has been high.

21:30.900 --> 21:33.020
So cholesterol is a classic example, okay?

21:33.020 --> 21:35.940
Cholesterol might not be the best example to illustrate the difference.

21:35.940 --> 21:39.140
Like the smoking properties of a friend, okay?

21:39.140 --> 21:45.700
If you're close friends mocks, the probability of having a heart rate, I guess, 0.3 or 0.4.

21:45.700 --> 21:51.580
How do you define that in a propositional or in a normal setting?

21:51.580 --> 21:52.580
Okay.

21:52.580 --> 21:56.420
I want to say if a closer and closer features fall down because they're not related

21:56.420 --> 22:02.180
to the individual entity, whereas you've got this broad universes captured by all these

22:02.180 --> 22:03.180
relationships.

22:03.180 --> 22:06.940
And it's all features, but they're not like features of me.

22:06.940 --> 22:08.540
It's features of this network.

22:08.540 --> 22:12.100
It's features of this network and the size of the network varies between people.

22:12.100 --> 22:13.100
Right.

22:13.100 --> 22:14.100
So in the end, everything is a feature.

22:14.100 --> 22:15.100
You're right.

22:15.100 --> 22:16.100
Right.

22:16.100 --> 22:17.100
But what is the feature of?

22:17.100 --> 22:20.300
My point is that we don't do specific feature engineering continuously.

22:20.300 --> 22:24.100
We let the data in its natural form, which is I could talk about objects and relations

22:24.100 --> 22:26.500
and features of those objects and features of those relations.

22:26.500 --> 22:27.500
Right.

22:27.500 --> 22:28.500
Okay.

22:28.500 --> 22:31.100
I just have to say, well, again, you might have five close friends.

22:31.100 --> 22:32.740
I might have three close friends.

22:32.740 --> 22:33.740
How do you encode this?

22:33.740 --> 22:34.740
Right.

22:34.740 --> 22:35.740
Right.

22:35.740 --> 22:36.740
Right.

22:36.740 --> 22:39.380
But as our models can just handle them because it just says, who is that one friend?

22:39.380 --> 22:40.380
Is that two friends?

22:40.380 --> 22:42.180
You can just look at it and you can learn from it.

22:42.180 --> 22:45.180
It's easy to do that.

22:45.180 --> 22:49.700
I thought you were going someplace that you didn't end up going.

22:49.700 --> 22:57.260
It strikes me that you gave the example of the multiple.

22:57.260 --> 23:01.380
You go into the hospital multiple times a year and get your four A1C readings.

23:01.380 --> 23:04.660
I go in one and get my one.

23:04.660 --> 23:13.180
And then you write your rule that basically looks to see if I have had a higher A1C reading

23:13.180 --> 23:15.900
over the past 10 years.

23:15.900 --> 23:19.660
It strikes me that you've lost a lot of information.

23:19.660 --> 23:23.260
Maybe there's information in the fact that you actually want four times and I only want

23:23.260 --> 23:24.260
one.

23:24.260 --> 23:25.260
That's a great point.

23:25.260 --> 23:27.980
It's not necessarily that we lose that information.

23:27.980 --> 23:30.660
If you want to take that, you can record that too.

23:30.660 --> 23:35.860
Actually, the two things that I want to clarify, first, I don't write the rules.

23:35.860 --> 23:39.020
Because of this boosting algorithm, rules are automatically learned from data.

23:39.020 --> 23:41.780
So we use only data and we use some domain knowledge.

23:41.780 --> 23:45.140
Actually, I work on what is called a knowledge-based machine learning.

23:45.140 --> 23:48.060
Now we call it a human and the loop machine learning.

23:48.060 --> 23:50.740
There is a human expert.

23:50.740 --> 23:51.740
We work with cardiologists.

23:51.740 --> 23:52.740
We work with radiologists.

23:52.740 --> 23:57.780
We work with neuro-radiologists and so on and so forth.

23:57.780 --> 24:05.380
We have a lot of collaborations in medical groups, so that's a slightly more of the rules

24:05.380 --> 24:07.100
learned by algorithms.

24:07.100 --> 24:09.100
The rules are learned by algorithms.

24:09.100 --> 24:11.740
What they give us is a little bit more knowledge.

24:11.740 --> 24:16.780
They can say things like, this guy is much more difficult to predict than this person.

24:16.780 --> 24:21.980
Then we tell the data, focus on this person or a person could come and say, I'll come

24:21.980 --> 24:25.580
to that about the experts in a little bit after I answer this question.

24:25.580 --> 24:27.820
So we don't provide the rules.

24:27.820 --> 24:29.980
The rules are learned automatically from the data.

24:29.980 --> 24:32.820
And the rules themselves are capable of learning that.

24:32.820 --> 24:38.140
The rules themselves are capable of saying, if the number of misses matter, then it can

24:38.140 --> 24:45.340
say, actually, if the number of misses is greater than 4, and in that number, if there

24:45.340 --> 24:48.380
is a1c reading greater than this much.

24:48.380 --> 24:52.580
So when you say you don't learn the rules, does that mean you don't learn?

24:52.580 --> 24:54.220
No, we don't write the rules.

24:54.220 --> 24:55.220
Right.

24:55.220 --> 24:56.220
We learn the rules.

24:56.220 --> 25:02.940
When you say you learn the rules, are you learning the parameters of the rules?

25:02.940 --> 25:03.940
No.

25:03.940 --> 25:04.940
You learn the rules themselves.

25:04.940 --> 25:11.980
We learn the rules, along with the parameters, which is why, so if you think of complex neural

25:11.980 --> 25:16.660
network or deep network, where writing these features out, the features themselves can

25:16.660 --> 25:18.180
be learned by us.

25:18.180 --> 25:23.980
So we learn these rules, which are basically if then else statements, and then we parameterize

25:23.980 --> 25:29.340
them with probabilities, or real numbers, depending on whatever interpretation you take.

25:29.340 --> 25:31.580
But we learn the rules from data.

25:31.580 --> 25:33.340
So yeah, so we learn it from data.

25:33.340 --> 25:38.060
Now, going back to the other point that I wanted to make about experts.

25:38.060 --> 25:39.540
Experts can provide a lot of things.

25:39.540 --> 25:43.900
Experts can tell us things like, in some cases, false positives are more important than

25:43.900 --> 25:44.900
false negatives.

25:44.900 --> 25:45.900
Okay.

25:45.900 --> 25:53.140
So for instance, in recommendation systems, I'm recommending a job to you, or I get

25:53.140 --> 25:57.460
these linked in recommendations, which keep telling me there is a post doc position open.

25:57.460 --> 25:58.460
Okay.

25:58.460 --> 26:00.900
And I'm like, come on, man, I did my time, that was seven years back, I don't want to

26:00.900 --> 26:01.900
do this again.

26:01.900 --> 26:02.900
Right.

26:02.900 --> 26:03.900
Right.

26:03.900 --> 26:05.620
So that is a false positive, it's a false positive.

26:05.620 --> 26:09.860
There, from a recommendation system perspective, that needs to be eliminated.

26:09.860 --> 26:14.220
If you tell me four jobs, only four jobs, and those are all relevant to what I'm looking

26:14.220 --> 26:16.060
at, I'm going to trust your system.

26:16.060 --> 26:20.500
So out of the potentially four thousand jobs, you may list only four jobs, but those are

26:20.500 --> 26:21.500
important to me.

26:21.500 --> 26:22.500
So I'll look at it.

26:22.500 --> 26:27.620
If you give me 20 jobs, sort of which only four are relevant, the rest are not false positives.

26:27.620 --> 26:30.020
Then I'm going to lose trust in your system.

26:30.020 --> 26:35.180
Now think exactly the opposite side of an epidemic, you're interested in predicting Ebola,

26:35.180 --> 26:36.180
for instance.

26:36.180 --> 26:37.180
Right.

26:37.180 --> 26:40.460
It's okay to quarantine four more people.

26:40.460 --> 26:41.460
What is the worst thing they're going to do?

26:41.460 --> 26:44.580
They're going to sue the city.

26:44.580 --> 26:49.620
Million dollars each, at most 10 million dollars is what we're going to lose.

26:49.620 --> 26:54.540
Think about releasing four people with Ebola into the general community, then it becomes

26:54.540 --> 26:58.420
an epidemic and costs us billions of dollars just to talk in terms of numbers.

26:58.420 --> 27:02.580
So what I'm trying to illustrate here is, in one case of recommendation system, false

27:02.580 --> 27:05.580
positives are more important than false negatives.

27:05.580 --> 27:09.620
In another case, false negatives are much more dangerous than false positives.

27:09.620 --> 27:13.140
An expert could come and tell us, no, no, no, this is the case where this is more important

27:13.140 --> 27:14.740
than the other one.

27:14.740 --> 27:18.740
And what we do is we think of it as a knob and turn this knob on the classifier and say,

27:18.740 --> 27:21.380
oh, you know what, that's what it is.

27:21.380 --> 27:28.660
And in another example, a domain expert could say, Shriram, for people, what I have observed

27:28.660 --> 27:34.700
in my experience is that, for people with high cholesterol, if they have high BMI, then

27:34.700 --> 27:38.500
the risk of a heart attack is higher than people with lower cholesterol.

27:38.500 --> 27:41.580
So even people with high BMI have two different effects.

27:41.580 --> 27:47.940
They are not exactly telling me how these things influence, except they give a qualitative

27:47.940 --> 27:52.420
understanding between two things and a target of heart attack.

27:52.420 --> 27:56.700
So that's called monotonicity, synergies, they call qualitative relationships.

27:56.700 --> 27:57.700
We can take that.

27:57.700 --> 27:58.700
You can take preferences.

27:58.700 --> 28:01.500
How do you encode that kind of thing?

28:01.500 --> 28:03.260
I'll come to that in a second.

28:03.260 --> 28:05.740
I'll just illustrate this one thing and I'll come to that.

28:05.740 --> 28:09.940
So third thing, for instance, I was in a college town, Bloomington, Indiana.

28:09.940 --> 28:13.420
Let's assume that you're trying to build a robot and the robot is sitting on top of

28:13.420 --> 28:19.700
a building, having a donut in one hand, and observing how people stop at stop signs.

28:19.700 --> 28:21.460
People stop at stop signs.

28:21.460 --> 28:27.140
I'm in a university town, which means only 50% of the people stop at stop signs.

28:27.140 --> 28:33.020
Then the machine could ask the human, hey, I'm looking at this data, but only 50% seem

28:33.020 --> 28:34.020
to be stopping.

28:34.020 --> 28:35.220
What do you think I should do?

28:35.220 --> 28:39.860
Then the expert could say, well, I prefer that you stop at stop signs if it is safe to

28:39.860 --> 28:40.860
stop.

28:40.860 --> 28:44.660
Maybe there is an extreme case where, you know, you don't want to stop in a stop sign

28:44.660 --> 28:47.180
because it's dangerous actually to stop, right?

28:47.180 --> 28:49.900
You might put you in danger or somebody in the, I don't know.

28:49.900 --> 28:54.460
I can imagine such situations with autonomous cars, for instance.

28:54.460 --> 28:59.380
And you say something soft that says, I prefer that you stop at stop signs.

28:59.380 --> 29:01.140
And then it says, okay, great.

29:01.140 --> 29:06.980
Then what our algorithm does is it takes these statements as constraints.

29:06.980 --> 29:10.380
And then it combines that with the data that I have, okay?

29:10.380 --> 29:15.580
So there could be regions in the data where there is a lot of mistakes, like stop signs.

29:15.580 --> 29:19.740
Then it says, ooh, the data has a lot of mistakes, but the expert has told me that this is

29:19.740 --> 29:21.860
what he or she prefers.

29:21.860 --> 29:25.940
So what I'm going to do is take what he or she tells me, combine it with my data.

29:25.940 --> 29:30.460
And wherever the data conflicts the expert, or the expert conflicts the data, I'm going

29:30.460 --> 29:34.900
to weigh that lower, wherever the agree, I'm going to weigh that higher, and kind of

29:34.900 --> 29:36.580
incorporate that into my model.

29:36.580 --> 29:43.100
So what I do is I get these human inputs as constraints to the learning algorithm.

29:43.100 --> 29:50.220
And we have shown across several publications that such constraints improve performance.

29:50.220 --> 29:54.580
And very recently what we have been trying to do is also do a little bit more, which is

29:54.580 --> 30:00.460
let the algorithm ask questions, instead of the expert telling, so when we started this

30:00.460 --> 30:04.820
research, the expert has to say everything that he or she knows about the problem.

30:04.820 --> 30:06.740
Then we take them as constraints and learn.

30:06.740 --> 30:10.500
But now we have gone the other way and we have said, we'll start looking at the data.

30:10.500 --> 30:15.660
In the regions where we have extreme uncertainty about things that we are trying to learn,

30:15.660 --> 30:18.340
we will solicit information from the expert.

30:18.340 --> 30:22.100
So we call it actively advice, seeking advice.

30:22.100 --> 30:26.900
So the key is for the machine to know what it knows, and solicit information about what

30:26.900 --> 30:28.220
it does not know.

30:28.220 --> 30:32.980
And the expert could say something, it takes that back into the model, adjust it again

30:32.980 --> 30:36.700
because it's an iterative learning method that we have.

30:36.700 --> 30:39.100
It can go back and try and fix its mistakes.

30:39.100 --> 30:43.820
So let's put this on the stack after the answer to the encoding question.

30:43.820 --> 30:51.620
But it strikes me there that there's a huge gap between what the algorithm is likely to

30:51.620 --> 30:53.100
surface.

30:53.100 --> 30:59.980
Some factor is, there's a high degree of ambiguity in some factor in something that you

30:59.980 --> 31:06.460
can present to a doctor that makes that intelligible and kind of elicits useful advice to feed

31:06.460 --> 31:10.140
back into the algorithm, which goes back to the encoding question.

31:10.140 --> 31:12.180
Like how does all that part work?

31:12.180 --> 31:13.180
Wonderful question.

31:13.180 --> 31:14.420
That is a fantastic question.

31:14.420 --> 31:18.340
And that is another reason why these logical models are more useful.

31:18.340 --> 31:19.340
Okay.

31:19.340 --> 31:20.340
Here's the problem.

31:20.340 --> 31:26.660
If I now use a very complex learning system underneath, then it's going to say, feature

31:26.660 --> 31:31.020
one, seven, six, three, five, four, two, one, seems very feared for me.

31:31.020 --> 31:32.020
And I go to the doctor.

31:32.020 --> 31:34.020
The doctors are going to know anything, right?

31:34.020 --> 31:36.340
On the other hand, mine is a logical class.

31:36.340 --> 31:38.100
It's an if-then-else statement.

31:38.100 --> 31:45.900
So it could say, for people who have slightly lower HDL level, but on the other hand,

31:45.900 --> 31:48.940
they triglycerate level seems to be good.

31:48.940 --> 31:52.980
There seems to be a lot of distribution over these hot attacks and diabetes.

31:52.980 --> 31:54.500
What do I do?

31:54.500 --> 32:01.580
So what happens is because of the fact that we are learning these knowledge at the level

32:01.580 --> 32:06.460
of the data themselves, the relational schema themselves, it's easier to present to the

32:06.460 --> 32:09.060
doctor because they know what the schema looks like.

32:09.060 --> 32:10.300
It's not just the doctors.

32:10.300 --> 32:12.300
We work again with financial experts.

32:12.300 --> 32:14.460
So again, our legal experts.

32:14.460 --> 32:15.500
So we have these documents.

32:15.500 --> 32:19.420
And we can say, well, you can't say something like, if the parse tree comes up with this

32:19.420 --> 32:23.860
noun phrase, no, but we can extract the says that says this part of the sentence that

32:23.860 --> 32:27.820
seems to be always conflicted with this part of the sentence, ways that.

32:27.820 --> 32:29.700
And we can ask that question.

32:29.700 --> 32:34.580
And that is only because of the fact that we are learning a much more representative model,

32:34.580 --> 32:38.900
much more interpretable model than a typical complex model errors.

32:38.900 --> 32:44.420
So that's because of the fact that you're learning if object, object type, objects value,

32:44.420 --> 32:47.820
objects relations are like this, then something, right?

32:47.820 --> 32:48.820
Okay.

32:48.820 --> 32:50.060
So then you can present this to the user.

32:50.060 --> 32:56.260
So that's another power that comes from these relational models that you don't get with

32:56.260 --> 32:57.260
standard models.

32:57.260 --> 32:58.260
Yeah.

32:58.260 --> 33:02.780
So can you talk a little bit about how you benchmark this relative to other approaches

33:02.780 --> 33:04.980
and what kind of results you've seen?

33:04.980 --> 33:07.820
So that's something that we do all the time extensively.

33:07.820 --> 33:12.180
What we try and do is we take these models and try to, for instance, if I have to run

33:12.180 --> 33:16.620
a deep belief network or something, we try to create as we spend a lot of time engineering

33:16.620 --> 33:18.540
as much as possible.

33:18.540 --> 33:23.820
And try to engineer this a lot and then create many, many, many features.

33:23.820 --> 33:28.260
And you try to use a standard machine learning algorithm as your baseline.

33:28.260 --> 33:32.860
Now what happens is for the current data set, it might learn a good performance, right?

33:32.860 --> 33:37.580
On your training set, it could actually give pretty good performance because most of these

33:37.580 --> 33:42.580
machine learning algorithms can learn well if you engineer your features very well.

33:42.580 --> 33:43.580
And that's doable.

33:43.580 --> 33:44.580
We do that.

33:44.580 --> 33:45.580
Right.

33:45.580 --> 33:49.220
So the real problem comes when it comes to generalization.

33:49.220 --> 33:52.140
So most of the benchmarking that we do is on generalization.

33:52.140 --> 33:56.900
So for instance, you learn about whether a student works with a professor from one university,

33:56.900 --> 33:57.900
right?

33:57.900 --> 33:59.220
You test on that university.

33:59.220 --> 34:01.020
Most of these algorithm works great.

34:01.020 --> 34:03.620
Now what you do is you go to a different university and deploy this model.

34:03.620 --> 34:04.620
Right.

34:04.620 --> 34:08.340
And they kind of break down because you have to create new features, new things.

34:08.340 --> 34:12.100
Whereas these relational learning algorithms, because of the fact that they learn not at

34:12.100 --> 34:16.100
the level of the individual professors, but at the level of these groups of people, sets

34:16.100 --> 34:19.700
of people, it's much more easy to adapt and transfer.

34:19.700 --> 34:24.300
So when we do this benchmarking, we try to figure out, let's say, other networks, which

34:24.300 --> 34:29.540
is why a classic example is, let's say you've used all of Shredam's family network data.

34:29.540 --> 34:32.460
What I'll do is when I'm testing it, I'll go to Sam's data.

34:32.460 --> 34:36.340
And I see, does Sam's data work on my model?

34:36.340 --> 34:41.660
And then you do that with these classical methods, some of the times you don't get them.

34:41.660 --> 34:46.380
Sometimes, if you know what you're going to test on, you can certainly engineer and try

34:46.380 --> 34:47.380
and do that.

34:47.380 --> 34:53.340
But most of the times, these relational models work beautifully on this problem as well.

34:53.340 --> 34:57.100
Yeah, I hear people talk all the time about like overfitting on ImageNet and things like

34:57.100 --> 35:00.700
that in the industry, in the field.

35:00.700 --> 35:06.340
Do you feel that this notion of kind of focusing on generalization as opposed to performance

35:06.340 --> 35:12.780
is like underappreciated or under pursued in the space? Definitely, definitely, but the

35:12.780 --> 35:19.660
good thing is that there's been a new conscience on looking at generalization as an important

35:19.660 --> 35:20.660
aspect.

35:20.660 --> 35:27.220
But initially, there's been a lot of work on the science of experimental methodology of

35:27.220 --> 35:28.220
machine learning.

35:28.220 --> 35:30.580
There's been a lot of work on empirical machine learning.

35:30.580 --> 35:34.420
I think that's an extremely important research direction.

35:34.420 --> 35:41.220
People kind of lost a little bit of that oversight when we went and developed aggressively more

35:41.220 --> 35:43.420
and more and more newer models.

35:43.420 --> 35:48.100
But I think now people have started realizing that because of this abundance of data sources,

35:48.100 --> 35:51.460
we have these multiple data sources that we have to somehow integrate and work with.

35:51.460 --> 35:55.220
Suddenly, generalization again has become an important issue.

35:55.220 --> 36:01.980
So from my limited whatever, 15, 16 year experience, I've seen that there is a lot of interest

36:01.980 --> 36:06.940
now in making sure that we build models that generalize across populations, across data

36:06.940 --> 36:10.740
sets, across diverse data sources.

36:10.740 --> 36:14.300
And I think that's certainly an important research direction.

36:14.300 --> 36:18.740
And which is one of the reasons we want to make sure that any model that you develop is

36:18.740 --> 36:20.540
not pipe to that particular thing.

36:20.540 --> 36:21.540
Right.

36:21.540 --> 36:25.660
Which is why when I see many of these people talk about AI, we're all a little bit worried

36:25.660 --> 36:33.260
because it is such a specific domain specific solution that you are always worried when

36:33.260 --> 36:34.260
it's going to break.

36:34.260 --> 36:38.780
And it could break very easily if you don't think about generalization.

36:38.780 --> 36:44.740
Are there standard and well accepted ways of measuring generalization or do you feel

36:44.740 --> 36:48.780
like everyone kind of figures it out, you know, presents their own results in their

36:48.780 --> 36:51.500
own way in papers?

36:51.500 --> 36:58.260
There's a lot of work on generalization errors and understanding generalization errors,

36:58.260 --> 37:05.260
cross validation and the whole bias variance theory is trying to figure out how the bias

37:05.260 --> 37:10.300
on a particular data set is going to affect the variance across multiple data sets.

37:10.300 --> 37:14.460
So there's definitely a lot of understanding and statistical machine learning on this

37:14.460 --> 37:16.220
notion of generalization error.

37:16.220 --> 37:19.220
And people have definitely looked at it.

37:19.220 --> 37:23.020
But I think when it comes to the practical implementation, people tend to ignore it.

37:23.020 --> 37:27.980
They tend to kind of evaluate them in a very narrow field.

37:27.980 --> 37:33.380
I've even had students, both in my class and in my research group, sometimes build these

37:33.380 --> 37:38.260
so-called tuning sets where you fix your parameters using these tuning set for your learning

37:38.260 --> 37:39.900
algorithm from test sets.

37:39.900 --> 37:40.900
You should never do that.

37:40.900 --> 37:41.900
That's like cheating.

37:41.900 --> 37:45.620
That's like having a practice question from the final midterm or the final exam that

37:45.620 --> 37:46.620
you have.

37:46.620 --> 37:48.060
You're going to have a practice question on that.

37:48.060 --> 37:49.060
So that's cheating.

37:49.060 --> 37:50.740
There's students who do that all the time.

37:50.740 --> 37:56.140
So we tend to try and teach them that's not the right way to do it and we try to show

37:56.140 --> 37:57.300
them a proper way to do it.

37:57.300 --> 38:04.900
So there's definitely a lot of work on generalization and understanding generalization.

38:04.900 --> 38:07.020
But remember, it's within a certain domain.

38:07.020 --> 38:09.060
Can we do it across domains?

38:09.060 --> 38:10.820
Maybe, maybe not.

38:10.820 --> 38:15.900
Can you take something like learning how to drive a car and figure out how to drive a plane

38:15.900 --> 38:18.260
or fly a plane?

38:18.260 --> 38:19.260
No.

38:19.260 --> 38:24.300
But maybe driving a car in US or India, I can't imagine doing that easily, but you could

38:24.300 --> 38:25.300
do that.

38:25.300 --> 38:31.340
You could try and understand how does it generalize across multiple countries and try to understand

38:31.340 --> 38:34.660
how the rules of the domains change.

38:34.660 --> 38:38.340
At some point it gets into artificial general intelligence, right?

38:38.340 --> 38:39.340
Exactly.

38:39.340 --> 38:41.540
If you can generalize across every domain, that's what that's the goal.

38:41.540 --> 38:43.260
Yeah, well, that's a goal.

38:43.260 --> 38:45.500
That's a goal, but I don't know.

38:45.500 --> 38:48.740
I'm not saying that you need something that is very general.

38:48.740 --> 38:54.020
I'm saying that you need something that is generalizable enough inside the same problem

38:54.020 --> 38:55.020
domain.

38:55.020 --> 38:56.020
Right.

38:56.020 --> 38:57.020
Yeah, I think we are not.

38:57.020 --> 39:02.780
If I build a model, as I said, for driving in US, that should be able to drive in Europe,

39:02.780 --> 39:06.820
if not in India, let's say, that level of chaos may be difficult, but maybe at least

39:06.820 --> 39:09.500
in Europe, maybe in London, it should be able to drive a car.

39:09.500 --> 39:13.420
If I teach it how to drive a car in LA, and that amount of generalization should be there

39:13.420 --> 39:14.420
at the very least.

39:14.420 --> 39:16.660
That would be cool to drive a car in India.

39:16.660 --> 39:18.420
That would be awesome to achieve it.

39:18.420 --> 39:22.940
But I'm not really looking for this one system that drives and flies at the same time.

39:22.940 --> 39:23.940
Right.

39:23.940 --> 39:28.940
In fact, I'm worried that that might lose some specific knowledge that a driving agent

39:28.940 --> 39:30.940
will have, that the flying agent will not.

39:30.940 --> 39:31.940
Yeah.

39:31.940 --> 39:32.940
So that's the balance.

39:32.940 --> 39:40.460
How much general knowledge do you want and how much specific, I guess, what's the right

39:40.460 --> 39:43.420
word here, specific skill set you want for solving that problem?

39:43.420 --> 39:47.980
I think that's the difference that we have to find, and the sweet spot depends on the

39:47.980 --> 39:49.540
problem domain.

39:49.540 --> 39:55.060
In some problems, it's, so think about it, like, you have a problem and you go to a doctor,

39:55.060 --> 40:00.020
and this doctor is a neurologist, and now you want to talk a little bit about diabetes.

40:00.020 --> 40:03.900
This person is going to tell you something, but they're going to say, this is what I know,

40:03.900 --> 40:09.180
and you better talk to somebody who knows this, and you go there, or like a cancer, you

40:09.180 --> 40:11.060
want to go to an oncologist, right?

40:11.060 --> 40:15.580
So, but before that, you have a family friend who's a radiologist, radiologist is going

40:15.580 --> 40:20.260
to give you a lot of information, but then you say, but I'll still refer you to this oncologist

40:20.260 --> 40:21.260
for your treatment plans.

40:21.260 --> 40:22.260
Yeah.

40:22.260 --> 40:23.700
And that's what you want, even with systems.

40:23.700 --> 40:26.900
You don't really want systems that say, oh, I can solve this, and this, and this, and

40:26.900 --> 40:27.900
this.

40:27.900 --> 40:28.900
Right.

40:28.900 --> 40:29.900
And so not too generalizable.

40:29.900 --> 40:30.900
Right.

40:30.900 --> 40:31.900
Right.

40:31.900 --> 40:36.900
But, so artificial general intelligence is great, but we also have to accept that, that

40:36.900 --> 40:37.900
comes at a cost.

40:37.900 --> 40:38.900
It's a contextual.

40:38.900 --> 40:39.900
Yes.

40:39.900 --> 40:43.020
Um, you mentioned you had code up on your site.

40:43.020 --> 40:44.020
Yeah.

40:44.020 --> 40:48.540
Tell me a little bit about the code, is it, is it kind of code applied to the healthcare

40:48.540 --> 40:49.540
site?

40:49.540 --> 40:50.540
So, that's a great question.

40:50.540 --> 40:51.540
No.

40:51.540 --> 40:52.540
You know, general algorithm or?

40:52.540 --> 40:53.540
It's a general algorithm.

40:53.540 --> 40:56.060
It's a boosting algorithm that anybody can download.

40:56.060 --> 40:59.140
It's a gradient boosting, but operates on relational databases.

40:59.140 --> 41:00.140
Okay.

41:00.140 --> 41:04.740
We have an extensive tutorial on how to convert your data to our format, how to run the

41:04.740 --> 41:05.740
code.

41:05.740 --> 41:08.620
We can learn multiple types of models, relational probabilistic models.

41:08.620 --> 41:12.100
We can learn multiple, we can take the human inputs on this.

41:12.100 --> 41:15.940
We actually have a wrapper that can do natural language extractions.

41:15.940 --> 41:20.620
So you have text data, but you want to, let's say, figure out who, who is married to

41:20.620 --> 41:24.580
whom based on paragraphs reading some, some CNN articles or something.

41:24.580 --> 41:25.580
Okay.

41:25.580 --> 41:27.620
And you can, you can post that problem.

41:27.620 --> 41:28.940
We have shown how to do it.

41:28.940 --> 41:32.820
So this is an extensive tutorial on how to use this.

41:32.820 --> 41:33.820
Okay.

41:33.820 --> 41:35.220
It's available off of my web page.

41:35.220 --> 41:36.220
Okay.

41:36.220 --> 41:40.940
Go to my lab deck page through my web page in the software.

41:40.940 --> 41:41.940
It's there.

41:41.940 --> 41:42.940
Okay.

41:42.940 --> 41:43.940
And people can download it and use it.

41:43.940 --> 41:44.940
It's a general purpose software.

41:44.940 --> 41:45.940
Okay.

41:45.940 --> 41:47.540
Not only applied to health problems.

41:47.540 --> 41:48.540
Okay.

41:48.540 --> 41:50.820
When it comes to health problems, there is always a catch.

41:50.820 --> 41:54.980
We have to be very careful on what we release and what we don't and so on and so forth.

41:54.980 --> 41:55.980
So yeah.

41:55.980 --> 41:56.980
Okay.

41:56.980 --> 42:04.940
And then we've been talking about relational databases, but there is a whole set of, a

42:04.940 --> 42:08.780
whole type of database called graph databases.

42:08.780 --> 42:09.780
Yes.

42:09.780 --> 42:12.180
Does your method apply to graph databases as well?

42:12.180 --> 42:13.180
Fantastic question.

42:13.180 --> 42:14.180
Yes.

42:14.180 --> 42:15.980
The answer is yes, because graph is a relation for us.

42:15.980 --> 42:16.980
Right.

42:16.980 --> 42:17.980
Right.

42:17.980 --> 42:18.980
Notes and adjust our relations.

42:18.980 --> 42:21.300
So relation is this common word that we use.

42:21.300 --> 42:22.300
Yeah.

42:22.300 --> 42:23.620
So graphs are just relations.

42:23.620 --> 42:28.540
And actually, because one of the most important step inside our optimization function is

42:28.540 --> 42:29.540
to count.

42:29.540 --> 42:33.740
You have to count the number of instances, so count the number of papers that somebody wrote.

42:33.740 --> 42:37.780
Count the number of friends that somebody has in Facebook and so on and so forth.

42:37.780 --> 42:39.860
And that counting is actually a complex problem.

42:39.860 --> 42:44.900
So we actually use graph databases to accelerate our counts.

42:44.900 --> 42:53.140
So we do use sometimes a temporary representation of a graph database to accelerate our line.

42:53.140 --> 42:58.340
So for us, graph databases and databases, graphs, they're all structured problems that

42:58.340 --> 43:01.140
we can handle using the formalism of logic.

43:01.140 --> 43:02.140
Absolutely.

43:02.140 --> 43:04.060
That's the answer is yes.

43:04.060 --> 43:10.220
And what we're trying to do is actually do exactly that, build a version that is more

43:10.220 --> 43:15.140
optimized for graph databases, because graph databases are much simpler sometimes than

43:15.140 --> 43:20.260
the full logic, like psych-carp knowledge base or never-ending learning material or

43:20.260 --> 43:21.580
nail-knowledge base.

43:21.580 --> 43:25.220
Those are logical knowledge bases and those are huge.

43:25.220 --> 43:27.220
What did they call it?

43:27.220 --> 43:28.220
Psych-carp.

43:28.220 --> 43:29.220
Yeah.

43:29.220 --> 43:31.900
Psych-carp is this company that's been running a learning.

43:31.900 --> 43:35.140
It's been building a knowledge base for over 35, 40 years.

43:35.140 --> 43:39.820
And so those are like giant knowledge bases, whereas graph databases are much easier

43:39.820 --> 43:41.700
to manage and work with.

43:41.700 --> 43:43.980
So they may be even faster for us to learn with.

43:43.980 --> 43:44.980
So yeah, absolutely.

43:44.980 --> 43:48.500
Learning with graph databases is something that we do all the time.

43:48.500 --> 43:53.740
And so you're out doing the tutorial kind of evangelizing this approach.

43:53.740 --> 43:54.940
What's the end goal for you?

43:54.940 --> 43:57.780
Is it you trying to get grad students and postdocs?

43:57.780 --> 44:03.620
You're trying to get industry to adapt this, what's the motivation and vision?

44:03.620 --> 44:07.820
I think we want people to be aware of this big goal of statistical relational AI.

44:07.820 --> 44:12.140
We want people to know that deep learning is not everything and we want people to know

44:12.140 --> 44:17.860
that there are other learning areas that focus on some richer, probably more important

44:17.860 --> 44:18.860
questions.

44:18.860 --> 44:23.300
In terms of the four of us who are giving the tutorial actually have large groups ourselves

44:23.300 --> 44:26.020
so it's not like we are trying to recruit people here.

44:26.020 --> 44:27.020
Okay.

44:27.020 --> 44:32.020
Learning one conference at a time to kind of show that this field is now actually much

44:32.020 --> 44:34.780
more matured than what it was 10 years back.

44:34.780 --> 44:39.620
And we are trying to tell people that there is a lot of opportunities inside our field.

44:39.620 --> 44:48.260
So combining deep models with logical models, combining matrix factorization with relational

44:48.260 --> 44:49.260
data.

44:49.260 --> 44:53.020
So there's a lot of opportunities for this and that's what we are highlighting in the

44:53.020 --> 44:57.940
tutorial tomorrow is inviting more people to work on similar problems like we do and

44:57.940 --> 44:59.580
that's the ultimate goal for us.

44:59.580 --> 45:04.100
It's kind of show people that this is an important research area and problem and that all of us

45:04.100 --> 45:05.980
together can contribute to.

45:05.980 --> 45:09.740
So we want more people to work on these problems and that's our end goal.

45:09.740 --> 45:10.740
Great.

45:10.740 --> 45:20.020
Are there specific examples of like case studies or like health centers or products that

45:20.020 --> 45:21.020
use this?

45:21.020 --> 45:23.020
Sure.

45:23.020 --> 45:24.020
That's a very good question.

45:24.020 --> 45:32.580
So I think one of the case is this deep dive by Professor Kirisre from Stanford that's

45:32.580 --> 45:34.660
now been bottled by Apple.

45:34.660 --> 45:40.820
So this is this probabilistic databases which kind of is statistical relational AI in some

45:40.820 --> 45:41.820
sense.

45:41.820 --> 45:46.860
That automatically extracted information, knowledge extraction from videos and images and

45:46.860 --> 45:47.860
text.

45:47.860 --> 45:49.740
And so that's a classic case.

45:49.740 --> 45:54.140
We have been working with hospitals on trying to see how we can put the data back into their

45:54.140 --> 45:59.340
learning system for making predictions on the number of hospital raid missions, the

45:59.340 --> 46:02.260
number of procedures that need to be done on some persons.

46:02.260 --> 46:06.980
We recently have got some very good success on using such models on predicting postpartum

46:06.980 --> 46:13.900
depression by looking at the network of women, sorry network of people that the women are

46:13.900 --> 46:15.820
in touch with.

46:15.820 --> 46:21.100
And so we are trying to talk to people to see how this can go out to the market.

46:21.100 --> 46:28.780
We work with a particular bank on looking at their legal documents and figuring out if

46:28.780 --> 46:35.460
a new document comes in, does this match the standards of the company, of the bank.

46:35.460 --> 46:39.540
And we can do that automatically with like 98% or 99% accuracy.

46:39.540 --> 46:44.740
And the ones that we fail, we can flag it and show it to the human expert who currently

46:44.740 --> 46:47.420
looks at it.

46:47.420 --> 46:51.580
So yes, there's a lot of these case studies out there.

46:51.580 --> 46:54.180
But again, we are trying to do the deployment at this point.

46:54.180 --> 46:58.700
So maybe hopefully if I talk to you in two years, I'll be able to say, well, I probably

46:58.700 --> 47:02.940
will not be able to say company A uses it, but I can say there is a company that uses it.

47:02.940 --> 47:04.180
Same thing with recommendation systems.

47:04.180 --> 47:07.940
The job recommendation system is actually a real project that we have done with the

47:07.940 --> 47:08.940
real company.

47:08.940 --> 47:09.940
Okay.

47:09.940 --> 47:13.020
And they are looking to use our product in their collaborative filtering system.

47:13.020 --> 47:14.020
Okay.

47:14.020 --> 47:17.100
There's a lot of success stories on this, actually, which is what we're going to highlight

47:17.100 --> 47:18.100
tomorrow.

47:18.100 --> 47:19.100
You should drop by.

47:19.100 --> 47:20.100
Okay.

47:20.100 --> 47:21.100
Awesome.

47:21.100 --> 47:22.100
Well, sure.

47:22.100 --> 47:23.100
Well, sure.

47:23.100 --> 47:24.100
Thanks so much for joining me.

47:24.100 --> 47:26.740
I really appreciate having an opportunity to learn about statistical relational AI.

47:26.740 --> 47:27.740
Thank you.

47:27.740 --> 47:33.060
Any final words or places that you like to point folks to or anything else?

47:33.060 --> 47:34.060
Oh, thanks.

47:34.060 --> 47:36.140
There is a book on statistical relational AI.

47:36.140 --> 47:37.340
Of course, that time I caught that off.

47:37.340 --> 47:41.500
Actually, the four of us who wrote the book are the four who are giving tutorials tomorrow.

47:41.500 --> 47:42.500
Okay.

47:42.500 --> 47:47.140
There's a series of workshops that happen every year that the four of us again founded

47:47.140 --> 47:52.740
10 years, eight, nine years back, I don't know, eight, nine years back, I'm getting old.

47:52.740 --> 47:55.300
And but now people are running it.

47:55.300 --> 47:57.220
It's kind of self-sustaining in its own.

47:57.220 --> 48:02.660
So I invite people to look at this problems of statistical relational AI and contact any

48:02.660 --> 48:03.660
of us.

48:03.660 --> 48:07.180
If you need any directions or anything else, that's all I'm very, very happy to help.

48:07.180 --> 48:08.180
Awesome.

48:08.180 --> 48:09.180
Thanks so much.

48:09.180 --> 48:10.180
Thank you.

48:10.180 --> 48:11.180
Thanks.

48:11.180 --> 48:13.660
All right, everyone.

48:13.660 --> 48:15.780
That's our show for today.

48:15.780 --> 48:20.820
For more information on Shrewroom or any of the topics covering in this episode, head

48:20.820 --> 48:26.580
on over to twimmolai.com slash talk slash 113.

48:26.580 --> 48:33.180
Definitely remember to submit your thoughts on AI in your life at twimmolai.com slash my

48:33.180 --> 48:34.660
AI.

48:34.660 --> 48:41.660
And of course, thanks so much for listening and catch you next time.

