WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twomo AI Podcast.

00:13.400 --> 00:18.800
I'm your host Sam Charrington.

00:18.800 --> 00:23.720
Hey, what's up everyone?

00:23.720 --> 00:28.880
It's been nearly three years since we launched the Twomo Online Meetup, initially as a way

00:28.880 --> 00:34.040
for listeners to connect with one another and study academic research papers together.

00:34.040 --> 00:39.440
The group really took off in mid-2018 though when I mentioned offhandedly in my interview

00:39.440 --> 00:44.820
with fast.ai's Rachel Thomas that I'd be taking their course and I invited the Twomo

00:44.820 --> 00:47.480
listener community to join me.

00:47.480 --> 00:52.240
That led to the very first Twomo study group and the folks who came together for that group

00:52.240 --> 00:57.960
have become the backbone of a community that's now over 3,000 members strong.

00:57.960 --> 01:02.600
Fast forward 18 months and we've supported each other on our personal learning journeys

01:02.600 --> 01:10.640
and a wide variety of courses from stanfordfast.ai, deeplearning.ai, on Kaggle projects and much

01:10.640 --> 01:11.880
more.

01:11.880 --> 01:16.040
I say all that to say that I am humbled by the role that we've been able to play in helping

01:16.040 --> 01:21.400
folks learn machine learning and I am excited about the new educational programs that we're

01:21.400 --> 01:24.320
bringing to the Twomo community this year.

01:24.320 --> 01:28.440
We recently launched a collaboration to bring you the causal modeling and machine learning

01:28.440 --> 01:32.880
course and study group which I've mentioned here before and today I'd like to share some

01:32.880 --> 01:37.600
new details on the AI enterprise workflow study group that we're launching in just a

01:37.600 --> 01:39.200
few weeks.

01:39.200 --> 01:42.600
If you've been listening for a while you know that I am very excited about the work going

01:42.600 --> 01:47.360
on in ML and AI communities to make developing and deploying machine learning and deep learning

01:47.360 --> 01:51.640
models in the enterprise more accessible and efficient.

01:51.640 --> 01:59.320
In fact we hosted an entire conference TwomoCon AI platforms on this topic just last fall.

01:59.320 --> 02:03.720
Until now there have been very few formal resources that folks could turn to to learn

02:03.720 --> 02:08.400
real world machine learning workflows and deployment strategies.

02:08.400 --> 02:13.760
Our folks at IBM are working to change this though with the AI enterprise workflow specialization

02:13.760 --> 02:17.800
that they recently made available on Coursera.

02:17.800 --> 02:21.920
And I am super excited to share that we've partnered with them to host a study group for

02:21.920 --> 02:27.760
this six course specialization program which I will personally be hosting.

02:27.760 --> 02:31.920
The courses in the sequence teach real world machine learning and an enterprise environment

02:31.920 --> 02:37.360
including applying the scientific process to understanding business use cases and structuring

02:37.360 --> 02:45.200
data collection visualizing and analyzing data and hypothesis testing feature engineering

02:45.200 --> 02:51.360
and identifying and addressing data biases selecting the best models for machine learning

02:51.360 --> 02:59.640
vision and NLP use cases and using ensembles deploying models using microservices and containers

02:59.640 --> 03:06.720
Kubernetes and Apache spark and applying unit testing to ML models and monitoring model

03:06.720 --> 03:09.400
performance in production.

03:09.400 --> 03:13.440
I am really looking forward to working through these courses and I would love for anyone

03:13.440 --> 03:15.920
interested in these topics to join me.

03:15.920 --> 03:20.200
If this sounds interesting and you'd like to learn more, I invite you to join a webinar

03:20.200 --> 03:27.080
that I'm hosting with Ray Lopez, the courses instructor on Saturday, February 15th at 9.30

03:27.080 --> 03:37.680
a.m. Pacific time to register visit twimmelai.com slash AI workflow and now on to the show.

03:37.680 --> 03:44.360
Everyone, I am here in San Diego at TubeCon and I am with Eris Cohen, Eris's Vice President

03:44.360 --> 03:47.160
for Cloud and AI at Melanox.

03:47.160 --> 03:49.400
Eris, welcome to the Twimmelai.com podcast.

03:49.400 --> 03:50.400
Thank you very much.

03:50.400 --> 03:51.400
I'm great to be here.

03:51.400 --> 03:53.920
Awesome to be chatting with you.

03:53.920 --> 03:56.280
Tell us a little bit about your background.

03:56.280 --> 04:01.800
So I am leading the Cloud and AI at Melanox Technologies.

04:01.800 --> 04:05.720
I'm actually with the company for some time now over 19 years.

04:05.720 --> 04:09.040
So I've been with the company from the very, very early days.

04:09.040 --> 04:15.880
I am coming from an engineering background, I'm a computer engineer based out of Israel.

04:15.880 --> 04:23.360
And over the past five years or so, I'm focusing mostly on cloud and containerized infrastructure

04:23.360 --> 04:28.440
for high speed networking and obviously machine learning is a big part of it.

04:28.440 --> 04:34.240
Yeah, now I've been familiar with Melanox for quite a while primarily in the context

04:34.240 --> 04:41.280
of high performance computing and infinite band and stuff like that, but not too long ago

04:41.280 --> 04:50.680
the company was acquired by NVIDIA who obviously has a big stake in deep learning and AI.

04:50.680 --> 04:52.480
What's the idea there?

04:52.480 --> 04:53.480
Yeah.

04:53.480 --> 04:57.320
So Melanox was, you know, when we started, it was a networking company and at the time

04:57.320 --> 05:03.040
we focused on Infinibin, which was the new generation of networking at the time that

05:03.040 --> 05:04.040
was supposed to be.

05:04.040 --> 05:05.040
Did I date myself?

05:05.040 --> 05:06.040
I'm sorry?

05:06.040 --> 05:07.040
Did I date myself?

05:07.040 --> 05:08.040
Yes, definitely.

05:08.040 --> 05:15.600
Well, actually, to be honest, Infinibin is still very much a very live technology.

05:15.600 --> 05:21.160
We just this week we have super computing conference, which is the large, you know, gathering

05:21.160 --> 05:24.400
all the high performance components and then very exactly.

05:24.400 --> 05:31.040
And if you look at the top 500 strongest computers in the world today, a lot of them still

05:31.040 --> 05:36.560
run Infinibin, which is the most efficient, still interconnected, highest performance,

05:36.560 --> 05:37.960
most of cost effective.

05:37.960 --> 05:44.080
So it's 20 years old technology, but it's still the cutting edge, definitely running

05:44.080 --> 05:49.440
in on rates of 200 gigabit per second, which is phenomenal if you think about it.

05:49.440 --> 05:53.040
But Melanox wasn't designed to be a high performance computing company.

05:53.040 --> 05:57.880
It was designed to be a networking company that provides the best interconnect for data

05:57.880 --> 05:58.880
centers in the world.

05:58.880 --> 06:03.400
And it doesn't matter if it is high performance computing, when you're running 10,000 servers

06:03.400 --> 06:09.960
doing some crash simulation of the universe, or you're running your telecom infrastructure,

06:09.960 --> 06:13.000
or you're running your machine learning and big data.

06:13.000 --> 06:19.160
So if you look at the essence of networking in the end of the day, it is similar to many

06:19.160 --> 06:20.320
of those applications.

06:20.320 --> 06:25.920
You need a very low latency high bandwidth, very efficient and offloaded infrastructure.

06:25.920 --> 06:30.640
This is exactly what Melanox does, we do it in a way that it can fit a lot of different

06:30.640 --> 06:32.240
verticals.

06:32.240 --> 06:36.720
And machine learning and big data, and I will bundle them in a way together, even though

06:36.720 --> 06:38.360
they're not necessarily bundled together.

06:38.360 --> 06:46.080
But if you look at data analytics, machine learning is part of the data pipeline, and there's

06:46.080 --> 06:50.560
typically also big data because we usually work on large data sets.

06:50.560 --> 06:55.760
If you look at big data and machine learning, they have many high performance networking

06:55.760 --> 06:59.600
requirements that I will talk probably about in a few minutes.

06:59.600 --> 07:04.520
And this is where a lot of the focus of Melanox is these days.

07:04.520 --> 07:10.400
We're looking at acceleration, accelerating networking and providing more efficient networking.

07:10.400 --> 07:15.960
And if you look at what Nvidia does, they do something very similar in a different domain

07:15.960 --> 07:18.200
they're accelerating compute.

07:18.200 --> 07:25.680
And so it was very natural for a company like Nvidia who is accelerating compute and providing

07:25.680 --> 07:31.200
a much more efficient compute with GPUs to look at a company like Melanox which accelerate

07:31.200 --> 07:32.200
networking.

07:32.200 --> 07:37.840
And together we're forming the two or two and a half pillars of networks or of clouds

07:37.840 --> 07:41.560
basically which is compute, networking, and also storage.

07:41.560 --> 07:47.280
The reason I'm saying two and a half pillars is because there's a lot of networking in storage.

07:47.280 --> 07:53.480
So we're touching, Melanox is touching mostly networking and storage in a sense, the networking

07:53.480 --> 07:55.040
part of the storage.

07:55.040 --> 07:59.080
And we're moving a lot of compute into the networking, we can talk about that.

07:59.080 --> 08:04.440
And Nvidia is focusing a lot about the compute, the engines themselves.

08:04.440 --> 08:05.440
Okay.

08:05.440 --> 08:09.520
So if you've got a talk here at the conference tomorrow on networking optimizations for

08:09.520 --> 08:16.440
multi-node deep learning on Kubernetes, when I think about deep learning and in particular

08:16.440 --> 08:22.360
the training phase of deep learning, I think of that as a primarily a compute bound process,

08:22.360 --> 08:26.400
compute bound exercise, you know, why do we care about networking in that?

08:26.400 --> 08:27.400
Exactly.

08:27.400 --> 08:28.400
This is exactly the point.

08:28.400 --> 08:30.360
It is a compute bound process.

08:30.360 --> 08:37.000
And one of the ways to solve compute bound processes is to add more compute.

08:37.000 --> 08:38.480
What we call scale out.

08:38.480 --> 08:44.800
Now when you add more compute to distribute the workloads so that it can run faster, you're

08:44.800 --> 08:48.000
hitting a networking situation.

08:48.000 --> 08:55.640
If you look at models today, models grew enormously over the past several years.

08:55.640 --> 09:02.160
It's not in several percentages, you know, 100 or 200X, the size six years ago.

09:02.160 --> 09:05.520
And the computation time and complexity is enormous.

09:05.520 --> 09:16.040
We see some models training takes weeks on the state-of-the-art GPUs or multiple GPUs.

09:16.040 --> 09:20.040
And the only way to shorten this time, and it has to be shortened because we need those

09:20.040 --> 09:25.720
learning phases to happen much frequently, then, you know, once every three weeks, is

09:25.720 --> 09:26.720
to scale out.

09:26.720 --> 09:34.120
So basically we add multiple servers, each of them with multiple GPUs, and the expectation

09:34.120 --> 09:39.560
or the plan is that the more servers you add, you reduce the time in a linear manner.

09:39.560 --> 09:41.400
That's kind of our goal, right?

09:41.400 --> 09:46.560
So if it takes me one hour with a single server, adding two servers, I would love to have

09:46.560 --> 09:48.040
it in half an hour.

09:48.040 --> 09:54.000
And if I have 60 servers, it should take a minute, and you can do the math from here, right?

09:54.000 --> 09:58.360
Unfortunately, it's not that easy just adding servers and cutting the time.

09:58.360 --> 10:05.960
What happens is that you have now a distributed problem, moving a problem that runs on a single

10:05.960 --> 10:12.080
GPU to run on multiple GPUs and multiple nodes requires a lot of synchronization, requires

10:12.080 --> 10:16.760
a lot of data that needs to go between the servers in a very efficient way.

10:16.760 --> 10:20.840
And this is exactly where the networking is really making a big difference.

10:20.840 --> 10:21.840
Yeah.

10:21.840 --> 10:26.520
Our listeners probably have heard of things like distributed TensorFlow and Horavod and

10:26.520 --> 10:31.960
other technologies that are more focused on kind of the software aspect of that and the

10:31.960 --> 10:37.600
coordination, the state, the shared state that enables distributed training.

10:37.600 --> 10:41.400
But you're saying that's not that part of it, but not all of it, right?

10:41.400 --> 10:42.400
That's part of it.

10:42.400 --> 10:43.400
Yeah.

10:43.400 --> 10:44.400
So definitely.

10:44.400 --> 10:50.800
I mean, if you look at distributed TensorFlow and things like Horavod, which all they do

10:50.800 --> 10:57.600
is they try to improve the way the workers communicate with each other, you know, the

10:57.600 --> 11:06.480
very kind of basic, most simple approach was to have a kind of a node that is doing all

11:06.480 --> 11:07.480
the coordination.

11:07.480 --> 11:15.760
So, you know, you have X amount of workers and they do training of mini batch, a small set

11:15.760 --> 11:23.840
of the data and then they send the information, the gradient factor to the server that usually

11:23.840 --> 11:25.760
we call it a parameter server.

11:25.760 --> 11:30.160
He would gather all the gradients from all the nodes, we'll do some crashing of the data

11:30.160 --> 11:37.280
and distribute new new weights for the different computes and they would do that over and over

11:37.280 --> 11:39.120
again in iteration.

11:39.120 --> 11:44.360
That is a very naive implementation, obviously, when you look at large scale models, every

11:44.360 --> 11:50.880
node will send tens or more of gigabit per second to a single node that needs later on to,

11:50.880 --> 11:55.080
you know, crash it locally and distribute that doesn't scale, that doesn't work, right?

11:55.080 --> 12:03.200
So Horavod and other solutions try to look at different ways of doing this, what we call

12:03.200 --> 12:08.920
collective operation, collective operation in a sense that you have a number of nodes that

12:08.920 --> 12:12.720
are working in a collective way on a single problem and they need to synchronize each

12:12.720 --> 12:14.600
other, right?

12:14.600 --> 12:19.360
So sending to a single server and get it back is one very simple way and then there are

12:19.360 --> 12:25.080
other ways like rings that each send to these neighbors and so on.

12:25.080 --> 12:31.400
Those are all ways to drive the synchronization in a much more efficient way, but in the end

12:31.400 --> 12:36.360
of the day you actually send and receive data and you send and receive data in large quantities

12:36.360 --> 12:41.080
that needs to be very low latency, you don't want to burden your CPUs with all this data

12:41.080 --> 12:47.160
sending and receiving and a lot of the data that you send is not necessarily originating

12:47.160 --> 12:52.040
in the GPU, in the host memory, it is in the GPU memory, the question is how do you send

12:52.040 --> 12:55.000
data from GPU through the network?

12:55.000 --> 12:59.680
Again, the simple implementation is copy that data to the host memory and then copy it

12:59.680 --> 13:02.840
again to the network buffers and then send it out.

13:02.840 --> 13:06.240
That's a very, very inefficient way to do that.

13:06.240 --> 13:12.440
So it's an efficient but again for a CPU bound, compute bound problem rather, we're not

13:12.440 --> 13:15.000
as worried about latency, why do we?

13:15.000 --> 13:17.840
So think about how does it work?

13:17.840 --> 13:25.360
You do the training on a mini batch, everybody, you have X amount of servers, they all do

13:25.360 --> 13:28.120
this training and then they do the communication phase.

13:28.120 --> 13:35.160
Now if the communication phase is long then and you do those cycles, hundreds of thousands

13:35.160 --> 13:40.880
or millions of times, this adds up significantly adds up.

13:40.880 --> 13:46.200
I can tell you that when we use technologies like RDMA, which is an invest transport service

13:46.200 --> 13:52.080
or GPU direct, which is a way to send and receive data from the GPU, we can improve the

13:52.080 --> 13:58.680
total system efficiency by multiple axis and those systems are very expensive.

13:58.680 --> 14:06.880
If you look at a 10 node GPU system with 8 GPUs each, that's a lot of money.

14:06.880 --> 14:11.440
If you can do two X performance, you just save several million dollars or watch yourself

14:11.440 --> 14:12.440
another system.

14:12.440 --> 14:13.440
Exactly.

14:13.440 --> 14:15.680
And this is only networking.

14:15.680 --> 14:17.920
And this is what people don't realize.

14:17.920 --> 14:22.560
Actually people do realize, if you look at all the big guys, all the hyperscalers, they

14:22.560 --> 14:26.080
all run this technology already for many years.

14:26.080 --> 14:30.640
They're running RDMA and they're running GPU direct and they are now looking at additional

14:30.640 --> 14:36.680
technologies that we bring to move some of the computation into the network, actually,

14:36.680 --> 14:38.000
called sharp.

14:38.000 --> 14:41.280
We just announced it in super computing.

14:41.280 --> 14:42.520
It would be part of nickel.

14:42.520 --> 14:48.800
Nickel is the Nvidia communication library, so it will be available together with nickel.

14:48.800 --> 14:51.880
So we kind of talked about the setup in a while, this is important.

14:51.880 --> 14:55.520
And then we've been rattling off a bunch of different technologies that play a role

14:55.520 --> 14:56.520
in here.

14:56.520 --> 15:01.120
I imagine part of your presentation was kind of walking through that stack or the evolution

15:01.120 --> 15:02.120
of technologies.

15:02.120 --> 15:08.080
It's a good time to drill into what is GPU direct and RDMA and nickel and sharp and all

15:08.080 --> 15:09.080
these things.

15:09.080 --> 15:10.080
Yeah.

15:10.080 --> 15:14.360
So the talk we'll be talking about the challenges that we just discussed.

15:14.360 --> 15:19.560
And very briefly talk about RDMA and GPU direct, there's two key technologies that enable

15:19.560 --> 15:21.360
better networking.

15:21.360 --> 15:25.800
And then it will go and talk about the actual implementation in Kubernetes.

15:25.800 --> 15:28.120
So we are in Cubcon after all.

15:28.120 --> 15:34.480
And one of the things we want to make sure is we can enable advanced platforms like a

15:34.480 --> 15:37.560
Kubernetes class to take advantage of these technologies.

15:37.560 --> 15:39.160
It doesn't come up free.

15:39.160 --> 15:46.600
You know, if you look at Kubernetes, networking in Kubernetes is very naive in a sense.

15:46.600 --> 15:47.600
Still.

15:47.600 --> 15:53.480
And evolving obviously it's a new platform or framework.

15:53.480 --> 15:57.200
And it is evolving very fast now, you know, you could see at the amount of people here

15:57.200 --> 15:58.200
in the conference.

15:58.200 --> 16:06.160
It's amazing to see how much energy and how much enthusiasm there is here.

16:06.160 --> 16:08.600
And we, you know, networking will evolve.

16:08.600 --> 16:14.240
What we are trying to do, melanox together with the partners in the ecosystem and definitely

16:14.240 --> 16:18.360
Nvidia is a big part, by the way, they're part of the talk, they're actually a joint

16:18.360 --> 16:27.080
talk between Nvidia and melanox, is we are trying to make Kubernetes networking to be

16:27.080 --> 16:35.440
able to consume advanced network solutions in a very natural, upstream, standard, transparent

16:35.440 --> 16:36.440
way.

16:36.440 --> 16:41.240
This is really, and the talk is talking about mostly that, obviously we need to go through

16:41.240 --> 16:46.480
the concept of what are we trying to solve and what are the technologies, but we will

16:46.480 --> 16:50.880
try mostly to focus about how do we integrate it into Kubernetes, and then talk a little

16:50.880 --> 16:57.040
bit about that will be the Nvidia side, they're running this at scale in large clusters,

16:57.040 --> 17:02.840
and they will talk about what do you need to do in your data center in order to properly

17:02.840 --> 17:06.240
run this and what's the best practices.

17:06.240 --> 17:07.240
Okay.

17:07.240 --> 17:08.240
So that will be the talk most.

17:08.240 --> 17:09.240
Okay.

17:09.240 --> 17:14.680
So you were kind of running through the technologies and the acronyms that were thrown around

17:14.680 --> 17:17.600
and then we'll dive into the Kubernetes therapies.

17:17.600 --> 17:26.200
So I just throw three different technologies, RDMA and GPU directives that are the ones

17:26.200 --> 17:32.120
that we will talk on the talk tomorrow, and I'll just mention that it will not be part

17:32.120 --> 17:34.240
of the talk, but I'll be happy to explain.

17:34.240 --> 17:39.200
RDMA is remote direct memory access, and this is a technology,

17:39.200 --> 17:44.200
which it's a transport service, if you like, it's the same way or it is.

17:44.200 --> 17:50.200
Yeah, it came from the infiniband days, it is a transport service, very much like TCP

17:50.200 --> 17:57.240
or UDP, I think most of the listeners at least heard about TCP and UDP, but unlike those

17:57.240 --> 18:05.200
TCP and UDP, it was designed in 2000 rather than the 70s of the last decade, and it is

18:05.200 --> 18:06.720
much more advanced.

18:06.720 --> 18:13.680
It has few very important capabilities, first is the notion of read and write versus

18:13.680 --> 18:18.360
only send and receive in TCP or UDP or just send a packet, and somebody on the other side

18:18.360 --> 18:24.320
needs to get it and decide what to do with that, it is a send, receive mechanism.

18:24.320 --> 18:29.880
RDMA allows you not only to do send and receive, but actually write to a specific memory

18:29.880 --> 18:34.800
address on the remote host or read from there, just like a local DMA.

18:34.800 --> 18:40.800
And that's open up a lot of very interesting use cases for writing applications, truly

18:40.800 --> 18:43.080
change how you write your application.

18:43.080 --> 18:50.320
This is one thing, the other thing is something called kernel bypass, in TCP and UDP, an

18:50.320 --> 18:55.800
application that wants to send and receive data will do a socket call, and then it will

18:55.800 --> 19:00.720
go to the Linux kernel or Windows kernel or whatever kernel, and the kernel will do a bunch

19:00.720 --> 19:06.560
of work preparing the packet and send it out, so there's a lot of dependencies on the kernel.

19:06.560 --> 19:12.160
In RDMA, you just from the user space can send and receive data directly, and there's

19:12.160 --> 19:18.720
no basically kernel involvement at all, so it allows you first low-verlo latency.

19:18.720 --> 19:23.920
We're looking at micro-second latency sending and receiving packet versus at least older

19:23.920 --> 19:27.840
of magnitude higher when you look at TCP, minimum.

19:27.840 --> 19:32.880
And then the fact that you don't go to the kernel and the NIC itself, the network adapter,

19:32.880 --> 19:37.800
implements all the transport and how there allows you to move enormous amount of data without

19:37.800 --> 19:43.440
any CPU utilization, which means that if anybody been working on high performance networking,

19:43.440 --> 19:49.560
you know that you need to tweak the kernel and the CPUs like crazy to get the juice out

19:49.560 --> 19:55.800
of the network, but in Finland it's not with RDMA, it's not the case, you just tell it

19:55.800 --> 20:00.680
to send and it will send 100 gigabit without CPU, 200 gig doesn't really matter.

20:00.680 --> 20:05.360
RDMA is available on Finland initially, but now it's a part of Ethernet as well, it's

20:05.360 --> 20:09.400
a standard, it's not a proprietary intercom.

20:09.400 --> 20:13.120
And is available on any kind of enterprise Ethernet NIC?

20:13.120 --> 20:17.600
Yeah, so in the Melanox is definitely one of the leaders in this space we've been doing

20:17.600 --> 20:19.600
that for 20 years.

20:19.600 --> 20:24.920
These days all the network adapters have Ethernet that would have RDMA, I would argue

20:24.920 --> 20:29.400
that Melanox is a far more feature rich and stable in this space.

20:29.400 --> 20:30.400
Nothing less.

20:30.400 --> 20:35.000
Yeah, but you know there's the benefits of experience I guess.

20:35.000 --> 20:41.240
And so my guess then is the GPU direct is RDMA for GPU memory?

20:41.240 --> 20:42.840
Exactly, so exactly right.

20:42.840 --> 20:49.280
So RDMA allows you to write and read directly to memory so the NIC can access the host memory,

20:49.280 --> 20:54.120
there's a lot of relationship in RDMA between the NIC and the host memory and there's a lot

20:54.120 --> 20:59.480
of mechanism that allows it to be very efficient, including like an IOMMU implemented on the

20:59.480 --> 21:00.480
NIC and IOMMU.

21:00.480 --> 21:01.480
IOMMU?

21:01.480 --> 21:08.080
Yeah, I'm sorry for all the heckering, but this is basically a mechanism that makes sure that

21:08.080 --> 21:12.840
you write the data to the right address in the memory and that you allow to do that.

21:12.840 --> 21:17.920
So you don't expect input, output memory, something unit or something.

21:17.920 --> 21:26.400
Yeah, this is typically an Intel component of the server, but the point is that if you think

21:26.400 --> 21:36.240
about it when a remote host writes to your memory of local host, this memory may be in different

21:36.240 --> 21:40.520
physical, there's virtual addresses and physical addresses so you need somebody to make sure

21:40.520 --> 21:45.200
that first you're allowed and you're not bridging in their security breaches and then that

21:45.200 --> 21:50.640
you're writing to the right place because the physical memory kept on shifting and changing.

21:50.640 --> 21:58.560
So there is a mechanism to make sure that the memory is intact properly.

21:58.560 --> 22:05.360
GPU direct is a co-development that NVIDIA and Melanox did together, actually it started

22:05.360 --> 22:11.400
a while back, it started from the high performance computing days when NVIDIA started getting

22:11.400 --> 22:13.240
into this space.

22:13.240 --> 22:20.640
I think it was over 10 years ago, I don't remember exactly, but at least over 10 years ago

22:20.640 --> 22:25.440
and basically what it allows is it allows the NIC and the GPU to be able to communicate

22:25.440 --> 22:31.800
over PCI directly without going through the host memory and so there's no CPU intervention

22:31.800 --> 22:39.040
and there's no copies, there's no other challenges, blockers and so on that we had before.

22:39.040 --> 22:45.440
And that really opens up a huge button like that GPU networking had, all those copies

22:45.440 --> 22:51.160
took a lot of time and they killed performance completely and they synced up a lot of CPU cycles

22:51.160 --> 22:52.400
for that.

22:52.400 --> 22:59.520
So that is GPU direct, it is almost transparent or some because the NIC and the GPU need

22:59.520 --> 23:05.440
to talk there, there's some system requirements that needs to be met on how the PCI layout

23:05.440 --> 23:10.480
is done and where's the NIC and where's the GPU.

23:10.480 --> 23:16.320
It is especially important with the large GPU boxes and typically in large GPU boxes you

23:16.320 --> 23:22.600
will see that there's a GPU in a box like the DGX one, for example and there's a very

23:22.600 --> 23:27.960
interesting PCI structure, it is not a single PCI switch that connects all the GPUs and

23:27.960 --> 23:33.880
the NICs there, there's a topology that and then you need to be very careful on which

23:33.880 --> 23:39.920
NIC talks to with GPU, to enable GPU direct, but other than that it is quite transparent

23:39.920 --> 23:43.840
for the user if they use the right drivers and applications.

23:43.840 --> 23:50.120
The last technology that I mentioned and this is not going to be on the talk is sharp, sharp

23:50.120 --> 23:52.720
here.

23:52.720 --> 23:58.120
I explained earlier about the need to get all the vectors from when you do the, the

23:58.120 --> 24:04.080
needs to be the training, you get all the vector to a single location that does some crunching

24:04.080 --> 24:08.640
and redistribute the new weight vector.

24:08.640 --> 24:14.720
What if you could do that instead of a server or in a ring, do that on the network itself.

24:14.720 --> 24:18.600
So this is what Malanox does, basically we are in our infinivance switches and this

24:18.600 --> 24:21.240
is specifically for infinivance right now.

24:21.240 --> 24:25.720
We added a capability to run compute inside the network.

24:25.720 --> 24:35.800
So doing the, what we call reduction, the vector reduction, we do that actually on the switch

24:35.800 --> 24:37.520
systems themselves.

24:37.520 --> 24:41.160
And we can do that in line rate through a lot of many ports, you know, those switches

24:41.160 --> 24:46.600
has 30, 30 ports or 40 ports depends on the generation.

24:46.600 --> 24:51.960
Switch is not the adapters, the adapter sends the data to the switch, each server sends

24:51.960 --> 24:57.200
the vector of the gradients to the switch and the switch think, let's say that it has,

24:57.200 --> 25:02.960
let's say 30 ports, it gets 30 such streams, the switch will do the reduction, we'll get

25:02.960 --> 25:08.680
all the vectors, we'll do some crunching and we will redistribute that to the servers

25:08.680 --> 25:11.920
to continue the next phase of the training.

25:11.920 --> 25:16.040
And that allows you to open up a lot of bottlenecks.

25:16.040 --> 25:22.920
And is the, the crunching that the switch is doing, is it like how configurable is that,

25:22.920 --> 25:28.840
is it, you know, there's the way that it does it today or the, kind of the way that, you

25:28.840 --> 25:33.560
know, melanox says that it should do it or is it a software defined thing where I can

25:33.560 --> 25:37.640
have a strategy and push it to the switch and it's going to do what I want, is it, to

25:37.640 --> 25:40.480
what, how generalized is the compute available on the switch?

25:40.480 --> 25:41.800
So I think it's a good question.

25:41.800 --> 25:46.600
One of the things that I haven't mentioned is our strategy around software, melanox is

25:46.600 --> 25:54.560
very open source, centric company, on, you know, we do, we believe the best hardware.

25:54.560 --> 26:04.040
And we, the software piece is all upstream first, we, everything we do is upstream, not

26:04.040 --> 26:10.840
private or no, no black box software, we try to open up everything and have standard

26:10.840 --> 26:17.760
APIs as part of this for shop, for example, we have standard APIs that is developed as part

26:17.760 --> 26:19.480
of the RDMA interface.

26:19.480 --> 26:20.480
Okay.

26:20.480 --> 26:24.400
And then from a software perspective, you know, it's just a standard API.

26:24.400 --> 26:30.080
The hardware implementation, obviously there is some flexibility, but that's, I think,

26:30.080 --> 26:35.640
less of the point, the point is that any application that uses a standard open source API,

26:35.640 --> 26:42.000
like at an advantage of any hardware that implements this API capabilities, specifically

26:42.000 --> 26:47.600
for our implementation, it is flexible, but the flexibility is not exposed outside,

26:47.600 --> 26:50.200
it's exposed through those standard APIs.

26:50.200 --> 26:51.200
Okay.

26:51.200 --> 26:52.200
Okay.

26:52.200 --> 26:58.360
So that the idea then would be that somewhere between melanox and NVIDIA, you would identify

26:58.360 --> 27:03.120
what the strategies are of interest, whether it's Horavod or distributed TensorFlow or

27:03.120 --> 27:07.920
something else, and make the module available for the switch.

27:07.920 --> 27:08.920
Yeah.

27:08.920 --> 27:13.920
So the way you consume this is through applications that do distributed training.

27:13.920 --> 27:23.640
Now, obviously the market is quite active with a lot of different projects and so on.

27:23.640 --> 27:30.240
We try to focus on the most dominant frameworks, TensorFlow is definitely one of the frameworks

27:30.240 --> 27:38.960
we see for performance-oriented environments, which is mostly used, and we work mostly with

27:38.960 --> 27:45.160
TensorFlow, not only not limited, we also work with PyTorch and Cafe and others, but definitely

27:45.160 --> 27:52.680
TensorFlow is where we invest more of our time, we're part of the community, the open source

27:52.680 --> 27:55.040
community of TensorFlow.

27:55.040 --> 28:01.760
If you look at the distributed implementation, today the most common configuration, it's

28:01.760 --> 28:06.320
like a Lego, you can put it in multiple ways, but if I try to look at what is common in

28:06.320 --> 28:14.160
the industry today, people will typically use TensorFlow together with Horavod and Nikol,

28:14.160 --> 28:21.040
Nikol is the distributed library of NVIDIA.

28:21.040 --> 28:26.960
We usually look at how do you do the same challenge of distributed workload, but within a server,

28:26.960 --> 28:30.960
right, in a server you have multiple GPUs, you also need to distribute the workload between

28:30.960 --> 28:31.960
those GPUs.

28:31.960 --> 28:36.560
That's kind of a very similar way, and then when they scale out multiple servers, they

28:36.560 --> 28:41.000
use RDMA and GPU direct and all those technologies.

28:41.000 --> 28:42.000
Okay.

28:42.000 --> 28:49.320
We layer those together, Nikol, TensorFlow and then Nikol and then Horavod, and the library

28:49.320 --> 28:56.640
that enables sharp NRDMA and GPU direct is Nikol, okay, and that is something that will

28:56.640 --> 28:57.640
be available very soon.

28:57.640 --> 29:02.000
It's not limited to Nikol, I mean, anybody can use that, it's an open source, anybody

29:02.000 --> 29:06.520
if there's somebody here in the audience that listen and want to enable that for Python,

29:06.520 --> 29:09.240
should we, we definitely can do that, there's no problem.

29:09.240 --> 29:13.880
And so then real quick, the Kubernetes piece of this, like we're talking about hardware

29:13.880 --> 29:18.560
and network adapters, you know, I must, it's part of me that says, you know, this is

29:18.560 --> 29:23.480
of, is in the box, the kind of host operating system, you know, has drivers for all the

29:23.480 --> 29:26.480
stuff, like why do we care about this at the Kubernetes level?

29:26.480 --> 29:28.480
What are the challenges there?

29:28.480 --> 29:29.480
Excellent question.

29:29.480 --> 29:35.000
You know, somebody once said that advanced or very smart silicon is nothing but very

29:35.000 --> 29:39.200
expensive send if you don't have the right software, right?

29:39.200 --> 29:43.680
So everything I said here, if you remember, is talking about how do I connect the application

29:43.680 --> 29:48.360
directly to the hardware almost, right, all the bypasses and all the acceleration.

29:48.360 --> 29:53.320
You're looking at layers that provide direct one activity as much as possible from the

29:53.320 --> 29:56.240
application all the way to the hardware.

29:56.240 --> 30:02.720
And when you go to virtualization or containerization, you actually go the opposite direction, right?

30:02.720 --> 30:08.320
You go and you add software layers, like service, and all the other things you hear as

30:08.320 --> 30:14.560
the ends and so on, that all they want to do is virtualize the network and provide a

30:14.560 --> 30:21.760
layer that completely disconnect the application from physical resources, like networked up.

30:21.760 --> 30:27.600
So you're trying to couple the two to gain optimization, but these things are trying

30:27.600 --> 30:28.960
to decouple the two.

30:28.960 --> 30:32.160
So I think they're not exactly working against each other.

30:32.160 --> 30:35.720
Each of them is looking for a different direction.

30:35.720 --> 30:40.640
And what we try to do is marry all the direction to something which is cohesive and high performance.

30:40.640 --> 30:48.920
So what all those SDN software-defined networks and service meshes and virtualization technologies

30:48.920 --> 30:54.960
are trying to do is they're trying to provide you a layer of standardization so you don't

30:54.960 --> 31:01.200
have to write your application multiple times for different hardware and automations and

31:01.200 --> 31:06.480
all the other goodies, like provision network through code, software-defined, and so on.

31:06.480 --> 31:11.640
This is very important, you do want to have a controller that you set security elements,

31:11.640 --> 31:17.040
you set behavioral, you create networks and ports and so everything in software.

31:17.040 --> 31:22.320
But at the same time, if you add multiple layers of software, you have the flexibility,

31:22.320 --> 31:26.760
you have the features, you don't have the efficiency, you don't have the performance,

31:26.760 --> 31:30.320
which you know, you're kind of shooting yourself in the leg.

31:30.320 --> 31:35.280
And what we are trying to do is we're trying to provide a high performance together with

31:35.280 --> 31:36.480
all the flexibility.

31:36.480 --> 31:42.920
So we're trying to marry software-defined service meshes and all these technologies and still

31:42.920 --> 31:51.040
allow in a transparent manner and without any compromises to run in full, bare metal kind

31:51.040 --> 31:52.040
of performance.

31:52.040 --> 31:57.440
You know, getting the same performance as you would get with a bare metal server in a very

31:57.440 --> 32:00.480
highly virtualized, containerized environment.

32:00.480 --> 32:03.480
This is the challenge that we're trying to address.

32:03.480 --> 32:05.320
And so is there a solution?

32:05.320 --> 32:08.680
Is there a project or something that people can go take a look at?

32:08.680 --> 32:09.680
What is that?

32:09.680 --> 32:10.680
Absolutely.

32:10.680 --> 32:11.680
So it's called Kubernetes.

32:11.680 --> 32:13.280
We're doing that as part of Kubernetes, right?

32:13.280 --> 32:14.280
Okay.

32:14.280 --> 32:15.280
So it's going to get vacant.

32:15.280 --> 32:21.080
In Kubernetes, we are developing a CNI, CNI is a network plugin.

32:21.080 --> 32:24.840
Kubernetes has this concept of CNI's.

32:24.840 --> 32:32.280
And we are today using a CNI for technology called Desarai UV, which is a PCI technology.

32:32.280 --> 32:34.880
I don't think we have a lot of time to go through that.

32:34.880 --> 32:39.920
But that is the direction that we provide the direct connectivity.

32:39.920 --> 32:41.800
It's still not fully complete.

32:41.800 --> 32:46.760
There's more and more things that will go in like SDN offloads and other things that today

32:46.760 --> 32:51.800
are available for, say, solution like VMware or OpenStack and other places.

32:51.800 --> 32:54.280
Kubernetes is still young.

32:54.280 --> 32:55.280
And but we are working.

32:55.280 --> 33:00.000
We're working on this with a lot of partners in the open source community where basically

33:00.000 --> 33:06.400
we want to provide the full flexible secure network with the highest performance possible.

33:06.400 --> 33:07.400
Awesome.

33:07.400 --> 33:10.000
Well, Edas, thanks so much for taking the time to walk through all this stuff.

33:10.000 --> 33:11.000
Really good.

33:11.000 --> 33:12.000
Sure.

33:12.000 --> 33:13.000
My pleasure.

33:13.000 --> 33:14.000
Thank you very much.

33:14.000 --> 33:15.000
Thank you.

33:15.000 --> 33:16.000
All right, everyone.

33:16.000 --> 33:21.280
That's our show for today for more information about today's guests or any of the topics mentioned

33:21.280 --> 33:26.600
in the interview, visit twomelai.com slash shows.

33:26.600 --> 33:31.240
To learn more about the IBM AI Enterprise workflow study group, I'll be leading, visit

33:31.240 --> 33:35.160
twomelai.com slash AI workflow.

33:35.160 --> 33:40.080
Of course, if you like what you hear on the podcast, please subscribe, rate, and review

33:40.080 --> 33:42.800
the show on your favorite pod catcher.

33:42.800 --> 33:59.960
Thanks so much for listening and catch you next time.

