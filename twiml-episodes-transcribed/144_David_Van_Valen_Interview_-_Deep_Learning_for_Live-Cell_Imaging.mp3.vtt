WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.880
I'm your host Sam Charrington.

00:31.880 --> 00:36.520
This week we're celebrating the second anniversary of this podcast.

00:36.520 --> 00:41.920
If the pod has made a difference for you, please show us some love by letting us know how

00:41.920 --> 00:48.960
over on our second anniversary page at twimlai.com slash 2av.

00:48.960 --> 00:53.280
You want to hear the ways the podcast has helped you or your business, how it's enabled

00:53.280 --> 00:58.200
you to find or connect to resources you found valuable, or how it's educated you about

00:58.200 --> 01:00.800
new and interesting topics.

01:00.800 --> 01:04.960
Submit your written or audio comments via the page we've set up, and we'll be sharing

01:04.960 --> 01:10.760
a few of your stories with your permission of course, in a special podcast episode celebrating

01:10.760 --> 01:13.480
the Twimble community.

01:13.480 --> 01:18.720
In today's show, I sit down with David Van Veilin, assistant professor of bioengineering

01:18.720 --> 01:21.040
and biology at Caltech.

01:21.040 --> 01:25.840
David joined me after his talk at the Figure 8 train AI conference to chat about his research

01:25.840 --> 01:31.280
using image recognition and segmentation techniques in biological settings.

01:31.280 --> 01:35.800
In particular, we discuss his use of deep learning to automate the analysis of individual

01:35.800 --> 01:39.360
cells and live cell imaging experiments.

01:39.360 --> 01:44.000
We had a really interesting discussion around the various practicalities he's learned

01:44.000 --> 01:48.640
about training deep neural networks for image analysis, and he shares some awesome insights

01:48.640 --> 01:52.360
into which of the techniques from the deep learning research have worked for him and

01:52.360 --> 01:53.840
which haven't.

01:53.840 --> 01:57.600
If you're a fan of our nerd alert shows, you'll really like this one.

01:57.600 --> 02:01.880
Before we jump in, I'd like to send a shout out to our friends over at Figure 8 for their

02:01.880 --> 02:07.280
continued support of the show and their sponsorship of this week's series which all took place

02:07.280 --> 02:09.320
at train AI.

02:09.320 --> 02:15.080
Figure 8 is the essential human and loop AI platform for data science and machine learning teams.

02:15.080 --> 02:19.920
The Figure 8 software platform trains, tests, and tunes machine learning models to make

02:19.920 --> 02:22.240
AI work in the real world.

02:22.240 --> 02:27.800
Learn more at www.figure-8.com

02:27.800 --> 02:32.600
Just a reminder, this episode was recorded live on site so there's some unavoidable

02:32.600 --> 02:34.200
background noise.

02:34.200 --> 02:39.240
And now on to the show.

02:39.240 --> 02:44.600
All right everyone, I am here with David Van Bellen at the train AI conference.

02:44.600 --> 02:49.440
David is a system professor in the biology and bioengineering department at Caltech.

02:49.440 --> 02:52.040
David, welcome to this week in machine learning and AI.

02:52.040 --> 02:53.520
Thank you for having me.

02:53.520 --> 02:54.520
Awesome.

02:54.520 --> 02:57.440
Why don't we get started by having you tell us a little bit about your background and

02:57.440 --> 03:00.720
how you came into AI as a biologist?

03:00.720 --> 03:01.720
Yeah.

03:01.720 --> 03:05.680
So I can start from the beginning.

03:05.680 --> 03:13.080
So I was born in Los Angeles, grew up on the East Coast, upstate New York for middle school

03:13.080 --> 03:16.400
and West Virginia for high school, where upstate.

03:16.400 --> 03:24.040
So we lived in Liverpool, which is, I want to say like within like an hour of Syracuse.

03:24.040 --> 03:30.800
So it was very, it was upstate enough where things like like effects know were a very,

03:30.800 --> 03:31.800
is a reality.

03:31.800 --> 03:36.680
Yeah, I asked because I grew up in New York City and went to school, upstate it.

03:36.680 --> 03:37.680
Okay.

03:37.680 --> 03:38.680
Awesome.

03:38.680 --> 03:39.680
And that's our New York.

03:39.680 --> 03:40.680
Yeah.

03:40.680 --> 03:41.680
I'm familiar with RPI.

03:41.680 --> 03:44.880
They have very, very good students.

03:44.880 --> 03:45.880
Yeah.

03:45.880 --> 03:46.880
Yeah.

03:46.880 --> 03:53.680
And so I went to high school in West Virginia and graduated, went to college.

03:53.680 --> 03:57.480
So I went to MIT and I was a double major in math and physics.

03:57.480 --> 04:05.000
At the same time, I had the luxury of having like fresh and biology taught by like Eric

04:05.000 --> 04:10.000
Lander, who's, you know, big shot scientists who like runs like the Broad Institute, between

04:10.000 --> 04:11.800
like MIT and Harvard.

04:11.800 --> 04:17.600
You know, he was like one of the big early pioneers in like the human genome research project.

04:17.600 --> 04:20.280
And he was also as a background as a mathematician.

04:20.280 --> 04:25.560
And so the way he explained biology really resonated with me and made me want to like sort

04:25.560 --> 04:29.160
of continue that thread further.

04:29.160 --> 04:37.560
And so after I finished at MIT, I went to the MD PhD program at UCLA and Caltech.

04:37.560 --> 04:42.400
And so the way that program works is you spend two years, first two years, doing medical

04:42.400 --> 04:46.040
school, then you do your PhD and then you return for your clinical training.

04:46.040 --> 04:53.040
And I did my PhD at Caltech in the employee phase of the department with Rob Phillips.

04:53.040 --> 04:59.080
And there, it was a really great environment because there's exposure to sort of, you

04:59.080 --> 05:03.760
know, thinking about problems in biology, using like ideas from physics.

05:03.760 --> 05:08.600
But then there was also some very quantitative experiments that we're doing as well.

05:08.600 --> 05:11.000
And so there was a lot of imaging.

05:11.000 --> 05:13.480
There was a lot of image analysis.

05:13.480 --> 05:17.440
And there was a lot of image segmentation and so image segmentation, you know, figuring

05:17.440 --> 05:21.400
out what parts of your images correspond to like which objects you care about.

05:21.400 --> 05:22.400
We have to do a lot of that.

05:22.400 --> 05:26.080
So that was sort of like my first exposure to those sorts of problems.

05:26.080 --> 05:30.320
And there was sort of this given where if you were sort of the more mathy type and you

05:30.320 --> 05:34.480
could do the physics calculations, then people just assume that you're going to be good

05:34.480 --> 05:38.480
at doing the image segmentation and that you'd figure out how to get those tools to work

05:38.480 --> 05:40.960
to solve like your problems of interest.

05:40.960 --> 05:45.280
And by and large, like, you know, they're right, but it's still like a separate skill set

05:45.280 --> 05:47.560
that you have to learn.

05:47.560 --> 05:53.520
And yeah, and so that was probably like my first introduction to computer vision, to image

05:53.520 --> 05:57.560
segmentation, image recognition was during my graduate training.

05:57.560 --> 06:03.440
Finished my PhD on 2011, finished my MD 2013.

06:03.440 --> 06:09.960
So I went back to clinical training from 2011 to 2013 and you know, got like more exposure

06:09.960 --> 06:14.240
to problems in clinical medicine.

06:14.240 --> 06:19.920
And then after that, I went up to Stanford to continue training as a postdoctoral fellow.

06:19.920 --> 06:24.760
And so I was up at Stanford for about four or five years with Marcus Cover in the bioengineering

06:24.760 --> 06:26.560
department.

06:26.560 --> 06:33.400
And there there's when I started getting into sort of like the more machine learning

06:33.400 --> 06:40.680
slash AI space and so your initial exposure to computer vision was more traditional tools

06:40.680 --> 06:47.400
that are close to machine learning, so if you're doing edge finding filtering, thresholding,

06:47.400 --> 06:56.240
watershed transforms, you know, wavelets, all of that, you know, that was my first exposure.

06:56.240 --> 06:59.760
And so what you would usually end up doing is you, you know, you have an idea of like what

06:59.760 --> 07:02.960
these tools did and how each one worked.

07:02.960 --> 07:10.000
And then you'd basically bash some collection of these tools together to work on like your

07:10.000 --> 07:12.040
problem like of interest.

07:12.040 --> 07:15.880
And so the experiments that we were doing at the time, and so I think you probably saw

07:15.880 --> 07:21.880
some of it at the talk that I gave yesterday is, you know, we have these images of cells

07:21.880 --> 07:26.600
and so at the, so let me just give it back story of the talk for like your listeners.

07:26.600 --> 07:30.320
So part of the talk I was sort of showing like what I was doing during my graduate work.

07:30.320 --> 07:34.120
We were very interested in like the life cycle of these viruses and in particular how these

07:34.120 --> 07:38.680
viruses got their DNA from inside the viral capsid into like the bacterial cell.

07:38.680 --> 07:42.440
We didn't really have an idea of how it worked and so we had this idea to create an experiment

07:42.440 --> 07:43.680
to like let us watch.

07:43.680 --> 07:49.080
And so the challenge was, can you figure out a way to look at one piece of DNA being transferred

07:49.080 --> 07:52.360
from like inside the virus to inside the cell?

07:52.360 --> 07:57.120
The answer is yes you can, you have to use these very particular kind of dye that will

07:57.120 --> 08:00.880
stain the DNA while it's inside the viral capsid and then you can watch the dye molecules

08:00.880 --> 08:05.040
get transferred between the virus into the bacterial cell.

08:05.040 --> 08:08.800
So you get these gorgeous movies but we then have to do is quantify these movies so you

08:08.800 --> 08:12.120
can, you know, have some sense of the dynamics of this process.

08:12.120 --> 08:18.120
And to get a sense of the type of movie that we're looking at, like what's the field of

08:18.120 --> 08:23.400
view and in terms of how zoomed in this is this, is are you looking at one transfer, one

08:23.400 --> 08:26.440
organism or is it many, many that you have to figure out?

08:26.440 --> 08:32.440
For one paper's worth of data, there's roughly about 50 or so events that you're looking

08:32.440 --> 08:33.440
at.

08:33.440 --> 08:37.240
And it's not that often.

08:37.240 --> 08:42.520
So you're generally looking at maybe like a few hundreds of you live you, there might

08:42.520 --> 08:48.320
be, you know, somewhere between 20 to 100 is those cells within that field of view.

08:48.320 --> 08:54.280
And you know, all of those cells maybe like a couple will have like an infection event

08:54.280 --> 08:58.240
within what time period within the time period of about 30 minutes.

08:58.240 --> 09:04.800
So these movies were taken in roughly like one snapshot every 30 seconds to every minute.

09:04.800 --> 09:09.480
And you know, that's sort of like how the, that data was collected, granted it's been

09:09.480 --> 09:14.000
about seven years since I collected that data set.

09:14.000 --> 09:17.880
But from what I remember, that's, that's what we, that's what we did.

09:17.880 --> 09:21.000
And so you get these like, you get these gorgeous movies.

09:21.000 --> 09:26.120
And so what you need to then do is, you know, identify, okay, which part of the movie

09:26.120 --> 09:30.320
are which pixels within each frame correspond to like the cells, which ones correspond to

09:30.320 --> 09:34.880
background, which ones correspond to the virus, which ones correspond to the viruses that

09:34.880 --> 09:36.600
we actually care about.

09:36.600 --> 09:39.680
And then once you've done that, then you can go and quantify your images, right?

09:39.680 --> 09:43.520
And so you can ask, you know, how bright were the viruses over time, how bright were the

09:43.520 --> 09:49.960
cells over time and use that to generate, you know, some like quantitative curves of,

09:49.960 --> 09:54.120
here's how much DNA was being transferred as a function of time.

09:54.120 --> 09:59.200
And then once you have like that, those curves, then you can go and do your, you know, your

09:59.200 --> 10:05.000
fancy modeling, your physics and, you know, determine, okay, well, from this set of hypotheses

10:05.000 --> 10:08.960
for how I think this process is happening, you know, which ones are actually like consistent

10:08.960 --> 10:10.880
with like the data that we've generated.

10:10.880 --> 10:15.880
And so when you're trying to determine the, the brightness, which tells you how much DNA

10:15.880 --> 10:21.200
was transferred, is this, how much in terms of the length of the sequences transferred

10:21.200 --> 10:22.200
or some of the other.

10:22.200 --> 10:27.080
It's ideally like what you want to, that's ideally like what you want is like how many

10:27.080 --> 10:33.280
base pairs or how many nanometers worth of DNA has been transferred like over time.

10:33.280 --> 10:35.920
And so you kind of use a calibration curve.

10:35.920 --> 10:40.360
And so you know at the beginning of the event, you know, the virus has about 48 kilobase

10:40.360 --> 10:45.960
pairs of DNA at the end of the event, like it should have zero and use some like linear

10:45.960 --> 10:51.200
interpolation, you know, assuming that, you know, you know, linearly related, you know,

10:51.200 --> 10:55.800
brightness versus like how much DNA, and then you can like infer from the movies, go from

10:55.800 --> 10:59.720
brightness to like the amount of DNA.

10:59.720 --> 11:02.520
So where does machine learning and AI fit into all that?

11:02.520 --> 11:03.520
Yeah.

11:03.520 --> 11:08.000
So machine learning and AI fits in, and how do you do like the object, how do you do like

11:08.000 --> 11:10.160
the object recognition, the image segmentation?

11:10.160 --> 11:15.440
And, you know, that experiment was, you know, it was sort of like the classical computer

11:15.440 --> 11:18.080
vision approaches that we used to analyze that data.

11:18.080 --> 11:22.240
And when I started my postdoctoral fellowship at Stanford, I had ideas for experiments that

11:22.240 --> 11:23.520
I'd like to do.

11:23.520 --> 11:29.720
And basically what I wanted to do was, you know, look at genome scale knockout libraries

11:29.720 --> 11:34.520
with image them and then look at them with single cell resolution.

11:34.520 --> 11:36.280
So what's a genome scale knockout library?

11:36.280 --> 11:42.760
Genome scale knockout library is a collection of strains where in each strain, you have one

11:42.760 --> 11:45.880
gene that's been removed or inactivated.

11:45.880 --> 11:51.720
And so if you're interested in, you know, say like a particular like biological process.

11:51.720 --> 11:55.680
So the ones that the one that I was interested in at the time and that my lab is currently

11:55.680 --> 11:58.040
interested in are host virus interactions.

11:58.040 --> 12:02.320
And so you want to understand how do viruses and their host talk to each other and what

12:02.320 --> 12:06.080
parts of the host are important for that communication.

12:06.080 --> 12:12.600
There are experiments that you can do where if you have a way to look at the communication

12:12.600 --> 12:19.840
using imaging, then you can basically go one by one, remove a gene from like the host.

12:19.840 --> 12:26.240
And if that gene was important, then that's going to alter what the communication was

12:26.240 --> 12:31.720
or what like the outcome of the violent infection ended up being.

12:31.720 --> 12:36.600
And you know, that's what you know, the host in this case is the host in this case is not

12:36.600 --> 12:38.560
the virus, but the cell that's infecting.

12:38.560 --> 12:39.560
Right.

12:39.560 --> 12:40.560
Yeah.

12:40.560 --> 12:47.760
So moving that particular sequence prior to the infection, prior to the infection.

12:47.760 --> 12:51.800
And so you've got a control experiment, you've got a control where nothing's been removed

12:51.800 --> 12:56.560
and then you have a collection of strains where in each strain, one gene has been removed.

12:56.560 --> 13:03.400
And so there are collections that like this that exists for certain strains of E. coli,

13:03.400 --> 13:06.920
there are collections that exist for East.

13:06.920 --> 13:13.080
People at the Chan's upper initiative are creating collections like this that exist for, you

13:13.080 --> 13:15.160
know, mammalian cells.

13:15.160 --> 13:19.680
And these are really powerful tools because, you know, because they exist in like this

13:19.680 --> 13:24.400
array format where one well of additional have one strain that has one gene removed, then

13:24.400 --> 13:32.160
you can systematically go one by one and ask, okay, does this removal, does that influence

13:32.160 --> 13:33.520
what I was interested in studying?

13:33.520 --> 13:37.760
So in this case, does it influence the outcome of particular viral infection?

13:37.760 --> 13:42.480
So what we want to do is to be able to read out a screen of this library using imaging

13:42.480 --> 13:47.280
and imaging with single star resolution, there you're going from the case of where you're

13:47.280 --> 13:53.320
doing, you know, 50 or 100 or 1000 cells to doing millions.

13:53.320 --> 13:59.320
And there you don't have the luxury of being able to like go back and like manually correct

13:59.320 --> 14:00.320
like segmentation errors.

14:00.320 --> 14:06.120
You really need something that's going to work 99% of the time if not, if not better.

14:06.120 --> 14:08.080
And that's where the machine learning comes in.

14:08.080 --> 14:13.200
And so I remember when I was a medical student, I was sort of like browsing Facebook and

14:13.200 --> 14:21.280
I realized like, hey, you know, Facebook actually knows like where the faces are in, you know,

14:21.280 --> 14:25.400
in like all the images I'm uploading, not only that, but it has like ideas of like who's

14:25.400 --> 14:27.320
in these, who's in these pictures?

14:27.320 --> 14:32.480
Like, is it you? Is it like your, you know, like you're one of your three like best friends?

14:32.480 --> 14:36.240
And you know, just, you know, I've done like a lot of microscopy in my, um, over my

14:36.240 --> 14:37.240
career.

14:37.240 --> 14:43.200
And it's like, I know that the images that I collect on microscope are a lot simpler than

14:43.200 --> 14:48.160
the images of like me doing Brazilian Jiu-Jitsu or hanging out with my friends that like

14:48.160 --> 14:49.160
a part of your whatnot.

14:49.160 --> 14:50.160
Right.

14:50.160 --> 14:55.240
You know, what are those guys doing and can I steal it, um, or borrow or co-op, but, you

14:55.240 --> 15:00.240
know, rallies like, you know, blatant theft, you know, can I take those tools and then repurpose

15:00.240 --> 15:02.760
them to like work on our microscopy data?

15:02.760 --> 15:07.680
And so the answer was, you know, the answer was yes, um, and on the time that I started

15:07.680 --> 15:11.200
my postdoc and started like, think about these issues again, was literally right as the

15:11.200 --> 15:13.840
deep learning and fletch and curve, um, started to take off.

15:13.840 --> 15:14.840
Okay.

15:14.840 --> 15:20.320
And so it turned out, you know, yes, you can repurpose these tools and it, you know, it

15:20.320 --> 15:23.520
ends up working like really, really, really well.

15:23.520 --> 15:28.440
And so we had a paper that we did, um, and plus computation biology, um, basically showing

15:28.440 --> 15:35.960
that for the single cell image segmentation problem, the existing tools, um, work as

15:35.960 --> 15:39.760
well as a state of the art, um, for a cross a variety of different cell types.

15:39.760 --> 15:45.920
So whether you're imaging bacteria, whether you're imaging mammalian cells in cell culture,

15:45.920 --> 15:51.040
if you're looking at phase images, if you're looking at fluorescence, microscopy images,

15:51.040 --> 15:55.160
that, you know, the tools work really well and it's at a point that, you know, people

15:55.160 --> 15:59.160
in the cell biology community, you know, really need to start taking like a really close

15:59.160 --> 16:04.000
look at these tools and, you know, start thinking about like, how can these, um, how can

16:04.000 --> 16:08.520
these change the way that I do my experiments and what experiments will they let me do?

16:08.520 --> 16:10.520
That would previously have been like impossible.

16:10.520 --> 16:11.520
Okay.

16:11.520 --> 16:12.520
Yeah.

16:12.520 --> 16:16.840
And so did you use off the shelf CNNs and all the kind of stuff, or did you have to start?

16:16.840 --> 16:21.000
And yeah, so like I started at the beginning.

16:21.000 --> 16:28.480
And so yeah, so at the time, um, when I first started, um, Andre Carpathi put together

16:28.480 --> 16:32.840
and faithfully put together like their first CS231 end course, which I think is now kind

16:32.840 --> 16:34.360
of like iconic in the field.

16:34.360 --> 16:38.760
I got to sit on that for a couple of weeks, um, to like sort of like get, um, familiarize

16:38.760 --> 16:42.280
myself with the field, enough of the point where I can go and like, you know, hack some

16:42.280 --> 16:47.640
piece of code together, Karrist didn't really, um, was didn't really exist at the time.

16:47.640 --> 16:49.840
Um, cafe was like a new thing.

16:49.840 --> 16:54.560
Um, TensorFlow didn't exist and it was basically, you know, do you want to learn kuda?

16:54.560 --> 16:57.120
Or were you like, okay, like learning like piano?

16:57.120 --> 17:01.160
And so when I first started, um, I was programming in like naked piano, had to figure

17:01.160 --> 17:05.880
out how to pipe data into like, you know, simple like CNN, um, architectures.

17:05.880 --> 17:07.200
How do you like save models?

17:07.200 --> 17:10.360
How do you like, you know, once you like save parameters, like how do you load it?

17:10.360 --> 17:15.360
Like the really nice frameworks that exist today, like those didn't, um, those didn't

17:15.360 --> 17:16.960
exist at the time.

17:16.960 --> 17:19.480
And so, you know, sort of like had to like figure like all those things out, get some like

17:19.480 --> 17:21.720
hacky pieces of code that would work.

17:21.720 --> 17:26.920
But the first, like, basically like the very simplest, you know, CNN architectures that

17:26.920 --> 17:31.520
you could write down ended up working, um, really well.

17:31.520 --> 17:35.520
And so actually like, as far as this stuff that my lab uses, um, in production for the

17:35.520 --> 17:41.120
data that we analyze, um, the data that we analyze with, um, well, with our collaborators,

17:41.120 --> 17:45.520
um, we, you know, still use those like very simple, um, neural network architectures.

17:45.520 --> 17:50.760
Um, I would say like there's, um, there's a lot of like really cool stuff that people

17:50.760 --> 17:51.760
have done.

17:51.760 --> 17:58.360
Um, but I found it very useful to pay attention to what's going to generalize on small datasets,

17:58.360 --> 18:01.640
as opposed to, you know, what's the latest and greatest that are out there.

18:01.640 --> 18:06.840
And, you know, just from like my own personal experience, you know, some of the things that

18:06.840 --> 18:11.040
will perform like really well on image net, like say like residual networks, um, they have

18:11.040 --> 18:12.040
issues with overfitting.

18:12.040 --> 18:16.720
And if you don't have like that large corpus of data where your network can learn all

18:16.720 --> 18:22.360
of the edge cases, you'll still, you'll get more like practical utility from like, you

18:22.360 --> 18:26.920
know, the simpler neural networks trained, um, in an approach, in a way where you like,

18:26.920 --> 18:32.600
you're doing like the regularization, um, and the, you know, normalization and like the

18:32.600 --> 18:34.360
post processing, like correctly.

18:34.360 --> 18:41.000
And so was your, your training data, the datasets that you had traditionally created using

18:41.000 --> 18:43.960
manual segmentation or traditional computer vision?

18:43.960 --> 18:44.960
Yeah.

18:44.960 --> 18:49.920
So those were, so I would say like this is true and academia is true in life is that if

18:49.920 --> 18:54.160
you want to get people excited about something, like first you have to prove that it's going

18:54.160 --> 18:55.160
to work.

18:55.160 --> 19:01.280
Um, and so the first generation of training data, um, like I created like myself.

19:01.280 --> 19:10.240
And so I annotated a couple images of bacterial cells in image J and, you know, image J is

19:10.240 --> 19:16.000
a, um, Java, a Java program that people have labeling box bounding box tools.

19:16.000 --> 19:21.760
It's like a general, it's a general purpose, um, image visualization and also image analysis,

19:21.760 --> 19:23.600
um, toolkit that the NIH supports.

19:23.600 --> 19:24.600
Okay.

19:24.600 --> 19:27.800
So they have tools for annotating images.

19:27.800 --> 19:33.120
It's relatively clunky, but it worked then and it works now, um, for some cases like

19:33.120 --> 19:34.880
we, um, we still use it.

19:34.880 --> 19:35.880
Mm-hmm.

19:35.880 --> 19:37.120
But yeah, I generated it.

19:37.120 --> 19:38.120
It worked.

19:38.120 --> 19:41.880
You threw out a few different numbers when you said a couple of images.

19:41.880 --> 19:42.880
Yeah.

19:42.880 --> 19:46.560
Is this a couple of images with 50 things that need to be annotated or a couple of images

19:46.560 --> 19:48.480
with a million things that need to be annotated?

19:48.480 --> 19:52.720
A couple of images with a couple with about a hundred-ish things that need to be annotated.

19:52.720 --> 19:53.720
Okay.

19:53.720 --> 19:57.840
And we paid, um, for the plus computation biology paper and even now, we paid very close

19:57.840 --> 20:01.080
attention to data augmentation.

20:01.080 --> 20:06.040
We paid a lot of attention to normalizing images before they go into like the neural network.

20:06.040 --> 20:09.320
So you can account for like variations in image acquisition.

20:09.320 --> 20:14.480
And we paid a lot of a tent, we paid like a good amount of attention to post-processing

20:14.480 --> 20:17.800
what happens to like the output of the neural networks afterwards.

20:17.800 --> 20:21.640
And can you still leverage some of the, some of the classical tools that you use in computer

20:21.640 --> 20:27.760
vision, like the watershed transform, like active contours or whatnot to sort of refine

20:27.760 --> 20:31.520
like what the neural network produces to produce something that you can like actually like

20:31.520 --> 20:34.360
use on like real data.

20:34.360 --> 20:38.040
And we found that by like paying attention like those things, you know, you don't have to

20:38.040 --> 20:41.880
use like the latest and greatest, you know, stuff in the neural networks in the deep learning

20:41.880 --> 20:48.240
space, relatively simple things with those things like paid attention to work like really

20:48.240 --> 20:49.240
well.

20:49.240 --> 20:52.480
And did you iterate to paying attention to all those things?

20:52.480 --> 20:55.640
Or did you just kind of start there and that's what work?

20:55.640 --> 21:02.880
We were always iterating people create like really, really great pieces of work all the

21:02.880 --> 21:03.880
time.

21:03.880 --> 21:07.880
And like there's just an innately curious part of me wondering like how well will that

21:07.880 --> 21:12.800
stuff work on the data sets that we've generated and the other people have generated sort

21:12.800 --> 21:14.400
of like in this space.

21:14.400 --> 21:20.160
And now that I'm running my own group, I have people to sort of like help explore like

21:20.160 --> 21:21.160
that space.

21:21.160 --> 21:27.880
I will say that we're primarily interested in like the scientific questions that we can

21:27.880 --> 21:30.440
answer with these tools.

21:30.440 --> 21:34.040
And that curiosity kind of like drives our interest in the deep learning space.

21:34.040 --> 21:38.080
We're well, I find it like interesting and mathematically elegant.

21:38.080 --> 21:40.880
We're not necessarily like deep learning for like deep learning sake.

21:40.880 --> 21:44.200
It's more like deep learning, you know, because like we have like real problems that

21:44.200 --> 21:46.600
we're trying to, um, that we're trying to address.

21:46.600 --> 21:47.600
That's the way it should be.

21:47.600 --> 21:48.600
I think so.

21:48.600 --> 21:49.600
Yeah.

21:49.600 --> 21:51.600
I think so.

21:51.600 --> 21:55.480
So you mentioned a few kind of tricks and techniques that you paid attention to, data

21:55.480 --> 21:56.480
augmentation.

21:56.480 --> 21:57.480
Yeah.

21:57.480 --> 22:00.320
How did you go about that or what were some of the highlights or anything stand out in

22:00.320 --> 22:04.440
terms of, you know, doing this really made it work for us.

22:04.440 --> 22:08.560
Was it standard kind of, you know, rotating things around and changing brightness levels

22:08.560 --> 22:09.840
and that kind of thing?

22:09.840 --> 22:13.600
I would say like the things that like, the things that like we found that worked, um,

22:13.600 --> 22:19.160
is just like being like trying to be a systematic, um, as we could with the things that we tried.

22:19.160 --> 22:23.080
And so like we have tried, um, so I can like just go through the list of things that we

22:23.080 --> 22:26.680
tried for our plus computational, um, biology paper.

22:26.680 --> 22:31.480
The usual like data augmentation things that we've tried, um, so image rotations, image

22:31.480 --> 22:37.480
flipping, those things like really matter, um, I would say there were some things that

22:37.480 --> 22:41.240
we tried that didn't really make so much of an impact.

22:41.240 --> 22:47.160
So image sharing and so sharing is like another like another operation that you can, um, that

22:47.160 --> 22:48.280
you can do to augment your data.

22:48.280 --> 22:51.720
We found that that didn't really like impact like performance of like the neural networks

22:51.720 --> 22:52.720
that we got.

22:52.720 --> 22:59.520
Um, I would say paying attention to how you're doing the training, um, made an impact.

22:59.520 --> 23:06.720
And so we found that for us, the simpler, you know, simpler stochastic gradient is sent

23:06.720 --> 23:14.760
with, with Nesterov momentum that ended up being, um, better, roughly speaking than, you

23:14.760 --> 23:17.440
know, RMS prop, Adam, et cetera, et cetera.

23:17.440 --> 23:21.320
But that was also conditioned on like what networks that we were using and like what features

23:21.320 --> 23:23.000
we had within those networks.

23:23.000 --> 23:29.280
And so bash normalization, um, sort of influences like which one is going to work better, um,

23:29.280 --> 23:33.160
whether using like dropout or using like bash normalization, like with dropout, we tend

23:33.160 --> 23:34.160
not to do that.

23:34.160 --> 23:39.320
To not to use dropout or to not to use bash normalization, then we generally speaking,

23:39.320 --> 23:40.800
don't use dropout.

23:40.800 --> 23:45.640
And I would say like I, we generally speaking, don't use dropout that much.

23:45.640 --> 23:49.360
Uh, I don't know how general this quote unquote finding is.

23:49.360 --> 23:56.120
But one thing that we noticed was if you look at the, let me just take like a brief second

23:56.120 --> 23:59.480
to sort of like stay like how we do like our neural network training.

23:59.480 --> 24:00.960
And so we sort of repose.

24:00.960 --> 24:01.960
Yeah.

24:01.960 --> 24:05.560
So basically like the task that we're trying to do is instant segmentation.

24:05.560 --> 24:09.720
And so for your listeners who are sort of like familiar with this space, it's the same

24:09.720 --> 24:13.200
task that networks like mask our CNN, um, solve.

24:13.200 --> 24:16.520
We started working on this before mask our CNN came out.

24:16.520 --> 24:19.280
And so there's sort of like, you know, different ways of like slicing that up.

24:19.280 --> 24:25.160
Well, the way that, uh, we did it is, is a framework that Dan Cirisson posed in a paper,

24:25.160 --> 24:29.120
um, many years ago, which is recasting the instant segmentation problem as an image

24:29.120 --> 24:30.920
classification problem.

24:30.920 --> 24:35.720
And trying to classify like every pixel as either being, um, in interior, being inside

24:35.720 --> 24:39.520
a object, in this case, inside a single cell, being at the boundary of a single cell,

24:39.520 --> 24:41.800
or being as part of the background.

24:41.800 --> 24:46.560
And you can looking at things in that way, then, you know, it's basically like an image

24:46.560 --> 24:51.360
classification problem, um, we train using like sample wise, um, training.

24:51.360 --> 24:56.400
And so we basically have these, we have like these maps of what every pixel like should

24:56.400 --> 25:01.240
be, we crop like a small area around like each pixel, feed that into a deep convolutional

25:01.240 --> 25:03.880
neural network to do the classification.

25:03.880 --> 25:08.200
And once it's trained, um, we basically use dilated convolutions and dilated pooling

25:08.200 --> 25:13.640
kernels to run that classification neural network on entire image, um, to generate like

25:13.640 --> 25:14.640
dance predictions.

25:14.640 --> 25:19.440
What are the, what are dilated convolutions and dilated, um, so dilated convolutions

25:19.440 --> 25:26.880
are basically, if you take your convolutional kernel, um, dilations are basically just like

25:26.880 --> 25:33.080
inserting zeros in between, um, the weights of your, of your convolutional kernel.

25:33.080 --> 25:36.480
And there's a similar, um, there's a similar thing, um, in the pooling kernel.

25:36.480 --> 25:37.480
Yeah.

25:37.480 --> 25:42.720
And so it turns out you, you kind of have to do that because, because you're training

25:42.720 --> 25:50.200
this neural network, uh, to do pixel wise classification, once it's trained to do dance prediction, if

25:50.200 --> 25:55.800
you try to feed it in patch by patch, if you have two neighboring pixels, then basically

25:55.800 --> 26:00.280
all of the patches around it are essentially redundant computations.

26:00.280 --> 26:04.680
And so you need, um, you need to use like the dilated, um, kernels to, you know, get like

26:04.680 --> 26:11.160
a fully convolutional, um, execution instead of your, your dilation factor end up being

26:11.160 --> 26:13.160
like a hyper parameter.

26:13.160 --> 26:15.800
No, it's like, it's like mathematically like exact.

26:15.800 --> 26:20.160
If you dilate things in the right way, then it ends up being like mathematically the same

26:20.160 --> 26:24.200
as like running everything like, um, doing like patch wise, um, patch wise predict show.

26:24.200 --> 26:25.200
Okay.

26:25.200 --> 26:28.720
So I got like sidetracked on, on, on, on that.

26:28.720 --> 26:32.240
So like there's a reason, I mean, this happens to me like all the time as I'm getting, as

26:32.240 --> 26:38.040
I'm getting older, um, we were talking about the kind of various ways you've tweaked

26:38.040 --> 26:43.800
the process, and you mentioned that, um, you know, while there are some, uh, newer approaches

26:43.800 --> 26:50.880
to segmentation, you, um, kind of rolled your own based on a paper, yeah, and, uh, ended

26:50.880 --> 26:54.000
up using this dilation as a way to do the pixel by pixel.

26:54.000 --> 26:55.000
Yeah.

26:55.000 --> 26:56.000
Okay.

26:56.000 --> 26:57.000
Yeah.

26:57.000 --> 26:58.840
So, and we're all, I think we're also talking about like what parts did we find, or what

26:58.840 --> 27:00.240
authentic, and what wasn't.

27:00.240 --> 27:01.240
Yeah.

27:01.240 --> 27:04.280
So like the output of this, um, the output of like this approach is you get like probably

27:04.280 --> 27:10.040
maps for what's like inside of it, so what's the cell boundary, and what's like a background.

27:10.040 --> 27:16.400
Um, if you use drop, and so for the post processing, it's actually helpful to have like some notion

27:16.400 --> 27:17.400
of uncertainty.

27:17.400 --> 27:23.720
And so if the neural network is like not sure, it's easier for the post processing for, let's

27:23.720 --> 27:26.520
say, the picture was not, pixel where it's not sure.

27:26.520 --> 27:32.640
It's more useful to have that pixel value be like 0.5 instead of, you know, making a very

27:32.640 --> 27:34.720
certain like false call.

27:34.720 --> 27:40.200
And so what we found, uh, with some of our experiments with dropout is that it was essentially

27:40.200 --> 27:44.440
removing some of the uncertainty that was encoded in these probability maps, and that

27:44.440 --> 27:48.880
made it a little harder to do like the downstream, um, processing.

27:48.880 --> 27:51.400
That's why I, I kind of like shy away from dropout.

27:51.400 --> 27:56.280
Um, I don't know like how general this is, and, but this is like that was like one of

27:56.280 --> 28:02.600
the, um, empirical observations on that, um, you know, I've noted during like my, um,

28:02.600 --> 28:07.440
time-experiment of these things, um, other things that like really help, um, more, like

28:07.440 --> 28:13.880
more training data, the more data that you have to capture more and more edge cases, then

28:13.880 --> 28:18.240
like the better, um, the better the neural networks are going to perform, uh, we found that

28:18.240 --> 28:23.920
roughly if you have like standardized like acquisition, um, conditions, and you collect

28:23.920 --> 28:28.120
your training data on like the same acquisition conditions, then you really only need, I'd

28:28.120 --> 28:32.000
say like on the order of like a few hundred, um, to a thousand cells to get like neural

28:32.000 --> 28:37.440
networks and it'd be like good enough to analyze like subsequent experiments, but the more,

28:37.440 --> 28:40.480
the more training data that you get, then the better that it's going to be.

28:40.480 --> 28:43.560
And I think now we're getting to the point, I mean, we're getting the point we're having

28:43.560 --> 28:46.240
like more and more like data sets that are like publicly available.

28:46.240 --> 28:52.000
So when we published our paper in plus computational biology, we released like all the data sets

28:52.000 --> 28:57.960
we annotated, I think it was on the order of like maybe like, you know, 10 to 15-ish

28:57.960 --> 29:04.000
images, it was quite a bit of work to do, um, but you know, I'm a big fan of if you're

29:04.000 --> 29:08.720
publishing something, you know, make everything they're doing like openly available, um, and

29:08.720 --> 29:13.080
I think like as we're seeing, there is more people are recognizing that the challenges

29:13.080 --> 29:16.360
in this space aren't necessarily like the algorithms anymore.

29:16.360 --> 29:20.080
It's like having access to like good like high quality data, right?

29:20.080 --> 29:25.040
If you give someone, you know, a good data set, you know, there's going to be like some

29:25.040 --> 29:29.120
nerd in some basement, you know, who's like lives on stack overflow is going to be able

29:29.120 --> 29:33.720
to put together like some kind of machine learning solution, uh, using that data set.

29:33.720 --> 29:36.880
It's just are there data sets that are out there.

29:36.880 --> 29:40.680
And I think now we're starting to see like more and more data sets, um, like in this space.

29:40.680 --> 29:42.920
And so the ones that come to mind.

29:42.920 --> 29:49.920
So I think there's, there was one that was released by, um, the Broad Institute, um, it was

29:49.920 --> 29:54.160
like part of like the 2018, um, Kaggle data science competition, uh, for doing nuclear

29:54.160 --> 29:55.160
segmentation.

29:55.160 --> 29:59.320
Um, that was one of the things that we tackled in our plus computation biology paper.

29:59.320 --> 30:05.400
But more data sets on more data types and you'll get better on better neural networks.

30:05.400 --> 30:12.240
And as we saw today, um, you know, figure eight is releasing annotated data on H&E pathology

30:12.240 --> 30:16.800
images, um, for doing nuclear segmentation on a variety of like different tissue types.

30:16.800 --> 30:22.800
And so I think like the more and more data sets that we have that cover more and more,

30:22.800 --> 30:26.840
um, use cases, then like the closer we're going to get to actually having like general tools

30:26.840 --> 30:31.880
that, you know, people be able to use like off the shelf, um, which I think is going to

30:31.880 --> 30:33.440
be like really, really awesome.

30:33.440 --> 30:34.440
So yeah.

30:34.440 --> 30:40.000
Uh, in my view, like I view this space as the challenges, I don't want to like downplay

30:40.000 --> 30:42.880
how hard it is to like it started like in machine learning and deep learning.

30:42.880 --> 30:46.200
It's like, you know, of course, there's like, there's like challenges and issues, but

30:46.200 --> 30:52.240
it's really, I'd say like data sets, um, are like one big challenge and deployment, um,

30:52.240 --> 30:56.200
challenges and other challenge, um, once you have like a neural network, um, that's trained

30:56.200 --> 30:57.600
to like do your task.

30:57.600 --> 31:02.040
How do you give it to like other people so they can use it for their problems and like

31:02.040 --> 31:03.040
their data?

31:03.040 --> 31:04.040
Yeah.

31:04.040 --> 31:05.040
Yeah.

31:05.040 --> 31:08.320
And have you, have you started to tackle that one at all or one thing that I talked about

31:08.320 --> 31:14.000
at my talk, um, yesterday was, you know, sort of like what are, what are we working on

31:14.000 --> 31:18.280
on the deep learning space, um, one is, yeah.

31:18.280 --> 31:21.840
So one is just deployment and experiments, experiments that we do in our lab.

31:21.840 --> 31:25.000
That requires, you know, a little bit more training data because like the experience that

31:25.000 --> 31:31.280
we like to do or sort of like, like large scale imaging and then also deploying, um, these

31:31.280 --> 31:36.080
neural networks so you can do like segmentation in, in tissues, right?

31:36.080 --> 31:41.840
And so there's sort of like the, you know, well controls, you know, cell culture type experiments,

31:41.840 --> 31:46.200
but then there's a messier world of, you know, imaging and, you know, real life tissues

31:46.200 --> 31:49.720
that, you know, pathologists, um, pathologists do.

31:49.720 --> 31:54.560
And so these require different scales of training data and the things that you'd really like

31:54.560 --> 32:00.560
to do in these spaces are either like very complicated tissues or having three dimensional

32:00.560 --> 32:01.560
data sets.

32:01.560 --> 32:05.800
So being able to do two dimensions in space plus time or being able to do like three

32:05.800 --> 32:10.800
dimensional, um, spatial, um, three dimensions in space, say like you're using like a confocal

32:10.800 --> 32:15.480
microscopy, um, setup or, um, another like more sophisticated instrument.

32:15.480 --> 32:18.600
And so that requires a different scale of training data because instead of annotating

32:18.600 --> 32:22.520
a couple of frames to be able to look at the objects, you have to be able to like annotate

32:22.520 --> 32:24.240
like entire movies.

32:24.240 --> 32:28.760
And that's what I've been working with, um, people that figure it with, um, is, how do

32:28.760 --> 32:32.040
you train people on the crowd to like do these, um, sorts of tasks?

32:32.040 --> 32:34.560
Oh, I think we're just, I think they ended up talking.

32:34.560 --> 32:38.160
I think we showed, you know, sort of like some of like the stuff that's off the press.

32:38.160 --> 32:43.000
I think we're just now getting people, um, in the crowd able to annotate the 3D data

32:43.000 --> 32:49.200
sets, um, Robert Monroe show being able to, you know, do like object tracking, um, in

32:49.200 --> 32:50.200
the crowd.

32:50.200 --> 32:54.280
I'm super, super excited about that because I think that's one of the, you know, one of

32:54.280 --> 32:59.240
the big, um, challenges, um, in lifestyle imaging and, you know, to do that, what you need

32:59.240 --> 33:03.040
are, you know, nice, um, annotated, um, annotated data sets.

33:03.040 --> 33:08.240
And so we've been working on annotating on issues with like crowdsourcing, like, um,

33:08.240 --> 33:13.280
data set annotation, we're also, um, thinking about deployment as well.

33:13.280 --> 33:17.080
And so there are a lot of things to think about there.

33:17.080 --> 33:23.520
Um, so yeah, I would say there's a difference between what I like to call like academic

33:23.520 --> 33:30.720
software, which is, you know, someone has like a cool idea they put together a nice set

33:30.720 --> 33:37.800
of scripts, um, in Python or MATLAB or R. And for, well, for MATLAB and R, they're relatively

33:37.800 --> 33:42.840
like well-contained environments and you can give someone a dot M file, um, or an R file

33:42.840 --> 33:45.400
and it'll kind of work on another MATLAB installation.

33:45.400 --> 33:47.360
But Python's a different beast.

33:47.360 --> 33:51.040
Everybody's got like their own version, their own limitation and for all of the different

33:51.040 --> 33:55.120
stacks on the library, does a different libraries.

33:55.120 --> 33:57.040
And so that's a, that's a problem, right?

33:57.040 --> 34:01.920
Is that I can't give somebody like a link to my GitHub and say like, good luck.

34:01.920 --> 34:07.040
Um, you have to, there's a lot more, um, there's a lot more work that has to be done.

34:07.040 --> 34:11.600
So, you know, it's basically, you know, doing your due diligence, looking at like the best

34:11.600 --> 34:15.920
tools that are out there for solving these problems, um, we have switched to doing like

34:15.920 --> 34:18.960
our development inside of NVIDIA Docker now.

34:18.960 --> 34:22.800
And we're looking as, um, to use that for, you know, deployment.

34:22.800 --> 34:26.200
So like we have collaborators who use some of those scripts that we generate, um, and

34:26.200 --> 34:31.400
now instead of, you know, giving them like, hey, here's like the Python file, um, you

34:31.400 --> 34:35.680
know, hey, here's like a Docker container, um, that has like all of the packages, um,

34:35.680 --> 34:36.680
installed.

34:36.680 --> 34:42.280
And, you know, there's layers of work to, uh, that has to be done like on top of that.

34:42.280 --> 34:46.440
Ideally, you know, you'd have like your Docker container, um, up and running, you'd have

34:46.440 --> 34:48.280
stuff like wrap within like a web framework.

34:48.280 --> 34:52.120
You have, you know, that backend talking to like a front end with some semblance of like

34:52.120 --> 34:56.640
user interface, um, and now that I have like a team of people thinking of these sorts

34:56.640 --> 35:00.840
of issues, um, you know, we're thinking of, um, we're thinking of like just implementing

35:00.840 --> 35:04.920
on some of these things to like have it easier for people like within the lab, um, people

35:04.920 --> 35:10.320
within the labs that we collaborate with and also like the people who, um, you know, download

35:10.320 --> 35:13.440
our stuff and use it, um, make it easier for them to use too.

35:13.440 --> 35:14.440
Awesome.

35:14.440 --> 35:15.440
Awesome.

35:15.440 --> 35:16.440
Yeah.

35:16.440 --> 35:17.440
Uh, well, David, this has been really, really interesting.

35:17.440 --> 35:19.760
Thanks so much for taking the time to chat with us.

35:19.760 --> 35:21.960
Um, it's been absolute pleasure.

35:21.960 --> 35:23.040
Um, thank you very much.

35:23.040 --> 35:27.440
Uh, I should like, I mean, I would like to think like a few people, um, if I'm like a

35:27.440 --> 35:28.440
lot, go right ahead.

35:28.440 --> 35:29.440
Yeah.

35:29.440 --> 35:33.680
Uh, like I think the graduate students, um, who have, um, been worth rotating with me,

35:33.680 --> 35:35.880
like the last, um, this quarter.

35:35.880 --> 35:41.360
So Dylan Bannon and Nadia Vilevich, uh, I've got to thank, um, people I've worked with

35:41.360 --> 35:42.360
at Stanford.

35:42.360 --> 35:45.160
And so I had the privilege of working with Nicholas Quatt.

35:45.160 --> 35:48.320
She's a very talented undergrad who did a lot of the experiments I talked about yesterday.

35:48.320 --> 35:52.160
Um, my postdoctoral advisor, um, Marcus covert, um, he was great.

35:52.160 --> 35:55.560
And it was his lab that, um, sort of gave me like the intellectual freedom to start exploring

35:55.560 --> 36:00.760
like these, um, sets of issues, um, you know, looking at like AI and the interface with

36:00.760 --> 36:07.160
uh, the biological sciences, um, I had a great, great ongoing collaboration with, uh, Michael

36:07.160 --> 36:09.600
Angelo, who's a professor at the college department at Stanford.

36:09.600 --> 36:15.040
Um, he's done amazing, um, amazing stuff, um, on digital pathology and developing like

36:15.040 --> 36:17.640
the next platform of digital pathology instruments.

36:17.640 --> 36:21.480
I will let you look at literally, you know, dozens of different biological markers within

36:21.480 --> 36:22.480
tissues.

36:22.480 --> 36:27.480
And he's had an amazing postdoctoral fellow, um, Leot Karen, um, who I worked with as

36:27.480 --> 36:31.080
well, uh, unfortunately, you didn't get to stick around for the talk yesterday, um, but

36:31.080 --> 36:33.400
I talked about some of the work I was doing with them.

36:33.400 --> 36:39.920
And also like to think, um, Casey Wong and long, hi, um, I do, I do work with their labs,

36:39.920 --> 36:40.920
um, as well.

36:40.920 --> 36:41.920
Awesome.

36:41.920 --> 36:42.920
So yeah.

36:42.920 --> 36:43.920
Awesome.

36:43.920 --> 36:44.920
And of course, the folks that figure eight for the support of your work and having us

36:44.920 --> 36:52.120
here at the podcast, folks that figure eight, um, Andy, Justin, Robert, um, everybody's

36:52.120 --> 36:54.800
been like, everybody's been amazing.

36:54.800 --> 37:02.560
Their openness has, I think is, I was like, it's a model that people in academia should

37:02.560 --> 37:07.840
pay attention to as far as like collaborations between like academia and industry, like the

37:07.840 --> 37:11.040
things that figure eight's allowing my lab to do.

37:11.040 --> 37:14.320
And the resources they've given us, like the work that, you know, we're doing and we're

37:14.320 --> 37:17.280
going to do would not be possible like without them.

37:17.280 --> 37:18.280
Fantastic.

37:18.280 --> 37:19.280
Yeah.

37:19.280 --> 37:25.360
Other funders, uh, NH, or welcome fund, uh, Paul Allen Family Foundation, um, they funded,

37:25.360 --> 37:30.760
um, a large, um, large amount of work and also the division of biology and engineering

37:30.760 --> 37:34.720
at Caltech for posting my laboratory and also, um, funding us as well.

37:34.720 --> 37:35.720
Awesome.

37:35.720 --> 37:36.720
Well, thanks so much, David.

37:36.720 --> 37:37.720
Thank you.

37:37.720 --> 37:40.960
All right, everyone.

37:40.960 --> 37:49.240
That's our show for today for more information on David or any of the topics covered

37:49.240 --> 37:56.320
in this episode, head on over to twimlai.com slash talk slash one forty one.

37:56.320 --> 37:59.520
Thanks again to figure eight for their sponsorship of this episode.

37:59.520 --> 38:06.920
To follow along with the train AI series, visit twimlai.com slash train AI 2018.

38:06.920 --> 38:12.720
And last, but not least, go ahead and show us some love for our second anniversary and

38:12.720 --> 38:19.240
share how the podcast has been helpful to you over at twimlai.com slash two A V, the

38:19.240 --> 38:21.440
number two A V.

38:21.440 --> 38:51.400
Thanks so much for listening and catch you next time.

