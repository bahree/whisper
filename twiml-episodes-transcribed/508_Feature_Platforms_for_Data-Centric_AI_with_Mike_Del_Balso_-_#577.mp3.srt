1
00:00:00,000 --> 00:00:10,120
Alright everyone, welcome to another episode of the Twimmel AI Podcast.

2
00:00:10,120 --> 00:00:16,200
I am of course your host Sam Charrington, and today I'm joined by Mike DelBalso, co-founder

3
00:00:16,200 --> 00:00:19,040
and CEO of Techton.

4
00:00:19,040 --> 00:00:22,880
Before we get going, be sure to take a moment to hit that subscribe button wherever you're

5
00:00:22,880 --> 00:00:24,680
listening to today's show.

6
00:00:24,680 --> 00:00:26,680
Mike, welcome back to the podcast.

7
00:00:26,680 --> 00:00:27,680
Thank you for having me.

8
00:00:27,680 --> 00:00:28,680
It's great to be here.

9
00:00:28,680 --> 00:00:29,680
I'm excited for the conversation today.

10
00:00:29,680 --> 00:00:30,680
I am as well.

11
00:00:30,680 --> 00:00:33,080
It's been a while since we spoke.

12
00:00:33,080 --> 00:00:36,080
I think the most recent time was about a year and a half ago.

13
00:00:36,080 --> 00:00:38,880
We were talking about feature stores for MLOPS.

14
00:00:38,880 --> 00:00:42,320
I suspect that topic will come up again.

15
00:00:42,320 --> 00:00:44,080
But let's dig in.

16
00:00:44,080 --> 00:00:49,520
I'm looking forward to talking about what you've been up to, the idea of feature platforms

17
00:00:49,520 --> 00:00:54,880
and of course the intersection with all that and data-centric AI.

18
00:00:54,880 --> 00:01:00,080
For those who haven't heard any of our interviews or don't know your background, why don't you

19
00:01:00,080 --> 00:01:02,520
share a little bit about how you came to work in ML?

20
00:01:02,520 --> 00:01:05,280
I first got involved in machine learning.

21
00:01:05,280 --> 00:01:09,960
I was a product manager at Google and dating myself now.

22
00:01:09,960 --> 00:01:17,000
But back in 2013, I joined the ads team at Google and as you know, Google uses a lot

23
00:01:17,000 --> 00:01:20,920
of machine learning to determine which ads to show people.

24
00:01:20,920 --> 00:01:23,520
This is before machine learning was super cool and stuff like that.

25
00:01:23,520 --> 00:01:27,480
I don't even think we use the word machine learning at the time, but we had a lot of models

26
00:01:27,480 --> 00:01:35,320
in production that were doing some real time decision making around who is this person,

27
00:01:35,320 --> 00:01:40,240
what kind of interests do they have, which had his most relevant to show to them.

28
00:01:40,240 --> 00:01:45,240
The team was really excellent at what we did not at a term we didn't have at the time,

29
00:01:45,240 --> 00:01:46,240
which was MLOPS.

30
00:01:46,240 --> 00:01:51,600
There's a lot of MLOPS that we were doing at that time, production machine learning.

31
00:01:51,600 --> 00:01:57,800
That was kind of like phase one of how I got involved in machine learning.

32
00:01:57,800 --> 00:02:05,760
After that, I actually joined Uber in the early data ML days of Uber at the time when there

33
00:02:05,760 --> 00:02:12,280
wasn't a lot of, there was very little machine learning happening in production at Uber.

34
00:02:12,280 --> 00:02:19,280
It was kind of the task for me to start the ML team and help us figure out what to do

35
00:02:19,280 --> 00:02:21,800
with all of this data, we had all this data at Uber.

36
00:02:21,800 --> 00:02:28,080
How do we help Uber make a lot of excellent automated and smart decisions and experiences

37
00:02:28,080 --> 00:02:30,760
in the product?

38
00:02:30,760 --> 00:02:37,000
We built the ML platform team at Uber and we made a platform called Michelangelo, which

39
00:02:37,000 --> 00:02:41,600
was really exciting and we can talk about that, of course.

40
00:02:41,600 --> 00:02:46,600
In the process of doing that, we developed a lot of cool patterns, really spent a lot

41
00:02:46,600 --> 00:02:52,680
of time developing a lot of MLOPS workflows and different pieces of infrastructure, one

42
00:02:52,680 --> 00:03:00,000
of which is the feature store and that is a really big inspiration and a really big

43
00:03:00,000 --> 00:03:04,760
kind of like theme of tech content that I'm working on today.

44
00:03:04,760 --> 00:03:10,240
You know, when I think back to Michelangelo and that blog post that got so many of us

45
00:03:10,240 --> 00:03:18,280
excited about what you were doing there at Uber and the state of play at the time, like

46
00:03:18,280 --> 00:03:24,120
compared to now, you know, we've built out a lot of the stack for MLOPS now or at least

47
00:03:24,120 --> 00:03:27,520
we've got contenders for various pieces.

48
00:03:27,520 --> 00:03:33,080
But when you left Uber to start tech time, the field was pretty wide open.

49
00:03:33,080 --> 00:03:34,560
Why'd you start with a feature store?

50
00:03:34,560 --> 00:03:36,560
I mean, you had all the pieces, right?

51
00:03:36,560 --> 00:03:43,360
That's a really good question. And I think we all have to think back to that time and think

52
00:03:43,360 --> 00:03:48,400
about a very different time where, you know, if you look today at the like, what are all

53
00:03:48,400 --> 00:03:53,520
the MLOPS tools in the industry, the map of the industry, there's 4,000 MLOPS companies

54
00:03:53,520 --> 00:03:58,360
and like, what plugs into what and how do I choose what it, you know, it's crazy.

55
00:03:58,360 --> 00:04:02,280
At that time, it felt like there was a lot of stuff, but there actually wasn't a lot

56
00:04:02,280 --> 00:04:04,120
of stuff relative to today, right?

57
00:04:04,120 --> 00:04:07,880
But it always feels complicated and, you know, a lot of the terms, there wasn't the same

58
00:04:07,880 --> 00:04:10,120
level of specialization and stuff like that.

59
00:04:10,120 --> 00:04:14,520
So back in the time, when we started Michelangelo, we started it in 2015.

60
00:04:14,520 --> 00:04:20,640
So, you know, it's a very different kind of generation of ML tooling and ML techniques

61
00:04:20,640 --> 00:04:22,800
and stuff like that.

62
00:04:22,800 --> 00:04:26,840
Back at that time, the thinking was an end to end ML platform.

63
00:04:26,840 --> 00:04:31,800
Let's just build a solid experience for, but it was all about democratizing ML.

64
00:04:31,800 --> 00:04:35,120
How do we make it possible for a data scientist to do machine learning?

65
00:04:35,120 --> 00:04:39,360
You know, and it was kind of like enabling them to do it for the first time.

66
00:04:39,360 --> 00:04:45,640
And so we built an end-to-end platform that enabled them to, you know, choose their label,

67
00:04:45,640 --> 00:04:53,560
choose their features in the Web UI and get their model built in a Web UI and, you know,

68
00:04:53,560 --> 00:04:56,280
manage it in a Web UI all the way to deploy into production.

69
00:04:56,280 --> 00:04:57,720
It was revolutionary at the time.

70
00:04:57,720 --> 00:05:03,640
It was something that really enabled people to go from zero to one.

71
00:05:03,640 --> 00:05:07,880
And it handled the full workflow.

72
00:05:07,880 --> 00:05:17,840
The industry was moving from this general idea, the dream of having the one size fits

73
00:05:17,840 --> 00:05:22,520
all platform that will enable all of the people within the organization really democratize

74
00:05:22,520 --> 00:05:26,880
everything to the kind of point where people are disillusioned with that dream and they

75
00:05:26,880 --> 00:05:32,240
really realize, hey, you know what we need is, you know, a bunch of reusable components

76
00:05:32,240 --> 00:05:35,720
because there's not a one size fits all one system.

77
00:05:35,720 --> 00:05:40,920
There's a bunch of reusable components that we can piece together to form the ML application

78
00:05:40,920 --> 00:05:43,720
that I'm trying to build, you know, so it would have the right kind of serving system

79
00:05:43,720 --> 00:05:48,360
for me and it will, and I can plug into the training system that's relevant for me.

80
00:05:48,360 --> 00:05:54,000
And so we actually did that that we brought Michelangelo through that journey as well

81
00:05:54,000 --> 00:05:59,520
from being a monolithic system to more of like a collection of best best practice components

82
00:05:59,520 --> 00:06:03,480
that are compatible and fit together quite well.

83
00:06:03,480 --> 00:06:09,800
And so in that going through that journey and building Michelangelo in the first place,

84
00:06:09,800 --> 00:06:15,480
you know, we were really focused on how do we help people get, not a model built, not

85
00:06:15,480 --> 00:06:19,880
like showing some results in AUC curve to their team, but how do we, you know, help the

86
00:06:19,880 --> 00:06:25,200
fraud team, help the ETA team at Uber, all these different teams, help them actually get

87
00:06:25,200 --> 00:06:28,680
into production and get value, you know, they were use case teams, they were trying to

88
00:06:28,680 --> 00:06:29,680
get something done.

89
00:06:29,680 --> 00:06:33,320
They didn't really care about like, cool, I've trained some models and they, they're

90
00:06:33,320 --> 00:06:35,600
not plugged into the product.

91
00:06:35,600 --> 00:06:38,920
And so we thought about it as they're building an ML application.

92
00:06:38,920 --> 00:06:45,040
And so this is super related to the concept of data centric AI, you know, what's part

93
00:06:45,040 --> 00:06:46,040
of an ML application?

94
00:06:46,040 --> 00:06:49,760
Well, of course you have, you got to build your models and manage your models.

95
00:06:49,760 --> 00:06:57,200
And then secondly, there's a variety of data pipelines that are also part of that ML application,

96
00:06:57,200 --> 00:07:01,600
the things that generate the data that your model consumes, your training system consumes

97
00:07:01,600 --> 00:07:08,400
to generate a model, or the data pipelines that your model in production uses to generate

98
00:07:08,400 --> 00:07:12,360
inference in real time, all kinds of data pipelines happening there.

99
00:07:12,360 --> 00:07:16,080
And so we realized, hey, in the Michelangelo system, we really built a lot of model management

100
00:07:16,080 --> 00:07:20,680
stuff, but what we spent most of our time building is a bunch of data management stuff.

101
00:07:20,680 --> 00:07:22,880
And that's actually what we called the feature store.

102
00:07:22,880 --> 00:07:27,560
And so it was a lot of kind of centralized and automated data engineering that we found

103
00:07:27,560 --> 00:07:30,520
that we were doing again and again across all these different use cases.

104
00:07:30,520 --> 00:07:34,440
And we brought that together into this one layer in the Michelangelo system and became

105
00:07:34,440 --> 00:07:36,320
our kind of its own component.

106
00:07:36,320 --> 00:07:38,200
We called it the feature store.

107
00:07:38,200 --> 00:07:42,320
We found that it was one of the most impactful systems to help someone go from zero to one

108
00:07:42,320 --> 00:07:48,480
to get into production quickly, and to be able to reuse machine learning across these

109
00:07:48,480 --> 00:07:53,660
components across different use cases to share and to help reduce the incremental cost

110
00:07:53,660 --> 00:07:55,960
of creating the next machine learning model.

111
00:07:55,960 --> 00:08:01,960
And so reflecting on that at the time we thought we glimpsed to the future.

112
00:08:01,960 --> 00:08:07,360
We realized it's really about the data, the feature store, the concept of the feature store,

113
00:08:07,360 --> 00:08:13,400
and later the feature platform is really the kind of the data layer for machine learning.

114
00:08:13,400 --> 00:08:17,880
And that's what got us so excited after publishing a blog post and everybody had like all kinds

115
00:08:17,880 --> 00:08:20,360
of extremely positive feedback around that.

116
00:08:20,360 --> 00:08:24,760
And I'm trying to build something like that, you know, can you guys come work for us kind

117
00:08:24,760 --> 00:08:25,760
of thing?

118
00:08:25,760 --> 00:08:27,160
There's a lot of attention there.

119
00:08:27,160 --> 00:08:31,440
And that made it clear to us that this is a design that's here to stay.

120
00:08:31,440 --> 00:08:39,920
And this is a design where this is a pattern that there should be a proper enterprise solution

121
00:08:39,920 --> 00:08:40,920
built around.

122
00:08:40,920 --> 00:08:48,320
And so that motivated us to create tech on, tech on is an enterprise feature platform.

123
00:08:48,320 --> 00:08:54,400
And you know, we help teams who are putting machine learning and production manage their

124
00:08:54,400 --> 00:08:59,360
data flows for their ML application through all stages of the ML lifecycle.

125
00:08:59,360 --> 00:09:09,600
You referenced the complexity of data infrastructure when you were building Michael Angelo.

126
00:09:09,600 --> 00:09:13,040
Speak a little bit to how you've seen that evolve over the past five years.

127
00:09:13,040 --> 00:09:15,520
Has it been relatively static?

128
00:09:15,520 --> 00:09:19,160
Has it changed significantly a little above?

129
00:09:19,160 --> 00:09:26,280
I think we have seen some things change quite a bit.

130
00:09:26,280 --> 00:09:30,040
Some things change a lot less than I would have liked.

131
00:09:30,040 --> 00:09:37,640
I think one of the biggest changes in our journey and the journey for many companies is

132
00:09:37,640 --> 00:09:43,960
frankly going from having a lot of data on-prem in a Hadoop cluster, you know, five years

133
00:09:43,960 --> 00:09:53,800
ago to now I'm using a cloud data platform and all my data is on a hyperscaler cloud provider.

134
00:09:53,800 --> 00:10:00,960
So what does that mean for a data team and an ML team?

135
00:10:00,960 --> 00:10:02,440
What does it mean for a data team?

136
00:10:02,440 --> 00:10:05,480
It means you're not managing a bunch of Hadoop clusters.

137
00:10:05,480 --> 00:10:09,560
You don't need a 30 person data infrastructure team to manage basic.

138
00:10:09,560 --> 00:10:12,600
How do I store my data and keep it up and running?

139
00:10:12,600 --> 00:10:16,480
Like for example, when I joined Uber, there was just outages like the data wasn't available.

140
00:10:16,480 --> 00:10:20,800
Like the whole Uber app would go down all the time because there was just issues with

141
00:10:20,800 --> 00:10:23,240
maintaining all of the data systems.

142
00:10:23,240 --> 00:10:29,400
Now a lot of those basic data capabilities are largely resolved.

143
00:10:29,400 --> 00:10:34,280
You know, you put your data on the cloud, you adopt a data lake house or a data warehouse,

144
00:10:34,280 --> 00:10:35,280
put your data in them.

145
00:10:35,280 --> 00:10:39,000
For example, snowflake or Databricks, they're great solutions and you're going to get

146
00:10:39,000 --> 00:10:45,360
really good performance, great reliability and it's going to be likely at a price point

147
00:10:45,360 --> 00:10:46,360
that works for your team.

148
00:10:46,360 --> 00:10:50,880
It's going to be cheaper than having maintaining your own team to build all of this stuff.

149
00:10:50,880 --> 00:10:56,320
And then it's interesting to reflect on what does that mean for a machine learning team?

150
00:10:56,320 --> 00:11:03,400
Because machine learning teams, previously in that previous world, were building custom

151
00:11:03,400 --> 00:11:08,760
connectors to the custom data systems that existed in their business is, how do I connect

152
00:11:08,760 --> 00:11:14,360
to our weird file format and the lack of reliability?

153
00:11:14,360 --> 00:11:22,200
Yeah, I mean, there's all kinds of, the ML teams would be constrained by the data choices

154
00:11:22,200 --> 00:11:26,240
of the underlying data platform team that they dependent on.

155
00:11:26,240 --> 00:11:30,560
And that would come with a lot of constraints at Uber, we were super fortunate because

156
00:11:30,560 --> 00:11:36,200
we had a super legit data platform team that did a lot of stuff.

157
00:11:36,200 --> 00:11:42,120
We had real time data, really high quality streaming data and an excellent kind of like

158
00:11:42,120 --> 00:11:45,400
a batch data system of gigantic Hadoop cluster.

159
00:11:45,400 --> 00:11:50,560
But that's not something that everyone was so fortunate to have.

160
00:11:50,560 --> 00:11:54,880
And moving to the cloud, you kind of get all of this stuff now very easily accessible,

161
00:11:54,880 --> 00:11:56,560
often serverless.

162
00:11:56,560 --> 00:12:01,680
And it makes for, and then I think that has a really big impact on the machine learning

163
00:12:01,680 --> 00:12:08,440
space or any consumer of data of underlying data systems because now they're standardization.

164
00:12:08,440 --> 00:12:14,040
I'm a machine learning team in company A, I build a connector to Snowflake and some

165
00:12:14,040 --> 00:12:17,080
cool thing that plugs into Snowflake and I can build models with it.

166
00:12:17,080 --> 00:12:21,840
Well I can share it with company B and now they can use my machine learning tool that

167
00:12:21,840 --> 00:12:27,520
was built for that underlying data system because we share the same underlying data system.

168
00:12:27,520 --> 00:12:31,560
And so there's a lot of commonality at this foundation layer that allows people to

169
00:12:31,560 --> 00:12:36,920
share at the ML layer, share things that they've built and have them be reusable across

170
00:12:36,920 --> 00:12:37,920
teams.

171
00:12:37,920 --> 00:12:39,840
They increase the pace of innovation there.

172
00:12:39,840 --> 00:12:48,680
And thinking about data lake house data warehouse, those are more about static data so to speak

173
00:12:48,680 --> 00:12:55,640
relative to streaming as streaming matured as much or to the same degree.

174
00:12:55,640 --> 00:13:02,720
Streaming has matured a bunch, but it is, there's still a long way to go, honestly, and we

175
00:13:02,720 --> 00:13:07,040
find that a lot of our customers are still struggling with it.

176
00:13:07,040 --> 00:13:14,560
And frankly part of the value proposition of the feature platform that we build that

177
00:13:14,560 --> 00:13:19,200
tech ton is, it makes a lot of this stuff just a lot easier.

178
00:13:19,200 --> 00:13:25,400
So how streaming used to be, you needed to have a whole team spin up a Kafka deployment

179
00:13:25,400 --> 00:13:28,280
for you and maintain a Kafka cluster.

180
00:13:28,280 --> 00:13:33,680
Now there's really good cloud solutions, there's confluent has confluent cloud.

181
00:13:33,680 --> 00:13:36,960
So it's much simpler with confluent cloud nowadays.

182
00:13:36,960 --> 00:13:44,120
There's a variety of other online streaming solutions that are, that make things much simpler.

183
00:13:44,120 --> 00:13:47,720
But we're finding that ML teams are still struggling to use them.

184
00:13:47,720 --> 00:13:52,760
So for example, a machine learning team is predicting fraud at a big bank.

185
00:13:52,760 --> 00:14:03,320
They want to find a way to say, hey, if this user has sent more than 100 transactions

186
00:14:03,320 --> 00:14:07,920
in the past five minutes, let's ban, let's not allow any other transactions, because it's

187
00:14:07,920 --> 00:14:09,880
likely that it's going to be fraud, right?

188
00:14:09,880 --> 00:14:14,320
Well, what do they need to do to get there, and they want to do this at scale for a lot

189
00:14:14,320 --> 00:14:19,080
of different users, they need to run a lot of this streaming infrastructure, and they need

190
00:14:19,080 --> 00:14:25,360
to be running stream processing to aggregate over all of those transaction events, turn

191
00:14:25,360 --> 00:14:30,640
them into feature aggregations, and then plug that into their model.

192
00:14:30,640 --> 00:14:37,560
And then, not just set this up once, but staff up a team to ensure that these systems don't

193
00:14:37,560 --> 00:14:41,080
go down, they don't go out of memory, if it goes down, that they're retried, that they're

194
00:14:41,080 --> 00:14:47,560
available, and high availability, and debuggable, and someone's on call for it.

195
00:14:47,560 --> 00:14:50,280
That's the productionization side of streaming.

196
00:14:50,280 --> 00:14:56,960
That's still pretty tough, and we find that a lot of teams that are trying to build interactive

197
00:14:56,960 --> 00:15:04,120
machine learning driven experiences, you want your recommendations on your website to

198
00:15:04,120 --> 00:15:06,840
react to what the customer was just clicking on.

199
00:15:06,840 --> 00:15:12,760
You want your fraud detection system to take into account like what the person's actions

200
00:15:12,760 --> 00:15:15,640
were, just were on this account.

201
00:15:15,640 --> 00:15:20,360
That kind of stuff productionizing it relies very frequently on streaming, it's pretty

202
00:15:20,360 --> 00:15:25,880
hard to do today, and it's one of the things that actually we've built a lot of IP and

203
00:15:25,880 --> 00:15:28,400
making a lot simpler for ML teams.

204
00:15:28,400 --> 00:15:34,480
All these real-time ML capabilities are an area where there's a lot of innovation happening

205
00:15:34,480 --> 00:15:35,480
right now.

206
00:15:35,480 --> 00:15:43,720
And so I think it is clear, based on that, that what you're seeing with regards to feature

207
00:15:43,720 --> 00:15:49,920
platforms isn't about replacing all of this infrastructure that we've only recently

208
00:15:49,920 --> 00:15:52,560
standardized, but rather it hooks into them.

209
00:15:52,560 --> 00:15:58,160
Well, let's just talk about what a feature platform is, and then we'll talk about how

210
00:15:58,160 --> 00:16:04,200
it can connect to your systems, and what does it do, is it replace your existing systems

211
00:16:04,200 --> 00:16:08,360
or not?

212
00:16:08,360 --> 00:16:12,560
When we work with teams who are trying to put machine learning into production, what

213
00:16:12,560 --> 00:16:17,000
they have to do is not just build a model and put a model into production, but they need

214
00:16:17,000 --> 00:16:24,760
to stand up, they need to develop, productionize, and operate data flows, feature pipelines

215
00:16:24,760 --> 00:16:32,280
that are constantly computing the up-to-date and very fresh feature signals that are going

216
00:16:32,280 --> 00:16:36,080
to be available, are going to be used for real-time inference.

217
00:16:36,080 --> 00:16:43,640
So we just gave the example of real-time fraud where I'm doing some aggregations over

218
00:16:43,640 --> 00:16:46,280
some recent web events, let's say.

219
00:16:46,280 --> 00:16:50,840
There may be a search use case, and so a very different type of data path, the data might

220
00:16:50,840 --> 00:16:56,080
come from a search box that someone typed a query into.

221
00:16:56,080 --> 00:17:00,760
That's more like a real-time signal that comes in from the end user application.

222
00:17:00,760 --> 00:17:03,320
And some signals, they're just pre-computed.

223
00:17:03,320 --> 00:17:05,320
Is this person in a banned country?

224
00:17:05,320 --> 00:17:10,880
Well, we can pre-comput that, and we can just make that ready to serve right when we

225
00:17:10,880 --> 00:17:12,560
want to make that prediction.

226
00:17:12,560 --> 00:17:16,480
There's a lot of data engineering that goes into each type of these features.

227
00:17:16,480 --> 00:17:26,360
People trip up with a lot of different types of data challenges that comes up with them.

228
00:17:26,360 --> 00:17:34,600
For example, consistency between that data at inference time compared to consistency

229
00:17:34,600 --> 00:17:38,440
with that data at training times, you want that data to be the same.

230
00:17:38,440 --> 00:17:42,400
When you generate a training data set, you need to go back in history for all of that

231
00:17:42,400 --> 00:17:43,400
data.

232
00:17:43,400 --> 00:17:47,880
You need to have all of that data log so you can generate historical training examples.

233
00:17:47,880 --> 00:17:52,480
You need to monitor this data to make sure it is serving at the right, operationally

234
00:17:52,480 --> 00:17:58,840
serving at the right speed, it's staying real-time, it's fresh, it's available.

235
00:17:58,840 --> 00:18:03,760
But also, you want to make sure that it's accurate, the data quality is high.

236
00:18:03,760 --> 00:18:10,280
And then there's a variety of data infrastructure that you end up having to spin up and support

237
00:18:10,280 --> 00:18:17,240
in production, for example, systems to serve this data at scale, systems to operate, to

238
00:18:17,240 --> 00:18:20,200
do the stream processing to generate your features.

239
00:18:20,200 --> 00:18:25,600
We think about it as transforming your raw data into features, storing that data both

240
00:18:25,600 --> 00:18:32,320
for training and for inference, serving that data for inference in real-time, monitoring

241
00:18:32,320 --> 00:18:38,880
the data, and building an excellent developer workflow for MLOps and for the engineers

242
00:18:38,880 --> 00:18:41,320
to fit it into their DevOps processes.

243
00:18:41,320 --> 00:18:44,840
These are all within the scope of the feature platform.

244
00:18:44,840 --> 00:18:50,520
And the reason why there's so many things here is because there's a lot of challenges

245
00:18:50,520 --> 00:18:56,920
that you need to solve before you can actually credibly claim that your model is running

246
00:18:56,920 --> 00:19:05,440
in production in a reliable enough way that you're comfortable depending your revenue

247
00:19:05,440 --> 00:19:09,600
on it, you want to actually depend your product on it, type of thing.

248
00:19:09,600 --> 00:19:15,320
And so we've built a platform to make all of those data challenges much simpler.

249
00:19:15,320 --> 00:19:17,800
And so how do we do this, though?

250
00:19:17,800 --> 00:19:28,800
Well, we are not delivering a completely new data and ML stack to the ML user, to our customer.

251
00:19:28,800 --> 00:19:34,080
We are plugging into the data sets, the data infrastructure that they have.

252
00:19:34,080 --> 00:19:41,280
So people come to us and they say, hey, I have Snowflake and I have Databricks and I have

253
00:19:41,280 --> 00:19:42,520
a Redis cluster.

254
00:19:42,520 --> 00:19:44,600
And we'll say, that's great.

255
00:19:44,600 --> 00:19:48,400
We will actually orchestrate those systems, so we'll orchestrate transformations on both

256
00:19:48,400 --> 00:19:49,400
of those.

257
00:19:49,400 --> 00:19:54,040
We'll plug into those, take data out of them, we'll load it up into Redis, we'll maintain

258
00:19:54,040 --> 00:19:57,760
an online serving layer for your model.

259
00:19:57,760 --> 00:20:05,440
So the interfaces here are, we plug into raw data and we serve that data to the model

260
00:20:05,440 --> 00:20:09,080
that's in production or the training system that's building the model.

261
00:20:09,080 --> 00:20:15,680
But under the hood, we're not implementing all of those different data processes from

262
00:20:15,680 --> 00:20:19,800
scratch and running all of that infrastructure in our domain.

263
00:20:19,800 --> 00:20:28,040
We're actually orchestrating the best in class cloud data infrastructure that your team

264
00:20:28,040 --> 00:20:30,040
is already running within your own stack.

265
00:20:30,040 --> 00:20:35,160
So we're a lot more of an orchestration layer for like a data orchestration layer for

266
00:20:35,160 --> 00:20:37,640
machine learning than anything else.

267
00:20:37,640 --> 00:20:46,920
To relate to us a bit back to this idea of data-centric AI, you mentioned before we started

268
00:20:46,920 --> 00:20:53,920
recording the interview as we were chatting this idea about a ML flywheel that kind of reiterated

269
00:20:53,920 --> 00:20:58,120
the importance of data for ML.

270
00:20:58,120 --> 00:21:01,120
Can you talk a little bit about that idea?

271
00:21:01,120 --> 00:21:02,520
For sure.

272
00:21:02,520 --> 00:21:09,640
So one thing that I think is really nice about data-centric ML is it's a theme that comes

273
00:21:09,640 --> 00:21:16,640
along with it is declarative interfaces and maybe some of the other folks that you've

274
00:21:16,640 --> 00:21:21,320
been speaking to in recent episodes have been talking about this as well.

275
00:21:21,320 --> 00:21:24,880
But this is one of the key things that we have really focused on.

276
00:21:24,880 --> 00:21:31,520
We don't have our users say, hey, plug this infrastructure into this infrastructure, run

277
00:21:31,520 --> 00:21:36,160
this operation, retry it if it doesn't work, and then plug it in here and then run it

278
00:21:36,160 --> 00:21:37,320
at this frequency.

279
00:21:37,320 --> 00:21:43,400
We just have the person say, this is my feature, this is the feature transformation, you take

280
00:21:43,400 --> 00:21:44,400
care of everything.

281
00:21:44,400 --> 00:21:51,520
To be in production and the tech-ton system handles a lot of things behind the scenes to make

282
00:21:51,520 --> 00:21:52,520
that happen.

283
00:21:52,520 --> 00:21:55,640
Earlier on in the conversation, I mentioned like at Michelangelo, one of the things that

284
00:21:55,640 --> 00:22:00,880
made us really successful was we were focused on the end-to-end machine learning application.

285
00:22:00,880 --> 00:22:06,200
It wasn't, we weren't just focused on helping someone train a model or manage models.

286
00:22:06,200 --> 00:22:10,400
It was, hey, what's the whole series of workflows that you have to go through to actually

287
00:22:10,400 --> 00:22:18,320
have an application that's both reliable and accurate and making repeated predictions.

288
00:22:18,320 --> 00:22:22,240
It's like a live production application that you're operating.

289
00:22:22,240 --> 00:22:29,760
There's a couple of steps to that that we see that the best teams are doing when they're

290
00:22:29,760 --> 00:22:33,320
building their production ML applications.

291
00:22:33,320 --> 00:22:35,480
We can walk through these steps, right?

292
00:22:35,480 --> 00:22:37,600
First, you got to build the training data set.

293
00:22:37,600 --> 00:22:40,560
What is that training data set come from?

294
00:22:40,560 --> 00:22:45,960
It comes from your company's underlying data sets that are model data sets that are

295
00:22:45,960 --> 00:22:49,720
shared across the business, wherever your company's data is.

296
00:22:49,720 --> 00:22:55,960
That data is typically a bunch of logs of user events that come from your product.

297
00:22:55,960 --> 00:22:59,920
There's just this data loop that we found our customer is building, right?

298
00:22:59,920 --> 00:23:05,600
You make some prediction and so maybe you're predicting like, is someone going to click

299
00:23:05,600 --> 00:23:08,120
on this product or not?

300
00:23:08,120 --> 00:23:12,120
Then based on if they click on that product or not, you log that data.

301
00:23:12,120 --> 00:23:16,160
That's going to become a future ground truth, a future label.

302
00:23:16,160 --> 00:23:22,920
You may join that data together with other labels that you've logged.

303
00:23:22,920 --> 00:23:29,280
You have a, like, now they're joined label data set and maybe you log some features.

304
00:23:29,280 --> 00:23:34,120
You have a feature data set as well, feature logs.

305
00:23:34,120 --> 00:23:38,880
Then you may assemble these data sets into a training data set and so now you want to

306
00:23:38,880 --> 00:23:41,000
take all that data and you want to build a model.

307
00:23:41,000 --> 00:23:44,720
Now you build a model and then you want to use that model.

308
00:23:44,720 --> 00:23:50,000
There's a variety of other data sets that you're building on that time and real-time inference

309
00:23:50,000 --> 00:23:55,520
type of workflows where you're maybe generating some candidates and then generating a feature,

310
00:23:55,520 --> 00:24:00,400
some features for each of those candidates and then you want to score those candidates

311
00:24:00,400 --> 00:24:04,080
and you ultimately are making a prediction and you're delivering that back to the

312
00:24:04,080 --> 00:24:08,160
product, to the user, to the customer.

313
00:24:08,160 --> 00:24:14,000
This is kind of data loop of, like, collect some data, organize it, learn from it and decide

314
00:24:14,000 --> 00:24:18,240
and then based on, you know, make a prediction and then based on that, collecting again,

315
00:24:18,240 --> 00:24:23,000
organizing, you know, and we want to, we want, this data goes around in this loop and

316
00:24:23,000 --> 00:24:30,160
what we've seen is that the best ML teams, the teams across the industry that are the

317
00:24:30,160 --> 00:24:36,440
most successful at machine learning, are really, really good at managing that loop and

318
00:24:36,440 --> 00:24:37,520
building that loop.

319
00:24:37,520 --> 00:24:42,720
They're really, really good at getting that flywheel to really be running.

320
00:24:42,720 --> 00:24:46,600
The more, every iteration of that flywheel, you're collecting more data, you're training

321
00:24:46,600 --> 00:24:49,960
your model, your model's getting better, you're making better predictions.

322
00:24:49,960 --> 00:24:56,360
And so there's a variety of things that help people become, there's a variety of, like,

323
00:24:56,360 --> 00:25:01,800
good things about that come out of being excellent at that flywheel.

324
00:25:01,800 --> 00:25:06,800
You have really clear ownership of who, you know, runs each part of that flywheel and

325
00:25:06,800 --> 00:25:12,600
it's easier to make changes, everything's more debuggable and more easily monitored and

326
00:25:12,600 --> 00:25:18,080
just small changes feel like small changes and ML just feels natural and easy.

327
00:25:18,080 --> 00:25:21,360
If you're ML flywheel, you don't really have it working well, maybe you don't know,

328
00:25:21,360 --> 00:25:28,320
hey, who's actually the person that logs data from the application into the data warehouse?

329
00:25:28,320 --> 00:25:31,680
Because now I want to add a new feature to my model, but I've got to go find that person

330
00:25:31,680 --> 00:25:34,040
so they can log some data for me.

331
00:25:34,040 --> 00:25:39,280
In that world, every small change you want to make, it ends up becoming like a big thing.

332
00:25:39,280 --> 00:25:43,880
Like, you don't know who runs this system and then it's broken, you've got to go debug

333
00:25:43,880 --> 00:25:46,920
it with and you've got to go find the person kind of thing.

334
00:25:46,920 --> 00:25:53,400
And so ML becomes really hard, you know, models don't update as often, you tend to see

335
00:25:53,400 --> 00:25:57,640
those teams, a symptom of those teams as they're still stuck in version one as the model

336
00:25:57,640 --> 00:25:59,240
that's in production.

337
00:25:59,240 --> 00:26:07,000
And so the machine learning, the teams that are really good at ML, they've really focused

338
00:26:07,000 --> 00:26:15,040
on building and managing this ML flywheel and it's my claim that great ML applications

339
00:26:15,040 --> 00:26:17,440
require a great ML flywheel.

340
00:26:17,440 --> 00:26:25,240
And so I was talking about the declarative definitions there, that fits into this whole

341
00:26:25,240 --> 00:26:28,440
thing because it's really tough to, there's a lot of parts, a lot of different technology

342
00:26:28,440 --> 00:26:30,280
all throughout the ML flywheel.

343
00:26:30,280 --> 00:26:34,240
And so I think it's going to be really important, you know, we're talking about data-centric

344
00:26:34,240 --> 00:26:44,560
AI now, the declarative definitions, patterns that are central to data-centric AI, that's

345
00:26:44,560 --> 00:26:45,560
going to be applied.

346
00:26:45,560 --> 00:26:50,840
I think, I think applying those to the whole ML flywheel is going to be a big unlock.

347
00:26:50,840 --> 00:26:57,520
It's going to make things a lot easier for teams to really build and maintain their whole

348
00:26:57,520 --> 00:27:02,040
ML flywheel because it manages and it hides away a lot of the complexity of all of the

349
00:27:02,040 --> 00:27:09,480
different technologies that span from the analytic world to the production world, from the

350
00:27:09,480 --> 00:27:13,760
forward pass of the data, you know, learning and making predictions to the bringing the

351
00:27:13,760 --> 00:27:18,440
data back into the analytic side of logging and organizing the data.

352
00:27:18,440 --> 00:27:25,160
It's a lot of stuff and teams need some much simpler interfaces to define these things.

353
00:27:25,160 --> 00:27:30,880
And I think these declarative interfaces are really the key to unlocking the ability to

354
00:27:30,880 --> 00:27:34,440
manage the ML flywheel for the average team.

355
00:27:34,440 --> 00:27:43,680
And a lot of ways the promise of ML ops was giving teams a platform technology for managing

356
00:27:43,680 --> 00:27:49,760
this flywheel or, you know, even more strongly creating a flywheel where before there was

357
00:27:49,760 --> 00:27:56,440
kind of bespoke one-off transitions and handoffs from one team to the next.

358
00:27:56,440 --> 00:28:04,320
And part of that idea was like applying some of popular ideas like DevOps and continuous

359
00:28:04,320 --> 00:28:12,320
delivery from software engineering to ML, how have you seen that evolve?

360
00:28:12,320 --> 00:28:18,800
Do you think that that has played out the way, you know, folks have wanted to or do you

361
00:28:18,800 --> 00:28:20,680
where do you still see gaps?

362
00:28:20,680 --> 00:28:26,600
So one kind of symptom that things are not as they should be is that there's, we talked

363
00:28:26,600 --> 00:28:30,760
about the ML ops landscape, there's like 4,000 different things there.

364
00:28:30,760 --> 00:28:34,120
And it's just in here, even if you know the space really well, like you do, it's still

365
00:28:34,120 --> 00:28:37,160
like, whoa, there's a lot of things here.

366
00:28:37,160 --> 00:28:43,480
And an average ML team, firstly, realistically, the average ML team is not expert at all

367
00:28:43,480 --> 00:28:44,960
of those different things.

368
00:28:44,960 --> 00:28:49,320
But secondly, they have to piece together a lot of these different items and kind of ductate

369
00:28:49,320 --> 00:28:54,080
them all together into like a coherent application that runs smoothly.

370
00:28:54,080 --> 00:29:02,320
And I think there's been a variety of ML ops efforts to make that process smoother.

371
00:29:02,320 --> 00:29:05,640
What we've seen is people have gotten it wrong in two different ways.

372
00:29:05,640 --> 00:29:08,560
One way is their scope was too small.

373
00:29:08,560 --> 00:29:15,480
So there's a variety of systems that focus only on a subset of that whole flywheel, right?

374
00:29:15,480 --> 00:29:19,920
So they may just focus on, let me make the process of training a model really good.

375
00:29:19,920 --> 00:29:25,680
Like I'm going to do some, you know, experiment management just in the learning phase, for example.

376
00:29:25,680 --> 00:29:30,520
Or let me just, you know, this is, I'm a tool to help, you know, do some prediction,

377
00:29:30,520 --> 00:29:35,120
inference stuff better, but it doesn't really address the whole flywheel part of it.

378
00:29:35,120 --> 00:29:41,760
And then the other way in which a variety of tools just have not been successful is I think

379
00:29:41,760 --> 00:29:44,320
they just bit off too much more than they can chew.

380
00:29:44,320 --> 00:29:48,040
And so they got a little bit too ambitious and they said, well, you know, we're the system

381
00:29:48,040 --> 00:29:49,600
that will do everything for you.

382
00:29:49,600 --> 00:29:53,560
And so they really set their scope on the whole ML flywheel, even if they weren't, you

383
00:29:53,560 --> 00:29:59,480
know, actively, consciously talking about like that bottom half of that flywheel that

384
00:29:59,480 --> 00:30:03,840
brings the data back into your, from production back into your training data sets.

385
00:30:03,840 --> 00:30:08,560
But, you know, it's just the dream of like, we handle everything, just just use our tool

386
00:30:08,560 --> 00:30:10,720
and everything will be so much easier for you.

387
00:30:10,720 --> 00:30:11,720
Yeah.

388
00:30:11,720 --> 00:30:15,200
I love that you bring up both those points because I've written about this previously in

389
00:30:15,200 --> 00:30:20,920
the, okay, definitive guide to ML platforms ebook and called it this wide versus deep paradox.

390
00:30:20,920 --> 00:30:25,960
Like, you know, you have just as you said, a bunch of tools that are trying to, you know,

391
00:30:25,960 --> 00:30:30,360
solve the end and platform, but don't have sufficient depth in any particular area.

392
00:30:30,360 --> 00:30:37,080
And then you have others that are, you know, they only do one thing, but then, you know,

393
00:30:37,080 --> 00:30:40,200
they don't necessarily integrate the pieces together.

394
00:30:40,200 --> 00:30:42,360
And it's a tough spot.

395
00:30:42,360 --> 00:30:51,040
And I think that's a part of that just state of play is, you know, what's led to the rise

396
00:30:51,040 --> 00:30:56,160
of platform teams that are, you know, forming to kind of have been forming to pull all the

397
00:30:56,160 --> 00:31:00,200
pieces together and try to ensure an end-to-end experience.

398
00:31:00,200 --> 00:31:05,360
If not based on end-to-end tools, that's a really good point that that does speak to the

399
00:31:05,360 --> 00:31:10,480
need for platform teams and that helps and platform teams justify their existence with

400
00:31:10,480 --> 00:31:17,680
that to finish up on like in which ways the very broad folks get it wrong.

401
00:31:17,680 --> 00:31:19,440
I think we're still evolving as an industry.

402
00:31:19,440 --> 00:31:23,320
The best patterns are still emerging here, but I think what we're seeing is those folks

403
00:31:23,320 --> 00:31:27,160
that promise everything, you know, it's just a really hard problem.

404
00:31:27,160 --> 00:31:33,280
And there's a lot of things to get right and, you know, realistically, if you're depending,

405
00:31:33,280 --> 00:31:37,400
if your product is, we're going to get every single thing, right?

406
00:31:37,400 --> 00:31:45,640
That's unrealistic and customers, you know, average ML teams, they don't want to get stuck

407
00:31:45,640 --> 00:31:51,160
building the, they don't want to get stuck with an inflexible ML platform that only lets

408
00:31:51,160 --> 00:31:53,040
them do certain things.

409
00:31:53,040 --> 00:31:59,600
So, you know, what we're, our approach with the ML flywheel is not to say we do everything.

410
00:31:59,600 --> 00:32:04,440
We, our scope is the whole ML flywheel, but we're really focused on managing the data

411
00:32:04,440 --> 00:32:07,840
sets and enabling the data flows through that ML flywheel.

412
00:32:07,840 --> 00:32:13,960
So we're focused on helping you create your feature logs, create your training data sets,

413
00:32:13,960 --> 00:32:19,280
create your, you know, calculate your features in real time, have a unified data model across

414
00:32:19,280 --> 00:32:25,000
this whole ML flywheel and generate compatible schemas and all of the different pieces of infrastructure

415
00:32:25,000 --> 00:32:28,640
and manage as much of the data engineering through that as possible.

416
00:32:28,640 --> 00:32:33,520
So we're taking a really big chunk of all of the work that needs to get done, but there's

417
00:32:33,520 --> 00:32:34,680
a lot of stuff that we're not doing.

418
00:32:34,680 --> 00:32:39,560
We don't touch models, we don't build models or not the model management system.

419
00:32:39,560 --> 00:32:46,440
And so we think that that's a much more manageable domain that's very valuable that we can

420
00:32:46,440 --> 00:32:51,720
really focus on the workflows, we can really focus on improving those workflows and making

421
00:32:51,720 --> 00:32:53,760
an amazing user experience there.

422
00:32:53,760 --> 00:32:57,840
So that's our approach and how we really see us doing something different than the whole

423
00:32:57,840 --> 00:33:02,080
end-to-end ML platforms that, you know, take on too much and the very specialized tools

424
00:33:02,080 --> 00:33:04,040
that only solve one part of the workflow.

425
00:33:04,040 --> 00:33:08,360
And so you brought us back to this flywheel part of the discussion, but I know you've

426
00:33:08,360 --> 00:33:12,200
got an interesting take on platforms teams as well.

427
00:33:12,200 --> 00:33:14,960
Yeah, I mean, what are you saying there?

428
00:33:14,960 --> 00:33:21,000
Well, on the platform teams, we were just talking about like how a lot of these challenges

429
00:33:21,000 --> 00:33:24,840
allow ML platform teams to, you know, justify their existence.

430
00:33:24,840 --> 00:33:28,160
And so if you can kind of think of both of those domains, right?

431
00:33:28,160 --> 00:33:31,840
If they're either piecing together a bunch of small things together and they're really

432
00:33:31,840 --> 00:33:36,680
running a bunch of glue code and building kind of like a brittle infrastructure within

433
00:33:36,680 --> 00:33:41,640
the company, or they may not want to take on that type of challenge.

434
00:33:41,640 --> 00:33:51,360
And then the platform team is all about evaluating different end-to-end platforms and just choosing

435
00:33:51,360 --> 00:33:54,360
the right one and operating that within the business.

436
00:33:54,360 --> 00:33:57,960
And there's different failure modes for both of those technically.

437
00:33:57,960 --> 00:34:02,240
So there's, you know, it's really hard to maintain your glue code and your duct tape

438
00:34:02,240 --> 00:34:07,160
and, you know, keep everything running in a reliable way on the first one.

439
00:34:07,160 --> 00:34:10,440
On the second one, you're fundamentally signing up.

440
00:34:10,440 --> 00:34:16,440
You're putting all your eggs in one basket and you're investing in a system that inherently

441
00:34:16,440 --> 00:34:20,920
is going to be, have the flexibility of just a single system.

442
00:34:20,920 --> 00:34:26,280
And as soon as your business needs, I'll grow that then it's going to be on you to solve

443
00:34:26,280 --> 00:34:27,280
that.

444
00:34:27,280 --> 00:34:31,320
And that's even a much harder position to be in because then you have to expand from

445
00:34:31,320 --> 00:34:38,800
a single platform that you have within your business to building a whole stack from scratch.

446
00:34:38,800 --> 00:34:44,800
So that's a little bit on the technical side, but I have to say that I've seen, I haven't

447
00:34:44,800 --> 00:34:49,480
seen ML platform teams be very successful in industry generally.

448
00:34:49,480 --> 00:34:54,960
And this is a little bit of, you know, it's not something that like ML platform teams

449
00:34:54,960 --> 00:34:59,320
necessarily want to hear, but, you know, we work with a lot of ML platform teams and a

450
00:34:59,320 --> 00:35:01,680
lot of ML use case teams.

451
00:35:01,680 --> 00:35:07,760
And so the use case teams are the folks who are building the recommender system to power,

452
00:35:07,760 --> 00:35:12,240
you know, the website or they're building the fraud detection system to block certain transactions,

453
00:35:12,240 --> 00:35:13,240
stuff like that.

454
00:35:13,240 --> 00:35:18,680
They have a very specific business impact that they're measuring their success on and they're

455
00:35:18,680 --> 00:35:19,680
under the gut.

456
00:35:19,680 --> 00:35:22,760
They're trying to get this stuff going and launched as soon as possible and they're working

457
00:35:22,760 --> 00:35:26,840
with business people and it's a team of data scientists, engineers, I'll mix together

458
00:35:26,840 --> 00:35:30,280
and the product engineers as well.

459
00:35:30,280 --> 00:35:34,080
The platform teams are really focused on, hey, let me make some, let me kind of like

460
00:35:34,080 --> 00:35:43,360
centralize a lot of our engineering investments that the use case teams are making and bring

461
00:35:43,360 --> 00:35:47,920
a lot of that engineering into one place and build some reusable components to better enable

462
00:35:47,920 --> 00:35:49,720
those use case teams.

463
00:35:49,720 --> 00:35:53,360
And so that's great and that's actually is a really good model.

464
00:35:53,360 --> 00:35:57,080
The challenge that we have and that we see a lot of like mistakes that we see a lot

465
00:35:57,080 --> 00:36:04,280
of companies making is it's not uncommon to see a platform team exist before the use case

466
00:36:04,280 --> 00:36:07,880
teams exist and that's a problem because what is the platform team trying to do if there's

467
00:36:07,880 --> 00:36:13,400
no use cases, you got to start with the use case, you have to have concrete business outcomes

468
00:36:13,400 --> 00:36:18,440
that you're trying to drive with machine learning and then only then once you understand

469
00:36:18,440 --> 00:36:24,800
those requirements, does it make sense to build a platform team and when you have multiple

470
00:36:24,800 --> 00:36:28,400
of those use case teams so you can support them.

471
00:36:28,400 --> 00:36:34,440
One thing that we, you know, a really good sign that a platform team is struggling and

472
00:36:34,440 --> 00:36:40,760
is not set up for success is when they don't have requirements.

473
00:36:40,760 --> 00:36:45,880
When you talk to them and you say, so what exactly are you trying to build, like what kind

474
00:36:45,880 --> 00:36:49,720
of, what kind of scale do you need, what kind of latencies do you need, like what do you

475
00:36:49,720 --> 00:36:51,800
need and they don't know.

476
00:36:51,800 --> 00:36:57,840
And when they don't know, it's because they don't have use cases yet and then they also

477
00:36:57,840 --> 00:37:02,160
don't have a clear decision making process as well because when there's not concrete

478
00:37:02,160 --> 00:37:09,200
business use case tied to it, it's kind of like building for the future, it's all speculative,

479
00:37:09,200 --> 00:37:13,600
who should make this decision and we see those teams struggle and they spend their wheels

480
00:37:13,600 --> 00:37:18,200
a lot because they don't really have like a concrete direction that they're going towards

481
00:37:18,200 --> 00:37:23,640
and so I would recommend that any ML, if you're on an ML platform team, think really carefully

482
00:37:23,640 --> 00:37:29,560
about who your internal customers are, what those use cases use case teams are because

483
00:37:29,560 --> 00:37:35,320
the ML platform is a product just like any other company is building a product and the

484
00:37:35,320 --> 00:37:41,280
most, like the best advice for anybody building a product is to focus on your customers.

485
00:37:41,280 --> 00:37:44,560
And if you don't know who your customers are as an ML platform team, it's really hard

486
00:37:44,560 --> 00:37:45,560
to find success.

487
00:37:45,560 --> 00:37:50,920
Now, that's awesome. It prompts me to plug our TwilmoCon event because we've spent a

488
00:37:50,920 --> 00:37:55,640
lot of time trying to help platform teams understand what the best teams in the industry

489
00:37:55,640 --> 00:38:02,680
are doing and you know, we've got this event coming up in the fall, but also we just revamped

490
00:38:02,680 --> 00:38:08,160
our website and all of the content from the past due conferences is up there and I'm

491
00:38:08,160 --> 00:38:11,840
thinking of a couple in particular that have talked a lot about this kind of thing.

492
00:38:11,840 --> 00:38:15,920
We've been doing these team tear downs when we talk about how the platform teams engage

493
00:38:15,920 --> 00:38:25,400
with the user teams and why those interaction modes are so important as well as thinking

494
00:38:25,400 --> 00:38:30,600
about an interview that I did with Fran Bell who ran a platform team at Uber that was

495
00:38:30,600 --> 00:38:40,200
focused on forecasting and kind of higher level platform functionality and really interesting.

496
00:38:40,200 --> 00:38:44,320
Their approach was to, you know, these teams were off doing what they were doing, these

497
00:38:44,320 --> 00:38:49,800
use case teams that you were describing and they would kind of talk to all these teams

498
00:38:49,800 --> 00:38:54,680
and identify the things that the wheel that everyone was reinventing and kind of build

499
00:38:54,680 --> 00:38:57,920
the platform around that kind of functionality.

500
00:38:57,920 --> 00:39:01,760
So lots of great insights there from folks that have been doing it.

501
00:39:01,760 --> 00:39:08,680
And you mentioned Fran Bell. I worked with Fran at Uber and that platform was very successful

502
00:39:08,680 --> 00:39:14,560
that she built and the approach that they had was not, hey, you know what would be cool?

503
00:39:14,560 --> 00:39:18,040
Let's build a time series for passing platform. It was definitely not that.

504
00:39:18,040 --> 00:39:24,280
It was, it was, we have a bunch of time series forecasting problems that were solving and

505
00:39:24,280 --> 00:39:29,360
they went and built them and then they had the three, four and then it became 10 different

506
00:39:29,360 --> 00:39:33,920
teams that were doing the same thing. They were like, you know, why don't we just build,

507
00:39:33,920 --> 00:39:38,640
you know, a thin layer that these different teams can reuse and they kind of just added

508
00:39:38,640 --> 00:39:43,520
more automation into this layer and centralize more, more into it over time.

509
00:39:43,520 --> 00:39:47,880
And as they were able to bring in the different use cases, that requirements from the different

510
00:39:47,880 --> 00:39:51,840
use cases, they were, they had a purview that individual use cases didn't have and they

511
00:39:51,840 --> 00:39:56,200
were able to think more creatively and more innovatively about how to solve this stuff.

512
00:39:56,200 --> 00:40:02,360
And that's a, a layer where they made some really amazing inventions. And, and that works

513
00:40:02,360 --> 00:40:08,280
both at the, at the kind of like platform team looking at the use case team direction.

514
00:40:08,280 --> 00:40:12,360
And it also works at the platform team looking upstream at other infrastructure dimension

515
00:40:12,360 --> 00:40:17,480
because they were able to build on top of the Michelangelo system. And so we actually

516
00:40:17,480 --> 00:40:22,160
integrated those systems over time and so all the time series, the time series stuff that

517
00:40:22,160 --> 00:40:26,480
the time series platform was building was actually, you know, compiled down to Michelangelo

518
00:40:26,480 --> 00:40:32,520
jobs and Michelangelo model training and model serving types of things. And, and that

519
00:40:32,520 --> 00:40:37,560
was like an excellent example of like a great value added by a platform team, but none

520
00:40:37,560 --> 00:40:43,800
of that would have been possible if the, if friends original attitude was not to just

521
00:40:43,800 --> 00:40:48,000
solve the problem. Like just get, you know, get your first time series thing solved and

522
00:40:48,000 --> 00:40:51,720
then you can figure out how to do some cool platform thing later on. Yeah. Yeah. And

523
00:40:51,720 --> 00:40:58,920
I'll link to that particular interview in the show notes for folks that want to, for

524
00:40:58,920 --> 00:41:03,560
folks who this problem of how to build a successful platform team resonates. He said something

525
00:41:03,560 --> 00:41:13,600
else in our kind of pre live chat that I thought was interesting around kind of what really

526
00:41:13,600 --> 00:41:21,600
matters when trying to, you know, when trying to do machine learning, you know, what matters

527
00:41:21,600 --> 00:41:27,760
ultimately is creating business value. And so it's either helping your company have more

528
00:41:27,760 --> 00:41:36,680
revenue, reducing the costs adding or reducing risks. And, and, and so, you know, we talked

529
00:41:36,680 --> 00:41:42,960
about ways in which platform teams things go wrong for them. But there's also individual

530
00:41:42,960 --> 00:41:50,680
ML products projects rather that, that trip up in a variety of different ways. And I think

531
00:41:50,680 --> 00:41:58,680
the biggest, the biggest theme that I see is focus focusing on problems that are just

532
00:41:58,680 --> 00:42:03,040
not that important to the business. And I saw this a bunch of times at Uber, a data science

533
00:42:03,040 --> 00:42:07,920
team, you know, in some corner, there's three people spending a lot of time solving this

534
00:42:07,920 --> 00:42:12,960
problem that's, you know, it's an interesting machine learning problem. It's really cool

535
00:42:12,960 --> 00:42:19,200
technology. It's fun to work on. But it takes them, of course, it's just like any project.

536
00:42:19,200 --> 00:42:24,800
It takes some engineering investment. It takes any type of different like cross team collaboration

537
00:42:24,800 --> 00:42:30,040
to actually get this thing across the finish line and into production when you want to

538
00:42:30,040 --> 00:42:36,080
deploy it into production. And, and, you know, that requires someone up to change some

539
00:42:36,080 --> 00:42:39,960
executive to say, yeah, we should spend engineers on this. This is important enough that we

540
00:42:39,960 --> 00:42:44,880
should, you know, staff the team against this. And often that doesn't happen. And why?

541
00:42:44,880 --> 00:42:48,840
Because the problem is just, you know, it's a small thing. Who cares? We didn't really

542
00:42:48,840 --> 00:42:52,720
need to, it wasn't that important for us that, that random problem that you chose. And

543
00:42:52,720 --> 00:42:59,000
so I think this is like a really important thing for like ML practitioners to think about.

544
00:42:59,000 --> 00:43:04,440
And I've seen it again and again have big impact on people's careers, the ability to choose

545
00:43:04,440 --> 00:43:08,960
the important problems. You know, you have a company, there's so many different cool

546
00:43:08,960 --> 00:43:13,080
things you can apply machine learning to. But how do you choose the ones that are, that

547
00:43:13,080 --> 00:43:17,880
are most important to the business that are going to have the biggest impact at the end

548
00:43:17,880 --> 00:43:22,800
of the day? I think that really requires understanding what is the, what matters to the business?

549
00:43:22,800 --> 00:43:28,640
What are the company's goals? What metrics are they trying to move? And then which levers,

550
00:43:28,640 --> 00:43:35,960
levers do I have at my, from my position to help me have some impact there? And not think

551
00:43:35,960 --> 00:43:40,600
about it like as a technology first, what's a cool problem? I heard about this cool technique.

552
00:43:40,600 --> 00:43:46,080
Let me apply it to this problem type of thing. That's, that's where a lot of teams on the

553
00:43:46,080 --> 00:43:50,120
use case side of things, not the platform teams get things wrong.

554
00:43:50,120 --> 00:43:56,520
As you're, as you're talking to, you know, use case teams and platform teams, you've identified

555
00:43:56,520 --> 00:44:02,720
a bunch of the pitfalls that you see them falling into, you know, how do you, you know,

556
00:44:02,720 --> 00:44:09,320
characterize the, the different teams that you speak in in terms of their level of maturity

557
00:44:09,320 --> 00:44:16,280
and how effective they are at side stepping these, these traps.

558
00:44:16,280 --> 00:44:26,120
I think a lot of, there are a variety of different ways to kind of plot out a like steps of,

559
00:44:26,120 --> 00:44:33,280
of maturity. And one of the things that's that we think about a lot is as a, as a, a

560
00:44:33,280 --> 00:44:39,080
sign of being at a minimal level of maturity for us to really care about working with those

561
00:44:39,080 --> 00:44:43,880
people or, or to feel like they're mature enough that we should work with them is do they

562
00:44:43,880 --> 00:44:50,000
have one model in production? That's a really good sign of the health of your ML team.

563
00:44:50,000 --> 00:44:57,080
Because if you can't get one model in production, what, what is it? It may not be, the problem

564
00:44:57,080 --> 00:45:02,440
is it may not be a machine learning thing. It may be you, I mean, you may not have executive

565
00:45:02,440 --> 00:45:06,120
buy-in. You may be working on the wrong problem. Maybe nobody in your company cares about

566
00:45:06,120 --> 00:45:09,640
that problem. And so no one's helping you get to production. Maybe you don't have the

567
00:45:09,640 --> 00:45:14,520
right underlying data infrastructure. Maybe the engineering team just sucks. And so you're

568
00:45:14,520 --> 00:45:20,240
just not, you know, there's all these possible things. And so you could be super skilled

569
00:45:20,240 --> 00:45:23,280
at machine learning. But then there's all of these other things that are holding your

570
00:45:23,280 --> 00:45:30,920
team back. And so we de-risk that by only working with people who have one model in production.

571
00:45:30,920 --> 00:45:34,720
And that's a sense of mature, that gives us a sense of not your ML maturity, actually,

572
00:45:34,720 --> 00:45:38,320
but all of the rest of the maturity. Do you have your data in the right place and stuff

573
00:45:38,320 --> 00:45:43,840
like that? You were at the right competence to get past the finish line once at least.

574
00:45:43,840 --> 00:45:51,680
And then what we can help you with is help it help you make that process a positive ROI.

575
00:45:51,680 --> 00:45:57,760
Make that process, you know, valuable for you, make it repeatable, make it reliable, all

576
00:45:57,760 --> 00:46:03,040
of that kind of stuff. But there's some kind of like core foundational things that teams

577
00:46:03,040 --> 00:46:11,160
need to get right first. But I think in terms of different types of ML use cases, some teams

578
00:46:11,160 --> 00:46:15,480
are focused on operational machine learning and the care about real-time ML. You can imagine

579
00:46:15,480 --> 00:46:25,760
a different maturity curve and kind of milestones for a team to get there than a similar maturity

580
00:46:25,760 --> 00:46:35,520
curve that would apply to a team that is doing offline, like document analysis or something

581
00:46:35,520 --> 00:46:36,520
like that.

582
00:46:36,520 --> 00:46:43,040
So I don't think there's a general maturity curve that applies to every team. But it's

583
00:46:43,040 --> 00:46:48,800
more like on a per-use case basis of like, what is your role in the interaction there?

584
00:46:48,800 --> 00:46:52,160
And then how can you find the milestones that matter to you? And then that example of

585
00:46:52,160 --> 00:46:56,280
getting the past to finish line once is one important one. And it strikes me that one

586
00:46:56,280 --> 00:47:00,800
model in production, yeah, that seems like a really low bar for 2022.

587
00:47:00,800 --> 00:47:08,040
Yeah, and you'd be surprised how many teams are still working on that. And it's not that

588
00:47:08,040 --> 00:47:14,200
wow, like machine learning is really hard there. But the types of problems are, hey, our

589
00:47:14,200 --> 00:47:21,400
data is still on prem. And our data science team started on AWS. And so they can kind

590
00:47:21,400 --> 00:47:26,840
of like pull in data into the cloud. But like, is the data productionized into the cloud?

591
00:47:26,840 --> 00:47:31,320
Are we going to run the ML application in the cloud? Are we going to bring the ML application

592
00:47:31,320 --> 00:47:36,000
back to our data center? We're still figuring out these details and there's a lot of like

593
00:47:36,000 --> 00:47:42,120
political things. And then those kind of things just kind of keep going. And that's not

594
00:47:42,120 --> 00:47:46,080
where if you're trying to help someone out with machine learning, you want to, you can't

595
00:47:46,080 --> 00:47:50,200
really help with the internal political stuff. You want to help them, you know, do their

596
00:47:50,200 --> 00:47:51,920
workflows and their processes properly?

597
00:47:51,920 --> 00:47:58,760
Yeah, I was going to ask if part of it is a semantic thing where, you know, folks will

598
00:47:58,760 --> 00:48:03,400
say my model is in production, but it's in production in the sense that it's making

599
00:48:03,400 --> 00:48:09,160
predictions that end up in a exosperate sheet that someone's manually analyzing versus

600
00:48:09,160 --> 00:48:15,720
kind of closing the loop so that a system is reacting to the decisions made by the model.

601
00:48:15,720 --> 00:48:20,080
That's a really good distinction. And so people say we have a bunch of models in production

602
00:48:20,080 --> 00:48:24,120
and we talk about operational machine learning. So I kind of break the machine learning into

603
00:48:24,120 --> 00:48:30,680
two buckets. Analytical machine learning, which is where it can be offline. It's mainly

604
00:48:30,680 --> 00:48:37,080
its main purpose is to influence a human driven decision. So let me, let me, let me score

605
00:48:37,080 --> 00:48:40,840
some leads and then I'll look at the scores and then maybe I'll send some emails or something

606
00:48:40,840 --> 00:48:45,560
like that based on them. And we'll do that on a weekly basis. That's an analytic machine

607
00:48:45,560 --> 00:48:51,920
learning type of use case. Operational machine learning is online. It needs to be monitored.

608
00:48:51,920 --> 00:48:57,360
It affects the customers directly drives automated decisions and it's often real time. And

609
00:48:57,360 --> 00:49:01,320
those are the types of use cases where ML is powering your product. Those are the types

610
00:49:01,320 --> 00:49:06,680
of use cases where being in production means something very different. Yeah. Yeah. Mike,

611
00:49:06,680 --> 00:49:14,680
as always, this has been a wonderful chat. You know, maybe a place to wrap up is for,

612
00:49:14,680 --> 00:49:19,800
you know, folks that are interested in what you've been talking at your approach to data

613
00:49:19,800 --> 00:49:25,920
centric AI kind of where this should they be looking. Yeah. Thanks, Sam. Two places.

614
00:49:25,920 --> 00:49:36,360
So and maybe one update since we last spoke. TechCon is now the main maintainer of the

615
00:49:36,360 --> 00:49:42,000
popular open source feature store called Feast. So that's a solution that anybody can

616
00:49:42,000 --> 00:49:47,600
try very lightweight. It's the kind of build your own feature store framework. And it's

617
00:49:47,600 --> 00:49:54,400
the most certainly the most popular feature store. You can find that at feast.dev. And

618
00:49:54,400 --> 00:49:59,720
if if there's any listeners from companies who are really trying to figure out like, hey,

619
00:49:59,720 --> 00:50:06,200
how do we build real production applications? ML applications and we need some help on

620
00:50:06,200 --> 00:50:11,040
the data side of things to help this stuff get into production either in real time, super

621
00:50:11,040 --> 00:50:17,040
reliably. That's what the TechCon feature platform is for. And so folks can check us

622
00:50:17,040 --> 00:50:46,040
out. So that TechCon dot AI. Awesome. Awesome. Well, thanks so much, Mike. Thank you.

