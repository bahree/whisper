WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:33.960
I'm your host Sam Charrington, a quick update on our deep learning study group.

00:33.960 --> 00:38.760
As you know, we're huge fans of the fast.ai courses and we recently had our second group

00:38.760 --> 00:44.720
of deep learning learners complete the fast.ai deep learning for Kodo's course back in December.

00:44.720 --> 00:48.080
I'm excited to announce that we'll be hosting our third group of students taking the

00:48.080 --> 00:52.800
part one course starting this Saturday morning, February 2nd.

00:52.800 --> 00:57.040
This study group will run seven weeks finishing just in time for participants to jump right

00:57.040 --> 01:02.560
into the deep learning for Kodo's part two course, which is set to start in mid-march.

01:02.560 --> 01:10.160
For details on the study groups or to get registered, visit twimbleai.com slash meetup.

01:10.160 --> 01:14.160
While it nerfs this past December, I had the pleasure of attending the second annual

01:14.160 --> 01:18.840
Black and AI workshop in dinner, which brings in participants from all over the world to

01:18.840 --> 01:23.080
showcase their research, share experiences, and support one another.

01:23.080 --> 01:26.760
I was fortunate enough to spend the day at the workshop and I'm excited to share with

01:26.760 --> 01:31.200
you over the course of this month conversations with just a few of the great members of this

01:31.200 --> 01:32.600
community.

01:32.600 --> 01:38.080
To keep up with the series, visit twimbleai.com slash Black and AI 19.

01:38.080 --> 01:44.720
To get this series kicked off, we're joined by Randy Williams, PhD student at the MIT Media

01:44.720 --> 01:45.720
Lab.

01:45.720 --> 01:49.720
At the Black and AI workshop, Randy presented her research on pop-bots, an early childhood

01:49.720 --> 01:57.120
AI curriculum, which is geared toward teaching preschoolers the fundamentals of artificial intelligence.

01:57.120 --> 02:01.840
In our conversation, we discussed the origins of the project, the three AI concepts that

02:01.840 --> 02:06.840
are taught in the program, and the goals that Randy hopes to accomplish with her work.

02:06.840 --> 02:10.360
This was a fun conversation, it was super thought-provoking.

02:10.360 --> 02:12.360
Enjoy.

02:12.360 --> 02:17.920
All right, everyone, I am on the line with Randy Williams.

02:17.920 --> 02:21.840
Randy is a PhD student at the MIT Media Lab.

02:21.840 --> 02:24.480
Randy, welcome to this week in machine learning and AI.

02:24.480 --> 02:26.680
Hi, thank you Sam for inviting me.

02:26.680 --> 02:30.920
I'm happy to be here to talk to everyone about my work.

02:30.920 --> 02:31.920
Absolutely.

02:31.920 --> 02:37.800
So we had an opportunity to meet at NURBS recently, in fact, you presented at the Black

02:37.800 --> 02:48.120
and AI workshop there, and I was really fascinated by the work you're doing in teaching preschool

02:48.120 --> 02:51.920
children about artificial intelligence.

02:51.920 --> 02:58.360
What sparked your interest in doing that, in teaching, you know, young children about

02:58.360 --> 02:59.360
AI?

02:59.360 --> 03:06.200
Thanks, the workshop that was definitely incredible, and I was happy to share my work with the

03:06.200 --> 03:07.920
people there.

03:07.920 --> 03:11.480
So I am a PhD student at the Media Lab.

03:11.480 --> 03:15.800
I've been working on this project for about three years, and when it started, it wasn't

03:15.800 --> 03:19.920
about AI, and it wasn't necessarily about preschool children either.

03:19.920 --> 03:27.120
It was about computational thinking and how do we help students who might not have access

03:27.120 --> 03:30.200
to fancy robotic toolkits or to teachers?

03:30.200 --> 03:32.640
How do we help them start to learn about these things?

03:32.640 --> 03:39.920
How do we spread the influence of the whole craze about CS to different populations?

03:39.920 --> 03:44.120
So I personally am from, well, Prinshire just County, Maryland, but my family is from

03:44.120 --> 03:48.360
Baltimore, and I went to school in Baltimore, and while I was in undergrad, I spent a lot

03:48.360 --> 03:54.320
of time working with inner-city children and, you know, doing maker spaces or doing

03:54.320 --> 03:59.480
workshops, that kind of thing, and what was really awesome about it was how engaged students

03:59.480 --> 04:04.000
were with, you know, learning these different things, but it also made me a bit sad that,

04:04.000 --> 04:09.240
you know, so many of my classmates in school, they were like, oh, yeah, I've been programming

04:09.240 --> 04:12.560
since I was seven, and, you know, these students, you know, they were like high schoolers,

04:12.560 --> 04:14.760
and they're just getting started with this.

04:14.760 --> 04:20.280
So I just felt really strongly that, you know, there needs to be more done to help everyone

04:20.280 --> 04:24.280
have a chance to learn about these things early on, and it was really difficult in Baltimore

04:24.280 --> 04:28.520
was that there just weren't a lot of teachers, there weren't a lot of people who knew about

04:28.520 --> 04:32.920
the field to come and, you know, teach the kids and share the expertise.

04:32.920 --> 04:36.480
So within the group that I started working in, in the MIT Media Lab, it's called the

04:36.480 --> 04:41.600
Personal Robots Group, and my professor is in Theore Brazil, and she's really passionate

04:41.600 --> 04:46.880
about how AI robotics can help us flourish as human beings, and so a lot of our work

04:46.880 --> 04:50.240
has been about education and how robots play a role in that.

04:50.240 --> 04:55.320
So I started out just building a robot that could help children learn how to program and

04:55.320 --> 05:00.320
sort of, like, be the fun, interactive learning companion to help them, you know, figure things

05:00.320 --> 05:04.960
out and push them to solve new problems and things like that, so the absence of a trained

05:04.960 --> 05:10.480
teacher, how can a robot help children learn about these kinds of things?

05:10.480 --> 05:13.000
And it was really fun.

05:13.000 --> 05:17.360
I started thinking about, you know, like, so what makes the most sense, you know, having

05:17.360 --> 05:21.200
some experience in Baltimore, I was like, well, no one's going to go buy a $200 robot

05:21.200 --> 05:24.920
to do this, so how do we make something less expensive?

05:24.920 --> 05:28.680
So the pop-up project that I'm working on is mostly based around a mobile phone, so

05:28.680 --> 05:34.320
the mobile phone is the intelligent robot that children program, and then I was also thinking,

05:34.320 --> 05:40.040
well, how do we break away from computer sciences, you know, solving mases or doing puzzles

05:40.040 --> 05:44.880
and really open it up to different interests that students might have?

05:44.880 --> 05:49.400
So the robot, you know, can become a character, you can make it look like whatever you want,

05:49.400 --> 05:53.440
it can control the lights around your room and things like that, so it's sort of like

05:53.440 --> 05:58.720
opening doors for, you know, art and theater and music even.

05:58.720 --> 06:04.760
And then somewhere along the way, I was having a conversation with Cynthia one day about

06:04.760 --> 06:09.320
the project in the direction, and she was like, you know, Randy, it work is great, but

06:09.320 --> 06:12.440
you should really think about AI, because AI is the next big thing, and no one's really

06:12.440 --> 06:17.560
doing like AI education, and, you know, as a student, I'm like, okay, well, I've taken

06:17.560 --> 06:22.520
AI classes in college, I don't really know how I'm going to teach children AI, but yeah,

06:22.520 --> 06:25.800
sure, you know, let's go ahead and try it.

06:25.800 --> 06:32.600
So I quickly pivoted and how did I end up with preschool children?

06:32.600 --> 06:39.280
Well, there were a lot of like robot kits for like the seven to ten year old age, and

06:39.280 --> 06:43.600
I just, I guess, enjoy not sleeping and like solving really crazy problems.

06:43.600 --> 06:45.360
So I was like, I'm going to go like right below that.

06:45.360 --> 06:50.640
I'm going to do the five to seven year olds, and that's, it worked out, I guess.

06:50.640 --> 06:56.760
So we have a five to seven year old preschool AI toolkit that I'm working on.

06:56.760 --> 06:57.760
That's awesome.

06:57.760 --> 06:58.760
That's awesome.

06:58.760 --> 07:03.840
So is the Nielab, is that its own department, or are you affiliated with computer science

07:03.840 --> 07:11.840
or robotics department, and kind of what I'm also curious about here is, have you also

07:11.840 --> 07:18.440
received any formal training in education, and how do you kind of think about the interdisciplinary

07:18.440 --> 07:19.840
nature of your project?

07:19.840 --> 07:20.840
Yeah.

07:20.840 --> 07:23.480
So the Nielab is this weird crazy place.

07:23.480 --> 07:25.400
I work in a robotics lab.

07:25.400 --> 07:30.280
The people next to me, they do devices that go underneath your skin to help monitor

07:30.280 --> 07:33.640
your health, and the people on the other side of me do neurobiology.

07:33.640 --> 07:39.480
So it's a department with pretty much anyone who has a crazy idea that doesn't fit into

07:39.480 --> 07:43.240
like normal science or engineering department, you know, that's the place that you go to

07:43.240 --> 07:44.720
do work.

07:44.720 --> 07:49.800
So as a result, I get to do this project, this very interdisciplinary, I get to think

07:49.800 --> 07:58.040
about art, I get to go to schools and do education work, I get to do robots, and you know, I also

07:58.040 --> 08:01.960
have lots of resources around the lab where people do all of these things and can sort

08:01.960 --> 08:06.320
of help contribute to the project and help me grow my ideas.

08:06.320 --> 08:12.120
So I don't actually have formal training and education, however, a lot of the work

08:12.120 --> 08:16.200
that I'm doing is built on this program called Scratch.

08:16.200 --> 08:22.240
So Scratch is this website where children ages seven and up can go and learn about CS,

08:22.240 --> 08:29.040
and it was, I don't want to, so currently the leader of the lab is Mitchell Resnick, but

08:29.040 --> 08:35.320
I believe it started by, like, Seymour Pappert, like, you know, logo turtles.

08:35.320 --> 08:40.120
If you don't know what that is, it's these turtles where, you know, Seymour Pappert

08:40.120 --> 08:44.600
back in the 60s was like, all kids should know how to program, and all kids should, you

08:44.600 --> 08:45.600
know, be able to do this.

08:45.600 --> 08:49.120
And this is when like computers weren't even very popular, so everyone was like, dude, you're

08:49.120 --> 08:50.120
crazy.

08:50.120 --> 08:53.160
But, you know, he's like, yeah, I'm going to do it, and so he started building this

08:53.160 --> 08:57.960
programming language for children, and you know, generations later, there's this online

08:57.960 --> 09:02.800
portal where literally children, millions of children all over the world are learning

09:02.800 --> 09:04.240
how to program.

09:04.240 --> 09:07.440
So even though I didn't have the background, the CS education background to know how to

09:07.440 --> 09:11.680
do it, I got to work with Mitch, and I took his class, and I learned from his students

09:11.680 --> 09:14.160
and can start to pull those things in.

09:14.160 --> 09:17.600
I also didn't necessarily have a background in robotics.

09:17.600 --> 09:22.120
I did computer engineering and undergrad, but mostly, like, building little devices,

09:22.120 --> 09:25.720
not things that were, like, big and interacted with people.

09:25.720 --> 09:31.000
So I learned a lot from my group, and then everyone at the media lab, they're kind of just,

09:31.000 --> 09:32.280
you know, like artists.

09:32.280 --> 09:36.080
So I'm not an artist, I'm an engineer, but I became an artist, and it was able to pull

09:36.080 --> 09:37.080
that in.

09:37.080 --> 09:42.120
So really, what's awesome about interdisciplinary work is that you get to pull in from all

09:42.120 --> 09:46.040
of these different fields, you know, talking to developmental psychologists, can kids

09:46.040 --> 09:47.040
understand AI?

09:47.040 --> 09:48.920
Are they ready to do that yet?

09:48.920 --> 09:52.440
What are the right ways to translate the information so that it makes sense to them?

09:52.440 --> 09:57.040
I've really actually been inspired by all the people I've gotten to work with.

09:57.040 --> 10:01.960
So when you think about teaching AI to preschool kids, obviously, we're not trying to teach

10:01.960 --> 10:08.520
gradient descent algorithm or anything like that, like, how do you, how do you kind of

10:08.520 --> 10:12.360
break down, or maybe take it from the other direction?

10:12.360 --> 10:18.160
What are your goals in trying to teach AI to children at this age level?

10:18.160 --> 10:24.680
Yeah, I would say my primary goal is to give children agency and the world around them.

10:24.680 --> 10:30.200
So before I even, you know, put out this little kid and started, like, actually testing

10:30.200 --> 10:35.240
with children and building things, I did a whole series of studies with other people

10:35.240 --> 10:38.480
in my group around what do children think about AI?

10:38.480 --> 10:41.120
So we would have them interact with toys.

10:41.120 --> 10:44.120
If you look at kids' toys now, they're like amazing, they're really cool.

10:44.120 --> 10:49.200
So they have, like, these little robot things called Cosmo, which, like, they move around

10:49.200 --> 10:53.280
and they can play games against you and they're super cute and you can program them and

10:53.280 --> 10:54.280
stuff too.

10:54.280 --> 10:58.920
But, you know, it's like real AI that is being marketed to children.

10:58.920 --> 11:03.040
Then there's all this controversy about this Barbie doll, hello Barbie, that they talk

11:03.040 --> 11:08.400
to you and every kid in the world, I think, has had a conversation with Siri or Alexa,

11:08.400 --> 11:11.360
you know, not every kid in the world, but quite a few have.

11:11.360 --> 11:16.280
So it's interesting to see how in our time, you know, computers were just kind of, like,

11:16.280 --> 11:20.200
coming to be, and the internet was just coming to be, and children are growing up in a

11:20.200 --> 11:24.880
world now where it's like, yeah, AI is kind of a thing, like, it's normal to see that.

11:24.880 --> 11:30.120
So I was like, okay, so when a child interacts with this thing, that's not a life, not

11:30.120 --> 11:34.120
a human, but can talk to them and seems kind of smart, you know, what are they thinking?

11:34.120 --> 11:36.440
What's going on in their head?

11:36.440 --> 11:39.560
And oftentimes, they're, like, kind of just figuring it out.

11:39.560 --> 11:42.000
They're like, okay, it talks to me.

11:42.000 --> 11:43.000
Like, you know, let's poke it.

11:43.000 --> 11:48.040
We'll see if it can answer questions about dinosaurs or sloths, or does it know what's

11:48.040 --> 11:53.800
in the grocery store down the street, things like that, and then they're like, well, can

11:53.800 --> 11:54.800
I break it?

11:54.800 --> 11:59.320
Hey, do you want this apple, you know, asking these kinds of things to computers, and

11:59.320 --> 12:04.240
just just do what it will say, or, hey, do you have a boyfriend, do you have a girlfriend?

12:04.240 --> 12:08.840
Funny things like that, but even worse, so they kind of didn't really understand how

12:08.840 --> 12:09.840
it worked.

12:09.840 --> 12:14.720
And to me, that's like an opportunity and a challenge for today, because we don't really

12:14.720 --> 12:19.160
want children to have toys that they can't pick apart and understand.

12:19.160 --> 12:22.120
You know, they're like, I like so well, answer my question.

12:22.120 --> 12:23.120
Does she not like me?

12:23.120 --> 12:28.040
And it's like, no, Alexis, you know, NLP algorithm, just isn't programmed for young children's

12:28.040 --> 12:32.240
voices, because it was made by adults who thought only adults would be using it.

12:32.240 --> 12:35.440
So, you know, that's what's going on, but if you say that to a kid, they'll look at

12:35.440 --> 12:36.960
you like, what are you saying?

12:36.960 --> 12:39.200
I have no idea what's going on.

12:39.200 --> 12:43.520
So the goals of the curriculum are really to help children break those kinds of ideas

12:43.520 --> 12:44.520
down.

12:44.520 --> 12:51.400
Like, oh, Alexa isn't working because Alexa was trained a certain way, and if you try

12:51.400 --> 12:55.040
and have Alexa do things outside of the way that she was trained, then she's not going

12:55.040 --> 12:56.040
to get it.

12:56.040 --> 12:59.480
Like, that's kind of the right level that I think any child should have.

12:59.480 --> 13:02.920
It makes me think a little bit of a couple of examples.

13:02.920 --> 13:11.680
One that comes to mind is when children like, you know, who are raised on iPads, see magazines

13:11.680 --> 13:18.040
and start me capping at a magazine like wanting it to do something, or the other example

13:18.040 --> 13:23.320
that comes to mind is knowing how to really effectively search Google, it's a powerful

13:23.320 --> 13:24.320
skill.

13:24.320 --> 13:28.720
But, you know, both of these things, I think, illustrate, you know, like mental models

13:28.720 --> 13:34.880
that are created over time about this thing that you're interacting with, that, you

13:34.880 --> 13:38.720
know, in the case of an iPad, it's just like this piece of glass, but you kind of develop

13:38.720 --> 13:43.960
this model about like how, you know, flat things work, I guess, or, you know, in the case

13:43.960 --> 13:48.360
of a search box, like, you know, how do you can really effectively use, you know, this

13:48.360 --> 13:50.560
world that's behind the search box?

13:50.560 --> 13:56.400
It is part of your work here, trying to shift the mental model that kids have about AI.

13:56.400 --> 14:02.120
I actually really love that framing, and I might have to use that in the future, you know,

14:02.120 --> 14:09.480
like, we're in children's mental models about AI, yeah, so that is literally what I'm

14:09.480 --> 14:10.480
doing.

14:10.480 --> 14:16.320
So, a part of the actual pop-up study that I did, so I have children interact with the AI,

14:16.320 --> 14:20.360
and not just interact with it, I also have them, like, building algorithms from scratch,

14:20.360 --> 14:21.360
not the whole thing.

14:21.360 --> 14:24.960
They're not writing the programming, but pretty much they have a lot of control over the

14:24.960 --> 14:26.360
way the algorithm works.

14:26.360 --> 14:30.920
So, we do that, and at the end of the day, they have this finished AI product that seems

14:30.920 --> 14:36.240
intelligent, and before and after, they're learning about AI, ask them, what do you think

14:36.240 --> 14:37.960
about this thing that's in front of you?

14:37.960 --> 14:38.960
Do you think it's alive?

14:38.960 --> 14:39.960
Is it a person?

14:39.960 --> 14:40.960
Is it a toy?

14:40.960 --> 14:41.960
Is an adult?

14:41.960 --> 14:42.960
Is it a child?

14:42.960 --> 14:43.960
Is it smarter than you?

14:43.960 --> 14:44.960
Are you smarter than that?

14:44.960 --> 14:49.480
And I'm asking all of these questions, because I expect a mental model change to

14:49.480 --> 14:55.120
happen when children are learning about AI, and I'm wondering, you know, how can we make

14:55.120 --> 15:01.520
sure that, you know, there's so many privacy and security, like, safety concerns around

15:01.520 --> 15:04.120
having something that's always recorded you in the house.

15:04.120 --> 15:08.960
So, what are children's mental models, and how does learning about AI impact that?

15:08.960 --> 15:12.160
How does it change the way that they want to interact with these things in the future?

15:12.160 --> 15:17.920
And what were some of the results you saw in these before and after surveys?

15:17.920 --> 15:20.800
Mostly very strange things.

15:20.800 --> 15:28.120
So before, well, the studies that I did, like, two years ago, I interviewed four to ten

15:28.120 --> 15:33.160
year olds about AI, and the older kids, the eight to ten year olds, they were, like,

15:33.160 --> 15:34.160
solid.

15:34.160 --> 15:37.800
They knew exactly what they thought about AI, and, you know, they're like, it's not a

15:37.800 --> 15:41.400
person, it's not quite a toy, it's somewhere in the middle, like, they knew what they were

15:41.400 --> 15:42.400
doing.

15:42.400 --> 15:45.800
But the younger children, they kept telling me, I don't know, I don't know, I don't

15:45.800 --> 15:46.800
know.

15:46.800 --> 15:48.920
And I was getting frustrated, you know, it's a research, you're saying, like, I don't

15:48.920 --> 15:53.720
know, it's not going to get me a paper set into a conference.

15:53.720 --> 15:57.560
And so what I found before was pretty much the same children were like, I'm not really

15:57.560 --> 16:02.600
sure what this thing is, like, you know, we're all over the place.

16:02.600 --> 16:05.440
People saying, yes, people saying, no, people saying, oh, it said my name.

16:05.440 --> 16:09.320
So I guess it's pretty smart, or it didn't say my name, or it doesn't know my favorite

16:09.320 --> 16:10.320
song about the train.

16:10.320 --> 16:11.320
So it's not smart.

16:11.320 --> 16:16.120
You know, just things that were very hard to understand, which is interesting because

16:16.120 --> 16:19.400
these parents are also looking at this, and they're like, I'm not sure, but my child thinks

16:19.400 --> 16:22.200
about it, like, so you know, they keep calling it their best friends.

16:22.200 --> 16:25.720
So, you know, what is this relationship?

16:25.720 --> 16:29.520
But afterwards, there were some interesting differences.

16:29.520 --> 16:35.560
So I split my age grouping into, like, pre-K and kindergarten children.

16:35.560 --> 16:40.200
And so after they had learned about AI, the pre-K children were like, oh, now I understand

16:40.200 --> 16:41.200
it.

16:41.200 --> 16:42.640
So yeah, I would say this thing's pretty smart.

16:42.640 --> 16:46.080
And the kindergarten would children, like, oh, I don't think it's smart.

16:46.080 --> 16:47.080
anymore.

16:47.080 --> 16:48.080
Like, I thought it was smart before.

16:48.080 --> 16:52.160
But now that I get how it works, it's like, nope, like, okay.

16:52.160 --> 16:58.840
And then I also created, like, assessments, like, very simple, multiple choice questions

16:58.840 --> 17:04.440
to ask how much do children really understand about the activities that I've given them,

17:04.440 --> 17:07.520
and how much do they understand about the AI concepts?

17:07.520 --> 17:12.320
So the ones who did the best of the AI concepts were like, you know, not that I played with

17:12.320 --> 17:13.320
this thing.

17:13.320 --> 17:14.320
I'm like, yeah, it's kind of like a person.

17:14.320 --> 17:16.600
Like, it thinks sometimes in ways that I think.

17:16.600 --> 17:18.560
And yeah, it could be smarter than me, too.

17:18.560 --> 17:19.560
It could learn.

17:19.560 --> 17:21.920
It can get better and better.

17:21.920 --> 17:26.440
But interestingly, the children who didn't understand the activities very well were the

17:26.440 --> 17:27.440
opposite.

17:27.440 --> 17:28.440
They're like, nope, not smarter than me.

17:28.440 --> 17:29.600
It's just a toy.

17:29.600 --> 17:35.840
You know, it's fun, but it doesn't seem to be as alive or as human to me.

17:35.840 --> 17:43.040
And so I'm still undecided about what conclusions I want to draw from that.

17:43.040 --> 17:45.360
But I definitely think it's interesting that there is a big difference.

17:45.360 --> 17:49.200
Like, the children who understood AI versus the children who didn't understand AI saw

17:49.200 --> 17:52.880
the technology in very different ways.

17:52.880 --> 17:58.160
So at the very least, it sounds like teaching children about AI does cause something interesting

17:58.160 --> 18:01.240
to happen, something interesting, and hopefully not negative.

18:01.240 --> 18:07.840
So it seems like a good motivator to continue with the work and continue to explore this.

18:07.840 --> 18:13.920
I can't help but think that teaching adults about AI would have the same positive effects.

18:13.920 --> 18:21.080
You know, you think about kind of some of the mass media coverage of AI and some of

18:21.080 --> 18:25.320
the, you know, hysteria is that you read about.

18:25.320 --> 18:30.840
They often kind of belive this, you just lack of understanding.

18:30.840 --> 18:36.640
And have you thought about creating an adult version of your curriculum?

18:36.640 --> 18:43.240
So I haven't thought about it myself, but sort of, you know, even to discuss with you,

18:43.240 --> 18:49.880
we're mentioning about the more that people understand the less scared they might be.

18:49.880 --> 18:55.440
I think that the way that, you know, AI conversations are just playing out in society right

18:55.440 --> 18:56.440
now.

18:56.440 --> 18:57.640
It's kind of like a little bit of both.

18:57.640 --> 19:03.080
So there are some people, I guess, like myself and people in my lab who are like, you

19:03.080 --> 19:05.840
know, look at AI, look at how much good it can do.

19:05.840 --> 19:10.840
We can use it to build, you know, this good thing, that good thing, this good thing.

19:10.840 --> 19:15.920
And I would say that we're experts, but then there are also people who are extremely

19:15.920 --> 19:16.920
wary of AI.

19:16.920 --> 19:22.720
So they're like, they see this technology coming and they see all the negative ways that

19:22.720 --> 19:23.720
it can be used.

19:23.720 --> 19:31.200
And they're like cautious to like, you know, completely repulsed by it, like, no, even

19:31.200 --> 19:34.600
if AI can do good things, we shouldn't build it because in the wrong hands, it can just

19:34.600 --> 19:37.680
be too powerful and too destructive.

19:37.680 --> 19:44.560
And so I think that any adult education, AI, I think would probably have an interesting

19:44.560 --> 19:48.440
time trying to wrestle with that, you know, not trying to push people in any particular

19:48.440 --> 19:52.800
direction, but to let them come up with their own interpretation of how they think the

19:52.800 --> 19:54.600
technology should continue.

19:54.600 --> 19:57.320
Yeah, that's a really interesting point.

19:57.320 --> 20:00.600
I definitely get where you're coming from.

20:00.600 --> 20:04.680
You know, it's almost like the thing that you're afraid of, is it really the thing you

20:04.680 --> 20:05.680
should be afraid of?

20:05.680 --> 20:06.680
That's true.

20:06.680 --> 20:11.400
You know, it's not that there's nothing to be afraid of or not even afraid, but worried

20:11.400 --> 20:12.400
about.

20:12.400 --> 20:19.200
There are, there are genuine concerns, but, you know, it's, it's not necessarily kind of

20:19.200 --> 20:22.360
this terminator scenario in the next five years.

20:22.360 --> 20:23.680
I can tell you, it's a roboticist.

20:23.680 --> 20:29.160
There will be no terminator, because robots love to break, like, they're just not going

20:29.160 --> 20:30.160
to work out.

20:30.160 --> 20:34.760
There could be a, what is the movie, her, like, the thing where it's like all of mine,

20:34.760 --> 20:41.080
like, that's a bit more likely, but I wish you guys no terminators are going to come.

20:41.080 --> 20:47.880
Let's maybe talk about the curriculum that you developed as a part of pop bots.

20:47.880 --> 20:52.720
What are the core AI concepts that you're trying to teach these children?

20:52.720 --> 20:59.720
So the three that I started with were knowledge based expert systems, supervised machine

20:59.720 --> 21:02.360
learning and generative AI.

21:02.360 --> 21:06.280
And I started with these because they seem to be the most relevant to what children

21:06.280 --> 21:07.280
were experiencing.

21:07.280 --> 21:12.440
So a lot of their simple toys were using these, you know, kinds of concepts in them, and

21:12.440 --> 21:17.080
I could easily make connections between, when you toyed as this, this is what's happening

21:17.080 --> 21:18.080
underneath it.

21:18.080 --> 21:22.800
And to me, that seemed like the most important things to teach children at first.

21:22.800 --> 21:25.600
So, you know, knowledge based systems.

21:25.600 --> 21:30.560
Let's go through each of those and maybe you can provide an example of the way a child

21:30.560 --> 21:39.520
might experience that in their kind of everyday toys and how you introduce that in the curriculum.

21:39.520 --> 21:44.280
Knowledge based systems for rule based expert systems or expert systems, they have multiple

21:44.280 --> 21:49.040
names, often come up in natural language processing.

21:49.040 --> 21:56.000
Now a lot of NLP also uses, you know, deep learning, but, you know, in the past, you know,

21:56.000 --> 21:59.720
back in the good old days, it came up a lot in natural language processing as well as

21:59.720 --> 22:05.040
medical diagnosis and even the video game characters that, you know, aren't the main characters

22:05.040 --> 22:10.360
but the ones that, I don't know, if you play a lot of RPG games when you're battling

22:10.360 --> 22:15.600
the random townsmen, like those kinds of characters were all controlled by knowledge based

22:15.600 --> 22:20.720
expert systems or rule based systems, and so it was easy to, you know, talk to children

22:20.720 --> 22:26.360
about, it's like, oh, when you're playing tic-tac-toe against your smart computer or when

22:26.360 --> 22:30.920
you're doing that video game or when you're talking to Alexa, there's probably a bit of

22:30.920 --> 22:35.880
this going on underneath and the way that we did the activity was we played rock paper

22:35.880 --> 22:36.880
scissors.

22:36.880 --> 22:41.800
So, rock paper scissors, you need knowledge, like there are rules, which is, you know,

22:41.800 --> 22:47.840
arts, scissors, paper beads, rock, et cetera, and so children would literally program these

22:47.840 --> 22:50.560
rules into the robot.

22:50.560 --> 22:54.760
All of the interfaces are completely picture-based because I was working with children who were

22:54.760 --> 22:58.560
so young that they couldn't read, so it was an interesting design challenge to say,

22:58.560 --> 23:04.760
okay, powerful AI, absolutely no words whatsoever, and like no math and stuff like that.

23:04.760 --> 23:11.640
So children pretty much put, like, pictures of a paper hand and then an air will like greater

23:11.640 --> 23:15.760
than sign and then a rock, which means paper beads rock.

23:15.760 --> 23:20.320
And so they put in all the rules and now the robot has its knowledge base and it can

23:20.320 --> 23:22.960
use that to make decisions about what to play.

23:22.960 --> 23:27.980
So when the child's actually playing against the robot, then the robot will kind of keep

23:27.980 --> 23:32.080
packing their moves and it'll use that to say, oh, well, I think that you're going

23:32.080 --> 23:37.680
to play paper next and you told me that scissors beats paper, so I'm going to play scissors

23:37.680 --> 23:43.400
and then it's revealed, you know, did the child actually play a paper and a lot of times,

23:43.400 --> 23:46.920
you know, after a couple rounds of the game, yeah, the robot's pretty good at guessing

23:46.920 --> 23:52.160
and they're like, oh my god, the robot got so smart, but what's really cool about that

23:52.160 --> 23:57.400
is it didn't start off smart, it actually starts off losing a lot and as it keeps going,

23:57.400 --> 23:58.400
it gets smarter.

23:58.400 --> 24:04.120
The children start to see how this intelligence didn't disappear, it was learned over time.

24:04.120 --> 24:08.120
Also the children gave the robot the rules to the game, so the children actually played

24:08.120 --> 24:13.880
at part in helping the robot become intelligent, what's even more funny is that the kids

24:13.880 --> 24:17.360
will be like, I'm going to cheat and they'll like switch all the rules around backwards

24:17.360 --> 24:21.440
and the robot will be like, well, I guess that paper beats scissors, so does that mean

24:21.440 --> 24:26.240
that I win and the child's like laughing their head off like, ha, ha, ha, I tricked you.

24:26.240 --> 24:31.680
But that's actually an interesting point because, you know, expert-based rule systems, one

24:31.680 --> 24:35.720
of the ethical issues is what if the rule that you're teaching isn't correct?

24:35.720 --> 24:40.120
So children even get to explore that idea like, okay, if I have a car that's driving by

24:40.120 --> 24:43.640
itself and I teach it the wrong rules, what happens?

24:43.640 --> 24:48.120
And you can see like this look of realization, like, you know, move over the child's face

24:48.120 --> 24:50.480
and they're like, oh my god, that would be so bad.

24:50.480 --> 24:54.120
I'm like, yes, like you're understanding, like, you know, the impact, the real world,

24:54.120 --> 25:00.840
the impact of AI. And then also, children will teach the robot how to react to winning,

25:00.840 --> 25:05.240
losing and getting a tie. So, you know, like, the robot can be a sore winner and like,

25:05.240 --> 25:09.840
every time it wins, it's like, ha, ha, win, you lose. And like, it makes the sparring

25:09.840 --> 25:14.880
sound. So that's like, one iteration that you can have it win humbly. It's like, oh,

25:14.880 --> 25:18.760
that was a good game. So children are also, you know, it's kindergarten, it's free

25:18.760 --> 25:25.120
school. Children also need to have some way that, you know, social interaction, social

25:25.120 --> 25:29.560
learning is coming into this as well. So that's a part of it too.

25:29.560 --> 25:38.080
I'm curious in this first part, the rules-based systems, when you, when the child was programming

25:38.080 --> 25:44.160
the robot, teaching the robot how to respond via these rules, were you also introducing

25:44.160 --> 25:49.120
some notion of probabilistic systems or responses?

25:49.120 --> 25:54.840
So in the sense that the robot was learning over time, what the child was most likely

25:54.840 --> 26:01.200
to do next, yes. It was a little bit tricky, because probability doesn't come up for

26:01.200 --> 26:09.400
a while in, like, early education, but we would make these rule trackers, or these game

26:09.400 --> 26:14.160
trackers, rather, where the children would write down, you know, what moves they put. And

26:14.160 --> 26:19.280
the robot would say, well, I think you're going to put paper next, because you put paper,

26:19.280 --> 26:23.440
like, three times out of the five times that we just played, and the child can look back

26:23.440 --> 26:27.520
at what they did, and they can start to see, oh, yeah, like, one, two, three, like, those

26:27.520 --> 26:34.040
are the times that I picked paper. So yeah, that's the sense in which I envisioned it,

26:34.040 --> 26:39.920
yeah. Yeah, it's like, it's not like super deep, but yeah, that's about how far we can

26:39.920 --> 26:46.360
go. But you kind of raised that as, like, that was made explicit in the curriculum, thinking

26:46.360 --> 26:55.960
about, you know, these, you know, percentages or frequency types of numbers. Yeah, absolutely.

26:55.960 --> 27:00.160
And the entire time the robot is, like, saying these things, it's pointing out what its

27:00.160 --> 27:05.320
knowledge is, and it's explaining why it's making decisions, so that the child can understand

27:05.320 --> 27:09.760
it. And I think my hope is that, you know, when they get older, if they have another AI

27:09.760 --> 27:14.520
class, they can revisit these ideas and actually learn about probability, and it all starts

27:14.520 --> 27:19.360
to make sense. So, like, even reinforcing those ideas later.

27:19.360 --> 27:23.920
And so just a point of clarification, you are referring back to this idea of a robot.

27:23.920 --> 27:28.360
Is this an entirely software based robot just on the smartphone, or is there a hardware

27:28.360 --> 27:34.400
component as well? Ah, yes, I should have explained that. So the robot is a mobile phone with

27:34.400 --> 27:40.000
this social robot technology that we developed in our lab, so it can talk. It has all these

27:40.000 --> 27:46.840
really cute, fun animations. I can listen to you. It has a camera, but around it, I've

27:46.840 --> 27:53.320
built a Lego body. So there's two different bodies that I'm working on now. One uses

27:53.320 --> 27:59.440
Lego We Do, which is like this $200 motor kit, and you can use normal Legos, and then you

27:59.440 --> 28:04.160
could add these motors to it. So now the robot can move and dance, but because it's Lego,

28:04.160 --> 28:08.440
you can change the way that it looks. So sometimes it's a car. Some children really wanted

28:08.440 --> 28:13.000
to play with one that's spun around, so they can sort of have total control over the way

28:13.000 --> 28:16.800
that their robot is. So they're programming it, they're building it, like all of it is

28:16.800 --> 28:21.960
brought down to the child's level. And then the other one, I imagine, is Arduino. So

28:21.960 --> 28:27.200
now I'm thinking about slightly older kids and even more fun things to build, so I'm

28:27.200 --> 28:31.320
building an Arduino platform for it too. So then back to these concepts, we just talked

28:31.320 --> 28:36.480
about the rules-based types of systems. Do you mention supervised machine learning as

28:36.480 --> 28:42.560
well? Yeah. So supervised machine learning comes up in YouTube kids, which surprisingly

28:42.560 --> 28:48.200
a lot of children were interacting with. So I would ask them, how does YouTube know

28:48.200 --> 28:54.040
which movie you want to watch next? And they're like, oh, it just picks whatever is random.

28:54.040 --> 28:57.200
And I'm like, no, it's not random. It usually picks things that are kind of like the video

28:57.200 --> 29:00.880
you just watch. And they're like, oh, yeah, I guess you're right. And so we can talk

29:00.880 --> 29:06.720
about supervised machine learning. So that's when you label some things, for example, as

29:06.720 --> 29:12.040
good and bad. So in the case of YouTube kids, children are labeling things as things

29:12.040 --> 29:16.640
that I want to watch by watching it. And then they can also give extra feedback, thumbs

29:16.640 --> 29:20.960
up, thumbs down, so that the robot, sorry, not the robot. YouTube's algorithm can learn

29:20.960 --> 29:28.360
better. But it's often used for like recommender systems. So it can be YouTube, it can be Netflix.

29:28.360 --> 29:32.640
Children don't have email yet, but sometimes they can kind of get what I mean if I throw

29:32.640 --> 29:37.600
that in there. But for children what we do is we sort foods into healthy and unhealthy

29:37.600 --> 29:43.040
groups. So rock paper scissors is nice because I have street rules. But if you want to

29:43.040 --> 29:47.000
teach a robot about which foods are healthy and unhealthy, you know, I have children think

29:47.000 --> 29:49.760
about like, how many foods are there? Like how many foods would you have to teach the

29:49.760 --> 29:53.640
robot about? And after they kind of like hit the dirty, they backs out and they're like,

29:53.640 --> 29:57.760
oh my god, that's so many foods. So I'm like, okay, there's a better way. We can give

29:57.760 --> 30:03.240
the robot a few examples and it can learn to make guesses on its own. So sort of on the

30:03.240 --> 30:07.880
back end, something I do beforehand, the robot has this database where it has like the

30:07.880 --> 30:13.120
color of foods with food groups and how much sugar it has all of these different features.

30:13.120 --> 30:18.480
And then it's going to use a K-nearest neighbors algorithm to sort of say, well, this food

30:18.480 --> 30:24.080
has this many features similar to this other food. So maybe these two are nearest neighbors

30:24.080 --> 30:31.520
as opposed to this other food. So a good example is like bananas would be more similar to

30:31.520 --> 30:38.960
lemons than chocolate. So what I have children do, they have this like list of 20 foods and

30:38.960 --> 30:44.600
I say we're going to label two of them. So they label, you know, either both of them good

30:44.600 --> 30:49.080
or both of them bad. They take whatever food they want. And then we're say, okay, now let's

30:49.080 --> 30:53.760
ask the robot to guess like whether this food is healthy or not. And so they start to see

30:53.760 --> 30:58.560
like, okay, if I tell the robot that strawberries and tomatoes are healthy and then I ask about

30:58.560 --> 31:01.800
chocolate, it's going to think chocolate's healthy too because I haven't given it any

31:01.800 --> 31:05.640
bad examples. So I have to do better. I'm like, okay, let's teach the robot that chocolate

31:05.640 --> 31:09.800
is not healthy. So now we have strawberries and tomatoes and the good chocolate and the

31:09.800 --> 31:15.880
bad. Let's ask it about ice cream. And the robot can say, well, ice cream is probably

31:15.880 --> 31:19.160
closer to chocolate than it is to the other things because they both, you know, are in

31:19.160 --> 31:23.680
the sweet section. They have a lot of sugar. So it's chocolate unhealthy too. And boom,

31:23.680 --> 31:28.520
like magically the robot seems intelligent, seems like it's learning. It ends up being that

31:28.520 --> 31:34.600
after we teach the robot about like five foods, then it can sort of guess the other 15 foods

31:34.600 --> 31:39.680
that remain. But it all depends on how good the training set is. So I don't use the

31:39.680 --> 31:44.960
words training set with a five year old. But the idea is still the same. It's like, so

31:44.960 --> 31:49.760
we only told it about these five foods and it learned about these 15 foods. Like would

31:49.760 --> 31:53.600
of all the five foods have been good would it do a good job? What if they were all bad?

31:53.600 --> 31:58.640
Would that do a good job? And they can start to seeing how the robot, you know, needs

31:58.640 --> 32:02.400
certain examples of certain quality, like if we only teach it about red foods and we

32:02.400 --> 32:06.680
ask it about blue foods, it's probably going to be a bit confused because like you haven't

32:06.680 --> 32:12.800
given it a good enough training set. That one's usually really fun. And then again, of course

32:12.800 --> 32:16.440
children want to trick the robot. It's like, well, I like chocolate. So I'm putting chocolate

32:16.440 --> 32:20.400
on the good side and like, okay, well, we can do that, of course. How does that impact

32:20.400 --> 32:25.640
the robot? And then we can discuss that as well. And then the last activity is genitive

32:25.640 --> 32:32.400
AI. And this was one that I thought was really important because AI doesn't just follow

32:32.400 --> 32:37.560
rules and it doesn't just classify things and make rules. Sometimes it is creative and

32:37.560 --> 32:43.720
it can be used in art. This particular activity is about music. So first children give the

32:43.720 --> 32:49.760
robot parameters about different emotions and how they would sound as music. So happy

32:49.760 --> 32:56.040
music sounds, you know, kind of fast and upbeat. And it also sort of goes up in core progression.

32:56.040 --> 33:02.080
So like rather than going like during, or it's going to go up. So they teach the robot

33:02.080 --> 33:06.640
that by sliding these two bars, like core progression up and music fast. And then they

33:06.640 --> 33:12.840
do sad. And they're like core progression down music, maybe a little bit slow and then

33:12.840 --> 33:18.440
excited. So core progression up, music fast or scared, core progression down, but music

33:18.440 --> 33:24.280
fast. And they teach the robot about what different emotions should sound like a song.

33:24.280 --> 33:30.160
And then I drive all the teachers in the room crazy. We start playing music with the robots.

33:30.160 --> 33:37.160
So children have this piano where they can play a song. And then the robot will take whatever

33:37.160 --> 33:42.400
song they make and will remix it according to the different emotions. So it's just like

33:42.400 --> 33:48.160
really noisy. But a lot of fun is like children are like, you know, playing songs and hearing

33:48.160 --> 33:54.080
the robot play their song back and sort of like going back and forth with this turn taking.

33:54.080 --> 34:00.600
And then, you know, we ask questions like, so did the robot song sound like your song?

34:00.600 --> 34:04.560
So if you tell it not to change anything, yes, if you tell it to go faster, it'll just

34:04.560 --> 34:08.280
change the progression a little bit. If you tell it to go slower, it'll, you know,

34:08.280 --> 34:12.240
make it a bit slower. Sometimes it'll add new notes. If you tell the core progression

34:12.240 --> 34:17.960
to go up, but you play a down court progression. And then how does that impact the emotions?

34:17.960 --> 34:22.200
It's like, oh, well, it seems like it's kind of picking randomly, but all the happy songs

34:22.200 --> 34:28.280
kind of start to sound the same. So it's really cool to watch children sort of do this

34:28.280 --> 34:32.240
less structured. Like, you know, it's not bright, long answer. It's how does it sound

34:32.240 --> 34:36.840
and they're making music. And then they get up and they play a class orchestra. And then

34:36.840 --> 34:43.240
we turn the tablets off with the rest of the activity after all of that stimulation.

34:43.240 --> 34:51.120
Yeah. So to be clear in this, this third concept, how are you getting at the AI element

34:51.120 --> 34:59.600
of what's happening? So examples of this AI come up. Like, if you look at some of Google's

34:59.600 --> 35:05.240
AI experiments, they have this piano where you can play music along with a computer. But

35:05.240 --> 35:09.720
the question is, how does that computer know what to play? And so in this activity, children

35:09.720 --> 35:14.880
are actually setting parameters for how the computer should change the song to make it

35:14.880 --> 35:22.120
sound a particular way. So if I give you, if they give the robot rather an input with

35:22.120 --> 35:27.000
three notes, C-D-E, and they tell it to make it faster and happier than the robot should

35:27.000 --> 35:33.240
return something according to the parameters faster. And maybe it'll go up C-E-G, like

35:33.240 --> 35:39.120
even higher than the child's output. So they start to see how they can use AI to create

35:39.120 --> 35:47.080
and to create new things. And then you mentioned previously the, some of the surveys you did

35:47.080 --> 35:54.560
before and after, was that before and after children go through this curriculum? Are there

35:54.560 --> 35:59.440
additional observations that you made about their experiences and what they've learned

35:59.440 --> 36:04.240
about having gone through this curriculum beyond what we've already talked about? Yeah. So

36:04.240 --> 36:09.960
there was the before and after about how children feel about AI and about robots. I also did

36:09.960 --> 36:16.880
a before and after about how children feel about engineers. So one of my, I guess, pet

36:16.880 --> 36:22.560
peas as an engineer is that, you know, there's a lot of emphasis on science and mathematics

36:22.560 --> 36:27.200
and STEM, but often technology and engineering is a lot harder to do and so it gets less

36:27.200 --> 36:32.440
attention. So as I went through the curriculum, I was hoping that children would have a better

36:32.440 --> 36:37.880
sense of what engineers do and why engineering is fun. And unfortunately, they did it. And

36:37.880 --> 36:45.440
I think that, you know, a lot of that is because it was a very new concept to them. So to

36:45.440 --> 36:51.600
tell a hilarious story, the first thing that I went in, I was like, okay, who here knows

36:51.600 --> 36:57.360
who an engineer is and, you know, in this classroom of 20, like two children raised their hand.

36:57.360 --> 37:00.840
And I was like, okay, that's not good. We have to do better than that. And so I pointed

37:00.840 --> 37:05.000
to one of the children and I said, okay, you tell me, what is an engineer? Tell us all.

37:05.000 --> 37:09.960
He's like, an engineer is someone who drives a train. And I was like, like, we're really,

37:09.960 --> 37:17.680
really far further than I even thought we were. So, you know, part of doing this research

37:17.680 --> 37:22.080
that I find, you know, like personally enjoyable is that I get to say, well, I'm an engineer.

37:22.080 --> 37:25.720
I'm an engineer because I build things and I build things to help people and starting

37:25.720 --> 37:32.280
to have children, you know, think about that as a different new career path. I kind of

37:32.280 --> 37:36.400
wish going back that I had built more of that into this curriculum that I built. So,

37:36.400 --> 37:39.360
you know, these activities were fun and they were playing games. But at the end of the

37:39.360 --> 37:43.600
day, they didn't get to see how the things they were building could be useful or how they

37:43.600 --> 37:47.960
could help other people or how they could bring joy to other people. And I think that's

37:47.960 --> 37:51.360
why you know at the end when I was like, okay, who wants to be an engineer? I still got

37:51.360 --> 37:56.720
crickets because it's like, all right, we need to do a better job of helping children

37:56.720 --> 38:01.640
sort of like see themselves as this, but also see the value of it in society. And then

38:01.640 --> 38:06.920
also just, like I said, I did a sort of AI assessment. So how much did children learn

38:06.920 --> 38:12.160
about these things? The AI assessment was like 10 questions, all about the different

38:12.160 --> 38:16.680
activities. And some of them are kind of tricky. It was like, I think I mentioned before,

38:16.680 --> 38:20.440
if you only teach the robot about good foods, where will it take you to chocolate goes?

38:20.440 --> 38:24.320
For five-year-old, you're like, of course, everyone knows chocolate's unhealthy, but it's

38:24.320 --> 38:31.320
very difficult for them to see like, okay, wait, but this AI algorithm only knows foods

38:31.320 --> 38:36.080
that I taught it, only foods that I've labeled. So it'll always use those labels to make

38:36.080 --> 38:40.200
its guesses. So it's really cool to see like a lot of children start to get those things

38:40.200 --> 38:46.560
right because it kind of like blew the developments of psychology literature, you know, out of

38:46.560 --> 38:49.160
the water. They were like, I'm not sure if children could do this kind of reasoning yet.

38:49.160 --> 38:51.640
I'm like, no, they did it. It was awesome.

38:51.640 --> 38:52.880
That's fantastic.

38:52.880 --> 38:58.360
Yeah. So yeah, anyways, 10 questions. Some of them a bit tricky. And I think the median

38:58.360 --> 39:05.160
score was like 70%. So I mean, obviously, we shouldn't assess children to have a lead.

39:05.160 --> 39:09.400
That's probably not healthy for them, but they understood a good amount of what was presented

39:09.400 --> 39:15.440
in front of them. And I think that's really encouraging and important. It's, I mean, the

39:15.440 --> 39:20.600
same with like early computer science education. It was very easy to say children can't understand

39:20.600 --> 39:26.240
this is too complex, but the way that something is designed, it can be made accessible to

39:26.240 --> 39:33.360
children. So, you know, right now I'm working on a deep reinforcement learning activity.

39:33.360 --> 39:37.960
So pretty much we're going to build agents that can play snake. So first, we're going

39:37.960 --> 39:43.280
to hand code it. And then we're going to use a simple neural net where we like give

39:43.280 --> 39:47.400
it a bunch of examples. And then we're going to have it do deep learning. And I'm building

39:47.400 --> 39:52.040
very confident that these children can understand it because, you know, if you just are able

39:52.040 --> 39:57.800
to break something down enough, they can get it. So if you're a deep RL person, I'm going

39:57.800 --> 40:03.160
to have some five year olds coming for your job. Pretty. So get ready.

40:03.160 --> 40:11.440
Nice. Nice. This is awesome work. Where do you go from here? So this was really the center

40:11.440 --> 40:17.800
of your master's thesis. Where do you see it going beyond that?

40:17.800 --> 40:22.680
There's so many different things that I want to do and so much work to be done. So some

40:22.680 --> 40:28.400
really things that are going on that there are others in my lab who I was kind of like

40:28.400 --> 40:33.640
the person to go and try this and see if it works or not. So now that it works, other people

40:33.640 --> 40:40.040
in my lab are also trying out their own experiments. One of my lab mates, Athena is doing this work

40:40.040 --> 40:44.400
around, you know, when children are learning with AI, how can that impact their creativity?

40:44.400 --> 40:48.040
So they're not just learning about AI anymore. They're also learning to be more creative

40:48.040 --> 40:54.000
and to be explorative as they're learning, which will have like huge benefits for them,

40:54.000 --> 41:00.520
you know, beyond just learning a particular skill. Another student in my group, Blakely,

41:00.520 --> 41:04.920
not student in my group, like I'm their professor. Another one of my lab mates, Blakely is working

41:04.920 --> 41:11.120
on AI ethics curriculum. So really helping children be able to understand the ethics behind

41:11.120 --> 41:15.920
every AI decision so that they can critically evaluate the things that are on them. But

41:15.920 --> 41:19.720
also when they're building things, you know, why teach children to build something if

41:19.720 --> 41:22.760
they don't know how to build it ethically. Like at the same time, they should be thinking

41:22.760 --> 41:27.120
about both of these things. So I'm really excited about that work. Personally, I've gotten

41:27.120 --> 41:31.040
a lot of feedback from teachers like, this is great. I have no idea about anything

41:31.040 --> 41:35.320
that they had. So can you teach me? So I'm trying to figure out the problem of how can

41:35.320 --> 41:40.520
we actually make this something that teachers can like use and feel empowered to use and

41:40.520 --> 41:46.520
not scared of so that it can really get into classrooms and get into the spaces and

41:46.520 --> 41:53.160
also more that I used to work in. And then I think also just making more cool activities.

41:53.160 --> 41:57.600
In my dream of dreams, this will become like this big online platform and children everywhere

41:57.600 --> 42:02.840
can learn about AI and ways that are meaningful to them. So you know, there has to be a lot

42:02.840 --> 42:08.160
more content behind it beyond these three activities. What other things can children learn

42:08.160 --> 42:13.720
and what are other metaphors that make sense for them, right? So yeah, that's all the

42:13.720 --> 42:24.400
things I'm going to do. Yeah. Awesome. Awesome. Are there things that you have identified

42:24.400 --> 42:29.680
that you need? Meaning if there's, you know, some potential partner out there that,

42:29.680 --> 42:35.640
you know, someone in our listening community might, you know, be connected to anything

42:35.640 --> 42:42.360
come to mind in that regard? I mean, yes. I can plug for things. So I think one thing

42:42.360 --> 42:47.320
that would be really awesome is to have an AI person, somebody feels comfortable with

42:47.320 --> 42:54.600
AI who's really passionate about teaching this, who I can sort of help get started with

42:54.600 --> 42:58.680
their own activities. So I'm doing that with teachers right now and I think the biggest

42:58.680 --> 43:02.600
problem is that they're not comfortable with AI and there's a lot of work that I have

43:02.600 --> 43:07.720
to do to get them there. So I'm wondering how might it be different if I take an AI person

43:07.720 --> 43:14.080
and start to give them tools to be teachers? That would be really cool. Also, if you kind

43:14.080 --> 43:19.920
of just want to chime things out with your kids and experiment and ask questions, some

43:19.920 --> 43:25.360
of the papers that are linked in the website that we have have actual AI resources that

43:25.360 --> 43:29.840
parents can go on and chime right now. I'm just like short activities based on scratch.

43:29.840 --> 43:35.680
So unfortunately, you have to have a kid between the ages of seven or maybe over there.

43:35.680 --> 43:39.480
But there are already things that exist that people can try that I highly recommend them

43:39.480 --> 43:43.720
try and give a feedback on. Well, Randy, thanks so much for taking

43:43.720 --> 43:50.080
the time to share what you're working on with us is really cool stuff and I'm looking

43:50.080 --> 43:55.520
forward to seeing how it evolves. Oh, thank you. I really appreciate the opportunity.

43:55.520 --> 44:04.240
Thanks. All right, everyone. That's our show for today. For more information on Randy

44:04.240 --> 44:11.320
or any of the topics covered in the show, visit twimmelai.com slash talk slash 225.

44:11.320 --> 44:17.200
For more information on the black and AI series, visit twimmelai.com slash black and AI

44:17.200 --> 44:36.840
team. As always, thanks so much for listening and catch you next time.

