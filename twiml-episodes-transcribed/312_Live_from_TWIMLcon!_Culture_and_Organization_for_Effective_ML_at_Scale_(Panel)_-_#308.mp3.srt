1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:22,360
I'm your host Sam Charrington.

3
00:00:22,360 --> 00:00:23,680
What's up everyone?

4
00:00:23,680 --> 00:00:28,040
This is Amari, producer of the Twimal AI Podcast.

5
00:00:28,040 --> 00:00:29,040
What about now?

6
00:00:29,040 --> 00:00:32,120
You're probably wondering, where's Sam?

7
00:00:32,120 --> 00:00:37,880
Well on the heels of such an amazing event, we decided it was time for him to take a break.

8
00:00:37,880 --> 00:00:40,040
So he's currently on vacation.

9
00:00:40,040 --> 00:00:45,360
But as they say, the show must go on.

10
00:00:45,360 --> 00:00:51,160
Today we continue our Twimal Con coverage with the first of our panel discussions, culture

11
00:00:51,160 --> 00:00:55,840
and organization for effective machine learning at scale.

12
00:00:55,840 --> 00:01:01,400
In this discussion, moderator Maribel Lopez, founder and principal analyst of Lopez

13
00:01:01,400 --> 00:01:09,440
research, is joined by panelists Jennifer Prinky, founder and CEO of Electio, Eric Colson,

14
00:01:09,440 --> 00:01:16,160
emeritus chief algorithms officer of stitchfix, and partist Norzad, data science manager

15
00:01:16,160 --> 00:01:17,800
at Twitter.

16
00:01:17,800 --> 00:01:22,320
Before we move on, I'd like to extend a huge thank you to everyone that joined us at

17
00:01:22,320 --> 00:01:24,880
Twimal Con earlier this month.

18
00:01:24,880 --> 00:01:29,600
We had an amazing time and we can't wait to see you all again next year at Twimal Con

19
00:01:29,600 --> 00:01:31,600
2020.

20
00:01:31,600 --> 00:01:38,280
And now on to the show.

21
00:01:38,280 --> 00:01:44,960
Without further ado, I would like to welcome up the moderator of our first panel, Maribel

22
00:01:44,960 --> 00:01:48,040
Lopez, founder of Lopez research.

23
00:01:48,040 --> 00:01:49,040
Maribel?

24
00:01:49,040 --> 00:01:54,320
Hi, I'm Maribel Lopez, so I'm the founder of Lopez research and also the founder of

25
00:01:54,320 --> 00:01:59,560
data for betterment with this nonprofit organization that helps companies figure out how to move

26
00:01:59,560 --> 00:02:04,720
forward in the land of AI and giving them information about career change and other things

27
00:02:04,720 --> 00:02:05,720
in that area.

28
00:02:05,720 --> 00:02:09,080
And I could think of no better thing to do than to be here today talking about culture

29
00:02:09,080 --> 00:02:14,280
and organization and how to do things like effectively have machine learning at scale.

30
00:02:14,280 --> 00:02:16,800
So without further ado, I'm going to jump right in it.

31
00:02:16,800 --> 00:02:21,360
One of the things that we're seeing with organizations is that there's a tremendous amount

32
00:02:21,360 --> 00:02:24,920
of talk about artificial intelligence in organizations.

33
00:02:24,920 --> 00:02:30,160
Not necessarily a lot of companies or executives or a line of business people actually really

34
00:02:30,160 --> 00:02:34,560
understanding what that means, how to do it.

35
00:02:34,560 --> 00:02:37,560
Sometimes it's like I'm just going to go out and buy something with machine learning

36
00:02:37,560 --> 00:02:39,400
in it and see how that works out.

37
00:02:39,400 --> 00:02:45,240
So one of the challenges I think that all of you in the organization are facing is how

38
00:02:45,240 --> 00:02:50,880
do we get the different groups in an organization to effectively work together?

39
00:02:50,880 --> 00:02:55,640
So if we look at this, if you have a data science team, data engineering, how do you actually

40
00:02:55,640 --> 00:03:00,080
work with line of business managers, how do you work with the C-suite so that you can

41
00:03:00,080 --> 00:03:05,920
actually get to delivering business value with AI technologies and tools?

42
00:03:05,920 --> 00:03:09,560
So I thought that might be a good first place to start as we're talking about culture

43
00:03:09,560 --> 00:03:16,560
and organization because technically you probably can't decide what kind of organization

44
00:03:16,560 --> 00:03:20,240
you want to switch it to, centralize or decentralize, but you will have to try to make

45
00:03:20,240 --> 00:03:23,400
this effective and efficient and delivering business value.

46
00:03:23,400 --> 00:03:26,600
So maybe we can start there, maybe we will start with Jennifer.

47
00:03:26,600 --> 00:03:32,240
You want to give us a few pointer tips or how you've seen this work in terms of talking

48
00:03:32,240 --> 00:03:35,560
to the different groups within an organization.

49
00:03:35,560 --> 00:03:43,280
Yeah, no, I saw as you stated before completely agree that this is certainly probably a

50
00:03:43,280 --> 00:03:46,280
bigger challenge than the technology itself, right?

51
00:03:46,280 --> 00:03:51,520
So the first thing I would say is like first of all, I have very good news is that there

52
00:03:51,520 --> 00:03:56,480
is a very easy way to solve the way you communicate with the C-suite or product management and

53
00:03:56,480 --> 00:04:02,040
that is by educating the people you're going to communicate with.

54
00:04:02,040 --> 00:04:07,800
And the easiest way to do this is to go all the way to an MBA program and teach them and

55
00:04:07,800 --> 00:04:14,400
basically prepare the next generation of leaders and product managers to be ready to interact

56
00:04:14,400 --> 00:04:19,640
with people who are going to handle machine learning research and machine learning product,

57
00:04:19,640 --> 00:04:20,640
right?

58
00:04:20,640 --> 00:04:24,920
So we're basically in survival mode right now, it means that we have to figure out a way

59
00:04:24,920 --> 00:04:32,240
to make it until this next generation of leaders becomes, comes in place and these are actually

60
00:04:32,240 --> 00:04:35,320
the people who are either reporting to or working with, right?

61
00:04:35,320 --> 00:04:40,760
I mean, so in the meantime, obviously, there are different segments and so to me personally

62
00:04:40,760 --> 00:04:46,080
like one of the biggest challenges I always had was like, define what product management

63
00:04:46,080 --> 00:04:50,480
means for machine learning, but that's a long, long story probably.

64
00:04:50,480 --> 00:04:55,120
Okay, we're going to circle back to defining what project management means in machine learning

65
00:04:55,120 --> 00:04:56,120
in a second.

66
00:04:56,120 --> 00:04:58,800
Eric, maybe have some thoughts on this?

67
00:04:58,800 --> 00:04:59,800
Well, sure.

68
00:04:59,800 --> 00:05:05,720
I think one thing you can do to align yourself with the C-suite is you can become the C-suite.

69
00:05:05,720 --> 00:05:09,800
So at StitchFix, we do have a special arrangement, the chief algorithms officer, an officer

70
00:05:09,800 --> 00:05:14,320
of the company, up their peers with the CFO and CTO and so forth.

71
00:05:14,320 --> 00:05:18,280
So that does help change things easier to get alignment as peers rather than somewhere

72
00:05:18,280 --> 00:05:21,120
buried down the org structure.

73
00:05:21,120 --> 00:05:26,120
With officer representation, what happens is you become accountable, you're not a supportive

74
00:05:26,120 --> 00:05:30,360
team to other folks, instead you have your own goals, revenue goals or other metrics,

75
00:05:30,360 --> 00:05:31,360
right?

76
00:05:31,360 --> 00:05:33,480
That you are accountable for not supporting another team.

77
00:05:33,480 --> 00:05:38,240
And so that changes things dramatically on how the company embraces data science and

78
00:05:38,240 --> 00:05:41,560
the other thing that happens with officer representation is you get influence.

79
00:05:41,560 --> 00:05:46,760
You can influence the way companies work and you can do away with the notion of big ideas

80
00:05:46,760 --> 00:05:51,400
and instead companies have hypotheses that are put to the test and it, and once the company

81
00:05:51,400 --> 00:05:55,840
learns after a short period of time that they're wrong a lot, then they set the change of

82
00:05:55,840 --> 00:05:59,680
the way they do even engineering where they're not going to build a certain feature, they're

83
00:05:59,680 --> 00:06:04,440
going to build a general platform to try 100 features because Lord knows the first one's

84
00:06:04,440 --> 00:06:05,440
going to be wrong.

85
00:06:05,440 --> 00:06:09,400
And you need to iterate and do things a little more generally.

86
00:06:09,400 --> 00:06:14,200
So that's one way to go about it is if you get the actual officer representation and

87
00:06:14,200 --> 00:06:18,400
it's, I'll be honest, it's a much easier way to do it than to try to manage it from down

88
00:06:18,400 --> 00:06:19,400
below.

89
00:06:19,400 --> 00:06:25,120
So if you're fortunate enough to be in that situation, it actually is something I highly recommend.

90
00:06:25,120 --> 00:06:31,480
Yeah, I know part of you actually are in a company that has a lot of data behind it and

91
00:06:31,480 --> 00:06:36,080
a lot of data science behind it, but there's also a lot with the marketing organization

92
00:06:36,080 --> 00:06:37,400
and sales.

93
00:06:37,400 --> 00:06:41,840
So how have you looked at approaching this problem in your organization?

94
00:06:41,840 --> 00:06:44,560
Absolutely.

95
00:06:44,560 --> 00:06:51,720
The way I approached working with sales and marketing is kind of the same way as I think

96
00:06:51,720 --> 00:06:56,280
about data science working with these various product teams.

97
00:06:56,280 --> 00:07:03,720
Nowadays, you have continuous integration and you have online products, products in the

98
00:07:03,720 --> 00:07:04,720
cloud.

99
00:07:04,720 --> 00:07:12,000
So products are being launched almost every day and product is changing every day.

100
00:07:12,000 --> 00:07:16,920
And because of that, the way data science is working with the engineering teams needs

101
00:07:16,920 --> 00:07:25,640
to be fully integrated, looking at every feature, launch, every experiment, every kind of change

102
00:07:25,640 --> 00:07:30,680
in order to make sure that the data reflects that and we're accounting for it for in building

103
00:07:30,680 --> 00:07:34,680
our models, updating our models and so on.

104
00:07:34,680 --> 00:07:40,920
And so is the same with sales and marketing because in the past, there were various versions

105
00:07:40,920 --> 00:07:48,680
of, let's say, a product, Windows 95 and the specifications were already made clear.

106
00:07:48,680 --> 00:07:54,400
There was enough time for sales marketing to understand what the market would be like,

107
00:07:54,400 --> 00:07:59,520
but also be able to fully understand the product to go and sell.

108
00:07:59,520 --> 00:08:01,000
Now that's changing.

109
00:08:01,000 --> 00:08:07,000
The product is changing so fast in order for sales to know what to sell.

110
00:08:07,000 --> 00:08:14,440
They need to be as integrated within the kind of product as much as possible to know exactly.

111
00:08:14,440 --> 00:08:21,160
Okay, the real estate on the app is going to be one-tenth of what it used to be so we

112
00:08:21,160 --> 00:08:23,960
need to communicate that to our partners.

113
00:08:23,960 --> 00:08:30,160
Or the targeting algorithm will change and so we're actually going to bring in 10 times

114
00:08:30,160 --> 00:08:38,000
more traffic to your product and so sales could kind of be in the loop for that to be able

115
00:08:38,000 --> 00:08:43,600
to communicate that to the partners and that's how we can like marketing and sales up for

116
00:08:43,600 --> 00:08:44,600
success.

117
00:08:44,600 --> 00:08:45,600
That makes sense.

118
00:08:45,600 --> 00:08:50,400
Having that data and insight to know what's going to happen next so that they can plan

119
00:08:50,400 --> 00:08:53,200
and make changes to their models important.

120
00:08:53,200 --> 00:08:58,800
Now we had a session earlier where Libas was talking about some other things that they

121
00:08:58,800 --> 00:09:01,800
did so that's a longstanding established company.

122
00:09:01,800 --> 00:09:05,280
Jennifer, I know that you actually have done some work with Walmart in the past.

123
00:09:05,280 --> 00:09:09,320
Have you seen any differences in terms of how to think about that?

124
00:09:09,320 --> 00:09:13,200
They're a company that's had a lot of data that's been working with data for a long time.

125
00:09:13,200 --> 00:09:17,880
Did you see a change happen over the course of the time that you worked there in terms

126
00:09:17,880 --> 00:09:19,960
of how they worked with it?

127
00:09:19,960 --> 00:09:26,480
Also when I joined Walmart, one of the big change I had to provide to the company is like,

128
00:09:26,480 --> 00:09:29,600
I think you were talking about integration earlier.

129
00:09:29,600 --> 00:09:33,480
Something I would also like to state here is that I think one of the biggest challenge

130
00:09:33,480 --> 00:09:41,160
like a machine learning team is going to have is lack of integration with engineering.

131
00:09:41,160 --> 00:09:45,880
If you do have that integration, it puts you in a position where you are fully responsible

132
00:09:45,880 --> 00:09:47,200
for what you're building.

133
00:09:47,200 --> 00:09:50,720
It's almost like you do some machine learning research.

134
00:09:50,720 --> 00:09:54,920
You can create a product with it and prove to the C suite and the rest of the organization

135
00:09:54,920 --> 00:09:58,920
that this is actually going to provide money for the company.

136
00:09:58,920 --> 00:10:01,760
Before we talked about Walmart, I would like to state something else that's actually

137
00:10:01,760 --> 00:10:02,760
pretty interesting.

138
00:10:02,760 --> 00:10:09,440
I was lucky to have both like a machine learning at Atlassian and I managed a machine learning

139
00:10:09,440 --> 00:10:12,680
at a smaller startup figure eight.

140
00:10:12,680 --> 00:10:14,680
In both cases, we have the different structures.

141
00:10:14,680 --> 00:10:18,720
For Atlassian, for instance, I was really lucky to have the opportunity to build the

142
00:10:18,720 --> 00:10:25,040
entire team from scratch and basically I brought in like a basically 50% of the team was

143
00:10:25,040 --> 00:10:26,040
engineering.

144
00:10:26,040 --> 00:10:31,640
I mean, so it meant that the algorithms we were building, we were actually able to create

145
00:10:31,640 --> 00:10:36,080
and fully prototype on our own and it was much easier to push this production.

146
00:10:36,080 --> 00:10:40,880
We had other challenges related to the internal organization within Atlassian where you had

147
00:10:40,880 --> 00:10:43,200
to push that into specific products.

148
00:10:43,200 --> 00:10:46,320
But at least there was a prototype that could exist.

149
00:10:46,320 --> 00:10:52,200
By a position when I was a figure eight, my team was 100% machine learning scientist

150
00:10:52,200 --> 00:10:55,000
and this is something I had no control of because the team was already established when

151
00:10:55,000 --> 00:10:56,000
I joined.

152
00:10:56,000 --> 00:11:00,760
Unfortunately, basically we were constantly frustrated that things would not go to production

153
00:11:00,760 --> 00:11:07,040
because there was no engineering that could actually bring these things to production.

154
00:11:07,040 --> 00:11:09,360
That brings me back to Walmart.

155
00:11:09,360 --> 00:11:14,080
I think one of the challenges like companies like a few years ago did not necessarily have

156
00:11:14,080 --> 00:11:19,160
this understanding that a machine learning team is not an engineering team.

157
00:11:19,160 --> 00:11:22,360
They don't think the same way, they don't function the same way and a machine learning

158
00:11:22,360 --> 00:11:27,320
scientist or data scientist is not trained to write production level code and push this

159
00:11:27,320 --> 00:11:28,800
to production.

160
00:11:28,800 --> 00:11:34,040
And so basically this means you have to rethink the organization.

161
00:11:34,040 --> 00:11:40,160
At Walmart, that was a huge challenge because when the machine learning teams started building

162
00:11:40,160 --> 00:11:47,520
new algorithms, for instance, and in particular when we know that because we are machine learning

163
00:11:47,520 --> 00:11:50,960
people, we know that models need to be retrieved on a regular basis.

164
00:11:50,960 --> 00:11:56,640
But the C-suite, except if the C-suite is an engineering scientist, does not know that

165
00:11:56,640 --> 00:12:00,680
you would be surprised the number of people, you created this algorithm, why do you have

166
00:12:00,680 --> 00:12:01,680
to retrain it?

167
00:12:01,680 --> 00:12:06,080
And this is not something that's, and so this is where machine learning life cycle management

168
00:12:06,080 --> 00:12:10,640
is something you have to push onto the organization if you want to be successful.

169
00:12:10,640 --> 00:12:15,280
I actually think this point of machine learning life cycle management is a huge issue and

170
00:12:15,280 --> 00:12:19,080
largely under-addressed in our communities.

171
00:12:19,080 --> 00:12:25,880
Eric, do you have any concepts or thoughts around either a full stack or life cycle management?

172
00:12:25,880 --> 00:12:32,400
Well, yeah, I have strong opinions on the full stack thing, so the team at StitchFix we

173
00:12:32,400 --> 00:12:35,320
built to avoid what we call handoffs.

174
00:12:35,320 --> 00:12:41,280
We don't like when a research scientist wants to do some modeling, so they partner with

175
00:12:41,280 --> 00:12:44,200
somebody called an ETL developer to get them the data.

176
00:12:44,200 --> 00:12:48,960
And then once they find the model, they have a train, they hand it off to a machine learning

177
00:12:48,960 --> 00:12:50,720
engineer to implement it.

178
00:12:50,720 --> 00:12:54,560
And then they also might employ an inference engineer to measure it, right?

179
00:12:54,560 --> 00:12:58,240
So all that is fine and dandy, but it requires handoffs between that.

180
00:12:58,240 --> 00:13:00,880
And that slows you down.

181
00:13:00,880 --> 00:13:04,080
Handoffs are appropriate when you know what it is you're building.

182
00:13:04,080 --> 00:13:07,440
Say if you're manufacturing something, you've got the requirements are crystal clear

183
00:13:07,440 --> 00:13:11,760
down the millimeter of precision, then you can specialize and do those handoffs.

184
00:13:11,760 --> 00:13:17,160
But when it needs to be iterated on, you're not clear what it is you're building or learning,

185
00:13:17,160 --> 00:13:21,840
then I really prefer to have it in as few hands as possible, ideally, even sometimes

186
00:13:21,840 --> 00:13:25,960
one, one full stack data scientist that can do all those parts through the ETL, through

187
00:13:25,960 --> 00:13:30,920
the modeling, implement it him or herself, and set up the AB test appropriately to measure

188
00:13:30,920 --> 00:13:32,080
it.

189
00:13:32,080 --> 00:13:36,200
That's ideal because that person can move as quickly as possible with no handoffs.

190
00:13:36,200 --> 00:13:42,360
So and again, the benefit of the golden data science is to learn something to implement

191
00:13:42,360 --> 00:13:48,160
some new algorithmic capability, preferably that makes a lot of money, but to learn it,

192
00:13:48,160 --> 00:13:49,160
to figure out how to do it.

193
00:13:49,160 --> 00:13:55,080
It's not to do the fine tuning, not to do something more efficient or just a skim, a few pennies

194
00:13:55,080 --> 00:13:56,080
off of something.

195
00:13:56,080 --> 00:14:01,680
It's usually to do something big and profound and something that was new to the company,

196
00:14:01,680 --> 00:14:03,360
a new capability.

197
00:14:03,360 --> 00:14:06,600
And that type of thing, usually with data products, you can't design it up front.

198
00:14:06,600 --> 00:14:07,920
You need to learn as you go.

199
00:14:07,920 --> 00:14:12,000
So you have to design the team to enable them to learn as they go, so they don't get caught

200
00:14:12,000 --> 00:14:14,200
up in so many handoffs.

201
00:14:14,200 --> 00:14:18,480
So that's the thought on the full stack data scientist, as we call it, at Citrix.

202
00:14:18,480 --> 00:14:22,600
Yeah, so what I like about this is it highlights that there are different ways to do it, but

203
00:14:22,600 --> 00:14:29,520
you have to be very specific about understanding what are the different levels that you need,

204
00:14:29,520 --> 00:14:33,200
and who is going to take care of those different levels.

205
00:14:33,200 --> 00:14:37,960
And I think if you're lucky enough and you can find full stack people, that works great.

206
00:14:37,960 --> 00:14:41,120
If you don't have full stack people, you need to know how that you're going to break this

207
00:14:41,120 --> 00:14:45,600
up so that the right people are doing what they're skilled at, but that they're coordinated

208
00:14:45,600 --> 00:14:50,880
enough to make that happen, part of any thought on your side.

209
00:14:50,880 --> 00:14:58,680
I guess the only thing I would add would be around the fact that the data is always changing

210
00:14:58,680 --> 00:15:03,280
because, as we spoke before, the product is always changing.

211
00:15:03,280 --> 00:15:08,280
It's also the fact that sometimes company strategy might be changing, and so you'd want

212
00:15:08,280 --> 00:15:17,760
to update your objective functions accordingly, and another thing is the data could change

213
00:15:17,760 --> 00:15:24,520
because the, you know, your product is growing so fast in additional markets, and suddenly

214
00:15:24,520 --> 00:15:31,320
you have users using the product in a completely different way if you're lucky.

215
00:15:31,320 --> 00:15:38,880
And so all of those various factors kind of change, you know, your product completely.

216
00:15:38,880 --> 00:15:46,160
And so you'd want to be, have people providing always on support to be able to make those

217
00:15:46,160 --> 00:15:48,760
changes in updates as soon as possible.

218
00:15:48,760 --> 00:15:53,000
I'm going to circle back to lifecycle management and maybe we can go down this way and just

219
00:15:53,000 --> 00:15:57,400
talk a little bit about what you think is or isn't happening there and what you think

220
00:15:57,400 --> 00:15:58,800
needs to happen there.

221
00:15:58,800 --> 00:16:01,560
So Eric, why don't we kick off with you?

222
00:16:01,560 --> 00:16:02,560
Sure.

223
00:16:02,560 --> 00:16:07,160
For lifecycle management, so the process I just described earlier is about an initial

224
00:16:07,160 --> 00:16:11,440
implementation of some new capability, and that's what I think goes fastest without

225
00:16:11,440 --> 00:16:16,800
the handoffs, without the specialization you have more of a general full stack data scientist.

226
00:16:16,800 --> 00:16:20,880
That said, once it's in production, supporting it is hard, and so you'll probably need some

227
00:16:20,880 --> 00:16:21,880
help.

228
00:16:21,880 --> 00:16:26,120
And in fact, that's something that's often unintuitive to folks either from the business

229
00:16:26,120 --> 00:16:30,600
side or engineering side is they think that resources will roll off a project once it's

230
00:16:30,600 --> 00:16:31,600
implemented.

231
00:16:31,600 --> 00:16:32,760
And that doesn't happen.

232
00:16:32,760 --> 00:16:35,480
You increase the number of resources you put.

233
00:16:35,480 --> 00:16:39,840
So if something is successful, a new recommendation engine or something, and it's live now, it's

234
00:16:39,840 --> 00:16:42,400
not like you deploy those people to go somewhere else.

235
00:16:42,400 --> 00:16:43,400
No, they double down on it.

236
00:16:43,400 --> 00:16:48,560
They have their best ideas come after implementation, and they can get better, better and better algorithms

237
00:16:48,560 --> 00:16:54,440
in there by testing them against each other and making successive changes to them.

238
00:16:54,440 --> 00:16:59,040
So you end up usually increasing, not decreasing after implementation.

239
00:16:59,040 --> 00:17:01,680
And then the support stuff is problematic.

240
00:17:01,680 --> 00:17:04,960
It becomes more of a burden than actually building the thing.

241
00:17:04,960 --> 00:17:08,680
As you mentioned, data changes a lot, and it could wreak havoc on your stuff.

242
00:17:08,680 --> 00:17:13,240
A new engineering feature goes out and isn't compatible with your algorithm.

243
00:17:13,240 --> 00:17:17,040
Things need to be adjusted and changed pretty constantly.

244
00:17:17,040 --> 00:17:21,500
And that's when you, so you, again, it might take one single person to implement the

245
00:17:21,500 --> 00:17:26,000
capability, but you need to start staffing up that's team to support it.

246
00:17:26,000 --> 00:17:29,720
As that person eventually wants to take a vacation now and then, right, and doesn't want

247
00:17:29,720 --> 00:17:33,920
to be left supporting from wherever they go on vacation.

248
00:17:33,920 --> 00:17:37,680
And so you need to add resources later, and that's something that needs to be kind of explained

249
00:17:37,680 --> 00:17:39,080
and anticipated.

250
00:17:39,080 --> 00:17:44,160
But I wouldn't, we do kind of a tough thing, we wait till it's successful and then start

251
00:17:44,160 --> 00:17:45,160
adding the resources.

252
00:17:45,160 --> 00:17:48,560
So it's always a scramble, because a lot of things do fail, and you don't want to set up

253
00:17:48,560 --> 00:17:51,200
people, hey, you're going to support that thing once it's in production, but it never

254
00:17:51,200 --> 00:17:52,840
gets to production.

255
00:17:52,840 --> 00:17:57,400
So you end up in this game of catch-up by design, we consider it as the lesser of the

256
00:17:57,400 --> 00:17:59,080
two evils.

257
00:17:59,080 --> 00:18:02,920
But you do have to plan as best you can that, okay, if these things are successful, you

258
00:18:02,920 --> 00:18:05,440
got to get ready to staff up to help support.

259
00:18:05,440 --> 00:18:10,120
All right, no, I mean, so going back to the example with Walmart, right?

260
00:18:10,120 --> 00:18:15,000
I mean, so actually when I was at Walmart, and we had more and more models coming out

261
00:18:15,000 --> 00:18:20,000
to production, so I actually started serving my team for, basically, ask them, like, for

262
00:18:20,000 --> 00:18:23,360
every, like, a new model that's coming in how much more work do you have?

263
00:18:23,360 --> 00:18:29,800
So after I served my entire team, turns out that when we were, like, basically, like in periods

264
00:18:29,800 --> 00:18:33,680
where the models had to be retrained frequently, for example, close to Black Friday, or this

265
00:18:33,680 --> 00:18:39,040
type of periods, or close to Christmas, people were spending, like, almost 75% of their

266
00:18:39,040 --> 00:18:44,680
time retraining models, relaunching things, because there was no pipeline to do this

267
00:18:44,680 --> 00:18:45,680
automatically.

268
00:18:45,680 --> 00:18:52,160
Actually, like, one of my pushbacks on management was, like, we need, like, a way to systematize

269
00:18:52,160 --> 00:18:57,120
the way that you actually, like, deploy and retrain the models specifically in an environment

270
00:18:57,120 --> 00:19:02,160
where you need to retrain models every day, like, it's very typical for e-commerce or

271
00:19:02,160 --> 00:19:06,840
similar spaces, I'm sure it's true for social media as well, right?

272
00:19:06,840 --> 00:19:11,240
And yeah, and so eventually, what I saw as being a problem, so I almost had the reverse

273
00:19:11,240 --> 00:19:15,480
problem at some point where it's almost like, you have to think about, like, what really

274
00:19:15,480 --> 00:19:19,240
life cycle management means, like, when you come back to this later, right?

275
00:19:19,240 --> 00:19:24,480
But finally, I convinced the company, like, we need a systematic way to retrain this,

276
00:19:24,480 --> 00:19:29,160
like, using technology such as airflow to, you know, like, keep things going even after

277
00:19:29,160 --> 00:19:31,960
you move forward.

278
00:19:31,960 --> 00:19:38,240
Then the management team almost had the sense that the model is taking care of itself,

279
00:19:38,240 --> 00:19:39,240
right?

280
00:19:39,240 --> 00:19:42,840
I mean, so it's going to be retrained fully automatically, and so this is where I think

281
00:19:42,840 --> 00:19:48,440
people are missing the point, like, in life cycle management, you have life cycle management.

282
00:19:48,440 --> 00:19:54,600
So cycles, and this is actually kind of important because, to me, cycle means feedback, right?

283
00:19:54,600 --> 00:20:00,000
And so there is a huge missed opportunity when you try to fully automate this, to take

284
00:20:00,000 --> 00:20:04,640
the feedback of the model you've trained and basically understand the failures, the

285
00:20:04,640 --> 00:20:09,480
reason why, you know, like, you didn't reach 100% accuracy and feed that information

286
00:20:09,480 --> 00:20:11,280
back into the system, right?

287
00:20:11,280 --> 00:20:15,120
So I think, like, it goes back to, like, maintaining in the long term, right?

288
00:20:15,120 --> 00:20:19,920
I mean, the model and, like, fixing the failures you had at first and so forth and so on.

289
00:20:19,920 --> 00:20:24,680
And so I think it's also important to see life cycle management as being an opportunity

290
00:20:24,680 --> 00:20:30,480
to get your model in a better place, not just keeping it up to par or up to speed with

291
00:20:30,480 --> 00:20:31,480
the current data.

292
00:20:31,480 --> 00:20:32,480
Definitely.

293
00:20:32,480 --> 00:20:33,480
Yeah.

294
00:20:33,480 --> 00:20:39,480
100% agree with that just because so many things are changing around the model, whether

295
00:20:39,480 --> 00:20:45,920
it be, like, sometimes policy, there are policy changes and so the way you're labeling

296
00:20:45,920 --> 00:20:53,440
your data will be, will change and you will need to then think about your model differently

297
00:20:53,440 --> 00:20:55,760
in that kind of space.

298
00:20:55,760 --> 00:21:05,200
Again, the way that you will be, your objective functions might change and the user's

299
00:21:05,200 --> 00:21:09,520
might change as we talked about in a different geography and things like that.

300
00:21:09,520 --> 00:21:17,880
And I also wanted to, like, reiterate something that Eric was talking about around kind of

301
00:21:17,880 --> 00:21:24,120
headcount and the fact that if something is successful, you will need more or not less

302
00:21:24,120 --> 00:21:26,800
people to kind of work on that project.

303
00:21:26,800 --> 00:21:32,440
And so if something is a high priority, a lot of times you would hear, could we just borrow

304
00:21:32,440 --> 00:21:38,000
or two data scientists to kind of think about or opportunity size this project?

305
00:21:38,000 --> 00:21:42,280
And it's usually, well, if this is going to be a priority, we need to, like, think about

306
00:21:42,280 --> 00:21:46,280
long-term data science support for this data product.

307
00:21:46,280 --> 00:21:47,280
Yeah.

308
00:21:47,280 --> 00:21:50,240
I think the one thing that you've heard a lot that you should really take away from this

309
00:21:50,240 --> 00:21:56,920
is that one, because of some of the marketing messaging that's gone out around AI, people

310
00:21:56,920 --> 00:22:02,800
do think it's very automated, and that once you get something that it does just take care

311
00:22:02,800 --> 00:22:03,800
of itself.

312
00:22:03,800 --> 00:22:07,520
And that is something that you really have to get ahead of, because this concept of

313
00:22:07,520 --> 00:22:13,600
resources for whether it's new software or tooling so that you can create a more automated

314
00:22:13,600 --> 00:22:19,240
retraining pipeline flow or whether or not it's actual more headcount, this is going

315
00:22:19,240 --> 00:22:23,160
to be a thing in the culture and organization discussion that you definitely have to get

316
00:22:23,160 --> 00:22:24,160
ahead of.

317
00:22:24,160 --> 00:22:28,160
And we're talking to one bank that did not realize that they were, you know, they spent

318
00:22:28,160 --> 00:22:32,480
a long time going through their model, and it was in the chat bot section.

319
00:22:32,480 --> 00:22:36,600
And they had to retrain it every day for a very long time, because they didn't think

320
00:22:36,600 --> 00:22:40,960
that, well, people don't speak in terms such as activate my card.

321
00:22:40,960 --> 00:22:44,320
You know, something a bank would say, not something a human would typically say, you might

322
00:22:44,320 --> 00:22:47,520
say, turn on my card or some other variant of that.

323
00:22:47,520 --> 00:22:51,760
And those types of, you know, feedback loops and interesting learning meant that it takes

324
00:22:51,760 --> 00:22:56,600
a very long time, a lot longer than people think, which means also that programmatically,

325
00:22:56,600 --> 00:23:00,200
you probably have a pipeline, and your pipeline is going to take a lot longer to get through

326
00:23:00,200 --> 00:23:03,920
than your management originally thought.

327
00:23:03,920 --> 00:23:07,040
We have about three minutes left, and I was wondering if there was one question in the

328
00:23:07,040 --> 00:23:13,200
audience that anybody would like to ask if so, when to raise a hand stand up or anything.

329
00:23:13,200 --> 00:23:21,080
Yeah, great talk about educating the whole company about how building ML machine and

330
00:23:21,080 --> 00:23:22,080
AI product is.

331
00:23:22,080 --> 00:23:28,480
I hear a lot, some solution is to hire different people into the team, so the team actually

332
00:23:28,480 --> 00:23:31,760
have different kind of knowledge they can help each other out.

333
00:23:31,760 --> 00:23:37,280
I think there's another dimension of what's your take is there's a whole company.

334
00:23:37,280 --> 00:23:39,160
You can't solve all the problem.

335
00:23:39,160 --> 00:23:40,160
Like you need data.

336
00:23:40,160 --> 00:23:42,440
Are you going to build the entire data lake yourself?

337
00:23:42,440 --> 00:23:46,920
Are you going to build the entire computational layer for yourself as well?

338
00:23:46,920 --> 00:23:51,640
How would you educate almost like it takes a whole village to build it?

339
00:23:51,640 --> 00:23:52,640
But you're right.

340
00:23:52,640 --> 00:24:00,800
There are certain parts of certain capabilities that the data science team is totally autonomous

341
00:24:00,800 --> 00:24:04,960
for, but most things you're partnering with somebody, they're something with marketing

342
00:24:04,960 --> 00:24:08,360
or merchandising other teams, and it does take a village.

343
00:24:08,360 --> 00:24:13,480
You need a lot of it, and a lot of times integration with engineering, but because data science

344
00:24:13,480 --> 00:24:18,240
works differently, right, much more iterative and much more uncertain inherent uncertainty

345
00:24:18,240 --> 00:24:23,160
in what it is we're building and what we'll find and what becomes significant, you have

346
00:24:23,160 --> 00:24:27,200
to plan for that iteration, and since it's the data science team that works differently

347
00:24:27,200 --> 00:24:33,320
from the rest, what we've done is we've made it a combat that we build the APIs that engineering

348
00:24:33,320 --> 00:24:34,320
will integrate with.

349
00:24:34,320 --> 00:24:38,280
They call the APIs, and that abstracts us, right, we're now behind an API that they're

350
00:24:38,280 --> 00:24:43,200
going to call, so we can change things and test things as much as we can and we don't

351
00:24:43,200 --> 00:24:46,880
need to coordinate those, right, because they're just calling the same API before.

352
00:24:46,880 --> 00:24:53,640
So the little tricks like that really help to decouple teams, and yet still work together.

353
00:24:53,640 --> 00:24:57,600
We have to be aligned on the goals of what we're doing, and then when we work with marketing

354
00:24:57,600 --> 00:25:04,040
and merchants, for example, there's no APIs for buying merchandise or for making new

355
00:25:04,040 --> 00:25:05,040
creatives.

356
00:25:05,040 --> 00:25:11,080
So those are more of a formal relationship that you have, and you want to blend both of

357
00:25:11,080 --> 00:25:15,760
their experiences, you get some of the domain expertise from marketers and merchants,

358
00:25:15,760 --> 00:25:19,880
what they know, and then also get what the data is telling you, which are sometimes very

359
00:25:19,880 --> 00:25:24,720
complementary or different or even contradictory, and both together are usually much better

360
00:25:24,720 --> 00:25:26,720
than anyone on their own.

361
00:25:26,720 --> 00:25:27,720
Any other comments?

362
00:25:27,720 --> 00:25:32,800
No, so I was going to say, so you're actually like a similar comment to what you just

363
00:25:32,800 --> 00:25:33,800
said, right?

364
00:25:33,800 --> 00:25:37,960
I mean, I just want to build on top of that, so Atlassian, we had like a similar kind

365
00:25:37,960 --> 00:25:43,880
of model, so we actually like the data science-lash machine learning team was under the platform

366
00:25:43,880 --> 00:25:49,160
department, and so basically we're like a team that would service the other teams.

367
00:25:49,160 --> 00:25:52,800
And so similarly to what you said, like we had like either APIs, and we're like basically

368
00:25:52,800 --> 00:25:54,640
in charge of building a model, right?

369
00:25:54,640 --> 00:25:59,120
So I think something that's very important to note here, and I think like a not specific

370
00:25:59,120 --> 00:26:01,280
to Atlassian or any company, right?

371
00:26:01,280 --> 00:26:05,720
There is like really like machine learning research, and there is machine learning product,

372
00:26:05,720 --> 00:26:06,720
right?

373
00:26:06,720 --> 00:26:12,960
So for instance, like you, I can imagine a company in which you have like the machine learning

374
00:26:12,960 --> 00:26:17,800
team, the deep, deep down machine learning team that creates, let's say, an OCR model,

375
00:26:17,800 --> 00:26:21,640
and then there is a product or an applied machine learning team that actually applies this

376
00:26:21,640 --> 00:26:25,280
model to create cool new AI products, right?

377
00:26:25,280 --> 00:26:27,280
And these don't need to be the same team, right?

378
00:26:27,280 --> 00:26:31,040
I mean, that's almost the way that I see that it was actually pretty successful to do

379
00:26:31,040 --> 00:26:35,720
that at Atlassian because we would empower the rest of the organization of integrating

380
00:26:35,720 --> 00:26:40,200
AI in their own products, even though they were not AI engineers or machine learning scientists

381
00:26:40,200 --> 00:26:42,200
themselves.

382
00:26:42,200 --> 00:26:43,720
Closing common partners?

383
00:26:43,720 --> 00:26:50,960
I think one aspect of being able to get by and to get more support from engineering on

384
00:26:50,960 --> 00:26:57,760
let's say issues of data quality or building data pipelines was to show kind of the impact

385
00:26:57,760 --> 00:27:04,040
and the fact that they could move faster if, you know, through that collaboration, they

386
00:27:04,040 --> 00:27:10,520
would be able to ship with more confidence and kind of showing that over a couple of

387
00:27:10,520 --> 00:27:11,520
screens.

388
00:27:11,520 --> 00:27:13,200
So slow them down, but speeds them up.

389
00:27:13,200 --> 00:27:14,200
Exactly.

390
00:27:14,200 --> 00:27:15,200
Excellent.

391
00:27:15,200 --> 00:27:16,200
So with that, thank you for your time and attention.

392
00:27:16,200 --> 00:27:17,200
We're finished.

393
00:27:17,200 --> 00:27:28,920
All right, everyone, that's our show for today.

394
00:27:28,920 --> 00:27:36,200
To learn more about today's show or any of our panelists, visit twemalai.com slash shows.

395
00:27:36,200 --> 00:27:41,200
Head over to twemalcon.com slash news to check out Twemalcon shorts.

396
00:27:41,200 --> 00:27:46,960
The series of short interviews recorded straight from the Twemalcon community hall.

397
00:27:46,960 --> 00:27:59,360
Thanks.

