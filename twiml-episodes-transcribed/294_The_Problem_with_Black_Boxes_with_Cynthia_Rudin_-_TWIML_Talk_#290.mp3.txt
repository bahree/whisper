Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington, hey what's up everyone, I'm on the road so I hope
you'll forgive that this intro may not be up to our usual audio standards.
Last week we dropped a huge Twimble Con content update announcing that we double the number
of sessions and sharing a bunch of the amazing speakers and sessions that we've got in store
for you.
One content, another important focus for us in creating this event is community.
We see this conference as an opportunity to help create and support a community of professionals
dedicated to helping their organizations be more successful with machine learning and AI
by sharing and learning from one another.
As a part of this commitment to community we want to make sure that you're aware of our
volunteer program and diversity scholarships.
To learn more about both of these great initiatives visit twimblecon.com slash volunteer and if
you're interested fill out the appropriate application.
Submissions will be accepted through August 30th so get them in as soon as possible.
And now on to the show.
Alright everyone, I am on the line with Cynthia Rudin, Cynthia is a professor of computer
science, electrical and computer engineering and statistical science at Duke University.
Cynthia, welcome to this week in machine learning and AI.
Thanks for having me.
Let's jump right in and get started with a little bit of your background and how you came
to work in machine learning.
Yeah, I think I ended up working in machine learning because I like the name support vector
machines.
No, I'm just kidding.
Well, I'm actually sort of only partially kidding.
I happen to be walking around in Princeton at any sea labs and at the time there are
a lot of researchers there who are really, really good at support vector machines and I walked
into Gary Flakes office and he gave me Vapnik's book and I read it and it was, it was very
difficult to read for me at that time, but after that I was sold, you know, it was, it
was going to be machine learning all the way so that's it.
Yeah, nice and were you, this was during grad school or, okay, awesome, I had, I ended up
having two PhD advisors, so I'm Anchored Dubsheet was one of them, the first one and she
said to me, you know, I haven't, you know, I don't work in machine learning is it okay,
you know, after I asked her to be my advisor, you know, I said, Anchored, will you be my
advisor after Gary left and she said, is it okay that there's no, that I, I don't work
in machine learning and I've never advised a student in machine learning and I said,
yeah, don't worry about it, everything will be fine.
Nice, nice.
Yeah.
I also noticed that you're a three-time winner of the Informed Innovative Applications
in Analytics Award.
I, Informed was one of the conferences that I went to in grad school, but I've always
associated with like industrial engineering more so than computer science or machine learning.
You're a chair of some element of that conference as well, is that right?
Well, okay, so Informed is, is the, the organization that, like the main organization in the
United States that is, represents operations research and management science and the
work that I do is very closely related to aspects of decision making and management.
And so that's why I became involved with this organization.
Informed does have a very active data mining section and, and I was a past, I'm a past
chair of that data mining section and I'm also an editor for one of the journals, one
of the top journals in that, sponsored by the organization called Management Science.
Tell us a little bit about your, your research into, into that area.
It sounds like it's fairly interdisciplinary.
Yeah, I started off as a theorist working on kind of theoretical machine learning problems,
convergence of adabuse specifically.
But then after I, you know, graduated and after I finished my postdoc, I went to work on
a project, an interdisciplinary project with the power company in New York City.
The power company is called Con Edison and they, their job is to maintain the, the oldest
and largest underground power network in the world, which is the New York City power
grid.
And at the time we were doing something that was, you know, totally crazy, which is, can
you maintain the power grid with machine learning?
Like can you use machine learning to help prioritize inspections and repairs on the grid?
One was this?
This was between 2007 and I think I worked on it all the way up through 2012 and beyond.
Okay.
Yeah.
And, you know, no one, no one had done this before and we were using data that was from
the 1890s.
I mean, all these, all these huge, right?
This was a huge amount of very complicated data that involved free text documents written
by people who were trying to maintain the power network.
There were engineers going into the, the manholes and doing repairs and they were giving
that information to the dispatchers who were typing it in.
And we also had accounting records dating back from all the way since the power grid essentially
started, you know, since the days of, of Edison, Thomas Edison.
So it was a really, really interesting, very, very challenging data set.
A lot of the data we didn't know what it was.
It was just sort of a pile of text and a pile of, just like garbage to us.
We had no idea what it was, but it, you know, and data science as a field hadn't really
been invented yet when we were working on this project.
So the job of my team was to try to put all of these data together and figure out which
manholes were the, were the ones that were the most likely to explode.
So that, and do the repair work before, you know, that bad event actually happened.
Wow.
So this was a kind of big change from doing the theoretical work that I had done in the
past.
And while I, while I was working on this power grid reliability project, we made some errors
that, you know, people make when you're working, when you're doing data science, we made
some of the classic errors that, that you'd make.
If you, if you didn't, you know, if you didn't have that experience, and I learned from
that very quickly that black box models were not the way to go, because, you know, because
the mistakes we made were things like not really understanding what the important variables
were that, that we were using to make predictions.
And if you don't really understand that stuff, you can't really make very good decisions.
So I switched to kind of working on projects that were more along the lines of interpretable
machine learning.
So still keep everything machine learning, keep it data driven, but be able to understand
exactly what the important variables were, how the variables were being combined, and
then be able to make a decision that was informed.
And not just a blind, oh, the model said this kind of decision.
When I talk to folks about interpretability, I'll often ask them if they make a distinction
between interpretability and explainability.
And that's something that you, is, is very significant distinction to you.
In fact, one of your papers is stop explaining black box machine learning models for high
stakes decision and use interpretable models instead.
Can you talk a little bit about that distinction and, and kind of lead us into a discussion
of the main problem you're taking on with that paper?
Well, let me give you a little bit of history.
The field of data science sort of is evolving so quickly that things go off in different
directions without really carefully thinking about them.
And explainability was one of those things.
So, yeah.
So interpretability is a really old concept that people have been working on since the,
you know, the 80s and 90s, you know, Leo Breiman and Cart and Quinlan, you know, these people
were working on this in the, in literally the early 90s because they realized how important
it was to have models that were interpretable.
And that field has existed since that time, but then recently in the last few years,
people have been working kind of only with black box models.
And I think this is partly an illusion due to the fact that neural networks were performing
really well for computer vision, starting in around 2012.
And so people decided that, you know, we should only be teaching neural networks and that
was the only type of machine learning that should be going forward.
And so there was this whole group of people that could really, that really only knew how
to do neural networks and black box models.
There's also kind of the industry is propelled by these complex models because if these very
simple models, then they can't license them and they can't make a profit off of them.
So they really like to keep their models complicated.
So because of that, people then tried to explain the black box models because they either
couldn't produce interpretable models or they wanted to use black box models, but produced
some sort of explanation for what they were doing.
And you know, that, that evolved sort of very quickly kind of around 2015, 2016.
And then sort of people forgot that, wait, you know, we might be able to produce interpretable
models that are just as accurate as these black box models that, that wasn't sort of
a thought that, that, that people were thinking in the last few years.
And I wrote this paper to kind of remind people that, hey, you know, you don't always need
a black box.
You, and in fact, if you try to use black box models for high stakes decisions, really bad
things can happen.
And I gave a bunch of examples in that paper where bad things have happened because people
tried to use overly complicated models when they didn't need them.
I can go through some of the examples if you think it would help.
But please, okay.
So well, one of the examples is in criminal recidivism prediction.
So there's a model that's used sort of throughout the US justice system, which is a very, it's
a very unusual model for the justice system in that it's a very complicated model.
It involves 137 factors.
And nobody knows exactly how those factors are combined.
Is this the compass model?
Yes, this is compass.
Okay.
It's used by a private company that licenses access to software.
And compass is used in, it's used regularly in parole decisions.
And there was at least one parole decision that was very famous where it led to like an
article in the New York Times, where the person was denied parole.
And they figured out afterward that it was because of a typographical error on their
compass score sheet.
So in other words, the factors that went into that black box model, one of them had a typographical
error in it.
And nobody spotted it until after the parole decision was made.
And by that point, you couldn't reverse the parole decision.
So here you have a typographical error making a high stakes decision about someone's, you
know, future.
And I, you know, I don't, I think that's a kind of procedural unfairness that really kind
of shouldn't exist, right?
We shouldn't be making high stakes decisions that deeply affect people's lives based on
typographical errors and those types of kind of clerical issues.
Right.
Right.
And the underlying implication is that if this wasn't a black box model and we understood
the basis upon which it was making this decision, we would have easily seen that in the process.
Or the people that were responsible for this process.
Yeah, that's the implication.
I mean, there have also been typographical errors in very simple models.
But if you think about, you know, the number of typographical errors in the simple models
versus the more complicated models, you know, even if you do a very simple calculation,
you know, if you have kind of a 1% error rate in your typographical errors, then it means
that on almost every compass score sheet, there should be one typographical error.
I'm not sure if that if that calculation kind of holds water, but the fact is that the
more complicated the model, the much, it's much, much, much more easier to make an error
than if it's a much simpler model.
And also the simpler models can be double checked and triple checked, whereas the more complicated
models is very, very difficult to check.
There were other errors too that kind of propagated throughout society because of black box
models.
So for instance, another example was given to me by someone at the EPA at the Environmental
Protection Agency.
They had a very understandable model, they have a very understandable model for air quality
that's used to assess whether it's safe to go outside.
And at some point during the California wildfires, Google replaced the EPA's air quality
measure with one from a, that was a proprietary model from a company.
And that model told everyone that it was safe to go outside on a day when people saw
a layer of ash on their cars.
So yeah, so what went wrong with that model?
I'm not sure.
And it's pretty clear that the company who released that model wasn't, you know, there's
something that went wrong.
And nobody will know what it is.
But clearly they didn't troubleshoot as carefully as they could have.
And I'm guessing if they had used a model that they actually understood, then this kind
of blatant mistake wouldn't have happened.
Is there a framework or taxonomy for these kinds of errors or thinking about the different
failure modes of black box systems or is it just, you know, black box systems, we don't
understand them and that's a problem.
Well, black box models can come in a couple of different varieties.
Usually when I say a black box model, I mean, either a model that's too complicated
for any human to understand or it's, you know, it's a function that's too complicated
for humans to understand, like it has a million logical conditions in it or a bunch of things
added together and then transformed and then added together again and then transformed.
Or another type of way a model can be black boxes if it's proprietary, like if it's owned
by a company that doesn't want to release the secret formula.
And very often black boxes are both.
So I think what you're asking me is to kind of talk a little bit more about kind of a way
to go from black box to interpretable on sort of a gray scale, if you will, is that what
you're asking about?
I guess the step before that in my mind is, so you have, you know, this kind of class
of models that are black box or, you know, otherwise opaque and you gave some examples of kind
of the ways in which they fail.
And I'm curious whether, you know, there are categories of, you know, failure modes,
like, you know, you know, these fail because, and I don't even know what that might be,
right?
Like, you know, problems with the inputs.
Like, you mentioned the, the, the, the typographic errors, like you, you know, they fit, you
know, they could fail because, you know, you give them inputs and inputs are malformed
and you kind of don't understand that, you know, it could be, you know, the underlying
function doesn't well represent, you know, there's some kind of data drift and the, the
model is trained on, you know, oh, yeah, there's, there's all kinds of stuff like that.
I mean, it can go wrong anywhere, you know, when you trust a model, you, if you're going
to trust a black box model, you don't just trust the formula, you actually have to trust
the whole database that the model was trained from.
And there's a lot of issues with trusting data, you know, data, I, I don't know if I've
ever seen a clean database in my life, you know, if data can be very complicated and
have all kinds of things wrong with it, it can be, you know, a lot of data can be missing
or data can not represent the cases that you actually care about, or it can simply
not represent the full range of cases that exist.
Models can go wrong because they can overfit the data, so they can sort of memorize the
data without actually generalizing to new cases, yeah, you can, you can have just about
anything go wrong with a model or a data set, also, you know, a lot of these decisions,
there's a cost involved to making a wrong decision.
And machine learning is a field developed in kind of a low cost environment, you know,
like predicting which advertisements some online user is going to click on.
Now if you give the user the wrong advertisement, it's not a big deal.
But if you deny someone's parole, that's a high cost decision.
And then you also have the problem that the people who create the models are not the
people who suffer those costs.
So the people who created the compass model, they're not subject to it.
So if they make a, you know, a bad prediction on someone because of a typographical error,
the person who suffers is not the person who designs the model.
And there's that, that sort of misaligned incentives that's a serious problem for machine
learning deployment right now.
One more question before we jump into kind of the path to addressing these issues.
Do you, I think early I use opaque and black box kind of interchangeably are those interchangeable
to you or do you, are there nuances there that are important to you?
I think yeah, opaque and black box to me mean the same thing, but interpretable and explainable
mean different things to me, because, you know, interpretable means that you can fully understand
or you can understand the path of computations leading to the prediction, whereas explainable
to me is like a post hoc, can you, can you somehow justify what the model did?
And there are a wide variety of, well, there's a ton of research happening now to your earlier
point on explaining these black box, black box models, but in the end, they're not interpreting
what the model is doing.
They are coming up with an explanation that hopefully correlates to that in some way.
Yeah, but some of those explanations are such poor explanations that it's, you know,
the explanations have almost nothing to do with the original model other than the fact
that they produce predictions that are fairly similar.
So they may not use the same important variables.
They may claim the black box is depend on specific variables that they don't depend on.
And yeah, they're sometimes their explanations are so incomplete that you actually don't
really understand what the black box did at all, like, you know, they might give the same
explanation for all of the different classes.
So you really don't, you really don't actually know what the black box is doing.
What's an example of that?
It sounds like you're describing a system in which the explainability algorithm is like
literally just given explanation that has nothing to do with anything.
Well, you know, it's, it's funny.
It's funny because people think that the explanations are meaningful, but they're not.
So we give an example in the paper of saliency maps.
And this is from my interpretable neural networks group.
This is Oscar Lee and Chef Anchen and several other students where we're, you know, we're
trying to understand what the saliency maps do.
And what are saliency maps?
Okay.
Yeah, it's supposed to highlight the part of an image that the neural network is using
to make its prediction.
Okay.
I like this attention heat maps that we've many of us have seen exactly, exactly.
And so you get this neural network that says, okay, this is a picture of a golden retriever.
And then it highlights the golden retriever's face.
And you say, oh, yes, the network must be right because it's highlighting the golden retriever
space.
That's why it thinks it's a golden retriever.
But then you ask the network, why do you think this image and you give the same image,
okay?
The same image of the golden retriever and you say, why do you think this image is a tennis
ball?
And it highlights exactly the same thing, the golden retriever's face.
And it says, this is why I think it's a tennis ball.
So it's giving you, it's just giving you this meaningless explanation for, you know, you
think it's useful because it's telling you the correct pixels for the correct class.
But it's just edges, you know, it's just telling you, I'm kind of looking over here.
I guess my, my reaction to that is that there's, there's some, maybe there's some nuance
missing there.
Like it's more like conditioned on if this were a tennis ball, this would be why I think
it's a tennis ball.
But that's not what it told you, it told you it thought it was a golden retriever.
So does that make that explanation wrong?
Well, I mean, the, what happens when the neural network predicts, I mean, if it predicts
correctly, fine, it's looking at the pixels that you think it should look at.
But what if it predicts incorrectly, then it's just highlighting the edges as it did before.
You know, there's no way to troubleshoot it because you, you're saying, well, it's looking
at this, you know, there's an image of a golden retriever here.
It says it's a conquer spaniel, but it's highlighting the face of the golden retriever
and saying, this is why I think it's a conquer spaniel, like, right, right.
You know, there's just no way to troubleshoot it.
Yeah, yeah.
Like it's the same thing maps are certainly helpful if you can find out that the network
is looking in the wrong part of an image to make its prediction, but if it's looking
at the correct pixels, you still have no idea what it's doing with those pixels.
Going back to the, your title for this paper, stop explaining black box models.
It sounds like your general contention is that none of these explainability approaches
are adequate, and therefore we should just not be using black box models for high-stakes
decisions.
Well, if I want, if my credit is going to get denied when I go to the bank, I want to
know why it got denied.
I don't want somebody to say, oh, there were a million factors that went into it, but,
you know, the primary one is, is that your credit history is not long enough.
No, that's not good enough for me.
I want to know exactly what part of my credit history made me get this loan, you know,
made my loan be denied.
And if I'm going to be denied parole, I don't want some 137 factors making that decision
for me when the truth is that only three factors are important and I could get an equally
accurate decision with only three factors, right?
What if it was a typo that made the decision for me?
What if my credit history wasn't entered in the computer correctly?
There's a lot of reasons, really good reasons why for high-stakes decisions we should be
using interpretable models rather than black boxes with post-doc explanations.
Given those kinds of examples, one would think you wouldn't get a lot of resistance to
that, but, you know, yet models like Compass and others exist, suggesting that, you know,
maybe those folks aren't reading your paper.
What needs to happen and, you know, what do you, do you propose something in the paper
for, you know, getting us from, you know, where we are today to where we need to be?
Well, so, yeah, I have gotten some resistance and I think part of the problem is that people
really love their black boxes.
They love the idea that a black box can uncover secret hidden patterns that they don't think
an interpretable model could, you know, they say it's too simple, it couldn't possibly
be that accurate, which for many problems is not true.
In particular, for a criminal recidivism prediction, we've been able to produce interpretable
models that are just as accurate as the best black box machine learning models.
For credit risk assessment, we've worked on some data from FICO for an interpretable
or an explainable machine learning challenge, but even in that challenge, you didn't need
a black box model, you could, you could do it with an interpretable model.
So, I think, you know, a lot of people, first of all, they love their black box models
because they don't believe interpretable models exist, but also they want to make money
from their black boxes.
And they think the black boxes, you know, they uncover secret hidden patterns.
And also black boxes are much easier to train than interpretable models, so that's a major
issue.
Easier to train?
Yeah, it's much easier to train a black box than to train something that's interpretable,
because if you're training the black box, you don't have to have constraints, you know,
technical constraints on the model that force it to be interpretable.
Whereas if you're creating interpretable models, you have to, you have to minimize loss,
you know, make it accurate, subject to some kind of interpretability constraint.
And those constraints make the problems, the optimization problems harder.
So it's actually much harder to design an interpretable model than it is to design a
black box model.
But the software for designing interpretable models has advanced considerably over the
last however many years.
And so, you know, it's kind of sad that people are not even trying to construct interpretable
models that they want to instead just run their black box algorithms and then try to explain
what they're doing afterward rather than actually go to the trouble of correcting the underlying
problem and designing the model to be interpretable in the first place.
Let's dig into this a little bit more. When I hear interpretable models, I think the,
I tend to interpret that, or let's not overuse interpret.
The picture that forms for me is one of using kind of simpler models that have some
inherent trait of interpretability, like a decision tree.
For example, as opposed to what I thought I heard you suggesting was a model that may
be more complicated than that, but has, you know, it's trained against some interpretability
constraint.
Well, decision trees you could write down as an optimization problem.
No.
So, you know, maximize accuracy, subject to a constraint on the size of the decision tree.
Now, you know, the most popular algorithms for decision trees are Carton C4.5 and those
algorithms stay back from the 90s and so they're not that good.
I mean, they work, you know, they work surprisingly well for an algorithm being from the 90s, but
they aren't as good as, you know, the modern machine learning methods because Carton C4.5
don't actually optimize anything, you know.
They're not optimization techniques.
So, you know, over the last several years, there have been several teams working on optimal
decision trees.
And I gave an example in the please stop explaining paper of the work that we've done in the
corals project, which is an optimal decision list paper.
So the goal is to globally optimize over all possible decision lists, the, you know, minimize
the loss, subject to the model being sparse.
So, you know, that code is available and people could download and use it.
Can you elaborate on that a little bit more?
I have a decision lists relative to decision trees is something that I'm not too familiar
with.
Oh, a decision list is a one-sided decision tree.
It's a series of if-then rules.
Got it.
Okay.
Yeah.
Decision lists are exponentially easier to create than decision than full blown decision
trees.
And both decision trees and decision lists to create them optimally is computationally hard.
It's NP hard with no polynomial time approximation.
So these are actually very, very hard optimization problems, but, you know, computers have increased
their speed, you know, a million times in the last, oh, 20 years or something like that.
So we actually can solve optimal decision list problems in reasonable amounts of time
for reasonably large data sets.
And that's what the corals project is about.
With corals, have you applied it to some of the same types of problems, you know, for
which you've kind of given these examples of, you know, the failures of black box models?
Yeah.
So we actually applied corals to the criminal recidivism problem.
Okay.
And what we found was that we found a very, very tiny model that I can probably tell you
what it is if I can just grab it real quick, but we found a really small model that was
just as accurate as, as compass or any of the other machine learning methods.
And the model is so simple, it says, like, if you're between 18 to 20 years old in your
mail, predict, you'll be arrested within two years, Elsa, if you're between 21 and 23
and you have two to three prior offenses, predict, arrest, Elsa, if you have more than three
priors, predict, arrest, otherwise predict, no arrest.
So it's basically if you're younger and you have more priors, more prior crimes, then
the algorithm predicts that you're more likely to be arrested.
And this is, this is the algorithm making a prediction kind of independently based on
the data.
This is not, you're not trying to fit against what compass might do.
No, this is just, this is just trying to predict the outcome, which is whether the person
will be arrested within two years, this is data from Broward County, Florida.
And we compared it directly against the compass scores.
And what we found was that the corals model and the compass model were equally accurate
in predicting whether someone will be arrested within two years of their release.
So that tiny little model that I just told you, compared to a complicated black box
model with over 130 factors, both are about equally accurate to predicting arrest.
And also, if you try any other machine learning method, boosted decision trees right
and for us, anything you want, you try it on the same data from Florida and it'll be
almost equally accurate to both compass and corals.
So as far as we can tell, for predicting arrest, there doesn't seem to be any benefit
of using a very complicated black box model, you can use a very small decision tree to get
it.
It's a lot of computational work to produce that decision tree.
But once you have the decision tree, there's no clear reason to use anything more complicated.
When I think of black box models and you referenced this earlier, I think a lot of neural networks
and deep learning and they are also notoriously computationally expensive and also data,
you know, sample intensive, do the models that you're the interpretable models like corals,
which you're also describing as computationally expensive?
Do they, are they applicable for similar problems?
And if so, do you have a sense for their relative computational expensiveness?
Well, okay, so that gets us into computer vision.
So there are certain problems that are just different from the other problems.
So computer vision, speech, there are certain natural language processing problems.
Some of these particular problems are the ones that machine learning algorithms have
been super successful at starting from around 2012.
And also for computer vision, the notion of interpretability changes, right?
Even before we get to computer vision, there is work that's starting to happen and people
starting to look at applying deep learning to tabular data.
This has been done, you know, for a couple of, well, a couple of the examples that come
to mind for me are some of the Kaggle competitions, and this is discussed in the Fast.ai course,
quite a bit.
The idea being that, hey, we can throw a deep learning network against this, and we
don't have to have data scientists with lots of domain experience, people that know how
to create deep learning models, you know, but don't know anything about the domain can
perform all on these products, you know, just by throwing a deep learning model against
these kind of tabular problems, the one that, one example is the Rothman stores, I think
was the name Kaggle competition, where you're trying to predict, I forget store sales
or something like that based on, you know, some set of tabular data, meaning not image
or audio or any kind of media, you know, just regular data, like you might apply to, you
know, a more simple machine learning model.
Yeah, I mean, there's definitely a lot of domains in which, well, not a lot, but there's
at least several domains in which neural networks are just the best, and there's no question
about that.
But there's no reason you can't also have neural networks that are interpretable.
So we've been trying to design these neural networks for computer vision that are, that
they do a form of case-based reasoning, so that they don't just produce a prediction,
like, yes, this is a Siberian husky.
The networks that we're trying to produce give you the reasoning process behind why it
thinks this is a Siberian husky.
It says, the network says, I think that this part of the image looks like this prototypical
ear of a Siberian husky that I've seen before, and this prototypical, and this part of
the paw of the Siberian husky looks like this prototypical paw of a Siberian husky that
I've seen before.
And so, you know, even with neural networks, you can still add interpretability constraints
and, you know, not lose accuracy when you make predictions.
So even a deep neural network doesn't have to be a complete black box.
And one of the things that I've come across is like this, you know, turtles all the way
down problem.
Like, is it, is the, the interpretable model some kind of hybrid of, you know, explainable
and, you know, the model, or is it like, what makes it inherently interpretable versus
some explainable feature kind of bolted deep inside the architecture of the, the black
box model?
Does that make sense?
Yeah, so, you know, what we're trying to do is design a deep neural network that will
explain its predictions in a way that's similar to how a human would describe its reasoning
process behind the predictions.
We've been doing a lot of work on the CUB data sets, the bird identification data sets
because bird identification is kind of difficult for humans and we can get reasonable explanations
out of them.
But the goal is to actually embed the network with the explanations, you know, with constraints
as part of the explanation so that when the network says, I'm using, I'm comparing this
image to these other images and that's how I'm making my decision.
The network is, it's, this is not a post-hawk thing.
This is actually part of the network.
It's saying, it's saying, here is my decision process.
I'm taking the image.
I'm comparing it to this, this, this and that image.
And because I think this looks like that, this looks like that, this looks like that,
and this looks like that, and that's why I think this is a clay-colored sparrow as opposed
to a robin or something.
Okay.
So it's not a post-hawk.
It's, it's not, I trained the network and then afterward I tried to figure out what
the network was doing.
This is instead the network saying, this is the computation I'm doing and the, the, the
ex, you know, this is the reasoning process behind why I made that prediction.
Do you, do you understand the difference?
I do understand the, I do understand the difference between those two, trying to think if there's
a better way to describe the distinction that I was trying to make.
The way I like to think about it is kind of like a real estate agent pricing houses.
Mm-hmm.
You know, the, the way real estate agents price houses that they look for comps, right?
They look for comparable houses in the same, you know, maybe in the same neighborhood.
And they say, well, that house has a, it's about the same square footage as yours.
And it has a big backyard and yours has a big backyard.
And this other house is right down the block.
So the, you know, I'm getting a price from the location.
And so the real estate agent combines all of that information from the comparable houses
to try to create a price that is the real estate agent's prediction.
And they're explaining to you how they came up with that price.
It's like, I'm comparing, oh, they're backyard to your backyard and so on.
And so the models that these interpretable computer vision models are they, at least
the ones that you're working with, are they explicitly referencing some database of,
you know, they, when they're making their decisions, are they referencing, is it like an
information retrieval thing where they're referencing some database or some set of images
from the training data, or is this, yes, the referencing images from the training data.
They're saying, you know, I've seen a prototypical Robin before.
I have a whole bunch of them in my database, you know, they, they, the throat of a Robin
looks like this prototypical throat of a Robin that I've seen before.
And it doesn't know what a throat of a Robin is, it's just highlighting part of the image
and saying, I think this part of your image is similar to this part of this prototypical
Robin image.
And that similarity is what I'm using to help make my prediction.
And so you, you've alluded to the difficulty that humans have in explaining some of their
decisions in bird watching and in other areas, is the, you know, does that mean in a sense
that the goal is to, to have kind of superhuman performance, not, not in a sensationalistic
way, but to create the models that are better than humans at explaining what they're doing.
I mean, there's a lot of critique of kind of the whole explainability thing that there's
research that says that we make up things when we're asked to explain things kind of like
how you're saying models do.
Yeah, that, that, that's part of the reason why we're working on the recidivism project
is because, you know, judges are humans and humans are biased black boxes, right?
Humans, right.
They, they claim they're making a decision because of acts, but the real reason that they're
making the decision is unknown to them and to everyone else.
And so you have different judges making different decisions for different reasons.
And so the whole point was to have these very simple models that would kind of get everybody
in the same page and, you know, be more consistent and be more accurate.
Because also, you know, humans, humans can't process whole databases in their head and
come up with accurate predictions, we're just not good at that.
And that's why we rely on machine learning algorithms.
So you know, we're hoping that by, by sort of leveraging large databases to make accurate
predictions and having explanations that are, you know, understandable to human experts
that will hopefully be able to help get everyone on the same page.
And then, you know, if the human decision maker has extra factors that are not in the database,
they can calibrate those extra factors into the model that they started with, right.
So if the recidivism prediction model says there's a 76% chance you'll recidivate, but
let's say there's some additional factor that the judge knows about that's not in the database.
The judge can use that as a mitigating factor to change their decision.
So can you tell a little bit about your future directions in this area?
Well, we've been working on optimal decision trees for a really long time.
I didn't even know how many years I've been working on optimal decision trees.
I've also been working on optimal sparse linear models with integer coefficients for a long
time.
So these are like medical scoring systems.
These are models that give you like one point if your age is above 50 and then two points
if you've had a prior heart condition and so on.
So those point scores, we're trying to fully optimize them based on data which substitutes
for having a team of doctors in a room trying to decide the point scores.
We're using, we're instead using large databases and machine learning algorithms to design
the point scores, but the final models are very similar to what the doctors in the room
might have constructed themselves.
So we've been working on that for quite a while.
So these are, this is optimal scoring systems and then the optimal neural networks project
we've had going for a few years, sorry, not optimal neural networks, interpretable neural
networks.
We've had the interpretable neural networks project going for a few years.
And then I also have several projects on causal inference where we're trying to do interpretable
matching for causal inference.
So if you have a treatment group and a control group, so people who've taken the drug and
who haven't taken the drug and you want to determine what the effect of the drug is, then
you would normally try to match, you normally try to match people with their identical twin.
People who've had the drug match them with an identical twin who didn't have the drug
and then you could figure out what the effect of the drug is.
So we have a project on doing that in a more accurate way.
I'm also still working on criminal recidivism.
We've been working on that for several years now.
We had a paper published in 2015 called interpretable classification models for recidivism prediction
where we showed that the interpretable machine learning models were just as accurate for
predicting all different kinds of recidivism than complicated machine learning models.
So they're just as accurate as the complicated machine learning models.
So we were still working on recidivism and then I have a lot of projects on health care,
which is another high stakes domain where you care about interpretability.
You want to make very careful decisions in health care.
If you had to call out an area that you think needs more work or attention but is not
one that you're working on, is there any particular thing that comes to mind?
I mean, there's a lot of societal good applications that are almost impossible to work on because
gathering the data is very, very difficult.
I wish I could work on more problems like that but the data is just so hard to get that.
Any particular example comes to mind?
Well, I think income inequality in the world is a huge problem for instance.
Like any major world problem, we should be throwing data and algorithms at it because
we can.
We have people, we have expertise.
People want to work on those problems.
It's just that it's hard to do it.
Yeah.
Those are more data problems than algorithmic problems.
I think health care is a huge frontier.
There's a lot of people working on health care.
I think a lot of people realize how important it is but the data is really messy, is confounded
in every possible way you could imagine.
But it's also really important because if we understood what the right cancer treatments
are and what to do about the opioid epidemic, that would be amazing if we could solve some
of these problems with algorithms and so on.
In terms of algorithmic challenges, one problem that we're just starting to try to tackle
is whether you can know if an interpretable model exists before going to the trouble of
finding one because finding them is computationally very demanding.
Right.
Before you go and do all that extra work, it'd be nice to know in advance whether you're
likely to find an interpretable model.
So we're just starting to think about that question now.
Well Cynthia, thanks so much for taking the time to chat with me really, definitely really
interesting work and a lot of thought-provoking issues that are involved in it.
Okay, my pleasure.
Thank you.
Alright everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platforms
Conference.
As always, thanks so much for listening and catch you next time.
