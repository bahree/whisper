Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
In today's episode of our strata data series, we're joined by Viviana Aquaviva, associate
professor at CityTech, the New York City College of Technology.
Viviana led a tutorial at the conference titled, Learning Machine Learning Using Astronomy
Data Sets. In our conversation, Viviana and I discuss an ongoing project that she's
a part of called the Hobby Everly Telescope Dark Energy Experiment. In this project, Viviana
tackles the challenge of understanding how and why the expansion of the universe is accelerating.
We discuss her motivation for undertaking this project, how she gets her data, the model
she uses and how she evaluates their performance. Before we move on, I'd like to send a big
shout out to our friends at Cloudera and Capital One for their continued support of this
show and their sponsorship of this series. Cloudera's modern platform for machine learning
and analytics, optimized for the cloud, lets you build and deploy AI solutions at scale,
efficiently and securely, anywhere you want.
In addition, Cloudera Fast Forward Labs' expert guidance helps you realize your AI future
faster. To learn more, visit Cloudera's machine learning resource center at cladera.com-slash-ml.
At the NIPPS conference in Montreal this December, Capital One will be co-hosting a workshop
focused on challenges and opportunities for AI and financial services, and the impact
of fairness, explainability, accuracy and privacy. A call for papers is now open through
October 25th. For more information or to submit a paper, visit twimlai.com-c1nIPPS.
That's C the number one NIPPS. If you love this show, you've got to love our sponsors
because they help make it possible, so please take a look at what they're up to and tell
them Tommel sent you. And now, onto the show.
Alright everyone, I am on the line with Viviana Aquaviva. Viviana is an associate professor
at City Tech, the technical college at the City University of New York. Viviana, welcome
to this week in machine learning and AI.
Thank you very much Sam, it's a pleasure to be here.
It's a pleasure to be chatting with you. You recently did a tutorial at the Stratocomference
in New York, where you presented on learning machine learning using astronomy data sets.
Before we dive into some of your work with machine learning and astronomy, can you tell
us a little bit about your background?
Yes, absolutely, so I'm an astrophysicist by training, so I did my bachelor degree
work actually in theoretical physics, move on to astrophysics for my PhD, and I got my
PhD in 2006. And I've always been fascinating with numerical techniques, so even before
studying working with machine learning, I was working on Bayesian techniques for parameter
inference. And then, just before joining the faculty at the City University of New York,
the year before decided, okay, I'm going to take a graduate course in computer science.
This may be my last chance to actually see what is about, and I took a graduate course
in machine learning, and I guess I was hooked at this point.
And so, a little by little, I started incorporating my research. I took a summer off to do some
Kaggle competitions, sort of to see where I stack, compared to other data scientists.
And now, I guess that most of my research work in astronomy actually is based on one
and another machine learning technique. And how did you end up stacking when you compared
yourself to other data scientists on Kaggle? Well, not too bad, I guess. Otherwise, it
would have probably stopped. But I guess I got, like, you know, a typically was I felt
like in the first, like I could get to the top 15, 20%, and then I think at that point,
it's really hard to keep climbing the leaderboard, and I wasn't too interested in becoming
the best. I just wanted to make sure I wasn't like a complete fool.
It's so funny. So, a bunch of folks in the Meetup associated with the podcast have gone
through the fast.ai course, and that course is taught by Jeremy Howard, who for a long
time was the chief strategist or chief scientist, I should say, at Kaggle. And so, competing
in Kaggle competitions and using Kaggle data is a big part of the methodology for this
course. And you're right, it is on the one hand, you know, it takes a lot of work, but
it's not too hard to get within that top even 10%, 15%, but to get from there to the
top, I guess like in anything else, like that's a lot more work.
Okay, the last 10% of a project is at 90% of the time, isn't it? Exactly, exactly, exactly.
One of the things that you mentioned in our chat prior to starting was that a lot of
your recent research has been focused on a large galaxy survey and trying to detect dark
energy. Can you tell us about this large galaxy survey, what's the data that that has
created and what is dark energy? Well, I can't tell you what is dark energy, unfortunately,
I can tell you how much we know about it. And the other thing, most of the work is with
a bunch of galaxy surveys. So I'll give you like a very high level, I promise, overview.
So what is dark energy? Let's say that like about like 20 years ago, scientists were
quite puzzled to discover that the expansion of the universe is accelerating. Now this may
not sound like much, but we are unaware of any force that could cause this to happen.
In fact, if anything we thought that if the expansion of the universe started with a
big bang that sort of provides the initial momentum, the only other force that we expected
matter at distances typical of galaxies, but even like planets really is gravity. And gravity
wants the opposite thing. It likes all things to be bench close together. And so it would
have the effect of slowing down the expansion, not accelerating. So when we discovered that
this was happening, it was a big shock. It took us about a decade to accept it. It was
the Nobel Prize for Physics in 2011. And then it left all of us scientists quite at
a loss for what to do and how to search for these mysterious components. So we called
whatever mechanism is responsible for that dark energy. And I guess one of the big challenges
of the last 10 or 15 years has been to understand how dark energy has evolved throughout cosmic
time. So the universe is about 14 billion years old. We have so far been able to measure
the fact that the expansion started as slowing down as expected and then sort of picked
up speed, sort of halfway throughout the life of the universe. And now what we are trying
to do is to understand what happened even earlier to see if this gives us a way of distinguishing
between different models. So this is what this big galaxy survey
issue mentioned is called Hectics. The Hobby Eberly Telescope dark energy experiment has set
out to do. And so we want to look for probes of how the universe was expanding say 10 billion
years ago. How can we do that? Well, we rely on the fact that I feel like it's pretty amazing
that when you look far away into space, you're also looking back in time. And that's because
light from distant objects say galaxies take a certain amount of time to come to us.
So if you're looking at something which is one million light years away, you're looking
one million years back in the past. So if we can do the same game with galaxies that
are say 12 billion light years away, now we're looking 12 billion years in the past. So
what this service is doing is I guess looking for about a million of these very, very far
away galaxies. And it's selecting them on the base of distance because this corresponds
to a selection in time back in the past. And then the distances to these galaxies tell
us basically how much the universe is expanding since then. So why machine learning? I guess
the answer there is the problem with these galaxies is that we have these observations
and observation just basically tell us, hey, there's a spike in luminosity at this point
in the sky. But you know, the sky is a two dimensional object. And so it doesn't come
with distances. When we observe something, we don't know a priori how far away it is.
So the problem is that some other galaxies that are nearby may mask as the firewall ones
that we want to find. And so we're using machine learning to create a learning set using
galaxies for which the distance are known and then to develop a method to tell us whether
the galaxy is the far away one that we want to keep or some sort of nearby contaminant.
There is so much there to dig into. Even the basic astronomy and physics of this creates
a lot of questions for me. So we're, when we're looking at these distant galaxies through
this survey data, to what degree are we able to kind of infer expansion from the galaxies
from a astronomical perspective is a relatively short period of time. If we're looking at this
over the course of earth years, can that really tell us a lot about the motion of these
galaxies billions of years ago? Well, no, you're absolutely right. So we don't do for every
galaxy, we only get one and one only snapshot. So we can't hope to detect some sort of evolution
because as you mentioned, earth years is basically nothing compared to the evolutionary scales
of these galaxies. But what we can do is to say, okay, if I can pinpoint how long ago
this galaxy is observed and how fast it was moving. And if I can do this for several
epochs of the universe, I basically get the chart of velocity of expansion versus distance.
And this is what the expansion history of the universe is. So for it's true that I cannot
observe this single galaxy more than once. But if I can get the distance at that time and
the velocity of the expansion at that time, then I can tell whether the expansion overall
was accelerating or decelerating. And so how do we get the velocity of expansion at
a given snapshot in time? Well, the velocity, great point, comes from something called
predshift. So what happens is that when we measure, say, the light emission from certain
atoms, for example, hydrogen, which is the most common atom in the universe, in the lab,
we observe some spikes corresponding to atomic transition, say, of this atom at certain
wavelength. However, when there is motion between source and observer, as is the case when
we're looking at, say, hydrogen atoms in distant galaxies that are moving away from us,
these same transition lines, they are observed at different wavelengths. And there is a very
simple relationship between these two wavelengths that gives us exactly sort of the expansion
factor of the universe in between or if you will, the velocity of the expansion. So
there's something similar to Doppler effect for sound. The fact that when we hear an ambulance
coming towards us, the pitch becomes higher. So this is the same thing, but for light waves
rather than from sound waves. And so the task is then to measure the light that we're receiving
from as many of these galaxies as possible. And because galaxies can be obscured by other
galaxies, we need techniques like machine learning to tell us. It sounds like an attribution
problem. We've got some light. What galaxy do we attribute it to?
Not quite in the sense that it's not like confusion of the sources. The point is like,
you know, when you look at a galaxy through a telescope, you just see a speck of light.
And so, you know, without a proper setup, it's difficult to understand what it is looking
at a spectrum with lines in a certain place. It would be different lines, for example,
lines from oxygen from a spectrum that is not very redshifted, meaning a galaxy that
is close by. Or if you're looking at a spike that looks like that is an hydrogen line coming
from a much further away galaxy, just they look the same.
So the machine learning is used to do, is it classification? Then is the underlying problem
of, you know, what is happening in this...
Supervised classification problem.
Supervised class of now, you're speaking my language.
Finally, we can go back to Earth.
Right, exactly. So the nature of this data set is there, you mentioned a data set that
is from the Hubble and another telescope. What does the data set look like?
Well, it's actually not the Hubble. The telescope is called the Hobby Eberli.
Hobby Eberli telescope, it's a spectrograph and it's mounted on a telescope and McDonald Observatory
in Texas.
And so you've got a time series of spectrograph readings?
Well, we got, let's see, it's not even time series, it's just sort of like combined observations,
let's say, you know, and for us, I guess longer exposure time usually helps.
But what we have is, let's say, where we detect a spike in the spectrum of the galaxy and
by say, where I mean, like a which wavelength? And what is the strength of the signal?
This is the primary data. Now, just with that, you couldn't do...
You couldn't set this up as a classification problem. So what we do is that we also do use
so-called imaging data, which are basically pictures of galaxies.
And these are usually data that are less deep, so they are less able to see very faint objects,
but they have observation in several bands or you can think of them in several colors.
And this is the case where, for example, it may happen that some of these objects have been observed
by Hubble's face telescope. And so we array had data for those or data from other surveys.
And so when we combine data from the spectrograph and these images,
now we can, on the one hand, get our learning set, get some galaxies for which we know
enough to tell how far away they are.
For this image data, when we're talking about galaxies that are 12 billion light years away,
what kind of resolution are we able to image these at? How many pixels across
to be get for a galaxy? Great question. They're tiny. They're little specks most of the time.
So all the shapes that you may think, when you think about a galaxy with spiral arms, these are all gone.
Yeah, yeah.
Sometimes we have a few pixel resolution. If you're looking at space telescope data,
sometimes we just get like one little blob.
And you're talking, you kind of walk folks through the application of basic machine learning techniques
using astronomy data sets is the implication that these data sets are available
for anyone who wants to experiment with them.
It depends. In this case, in the case of this particular service,
the simulations that we used and also the we scientists used up to, let's say a year ago,
when data started coming in are often available.
The data from the Hubble telescope are at the moment not public yet.
But I have to say that I think one great thing about astronomy is that most of the best data
are publicly available very shortly after having been taken.
For example, we present proposals to take data with the Hubble.
And in most cases, we have to actually sign a close, it says,
we only have six months or one year property period.
And so maybe walk us through some of the ways that you can start with this data
and learn things about these distant galaxies.
Sure, well, I guess that for us, the important thing, as you know,
this is a supervised learning problem.
And so we have, we say, okay, we have some data from the spectrograph that tells us basically,
okay, I'm seeing one of these lines.
I don't know what line it is.
It could be hydrogen from a distant galaxy, yay.
Or it could be, say, oxygen from a nearby galaxy, though.
So the first thing that we had to do is understand what kind of learning cell we can create
and to do that, we assemble ancillary data from other telescopes.
And in this case, in other words, we calculated for about 10% of the galaxy in our survey.
We had enough information to be able to give them a clear label.
This is a very far away galaxy.
This is a nearby galaxy.
And then we used these sets of simulations, because we didn't have the data at the point yet,
but still assuming that for about 10% of the objects, we would have the labels.
And what we did in the tutorial was actually how we started out.
So saying, okay, we have our features.
We took a look at, you know, we did a little bit of basic data visualization
to understand which ones were the most important ones and which ones may be redundant.
We got rid of our layers and things like that.
And then we could see that, you know, we started with things like decision trees and caneers neighbors.
And we could see that we could obtain accuracy scores of around 90% right away.
Now we moved on to more sophisticated algorithm doing parameter optimization.
So we took a look at support vector machines.
I know that these are all fashion, but they're really, I feel like I'm quite affectionate to them.
So then for pedagogical purposes, they agree.
And we also, they run on forest.
And you know, I felt like I had to went to go into actually boost since it's, you know, all over Kaggle competitions.
And I just showed them though, I feel like for me, the most important things were to show them looks in real life.
Picking an algorithm, even the best of them and optimizing it is never enough.
And actually, it doesn't have nearly as much impact as, you know, being able to tailor your algorithm to your data.
So for us, for example, one of the issues that we ran into was that the two populations of objects that we are looking at,
let's say let's call them like close and far galaxies, they're not present in equal measure.
This is a fairly imbalanced data set.
So as you know, accuracy becomes a very poor metric in that case.
How did you address that?
Well, for us, and I believe that this should be a case.
What we did is we defined an ad hoc metric.
So rather than, you know, because I mean, you can do different things.
You can decide, okay, I want to be as complete as possible and optimize recall.
These are fairly standard things, right?
But I think in general, what you want to do is to ask yourself, okay, what do I really care about?
And the metric that you probably care about for us, it was the error in the determination of distance to these galaxies.
And of course, this is not something that is readily available.
So what we did is we used computer simulation to create a fitting formula,
where we mapped the relationship between the error in the distance,
which is the thing that we wanted to keep as small as possible.
And you know, a complicated polynomial function with a coefficients of precision recall.
And then once we could fit our coefficients, we use these.
We introduce this metric as the evaluation metric in all of our algorithm and we optimize for that.
And I feel like this has a much bigger impact than choosing one algorithm over another.
So just to recap that, you, the fundamental problem you're faced with here is a limited amount of training data.
So in order to overcome that problem, you looked at other data sources that could provide labels for you.
You were able to collect labels on some 10% of your ultimate data set, but that wasn't enough.
So in order to synthesize additional labels, you fit your original labels to kind of a polynomial formula.
Is that am I interpreting that correctly?
I felt like I would just maybe want to lightly edit the last stop.
So well, I guess one thing that I felt quite different in academia and industry is that in industry,
there is such a thing as good enough.
Sure.
But for us, we are never going for good enough.
You know, for us, I guess that always the ultimate goal is to come up with the best possible.
And so in some sense, we never stop optimizing.
So for us, what we wanted to do was to define and had a hot evaluation metric.
Because we felt that the ones that say, you know, you can pick from a standard package, say, you know, we program in Python.
So we use scikit-learn.
And so you can train, you can build your models, optimizing for standard metrics, such as accuracy, precision or recall.
But for us, really, what we wanted to optimize was a different quantity that becomes, sorry, that depends in a non-trivial way on all of these.
And so we were able to write our metric as a complicated formula that involves all of these things, like precision and recall in particular.
And then we introduced this ad hoc metric.
Okay.
So this wasn't about data generation so much as coming out with your optimization metric.
And you came up with a polynomial formula for the optimization metric that was more robust than any of the simpler metrics that you could have used.
Right.
And I don't know if I would say that more robust, but just better tailored to our problem.
It gave us a better result in the end.
Did you iterate to that solution or did was it clear early on that you had to go in that direction?
Well, this is research, right? So this is definitely not the first thing that we tried.
But as I mentioned, you know, like another thing that we did was to, you know, try a whole bunch of different algorithms.
But then I feel like this was the key point that, you know, photos gave us a larger improvement overall compared to, you know, say picking one method over the other.
And so what was it in your experience up to the point that told you that this should be something that you try?
Great question. I don't know. I think that in general, you're looking at methods and you're saying, OK, what is the, you know, workspace for me?
And, you know, as if I actually, a Strada, I heard one of my favorite talks was exactly about this, about how often, you know, many businesses sort of base their projects and their plans on one single metric that may not enclose all the information that we need.
Because it's not one number that drives the business value of a company. And so I felt exactly the same.
I felt that neither of these simple numbers was actually most directly related to what we really wanted to minimize, which was the uncertainty in the measurements of our distances.
And so I thought, OK, I'm going to write in myself. And then I had the problem of saying, OK, my language is science. So I can tell you what I want to minimize is this quantity.
But the language of algorithm talks about accuracy and precision, recourse and full positive and false negatives.
So how can I translate my metric into their metric?
And so I figured out that, you know, I could write a formula and that I could, you know, use simulations to find the best coefficient to complete my mapping.
And then it kind of worked.
And so you've got this optimization metric. You've got the data. What was the next step?
The next step is a bit sad. It was to take more data. Well, actually, it's not sad at all. But I think what we learned from our exercise was that we were not ready yet to measure their energy at the desired accuracy point.
We just realized that even after trying to push all the frontiers we had, we were still not doing well enough to be able to constrain dark energy at the early times.
And so we sort of reversed the problem and said, OK, now that we understand how data and uncertainties are related, what addition data would we need?
And so we gave recommendations to our observer colleagues and say, hey, you know, in order to get to where we want to be, we need say an imaging survey that is wider, that is deeper than what we had so far.
And we could even recommend, OK, we should, you know, say preferably observe in the near infrared as opposed to in the optical.
And does that think it's an aspect of much learning and learning that I really love you can use your argument to inform the best way of collecting your data.
The immediate question for me is, you realize that hey, it would be great if we had this additional data set, you know, it was in infrared band.
You know, in astronomy, if that data is not there somewhere, right, it's very expensive to deploy a kind of collecting apparatus for that data.
Were you so fortunate that the data that you needed was available?
Oh, no, we knew already that it wasn't available.
And we knew that we needed it.
So then how do you go about getting it?
Oh, you write proposals. So there are, you know, a few, I would say several good observatories in the world.
And in general, they all work in similar manners, astronomy write proposals.
And they describe the project they would like to carry out.
And why it is important and why it is needed and what's going to be the usefulness for the larger astronomy community.
And you go through a peer review process is a very competitive process.
I mean, getting time on the Hubble Space Telescope in general is only awarded to about 10% of the all the observing proposals.
Some ground based telescopes are a bit easier to get time on.
And we actually, we are underway.
I would say that about 70% of the additional data that we need has already been awarded.
So we are carrying out the observation at the moment.
And then we continue this process, you know, every couple of months there is a call for proposals.
And every couple of months we present our problem and need for new data.
And we hope that the panel is understanding.
And that's how it works.
And so how do you anticipate using the new data once you get access to it?
I think what we are doing is introducing additional features.
Just a discriminatory power of the features that we have wasn't sufficient.
And we realized that, okay, if we can have, you know, X amount of additional time devoted to every snapshot of the sky,
then these additional features will take us to where we want to be.
This is a great example of how the real world differs from a Kaggle competition, no?
Absolutely.
Well, I don't know.
I always feel I'm also not quite part of the real world because hell we think of industry as the real world as academics still, you know.
It's in a fairy tale.
But yes.
So this was the dark energy detection project.
And that's one of, one of several that you've worked on that apply a machine learning to this, in this general domain of astronomy.
What are some other things that you've done using machine learning?
Cool. So I have like a project that I'm really excited about that.
And now we have the first paper draft is near completion.
It was done with a grad student called Chris Lovell from the UK.
He came to visit me because he wanted to do a machine learning project as part of his dissertation.
I said, amazing.
Let's, that's with something new and crazy that I don't ruin your career.
But he's very bright, so hopefully not a case.
And so what we're trying to do is to recover the so called star formation histories of galaxies using machine learning.
So the whole idea here is that, you know, galaxies are collections of stars.
But stars in galaxies, they were not all sort of born at the same time.
There is a process through which galaxies tend to assemble their stars.
And now we know that in general, you know, we start for small objects and then they keep growing.
And so, you know, the basic picture is that if you think about the early universe, all you have is gas, hydrogen and helium.
And gas is the basis for stars is the raw material for stars.
You need cold gas and then you need gravity and then little by little, you make single stars.
So, you know, the very basic picture that we had is that, okay, in the beginning, you have a lot of this gas.
It's still too hot. Once it cools down enough, then gravity can sort of take over form stars.
And so typically, galaxies will have a fairly active phase of star formation.
At one point, all the gases locked in stars and most stars live long lives.
So once a gas is locked into stars, there is no way you can get it out.
If the star doesn't explode or just ends its life.
And so, then at some point, galaxies are thought to so-called quench. Stop their star formation.
Now, this is a very simplistic model.
Because we know that there are a whole bunch of other processes that can affect this star assembly process.
For example, galaxies may collide with one another, they can merge with one another, they can be affected by environment.
And so, capturing the complexity of this process of star formation history using the traditional method of fitting models is actually quite complicated and has some biases.
And so we thought, well, you know what, in the last five years, because we have big data, we have big computers, we have big simulations too.
So we have some amazing cosmological simulations, which are basically simulation of the process of star and galaxies formation through portions of the volume of the universe that are respectable, not too small.
And so we thought maybe we could train some machine learning algorithms on the simulations and then apply them to data and get the star formation history of galaxies another way.
So this is what we're doing. Here's in mostly convolution neural networks.
And I have to say that it seems to work decently well.
Is the input data that you're receiving, is it visual in nature, or are you using CNN's on non-visual data?
Great question. Yes and no. So CNN traditionally are built for images, right? As you mentioned.
But we thought actually, well, we are fitting the CNN is sort of like a one dimensional image called a spectrum.
Okay. It's, you know, a very weekly line, but the wiggles mean something. And so what we actually thought is that in some way, even if we're not fitting an image, CNN should be good at picking up special correlations in our spectrum that are meaningful.
And we know that some of these sort of like jams and lines and wiggles are associated to signatures of active star formation. And so that's why we thought that this could be a good matter. It's not the only one that we use, but we also use one of these extremely randomized trees, which I think, you know, they're much easier to understand the CNN.
And so I felt like for exploratory purposes, we use that as well. But CNN's were overperforming other metals. And so this is what we stick to.
It's touched on this very briefly, but did you do as much like the optimization, you know, crafting the optimization function or using off the shelf, CNN's and standard optimizers.
So, well, we did have to put in our own evaluation metric once again. And this is just because of the nature of our problem. We are looking at basically what we, our output in this case is a whole history of star formation. This means a basically we're outputting eight numbers. I mean, this is eight is a parameter, but it's called eight.
And so we're saying, okay, in eight different snapshots of the history of the universe, how many stars were being formed. So, and this is because, you know, we are basically outputting a function of time. So we had to split it just so, you know, we can write in numbers. And as you can imagine, these are fairly correlated outputs.
And so understanding what's a good fit, not to a number, but to a series is complicated. And so in the end, you know, we wanted something that looked a little bit like percent error.
Sort of the average percentage error at each of these time shots, but we couldn't do percentage error because there are plenty of times in which a galaxy is not forming star at a significant rate.
And this make percent error becomes fairly large for no reason. And so we're using sort of a version like that is corrective or that calls made symmetric mean absolute percentage error.
That's also like attempts to correct for times, and which not much information is going on.
So, yeah, putting on our own metric was, of course, one of the steps. Now CNNs are hard to optimize just because there are so many parameters. And also, you know, the timescaling time complexity is not the most user friendly.
And so other than that, we just played a little bit with the number of layers and other basic parameters. And we didn't do, I feel like a full optimization.
And you mentioned the time scaling is difficult. What aspects of that heavy phone challenging.
Oh, just how long they take.
Ah, OK, so training time.
Yes, just compare the scaling. I mean, with the size of the data set and the fact that our data set, we are feeding the spectrum, which means that each input has several thousand points training times scale up fairly clean.
So a couple of really interesting examples of how you're applying machine learning to astronomy.
If I'm whenever I do a show on astronomy, I get a lot of feedback from folks who have kind of, you know, been long fascinated with, you know, the universe astronomy and are curious about how to get started, how to play with this data.
What practically is required to play with this kind of data? Is that something you spoke to at all during your tutorial?
Absolutely, because I also feel like the audience of my tutorial was, you know, mainly people as you describe, I just felt, you know, astronomy is fascinating.
And so many of the people who came were not just interested in learning basic machine learning or even, you know, intermediate machine learning, which you can do in many ways.
But specifically in, you know, how can we play with astronomy data? And so to, you know, get a glimpse on different problems in astronomy research.
So I don't know, I guess I don't have an answer right away. I think that it's true that I have collaborated in the past with people who have approached me at conferences, known astronomy, perhaps computer scientists, and ask, hey, you know, can I do something with you?
And in general, I think that it's very interesting, and I find it very useful, but there is often a little bit of a language barrier.
And I do think that the knowledge of the subject is quite essential. But I have advised several of the people I've seen at Stradah, who asked me a similar question, that if they do have an interest, I think they will be most welcome to email, you know, professors in different departments.
In their community, just in the New York area, we have so many of these institutions and say, well, I'm, you know, interested in working or volunteering some hours for an astronomy project, we have anything that is available.
I feel like in some cases, the answer may be no, just because, you know, when you're ready, I feel like deep enough into a project, you need to be talking about the science aspects more.
But I feel like a lot of sort of like exploratory problems. I feel like their help will be really welcome and perhaps fun.
Awesome. Well, Viviana, thanks so much for taking the time to chat with us. I was great hearing about what you're up to, and I appreciate it.
Thank you so much, Sam. It was a pleasure to chat with you, and have a great rest of the day.
You too.
All right, everyone, that's our show for today. For more information on Viviana or any of the topics covered in this show, visit twimmalai.com slash talk slash 184.
For more information on the entire strata data podcast series, visit twimmalai.com slash strata and Y 2018.
Thanks again to our sponsors, Cladera, and Capital One for their sponsorship of this series.
As always, thanks so much for listening, and catch you next time.
