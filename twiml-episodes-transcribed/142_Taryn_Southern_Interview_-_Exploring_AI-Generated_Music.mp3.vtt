WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:33.520
I'm your host Sam Charrington. A big thanks to everyone who made it out to our meet-up

00:33.520 --> 00:38.680
this week. It was our biggest event yet with nearly 50 participants online to discuss

00:38.680 --> 00:47.320
the YOLO Object Detection System. If you missed it, you can check it out at twimlai.com-meetup.

00:47.320 --> 00:51.880
While you're there, you can also sign up to learn deep learning with us this summer.

00:51.880 --> 00:56.400
I'll be working through the fast AI practical deep learning for coders course starting in

00:56.400 --> 01:00.880
June and I'm organizing a study and support group via the meet-up.

01:00.880 --> 01:05.960
I've been blown away by the interest in this. This is a great course and fast AI co-founder

01:05.960 --> 01:11.520
Jeremy Howard encouraged our group on Twitter noting that groups that take the course together

01:11.520 --> 01:15.000
have a higher success rate. So let's do this.

01:15.000 --> 01:20.480
Three simple steps to join. One, sign up for the meet-up at twimlai.com-meetup, noting

01:20.480 --> 01:27.400
fast AI and know what you hope to learn box. Two, using the email invitation you'll receive,

01:27.400 --> 01:33.480
join our Slack group. And three, once you're there, join the fast AI channel. This is going

01:33.480 --> 01:34.480
to be great.

01:34.480 --> 01:39.960
Alright, in this episode, I'm joined by Taran Southerne, a singer, digital storyteller

01:39.960 --> 01:47.800
and YouTuber whose upcoming album, IMAI, will be produced completely with AI-based tools.

01:47.800 --> 01:52.480
Taran and I explore all aspects of what it means to create music with modern AI tools

01:52.480 --> 01:57.600
and the different processes she's used to create her singles, break free, voices in my

01:57.600 --> 02:02.440
head, and more. She also provides a rundown of the many tools that she's used in this

02:02.440 --> 02:08.320
space, including Google Magenta, Watson Beat, Amper, Lander, and more. This was a super

02:08.320 --> 02:11.320
fun interview that I think you'll get a kick out of.

02:11.320 --> 02:15.320
And now on to the show.

02:15.320 --> 02:22.480
Alright, everyone. I am on the line with Taran Southerne. Taran is a digital storyteller,

02:22.480 --> 02:28.600
a YouTuber, and a singer whose new album, IMAI, is composed and produced entirely with

02:28.600 --> 02:33.320
artificial intelligence. Taran, welcome to this week in Machine Learning and AI.

02:33.320 --> 02:35.360
Thank you so much for having me.

02:35.360 --> 02:39.480
How do we get started by having you tell us a little bit about your background and what

02:39.480 --> 02:42.360
sparked your interest in AI-generated music?

02:42.360 --> 02:49.040
Sure. I've done a lot of things in my background as an artist. I started out in Los Angeles

02:49.040 --> 02:53.480
as an actress and a writer for television. And then a few years into that, I started

02:53.480 --> 02:59.200
making YouTube videos, mainly just as a way to appease my boredom. And it became very

02:59.200 --> 03:04.280
clear after a few years of doing that that there was actually a way to make a living.

03:04.280 --> 03:08.440
I had a number of friends who were also in the YouTube world who had been early pioneers

03:08.440 --> 03:13.600
on the platform and had built up very large audiences and were living the dream, making

03:13.600 --> 03:18.240
whatever content they wanted to make. And I thought, I want to do that. So I started

03:18.240 --> 03:26.000
making weekly videos in 2012 and did that for about four years. I made a weekly video,

03:26.000 --> 03:31.920
comedy, sketches, music videos, you name it pretty much anything and everything. And it

03:31.920 --> 03:36.040
was a really great experience. I also started producing videos for other companies as well

03:36.040 --> 03:41.040
and built up a digital production company. So over the course of ten years, I made approximately

03:41.040 --> 03:47.080
1500 videos, which is a lot of content. And I think, yeah, it's a tremendous amount.

03:47.080 --> 03:52.400
And I think why this is relevant to what we're talking about now is as a YouTuber or content

03:52.400 --> 03:59.680
creator in the digital world, for better or for worse, your success is ultimately defined

03:59.680 --> 04:06.760
more by the quantity of creation than the quality. And so as a result, you start finding

04:06.760 --> 04:15.040
a lot of tools to help increase your production and optimize your content to look as good as

04:15.040 --> 04:20.720
possible and be as good as possible in the shortest amount of time possible. And so you

04:20.720 --> 04:25.760
almost become like a hacker of sorts in terms of how you edit, how you shoot. You're always

04:25.760 --> 04:31.520
trying to find like the quickest but best path forward to make the best possible content

04:31.520 --> 04:38.240
in a very, very quick period. So as a YouTuber, I learned everything. I learned how to edit,

04:38.240 --> 04:46.400
how to light, how to do makeup, how to write, how to market, how to Photoshop photos, and

04:46.400 --> 04:51.880
how to do all of those things in an extremely short period of time. And with usually with

04:51.880 --> 04:58.200
simpler tools than perhaps what professionals used in each of those categories. And so

04:58.200 --> 05:05.160
ultimately around 2015, 2016, I grew very tired of the hamster wheel of content. And so I

05:05.160 --> 05:12.360
decided to throw in the towel and jump ship into a new area, something that would just really

05:12.360 --> 05:17.040
excite me and interest me. So I found myself drawn to VR and wanting to learn how to make

05:17.040 --> 05:23.120
VR content and do something new and different that didn't require me to make content super

05:23.120 --> 05:29.360
fast. And through that, I began working on a project utilizing artificial intelligence.

05:29.360 --> 05:35.680
I really wanted to explore how humans were using technology. And so due to a grant from Google,

05:35.680 --> 05:41.760
I was working on this experiential VR piece. And I really wanted to incorporate AI as

05:41.760 --> 05:47.120
much as I could. So I was just researching every tool that there was out there. And in my

05:47.120 --> 05:52.800
similar to my YouTube roots, I wanted to find tools that are user friendly, easy to use,

05:53.760 --> 05:58.960
don't require me to learn a bunch of code. And that's how I ended up stumbling upon some of

05:58.960 --> 06:04.080
the current tools that I'm using on my album. And it really was just one of those happy accidents

06:04.080 --> 06:09.520
where I wanted to create something new and different. I liked the idea of not having a roadmap.

06:09.520 --> 06:15.280
And within a few weeks of working with some of the AI tools that I'd stumbled upon, I realized,

06:15.280 --> 06:20.000
wow, I can actually make some cool music with this. Maybe I should turn this into a separate

06:20.000 --> 06:23.120
project all on its own. And that's exactly what I ended up doing with the album.

06:23.120 --> 06:27.600
Oh, wow. On your website, you describe yourself as the first artist to work with artificial

06:27.600 --> 06:34.240
intelligence as the sole composition and instrumentation tool on a music album. What exactly does that mean?

06:34.240 --> 06:40.480
Good question. And I think that the definitions get a little bit hazy when you're looking at how AI

06:40.480 --> 06:45.920
is currently used for music today. And quite frankly, I'm not the first album to incorporate AI.

06:45.920 --> 06:51.040
In fact, Flow Machines just released an album two months ago. I haven't even released my album yet.

06:51.040 --> 06:56.480
I've released two singles publicly. I've got a third one coming out in two weeks and then

06:56.480 --> 07:00.560
a fourth one coming out in June. And then I release the album in September.

07:00.560 --> 07:07.360
What was, I suppose, unique about the first song I released and the second song is September

07:07.360 --> 07:13.360
of last year in January of this year, respectively, was that all of the instrumentation that you hear

07:13.360 --> 07:20.720
in that music was spit out from the AI tool that I used to compose with it. Whereas a lot of the

07:20.720 --> 07:27.280
other AI tools or AI songs that you'll hear currently on the market today, those were software,

07:27.280 --> 07:34.720
those were built from software that essentially created a MIDI track, which then a human used

07:34.720 --> 07:41.280
to transpose into instruments of their choosing. So they could essentially extract the notes

07:41.280 --> 07:48.000
from the MIDI track, separate them according to how they want those notes to be heard and

07:48.000 --> 07:53.360
rearrange those notes, reconstruct those notes. I mean, there was a lot of, there's a lot of human

07:53.360 --> 07:58.160
interaction with those, with those musical pieces and then transpose it into the instrumentation,

07:58.160 --> 08:02.720
which is not a bad thing. It's a great, I mean, it's just, it's another tool that humans can use,

08:02.720 --> 08:08.320
but it requires a certain level, I think, of musical acumen on the part of the user to be able to

08:08.320 --> 08:12.800
make those kinds of choices. Whereas the first two songs I released, you could, you could say that

08:12.800 --> 08:19.200
the tool I used was like the AI for beginners tool, or you don't need to know anything about music.

08:19.200 --> 08:25.600
All you need to do is make choices like a, like a film editor would make choices with, with film

08:25.600 --> 08:30.480
footage. So I think, I think that's actually the best possible example of what I'm talking about

08:30.480 --> 08:37.360
is the tool I use for those two songs ampere. Very easy user interface, front facing, you make choices

08:37.360 --> 08:43.760
like what BPM you want the music to be, what genre, what types of instruments you want to be included,

08:43.760 --> 08:49.680
you can specify them, and then it spits out a finish track. Now from that finish track, you can

08:49.680 --> 08:55.520
download the stems and you can remix them, you can make them, you can take some instruments and cut

08:55.520 --> 09:04.480
them out, et cetera, but essentially those instruments were inputted into the AI system with the

09:04.480 --> 09:08.640
thousands of different notes of variation, different ways of playing the instruments and then the

09:08.640 --> 09:13.920
AI is actually making those choices for you. So, so I was not the one choosing those instruments

09:13.920 --> 09:18.080
or making any of those sounds unlike a lot of the AI other, other AI projects that you might see

09:18.080 --> 09:24.800
out there. Hmm, you mentioned stems. What are those? Stems are like the individual pieces of,

09:24.800 --> 09:28.480
it's like the individual file for each instrument. So when you download a finished song, you've got

09:28.480 --> 09:34.160
an MP3, but a stem would be separating out the trombones, the drums, the guitar. So that way

09:34.160 --> 09:39.840
in a mix, you can actually have them at different volumes come in and out, have more variation of

09:39.840 --> 09:44.080
sound. That's what gives you that full sound when you're listening to the radio. So you mentioned

09:44.720 --> 09:50.240
some of the parameters that you kind of tune with this, the beats per minute and some others.

09:50.240 --> 09:56.320
What's the, how much control do you feel like you have as an artist given this approach to creating

09:56.320 --> 10:05.440
music? The question is how much time you have. Yeah, I mean, you really are, when people ask

10:05.440 --> 10:10.800
whether or not working with AI reduces the creativity of the artist, I don't think so at all,

10:10.800 --> 10:16.000
I think it just changes the role of the artist. So, for instance, in my case, I don't come from

10:16.000 --> 10:23.520
a musical background, I can sing, I write melodies, vocal melodies, I can hear harmonies in my head,

10:23.520 --> 10:27.520
but if I had to actually put my hands on a piano or a guitar, I wouldn't know what in the world

10:27.520 --> 10:31.840
to do with it because I just don't have that tactile memory and I haven't taken the time to learn

10:31.840 --> 10:36.320
and it's very hard to learn those things and this is an adult. When I'm working with these tools,

10:36.320 --> 10:42.000
I'm basically getting a bunch of raw data. I can then pick and choose from that raw data

10:42.000 --> 10:47.040
for as much time as I have in a day. I can listen to hundreds, if not thousands, of pieces of music,

10:47.840 --> 10:53.120
discern which piece I find interesting and then from there, I can iterate on that one piece of

10:53.120 --> 10:56.880
music as many times as I'd like. I can change the instruments, I can change the BPM, I can change

10:56.880 --> 11:03.840
the key, I can change how, you know, whether that's a mixolodian key or a major or I can change

11:03.840 --> 11:08.080
the genre so that it's got more of an empowering and themic feel rather than like a sad

11:09.920 --> 11:15.200
ballad type of feel. And so I can make all of these choices and those choices end up changing

11:15.200 --> 11:19.840
the piece dramatically. It's almost as if you handed three editors, a bunch of raw film footage

11:19.840 --> 11:24.160
and you told them all to make a movie and they're all going to make a very, very different kind of

11:24.160 --> 11:29.520
movie and it ultimately just depends on how much time you have. But I just think the work becomes

11:29.520 --> 11:38.880
a lot more editorial based. But it is, I think these kinds of tools will see a ton of creative

11:38.880 --> 11:44.240
input and it's just, it's a matter of time to see how people will utilize them and what other

11:44.240 --> 11:52.400
things will come out of their use. Where did the music creation part of the process fit in

11:52.400 --> 12:00.080
relative to creating the song and other aspects of the songwriting process? Did you already have

12:00.080 --> 12:05.280
the song and you created the music for it? Did you create the song for the music? Did somehow the

12:05.280 --> 12:11.520
AI tool, did you sing the song and the AI tool map to the song as song or did you change the way

12:11.520 --> 12:16.320
you sung the song to the tool? How does that whole workflow come together for you?

12:16.320 --> 12:21.040
So it's different for every song and I'll give you two different examples. The two songs that you

12:21.040 --> 12:27.200
heard that have already been released break free in life support. I wrote those songs to the music

12:27.200 --> 12:33.040
once the music was almost complete. Basically, I got to a point where I was very happy with

12:33.760 --> 12:40.480
one of the exported songs probably after dozens of iterations within Amper. And from there,

12:40.480 --> 12:46.400
the song had a certain feeling to it and I was really writing to that feeling and imagining

12:47.120 --> 12:53.280
imagining the world that I wanted to create based on what the music was sort of doing inside of me.

12:53.280 --> 12:59.520
And so the first song I wrote, I wrote a song about what it might be like to be an AI in the future

12:59.520 --> 13:04.400
who's so intelligent that she doesn't know whether she's AI or human. And the song had a lot of

13:04.400 --> 13:11.200
synth elements and kind of quirky machine-like sounds. So it felt like it fit. And then once I was

13:11.200 --> 13:15.440
done with the lyrics, then I went back to the song and I said, is there any part of this that

13:15.440 --> 13:19.280
doesn't really work or that needs to be changed? And so I went back in and iterated. So it was a

13:19.280 --> 13:25.520
little bit of a back and forth process, but definitely the lyrics and even the vocal melodies,

13:25.520 --> 13:33.760
those were all born out of the music that had essentially been finished. At least the melodies were

13:33.760 --> 13:39.120
in a place that I was really happy with. And then when I'm working with the tool like IBM Watson,

13:39.120 --> 13:45.360
it's quite a bit different. I wrote a song recently with a couple hundred people, which sounds absurd,

13:45.360 --> 13:52.960
but and it kind of is, but it was really fun, utilizing some blockchain technology. And we basically,

13:52.960 --> 13:58.640
we had people submitting song lyrics in a certain category. We decided we wanted to write a song that

13:58.640 --> 14:06.960
was essentially the blockchain anthem for this new generation of people who kind of want to do

14:06.960 --> 14:11.920
things differently, want to break the rules, want to build something from scratch again,

14:13.360 --> 14:18.800
and just write something that kind of had that impact for them. And so we did that. And that was

14:18.800 --> 14:24.800
just based on submissions of lyrics from everyone. And then I decided to start ingesting everyone's

14:24.800 --> 14:30.000
favorite anthem into IBM Watson. So I actually asked the group, what are your favorite anthemic

14:30.000 --> 14:35.360
songs from the 1700s, 1800s, 1900s? I don't care, just give me an anthem. And we did once I

14:36.080 --> 14:40.080
transpose those songs into middies, I was able to feed it into Watson and then Watson was able to

14:40.080 --> 14:45.680
learn from those anthems and create something new out of it. And I found that to be really fun,

14:45.680 --> 14:53.280
like giving the software inspiration collectively from the group that represented something to them.

14:53.280 --> 15:02.720
And so the result is a mixture of sort of an anthemic synth pop song. And I still wrote the lyrics

15:02.720 --> 15:09.680
had been done, but I really made sure that whatever IBM, whatever Watson was spitting out,

15:11.040 --> 15:15.200
kind of fit the lyrics. So there were a few songs that had to throw out even though I liked them,

15:15.200 --> 15:20.640
just because they just weren't good foot fits. And so I think the process can go both ways.

15:20.640 --> 15:29.520
What does it mean to feed those MIDI files into Watson? For example, which of the Watson products did

15:29.520 --> 15:35.680
you use? I used Watson Beat, which you can currently download on GitHub. It runs entirely

15:35.680 --> 15:42.000
in terminals. So it's not the most user friendly. It's not like Amper where anyone can just go in

15:42.000 --> 15:46.800
and use it. You have to have some some basic knowledge of code. And there's a you know, there's a

15:46.800 --> 15:53.920
user guide in the GitHub station that you can use to also find it. But you can feed a number of

15:53.920 --> 15:58.640
different things into the system. You can feed it data, but you can also utilize some of the existing

15:59.920 --> 16:03.920
MIDI tracks that they have available to you. Like you can make a song based on the

16:03.920 --> 16:08.640
the learnings of Mary had a little lamb or other royalty free songs. And then you can set

16:08.640 --> 16:15.120
parameters similar to how you can do with Amper around BPM, key, the instrumentation at what point

16:15.120 --> 16:20.080
section two or section three or section four of the song begins and ends. There's actually a lot

16:20.080 --> 16:27.600
of differentiation that you can utilize with Watson. And what's the what's the relationship

16:27.600 --> 16:33.840
fundamentally between the the MIDI files, the data that you're feeding into this tool and what

16:33.840 --> 16:39.920
it's producing at the on the output? And where does really artificial intelligence come in as

16:39.920 --> 16:45.600
opposed to just some kind of computer generated aggregation of stuff that you've provided?

16:46.240 --> 16:52.160
Right. Well, I'm sadly not the programmer using it. So I can only tell you what I have been told.

16:52.160 --> 16:57.680
And I would love for other AI programmers and researchers to look further into some of these

16:57.680 --> 17:04.800
tools and and actually analyze, you know, their integrity in terms of of what type of network they

17:04.800 --> 17:10.160
are and and how they work. But from what I understand with Google magenta with both Google magenta

17:10.160 --> 17:16.400
and IBM Watson, they study the patterns in the music that it's being fed. And so from those

17:16.400 --> 17:22.640
patterns, they can discern what types of music should be coming next. It sounds like a lot of

17:22.640 --> 17:29.520
its statistical analysis running, you know, if you have a G chord, then 70% of the time you'll move

17:29.520 --> 17:36.560
to a D. If this is a anthemic pop song versus if this is a sad ballad, you know, 30% of the time

17:36.560 --> 17:42.560
you'll move to a D. And then it will sort of transpose and learn from there. That's the general

17:42.560 --> 17:50.720
model. But with magenta, you can feed it a lot more music in a grouping. And same with Ava,

17:50.720 --> 17:56.960
which is another, it's an orchestral AI where they still feed it up to 10,000 songs at one time.

17:56.960 --> 18:01.760
So there's a lot more generative analysis. Okay. So the idea is that you're feeding

18:02.480 --> 18:08.400
these tools, some input data and it's going to produce output that's similar to

18:09.440 --> 18:15.200
whatever the input tunes that you're giving it. But within, you know, some sort of parameters or

18:15.200 --> 18:19.040
constraints that you've. That's right. Because you're applying new constraints or parameters,

18:19.040 --> 18:28.080
like new styles, new genres on top of an existing set of music that gives it some sense of statistical

18:28.080 --> 18:36.560
patterning. And so your inspiration for this was coming out of the YouTube world and kind of going

18:36.560 --> 18:45.200
for volume or quantity over quality. Is that do you see that as being really the only role for

18:45.200 --> 18:55.040
this type of creativity or is there? Definitely not. Yeah. So I mean, like using AI is not fast or easy

18:56.080 --> 19:01.280
to make something good takes a significant amount of time. I think what I intended with my

19:01.280 --> 19:07.040
statement was to demonstrate that there's a whole new group of people out there who I think

19:07.040 --> 19:11.440
as a result of the internet era, really, we've just seen a whole new group of people who

19:11.440 --> 19:19.040
do in order to, to wear a hundred hats, which they typically do now in blogging and in video

19:19.040 --> 19:25.520
creation, they have to find really great tools to do so. And I'm not a fan of like the fast, quick

19:25.520 --> 19:30.160
and dirty way of doing things because to me, I really want to make great art. And I really want

19:30.160 --> 19:35.920
to tell great stories and doing things super quickly doesn't necessarily work. But being able to

19:35.920 --> 19:43.600
have complete control over your over the entire process can be really important for artistry.

19:43.600 --> 19:49.840
And so I know in my case, like if I want to make a song on my own, I've got to call up a music producer.

19:49.840 --> 19:55.760
I probably have to pay them a couple thousand dollars if they're good. And I've got to drive halfway

19:55.760 --> 20:00.640
across the 405 in Los Angeles to get there and record with them. And there's nothing wrong with

20:00.640 --> 20:05.680
that model, but that model is inherently prohibitive. It keeps a lot of people from being

20:05.680 --> 20:10.000
able to create art. It certainly, there's a, you know, high barrier to entry in terms of financial

20:10.000 --> 20:15.920
resources and or skill sets that are required to do that. And so now I can use these tools to actually

20:15.920 --> 20:21.520
make music on my own. That's really exciting. And I think that that same kind of hacker DIY

20:21.520 --> 20:28.240
mentality has, has proliferated across, you know, all types of content creation because of,

20:29.600 --> 20:33.280
because of the internet and because of the tools that we have at our disposal. And everyone just

20:33.280 --> 20:38.960
wants to, everyone wants to create. I mean, if we look at like photography alone, right? I mean,

20:38.960 --> 20:43.360
thanks to filters and Instagram. It's like everyone's a photographer. And that might be a bad thing

20:43.360 --> 20:49.120
for photographers. So, but it's also a really good thing for people that want to express themselves

20:49.120 --> 20:54.800
and have a hand in the creative process. So who are the photographers?

20:56.160 --> 21:01.440
Thousands of people who post on Instagram and think they're photographers. You know, or who

21:01.440 --> 21:07.600
who are able to make decent photographs, who may not have been able to do, I mean, look at all,

21:07.600 --> 21:13.840
even like it's crazy to think about the, the, the huge boom in, in makeup over the last five,

21:13.840 --> 21:18.720
six years because of YouTube makeup tutorials. It's one of the largest categories of videos

21:19.360 --> 21:26.800
online. And so all these girls, they, they have learned new ways of creating makeup looks. And so

21:26.800 --> 21:31.040
they're like makeup artists in their own right. And I'm sure that it's been pretty hard on the makeup

21:31.040 --> 21:36.080
artist community. But so all of these things are, are both suffering and benefiting from these

21:36.080 --> 21:42.400
new tools that allow for us to create quickly, easily. But I do think that everywhere you find

21:42.400 --> 21:47.040
these new tools, you also find artists who are using them and spending hours and hours and hours

21:47.040 --> 21:53.040
hours to make something awesome with them. So like, even for me, I would not say that my time

21:53.040 --> 21:58.960
has been reduced and being able to make this album. You mentioned so far,

21:58.960 --> 22:05.280
Amber Watson Magenta and Eva Eva, are there other tools that you've come across or used in the

22:05.280 --> 22:12.000
process of creating this album and AI tools in particular? There are, there's a, there's a tool

22:12.000 --> 22:17.040
called Lander, which allows, it's an AI tool that masters the music, which is essentially a

22:17.040 --> 22:24.080
process of taking a finished mix and then pulling out certain high and low decibels within the range

22:24.080 --> 22:32.800
for like a radio or a surround stereo mix. That's typically a process that's expensive and

22:33.360 --> 22:39.360
and you need like a very specialized audio engineer to do. There's also a company called AI Music

22:39.360 --> 22:43.680
out of the UK, which is doing some very interesting things. I'm not sure if they've launched yet,

22:43.680 --> 22:49.760
but it will, I believe allow people to, I saw an early demo of it, which will allow people to

22:49.760 --> 22:58.320
sing into a microphone and the AI can analyze the melody and basically play music in tandem

22:58.320 --> 23:02.880
while you are singing. So you kind of have a live duet partner. So it's so to speak. And then

23:02.880 --> 23:09.760
there are also tools through the, through both Google and, and IBM that do other things in the

23:09.760 --> 23:16.240
music sphere like AI duet or incense, which allows you to create new sounds by combining sounds

23:16.240 --> 23:22.720
like a cat and a harp together. Now you can have a new instrument called a carp. I mean, there's

23:22.720 --> 23:30.160
all kinds of wacky things that have a variation of of uses, but I'm trying to utilize as much as I

23:30.160 --> 23:35.920
can on the album in fun ways, but I'm sure that we'll just see more and more time goes on.

23:36.800 --> 23:43.840
So also juke deck, juke deck is another UK based company that's great for jingles and very

23:43.840 --> 23:47.680
quick kind of little diddies that you might use for a corporate video or something like that.

23:48.320 --> 23:53.520
What are the things that you've learned, maybe the top two or three things that you've learned

23:53.520 --> 23:58.640
that, you know, might be useful for someone who wants to experiment with AI-generated music?

24:00.800 --> 24:09.040
Top two things I've learned. Well, I would say you have to, oh my goodness, I've learned a lot.

24:09.040 --> 24:13.840
I think it just depends on the software that you're using. There aren't a lot of people making

24:14.560 --> 24:19.680
music right now with AI software, so it takes a lot of trial and error. There aren't any great

24:19.680 --> 24:26.240
user guides. I think understanding the limitations of each program is important because you can really

24:27.680 --> 24:34.240
devise a strategy based on those limitations and make what you feel is the best possible thing

24:34.240 --> 24:42.000
that you can make with that software. I would also say that it's good to go in with a general

24:42.000 --> 24:47.360
idea of what kind of style and tastes you might have in the music sphere. So, for instance,

24:47.360 --> 24:53.840
when I use Amper, I really focus on cinematic and symphonic electronic sounds.

24:54.800 --> 25:00.160
I really like kind of soundtrack, movie soundtrack sounding music, and so that was my

25:00.160 --> 25:04.960
my north star in creating. Otherwise, you just end up with way too many options,

25:04.960 --> 25:12.080
and it's hard to actually boil something down that you like. Was that related to that particular

25:12.080 --> 25:19.200
project or a particular strength of Amper? I think it was both strength of Amper for sure.

25:19.200 --> 25:24.160
I went through a bunch of their styles and I found that the cinematic style was one of my favorites,

25:24.160 --> 25:31.680
whereas other styles might have been weaker in terms of my musical preferences. Then I also just

25:31.680 --> 25:39.200
felt like I wanted this project to have an epic feel to it. I wanted there to be that movie

25:39.200 --> 25:44.800
magic feeling to it, but I also love electronic music, so I had to combine those two things.

25:45.440 --> 25:51.200
There are a bunch of interesting parallels here with Data Science, which is the typical

25:51.200 --> 25:55.680
conversation topic on this podcast. One of the things that you mentioned was

25:56.720 --> 26:02.800
just the iterative nature of creation with these tools, and that's certainly the case for folks

26:02.800 --> 26:12.320
that are trying to solve business or engineering types of problems with these AI tools,

26:12.320 --> 26:17.840
and you also mentioned just understanding the limitations of the tool, and I guess that's

26:17.840 --> 26:25.520
important with any use of any tool, particularly technology tools. Are there any other observations

26:25.520 --> 26:32.320
like that that you've come across? I think that's primarily it. What I found is that the AI

26:32.320 --> 26:38.560
is not just giving me data. It's giving me new sources of inspiration. What I try to do

26:39.280 --> 26:44.880
is stretch outside my comfort zone, because if I were to collaborate with someone here in LA,

26:44.880 --> 26:50.400
for instance, let's say I was able to find a couple thousand dollars to collaborate with a record

26:50.400 --> 26:57.200
producer here. The likelihood of that person being a pop producer who's trained in radio,

26:57.200 --> 27:01.760
pop hits that understands that formula is very high, because that's what you're going to find.

27:02.320 --> 27:08.240
Whereas if I'm working with an AI, depending on what software or program I'm working with,

27:08.240 --> 27:12.800
and the parameters that the engineers have set, and depending on what type of data I feed it,

27:12.800 --> 27:18.560
I could get something totally random. That's exactly what I found with all of these programs,

27:18.560 --> 27:23.760
to varying degrees, is that there's a lot of randomness that comes out of collaborating with

27:23.760 --> 27:29.040
this software. I love that because I think what it does is it forces me to think outside of my own

27:29.040 --> 27:36.240
box, and it gives me a new collaborative partner that's not in obvious, that's not going to go in

27:36.240 --> 27:44.320
an obvious direction. I guess I would just encourage people to think about these new tools as really

27:44.320 --> 27:50.480
new and unique sources of inspiration, and it might be the thing that's very strange or offbeat

27:50.480 --> 27:56.960
that is actually the most brilliant seed of an idea. We're clearly just at the beginning of

27:56.960 --> 28:02.880
all of this. Where do you see it going based on your experience? Creating this album,

28:02.880 --> 28:09.440
working with these tools, do you have a sense of the direction this will all take?

28:10.320 --> 28:16.080
Oh my goodness. Yeah, a little bit. In the same way, when I started making YouTube videos,

28:16.080 --> 28:24.000
I thought to myself in a few years, everyone is going to be a YouTuber. Not everyone, but I

28:24.000 --> 28:28.640
thought a lot of people are going to become content creators, because making content is fun,

28:28.640 --> 28:34.320
and be because people love to express themselves, and they want to be seen. Before YouTube was around,

28:34.320 --> 28:39.760
you couldn't just be seen. There were huge gatekeepers. You had to live in LA, have an agent,

28:39.760 --> 28:44.560
be going out on additions. That was the only way that you could actually have your material seen,

28:44.560 --> 28:49.040
and all of a sudden you had this platform where millions of people could listen to you, whether or not

28:49.040 --> 28:54.960
you deserve to be watched. It was another question, but I think with these new AI tools,

28:54.960 --> 29:02.400
you'll see the same thing. Specifically with music, I think we'll see a lot of new artists,

29:02.400 --> 29:09.040
a huge democratization of the music creation process. It's just becoming so much easier

29:09.040 --> 29:14.400
to make something that sounds great without all of the super expensive tools that you needed 10

29:14.400 --> 29:19.520
years ago. They had to go into a studio to use. Some people will say that's not a good thing,

29:19.520 --> 29:24.320
and maybe that's the case. I think there's an argument on the other side as well,

29:24.320 --> 29:30.880
that everyone deserves their shot, and we are all artists that deep down want to create,

29:30.880 --> 29:39.120
and make stuff. That's part of the human experience. I think we'll see a democratization

29:39.120 --> 29:46.160
in that regard. Honestly, what I'm most excited about is the fact that out of this democratization

29:46.160 --> 29:53.680
will inevitably come new forms of art that we can't even conceive, whether that's figuring out

29:53.680 --> 30:02.800
how to create music that shifts someone's mood through biotracking devices, or is immersive

30:02.800 --> 30:10.000
musical art in the form of not just sounds, but three-dimensional visuals, augmented reality,

30:10.000 --> 30:16.880
virtual reality. I think we'll see a whole new crop of artists come up that are not just

30:16.880 --> 30:24.400
music artists or visual artists or video artists, but create an entire experience for someone.

30:26.000 --> 30:32.560
I think that that's really what we have to be excited about is the fact that every time we get

30:32.560 --> 30:40.080
really good at one thing, e.s. humans usually figure out something new as a result of that,

30:40.080 --> 30:50.560
and that becomes part of our culture. It just occurred to me that this would be a good time

30:50.560 --> 30:57.520
to throw this out there for our audience, really. Maybe you can give them some tips if anyone

30:57.520 --> 31:05.760
wants to take on this challenge. My producer and editor are not huge fans of the royalty-free

31:05.760 --> 31:13.040
intro music that I use here at the podcast and have used for the past couple of years. We really

31:13.040 --> 31:20.160
should have an AI-generated track, AI-generated intro theme song. If anyone in the audience wants to

31:20.160 --> 31:26.480
take that on, you are certainly welcome to do that. Taren, do you have any tips as to where they

31:26.480 --> 31:31.680
should start? Where would you say your audience sits on the spectrum of AI capabilities?

31:31.680 --> 31:41.040
I'd say they're pretty capable. They tend to be fairly technical and fairly excited about AI.

31:41.680 --> 31:47.760
Awesome. Well, if they have any musical experience whatsoever with the DAW workstation, like logic

31:47.760 --> 31:53.280
or protocols or garage band even is sufficient, I would recommend they use either Google Magenta

31:53.280 --> 31:57.840
or IBM Watson's tool, just because they'll have more control over the inputs and parameters,

31:57.840 --> 32:05.120
and they can actually go in and change the code if they so choose. If they are very AI proficient,

32:05.120 --> 32:10.720
but musically not proficient, then maybe Ampers a good place for them to go, just to play around,

32:10.720 --> 32:16.240
and they could certainly make you something very quickly. So I would recommend any of those

32:16.240 --> 32:21.680
tools to start. And I might add that if anyone else stumbles upon anything new, just because I'm

32:21.680 --> 32:26.000
head down with the album right now, please send it my way. They can just tweet it to me. I'm always

32:26.000 --> 32:33.360
excited to learn about new interfaces. And I'm also simultaneously running a contest for the last

32:33.360 --> 32:39.520
song on my album. I'm putting it out there so we can also extend this to your audience as well

32:39.520 --> 32:47.600
that anyone who writes a composes a track that I like using one of the tools that are out there

32:47.600 --> 32:53.440
with AI, I will co-write the song with them and they will be a co-writer on that track on my album.

32:53.440 --> 33:02.800
So taking submissions up until June 30th. Awesome. So two AI music contests announced right here

33:02.800 --> 33:11.440
the Twimmel intro contest and Taren Sutheran's last track on her album, AI Music Contest. And

33:12.240 --> 33:18.080
if we have any overachievers in the audience, what you should be doing is writing a composition

33:18.080 --> 33:26.000
that can serve as both the Twimmel intro track and Taren's last track on her album and maybe

33:26.000 --> 33:34.240
she'll write the track about the podcast or something. Awesome. Awesome. Well Taren, it's been really,

33:34.240 --> 33:40.320
really great to get to chat with you about this. Thanks so much for taking the time. Thanks Sam,

33:40.320 --> 33:49.440
you too. Take care. All right everyone, that's our show for today. For more information on Taren

33:49.440 --> 33:56.240
or any of the topics covered in this episode, head on over to twimmelai.com slash talk slash 139.

33:56.240 --> 34:09.760
Thanks so much for listening and catch you next time.

