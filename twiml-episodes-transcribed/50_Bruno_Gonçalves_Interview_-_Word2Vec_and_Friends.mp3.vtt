WEBVTT

00:00.000 --> 00:15.920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

00:15.920 --> 00:20.880
people doing interesting things in machine learning and artificial intelligence.

00:20.880 --> 00:23.360
I'm your host Sam Charrington.

00:23.360 --> 00:30.240
A big thanks to everyone who joined us last week for our second Twimble online meetup.

00:30.240 --> 00:36.720
Led by Nicola Cucherova, we discuss the paper Learning Long-Term Dependencies with Gradient Descent

00:36.720 --> 00:40.320
is Difficult by Yasuo Benjio and Company.

00:40.320 --> 00:43.760
I had a great time and learned a ton.

00:43.760 --> 00:50.400
For those who weren't able to attend live, we have the video posted for you on Twimbleai.com

00:50.400 --> 00:56.480
slash meetup and if you're interested in joining us next month, please head on over to that site

00:56.480 --> 01:02.880
to get signed up. We're also accepting presenters so feel free to shoot me a note with your ideas.

01:03.680 --> 01:09.520
Next up, the time has come for the Artificial Intelligence Conference brought to you by O'Reilly

01:09.520 --> 01:16.000
and Intel Nirvana. I'll be in San Francisco the entire week next week and we have a ton of great

01:16.000 --> 01:21.600
interviews lined up. So this should be another awesome series. If you haven't had a chance to

01:21.600 --> 01:28.080
check out our series from the New York event, you can find it at twimbleai.com slash O'Reilly AI

01:28.080 --> 01:35.520
and Y. Also, if you'll be at the conference, please send me a shout out on Twitter or via email.

01:35.520 --> 01:43.680
I'd love to connect. See you then. Okay, about our show. This week I'm bringing you an interview

01:43.680 --> 01:51.200
with Bruno Gunn's office, a more Sloan Data Science Fellow at NYU. As you're hearing the interview,

01:51.200 --> 01:57.120
Bruno is a longtime listener of the podcast. We were able to connect at the NY AI conference back in

01:57.120 --> 02:02.960
June after I noted on a previous show that I was interested in learning more about Word Tevek.

02:02.960 --> 02:07.920
Bruno graciously agreed to come on the show and walk us through an overview of Word embeddings,

02:07.920 --> 02:14.640
Word Tevek, and a bunch of related ideas. He provided a great overview of not only Word Tevek,

02:14.640 --> 02:20.880
but related natural language processing concepts such as skipgram, continuous bag of words,

02:20.880 --> 02:28.080
no Tevek, TF IDF, and much more. By the time you hear this, it'll be too late to catch it live,

02:28.080 --> 02:34.320
but Bruno is doing a half day tutorial on Word Tevek and friends on Monday at the AI conference.

02:34.320 --> 02:41.760
If you'd like to see it, I'm sure it'll be available via the O'Reilly Safari website and now on to the show.

02:49.120 --> 02:54.480
All right everyone, so I am here on location at the O'Reilly AI conference and I have the

02:54.480 --> 03:00.560
pleasure of being here with Bruno Gunn's office who is apparently a longtime listener of the podcast

03:00.560 --> 03:08.160
and reached out to me after hearing me note that I wanted to learn more about Word Tevek and embeddings

03:08.160 --> 03:13.360
and made it happen. So we're here to talk about that. Welcome, Bruno. Thank you. It's a pleasure

03:13.360 --> 03:17.760
to be here. So I've been listening for a very long time and I'm very happy to be here and be able

03:17.760 --> 03:23.200
to participate. Awesome. Awesome. I was just asking you, you mentioned that you've been listening since

03:23.200 --> 03:28.720
before the interview format and I don't know, I'm still stuck on like the transition from the

03:28.720 --> 03:33.040
from the news format to the interview format. How was that for you? Like what's your take on kind

03:33.040 --> 03:39.520
of the before and after? No, I like the new interview format actually. But this might be a

03:39.520 --> 03:44.400
personal buy as well. When it was news, a lot of the news I'd already seen or listened somewhere

03:44.400 --> 03:49.040
else. So it wasn't as new. In this way, since you're interviewing people that I don't necessarily

03:49.040 --> 03:54.160
listen to it and then no personally, it's good to have a fresh view from them. That's great.

03:54.160 --> 03:59.440
That's great. Like I said, I was, you know, and I guess I still am hung up on the fact that I

03:59.440 --> 04:04.640
switched the format. But I've been told by people that things along the same lines that the news

04:04.640 --> 04:10.320
is essentially commoditized and it's hard to, you know, beat out tech crunch or whatever that people

04:10.320 --> 04:15.440
count on for their news. But the folks really seem to be enjoying the interview format. And so,

04:15.440 --> 04:20.080
you know, again, it's great to have you on for an interview instead of talking about the news.

04:20.080 --> 04:25.040
So tell us a little bit about kind of your background and what you're up to. So I'm,

04:25.040 --> 04:29.280
I can say, I'm originally a physicist, but I've always been involved in optimization problems,

04:29.280 --> 04:34.320
originally in spin glasses, soft optimization and data mining. Spin glasses. Spin glasses. So it's

04:34.320 --> 04:38.480
this, it actually has interesting connections with neural networks and how they work. But

04:38.480 --> 04:44.880
okay, basically a disorder magnetic system. Okay. Put it very, very simply. But the mathematical

04:44.880 --> 04:51.120
structure is very rich and it leads itself to exploring different types of optimization problems.

04:51.120 --> 04:56.240
Okay. From there, I evolved very quickly. And this was because I noticed that in spin glasses,

04:56.240 --> 05:00.560
a lot of the behavior you see has to do with the way that the different elements are connected.

05:01.680 --> 05:06.400
I moved very quickly towards networks and studying basically connections between components of

05:06.400 --> 05:11.120
a system. Okay. More deeply. And when you're working with network and complex networks in general,

05:11.120 --> 05:16.720
what you do a lot is basically data mining. You're, you're crawling websites, you're parsing a

05:16.720 --> 05:22.960
patchy logs to look for connection between a URLs. For example, you're, you're basically doing

05:22.960 --> 05:28.240
in a sense applied graph theory. Okay. But applied to real data. So the transition towards data

05:28.240 --> 05:33.280
came very early and very naturally, even before, I mean, people were talking about data science.

05:33.280 --> 05:40.080
This was back in maybe 2006, 2007. Okay. And then more recently, I've been moving more towards

05:40.080 --> 05:45.680
more data science, intensive aspects and hence my interest in the work to vac and that type of

05:45.680 --> 05:51.520
algorithms. Okay. You know, I think the best way to do this is to just jump in and have you

05:51.520 --> 05:58.400
kind of walk us through work to vac, but the kind of the foundational thinking that led to work to

05:58.400 --> 06:04.160
vac and embeddings. As I mentioned the other day, it's, it's an area that I've been meaning to dive

06:04.160 --> 06:10.560
into and I'm glad you're, you're here to tell us about it. So the original idea is actually as

06:10.560 --> 06:16.400
many things doing this field comes from outside the field. So it's comes from linguistics. Okay.

06:16.400 --> 06:23.440
And it's expressed more or less very in a very general way by James Firth, if I'm not mistaken,

06:23.440 --> 06:29.280
around the 1953. Okay. And basically what he said is you, you'll know the meaning of a world by

06:29.280 --> 06:34.080
the company it keeps. So if you don't understand what a world means, you kind of look at the context

06:34.080 --> 06:39.040
and the context gives clues about what the world says. Okay. Intense to say. So we're to vectorize

06:39.040 --> 06:46.800
to take advantage of this by saying that the embedding of a world is defined by the context in

06:46.800 --> 06:53.200
which it appears. So words that appear in the same context are more somewhat equivalent. So they

06:53.200 --> 06:57.280
will have vectors that are close to each other. While words that tend to appear in very different

06:57.280 --> 07:02.960
contexts will be very different and have very different vectors or vector representations in this

07:02.960 --> 07:10.240
case. And so how do you get to the word vectors? I guess my impression is that you're, it's all

07:10.240 --> 07:17.280
relative to a specific corpus, right? There's not like a, you know, we haven't done word to veck on

07:17.280 --> 07:21.840
everything like some grand unification of word to veck. Is that the right way to think about it?

07:21.840 --> 07:26.240
And then you're doing some math on that corpus to get you to the vectors. Can you talk a little

07:26.240 --> 07:30.880
bit about that process? Yes. So basically what it does, it's a very simple neural network. So

07:30.880 --> 07:35.520
it just says one hidden layer. The activation function is linear. So it just passes through.

07:36.080 --> 07:41.840
It's very simple. What the actual wording embedding that are basically the weight on the

07:41.840 --> 07:48.480
hidden layer. Depending on which model can be skip grammar, can be continuous bag of words. It

07:48.480 --> 07:55.120
can be the weight on the leading to the hidden layer. It can be the weights outside heading out

07:55.120 --> 07:59.840
of the hidden layer. But in practice, it's the same mathematically. And so the skip

07:59.840 --> 08:04.160
ram refer to one of those and continuous bag of words refer to the other skip grammar is

08:04.720 --> 08:09.520
leading in the weight's leading in. And yes, so the way you look at what first was saying that

08:09.520 --> 08:14.000
you know a word by the company it keeps is if you have the word, you can kind of guess the

08:14.000 --> 08:17.920
context in which it appears, right? Or if you have the context, you can kind of guess what word

08:17.920 --> 08:23.040
would fit in the middle. So the skip ram and the continuous bag of words basically look at these

08:23.040 --> 08:28.880
two approaches. In one case, your input is the word and you're trying to guess in the output

08:28.880 --> 08:34.960
layer what the context is. On the other case, you have the context words as input and you're trying

08:34.960 --> 08:40.000
to guess what is the word that might go in the middle. And here, just to clarify, the context is

08:40.000 --> 08:45.840
usually defined by a window of words before that word and the words after that word. So if you're

08:45.840 --> 08:51.280
looking at word, I, so your context would be, for instance, I minus one, I minus two and I plus

08:51.280 --> 08:55.920
one, I plus two, like two words before, two words after or five words before five words after.

08:56.480 --> 09:03.920
Okay. And so that windows what's used to compute the vector for that representation for

09:03.920 --> 09:08.080
a given word. Yes. So that window will give you what is the context and the context will define

09:08.080 --> 09:13.360
what is the word. Okay. Now, like you're saying, people haven't done word-to-vec in all the text in

09:13.360 --> 09:18.720
the world. So every time you run this, you will get different vectors. Right. Also, if you run

09:18.720 --> 09:23.760
twice in the same corpus, the vectors will be different. The reason for this is you're initializing

09:23.760 --> 09:28.560
all the weights or the vectors randomly. Right. And then you're adjusting them. Okay. So you

09:28.560 --> 09:33.200
will end up with something different. However, if you do it well enough and it converges and you get

09:33.200 --> 09:37.360
something that this reasonable, the vectors will be different, but they'll basically be a rotation.

09:37.360 --> 09:43.680
So you can get vectors trained into corpus and align them so that they match. Because what the word

09:43.680 --> 09:50.560
-to-vec algorithm and similar algorithms are trying to do is learn the relations between vectors.

09:50.560 --> 09:56.160
So you're basically, you're trying to define the differences of vectors, but not the actual vectors.

09:56.160 --> 10:00.880
Right. So there's some distance metric or something. So if you rotate the distances all remain

10:00.880 --> 10:05.040
the same. Right. So there's still valid. Okay. And it's because the distances are preserved.

10:05.680 --> 10:10.000
Right. It's the the distance that preserves the semantic meaning that gives you the semantic

10:10.000 --> 10:15.440
relations. Right. And the idea is very simple. So if the same word, or rather if two words,

10:15.440 --> 10:21.520
appearing in the same context, they have to be defined by similar vectors. And this also means

10:21.520 --> 10:26.400
that if the relation between these pair of words and relation between these pair of words is similar,

10:26.400 --> 10:31.360
the difference between them in terms of vectors will also be similar. Okay. So this is why you can do

10:32.160 --> 10:37.920
word arithmetic and say the vector for Italy minus the vector for Rome has to be equal to the

10:37.920 --> 10:42.000
vector for France minus the vector for Paris. Right. Right. So if you have three of these,

10:42.000 --> 10:46.080
you can calculate the other one. Right. And basically, it's one of the ways they use to calculate

10:46.080 --> 10:51.120
or to measure how good the embeddings are. They use this called analogies. Right. So

10:51.120 --> 10:57.120
Okay. Paris is to France's Rome is to and they'll give you Italy. If you look for the vector that's

10:57.120 --> 11:02.640
closest to that difference. Okay. Interesting. So one of the questions that, well, two questions

11:02.640 --> 11:10.640
come off from me. One is if the context is defined by this narrow window, you said it's usually

11:10.640 --> 11:15.840
used two words before two words after is that example of two words in the in the official implementation.

11:15.840 --> 11:21.520
I think the default is set to five, but it depends. You can vary it. It's an input parameter. And

11:21.520 --> 11:26.880
also in practice, if you're going to the details, when you're you're pre-processing the corpus,

11:27.600 --> 11:31.760
you will sometimes remove some words because they're too common, kind of like what you do with

11:31.760 --> 11:35.920
stop words. Right. And that effectively changes the size of the window. Right. Because you do this

11:35.920 --> 11:40.560
before you calculate the context. So it's almost as if sometimes your windows are a bit bigger.

11:41.360 --> 11:45.360
Okay. And that is because you remove the word. Right. So you're catching more information.

11:45.360 --> 11:51.200
Okay. So the question is, do folks experiment with a lot with bigger windows or is it possible

11:51.200 --> 11:58.320
to do this on the entire corpus and get, I guess in my mind, what I'm hearing is the word

11:58.320 --> 12:03.280
relationships are only relative to this very small window. And wouldn't we have, wouldn't the

12:03.280 --> 12:08.880
vectors capture more information if we were somehow creating them based on bigger windows or the

12:08.880 --> 12:14.400
entire corpus? Is that the right way to think about it? Yes, and no. So if you make the window too

12:14.400 --> 12:20.640
big, you're basically including information that's not relevant for the word. Right. You can have

12:20.640 --> 12:24.800
a very long sentence. The word at the end of the sentence doesn't necessarily have anything to do

12:24.800 --> 12:29.200
with the word at the beginning of the sentence. So you're trying to keep nearby words that help you

12:29.200 --> 12:36.560
define what that word means. So adjectives verbs that directly relate to what the concept is.

12:36.560 --> 12:42.000
So what the word is in particular. Right. I guess I'm thinking about it in the context of

12:42.960 --> 12:50.240
like running TF IDF on a Wikipedia article. Right. If I'm looking at a Wikipedia article that's

12:50.240 --> 12:57.600
talking about neural networks and CNNs and RNNs and, you know, artificial neural networks, all

12:57.600 --> 13:05.040
these, these are all related terms. And I'd want to capture that relatedness. But they may be in,

13:05.040 --> 13:11.840
you know, very different. They may be far from one another spatially in my document. But I still

13:11.840 --> 13:17.600
want to capture that context. Can I just not use word to effect for that? You can. So basically what

13:17.600 --> 13:22.320
you're doing is you're scanning through the entire document. Right. Right. And and embedding the

13:22.320 --> 13:28.320
that you learn for each word as to do with all the contexts it appears in. So it's not just one.

13:29.200 --> 13:34.640
So I can give the example of artificial neural network. Right. Artificial will appear next to

13:34.640 --> 13:39.680
neuron in this context. But in a maybe a few paragraphs later it will appear next to intelligence

13:39.680 --> 13:45.040
or to approach. Right. So that means that artificial will be defined by all of these contexts.

13:45.040 --> 13:51.680
Mm-hmm. And do look very differently than the word maybe like convolutional that always appears

13:51.680 --> 13:56.080
next to neural network. Right. And it doesn't appear in other contexts. Right. So it does take

13:56.080 --> 14:01.280
the whole information of the of the corpus into account. But if, for example, I'm running it on

14:01.280 --> 14:07.360
a Wikipedia article on neural networks. And there's one section on convolutional neural network. And

14:07.360 --> 14:14.800
then another section on RNNs and another section on LSTMs. Since those individual terms are separated,

14:14.800 --> 14:21.280
by these sections, will it capture those relationships? Yes. Just the neural network part or...

14:21.280 --> 14:26.080
Yes. So one thing maybe I should be clear. So when I say corpus in this case, I mean,

14:26.080 --> 14:31.840
all of Wikipedia. I don't mean one page of Wikipedia. Okay. So these are very large corpus. Got it.

14:31.840 --> 14:36.640
And the reason why you use a very large corpus is because you want to learn what is the meaning of

14:36.640 --> 14:43.040
the word in a sense. Okay. So this is what you're trying to capture. It's not necessarily what does

14:43.040 --> 14:47.760
this word mean in this document? Got it. So it's more of a it's a more generic thing. Okay. And this

14:47.760 --> 14:53.840
is why actually people have started publishing high quality embeddings. Google has published some

14:53.840 --> 14:59.440
Facebook as far as the trend on billions of words or rather corporate that are billions of words

14:59.440 --> 15:04.240
long because they're trying to capture what is a exact meaning. So you can use, for instance,

15:04.240 --> 15:09.680
these vectors. One very simple application is, for example, for queries and biguation. Because

15:09.680 --> 15:13.360
you know the word, you know, what the vector is, you can look around that word to see what

15:13.360 --> 15:17.760
are words that are related to that one. And maybe you can show results that use those other words.

15:17.760 --> 15:24.400
Okay. You can use the vectors and this translation invariance distance preservation as a way of

15:24.400 --> 15:29.600
mapping the word, for instance, to a verb version of the word or a noun version of the word

15:29.600 --> 15:35.760
or a past tense version. So you can use all of these relations in a sense, all of these linear algebra,

15:35.760 --> 15:41.440
as a way of getting more knowledge about what the text is. Okay. Interesting. So the other question

15:41.440 --> 15:49.280
I had was the length of the vector. How do you know what the right dimensionality is for these

15:49.280 --> 15:54.400
vectors? As far as I know, there is no well-defined way of measuring what is the optimal size.

15:54.400 --> 16:00.960
Okay. In practice, people tend to use dimensions between about a hundred and five hundred.

16:00.960 --> 16:07.440
Okay. Which is relatively small for corporate or other dictionaries. So the number of individual

16:07.440 --> 16:13.360
words of the order of hundreds of thousands. In a sense, what one of the things you're also doing

16:13.360 --> 16:18.400
is you're doing very much a dimensionality reduction. You're mapping this from this very large

16:18.400 --> 16:24.080
high dimensional space, which dimension is a word into this very small space in a way that

16:24.080 --> 16:30.000
is still preserving the meaning of the semantic value of the words. Okay. Okay. Do you recall the

16:30.000 --> 16:37.040
show that I did with Francisco Weber? Maybe. It was from a cortical that I only talked about

16:37.040 --> 16:42.000
the kind of neural representations. Yes. I actually remember that I wanted to look into that

16:42.000 --> 16:46.960
more carefully. Okay. It's explicitly mentioned that this is actually related with this type of

16:46.960 --> 16:50.960
vector representations, because yeah, it seems that each word is actually represented in different parts

16:50.960 --> 16:55.840
of the brain at the same time. Right. So it's something I'm curious to look more deeply into it.

16:55.840 --> 17:01.600
I tried looking at their website, but I couldn't get the technical details to it. Okay. I was curious

17:01.600 --> 17:05.920
if you had looked into that at all. And if you were familiar with that model and and it's

17:05.920 --> 17:10.480
thought you might have on. I don't know the details. I found some things online, but it seemed to be

17:10.480 --> 17:16.160
more marketing oriented. So there wasn't like the scientific articles behind it. Okay. So embeddings

17:16.160 --> 17:22.880
has been around since the 50s word to back now is a did you say 1950s something? So 95s is the

17:22.880 --> 17:27.520
is the idea is this idea that the meaning of the word is coming from the context appears. So this

17:27.520 --> 17:32.640
got the distribution of the hypothesis in linguistics. Okay. We're embedding some so I think they've

17:32.640 --> 17:38.160
been around for maybe 10 years, maybe maybe a little bit more. Okay. In practice or like in

17:38.160 --> 17:42.160
wide adoption, they became very popular and get a lot of attention with word to back when

17:42.160 --> 17:48.000
we were published in 2013. Right. But I might as me call off if I'm not mispronouncing his name horribly.

17:48.000 --> 17:54.880
Okay. Okay. And so, you know, we're a few years into word to back and now there are a bunch of

17:54.880 --> 18:00.880
other to Vex. Right. Have you looked into any of those as well? Yes. So there's some that are

18:00.880 --> 18:05.280
very interesting. There's one for instance that is DNA to back and you try to find embeddings for

18:05.280 --> 18:12.080
sequences of DNA. Okay. And see how that might relate to protein structure and

18:12.080 --> 18:19.920
genome organization, which I find fascinating. There is one by Eurel Laskovek in Stanford

18:19.920 --> 18:27.280
where it's called note to Vex and it tries to use this note like back in a graph where it's

18:27.280 --> 18:31.600
basically trying to do that to find embeddings for nodes and graphs as a way of measuring

18:31.600 --> 18:36.640
relations between nodes. And basically the way it does it since it doesn't have a sequence

18:36.640 --> 18:41.360
of words. It doesn't have a sequence of nodes. So it basically runs a random walk process on the

18:41.360 --> 18:45.520
net on the node. Okay. And that generates the sequence. And then based on that sequence,

18:45.520 --> 18:51.120
then you can treat each node as if it was a word. It appears next to other nodes because there's

18:51.120 --> 18:55.440
some how it's the neighborhood. And that can and from there you can define. And it's from that

18:55.440 --> 18:59.680
is able to capture some of the structure of the network. I wouldn't. So the idea is that you're

19:00.240 --> 19:06.960
I guess I would have thought that that a graph is its own kind of representation of all this

19:06.960 --> 19:12.640
information. It is and it isn't in a sense right. The point is it's not necessarily easy and this

19:12.640 --> 19:19.680
is a one on problem to compare graphs. So I'll give you a graph and tell you the nodes here

19:20.480 --> 19:25.680
are words. I give you another graph and I tell you the nodes here are people. So you can't just

19:25.680 --> 19:30.320
match the labels directly. Right. It's very hard to see if the structure of the graph is the

19:30.320 --> 19:35.040
same. It's actually the total permutations and all the things. So if I'm not mistaken, I think

19:35.040 --> 19:39.280
that can be complete problem. Okay. So people have been trying this idea of graph embeddings

19:39.280 --> 19:44.560
for a while. Okay. When you try to map the graph into some set of points that does not depend

19:44.560 --> 19:48.720
on the details of the graph or the labels that you give to nodes or anything like that. Okay.

19:48.720 --> 19:54.240
Or any permutations you do. Hmm. Interesting. So I've been fascinated by basically how

19:54.240 --> 20:00.240
versatile this idea of world to back is. Yeah. Are there others? I'm trying to I'm

20:00.240 --> 20:06.880
meta-loss for I know I've seen it. It seems like a dozen of these different X to X of late.

20:06.880 --> 20:12.960
But it sounds like anytime you have a space with some complex structure, this is a tool that you

20:12.960 --> 20:21.280
can use to allow you to compare both individual points within that space to other spaces. Is that

20:21.280 --> 20:26.720
a general way to think about it? More. It's more whenever you have a sequence of tokens. Let's say

20:26.720 --> 20:31.600
you can use it as a way of finding a representation of these tokens that then you can use to find

20:31.600 --> 20:36.080
the relationships. Okay. In this sequence. Right. So it works very well for text because you have

20:36.080 --> 20:41.600
sequences of words. Hmm. Like in the case of DNA2VAC, it works very well because you have a sequence

20:41.600 --> 20:47.600
of nucleotides in DNA. Hmm. For node2VAC, you have a sequence of nodes that generated by these

20:47.600 --> 20:52.560
random processes. This random walk process is on the graph. And they actually tried different

20:52.560 --> 20:57.680
definitions of the random walk, different rules. And so that that's somehow able to capture

20:57.680 --> 21:04.240
different aspects of the of the network structure. Hmm. Interesting. And the sequences have to have

21:05.280 --> 21:10.400
I guess the the relationships between the tokens and the sequences are defined by the corpus.

21:10.400 --> 21:16.480
I guess the I was thinking of I wonder if you can do like transaction to VEC and like do word

21:16.480 --> 21:22.400
of embedding on the sequence of transactions and use that for fraud detection or something like that.

21:22.400 --> 21:27.600
I have actually saw something like this yesterday on medium. I didn't I didn't really because

21:27.600 --> 21:31.520
I didn't have time. But there was literally I don't remember the details. It was really something

21:31.520 --> 21:36.240
like stock to VEC that somebody published on medium yesterday. It's on my reading list,

21:36.240 --> 21:40.400
but I have not read it yet. Interesting. Interesting. I don't know how they're doing it. I just saw

21:40.400 --> 21:46.000
the text. Okay. But notionally, there's there's something in there somewhere. Yes. So in principle,

21:46.000 --> 21:51.120
every time you have a sequence, you can do something like this. Yeah. I've also been getting more

21:51.120 --> 21:56.080
into learning about blockchain and how that all works. Yes. I'm wondering now if there are some

21:56.080 --> 22:01.280
applications to that as well. Possibly. I have not seen anything with blockchain using this.

22:01.920 --> 22:07.360
I've looked in detail at blockchain a couple of years ago. Those interested in basically these

22:07.360 --> 22:13.680
transaction networks. It might be possible to do something. I said, I haven't seen it yet.

22:13.680 --> 22:19.440
Interesting. Maybe an idea for some of your listeners. Yeah. Yeah. What are some of the most

22:19.440 --> 22:25.920
interesting or what are some of the interesting applications you've seen of word to VEC and friends?

22:25.920 --> 22:30.720
I've fascinated by basically how much power these word vectors have. Because they are capturing

22:30.720 --> 22:34.960
these semantic relations, means that you can use them for disambiguation, query expansion,

22:35.600 --> 22:42.400
analogies, like the original metric that they used. So basically, they're a very powerful tool to

22:42.400 --> 22:49.200
map text into a vector representation or to an numerical representation. The then is very powerful

22:49.200 --> 22:54.560
in how you can manipulate it. Because everybody knows computers have a hard time understanding text,

22:54.560 --> 22:59.040
but they're very good at understanding vectors. So here you're mapping text to vectors. So you're

22:59.040 --> 23:04.240
making computers' lives much easier. So people have used this for machine translation. Because you

23:04.240 --> 23:12.240
can translate vectors into different languages. Then you align them. Because of relationships

23:12.240 --> 23:18.160
have to be the same mostly. And then you can use them basically too much. So you find the word in

23:18.160 --> 23:23.280
embedding in one language. You find what is the most similar word, not similar vector in the

23:23.280 --> 23:28.240
other language. And you can find the translation. Also, an idea that came up in the Francisco

23:28.240 --> 23:33.760
Webber conversation. So yeah, we both need to dig into that one and try to figure out what they're

23:33.760 --> 23:38.000
doing. That's different than regular embeddings. I mean, and then there's that variation. There's

23:38.000 --> 23:42.400
also glove, glove, glove, glove, glove of vectors that tries to do a different formulation.

23:43.680 --> 23:47.680
Another very interesting application, which is actually the reason why I started getting

23:47.680 --> 23:53.840
interested in word-to-vec in particular is this paper I saw, which is actually tracking linguistic

23:53.840 --> 24:01.200
change over time. So they train using word-to-vec. They train word vectors using Google and Gramps

24:01.200 --> 24:07.680
data for different decades. And then they align the vectors. And then they look for the words that

24:07.680 --> 24:12.880
are changing over time. Oh, that's interesting. So you can track basically how the meaning of a word

24:12.880 --> 24:22.240
is evolving. Is there a metric for measuring the, I guess, like the dispersion of the vectors

24:22.240 --> 24:27.840
in a given embedding? Like I'm wondering if in that example, where you train an embedding on

24:27.840 --> 24:34.960
this engram data, can you look at something analogous to like the standard deviation of the words

24:34.960 --> 24:42.560
or like the spread of the degree to which the not really I don't think? I mean, if you look at the

24:42.560 --> 24:47.120
entire set of vectors that made itself, I don't think so. Because what you're doing is you're

24:47.120 --> 24:52.800
starting with a bunch of random vectors. Then you're slightly adjusting them. So that specific

24:52.800 --> 24:58.880
pairs have specific relations between them. And that specific groups have specific relations

24:58.880 --> 25:03.040
between them. So they're more or less at the same distance from each other. But all of these

25:03.040 --> 25:08.080
groups and all of these words can be arbitrarily rotated with respect to each other. So I suspect

25:08.080 --> 25:13.200
that if you just get even very high quality embeddings and you just start measuring distances between

25:13.200 --> 25:19.920
them, it will look random. Because you're putting in too much noise. The distance between, I don't know,

25:19.920 --> 25:27.920
bed and blue might, doesn't necessarily mean any material. But then more generally, there's not

25:27.920 --> 25:36.800
a set of statistics or metrics or things that are relevant at the aggregate level for the embedding.

25:36.800 --> 25:41.520
Not that I know of now. Usually what people do is they look for specific relations between

25:41.520 --> 25:47.760
specific words. So they do this analogy problem. Yeah. Yeah. Because that's kind of how you check

25:47.760 --> 25:52.000
that what you're capturing is what you think it is. They're actually capturing the semantic

25:52.000 --> 25:58.240
meaning of the words and the semantic relations between between words. Why do we end up using

25:58.240 --> 26:04.800
neural nets to do embeddings that seem like pretty basic math that you can do like more

26:04.800 --> 26:12.080
deterministically? Yes. So I think that's one of the one of the motivations for people to do

26:12.080 --> 26:17.920
to invest in glove and glove is the reaction for global vectors. And this is I want to say it's

26:17.920 --> 26:23.200
coming from Stanford. Okay. And basically they try to do it basically in a deterministic

26:23.200 --> 26:27.120
way, in an optimization process, defining an optimization process by specifically saying,

26:27.120 --> 26:32.080
I want factors that where the relationships are given in this specific way. Right. The reason

26:32.080 --> 26:38.720
I think one of the motivations for using neural networks for this is because now you have very powerful

26:38.720 --> 26:47.680
tools to optimize and train neural networks in large scales. So you can use all of that machinery.

26:47.680 --> 26:51.760
Because when you actually look at the mathematics and the network structure it's actually using

26:51.760 --> 26:57.040
it's actually very simple. So it's an extremely simple neural network. Right. And where it's mostly

26:57.680 --> 27:02.240
vector multiplications and then a soft max at the end with some exponentials. So it's nothing

27:02.240 --> 27:08.400
particularly sophisticated. If you look at networks like ImageNet or AlexNet or something that have

27:08.400 --> 27:13.040
dozens of layers, they're very complex. Right. So you have all the technology to develop for

27:13.040 --> 27:16.480
these very complex things. This being applied to something that's very simple, so it makes it

27:16.480 --> 27:22.240
very efficient. So single layer, the matrix multiplications are just applying your weights coming in

27:22.240 --> 27:27.200
and coming out and then remind us what softmax is. So softmax is just an optimized way basically

27:27.200 --> 27:32.960
to calculate what is the maximum value of a vector. In a very simplified way, while at the same

27:32.960 --> 27:38.000
time basically turning a vector of numbers into something that's normalized. So the only thing you

27:38.000 --> 27:44.000
do is for each element of the vector, just take the exponential of that value and then you divide by

27:44.000 --> 27:50.560
the sum of all the exponentials. That's a softmax. And what that means is basically makes any value that

27:50.560 --> 27:56.320
is slightly larger than the others will become much larger. So it's easier to capture the maximum

27:56.320 --> 28:00.560
value. And then of course, there's many optimizations on top of that. That's the general idea.

28:00.560 --> 28:05.440
Because you don't necessarily want to calculate all the exponentials for the entire vector.

28:05.440 --> 28:11.520
Because these vectors are basically one hot team bedding of the words in the in the corpus.

28:11.520 --> 28:16.400
So this can be 100,000 or a million words. Okay. So you don't necessarily want to have to do that

28:16.400 --> 28:21.280
at every iteration. So then there's hierarchical softmax. There's all sorts of tricks to train

28:21.280 --> 28:25.920
optimizations on the more like computational tricks. But the concept is very simple. And then there's

28:25.920 --> 28:30.960
of course optimizations that people apply to it to make it more efficient and more robust.

28:30.960 --> 28:39.280
Okay. And so what's the relationship between the one hot encoded, 10,000 dimensionality vectors

28:39.280 --> 28:46.720
and 100, 500 vectors. So like I said before, the computers have a hard time understanding text,

28:46.720 --> 28:50.800
but they're very good at understanding numbers. Yep. So what you do is you map in the beginning,

28:50.800 --> 28:56.160
the first approximation is you map words to numbers. So just have a dictionary. This is word number

28:56.160 --> 29:00.160
one. This is word number two. That's your one hot encoding. And that's one hot encoding.

29:00.160 --> 29:03.920
So it's just the way you represent the world so that then you can manipulate them

29:03.920 --> 29:10.320
numerically to so that you can calculate these vectors. So you I'm imagining that you're very

29:10.320 --> 29:16.720
first step. Then your your input layers and your, well, it's just one layer, but your inputs,

29:16.720 --> 29:23.040
you're sending in the inputs to the hidden network. And then it's the one hot encoded. So basically

29:23.040 --> 29:30.000
corpus, you have the input values here. And these are, let's say in the case of skipgram,

29:30.000 --> 29:36.080
this is just the context word that you have in them. And this would be if your dictionary is

29:36.080 --> 29:43.680
10,000 words, this would be a one hot encoding, 10,000 dimensional vector. Right. This gets fed

29:43.680 --> 29:49.040
into the hidden layer in the center, which is the dimension of the embedding, say 300.

29:49.840 --> 29:55.920
And then from this hidden layer, you're trying to calculate the context. And the context can be

29:55.920 --> 30:00.480
depending on the window size, can be, let's say, 10 times the size of the dictionary.

30:01.360 --> 30:06.400
Okay. So it'd be 10 times 10,000, so the 100,000, because they're trying to predict 10 words,

30:06.400 --> 30:14.240
5 words before 5 words after. Okay. So then you've got your, so dimensionality 10,000 or so vector

30:14.240 --> 30:21.360
coming in your network is your, the dimensionality of your hidden vector, that's your hidden layer.

30:21.840 --> 30:28.720
And then the output side is 100,000, it's like 10 times 10,000. If your window size is 5,

30:28.720 --> 30:33.280
you're trying to predict all the context from the single word, then you're trying to go from 10,000

30:33.280 --> 30:41.760
to 300, to 100,000. But you're not really using the 100,000, you're just using that as kind of

30:41.760 --> 30:46.640
an optimization to get at the weights for the 300. And that's what you use. Yes. So basically,

30:46.640 --> 30:52.400
you can think of over to back actually as unsupervised learning problem, because you're feeding it,

30:53.520 --> 30:57.920
you're feeding it, the inputs and the outputs, so that you can learn something about the system.

30:57.920 --> 31:03.680
So you're not really in practice, you're never really trying to predict the output. In the set,

31:03.680 --> 31:07.280
I guess you could use that if you're trying to generate, use a generate text without

31:07.280 --> 31:12.880
anything unusual. Okay. So you're trying to basically see what the network is learning from the

31:12.880 --> 31:17.280
text that you're giving it, and you're giving it the word and the context, and you say, okay,

31:17.280 --> 31:21.520
figure this out, figure out what is the right representation that from this input can generate

31:21.520 --> 31:25.360
this output. Okay. And then in the end, what you're interested in is actually this internal

31:25.360 --> 31:31.040
representation, this wording beddings, inspectors. Awesome. Awesome. This has been super helpful.

31:31.040 --> 31:35.360
Like, I feel like I finally understand it. It's easier to explain with pictures and drawings,

31:35.360 --> 31:42.000
but yeah, for folks that are not in the room, we're not moving our hands around and drawing on,

31:42.000 --> 31:46.000
you know, layer in the air and stuff like that. I can send you a link to my slides. Actually,

31:46.000 --> 31:49.680
I was actually just going to ask, like, if someone wants to learn more, what's the best way for them

31:49.680 --> 31:54.240
to learn more? I mean, all of these papers are online, you can find them easily. I'm preparing

31:54.240 --> 31:59.600
this tutorial for O'Reilly AI in San Francisco in September. Okay. I will post all the slides and

31:59.600 --> 32:06.000
all the code in my GitHub. Okay. Right now, there is already the slides for a shortened version that

32:06.000 --> 32:10.640
I presented a couple of weeks ago. So I can, I can share that with you. And then eventually,

32:10.640 --> 32:15.120
after O'Reilly AI, I will update it with all the, the newest stuff. Okay. Great. Yeah. So,

32:15.120 --> 32:19.040
send that over and we'll get that in the show notes. Excellent. Yeah. Great. Thanks so much for

32:19.040 --> 32:28.480
stopping by. Thank you for inviting to the pleasure. Awesome. All right, everyone. That's our show

32:28.480 --> 32:35.760
for today. Thank you so much for listening. And of course, for your ongoing feedback and support.

32:36.400 --> 32:42.720
For the notes for this episode, head on over to twimolei.com slash talk slash 48.

32:43.600 --> 32:48.560
If you like this episode or have been a listener for a while and haven't done so yet,

32:48.560 --> 32:53.840
please, please, please take a moment to jump on over to iTunes and leave us a five-star review.

32:54.400 --> 32:59.760
We love reading these and it lets others know that the podcast is worth tuning into.

33:00.480 --> 33:07.120
One last note, you've probably heard me mention Strange Loop. A great technical conference held

33:07.120 --> 33:13.680
each year right here in St. Louis. We're a bit over a week away from that conference,

33:13.680 --> 33:19.440
so I encourage you to check them out. It's thesestrangeloop.com. I'll be there,

33:20.080 --> 33:25.600
let me know if you'll be there too. For more info on any of these events, check out the show notes.

33:25.600 --> 33:44.400
Thanks again for listening and catch you next time.

