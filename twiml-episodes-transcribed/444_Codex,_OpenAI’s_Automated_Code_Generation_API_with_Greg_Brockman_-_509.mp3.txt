All right, everyone. I am here with Greg Brockman. Greg is co-founder and CTO at OpenAI. Greg,
welcome back to the Twomol AI podcast. Thanks for having me, Sam. Hey, it's been a while since we
spoke. It was back in November of 2017. Believe it or not, episode 74 of the podcast, we're over 500
now and we were then talking about AGI. I am really looking forward to this chat where we'll be talking
about something new that OpenAI has been working on for a while, Codex. But before we do, why don't
you reintroduce yourself to our audience and tell them how you came to work in the field of AI?
Hey, everyone. I'm Greg as I, Sam said, and I am one of the co-founders of OpenAI.
You know, for me, I've read the Alan Turing 1950 paper, Computing Machinery and Intelligence
Paper back before I knew how to code. And I remember reading it. You know, it lays out the Turing
Test. But then it says, look, you're never going to be able to program a solution to this test.
The only way to do it is you have to have a learning machine. And he goes into quite some detail.
You know, he says, like, look, you're going to have to do this, like, to have a little machine.
It's almost like a child machine that you give rewards when it does good things, punishment when it
does bad things. And from there, you can hope to build up a solution to this. Really visionary
stuff, honestly. And for me, I was captivated by the idea that you could build a machine that
could understand problems that you yourself could not. And I just saw being able to build machines
that could themselves help you solve problems that were outside of your reach, be the thing I wanted
to do. So I went to a professor and was like, hey, can I do some MLP research with you? And he's
like, great. Yep. Here's these, like, parse trees and things like that. And sadly, it didn't look
like that was going to quite get you there. So I got distracted by programming languages, which,
you know, I think kind of captures the same idea, right? If you can build a compiler,
you can kind of understand this program and can really amplify what a human can do.
And then, you know, it did startups and it was really 2015 that I first encountered deep learning.
And for me, I was watching hacker news every day. And I felt like there was a new deep learning
for this, deep learning for that. But I didn't know what deep learning was. And it was actually
surprisingly difficult to just Google around and learn what deep learning actually meant. So I
asked some friends about it. And as I started going around, I realized all of my smartest friends
from school were now in deep learning. And that for me was a real sign of, okay, maybe there's
some real substance here. And the deeper I dug, the more it felt to me like the old direction,
that it just didn't feel right, the new direction actually did feel right. And to me,
you know, looking back at it now, the thing I find most fascinating is that really this neural
in that direction, it's not a, you know, five, 10-year thing. It's really a 70-year journey to get
to where we are. So it's just exciting to be pushing the frontier of what these neural networks
can do. And that's basically what we've been doing in OpenAI the whole time.
Nice. You mentioned your interest in programming early on and parse trees and all that kind of
stuff. And you know, there's maybe a connection to what we're going to be talking about today.
Again, which is Codex. OpenAI recently announced co-pilot, which is another project in the same
vein. Maybe tell us a little bit about these projects, what they are and how they're related to one
another. Yep. So we've been building the Codex model for about a year now. We really started
when we saw GPT-3 be released. And people's, you know, most excited reactions were actually
using it for programming. And we looked at that and we said, well, we didn't build this model
of the program at all. What happens if we actually put some effort into it? And so we've actually
teamed up with GitHub and Microsoft. You know, GitHub, I think, is probably best in the world that,
you know, knowing what developers want and have great, your great community. And obviously,
you know, that they have, they have lots of data as well. And so we worked really closely with
them to try to build a product that people wanted, right, to really validate that what we were doing
wasn't just a cool research project, but was actually useful from day one. So a month ago,
we released co-pilot together with GitHub, which is the first product built on top of Codex.
And that they use the Codex API that is the same API that we, I guess by the time people
watch this podcast, will have released on Tuesday. And so, you know, talk a little bit about the
relationship between Codex and GPT-3. Is it an entirely separate model? Are they the same model
with different training data, different training processes? Yep, I would think of Codex as a descendant
of GPT-3. So spiritually, you do the same kind of task. GPT-3 is take all of the text on the internet
and just do an auto-complete task, predict what word is going to come next. Codex is take all
the text on the internet and all of the public code and do that same process. And we made lots of
improvements all across the board. Really, this has been an effort of a quarter of open AI to make
it happen. So we've really had to put in efforts from everything to, you know, we have architectural
improvements, we have training improvements, we have a lot of just like the good old-fashioned
engineering, maybe these models be fast and responsive has been a huge amount of work as well.
So it's really been improvements all across the board. So kind of talking about Codex relative
to GPT-3, you mentioned, take all of the text on the internet and all of the code on the internet.
In creating something like a Codex, are those given equal weight or
is the code somehow, you know, more relevant for the task that Codex is likely to see?
Yeah, the short answer is I think we're still figuring out exactly the right way of doing it
ultimately. I mean, I think that, you know, right now our process is, you know, I think you basically
end up with, you know, you end up seeing much more code more recently than text. But it's still
an open question, I think, exactly what you want. And we've kind of found that when you actually
look at the models in terms of how people want to use them, that part of what makes Codex really
shine is the fact that it has all this world knowledge built in. And so you can actually end up with
a model that's very, very good at doing just like, you know, sort of very narrowly defined,
complete this programming, you know, this function or something, without actually being very useful
to people. So I think that finding the right evaluations is actually one real trick to make
this model work. And so, you know, what we really focused on is, and has actually got it,
it's pretty well so far, is at the very beginning of the project, we've wrote down this data set,
this now open source called, we call it human eval, which is a list of problems written by humans
that are just programming puzzles. And we kind of designed them to be ones that are kind of,
they're a little bit of tricky wording and a little bit like, you know, it's different from what
you would find in, you know, some already out there in the training corpus. So kind of intentionally
chosen to have some twist them and that sort of thing. And what we found is that by pursuing
that metric, it is actually our best north star metric, like everything else, if you just look at
perplexity, you know, basically like, how good it is exactly at predicting next token in text,
that that particular metric breaks down a little bit for us, because you kind of want this holistic,
not just how certain do you get that there should be a period here, but you really want just given
a pretty natural description of what the problem is going to be, can you solve that problem?
And so when you created that that data set and that metric, did you, was there a closed loop
there where the things that the programs that Codex created against that training set had to
actually run and produce the desired result? I understand, yes. I'm mostly asking just from
the perspective of evaluating the formats of Codex in terms of producing runable code.
An aspect of that data set. 100%, 100%. So you literally take, you provide the model with,
you know, maybe a doc string and maybe a little bit of a function definition, it generates a bunch
of code, you literally eval that code. Now the details of the eval actually I think are pretty
interesting because you just had some code come out from your model. What's it going to do?
Is it going to lead all the files on your computer? It's all possible, right? And so you really
need to have a good sandbox. And so, you know, I think that one thing people miss in this field is,
you know, it's all about the big idea, but what people miss is that actually it's about the small
ideas, right? It's about getting the engineering really right. And so yeah, you want to actually train
a model to run some arbitrary code and eval that makes sure it's doing the right thing. You need to
have a world class sandbox to make that happen. And so you need to make sure both that the execution
is like not able to do anything, you know, tamper with your system, but also that, you know,
just even little things like resource consumption and being able to crash your system and things
like that are held in check. And we actually have found multiple times that the model would generate
code that kind of broke our current sandbox. So we've upgraded it since then. Interesting, interesting.
So I think that is suggesting the folks that play this play around with this via the API that they
take care to inspect the results they get before they just run them if they don't have a sandbox
environment that they're. Yeah. I definitely, I definitely recommend that for any code you take
from the internet. You know, if you just download some code from even my GitHub, I will not take
a fence if you double check it before just running it. I think it's, it's just an important thing
generally. But I would say that this, this like the model doing unpredictable things is really
early in training, right? So when the model isn't very smart, isn't very capable. It's sort of
less predictable exactly what it'll do. The more capable the model gets, the more it's going to be
faithful to your instruction. So I've been, I've been using this model for, for, you know, I spent
quite, quite a bit of time playing with it. And that I've actually, you know, found that it's,
that it's quite reliable in contrast in some ways to GPT-3.
Can you talk about those distinctions a little bit more in terms of the types of
results that it tends to see versus GPT-3 relative to the, the prompts that you're giving it?
Yeah, see the thing about GPT-3 is that I always, and I really, like when we get these new models,
I really spend a lot of time with them trying to really understand them, trying to like just sort of
feel like I get the personality of these models if you'll forgive the term because these models,
you know, they're not one thing, right? They're really this whole distribution of things.
But so for GPT-3, I really spend a lot of time trying to teach it. You know, I have this whole chat
session where I was a teacher and I was explaining to it how to sort of list of numbers. And it would
do one example, get it right, and I'd be like, wow, I've really taught it the process of sorting.
And then I'd give it another example and it would totally go off the rails and do something wrong.
And I think that the feeling that I had was GPT-3 didn't really want to list. And like it really
felt like this, this, you know, this, this being that like had a short attention span and would
just kind of like do random things sometimes. And I think that that's probably a reflection of
the training data in some ways, right? If you're out there on the internet and you read some text saying,
okay, now I'm going to sort of list of numbers. I mean, maybe you're in the middle of the fiction
story, right? And then like, you know, that some aliens arrive or something. And so it's actually
reasonable for GPT-3 to make pretty, pretty arbitrary predictions when it's not very confident
what should come next. But by contrast and code, what I found with Codex is that when it fails,
it does half my instruction, but not the full instruction, right? And sometimes, you know,
sometimes you can end up with the traditional failure modes of autoregressive models where it fails
by repeating a token over and over if that's the most certain one. And basically most of my
experiments have been, I haven't had to fuss with hyper parameters and I really just set
temperature equals zero, so it's just always picking the most likely token. And it's worked out
way, way better than for any model that I've tried before. And I think that a lot of this comes
back to the structure of the data, right? That in code, if I have a comment saying, now I'm going to
sort some numbers, you're really going to sort numbers next, right? There's really nothing else
that's about to happen. And so it's almost like we have this great data set that we've built up
of instruction following. And I think that that idea we found in GPTland was pretty key to
getting something that's even more useful to people. And in code, it's almost built in.
Yeah, I'm very curious about this idea of, you know, the code is a data set and the self-documenting
nature of it. When you think about just kind of raw code that you might find in GitHub,
you know, there's documentation that's going to be at a, I would think a pretty low kind of
semantic level, like, you know, this loop is going to do thing X. You know, I think of something
like a stack overflow that's talking about the code that you might see in a post at a much higher
level. And I wonder a little bit about, you know, as all of, you know, to what degree is the code
that codex is trained on, you know, GitHub style versus something that might have some more like
higher level semantic meaning. And, you know, just your thoughts on whether that matters and,
you know, how codex might evolve with different types of data that you trained it on.
Yep. I think the answer for this stuff has probably got to catch them all.
Like, I think we're at a point with these models and I think GPT kind of set the stage for it,
is that the broader you go, the more capable you're going to get. Yeah. And the part of it is that
when we do a task, it's kind of impossible to predict exactly what skills people want to bring to
bear, right? Like, that the, you know, it's almost like, if you rewind to the, before the, you know,
general purpose computers were obviously the right solution, which by the way, they're not even
obviously the right solution for all problems, but for most problems they are.
There were specialized machines for each individual application. And people were always like,
well, your general purpose computer is cool and all. It's great demo, but if you really want to do,
like, you know, your, your, your contact book, you need to use this specialized, you know,
IBM, whatever, you know, machine that existed at the time. And I think that the basically just turns
out that many tasks require mixing and matching between lots of different things. And so it's kind
of hard to pre-bake one answer to everything. And so where we've started has been, you know, again,
kind of all the text out there and all the public code. But I think that within code, you know,
it's not just like, you know, Django and projects like that. It's also think about all the ipython
notebooks that people put on GitHub, right? And that those ones tend to be very much like a
tutorial, right? There's lots of lots and lots of casual tutorials and things like that that
are out there. And so you get kind of a very different slice of intelligence from those.
And I think that what we've been looking at, like, I think kind of a big next step really is
figuring out what are the best sources? You know, what do you learn from each one? How do you
figure out what you want to balance in that model? And one thing that people probably might be
surprised to hear is that codex, you know, it can do lots of things in lots of different languages.
You know, it's probably pretty good at about it doesn't different languages. But we really trained
it just for Python. Like, we actually were just like, we just want this thing to be as good
as Python as we can. And all the other stuff kind of fell out as almost an accident. So I think that,
you know, if you test it, it'll be interesting to see, you know, do people find it very useful
for the broad range of languages? Or is, you know, sort of that focus on Python? Does that shine
through? Nice. Nice. I can tell you that it does do hello world and list.
Okay, good. There we go. Believe me, we did not try to make a good list.
You know, also fascinated by this, this idea that we talked about earlier, you know, the
language, the natural language, plus the code. And there's part of me that would love to like
tweak some hyper parameter that lets you wait one versus the other. Any, you know, any thoughts
on that? Or I suspect that your answer is going to be similar to the last one, which is kind of
the more the merrier, all having all the data, you know, is going to get you better results and
trying to over optimize or yeah, yeah, I think some of the stuff you're you're hitting on the
right front here, as I think, right? And look, like just to zoom out to the big picture,
to me, the most fascinating thing, first of all, is that this is all just a neural net, right? Like,
you rewind back to the 40s and, you know, pits and McCullough, like that model of the information
processing of the brain, like that's the thing we're still doing today. And so, you know, you can
actually find this great paper on Wikipedia called like, you know, an interpretation of the
history of the, of the perceptron. And the, you know, the story ever and always told about
the perceptron was like, hey, in the 60s, these neural net people overhyped everything and,
you know, all the funding went away. And if you actually look at the historical documents,
kind of what was going on is there were two competing camps. There was the symbolic systems
people. And then there were the neural net people. And that this symbolic systems people had a
very concerted campaign to try to drive all the funding for the neural net people. And that they
had all these disparaging things to say, like those neural net people, they have no new ideas.
They just want to build a bigger computer. Like that's all they want to do. And, you know, here we are,
you know, 40 years later, 50 years later. And yeah, we just want bigger computers and more data.
And so I think that is actually the most core answer. Like, you know, I think that we all kind of
want the great scientific insight of like, you know, to, to figure things out and to figure out
the exact theory of mixing. And I think actually the funny thing is I think we can make progress
on those problems. But the highest, highest order bit is you need to have a big machine with
thoughts of compute and pour in all the data you can. And like, you know, at some point that that
that the details that mix start to really matter. But the highest order bit is actually achieving
that first thing. Does that, you know, to what degree does that like cap innovation? If you've
already, you know, pulled all the language in the world, all the text in the world into GPT-3
and all the code in the world into codex. And it's all about, you know, data and size of compute,
where do you go to innovate? Yeah. So I think it's a great question. So on the one hand,
you can look at what I said as a pretty depressing thing. I'm just like, okay, it's just, you know,
just a simple matter of, you know, doing this, this large-scale engineering. And you need to have
your particle accelerator equivalent in order to do it. But actually, if you dig into the source
of progress in recent years, you know, we've published a couple studies on this. And so we have one
study that shows the compute ramp is insane. Like, it's just faster than any exponential that I'm,
that I'm aware of. But we also have another study showing the algorithmic ramp, showing the
efficiencies due to algorithms is also exponential. And you know, rather than being like, you know,
doubling every 3.5 months, it's like, you know, more like, you know, doubling every year, year and a
half, you know, something much more like Moore's Law. I forgot the exact number. But that's still
a pretty insane rate of progress. And so I think that the truth of all of this is that if you have
a paradigm that is worthwhile, right, that like, making it a more capable neural net, clearly a
worthwhile thing at this point, you're going to innovate to the max in all dimensions. And yeah,
we've had a pretty big compute overhang because people just weren't willing to spend lots of money
on computers. And now people are. So they just spend more money to get ahead of Moore's Law. So that's
one dimension. Similar story for data, you know, that there's been lots of data out there, just like,
it just wasn't really worth people's effort to collect it or people didn't really know to do it,
whatever it is, there's an overhang of just gather all that data. But on the algorithmic front,
you know, I think that's been the one that people have been pushing on. And so there isn't as much
of an overhang, you know, there's not like low hanging fruit left around that just no one's
thought of that just, you know, you just show up and you're just like, gold's at my feet,
it takes effort. But I think that the fruit is still there, right, that it's still the case that
we are making this exponential progress there. So I think it's like, just because we're making
big progress in certain dimensions, you know, that's just temporal, right, that like we cannot
keep up the rate of improvement from those dimensions. And so, yeah, once you've saturated them,
the only thing left is going to be this other dimension. So I think it's really important,
we as a community don't lose that muscle that we really build it up.
And now if I asked you to comment on that algorithmic dimension, would it be,
would it be asking you to speculate into the future or is there, you know, a set of kind of,
you know, relatively low hanging fruit that, you know, things that you know that directions
that you know that you want to head on the algorithmic side? Yep. Well, I want to talk again about
just, you know, first of all, my personal philosophy, you know, is very much like, you know,
greatness through a thousand small steps that I really, you know, I think that there are some
people who are extremely good at the like one big idea to change everything. And I think that,
you know, like Ilya, who's one of my co-founders is extremely good at that. You look at like,
you know, I think he's, you know, with work like Alex, and I think he's very good at sort of
setting the direction. But for me, I tend to think in terms of, okay, like what are all the small
details we have to get right to make this happen? And if you look at the current models, you know,
the funny thing about GPT-3 is that it actually used the same, the tokenizer that Alec Radford,
who works at OpenAI, wrote kind of like overnight, right before the deadline, you know, three years
prior for GPT-1. And like, you know, that thing is not optimal. It's actually become kind of the
standard lots of people use it. I mean, you know, people have done a little bit to, to, you know,
play with different organizations and retrain the things like that. But fundamentally, I think that
that there's like a big, you know, big shift in some ways in kind of small detail in other ways
of just, we should be really doing by level models, right? We shouldn't be doing this like, let's
like, you know, sort of tokenize things and duck and chunk them up in this like way that kind of maps
to, you know, it's almost like hard coding that's in the model that probably would do a lot better
if it wasn't there. I think a lot of the story of neural nets has been removed the hard coded stuff
and add in learning. So I think that's one example of the kind of thing that I would really love
to see someone work on and just see, to see great results from and for us to incorporate that.
So I think basically little bits of the architecture that are still like, yeah, we really should be
doing this differently. I think that that for me is actually where I put a lot of focus.
I wanted to kind of transition to, you know, how we should think about codex as like users and
practitioners. Those folks that want to play with it, like, how should we, you know, think about
interacting with this API to get the most out of it? And let's maybe start with, what is it best
that versus where the, you know, the soft edges? Yes. Well, I will first say, I think that no one
knows yet to the answer of, what is it best at? Like, I can tell you what I've discovered in my
efforts, right? And I'll say, for me, I know I'm scratching the surface. Like, I know I am.
Fair enough. But, and that's, that's the wonderful thing, by the way, you know, if you train a vision
model on an image net, you know what it's good at, right? It's very, very good at all the dog breeds.
Um, this model, general purpose, so it's quite good at lots of things. Um, I have, so for, for me,
you know, I really latched on to this, uh, being able to provide instructions in natural language
and have it generate an executable output, right? So basically talk to your computer, does it?
Um, when, you know, when we first started playing with the model, like, it wasn't clear that it
would be good at that. And I just kind of realized, like, hey, this model, when I give it these, like,
because I actually started out the other, on the other side, I started out with trying to say,
if I just want to provide one big instruction and have it write a whole program. And, you know,
it's quite reliable at doing things like, I'd say make it to Kinter UI that, like, has a button that
says, hello world, then you click it and send an email, like, that level of instruction, it could
actually write, like, you know, the 30, 40 lines of Python to do it. And sometimes I make a little
mistake. You'd forget to, like, wire up the button, or, you know, it kind of like a placeholder for
whatever. Um, but the way it would fail was, again, very interpretable, right? Because look at
it, I'd be like, oh, okay, just kind of forgot this piece. And so then I started thinking about, well,
what I really want is I want to be able to chunk this instruction up into smaller pieces,
because, you know, it did 80% of it. And so if I just had a 50% size instruction,
maybe I'll do 100% of it. Um, and so I think that that's kind of the highest level picture of
where we are is the model. I think it's not yet ready to do big things, right? On its own. But
it's ready to do small things. And honestly, for programming, like, I like doing the big things.
I don't like doing the small things myself, right? The, uh, the like, you know, okay, like,
here's this very specific fiddley thing. And like, get the details of the indexing right. Like
that kind of thing, the model, it knocks it out in the park, or memorizing the details of,
you know, this, whatever framework, like, you know, I used to write and Ruby on Rails. And most
of Ruby on Rails is just knowing what the Railsism is to do. Any particular thing. Right. Right.
And, um, yeah. And by the way, I mean, like, ID's are just not very good at Ruby on Rails,
because there's so much dynamism and things like that. But this model, I think, would be,
would be quite good at it. So I think basically figuring out how to work with those strengths is
important. And so part of it is, I think, like, one dimension that I think is very exciting is
baking it in as an interface to lots of existing applications. So we have a example of baking to
Microsoft Word. I think that for any website, really, you should now be able to very easily build
an interface where you just say, like, you know, you know, what, if you're depending on what
your web app is, you know, go and look up the, you know, go send an email to this person or,
I, you know, like, yeah, any of the functionality that's in your website should become voice
controllable or, you know, natural language controllable without having to nest or you click through
a bunch of buttons in order to get there. You mentioned that some of your initial observations
were that you, you construct this prompt and it will spit out results that, you know,
were 80% there or missing some detail or something like that. And that you solve that by kind of
chunking your prompts and making them smaller and more compact. Do you think the issue that you
experienced originally was, you know, was on the input side or the the output side, if that makes
sense, was it a, you know, issue in the, a fault in kind of the generation process or it, you
know, couldn't pull the piece for that, you know, couldn't make the connection necessary required
for that last 20% or was it, you know, forgot it in the parsing stage, using really rough language
here to, yeah, yeah, you're asking the great mysteries of the what's going on inside the neural net,
which, you know, I think is always very interesting. And, you know, for me,
well, first of all, I also think that if you asked literally me to do the same task without
access to an interpreter, so I just have to write the program once without ever being able to push
backspace, I'm not going to do a good job either, like trust me, like I will not. Most of programming
for me is I write a little bit and I run it and it doesn't work and I change it and I fix it and
I iterate and I fix it, you know, and that, that other piece, this model doesn't get to do it at all.
So I think that it's very possible that the model simply cannot, like, you know, just from reading
all that text and really deeply thinking through all the details about the interface should work
is a bottleneck. And then secondly, it's very possible that just like, it just as it's writing,
it just runs, oh no, I really wish that I'd like implemented this function beforehand. So you
know what, I'll just pretend that it's implemented later and like, you know, that never gets to it.
So I don't know which of those stories is more true, my guess is that it's a mix of both.
And partly, I just look at myself, like, you know, look, this is not a human-like intelligence,
so it may be too, you know, a little bit too egocentric to think that I can look to what I'm good at
and bad at to map to where the model makes mistakes. But I will say that for me, it's been actually,
like, I feel much more in tune with the failures and successes of Codex than I did with GPT-3.
For me, it does feel like when it fails, I'm a little bit like, you know, sometimes,
sometimes the way it fails, by the way, is it'll just put in pass, you know, so it's like, you know,
I have a nice Python, you know, death, whatever, and like, I put it in a doc strand, like, okay,
model, you go now, and that's solution is just to put in pass or, you know, comment to do,
something like that. And I get it, it's a little bit like, it's like, okay, I'm not going to be
able to do this, so I'm not even going to try, right? And, you know, I don't think that's necessarily,
the, you know, the only characterization. But it really feels like, you know, if you think of
how code is usually structured, that I think that it actually starts to feel a little bit more,
like, constrained in terms of the, the, again, you know, you have this pattern of comment,
complete, or total out, and kind of nothing in between.
Yeah, you just mentioned the structure that code tends to have the, you know, codex operates
like GPT, GPT-3 in this kind of input, you know, process output paradigm. Have you done any
playing around or experimentation to try to force fit structure into that input in a way that
it understands that it can produce more, you know, structure on the output? Well, so I have one,
so I have, I have a couple of different dimensions that I think are very interesting, right? So,
look, there's one dimension that I think is kind of fun, which is translating between languages.
And so I have a little demo of a writing, writing a Python program, so I wrote a Python program,
that then you run it, make some calls to the API, generate some Ruby code, and that Ruby code is
just a program that calls the API to generate some Python code, and you get this Python Ruby oscillator
forever and ever. It's a little bit like writing a coin. It's just like kind of a fun, fun little thing.
I actually tried doing the same thing for Python to Ruby to JavaScript to Python to Ruby JavaScript.
I got it to do like six cycles of Python Ruby JavaScript before it broke. So it actually was like
each time writing a little bit of unique code, which is kind of a cool thing to see. So setting it
up for that, I think, was a very interesting challenge, because there you really have to make sure
that your your prompt which is kind of contained within the program is something that kind of like
gives the enough context to the API for it to actually generate the whole new program, but
you know, it's like you really got to play some some some nice fiddley games to make it happen.
So, you know, that I think is more of a proof of concept. It's more of like an interesting
exercise than it is something very practical. But there's actually another direction that I was
experimenting with that I I think it's like interesting and very fruitful. Someone can make it work of
you know, look, programming is two things. It's understand the super hard problem and decompose it.
Right. So it's basically problem decomposition and then mapping the small problem to code. We've
already said codex is really good at that second thing, probably better than I am. That first one
is it actually bad to it. And all I know is that the obvious ways of making it good at it,
I haven't succeeded at, but I but using codex for task decomposition is something I've tried a
little bit and got some interesting results on. And you know, you can do things like you have
codex call into, you know, you basically tell it, oh, there's this like magical oracle function.
And so oracle is you give it some natural language and then just like the machine will magically
implement it for you. And then you say, okay, do this hard task and you get access to call
the oracle thing. And then you can see it can codex generate good calls, subcalls to oracle.
And I've actually gotten it to as a little bit of like a, you know, together working with codex
to be able to get it to do things like, you know, go on Google and like download an image of,
you know, a particular person and put into a website and things like that. And you know, you
use Selenium to to orchestrate all of this. And I think that ideals like this are very interesting
because maybe you can actually have codex as a tool that helps in more of the cognitive domain
in addition to this like very mechanical like code emission domain. Is there a, you know, input
pattern that you've seen or a hyper parameter that can kind of guide it towards a degree of
complexity in the solution? Like there's a, the length of the output, you know, as a, as a, you
know, one idea that might be that, you know, hey, if I say, you know, give me hello world and I
want it to be, you know, 300 characters in length or a thousand characters in length, that's going
to be, you know, one thing. If I say, you know, 10,000 like, is it going to give me the, you know,
the J2EE enterprise? I mean, I think the best, the best starting point, by the way, for all
these things, the only real answers you got to try it, right? Like you really seem to play with it.
But I think the place to start is just by asking the model for what you want. And if the model
doesn't quite seem to get it, you try to spell it out more clearly, expand how you're asking,
like really think about if this were a junior programmer and I had to really hold their hand and
walk them through it, how would I do it? Right? And sometimes that's breaking up into multiple
instructions. Sometimes that's just expand more of what you're asking for. So I think that's
definitely the starting point. Another very powerful thing is providing more examples, right? So
one thing we really haven't done very much of yet is trying to do GPT-3 style prompt engineering
and trying to provide prompts to the model that really show examples of the behavior you want.
And like all the indications so far and all the times that we've tried is that it's quite good at
that. But we just haven't really pushed it in the way that we push GPT-3. In part because
it's already capable of the tasks we want simply by asking. So we just kind of didn't have to go
down that road. And then the third thing, of course, is fine tuning. And so we have a GPT-3
fine tuning API these days. We'll be wrong that out for codex. And I think that that will open
a new dimension to what you're able to make it do. Awesome. One of the interesting examples I saw
in some of the materials was a, you know, not your traditional kind of creative program like,
you know, XYZ, but it was to solve this word problem like, you know, from an elementary school,
you know, Jason has six apples and four apples, something like that. But it created a program to
figure out this word problem. Yes, right? I thought that was really interesting. And it made me immediately
think about the implications of something like this in education, you know, both coding education,
but, you know, more broadly in education. Any thoughts on that? Yeah. So the funny thing is when
we were starting open AI, I, you know, I left my previous job and I knew I wanted to start a company.
And I had three possible domains on my list. Number one was AI, which turned out to pan out.
Number two was VR slash AR and I kind of scratched that off very quickly. But number three was
programming education. You know, this is an area that's very near and dear to my heart. I feel like,
you know, for my programming education, it was, you know, I started out very self-taught,
just building stuff that I was excited about. And it was just hard, you know, it's just not very
much fun to like, you know, it's like, you do W3 school's tutorial back in the day. I'm sure
there are better tutorials now. But then you're just stuck staring at an editor and thinking about what
do I build, right? And you run your thing and it doesn't work. And what do you do, right? And you
don't know about a lot of concepts, you know, I didn't know about serialization. And so I was building,
actually I built one of the first things I built was a chatbot game. So it was that you had a
little chatbot that you could train by talking to it. And then you can have a little chatbot
battle where you would like play this game where one window that you were talking to with a chatbot,
one was a person and you had to distinguish which which which which which was which before your
opponent would. And all this stuff, you know, I didn't know what serialization was. So I just like
had this like I came up with a magical identifier that or you know, like a string of characters
that I thought no one else would whatever type. And I use that as my record separator.
And just looking back, I just wish that someone was there to say, oh, you should probably use JSON here.
And then I'd be like, what's JSON, right? I'd go around, I'd figure out how to use JSON. And I would
have just sort of cut off this whole tree. You know, there's a little bit of the tree that was very
useful for me to figure out why is it useful to have serialization? Like, you know, why don't you just
want to do your own record separator? You know, what are the problems? But there's a bigger tree
of really implementing it and building up the library and trying to make it work and like that kind
of thing that was a little bit of wasted effort. And so what I am excited to see with Codex is that
we have a model that for the first time you can show it code and can actually kind of understand it.
And so we've done a little bit of playing around with code explaining, right? And actually can do
a decent job of taking a function and explains how it works or can generate comments for it or
generate doc strings, generate unit tests. I think that all those things really open up the
possibility of having a personalized programming tutor, right? And that to me is just like, it's
amazing. I would love to be able to see programming education fall out from, you know, pursuing the
AI passion and we will get there. It's just a question of, you know, I'm hopeful that Codex is
enough at least to take the first steps. Does there need to be an element of, I guess I made a
mental connection to like explainability in these kinds of models and, you know, tutor, you want
your tutor to be able to explain to you the connections beyond just, you know, showing you an example
which is kind of what Codex does now. That kind of called a mind, the whole explainability around
these kinds of models to you and do you think that's a piece that would be interesting in that
context? Yeah. So I think maybe in a non-traditional way. Like I think that the traditional explainability
has been, we want to look at the connections of the neural net and explain why it made a decision
that it made, right? But I mean, if you think about the equivalent problem for humans, we're not very
good at either, right? You know, we don't open up the neurons of the brain and be like, oh, wow,
look at the connection between these two neurons, right? You asked someone, why did you make that
decision? And I think most of behavioral science is basically realizing that our own explanations
of our actions are quite poor, right? That like, you know, you kind of do something and you come up
with some like back narrative or why you did it. So I kind of feel like the baseline we should
shoot for is that we should shoot, you know, look, we should get to a better place than where we are
with humans in terms of being able to explain why decisions were made. But at the very least, I
think it's a good baseline to hit. And so I think that what we should be trying to focus on with these
models is that, you know, they rate some, you know, they're given a function and that they
should explain how it works, that they wrote their own function, they explain why they wrote it,
and that explanation actually adds up. You know, and like, maybe it turns out that in fact, just like
the human version, that it doesn't quite correspond to, you know, sort of objective truth in some
ways and that it says, well, I mean, the decision because of this variable and that variable,
I mean, it changed that variable and it still does the same thing. You know, that kind of experiment,
I think would be very interesting to see. But on the other hand, I think that for the super
complicated task, and let's not hit ourselves, I mean, like, even writing the simplest program is
a super complicated task of like, you just got to understand so many different concepts, you need to
know this whole library of all these different functions. Like, that is really hard and I think that
to even fit in our brain exactly like, okay, like, you know, how do I translate, how would I even
write a program for, you know, say, you know, say it five times, you know, like something like that,
what is it supposed to reference? Like, you know, five, like, how is that represented? Like,
all the different ways you could see it. I think that for us to write a program that can do that
is just going to be such a giant complex tree that even a trace through it would be extremely
complicated and probably, you know, something that's outside of humans, humanity's ability to
understand. So I think the trick is number one, having the, you know, just focusing these models on
being able to provide good explanations that feel right at an intuitive level, but the feel like
they were written by a person. And I think that that, we're on trajectory four, you know, I think
that you can ask codex for this stuff today, and maybe it'll do a good job, you know, maybe it's
not exactly what it was trained for, so maybe it won't, but I think that you can at least get started.
But I think there's a next step, and this is actually part of our alignment work at OpenAI,
is thinking about models that themselves are really optimized for explaining what another model did,
right? Because here we have these, you know, with the super complicated problem that this model
I came up with a solution for, and that it did in a super complicated way that we can't understand,
but hey, we know what to train models that can do super complicated things that we don't understand,
and so maybe you can't explain your model to do it. And I think that really finding the right
balance here where you can have a very trustworthy model, and you know, that there's ideas that we
have for how to actually do it, but maybe you can bootstrap your way to models that can actually
solve problems where we don't even understand the solution, but then they explain, and they have
to really prove to these other models that what they're doing is legit. And I think that this
kind of thing is in our, you know, might take a while to get there, but that isn't our future.
Yeah, some of the broader societal issues that, you know, something like a codex, codex gives rise
to or questions like jobs, copyright, and potentially fairness bias. Who may be dig into those
really quick thoughts on kind of job implications? Yeah, so I think that the interesting thing about
codex in particular, as an example of AI in general, is that it's just not playing out how people
expected, right? I think that the expectation was that AI is going to take this job, and then that
job, and then this job, and the only question is just ordering the jobs in order of automation.
But in reality, I think AI is kind of taking no jobs, and it's taking a percentage of all jobs
at once, and that percentage tends to be the kind of boring, drudge work stuff. And I think that's
actually a pretty inspiring picture, right? You look at it in the case of codex that programming,
you know, being a software engineer requires you to talk to users, understand what users want,
come up with an idea of the thing that they are going to be excited to use, and have this picture
of like how you're going to build it. So there's the architecture of the system. When you come
to implementing, you want to design in a way that will be future compatible. So, you know, tomorrow
users are going to ask you for something else, and you should make it so, you should make it so
it's really easy to build that feature, right? So you kind of anticipate all the different ways
that you might want to modify your system. None of that is, and then you also want to write, you
know, you want to implement using a framework and, you know, know that exactly. After all that,
it's API docs and Stack Overflow. Exactly. Exactly. Exactly. So we actually have very poor tools
for those those that last piece, but that's not what we want to spend our time on. And so I think
what we're going to see with codecs, and I think that this again, I think is representative
of the kind of AI we're building is we're going to find that the kind of like the hard, you know,
the drug work, the part that is like you need to know the whole encyclopedia of your field that,
you know, just like even coming with an idea of where to start, like those problems that I think
are real barriers to people getting started, those are going to start really melting away.
And then that will free up people to actually work on the exciting stuff.
Copyright is the the next one I know that, you know, the big issue here is that there are no
answers, and the system hasn't quite figured it out yet. But I'm wondering what your quick take
is on that. Yep. So, you know, I think that, you know, our position is definitely that, you know,
training training on, you know, publicly available code and and text is very use. But I think that
it's definitely the case that the technology here is running ahead of the law, right? I think that,
you know, that's something that I think is has happened many times in the past. And so I think
that it's time for a public conversation about this, like part of the reason that we're doing
a preview here, you know, that this is a API that will be available starting to roll out now,
is that we want that feedback. We want to start that conversation. And, you know, technologies
like Codex, you know, I think they have a lot of potential. I think we would, you know, be doing
it to service to ourselves if they weren't easy to build on the work, lots of people weren't able
to use them. So I'm very hopeful that we can figure out how do we get the good of these systems
and get lots of benefits. And, you know, just really help, help it, help it supercharge the economy
in a way that we think is, you know, doing the right thing for everyone.
And are there fairness bias types of issues that have come up through in the context of Codex?
For sure. Yeah, I think that fairness and bias are kind of a key part of AI. And I think that,
you know, one thing that, you know, first of all, I think that those issues themselves, I think,
you know, deserve a lot of space because, you know, we're building these systems that, you know,
that they are being trained on data that is generated by all of us, right? And that if you're,
if you're, you know, sort of not careful, you're going to lash onto the wrong things or help amplify
biases that exist in the system. So I think that this is always going to be an important thing.
And the stakes are just going to raise as we go. But I also want to point out that I think that
Codex also represents a bit of a raising of the stakes of the kinds of fallout that you can get from,
from a misbehaving system, right? You know, that if you generate some code with Codex and it does
decide to delete all your files, that's probably not something you want, right? So I think that we need
to figure out what values go into these systems and that, you know, we have some preliminary work on
this that I think we've, we've, you know, published, published a bit on already. But I think you
also need to think about how do you really align these, how do you technically align these systems
with whatever values should be in there? And I think that, you know, look like we've got some
technical problems ahead of us, but I think the question of, you know, both who are the people who
are actually building it and making sure that that that is diverse and representative enough,
I think is pretty, pretty critical. But also the question of, you know, how exactly are those values
chosen, you know, who makes that decision? I think one day that's going to be kind of the most
important problem that we as a community and, you know, we as a society are facing. And so I think
that, you know, it's never too soon to start really, really working hard on these problems.
Related issue is access and accessibility. And that's maybe a segue to kind of the rollout plan
for Codex. Yes. A little bit about that. Yeah. So we really want this technology to be out there
and used. We think you can deliver a lot of value. And we think that it's like a little taste of
a future to come. So that's really important to us. We're going to do the same kind of playbook
we did with the GPT-3, where we're going to have a private beta. We're going to roll it out as
quickly as we can safely. We're going to be scaling it up that the invites will start flowing on
Tuesday. So again, whenever we seize this podcast, that the first invites will all be out. And,
you know, honestly, we just want to learn, right, that we have a new technology here in the best
way to understand how it will impact the world, is by actually seeing it impact the world.
And our philosophy is very much try to get, you know, a broad slice of usage at smaller scale
and scale it up as you go. And there's very particular things that we did for GPT-3. You know,
we have an academic access program in order to make sure that the, you know, the academics are
able to get access. I think that, you know, for this, I think that there's going to be different
segments that are going to be excited about using it. You know, I think that people who are
programming, you know, students, I think are like one segment who we want to make sure that this
is accessible to. So we really want feedback. We really want to see how people are excited about
using our technology. And we are very excited about you using it. And honestly, we need,
need your help to understand it. Awesome. Awesome. And is there, was there something about a
competition that you're hosting for this? Yes. So Thursday, 10 AM. So I don't know what time
you're planning on releasing the podcast. But Thursday, 10 AM, we are going to have a new kind
of programming competition. So you will be able to use Codex as both your teammate and a competitor.
So everyone's going to get access to some number of queries to Codex while doing Python programming
challenges. And it should be very exciting. There will be a leaderboard for the whole internet
racing to solve these challenges. But really, the goal is to get a sense of what does it like to
work alongside Codex. And this is one way we can really accelerate access to everyone and give
them a chance to get a little taste of it. Awesome. Well, of course, we'll have pointers in the
show notes for this episode. But Greg, thanks so much for taking the time to give us what is
effectively a preview, a sneak peek, although it will be released by the time this shows public.
Like, great to have you on the show once again. Yep, great to be back. Thank you so much.
