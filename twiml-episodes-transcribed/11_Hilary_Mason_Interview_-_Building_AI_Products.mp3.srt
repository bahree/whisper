1
00:00:00,000 --> 00:00:16,080
Hello and welcome to another episode of Twimmel Talk, the podcast where I interview

2
00:00:16,080 --> 00:00:21,600
interesting people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,600 --> 00:00:27,320
I'm your host Sam Charrington.

4
00:00:27,320 --> 00:00:32,200
I want to start out by wishing everyone a very happy and very belated new year.

5
00:00:32,200 --> 00:00:36,280
I'm finding it really hard to believe just how quickly the last few weeks of last year

6
00:00:36,280 --> 00:00:39,000
and the first few weeks of this year flew by.

7
00:00:39,000 --> 00:00:44,120
Needless to say, I'm super pumped to bring you this new episode of the show.

8
00:00:44,120 --> 00:00:47,920
Before we get going, I've got a bit of a holiday gift for some of you.

9
00:00:47,920 --> 00:00:52,400
That's right, over the last few weeks I've received a few requests from listeners who've

10
00:00:52,400 --> 00:00:56,400
wanted to listen to the podcast on their favorite home assistance.

11
00:00:56,400 --> 00:01:02,040
Well, it's taken a bit of doing, but I'm happy to report that the podcast is now available

12
00:01:02,040 --> 00:01:05,760
on both Amazon Alexa and Google Home.

13
00:01:05,760 --> 00:01:06,760
Check this out.

14
00:01:06,760 --> 00:01:12,680
Alexa, play the podcast this week in machine learning.

15
00:01:12,680 --> 00:01:16,840
You'd like to play the program called this week in machine learning, right?

16
00:01:16,840 --> 00:01:19,160
Yes.

17
00:01:19,160 --> 00:01:23,160
This week in machine learning in AI, getting the latest episode.

18
00:01:23,160 --> 00:01:36,400
Where it is from tune in, Hey, Google, play the podcast this week in machine learning.

19
00:01:36,400 --> 00:01:41,320
Learning in AI podcast, twimmel talk number 10 Francisco Weber statistics versus semantics

20
00:01:41,320 --> 00:01:47,240
for natural language processing.

21
00:01:47,240 --> 00:01:52,080
Note that for whatever reason, Alexa doesn't like when you ask for the podcast using its

22
00:01:52,080 --> 00:01:55,720
full name this week in machine learning and AI.

23
00:01:55,720 --> 00:02:00,680
But this week in machine learning works fine on Google either works.

24
00:02:00,680 --> 00:02:05,320
If you have any problems, just repeat the commands that I used in the demo.

25
00:02:05,320 --> 00:02:11,160
Now, I like to think that at least some of you are listening at home on your phone speakers

26
00:02:11,160 --> 00:02:15,400
and I've just commanded your device to play the podcast if that's the case.

27
00:02:15,400 --> 00:02:16,400
Enjoy it.

28
00:02:16,400 --> 00:02:17,400
Nice.

29
00:02:17,400 --> 00:02:20,040
All right, moving along to our program.

30
00:02:20,040 --> 00:02:25,160
This time around, our guest is Hillary Mason, who I interviewed last year at the O'Reilly

31
00:02:25,160 --> 00:02:28,280
AI and strata conference in New York City.

32
00:02:28,280 --> 00:02:32,160
I don't know that she'd refer to herself this way, but Hillary was really one of the

33
00:02:32,160 --> 00:02:35,280
first quote unquote famous data scientists.

34
00:02:35,280 --> 00:02:39,840
I remember the first opportunity I had to hear her speak was back in 2011 at the strange

35
00:02:39,840 --> 00:02:42,120
loop conference in St. Louis.

36
00:02:42,120 --> 00:02:46,600
At the time, she was chief scientist for Bittley, the company that popularized short links

37
00:02:46,600 --> 00:02:47,600
on the web.

38
00:02:47,600 --> 00:02:52,200
One of the days she's running fast forward labs, which helps organizations accelerate their

39
00:02:52,200 --> 00:02:58,920
data science and machine intelligence capabilities through a variety of research and consulting offerings.

40
00:02:58,920 --> 00:03:02,920
I tracked Hillary down at the AI conference after hearing from an attendee that her talk

41
00:03:02,920 --> 00:03:08,840
on practical AI product development was their absolute favorite session.

42
00:03:08,840 --> 00:03:13,040
Hillary and I had a wonderful, although somewhat brief chat, that I'm sure you're going

43
00:03:13,040 --> 00:03:15,960
to enjoy and learn a lot from.

44
00:03:15,960 --> 00:03:22,520
Of course, you can find this week's show notes at twimmelai.com slash talk slash 11.

45
00:03:22,520 --> 00:03:24,200
And now on to the show.

46
00:03:24,200 --> 00:03:25,200
All right.

47
00:03:25,200 --> 00:03:26,200
Hey, everyone.

48
00:03:26,200 --> 00:03:29,040
I'm here with Hillary Mason of fast forward labs.

49
00:03:29,040 --> 00:03:34,840
And we're at day two of the O'Reilly AI conference, the first actual O'Reilly AI conference

50
00:03:34,840 --> 00:03:36,200
as we were just discussing.

51
00:03:36,200 --> 00:03:37,200
That's right.

52
00:03:37,200 --> 00:03:41,360
And Hillary gave a talk yesterday that I didn't get a chance to see, but I heard great things

53
00:03:41,360 --> 00:03:42,360
about it.

54
00:03:42,360 --> 00:03:43,360
Oh, thank you.

55
00:03:43,360 --> 00:03:47,600
Now we start by having you introduce yourself and then you can tell us what your talk was

56
00:03:47,600 --> 00:03:48,600
about.

57
00:03:48,600 --> 00:03:49,600
Sure.

58
00:03:49,600 --> 00:03:50,600
So I'm Hillary Mason.

59
00:03:50,600 --> 00:03:56,320
I'm the founder of an independent machine intelligence research company called fast forward

60
00:03:56,320 --> 00:03:57,320
labs.

61
00:03:57,320 --> 00:04:03,600
And we look into approaches and algorithms that are emerging in the machine learning AI

62
00:04:03,600 --> 00:04:06,920
space, but that are not yet widely understood.

63
00:04:06,920 --> 00:04:11,800
And we do our own independent research to make them useful to people.

64
00:04:11,800 --> 00:04:16,800
So we write reports that are a survey of the techniques from a technical perspective

65
00:04:16,800 --> 00:04:20,880
at a conceptual level, talking about where we think it's going to go, any ethical issues

66
00:04:20,880 --> 00:04:25,000
that might come up, do a survey of the commercial landscape.

67
00:04:25,000 --> 00:04:29,320
So what vendors are out there, what we think the interesting application opportunities are.

68
00:04:29,320 --> 00:04:32,120
We also build working prototypes of these things.

69
00:04:32,120 --> 00:04:37,200
And finally, we act as technical advisors to our clients, like their nerd friend and

70
00:04:37,200 --> 00:04:42,720
help them actually build their AI machine learning products more effectively.

71
00:04:42,720 --> 00:04:44,920
So yeah, that's what we do.

72
00:04:44,920 --> 00:04:46,160
Everyone needs a nerd friend.

73
00:04:46,160 --> 00:04:48,520
Yeah, I mean, we all have that friend.

74
00:04:48,520 --> 00:04:53,600
Even if you are a nerd, you have your nerd friend on your music nerd friend and your

75
00:04:53,600 --> 00:04:57,920
friend who's most likely sitting in front of a computer at 9 p.m., we all have those

76
00:04:57,920 --> 00:04:58,920
people.

77
00:04:58,920 --> 00:04:59,920
Right.

78
00:04:59,920 --> 00:05:00,920
Right.

79
00:05:00,920 --> 00:05:02,160
So your talk, what was the title of your talk?

80
00:05:02,160 --> 00:05:06,120
So my talk was practical AI product development.

81
00:05:06,120 --> 00:05:11,000
And what I was trying to accomplish with this talk was coming into this AI conference.

82
00:05:11,000 --> 00:05:17,280
There's a lot of hype and a lot of lack of clarity around what it means to actually build

83
00:05:17,280 --> 00:05:18,760
an AI product.

84
00:05:18,760 --> 00:05:20,600
What an AI product is.

85
00:05:20,600 --> 00:05:24,840
So what I was trying to talk through are some of the challenges we've seen going from

86
00:05:24,840 --> 00:05:29,520
the idea and the algorithm, going from the press release, if you want to say it that way

87
00:05:29,520 --> 00:05:31,200
to the product.

88
00:05:31,200 --> 00:05:36,440
So being able to say, we have a data set, we have a business problem, we understand, and

89
00:05:36,440 --> 00:05:40,160
we have some, you know, we're willing to invest in trying to make something, how do

90
00:05:40,160 --> 00:05:41,160
we actually do that?

91
00:05:41,160 --> 00:05:45,880
And how does it differ from data analytics and how does it differ from software product

92
00:05:45,880 --> 00:05:46,880
development?

93
00:05:46,880 --> 00:05:51,840
There's a lot of people today are trying to take a machine learning product and sort

94
00:05:51,840 --> 00:05:56,080
of put it into the software development framework, and they tend to run into a few common

95
00:05:56,080 --> 00:05:58,040
friction points when they do that.

96
00:05:58,040 --> 00:06:02,280
Okay, and so I suppose those friction points were the body of your talk?

97
00:06:02,280 --> 00:06:08,840
Yes, so things like how agile software development is really optimized for building a product

98
00:06:08,840 --> 00:06:16,560
with commodity technology, but that isn't how you build a data product because you have

99
00:06:16,560 --> 00:06:24,040
to understand that maybe, even if it's a good idea, sometimes the algorithm you've chosen

100
00:06:24,040 --> 00:06:30,280
won't work, you have to make sure that the accuracy of the system is within sufficient

101
00:06:30,280 --> 00:06:31,280
bounds.

102
00:06:31,280 --> 00:06:36,200
There's a lot of work to do around how you productionize and operationalize these things,

103
00:06:36,200 --> 00:06:40,400
how you monitor, not just that the server is up, but that the model continues to return

104
00:06:40,400 --> 00:06:44,920
high quality results over time as the context and the data changes.

105
00:06:44,920 --> 00:06:50,120
And so all of these details are something that you really have to learn right now by doing

106
00:06:50,120 --> 00:06:55,440
it, and we have yet to really standardize on a common accepted practice.

107
00:06:55,440 --> 00:07:00,360
And so in my talk, I was sharing what we've learned and what we do, and then hoping to

108
00:07:00,360 --> 00:07:06,160
have conversations with people around what they do and what they're trying to do.

109
00:07:06,160 --> 00:07:08,640
And so yes, that's what the talk was.

110
00:07:08,640 --> 00:07:15,600
It seems like agile would be perfect for this environment where things like working closely

111
00:07:15,600 --> 00:07:23,800
with your customer, the end user of your product, failing fast, at least the things we commonly

112
00:07:23,800 --> 00:07:24,800
think about agile.

113
00:07:24,800 --> 00:07:28,800
There's also a whole software development lifecycle thing, which may be what you're referring

114
00:07:28,800 --> 00:07:29,800
to.

115
00:07:29,800 --> 00:07:30,800
Right.

116
00:07:30,800 --> 00:07:33,800
So on the surface, it's absolutely a compatible philosophy.

117
00:07:33,800 --> 00:07:35,800
Which is why everyone falls into the trap.

118
00:07:35,800 --> 00:07:36,800
Exactly.

119
00:07:36,800 --> 00:07:40,640
Because when you go to implement the details is when you run into the problem, when you

120
00:07:40,640 --> 00:07:46,000
have to say, you know, how long do I, how many points will do I think it's going to take

121
00:07:46,000 --> 00:07:50,280
me to find an algorithm that can produce a useful result?

122
00:07:50,280 --> 00:07:58,200
And it doesn't take into account the machine learning process of developing, really experimenting

123
00:07:58,200 --> 00:08:02,840
and saying that, you know, I might try to solve problem A, but it turns out problem A is

124
00:08:02,840 --> 00:08:06,520
really a lot harder than I thought it would be, but I can solve problem B that's also

125
00:08:06,520 --> 00:08:12,000
useful in this product context, and it also doesn't deal with the, once you have something

126
00:08:12,000 --> 00:08:18,760
that sort of works, doing the simplification and scalability work, which is just as hard

127
00:08:18,760 --> 00:08:24,960
as the initial algorithmic work, but often gets overlooked in a, you know, AI conference

128
00:08:24,960 --> 00:08:27,520
where everyone's excited about what's shining in me.

129
00:08:27,520 --> 00:08:28,520
Okay.

130
00:08:28,520 --> 00:08:35,520
So to me, that says that it's not that agile methodologies are fundamentally, you know,

131
00:08:35,520 --> 00:08:43,280
placed in these types of problems, it's that our sensibilities for estimating and, you

132
00:08:43,280 --> 00:08:49,320
know, understanding the development process that kind of feed into an agile methodology

133
00:08:49,320 --> 00:08:50,320
are off.

134
00:08:50,320 --> 00:08:51,320
Like we don't have.

135
00:08:51,320 --> 00:08:52,320
We don't have a philosophy.

136
00:08:52,320 --> 00:08:53,320
Right.

137
00:08:53,320 --> 00:08:54,320
They agree.

138
00:08:54,320 --> 00:08:58,400
But the, the mechanisms are second-class citizens.

139
00:08:58,400 --> 00:09:03,680
So you can allocate a spike time to go figure out an algorithm that that's a hack and there's

140
00:09:03,680 --> 00:09:07,920
no first-class mechanism for this sort of experimentation and iteration.

141
00:09:07,920 --> 00:09:08,920
Okay.

142
00:09:08,920 --> 00:09:09,920
Okay.

143
00:09:09,920 --> 00:09:13,520
And so how did you, with that being kind of the premise for your talk, what were some of

144
00:09:13,520 --> 00:09:16,720
the things that you, that you dove into?

145
00:09:16,720 --> 00:09:18,240
So I love to tell stories.

146
00:09:18,240 --> 00:09:22,040
So I talked about a couple of projects we've attempted that didn't work out.

147
00:09:22,040 --> 00:09:26,760
So one was using a deep learning image classifier to let you take a picture of your plate and

148
00:09:26,760 --> 00:09:31,600
get a calorie estimate, which, okay, that sounds, maybe that sounds like a good idea.

149
00:09:31,600 --> 00:09:34,480
My team, we thought it was a good idea, at least worth trying.

150
00:09:34,480 --> 00:09:36,040
We found a lot of data.

151
00:09:36,040 --> 00:09:39,000
There's a lot of food photography out there.

152
00:09:39,000 --> 00:09:43,400
And there's also a lot of data on, you know, a cheeseburger has this many calories.

153
00:09:43,400 --> 00:09:44,400
Right.

154
00:09:44,400 --> 00:09:50,600
It did not work because that data, a cheeseburger can have anywhere from 300 to 2,400 calories

155
00:09:50,600 --> 00:09:54,360
and these data sets just simply don't agree.

156
00:09:54,360 --> 00:09:58,240
And we did, you know, first we're like, okay, we want the actual calorie count from the

157
00:09:58,240 --> 00:10:02,000
plate and then we decided on a more modest problem, which was, can you tell us if it's

158
00:10:02,000 --> 00:10:05,480
very healthy, healthy or not at all healthy?

159
00:10:05,480 --> 00:10:11,480
And eventually we decided that it was no longer worth the time and investment because

160
00:10:11,480 --> 00:10:14,920
the quality of result we could get was not actually useful.

161
00:10:14,920 --> 00:10:18,760
And of course, this is a fun story to tell because a couple months after we did this whole

162
00:10:18,760 --> 00:10:23,520
process, Google announced that they had in fact solved this problem.

163
00:10:23,520 --> 00:10:27,920
And you know, to me, that sort of validates that it was a good idea, but we didn't have

164
00:10:27,920 --> 00:10:30,120
the resources to make it work.

165
00:10:30,120 --> 00:10:34,680
So I talked about that story also went into depth on a brief, which is an extractive

166
00:10:34,680 --> 00:10:40,160
summarization prototype we built using neural networks for articles.

167
00:10:40,160 --> 00:10:45,280
So being able to take an article and pull sentences out of that article that are an effective

168
00:10:45,280 --> 00:10:48,760
summary of the entire content of the article.

169
00:10:48,760 --> 00:10:52,640
And that's something where there's a product design piece and there's an algorithm design

170
00:10:52,640 --> 00:10:59,160
piece and they have to work together well in order to make a usable, useful, fun prototype.

171
00:10:59,160 --> 00:11:02,200
And so I went through that whole example in the talk.

172
00:11:02,200 --> 00:11:04,480
Can you talk about that one in a little bit more detail?

173
00:11:04,480 --> 00:11:07,480
How did you go about that and what was the process like?

174
00:11:07,480 --> 00:11:11,560
Yeah, so the work we do is always framed around an application.

175
00:11:11,560 --> 00:11:16,360
And so as much fun as it might be to say, like, okay, we want to spend four months using

176
00:11:16,360 --> 00:11:20,560
deep learning to analyze text, which is really what we did.

177
00:11:20,560 --> 00:11:25,720
We decided to focus first on summarization and then under summarization or sort of two

178
00:11:25,720 --> 00:11:28,920
major schools of system one is extractive.

179
00:11:28,920 --> 00:11:33,840
So pulling words and sentences out of the body of text.

180
00:11:33,840 --> 00:11:37,880
And there's abstractive, which is constructing a summary that may contain language that does

181
00:11:37,880 --> 00:11:41,680
not appear in the underlying text that's new language.

182
00:11:41,680 --> 00:11:46,120
We focused on extractive because again, in the product context, we could actually build

183
00:11:46,120 --> 00:11:49,120
something with a high enough quality result to be useful.

184
00:11:49,120 --> 00:11:53,640
Whereas on the abstractive side, we're still as a community very early.

185
00:11:53,640 --> 00:11:55,920
And so the results are kind of variable.

186
00:11:55,920 --> 00:11:57,320
So again, there was that focus.

187
00:11:57,320 --> 00:12:01,120
And then within that, we looked at a couple formulations of the problem.

188
00:12:01,120 --> 00:12:06,120
So one is, can I take any article and extract those sentences?

189
00:12:06,120 --> 00:12:08,680
And that's the system we ended up building.

190
00:12:08,680 --> 00:12:15,720
It's trained on about 18,000 human authored summaries with quotations of news articles.

191
00:12:15,720 --> 00:12:18,640
And it works very effectively on those.

192
00:12:18,640 --> 00:12:23,280
We also did a second formulation of the problem around multi-document summarizations.

193
00:12:23,280 --> 00:12:29,080
So if you have 5,000 documents on the same topic, can you cluster them effectively and

194
00:12:29,080 --> 00:12:30,560
then summarize each cluster?

195
00:12:30,560 --> 00:12:33,920
And for that, we used LDA for that first step.

196
00:12:33,920 --> 00:12:38,200
And actually, my colleague Mike Williams will be at strata tomorrow talking about all of

197
00:12:38,200 --> 00:12:41,360
the technical fun stuff underneath it.

198
00:12:41,360 --> 00:12:43,320
Yeah, if you're interested in that.

199
00:12:43,320 --> 00:12:50,640
OK, and so for that example, the data set that you use, was that a public data set?

200
00:12:50,640 --> 00:12:55,400
Yes, it's from a website called The Browser, which is the terrible website name.

201
00:12:55,400 --> 00:12:57,080
Because of the ambiguity there.

202
00:12:57,080 --> 00:13:00,240
But yeah, so it's a public data set.

203
00:13:00,240 --> 00:13:02,680
And one that turned out to be quite effective.

204
00:13:02,680 --> 00:13:03,440
Oh, interesting.

205
00:13:03,440 --> 00:13:06,680
And LDA, latent, dearest lay allocation.

206
00:13:06,680 --> 00:13:07,440
Absolutely.

207
00:13:07,440 --> 00:13:09,800
And how does that, I've heard that come up a few times.

208
00:13:09,800 --> 00:13:11,040
I don't really know how it works.

209
00:13:11,040 --> 00:13:12,520
What's the 30,000 foot on that?

210
00:13:12,520 --> 00:13:20,120
OK, so the quick conceptual overview is that it's a non-supervised or unsupervised algorithm,

211
00:13:20,120 --> 00:13:26,320
meaning you take the stream of text, and it is able to infer related clusters in the

212
00:13:26,320 --> 00:13:28,720
text fairly effectively.

213
00:13:28,720 --> 00:13:32,760
One of the limitations is that you have to tell it how many clusters to look for, which

214
00:13:32,760 --> 00:13:38,000
you may or may not have an intuition for going into an analysis.

215
00:13:38,000 --> 00:13:42,480
Which again, means that practically the way people handle that on any given body of text

216
00:13:42,480 --> 00:13:48,480
is to sort of try 10 clusters, 100 clusters, and then narrow their way intuitively.

217
00:13:48,480 --> 00:13:52,720
And by clusters, are we talking like N-grams, or are we talking conceptual clusters?

218
00:13:52,720 --> 00:13:56,120
We're talking groups of documents in this particular case.

219
00:13:56,120 --> 00:13:59,440
So we applied it to Amazon product reviews.

220
00:13:59,440 --> 00:14:04,640
And we found particularly great results in the pet product review category, because this

221
00:14:04,640 --> 00:14:13,240
is a section where people are quite passionate about the items they, it's no surprise, I guess.

222
00:14:13,240 --> 00:14:19,560
But we were, you know, a couple of examples we ran into were things like a dog toy that,

223
00:14:19,560 --> 00:14:24,360
you know, 90% of the reviews were five-star and 10% were one-star.

224
00:14:24,360 --> 00:14:28,680
And so when you look at the clusters of those reviews, you see that, you know, most of

225
00:14:28,680 --> 00:14:32,200
them are things like, this is cheap, I can buy it in Amazon, it's great.

226
00:14:32,200 --> 00:14:36,520
This is really good for my dog's emotional well-being, and yes, people are very concerned

227
00:14:36,520 --> 00:14:41,480
with their dog's emotional well-being, and then the 10% were sort of like, yeah, my dog

228
00:14:41,480 --> 00:14:46,240
ate part of this and had to have a $4,000 surgery.

229
00:14:46,240 --> 00:14:52,280
And so that's the kind of structure you're able to pull out with LDA.

230
00:14:52,280 --> 00:14:58,840
And the utility there, I think, is fairly obvious, or rather, one of the things I mentioned

231
00:14:58,840 --> 00:15:03,600
in the talk is that we tend to see these algorithms applied to making things we already do more

232
00:15:03,600 --> 00:15:04,880
efficient.

233
00:15:04,880 --> 00:15:10,560
So if you can make that 20-page article down to two pages, that's making me more efficient.

234
00:15:10,560 --> 00:15:15,880
But if you can make me able to read 5,000 documents, which I could not possibly, I could

235
00:15:15,880 --> 00:15:21,360
not possibly ever stand to read 5,000 reviews of the same dog toy, I can tell you that.

236
00:15:21,360 --> 00:15:24,480
But now I can get a similar amount of value.

237
00:15:24,480 --> 00:15:29,040
That's sort of a really useful AI product.

238
00:15:29,040 --> 00:15:34,400
When you say a similar amount of value, what was your optimization function?

239
00:15:34,400 --> 00:15:37,920
How did you measure whether the value was similar?

240
00:15:37,920 --> 00:15:39,880
So that's a really good question.

241
00:15:39,880 --> 00:15:47,080
And in the case of our brief prototype, we had some human curated test data.

242
00:15:47,080 --> 00:15:53,200
But to be honest, a lot of this is really intuition, which I know is a dirty word in this

243
00:15:53,200 --> 00:15:55,160
context, the world of AI.

244
00:15:55,160 --> 00:16:02,440
But I really do believe in the value of user testing feedback loops and human intuition

245
00:16:02,440 --> 00:16:07,800
and guiding the product aspects of this sort of work.

246
00:16:07,800 --> 00:16:14,320
So what were the, did you have kind of an enumerated list of takeaways from the talk?

247
00:16:14,320 --> 00:16:16,640
Was it prescriptive or was it?

248
00:16:16,640 --> 00:16:22,480
So it was more laying out a shared vocabulary and then sharing some experiences, but I'm

249
00:16:22,480 --> 00:16:29,640
not going to presume to tell you how it's done, because I think that where we are in the

250
00:16:29,640 --> 00:16:36,360
development of the practice of AI product building is still very early.

251
00:16:36,360 --> 00:16:42,480
And this is, you know, I've been a data scientist since the very beginning, and it's very similar

252
00:16:42,480 --> 00:16:47,320
to what happened with the evolution of the profession of data science, where a lot of

253
00:16:47,320 --> 00:16:52,920
people are doing a lot of different interesting things that are all related.

254
00:16:52,920 --> 00:16:58,160
But there's no one vocabulary, no one process that everyone has agreed on yet.

255
00:16:58,160 --> 00:16:59,960
And so I shared my point of view.

256
00:16:59,960 --> 00:17:05,080
I got to talk to people afterwards for an hour and a half out here hearing other people's

257
00:17:05,080 --> 00:17:06,080
point of view.

258
00:17:06,080 --> 00:17:10,520
And it's just, we're at one of those really exciting moments, I think.

259
00:17:10,520 --> 00:17:11,520
Yeah.

260
00:17:11,520 --> 00:17:12,520
Yeah.

261
00:17:12,520 --> 00:17:16,440
Have you done, have you set in on anything else at the event?

262
00:17:16,440 --> 00:17:20,680
Like what else do you think is cool and interesting kind of in this realm?

263
00:17:20,680 --> 00:17:26,080
So at this particular conference, one thing I'm really impressed by is the different perspectives

264
00:17:26,080 --> 00:17:27,080
in the room.

265
00:17:27,080 --> 00:17:32,680
So most of the conferences I've been to are either technical or sort of business or sort

266
00:17:32,680 --> 00:17:36,040
of product design.

267
00:17:36,040 --> 00:17:41,120
Here we have everyone in the same room, which is great, you know, VCs, business folks start

268
00:17:41,120 --> 00:17:47,640
up people, big company people, you know, software developers and machine learning professors

269
00:17:47,640 --> 00:17:48,640
all here.

270
00:17:48,640 --> 00:17:50,840
So that's really cool.

271
00:17:50,840 --> 00:17:55,520
I've heard a couple of, you know, I always love the opening keynotes.

272
00:17:55,520 --> 00:17:56,920
They were pretty great.

273
00:17:56,920 --> 00:18:01,240
And then there have been talks on everything from, you know, TensorFlow for mobile poets,

274
00:18:01,240 --> 00:18:02,840
which is Pete Wardens talk.

275
00:18:02,840 --> 00:18:07,520
And he is a great blog post if you haven't seen it all the way over to the future of natural

276
00:18:07,520 --> 00:18:11,200
language generation from the folks that automated insights.

277
00:18:11,200 --> 00:18:14,040
So it's, you know, just a few of the things I've been enjoying.

278
00:18:14,040 --> 00:18:15,040
Yeah.

279
00:18:15,040 --> 00:18:16,040
Nice.

280
00:18:16,040 --> 00:18:17,040
Nice.

281
00:18:17,040 --> 00:18:18,640
So how long have you been doing fast forward labs?

282
00:18:18,640 --> 00:18:22,840
So fast forward labs is going to be two and a half years old soon.

283
00:18:22,840 --> 00:18:24,160
Okay.

284
00:18:24,160 --> 00:18:28,080
And we are eight people plus two interns based in Brooklyn.

285
00:18:28,080 --> 00:18:29,080
Oh, nice.

286
00:18:29,080 --> 00:18:34,440
We are actually moving our office this week over to Atlantic Ave Barclays Center.

287
00:18:34,440 --> 00:18:35,440
Oh, cool.

288
00:18:35,440 --> 00:18:39,800
So yeah, you were any of your audience should let us know and come stop by if you're in

289
00:18:39,800 --> 00:18:40,800
the new your head.

290
00:18:40,800 --> 00:18:41,800
Nice.

291
00:18:41,800 --> 00:18:42,800
Nice.

292
00:18:42,800 --> 00:18:43,800
Awesome.

293
00:18:43,800 --> 00:18:44,800
Well, I appreciate you taking the time.

294
00:18:44,800 --> 00:18:45,800
I know you've got a meeting and run off too.

295
00:18:45,800 --> 00:18:46,800
Well, thank you so much.

296
00:18:46,800 --> 00:18:48,640
It's great to get an overview of your talk.

297
00:18:48,640 --> 00:18:49,640
Oh, yes.

298
00:18:49,640 --> 00:18:50,640
Great to have this conversation.

299
00:18:50,640 --> 00:18:51,640
Thank you.

300
00:18:51,640 --> 00:18:52,640
All right.

301
00:18:52,640 --> 00:18:53,640
Thanks a lot.

302
00:18:53,640 --> 00:18:54,640
All right, everyone.

303
00:18:54,640 --> 00:18:56,280
That's it for today's show.

304
00:18:56,280 --> 00:18:58,160
A quick note for you guys tomorrow.

305
00:18:58,160 --> 00:19:02,640
I'm off to reworks deep learning summit in San Francisco.

306
00:19:02,640 --> 00:19:08,040
If any twimmel listeners are attending or will be in the area, please reach out to me.

307
00:19:08,040 --> 00:19:11,240
I would love, love, love to connect up with you.

308
00:19:11,240 --> 00:19:17,720
Also, please do leave a comment on the show notes page at twimmelai.com slash talk slash

309
00:19:17,720 --> 00:19:25,560
11 or tweet to me at at Sam Charrington or at twimmelai to discuss this show and let

310
00:19:25,560 --> 00:19:27,160
me know how you liked it.

311
00:19:27,160 --> 00:19:28,920
Thanks so much for listening.

312
00:19:28,920 --> 00:19:35,920
Catch you next time.

