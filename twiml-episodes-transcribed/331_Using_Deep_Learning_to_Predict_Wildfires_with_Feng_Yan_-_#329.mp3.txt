Welcome to the Tumel AI Podcast.
I'm your host Sam Charrington.
Hey, what's up everyone?
I recently attended the AWS re-invent conference in Las Vegas, and I'm excited to share
a few of my many interesting conversations from that event here on the podcast this week.
Before we dive in, I'd like to thank our friends at Capital One for sponsoring our re-invent
series.
Capital One has been a huge friend and supporter of this podcast for some time now, and
I'm looking forward to sharing my interview with Dave Castillo, Capital One's managing
VP of Machine Learning with you on Thursday.
Dave and I discussed the unique approach being taken at the company's Center for Machine
Learning, as well as some of the interesting AI use cases being developed at the bank,
and the platform they're building to support their ML and AI efforts.
To learn more about Capital One's Machine Learning and AI efforts and research, visit capitalone.com-slash-tech-slash-explore.
And now on to the show.
All right, everyone.
I am here at AWS re-invent in Las Vegas, and I am with Fung Yen.
Fung is an assistant professor at the University of Nevada, Reno.
Fung, welcome to the Tumel AI podcast.
Thank you.
So you just did a talk here on one of your research projects, which is focused on,
or it's called alert wildfire, and it's focused on detecting wildfire smoke using machine
learning.
We're going to dive deep into that, but before we do, how did you get started working on
this problem?
What's your background, and how did your interest align with this particular problem?
So my background actually, you know, I see machine learning and the computer system,
so I thought it was going to cross this plane between these two areas.
So actually, when I'm doing my PhD, I actually have like a more computer system background,
so we kind of build a, you know, efficient systems for data centers for, you know, cloud.
And then, you know, I had an internship in Microsoft research, and that's where I actually
started working on machine learning.
So I still remember like back then, you know, we had like machine learning system.
At that time, we didn't have tens of loads, and we had a system called Adam.
So that's actually, you know, the state of the ad system back then, and you know, I'm
extremely lucky to have the opportunity to work with the group to work on this system.
So that's where I started actually working on the machine learning, and it's actually
back in 2014.
So it's actually, you know, before, you know, even the AI machine learning, you know,
becomes really hot and hot.
It's current wave.
Right.
Before I treat the wave.
Yeah.
So, and then later I joined the University of Nevada Reno, and become faculty there.
One like a special thing, right?
Because I used to be in the East Coast after I moved to the West Coast, we do say, right?
Like one specific, like, situation here is about the White Files.
They actually, Nevada and California, they have a lot of White Files, which, you know,
significantly impacts the air quality and the people's life, because, you know, the smoke
contains a lot of particles that's bad for your hairs.
And also, it will, you know, create these, like, low visibility situation that's, you
know, dangerous for a lot of things.
And it also creates this dry condition, and which will lead to even more White Files.
Okay.
So that's why, you know, I started to get kind of curious about, right?
How, whether there's any efforts has been made into kind of monitoring, control, you
know, the White Files and also the White Files smoke and air quality.
So, and, like, very likely, like, Yonger has a team, a lead by Dr. Ken Graham Ken, and
Ken Smith's daily seismology lab there, and they actually had an effort to build like
a camera system to help, you know, to monitor the White Files.
However, current stage, they are basically, you know, doing things manually.
So from, like, computer science, you know, perspective, I'm thinking, right?
So with my background in machine learning and the cloud computing, I'm wondering, with
her, you know, I can help them to build, like, a more intelligent system.
So that's, you know, where are we actually started to think about this project.
And later, we worked together and actually got, like, an SF and AWS cost months or the
big data, you know, grant to work on this project.
So, you know, that's how I actually get into, you know, this topic.
And so, yeah, and that's very fortunate that you walked into this environment where they've
already got these cameras deployed to try to solve this problem, but no automation system
in place.
Right, right.
How many cameras have been put in place to monitor the, the wildfire situation?
Right now, we have, like, more than 150 cameras already deployed, and then there are hundreds
more, you know, it's going to be deployed as soon.
This is the camera system that's specifically built for monitoring wildfire.
And the technology is way developed to actually, you know, use machine learning, age computing,
cloud computing to help, you know, the monitoring accurate can be extended to, you know, the
general camera system, we then have to be constrained by, like, a specific, you know,
network, because right now, there's a lot of camera system, right, the use for traffic
and that's the purpose.
They're already being there, right.
That's a technology can actually can be extended to other systems if needed.
Okay.
And how are the cameras deployed?
Are they in a grid, like, in forested areas, or are they in more populated areas, but,
you know, facing edges of forested areas?
What's the rationale or methodology behind the camera system?
The camera systems, they basically based on, you know, the areas that are more like prone
to fires.
Okay.
Right.
And of course, the specific actually technology about the camera itself is actually, you
know, lead by, you know, it's a group of the system, only your lab.
So we are more kind of on the software side.
And so when I think about, you know, monitoring for wildfires and images, one of the immediate
thoughts is, like, satellite imagery.
Oh, yes.
That's a very good question.
Conventionally, you know, people are using satellite imaging, as well as mediaology to,
you know, to do the wide-file smoke for casting and also the air quality prediction for
casting.
Okay.
So the limitation of this conventional technique is the resolution.
So actually, wide-file smoke, they can transfer accurate very fast.
Like we had like a video, you know, this morning to show to the audience that, you know,
for example, for like a major area of arena, like the whole major area has close to 1 million
people.
So it's not as big as Vegas, but still quite a big area.
So actually, in a windy day, the smoke actually can spread over the entire city in just 22-30
minutes.
So, yeah.
So for the conventional methodology, their data is very coarse-grand, for example, right?
In terms of both time dimension and also the spatial dimension.
For example, for the time dimension, actually, the data, right, no matter satellite image,
data or media origin data, they refresh like every a few hours or even feel, you know,
like at the day's level, right?
Right.
Because you've got to wait for a satellite to be overhead.
Right.
Right.
So, and another thing about the spatial dimension, right, they basically have like, for example,
the typical resolution is like 10 by 10 kilometers.
So this is like pretty big, right, in terms of, you know, the forecasting.
And think about, right, if we do want to like accurate forecasting, for example, for
an area of a city or like a neighborhood, right?
So we do need a much finer grand way to track the smoke and also to the air quality prediction.
So that's why, right, we are thinking about, you know, using the camera data, right?
Because camera data is very fragrant, right?
You can, you know, capture the image like a dozen of them per second, right?
And you can basically have infinite resolution as well as you have enough cameras, right?
We thought to use like a hybrid approach to combine both the conventional data, like satellite
imaging and as well as the media-oriented data together with this camera data.
So that, you know, we can have like a much finer grand data.
And another important thing about the traditional approach is, for example, for the satellite
data, since it's shooting from the app, right?
Actually like cloud and other situations can block the view, right?
So that you will have lots of missing data.
And while camera is actually on the ground, so it basically have like, you know, like a
better view, right?
But of course, problem with camera data is, right?
They only have local, you know, views, they didn't have like a very good global, you
know, view.
So that's why it's kind of complementary to each other, right?
That's why we need both of them.
Your collaborators that deployed the camera system, did you say they're with the Department
of SizeMology?
Like, are they originally trying to detect earthquakes with the cameras or?
Actually not, like, they, you know, I think they just had this idea, right?
Because, you know, in Reno, we have a lot of, you know, wide file and smoke like situation.
Like, by right now, they sort of just deliver the camera data to the, for example, the file
fighters, right?
They can manually check whether this area has a fire, and if so, they can use it to monitor
it.
But it doesn't have any kind of intelligent way to automatically alert when there's a
fire or smoke, and it doesn't actually track, you know, it's a fire or smoke, right?
Automatically.
So you've got this data coming off of these cameras, 150 today, sounds like 10 frames
a second.
You were saying?
10 frames a minute.
I remember 10.
Right.
This is actually a parameter you can control, right?
Sure.
You can, you know, make it like one frame per second, or even one frame per minute, right?
But you can also add to like maybe 30 frames per second, right?
Yeah.
So that's why, you know, you have a lot of data actually.
It's significantly getting more.
Exactly.
Yeah.
The scale is so much bigger than conventional satellite data or the media-oriented data.
And so that's actually our focus, right?
So think about in practice, right?
Even though you have these cameras really, right?
You can, you know, have a lot of data coming from the sensors, and then you also have
the models, right, for conventional statistical models, as well as the new kind of machine
or any different models, right?
But between the data and the model, right, you actually need to collect and deliver the
data to the model, right, in a timely way, right?
Yeah.
So that's why, you know, one of our efforts is how can we do some pre-processing at the
age side, and then, you know, to make the communication or transferring the data more
efficiently to the cloud.
So that's why we have this age and the cloud working together to actually deliver the
data in a timely way.
And another important thing is when you have this massive amount of data, right?
You do need to find a efficient way to process it, right?
Because if your prediction detection is too slow, then basically if the file or the
smoke is already passed by region, right?
Right.
Then it doesn't matter, right?
So you have to do this, you know, fast enough to make it proactive, right?
So that's why, you know, we, a lot of effort also being spent on how can we do like since
in like low latency, give good scalability, and as well as more efficient, right?
Because whenever you talk about this massive amount of data, and we're complicated
machining the algorithms, right?
So they basically consume a lot of computing resources, right?
A lot of energy, a lot of computing resource, that's actually at this scale, that's matter,
right?
So that's why, you know, we consider both latency, scalability, and efficiency into consideration
for designing our system.
This may be dig into the model itself, and the process you took to develop the model.
Sounds like the inputs are these images, or there are other features that you're feeding
into the model, and what is the model ultimately trying to do?
Yeah, so yeah, this is just between us.
So actually, you know, since this project starts from the beginning of this year, so we
are still at quite early stage.
So some of our efforts in the, you know, models and the training pathway is still ongoing.
So that way, it's, you know, not yet to show, because some of them are submitted for
publication.
Some of them are still, you know, you know, okay.
So actually, to talk this morning, of course, we showed some preliminary results about how
can we use machine learning models to actually detect the smoke, and I can share a little
bit about that.
Okay.
And then actually another thing we have already accomplished at this stage is, after you
have the model, right, you do need to do the inference or classification or detection,
right?
So, so this is also like a big thing, right?
You need to do this in like a real time, right?
Yeah.
I can't talk a little bit more about this.
Okay.
Well, okay.
So maybe let's start with the results that you presented this morning.
Okay.
Maybe a good place to start there is the, how are you formulating the problem that you're
trying to solve?
Is it a classification problem that you're trying to express it as smoke or not smoke?
Or are you trying to determine how much smoke there is?
Are you trying to determine the, you know, predict the future likelihood of smoke?
How do you structure the problem?
Okay.
So, for the problem, first of all, right, for the processing the camera data is actually
our focus of this project.
Okay.
So, of course, we do have a lot of fancy machine learning models right now, right?
However, you know, these are more kind of towards like this benchmarking, right?
So the data is more or less defined, right?
So one big challenge of detecting smoke is think about it.
The smoke is actually has like a various ship, right?
I would say, right, in this world, maybe no two smoke looks the same.
Right.
Right.
So, in other words, like you've got all these, you know, we've got, you know, these models
like ResNet and other things that are, you know, trained on things like ImageNet where
you have these very well-defined objects in the scene.
And I guess they also have kind of technical parameters like, you know, they're 224, you
know, dimension images.
And you've got this stuff coming off of a camera, you know, so real images, you know, of
things that for, you know, most of the time you see them in a picture aren't even really
there.
Right.
Right.
Exactly.
So, actually, there are two challenges, right?
One is about the environment, right?
Thinking about in a real situation, you have different light conditions, right?
And then you also have a various background, right?
For example, in like a mountain area, right, when there's cloud, right?
And basically, the cloud and the smoke can look very identical, right?
Yeah.
Even, you know, humans are difficult to tell the difference, right?
And also, the smoke itself, right?
First of all, it has various shifts, and the second is actually changing, right?
The shift is changing over time, right?
So that's, you know, the two challenges when you, you know, want to classify this, right?
And so what has been your approach for overcoming those challenges?
Yeah.
So, yeah, actually, you know, we are at this stage, right?
We tried some, you know, standard kind of models and we did transfer learning.
And for some, like, either, like, things we actually can, you know, do already very well,
you know, with existing techniques, better for some more kind of challenge, right?
Background, or some, like, faster changing, you know, smoke, so we are still trying to
find a way to solve it.
It's still like ongoing project.
Okay.
Okay.
It sounds like at this stage, the model formulation or the problem formulation would probably
be something on the simpler side, like, just a classifier, smoke or no smoke.
Yeah.
So, for smoke and no smoke, right?
We actually have already done some, you know, preliminary testing, and it's actually
can work very well, except some, like, really challenging, you know, since, yeah.
And our eventual goal is, right?
So we can actually not only identify with smoke or no smoke, but also we can identify
the density of the smoke, right?
Okay.
So, and then use that to map to, like, air quality, you know, measure.
So that we can directly tell from the image about the air quality, basically.
Okay.
Yeah.
And another big challenge when you want to do the machine learning in this classification
task is you don't have the label data, right?
So, so currently, we are, you know, still trying to use some supervised ways to train the
model and to do the classification, but eventually, we do want to, you know, utilize, you
know, semi-supervised learning and on-supervised learning, so that way, you know, use the camera
data directly, right?
So data to help building models and improve the models.
So you've got some preliminary models that you've developed that can start to differentiate
between smoke and non-smoke and you're continuing to work in this area so that you can do things
like identify density patterns and things like that.
But the main focus of the work that you've done thus far and if published on is, it sounds
like on the inference side and maybe some of the characteristics related to the edge nature
of the camera deployment, are you taking advantage of, actually, it is more of a question
than the statement?
Are you doing, like, inference at the edge?
Is that part of what you're, what you're trying to build towards?
Oh, yes.
So, for the edge pattern, it's still ongoing.
So what we have done so far is about how you can do efficient inference as a cloud
side.
On the cloud side.
Got it.
So on the cloud side, if you think about it, about a camera network system, when you
do this tracking and prediction, basically, there will be cameras joining the network and
leave the network because of smoke spreading to a new area as well as the network condition
is very unstable.
So basically, you have this kind of dynamic workload, because the number of cameras and
number of requests send it to the cloud for classification and detection is actually
changing over the time.
And especially, for example, when the smoke moving to a metro area, we do need to pay more
attention, which means we may need a final grand prediction.
Right?
Because the smoke can transport over to a neighborhood in a second level.
So that's why, usually, when the smoke is approaching to a metro area, you will have
suddenly a very high number of requests you need to process, because one thing is you
have a lot of cameras in metro areas.
The second is, it's very important to detect and to classify and detect things timely
in a metro area, because there are denser populations of people.
So in this case, when you have this kind of suddenly increased demand for inference,
then you do need to have a good way to scale in the cloud.
So we have a joint work with Hong Kong University of Science and Technology, where we did
like auto scaling work.
So right now, Amazon do have something called SageMaker that can automatically scale
your number of instances to a high level when it observes increased load.
However, this is a very coarse grand approach.
First of all, it's a feedback approach, so it observes and then acts, which means it
takes quite a while between it observes the high load and its actually scales.
Usually, this is at a few minutes' level right now.
However, if you want to do this real-time fine-grained monitoring of smoke, then basically you
need to do things at a second level.
So that's why we created a new system called Mark that can have much quicker scalability
than SageMaker.
So the key idea here is, right?
So SageMaker is a sort of feedback way of doing things, right?
It's observed when they interact, right?
And there's another way you can do it over provisioning, right?
So basically, I predict there might be like a high load, right?
And then I increase the resources in advance, right?
And of course, right?
In this way, you will have extra cost.
And another thing is you cannot always predict, right?
Correctly.
For example, if you look at the load curve, usually just like a stock market, right?
It's randomly and can surge very high sometimes and suddenly, you know, jobs, right?
So in this case, if you can not predict the load like accurately, right, then what can
you do, right?
So our solution is combining, you know, the infrastructure as a service together with
function as a service or, you know, the popular service provided by a call the providers.
And the idea here is, right?
We still would like to use infrastructure as a service because it's much cheaper in terms
of cost.
However, for function service, it uses the container, right?
And it can scale very fast at like a second level.
Right.
So for the normal load, right, we just use the infrastructure as a service, those virtual
machines to provide the main computing power.
And better when there is a surge of load, right?
Of course, we will do some predictions, right?
If the prediction is accurate, then we can do provisioning.
But if it fails, we will use the serverless instance as a transition period so that it can
immediately, you know, take the new load while you are starting new virtual machines.
And once the new virtual machines has been studied, basically you can, the serverless instance
can transfer the work back to the infrastructure service so that you will have like instant
scalability while also, you know, control the cost.
Thinking about this in the context of cloud, you know, this is maybe, I don't know if we
talk about this a whole lot nowadays, but we used to talk about this concept of cloud
bursting where you'd have like some normalized infrastructure capacity within your enterprise.
And then you have some burst of activity and you process that off in the cloud.
This is like function bursting or something like that where you're running everything
on instances in the cloud and then bursting into Lambda for access capacity.
Oh, yes.
That's the general idea.
Yeah, yeah, yeah.
It's very similar to the concept, but instead of using purely, you know, Lambda, for
example, right, we actually, you know, we actually do some experiment and find it, right?
If you are, everything is on Lambda, since, you know, machine learning is very expensive,
right?
You actually need to like much higher, you know, cost eventually.
So, so that's why, right?
So we, you know, had this idea to combine the infrastructure as a service together with
the function as a service to enjoy the benefit of both worlds, right?
Right.
Right.
Right.
So, the function is instant, you know, scalability.
Yeah, another thing, you know, we didn't use the function as a service at this stage is
they do have some limitations about, you know, the model size like the function size you
can actually have.
You only get certain amount of memory, right?
Right.
Certain amount of execution to that kind of thing.
Yeah.
And we actually have some effort is pushing on, you know, making, enabling the functions
as a service to support these larger models, but it's still, you know, on their going.
So I cannot show too much.
Okay.
That's the stage.
So what's interesting about this is it, to me, is that, you know, independent of the
domain that you're applying this to, yeah, as more folks are moving machine learning
workflows to the cloud, I guess, more and more important to figure out creative ways
to cost optimize, like, you know, building systems to move stuff over the spot instances
versus, you know, regular instances.
And this is another kind of way to arbitrage the cost difference between, you know, one
service, the functions versus the infrastructure.
Oh, yes.
So actually, the cost of place are very important role here, right?
Because think about this is that service, this is that continuous, you know, process, right?
It's just not just one time effort, right?
Right.
So that's why, right?
You know, if you think about the accumulated, right, the cost and the benefits is actually
it's quite significant.
Especially for these machine learning, right, models, they are quite kind of intensive,
right?
They consume a lot of resources.
And so have you been successfully doing inference on Lambda, or is it, are you using some
other functions capability?
Oh, yes.
We did actually being able to deprive some relatively smaller models on Lambda.
However, for some later models, right now, we asked there are, you know, have some ongoing
efforts to make it to be deprived on Lambda.
There are two aspects of what you're doing.
One is focused on how to achieve the scalability requirements.
And this is this idea of bursting the Lambda or the functions.
You also mentioned an element of this that's focused on latency.
Now, what's the driver?
There's just decreasing the amount of time it takes to get a prediction out to someone
who can act on it, or is there another consideration?
Oh, yes.
This is a very good question.
So about the latency part, actually, you know, it's very critical because all these, you
know, machine learning models, they do take a lot of time to process a request, right?
Think about, for example, an inception model, right?
They have hundreds or even thousands of operations, right?
So basically, each image you send, they need to go through all these operators, right?
So if you're doing the sequential way, of course, it would be like a lot of time, right?
It can be like seconds to even minutes, right?
So one way to accelerate the processing speed is through parallelism and also batching.
So for the parallelism part, so basically, almost all modern machine learning inference
or serving frameworks, they do provide these control knobs.
Basically you can have the request-level parallelism as well as operator-level parallelism.
For example, as a request-level basically means you can process multiple requests in parallel,
right?
This is like, you know, just like traditional servings, it's very easy to understand.
For the operation-level, basically, it means how many threads or how many cores will
be assigned to execute each operator.
So since operators, they do have dependency, right?
So it's quite complicated, you know, how you configure, you know, this parallelism, right?
Some people may say, I just said it to maximum, right?
If my CPU have 20 cores, why I just said, you know, 20, right?
However, this way, the experiment shows it actually doesn't, you know, work idea like
this because there are more threads, you have, you will have more contention for the results
because we know, you know, we have limited cache bandwidth also the cache capacity, right?
So if you have too many threads working on just one operation, sometimes it will cause
contention and actually this will create even more overhead and slow down your processing.
So that's why, you know, it's not always like your since that maximum would give you the benefit.
We actually, you know, find most of the time, actually, you know, it's just a random
value in between depending on your model, depending on the specific system, yeah?
And another important aspect is batching.
So the batching basically-
Before we move on to batching, when we're talking about operator parallelism, what's
the operator in this context?
Oh, yes, that's a good question.
So for the operators, right, basically you can think about, right, all the, no matter,
you know, what models you have, we're talking about things like low level things like
multiplies and accumulates and that kind of thing or, yeah, so modern machine learning frameworks
they actually take your model as input and then they will create a computational graph,
right? For example, some operators, you know, doing like a matrix multiplication, right?
Some just like a symbol, like element wise operation, right?
So all these operations basically are executed in your system.
So we're talking about kind of unrolling the computational graph and understanding how
it can be parallelized across multiple cores, right, exactly.
So however, since all these frameworks, they provide, you know, a control knob so that
you can set, you know, different paradigms for operators as well as for request, for request
basically, it's admission policy, right?
However, the sense they don't tell you how to set it for a specific model and deploy
the specific system.
And we find if you set it in a different way, it actually can significantly impact your
latency.
All right.
And so you're about to mention the second part of latency, which is the batch size.
Yeah.
Yeah.
So another important thing people do for accelerating the processing speed is batch.
So basically batch means you just, you don't execute a request like one by one, but rather
you will form like you will put several images together into a batch.
So the benefit of batch is, right, it creates more opportunities for optimization, right,
parallelism optimization, right?
For example, right, if you have like a small matrix, right, because it will do, for example,
matrix multiplication, right?
If you have two small matrix, of course, since can be done in power, but in like, you
know, very limited degree.
However, if you have two big matrix, right, then basically you can, you know, divide
ism and, you know, beta-accelerate, right?
So that's why, you know, the batching can actually help, you know, the efficiency of the
computation, right?
So all these, you know, low-level libraries, they actually, you know, develop to optimize,
you know, the computation when you have like a larger degree of input dimension.
Okay.
So, however, right, batching has two sides, right?
One is it can increase the computational efficiency and then accelerate, you know, to all
computation.
However, if you think about it, in order to form like a large batch, right, you're increasing
latency.
Right, right?
It's not in a training, right?
Because training all your data is there, right?
You can create whatever batch size you want, right?
But in the real-time inference, actually, request arrives in a random pattern, right?
Sometimes, you know, there are more requests, sometimes less requests, right?
And if you want to create like a larger batch, means the earlier arrived the request, they
actually have to wait the later request, right?
Then this penalized the earlier request.
Another thing is, right, even though when you, for example, pretend requests together,
right, it's faster than, you know, it's secure to the individual of them.
However, it still takes longer, right?
Right.
Say like each request has takes 100 million seconds to process, right?
Then if you do one by one, then you need to once again, right?
However, if you do it in a batch, maybe it's just a 500 million seconds, right?
Then still think about the first request, originally it only takes 100 million seconds, but not
it takes 500 million seconds, right?
Of course, the later request will benefit a lot from this, because think about the
killing, waiting, perspective, right?
They need to wait a little request finish, right?
They can immediately, you know, go through the process and it's, you know, doing faster,
right?
So that's, you know, the reason you cannot do an arbitrary batch size, right?
Actually, more than machine learning frameworks, they do provide two control knobs here.
One is the batch size, maximum batch size, which means once you hit the threshold, it will
be sent to the system, right?
Even you have even more to, you know, still just send this size.
The second is with something called the waiting timeout, which means you don't want to wait
forever to create a batch, right?
Since we are talking about latency, right?
Every request matters, right?
So I will say these two parameters in a real system is really complicated, right?
It depends on a lot of factors.
So if you think both parallelism and batch parameters, right?
We have a lot of parameters here, you need to tune, right?
I would say even system expert and machine learning expert, right?
They work together, it's difficult, you know, to find a way to just manually tune these
parameters.
Plus, you know, based on different system situation and different workload, right?
These parameters need to be changed, right?
So that you can achieve the optimal, right?
So that's why, you know, we created an automatic way, you know, to help people to configure
these parameters.
So I will actually first work published in supercomputing 2016, we actually built it like a coding
model to actually, you know, to model the situation, you know, when you have, you know, these
different parameters.
Okay.
And then we can mathematically, you know, compute our optimal solution for the scheduling.
And this is all bringing me back to the stuff that I did in grad school.
Oh, really?
Wow.
Doing theory and stochastic modeling and MMN cues and all that kind of stuff, right?
And of course, the limitation of that work is, right?
You do need to have some assumptions, right?
For example, I have the arrival.
Yeah, yeah, yeah.
You do need to assume the random arrival, right, which follows the ID distribution, right?
And also, you know, for that work, we also assume, you know, the request size deterministic,
which means it's actually true for a lot of computer vision tasks, right?
Think about all the pictures they have the same dimension, right?
And the model is the same.
Of course, they take roughly the same time to process.
However, this is not true, actually, for some other applications like, you know, speech
recognition, right, for natural language processing, right?
So depends on your sentence is short or longer, right?
The present time can be different.
Think about it, right?
So basically, the model based approach, they do need some assumptions, right?
So, and make, if we want to make it more general in practice, we actually, you know, there's
another way is doing model free approach, right?
Of course, there are tons of different learning based approach for model free, right?
So, of course, there are some simple things like a patient optimization, right?
It's something is also very popular, right?
So the reason we choose reinforcement learning is because, think about the dimension here,
right?
We actually have a lot of parameters you need to tune, right?
So it's actually, you know, multi-dimensional problem, so quite complicated.
So that's why we choose reinforcement learning as our approach, right?
However, for reinforcement learning, there are some big limitations.
It needs a lot of training samples, and it converges very slowly, right?
So this is actually not ideal, right, in like a real time system, right?
Right.
So also, it quits a lot of, you know, overheads.
Those problems are training time problems as opposed to inference time problems, yes or
now?
Well, if you want to use a reinforcement learning to configure your system, right, then this
is like a training problem, but of course it's different then.
So we have two machine learning parts, right?
One is machine learning itself, right?
And the other is to configure the system parameters, right?
Right.
So we actually use the machine learning model to find, you know, an optimal solution to
do the machine learning inference, right?
Right.
Right.
Yeah.
So the conventional reinforcement learning, right, they do have like a relatively long learning
cycle.
So what we observe is for this specific program, right?
So think about it, right?
When you have, you know, this complex computational graph, right?
You may have, you know, hundreds of different operators, right?
So some operators may be more sensitive to this, you know, a parameter, right?
Others may be more sensitive to other system parameters, right?
So when you do like a very slight change, right?
For example, just one of the parameters increased, maybe from two to three, right?
So in this case, actually, a lot of maybe only some of the parameters they, you know, have
like pronounced change, but most of others may be not so much, right?
But all global, you know, situation is that it doesn't show much of the change, right?
However, if you did a big change, right?
Then almost all of them have different behaviors, then this will show like a more kind of pronounced
changes globally, right?
So if you draw like a hit map, right?
You will see, right?
When you have a big change, the latency will be totally different.
However, if you do mean, right?
See, I just do a small change.
Actually, it's locally, it's very smooth.
It's actually doesn't, you know, change latency much, right?
So then we are thinking about, right?
Why we just, you know, do less of the learning samples?
And then we use, you know, the long-running sample to actually help estimate, right?
Then you're by regions.
So that's, you know, why we created an approach called region-based reinforcement
running, or L in short.
Okay.
Yeah.
So we find, you know, compared to state-of-the-art, a reinforcement running approach, you
know, used for system configuration, we can significantly reduce, you know, the learning
curve.
So basically, the reinforcement running models can converge so much faster.
And it also, think about, right?
It's a continuous learning process, right?
So whenever you have your models changed, or, you know, even, you know, the system can
have, you know, changes, right?
Especially if it's a cloud system, right?
It's self-have-lot of randomness, right?
So whenever there are such change, right?
Basically, the models will learn to adapt to it, right?
So that's why, you know, the faster convergence is critical here.
I feel like I'm close to understanding what you're doing with the region-based RL.
What I am envisioning is you're doing something where you're changing your optimization,
your cost function, so that you're more focused on kind of this coarse-grained sensitivity
analysis kind of thing.
If you think about reinforcement running, right?
It basically, you know, have two key things, right?
One is action, one is state, right?
Right.
So it's just like, you know, we are running scenes or playing a game, right?
Yeah.
We would try different actions, right?
And then we would get different rewards, right?
Of course, in different states, you'll try different actions, you'll get different rewards,
right?
So that's basically the space of the whole learning process, right?
So our approach basically says, right, if you try one action in a certain state, right?
State.
Basically, you probably didn't need to try another very close-by action because it gives
you, well, be roughly, you know, similar amount of reward.
So in other words, you're kind of quantizing the action space a little bit differently,
more coarse-grained.
Yeah, you can imagine kind of think about it, right?
Sort of like that.
Yes, yes, yes.
So, and of course, you know, we do introduce like a hyper-primiter, right?
To see how big is the region, right?
So, if you have like, you know, too large region, then, you know, it will sort of, you know.
So, so the region size basically means, right?
If you have like larger region size, you are then faster, but if it's too big, then
you will never converge, right?
It's kind of like a learning rate.
Right, right.
If you have like two small ones, of course, you know, it will have less risk, but it's
longer, right?
If you have size one, basically, it's traditional conventional reinforcement, right?
Right, right.
I think the key message here, right, is even if you have good training model, right?
So in order to do this in a real time detection, it's actually not that easy.
So you do need to consider, you know, the machine learning actually are quite expensive
compared to conventional statistical models.
And as well as, right, you do have this changing demanding of the workload, right?
So that's why, right?
You need to consider, you know, the latency, as well as the scalability.
And of course, right, if you want to go to cloud, right?
The efficiency is always a problem, right?
If you have infinite amount of budget, right?
Of course, you just go for the most expensive equipment, right?
Right.
Actually, it's quite costly, right?
So it's usually for machine learning inference, right?
If you want to continuously rent, you know, a lot of machines, right?
The actual computing resource and energy is tremendous, right?
So any savings that on this line would be, you know, give you like a huge benefit.
Awesome.
Awesome.
Well, thanks so much for taking the time to share with us what you're up to.
Okay.
Thank you.
Thank you.
All right, everyone, that's our show for today.
To follow along with our reinvent series, visit twimmelai.com slash reinvent 2019.
Thanks once again to Capital One for their sponsorship of this series.
Be sure to check out capital one dot com slash tech slash explore to learn more about their
ML and AI research.
Thanks so much for listening and catch you next time.
