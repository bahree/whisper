1
00:00:00,000 --> 00:00:16,000
Hello and welcome to another episode of Twimal Talk, the podcast where I interview interesting

2
00:00:16,000 --> 00:00:20,920
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:20,920 --> 00:00:23,520
I'm your host, Sam Charrington.

4
00:00:23,520 --> 00:00:28,120
This is the second show in our series of podcasts from the recent Wrangle Conference.

5
00:00:28,120 --> 00:00:32,680
As you might know, a few weeks ago I was in San Francisco for Wrangle, which is a great

6
00:00:32,680 --> 00:00:36,600
little conference brought to you by our friends over at Claudeira.

7
00:00:36,600 --> 00:00:38,960
Wrangle is such a fun event each year.

8
00:00:38,960 --> 00:00:43,720
It brings an interesting and diverse community of data scientists to an intimate and informal

9
00:00:43,720 --> 00:00:48,960
setting for great talks on real data science projects and issues, not to mention Cowboy

10
00:00:48,960 --> 00:00:50,560
Hatson BBQ.

11
00:00:50,560 --> 00:00:55,560
If you haven't caught the first episode in our Wrangle series, Twimal Talk No. 39 with

12
00:00:55,560 --> 00:00:57,080
Drew Conway.

13
00:00:57,080 --> 00:00:59,360
You'll want to be sure to check that out.

14
00:00:59,360 --> 00:01:03,800
It's a great interview and the intro includes important announcements about this series

15
00:01:03,800 --> 00:01:08,960
as well as our latest ticket giveaway, our online research paper discussion group, and

16
00:01:08,960 --> 00:01:10,600
my email newsletter.

17
00:01:10,600 --> 00:01:14,600
The show you're listening to now features my interview with Sharath Rao.

18
00:01:14,600 --> 00:01:19,040
I reached out to Sharath about being on the show and was blown away when he replied that

19
00:01:19,040 --> 00:01:24,760
not only had he heard about the show, but that he was a fan and an avid listener.

20
00:01:24,760 --> 00:01:29,160
My conversation with him digs into some of the practical lessons and patterns he's learned

21
00:01:29,160 --> 00:01:34,960
by building production ready, web scale data products based on machine learning models,

22
00:01:34,960 --> 00:01:38,640
including the search and recommendation systems at Instacart.

23
00:01:38,640 --> 00:01:43,360
A quick note before we dive in, as is the case with my other field recordings, there's

24
00:01:43,360 --> 00:01:46,400
a bit of unavoidable background noise in this interview.

25
00:01:46,400 --> 00:01:57,880
Sorry about that, and now on to the show.

26
00:01:57,880 --> 00:02:01,040
All right everyone, I am here with Sharath Rao.

27
00:02:01,040 --> 00:02:08,560
He's an engineering manager at Instacart, and we are on location at the Wrangle conference,

28
00:02:08,560 --> 00:02:14,680
and Sharath has a talk later on, and I'm fortunate enough to have him here to tell us a little

29
00:02:14,680 --> 00:02:18,360
bit about what he's going to tell the Wrangle audience about.

30
00:02:18,360 --> 00:02:21,440
So Sharath, welcome to the show, it's great to have you.

31
00:02:21,440 --> 00:02:27,000
Great, thanks for having me Sam, I'm a big fan of the show from having heard earlier episodes.

32
00:02:27,000 --> 00:02:29,680
Nice, nice, well I really appreciate that.

33
00:02:29,680 --> 00:02:33,600
As you know, one of the places I like to get started then is to have folks introduce

34
00:02:33,600 --> 00:02:38,240
themselves, tell us a little bit about what you're up to, and how you got there.

35
00:02:38,240 --> 00:02:39,720
Yeah, yeah definitely.

36
00:02:39,720 --> 00:02:45,640
So I'm at Instacart, I've been here for a couple of years, Instacart is an on-demand grocery

37
00:02:45,640 --> 00:02:52,760
delivery service, and my role here is, I started off as the data scientist or machine learning

38
00:02:52,760 --> 00:02:59,400
engineer focused on search personalization and recommendations, and now I do that, but

39
00:02:59,400 --> 00:03:03,360
also lead a team that is working on that effort.

40
00:03:03,360 --> 00:03:09,440
So prior to this, I've spent time at a couple of companies doing search, advertising,

41
00:03:09,440 --> 00:03:13,840
and auctions, that sort of stuff over the last 10 years.

42
00:03:13,840 --> 00:03:19,360
And even going back further, grad school, I worked on speech recognition and speech translation

43
00:03:19,360 --> 00:03:22,760
before it was practically useful in the field.

44
00:03:22,760 --> 00:03:28,560
That's part of the reason why I started working on other things after grad school.

45
00:03:28,560 --> 00:03:31,720
So yeah, so that's probably captures the range of things.

46
00:03:31,720 --> 00:03:32,720
Awesome, awesome.

47
00:03:32,720 --> 00:03:37,000
I hear a lot of stories from folks who, you know, I was working on this stuff in grad school

48
00:03:37,000 --> 00:03:42,000
and it just wasn't ready, or we were in the middle of the AI winter, and it kind of went

49
00:03:42,000 --> 00:03:43,000
into hibernation.

50
00:03:43,000 --> 00:03:49,240
And now that we're all so focused on doing this stuff, it's like time to dust off those

51
00:03:49,240 --> 00:03:50,240
skills.

52
00:03:50,240 --> 00:03:51,240
Absolutely.

53
00:03:51,240 --> 00:03:52,240
Yeah.

54
00:03:52,240 --> 00:04:00,480
So in your description, you said data science, slash machine learning engineering, how

55
00:04:00,480 --> 00:04:05,640
evolved is the distinction between the two of those at Instacart, and do you consider

56
00:04:05,640 --> 00:04:07,360
yourself as spanning both?

57
00:04:07,360 --> 00:04:08,800
Yeah, let me take the first one first.

58
00:04:08,800 --> 00:04:15,000
I think at Instacart, well, or was industry, I think we are moving better, you know, towards

59
00:04:15,000 --> 00:04:17,040
understanding these two roles.

60
00:04:17,040 --> 00:04:22,240
And so at least we are sort of settling around maybe three roles, actually data analysts,

61
00:04:22,240 --> 00:04:25,320
data scientists, and data engineers.

62
00:04:25,320 --> 00:04:29,080
Even if they're slightly called separately, I think maybe their companies like maybe LinkedIn

63
00:04:29,080 --> 00:04:35,600
or maybe NB where probably have data scientists and machine learning engineers and data engineers.

64
00:04:35,600 --> 00:04:41,640
But broadly, the difference is around, you know, people working on data products, building

65
00:04:41,640 --> 00:04:46,520
systems and algorithms, essentially things that are consumed by other algorithms and systems

66
00:04:46,520 --> 00:04:50,800
who might be like machine learning engineers, people working on things, the output of which

67
00:04:50,800 --> 00:04:55,840
is used for, you know, decision making, you know, by the team that they serve or the executive

68
00:04:55,840 --> 00:04:56,840
leadership.

69
00:04:56,840 --> 00:05:00,840
We call them data analysts at Instacart, but data scientists elsewhere, and then finally

70
00:05:00,840 --> 00:05:06,280
data engineers who are working on like platform infrastructure typically comes in, you know,

71
00:05:06,280 --> 00:05:07,880
maybe later in the company stage.

72
00:05:07,880 --> 00:05:08,880
Right.

73
00:05:08,880 --> 00:05:09,880
Right.

74
00:05:09,880 --> 00:05:15,200
I think it's a lot of progress we've made from just a few years ago, and, you know, data

75
00:05:15,200 --> 00:05:19,320
scientists was this unicorn that no one could really hire because we've defined it in

76
00:05:19,320 --> 00:05:24,760
such a way that, you know, it requires these disparate skills that, you know, aren't traditionally

77
00:05:24,760 --> 00:05:25,760
paired.

78
00:05:25,760 --> 00:05:31,920
Yeah, I remember back in maybe 2012, 2013, the term of unicorn data scientist was probably

79
00:05:31,920 --> 00:05:35,560
one that captures what he just described, but I don't hear that too often.

80
00:05:35,560 --> 00:05:39,240
There's more like specialization and people understand that over time, most professions

81
00:05:39,240 --> 00:05:43,120
end up specializing, although I'd imagine, in depends on the context of the companies,

82
00:05:43,120 --> 00:05:46,720
I would imagine say startups, if they're hiring their first data scientist, they probably

83
00:05:46,720 --> 00:05:51,840
want somebody who can also do like the other two things that are reasonable level of competence.

84
00:05:51,840 --> 00:05:56,320
Overall, in a macro view, I think, the roles have sort of specialized over time.

85
00:05:56,320 --> 00:05:57,320
Yeah.

86
00:05:57,320 --> 00:06:03,160
And I remember back in that same time period, 2012, I think as a community, we thought

87
00:06:03,160 --> 00:06:07,880
that it would take way longer for us to reach this level of maturity, because like you'd

88
00:06:07,880 --> 00:06:12,380
see these stats that say, you know, we're going to have, you know, we're going to be under

89
00:06:12,380 --> 00:06:17,320
resource in terms of the number of data scientists until like the year 2050 or something like

90
00:06:17,320 --> 00:06:18,320
that.

91
00:06:18,320 --> 00:06:26,080
I think this specialization has been part of the key for alleviating that stress, although

92
00:06:26,080 --> 00:06:32,000
we're far from having enough, you know, people with data competencies to meet all the

93
00:06:32,000 --> 00:06:33,000
demand.

94
00:06:33,000 --> 00:06:34,000
Right.

95
00:06:34,000 --> 00:06:37,360
I think every speaker at an event like this gets up and says, oh, and by the way, we're hiring.

96
00:06:37,360 --> 00:06:38,360
Yes.

97
00:06:38,360 --> 00:06:39,360
Right.

98
00:06:39,360 --> 00:06:40,360
Yeah.

99
00:06:40,360 --> 00:06:42,400
So I've heard that and also more recently I've heard about and maybe even seen it in

100
00:06:42,400 --> 00:06:47,240
the field where there's, you know, with, you know, obviously the market has responded to

101
00:06:47,240 --> 00:06:50,960
at least a perceived lack of, you know, strong talent.

102
00:06:50,960 --> 00:06:55,880
And so we have quick, like smaller one of your masters or bootcamps that have served the

103
00:06:55,880 --> 00:06:56,880
need.

104
00:06:56,880 --> 00:07:00,320
So as a result, I think in maybe in some part of the market, they might, maybe at like

105
00:07:00,320 --> 00:07:05,600
optimum or even oversupply of, you know, data scientists at arguably at starting level.

106
00:07:05,600 --> 00:07:06,600
Really?

107
00:07:06,600 --> 00:07:08,240
Well, that's happening quickly.

108
00:07:08,240 --> 00:07:13,960
Maybe I mean, just generally that I guess everything is moving faster and now even for

109
00:07:13,960 --> 00:07:19,840
example, with newer areas around, say, deep learning and AI, there is, you know, there

110
00:07:19,840 --> 00:07:24,760
are more research institutes, there are more, again, training, you know, workshops and

111
00:07:24,760 --> 00:07:30,240
so on that people respond faster to needs than maybe they used to.

112
00:07:30,240 --> 00:07:31,240
Mm-hmm.

113
00:07:31,240 --> 00:07:32,240
Interesting.

114
00:07:32,240 --> 00:07:33,240
All right.

115
00:07:33,240 --> 00:07:34,240
So you talk.

116
00:07:34,240 --> 00:07:35,240
Yes.

117
00:07:35,240 --> 00:07:36,240
You've got to talk in an hour.

118
00:07:36,240 --> 00:07:37,240
Yeah.

119
00:07:37,240 --> 00:07:38,240
What's your talk about?

120
00:07:38,240 --> 00:07:39,240
Sure.

121
00:07:39,240 --> 00:07:43,240
So my talk is title lessons from integrating data, machine learning models into data products.

122
00:07:43,240 --> 00:07:47,000
So initially I had a two part talk, but you know, today I'm talking about like the part

123
00:07:47,000 --> 00:07:48,200
one of that.

124
00:07:48,200 --> 00:07:54,960
But really the, the genesis of that comes from having worked at first of all, you know,

125
00:07:54,960 --> 00:07:58,800
relatively a smaller company as opposed to some of the bigger companies that were in

126
00:07:58,800 --> 00:08:04,880
we have, there is a product per se and there are machine learning models that are integrated

127
00:08:04,880 --> 00:08:09,920
into various parts of the product that are making different decisions for the customers.

128
00:08:09,920 --> 00:08:16,520
So there's that and then there is aspects about like data scientists building models, model

129
00:08:16,520 --> 00:08:22,360
prototypes and even productionizing them, but also interfacing with product engineers

130
00:08:22,360 --> 00:08:24,320
within a single team.

131
00:08:24,320 --> 00:08:28,920
And the fact that what I call like all model, all model prototypes look familiar, but every

132
00:08:28,920 --> 00:08:32,360
model and production is unique in its own way.

133
00:08:32,360 --> 00:08:37,480
So I'm trying to understand, try to frame a conversation about what does it mean to have

134
00:08:37,480 --> 00:08:38,480
a model and production?

135
00:08:38,480 --> 00:08:44,360
You know, what are the questions to ask yourself and how do we communicate as data scientists

136
00:08:44,360 --> 00:08:50,480
with in a product managers and product engineers about what this model does, what are its requirements,

137
00:08:50,480 --> 00:08:52,240
what are the constraints?

138
00:08:52,240 --> 00:08:54,560
And so that's, that's what I'll be talking about.

139
00:08:54,560 --> 00:08:58,840
Okay, so how do you, how do you structure walking people through that?

140
00:08:58,840 --> 00:09:04,000
Yeah, I try to keep it, try to find, you know, going back to conversations within the

141
00:09:04,000 --> 00:09:08,440
team, for example, each time we, we have one team that has, you know, three or four product

142
00:09:08,440 --> 00:09:13,000
engineers, you know, a couple of data scientists in a designer and a PM, right?

143
00:09:13,000 --> 00:09:16,280
And largely most of the work happens, you know, within the team.

144
00:09:16,280 --> 00:09:22,000
And each time we work on an experiment or a feature that that data's data scientists build,

145
00:09:22,000 --> 00:09:25,600
we have this conversation about how will this go into production?

146
00:09:25,600 --> 00:09:29,960
And typically the conversations, you know, if you think about it like the process of building

147
00:09:29,960 --> 00:09:34,920
a model, thinking about a problem, building a model prototype, largely happens, you know,

148
00:09:34,920 --> 00:09:39,280
within the domain of data science and maybe, you know, with some interaction at the PM.

149
00:09:39,280 --> 00:09:42,240
But the moment it goes into production, it's now touching like so many different parts

150
00:09:42,240 --> 00:09:43,240
of the system.

151
00:09:43,240 --> 00:09:47,680
And, you know, a couple of things come in like, how much time do we have to make, make

152
00:09:47,680 --> 00:09:51,920
a decision in terms of like the model, you know, running in production, you know, many

153
00:09:51,920 --> 00:09:55,800
if you're serving up pages live to the web, you know, you've got to serve, yeah, serving

154
00:09:55,800 --> 00:09:58,200
latency, for example, yeah, address right?

155
00:09:58,200 --> 00:10:02,000
Yeah, so there's a constraint that very often gets dictated by the product itself and

156
00:10:02,000 --> 00:10:03,480
rightly so.

157
00:10:03,480 --> 00:10:07,920
And then there's this question of, you know, for the model to be successful, how much information

158
00:10:07,920 --> 00:10:09,400
does it need?

159
00:10:09,400 --> 00:10:10,760
And how can I get that information?

160
00:10:10,760 --> 00:10:15,200
By information, I mean like, for example, let's say you're trying to recommend a product.

161
00:10:15,200 --> 00:10:18,880
Maybe you definitely need to know if you want to, you know, it's a personalized recommendation,

162
00:10:18,880 --> 00:10:24,040
you need to know the user, you know, user identity of their past data, you probably need

163
00:10:24,040 --> 00:10:26,640
to know like what page they're on and so on.

164
00:10:26,640 --> 00:10:30,920
But do you really need to know their recent searches and, you know, recent activity, like

165
00:10:30,920 --> 00:10:32,840
that certainly is like short-term context.

166
00:10:32,840 --> 00:10:39,040
So how much context do we need, does the model need rather to operate successfully?

167
00:10:39,040 --> 00:10:44,040
Both of these sort of give you a way to think about, can I cash my recommendations?

168
00:10:44,040 --> 00:10:48,120
You know, what can I cash versus what needs to be served in real time?

169
00:10:48,120 --> 00:10:53,480
If it's real time, can I do a reasonably well if, you know, let's say, let's say I'm making

170
00:10:53,480 --> 00:11:00,000
a decision about what product to show based on things that you've added to the cart recently.

171
00:11:00,000 --> 00:11:05,600
So think of a model that is continuously producing a, you know, prediction of what, what part

172
00:11:05,600 --> 00:11:07,400
is a good recommendation?

173
00:11:07,400 --> 00:11:08,800
But it doesn't necessarily surface that.

174
00:11:08,800 --> 00:11:13,200
It's always, you know, continuing to score and maintain a best estimate of a recommendation

175
00:11:13,200 --> 00:11:14,200
in the background.

176
00:11:14,200 --> 00:11:18,000
But, you know, at any point it's available with a best effort response.

177
00:11:18,000 --> 00:11:24,240
So this is an example where you don't, you know, you have a good response whenever there's

178
00:11:24,240 --> 00:11:25,240
a need for it.

179
00:11:25,240 --> 00:11:28,000
We can wait until the last moment to actually serve that recommendation.

180
00:11:28,000 --> 00:11:30,520
So those things that happen in the background.

181
00:11:30,520 --> 00:11:36,480
Are you describing a system that is doing like online learning or active learning, which

182
00:11:36,480 --> 00:11:40,280
is where you're updating your model kind of in place?

183
00:11:40,280 --> 00:11:41,280
Well, yeah.

184
00:11:41,280 --> 00:11:46,080
So the model could be static, model need not be updated, but the model predictions could

185
00:11:46,080 --> 00:11:51,520
be happening continuously in the background based on, based on whatever data it sees.

186
00:11:51,520 --> 00:11:55,480
It doesn't need to score in immediately reporting, it can keep scoring as it gets more data

187
00:11:55,480 --> 00:11:59,000
and then always be ready to serve a recommendation when necessary.

188
00:11:59,000 --> 00:12:00,000
Okay.

189
00:12:00,000 --> 00:12:03,760
So that gives us like, you know, so we have this, you know, four quadrant because I'm considering

190
00:12:03,760 --> 00:12:07,680
latency and context sensitivity of the model behavior.

191
00:12:07,680 --> 00:12:12,360
And we get these four quadrants where, you know, you obviously have like high latency is

192
00:12:12,360 --> 00:12:16,600
okay, but you have context sensitive and the other, basically, the other four possibilities,

193
00:12:16,600 --> 00:12:18,600
other three possibilities.

194
00:12:18,600 --> 00:12:25,720
And so it's part of what you've observed, then, you know, are we at the point of maturity

195
00:12:25,720 --> 00:12:30,120
and thinking about this that, you know, for each of these quadrants, we've got like design

196
00:12:30,120 --> 00:12:35,520
patterns or best practices that teams, you know, either at InstaCard or in the industry

197
00:12:35,520 --> 00:12:36,520
are following?

198
00:12:36,520 --> 00:12:40,520
Certainly, InstaCard, we try to place ourselves and don't like, where does this model sort

199
00:12:40,520 --> 00:12:42,960
of lie, you know, just mentally and internally.

200
00:12:42,960 --> 00:12:48,240
And that's sort of all, given that we've built a few experiences, few models over the past

201
00:12:48,240 --> 00:12:54,080
a couple of years, we have patterns, you know, we know, we sort of know 90% of, you know,

202
00:12:54,080 --> 00:12:57,880
the way the model is integrated in the rest of the product, when we are thinking about

203
00:12:57,880 --> 00:13:01,200
the model, we don't have to like start from scratch each time.

204
00:13:01,200 --> 00:13:06,640
So for example, search, search ranking, you know, we latency needs to be really low.

205
00:13:06,640 --> 00:13:10,800
And context sensitivity, we can start off with, you know, low context sensitivity, I mean,

206
00:13:10,800 --> 00:13:14,920
let's say you're simply matching the search query and the bunch of products without like

207
00:13:14,920 --> 00:13:15,920
recent context.

208
00:13:15,920 --> 00:13:19,760
If you could start there, then as we, the way to improve the model would be to, well,

209
00:13:19,760 --> 00:13:25,240
continue to have latency requirement being low, but make it more context sensitive, obviously

210
00:13:25,240 --> 00:13:29,360
there's a lot of work that might involve a model that is ranking, re-ranking products

211
00:13:29,360 --> 00:13:32,000
based on, you know, recent activity.

212
00:13:32,000 --> 00:13:33,000
So that's one example.

213
00:13:33,000 --> 00:13:37,160
So with search, we start, I call it we start from quadrant four and go to quadrant one.

214
00:13:37,160 --> 00:13:43,240
Sort of will be clear from the slides, I guess, but with the recommendations, it's almost

215
00:13:43,240 --> 00:13:47,400
always the case that you start with, you know, in start in quadrant three, where you cache

216
00:13:47,400 --> 00:13:51,920
as much as possible, you score them, you know, in batch mode, you cache them and then you

217
00:13:51,920 --> 00:13:53,000
serve.

218
00:13:53,000 --> 00:13:57,840
So that keeps, that gives you, you know, you can use as much information as you want,

219
00:13:57,840 --> 00:14:02,760
and you know, gives you the latitude to use more data and have like recommendations.

220
00:14:02,760 --> 00:14:07,160
The way to improve that would be to go from quadrant four to, or quadrant three to two,

221
00:14:07,160 --> 00:14:12,040
I guess, where, you know, you still, you know, latency is still not a consideration.

222
00:14:12,040 --> 00:14:17,800
You're re-ranking your products, your, you know, candidate products, and, you know,

223
00:14:17,800 --> 00:14:20,000
in the background, be very, very, to serve.

224
00:14:20,000 --> 00:14:22,960
And then finally, you might have like, you know, contextual recommendations wherein you

225
00:14:22,960 --> 00:14:28,920
have to recommend something, you know, immediately, and so that becomes like a real-time recommendation,

226
00:14:28,920 --> 00:14:34,440
which is, you know, a model that, you know, re-ranks a set of products that you've cached,

227
00:14:34,440 --> 00:14:35,720
you know, based on the context.

228
00:14:35,720 --> 00:14:39,160
So the set of products you've cached may be personalized or maybe not personalized, but

229
00:14:39,160 --> 00:14:44,320
then when you serve, you re-rank it based on user activity and make it personalized.

230
00:14:44,320 --> 00:14:45,320
Yeah.

231
00:14:45,320 --> 00:14:53,120
So how much of this machine learning engineering is machine learning and how much is engineering,

232
00:14:53,120 --> 00:15:00,480
meaning a lot of what we're talking about is, you know, system level, scalability, like,

233
00:15:00,480 --> 00:15:05,320
you know, just hardcore software engineering that, you know, in a lot of ways is orthogonal

234
00:15:05,320 --> 00:15:07,120
to the machine learning.

235
00:15:07,120 --> 00:15:11,400
Well, I guess the model stuff isn't, but in a lot of ways it seems orthogonal.

236
00:15:11,400 --> 00:15:15,960
There are principles at least that seem orthogonal to the thing that you're scaling, right?

237
00:15:15,960 --> 00:15:19,920
It's, it's web-scale engineering, yeah, applied to M-O.

238
00:15:19,920 --> 00:15:25,240
So like fine tune that statement, how much of it, you know, a specific is domain specific

239
00:15:25,240 --> 00:15:28,920
and where are those points that it gets super domain specific?

240
00:15:28,920 --> 00:15:31,160
That's a really good question.

241
00:15:31,160 --> 00:15:35,960
And I think the point at which it gets domain specific is that, as a data scientist,

242
00:15:35,960 --> 00:15:40,400
you understand the model, its assumptions, its requirements.

243
00:15:40,400 --> 00:15:43,080
At what point, you know, it'll fail to perform.

244
00:15:43,080 --> 00:15:44,320
What does it absolutely need?

245
00:15:44,320 --> 00:15:47,880
What is table stakes versus, you know, what is a cherry on the top?

246
00:15:47,880 --> 00:15:51,160
You understand that and you, well, you need to understand that and then articulate it

247
00:15:51,160 --> 00:15:55,840
to, you know, product engineers and to others on the team.

248
00:15:55,840 --> 00:15:58,920
Obviously, you don't want to do that right at the end when you've already built the model

249
00:15:58,920 --> 00:16:02,040
but you want to do it right at the beginning so that you can know up front if there are

250
00:16:02,040 --> 00:16:04,640
any constraints on the engineering side.

251
00:16:04,640 --> 00:16:05,640
Right.

252
00:16:05,640 --> 00:16:10,600
So it is sort of good mix of machine learning, you know, and software engineering.

253
00:16:10,600 --> 00:16:14,760
And even product design, frankly, you know, I'd imagine we've one of one of the things

254
00:16:14,760 --> 00:16:22,080
you found, for example, is we may have some latitude in how we change the product experience

255
00:16:22,080 --> 00:16:26,760
for a user based on some of the constraints on the engineering and machine learning

256
00:16:26,760 --> 00:16:27,760
side.

257
00:16:27,760 --> 00:16:32,320
For example, if you want to buy yourself time to be able to score, what would that interface?

258
00:16:32,320 --> 00:16:34,520
What will we do with the customer at that time?

259
00:16:34,520 --> 00:16:37,000
How do we design the product experience?

260
00:16:37,000 --> 00:16:38,000
Yeah.

261
00:16:38,000 --> 00:16:45,000
Interesting, I've been, I've talked to a few people about this, like, I think there's

262
00:16:45,000 --> 00:16:50,820
an evolving field of like, I always call it intelligent design but that's like way over

263
00:16:50,820 --> 00:16:58,560
loaded but there needs to be, and I think there will be, you know, some, you know, thought

264
00:16:58,560 --> 00:17:04,240
best practices for lack of a better term, like just a way of thinking about designing

265
00:17:04,240 --> 00:17:09,800
kind of in light of machine learning and intelligence and things like that, that, you know, I don't

266
00:17:09,800 --> 00:17:13,040
see a lot of people talking about that kind of stuff yet.

267
00:17:13,040 --> 00:17:16,600
And it's not necessarily exactly what you're describing here, what you're describing is

268
00:17:16,600 --> 00:17:21,680
even, you know, kind of going back to the previous question, it's really more the interaction

269
00:17:21,680 --> 00:17:26,800
between, you know, design and product engineering and like performance engineering and things

270
00:17:26,800 --> 00:17:31,600
like that, like how does our, how does the system, how does a user experience change because

271
00:17:31,600 --> 00:17:36,800
of the limitations, you know, in, you know, performance and product engineering and stuff

272
00:17:36,800 --> 00:17:37,800
like that?

273
00:17:37,800 --> 00:17:44,400
Yeah, I definitely think it's definitely controversial to suggest that we have to change user experiences

274
00:17:44,400 --> 00:17:51,360
to work with the limitations of, you know, engineering or machine learning, right?

275
00:17:51,360 --> 00:17:57,000
And it may not be common to do that but I wish I had like more examples where it's

276
00:17:57,000 --> 00:18:01,840
possible to do maybe, maybe it's not something that we generally talk about, if indeed it happens

277
00:18:01,840 --> 00:18:02,840
in the field.

278
00:18:02,840 --> 00:18:06,640
And that's the other thing, which is like, it's hard to talk like about machine learning

279
00:18:06,640 --> 00:18:11,080
models and production, widely, simply because there are a lot of details that are probably

280
00:18:11,080 --> 00:18:14,320
very specific to the product and, you know, one just assumes that it's not interesting

281
00:18:14,320 --> 00:18:17,880
to somebody else or that these details don't really generalize.

282
00:18:17,880 --> 00:18:22,280
So there is no, there are definitely basic principles, you know, software engineering,

283
00:18:22,280 --> 00:18:27,960
but beyond that are there other principles that are specific or that are more important

284
00:18:27,960 --> 00:18:33,000
when it comes to like software engineering with like machine learning systems or data products.

285
00:18:33,000 --> 00:18:37,720
So I called out a couple of examples of some work in the community like recently, maybe

286
00:18:37,720 --> 00:18:42,360
about early this year, there was a document from one of the machine learning engineers

287
00:18:42,360 --> 00:18:46,360
at Google about, I think it's called rules of machine learning or so.

288
00:18:46,360 --> 00:18:50,760
It talks about a really nice, what I think is it quite reading for data scientists building

289
00:18:50,760 --> 00:18:52,080
data products.

290
00:18:52,080 --> 00:18:55,960
And what are the different things, what are the different trade-offs, what are different

291
00:18:55,960 --> 00:18:59,360
stages of building a data product?

292
00:18:59,360 --> 00:19:06,240
And talks a lot about the engineering aspects, the training, the data management, metrics,

293
00:19:06,240 --> 00:19:11,440
the interface between like metrics and our model formulations and so on.

294
00:19:11,440 --> 00:19:16,360
It's kind of like, you know, it's a four-page PDF with such a density of knowledge.

295
00:19:16,360 --> 00:19:20,240
Like, at Instacard, we had like a reading group session, we spent like, I let a group

296
00:19:20,240 --> 00:19:25,200
with like two hours of going through that and they're talking through like salient ideas

297
00:19:25,200 --> 00:19:26,200
from that paper.

298
00:19:26,200 --> 00:19:27,200
Okay.

299
00:19:27,200 --> 00:19:30,360
Well, I have to make sure I get a link for that one and post it up in the show.

300
00:19:30,360 --> 00:19:31,360
Definitely, yeah.

301
00:19:31,360 --> 00:19:34,560
There's one more people that are like, quickly, which from Pinterest, where they talk about

302
00:19:34,560 --> 00:19:38,200
the evolution of the recommendation systems over three years.

303
00:19:38,200 --> 00:19:40,800
And this is, it's a really good one, I think.

304
00:19:40,800 --> 00:19:45,920
It talks about, incidentally, it talks about how they started it off with a simple system

305
00:19:45,920 --> 00:19:50,000
that was like caching recommendations of the Quadrant 3 world.

306
00:19:50,000 --> 00:19:55,240
And then from the how they progress from that to, you know, Quadrant 1, where they use

307
00:19:55,240 --> 00:19:58,320
what you get, what you cash, and then you re-rank it with a model.

308
00:19:58,320 --> 00:20:03,480
It's helpful to have this, you know, view of how this really massive, you know, at-scale

309
00:20:03,480 --> 00:20:07,960
recommendation system evolved so that, you know, people who haven't done that before

310
00:20:07,960 --> 00:20:11,880
don't need to believe that we need to start with what Pinterest has in production after

311
00:20:11,880 --> 00:20:12,880
three years of work.

312
00:20:12,880 --> 00:20:13,880
Right.

313
00:20:13,880 --> 00:20:14,880
Right.

314
00:20:14,880 --> 00:20:17,280
So, it's sort of underappreciated part of literature, I think.

315
00:20:17,280 --> 00:20:18,280
Yeah.

316
00:20:18,280 --> 00:20:22,920
So, this is maybe a little bit of a tangent, but I don't know if you've heard on some

317
00:20:22,920 --> 00:20:28,360
recent podcasts I've mentioned that I'm going to be starting a, like, an online paper

318
00:20:28,360 --> 00:20:31,160
reading group of podcast listeners.

319
00:20:31,160 --> 00:20:36,200
I mentioned it randomly in a podcast, and like, there were a bunch of people who expressed

320
00:20:36,200 --> 00:20:37,200
interest in it.

321
00:20:37,200 --> 00:20:41,480
So, I'm going to be getting that kicked off, but since you mentioned that you are involved

322
00:20:41,480 --> 00:20:46,840
in one at Instacart, any tips for running a paper reading group?

323
00:20:46,840 --> 00:20:47,840
Great.

324
00:20:47,840 --> 00:20:48,840
So, yeah, let me see.

325
00:20:48,840 --> 00:20:52,960
We have, like, two different sessions where all the data scientists come together, you

326
00:20:52,960 --> 00:20:57,280
know, sort of alternate weeks, and one of them is, or the one that I was talking about,

327
00:20:57,280 --> 00:21:01,320
we call that lunch and learn, we have, we do it over lunch every Tuesday.

328
00:21:01,320 --> 00:21:06,680
And the bar, in a sense, is that, you know, I had a time we talk about, like, what paper

329
00:21:06,680 --> 00:21:09,080
we are, what somebody is going to lead a discussion.

330
00:21:09,080 --> 00:21:13,560
That person doesn't need to have completely understood, but it's helpful for them to be

331
00:21:13,560 --> 00:21:16,200
able to know enough to drive the discussion.

332
00:21:16,200 --> 00:21:20,200
It's obviously optional for, you know, people who want to attend, because, you know, we

333
00:21:20,200 --> 00:21:24,040
have, we touch different domains in Instacart so somebody may not be particularly, you know,

334
00:21:24,040 --> 00:21:26,200
keen on a paper.

335
00:21:26,200 --> 00:21:29,960
And, but then it helps to have, like, discussion, you know, if you come in, make sure you've

336
00:21:29,960 --> 00:21:33,560
skimmed the paper and come in for discussion.

337
00:21:33,560 --> 00:21:37,520
We talk about what the paper, some of the, you know, what it takes away from the paper,

338
00:21:37,520 --> 00:21:41,000
and also, like, how that immediately might apply to what we're working on, you know, in

339
00:21:41,000 --> 00:21:42,000
our regular work.

340
00:21:42,000 --> 00:21:44,040
How long did they turn around?

341
00:21:44,040 --> 00:21:48,320
It's a one-hour, and we typically have anything from, like, eight to twelve people and

342
00:21:48,320 --> 00:21:53,040
odd, you know, group, you know, I would say maybe, like, four to five of them will live,

343
00:21:53,040 --> 00:21:54,680
like, let's put the paper a little bit.

344
00:21:54,680 --> 00:21:55,680
Yeah.

345
00:21:55,680 --> 00:22:01,880
But I think these are opportunities for people to not just learn, but also lead a discussion

346
00:22:01,880 --> 00:22:05,320
and people get better at it as they go.

347
00:22:05,320 --> 00:22:06,320
Yeah.

348
00:22:06,320 --> 00:22:07,320
Yeah.

349
00:22:07,320 --> 00:22:08,320
Okay.

350
00:22:08,320 --> 00:22:09,320
Well, I'm really looking forward to it.

351
00:22:09,320 --> 00:22:12,120
And, like I said, a lot of people, a lot of other people seem to be looking forward to

352
00:22:12,120 --> 00:22:13,120
it as well.

353
00:22:13,120 --> 00:22:14,120
Yeah.

354
00:22:14,120 --> 00:22:17,840
Also, have you joined us and then walk us to a paper over here of your choice?

355
00:22:17,840 --> 00:22:18,840
Really, yeah, definitely.

356
00:22:18,840 --> 00:22:22,480
Yeah, I think it's very common in, like, grad school and, like, research labs to be,

357
00:22:22,480 --> 00:22:26,880
like, doing this, like, reading the paper is an art and understanding, like, what is

358
00:22:26,880 --> 00:22:30,760
the next, what are the assumptions, what are the achieved, what are the opportunities

359
00:22:30,760 --> 00:22:37,040
from here, like, how can we build on that, what is relevant to us, how hard is it to reproduce,

360
00:22:37,040 --> 00:22:41,160
you know, in our environment, right, that, you know, what are the generalizable lessons,

361
00:22:41,160 --> 00:22:42,160
so to speak.

362
00:22:42,160 --> 00:22:43,160
Yeah.

363
00:22:43,160 --> 00:22:44,160
Those are sort of the questions we are thinking about.

364
00:22:44,160 --> 00:22:45,160
Yeah.

365
00:22:45,160 --> 00:22:46,160
Yeah.

366
00:22:46,160 --> 00:22:50,920
So, kind of going back to your presentation, what are the generalizable lessons from your

367
00:22:50,920 --> 00:22:51,920
presentation?

368
00:22:51,920 --> 00:22:52,920
Sure.

369
00:22:52,920 --> 00:23:00,760
So, one of them is to think deeply about how the model integrates into your product.

370
00:23:00,760 --> 00:23:06,120
Have the discussion ahead of time with people outside, you know, your own group, like, off

371
00:23:06,120 --> 00:23:10,040
data scientists, maybe, you know, product managers, product engineers, like, the kickoff

372
00:23:10,040 --> 00:23:12,360
meetings should probably talk about that.

373
00:23:12,360 --> 00:23:16,040
And at what level are you having that conversation, because there's integration from a perspective

374
00:23:16,040 --> 00:23:20,400
of data, there's integration from a perspective of data sources, APIs.

375
00:23:20,400 --> 00:23:21,400
Yeah.

376
00:23:21,400 --> 00:23:23,800
Generally, at the level of APIs.

377
00:23:23,800 --> 00:23:24,800
Okay.

378
00:23:24,800 --> 00:23:30,960
And it's also a way to sort of get a buy-in from product engineers, build that relationship

379
00:23:30,960 --> 00:23:35,440
so that, you know, think shouldn't look like you're just throwing things or the fence

380
00:23:35,440 --> 00:23:38,040
to the engineer and asking them to, like, productionize it.

381
00:23:38,040 --> 00:23:44,680
I think, for them to understand it in a way that is at a level that they can even provide

382
00:23:44,680 --> 00:23:48,880
an input to and, you know, contribute to improve the design, like, the technical implementation

383
00:23:48,880 --> 00:23:50,560
would be helpful.

384
00:23:50,560 --> 00:23:54,440
And I picked out, like, two different aspects of, like, latency and context sensitivity.

385
00:23:54,440 --> 00:23:55,440
There might be others.

386
00:23:55,440 --> 00:23:57,840
I mean, I picked up two and I got a four quadrant hub.

387
00:23:57,840 --> 00:23:58,840
Yeah.

388
00:23:58,840 --> 00:24:04,360
We can pick, look at, you know, more aspects of it that are generalizable across models.

389
00:24:04,360 --> 00:24:09,800
And, yeah, this, I hope it sort of evolves into a discussion about how we can, you know,

390
00:24:09,800 --> 00:24:16,000
we have ways to talk about model prototypes, you know, how a problem is set up and how

391
00:24:16,000 --> 00:24:20,400
training data is generated, you know, at someone, you know, the model is fit and we persist

392
00:24:20,400 --> 00:24:26,480
the model we have, you know, abstractions about what happens, you know, in training phases.

393
00:24:26,480 --> 00:24:32,800
But abstractions that help us understand how a model is, you know, being served or the

394
00:24:32,800 --> 00:24:38,280
model predictions are being served and, you know, how it's actually executed and implemented

395
00:24:38,280 --> 00:24:39,600
might be helpful.

396
00:24:39,600 --> 00:24:40,600
Mm-hmm.

397
00:24:40,600 --> 00:24:45,800
Yeah, I keep coming back to this idea of design patterns, like, you know, you may be familiar

398
00:24:45,800 --> 00:24:50,400
with the gang of four design patterns book where there's, you know, they've cataloged,

399
00:24:50,400 --> 00:24:54,720
I don't remember the year, but, you know, cataloged a lot of, I've been oriented design

400
00:24:54,720 --> 00:24:55,720
principles.

401
00:24:55,720 --> 00:24:58,280
Like, are we there yet with machine learning or?

402
00:24:58,280 --> 00:24:59,280
Great question.

403
00:24:59,280 --> 00:25:04,600
And I think I probably had a, I tweeted this question a few, maybe, maybe a year ago,

404
00:25:04,600 --> 00:25:09,800
like, what sort of abstractions do we have for, you know, serving machine learning models

405
00:25:09,800 --> 00:25:10,800
in production?

406
00:25:10,800 --> 00:25:11,800
Right.

407
00:25:11,800 --> 00:25:12,800
Right.

408
00:25:12,800 --> 00:25:19,120
I don't know if any that is, that substantially builds on, like, you know, software engineering

409
00:25:19,120 --> 00:25:22,360
patterns, like, or I don't know if anything that the community has agreed on.

410
00:25:22,360 --> 00:25:24,760
I'm sure there are a lot of things that in people's heads.

411
00:25:24,760 --> 00:25:25,760
Yeah.

412
00:25:25,760 --> 00:25:28,520
And a little bit of that I'm talking about today will be about, like, what I, you know,

413
00:25:28,520 --> 00:25:31,400
we've seen and working in our team.

414
00:25:31,400 --> 00:25:34,880
But going back to the Google doc, I think that, again, although it talks about more than

415
00:25:34,880 --> 00:25:39,040
just serving, I think we need more of that in the community, yeah.

416
00:25:39,040 --> 00:25:44,480
To mention that there isn't enough generalizable because things are just too coupled with the,

417
00:25:44,480 --> 00:25:48,360
with the product is probably not, not true.

418
00:25:48,360 --> 00:25:49,360
Hmm.

419
00:25:49,360 --> 00:25:53,760
Yeah, you made an interesting comment early in the conversation about how you have a lot

420
00:25:53,760 --> 00:25:58,040
of these machine learning models in production, but they, you know, at some level, they all

421
00:25:58,040 --> 00:26:01,360
seem like snowflakes, I guess, is what I took from that.

422
00:26:01,360 --> 00:26:05,800
And that, you know, it's been, it's been an effort and a challenge to kind of extract

423
00:26:05,800 --> 00:26:10,480
the general principles from across these different environments, is that kind of what you

424
00:26:10,480 --> 00:26:11,480
were getting at?

425
00:26:11,480 --> 00:26:12,480
Yeah.

426
00:26:12,480 --> 00:26:14,480
Insofar as we're talking about, like, implementation.

427
00:26:14,480 --> 00:26:15,480
Right.

428
00:26:15,480 --> 00:26:16,480
Yeah.

429
00:26:16,480 --> 00:26:17,480
Right.

430
00:26:17,480 --> 00:26:18,720
That's all in different problems, but there might be, like, in terms of, like, software

431
00:26:18,720 --> 00:26:23,960
engineering patterns, they share, like, some of these aspects that they then, in one

432
00:26:23,960 --> 00:26:25,360
of these quadrants.

433
00:26:25,360 --> 00:26:32,720
And are there common implementation slash architectural patterns that, you know, that are kind

434
00:26:32,720 --> 00:26:37,840
of assumptions for you guys that, you know, you're doing across all, at least all new

435
00:26:37,840 --> 00:26:38,840
efforts.

436
00:26:38,840 --> 00:26:42,960
Like, for example, microservices, are you doing microservices, containers, are you doing

437
00:26:42,960 --> 00:26:43,960
containers?

438
00:26:43,960 --> 00:26:49,360
Like, does any of that, where does the kind of evolution of the state of the art and, you

439
00:26:49,360 --> 00:26:54,000
know, software engineering for non-ML stuff intersect with ML stuff?

440
00:26:54,000 --> 00:26:55,000
Good question.

441
00:26:55,000 --> 00:27:01,640
And, well, at the early point, yeah, we have services that are, well, we can call them

442
00:27:01,640 --> 00:27:06,080
microservices, I guess, but, yeah, we have, you know, engineers, product engineers and

443
00:27:06,080 --> 00:27:10,960
mission engineers agree on, you know, certain contracts.

444
00:27:10,960 --> 00:27:14,360
And so that is, like, table stakes.

445
00:27:14,360 --> 00:27:19,480
There are questions about, like, what can we cache, what can we not cache?

446
00:27:19,480 --> 00:27:23,920
There are questions about, well, yeah, how long can we cache something, you know, what

447
00:27:23,920 --> 00:27:30,320
sort of, what sort of data stores we might use, and what is configurable, what is not, what

448
00:27:30,320 --> 00:27:34,120
is, yeah, what is an experiment versus what is not.

449
00:27:34,120 --> 00:27:39,920
Most things are experiments, how much do we expect to iterate on this feature before,

450
00:27:39,920 --> 00:27:45,400
you know, we think we're done for a while, which is, I think, for a company like Instacart,

451
00:27:45,400 --> 00:27:48,160
where there are a lot of different places we can invest in.

452
00:27:48,160 --> 00:27:51,800
It might be, like, quite a few months before we come back to, you know, we too of something

453
00:27:51,800 --> 00:27:52,800
that we built.

454
00:27:52,800 --> 00:27:53,800
Yeah.

455
00:27:53,800 --> 00:27:57,280
Because, you know, we're still exploring the space of areas where machine learning can

456
00:27:57,280 --> 00:28:01,200
help, and, you know, where help the most, I guess, yeah.

457
00:28:01,200 --> 00:28:06,160
But do you, to a point about, like, beyond software editing, what is new, yeah, I don't

458
00:28:06,160 --> 00:28:07,640
think I have an answer right now.

459
00:28:07,640 --> 00:28:08,640
Okay.

460
00:28:08,640 --> 00:28:09,640
Okay.

461
00:28:09,640 --> 00:28:16,000
To follow up on your last comment on just the opportunity prioritization, at Instacart

462
00:28:16,000 --> 00:28:23,000
are you generally taking an approach of, like, trying to take on, you know, big moonshot

463
00:28:23,000 --> 00:28:28,560
types of problems and, you know, make a few small big bets to kind of, you know, ensure

464
00:28:28,560 --> 00:28:32,360
that your resources are focused on, you know, these things that could have outsized

465
00:28:32,360 --> 00:28:36,840
an impact, or is it more, you know, we're going to try to, you know, we're going to try

466
00:28:36,840 --> 00:28:43,320
to touch, you know, broadly, you know, in, you know, high impact, but more concentrated

467
00:28:43,320 --> 00:28:48,800
ways to kind of spread that impact around, you know, the systems in the various business

468
00:28:48,800 --> 00:28:49,800
teams and processes.

469
00:28:49,800 --> 00:28:50,800
Yeah.

470
00:28:50,800 --> 00:28:57,480
I think there's probably the, there's a top down and bottom up in terms of what sort of

471
00:28:57,480 --> 00:29:03,080
projects get worked on and explored, you know, within, you know, within, say, data

472
00:29:03,080 --> 00:29:06,880
science for, just to stay in data science for a moment.

473
00:29:06,880 --> 00:29:11,200
So we have company goals about, like, what are we, quarterly goals, long-term goals,

474
00:29:11,200 --> 00:29:16,800
translate the company goals, which typically talk, you know, are aligned to some metrics,

475
00:29:16,800 --> 00:29:21,280
and, you know, different teams should know how they can move certain metrics.

476
00:29:21,280 --> 00:29:27,280
And, you know, there is, and then we think about, like, what sort of a data science effort

477
00:29:27,280 --> 00:29:28,440
could serve that metric.

478
00:29:28,440 --> 00:29:29,760
So there's that.

479
00:29:29,760 --> 00:29:35,840
What I call the bottom up is, well, you know, in the end, we are still, we are e-commerce,

480
00:29:35,840 --> 00:29:40,040
but also last mile, you know, logistics operation.

481
00:29:40,040 --> 00:29:44,840
So naturally with that domain, some problems just are there.

482
00:29:44,840 --> 00:29:49,120
Like if you have e-commerce operation, you have a search engine, and if you have a search

483
00:29:49,120 --> 00:29:52,800
engine, you're probably working on, like, you know, spell correction and autocomplete

484
00:29:52,800 --> 00:29:57,880
and, you know, search ranking, matching, business understanding, the queries better,

485
00:29:57,880 --> 00:29:59,880
and recommendation systems and so on.

486
00:29:59,880 --> 00:30:07,760
So we already know, given the kind of data we have, and we collect, what sort of problems

487
00:30:07,760 --> 00:30:13,360
have been addressed, like, what sort of product features have been useful, in that sense.

488
00:30:13,360 --> 00:30:18,760
So between the top down and bottom up, I think we generally find, you know, tactically projects

489
00:30:18,760 --> 00:30:23,400
to work on, you know, given, you know, current goals, yeah.

490
00:30:23,400 --> 00:30:29,720
And oftentimes, you know, we hit, like, dimension utilities, soon enough for plateau,

491
00:30:29,720 --> 00:30:34,520
soon enough, maybe in the first version, we might wait for, like, a while before we come

492
00:30:34,520 --> 00:30:35,520
back to it.

493
00:30:35,520 --> 00:30:36,520
Okay.

494
00:30:36,520 --> 00:30:37,520
Interesting.

495
00:30:37,520 --> 00:30:41,280
Well, I think you've got a presentation to get ready for, but I really appreciate

496
00:30:41,280 --> 00:30:44,280
you taking the time to chat with us, and I'm looking forward to your talk.

497
00:30:44,280 --> 00:30:45,280
Sure.

498
00:30:45,280 --> 00:30:46,280
Thank you, Sam.

499
00:30:46,280 --> 00:30:47,280
Thanks, sure.

500
00:30:47,280 --> 00:30:53,200
All right, everyone, that's our show for today.

501
00:30:53,200 --> 00:30:58,480
Thanks so much for listening, and for your continued support of this podcast.

502
00:30:58,480 --> 00:31:03,480
For the notes for this episode, to ask any questions, or to let us know how you like

503
00:31:03,480 --> 00:31:11,320
the show, leave a comment on the show notes page at twimmelai.com slash talk slash 39.

504
00:31:11,320 --> 00:31:16,120
Thanks again to our sponsor for the Wrangle Conference series, Cloud era, to learn more

505
00:31:16,120 --> 00:31:21,480
about Cloud era and the company's data science workbench family of products, visit them at

506
00:31:21,480 --> 00:31:26,320
cloudera.com, and be sure to let them know how much you appreciate their support of the

507
00:31:26,320 --> 00:31:30,160
podcast by tweeting to them at at cloud era.

508
00:31:30,160 --> 00:31:34,360
If you're interested in joining our first Twimmel online meetup, where we'll discuss

509
00:31:34,360 --> 00:31:39,400
Apple's recent research paper on generative adversarial networks, you can register for

510
00:31:39,400 --> 00:31:42,600
that at twimmelai.com slash meetup.

511
00:31:42,600 --> 00:31:49,320
And don't forget to sign up for our email newsletter at twimmelai.com slash newsletter.

512
00:31:49,320 --> 00:32:00,800
Thanks again for listening, and catch you next time.

