Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Are you looking forward to the role AI will play in your life or in your children's lives?
Are you afraid of what's to come and the changes AI will bring?
Or maybe you're skeptical and don't think we'll ever really achieve enough with AI
to make a difference?
In any case, if you're a Twimal listener, you probably have an opinion on the role AI
will play in our lives and we want to hear your take.
Sharing your thoughts takes two minutes, can be done from anywhere and qualifies you to
win some great prizes.
So hit pause and jump on over to twimbleai.com slash myai right now to share or learn more.
In this episode, the third in our Black and AI series, I speak with Iyana Howard, chair
of the interactive school of computing at Georgia Tech.
Iyana joined me for a lively discussion about her work in the field of human robot interaction.
In our discussion, we dig deep into a couple of major areas she's active in that have significant
implications for the way we design and use artificial intelligence, namely, pediatric
robotics and human robot trust.
I found the latter bit particularly fascinating and Iyana provides a nice overview of a few
of her experiments, including a simulation of an emergency situation where, well, I don't
want to spoil it, but let's just say that as the actual intelligent beings, we really
need to make some better decisions.
This was a really fun interview and I'm happy to share it with you.
Let's get to it.
All right, everyone, I am on the line with Iyana Howard. Iyana is chair of the school of
interactive computing in the college of computing at Georgia Tech.
Iyana, welcome to this weekend machine learning and AI.
Thank you. I appreciate the invite. I think we're going to have a beautiful conversation.
I am really looking forward to it and why don't we get it kicked off by having you tell
us a little bit about your background and how you got involved and interested in artificial
intelligence?
I was an old early adopter of AI.
What does that mean?
Well, because it's cool. Everyone's like, oh, I do AI. I'm doing machine learning.
It's like, no, no, no. My rusty faces where we had to draw stuff and try to figure out
how to put it, had a neural network in it.
Nice.
I've been doing this since 1994. I think I co-ed up my first neural network.
Oh, wow.
So that's what I'm saying. I'm old school. It wasn't a thing. It was just a way to make
my robots more intelligent.
My background, I consider myself a robotics person. Robotics and AI and embodied AI is really
what I do and started in this very, very early on. I knew I wanted to do robotics since
like middle school. Of course, it was a different robotics then. It was basically remote control
cars that you could figure out how to program. But then that evolved and when I was working
on my PhD thesis, I was also working at JPL, NASA JPL and I was coding up. I'm trying
to figure out how do I make rovers more intelligent? So that was the objective and I thought that
the best way to do it was try to figure out how people think and behave and process and
try to encapsulate that and put it into my robot brain. And so that's why I was doing
AI. It wasn't necessarily AI-ish. It was these algorithms allowed me to take human data,
human expertise and put it into a form that my robots could understand. So yeah, like I
always tell my students, I'm cool now, right? With enough time, hopefully, we'll all
be cool. Nice. So you mentioned kind of capturing human experience and kind of using,
you know, encoding that in your robots. You also mentioned neural nets. Like when I think
of the way we captured human experience and put them into systems at that time, a lot
of it was like expert systems and kind of the preview, you know, that origin, that wave
of AI from like the 80s. Is that the kind of thing you were doing or were you doing the
neural net stuff in that context as well? I was doing so my thesis, my approach was I
had to figure out how do I get a robot manipulator. So robot hands and arms to grab objects
that be formed. So the thing was is we wanted to bring robots into the hospital and we wanted
them to do things like pick up pillows and sheets and things like that. And these objects
weren't, they weren't fixed, right? They put the apply force and they changed the shape.
And there was really no way of mathematically modeling that. And so my thing was let's
learn. And so what I did is early, early human demonstration is I had sensors on these
manipulators and I'd have people grab objects with them. And I would kind of like that data
and I look and see, you know, how much force did they have apply? And then I would model
the object deformation, but the input was about the human. Okay, this is how much and
lift it up. Oh, it fell. Okay, this is how much they applied and it was solved. So that's
how I got the data and that data was then used to train on neural network. So that the robot
then could visually see this object. It would map that information in terms of a model
that I created. And then as it would grab, it would visually match that deformation
to the model it had stored in a neural network and say, okay, this is the force I think I
should have given the shape of the object that I learned before with previous knowledge.
Oh, interesting. And how much data did you have to collect? Do you remember?
Yeah, so this wasn't like the deep learning stuff. This was, I mean, I literally had ten
objects. This was this was a big thing. The fact that I could even do ten objects was
like amazing. So yeah, it's not the neural networks of today. And so, yeah, limited data
set. I mean, then though, the number of observations you had to do was a lot, considering it was
only ten objects. I remember we would be in the lab and I'd be like, okay, let's run
through another. Let's lift it up. So that felt like a lot of data then. I mean, it wasn't,
but at the time, it was a lot of data. Right, right, right. If we only knew back then.
I know. So fast forward, fast forward some years, what are you working on nowadays?
So now I'm working on two, and I would call them two buckets that are really interesting
to me. So one is looking at pediatric robotics. How do you create robot coaches, therapists
that can work with children with special needs in the home to do exercise? And why I really
interest me is because we have to do things like, how does a robot adapt to different kids?
So every child's unique. How does it you bring a robot in and it uniquely identifies the
needs of that child in fairly real time? You put emotions on a robot and emotions is
like, why do you need that? Well, emotions help with the bonding. So a child, you want
them to do something that's hard. So how do you get them to do something they may not
want to do? Emotions allows the child to connect with the robot. So then the robot says
it and the child just wants to please the robot because this is, this is, it's friend.
This is his or her friend. So that's really interesting because I get to play with all
of these things and the kind of science space and the psychology space to get the robot
to have this bond and guide. And then we, of course, use the classical things like, you
know, computer vision to extract what the child is doing in terms of their body movements
and eye gaze and facial expressions to see, are we getting the right emotional response
from the child? So some of the classical things are incorporated in that. So that's, I
would say, half of my life. And then the other is this work I'm looking at and involved
in with respect to trust in robots or trusted and embodied agents trust in AI. We have some
interesting experiments where we have evidence that people over trust robots. Yeah. It was
a scenario and it's like, I talk about it all the time and it's, it's one of these scenarios
where your own hypothesis were wrong and you're like, oh my gosh, my hypothesis was wrong.
This is interesting. So this was research that you and your group did? Yes. So this is
research I did with, I had a colleague at the time who was at GTI, which is our research
ramen in my students. And where I started off, of course, was in, with robots, you start
off in simulation because it's really hard to deploy real robots. So you all start off
in simulation and we were doing an emergency evacuation. So imagine you're in a building
and the alarm goes off and you need to evacuate. So as you're evacuating, imagine that a robot
comes and you know, shows you the directions of how to get out because it's chaos and
all these things. And so we were, that was the scenario and what we wanted to do was understand
if a robot makes mistakes, what would the person do with this regard? So we weren't even
looking at, looking at, you know, introducing mistakes and how optimal does a robot have
to be for people to follow? So that was, that was the real focus. That's how it started.
And what we found out was that very early on, the robots would make mistakes and the people
would still follow guidance of the robot. So this is kind of, this was interesting.
Can you give an example of the kind of mistake that the robot would make and that would
be followed? So I'm in a, again, this is, we started in simulation. So I'm in a simulated
building, you know, we have the fire, like a virtual reality game, a robot appears, you
know, follow me and you follow and then the robot goes and bumps into a wall and then
pops up and bumps into the wall again. And then bumps into the wall again, right? So,
and you have a choice. So the instructions are, you can find your own exit or you can
stay with the robot. And so you would think, oh, person would like, okay, the robot's
broken. Let me go find the person. Right. Right. We would not expect the person to just
stay there and just touch the robot, fascinated by this robot that was clearly not doing something
that it was supposed to be. Right. And again, it was more accidents. Our, our original
objective was not this trust objective. So we started to push that like, okay, there's
something strange about this. We're not quite sure, you know, what happens if we expose
you to a broken robot or a robot that has mistakes before you go into the building and then
do you? And so we just kept pushing it and kept pushing it. And our final experiment,
which was in, we got to hardware through this transition from simulation to hardware,
the guidance and our, our experiment that just baffled us was we had a abandoned building
that was off lab. This was one of these, these participants that we had, you know, fire
marshal was involved and things like that. We had the robot guide the person through
the building to an office room. And in the office, they had to close the door and, you
know, there was an article about some survey robot navigation and fill it out. So we
tried to prompt the user to think that this was the experiment, right? This survey and
things like that. And while the person was in the room, we filled the building up with
smoke. So smoke to simulate a fire alarm. And then we set off the fire alarm. And so what
happens is the door was closed. So fire alarm goes off like typical human behavior. You
get up and you walk to the door, you know, evacuate. But when you open the door, just
what you see, you see smoke and you see fire alarms and you see, yeah, right? And so
you, you, you, okay, what are you supposed to do? You're definitely going to find an exit.
I'm going to find an exit. What we did is we introduced the robot and intentionally the
robot was guiding you to an exit where you did not come in. So we intentionally did that.
So you come from different entrants, for example, entrance exit. And we guide you to a different
one. And we wanted to see, you know, what would you do? Now it's like it's a for real thing.
It's not simulation. It's right. It's real. What would you do? People follow the guidance
of robot. Meaning through the, the banging into the wall thing, did you, did you incorporate
that? Yeah. So we incorporated robots that would turn in place. They wouldn't do anything.
We incorporated robots that would guide you into dark rooms. Like there was no lights
with furniture blocking. And you would see people moving the furniture to go into these
dark rooms. We introduced mistakes, even when they entered the building. Like when you
came in, let's have the robot break down and do things like, you know, circle with places
up with the controls. And then later bring that same robot and see, you know, now you have
this notion, this robot doesn't work, you know, here's the robot again. What are you going
to do? Wow. And time and time and time again, it broke our hypothesis. Our
hypothesis would be, at some point, trust would be broken. Right. Right. And it was not
which, which actually surprised us. And if you look at the data, there was some suggestions
of why and we're teasing that out. So some suggestions were, well, it's a robot. It
can fix itself, right? Like, yeah, I knew it was broken before, but it's a robot. It's
a program, right? So of course, it was fixed. Or the robot had more information than
I probably did. So it kind of knew better. So things like this where people were following
this guidance and they were explaining why they should, like after the fact, explaining
why it was perfectly logical for them to do that. And then I look at things like, you
know, Tesla and the autonomous vehicles and crashes. And people are like, oh, how did
you run into? And I'm like, no, it now makes perfect sense. Interesting. So did you make
any attempts to baseline this against human human behavior, meaning, you know, you've
got a human that's playing the role of the robot in these scenarios. And that is, you
know, either making mistakes or clearly lost or something like that and try to determine
whether the results you saw were just, you know, based on kind of authority figures and
us, you know, blindly following authority figures, whether or not we deem them competent
objectively or, you know, is it specific to robots? No, we think it's the fact that we
as humans place these robots in a higher state. That's, that's what we think it is. Because
we did, so the human human in simulation, not in the real world. And, and we think that
is this aspect of like the robot knows better. The robot isn't expert in this, in this
scenario. And so what did you see when you did human human in simulation?
It was, it was the same. It was, well, so interesting enough, we did, again, the human human
is teleoperating. So it's, it's a, there's a human controller kind of thing. We also did
static signs like trying to compare robots and signs. So it was more of a comparison than
anything else. And we found in like the human aspect, peer pressure was more effective.
So if you had more than, so if you had an influencer that was very dominant, they usually can
influence the person. If not, you had to have like additional people to influence the
person. And so like if one person is like, I think it's over there, right versus no, it's
over there. Right. And again, that's exuding, I guess authority. And so there, I think it's
about three, like you need three people to say, I'm uncertain, but I think we, it's over
there for it to be fact in that regard. So we, we do think it's this, and I won't say
necessarily authority, but this fact of, this is the expert in this situation. And yet
that this is a totally human situation. And that's really what is, is a little bit disconcerting.
So we did another study also in this trust where we, in the therapy related to therapy,
where we looked at comparison between a robot therapist and a human therapist and looked
at aspects of trust. And so would you follow the guidance? And there we didn't make, we
didn't do the mistakes. You just wanted to get a baseline of this, would you talk about
authority, but basically feelings of, you know, this trust and following. And interesting
enough in that scenario, for the robot, they literally would self, you know, in terms
of our server results, they claimed that they trusted the robot. In the human scenario,
they said, trust has nothing to do with it. It was the same task. I mean, literally,
we had the robot and the human do exactly the same behaviors, because we didn't want
to put in any nuances like, oh, the human smiled instead of, you know, instance one versus
instance two. So we basically scripted the behavior of the robot and the therapist exactly
the same. And what was the, the task here? It was a therapy task. So they basically had
to follow the guidance of moving their arms in a certain configuration. So very, very,
not a, not a very strenuous task, just very simple exercise. So physical therapy.
Physical therapy. Right. And so the humans, when they were being guided by another human,
it was, you know, just this thing that they, you know, did, you know, whereas when they
were, there was a robot involved, it kind of evoked this, you know, question of whether
there was trust involved in the relationship. Is that the idea?
Correct. Correct. Correct. They weren't the same, even though the outcomes, because
we measure the outcomes, like, what did you actually do? And the, the people, participants
followed the exact same, like, trajectory and rules. And there was very little variation
in terms of even, you know, how well they did the task, but yet their perception of the
agent different. And even that, we even, we even modeled the exact same speech. Like,
here is what you say, like exactly the same with robot and the human scripted. And yet there
was a difference when it was the person. Now that's interesting, but I'm not sure what
exactly it tells you. What, what conclusions did you draw from that?
So the conclusions we drew is that because there's this aspect of, it goes with bonding,
but this aspect of trust, what that means is I think that when you have these scenarios
with these humans and these robots, that if a robot says something or does something or
tells you some information, you have this assumption that the robot must be correct. Because
I trust the robot is going to do the right thing. I trust the programmers that are programming
the robot to do the right thing. Whereas with a human, it's just a, and human's question
of the humans all the time, right? And I think it's because as soon as a human does something
wrong, you'll probably be like, oh, you're wrong. I'm not going to trust you. But I think because
based on a previous work, if a robot does something wrong, because you started off with this condition
of trust, it doesn't, it doesn't break. It's interesting. You mentioned Tesla in passing. And
you know, in this context, I can't help to think about how, you know, a lot of us are notorious
backseat drivers, like we wouldn't get in a car and just like totally see control over to someone
else without, you know, be constantly thinking about what they should be doing better. But yet,
so many of us would sit in a Tesla that tells us, you know, pay attention, keep your hands on the wheel.
And so I don't know. Right. And then, you know, there's a recent accident where a Tesla like slammed
into a fire truck and it was, I think it was at 65 miles an hour. And I don't, I didn't see in
the article whether the driver, you know, was claimed to, you know, being just, you know, was not
distracted or, you know, was like deeply engaged in something else or, you know, in this context,
you almost wonder if, you know, someone's thinking, oh, the car's going to handle it or something.
I don't know. I think I'll do a last minute maneuver. Right. Right. Is autonomous vehicles
an area that you're, that you're getting involved in and applying some of this stuff? We are. We are.
We've done our first study where we are looking at, again, you always have to have a baseline. So
we're at the baseline. What is your, if you know that there's another driver on the road that's
human and that human makes a mistake. So what is the baseline? No mistake and then the human
makes a mistake. What is your, what is your driving behavior? Does it change? And then do the same
thing with a self-driving car to see what happens. So that's our baseline and we just collected our,
I can't tell you the secret sauce yet. But over trust is there. Let's just put it that way.
And so we're going to push that. And ultimately, it's like, well, why are you doing? So yeah,
you prove that it's over trust. Ultimately, what I want to do with this research and with my lab
is then come up with methods to mitigate it. Because the fact is, is that we are going to be dependent
on these AI systems. Right. We're going to be reliant on them and we're going to trust them to do
what they're supposed to do. And I think as your bodice is, we also need to ensure that if there's
a scenario where we're uncertain, for example, you know, I can look at the data and be like, oh, yeah,
we're 85 percent. Oh, that's good enough. Give the answer, right? I know this. I know how accurate
my stuff is or how certain are, you know, oh, this data said I really haven't seen, but it's close
to math and I come up with a metric of what's close to math. I think that information would be valuable.
If I'm in a self-driving car, for example, and I see a scenario, I should be able to get feedback.
Like, there's something in front of me. I don't know what it is, right? It's not the driver
now. So then the driver could be like, oh, well, let me pay attention. Okay, because there's
something's wrong. You're giving me information. That's not that's a little bit different.
What are those things that we can do as roboticists to basically, I would say, you know, kind of jump
start a person to think a little bit about it. And I think it's our responsibility to do that.
But then also ensure that the humans also still follow the directions when they want to. I don't want,
you know, my, like, with emergency evacuation. I don't want the robot to come. You're like, oh,
no, no, no, no, I'm not going to follow you. And it's like, no, no, no, no, no, we want you to
leave. This is not the time to say no. It's interesting in that it's almost like there's this
compounding effect where, you know, you've got this over trust issue. But the things that we're
over trusting are, you know, increasingly probabilistic where we've traditionally associated
computers with, you know, deterministic correctness, right? Right. Right. And, you know, just the way
you're, you know, describing, I'm kind of thinking through like all different kinds of ways that
one might convey, you know, this, you know, probabilistic notions via, you know, robots and systems
like this. Like what, you know, what's the confused face on your Tesla or some other robot?
Right. What does that look like? Right. And how far have you gotten with that? Have you come up
with directions on in that, or have you come up with any initial research into, you know,
directions on conveying these kinds of probabilistic outcomes or we have, but it's not. And so with
anything, it's not statistically significant, which basically means we have more work. So one,
we've found that the way that you provide information, there's a timing constraint to it.
When you provide that information, it's important because we filter. So depending on the timing,
you'll filter out the information. Figure out what that timing is. We still, you know, we've
probably, and again, we're at the stage of, okay, here's the event, you know, what happens if we
provide the information right before, like at the point where they can still make a decision,
but maybe it's a split second type of decision, what happens? Okay. What happens if we put it so that
they have to think about it? And so we're playing with that to see maybe, because I can give you the
information, but I have to also give you the urgency to do something and also make sure that you
can do something. So if you think about the Tesla, maybe it's a blocky, you know, a way up ahead,
but I'm only going one mile per hour, right? Probably don't need to tell my user, I'm confused.
It may not make sense, but maybe if I'm, again, I'm hazy, I don't know what's going on,
and I have my map, and I know that, you know, one mile ahead is a really bad intersection,
because I've crowdsourced this data, and everyone says it's a really bad intersection,
and my sensors aren't, my sensors aren't certain at this point. Okay, I think the timing is about
right, you know, at this point. And so it's not like a magic bullet that says, this is it,
I think it's scenario dependent, I think it's timing is really, really important, and then,
of course, how you provide that information. You can't say, I'm 80% accurate, actually, means,
and we've done some studies on, again, related to the medical, like, when doctors and clinicians
get information, saying something like 80% is not as effective as just saying, I'm wrong, right?
There's even ways of providing that data, so that user will understand what that means.
Do you think the creators of these kinds of systems will be, you know, open to conveying
this uncertainty? Like, you know, is there a sense where, you know, you convey an uncertainty,
and the user might think that the system is broken as opposed to understanding that it's
inherently probabilistic, and thus the, you know, the system makers won't want to be able to
convey uncertainty. Have you explored that at all? Yeah, well, so what's, I'm very positive
about though, is that there's this whole push now on transparency and AI algorithms.
And AI and robotics are so tightly linked that it basically affects us as roboticists as well.
And this aspect of, you know, especially with respect to the deep learning algorithms, you know,
this concept of, you have a black box and you sort of know what goes in, but you sure don't know
what's going on inside. And how do you make that more transparent? And so I think that there is
now consensus that algorithms should be transparent. I think that there's still disagreement on
how transparent and how that information should be provided to the user. That's still an ongoing
debate. Because again, there's this balance. You need to still optimize the benefit,
but you want to minimize the risk. And so it's a balancing act. Because if you're fully transparent,
they may not, may, may or may not listen. I always say if you ever install software at any
point in your life, there's literally what three or four pages of text, fully transparent, right?
I agree. Like, no, right? It's only after the fact that like, I didn't realize I just signed my
life away. That was line number 10. So that's a full transparent, but that's not what we're trying
to get to, right? And so I think there's this balance. We have to figure this out. It's not about
being fully transparent is about providing the information that we need at the time that it's needed.
That's really the underlying issue. And does this, you mentioned earlier, we hadn't had a chance to
dive into it. The work that you are doing with pediatric robotics, does the, how does the trust
play into the pediatric robotics scenario? Yeah. So pediatric robotics, it relies on a bonding.
So having an established relationship, which basically means you have this aspect of trust.
And it's important because we're working with kids where we're doing something that might not
be very comfortable because we're doing physical therapy. We're doing exercise. And so it's,
it could get uncomfortable. We may want them to do it for longer than they want to. And so the
child has to trust that this robot has their best interests in heart and is, quote, unquote,
their friend. So it was really, really, really important. And in fact, we have shied away from
introducing the mistakes. Even though we're like, we're like, oh, let's see what happens. Only because
we're talking about this vulnerable population. And so you make mistakes. And if you have this
bonding of trust, you might actually, I mean, it's physical therapy. You might fundamentally
change the way they think about what's right or what's wrong. And so we've shied away from that
because we intentionally want this bonding. We intentionally want this trust to have an
optimal outcome. And so we can't play around with at least with children with special needs. Now
with adults, we're doing, like I talked about the human and the robot. For adults, we can do it.
Yeah. But for kids, we're very, very conscious of this population. And so when we're doing that,
we actually are going out to hopefully improve outcomes. That's our ultimate goal. And to improve
outcomes, you have to have bonding. You have to have this relationship building. You have to have
this, quote, unquote, friendship, which of course equals this aspects of trust.
And the is pediatric robotics to what degree is this a thing? I guess what degree are we there? Are
there systems that are commercial or production systems out that are doing this? Or is this more
in the research domain? I would say this is more in, so it's a combination. It's a combination
of in the research, but in the clinical research domain. And so for places that do clinical research,
like say hospitals, they are bringing in these platforms to look at outcomes. But it's not
something where say a local clinic that is just providing services, not doing research,
is going to bring into their home. So it's in that in between translational stage of proving.
And mainly is because it's well, the hardware itself is still difficult to use. But it's also
proving out these outcomes and proving out the interventions and basically saying, you know,
for this target population, this is the intervention that works. And so it's more becomes a prescription.
It's like that's trying to figure that out. And what's the research frontier in that domain?
What are the main problems you're trying to solve at this point?
So the main problem is long-term engagement and adaptation. So again, I like our longest
days or eight weeks, which is not long-term. But imagine you have this robot in the home with
this child for years. I would even be a year. I'd be happy with even eight years at this point.
So how do you ensure one that the system can adapt to the needs of the child? Because the child
is going to grow. They're going to improve on some measures of these outcomes. Also, the relationship
as well. How does the robot identify the progression of the child, even in terms of the emotional
state? They become more confident because they are getting better and having more outcomes.
Does the robot have to change its behavior because of that? And then the personalization
which is I bring in the same robot into a home. I bring it into home one. I bring it home to two.
I bring it into home three. And each child is different. How do you deal with that? The nuances
of the child, which also leads to this long-term adaptation because you have to be able to adapt
in that moment as your calibration routine.
And what are some of the research results that you've seen? I guess I'm curious, for examples of
things that you publish in this area. So our outcomes, so believe it or not, most of the stuff
we now publish is in the clinical literature. So we're at the outcomes state. And so we've shown
improvements in things like range of motion in movement time, so how fast you move. So we've
shown outcomes with children. Our primary target demographic has been children with cerebral
palsy. So we have shown improved outcomes. On the, I would say, more of the technical techy side,
we published, like the stuff that we published, like now we're done. We're like, we're done with
that. A long-term adaptation is really linked to the kids. It's directly linked to the kids.
It's about collecting the data and things like that. So the, I would say, the technology
infrastructure was things like, how do you create an expert system based on a knowledge base of
therapists interacting with kids? So that's how we started our initial training as we looked at
therapist child interaction and looked at how do they interact with the child? What kind of information
did they provide in terms of both verbal as well as gestural feedback and took that and started
with an initial system that we can then have the robot extract. Looking at correlating facial
expressions to an emotional state, you know, what does that look like? How do is that information
get fed to the robot so that the robot can then provide the right emotions, whether it's happy or
frustrated and things like that? So that was all of the work that then tied into more of these
pilot slash, I won't even call them clinical, but pilot studies with clinical collapse.
Okay. And so these robots that you're using in this scenario are these, you know, humanoid robots
or they arms or something different? Yeah, so they're humanoid robots. And I mean, I roughly say
that as humanoid, we don't use the lower, like we don't do... They're probably not a base or
something like that. So they have legs, but in all of our experiments, it's about the arm movement
and we just walk. So in theory, the robot doesn't have to have legs. So, but it does have to have
arms because we have to do the gesturing. So, for example, when a child needs more guidance on
a proper form, the robot has to use the arms to basically explain what the child should be doing.
So we have to have the upper arms to show that movement so the child can mimic. But that's really
the only requirement. Of course, the head, because we need to express emotions somehow. So, you know,
head turn and things like that. So, yeah, humanoid, they're not that. So, we use the now and the
Darwin. So, in terms of height wise, that's like 18 to the now's a little bit taller,
two-ish feet. So, they're small robots. So, they're not human size.
Okay. And so, these are... Their role is primarily as
exemplars for the children to follow as opposed to, you know, I recall from physical therapy being
physically manipulated to the project pain. Correct. Correct. So, yeah. So, what we do is
non-contact we have. So, contact-based rehabilitation would be, I grab your arm and I move it.
So, we do non-contact. And it's mainly... Well, there's a couple of reasons. One is because we want
kids to improve based on their own... Based on their own kind of intrinsic motivation, pushing
themselves. It's just like with... If you think about sports, like the only way you get better at
sports is practice, practice, practice, practice. And you get better. And yeah, you can stop
one and say an exoskeleton to a baseball player and they will perform very well, but as soon as
you remove the exoskeleton, may or may not perform as well. And so, our focus is on non-contact.
It takes longer time and longer term to get the improvements, but then they are their own and
they're retained. Got it. Super interesting. You also mentioned early on that your... Well,
one of the terms that you mentioned in passing was embodied AI. And I've had some... A few
conversations with folks about kind of the role, the relationship between AI and embodiment. And
I'm curious with all the work that you're doing in this area, what your perspective is.
And maybe the background is... One of the comments that was made was that really we'll never get
to true AI without embodiment because it is so inherent to what intelligence means for us as
humans. And I'm curious if that's your perspective as well. Interesting. That must have been a
roboticist, that's it. It wasn't a roboticist, that's it. Thank you. Yeah, that sounds like something
we would talk about in our closed room. So, I'm not going to talk about true intelligence.
That's like that's going down a rabbit hole. But the kind of the embodied AI versus AI,
there is a large overlap. So that's one thing as a fact. Now, maybe not say 15 years ago,
maybe it was a little bit more removed, but now there's a large overlap. And there are some
things that are unique about being embodied in the physical world. But I think, and I'm going to
put on my other hat, I think when you think about AI in general, and if it's a true agent,
like an agent that it's exploring a virtual world, I think you have some of the same characteristics
of an embodied AI agent, but not all of them. So if you, for example, decide, we'll use a chat bot,
a chat bot, but put the chat bot in a virtual reality environment that has to interact with people.
And so therefore, you might chat with someone, but then they look really weird or they might turn
their backs on you. And so you then have to, you know, so then you have to learn how to interact.
And so you're using physical motions. If you do the virtual reality environment correctly,
you know, you're realizing that, you know, if I say something and they bring me a cup,
I have to do something with the cup. And so maybe I shouldn't say that I'm thirsty if I'm really not.
So I think you can learn a lot in the virtual world that's similar to being embodied.
But then there's the uniqueness of having a physical agent in our real world because then the, like they say,
the world kicks you in the face, right? So you're like, I have the perfect intelligent algorithm.
And it's perfect in the virtual world. And you come into the real world and you realize that,
you know, the building that you have the beautiful, beautiful map of is actually not correct
because a human created that map. And of course, they might have taken shortcuts.
And so now you're in the physical world and you realize and therefore you then have to think
about alternatives because your map is 100% guaranteed not to be correct. And so then the way
you think about intelligence and adaptation becomes more about problem solving to get to a solution.
So it's a different way of thinking about intelligence versus just taking all the data and coming
up with a conclusion. But there's a large, large overlap between the two. And I hate to say that,
you know, literally 15 years ago, I'd be like, oh, no, no. Robotics is special, right?
I think it's like, no, there's so much overlap now because the computing has become so powerful
in terms of the computational aspects and the brain that we can now take these AI algorithms
and put them on a robotic platform. Because we can do that, we can now take advantage of some of
these more powerful AI algorithms and then incorporate these differences of being in the real world.
Super interesting. So what's next for you given all the things that you
have talked about and shared with us? What is the kind of future directions for you and your work?
So one is, of course, the pediatrics. And we're moving into the smaller space. We're now looking at
infants, which is actually unique because they don't actually respond in the same way as kids.
You know, they're nonverbal for the most part except that they do have emotions. So we're working,
we're pushing more into that space, younger and younger. And then on the trust aspects,
it's this one more study is about looking at the parameters of trust.
Is trust tied to things like education? Is it tied to economics? Is it tied to gender? Are there
certain things that we can start modeling about trust? Like, oh, this person here, if I get your
demographics, I can maybe more identify that you're more susceptible or not. Kind of looking at
that as ways of then being able to mitigate. Like, if I have someone, I'm like, oh, guess what?
All engineers will never trust. Then it doesn't make sense for me to provide any type of intervention
for trust. But if I'm like, oh, this type of demographic, like maybe teenagers of the age between
16 and 20, they will always trust, you know, if I can identify that, then my intervention methods
might be slightly different. So pushing that a lot more, pushing it in the healthcare domain,
but also in this autonomous, and I don't say autonomous vehicle, but these autonomous robots that
are on the road, that are on the road before we even realize, I think that we need to start
looking at this aspect of trust. And only I say that mainly because if anything really bad happens,
it would totally destroy the community. And so I think we need to get in front of it before it,
before it gets to that point. Right. Right. Well, I want to thank you so much for taking the time out
to chat with me. Great conversation as you predicted. And I really enjoyed learning about what
you're up to. Oh, no, thank you. Thank you. This was a beautiful conversation. I enjoyed it.
All right, everyone, that's our show for today. Remember, we want to hear your thoughts on
personal AI. If you were too excited about the interview to hit pause before,
now's your chance to head on over to Twomlai.com slash my AI to talk back to us.
For more information on Ayanna Howard or any of the topics covered in this episode,
head on over to twomlai.com slash talk slash 110. Thanks so much for listening and catch you next time.
