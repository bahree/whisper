1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,720
I'm your host Sam Charrington.

4
00:00:23,720 --> 00:00:28,520
A big thanks to everyone who participated in last week's Twimble Online Meetup, and

5
00:00:28,520 --> 00:00:31,280
it's Kevin T from Sigopt for presenting.

6
00:00:31,280 --> 00:00:35,520
You can find the slides for his presentation in the Meetup Slack channel, as well as in

7
00:00:35,520 --> 00:00:37,440
this week's show notes.

8
00:00:37,440 --> 00:00:41,800
Our final Meetup of the Year will be held on Wednesday, December 13th.

9
00:00:41,800 --> 00:00:47,000
Make sure to bring your thoughts on the top machine learning and AI stories for 2017

10
00:00:47,000 --> 00:00:49,160
for our discussion segment.

11
00:00:49,160 --> 00:00:54,800
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing

12
00:00:54,800 --> 00:01:01,280
the paper understanding deep learning requires rethinking generalization by Shi Huang Zhang

13
00:01:01,280 --> 00:01:04,680
from MIT and Google Brain and others.

14
00:01:04,680 --> 00:01:09,800
You can find more details and register at twimla.com slash Meetup.

15
00:01:09,800 --> 00:01:14,120
If you receive my newsletter, you already know this, but Twimble is growing and we're

16
00:01:14,120 --> 00:01:19,840
looking for an energetic and passionate community manager to help expand our programs.

17
00:01:19,840 --> 00:01:24,240
This position can be remote, but if you happen to be in St. Louis, all the better.

18
00:01:24,240 --> 00:01:27,960
If you're interested, please reach out to me for additional details.

19
00:01:27,960 --> 00:01:31,920
I should mention that if you don't already get my newsletter, you are really missing

20
00:01:31,920 --> 00:01:37,160
out and should visit twimla.com slash newsletter to sign up.

21
00:01:37,160 --> 00:01:43,240
Now the show you're about to hear is part of our Strange Loop 2017 series brought to you

22
00:01:43,240 --> 00:01:46,240
by our friends at Nexusos.

23
00:01:46,240 --> 00:01:50,640
Nexusos is a company focused on making machine learning more easily accessible to enterprise

24
00:01:50,640 --> 00:01:52,240
developers.

25
00:01:52,240 --> 00:01:56,160
Their machine learning API meets developers where they're at, regardless of their mastery

26
00:01:56,160 --> 00:02:01,400
of data science, so they can start cutting up predictive applications immediately and

27
00:02:01,400 --> 00:02:04,240
in their preferred programming language.

28
00:02:04,240 --> 00:02:08,600
It's as simple as loading your data and selecting the type of problem you want to solve.

29
00:02:08,600 --> 00:02:13,480
Their automated platform trains and selects the best model fit for your data and then outputs

30
00:02:13,480 --> 00:02:15,040
predictions.

31
00:02:15,040 --> 00:02:19,360
To learn more about Nexusos, be sure to check out the first episode in this series at

32
00:02:19,360 --> 00:02:27,640
twimla.com slash talk slash 69, where I speak with co-founders Ryan Sevy and Jason Montgomery.

33
00:02:27,640 --> 00:02:33,080
Be sure to also get your free Nexusos API key and discover how to start leveraging machine

34
00:02:33,080 --> 00:02:38,200
learning in your next project at nexosos.com slash twimble.

35
00:02:38,200 --> 00:02:44,400
In this episode, I speak with Allison Parrish, Poet and Professor at NYU in the Interactive

36
00:02:44,400 --> 00:02:46,520
Telecommunications Department.

37
00:02:46,520 --> 00:02:53,080
Allison's work centers around generated poetry via artificial intelligence and machine learning.

38
00:02:53,080 --> 00:02:56,920
She joins me prior to her conference talk on experimental creative writing with the

39
00:02:56,920 --> 00:02:58,680
vectorized word.

40
00:02:58,680 --> 00:03:04,480
In our time together, we discuss some of her research into computational poetry generation,

41
00:03:04,480 --> 00:03:09,720
actually performing AI-produced poetry and some of the methods and processes she uses

42
00:03:09,720 --> 00:03:20,040
for generating her work.

43
00:03:20,040 --> 00:03:26,600
Alright everyone, I am here at Strange Loop in St. Louis and I am with Allison Parrish.

44
00:03:26,600 --> 00:03:32,760
Allison is a teacher with the Interactive Telecommunications program at NYU and she is

45
00:03:32,760 --> 00:03:38,320
actually speaking later on today on experimental creative writing with the vectorized word.

46
00:03:38,320 --> 00:03:41,080
Welcome to the podcast Allison, it's great to have you.

47
00:03:41,080 --> 00:03:42,080
Thank you, it's great to be here.

48
00:03:42,080 --> 00:03:45,080
So why don't we get started by having you tell us a little bit about your background

49
00:03:45,080 --> 00:03:49,400
and how you got interested in machine learning and artificial intelligence?

50
00:03:49,400 --> 00:03:58,320
Okay, so I am a poet, sort of what I like to call myself primarily, but my educational

51
00:03:58,320 --> 00:04:02,640
background originally as an undergraduate was in linguistics and I've been a computer

52
00:04:02,640 --> 00:04:05,080
programmer for a really long time.

53
00:04:05,080 --> 00:04:09,040
Most of the poetry that I write has to do or is produced with procedures, so I write

54
00:04:09,040 --> 00:04:15,400
computer programs that produce poetry and I've been doing that for a really long time.

55
00:04:15,400 --> 00:04:18,760
Mostly I've done like Twitter bots and stuff like that.

56
00:04:18,760 --> 00:04:27,200
And sort of my poetic research right now is about like how do we find new ways of composition?

57
00:04:27,200 --> 00:04:32,640
Like what are new means of like putting text down onto the page?

58
00:04:32,640 --> 00:04:37,540
Just like me, generally I think of poetry we think of like a really artistic person

59
00:04:37,540 --> 00:04:41,640
or whatever who gets super inspired by nature or something and it's like looking out

60
00:04:41,640 --> 00:04:48,200
the kitchen window and sees the glint of dew off of the flower of a petal right to poem

61
00:04:48,200 --> 00:04:49,200
based on that inspiration.

62
00:04:49,200 --> 00:04:52,440
Oh, how do you do how you write it exactly?

63
00:04:52,440 --> 00:04:59,680
I'm interested in thinking about how can we expand the possible languages and methods

64
00:04:59,680 --> 00:05:05,800
that poets can use to write and how does that like you know what are the other poetic

65
00:05:05,800 --> 00:05:10,200
effects that can be made with text other than just like the sort of the conventional

66
00:05:10,200 --> 00:05:14,000
poetic or the conventional narrative way that the text works.

67
00:05:14,000 --> 00:05:18,400
So procedure is a great way to address that is the computer, you know the computer can

68
00:05:18,400 --> 00:05:22,720
choose something at random, the computer can have a system of rules for putting stuff

69
00:05:22,720 --> 00:05:27,880
together and those rules don't necessarily conform with our conventional ideas about how

70
00:05:27,880 --> 00:05:29,800
our text goes together.

71
00:05:29,800 --> 00:05:35,720
So I primarily have been thinking about artificial intelligence and machine learning as they've

72
00:05:35,720 --> 00:05:39,800
been in my practice for the past couple of years as a way to just do more of that work

73
00:05:39,800 --> 00:05:48,120
of trying to create tools for poets and programmers to be expressive with language in ways that

74
00:05:48,120 --> 00:05:49,720
they haven't been able to be.

75
00:05:49,720 --> 00:05:50,720
Okay.

76
00:05:50,720 --> 00:05:56,320
There's been a lot of work recently with folks basically training these LSTM neural

77
00:05:56,320 --> 00:06:02,120
formats with, you know, tons of text and then having them produce, I got to imagine

78
00:06:02,120 --> 00:06:06,160
someone's in poetry but movie scripts and all kinds of stuff.

79
00:06:06,160 --> 00:06:10,880
You do any of that as well or it sounds like you're looking for more structure in the

80
00:06:10,880 --> 00:06:11,880
type of things.

81
00:06:11,880 --> 00:06:18,640
I don't know if it's more structure or less structure or different structure.

82
00:06:18,640 --> 00:06:22,480
I mean, so I keep up with that research and I should have looked this up.

83
00:06:22,480 --> 00:06:28,960
There's the Turing tests and the creative arts, I think it's called, I haven't seen that

84
00:06:28,960 --> 00:06:33,120
but I imagine it's, when you look at this, do you think it was written by human?

85
00:06:33,120 --> 00:06:34,120
Yeah.

86
00:06:34,120 --> 00:06:35,120
That's exactly what it is.

87
00:06:35,120 --> 00:06:38,040
And they have this prize and when they first started out, it was just like a couple

88
00:06:38,040 --> 00:06:39,040
of different genres.

89
00:06:39,040 --> 00:06:40,040
They've had a different stuff.

90
00:06:40,040 --> 00:06:41,040
Now they haven't.

91
00:06:41,040 --> 00:06:44,920
They've had a prize for like the best sonnet and by best what they mean is the one that

92
00:06:44,920 --> 00:06:48,560
can like trick a person into thinking that it was written by a person.

93
00:06:48,560 --> 00:06:49,560
Okay.

94
00:06:49,560 --> 00:06:55,800
And that basically means that like there's always been a literature on computational poetry

95
00:06:55,800 --> 00:07:00,160
generation, but for like the past couple of years, it's all just been like sonnet generators,

96
00:07:00,160 --> 00:07:03,480
sonnet generators, sonnet generators, sonnet generators, sonnet generators.

97
00:07:03,480 --> 00:07:07,480
All with like slightly different methodologies, some of which you're using like deep learning

98
00:07:07,480 --> 00:07:08,680
some of which aren't.

99
00:07:08,680 --> 00:07:09,680
Okay.

100
00:07:09,680 --> 00:07:14,080
And I think that's super interesting research to keep up with that like those researchers

101
00:07:14,080 --> 00:07:18,640
are framing their problem that they're solving in terms of verisimilitude, like they're

102
00:07:18,640 --> 00:07:25,320
trying to make poems that resemble poems that are written by a person in a conventional

103
00:07:25,320 --> 00:07:27,120
typical manner.

104
00:07:27,120 --> 00:07:33,360
And then like in actually like in the paper in order to evaluate the work, they'll like

105
00:07:33,360 --> 00:07:38,800
do like a survey or something and ask people to judge whether or not it was conventionally

106
00:07:38,800 --> 00:07:41,240
created or not or what they do it.

107
00:07:41,240 --> 00:07:44,920
Which for me is like that's super uninteresting.

108
00:07:44,920 --> 00:07:47,640
It's like, I mean, that's a poet.

109
00:07:47,640 --> 00:07:51,000
As like a researcher, it's like, well, of course, that's cool.

110
00:07:51,000 --> 00:07:52,000
That's interesting.

111
00:07:52,000 --> 00:07:53,000
Right.

112
00:07:53,000 --> 00:07:57,600
But as a poet, it's like we already have a pretty efficient machine for writing sonnets

113
00:07:57,600 --> 00:07:59,000
and that's a person.

114
00:07:59,000 --> 00:08:03,440
Like if you find a person you give them $10,000 and you say, well, you write a sonnet,

115
00:08:03,440 --> 00:08:05,440
you're probably going to get a pretty good sonnet.

116
00:08:05,440 --> 00:08:09,880
I don't think anybody's been paid $10,000 to write a sonnet in the past, you know, a hundred

117
00:08:09,880 --> 00:08:11,680
years or whatever.

118
00:08:11,680 --> 00:08:17,040
So for me, I'm more interested in with that kind of stuff, I'm more interested in using

119
00:08:17,040 --> 00:08:24,080
machine learning for making open new avenues for being expressive in unexpected ways.

120
00:08:24,080 --> 00:08:28,120
I guess the answer is like I'm like trying to pick and choose from that research, the

121
00:08:28,120 --> 00:08:31,880
stuff that can be helpful to me and the stuff that's been really helpful to me over the

122
00:08:31,880 --> 00:08:37,440
past couple of years is just like this idea of a word vector and how it opens up this

123
00:08:37,440 --> 00:08:43,800
ability to compose texts differently because instead of working with discrete units,

124
00:08:43,800 --> 00:08:48,200
you're working with like just this kind of like blob that can be like stretched and molded

125
00:08:48,200 --> 00:08:50,080
and stuff like that.

126
00:08:50,080 --> 00:08:53,680
And I think that's like really useful raw material for a poet.

127
00:08:53,680 --> 00:08:54,680
Interesting.

128
00:08:54,680 --> 00:09:00,240
So I imagine your talk was about how you can stretch and mold the words with this blob.

129
00:09:00,240 --> 00:09:01,240
Right.

130
00:09:01,240 --> 00:09:05,960
So maybe walk us through kind of how you presented those ideas, what were you trying to

131
00:09:05,960 --> 00:09:06,960
leave folks with?

132
00:09:06,960 --> 00:09:14,760
The thing that I'm trying to leave people with is more just an idea of here's what computers

133
00:09:14,760 --> 00:09:17,800
can do with language that might not occur to people individually.

134
00:09:17,800 --> 00:09:23,280
I don't think that like the particular research that I'm doing is like that answer to computer

135
00:09:23,280 --> 00:09:25,320
generated poetry.

136
00:09:25,320 --> 00:09:29,800
But the stuff that I've been doing recently is, so if you take like these pre-trained

137
00:09:29,800 --> 00:09:35,480
word vectors from an algorithm like word to vac or the the glove vectors from Stanford,

138
00:09:35,480 --> 00:09:41,880
basically just get like, you know, four gigabytes of vectors that associate a word or a text

139
00:09:41,880 --> 00:09:45,240
file that associates a word with the vector of numbers.

140
00:09:45,240 --> 00:09:50,800
And once you have that vector of numbers, you can arrange them in sort of like a matrix

141
00:09:50,800 --> 00:09:54,640
that every word in a text corresponds to a vector.

142
00:09:54,640 --> 00:09:58,920
At that point, it's weirdly just like audio or image data.

143
00:09:58,920 --> 00:10:03,840
Like image data only has like four dimensions, let's say like the great red, green, blue,

144
00:10:03,840 --> 00:10:09,160
and alpha channel audio data usually just has the one dimension like the amplitude at

145
00:10:09,160 --> 00:10:11,320
that particular sample point.

146
00:10:11,320 --> 00:10:16,480
But text even though even though that data is like 300 dimensional or whatever from like

147
00:10:16,480 --> 00:10:18,640
a mathematical standpoint, it's the same.

148
00:10:18,640 --> 00:10:20,080
So there's like no reason.

149
00:10:20,080 --> 00:10:24,760
You can basically just like the same function that you would use in like, what's the name

150
00:10:24,760 --> 00:10:28,560
of the stupid Python library that has all of the.

151
00:10:28,560 --> 00:10:29,560
So I can learn.

152
00:10:29,560 --> 00:10:30,560
Yes.

153
00:10:30,560 --> 00:10:31,560
I use this every day.

154
00:10:31,560 --> 00:10:32,560
I don't know.

155
00:10:32,560 --> 00:10:36,560
I'd need any function inside get learned or numpy or whatever that you could like put

156
00:10:36,560 --> 00:10:41,000
an image file into and then get another image out and do that with the word vector as

157
00:10:41,000 --> 00:10:42,000
well.

158
00:10:42,000 --> 00:10:46,200
So this is like, this is an experiment that I did while I was doing a residency last

159
00:10:46,200 --> 00:10:52,240
summer, which is basically just like, what does it look like when you blend two text

160
00:10:52,240 --> 00:10:53,240
together?

161
00:10:53,240 --> 00:10:56,360
You just have like, here's here's text one, which is a matrix of word vectors, here's

162
00:10:56,360 --> 00:11:00,600
text two of the matrix of word vectors, what if you just like cross-fade between them?

163
00:11:00,600 --> 00:11:01,600
Okay.

164
00:11:01,600 --> 00:11:06,880
And then you can do things like blur the text, like apply, like algorithm to just like

165
00:11:06,880 --> 00:11:11,600
average the surrounding pixels, the same way that you would do it with an image or, oh

166
00:11:11,600 --> 00:11:16,640
wow, or resample it, like you can actually like take a text and then just say, instead

167
00:11:16,640 --> 00:11:22,040
of being 50 word vectors along, this will be five word vectors along with you again.

168
00:11:22,040 --> 00:11:27,320
And that's actually like one of the ways of doing text summarization is just like average

169
00:11:27,320 --> 00:11:28,920
all of the vectors in a sentence.

170
00:11:28,920 --> 00:11:32,200
And that average vector is like the thing that you use to represent the meaning of the

171
00:11:32,200 --> 00:11:33,200
sentence.

172
00:11:33,200 --> 00:11:36,840
There's like other more sophisticated ways to do that, but that's like surprisingly

173
00:11:36,840 --> 00:11:38,400
performs well on all of these tasks.

174
00:11:38,400 --> 00:11:39,400
Oh wow.

175
00:11:39,400 --> 00:11:43,480
Yeah, and then so that's the kind of like thoughts that I've been bringing to this recently

176
00:11:43,480 --> 00:11:47,360
in terms of the operations you can perform on the vectors.

177
00:11:47,360 --> 00:11:53,280
And the idea of that, the idea behind that is just that now you can with word vectors

178
00:11:53,280 --> 00:12:00,400
as the medium for text, suddenly you can think of composing a text not consisting of like

179
00:12:00,400 --> 00:12:07,000
typing letters on a keyboard or selecting like words from autocomplete, but instead you

180
00:12:07,000 --> 00:12:11,960
have this like very physical analog continuous way of saying like, you know, I can turn

181
00:12:11,960 --> 00:12:17,400
a knob, like I could turn like a physical knob to make this text longer or shorter or

182
00:12:17,400 --> 00:12:23,000
to add in these semantics from this other text, it's really interesting.

183
00:12:23,000 --> 00:12:30,120
And now I'm taking that very literally, well not a literal knob, but I'm thinking of it

184
00:12:30,120 --> 00:12:35,040
would be kind of cool to have some kind of goo representation of all of these vectors

185
00:12:35,040 --> 00:12:42,040
and use that in a way to generate poetry or other written works like, are we there yet?

186
00:12:42,040 --> 00:12:43,040
Is that something like that?

187
00:12:43,040 --> 00:12:49,640
Oh, I've actually, I've been performing poetry with an interface like that with an actual

188
00:12:49,640 --> 00:12:57,040
like MIDI controller with actual, and one of the things that is in the talk is I'm working

189
00:12:57,040 --> 00:13:02,120
on sort of like a prototype for an interactive environment that's sort of like, do you know

190
00:13:02,120 --> 00:13:05,080
like like processing the interactive programming framework?

191
00:13:05,080 --> 00:13:06,080
I've heard it.

192
00:13:06,080 --> 00:13:07,080
I've heard the phrase.

193
00:13:07,080 --> 00:13:11,920
It basically like, it opens up a canvas object in the browser and then gives you a function

194
00:13:11,920 --> 00:13:15,160
that runs 30 times a second or whatever.

195
00:13:15,160 --> 00:13:19,360
And then inside of that function, you can call like a function to draw any lips or whatever

196
00:13:19,360 --> 00:13:21,440
and you can have it follow the mouse and stuff like that.

197
00:13:21,440 --> 00:13:26,960
So I am working on a prototype interface that's sort of like that except for word vectors.

198
00:13:26,960 --> 00:13:31,200
So instead of like a pixel buffer, the main thing that you're operating on in real time

199
00:13:31,200 --> 00:13:34,520
is a buffer of word vectors.

200
00:13:34,520 --> 00:13:39,080
And then on every frame, it's like showing you the word vector in the database closest

201
00:13:39,080 --> 00:13:41,720
to whatever that value happens to be.

202
00:13:41,720 --> 00:13:46,760
So I mean, the thing about it is that like, I think a lot of the problems that I'm trying

203
00:13:46,760 --> 00:13:49,600
to solve are poetic problems, right?

204
00:13:49,600 --> 00:13:54,000
And I think that a lot of like artificial intelligence researchers, you know, because

205
00:13:54,000 --> 00:13:59,760
this is what they're interested in are interested in solving these problems of like, you know,

206
00:13:59,760 --> 00:14:04,280
let's think about how to formalize what a poem is so that we can generate better poems,

207
00:14:04,280 --> 00:14:05,280
right?

208
00:14:05,280 --> 00:14:09,400
Because that's sort of like the teleology of artificial intelligence research is to like

209
00:14:09,400 --> 00:14:16,280
duplicate flavor, but I'm more interested in so that like the stuff that my tool makes

210
00:14:16,280 --> 00:14:19,000
is not stuff that makes sense in the conventional way.

211
00:14:19,000 --> 00:14:24,840
It's less, it's more cartridge-stined, mainstream contemporary poet.

212
00:14:24,840 --> 00:14:29,040
I can't think of a mainstream contemporary poet that I want to like make fun of for making

213
00:14:29,040 --> 00:14:31,040
poetry that makes too much sense.

214
00:14:31,040 --> 00:14:33,000
So yeah, that's that's sort of my goal.

215
00:14:33,000 --> 00:14:36,040
Is it like a linearity?

216
00:14:36,040 --> 00:14:42,560
It's like less linear than what you typically expect or is that too simple a characterization?

217
00:14:42,560 --> 00:14:43,840
What do you mean by linear?

218
00:14:43,840 --> 00:14:51,000
I guess I'm imagining the poems at the tool, that the thing that you're, the poems that

219
00:14:51,000 --> 00:14:57,080
you're creating, being more, I guess I'm imagining just like random words popping up.

220
00:14:57,080 --> 00:14:58,080
Yeah.

221
00:14:58,080 --> 00:14:59,080
Is it kind of like that?

222
00:14:59,080 --> 00:15:00,080
Or is it kind of like that?

223
00:15:00,080 --> 00:15:01,080
Okay.

224
00:15:01,080 --> 00:15:02,080
It comes out looking like random words.

225
00:15:02,080 --> 00:15:07,960
But yet there's some fundamental element of it that's still at least to use as this

226
00:15:07,960 --> 00:15:08,960
is poetry.

227
00:15:08,960 --> 00:15:09,960
Yeah.

228
00:15:09,960 --> 00:15:13,720
I think the thing that's interesting to me is the thing that that has been interesting

229
00:15:13,720 --> 00:15:19,960
to me as I've been presenting this work, I mean I, I, I present this work a lot to,

230
00:15:19,960 --> 00:15:25,320
to people in, you know, machine learning, artificial intelligence research, but then

231
00:15:25,320 --> 00:15:28,920
I also just like perform at regular poetry readings.

232
00:15:28,920 --> 00:15:34,160
And the thing that's actually been interesting and encouraging to me is that like these metaphors

233
00:15:34,160 --> 00:15:40,800
actually make sense to people like that idea of blending between two texts like averaging

234
00:15:40,800 --> 00:15:45,880
two words and finding the word in the middle and then making an entire text out of those

235
00:15:45,880 --> 00:15:48,320
words that are blended between the two.

236
00:15:48,320 --> 00:15:53,040
As long as you can show like some delta as long as you can say like here's where it started

237
00:15:53,040 --> 00:15:57,160
and here's where it's ending up, people will appreciate what's happening at each of

238
00:15:57,160 --> 00:15:59,640
these of those intermediary steps.

239
00:15:59,640 --> 00:16:05,520
I think this is a rule about poetry in general, the more that people understand the rules

240
00:16:05,520 --> 00:16:11,400
that condition the text, the more that they understand like the aesthetics of the mechanisms,

241
00:16:11,400 --> 00:16:13,360
the more they actually understand the text itself.

242
00:16:13,360 --> 00:16:14,360
Okay.

243
00:16:14,360 --> 00:16:20,320
So that's what I'm, the, the idea is that like if you just looked at it in isolation,

244
00:16:20,320 --> 00:16:25,920
you might think oh this is nonsense, but once you understand the, the procedure behind

245
00:16:25,920 --> 00:16:30,320
it, once you understand what's going into it, then suddenly it becomes this like massive

246
00:16:30,320 --> 00:16:34,680
new ability of this massive new way of thinking about language and text.

247
00:16:34,680 --> 00:16:39,320
That question of interpretability is like super, super important to me.

248
00:16:39,320 --> 00:16:44,960
Interpretability in the sense you just described, understanding the context for a particular poem

249
00:16:44,960 --> 00:16:50,400
and the receiver's ability to interpret its significance.

250
00:16:50,400 --> 00:16:51,400
Right.

251
00:16:51,400 --> 00:16:57,920
I have like, if you think about in, in the visual arts and with music, there are all kinds

252
00:16:57,920 --> 00:17:01,200
of different ways of interpreting those artifacts.

253
00:17:01,200 --> 00:17:02,200
Sure.

254
00:17:02,200 --> 00:17:09,040
Like, since the invention of photography almost all visual art has turned away from depiction.

255
00:17:09,040 --> 00:17:14,840
Like, people still paint things, but since, you know, as soon as photography was invented,

256
00:17:14,840 --> 00:17:19,440
you had impressionism and interpreting and impressionist painting is very different

257
00:17:19,440 --> 00:17:25,200
from interpreting a painting that like actually depicts something, interpreting like Jackson

258
00:17:25,200 --> 00:17:30,080
Pollock painting or a Mark Rothko painting or something like that is also very different.

259
00:17:30,080 --> 00:17:33,280
So there's this like whole new language for interpreting art that doesn't have to do

260
00:17:33,280 --> 00:17:36,680
with that conventional way of interpreting it.

261
00:17:36,680 --> 00:17:41,200
And the same thing with, with music, like we have, music with music it's even more, it's

262
00:17:41,200 --> 00:17:47,200
even more striking because music is almost never, you know, intended to convey a thought,

263
00:17:47,200 --> 00:17:48,200
right?

264
00:17:48,200 --> 00:17:52,200
Sometimes it is, and we have languages of citizens for doing that.

265
00:17:52,200 --> 00:17:59,360
I mean, like, when Bach was writing a chorale or whatever, he wasn't like, that wasn't

266
00:17:59,360 --> 00:18:04,520
meaning to say, like, you know, remember to buy eggs at the grocery store or whatever,

267
00:18:04,520 --> 00:18:05,520
right?

268
00:18:05,520 --> 00:18:09,760
Like, it was, it was trying to, it was trying to create a particular aesthetic effect.

269
00:18:09,760 --> 00:18:10,760
Okay.

270
00:18:10,760 --> 00:18:16,040
And then there's some music, like, you know, music with lyrics or whatever, or like, music

271
00:18:16,040 --> 00:18:20,880
with light motif from the romantic era, like those do have like specific symbolism and

272
00:18:20,880 --> 00:18:22,720
they can be interpreted.

273
00:18:22,720 --> 00:18:29,000
But in general, with music, like it's not, you don't listen to a song hoping to like,

274
00:18:29,000 --> 00:18:33,960
immediately retrieve like a propositional sentence that the person says, right?

275
00:18:33,960 --> 00:18:36,040
The song isn't something coded phrase.

276
00:18:36,040 --> 00:18:37,040
Right.

277
00:18:37,040 --> 00:18:38,040
Right.

278
00:18:38,040 --> 00:18:39,040
Exactly.

279
00:18:39,040 --> 00:18:41,960
It might include phrases, but it's not, it is not itself a way of language to fix

280
00:18:41,960 --> 00:18:48,000
expression, or I should say, like, a way to communicate a single idea.

281
00:18:48,000 --> 00:18:51,440
And lots of poets have been working with that same concept for a really long time.

282
00:18:51,440 --> 00:18:59,000
Like, that's been part of like the modernist tradition since data, maybe even earlier.

283
00:18:59,000 --> 00:19:03,360
But I think a lot of contemporary poets who are working in this area are sort of stuck

284
00:19:03,360 --> 00:19:09,120
on the idea that either it has to make total sense in a conventional way, or it has to

285
00:19:09,120 --> 00:19:10,960
be completely nonsense at all.

286
00:19:10,960 --> 00:19:17,840
Like it's either just nonsense, unreadable nonsense, or it's this thing that is conventional

287
00:19:17,840 --> 00:19:19,920
and to brag the way that it means.

288
00:19:19,920 --> 00:19:23,320
And I'm trying to investigate that little middle area.

289
00:19:23,320 --> 00:19:24,320
Yeah.

290
00:19:24,320 --> 00:19:28,120
And that's where machine learning is so useful, because like that's sort of, for me, at

291
00:19:28,120 --> 00:19:31,360
least aesthetically, that's the same place that machine learning inhabits.

292
00:19:31,360 --> 00:19:38,200
It's like, well, it's not, it's not a person in society with all of that context.

293
00:19:38,200 --> 00:19:45,560
It's sort of like trying to imitate and learn some of those same patterns from data.

294
00:19:45,560 --> 00:19:49,520
And when machine learning is most interesting to me is when it's when it's like slightly

295
00:19:49,520 --> 00:19:52,040
broken, when it gives you stuff, it's unexpected.

296
00:19:52,040 --> 00:19:59,840
So you've got this apparatus that allows you to manipulate word vectors and produce poetry.

297
00:19:59,840 --> 00:20:05,200
But there's also this other step that is producing the word vectors, the vectorization from

298
00:20:05,200 --> 00:20:10,160
a corpus that usually influences the result.

299
00:20:10,160 --> 00:20:12,760
Do you incorporate that step in as well?

300
00:20:12,760 --> 00:20:19,680
Or do you take that as a given for a piece or a body of work?

301
00:20:19,680 --> 00:20:25,440
With the semantic stuff, I've been taking it as a given, because I don't want to spend

302
00:20:25,440 --> 00:20:33,800
the money on the easy-to-manage to do a big word-to-back thing on gigabytes and gigabytes.

303
00:20:33,800 --> 00:20:37,400
I think there's something to be said about using that materiality.

304
00:20:37,400 --> 00:20:43,600
A lot has been written about the bias that is in word vectors and how, you know, like

305
00:20:43,600 --> 00:20:49,920
the analogies it'll show like, man is too programmer as woman is to housewife for whatever.

306
00:20:49,920 --> 00:20:55,960
I kind of like how that materiality is present in the stuff that comes out of us.

307
00:20:55,960 --> 00:21:01,080
Part of the reason of making stuff, of making poetry is to expose those, to expose and

308
00:21:01,080 --> 00:21:08,040
help people understand the ways that language is biased.

309
00:21:08,040 --> 00:21:12,720
So I don't see that as being entirely incompatible with what I'm trying to do.

310
00:21:12,720 --> 00:21:18,320
And so do you use a specific pre-trained word-to-back model?

311
00:21:18,320 --> 00:21:21,120
I've been using the glove vectors recently.

312
00:21:21,120 --> 00:21:22,120
Glove vectors.

313
00:21:22,120 --> 00:21:26,960
So there's a, and those are from Stanford and they're not, yeah, it's a different algorithm

314
00:21:26,960 --> 00:21:29,720
from word-to-back but it basically ends up doing the same thing.

315
00:21:29,720 --> 00:21:34,320
But it's not just the algorithm, it's a vector system that's pre-trained for you.

316
00:21:34,320 --> 00:21:38,040
Yeah, you just download the text file and it's like word and then 300 numbers.

317
00:21:38,040 --> 00:21:39,840
Just got it, got it.

318
00:21:39,840 --> 00:21:45,640
And there's a wonderful library for Python called Spacey for doing natural language processing

319
00:21:45,640 --> 00:21:46,640
stuff.

320
00:21:46,640 --> 00:21:47,640
Okay.

321
00:21:47,640 --> 00:21:50,040
And it comes with those vectors by default or at least it did in most recent versions.

322
00:21:50,040 --> 00:21:51,040
Okay.

323
00:21:51,040 --> 00:21:52,320
So those are like super easy to use.

324
00:21:52,320 --> 00:21:53,320
Oh nice.

325
00:21:53,320 --> 00:21:56,520
The other thing I've been working on recently, and this is the thing that I'm giving

326
00:21:56,520 --> 00:22:04,120
a talk on next week, experimentally, eyeing games is phonetic similarity vectors.

327
00:22:04,120 --> 00:22:09,000
So the same way that word vectors give you like two words with similar meanings will have

328
00:22:09,000 --> 00:22:10,000
similar vectors.

329
00:22:10,000 --> 00:22:11,000
Right.

330
00:22:11,000 --> 00:22:14,960
The thing that I've been working on for like the past year and I'm an arts professor.

331
00:22:14,960 --> 00:22:17,440
So I don't have a lot of resources to work on this.

332
00:22:17,440 --> 00:22:21,840
I'm sure a professor in an actual computer science department would have been able to

333
00:22:21,840 --> 00:22:23,320
get a grad student to do this.

334
00:22:23,320 --> 00:22:30,800
But for me, it was a new adventure, was trying to come up with a system of vectors where two

335
00:22:30,800 --> 00:22:36,320
words with similar pronunciations will have similar vectors so that like the word back

336
00:22:36,320 --> 00:22:39,640
and pack would have vectors that are similar to each other.

337
00:22:39,640 --> 00:22:44,840
The idea being that then you could come up with vector representations of sentences or

338
00:22:44,840 --> 00:22:51,040
lines of poetry and then determine how similar they are in sound and then do other interesting

339
00:22:51,040 --> 00:22:52,040
stuff with that.

340
00:22:52,040 --> 00:22:56,480
The same way that you can like do analogies with word vectors, you'd be able to do analogies

341
00:22:56,480 --> 00:22:58,000
with the way the word sounds.

342
00:22:58,000 --> 00:22:59,000
Huh.

343
00:22:59,000 --> 00:23:00,000
Interesting.

344
00:23:00,000 --> 00:23:05,120
It also makes me think of, you know, so much of conventional poetry or rhyming words

345
00:23:05,120 --> 00:23:09,280
and you could do something similar with rhyming vectors or something.

346
00:23:09,280 --> 00:23:10,280
Yeah.

347
00:23:10,280 --> 00:23:11,280
Yeah.

348
00:23:11,280 --> 00:23:18,840
Rhyming is difficult because it's like really easy to, it's really easy to like mechanically

349
00:23:18,840 --> 00:23:21,720
identify rhymes and attacks.

350
00:23:21,720 --> 00:23:26,960
If you have a phonetic transcription of the text, then like you can just look at like particular

351
00:23:26,960 --> 00:23:31,800
phonemes in particular places and then you know whether or not it rhymes.

352
00:23:31,800 --> 00:23:37,920
Finding slant rhymes is harder because then you have to have some criterion that tells

353
00:23:37,920 --> 00:23:43,800
you how similar are these two sounds and if they're similar enough that it counts as a

354
00:23:43,800 --> 00:23:44,800
slant rhyme.

355
00:23:44,800 --> 00:23:45,800
What did example of a slant rhyme?

356
00:23:45,800 --> 00:23:50,760
So a slant rhyme would be like, it would be like in a song where somebody tries to rhyme

357
00:23:50,760 --> 00:23:57,280
like, hit with pick or something like that where the sound of the end doesn't quite match.

358
00:23:57,280 --> 00:23:58,280
Okay.

359
00:23:58,280 --> 00:24:02,280
And like almost all contemporary lyrics use slant rhymes all the time.

360
00:24:02,280 --> 00:24:03,280
Okay.

361
00:24:03,280 --> 00:24:05,800
I wish I had a better example at the top of my head.

362
00:24:05,800 --> 00:24:11,080
But the point is that like it doesn't have to be precise for a decount as a rhyme.

363
00:24:11,080 --> 00:24:12,080
Okay.

364
00:24:12,080 --> 00:24:17,120
And so a lot of the poetry papers, a lot of the poetry generation papers that you read,

365
00:24:17,120 --> 00:24:21,920
now we're like sort of trying to find rule-based ways to solve that problem.

366
00:24:21,920 --> 00:24:25,560
Like even in a paper that's otherwise all about deep learning, we'll have this like little

367
00:24:25,560 --> 00:24:30,720
section that's like, well and then we use the sequence of if-else statements to determine

368
00:24:30,720 --> 00:24:32,720
whether or not these are slant rhymes.

369
00:24:32,720 --> 00:24:33,720
Okay.

370
00:24:33,720 --> 00:24:39,440
One of the points of doing the phonetic similarity of actors would be able to like just say like,

371
00:24:39,440 --> 00:24:46,840
okay do your surface syntactic and semantic generation of the text and then compare the

372
00:24:46,840 --> 00:24:51,240
phonetic similarity, then you could even like compare the phonetic similarity across

373
00:24:51,240 --> 00:24:56,400
like moving windows inside of the text and you'd be able to like detect where two lines

374
00:24:56,400 --> 00:25:00,920
are most similar in the way that they sound so that you'd be able to have like sort of

375
00:25:00,920 --> 00:25:04,080
like a useful tool for doing that kind of set land detection.

376
00:25:04,080 --> 00:25:05,080
Okay.

377
00:25:05,080 --> 00:25:10,480
But then also poetry uses sound in ways that isn't just rhymes, right?

378
00:25:10,480 --> 00:25:11,480
Right.

379
00:25:11,480 --> 00:25:16,760
Especially in like contemporary hip-hop like there's just like all an entire line will

380
00:25:16,760 --> 00:25:21,960
consist of sounds that are similar in some way and there's like this really complicated

381
00:25:21,960 --> 00:25:26,840
interplay between between lines where sounds are not quite the same but they're similar

382
00:25:26,840 --> 00:25:31,680
and they produce these from like an articulatory standpoint, the way that your mouth moves,

383
00:25:31,680 --> 00:25:33,800
they have this particular feeling.

384
00:25:33,800 --> 00:25:38,000
And then like you know classical poets or even like romantic poets or whatever, you'll

385
00:25:38,000 --> 00:25:43,360
see those patches of things like eliteration and ascendance and stuff like that, all

386
00:25:43,360 --> 00:25:48,480
of which you can study on like a rule-based basis, but one of the things that I wanted

387
00:25:48,480 --> 00:25:54,640
to be able to do with these phonetic similarity vectors is allow you to just write an equation

388
00:25:54,640 --> 00:25:59,760
just like look at a text and then you have a number that represents what it sounds like

389
00:25:59,760 --> 00:26:04,160
and then you can just you know use regular Euclidian and Neurocene distance to or

390
00:26:04,160 --> 00:26:08,240
this similarity to compare those two.

391
00:26:08,240 --> 00:26:12,600
And so whereas the previous work that you were describing is more about generation, this

392
00:26:12,600 --> 00:26:14,320
is more about analysis?

393
00:26:14,320 --> 00:26:16,520
No, it's it's the more about generation.

394
00:26:16,520 --> 00:26:23,640
No, again, I'm trying to solve poetic problems, not like linguistic or research problems.

395
00:26:23,640 --> 00:26:27,800
That was just like this kind of happy result of making these vectors.

396
00:26:27,800 --> 00:26:32,720
I was I made the vectors because I was I was working on a book of poetry for for counter

397
00:26:32,720 --> 00:26:39,080
path and I what I wanted to be able to do is just take the entire corpus and poetry in

398
00:26:39,080 --> 00:26:45,120
Project Gutenberg and I basically do a random walk through the sounds of it.

399
00:26:45,120 --> 00:26:52,280
So arrange every line of poetry in this database of public domain text and then just be able

400
00:26:52,280 --> 00:26:57,240
to go from one line to the next based on how similar they sounded to each other.

401
00:26:57,240 --> 00:27:02,000
That was the original like poetic idea because I love I love like poetic flow.

402
00:27:02,000 --> 00:27:06,640
I love like that that effect that you have in a poem where it just feels like all of

403
00:27:06,640 --> 00:27:07,960
the sounds are coming together.

404
00:27:07,960 --> 00:27:12,720
And if you could get that from this huge repository of poems, that'd be pretty cool.

405
00:27:12,720 --> 00:27:13,720
Right.

406
00:27:13,720 --> 00:27:18,600
So that was like my main impetus, but then I was like, I should write a paper about this.

407
00:27:18,600 --> 00:27:23,400
Papers usually have a section on whether or not it meets some criterion and their success.

408
00:27:23,400 --> 00:27:26,200
I should try to make that determination.

409
00:27:26,200 --> 00:27:29,880
And so then there's like this whole literature about phonetic similarity and I was able

410
00:27:29,880 --> 00:27:34,320
to like pull a couple of test data sets out of there and prove that like actually the

411
00:27:34,320 --> 00:27:37,840
algorithm that I was working on, you know, was performed pretty well on these tests.

412
00:27:37,840 --> 00:27:39,080
Oh, wow.

413
00:27:39,080 --> 00:27:43,000
And so from that, like these ideas of how you might be able to apply it in these other

414
00:27:43,000 --> 00:27:44,880
contexts, just sort of valid naturally.

415
00:27:44,880 --> 00:27:49,520
And I don't think, you know, because I'm an artist and I only have undergraduate degree

416
00:27:49,520 --> 00:27:54,400
in linguistics, I don't think that the solution that I came up with is like the ultimate best

417
00:27:54,400 --> 00:27:57,880
solution, but it definitely helped me solve my problem.

418
00:27:57,880 --> 00:27:58,880
Awesome.

419
00:27:58,880 --> 00:27:59,880
Awesome.

420
00:27:59,880 --> 00:28:20,040
Well, thanks so much for sharing all this with us is really I don't get to talk about poetry

421
00:28:20,040 --> 00:28:21,040
very often.

422
00:28:21,040 --> 00:28:22,040
People don't.

423
00:28:22,040 --> 00:28:26,120
And it makes me realize how little I know about it, so I need to figure out how to fix

424
00:28:26,120 --> 00:28:27,120
that.

425
00:28:27,120 --> 00:28:31,120
I mean, as anything about poetry, even poets don't know anything about poetry.

426
00:28:31,120 --> 00:28:32,920
Is that part of the essence of the thing?

427
00:28:32,920 --> 00:28:34,680
I think it partially is.

428
00:28:34,680 --> 00:28:39,520
I found recently that I'd like to read about poetry more than I like poetry itself.

429
00:28:39,520 --> 00:28:42,600
So maybe that's a good spring thing.

430
00:28:42,600 --> 00:28:43,600
Awesome.

431
00:28:43,600 --> 00:28:47,600
Well, thanks so much, Allison.

432
00:28:47,600 --> 00:28:53,200
All right, everyone, that's our show for today.

433
00:28:53,200 --> 00:28:57,920
Thanks so much for listening and for your continued feedback and support.

434
00:28:57,920 --> 00:29:03,120
For more information on Allison or any of the topics covered in this episode, head on

435
00:29:03,120 --> 00:29:07,520
over to twomlai.com slash talk slash 72.

436
00:29:07,520 --> 00:29:15,000
To follow along with our strange loop 2017 series, visit twomlai.com slash ST loop.

437
00:29:15,000 --> 00:29:20,760
Of course, you can send along your feedback or questions via Twitter to act twomlai or

438
00:29:20,760 --> 00:29:25,760
at Sam Charrington or leave a comment right on the show notes page.

439
00:29:25,760 --> 00:29:29,120
Thanks again to Nexusos for their sponsorship of the show.

440
00:29:29,120 --> 00:29:36,160
Check out twomlai.com slash talk slash 69 to hear my interview with the company founders

441
00:29:36,160 --> 00:29:43,640
and visit nexosis.com slash twimmel for more information and to try their API for free.

442
00:29:43,640 --> 00:29:53,640
Thanks again for listening and catch you next time.

