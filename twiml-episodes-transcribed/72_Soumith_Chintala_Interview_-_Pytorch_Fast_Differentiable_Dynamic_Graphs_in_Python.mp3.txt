Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
A big thanks to everyone who participated in last week's Twimble Online Meetup, and
to Kevin T from SIGUP for presenting.
You can find the slides for his presentation in the Meetup Slack Channel, as well as
in this week's show notes.
Our final Meetup of the Year will be held on Wednesday, December 13th.
Make sure to bring your thoughts on the top machine learning and AI stories for 2017
for our discussion segment.
For the main presentation, prior Twimble Talk guest Bruno Gonzalez will be discussing
the paper understanding deep learning requires rethinking generalization, by Shi Huang Zhang
from MIT and Google Brain and others.
You can find more details and register at twimbleia.com slash Meetup.
If you receive my newsletter, you already know this, but Twimble is growing and we're
looking for an energetic and passionate community manager to help expand our programs.
This position can be remote, but if you happen to be in St. Louis, all the better.
If you're interested, please reach out to me for additional details.
I should mention that if you don't already get my newsletter, you are really missing
out and should visit twimbleia.com slash newsletter to sign up.
Now the show you are about to hear is part of our Strange Loop 2017 series, brought to
you by our friends at Nexusos.
Nexusos is a company focused on making machine learning more easily accessible to enterprise
developers.
Their machine learning API meets developers where they're at, regardless of their mastery
of data science, so they can start cutting up predictive applications immediately and
in their preferred programming language.
It's as simple as loading your data and selecting the type of problem you want to solve.
Their automated platform trains and selects the best model fit for your data and then outputs
predictions.
To learn more about Nexusos, be sure to check out the first episode in this series at
twimbleia.com slash talk slash 69 where I speak with co-founders Ryan Sevy and Jason Montgomery.
Be sure to also get your free Nexusos API key and discover how to start leveraging machine
learning in your next project at nexosos.com slash twimble.
In this show, I speak with Sumit Shintala, a research engineer in the Facebook AI Research
Lab.
Sumith joined me at Strange Loop before his talk on PyTorch, the deep learning framework.
In this conversation, we discussed the market evolution of deep learning frameworks, different
approaches to programming deep learning frameworks, Facebook's motivations for investing in
PyTorch and much more.
This was a fun interview and I hope you enjoy it.
And now on to the show.
Hey everyone, I am here at the Strange Loop Conference in St. Louis and I'm with Sumith
Shintala, who is a research engineer at Fair, the Facebook AI Research Lab.
And Sumith is giving a talk here at the conference tomorrow and graciously agreed to spend some
time with us to talk a little bit about what he's up to and about its talk.
So welcome, Sumith.
Hi Sam, nice to be here at Strange Loop for the first time.
Absolutely, absolutely, and welcome to St. Louis.
So why don't we get started by having you tell us a little bit about your background
and how you got into AI research?
Sure.
Let's see.
About eight years ago, I wanted to be a digital artist trying to make VFX for movies and
stuff.
I didn't internship there and as soon as I went into that internship a weekend, I realized
that I was terrible at this, I was not an artist by any standards.
And then I had to find second choices in life and I was looking at my interests and one
of the things that struck to me was I was a decent programmer since I was a young age.
And I kind of liked the whole computer vision, angle, object detection, you know, these
show picture and some machine shows tells you, oh, there's a person here, there's a cat
here, that just fascinated me.
So I went down that line, I did a small internship at a research lab in India at this place
called Triple IT and like there I did a little bit of random stuff, you know, as an undergrad
you explore all kinds of things and then I tried a little bit of face detection and a
little bit of taking a bunch of pictures of a monument and then stitching them together
in 3D.
Okay.
And I had no idea what I was doing.
I was just like trying to put together things based on various tools and snippets and
you know how you call programmers stack over flow bots, you just take snippets and you're
trying to put together something and I was guilty as charged and then I got an opportunity
to come to CMU and Pittsburgh just doing more exploration, trying to figure out what
I want to do.
Here I got to play robot soccer.
So program robots to play soccer autonomously, autonomously, so you basically like flashed
a robot with your program and then they just play soccer.
And I've seen pictures of a variety of scenarios of these, the humanoid ones.
Yes, the ones I worked on were these humanoid ones.
Okay.
They were called Naoki robots and they were cute, and they were, I mean, I came in with
the expectation that we figured out so much in robotics, we must be pretty good.
And we couldn't make them stand properly, like they would just stand, they would walk
and they would fall down and that's the state of the art.
If you could make them walk without falling down.
We've probably all seen the video, the Boston robotics robots that like tries to turn
the door, the door not pan, and then just fall over totally.
I saw that one.
So that like shocked me because I was like, oh, robotics was so much more advanced than
it is.
And it also, I saw opportunity there, I'm like, oh, this film is still kind of open.
And we were trying to do the whole algorithm of the robot playing soccer with vision.
Like, oh, can you identify where the ball is and walk towards it and stuff.
And that part as well was very, very rudimentary, not working very well.
It would sort of look for an orange pixel in your image and try to make that pixel bigger.
And even something as stupid as that, it wasn't working very well.
That's pretty funny.
So I did a bunch of random stuff there and I had a good time at CME and I decided I want
to go into artificial intelligence, computer vision, robotics.
And then I applied to a bunch of places, CME being one of them and I didn't get accepted
anywhere.
And I was looking for late applications and stuff and that also combined with someone
who there, like where there was computer vision.
And I saw this web page by this guy called Jan LeCouven and he had a like, janky web
page with some object detection stuff going on, but I was like, hey, it's NYU, I'll give
it a shot.
So I applied to NYU and a couple of other places last minute because I got rejected from
all of my top schools I wanted to go to.
No offense, Jan.
I told them the story right away.
And I got accepted at NYU, I wasn't super happy with myself because I was like, oh,
I can do so much better if I worked harder.
And then it gets even more hilarious.
I come in to NYU, I emailed Jan before and I'm like, hey, I did a little bit of work
here and they're at CMU and Triple ID.
I want to work with you, try to do more object detection research and he replied immediately.
He was like, hey, let's meet, once you get here, let's meet this day at this time.
I go, Jan's in his office and it's like, hey, listen, do you know anything about
neural networks?
You know what?
I do my kind of research.
I'm like, nope, I only heard the term neural networks at once and I have no idea what
you're doing.
And so he went on to explain to me how neural networks work.
This was in 2010, neural networks weren't hot and Jan Likun had a lot of time on his
head.
So that relationship went well, he introduced me to one of his PhD students, Pierre
Cervine, who was now at Google, and Pierre and I, I was Pierre's like understudy.
I worked on many things there, like I implemented neural networks, we built like Pierre built
this deep learning framework called eBLearn and I was kind of helping out on that and that
made me understand more about how neural networks work also got me stronger on the engineering
side of things.
That's roughly how I entered the field in my two years at NYU, we published one paper
and another paper on the work we did got published in another conference.
And then in 2012 May, I graduated, I couldn't find a job in deep learning.
Wow.
2012 December was when the whole deep learning boom started.
So 2012 May, I was going to go accept a job at Amazon as a test engineer.
Wow.
You're like, why did I waste the last two years of my life?
It was just so frustrating.
I was trying not to give up, I was still super interested in the field, but you know,
you have practical constraints, right?
Like you're working at that, you need to think of all these things.
So in the last minute, I think I had to accept the Amazon offer by Saturday and on Wednesday
Jan, like, and Jan, at that day, I don't even remember why I met Jan the day, he was like,
oh, where are you going?
And I told him I couldn't find a job anywhere else and I go to Amazon and he's like, oh,
just yesterday, one of the companies I co-founded got fresh funding and they're looking to
hire engineers.
Oh, wow.
So that was a conversation that happened on Wednesday.
I went and gave my interviews on Thursday in Princeton, New Jersey.
And on Friday, I signed for Musami and they were doing music and deep learning on phones.
Okay.
So that was a company that was like, like a shazam kind of thing?
No, it was basically you, you want to, if someone's playing music, you should be able
to transcribe it live onto sheet and if someone takes a picture of sheet, then you have
to be able to play it back for them.
So it was like a full cycle, I want to hear, I want to play, so it was like a tool for
musicians.
Okay.
Does that exist for guitar tab?
Do you have any idea?
There it should.
And it would be awesome.
They do, but it's not a solved problem to decouple, when you play multiple notes together.
If you play a single note at a time, it's very easy, but if you play like five or six
chords at a time, like decoupling and understanding which of those exactly maps to what you played,
okay.
It's still like an ongoing problem.
So I spent time at Musami for a couple of years.
We were building all kinds of mobile stuff.
I mean, we wanted the whole thing to run on phones, so I was training new on networks
on sheet music and it would be called music optical recognition or more.
And then the company kind of had to fold at some point.
In the meanwhile, while it was at Musami, I always started actively maintaining torch,
which was the deep learning framework that was one of the bigger ones at that time.
And I eventually wanted to get out of Musami because we're not sure how the business
set of things was going.
And then Jan started at Facebook six months before that and they were using torch as their
main deep learning framework.
And so they needed good engineers to maintain and develop torch.
And so by the time I was joining Facebook, I was the only maintainer of torch.
So it was perfect.
They just got the engineer who will help like had the side of things.
So I came to Facebook and there were so many smart people that I just learned so much from.
I was also interested in research.
And so I ended up going down this path of generative adversarial networks where we were trying
to synthesize images.
So the neural network kind of just synthesizes images from nothing or like a training or
for what use cases.
This was more of an unsupervised learning use case.
So an unsupervised learning, one of the things you do is generation.
And the motivation there is that if you can generate something, you have generally good concepts
about how that process works.
So the motivation is that if we can do a really good image generation neural network, we
can take parts of that neural network and bootstrap other neural networks which are
doing computer vision tasks to get better performance.
So we could take parts of this neural network and then make it work on a different task
like dog versus clats classification.
And without having as much data, you would still get where you could accuracy.
So that's the whole unsupervised semi-supervised learning motivation.
I worked on a few things on the adversarial network side and then coming to...
And this was back in 2012, 2013?
No, 2014 I joined Facebook.
Oh, got it.
2014, 2015, 2016.
Because the GANs in general have been more, you know, past two, three years, I think,
in the year 2015, 2016.
Yeah.
Coming to what I'm going to talk about tomorrow, what happened was torch has been an aging
design in general.
It's been seven years since the previous release of torch came out.
So it was becoming more flexible, you know, as the field changes, there's this concept
for researchers who use the tools that they have available to them best.
And they push those tools to the limit.
And the new tools come that will then, again, make the researchers more flexible and exploring
new things.
So torch was reaching its limits of flexibility.
So we wanted to develop a new tool.
And so we worked on it for four years, started off as an intern project and then we kept
developing it.
And we released it earlier this year.
It's called PyTorch.
And it's what I'm going to be talking about tomorrow, it's strange, look, I'll be talking
about PyTorch, how it came about, what engineering challenges we faced.
PyTorch generally is a fairly slow language, but it's the most popular language for machine
learning, for, you know, statistics, like all kinds of things.
So the most obvious choice to build something in was PyTorch, because all of the users
were familiar with it.
We have a huge ecosystem.
And the barrier to entry is smaller than the C++.
Yeah, you have so many tutorials and it's very easy to learn and stuff.
Yeah.
So you had that upside, but the downside was deep learning is one of those high performance
computing spaces.
Right.
Every, every second, every millisecond matters.
But PyTorch is slow, like how do you make some packages really fast, but taking a constraint
that the users want to use it from PyTorch.
So generally, like how we worked around these challenges in various ways, I'm just going
to talk about that.
Some of the things we did was we moved the most critical parts into C. We made a large
part of the implementation lock free.
Well, let's make sure not to kind of breeze by these topics, because we really want to
dive into some of these.
Sure.
But, you know, one of the things that is maybe an interesting place to start is, and I've
talked about this, I think possibly on the podcast, definitely in my newsletter, just
the idea that it's actually kind of interesting here in your story and how, you know, in a
lot of ways, it's like all about timing and mistiming and timing windows and things like
that.
I think PyTorch has kind of popped up on the scene, if you will, at a time when I think
a lot of people in Crown TensorFlow was like the heir apparent to the deep learning framework
world.
Right.
And, you know, I wonder if you're just hearing your story, if like your experiences with,
you know, the timing cycle of machine learning and deep learning, like if that influences
your perspective on this kind of the market evolution of, you know, tools and, you know,
where do you see what you see the opportunity is for PyTorch and just kind of where you
think things are going.
Right.
So TensorFlow popped up was a December 2015, I think.
It took the whole deep learning world by a bang and Google put so much effort evangelizing
TensorFlow.
I mean, from Sundar Pichai to like pretty much everyone was like, hey, this is TensorFlow,
this is what Google is going to be about for the next few years, right.
So that, and, you know, they're a huge team and they've been putting great effort into
generally making sure everyone are covered by TensorFlow, whether it's a data scientist
or deep learning researcher or like a production engineer.
And like what Google did was amazing.
They raised the bar for engineering of a deep learning framework so far high.
Until then, like if you think about it, Torch, Tiano Cafe, these were the dominant deep
learning frameworks before TensorFlow.
They were all like, they all started as one man, grad student projects.
And you could totally see that in them.
The quality control was really bad.
They were not planned properly.
If you want to install any of those back in 2014, you would spend a day, maybe more.
You'd install that dependency, this dependency.
The TensorFlow, it made the engineering bar very high, they were like, we're not screwing
around here.
Right.
We want to make the best product out there for people.
And they went with a Tiano style programming model.
Yeah.
So a Tiano style programming model is very, very low level, which means if you want to
write something like a convolution neural network, then you'd spend writing so much
boilerplate code.
And also like the TensorFlow style model, it's called symbolic, which means that like you
create a graph and then you run it later.
The problem with that is if you want to debug anything, you'd have to use the tooling
given by TensorFlow.
Like you can, for example, debug your code by itself.
You have to run your model in this other virtual machine.
And then you can set breakpoints in that virtual machine using TensorFlow's own tools.
Meaning because it's not standard language x Python, in the case of PyTorch, it's being
interpreted by some virtual machine that knows how to read this graph and schedule execution
against this graph.
So the only way to develop debugging and other tooling for it is via that virtual machine.
That is correct.
Okay.
So the upsides to it are that this virtual machine can be as big or small as possible.
You can ship it into phones, you can ship it into anything.
The downside is that now where you define your network was in your Python VM.
And you're running your network in a different VM, there's disconnect.
So as a developer, you always have to keep thinking about how the behavior is something
in the TensorFlow virtual machine maps back to what you wrote in the Python code.
So anyone kind of on the enterprise side that has some Java experience, you know, knows
how exactly how different the right ones run anywhere is from the practice of needing to
know the internals of your heap size and garbage collection strategies and stuff like that.
Exactly.
And also give you, or at least that the ecosystem, if not an individual developer, the flexibility
to decouple the VM from the your code base.
So meaning, you know, you could write your code using TensorFlow and Python and but have
the VM written in, you know, go or whatever the fast concurrent, highly concurrent language
flavor of the day is and even port that over to, you know, distributed models or HPC
or.
So the upside is that the VM can now be written and rewritten in many other things.
The downside is that you have to have a build a whole ecosystem around your VM so that
users don't feel depraved by tooling.
Yeah.
And Tiano was also like that.
That's the whole programming model of symbolic where you define a symbolic model and then
you compile it and then you run it somewhere.
The torch model has always been imperative, which is you don't really have a separate VM.
You just declare things and you don't even like you just like you just write one plus
two and it just executed like there's no like separation between declaration and execution.
Yeah.
We wanted to extend the same thing to PyTorch.
So you just write arbitrary imperative code and that is your neural network itself.
That also lends itself to I think at least in the data science community, there's a lot
of popularity and flexibility around like Jupyter notebooks as a primary you are having
an imperative execution lends itself to being able to yeah, you can basically see your
execution as it goes.
You can print things and all of that.
That's one of the biggest upsides.
So yeah, while we're building PyTorch, we wanted to continue the torch model, the imperative
style, the dynamic nature of things and we wanted to build it in a way that it also reaches
a very high bar of engineering.
So that's been our core philosophy.
And so for Google, I think the way most people have interpreted their strategy behind diving
deep into TensorFlow is they foresee this world where AI workloads, whether they're training
or inference workloads are going to drive a ton of compute.
Their business model, their non advertising business model is heavily geared towards providing
compute via the Google Cloud.
And so if they can own the model in which they can basically own the VM for AI, then they
can be the best place to run AI workloads, right?
What's the Facebook motivation for investing so heavily in PyTorch and tooling?
Is it just not to be controlled by Google or is there more to it?
So Facebook's motivation is to fold.
For Facebook AI research, which is the 100 odd researchers and in general for the community
of AI.
We have a single point agenda at fair, which is to try to solve AI.
And for that, we're building the best tools out there.
And we keep them open just because there's nothing secret to it.
We want to build the best possible AI and we keep publishing about how we're trying to
make progress.
Our motivation is not really on the business model so much.
It's more like, hey, we're trying to solve this very challenging problem.
And you see this manifest in various ways in the Facebook product as a second-hand effect.
Like the product teams are not sitting with the AI researchers and saying how do we improve
Facebook as a product.
Facebook AI research, the people are independently working on their AI research.
But as we build and publish these things, the product teams look at our research and
they're like, oh, we can implement this thing in our product in this way.
And that's just going to be a better product experience for everyone.
Some examples are we've had the accessibility interface improve quite a bit recently about
a year, a year and a half ago, where now if you're a user who is blind or near blind,
you can view Facebook like you can basically touch Facebook as you would and it will tell
you what's going on.
Or if you touch the picture, it would just not tell it's would say a picture posted
by this person.
But if you touch a picture now, it actually tells you, oh, it's a picture where a boy is
playing with a cat and it's very descriptive and similarly we're trying to do the same
for videos as well.
That's one manifestation, others are where you want to make language barriers.
So let's say I have 500 friends, like some of them I met in various places and trips.
Like if one of my good friends writes a huge post in Chinese, I still want to be able
to know what it's about, right?
So we have the translate feature embedded into Facebook, all powered by a Facebook research.
And you see these manifest in various other producty ways.
And I think you see that evolve as well, like if I remember correctly, the Facebook products
only relatively recently switched to switched the way they did translation to neural translation.
I think there was a blog post on a fair blog maybe two months ago.
So very, very interesting.
Okay, so your talk.
So kind of walk us through the main points of your talk.
Basically you're at the high level, you've got PyTorch, it's inherently built around Python,
but you need to find ways to overcome the limitations of Python and kind of how you do
that.
Was that the main thrust of the talk or?
Yeah, so it's mostly just a general engineering talk about PyTorch.
Okay.
So I'm going to give like an overview of PyTorch how it works.
Okay.
A strange little audience doesn't necessarily know about deep learning computations, right?
More of the developer focus audience.
Like a lot of diversity in the programming models we use and like strange loop has everyone
under the sun.
Yeah.
So, which is part of what makes it great, right?
It's awesome.
Like this is my first strange loop.
I've just seen the sessions so far and like looking at the sessions lined up for tomorrow,
it's awesome.
So interesting factoid, the guy who founded Strange Loop, a guy named Alex Miller, Strange
was a group out of a meetup he had here in town called Lambda Lounge that looked at
like functional programming languages and at a startup that I was at years ago, this
was probably eight years ago, maybe we basically hosted, incubated this meetup, they would
meet in office.
So I've been sitting there late at night trying to get finished with my work and hearing
people talk about monads and stuff that I just had no clue about.
Nice.
I didn't realize St. Louis had a big developer community and this is all new to me.
So I'll be talking a little bit about PyTorch, start off with how to relate it to other
things people know and then a little bit of deep learning workloads, the general challenges,
why they're very different from let's say.
Let's say those in turn, how should people think about PyTorch and relating it to things
that they know?
Let's say you're, I mean, the most common thing everyone knows about is JavaScript, right?
So let's look at JavaScript.
If you're trying to build a compiler for JavaScript, the things you most care about is JavaScript
code, which is very branchy, has a lot of control flow and it's very, very like cash
and sensitive.
If you're building a very high performance compiler for JavaScript, you will build it
in a way that you'll try to optimize for like branch prediction and like try to get
traces and do trace compilation, you're tracing, like for example, if you look at
Chrome's JavaScript compiler, V8, yeah, V8, it's tracing jit and or you can look at
Lua jit as another example, it's another tracing jit and they do, we'll quickly trace
through upcoming code and then if something's compatible, we'll run time code generator
really quickly and these are all in the order of nanoseconds even because they're very,
very small computations and they're very branchy and doing something like loop hoisting or
strength reduction, these are the things that would really go well with such workloads.
Okay.
If learning, what you do is you do operations on tensors, let's say like, and eventual
matrices and usually you're doing a computation between A and B and A and B are not two
integers, they're 2,000 integers or 200,000 integers and so tensors is this intervention
on matrix and when you're doing these calculations, you're basically doing a lot of multiplications
or like any kind of point wise or reduction or some kind of convolvingy, like a moving
window kind of operations, these are the most common things in deep learning.
So when you're trying to make these like something like this more efficient, you look
at how fast you can, how fast, like how much you can parallelize each of these operations
individually and it turns out almost all of these operations are bandwidth, so these
operations can one as fast as how fast your memory bandwidth is, like how fast you can
get it in and out of the CPU because inside the CPU, you're just doing a small multiplication
or like an exponential but it's still much more expensive to get 200,000 numbers into
the CPU and out of the CPU.
So the way you would do optimization when building such things, let's say you're building
a compiler for these things.
You would try to fuse a bunch of these point wise operations, a bunch of these reduction
operations into an inner loop and then what you would do is you would get these tensors
in instead of doing one operation, putting it back out in a result and doing another
operation, putting the result back out, you try to get the tensor in, do seven operations
at once and then get all like the result of the seven of them out because that would
make it more compute bond rather than bandwidth bond.
Interesting.
So taking a step back to the analogy that JavaScript is primarily to say you build these high
performance, compilation and execution environments by understanding the property of the language
that you're working with and optimizing around that.
And we can do that here by, in this example, you would take code that is fundamentally written
in a very iterative serial kind of way but maybe parallelize or unfold those loops.
Yeah.
I don't know if that's the right way of thinking about it but.
Say tile the loops.
What's that tile the loops?
Tiling.
It's called tiling because you can, you can break the computation to, oh, I have 200,000
of these.
I'll just make tiles of 20 and send them to like 20 different processors.
Okay.
Interesting.
That was very cool.
Especially with GPUs, with GPUs, you have 3,000 cores on your GPU.
Right.
So you want to like break this computation down and feed those into all the separate processors
and then like get the results back.
But they don't fundamentally change the bandwidth issue.
They don't.
The bandwidth issue is still exist.
Yeah.
So I'm going to be talking a little bit about the GIT that we built into PyTorch.
We built a just in time compiler.
It's also a tracing GIT but it's of a very different kind.
Art tracing is not in the order of nanoseconds.
It's in the order of microseconds.
Okay.
But that's completely fine because general deep learning workloads are in the order of
milliseconds.
Right.
And the kind of optimizations passes we write as well are, as I said, more like fusion
and batching by batching, I mean, let's say you do computation x, y, z.
But between x, computation x and z, they're shared operations.
Let's say x also does multiplies and z also does multiplies.
So the tensors involved in x and tensors involved in z are very small.
What you do is you create a multiply operation, combine the tensors that are involved there
and then after the result comes out separate them on.
So this is called dynamic batching.
Yeah.
So we've been writing this GIT that's very new for us, like as in not many people have
worked generally in this direction.
Tens of flow is building one called XLA.
It's a compiler.
And the kind of optimizations they're doing as well are very similar in nature.
Like everyone's exploring now these like how to make tensor computations faster.
We're just taking the just in time approach and they're taking the head of time analysis
approach.
And I think of GIT and this probably comes from really JavaScript as something that is
more relevant to interactive types of workloads than batch, but deep learning is primarily
a batch workload, at least the training part of it.
Depends on how you see it.
Like we want to keep the interactiveness because remember what we talked about, people
like to keep that interactive Pythani iPad on on book style programming model.
We want people to keep that flexibility, but that's not really where you need like the
super high performance.
No, that's every like that's people do interactively.
They're programming for AGP's like this is this is the norm and deep learning.
So you're backed by your super powerful GPU.
And I'll talk a little bit about this in the talk tomorrow about how you can your PyTorch
sensors can just be transferred to the GPU and you operate on them.
And all the operations are now being done on the GPU with very high performance, but you're
doing this in a very interactive way.
Okay.
It is a little bit of a mind shift for folks that have been kind of immersed in a tensor
flow oriented or a batch oriented world where you kind of create this job.
You send it off to the cloud or wherever to train and then you check on it a few days.
Yeah.
We are the worst nightmare for hardware developers because they're all building solutions
around.
Oh, let's say you build this model beforehand and then you give it to our hardware.
Yeah.
Let's say it takes 30 minutes, but it will map your model in the most effective way to
our hardware.
And after that 30 minutes is done, if you pump any images in or any inputs in, it'll
be like super fast.
And we're like, well, that's kind of dumb in our model.
Like we'll give you a different model in every single iteration.
What are you going to do?
Right, right.
Interesting.
So speaking of hardware and hardware developers, I don't know if this is something that you're
close to it all, but Facebook also is very involved in this OCP Open Compute projects.
I was involved in the Big Sur and the Big Basin.
Don't worry you.
Yeah.
Okay.
So do you see, as I understand it, OCP has primarily been oriented around kind of off the shelf
stuff like system architecture as opposed to, you know, board level architecture or anything
like that, but do you see a future where OCP takes on like this bandwidth problem to try
to make hardware that's more suitable for these kinds of workloads or are those other
people's problems?
I think OCP is one of those huge industry-wide efforts, right?
I think it's truly possible that under OCP you'll get something that's like a custom
ASIC.
And I'm not sure how that will happen in general.
Because there's so many players and so many people who can contribute to OCP just two
days ago, I think, or yesterday, Nvidia released an open spec with very log files and HDL
files off a chip that does convolutional neural networks, like a high performance convolutional
neural network-based thing.
I think I saw the headline for that is like, this is Nvidia's TVU.
Right.
I mean, that was the click-bady headline.
So they basically put it out there.
You can take that very log and HDL is basically the code for fabricating these chips.
Yeah.
You will have to still like remap, very log into actual process, your manufacturing under,
but that's a mechanical process that usually, like you then, you usually give your HDL
files to someone and then they'll spend, they had a company will spend remapping whatever
you have to the process most effectively.
But yeah, it's a fairly mechanical process at the point.
So now, Nvidia just released this open chip.
And that's totally one of the candidates, for example, for open compute.
Like, open compute is all about, okay, everything is open.
You can refer to this as your own, right?
So I think we're still not sure about what's going to come into OCP in that direction.
Like I personally don't know the details.
Right.
So we'll see.
We'll see how it goes.
Okay.
So you also mentioned that parts of the pie torch engine are written in C makes me think
a little bit of, I can, in just kind of the regular Python community, there's, I'm
or incorrectly, there's like pi pi and sithon and all these different implementations of
the Python interpreter.
Is it the same general idea there?
Not at all.
Pi pi is a replacement for the default Python implementation, which is called CPITON.
Okay.
So Python is a programming language and they have a base implementation of that programming
language that the language developers develop on, which is called CPITON.
It's written in C and if you type Python into your desktop, usually that's what it is.
CPITON.
And CITON is just a very cute way of writing extensions to Python, like to see Python
mostly.
Okay.
Pi pi is a replacement interpreter for CPITON.
For CPITON.
It's written in Python.
That can interpret Python.
I'm not sure what it's written in, but I don't think it's written in Python.
Okay.
The implementation is probably not in Python.
Okay.
Pi pi is a just in time interpreter for Python.
It's probably written in part assembly and part C or whatever.
Okay.
Okay.
So the idea of being the significance of it is not its implementation, but just in time
versus not just in time.
Yeah.
Okay.
Pi Torch is just a CPITON extension.
Like all the C bits we have are, the equivalent you can find is NumPy.
NumPy is 90% written in C.
Yeah.
Okay.
But it has, it's a CPITON extension.
That is, it's not an independent C library that you can run.
It's like heavily integrated into the CPITON API and stuff.
So Pi Torch is just like a Python extension.
Okay.
But there's recently been an announcement of, you know, one of the first like, I think
kind of broadly publicize Pi Torch wins, if you will, is like fast.ai, deciding to rewrite
all of their courseware and into Pi Torch.
That was a very pleasant surprise.
I wasn't.
As soon as Pi Torch came out, I think they've tried it and they found it really effective,
especially for teaching and the barrier of entry.
So they've switched over to Pi Torch from TensorFlow.
And we're supporting them in any way we can.
Yeah.
Yeah.
It's interesting.
I think if, you know, if there's anything that this community benefits from is options and
especially options that, you know, have major, you know, both major companies behind them
pushing them forward, but also that are open to community engagement and community contributions.
And so it's definitely great to see from a kind of industry observer point of view that,
you know, we've got, you know, what's starting to look like a second kind of really strong
contender at a time when, you know, again, I think a lot of people said, oh, yeah, it's
just, you know, it's TensorFlow.
So congrats for your part in that.
And thanks so much for taking the time to sit down with me.
Oh, my pleasure.
Appreciate it.
All right, everyone.
That's our show for today.
Thanks so much for listening and for your continued feedback and support.
For more information on to me or any of the topics covered in this episode, head on over
to twomlai.com slash talk slash 70.
To follow along with our strange loop 2017 series, visit twomlai.com slash ST loop.
Of course, you can send along your feedback or questions via Twitter to act twomlai or
at Sam Charrington, or leave a comment right on the show notes page.
Thanks again to Nexosis for their sponsorship of the show.
Check out twomlai.com slash talk slash 69 to hear my interview with the company founders
and visit nexosis.com slash twimmel for more information and to try their API for free.
Thanks again for listening and catch you next time.
