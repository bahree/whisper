1
00:00:00,000 --> 00:00:15,920
Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting

2
00:00:15,920 --> 00:00:20,880
people doing interesting things in machine learning and artificial intelligence.

3
00:00:20,880 --> 00:00:23,560
I'm your host Sam Charrington.

4
00:00:23,560 --> 00:00:28,440
This week on the podcast we're featuring a series of conversations from the AWS re-invent

5
00:00:28,440 --> 00:00:30,280
conference in Las Vegas.

6
00:00:30,280 --> 00:00:34,680
I had a great time at this event, getting caught up on the new machine learning and AI products

7
00:00:34,680 --> 00:00:38,000
and services offered by AWS and its partners.

8
00:00:38,000 --> 00:00:42,160
If you missed the news coming out of re-invent and want to know more about what one of the

9
00:00:42,160 --> 00:00:47,360
biggest AI platform providers is up to, make sure you check out Monday's show, Twimble

10
00:00:47,360 --> 00:00:52,540
Talk number 83, which was a roundtable discussion I held with Dave McCrory and Lawrence

11
00:00:52,540 --> 00:00:53,540
Chung.

12
00:00:53,540 --> 00:00:59,860
We cover all of AWS's most important news, including the new SageMaker, DeepLens, Recognition

13
00:00:59,860 --> 00:01:06,980
Video, Transcription Service, Alexa for Business, Greengrass ML inference and more.

14
00:01:06,980 --> 00:01:13,660
In this episode, I'll be speaking with Nikita Shamganov, co-founder and CEO of MemSQL,

15
00:01:13,660 --> 00:01:18,940
a company offering a distributed memory optimized data warehouse of the same name.

16
00:01:18,940 --> 00:01:23,180
Nikita and I take a deep dive into some of the features of their recently released 6.0

17
00:01:23,180 --> 00:01:28,540
version, which supports built-in vector operations to enable machine learning use cases like

18
00:01:28,540 --> 00:01:34,660
real-time image recognition, visual search and predictive analytics for IoT.

19
00:01:34,660 --> 00:01:38,500
We also discuss how to architect enterprise machine learning solutions around the data

20
00:01:38,500 --> 00:01:42,420
warehouse by including components like data lakes and spark.

21
00:01:42,420 --> 00:01:47,340
Finally, we touch on some of the performance advantages that MemSQL is seen by implementing

22
00:01:47,340 --> 00:01:54,660
vector operations using Intel's latest AVX2 and AVX512 instruction sets.

23
00:01:54,660 --> 00:01:58,900
And speaking of Intel, I'd like to thank our good friends over at Intel Nirvana for

24
00:01:58,900 --> 00:02:02,980
their sponsorship of this podcast and our reinvent series.

25
00:02:02,980 --> 00:02:07,980
One of the big announcements from reinvent this year was the release of Amazon DeepLens,

26
00:02:07,980 --> 00:02:13,380
a fully programmable deep learning enabled wireless video camera designed to help developers

27
00:02:13,380 --> 00:02:17,980
learn an experiment with AI in the cloud and at the edge.

28
00:02:17,980 --> 00:02:24,900
DeepLens is powered by an Intel Atom X5 processor, which delivers up to 100 gigaflops of processing

29
00:02:24,900 --> 00:02:27,820
power to onboard applications.

30
00:02:27,820 --> 00:02:31,580
To learn more about DeepLens and the other interesting things Intel has been up to in

31
00:02:31,580 --> 00:02:35,940
the AI space, make sure to check out intelnervana.com.

32
00:02:35,940 --> 00:02:38,740
Okay, just a couple more quick announcements.

33
00:02:38,740 --> 00:02:43,820
You may have heard me mention last time that over the weekend we hit a very exciting milestone

34
00:02:43,820 --> 00:02:45,500
for the podcast.

35
00:02:45,500 --> 00:02:48,340
One million listens.

36
00:02:48,340 --> 00:02:52,780
What an amazing way to close out an amazing year for the show.

37
00:02:52,780 --> 00:02:57,780
It occurred to us that we'd hate to miss an opportunity to show you some love.

38
00:02:57,780 --> 00:03:03,540
So we're launching a listener appreciation contest to celebrate the occasion.

39
00:03:03,540 --> 00:03:09,820
To enter, just tweet to us using the hashtag twimmel1mail.

40
00:03:09,820 --> 00:03:15,860
Every entry gets a fly twimmel1mail sticker plus a chance to win one of 10 limited run

41
00:03:15,860 --> 00:03:18,700
t-shirts commemorating the occasion.

42
00:03:18,700 --> 00:03:24,220
We'll be giving away some other mystery prizes as well from the magic twimmel swag bag

43
00:03:24,220 --> 00:03:26,780
so you should definitely enter.

44
00:03:26,780 --> 00:03:33,780
If you're not on Twitter or want more ways to enter, visit twimmelai.com slash twimmel1mail

45
00:03:33,780 --> 00:03:35,580
for the full rundown.

46
00:03:35,580 --> 00:03:41,300
Last but not least, we are quickly approaching our final twimmel online meetup of the year,

47
00:03:41,300 --> 00:03:46,740
which will be held on Wednesday, December 13th at 3pm Pacific time.

48
00:03:46,740 --> 00:03:52,580
We'll start out by discussing the top ML and AI stories of 2017 and then for our main

49
00:03:52,580 --> 00:03:58,180
presentation, Bruno Gonzalez will be discussing the paper understanding deep learning requires

50
00:03:58,180 --> 00:04:04,620
rethinking generalization by Shi Yuan Zhang from MIT and Google Brain and others.

51
00:04:04,620 --> 00:04:10,780
This will be a fun meetup and one you don't want to miss, so be sure to register at twimmelai.com

52
00:04:10,780 --> 00:04:14,900
slash meetup if you haven't already done so.

53
00:04:14,900 --> 00:04:24,340
And now on to the show.

54
00:04:24,340 --> 00:04:27,660
Alright everyone, I am on the line with Nikita Shamganov.

55
00:04:27,660 --> 00:04:32,260
Nikita is CEO and co-founder of MemSQL.

56
00:04:32,260 --> 00:04:35,020
Nikita, welcome to this week in machine learning and AI.

57
00:04:35,020 --> 00:04:36,020
Thank you.

58
00:04:36,020 --> 00:04:37,020
Thank you for having me.

59
00:04:37,020 --> 00:04:38,020
Absolutely.

60
00:04:38,020 --> 00:04:41,620
Nikita, why don't we get started by having you tell us a little bit about your background.

61
00:04:41,620 --> 00:04:46,420
You are the CEO of MemSQL, but you've got a pretty technical background, isn't that right?

62
00:04:46,420 --> 00:04:47,420
That's right.

63
00:04:47,420 --> 00:04:53,780
So I'm on my PhD in computer science from St. Petersburg, Russia and I moved to the stage to work

64
00:04:53,780 --> 00:04:58,780
on Microsoft SQL Server, which I did for a number of years.

65
00:04:58,780 --> 00:05:04,700
So I have a very strong database and query processing background.

66
00:05:04,700 --> 00:05:11,260
After that, I moved over to Facebook where I was blown away by the magnitude of data

67
00:05:11,260 --> 00:05:14,180
problem Facebook was solving back then.

68
00:05:14,180 --> 00:05:21,180
And since they only increased in magnitude, seeing that and combining the visibility into

69
00:05:21,180 --> 00:05:29,700
those workloads and kind of making that assumption that in five years, a lot more companies

70
00:05:29,700 --> 00:05:33,420
are going to be facing those challenges just like Facebook.

71
00:05:33,420 --> 00:05:40,100
I decided to start a company and combine that database background and expertise of building

72
00:05:40,100 --> 00:05:47,540
systems and the early stages and the glimpse of the workloads that I saw at Facebook.

73
00:05:47,540 --> 00:05:51,220
So I kind of knew where the world was going into.

74
00:05:51,220 --> 00:05:58,980
So that triggered my desire and gave me some insights into starting MemSQL, the company.

75
00:05:58,980 --> 00:06:05,660
So we've been in it for six plus years and certainly validated some of the assumptions

76
00:06:05,660 --> 00:06:09,460
we had starting that journey.

77
00:06:09,460 --> 00:06:16,300
So I've come across MemSQL and I think of the company as an in-memory database.

78
00:06:16,300 --> 00:06:22,620
It's also a little bit about what the focus is there and in particular, what's the intersection

79
00:06:22,620 --> 00:06:25,260
between what you're doing and machine learning in AI?

80
00:06:25,260 --> 00:06:27,820
Are you seeing a lot of those types of workloads nowadays?

81
00:06:27,820 --> 00:06:28,820
Definitely.

82
00:06:28,820 --> 00:06:33,420
That's certainly where our customers are moving towards.

83
00:06:33,420 --> 00:06:39,340
But let me step back for a second and talk about MemSQL and in-memory database

84
00:06:39,340 --> 00:06:41,220
and technology.

85
00:06:41,220 --> 00:06:48,620
We started as purely in-memory database and then since we've evolved to support a large

86
00:06:48,620 --> 00:06:55,580
class of applications that I built on top of MemSQL and in-memory became an enabling

87
00:06:55,580 --> 00:06:59,540
technology but it's not the technology at MemSQL.

88
00:06:59,540 --> 00:07:07,380
As a matter of fact, the key advantage that MemSQL brings to the world is the fact that

89
00:07:07,380 --> 00:07:14,180
it supports SQL which is a structured query language and the fact that it runs the database

90
00:07:14,180 --> 00:07:16,140
in a distributed environment.

91
00:07:16,140 --> 00:07:21,660
So you can run MemSQL on your laptop or you can run MemSQL on a thousand hosts that

92
00:07:21,660 --> 00:07:29,460
gives you an immense compute power to enable the new class of applications.

93
00:07:29,460 --> 00:07:34,180
So let me talk about what kinds of applications we support.

94
00:07:34,180 --> 00:07:42,140
Maybe some of them have to do very little with AI and ML and MemSQL enables scale, latency.

95
00:07:42,140 --> 00:07:46,980
You can build very low latency applications on top of MemSQL and that's where in-memory

96
00:07:46,980 --> 00:07:49,460
technologies come handy.

97
00:07:49,460 --> 00:07:54,980
And you also can build applications that require very high levels of concurrency.

98
00:07:54,980 --> 00:07:59,700
And that concurrency is enabled on top of the system of record.

99
00:07:59,700 --> 00:08:05,740
So MemSQL supports state and it supports full durable persistence.

100
00:08:05,740 --> 00:08:10,700
SQL also allows you to build what we call real-time applications.

101
00:08:10,700 --> 00:08:15,100
And kind of the idea of real-time applications is the opposite of batch.

102
00:08:15,100 --> 00:08:21,620
Every time you need to do analytics and you do some sort of pre-calculations upfront using

103
00:08:21,620 --> 00:08:28,980
head doops or data warehouses or any other offline and batch oriented technology, that's

104
00:08:28,980 --> 00:08:30,660
basically our enemy.

105
00:08:30,660 --> 00:08:36,300
So we're bringing the world to be completely real-time and perform all the computations

106
00:08:36,300 --> 00:08:38,180
that you need to live.

107
00:08:38,180 --> 00:08:45,260
And we deliver it on extremely low latency by leveraging an immense amount of compute.

108
00:08:45,260 --> 00:08:51,020
And that we can do very, very well because MemSQL is a scalable technology that can run

109
00:08:51,020 --> 00:08:53,700
on clusters of commodity hardware.

110
00:08:53,700 --> 00:08:58,900
So now where AI and ML comes in, well, because we support this new class of applications

111
00:08:58,900 --> 00:09:06,100
the fact that our customers are incredibly forward-looking, they want to do more with

112
00:09:06,100 --> 00:09:13,260
the technology and they want to blend classical database workloads with the new types of

113
00:09:13,260 --> 00:09:19,500
computations that are mostly stemmed from the AI and ML needs.

114
00:09:19,500 --> 00:09:24,580
And one of the big ones that we see all the time is image recognition.

115
00:09:24,580 --> 00:09:27,060
So we can talk a little more about it.

116
00:09:27,060 --> 00:09:31,500
So let's say you have an application and you use that to power and use MemSQL to power

117
00:09:31,500 --> 00:09:32,660
that application.

118
00:09:32,660 --> 00:09:37,900
The application is large scale, there's a ton of data that's stored in MemSQL.

119
00:09:37,900 --> 00:09:42,700
Like I said earlier in memory is an enabling technology, but it's not the technology.

120
00:09:42,700 --> 00:09:48,140
You can actually put a lot more data into MemSQL than the resmemory on the cluster.

121
00:09:48,140 --> 00:09:55,180
And what you want to do is you want to enable smart applications, the ones that make decisions

122
00:09:55,180 --> 00:10:01,820
on either on behalf of a user or they provide recommendations or they provide some sort

123
00:10:01,820 --> 00:10:07,380
of search capabilities on top of unstructured and semi-structured data.

124
00:10:07,380 --> 00:10:12,220
Imagine an app that has a camera, it runs only on your cell phone, you snap a picture

125
00:10:12,220 --> 00:10:20,700
with this app and just in a few tens of milliseconds this app finds similar images to the one

126
00:10:20,700 --> 00:10:22,900
that you just took a picture of.

127
00:10:22,900 --> 00:10:24,260
So why is it useful?

128
00:10:24,260 --> 00:10:28,420
Well, it's useful because you just enabled visual search.

129
00:10:28,420 --> 00:10:37,740
And the way we do it is we built some of the building blocks that allow you to run and

130
00:10:37,740 --> 00:10:42,580
operationalize the machine learning models and we built them straight into the database.

131
00:10:42,580 --> 00:10:47,460
And now the database allows you to scale them and deliver very low latencies for this type

132
00:10:47,460 --> 00:10:48,940
of operations.

133
00:10:48,940 --> 00:10:52,900
So that would be one example of an application.

134
00:10:52,900 --> 00:10:57,020
I can give you another one, which we see a lot in the IoT space.

135
00:10:57,020 --> 00:11:04,940
Before we go into the next example, can you drill down a little bit into what specifically

136
00:11:04,940 --> 00:11:07,980
MemSQL is doing to enable the first example?

137
00:11:07,980 --> 00:11:14,220
For example, are there pre-trained models for image recognition or image similarity in

138
00:11:14,220 --> 00:11:19,660
this case built into the database, kind of like you might think of a stored procedure

139
00:11:19,660 --> 00:11:22,780
or is it, you know, is there something else?

140
00:11:22,780 --> 00:11:24,860
Is it a different type of functionality?

141
00:11:24,860 --> 00:11:27,380
Yeah, it's even lower level than that.

142
00:11:27,380 --> 00:11:33,740
MemSQL certainly supports stored procedures, but in this particular case, we implemented

143
00:11:33,740 --> 00:11:39,260
a few building blocks, particularly dot product and Euclidean distance between vectors.

144
00:11:39,260 --> 00:11:42,020
Oh, that is pretty low level.

145
00:11:42,020 --> 00:11:48,100
And if you take a deep learning model and you look at the layers, you know, Matrix

146
00:11:48,100 --> 00:11:53,620
multiplication, tensor multiplication, vector multiplication, is the fundamental building

147
00:11:53,620 --> 00:11:54,620
block.

148
00:11:54,620 --> 00:12:00,660
So what we do is, as we train that model, we take all the layers except for the very

149
00:12:00,660 --> 00:12:07,540
last one and apply it on the database of images that we have and that allows us to extract

150
00:12:07,540 --> 00:12:13,380
what we call, you know, or anybody else calls a feature vector, which is just a vector.

151
00:12:13,380 --> 00:12:19,220
So once we have that and we have a model, applying that model to an incoming image, which

152
00:12:19,220 --> 00:12:24,620
you just took a picture with your cell phone, will produce another feature vector.

153
00:12:24,620 --> 00:12:30,620
And it just so happens that the multiplication of those two feature vectors normalized

154
00:12:30,620 --> 00:12:36,340
gives you the similarity score, how close those images are together.

155
00:12:36,340 --> 00:12:41,700
So the heavy lifting of building a model belongs to somewhere else.

156
00:12:41,700 --> 00:12:49,060
And the data may might as well be still stored in them SQL and we enabling very fast data

157
00:12:49,060 --> 00:12:52,420
transfer in and out of them SQL in a parallel way.

158
00:12:52,420 --> 00:12:58,980
So we can send it into Spark or TensorFlow or any other training framework.

159
00:12:58,980 --> 00:13:05,340
But once it gets to operationalizing, operationalizing that model and performing the last mile

160
00:13:05,340 --> 00:13:11,100
computation by really powering your app, then them SQL gives you that scale.

161
00:13:11,100 --> 00:13:16,340
And it allows you to perform those computations pretty much at the memory bandwidth speed

162
00:13:16,340 --> 00:13:20,020
that the labels really low latencies for that last mile computation.

163
00:13:20,020 --> 00:13:21,020
Okay.

164
00:13:21,020 --> 00:13:22,020
Doesn't make sense?

165
00:13:22,020 --> 00:13:23,020
Okay.

166
00:13:23,020 --> 00:13:24,020
No, it does.

167
00:13:24,020 --> 00:13:26,460
So you're computing these feature vectors.

168
00:13:26,460 --> 00:13:31,940
Is that happening as is that happening on right of new images or is it happening?

169
00:13:31,940 --> 00:13:34,660
I'm assuming that's the way you do it since your anti batch.

170
00:13:34,660 --> 00:13:38,620
You're not doing some big batch job that's like updating, you know, some column in your

171
00:13:38,620 --> 00:13:42,220
database with your feature vectors for all the images that are in there.

172
00:13:42,220 --> 00:13:43,220
Correct.

173
00:13:43,220 --> 00:13:47,940
And you can do, you can do either, but the typical workload is once you have that model,

174
00:13:47,940 --> 00:13:49,940
you apply that model on right.

175
00:13:49,940 --> 00:13:56,020
And we have technology, it's called pipelines that allows you to perform arbitrary computations

176
00:13:56,020 --> 00:13:59,300
either in a store procedure or an external piece of code.

177
00:13:59,300 --> 00:14:05,940
That's where you can invoke all third party libraries to apply that computation and then

178
00:14:05,940 --> 00:14:09,460
the store, the feature vector in the database.

179
00:14:09,460 --> 00:14:14,500
So you can do it in the ingest and if the, let's say you built something like you crawl

180
00:14:14,500 --> 00:14:22,380
the web or you crawl your own product catalogs and once you identify those images, you immediately

181
00:14:22,380 --> 00:14:27,380
stick them into the database as you do that, we trigger that computation.

182
00:14:27,380 --> 00:14:34,340
So the feature vector arrives into the database at the same time instantly as the actual data.

183
00:14:34,340 --> 00:14:38,380
And now it immediately participates in all sorts of computations.

184
00:14:38,380 --> 00:14:43,820
So that allows you to never have kind of stop and go computations.

185
00:14:43,820 --> 00:14:50,780
You never do, okay, step one, load all the data, step two, perform the batch computation

186
00:14:50,780 --> 00:14:55,700
on top of all the data into the database and step three, do something else with it.

187
00:14:55,700 --> 00:15:00,780
Rather than, it's all streamlined and it just flows in and out.

188
00:15:00,780 --> 00:15:04,580
So how does this play out in the IoT case?

189
00:15:04,580 --> 00:15:08,380
So in the IoT case, data is incumbent constantly.

190
00:15:08,380 --> 00:15:16,260
So one of the use cases we have with a large energy company is to ingest IoT data from

191
00:15:16,260 --> 00:15:17,740
drill bits.

192
00:15:17,740 --> 00:15:24,180
Apparently in the world of fracking, you tend to drill a lot more and then there's a

193
00:15:24,180 --> 00:15:31,220
non-trivial cost for a broken drill bit, those things are extremely expensive.

194
00:15:31,220 --> 00:15:35,340
You have to stop your operation if the drill bit breaks.

195
00:15:35,340 --> 00:15:41,620
So you're losing not only on the fact that you're fishing this thing out from the ground,

196
00:15:41,620 --> 00:15:47,220
from hundreds and maybe even thousands of miles deep, but you also not producing oil,

197
00:15:47,220 --> 00:15:49,660
which is an operational cost.

198
00:15:49,660 --> 00:15:56,460
So what we do, we ingest that real time data, IoT data, and we're scoring that data,

199
00:15:56,460 --> 00:15:59,660
applying a machine learning model in real time.

200
00:15:59,660 --> 00:16:07,100
And then there's an application built that arrest the drill bit before it hits a problem.

201
00:16:07,100 --> 00:16:12,500
Just by measuring temperature and all the very, you know, temperature, throughput, all

202
00:16:12,500 --> 00:16:18,900
sorts of kind of vital signs of the drill bit, of the drill bit as it goes through the ground.

203
00:16:18,900 --> 00:16:20,420
And so that was step one.

204
00:16:20,420 --> 00:16:27,700
Step two is obviously you feed all this information back to direct the drilling.

205
00:16:27,700 --> 00:16:30,740
So in the world of fracking, drilling is directional.

206
00:16:30,740 --> 00:16:37,460
So it's not just vertical down into the ground, but it's more like you change in the direction

207
00:16:37,460 --> 00:16:39,700
as you go and drilling.

208
00:16:39,700 --> 00:16:45,180
So using all that input, you can direct the drill bit to make the whole operation a lot

209
00:16:45,180 --> 00:16:46,180
more efficient.

210
00:16:46,180 --> 00:16:50,980
And what are some of the algorithms that come into play in that use case?

211
00:16:50,980 --> 00:16:55,260
So they started with, you know, like every typical data science, they started with some

212
00:16:55,260 --> 00:17:00,340
sort of linear regressions that they moved it to decision trees very quickly.

213
00:17:00,340 --> 00:17:04,300
And now they're experimenting with deep learning for that as well.

214
00:17:04,300 --> 00:17:09,740
And the beauty of that, of the solution that we presented is it integrates natively

215
00:17:09,740 --> 00:17:13,660
with all sorts of third party libraries.

216
00:17:13,660 --> 00:17:17,540
So we made the experimentation for them very, very straightforward.

217
00:17:17,540 --> 00:17:24,860
They started with SAS, where they produce models in SAS, as you know, SAS is a proprietary

218
00:17:24,860 --> 00:17:29,580
technology, but since they've played with Spark and now experimenting with TensorFlow

219
00:17:29,580 --> 00:17:30,580
as well.

220
00:17:30,580 --> 00:17:31,580
Okay.

221
00:17:31,580 --> 00:17:37,220
I was actually going to ask you about Spark and how that fits in with your model.

222
00:17:37,220 --> 00:17:38,220
Do you?

223
00:17:38,220 --> 00:17:43,900
I imagine you see it out and out working with customers, is it a competitive technology?

224
00:17:43,900 --> 00:17:48,820
Spark plus the rest of the Hadoop ecosystem, or is it complementary?

225
00:17:48,820 --> 00:17:50,900
Or how do you see that?

226
00:17:50,900 --> 00:17:54,260
So all the big data technologies overlap a little bit.

227
00:17:54,260 --> 00:17:57,420
In the case of Spark, I would say it's 9010.

228
00:17:57,420 --> 00:18:02,340
So it's 10% competitive, 90% complementary.

229
00:18:02,340 --> 00:18:05,100
And here is why Spark doesn't have state.

230
00:18:05,100 --> 00:18:08,500
The state is usually stored somewhere else.

231
00:18:08,500 --> 00:18:17,180
It's either HDFS, relational database, or S3, some sort of object store in the cloud.

232
00:18:17,180 --> 00:18:23,460
So what we do in this case is we provide an extremely performant state.

233
00:18:23,460 --> 00:18:27,820
And the combination of MemSQL and Spark, that's the 90% case.

234
00:18:27,820 --> 00:18:34,580
They work really, really well together because we give you that transactional, scalable

235
00:18:34,580 --> 00:18:39,260
state, and nothing else on the market can give you that state.

236
00:18:39,260 --> 00:18:45,620
So not only you can store, you can retrieve, but you can also compute.

237
00:18:45,620 --> 00:18:51,940
And we have a world class SQL query processing engine that allows you to produce reports,

238
00:18:51,940 --> 00:18:55,420
but also allows you to modify that state in a transactional fashion.

239
00:18:55,420 --> 00:18:59,420
You can say begin transaction, insert, update, delete.

240
00:18:59,420 --> 00:19:06,420
You can run it at high concurrency, and you have full durability and all sorts of guarantees

241
00:19:06,420 --> 00:19:07,660
for that data.

242
00:19:07,660 --> 00:19:10,860
So that's where we're extremely complementary.

243
00:19:10,860 --> 00:19:17,420
The typical deployment model is that there is a data lake, and the data lake stores hundreds

244
00:19:17,420 --> 00:19:20,100
of terabytes of petabytes of data.

245
00:19:20,100 --> 00:19:25,940
MemSQL is deployed alongside of the data lake to provide to power applications.

246
00:19:25,940 --> 00:19:31,420
Because you cannot write applications on top of Hadoop because Hadoop is batch.

247
00:19:31,420 --> 00:19:39,540
So now, and then Spark is the glue between those two, and it allows you to have rapid data

248
00:19:39,540 --> 00:19:45,860
transfer, all the data that is born in MemSQL based on the applications, based on the interactions

249
00:19:45,860 --> 00:19:47,540
with applications.

250
00:19:47,540 --> 00:19:48,740
That data is captured.

251
00:19:48,740 --> 00:19:54,620
You can pick it up and Spark very, very easily, we have a world class Spark connector, drop

252
00:19:54,620 --> 00:20:00,380
it in the data lake for historical, archival, compliance type storage, and then provide

253
00:20:00,380 --> 00:20:05,580
some, and then perform some overnight batch computations, take the results of those

254
00:20:05,580 --> 00:20:13,700
computations, stick it into MemSQL, and Spark often times give you that unified API.

255
00:20:13,700 --> 00:20:17,060
Because through that API, you can interact with MemSQL, you can interact with the data

256
00:20:17,060 --> 00:20:24,100
lake, and it becomes kind of the go-to API for application developers.

257
00:20:24,100 --> 00:20:29,020
And MemSQL, in this case, just gives you SQL, then you can attach a BI tool directly into

258
00:20:29,020 --> 00:20:36,660
MemSQL, and they can scale the concurrency of data scientists that attach their BI tools

259
00:20:36,660 --> 00:20:39,660
to MemSQL and look at the reports and visualizations.

260
00:20:39,660 --> 00:20:47,820
And is it primarily data scientists and folks that are end user use of MemSQL?

261
00:20:47,820 --> 00:20:55,580
Or you also, in this scenario, attaching your traditional applications to MemSQL or using

262
00:20:55,580 --> 00:21:02,140
some other state technology for building applications that refer to this data?

263
00:21:02,140 --> 00:21:07,980
Well, mostly it's actually application developers, because MemSQL powers applications.

264
00:21:07,980 --> 00:21:14,660
And because the nature of applications is changing all the time, and we have higher scale requirements

265
00:21:14,660 --> 00:21:20,340
for the applications, MemSQL is a perfect technology to power applications like this.

266
00:21:20,340 --> 00:21:24,220
I'll give you a few more examples of such applications.

267
00:21:24,220 --> 00:21:32,140
Now because modern applications have AI and ML requirements, that's where that intersection

268
00:21:32,140 --> 00:21:38,460
comes in, where you need to have those models that you produced somewhere in your data

269
00:21:38,460 --> 00:21:41,660
science lab, and you want to operationalize those models.

270
00:21:41,660 --> 00:21:44,340
And that's where MemSQL plays very, very strongly.

271
00:21:44,340 --> 00:21:49,700
So do you envision kind of a lot of lines of what Spark's done, kind of building higher

272
00:21:49,700 --> 00:21:56,580
level abstractions beyond that product and Euclidean distance to enable folks to do machine

273
00:21:56,580 --> 00:21:58,580
learning and AI more easily?

274
00:21:58,580 --> 00:21:59,580
Absolutely.

275
00:21:59,580 --> 00:22:00,580
Absolutely.

276
00:22:00,580 --> 00:22:06,820
We have a number of ideas that are circulating inside the engineering team and certainly

277
00:22:06,820 --> 00:22:11,820
influenced by our customers and what they want to do as a technology.

278
00:22:11,820 --> 00:22:20,820
So the customers see in MemSQL is that very, very fast, scalable state that they can deploy

279
00:22:20,820 --> 00:22:24,980
inside their data centers or in the cloud on the cheap, right?

280
00:22:24,980 --> 00:22:30,660
Because MemSQL provides world-class compression, it has column store technology, and that

281
00:22:30,660 --> 00:22:33,820
state is very fungible.

282
00:22:33,820 --> 00:22:39,780
Because we support transactions, you can change that state, you can enrich that data with

283
00:22:39,780 --> 00:22:44,220
attributes that you compute on the fly, et cetera, et cetera, et cetera.

284
00:22:44,220 --> 00:22:50,460
Now what people want to do is they want to perform computations that are beyond SQL.

285
00:22:50,460 --> 00:22:56,460
So we're world-class in SQL query processing, and that's great.

286
00:22:56,460 --> 00:22:59,740
And our customers want to do that and more.

287
00:22:59,740 --> 00:23:05,020
And when they want to do more right now, we resort to Spark, right?

288
00:23:05,020 --> 00:23:10,340
We say, OK, well, deploy Spark alongside of MemSQL, pull the data out, and we'll give

289
00:23:10,340 --> 00:23:16,660
you that data very quickly, perform the computation, put that data back, and then leverage some

290
00:23:16,660 --> 00:23:21,300
of the building blocks that we have, you know, basic arithmetic operations, vectorized

291
00:23:21,300 --> 00:23:28,540
operations, vector operations to do the last mile computation to power your applications.

292
00:23:28,540 --> 00:23:33,660
Now, like once you do that, you just want to take that loop and you want to tighten

293
00:23:33,660 --> 00:23:34,660
it up.

294
00:23:34,660 --> 00:23:40,620
So you start bringing all the computations that you today, today you're taking data out

295
00:23:40,620 --> 00:23:46,340
of MemSQL, put it somewhere else in a temporary store like Spark, data frames, and you want

296
00:23:46,340 --> 00:23:49,460
to perform those computations in place.

297
00:23:49,460 --> 00:23:53,700
And there are two interesting problems in that space.

298
00:23:53,700 --> 00:23:56,780
One is what the API should be.

299
00:23:56,780 --> 00:24:05,540
And today for machine learning and AI, the APIs tend to be either Spark driven or the Python

300
00:24:05,540 --> 00:24:11,260
library ecosystem driven, you know, the likes of non-py, sci-py, pandas, TensorFlow.

301
00:24:11,260 --> 00:24:15,260
So it seems like the Python world lives in one universe.

302
00:24:15,260 --> 00:24:18,540
The Spark world lives in another universe.

303
00:24:18,540 --> 00:24:23,620
And then there is the SQL universe that is pretty much ubiquitous because every database

304
00:24:23,620 --> 00:24:29,980
exposes SQL as an API and also data warehouses expose SQL as an API.

305
00:24:29,980 --> 00:24:38,880
Our view that the Spark universe and MemSQL universe should be enabled by rapid data transfer

306
00:24:38,880 --> 00:24:40,140
between the two.

307
00:24:40,140 --> 00:24:46,380
And then the Python universe should be enabled by pushing some of the computations that

308
00:24:46,380 --> 00:24:52,140
you express through Python API into the database and putting them on steroids.

309
00:24:52,140 --> 00:24:54,180
Now what exactly does that mean?

310
00:24:54,180 --> 00:24:55,460
And can you give an example?

311
00:24:55,460 --> 00:24:56,460
Yeah, totally.

312
00:24:56,460 --> 00:25:01,460
So let's say you perform an entrepreneurial computation on your data.

313
00:25:01,460 --> 00:25:05,300
Let's say you have hundreds of bytes of data, you know, certainly more data that can fit

314
00:25:05,300 --> 00:25:09,180
on your laptop and your data scientists.

315
00:25:09,180 --> 00:25:15,180
And as a data scientist, you stack, you live in the Python world.

316
00:25:15,180 --> 00:25:20,860
And you're a big expert in the libraries like pandas, non-py and sci-py.

317
00:25:20,860 --> 00:25:24,020
And let's say you're playing with TensorFlow as well.

318
00:25:24,020 --> 00:25:31,980
So it's very natural for you to express computations on that data to perform training or

319
00:25:31,980 --> 00:25:37,540
perform scoring on that data in that Python world.

320
00:25:37,540 --> 00:25:42,420
The problem is all the computations are single threaded and all the computations are in

321
00:25:42,420 --> 00:25:43,740
memory.

322
00:25:43,740 --> 00:25:49,700
So you're stuck at this point in time because you cannot access hundreds of terabytes of

323
00:25:49,700 --> 00:25:54,340
data because there's no way you can put hundreds of terabytes in memory.

324
00:25:54,340 --> 00:26:01,460
And that's where we see the world is going to where you want those computations to become

325
00:26:01,460 --> 00:26:08,100
instant and paralyze a bowl and scalable, not so you can just instead of bringing that

326
00:26:08,100 --> 00:26:15,340
data into your high memory machine, you can perform those computations in place in something

327
00:26:15,340 --> 00:26:18,900
that's very, very scalable, like MemSQL.

328
00:26:18,900 --> 00:26:21,500
So you're going to see more and more of that happening over time.

329
00:26:21,500 --> 00:26:29,660
You mentioned the three different modes of interacting with this for machine learning

330
00:26:29,660 --> 00:26:34,940
or in general, Spark, Python, and SQL.

331
00:26:34,940 --> 00:26:43,900
Are there any efforts to extend SQL to give it some kind of machine learning expressiveness

332
00:26:43,900 --> 00:26:54,020
like SQLs got all kinds of aggregation operations like select average of x, where, whatever.

333
00:26:54,020 --> 00:27:01,740
I can imagine something like select predict why, where, whatever, where you're kind of

334
00:27:01,740 --> 00:27:07,820
telling expressing in SQL that you want a prediction based on something.

335
00:27:07,820 --> 00:27:12,060
Does that exist or are folks playing with that today?

336
00:27:12,060 --> 00:27:16,660
Yeah, so if you look at the history and you look at products like, you know, good old

337
00:27:16,660 --> 00:27:23,100
products like Teradata or NETISA, or if you look at SQL server integration with R, that's

338
00:27:23,100 --> 00:27:28,860
certainly something that is happening where trading is not happening in database, but scoring

339
00:27:28,860 --> 00:27:31,980
is, that's a natural way to do things.

340
00:27:31,980 --> 00:27:37,660
We achieve the same functionality through pipelines where we score data on ingest.

341
00:27:37,660 --> 00:27:43,100
So I think this is valuable and you'll see database is exposing more and more primitives,

342
00:27:43,100 --> 00:27:48,380
the likes of dot product and Euclidean distance, but you'll have instead of two, you will have

343
00:27:48,380 --> 00:27:54,300
you know, a hundred of those built in into the database and then you will see packages

344
00:27:54,300 --> 00:28:02,940
where it's just a package of storage procedures, which allows you to to run those predictions.

345
00:28:02,940 --> 00:28:06,860
And those will work really well for simple use cases.

346
00:28:06,860 --> 00:28:12,580
Now, if you look at the world of data scientists today, they actually don't like that.

347
00:28:12,580 --> 00:28:19,060
They like to live in the world of data frames where there's a lot more control over the

348
00:28:19,060 --> 00:28:21,980
types of computations that people are expressing.

349
00:28:21,980 --> 00:28:27,300
People like to, people understand that world of data frames very, very well.

350
00:28:27,300 --> 00:28:34,540
So I think the future is going to be actually in paralyzing and speeding up computations

351
00:28:34,540 --> 00:28:39,420
inside data frames, so that's just my opinion.

352
00:28:39,420 --> 00:28:44,260
Do you envision doing training directly in the database?

353
00:28:44,260 --> 00:28:49,260
In the same way as I described the interaction with the data frames.

354
00:28:49,260 --> 00:28:55,900
So if you want to enable inference in the database, then you don't want to have the database,

355
00:28:55,900 --> 00:28:59,020
you don't want to have the database to be a crutch where you're constantly fighting and

356
00:28:59,020 --> 00:29:02,940
trying to shoehorn that computation into SQL.

357
00:29:02,940 --> 00:29:08,980
The natural way to expressing those computations is operations on top of data frames.

358
00:29:08,980 --> 00:29:16,740
However, if you make the database, naturally support those data frame computations, then

359
00:29:16,740 --> 00:29:21,700
you have tremendous value from the fact that the database actually owns the state.

360
00:29:21,700 --> 00:29:27,300
And you never need to transfer that state from your storage to something else.

361
00:29:27,300 --> 00:29:30,740
So you're bringing computations closer to data.

362
00:29:30,740 --> 00:29:36,820
So that's where I see the industry is moving in the next five years.

363
00:29:36,820 --> 00:29:39,340
You just mentioned bringing computations closer to data.

364
00:29:39,340 --> 00:29:45,020
And that's obviously been one of the things that the Hadoop ecosystem has made more readily

365
00:29:45,020 --> 00:29:50,180
accessible to folks, this idea of data locality.

366
00:29:50,180 --> 00:29:57,300
Do you get to take advantage of that with machine learning types of models in general and

367
00:29:57,300 --> 00:30:05,820
this pipelining approach that you have in particular, or are the pipelines run outside of separate

368
00:30:05,820 --> 00:30:08,460
from any concept of locality of data?

369
00:30:08,460 --> 00:30:15,980
Well, locality is a loaded concept, so you can start with, OK, well, I want to perform

370
00:30:15,980 --> 00:30:21,380
computation exactly on the same machine where the data is stored.

371
00:30:21,380 --> 00:30:23,980
So that's kind of the extreme case of locality.

372
00:30:23,980 --> 00:30:31,300
Another way to think about is as you look at the computation plan, let's say in the database

373
00:30:31,300 --> 00:30:34,420
terminology, there would be a query plan.

374
00:30:34,420 --> 00:30:41,100
So let's extend the definition of a SQL query plan to a broader concept of you performing

375
00:30:41,100 --> 00:30:44,220
some sort of arbitrary scalable computation.

376
00:30:44,220 --> 00:30:47,580
And you know all the steps in the computation up front.

377
00:30:47,580 --> 00:30:54,300
So you can optimize those computations and produce a query plan for those computations and

378
00:30:54,300 --> 00:30:59,460
you will run that query in the distributed environment.

379
00:30:59,460 --> 00:31:06,140
Now then you look at this and certain primitive computations, you really, really want to

380
00:31:06,140 --> 00:31:12,820
perform closer to the data because you will save tremendously on the amount of IO that

381
00:31:12,820 --> 00:31:15,580
is happening for in the data transfer.

382
00:31:15,580 --> 00:31:23,100
Now from there, if you do that, you also deliver on the concurrency of such computations.

383
00:31:23,100 --> 00:31:31,380
So now you can perform those computations at a highly concurrent environment where thousands

384
00:31:31,380 --> 00:31:39,100
of data sciences are attaching to the same data that is collected and centralized and stored

385
00:31:39,100 --> 00:31:44,540
in something like MCQL and then they are performing their computations concurrently.

386
00:31:44,540 --> 00:31:51,020
So there are no opening up all the data to the organization and not having any data silos.

387
00:31:51,020 --> 00:32:00,180
So data locality for computations is useful where it makes sense and not as useful in a

388
00:32:00,180 --> 00:32:03,940
broader general purpose way.

389
00:32:03,940 --> 00:32:10,140
And the perfect system would be engineered around pushing the computations closer to

390
00:32:10,140 --> 00:32:16,780
the data where it makes a ton of sense. For example, you do a lot of filtering or you

391
00:32:16,780 --> 00:32:20,100
performing a lot of what it's called group by operations.

392
00:32:20,100 --> 00:32:23,820
And those operations make sense to be done locally.

393
00:32:23,820 --> 00:32:29,980
Those operations make sense to be done in a vectorized fashion by leveraging the latest

394
00:32:29,980 --> 00:32:35,700
and greatest CPU instructions in the Intel hardware and so on.

395
00:32:35,700 --> 00:32:40,180
It makes sense to leverage indexes so you prune massive amounts of data so you don't

396
00:32:40,180 --> 00:32:42,900
even touch them for your computations.

397
00:32:42,900 --> 00:32:48,700
And then from there, it makes sense to bring all that data into a stateless distributed

398
00:32:48,700 --> 00:32:52,660
environment and perform the rest of the computations.

399
00:32:52,660 --> 00:33:00,420
So that approach allows you to build greater scalability for your system compared to a traditional

400
00:33:00,420 --> 00:33:03,380
database compared to Hadoop or compared to Spark.

401
00:33:03,380 --> 00:33:10,540
Okay. And now you just mentioned the taking advantage of the instruction sets of the underlying

402
00:33:10,540 --> 00:33:18,300
hardware. And you guys are making an announcement with Intel at Amazon re-invent next week.

403
00:33:18,300 --> 00:33:19,780
Is it related to that area?

404
00:33:19,780 --> 00:33:26,300
Definitely. Intel has been supporting vectorized instructions for some time for over a decade.

405
00:33:26,300 --> 00:33:33,860
And every new generation of CPUs increases the size of the vector that you can use and perform

406
00:33:33,860 --> 00:33:36,820
vectorized operations on top of.

407
00:33:36,820 --> 00:33:41,340
And where we stand right now, it's 512 bits.

408
00:33:41,340 --> 00:33:45,780
So compared to matrices in a GPU, this is tiny.

409
00:33:45,780 --> 00:33:51,860
However, when you perform a last mile computation like dot product, it allows you to perform that

410
00:33:51,860 --> 00:33:56,740
computation much faster than the memory bandwidth that you have.

411
00:33:56,740 --> 00:34:04,420
So you actually don't need more for that last mile computation than AVX 512 because your

412
00:34:04,420 --> 00:34:06,940
limitation is not your compute.

413
00:34:06,940 --> 00:34:09,620
Your limitation is memory bandwidth.

414
00:34:09,620 --> 00:34:16,540
This is very, very different when you train models and perform computations for building

415
00:34:16,540 --> 00:34:18,860
deep learning.

416
00:34:18,860 --> 00:34:23,180
That's where you perform a lot of tensor multiplication.

417
00:34:23,180 --> 00:34:29,260
And they mod of compute compared to the size of your data set is tremendously more when

418
00:34:29,260 --> 00:34:31,900
you perform something like dot product.

419
00:34:31,900 --> 00:34:34,420
That's where you want GPUs.

420
00:34:34,420 --> 00:34:39,220
But once the model is built and the model is trained, you actually don't need GPUs to

421
00:34:39,220 --> 00:34:42,260
score that model in many, many cases.

422
00:34:42,260 --> 00:34:48,580
What that gives you is it gives you the ability to democratize that computation because

423
00:34:48,580 --> 00:34:51,020
Intel CPU is everywhere.

424
00:34:51,020 --> 00:34:54,900
If a machine has a GPU, it also has an Intel CPU.

425
00:34:54,900 --> 00:34:59,260
And the point that I'm making that in many cases, all you need is a CPU and you're not going

426
00:34:59,260 --> 00:35:05,460
to get the computation faster if you add a GPU to the system simply because it's the memory

427
00:35:05,460 --> 00:35:09,020
bandwidth that's your bottleneck in the computation.

428
00:35:09,020 --> 00:35:16,060
Now we use AVX 512 for vector dot product, for Euclidean distance, and for other vector

429
00:35:16,060 --> 00:35:19,540
operations that were built into the database.

430
00:35:19,540 --> 00:35:25,860
We also use AVX 512 for general purpose SQL computations.

431
00:35:25,860 --> 00:35:34,460
And that allows you to just deliver on orders of magnitude faster SQL query processing

432
00:35:34,460 --> 00:35:38,780
that our competitors have and certainly what Hadoop has or spark.

433
00:35:38,780 --> 00:35:44,660
We're huge fans of that technology and we can't wait when Intel allows us to perform

434
00:35:44,660 --> 00:35:49,060
computations on larger vectors, not just 512 bits.

435
00:35:49,060 --> 00:35:55,500
Can you be more specific in terms of some of the actual results you've seen in production

436
00:35:55,500 --> 00:35:57,540
systems in terms of performance differences?

437
00:35:57,540 --> 00:35:58,860
Yeah, absolutely.

438
00:35:58,860 --> 00:36:06,060
So with MemSQL 6.0 that we just released in October, we can do things like a group by

439
00:36:06,060 --> 00:36:12,460
operation on 100 billion row table in a sub-second.

440
00:36:12,460 --> 00:36:15,060
So 100 billion?

441
00:36:15,060 --> 00:36:16,060
Yes, wow.

442
00:36:16,060 --> 00:36:22,860
So it says a sub-second operation that runs something like give me an average stock price

443
00:36:22,860 --> 00:36:30,020
over 100 billion data points and group it by security or by a stock value.

444
00:36:30,020 --> 00:36:36,060
The computation itself is relatively straightforward but because we perform that computation, quote

445
00:36:36,060 --> 00:36:45,140
unquote, on compressed data and we're using those vectorized operations, we achieve the

446
00:36:45,140 --> 00:36:49,820
performance of a billion operations per second per core.

447
00:36:49,820 --> 00:36:55,300
So if you throw 100 cores into the system and MemSQL is extremely scalable so you can

448
00:36:55,300 --> 00:37:01,500
throw 100 cores or you can throw a thousand cores, you can achieve this type of performance

449
00:37:01,500 --> 00:37:03,060
on your system.

450
00:37:03,060 --> 00:37:08,980
And so that's using the AVX512 instructions, what were you seeing prior to that?

451
00:37:08,980 --> 00:37:14,740
Well, there's a combination of two techniques that goes into this type of performance.

452
00:37:14,740 --> 00:37:19,580
The first is how can you perform operations on compressed data?

453
00:37:19,580 --> 00:37:24,740
So if, like I said earlier, oftentimes it's the memory bandwidth that's the bottleneck

454
00:37:24,740 --> 00:37:26,060
of your computation.

455
00:37:26,060 --> 00:37:32,420
So the better you compress but the better off you are in the final computation because at

456
00:37:32,420 --> 00:37:35,420
the end of the day you scan fewer bytes.

457
00:37:35,420 --> 00:37:37,220
What exactly do we mean by compressor?

458
00:37:37,220 --> 00:37:43,260
Is it kind of your traditional binary compression or are we also talking some kind of deduplication

459
00:37:43,260 --> 00:37:44,780
or something like that?

460
00:37:44,780 --> 00:37:45,780
It's a combination.

461
00:37:45,780 --> 00:37:52,780
So first of all, the compression is called column store compression and we also encode

462
00:37:52,780 --> 00:37:53,780
similar values.

463
00:37:53,780 --> 00:38:00,140
So let's say you, in that example that I said was stock trading, you have 10,000 different

464
00:38:00,140 --> 00:38:01,140
stocks.

465
00:38:01,140 --> 00:38:06,220
So if you have 10,000 different stocks, then each individual stock can be encoded with

466
00:38:06,220 --> 00:38:07,940
just a few bits.

467
00:38:07,940 --> 00:38:14,140
And so then you only use this many bits to represent each stock value.

468
00:38:14,140 --> 00:38:21,340
And as you perform your computations, you never go back decoding those bits into, let's

469
00:38:21,340 --> 00:38:26,620
say, an integer value or got for bit a string value of a stock, right?

470
00:38:26,620 --> 00:38:30,140
Because those computation will become much more expensive.

471
00:38:30,140 --> 00:38:36,420
So now that you perform computations on compressed data, the second step that you do is you optimize

472
00:38:36,420 --> 00:38:42,940
your computation for as few branch mispredictions and as few cash misses as possible.

473
00:38:42,940 --> 00:38:46,740
And those are a big deal because every time you have a branch misprediction, you flash

474
00:38:46,740 --> 00:38:48,420
your CPU pipeline.

475
00:38:48,420 --> 00:38:51,140
So that slows down your computation.

476
00:38:51,140 --> 00:38:55,900
So if you express your computation, so there are no branch, no branches basically.

477
00:38:55,900 --> 00:39:00,020
And there are some very, very cool papers out there and there are some innovation that

478
00:39:00,020 --> 00:39:03,100
we've done here at MCQL as well.

479
00:39:03,100 --> 00:39:06,300
So that will give you the second boost.

480
00:39:06,300 --> 00:39:12,340
And then the third boost is now you take all those bits that you represent your values

481
00:39:12,340 --> 00:39:18,580
and you perform vector operations on top of those encoded values.

482
00:39:18,580 --> 00:39:21,820
And then that's where you use AVX512.

483
00:39:21,820 --> 00:39:30,100
So now, and that gives you another, I would say, three to five X performance improvements

484
00:39:30,100 --> 00:39:32,380
just by using AVX512.

485
00:39:32,380 --> 00:39:37,900
But if you take that, plus you take operations on compressed data, plus you take care of

486
00:39:37,900 --> 00:39:39,660
branch misprediction.

487
00:39:39,660 --> 00:39:42,340
You multiply those all those together.

488
00:39:42,340 --> 00:39:47,500
And then you can routinely see two orders of magnitude improvement in performance.

489
00:39:47,500 --> 00:39:48,500
Wow.

490
00:39:48,500 --> 00:39:49,500
That's pretty fantastic.

491
00:39:49,500 --> 00:39:52,500
The kind of the point here is fancy hardware is cool.

492
00:39:52,500 --> 00:39:55,180
And you want to use it where it's needed.

493
00:39:55,180 --> 00:39:59,420
But also, there's a lot of potential in the hardware you have at hand.

494
00:39:59,420 --> 00:40:04,820
And if you take full advantage of this of that hardware, you will have remarkable results.

495
00:40:04,820 --> 00:40:05,820
Mm-hmm.

496
00:40:05,820 --> 00:40:06,820
Oh, that's great.

497
00:40:06,820 --> 00:40:15,500
I guess as we kind of wrap up, do you have any additional kind of advice to folks that

498
00:40:15,500 --> 00:40:21,900
are looking to, you know, get better, you know, to just to take better advantage of, you

499
00:40:21,900 --> 00:40:26,540
know, whether it's their, you know, their data storage systems, their, you know, hardware

500
00:40:26,540 --> 00:40:33,900
systems, and to use these to make better, you know, predictions and better utilized machine

501
00:40:33,900 --> 00:40:34,900
learning in AI.

502
00:40:34,900 --> 00:40:37,580
Well, my advice here is, don't settle.

503
00:40:37,580 --> 00:40:43,340
There are systems out there such as memtique will that allows you to take pretty much your

504
00:40:43,340 --> 00:40:50,580
end-to-end machine learning AI and application development pipeline and make them completely

505
00:40:50,580 --> 00:40:52,460
real-time.

506
00:40:52,460 --> 00:40:58,620
So whatever you do in batch, whatever you have to wait for, what the right technology

507
00:40:58,620 --> 00:41:02,700
can be squished down to zero.

508
00:41:02,700 --> 00:41:04,300
Is that the technical term?

509
00:41:04,300 --> 00:41:05,300
Squished to zero?

510
00:41:05,300 --> 00:41:08,340
Yeah, let's say that.

511
00:41:08,340 --> 00:41:09,820
And that's where the world is going.

512
00:41:09,820 --> 00:41:17,140
So we'll live in the world in the future where compute and storage are going to be utilities.

513
00:41:17,140 --> 00:41:21,860
And you know, if you're willing to pay for compute, you will have as much compute as you

514
00:41:21,860 --> 00:41:27,580
need, at any concurrency as you need, and any latency as you need.

515
00:41:27,580 --> 00:41:33,860
And the only thing that is going to be bounded by is the amount of money you pay for that

516
00:41:33,860 --> 00:41:34,860
compute.

517
00:41:34,860 --> 00:41:35,860
Great.

518
00:41:35,860 --> 00:41:41,140
Well, Nikita, thank you so much for spending the time to chat with me, I really appreciate

519
00:41:41,140 --> 00:41:42,140
it.

520
00:41:42,140 --> 00:41:43,140
Yeah, absolutely.

521
00:41:43,140 --> 00:41:47,380
Thanks for having me.

522
00:41:47,380 --> 00:41:50,340
Alright everyone, that's our show for today.

523
00:41:50,340 --> 00:41:55,780
Thanks so much for listening and for your continued feedback and support.

524
00:41:55,780 --> 00:42:00,860
For more information on Nikita or any of the topics covered in this episode, head on over

525
00:42:00,860 --> 00:42:05,140
to twomolei.com slash talk slash 84.

526
00:42:05,140 --> 00:42:11,980
To follow along with our AWS reinvent series, visit twomolei.com slash reinvent.

527
00:42:11,980 --> 00:42:16,820
Of course, we would love to receive your feedback or questions, either via a comment on the

528
00:42:16,820 --> 00:42:22,860
show notes page or via Twitter to at twomolei or at Sam Charrington.

529
00:42:22,860 --> 00:42:26,980
Thanks once again to Intel Nirvana for their sponsorship of this series.

530
00:42:26,980 --> 00:42:32,980
To learn more about deep lens and the other things they've been up to, visit intelnervana.com.

531
00:42:32,980 --> 00:42:36,740
And thank you once again for listening and catch you next time.

