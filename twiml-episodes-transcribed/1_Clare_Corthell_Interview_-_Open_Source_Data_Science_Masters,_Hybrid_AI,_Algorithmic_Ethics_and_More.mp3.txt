Hello everyone and welcome to the podcast.
If you're looking for this week in machine learning and AI, you are in the right place.
But if you're a regular listener, you may be wondering what happened to the teaser and
the awesome intro music.
Well, this is going to be a different kind of show, so I've skipped the usual intro to
avoid confusion.
This week we're going to take a break from the news format, and I've got a really interesting
interview to share with you in its place.
As you may recall, I spent Thursday at the Rangle conference in San Francisco, which
was organized by Claude Ara, who was kind enough to sponsor the past couple of episodes
of the podcast.
I'm really glad I went to that event.
The program was super solid and I met a bunch of great people.
One of those people was Claire Cortell, whose work was discussed in one of the very first
episodes of the podcast, and she was kind enough to agree to be interviewed for the show.
We had a really fun discussion and touched on a bunch of interesting topics, including
her background and what she's been up to, the open source data science master's project
that she created, getting beyond the beginner's plateau in machine learning and data science,
hybrid AI, which is of course the topic of the article of hers that we talked about
on the podcast, and a recurring topic both here on this week in machine learning and
AI, but also at the Rangle conference, and that is algorithmic ethics.
Before we jump into the interview, a few quick logistic notes.
First, what about the news?
Well, if you've already signed up for the email newsletter that I've been talking about
on the past few podcast episodes, you'll be receiving a summary of the week's news
right in your inbox on Monday morning.
If not, it's not too late to sign up at twomlai.com slash newsletter.
Second, if you're excited about machine learning and AI and you've got research or writing
skills, I'm looking for correspondence to contribute to the podcast and or the twomlai.com website.
Shoot me a note at sam at twomlai.com if you're interested.
Finally, I had a blast doing this interview and I want to know what you think about it and
the interview format in general.
As always, you can reach out to me at at Sam Charrington on Twitter, S-A-M-C-H-A-R-R-I-N-G-T-O-N
with your comments, questions, or suggestions.
All right, let's get to it onto the interview.
All right, so I'm here with Claire Corthel at the Rangle conference in San Francisco.
Hey, Claire, right to finally meet you in person.
Hi, great to meet you in person, too, Sam.
Yeah, so what's particularly exciting about getting to talk to you is I talked about
your post a few, I guess it was one like the second podcast I did.
You wrote that post around the same time on the hybrid AI and I thought that that was
a really interesting post and it was one of the things that I talked about on the podcast.
So I'd be looking forward to catching up with you on that as well as kind of getting an
update on what you're up to and what you've been digging into.
So maybe we can talk a little bit about your background and kind of how you got into
data science and machine learning.
Yeah, not a problem.
So I'm a little bit weird because I'm not a theoretical physicist or some of us in
data science are applied physicists, too, but I'm not in that camp.
I actually started with product design and came into it from that perspective.
So I was actually working on a very small product, kind of a startup within a startup when
I decided that I wanted to understand more about our users and what they were doing.
So I went to the parent company and I said, hey, I don't know much about this, but I think
I should look into our user logs and try and understand more about how people are accessing
the product and what might be happening and do some basic analytics and the head of
engineering kind of looked at me and goes, hey, what logs?
And I thought, well, this is problematic.
How am I going to learn about my users if I don't have that?
And it kind of started me on this long rabbit hole, which is now turned into my career.
And I actually very much overshot that I did not end up in analytics.
I work on machine learning applications and that problem space now, but I came at it from
that perspective.
And at that point, I was really looking for a new direction and decided to invest the
next seven months in learning everything I could to prepare myself for a career and data
side at data science.
So I built a curriculum because at the time, I think insight had just started.
So this was early 2013 and there weren't any academies that focused on this.
So I built a curriculum around that and published it on GitHub and that's become a popular
resource for people who want to get into data science and understand what it's all about
because there wasn't really a road map at that point.
But after the open data science masters, is that what that's all about?
Open source data science masters, yes, the very descriptive, unimaginative name that
I gave it.
So it's exactly what it is though.
So that was a really challenging project to work on.
And I'm really happy that I was able to put that up in the open source world and to give
you a preview.
I'm working on a second version.
There were breaking changes that floated up from Coursera who took a bunch of their
courses offline.
And so, yeah, I'm working on bubbling up those dependency problems and fixing them.
So after that, I went to a company called Madermark.
They're actually around the corner from where we are now.
And they try to measure private company growth.
So very similar to what Bloomberg does for public market companies and at the time when
I joined that company, the CTO was ready to divest in machine learning.
He was convinced it was not going to solve the problems that they were facing.
It wasn't going to pose a reasonable set of solutions for their near term and midterm
goals as a company.
And I, on my first day, designed the key components of how we would move that strategy forward.
That company now has a, I think it's a 10 person team working on data analysis and machine
learning and they're going strong and I, I moved on from that to consulting about a year
and a half ago and have been working with companies of various stages and sizes on getting
started with data science or getting started with new functions within data science that
they want to spin up on.
So helping them understand how to get from A to B and what it's going to cost them for
a solution space that they're investigating.
There's a lot there to dig into on the open source data science masters.
Now, was that a little bit of kind of building the, building the parachute as you're jumping
out of the plane or building the airplane as you're taking off that kind of thing where
you were learning as you went to want?
Right.
Or collecting laundry as I was falling out of the sky, I guess.
Something like that.
There's it.
There's an appropriate analogy.
Right.
I like that.
Absolutely.
It was perhaps most challenging because I had to rewrite it as it was going.
So I would continually check in with people I knew in industry and try and navigate to figure
out what, what skills were actually applicable, what kind of depth I needed to go in on particular
topics.
What was actually key to understand and to the state, this is something that I, I hear a
lot about from people who are hiring managers that when they try to hire people who are very
fresh to the fields, sometimes they don't have the wealth of intuition distributed into
the right places.
So they may know how to build a model, but they don't know how to validate it and they
don't know perhaps how to test data or work with data sets that are very messy.
There are various kind of drawbacks to having a self-guided education and having to retarget
that as you go is certainly challenging.
So.
And were you, did you learn yourself through a self-guided kind of approach?
Did you collect all this by, you know, in the process of learning it or what was, I
guess, what was the background that you brought to getting into data science?
Where did you start?
Yeah.
I will be clear.
I have a degree from Stanford in product design, but it's through a department called Science
Technology and Society, and it's actually a hybrid engineering program.
So you take two engineering tracks at once, and then it ties together with this ethics
component, which is part of the reason that I talk a lot about ethics publicly, which
we had an STS at RPI also.
Oh, that's great.
That's great.
They snuck it into various places.
It's a sister program to Simpsis, which became a more known program recently, because
one of the Instagram founders came from that program, in any case, a very small program
at that point in time.
Now it's one of the biggest, and I focused in computer science and product design through
mechanical engineering.
So it was very product focused, but through those two lenses of engineering.
So I came into this with a background in web stack engineering and UX and full digital
product design, but I wasn't coming at it from having no programming experience.
So moving into a Python workflow and using tools and technologies like SQL that I'd seen
before was not the primary challenge.
So I definitely wasn't starting from zero, like some people do.
Yeah.
What about the STATS component, where did that come from?
I had taken some STATS classes in college, but there was actually one that I loved, and
the professor thought I was the weirdest person in his course, I'm sure, because it was
a bunch of people who wanted to go into management consulting.
It was a kind of operational statistics class, like how do you understand how cars pass
through four toll booths when you have, you know, two of them open at any given time
block, you know, it's kind of convex optimization problems.
And I thought it was just the most interesting stuff, and I couldn't come up with an application
that was anywhere close to a career that I thought I might have.
I just had no idea how this stuff would be applicable.
Same thing with linguistics.
I, for a long time, would read linguistics textbooks and read a lot of known Trump ski
when I was in high school and more theory behind it, love that stuff.
My parents thought I was going to major in it, and I said, it's not applicable, I can't
use it.
Yeah.
Of course, flash forward to several years later, and I'm actually working quite a lot
with unstructured text, and that's actually the biggest request that I hear in the market
as a consultant.
How do we work with text and understand it through a lens that works for us and isn't
just a word cloud or a count of various themes coming up?
How do we understand it from the perspective of the customer service industry, or we talked
here earlier about data science and HR and understanding feedback, those types of applications
are becoming very popular and widely requested.
So things always come back, right?
One of my favorite designers has this, well, the thing that he paints on billboards.
These guys, Steven Seagmeister, he says, everything I do always comes back to me, and I think
about those all the time, because there are always these little things, these little vignettes
that you take, and you never quite know when you're going to come back to that, and it's
going to be relevant to what you're doing.
Yeah.
Yeah.
So you've built the open source data science masters for folks that are starting at
zero and trying to work their way to, or starting someplace, and trying to work their
way forward.
What are the things that you find folks struggle with the most?
The biggest challenge for the curriculum and for people going through it right now, I
will say this is the two-sided problem, is that people can't find problems that are
appropriately scoped to showcase their talent, and the curriculum can't necessarily provide
that right now.
I have investigated how I might go about providing sample data sets and questions alongside
them that would give you a take-home package of something that would showcase your skills,
but it's actually a lot more work than you do expect, and it's very difficult because
it's a scoping problem at its heart.
You have to have something that has enough depth, but isn't overwhelming, and can showcase
a bunch of different skills, so it's a big challenge for people to sell themselves through
providing that type of portfolio piece.
And at this point, I think Kaggle does a really good job of curating data sets and providing
conversations around analysis and modeling and predictive algorithms and ways to approach
problems, and I usually direct people.
I was going to ask you if Kaggle was one of the places that you point people.
Yeah.
I think they do a really good job with that, so they, I mean, their entire model is built
around that type of work, appropriately, scoping questions around a set of data, and a lot
of people to work on it, and sometimes rewarding them for that.
Yeah.
It's interesting that the podcast has a lot of folks that are somewhere on that curve.
I hear from folks every once in a while asking about how they might apply machine learning
AI to health care, or some problem that they have an interest in.
And it's difficult to manage that scope as a beginner in part because you don't know
what you don't know, right?
Absolutely.
But at the same time, a lot of times when you go to some of the public forums where people
are asking, how do I learn this stuff?
People will say, well, go take this course or course, that course or course, and then
go work on a project.
And the gap between take this course or course, and then go work on a project is actually
pretty huge.
Yes.
Yes.
And it's a gap that you have to fill with building analytical intuition, which is something
that is very hard to teach, but is very learnable.
So there's that counter intuition there that it's something that you can learn, and it's
best learned from other people, but it's very hard to learn it from a book.
So I do encourage people to use that practice, and for example, looking at a Kaggle competition
around health care data and taking a stab at it without seeing what other people are
working on, given a question, and then coming back to see how other people address that
question, very useful workflow, and does provide you some of the asynchronous communication
that you would otherwise have in person in a company.
The other key component there that I think is really helpful is to have questions that
are actually appropriate for the data, and to be very strict about your own workflow
when you're answering that question, because you can get lost in the weeds everywhere.
And in fact, I'd say most data science teams, their biggest struggle is not necessarily
with structure, but with the rigor of having questions that they can actually test and
hypotheses that they can actually test against.
And I certainly do know teams that have a more R&D approach, and that can lead you to interesting
places, but it doesn't necessarily help you answer a question, because you're not necessarily
restricting yourself to that path.
Yeah.
Yeah.
So you've got a teaching vent.
You put together this set of resources, and the natural step consulting, right?
We're here teaching clients that are actually trying to build these teams.
Yeah, exactly.
Exactly.
It is very natural.
And I think the biggest reward that I get in consulting is when I work with someone
who's a little less technical or more distant from the data science, and they start to understand
and into it as people who would be new to data science, they start to into it about what's
going on with the data and how you can answer the question with the data and why it's appropriate
or not appropriate and what manipulations they need to make and what type of data they
need to make in intuitions about what they can do with it.
So that's really rewarding.
I've had the pleasure of working with a couple of product teams, and product teams are great
because they have a vision for what they want as an outcome, and that outcome is really
helpful, much like a driving question or hypothesis to guide you through a set of possible solutions
with a lot of rigor and direction.
So that's been really rewarding to see product managers saying, hey, I think this will work
because I know that it worked in this other case, and we learned about that a couple of
weeks ago, and it is very much like teaching.
Now knowing a little bit about your background now, I can almost imagine the context out of
which the hybrid AI blog post came, you know, product teams telling you, oh, can we just
throw AI at this and not have any humans in the loop?
Yeah.
Did you hear a lot of that?
I've definitely heard that like, well, we know that we have to have some mostly human
approach, and we can use some predictive technology alongside them for a while, but we're
really shooting for 100% at the end.
And that ultimate vision is very problematic because as I explain in the post, and I can
summarize that briefly, but hybrid AI in cases where you need to make sure that you
need people to look at data where you're not certain how to predict an outcome or classify
or whatever your objective is, have them look at that poorly or less confidently predicted
data and make their own judgment about what should happen, allowing that to happen incorporates
the possibility of future outcomes and future inputs.
In cases where you haven't seen everything that you could possibly see because in the
future there will be new and different options, it's only necessary that you would always
involve people because you have to incorporate those new opportunities and those new possibilities.
So I think tempering our expectations about how much work computers will do and what type
of work they will do is really key to building the right solutions because otherwise we don't
have a good Pareto 8020 approach to our problems where we can say, hey, let's set aside this part
of the problem because we know it's always going to be too hard for the computer.
It will cost us 80% of our time to solve 20% of the problem and it doesn't actually make
sense.
It's just route that to people.
We might learn more about that problem space in the future, but we also know that there's
kind of slop that we always need to account for and that's important.
And it sounds like you don't think we're anywhere near, you know, closing that gap, getting
to the humans out of the loop.
It depends on the application you're looking at, absolutely.
We used a human and the loop system at Mattermark for various tasks on the machine learning
team and we actually had people in-house in addition to some systems where we had outside
labeling done.
And we had used vendors for that type of thing.
There are a couple good options there, but I think there's a lot of work to do.
I think the pragmatism on the shape of the solution, the solution space that's possible
to achieve in a reasonable amount of time and any sort of reasonable cost for a solution
all drive us toward this hybrid case.
And you also discover pretty interesting things when you use people or services that have
people labeling data or providing feedback because they will give you more information
than you asked for in some cases.
And we actually have had people in the past come back to us, find our email addresses.
I don't think they were given them, so they actually went out and did research, found
out how to email us and said, hey, you asked me this question.
I actually think there's kind of an issue with how you phrased it.
It doesn't fully address this other issue.
Have you thought about that? I'm worried that I answered the question wrongly.
I mean, these are people that were on your labeling team that felt so compelled to.
These are actually people that weren't even on the team.
They were a part of an outsource group of people that were paid to work on that data.
So you learn a lot because you get more perspectives and more eyes on the data, which is always
a good thing, especially when you're thinking about blind spots that you might have.
Yeah, I thought even one of the simple things that I thought was pretty interesting about
that post was you presented some kind of broad brush stats, and I don't remember the specific
stats about something along the lines of AI by itself right now, a machine learning solution
can get to 90% accuracy for generalized speech interpretation.
But in order to really be usable, it needs to be 98 or something like that.
I forget the numbers, but I think, you know, I don't think people think about that enough.
They don't. They don't. So it's funny.
When humans look at data, they have a very different perception of it than when they look
at the metrics about the data. So for example, if you have a classifier for five classes,
and you look at a 75% accurate classifier over all of those classes, it will look like garbage
to you as a human. Even though that's pretty high, relatively speaking, and you probably
did some work to get it to that point, you would probably still call it an unacceptable
option or a non-prefable classifier because that's, it looks like garbage to you.
I think the cases where that garbage matters is where we have to worry about the way that
we build hybrid into solving that last component and getting to 99% or 95% or whatever we need
to feel good about the application. For example, if we're predicting the health outcomes
for a person, that's a very high stakes prediction, we would probably want to skew much further
in the hybrid direction or in the human augmented direction than otherwise because the stakes
are actually very high. So I think when we start to discriminate between types of applications,
that's where we see this coming in. But even for consumer applications like Google
Knowledge Cards, things like that, people still curate a lot of that information. It's
not necessarily summaries that are generated by a computer. Sometimes there are people
that are taught to create that data in a particular way. And I think we saw a great example
of this a couple of weeks ago when news about how Facebook curates news articles came out.
And that's a very good example of how your definitions of taxonomies, your acceptance of
how things are classified and your incorporation of new information all impact your end user.
And sometimes in very critical ways, it might sway how someone votes. It might give someone
a perception of the world that they otherwise might not have in that case. So I think we're
starting to see the impacts as well from consumer applications that we thought were not so
high in terms of risk. And I look forward to seeing what they invest in at Facebook because
I think I would wager that they have people working on this that have an eye on how to
make this better. But at the end of the day, you do end up in a semi-political discussion
about what fair and balanced means in journalism. And it becomes very domain specific. So I think
it's healthy for society to grapple with that and for us to think very critically about
how these things are actually working and sort of just engineering them away and having
a 100% machine solution. Are you aware of anyone, any groups working on the problem of
hybrid, either from an academic, other academic research topic areas in there somewhere or tools,
platforms, or is it, you know, everyone kind of figuring this out on their own building
their own custom thing? And that's just the state of the art right now.
So the short story is that a lot of companies do build their own custom platforms for
doing this. They usually leverage some sort of marketplace for data entry, data annotation,
a question answering, and broader products like Amazon Turk is a very broad product. You
can arbitrarily give people tasks and you place a bid on how much you would pay people for
those tasks and they can choose to accept it. So a lot of companies will use that platform
and build on top of it and do a lot of integration of that type of system on the back end. So in
some ways, you know, they call this artificial artificial intelligence. In some ways, the
component is actually a technology interface itself, which is very interesting to think
about because there are people on the other side of the other side of the technology.
But there are a couple other vendors that do things to support hybrid. Crowdflower is one
in San Francisco that does some of that. To my knowledge, they do interobserver validation
basically to give you multiple, multiple sets of eyes on a given answer to a question
to ensure that it's correct so you don't have bigger sets of errors or unmeasurable error
and you know where things are going to be more ambiguous. That in itself can be very
valuable to because you can, you can basically say, here's this big set of data or this big
set of questions, let's say, how would you answer these questions and give it to multiple
people and you'll find out where people disagree and that tells you more about the ambiguity
of the problem space and where you're going to have to make stronger decisions about what
you think is right. So that's been a really helpful thing for clients to understand in
the past and I think they have a pretty good understanding of how that works and we'll
see if they build more products around that. Do you have a, you know, top three takeaways
that, you know, you found that clients, you know, as you look across a set of clients,
you know, these are the top three things that, you know, they all, you know, either learned
or need to learn in order to be successful at this stuff? I can tell you the first one
is always know what your question is. Be very precise and know exactly what the answer
would look like if you saw it. So if you see the answer, you'll know that the right thing
is happening. I certainly worked with companies that say, hey, we have all this data, we want
to learn from it and I say, great, what do you want to learn? And they say anything. And
so that's, that's a perfectly healthy and normal place to start, but at that point, you don't
have a question where you can build anything. So you have to formulate questions and decide
what's actually valuable for your business, which is more of a business and product space
question formulation task. So that strategic involvement has necessarily become part of
the business. Coming from a product background, I can appreciate that. I think there are
a lot of other independent consultants and people I know who work solely on questions after
they've been fully formed. And they say, you know, once you have the specs ready, happy
to work on it. But otherwise, it's, it's not, it's not what we do. And that initial step
of defining your question and knowing that it's an appropriate question for the data, it
really is the space where we, we thrive and help our clients succeed. So if they can come
in with a strong understanding of what they have and what they want, that's always better.
I think that's true broadly in business. But we think what else? So I was just having
a really good conversation over lunch with a couple people about how one of the things
that we don't see as often in data science, machine learning, land is a strong leadership
that knows how to market really well. So a lot of what I've seen data science team struggle
with is marketing themselves internally or marketing themselves up and managing up
to see sweet or the VP of engineering, whoever it is. And it's, it's really important
to develop those soft skills and understand what your value is relative to the company.
Sure. And I can say that. But at the end of the day, it's actually extremely difficult
to define that value because your systems may be giving some feedback to a business team
that allows them to make better decisions, but really they're making their own decisions
and they're supporting them with data in some cases. But you don't know what the investments
would have looked like otherwise. And so comparing the alternate universe that you might have
been in had you not had the technology that your team is building can be extremely difficult
to quantify. But that is part of the work of leadership. Right.
Clearly. So I look forward to seeing more breakout leaders that are really good at that.
And I think it'll necessarily be something that we see in the next few years. I wouldn't
call myself a pessimist, but I would say we're kind of high on the hype cycle right now.
And I'm not optimistic that we're going, the market will continue going up. So I'm
speaking. It goes in a cycle of company saying, hey, we're going to make this big investment
data science. We think that data science is a very valuable investment for us for these
reasons. And then a couple of years later, they come back to the team and they say, so
how have we done? And at that point, the team really needs to sell what they've done.
Ideally, they'd be selling that along the way as well. And I think we're coming to the
end of one of those periods where companies expect to see those big wins and teams really
need to justify their existence and be able to move the needle and describe how they're
moving the needle. Yeah. Yeah. Soft skills. Yes. Soft skills. Soft skills. Yeah. Take that
vent diagram of all the things you were supposed to be as an Amazon. I just add like four more
things to it. No problem. Yep. Nice. Nice. So that's two, but they're big. So. Yeah.
Well, on the third, the third is probably just manage expectations, right?
Oh, it's always. And any other, you know, any other of a number of sets of things in terms
of the expectations always. You said you said it exactly right. Yeah. Managing the expectations
is probably the biggest thing I do with clients. The first thing I say is I can't do anything
to pull a big win out of hat. I won't be pulling a big win out of hat for you. If you still
want to talk about this and you want to find out what this technology can do for you and
how it can incrementally improve your business and create new opportunities for products. Let's
talk about that. But it's not going to surface anything that you don't know about your own
business because frankly, you know about your business. Your business is an existence. So you've
must have some deeper understanding of what you're doing. And when I look at your deal flow,
your best customers are your best customers. I'm not going to tell you that there's,
there's a, there's a sleeper whale somewhere deep inside Salesforce. And that's okay.
I think give you better confidence to make decisions and understand the differential value
between things. But no promises. You really can't make promises. Yeah, absolutely.
So, you know, going back to this conversation around hybrid AI, we started to talk about, you know,
the role that humans in a loop play relative to, you know, their biases and, and, you know,
quote unquote algorithmic bias and things like that, which actually that was the kickoff panel
here at the Rangle conference. Is that that's something that you're spending some time looking
at now as well, right? Yes. So these things are all interwoven in some way. The active learning
and human and the loop patterns of hybrid are certainly ways to combat actively reinforcing
pre-existing bias. If you construct a system to do expensive or amplify or both, you can do either
end both. It depends on what you know about what you're doing, right? So if the example I give is
a model that was built at one of my previous employers where we wanted to predict who would start
a startup and leave their job and start a startup within the next six months. And the company had
an intent to build this model, create a list of people that were going to start companies soon,
and sell that list to investors as a type of pre-crime, basically algorithmic pre-crime for
seed stage funds. And they could get in early, before people even knew that they were going to start
companies, which is a like fascinating concept. So they used a number of factors to make this
prediction like where you had gone to college, what kind of degree you had, what your job title was,
what your previous employers were, there was a bucketing for the prestige of your college.
So you know the IVs were at the top and kind of cascaded down through bigger institutions,
and that included age. So what we ultimately saw when we predicted who
would be a founder in the next six months was pretty interesting because all of those factors
seemed to be directly relevant to how a person's career would develop them to be a founder in the
future. And interestingly, a lot of the people in the list were 30-year-olds management, ex-management
consulting, or ex-I bankers who were white males. And it didn't deviate too far from that.
And at the time, I thought, you know, this is this is pretty uncomfortable, but I don't really
know why. And it took me, it took me about a year to examine that emotional little more deeply.
And underneath is actually a very good reason to be concerned because though you are making
a prediction on characteristics that you think are fundamentally predictive of an outcome,
they have bias from the world rolled up into those factors. So all of the decisions that were made
to allow people to get to where they were and become founders in the previous
state of the world and the training data is your prior for your prediction of who will be
a future founder. And if you don't explicitly observe that, you know, I think I clicked through
on LinkedIn to maybe that top 120 people on this list. And that's the only way that I knew
that there was a certain split of like where people came from in terms of
home country or home state, where people came from in terms of in in terms of
age, all of these other characteristics, but even name can be ambiguous for what gender you are.
So I didn't get a sense of who was actually in this list until I went and looked at it and we
didn't have columns that said male female. We didn't test against that. We didn't predict on that,
but there were only 13 women at the top of the list or near the top of the list. And I thought,
well, that's that's somehow unfair, right? And I think looking back on that at the time,
it was we were not talking about that specific type of diversity in the market for founders.
Now that that conversation is happening more, it's become more unambiguous case where you can say
all of the priors, all of the pattern matching, so to speak, literally coming from VCs is being
encoded into the algorithm that's making this ultimate prediction. And that's not okay. So
the question then becomes what do we do? And there are a couple of people doing really great work
on this. I think there's one department at Carnegie Mellon where someone's coming up with validation
metrics that will help you test against the the characteristics you know you might have
bias outcomes on. So in this case, you would say how many men and women are there in our outcome?
Does it fit our expectation for what we would want to happen? And I think the the real key insight
here is that we want to build algorithms that will construct a world that we want to live in rather
than a world that existed in the past. We know the flaws of our current society to a large extent
and some people more than others, but as long as we can be vulnerable to one another
and try and validate that we are not reinforcing unjust actions from the past and just perpetuating
them with algorithms in the future, that is actually key to our work. And that's really important for
us to carry as a torch going forward. So I'm very excited that we had actually one talk so far
and we will have another talk today about algorithmic bias and harm and how these systems affect
users and I think it's a conversation that needs to gain more traction in the practitioner space
and we need to examine our own practices which more closely and know what we're doing.
Perhaps the most egregious example of this in the press lately was a
algorithm that police stations were using across the country to predict recidivism which
is the one that was exposed in the pro-publica article. Yes, exactly. And they did a bunch of work
that they put up on GitHub along with the data set to explain what was happening
and how they had analyzed the outcomes and as far as they could see what was happening in that
technology and I believe the company is still holding it as private IP so even the police
departments don't understand how this model works. The journalist did a really good job of saying,
this is a big problem, here are the metrics and here is the full explanation with the data of
what's so wrong with this beyond the anecdotal evidence of this is predicting that one person who
has committed one petty crime is more dangerous than someone who's a repeat criminal and has been
violent. I think it's a really egregious case but I don't want to say that it's good these
things happen but I think a few high profile cases will push the regulatory system to become
more serious about this so insofar as like it has to get worse if it gets better. I'm hoping that
that we can get out ahead of that as practitioners but regulation will certainly be getting there as
more and more of these cases are uncovered. Do you have a vision for how regulation can play here
without overly suppressing innovation which is a big concern that you hear on the other side?
It is and a good example of where you see that is in loan assessment and the finance space where
there are very strong regulations about how how you make decisions about what credit lines people
will get so given again like things that got worse before they got better redlining in the past
and other actions that have been taken that were deemed not legal after that. That environment has
responded extremely strongly to that. There's a pretty good understanding of what is important
in making that work transparent so that you can actually give someone feedback on why they were
rejected for a loan or why they were given a certain loan amount or a certain credit line
and while that's important I think there are improvements further to be made so I would
expect that if the regulation comes down really hard in the way that it has on that industry
if it comes down similarly on others as I think we're seeing that you just becoming interested in
then it can stifle innovation and probably grind it to a very slow pace but we're resilient,
we'll figure out ways to justify our existence and how we do our work and I think that's very healthy
in the ecosystem and the the oven flow of these factors and regulation and innovation are always
battling it out and the faster we can get out ahead of it and say no no no we actually know
what we're doing and we actually know how this works and we are justifying these things and we are
taking the appropriate precautions and trying to be as self-critical as possible and doing so
honestly if we can do that then the regulation will be the regulatory environment will be very
different when it finally comes to bear in these other areas that aren't just credit-worthyness.
Yeah great great well we've got additional talks here to go check out anything you want to
leave folks with point folks to I would say keep watching the algorithmic harm and ethics
arena there's a lot of work being done there and there are people that are finding great solutions
and people that are also of course always coming out with more critique and
interesting philosophical perspectives to consider so stay involved in that conversation
because it's an active one and everyone can be part of it. Yeah that's that's great and I think
the you know your comment about you know AI encoding the future that we want as opposed to the
past that we you know that we know I think it's a great one very very optimistic. Yes yes if you
care about that stay involved in the conversation and and yeah be a part of it. Great all right thanks
so much Claire. Thanks Sam. All right everyone that's our show for today I really hope you enjoyed
the interview and thanks so much for listening. Of course you can find the notes for this and every
show at the Twimalei.com website twimlai.com the notes for this particular show can be found at
twimalei.com slash 11 the number 11 as always I really appreciate getting your tweets and emails
and newsletter subscriptions and iTunes reviews so by all means keep them coming. Of course we'd
love to have you join the conversation you can tweet me at Sam Charrington. Claire is Claire
Corfell and I'm also increasingly using the Twimalei Twitter handle. T-W-I-M-L-A-I. Looking forward
to hearing from you and catch you next time.
