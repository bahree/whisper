Welcome to the Twimal AI Podcast.
I'm your host, Sam Charrington.
Hey, what's up everyone?
This is Sam.
A quick reminder that we've got a bunch of newly formed or forming study groups, including
groups focused on Kaggle competitions and the fast.aiNLP and deep learning for coders
part one courses.
It's not too late to join us, which you can do by visiting twimalai.com slash community.
Also, this week I'm at ReInvent and next week I'll be at NURRIPS.
If you're at either event, please reach out.
I'd love to connect.
Alright, this week on the podcast, I'm excited to share a series of shows recorded in
Orlando during the Microsoft Ignite conference.
Before we jump in, I'd like to thank Microsoft for their support of the show and their sponsorship
of this series.
Thanks to decades of breakthrough research and technology, Microsoft is making AI real
for businesses with Azure AI, a set of services that span vision, speech, language processing,
custom machine learning, and more.
Millions of developers and data scientists around the world are using Azure AI to build
innovative applications and machine learning models for their organizations, including
85% of the Fortune 100.
Microsoft customers like Spotify, Lexmark, and Airbus choose Azure AI because of its proven
enterprise grade capabilities and innovations, wide range of developer tools and services
and trusted approach.
Stay tuned to learn how Microsoft is enabling developers, data scientists, and MLOPS
and DevOps professionals across all skill levels to increase productivity, operationalize
models at scale, and innovate faster and more responsibly with Azure machine learning.
Learn more at aka.ms slash Azure ML.
Alright, onto the show.
Alright, everyone, I am here in Orlando at Microsoft Ignite and I've got the pleasure of
sitting with Eris Barak.
Eris is group manager for Azure AI.
And it is welcome to the Twoma AI podcast.
Thank you.
Thank you.
Great to be here.
Great to be here with you, Sam.
I'm super excited about this conversation.
We will be diving into a topic that is generating a lot of excitement in the industry and that
is AutoML and the automation of the data science process.
But before we dig into that, I'd love to hear how you got started working in ML and AI.
It's a great question because I've been working with data for quite a while.
And I think roughly about five to ten years ago, it became apparent that the next chapter
for anyone working with data has to weave itself through the AI world.
The world of opportunity with AI is really, really only limited by the amount of data you
have, the uniqueness of the data you have and the access you have to data.
And once you're able to connect those two worlds, a lot of things like predictions, new insights,
new directions, sort of come out of the woodwork.
So seeing that opportunity, imagining that potential has naturally led me to work with AI,
I was lucky enough to join the Azure AI group.
And there's really three focal areas within that group.
One of them is machine learning.
How do we enable data scientists of all skills to operate through the machine learning
lifecycle, starting from the data, to the training, to registering the models, to put
in them in productions and managing them, a process we call ML ops.
So just looking at that end-to-end, then I'm just understanding how we enable others to
really go through that process in a responsible, trusted, and a known way, it's been a super
exciting journey so far.
And so did you come at this primarily from a data science perspective, a research perspective,
an engineering perspective?
Well, I think none of the above or all of the above, I'm actually going to go with all
of that above.
I think it'd be remiss to think that, well, if you're going to hit it from a data science
perspective and you're trying to build a product, really looking to build the right set of products
for people to use as they go through their AI journey, you'd probably miss out on an aspect
of it.
If you're just thinking about the engineering perspective, you'll probably end up with
great info that doesn't align with any of the data science.
So you really got to think between the two worlds and how one empowers the other.
You really got to figure out where most data scientists of all skills need the help.
Want the help?
Are looking for tools and products and services on Azure to help them out?
And I think that's the part I find most compelling, sort of figuring that out and then really
going deep where you landed, right? Because if we end up building a new SDK, we're going
to spend a whole lot of time with our data science customers, our data science internal teams
and figure out, well, how should that SDK look like?
But if you're building something like AutoML that's targeted not only at the deeper data
scientists, but also the deeper rooted data professionals, you're going to spend some
time with them and understand not only what they need, but also how that applies to the
world of data science.
And what were you working on before Azure AI?
So before Azure AI in Microsoft, I worked for a team called ShareData, which really created
a set of data platforms for our internal teams.
And prior to joining Microsoft, I worked in the marketing automation space, completely
called the Optify.
And again, the unique assets we were able to bring to the table as part of Optify and
the world of marketing automations were always database.
We're always sort of looking at the data assets the marketers had and said, what else can
we get out of it?
Machine learning wasn't as prevalent at the time, but you could track back to a lot of
what we did at that time and how machine learning would have helped if it was used on such
a general basis.
Yeah, one of the first machine learning use cases that I worked with were with folks
that were doing trying to do like lead scoring and likelihood to buy propensity to buy types
of use cases.
I mean, that's been going on for a really long time.
So we're on a podcast.
So you can see me smiling, but we did a lot of work around building load, lead scoring.
Okay.
And you're a sticks and manual, you're a sticks and sort of general, you're a sticks and
you're a sticks that the customer could customize.
And today you've seen that to really evolve to a place where there's a lot of machine learning
behind it.
I mean, it's perfect for machine learning, right?
You've got all this data, it's fresh.
It's coming, you know, it does insights that are really hard to find out.
Once you start slicing and dicing it by regions or by size of customers, it gets even more
interesting.
So they're all the makings for having machine learning really make it shine.
Yeah, you are getting pretty excited.
Oh, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no,
no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no,
no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no,nice,
nice, nice.
Who want to dive into talking about auto ML.
I mean, this is.
Yeah, probably for the level of excitement that and demand for auto ML and enthusiasm
that folks have for the topic, not to mention the amount of confusion that there is for
the topic.
covered it nearly enough on the podcast, you know, certainly, you know, when I think of when I
think of AutoML, if there's, you know, a long kind of academic history behind the the technical
approaches that that drive it, but it was really popularized for many, you know, with Google's cloud
AutoML in 2018 and like before that, they had this New York Times, you know, they had this PR win,
it was like a New York Times article talking about how, you know, AI was going to create itself.
And I think that contributed a lot to, for lack of a better term in this space. But then we see it,
you know, all over the place, there are other approaches more focused on kind of citizen data science.
You know, I'd love to, you know, just start with how you define AutoML and, you know, is, is,
you know, what you're taking on it as a space and, you know, it's role and importance, that kind of thing.
You know, I think I can, I really relate to many of the things you touched on. So maybe I'll start,
and this is true for many things you do in Australia, but definitely for AutoML, on your point around
academic roots. Microsoft has this division called MSR, Microsoft Research, and it's really a set
of researchers who look into bleeding edge topics and drive the world of research in different areas.
And that is when we first got, in our team, introduced to AutoML. So they've been doing research,
a subset of that team has been doing research around the AutoML area for quite a few years.
They've been looking at it. They've been thinking, well, you know, it started. Yes, I've heard the
sentence AI making AI. But, you know, when you start reading into it, like, what does it mean? And,
it means, to be honest, it means a lot of things to many people. It's quite overused. I'll be
quite frank. There's no one standard industry standard definition that says, here's what AutoML is.
I can tell you what it is for us. I can tell you what it is for our customers. I can tell you
where we're seeing it make a ton of impact. And it comes to using machine learning capabilities
in order to help you, being the data scientist, create machine capabilities in a more efficient,
in a more accurate, in a more structured fashion. My reaction to that is that it's super high level.
And it, it kind of leaves the door open for all of, you know, this broad spectrum of definitions
that you just talked about. Like, for example, not to kind of over index on what Google's been doing,
but like Cloud AutoML Vision when it first came out was, you know, a way for folks to do a
vision cognitive services, but use some of their own data to tune it, right? Which is a lot different.
In fact, they, they caught a lot of flack from the, the academic AutoML community because
they totally redefined what that community had been working for, you know, for many years. And,
you know, started kind of creating the, the confusion. How do we, maybe the first question is,
do you see it as kind of, you know, being the kind of a broad spectrum of things? Or is it,
you know, how do we even get to a definition that kind of separates the personalized, you know,
cognitive services trained with my own data versus, you know, this other set of things?
I don't know if that's a question.
No, no, no, I think as you see it as more of that general sense. So, yeah, I would say,
probably not. I see it as a much more concrete set of capabilities that adhere to a well-known
process. Okay. That actually is agreed upon across the industry. When you build a model, what do you
do? You get data, you featureize that data. Once the features are in place, you choose a learner,
you choose an algorithm. You train that algorithm with the data, creating a model. At that point,
you want to assess the evaluate the model, make sure it's accurate. And you want to get some
understanding towards what are the underlining features that have most affected the model.
And you want to make sure, in addition to that, that you can explain that model is not biased.
You can explain that model is really fair towards all aspects of what it's looking at. That's a
well-known process. I think there's no argument around that in the sort of the machine learning field,
that's sort of the end to end. AutoML allows automating that process. So, at its purest, you feed
AutoML the data and you get the rest for free, if you may. Okay, that would be sort of where we're
heading, where we want to be. And I think that's at the heart of AutoML. So, where does the confusion
start? I could claim that what we or others do for custom vision follows that path. And it does.
I can also claim that some of what we do for custom vision is automated. And, you know,
then there's a short sort of hop to say, well, therefore it is AutoML. But I think that misses the
general point of what we're trying to do with AutoML. Custom vision is a great example where
AutoML can be leveraged. But AutoML can be leveraged wherever that end-to-end process happens in
machine learning. Nice. I like it. I like it. So, maybe we can kind of walk through that end-to-end
process and talk about some of the key areas where automation is applied to contribute to AutoML.
So, I'd like to start with Featureization. And, you know, at the end of the day, we want an
accurate model. A lot of that accuracy, a lot of the insights we can get, the predictions we can
get, and the output we can get from any model is really hinged on how effective your Featureization
is. So, you know, many times you hear that, well, 80% of the time on data sciences spend on data.
A lot of the time is, can I put it up? Do you know where that number comes from?
Oh, of course. Everyone says it. Everyone repeats it. It's a self-fulfilling prophecy.
You know, I'm going to say 79% of this, just to be sure. But I think it's more of an urban legend
at that point. I don't think there's a, you know, I am seeing customers who do spend that kind of
percentage of time. I am seeing experiments we run that take that amount of time.
Generalizing that number is just, it's too fun not to do.
So, you know, I was wondering if there's, I was thinking about this recently and I'm wondering
if there's some, you know, institute for data science that's been tracking this number over time.
It'd be interesting to see how it changes over time, I think, is the broader curiosity.
If you would, I should go figure that out. I think that's an interesting one.
I've got to be interested in the outside.
So, sorry.
So, but, you know, you can easily and anyone who's built a model can quickly see the effect of
featureization on the output. Now, a lot of what's done when building features can be automated.
I would even venture say that a part of it can be easily automated. But then, naturally,
what are some examples? Some examples are like, I want to take two columns and bring them together
into one. I want to change a date format to better align with the rest of my columns.
You know, not even an easy one. I'd like to enhance my data with some public holiday data
when I do my sales forecasting because that's really going to make it more accurate.
It's more a data enhancement, but you definitely want to build features into your data to do that.
So, getting that right is key. Now, start thinking of data sets that have many rows,
but more importantly, have many columns. Okay, and then the problem gets harder and harder.
You want to try a lot more options. There's a lot more ways of featureizing the data.
Some are more effective than others. Like, you know, we recently in AutoML have incorporated
the birth model into our auto-futurization capability. Now, that allows us to take text data
used for classification and quickly featureize it. It helps us featureize it in a way that
requires less input data to come in for the model to be accurate. I think that's a great example
of how deep and how far that can go. You mentioned that getting that featureization right is key.
To what extent is it an algorithmic methodological challenge versus a computational challenge?
If you can even separate these two, meaning, you know, there's this trade-off between like,
you know, we've got kind of this catalog of, you know, recipes, you know, like combining columns
and, you know, binning things and whatever that we can just throw at a data set that looks like it
might fit, you know, versus more intelligent or selective application of techniques based on,
you know, nuances, you know, whether predefined or learned about the data.
Yeah. So it extends on a few dimensions. Okay. I would say there are techniques. Some require more
compute than others. Some are easier to get done. Some require sort of a deeper integration with
existing miles. Like I mentioned, the bird before to be effective. But that's only one dimension.
The other dimension is the fit of the data into a specific learner. So, you know, we don't call it
experiments and machine learning for nothing. You experiment. Right. You try. Okay. Nobody really
knows exactly which features would affect the model in a proper way would drive accuracy.
So there's a lot of iteration and experimentation being done. Now think of this place where you have
a lot of data, creating a lot of features and you want to try multiple learners, multiple algorithms
if you may. And that becomes quickly quite a mundane process that automating can really,
really help with. And then at the top of that, we're seeing more and more models creating created with
just more and more features. The more features you have, the more nuance you can get about describing
your data, the more nuance the model can get about predicting what's going to happen next. So we're
not seeing models with millions and billions of features coming out. Now, auto mail is not yet
to prepare the prepared to deal with the billion feature model, but we see that dimension extent.
So extend compute one, extend the number of iterations you would have, extend to the number of features
you have. Now you got a problem that's quickly going to be referred to as mundane, hard to do.
Repetitive doesn't really require a lot of imagination. Automation just sounds perfect for that.
So that's why sort of one of the things we went after in the past, I'd say six to twelve
months is how we get featureization to a place where you do a lot of auto featureization.
I'm trying to parse whether the extent to which you got to, you know, whether you agree with this
kind of dichotomy that I presented. Like there's, you've got this, you know, this mundane problem that
if, you know, a human data scientist was doing it would be, you know, just, you know, extremely iterative.
There's certainly one way of automating is just, you know, do that iteration a lot quicker
because a machine can do that. Another way of automating is apply, you know, let's call it, you know,
more intelligent approaches to navigating that feature space or that, you know, iteration space.
And identifying, you know, through algorithmic techniques, you know, what are likely to be the,
the right combinations of features as opposed to just kind of throwing the kitchen sink at it and,
you know, putting that in a bunch of loops. And certainly that's, you know, that's not a dichotomy,
right? You do a bit of both. Can you elaborate on that kind of trade off or the relationship
between those two approaches? Is that even a right way to think about it? Or is that the wrong
way to think about it? I think it's a definitely a way to think about it. Like if you think about it,
it's a, no, I'm just thinking through that lens for a second. So I think you describe sort of the
brute force approach. Yeah. Yeah. On one side. Right. The other side is how nuanced can you get
about it? Yeah. So what we know is you can get quite nuanced. Those things that are known to work,
things that are not known to work, things that work with a certain type of data set that don't work
with another. Right. Things that work with a certain type of data set combined with the learner
that don't work with others. So as we build AutoML, I talked about machine learning used to
help with machine learning. We train a model to say, okay, in this kind of event, you might want to
try this kind of combination first, because if you're, I talked about the number of features,
brute force is not an option. So we got to get a lot more nuanced about it. So what AutoML does
is given those conditions, if you may, or those features for that model, it helps sort of the,
it helps shape the right set of experiments before others. That's allowing you to get a more
accurate model faster. So I think that's one aspect of it. I think another aspect which you may
have touched on. And I think it's really important throughout AutoML. But definitely in featureization is
while people are excited about that, the next thing you're going to hear is, but I want to see what
you did. And you got to show what kind of features you used. Yeah. And quickly follows is, I want to
change feature 950 out of the thousand features you gave me. And I want to add two more features at
the end, because I think they're important. That's where my innovation as a data scientist comes
into play. So you got to, and AutoML allows you to do that, be able to open up that aspect and say,
here's what I've come up with. Would you like to customize? Would you like to add? Would you like
to remove? Because that's where you as a data scientist shine and are able to innovate.
So we started with featureization. Next step is learner slash model selection. I think it's
probably the best next step to talk about. Okay. I think there's a lot of configuration that goes
into this. Like how many iterations do I want to do? Yeah. For instance, how accurate do I want to
get? What defines accuracy? But those are more sort of manual parameters. We ask the user to add
to it. But when automation again comes into play is learner selection. So what happens? You know,
putting AutoML aside, what's going to happen? Build a set of feature, choose a learner. One that I
happen to know is really good for this kind of a problem and try it out. See how accurate I get.
If it doesn't work, but even if it works, you're going to try another. Try another few, try a few
options. AutoML at the heart of it is what it does. Now going back to what we talked about in
featureization, we don't take a brute force approach. We have a model that's been trained over
millions of experiments, sort of knows what would be a good first choice. Given the data,
given the type of features, given the type of outcome you want, what do we try first? Because
people can't just run an endless number of iterations. It takes time, takes cost, and sort of takes
the, frankly, takes a lot of the ROI out of some statistics from AutoML. So you want to get there
as fast as possible based on learnings from the past. So what we've automated is that selection.
Put in the data, set the number of iterations or not set them. We have a default number that
goes in and then start using the learners based on the environment you're seeing out there
and choosing them out of that from that other model we've trained over time. By the way,
that's a place where we really leaned on the outputs and the outputs we got from MSR.
That's a place where they, as they were defining AutoML, as they were researching it,
really went deep into and really sort of created assets were then able to leverage a product
sort of evolves over time and technology evolves over time. But if I have to pick the most
or the deepest rooted area we've looked at from MSR, it's definitely the ability to choose
the right learner for the right job with a minimal amount of compute associated with it if you
may. And what are some of the core contributions of that research if you kind of go to the layer
deeper than that? Are you asking the context of choosing a model or in general?
Yeah, in the context of choosing a model. Like, for example, as you described this,
what is essentially a learner that's learning which model to use that created a bunch of
questions from me around like, okay, how do you represent this whole? What are the features of
that model and what is the structure of that model? I'm curious if that's something that came out
of MSR or that was more the, you know, came from the productization and then, you know, if there are
specific things that came out of that MSR research that come to mind as being, you know, pivotal to the
way you think about that process. So the first version coming out of MSR wasn't really of the
end to end product, but at the heart of it was the model that helps you pick learners as it relates
to the type size of data you have and the type of target you have. This is where a lot of the research
went into. This is where we publish papers around where which features matter when you choose that.
This is where MSR went and collected a lot of historical data around people running experiments
and trained that model. So the basis at the heart of our earliest versions, we really leaned on MSR
to get that model in place. You know, within the workflow to where the auto-futurization I talked
about some other aspects we'll talk about in a minute, but at the heart of it, they did all that
research to understand, well, first trained that model, just, you know, grabbing the data,
getting it right out of experiments. Is that model? Is it, you know, a single model? Is it relatively simple?
Is it fairly complex? Is it some kind of ensemble? Like, oversimplify a little bit, but remember,
it profiles your data. So it takes a profile of your data. It profiles your features. It takes a
profile of your features. It looks at the kind of outcome you want to achieve. Am I doing time
series forecasting here? I'm doing classification. I'm doing regression. That really matters.
And then based on those features, picks the first learner to go after. Then what it does is
uses the result of that first iteration, which included all the features I'm talking about,
but also now includes, hey, I also tried learner X. And I got this result. And that helps it
choose the next one. So what happens is you look at the base data you have, but you constantly
have additional features that show you, well, what have I tried and what were the results? And then
the next learner gets picked based on that. And that gets you in a place where the more you iterate,
the closer you get to that learner that gives you a more accurate, accurate result.
So then is it kind of at its core? Is it a, you know, it's, I'm hearing elements of both,
you know, supervised learning. You have a bunch of experiments and, you know, the models that
were chosen, ultimately, you know, but also elements of something, you know, more like, you know,
simple reinforcement learning, contextual bandics, explore, exploit kind of things as well.
It definitely does both. I would, if I could just sort of touch on one point, reinforcement
learning as it's defined, I wouldn't say we're doing reinforcement learning there. Saying that,
we're definitely sort of every time we have an iteration going or, you know, every X times we have
that, we do fine tune the training of the model to learn as it runs more and more. So I think
reinforcement learning is a lot more sort of a mem reactive. But taking that as an analogy,
we do sort of continuously collect more training data and then retrain the model that helps us choose
better and better over time. Interesting, interesting. So we've talked about a couple of these aspects of
the process, feature engineering model selection. You know, one of the next is once you've identified
the model like tuning hyper parameters and optimization, do you consider that its own step or is that
kind of, you know, a thing that you're doing all along or both? I consider it part of that Uber
process I talked about earlier. You know, we're just delving into starting to use deep learning
learners within AutoML. So that's where we're also going to automate the parameter selection,
hyper parameter selection. A lot of the learners we have today are classic machine learning if you
may. So that's where hyper parameter tuning is not as applicable. But seeing that every time
we see an opportunity like that, I think I mentioned earlier in our forecasting capabilities,
we're now adding them deep learning models. In order to make the forecasting more accurate,
that's where that tuning will also be automated. Then if you think about actually a library,
I think we chatted about that pre-interview. But you mentioned that you're doing some stuff with
TCN and Arima around time series forecasting. Can you elaborate on that? Yeah, so I talked about this
process of choosing a learner. Now, you also have to consider what is your possible set of learners
you can choose from. And what we've added recently are sort of deep learning models or networks that
actually are used within that process. So TCN and Arima are quite useful when doing time series
forecasting really drive the accuracy based on the data you have. So we've now embedded those
capabilities within our forecasting capability. So then when you say within forecasting meaning
a forecasting service that you're offering as opposed to within. Let me clarify. Yeah, there's
three core use cases. We support us part of AutoML. One for classification. The other for regression.
And the third for time series forecasting. So when I refer to that, I was referring more to that
use case within AutoML. Got it. Got it. So in other words, in the context of that forecasting
use case as opposed to building a system that is general and applying it to time series and using
you know more generalized models you're using now TCN and Arima as kind of quarter that which are
you know long proven models for time series forecast. Yeah, I would argue there also be generalized
but in the context of forecast. So but but let me tell you how we're thinking about it. There's
generally applicable models. Yeah. Now we'll see in different use cases like in forecasting,
there are generally applicable models for that area that are really useful in that area. That's
sort of the current state we're in right now. And we want to add a lot more sort of known
and generally applicable models to each area. In addition to that sort of where we're heading and
as I see this moving forward, more and more customers will want to add their own custom model.
Right. We've done forecasting for our manufacturing. We've tuned it to a place where it's just
amazing for what we do because we know a lot more about our business than anyone else.
We'd like to put that in the mix every time your AutoML considers the best option.
So I think we're going to see I'm already seeing a lot of that sort of they bring your own model
which makes sense. Yeah, it's an interesting extension to kind of bring your own data which was
the you know one of the first frontiers here. No, I think it's very like I mean you're coming in
to a world now it's not the hey there's no data science here. There's a lot of data science going
on. So I'm the data scientist. I've worked on this model for the past you name it weeks, months,
years and now this AutoML thing is really going to help me be better. I don't think that's a claim
we even want to make. I don't think that's a claim that fair to make. The whole idea is find
the user where they are. You have a custom model. Sure, let's plug that in. It's going to be
considered with the rest in a fair and visible way. Maybe with the auto-futurization it even
goes and becomes more accurate. Maybe you'll find out something else you want to do in your model.
Maybe you have five of those models and you're not sure which one is best. You plug in all five.
I think that's very much sort of where we're heading. Plugging into an existing process that's
already deep and rich wherever we land. The three areas that we've talked about again,
featureization model selection and parameter or optimization are I think kind of what we
tend to think of as the core of AutoML. Do you also see it playing in the tail end of that
process like the deployment after the models deployed? There are certainly opportunities to
automate there. A lot of that is very much related to DevOps and that kind of thing. Are there
elements of that that are more like what we're talking about here? I think there's two steps before
that. I think there's the evaluation of the model. How accurate is it? But again, you get into
this world of iterations. That's where automation is really helpful. That's one. The other
is the interpretation of the model. That's where automation really helps as well. Now especially when
I did a bunch of automation, I now want to make sure what which features really did affect this
thing. Explain them to me and work that into your automated processes. Did your process provide a
fair set of data for my model to learn from? Does it represent all, all genders probably?
Does all races probably? Does it represent all aspects of my problem? You choose them in a fair
way. Where do you see imbalance? I think automating those pieces are right before we jump into
deployment. I think it's really mandatory when you do AutoML to give that full picture. Otherwise,
you're creating the right set of tools, but I don't feel or I feel without doing that, you're
falling a bit short of providing everyone the right checks and balances to look at the work they're
doing. When I generalize the AutoML process, I definitely include that. Back to your question,
does deployment... Do I see deployment playing there? You know what? To be honest, I'm not sure.
So I think we're definitely the way we evaluate success is we look at the models deployed
with AutoML or via AutoML or that were created via AutoML or not deployed. We looked at their
inferences. We look at their scoring and we provide that view to the customer to assess
the real value of their model. Automation there, I think. You know, if I have to guess,
yes. Automation will stretch there. Do I see it today? Can I call it out today? Not just yet.
Well, I mean, a lot of conversation around this idea of deploying a model out into production
and we've thankfully, I think, convinced people that it's not just like deploy once and you're
not thinking about it anymore. You have to monitor the performance of that model and there's
a limited lifespan for most of the models that we're putting in production. And then kind of the
next thing that folks get excited about as well, I can just see when my model falls out of tolerance
and then like auto retrain. It's one of these. Everyone's talking about it, you know,
few are actually doing it. It sounds like you're kind of in agreement with that. Like we're kind
of not there yet at scale or no. So I think, you know, we often refer to that world as the world
of ML ops, machine learning operations in a more snappy way. I think there is a lot of automation
there. If you look at automation, you do it DevOps for just code. I mean, forget machine learning
code, but code let alone models is very much automation we need. Right. I do think there are like
two separate loops that have clear interface points, like deployed models, like maybe
data about data drift, but they sort of move in different cycles at different speeds. So I'm
maybe, you know, we're learning more about this, but I suspect that iteration of training,
improving accuracy, getting to a model where the data science team says, oh, this one's great,
let's use that. I suspect that's one cycle. And frankly, that's where we've been
hyper focused on automating with auto ML. There's naturally another cycle of that operations that
we're sort of looking at the automation opportunities with ML ops. Do they combine into one automation
cycle? I'm not sure. It does strike me that like when, you know, for example, the decision,
you know, do I kind of retrain from scratch? Do I incrementally retrain? Do I start all the way
over? Maybe that decision could be driven by, you know, some patterns or characteristics in the
nature of the drift and the performance shift that, you know, a model could be applied to. And then
we start to, you know, there are aspects of, you know, what we're thinking about and talking about
as auto ML that are applied to that like DevOps-y part. Who knows? I'd say who knows? And I think, you know,
listening to you, you know, I'm taking note to myself that while we sort of have a bit of a fixed
mindset on the definition, we definitely need to break through some of that and open up and see
well, what is it that we're hearing from the real world that should shape what we automate, how
we automate under which umbrella we put it? I think, and you will notice this moving so fast
and evolving so fast. I think we're just at the, you know, first step of it. Yeah. Yeah. Yeah.
A couple of quick points that I wanted to ask about. Another couple of areas that are generating
excitement under this umbrella are neural architecture search and neural evolution and kind of
techniques like that. Are you doing anything in those domains? Again, we're incorporating some of
those neural architectures into auto ML today. I talked about our deeper roots with MSR and sort of
how they got us that first model. Our MSR team is very much looking deeper into those areas. They're
not things that have formulated just yet, but the feeling is that sort of the same concepts we put
into auto ML or automated machine learning can be used there, can be automated there. I'm being a
little vague because it is a little vague for us, but the feeling is that there is something there.
And, you know, we're lucky enough to have the MSR arm that when there's a feeling there's something
there. Some research starts to pan out and the thinking of different ideas there. But
to be frank, I don't have much to share at that point in terms of more specifics. Yeah. Yeah.
And I guess as, you know, we've been focused on this auto ML is a kind of set of platform
capabilities that helps data scientists be more productive. There's a whole other aspect of,
you know, Microsoft delivering cognitive services for vision and, you know, other things
where they're using auto ML internally and where kind of, you know, it's primarily deep learning
based and I can only imagine that they're throwing things like architecture search and things
like that at the problem. Yeah. Yeah. So they do happen in many cases. I think custom vision is
a good example. We don't see the general patterns just yet. And for the ones we do see sort of the
means of automation haven't been put out that yet. So I think it's like if I look at where we were
with the auto ML thinking probably a few years back is where that is right now, meaning oh,
it's interesting. We know there's something there. The question is how we sort of further evolve
into something more specific. Well, Ed is thanks so much for taking the time to chat with us about
what you're up to. Great conversation and learned a ton. Thank you. Same here. Thanks for your time
and the questions were great. That a great time. All right, everyone. That's our show for today
to learn more about this episode. Visit Twomla.com. As always, thanks so much for listening and catch you
next time.
