Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Today we are joined by Andrew Trask, PhD student at the University of Oxford and leader of
the Open Mind project.
Open Mind is an open source community focused on researching, developing and promoting tools
for secure, privacy preserving, value aligned, artificial intelligence.
Andrew and I caught up back at Noreps to dig into why Open Mind is important and to explore
some of the basic research and technologies supporting private decentralized data science.
We touch on ideas such as differential privacy, secure, multi-party computation, and how
these come into play and for example federated learning.
Before we jump in, I'd like to join Pegasystems this episode's sponsor and inviting you to
meet me at the MGM Grand in Las Vegas from June 2nd to 5th at Pegaworld, the company's
annual digital transformation conference.
Pegasystems puts AI at the center of its customer engagement software so that every customer
touchpoint on every channel is optimized in real time.
So the customers find each interaction relevant and timely, whether a sales call, a marketing
campaign, or a customer service chat.
At Pegaworld, you'll hear great stories of AI applied to the customer experience at
real Pegacustomers.
The event is a great way to learn from a who's who of the Fortune 500, and of course, I'll
be there speaking as well.
To register, visit Pegaworld.com and use the promo code TWIMIL-19 when you sign up for
$200 off.
Again, that's TWIMIL-19, it's as easy as that.
Hope to see you there, and now on to the show.
All right, everyone.
I am here in Montreal at the NURPS conference, and I am with Andrew Trask.
Andrew is a PhD student at Oxford, as well as leader of the Open Source Open Mind project.
Andrew, welcome to this week in Machine Learning and AI.
Awesome.
Thank you.
Great to be here.
Awesome.
Why don't we get started by having you tell us a little bit about your background?
How did you get involved in machine learning and privacy in the intersection of those two?
Yeah, so I went to undergrad at a liberal arts college in Tennessee called Belmont University.
And while I was there, I was originally a music student, and I stumbled into a CS course
dabbled around, and there was an AI course, and that got me really into machine learning.
So as soon as I, there's just really great professor named Dr. Hooper, as many great
stories start.
I really compelling professor, teaching really exciting concepts, and I trained my first
neural nets, and got them to converge, and got a paper into undergraduate conference.
And at that point, I was pretty committed on the machine learning front.
After that, I joined a local company called Digital Reasoning that does deep learning and
AI at scale on private data sets.
Very natural, actually, right?
Yeah, they're natural.
Yeah, so Belmont is also a natural.
Okay.
Got it.
Yeah.
And that's when I really got exposed to the power of doing machine learning on data that
you don't have access to.
So I mean, they work with a lot of government agencies, like the intelligence coming across
them.
Oh, that's awesome.
I'll throw out that story.
I got the dabble in research as well as product management there, and really just came to
appreciate how challenging it is to do machine learning and deep learning on unseen data.
So when I left to do a PhD, I had already a very good appreciation for that, and also
some sort of high-level understanding of the challenges of that environment.
And then, during my first year at Oxford, the first year of PhD is often very exploratory.
And one of the first things that came across was homeomorphic encryption, which just to
me was absolutely magical.
I couldn't believe that you could do computation on encrypted data.
And I wrote a blog post, sort of just trying to jerry rig some homeomorphic encryption
code that I found with training and neural net.
And I got a tremendous amount of positive feedback from it, ended up chatting with some
folks at the Future Humanity Institute in Oxford.
So I don't know if you know Nick Bostrom and kind of the safety work that they do.
And it just became really clear that there was a really exciting research opportunity here
that one was underserved, so there weren't many people working on it, and secondarily,
and perhaps more importantly, that there was good reason to believe we can make a lot
of progress in a relatively short period of time on a topic with pretty significant
social gains.
And so that at the beginning of PhD is a no-brainer.
Like you actually want to jump on those kind of opportunities.
And since then, what I've discovered is that tons of great workers going on in sort
of the field of cryptography, tons of great workers going on in the field of statistics,
and of course in machine learning and deep learning.
But these three communities, they don't necessarily talk as much as they should.
And so by simply trying to become as familiar as fast as possible with sort of the core
primitives of these three different fields, and by just trying to combine them in ways
that solve important use cases, I think we've already been able to make quite a bit of progress
and expect to be so in the future.
So that's kind of where I'm at.
Nice, nice.
Here, some kind of core concepts, maybe from the cryptography side, that it makes sense
to kind of talk about, like mention homomorphic encryption that are fundamental to all the
rest of the stuff you're doing, or it's better to go top down.
Yeah, so I mean, the three core concepts that we deal with, two of which are from cryptography,
are differential privacy and secure multi-party computation.
So and then there's federated learning from the machine learning side.
So those three things kind of make up what I think is the future of privacy-reserving
machine learning.
So differential privacy is concerned with, I suppose your viewers already know this,
in the context of machine learning, differential privacy is focused on allowing models to sort
of learn from a training data set without them accidentally learning information we don't
want them to learn.
Like, you know, private training data itself, for example.
Yeah, yeah, exactly.
Or it's any specific features about a specific person, right?
So it might, a good example might be, you want a model to learn to identify cancer in
radiology scans without it memorizing which individual person has cancer, right?
And so yeah, differential privacy is super important.
That one is already, there's already good community form between that and the machine
learning community.
It's really exciting to see that develop.
Homomorphic encryption and secure multi-party computation are a little less known, in particular
secure multi-party, I'll just say secure NPC from now on, for short, because it's such
a wordy thing.
Homomorphic encryption, I think, is more intuitive for people because it's sort of a, it's
typically laid out in the form of a standard encryption.
So you have a public key and a private key, and you'll use the private key to encrypt
something, right?
So I can encrypt the number five, and then I can give it to you.
And while it's in its encrypted state, you know, while you have it, and I can't see it,
you can do arithmetic with that number, right?
And you could do computation with that number.
And then you can give the result back to me, still encrypted, and I can decrypt the result.
And so that's interesting from machine learning standpoint, because I can take, say, all
the parameters of my model, so a model, you know, a neural net, for example, is just a big
collection of numbers.
I can encrypt all of them.
I can give them to you, and then you can do prediction or training, or anything else
that you'd like to do with a machine learning model.
And then, so if you do training, for example, you can give me back the resulting model.
And you were able to, it's sort of a one-way information flow, where you're able to make
the model smarter, but you're not actually able to use it.
So there's lots of interesting commercial use cases that protect privacy in that context.
The one, and some people have heard about this.
So homework encryption is sort of mildly well-known.
But the problem is, is that it's very, very slow.
So I mean, it's several, meaning more than five orders of magnitude slower than typical,
what we call plain text computation, what we do normally.
And this is where NPC really starts to become interesting.
So there's, security NPC is a slightly different setup.
Instead of trying to encrypt a number, we actually split it into what's called shares.
So let's say we want to encrypt the number five.
I can take the number five, I can split it into two shares, we'll say a two and a three,
right?
And I can give you the three, and I can hang on to the two.
So the three is your share, the two is my share.
Now we could restore this number five by just adding these two shares together, if that's
an instance.
But the interesting thing about security NPC is that while it's in this state, neither of
us know what the number is, it's hidden between us, right?
And you may have no idea that this was a five.
But we can compute over it.
So let's say we want it to multiply at times two, like if you multiply your share at times
two, I multiply my share at times two, then this hidden value that's between us was
also multiplied by two.
And this is better than homeomorphic encryption for two reasons.
One, it's much faster, typically.
So there's a recent paper show and you could train a deep cognet that's only 20 times slower
than the normal, which I know that sounds mildly annoying to anyone who's training deep
known that's, but it's totally different than like 10,000 or 100,000 times slower, right?
And is it faster only in aggregate or for each individual participant?
So it is, in this specific setup, it's two or three party computations.
So we've split this deep neural net, all the parameters in it into shares that three
parties own.
And in that case, it's faster in aggregate.
So we typically try to measure it in the context of a use case because different functions.
So you imagine this is a protocol, right?
And I described how to multiply this hidden value five by two, but addition is different
from multiplication, which is different from comparison operators, which is different
than how you bootstrap sine and ray-lew and like, sigmoid and all these kinds of things.
So we typically try to look at the end use case and just sort of give a high level empirical
results for what the slowdown is.
But the interesting thing is, one more on that, if it's based on homeomorphic encryption,
how is it end up being faster than homeomorphic encryption?
Oh, so it's not always based on homeomorphic encryption.
Sometimes there's a homeomorphic encryption component.
They kind of steal ideas from each other.
But the cool thing is that this is not a form of encryption.
It actually, it's a protocol for sharing.
So encryption is not necessarily involved.
It's more...
Yeah.
So it's funny because it has an API that's very similar to encryption to the extent that
I can take a value and I can make it so that a group of people can't see it, right?
But we can still compute over it.
So to that extent, it can do everything that homeomorphic encryption can do.
But it has two specific properties that are different.
Well, so three.
One, it's a lot faster because you basically are exchanging heavy compute, typically,
which is homeomorphic encryption for a more network overhead.
So whereas with homeomorphic encryption, I could just send you an encrypted asset and
you can do things with it.
With NPC, we have to be online the whole time.
It's a protocol for how we can exchange messages in a way to compute functions on hidden
data.
And there's two properties that we really like.
One, it gets this privacy property.
But secondarily, and this is different from homeomorphic encryption, everyone has a private
key.
And this creates...
It's a lot more like shared ownership of a data structure instead of just encryption.
So in this particular case, if I take a neural net, I take all of its parameters and I
split it, split each parameter into four shares, then I have four governors.
I have four owners for this model.
And any one of those four people could say, oh, I'm not willing to perform computation
with you guys on this data point.
Because I haven't been paid or because I don't believe in it or for whatever reason, right?
And that's fundamentally different from homeomorphic encryption, which kind of defaults to a single
owner who has a single private key.
Okay.
So yeah, we really like that sort of distributed governance apparatus.
Okay.
And so what does OpenMind come in?
Yeah.
So OpenMind is basically about lowering the barrier to entry to being able to use these
tools.
As I described a minute ago, these communities have kind of lived in separate worlds.
So the crypto community has their own toolkits, typically in C++, the machine learning community
has their own toolkits that's like, you know, PyTorch and TensorFlow.
And statistics has their own, it's like R and all these kinds of things.
So the problem is that if I want to do deep learning with security and PC, I either
have to take an NPC framework and write all my autograd and my layer types and all
stuff from scratch, or I have to go to a deep learning framework and know all the cryptography
to be able to add.
And as it turns out, this is prohibitively difficult and nearly most papers that I come across,
people write some big amorphous C++ blog from scratch.
And then it becomes difficult to repurpose, difficult for industry to pick up.
So what OpenMind is all about is about installing these cryptography primitives inside of the
major deep learning toolkits.
So PyTorch and TensorFlow being the first two.
So that people in the machine learning community don't have to really know anything about cryptography
to be able to use them.
Or if you're able to do research on them to be able to study these different protocols.
So in particular, in one example, if that is in PyTorch, we have this new tensor type
that's an NPC shared tensor.
So it feels like a normal tensor has the same API, but every time you add two tensors
together, under the hood it actually sends messages to multiple different machines and
does the protocol correctly.
This is the multi-party aspect of it.
Is it like widely distributed?
Like you might think of the parties representing actual people that are in different corners
of the earth, or is it are the parties typically like computational processes that might be local
to an environment and under the same control?
Like how does it typically, what's the typical topology?
Great question.
So for the purpose of development, we've got nice sort of what we call virtual workers
that will simulate it on one machine so that you can build protocols and that kind of thing.
But in the context of a more real world use case, there's different philosophies on how
it will actually roll out.
So the thing to know is that the more people you have governing a data point, the more
people have to touch it for it to be decrypted or for it to be used in computation.
But meaning the more owners it has, the slower the computation is going to go.
So our vision is that most of privacy-preserving machine learning will operate with two owners
at a time because that's sort of the fastest version of it.
And this is where, as I mentioned, I may have mentioned this before, we really see a
trifecta of like three core technologies coming together, secure multi-party computation
being one of them, and then federated learning and differential privacy being the other.
So the nice thing about federated learning, as it's a protocol that allows me as a model
owner to train a model on a highly distributed data set by performing training with one
person at a time.
So I'll send a model to someone and it will train there for a while.
I think I could do this with 10 people in parallel.
But the nice thing is that I should probably come back to that.
So the short answer is I see it happening two people at a time.
And when you combine it with these other technologies, that actually works out to be quite
performed.
When do you want to go back to the federated?
Oh, yeah.
It's like an interesting topic.
How well developed is that in the literature, the models for federated machine learning.
I've seen a lot of work on distributed training.
And then more recently in the context of like IoT and like edge devices, federated training
in that sense, where you are trying to do some amount of learning kind of at the edge,
but also share the model back to some centralized thing so that it could incorporate learning
from other edge devices, but also so that it becomes like the master model.
And then you ship out updates back out to the other edge devices.
I don't know if the same kinds of techniques apply or what the overlap might be there.
They absolutely do.
If federated learning is out of these three things, like federated learning, multi-party
computation and differential privacy, federated learning is the most mature.
I mean, to that extent, so if you have a smartphone and you open it up and you go to
text someone, you know, try to recommend the next word for your text, that model for
both, to my knowledge, to both Apple and Google is trained using federated learning.
So to that extent, federated learning is already deployed on, you know, billions of devices
around the planet.
So to that extent, it's quite robust.
And the nice thing about federated learning, so I guess for anyone listening that doesn't
know what federated learning is, typically when you're training a machine learning model,
you would aggregate all the data to sort of one central server, then you'd initialize
a random model and then you'd train it on that data, right?
So federated learning kind of flips the script and says instead of bringing all the data
to the model in one location, you're going to bring the model out to the data wherever
it already lives.
And the general goal is that whoever owns the data doesn't want to have to send it to
someone.
They want to own and maintain the only copy.
So in this particular case, this means that, you know, if I've got a billion phones,
I as a central server, we'll then send the model with some instructions on how to train
it down to individual data owners.
And this is where the nice thing about that is it does two things.
One, it protects the privacy of the data.
And two, it typically actually results in less communication than sending the data to
the cloud.
Look at the original paper on federated learning from Brendan McMahon.
They, I think they decided 10 to 100x less network overhead and sending the data sets to
the cloud.
I mean, it depends on how big the data set is, but often it can actually be less network
intensive.
And so as you can imagine, this is where MPC comes in.
So the big, the nice thing here is you protect the privacy of the data, but the problem is
is that you send a plain text copy of the model to potentially thousands or millions of
people, right?
And if this is your $10 million healthcare model, well, then you'll be, you're sort of at
risk of any one of these millions of people stealing it and trying to monetize it for themselves.
This is where secure MPC becomes interesting.
So instead of sending the model, you would MPC share the model, and they would MPC share
their data set, and I would perform sort of encrypted training with each individual person
in parallel, and then only aggregate their gradients sort of at the end.
This is why federated learning allows you to do MPC sort of two people at a time, which
is quite a more optimal.
What's the kind of user experience of OpenMind is that like system level packets that they're
using, or is it more framework level?
Is the typical user a developer or a data scientist?
How do you kind of look at the world from that perspective?
Yeah.
So speaking from today, it's mostly the framework level.
So our target audience today is primarily sort of machine running researchers.
Basically a scientist, perhaps working at enterprise wanting to do a pilot in privacy
preserving machine learning.
So the vast majority of the code we've written is extending PyTorch Intensive Flow.
So if you're not a PyTorch Intensive Flow user, you don't have any, then we're not quite
ready for you yet.
I think in the future, this will likely get packaged more into sort of, here's your federated
learning system.
Here's how you spin up the server, and this is where all your data adapters will attach
to that kind of thing, but we still have quite a bit of code to write until we get there.
So is it extending your PyTorch Intensive Flow in an analogous way to the way different
distributed training methods are working?
Yeah.
So for PyTorch, for example, the first thing we built was a remote execution protocol.
So for anyone who knows PyTorch that's listening, it allows you to take a tensor, which is
a tensor as a simple list of numbers or nested list of numbers.
So if we have a texture called like x equals, you know, some tensor, maybe 5, 0 or something
like that, I can go x.send and send it to some machine and put an address for that machine.
And then what gets returned to me is a pointer to that tensor.
And then whatever that pointer has the exact same API as a tensor normally would.
So it feels like a normal tensor.
But the nice thing is that then we can use this underlying primitive to coordinate remote
executions and coordinate higher level protocols like security PC.
So most of it's built on top of that, kind of a remote execution paradigm.
But then the API.
And that was standard already existing in both PyTorch Intensive Flow, something analogous
to it.
So PyTorch Intensive Flow have not had pointers in the past, so that's a new thing for
us.
Okay.
But the thing that we try to keep the same is we want that pointer to have basically have
the exact same API as if the data was on your local machine, right?
Okay.
So we want to make it so that you don't have to relearn a new framework or really any
other paradigms to be able to use things like federated learning or NPC.
And that's why our NPC tensor feels like it's a normal tensor.
It feels like it's on your device, has the same API and protocol and you can do the same
things.
You can call back propagate on a lot of remote tensors.
But it's under the hood, it's actually doing all the protocol for you, such that you
don't have to learn that protocol yourself.
Yeah.
And you're using this API or to get the pointers, you're specifying the number of parties
and IP addresses or something like that, or?
Yeah.
So we do have the ability to spin up servers, which we call workers, that are connected via
you know, sockets or web sockets.
And what you'll get is actually a pointer to that worker on the client side.
So we typically name them Bob and Alice canonically, just because that's sort of the crypto literature.
Yeah, exactly.
So we have a big set of tutorials, actually, we just listed this shows how the whole protocol
goes.
So if you go right and get hub slash examples slash, so the PICIF library is the library
I'm referring to.
PICIF slash examples slash tutorials will have that.
But yeah, so if I'm going to, if I'm going to send it, I'll go X, my variable, dot send.
And then in parentheses, I can put in one or more workers that I want to send it to and
return to that.
Okay.
And then I pass in a list of people, a list of pointers to their workers that I want
to NPC share it.
Yeah.
And that's the sort of the encrypted version.
Do you also have kind of flexibility over whether you want to use encryption or just NPC
or whether you want to layer in differential privacy or is the framework making those kinds
of decisions for me?
No.
So the framework is totally agnostic to those decisions.
So obviously we have higher level libraries that try to give you nice defaults.
But the bulk of the framework is this low level protocol for allowing you to either design
your own sort of remote execution strategies or to pull one of the off the shelf ones that
you provide, like federated averaging or different kinds of NPC protocols.
I think we have two different NPC protocols.
Okay.
And what are the distinctions between the different protocols?
So they're APIs primarily, so and so the first one we implemented was called speeds.
So speeds is a protocol that allows you to do, in our case, addition and multiplication.
And then we extended it with this new protocol called Secure and In, which allows you to
do comparison operators.
So to compare two encrypted numbers, as it turns out, we can bootstrap sort of the rest
of the deep learning API just from those three core primitives.
How mature would you say is the, you know, the particular, the project and kind of the
ecosystem around it, like are, are you finding folks extra, is it like, you know, your personal
effort mostly that's kind of pushing along, or are there other folks that are kind of
jumping in and contributing?
How's that been going?
So that part's actually been going really well.
So I think open-mind Slack team at this point has around 3,400 people in it.
Wow.
Yeah.
Oh, wow.
And I think we're up to 170 or 180 people who have contributed code.
Oh.
So to that extent, like, I think a lot of people get that privacy is important at these
tools are a really compelling answer, or they could be in a long run and that by building
better research tools and bringing these two kind of crypto and machine learning community
together, that we can make a lot of progress, and then it's worth their nights and weekends
to work on.
That being said, I would describe PICIF today as kind of an alpha, alpha level project.
So PICIF, again, is the extension deep to PyTorch Intensive Flow.
Open Mind is just kind of the general community name.
So yeah, PICIF is kind of an alpha phase where it's got all the features that we would
like for it to have, but they're not necessarily like a product ready state, and they're also
using an older version.
It was in the particular case of PyTorch, they're using 0.3.1.
So the main push at the moment is to upgrade to PyTorch 1.0, which was recently released,
as well as to really make them robust enough that the institutions can do kind of pilots
and practice use cases and these kinds of things, and we're hoping to release that around
February and March.
Are there any examples of use cases that you can talk about?
So we are in the midst of crafting several early pilots, but I don't think they're quite
public yet, but I'm very excited about them.
We also do have several, I think about a half dozen firms that are in the process of
putting together research grants for people to work on PICIF in Open Mind full-time, which
will greatly help both their internal adoption as well as the health of the library, which
I'm excited about.
And so where do you see this going?
What are some of the next steps for the community and your research in general?
For my personal research, I think I want to sort of stay focused on whatever is needed
for the steps to become mainstream and why they adopted.
At the moment, that's raising awareness and lowering the barrier to entry for tools
that already exist and conducting research to solve the holes that still miss in the
technologies.
There are some sub-problems, especially in differential privacy that have not yet been
solved to the extent that the widest use cases could be implemented today.
So in the next three to six months, I think that it's still mostly a research and open-source
tool push.
In about a six-month to 18-month timeline, I think that we're going to start seeing
considerable enterprise adoption.
So there's already some interesting movement happening in Europe as kind of an echo to GDPR.
There's a handful of startups in the US that I've seen growing up in this space.
And so I think probably the six to 18-month timeline is when enterprises which already
have the data will be interested in moving into this space.
You mentioned some kind of point challenges on the differential privacy side that need
to be solved to better facilitate all of this.
Are there some examples of those that you can share?
So from what I've observed, there's two different kinds of differential privacy right?
There's local and there's global.
Local differential privacy focuses on actually adding noise to each individual row of
data to make sure that each individual row of data is protected before you even do anything
to it.
But as you can imagine, if you have a million data points, that means you're adding a ton
of noise to the data set as a whole.
And you need to have like a lot of people in order to be able to get interesting insights
about it because you've had to add so much noise to each individual data point.
Global differential privacy allows you to compute some function on the data and then only
add noise to the output, which can actually be relatively small amounts depending on what
that function is.
So one thing I have yet to see in deep learning literature is a global differential privacy
technique that really fits the business use case that people want to use it for.
The best one that I know of that comes to mind is the Pate algorithm, which I think if
people know different differential privacy and deep learning is probably the one that
they'd be most familiar with.
This is a really innovative approach wherein you split a data set into 10 different buckets
or in different buckets.
I'll just use 10 as an example and you train a model on each different one that's a private
model.
And then there's this clever trick where you can use these models to annotate a second
data set that is a public data set in a way that all of the labels, you can sort of enforce
differential privacy by the way that you label this second data set using these private
models such that you then train another model on this sort of synthetic, synthetically labeled
data set.
I think that one is Nicholas Paprono.
Yeah, that's right.
We did a podcast interview with him as well.
Oh, excellent.
Yeah.
That's probably the most famous and most successful general purpose deep learning for differential
privacy algorithm that we have.
But the problem is that it requires this, the second data set that you already have.
And in practice in industry, this is not usually the case, right?
So if I'm a hospital network and I've got say 50,000 examples that are unlabeled, the
friction for me to go to 10 other hospitals and train models with all of them and synthesize
this new model just to like annotate my data set and train my secondary model is it's
pretty rare use case.
Typically, it's cheaper, easier for them to just pay some of the annotated, right?
It's a little more straightforward and it's not as experimental, not kind of thing.
So if we can figure out a way where you can have a similar level of global differential
privacy like this, actually even it's debatable whether this is global or local, it's actually
it's probably closer to local, which doesn't have this core assumption that would that would
be a pretty big breakthrough and it would be the kind of thing where that's the form
of a DP technique that would be general purpose, I think.
And I would very much like to see sort of someone push through on that front, but obviously
lots of people are trying and we'll hopefully we'll have something in the next little while.
Any pointers or thoughts or folks that want to dig more into this area, where should they
start looking for ways to get up to speed on these three pillars or your project?
Yeah, so I mean the three activities that we do and open mind hopefully are designed
to make that as easy as possible.
So we build open source tools, we create learning resources and we build community through
like hackathons and through the Slack channel.
So the first thing that I'd recommend is join the Slack team and kind of scroll through
the articles people are posting.
So you can go to slack.openmind.org, openmind is spelled OPE in I in ED.
Yeah, I happen to general discussion, you know, say hello and then I just finished writing
a nine length notebook tutorial walking from the very basics of kind of remote execution
through federated learning and secure NPC, it'll be extended with differential privacy
in a bit.
And the real goal of that tutorial series is just to walk someone who you know just knows
in this case PyTorch through in a very, you know, hands-on kind of way how these techniques
work.
Awesome.
Yeah.
Awesome.
Well, Andrew, thanks so much for taking the time to chat with me.
It was great to meet you.
Great to meet you too.
All right, everyone, that's our show for today.
For more information on Andrew or any of the topics covered in this episode, visit twimmelai.com
slash talk slash 241.
As always, thanks so much for listening and catch you next time.
