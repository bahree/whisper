Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Contest alert.
This week we have a jam-packed intro, including a new contest we're launching.
So please bear with me, you don't want to miss this one.
First, a bit about this week's shows.
As you may know, I spent a few days at CES earlier this month.
While there, I spoke with a bunch of folks applying AI in the consumer electronics industry,
and I'm including you in those conversations via this series of shows.
Stay tuned as we explore some of the very cool ways that machine learning and AI are being
used to enhance our everyday lives.
This includes work being done at Anki, who built Cosmo, the cutest little computer vision-powered
robot.
Nighthouse, whose smart home security camera combines 3D sensing with deep learning and NLP.
Intel, who's using the single-shot multi-box image detection algorithm to personalize video
fees for the Ferrari Challenge North America.
First beat, a company whose machine learning algorithms analyzed your heartbeat data to
provide personalized insights into stress, exercise, and sleep patterns.
3AI and Koito, who have partnered to bring machine learning-based adaptive driving beams
or automatically adjusting high beams to the U.S.
And last but not least, aerial.ai, who applies sophisticated analytics to Wi-Fi signals to
enable some really interesting home automation and healthcare applications.
Now as if six amazing interviews wasn't enough, a few of these companies have been so
kind as to provide us with products for you, the Twimmel community.
And keeping with the theme of this series, our contest will be a little different this
time.
To enter, we want to hear from you about the role AI is playing in your home and personal
life, and where you see it going.
Just head on over to twimmelai.com slash myaicontest, fire up your webcam or smartphone camera,
and tell us your story in two minutes or less.
Go post the videos to YouTube, and the video with the most likes wins their choice of
great prizes, including an Anki Cosmo, a lighthouse smart home camera, and more.
Submissions will be taken until February 11th, and voting will remain open until February
18th.
Good luck.
Before we dive into today's show, I'd like to thank our friends at Intel AI for their
continued support of this podcast.
Intel was extremely active at this year's CES, with a bunch of AI autonomous driving
and VR related announcements.
One of the more interesting partnerships they announced was a collaboration with the Ferrari
Challenge North America race series.
Along with the folks at Ferrari Challenge, Intel AI aspires to make the race viewing experience
more personalized by using deep computer vision to detect and monitor individual race
cars via camera feeds and allow viewers to choose the specific cars feeds that they'd
like to watch.
Look for my conversation with Intel's Andy Keller and Emil Chindicki earlier in this series
for an in-depth discussion about this project, and be sure to visit ai.intel.com where you'll
find Andy's technical blog post on the topic.
Now about today's show.
In this episode, I'm joined by Stuart Feffer, co-founder and CEO of Reality AI, which provides
tools and services for engineers working with sensors and signals, and Brady Tsai, business
development manager at Kui Tu, which develops automotive lighting solutions for car manufacturers.
Stuart and Brady joined me at CES a few weeks ago after they announced a partnership to bring
the adaptive driving beam or ADB headlights to North America.
Brady explains what exactly ADB technology is and how it works, while Stuart walks me
through the technical aspects not only of this partnership, but of the reality AI platform
as a whole.
And now on to the show.
Hey everyone, we are here at CES and I am with Stuart Feffer of Reality AI and Brady
Tsai of Kui Tu.
Stuart and Brady, welcome to this week in machine learning and AI.
Hi.
Hi.
It's great to have you guys here, and thanks for braving the driving rain and horrendous
traffic at CES to make it here.
It is a phenomenon, that's for sure.
It is definitely a spectacle.
Why don't we get started by having the two of you guys introduce yourselves and tell
us a little bit about your background and kind of how you got to what you're up to nowadays.
Yeah, sure.
So yeah, I'm Stuart Feffer and I'm a co-founder and CEO of Reality AI.
And we're an AI startup, I'm sure you have a lot of them on these past podcasts.
And our focus is a little different than most I think.
We are very much focused on problems related to sensors and signals.
And we're not deep learning, we are, we use a different set of approaches that are very
much grounded in signal processing math, and I'm sure we'll talk about that.
We're here at CES with one of our customers, Coeto, also known in the United States as
North American lighting, and they're making a product announcement that features our
technology.
Nice.
Brady?
Hi, I'm Brady.
I'm business development manager with NL and Coeto.
I work in a Silicon Valley lab, which is based in San Jose, just to give you a brief introduction
of Coeto.
Coeto is a tier one supplier for automotive lighting.
And we are the supplier for major OEMs, such as Honda, Toyota, and Ford.
There is a product called ADP.
That's where we work with Stuart Reality AI, try to bring the AI into automotive lightings.
It's called ADP?
ADP.
Okay.
Adaptive driving beam.
Adaptive driving beam.
And tell me a little bit about what's the main idea there.
Yeah, go ahead, Brady.
Yeah, so adaptive driving beam, it's a vehicle lighting mechanism where it allows user to
have high beam always on.
Okay.
But in order to do that, we don't want to blind the income in traffic or the traffic
in front of you.
So now that all the headlights and tail lights are based on LEDs, that allows us to turn
on and off a section of our headlights.
And in order to know which section to shut off, we have to first be able to detect the
vehicle in front of us.
Oh, interesting.
And that's where AI technology comes in.
What sensors do you assume or require on the vehicle in order to be able to do that?
I'm imagining if we're talking about technology that's going to be available in the near-term,
we're not expecting every car to have a light hour on it.
Right.
So for ADB purpose, we only need cameras.
Okay.
Yeah.
All right, cool.
And ADB, by the way, it's available today, right, Brady?
This is a live product.
It's on the road in Japan and...
Yes, yes.
It's been widely used in Japan, also in Europe.
And it's come into North America in a very short period of time.
Yeah.
Okay.
So what we've been doing, what we've been showing today, starting today at CES, is the
next generation of ADB.
Okay.
The existing iteration is based more on traditional machine vision techniques.
And which is great, it works pretty well.
But it is prone to some false positives.
And the idea here is to use AI to reduce the false positive rate so that you've got to
be able to tell the difference between a headlight and a stoplight or a bright vending machine
that's not a truck, that type of thing.
So the idea is to refine the prediction and deliver a more accurate prediction.
Okay.
I'm using a machine learning.
What is it about the traditional techniques that lends itself to the false positives?
Well, you know, traditional machine vision techniques without getting into the specifics
of what COETO's current product does because we can't really do that.
But you know, these traditional machine vision like template matching, you know, they're
very good in constrained environments where you don't have a great deal of variation in
target and background.
That's when they tend to perform best.
Okay.
Right.
So pattern matching machine vision techniques are great, say, on an assembly line where
you're doing quality control.
But out in that dynamic real world where you have a lot more variation, both in your target
and in the background against what you're trying to separate that target.
Well, you know, you start to come up against the limitations of the technique.
Now one of the main constraints here though is, you know, we're not talking about an autonomous
vehicle necessarily.
This is a product that's going to go on regular cars starting in, you know, they're available
as I said.
It was pretty, we're saying they're available in Japan and Europe today.
Right.
And in the United States in 2019 or 2020, something like that.
Okay.
So it's got to fit within a certain price point.
Right.
Right.
Right.
Right.
You can't have an expensive processing brick just turning the high beams on them off.
It doesn't work as a product.
Right.
So the challenging bit here is not just accomplishing the detection using machine learning and suppressing
false positives where you need false positive suppress.
The real challenge is doing that and then delivering that prediction in a form that can
run on cheap hardware that meets the price point requirements of the product.
And this is cheap hardware that is presumably mounted, you know, within the lens, within
the light headlight assembly, right?
Yeah.
Inside the headlight assembly.
That's exactly right.
So it's got to, you know, and it's got to, you can't make the car too much more expensive.
Right.
You can make it only as more expensive as people perceive value in being able to turn their
high beams on it off automatically.
Yeah.
Right.
Right.
He was saying we're actually tracking vehicles leaving the high beams always on and tracking
that vehicle that's in front of you or that's on coming as it moves across the field
division so that he's selectively blocked out.
But you can still see animals, pedestrians or other things that are peripheral to that
car.
Yes.
So this is resonating really strongly with me because I, you know, while I'm, you know,
normally live in a city environment over the holidays, I was in a more rural environment
and made frequent use of the high beams.
Yeah.
And when you depend on the high beams and then you turn them off because there's an
upcoming traffic light.
It's like, where am I?
It's like it's totally dark.
And so I can relate to, you know, wanting to just, you know, a, you know, as cars get more
complex, they're going to have more knobs and stuff and, you know, one less thing to worry
about would be great.
But, you know, there's, you know, if you can have, you can offer me, you know, visibility
into kind of my field of view, even while oncoming cars are approaching, that sounds like a great
proposition and one that will increase safety.
You know, tell me a little bit about some of the technology that goes into making this
happen.
Yeah.
Sure.
So the, you know, on the sensing side, which is our contribution, right?
Coeto, North American lighting, they're the headlight experts.
And in terms of controlling the beam and shaping the beam and figuring out exactly how to adapt
the beam to the driving patterns, that's their area of expertise for sure.
Our area of expertise is sensing the car in front, so that in delivering that location
of the car to their control mechanism so that it can appropriately adapt.
So it knows where the car is, it can do the calculations, it needs to do to figure out
what to do with those LEDs.
So you give them like a vector of angles and distances, for example?
Yeah, basically, you know, we're delivered, it's very similar to what you would see in
an ADAS system, the sort of collision avoidance system for autonomous vehicles.
Where, you know, you see bounding boxes on cars and pedestrians, that kind of thing.
It's a very similar sort of output.
Okay.
So that's what, that's our contribution to this is the sensing piece.
And I think, you know, I mentioned in the introduction, reality AI, our approach to machine
learning is a little different, right?
We're not using deep learning.
Deep learning, unfortunately, would probably require more compute power than we can afford
on that, you know, on that control unit.
Even on the inference side alone?
Yeah.
Yeah.
So our approach in general to machine learning is we spend a lot of energy on
the feature engineering.
And we take a signal processing based approach.
And we have a process that will, you know, looks through to 300 different feature types
and compute each of them, test them and predicts which one's going to be best for any given
situation.
And what are the sensors that you have available on assuming you don't have access to vehicle
sensors?
The sensor is in the headlight assembly and this is maybe a camera or there are other
sensors.
Well, in this case, we're working with a camera.
Okay.
I mean, it's certainly conceivable that in the future, other sensors might be in play.
Okay.
But at the moment, it's purely camera based.
Okay.
And our technology is not camera specific.
I would, you know, if I'm going to be candid, I would actually say image based things is
probably our weakest, our weakest area in image based things where we tend to be strongest
is in problems that could be reduced to a question of texture.
So if you're doing object recognition, right, you want to know, that thing over there
is that a pedestrian or a person on a bicycle, right, that's a good deep learning problem,
object recognition, deep learning is good at that.
Yeah.
And it requires compute, but it can do it.
Our stuff tends to work much better on texture based problems.
Okay.
In fact, that's the way in which we approach this with the headlight detection and the
false positive suppression, is looking at spatial relationships between pixels of different
colors inside of a decision window.
Okay.
Right.
It's just a different way of going about it.
Now the fact is our stuff is much more widely used, I mean, we do, we do some things with
images that are texture based, but sound, vibration, accelerometry, electrical signals.
Those are really a sweeter spot for us most of the time.
And we are getting ready to launch a couple of things with coeto that will involve other
types of sensors beyond cameras as well, different kind of product, different kind of use.
And when you say your stuff, what are we talking about here, is it, you know, some hardware
that goes in the headlight assembly, is it some algorithms, is it IP, is it services?
Sure.
But the headlight is coeto's product.
Yep.
What reality AI offers is a tool for the R&D engineer to create that product.
Okay.
So by our stuff, what I mean is the algorithms and the application that allows an engineer
to use those algorithms to expose the algorithms to data and generate detection code, which can
then be either hung in the cloud, if it's a cloud-based application or pulled out of that
cloud-based environment, pulled into the IDE for an embedded environment, and then run
in the embedded target, which is, oh, we have plans to use this.
Right.
Right.
Maybe tell me a little bit about the, what can you tell me about kind of the experience
of your engineers working with this technology, do you?
I'm imagining that your engineers don't typically have machine learning and AI expertise, or am
I wrong about that?
Right.
So as a lighting company, most of our effort is optics and mechanical and how to control
the heat in the headlight.
So we're not experienced in putting sensors or more computing embedded system into our
headlamps.
So we're quite excited to be able to work with reality AI and try to find possibility
to put sensors into headlamps and try to make it a smarter headlights and rear lights.
And so the platform, do you refer to it as a platform, or a toolkit, or?
Yeah, we call it a toolkit or an application, even.
This is kind of like an SDK that's got some built-in.
Like how is the, think of it as a code generation application, right?
So there's, this is a podcast, so I can't pull up a demonstration here, right?
But think of this as a tool set where you can provide examples of what you're looking
for in the case of what we're doing with Coeto, what those are, you know, here are images
where this is, this over here is a, the, the tail light of a car that we're following
and we want to block out, we don't want to, we don't want to blind over, right next to
it over there, that's a, that's a reflection of, of a stop sign.
So that's a counter example, right, don't count that as a headlight.
That red light off in the distance, that's a stop light, don't count that either, right?
So that, that's our input.
We have snippets of images taken by the camera and with some labels on them, the tell us, tell
us what they are.
In our application, we can load it with these examples and run first a process we call AI
Explorer, what AI, and AI Explorer does the feature engineering.
It's a machine learning driven process, sort of, you could almost think of it as an expert
system, but it isn't really, but it, what it will do is go through and try to identify
an optimized feature set, which can then be exposed to a machine learning algorithm,
which is, you know, could be an SVM, could be a neural network.
We pick, we can pick, we can pick that based on what's the most appropriate form of
output forward with the customer, what we will need for their technical requirements for
the product.
But you know, I mean, your audience is all about machine learning and AI, so I'm sure
you're, you know, you and they know that when you have the right feature set, your choice
of algorithm becomes much less important.
And if you have good, really solid features that separate, that give you a good separation
between classes, well, heck, almost any algorithm will find what you're looking for,
right?
So, you know, that's really what the point of our application is, is to do that feature
engineering and identify the most optimized features possible, such that we can then use
the latest touch machine learning possible, and therefore delivered prediction code that
is as compact and computationally efficient as possible.
So what extent do the features that, that this tool spits out, you know, that, do they
tend to be kind of intuitive features versus, you know, kind of artificial features, kind
of mathematical combinations of the inputs that don't really have any intuitive interpretation?
Most of the time it's the latter.
The latter?
Yeah.
Okay.
So, you know, look, by the time someone gets to us, if it's a sound problem, for example,
a vibration problem, by the time a customer gets to us, their engineers have already tried
an FFT, put that into a neural net to see what would happen, right?
So, you know, if it was a problem, was that easy to solve, they wouldn't be calling us
in the first place.
So, you know, look, our algorithm will check an FFT just to be, you know, a couple of
different flavors and a couple of different varieties to it, just to make, just for
completeness sake.
But generally speaking, we're going to need to carve, we're going to need to carve up that
feature space in a very different way.
And how do you, how do you do that?
Well, you know, we use, we use mathematics, you'd find in the literature under sparse
coding, compressive sensing, that type of thing.
And what, what is, what are those things?
What's sparse coding?
What's compressive sensing?
Well, you know, you, I guess what, what we're doing is you could think of us as carving
up time, frequency in a much more complex way than an FFT, which is just using bands.
And so it can be very responsive to things like transients and phase and, you know, those
kinds of phenomenon.
Now, in the image kinds of problems like, like we're dealing with here with ADB, that stuff
isn't relevant.
But it turns out when we use these same mathematics on images, what that basically translates to
is a texture kind of relationship.
And we, we tend to be good at finding textures and discontinuities and textures.
But that's sort of a side, it's, it's a side usage, which turns out to be very useful
in certain cases like with ADB.
But our primary focus is more in the vibration, electrical signal, sound, light are even.
And so in the past, when I've talked to folks who have, are taking similar approaches to
kind of automating feature engineering, there's, you know, there's often a lot of like Monte
Carlo type simulations and that kind of thing.
Do you do that kind of stuff as well?
Yeah, not so much.
I mean, we'll basically judge which one, which feature set is the best on the basis.
I will take, well, basically with, with want with feature sets that look promising according
to their to our algorithm.
We train a quick machine learning model on a subset of the, on a subset of the training
data, do a quick k-fold analysis and we rank them on the basis of their performance under
that k-fold.
So it's a pretty straightforward accuracy based ranking.
The other thing we do is we do generate a relative measure of the complexity of the
feature computation.
Because again, our customers are, by and large, coming to us because they intend to deploy
to an embedded target where compute is going to be a limited resource, either cycles or
memory.
And so we'll give them a relative ranking of, you know, if it's green and the bar is
hardly filled in, well, you can probably fit it on a cortex M3, M4, right?
But if it's, if it's almost, the bar is almost filled up and it's turned red, well, you're
probably going to need server grade hardware to execute that particular model.
And we basically make that an engineering choice.
Now the engineer who's using this stuff can trade off computational complexity for accuracy
in some cases.
I think something trying to wrap my head around like the, so we get the problem.
The problem is you've got limited computational capacity and a lot of these environments.
And as exciting as deep learning is, it requires a significant compute capability, even
for the inference.
But deep learning is exciting because you don't have to do feature engineering.
Right.
You want to go the other way.
You got to do some feature engineering, which is difficult manually.
You guys automate it.
I'm trying to wrap my head around kind of the next level of detail, which is like if I
wanted to, you know, if I wanted to build something like this, like, you know, what are the things
that I should be thinking of as a, you know, data scientist or engineer, you know, if
I wanted to, you know, if I needed to build my own kind of automated feature engineering
pipeline, like understanding that there's proprietary IP and, yeah, yeah, yeah, yeah, sure,
and all that.
Like, what are the, the things that I should be thinking about?
Okay.
Well, so the first case and the first thing I would say is that, you know, features are domain
specific isn't quite the right word.
That's not what I'm going for.
But the, the, the, the, the differences of them or something like that.
Yeah.
So, you know, our approach, the kinds of features where you, we're going to try from
suit to nuts are going to be the kinds of features that are relevant when you're talking
about an, an input you could think of as a wave form in some way.
Yep.
Right.
And those kinds of features are going to be completely different than the kinds of features
you would use if, you know, you're looking at business records of some sort, right?
Obviously.
But even with sound, the kinds of features we're looking at are not going to be the same
kinds of features you're going to want to use if you're building a competitor to Amazon
Echo or Siri or OK Google, right, where the problem is natural language recognition.
Our stuff isn't actually the kinds of features we employ aren't actually very good at language
recognition at all.
But it's really good at machine hums.
Ah, OK.
OK.
So, you're doing, you probably have like, you're kind of doing different types of FFTs,
different types of windows, different kind of release.
Yeah.
We don't, we, we check FFTs for completeness, right, right, we're more likely to be using
sparse coding, compressive sensing and other kinds of more complex features sets.
Got it.
OK.
I think that's good.
So, you've got, so there's some set of algorithms that are particularly good at identifying,
you know, either frequency components or something like that, you know, in this type of
signal.
Transient phase frequency.
Yeah.
OK.
OK.
And so you, you're just kind of sweeping across those with different parameters and, you
know, maybe there's some kind of grid searching or something like that that you're doing
or randomized searching or something like that or something like that in there.
Yeah.
Something that, you know, it's guided, but there's still a fair amount of, we're going to
try a spectrum of things.
Yeah.
And, you know, we try a spectrum of things and we find a family of features that's promising
the algorithm will dive in and do more exploration within that promising family, right?
But yeah, you kind of have the idea.
And what's the, by what's the origin of the kind of the company and the product?
Yeah.
Yeah.
Great question.
So, you know, where this stuff really came from is really the other co-founder, truthfully.
OK.
So I'm the, I'm the business guy.
My background's Wall Street.
I've spent just enough time in, you know, math and physics and that type of thing to be
able to follow along.
But the real genesis of this came from our other co-founder, Jeff Strackey, and Jeff's
our CTO.
OK.
Turns out he's been my best friend since we were 13 years old.
Oh, wow.
Yeah.
But for the last, I guess, 10, 12 years or so before we started Reality AI, Jeff was
doing contract R&D for US federal government customers in military and intelligence community.
So, you know, always in this area of applying this new field of machine learning to complex
signal processing, signal recognition problems, surveillance, target acquisition, that kind
of thing.
And you know, during that developed a fairly comprehensive body of IP, we have 10
patents awarded, six patents pending, most of which come from that period.
OK.
And but that's really where the expertise for this came from.
And a couple of years ago when we decided to create Reality AI, we took all of that
IP out of the contracting entity used for the, those federal government customers.
Everything that wasn't classified wasn't subject to export control because we don't want
that headache.
But anything that could be freely used commercially, we moved that intellectual property into a new
company.
We sunsetted the old thing that had been used for the defense contracting.
And we created Reality Analytics Inc reality AI to apply this technology commercially.
We turned that into an application usable by an R&D engineer version 1.0 of that came
out in June of 2016.
OK.
2.0 is coming out in just a couple of weeks.
OK.
Nice.
Yeah.
And we've been, you know, adding customers and automotive, probably our number one area
right now, followed very closely by industrial.
And we also have a couple of consumer product customers that are doing interesting things.
And so for industrial, this might be an industrial machinery supplier who wants to be able to
do predictive maintenance.
You just drop in the algorithms and at home is home in kind of those kind of, you know,
frequency based vibration vibration and occasionally sound.
Yeah.
And even some of the automotive customers, by the way, are doing that same kind of thing
but on the vehicle.
But the industrial customers are always, you know, one former and other of, you know,
figuring out when the wing of a jigger needs to think of a barber place.
Right.
Right.
I'm trying to make it, you know, it's come up called the machine whisperer.
There's always like the machine whisperer that knows when it sounds like this, you need
to whack it here with a hammer four times or whatever.
You got it.
You got it.
So, you know, our approach commercially there is that we are generally working with the
equipment makers.
OK.
So that, you know, much like Kuido is trying to, who's building the smarts into the headlight,
we're working with the, you know, industrial equipment makers, the pump makers would have
you to build the smarts into their equipment, as opposed to some kind of aftermarket
add-on.
It's interesting.
I've asked several of the folks that I've talked to today, you know, what kind of things
have they learned trying to introduce artificial intelligence to consumer products?
And, you know, universally, the answer has to do with the user experience and, you know,
from the perspective of the consumer, like, they don't really care about AI.
And like, this is like, at the far end of that spectrum, nobody, no one who's driving
a car is even going to, even if they know that, you know, hey, I don't have to turn
on my headlights, I think my high beams anymore, you know, this is something that you just
want to be invisible to them and just work.
That being said, have you, as a company kind of learned anything about applying AI in
these kind of situations?
Well, it's still early days.
Yeah.
Yeah.
So, we're actually looking into possibility of embedding more sensors into headlights from
those driving as well, like, such as light hour and radars.
And after we, they, right now, the full factor of light are just too big to put into the
lightings.
But we're waiting for that size to shrink into a reasonable size so that we can put
into the headlights.
And is the idea there that, you know, just that Quido would be, would become a sensor
provider to the OEM in addition to a lighting provider or is it somehow tied to lighting
in the delivery of lighting?
That, I can't disclose that part yet, but we are looking into that kind of a possibility
and also a possibility of doing some kind of an edge computing in headlights for autonomous
driving.
Interesting.
You know, the cool thing about what Quido is doing here, right, is that because they're
providing the headlights, the tail lights, the turn signals, right, that's their market,
they own strategic real estate on the car.
They have, they have the, the placement on all of the corners, right, and if you want
to place sensors to get them the best possible field of view around the vehicle, where you
got to put them, right, plus these guys have, I mean, I've had, since I've been standing
next to them all day at CES and the booth, I've had a chance to hear them, hear them
pitch and, you know, putting, being able to put these sensors in a form factor where it
can stand up to a car wash and weather, you know, these guys are expert in creating electronics
that are protected from the elements and still can be, you know, can still see through.
So you know, that's actually, it turns out that as you sensor up a car, the real estate
that their stuff owns and their ability to deliver it in a form factor that fits with
a car's design that is protected from the elements that could stand up to a power washing
or whatever Mother Nature is going to throw at it, that's actually very, very important.
And something that, you know, the automotive industry looks to be only just beginning to
grapple with as they start to think about the reality of making cars that are instrumented
in this way.
Right, right.
Yeah, I can imagine the modularity being, you know, a lighting assembly is pretty plug
and play relative to, you know, changing out something that's kind of built into the
frame of the car or the sheet metal or something like that.
It seems like a, I can see how it would be a strategic place to be.
Is this something that you envision becoming available like as an aftermarket type of thing
or is it, you know, primarily you're going to market through the OEMs, the manufacturers?
Oh, you mean ADV?
So I think right now in North America, we're just waiting for the regulation to go through.
I think sometime in 2018 or 2019, the regulation will go through and we can see vehicles on
the street with ADV within two years.
And what's the nature of the regulation that is over this?
Like the transportation from the FTC, they have to approve everything.
Say that the ID, right, for, yeah, okay.
And apparently they have this kind of regulation in Japan already.
So that's why they, if you see vehicle in Japan, they already have ADV embedded in, this
is already on the street.
Oh, wow.
Yeah, wow.
The first versions of ADV are on the street in Japan and Europe today.
Huh.
Yeah, it's interesting how much of this stuff, you know, there's a lot of the stuff in
this space, AI in general, that is, you know, behind regulation and then there's still
even more of it that's like a head of regulation, we're never quite just right, right?
Yeah, yeah.
Yeah.
Yeah.
It's been a great conversation.
Yeah.
Awesome.
Any final parting words?
You know, I always like to say, because this is a machine learning audience, right?
Yep.
And, you know, there's so much focus on deep learning for a lot of good reasons, right?
I mean, deep learning is an incredibly powerful approach that has made progress on problems
where very little progress has been made in a long time before it.
So I'm certainly not knocking deep learning in any way, shape, or forms relevant to a very
wide class of issues.
But it's not the only, it's not the only tool at toolbox.
And there are cases in, as I said, in particular, with edge cases where you need real-time prediction
at the edge in a product with a price point, that, you know, it may not be the best tool.
So, think broadly about your options as you're trying to solve real problems.
That's it.
And, you know, we are, for certain kinds of problems, we are one of those kinds of options.
But I think the, I think the statement is true generally.
Cool.
And we'll, we'll make sure to link to, uh, to both your websites.
I'm anxiously awaiting the automated high beams as well as the other aspects of the, uh,
the smarter car.
Uh, thank you both for taking the time to chat.
Thank you.
Thank you.
Thank you.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
Remember, for your chance to win in our AI at home giveaway, head on over to Twimlai.com
slash my AI contest for complete details.
For more information on Stuart, Brady, or any of the topics covered in this episode, head
on over to twimlai.com slash talk slash 105.
Thanks once again to Intel AI for their sponsorship of this series.
To learn more about their partnership with Ferrari North America Challenge and the other
things they've been up to, visit ai.intel.com.
Of course, we'd be delighted to hear from you, either via a comment on the show notes page
or via Twitter directly to me at at Sam Sharrington or to the show at at Twimlai.
Thanks once again for listening and catch you next time.
