All right, everyone. I'm here with Virginia Smith. Virginia is an assistant professor of machine learning at Carnegie Mellon University Virginia. Welcome to the Twoma AI podcast.
Thanks. Thanks so much for having me.
Hey, I'm looking forward to diving into our conversation. We are going to be focusing on federated learning and some other topics.
But before we do, I'd love to have you share a little bit about your background and how you came to work in the field.
Yeah, absolutely. So I think for me, I always enjoyed math. I always wanted to take as many math classes as I could. But I think one question I had is, you know, how can I really put this math to use?
And just before my senior year in undergrad, I took my first computer science class and absolutely loved it.
And that's kind of what I wanted to focus on in my PhD, something at the intersection of computer science and math. And I think what she learning was a really natural fit.
In my PhD, there was around the time I started a lot of excitement around big data and deep learning was also taking off. And so there was a lot of focus on how to make models more accurate and how to make them more efficient.
And that was kind of what I focused on in my in my PhD is techniques for distributed learning and distributed optimization.
So taking a lot of the machine learning methods we knew and loved in the in the small scale setting and getting them to work across, you know, large data centers and massive amounts of data.
And since then, in my research as well, you know, as a lot of other researchers, I think we realized that, you know, big data is not just big. It's also very complex. And there's a lot more to ensure than just efficiency and accuracy.
So a lot of my recent work has been focusing on other constraints as well, things like robustness and fairness and privacy.
And one application that I think really makes these points salient and grounds these ideas is the application of a federated learning where the goal is to go beyond the data center and train across networks of remote remote devices or across, you know, private data silos like across different organizations.
And I think that this is, you know, a really exciting and ongoing area of research.
Awesome. Awesome. So you think of federated learning as an application that kind of grounds your research is there an application of federated learning that you like to think about when you're, you know, thinking about your research.
Yeah. So I think there's one thing to note is that I guess there's become a kind of important dichotomy in terms of the applications of a federated learning.
So there are applications and in cross device federated learning where the goal is to train across a large network of remote devices.
And there are also applications in what is known as cross silo federated learning where the goal might be to train again in a privacy preserving way, but across say a group of, you know, 10 organizations or, you know, could be hospitals or financial institutions.
And so I've done some work on both types of applications, but more of my work tends to be in the cross device federated learning setting.
Okay. And the main distinction between those is kind of a one to many set of concerns about privacy versus a few to few.
Is that a good way to characterize it?
Yeah. So there can be differences in terms of what you care about from a privacy point of view.
And the major difference is just the scale. So in the cross device setting, you're talking about maybe, you know, thousands to millions of devices that you're learning over each of those devices could be really, you know, constrained from a computational point of view.
The cross silo setting, it could be like a, like I said, 10, 10 hospitals that you're training over and you might have more compute power at each of those hospitals, but there could be similar concerns about, you know, not sharing private information across the organizations or across the devices.
So in that way, you know, they're fundamentally distributed learning problems. It's just a difference in terms of the scale. And then, as you mentioned, the kind of privacy characteristics.
Right, right. And how much of your research is focused on the.
Or even, you know, beyond your research, you know, in terms of where the field is today, the distributed learning aspect of these problems, relative to the privacy aspects of these problems.
We've done a number of interviews on privacy preserving machine learning differential privacy techniques like that.
And I'm curious if that's kind of the bulk of of your research versus, you know, are we still trying to figure out better ways to do the core learning itself across devices.
Yeah, that's a great question. It's really, it's both. And I think in federated learning, privacy is really a first class that isn't. So it's one of the main motivations for performing this, you know, distributed learning problem.
You don't want to move all of the raw data that you have from these user devices to some central location.
There can be some, you know, downstream privacy benefits for keeping that raw data local.
And so this is a privacy is a really important consideration. And I think a lot of the exciting work in federated learning is thinking exactly about this.
So how do we take the privacy, you know, notions that we've thought about in simpler centralized settings and understand them in this distributed learning context.
But certainly it's my work focuses on on both problems.
And it sounds like they're, you know, from a practical perspective, fairly tightly intertwined.
Yes, yes, they can be very related. So I think, you know, as I mentioned, privacy helps to motivate why we would want to perform this distributed learning problem.
So why we want to keep data on these devices as opposed to moving them, but it also makes it difficult to perform the distributed learning because you want to make sure that the information that you do send over the network doesn't reveal any any sensitive information.
One of the areas you've been focusing on from a research perspective is fairness and robustness. You've got an ICML paper on that topic.
Let's start with what fairness means in this context, because I think it's different from the type of fairness we think about from AI ethics perspective.
Yeah, that's a really great point. And I think this goes back to this earlier point I mentioned, which is one thing I think is interesting to me about federated learning is it helps to kind of ground these notions in a specific way.
And certainly I should say there are multiple notions of fairness that you could consider in federated settings.
One of the notions that we've been looking at and that we touch on in this work is related to the idea of representation disparity. And the idea is is basically that if you have a network of heterogeneous devices, so different user devices might be generating data that looks slightly different across the network.
You could imagine, you know, a network of mobile phones, people might be interacting with those phones in slightly different ways. And for that reason, the data might look slightly different across the network.
But you want to train a model that performs ideally equally well across these possibly differing diverse devices that you have in the network.
And so this is related to the idea of representation disparity. We want to, I think, a good way to phrase that is at a high level, you want to ensure some reasonable quality of service across the entire network. So you want to train a model that performs reasonably well across all of the different devices.
And so the premise is that if you apply, if you apply, apply distributed or or federated learning techniques without considering the specific needs of fairness, it's likely that you're going to run into problems where the results aren't fair in that way.
What are the particulars of the failure modes and why do you see them when you're not worried about them?
So what can happen is typically when we're training a model and a federated network, one of the most common objectives to consider is just traditional empirical risk optimization.
So you're trying to minimize the average error across the different devices in the network. And the concern is that if you just look at the average performance, it could be that you perform quite well on average, but at the expense of performing maybe very poorly on a small subset of the devices.
There are situations where if you have a small set of devices that differ in some way, then you can have a model that performs well on many of the devices, but could perform catastrophically on some of these devices.
And this is why you would care about looking at alternatives to empirical risk optimization and encoding this kind of notion of fairness for federated learning.
And when you're thinking about fairness in this way, is it independent of the, you know, what's the relationship between the model or the thing that you're trying to optimize across these the different devices.
Yeah, so the issue is that if you're training just kind of one model to perform well across all of these devices, and you have differing data coming from these different devices and the data might differ in some meaningful way, then there can be limited capacity for one model to kind of capture all of this diversity.
And this is where you can can have issues with with fairness being a concern. And this I shouldn't this can particularly happen because, you know, in federated settings, we're thinking about training models that we can deploy often on device that can run very efficiently and perform often, you know, kind of real time machine learning.
And that naturally limits the types of models that we were able to deploy any settings. And so this is a scenario where even, you know, if we have expressive models, there can be a real limit to how, how just a single model can capture this entire realm of diversity across the network.
And now fairness is, you know, just one of many attributes that you're looking to balance when you're training a model or, you know, federated or not.
Can you talk about some of the other the trade offs that you're making, in particular, your work focuses on trade off between fairness and robustness.
Yeah, so robustness is another really important concern in federated settings. And the idea here is that because you're using user devices as a computing substrate, there can be practical issues that happen with these devices.
So one might turn their phone off or you could potentially have an adversary in the network. And so we want to develop models that are robust to things like device failures or possibly to corrected data.
So what's interesting though is that if you think about the issue that I just talked about with fairness, which is that we want our model to get well possibly to diverse or heterogeneous looking data. This can be directly at odds with this issue of robustness.
So a common way that people handle robustness is they look at that diverse data or, you know, the outlier data that they're seeing and they get rid of it.
Maybe data coming from a corrupted device or a device where there's been some failure. And so an easy way to to think about encoding robustness is just to say let's ignore that information.
And the reason I'm saying this can be at odds with fairness is from a fairness point of view, if that data is actually, you know, just coming from a device, maybe that's generating some different looking data, then that's exactly the, you know, the device that we want to up wait that we want to ensure that our model fits well to.
And so this is why these these two notions can can be at odds in federated learning.
And so the big part of your research and this paper that I refer to the ICML paper is looking at the tradeoffs and how to ensure fairness while managing robustness kind of walk us through the, the approach that you take.
Yeah, so one of the insights that we have in this work is that if you're training, as I mentioned, just a single model across the entire network, there's limited capacity for this one model to be able to both ensure fairness and ensure robustness simultaneously.
And one of the techniques that we propose to help address both of these constraints is something called multitask learning and the idea is basically that intuitively if you have data that differs across the federated network, it makes sense to not just train a single model, but possibly to train multiple models.
So to personalize the model to the local data and multitask learning is one way of doing personalized federated learning, the idea that you're just solving multiple tasks, you're solving from multiple models simultaneously.
And this is something that I think again, it's intuitive, but what we've seen is that it's actually quite powerful on how this simple technique, we're not trying to do anything specific regarding fairness or robustness, we're just implementing actually a very simple multitask learning framework.
And does multi task learning always denote to models as opposed to a single model that's trained to do two things.
So multitask learning has many meanings for different applications.
So, you know, I think we're commonly in deep learning, people might think about multitask learning is learning across actually very diverse tasks, like you're training some NLP models simultaneously with an image classifier.
And here, the notion that I'm referring to for multitask learning is that we can view each device as being its own learning task. And so the overall learning objective can be similar between them, you could still be training just a single image classifier.
But the notion of a task is with respect to the local data set on the individual device. So you're still trying to train and image classifier, but now you have multiple different devices that are generating data and you model each of those devices as an individual task.
And so then you are, I'm trying to put the pieces together, I was thinking about it in the kind of the way I traditionally think of multitask learning where, as you might have, you know, one objective function that's focused on, you know, fairness and another that's focused on robustness and another that's focused on whatever your core task is.
And multitask is, you know, the way you are kind of optimizing across these three objectives, but it sounds like that's not really what we're talking about here.
No, yes. And that's what we're showing here. And I should say one of the reasons that we look at multitask learning in particular is that this is something that's been shown to improve just the accuracy.
So forgetting about fairness and robustness, just learning and accurate model in federated settings, multitask learning has and other forms of personal federated learning have been shown to really improve just the raw accuracy.
And the reason is exactly kind of this point that we mentioned earlier, which is that the data might differ across the network.
And so learning models that are personalized to each of the individual devices can help to improve the overall accuracy.
So, but what we show in this work is that there are also important benefits in terms of fairness and robustness, and especially when you care about both of these things simultaneously.
So basically, what's going on is that if you're learning models that are personalized to the individual devices, then those models have more capacity to to learn to the heterogeneous data.
Right, so you can learn models that are more fair to data that looks diverse, and you can also separate this kind of tension of having just a single global model that you're learning, which helps to deal with issues like robustness.
Right, so you can learn a separate model for all of the corrected data in a network, for example, and then that corrupted model doesn't affect the other parts of the network where you've learned other personalized models.
And is there something as simple as like a hyper parameter that you can dial that you can tune that waits the locally learned model or the, you know, the model trained on local data versus the centralized.
Yes, yes, I'm imagining there's multiple ways to do that you can kind of tune that model at inference time, as well as at training time.
Yeah, yeah, so we actually in this work, I think this is a much broader kind of research direction, which is looking at all the multi task learning for for federated learning or other forms of personalized federated learning.
But in this work, we actually look at a very simple objective, which is similar to what you're saying, so basically what the objective does is it's a simple form of multi task learning where there's there's basically two tasks there's a global model, so there's the model trained across all of the devices.
And then there's a local model, so the model that's personalized to the local data and there's a simple hyper parameter that you can tune to adjust how much you want to rely on the global model versus just your own local model that's just fitting to local data.
And I think, you know, the tension there is that the whole promise of federated learning, the reason that we care about doing this is that, you know, ideally we're getting something from sharing all this information across the network.
We would hope that the global model is providing some some useful information, but we also want to be able to trade off between learning just that one global model and learning more personalized or kind of local behavior on each of these devices.
This is this is exactly what you can do with this with this hyper parameter.
Nice and to when you've got this hyper parameter.
Does the are you is the implications of the local data confined to the local model, which is trained on the device and it stays on the device, and that's how you kind of ensure this separation between the local data.
And the central data or, you know, what ways do you.
And what ways are you kind of leveraging the local data and creating the centralized you still are you still sending the data are you sending weights like how is the centralized model trained.
Yeah, yeah, so there's two parts of training this this multitask objective. So there's the global component and then there's the local components.
So the local component and actually this hyper parameter that I just mentioned is trained completely, you know, in a distributed fashion on completely local data, so ignoring the information from all of the other devices and you can tune lambda just looking at local validation data.
That's all happening locally. What happens, you know, across the network where you end up sharing information is when you're training the global component of the multitask objective.
And what you can do here is you can apply basically a bunch of work and in federated learning to think about how to train this global component.
What you end up sharing is kind of exactly what you're I think you alluded to you end up sharing model updates that are curated based on the local data. So you're trying to basically you're trying to find one global model by aggregating a bunch of smaller model updates from each of the devices.
Got it got it. Very cool. And how do you what's the, you know, what what types of data sets do you evaluate this on.
And in fact, what you know, talk a little bit about evaluation of federated learning in general, what are the kind of the standard benchmarks and metrics that you're looking at.
I think this is a really important problem. Federated learning is very much an ongoing area of research, a lot of new applications coming out. And I think as such, it's really critical that we have a reasonable set of benchmarks to look at.
So this is actually some some motivation for me and as well as some collaborators at Carnegie Mellon and at Google, we came together and we created something called the leaf benchmark, which can be for evaluating federated learning on common common kind of applications that you would see in practice of federated learning.
So it includes a suite of open source data sets that you can use for evaluation, as well as complimentary sort of metrics that you would care about. So you could, you know, validate things like looking at the average accuracy across devices, or you could look at notions of fairness, for example, as well.
So that's kind of part of the benchmark that we developed in terms of evaluating what performance looks like looks like we're trying to simulate what performance looks like when you're actually running this on say a network of mobile phones.
There are a couple strategies here, so I think one of the most common ones is to train this in something like a data center setting, but then simulate what the performance might be if you were running it on a device. So you can, you can think about gathering kind of the raw metrics from training this in a data center, and then you can scale those in various ways, depending on what sorts of constraints you want to add to that, add to that training process.
Another one I should mention is that there's also some a few actually benchmarks that have come up from other groups one from Google is TensorFlow federated and the goal is is to make this.
I think, you know, easier for people to actually run on device so they provide some tools that you could potentially run these techniques on device as well.
Maybe even more fundamentally, what's the, is there kind of a well accepted metric for fairness and a network or robustness and a network, you know, a lot of blue score for, for these types of metrics or,
or is that still evolving? Yeah, I think there's still a lot of work to be done to make this more rigorous and to evaluate a lot of different metrics for fairness, I think it's.
I think there's more of a clear answer here right now and that a lot of the work and fairness has focused on this, this notion of representation representation disparity that I mentioned, and so the goal is to try to ensure a more uniform performance across the different devices.
So you could measure this by looking at say the variance of the test accuracy distribution, or you could look at the worst performing accuracy, so you could look at like the minimax performance, try to find the worst performing device and make sure that's above some threshold.
So those are those are two common metrics for fairness for robustness, there's a lot of different things you could think about, so you could could look at robustness to device failures, as I mentioned, so you could see what happens when devices drop out of the network.
Or you could look at all sorts of different attacks, and I think a lot of the attacks here mirror what you see in centralized settings, so you can look at traditional kind of data or model poisoning attacks, but just as applied in the federal setting.
Got it, got it, awesome, and then separately you've got another paper at ICML that's focused on unsupervised or federated learning and more of an unsupervised setting, can you tell us a little bit about that paper.
Yeah, so the motivation for this paper was actually that I think they were there two key motivations, one is that in practice for a lot of these federated learning applications, you don't have labeled data, and I think for that reason we wanted to to spearhead some work and unsupervised federated learning specifically looking at this idea of clustering and federated networks.
The second major motivation for the work is that so far a lot of the problems I've discussed revolve around this issue that data is diverse and federated networks, you have this issue of heterogeneity that the devices might be generating differing data.
This can result in a lot of problems, it can break the assumptions that we have for traditional distributed optimization methods, it can result in issues of unfairness, it can make it difficult to provide robustness, but what we show in this work is that there for a certain set of problems, there can actually be benefits of heterogeneity, and I think intuitively clustering is one where diversity can be beneficial.
And what I mean by that is the method that we propose in this work, which looks at federated clustering is a simple one shot clustering scheme where basically what you do is you cluster locally on each of the devices and then you aggregate that clustered information to form one global clustering of the data.
And intuitively, if you have data that's diverse across the different devices, this can actually make that method more effective.
So if you have some diversity, if you already sort of have natural clusters that form on the devices, it can be easier to do this in a totally distributed fashion.
And this is what we are kind of making rigorous in this work is the benefits that might exist of clustering in a federated network, specifically when you have heterogeneous data.
And so meaning the paper is not specifically focused on the techniques, but looking at kind of performance bounds, it's more theoretical paper, is that the idea? Yeah, yeah, so it's, it's good to both. We do propose this one shot communication scheme, it's, it's, it's basically just a distributed version of Lloyd's method, which is a very common method for canines clustering.
But then I think that the meat of it is really analyzing the performance guarantees for that method and showing in particular that this issue of heterogeneity can be beneficial for the analysis.
Okay, and what's the, can you kind of summarize the intuition around how this method.
You know, makes heterogeneity beneficial or kind of unlocks the power of the native heterogeneity and the data.
Yeah, yeah, so I think that the main idea is basically that if you, if you want to do this, this kind of simple, very communication efficient type of clustering, which makes a lot of sense in federated learning, you know, if you're training across a million devices, it makes sense to try to reduce the communication as much as possible.
So this technique that we're looking at is, is a, I think a really simple heuristic for how you might want to perform clustering and practice.
Basically, you can cluster your data locally on each device, send it to some central server, and then you can aggregate those, those local clusters into one global clustering.
And the reason I'm saying that heterogeneity can be beneficial for this process is that in clustering, the goal is basically right to, to split your model into separate separate, split into these separate clusters.
And if your data is heterogeneous, in some way, it's already, it's already been distributed based on these clusters, right? So you wouldn't, and that some devices might only have data from a small subset of the total clusters.
And given that, then it helps to, to make this process more decoupled, it makes it easier to distribute the clustering across the devices. And so this is the, this is the way that heterogeneity can, can benefit this, this analysis, specifically look at this idea that each of the devices only has data from a small number of clusters, which would, it's, it's an intuitive way to, to think about how the data might be heterogeneous.
Right, there's a small number of clusters that belong to each one of the devices basically.
And what we show then is just that by, by, by performing this one shot clustering scheme, with this heterogeneity assumption, you can show that the, the results are, are basically better than if you were performing it on, like totally randomly IID partition data.
Okay, okay. When you, how do you characterize the heterogeneity of your, your data and this, what's the assumption you're making in the paper?
Yeah, so in this paper, we're, we're making, exactly, so we're making this assumption that each device contains data from a small number of the underlying clusters.
So say that you know that all of your data is coming from, I don't know, 100 different clusters, then the assumption could be that every device contains data from only three of those clusters.
And this is, this, this is one notion of, of heterogeneity, but it makes sense in the, in the clustering context, if, you know, if you're, if your, if your goal is to perform clustering, it makes sense to think about heterogeneity in terms of the underlying clusters.
And so this is the, this is the notion that, that we look at is basically that there is, there's a small number of clusters that generate each of the local data sets at each of the devices.
And is the, is there a lot of prior work that has gone into this idea of thinking like local ID versus global ID and federated environments.
So I would say that there's a lot of work thinking about this issue of heterogeneity in, in federated setting. So this is really, I think it's a finding characteristic of federated learning compared compared to something like the data center setting.
And the reason is that in the data center, the idea is that even though you're still solving a distributed learning problem, you own and can access all of that data and you can repartition it anyway you want to.
Major difference in the federated setting is that each of these devices say a mobile phone, you're generating data on that phone and then you're not moving that data or repartitioning it across the network in any way.
And what this means is that, you know, in the data center, even though you were still distributing your data across different machines, you could partition that data in an IID manner and an independent and I can typically distributed manner across the machines in the federated setting.
You're getting the data as is well different devices might be generating different data and that results in this, you know, this issue of non IID or heterogeneous data across the network.
And I think I mentioned this earlier, but there's there's been work in thinking about how this affects fairness and robustness, but another major issue is that this can affect some of the convergence guarantees that we have for communication efficient optimization methods in federated settings.
So one of the main assumptions that you know it's typically made when you're performing distributed computing is that the data is is IID distributed across nodes. And so this actually breaks kind of a fundamental assumption in some of the common methods and analyses that are used for distributed learning.
Yeah, and I guess I drew a parallel between, you know, one of these, you know, devices are a subset of devices with heterogeneous data and, you know, what I thought I was like local ID, do you see that, you know, within one of these heterogeneous segments.
Yeah, there is an ID property and do you rely on that or do you assume that ID is just broken and it's replaced with this local notion of heterogeneity.
Yeah, so I think a good way to frame it would be that each device is generating data in an IID way, but according to its own separate distribution.
The distributions can differ across the devices, but each device might be generating data in an IID fashion, just according to its own unique distribution.
You know, one of the motivations for something like multitask learning would be that the distributions between different devices might be similar, so it makes sense to train them simultaneously and to learn about, you know, how these different devices might differ from one another.
And they, they differ in a meaningful way, so it's also just not to just train one model.
Yeah.
We talked a little bit about kind of, you know, applications of this work and federated learning generally, when you're looking at the kind of unsupervised setting, what are some specific applications there, is it something along the lines that you, you know, have an army of mobile devices and you're trying to segment them by type or something like that.
Yeah. So actually one, this relates to this idea of multitask learning and personalized learning more generally, a simple way to perform multitask learning is just to first cluster your devices into clusters.
Right. So if you knew that there was a natural clustering between the devices, then you could learn models specific to each of those clusters.
So that this would provide you this one shot clustering scheme that we, that we look at provides you a simple way to do multitask learning, you can just use this clustering procedure, and then you can learn models that are personalized to the individual clusters and the network.
But beyond that, I mean, clustering is also, as you know, clustering is obviously widely used for a lot of applications and in machine learning just as an important kind of pre processing tool to understand and analyze, you know, the underlying data distributions that you have.
And so this could also just be used as maybe a pre processing step to, to get a sense of what the data looks like in the network.
Does your, your first point kind of suggest a hierarchical kind of model tearing where you've got this, you know, centralized model, then you've got this intermediate type of model that's based on clusters, and then you've got a local device model, you know, all of the first conversation about robustness and fairness and instead of your one lambda parameter, now you've got kind of two that you're
focusing across these different models.
Yeah, you know, that's an interesting point. We haven't looked at that, but I think that's a very natural way that you could think about applying these things. Yeah, you could have maybe multitask learning happening within each of the clusters as well.
And I think, and this is something that we haven't looked at as well, but I think it makes sense. Another benefit of these multitask objectives or things like clustering is that you could also help to reduce communication in a meaningful way. So you could only in the scenario that you're describing, maybe you could have this nice hierarchical structure where you only actually communicate within a small cluster within the network as opposed to, you know, sending everything to this one central location.
Right, right. Awesome. So what, you know, what are some of the future research directions that you're looking at and excited about your work.
So one direction that we, I think we started at and then I just want to circle back to is the idea of privacy.
This is something that is really important in embedded settings. And in particular, you know, the common notion of privacy that's considered is that we want to be able to train models across these devices without necessarily being able to to know that any one device participated in the training procedure.
So a common tool to address this is through techniques like differential privacy. Some recent work that that I've been looking at and I think is really important is to think about how privacy then connects with issues of fairness and robustness and personalization. So a lot of the other topics that I, that I touched on.
And in particular, you know, one area we've been looking at recently is defining notions of privacy for multitask learning. So for these, for these personalized objectives, there's a real lack of work understanding how to make those models differently private.
So I think that's a really important area of work to ensure that we can, you know, simultaneously address all of these constraints, not just fairness and robustness and efficiency and accuracy with also the constraints of privacy.
Awesome. Awesome.
Well, Virginia, thanks so much for taking the time to chat. It's been great learning a bit about your research and what you've been up to.
Yeah, thank you so much. Thanks again for the opportunity. Thank you. Bye. Bye.
