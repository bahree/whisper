1
00:00:00,000 --> 00:00:14,000
All right. So without further ado, I would like to get started with our first session for today. This session is the great ML ops debate.

2
00:00:14,000 --> 00:00:27,000
When talking to ML and AI teams about platform strategy, two of the big questions that always come up are one build versus by which we addressed in our panel.

3
00:00:27,000 --> 00:00:37,000
Our panel on Tuesday as well as our keynote yesterday a bit as well as you know what are the best tools to use to create a platform.

4
00:00:37,000 --> 00:00:50,000
And this end to end versus specialized question is a really interesting one I wrote about it quite a bit in the definitive guide to ML platforms ebook, which I talked about the wide versus deep paradigm.

5
00:00:50,000 --> 00:01:02,000
I encourage you to check that out. I'm not going to go into the detail of that here, but rather I am going to cue things up for our panelists to debate it out.

6
00:01:02,000 --> 00:01:16,000
With that, I would like to hand things over to my friend and friend of the show, Demetrios Brinkman, organizer of the ML ops community to get things to get things started.

7
00:01:16,000 --> 00:01:23,000
If you can't tell from his avocado shirt, make sure that this is a fun session.

8
00:01:23,000 --> 00:01:35,000
That is right Sam. Thanks for the intro, man. I tried to create a little background, make it a little more interesting while we were in the green room there.

9
00:01:35,000 --> 00:01:38,000
So we've got the great debate, man.

10
00:01:38,000 --> 00:01:48,000
I'm thankful that you asked me to come and do this again. I had a great time last year when we did it. And so now I'm looking forward to doing it again.

11
00:01:48,000 --> 00:02:00,000
Awesome. Awesome. I'm going to turn it over to you. I guess I've got one more comment. And that is, let's see, I see a poll. Oh, the poll is already up. I'm going to let you take it away.

12
00:02:00,000 --> 00:02:11,000
Excellent. So I'll explain the poll in just a minute, but we've got so much good stuff that's about to happen here. I hope you all are as excited as I am.

13
00:02:11,000 --> 00:02:21,000
I want to go through a few of the logistical pieces before we jump into the full on session and get into the debate.

14
00:02:21,000 --> 00:02:26,000
So let me explain what exactly it is that we are planning on doing.

15
00:02:26,000 --> 00:02:46,000
I'm going to introduce everyone that is joining us in just a minute, but first, what I want to do is talk to you about this cool giveaways that we've got everywhere I go whenever I try and do some kind of

16
00:02:46,000 --> 00:02:58,000
virtual hosting. I try and get people to give us stuff so that we can give it to you that are listening. And today is no different. We've got no gift here who is one of the participants.

17
00:02:58,000 --> 00:03:18,000
And he wrote a book called practical ML ops. We've got a few of those copies to give away to people in the audience. So be active in the chat, just like comment on everything that's happening and all randomly be giving those away throughout the debate.

18
00:03:18,000 --> 00:03:31,000
The other thing that I can give away are points. So anyone that is interested in getting more points, just be active in the chat. I'm pretty liberal with how I give them away.

19
00:03:31,000 --> 00:03:38,000
Now the next piece that I want to talk about is the vote and the poll that we've got.

20
00:03:38,000 --> 00:03:52,000
You can see on the tab, there's a poll that is up right now. And what I want to do is figure out what's the Delta who wins this debate by how much they can swing the vote back and forth.

21
00:03:52,000 --> 00:04:15,000
So that means that you go and you vote in the poll right now. And at the end of this session, you're going to vote again. And we're going to see how much your mind has changed. So go into this. I just ask that you go into this open minded and you allow some of these incredible debaters to change your mind.

22
00:04:15,000 --> 00:04:33,000
If they have some things that make sense. All right. So what are the logistics of how this debate is going to work? Well, first of all, we're going to have each side. We've got two sides. We've got four contestants or four debaters.

23
00:04:33,000 --> 00:04:51,000
And each member, each debater is going to have three to four minutes to argue their point for or against and basically saying that there is one tool to rule them all or it's a best of breed solution.

24
00:04:51,000 --> 00:05:10,000
Next up, after they talk their talk for their three minutes, we're going to go and have them do one minute rebuttals because we don't know what everybody's going to bring up and they get to respond to some of the points that the other team has brought up.

25
00:05:10,000 --> 00:05:30,000
And then from there, I'll give some final thoughts. I'll try and wrap put everything that I've heard into a cohesive statement so that we can understand it. That's where I may need your help in the chat who wants some free books. And then we will ask you to vote again.

26
00:05:30,000 --> 00:05:49,000
And that's where we'll see what the Delta is and who wins this debate. All right. So without further ado, I want to introduce who is with us today. And for the best of breed team, the ones that are saying it is the best of breed.

27
00:05:49,000 --> 00:06:03,000
They're arguing against that one tool to rule them all. We've got Dan Jeffries, my man and Sean Mohanti. Let's let's talk about who these guys are real fast.

28
00:06:03,000 --> 00:06:13,000
Dan Jeffries just told me today that he is in Berlin. So I'm going to go ahead and go out on a limb, say his intro.

29
00:06:13,000 --> 00:06:24,000
He's a futurist. He's also the managing director and kind of the brainchild behind the AI infrastructure alliance. Awesome. If you have not seen that, definitely go check it out.

30
00:06:24,000 --> 00:06:43,000
And now he's the CIO of stability AI, you know, you may have heard of a little thing called stable diffusion. And last but not least, the dude isn't no mad. Every time I talk to him, he's in a different place. It makes me quite jealous. Anyway.

31
00:06:43,000 --> 00:07:01,000
Now we've got Shiamm who is the CEO and co-founder of watchful company that automates the process of creating label training data. Very useful stuff that we all know that we need.

32
00:07:01,000 --> 00:07:27,000
He's got decades of experience leading data engineering teams at companies like Facebook. That is a company you've probably heard of. And what he did while he was there was he served as the lead for the stream processing team responsible for processing 100% of the ad metrics data for all of Facebook products.

33
00:07:27,000 --> 00:07:41,000
If anyone has dove into the streaming ecosystem, you know what a feat that is right there. All right. So that is our best of breed team right there. Good luck fellas.

34
00:07:41,000 --> 00:07:54,000
We'll see you in just a second. Next up, I want to introduce the one tool to rule them all team. Now on this team, we've got no gift.

35
00:07:54,000 --> 00:08:14,000
And we've got Bindu ready. No gift. I mean, I, what's up, Bindu? I cannot say enough things about Mr. Noah. I have had the pleasure of meeting Noah in person in Portugal. He gave a talk at our MLOPS community local meetups.

36
00:08:14,000 --> 00:08:36,000
He was incredible. He's also given various talks on our MLOPS community virtual meetups. And as I mentioned before, he wrote the book practical MLOPS. You can find all kinds of his material on pragmatic AI YouTube channel. He's got so much good material on there. I encourage anyone to go check it out.

37
00:08:36,000 --> 00:08:50,000
He's teaching MLOPS courses. Maybe I think it might be the first MLOPS course I've ever heard of being taught at a university. So just good on you. No, I'm for doing that. And now Bindu.

38
00:08:50,000 --> 00:09:16,000
He's the CEO and co-founder of Abacus AI. Before Abacus, she was the general manager for AI verticals at AWS. She created Amazon personalization and Amazon forecasting. And prior to that, she was the co-founder of post intelligence, a deep learning company for social media influencers, which was ultimately acquired by Uber.

39
00:09:16,000 --> 00:09:35,000
And prior to that, she was at Google. So we've got quite the line up here. This is some top notch stuff. I'm excited to hear what you all got to say. I'm going to keep Noah on the screen because Noah, I'm going to ask you to argue your point first.

40
00:09:35,000 --> 00:09:39,000
Are you ready for this man? Sure, let's go for it.

41
00:09:39,000 --> 00:09:52,000
All right. I'm going to throw three minutes on the clock. And let's before we fully start, let's make sure everybody did the vote for the poll. Everybody do that. And let's see what we got. All right.

42
00:09:52,000 --> 00:10:00,000
We can we might be able to throw up what it's will at the end will throw up the differences. All right. Noah, I'm giving it to you, man.

43
00:10:00,000 --> 00:10:15,000
All right. So a little bit about my background, I worked in the barrier for startups, enterprise companies. And so I have experienced both in managing teams of people, managing companies, and then also doing the work myself.

44
00:10:15,000 --> 00:10:26,000
And one thing I really realized was that the building bespoke solutions is or picking risky technology is actually not a good strategy.

45
00:10:26,000 --> 00:10:39,000
If you're running a company or running a team as an individual, it actually feels great because you're you're actually getting more and more power. So that's the really the scope of what I'm going to discuss in these five key points.

46
00:10:39,000 --> 00:11:00,000
The first thing I'll mention is that one of the more surprising things about the startup industry is that people are still entering them because they're extremely risky. And if you look at startups, it's I think that the latest stats I found were that one out of 50 startup founders employ anyone other than the founder.

47
00:11:00,000 --> 00:11:17,000
After 10 years, and if you think about that from an expected value position, right, you take the probability times the outcome, most startup employees, their expected value is like 2000 versus regular company. It's maybe 250,000. So these are really risky companies.

48
00:11:17,000 --> 00:11:26,000
And similarly, you can think of technology as a proxy for that. And so I think you should be very careful about taking a risk just because it sounds fun.

49
00:11:26,000 --> 00:11:39,000
And the second is this concept of 90 and risk, which is a lack of any quantifiable knowledge about a possible occurrence. We don't know all of the details about what's going to happen with machine learning in the future.

50
00:11:39,000 --> 00:11:47,000
For example, are people going to go to jail if they made bad recommendation engines or, you know, is there going to be systematic bias that destroy society.

51
00:11:47,000 --> 00:11:57,000
So there's some real interesting problems here. And the more aligned you are with mainstream technology vendors, I think you're reducing your 90 and risk.

52
00:11:57,000 --> 00:12:14,000
Third is hiring and maintenance are way, way easier if you take the biggest market share. Right. So if you look about train talent, you're going to get a bigger share of talent from the biggest companies. If you look at training material, they're going to have a wider variety of training materials going to be easier to get your staff on board.

53
00:12:14,000 --> 00:12:24,000
You can get people in your company certified. And then finally, you can call someone up on the phone after your star engineer leaves and actually get enterprise support to actually help fill in that role.

54
00:12:24,000 --> 00:12:37,000
So really continuity is a huge issue with a platform. And then the other one, as I mentioned before, is that if you're an individual, and I guess I am right now, you know, independent for the most part, you have a lot of power.

55
00:12:37,000 --> 00:12:58,000
Or if you pick very exotic solutions, because if you leave your company is in big trouble on the flip side, if you're an employer and you want to have kind of uniformity and have consistent results, you want to pick things where there isn't a key employee that's going to have the hit by the bus problem where if they leave the company or get hit by a bus, all of a sudden your company fails.

56
00:12:58,000 --> 00:13:17,000
The thing I'll mention is that we actually have a lot of data around what's a good platform strategy. In fact, the business author Jim Collins mentioned in good to great that technology accelerates existing advantages in all of his analysis of the best companies in the world, it doesn't create them.

57
00:13:17,000 --> 00:13:32,000
So I think it's a mistake to think that you in a kind of single individual role are going to heroically solve everything by creating this, you know, best of breed scenario when really you should pick boring solutions and accelerate the existing technology advantage.

58
00:13:32,000 --> 00:13:47,000
That's my that's my pitch.

59
00:13:47,000 --> 00:14:05,000
All right, we've got somebody else that is going to argue the other side of this, let's see, Cheyenne, where you at man, where you at paging Cheyenne to the edge, you ready, you ready to rebuttal and give me a coach.

60
00:14:05,000 --> 00:14:10,000
Yeah, three minutes on the clock, here we go.

61
00:14:10,000 --> 00:14:19,000
All right, so I think that was a fair argument for kind of like end to end platforms. Let me, let me give like a counter argument.

62
00:14:19,000 --> 00:14:28,000
First of all, kind of just to set the stage, best and breed products or best of breed products tend to sort of push technological innovation forward.

63
00:14:28,000 --> 00:14:32,000
That's where a lot of technological innovation typically originates.

64
00:14:32,000 --> 00:14:42,000
And then platforms tend to be slow innovating and often not fully encompassing. We have to recognize the fact that real world machine learning problems are super diverse.

65
00:14:42,000 --> 00:14:49,000
And it's very likely that you'll run into a shape of problem that doesn't quite fit with an existing and then provider perfectly.

66
00:14:49,000 --> 00:14:55,000
It might be able to do 75 to 80% of what you want it to do, but the remaining 20% you're kind of out of luck.

67
00:14:55,000 --> 00:15:04,000
And that sucks. Now, on the other hand, with best of breed products, you're able to sort of like incrementally build up your internal systems.

68
00:15:04,000 --> 00:15:08,000
And as a result, you're able to sort of de-risk the overall system as a whole.

69
00:15:08,000 --> 00:15:15,000
So namely, you can start small like by maybe one to small products that can kind of fit together like Lego blocks.

70
00:15:15,000 --> 00:15:23,000
And as your uses your use cases expand, you can also expand your technology stack and sort of like incrementally build it up.

71
00:15:23,000 --> 00:15:33,000
Now, by combining several of these best of breed products, you're able to really like tailor design a system for your organization and for your needs.

72
00:15:33,000 --> 00:15:49,000
Now, I know that a lot of the arguments against best of breed products are actually around like integration and support and sort of like as as as no one mentioned earlier that that bus factor.

73
00:15:49,000 --> 00:16:01,000
But really, I think like in today's age, a lot of that has been de-risked. So especially with like the advent of containerization and like the widespread use of scheduler systems like Kubernetes.

74
00:16:01,000 --> 00:16:18,000
A lot of that integration risk is now de-risked simply because in the past, you needed like in a data ops analog, you needed folks like a cloud era or a Hortonworks implement things like Spark and Hadoop where you because it was very cumbersome to manage.

75
00:16:18,000 --> 00:16:29,000
And that sort of thing. But now with things like Kubernetes, it's like all right, just deploy a bunch of containers into the same environment and administrative with the same grammar that you use to administrative everything else.

76
00:16:29,000 --> 00:16:37,000
So in that way, there's very little integration risk because generally speaking most companies now are moving towards kind of a containerized future.

77
00:16:37,000 --> 00:16:45,000
So you can use the same sort of logical constructs and the same systems to administrative all of your platforms.

78
00:16:45,000 --> 00:17:06,000
And finally, like one last point, I kind of touched on it just now is we have prior art for this. There's nothing to indicate that the data science world, machine learning world is going to trend towards an end to end like winner takes all solution simply because most other industries haven't done that either.

79
00:17:06,000 --> 00:17:17,000
So if you take a look at most companies, you see installations for individual pieces like Salesforce, Dropbox, Slack, Zoom, JIRA, and so on.

80
00:17:17,000 --> 00:17:29,000
There's nothing indicating that data science or machine learning as a whole is going to be any different. So that's that's sort of my overall take.

81
00:17:29,000 --> 00:17:39,000
The history points to the fact that best of breed solutions sent to win because they fit best for the problems that you have.

82
00:17:39,000 --> 00:17:53,000
Okay, I see you. I like it. So now we've got a little intermission because the results from the poll are in.

83
00:17:53,000 --> 00:18:08,000
Let's see what we got. Can we bring those up onto the screen? Can we see what the results for the poll were? Is that is that possible here? Let me see IT IT.

84
00:18:08,000 --> 00:18:23,000
Maybe not. We'll wait until the next one after I'm about to bring up Bindu then let me or maybe they're up and I can see them.

85
00:18:23,000 --> 00:18:30,000
They're up and I just can't see them. So everyone, everyone can see them in the poll tab.

86
00:18:30,000 --> 00:18:50,000
All right, so if you go to the poll tab, you should be able to see it. Oh my God. This, the results for this are very strong, very strong. We've got best of breed that is shown up with a huge lead, gigantic lead.

87
00:18:50,000 --> 00:19:00,000
So Bindu, no pressure, but this is kind of all on you to bring us home.

88
00:19:00,000 --> 00:19:06,000
All right. You got three minutes. Let's put it on the clock.

89
00:19:06,000 --> 00:19:13,000
Hi, everyone. My name is Bindu Reddy. See your co-founder of advocacy, which is an end to end MLObs platform.

90
00:19:13,000 --> 00:19:21,000
So I'd like to like reframe this slightly. Obviously, there's never going to be one tool which roots all parts of AI that's impossible to do.

91
00:19:21,000 --> 00:19:30,000
But enter and MLObs platforms make sense in the enterprise AI cases for common enterprise AI use cases. Right.

92
00:19:30,000 --> 00:19:39,000
If you are building an autonomous air driving company or a robotics company. Hell no, don't use an end to end MLObs platform.

93
00:19:39,000 --> 00:19:48,000
I mean, go build your own platform because it makes sense in that context. But if you're an IP or a GM or a Macy's, then of course you should use an end to end MLObs platform.

94
00:19:48,000 --> 00:19:59,000
Here's why without a data machine learning is useless for common enterprise use cases like say forecasting, personalization, you know, text labeling and so on.

95
00:19:59,000 --> 00:20:08,000
So data and models are very intertwined with each other. And more importantly, if you build something and you don't know what you know what data sets went into that model.

96
00:20:08,000 --> 00:20:13,000
Or you don't know what cost the drift, then you're not going to be able to do kind of model CI CD.

97
00:20:13,000 --> 00:20:22,000
So for common simple enterprise use cases, it does actually make sense to have a tool which actually strings together all parts of the ML lifecycle.

98
00:20:22,000 --> 00:20:31,000
So you can actually see what data you used, what features you used, how the model was built, how, why is something is drifting and how to fix it.

99
00:20:31,000 --> 00:20:38,000
Imagine if you had a feature store, you know, from one vendor drift from another vendor and let's say there was a model drifting.

100
00:20:38,000 --> 00:20:43,000
You wouldn't even know how to exactly fix it without a lot of life kind of communicating between the two vendors.

101
00:20:43,000 --> 00:20:56,000
So far, you know, simple applications and to end helps you actually debug your models easily get to production easily and quickly go to, you know, multiple different models at scale.

102
00:20:56,000 --> 00:21:03,000
Now, you know, shy and mentioned that there is no kind of like that, you know, prior after this, I actually strongly disagree.

103
00:21:03,000 --> 00:21:08,000
Amazon, Google, Microsoft, all believe in entry and I'm a lot of platforms.

104
00:21:08,000 --> 00:21:14,000
Siege maker has the biggest market today, 99% of all enterprise AI today is an entry and platform.

105
00:21:14,000 --> 00:21:25,000
Again, we're not talking about, you know, some esoteric thing that and video might be where building or we're not talking about what crews this are doing, we're talking about standard enterprise AI and data science.

106
00:21:25,000 --> 00:21:31,000
And it makes a lot of sense to kind of build this in a way where you can see the beginning and the end of a particular problem.

107
00:21:31,000 --> 00:21:33,000
Classic problem would be churn.

108
00:21:33,000 --> 00:21:41,000
If you don't really know what feature is going to your churn model or if that's sitting in a different vendor, it's going to be very difficult for you to interface between these different systems.

109
00:21:41,000 --> 00:21:47,000
And have you been in an enterprise? Do you know how long it takes to actually get a best of breed or any tool going?

110
00:21:47,000 --> 00:21:49,000
Now, imagine getting 10 tools for it.

111
00:21:49,000 --> 00:21:54,000
Now, imagine getting those 10 tools to actually integrate with each other. Good luck with that.

112
00:21:54,000 --> 00:22:01,000
The other case that Cheyenne made was in a sales force drop box are all individual platforms.

113
00:22:01,000 --> 00:22:09,000
Exactly. And therefore, this is one, too. There are a few use cases for entry and MLObs to make sense.

114
00:22:09,000 --> 00:22:13,000
And they are today in production working a large number of enterprises.

115
00:22:13,000 --> 00:22:15,000
So the market is already saying that.

116
00:22:15,000 --> 00:22:23,000
The second piece of this is, you know, what do companies take, for example, someone like Macy's or someone like Nike?

117
00:22:23,000 --> 00:22:25,000
What is their end goal going to look like?

118
00:22:25,000 --> 00:22:32,000
I strongly believe that the end goal is going to be a hybrid, where one is they're going to build models on their own based on their first party data.

119
00:22:32,000 --> 00:22:34,000
It's going to be an MLObs like platform.

120
00:22:34,000 --> 00:22:41,000
And by the way, the same MLObs like platform exists today in Google. It exists today in Netflix, it exists today in Amazon, Uber everywhere.

121
00:22:41,000 --> 00:22:45,000
They all believe in enter-end platforms. They haven't thought best of breed.

122
00:22:45,000 --> 00:22:52,000
Facebook, for example, just released something called looper and Google Uber obviously had Michelangelo for a long time.

123
00:22:52,000 --> 00:22:59,000
So the end game is going to be an entry-end MLObs platform for the first party data, but also APIs.

124
00:22:59,000 --> 00:23:02,000
I really like what OpenAI is doing.

125
00:23:02,000 --> 00:23:05,000
I think into some extent stability will do that, too.

126
00:23:05,000 --> 00:23:07,000
What Hugging Facebook is doing is great.

127
00:23:07,000 --> 00:23:09,000
So it's going to be a mix of these.

128
00:23:09,000 --> 00:23:10,000
We're going to use APIs.

129
00:23:10,000 --> 00:23:13,000
And we're going to get results for certain tasks, simple tasks.

130
00:23:13,000 --> 00:23:17,000
We're also going to have an entry-end MLObs platform for first party data.

131
00:23:17,000 --> 00:23:19,000
That's how I see the future of enterprise there.

132
00:23:19,000 --> 00:23:24,000
And finding you to the point of advanced applications, you see Figma.

133
00:23:24,000 --> 00:23:28,000
Figma is a very good in-depth advanced application.

134
00:23:28,000 --> 00:23:30,000
It has a lot of depth as well as breadth.

135
00:23:30,000 --> 00:23:35,000
So I think there's a false dichotomy here of saying it should be either deep or wide.

136
00:23:35,000 --> 00:23:39,000
I completely disagree. Figma actually got sold to Adobe for $20 billion.

137
00:23:39,000 --> 00:23:40,000
It's one of the best apps.

138
00:23:40,000 --> 00:23:42,000
It has both depth and width.

139
00:23:42,000 --> 00:23:44,000
We're in 2023. We can get both done.

140
00:23:44,000 --> 00:23:49,000
In fact, not only at Cloud Renders, I would like to think we, advocacy AI,

141
00:23:49,000 --> 00:23:52,000
are the best of breed entry-end MLObs platform.

142
00:23:52,000 --> 00:23:54,000
Thank you.

143
00:23:54,000 --> 00:24:01,000
We've got one more full debate.

144
00:24:01,000 --> 00:24:05,000
The master debater is coming up right now.

145
00:24:05,000 --> 00:24:07,000
Dan, where are you at?

146
00:24:07,000 --> 00:24:11,000
You laid it on thick, man.

147
00:24:11,000 --> 00:24:14,000
You know, no, I got to deliver here.

148
00:24:14,000 --> 00:24:19,000
I'm just, I'm just sad that you're more not going to see each other tonight.

149
00:24:19,000 --> 00:24:21,000
I didn't realize that you were in Berlin.

150
00:24:21,000 --> 00:24:24,000
And now, so I got a.

151
00:24:24,000 --> 00:24:25,000
I'm leaving it 50-50.

152
00:24:25,000 --> 00:24:28,000
I mean, let's see, you know, where they were in the 50-50.

153
00:24:28,000 --> 00:24:34,000
Anyway, before you start, I want to mention that there are some people in the comments

154
00:24:34,000 --> 00:24:36,000
that are doing an amazing job.

155
00:24:36,000 --> 00:24:38,000
And I got some books to give away.

156
00:24:38,000 --> 00:24:41,000
So Peter, no.

157
00:24:41,000 --> 00:24:43,000
I think that's how you pronounce the name.

158
00:24:43,000 --> 00:24:44,000
Forgive me.

159
00:24:44,000 --> 00:24:49,000
If that is not how you pronounce it, you're getting a practical MLObs book.

160
00:24:49,000 --> 00:24:54,000
And is that you are getting some practical MLObs books?

161
00:24:54,000 --> 00:24:56,000
And I got some points to give away.

162
00:24:56,000 --> 00:25:00,000
So Angel, Mario, you get some points.

163
00:25:00,000 --> 00:25:02,000
We're going to be just given those points away.

164
00:25:02,000 --> 00:25:04,000
I feel like Oprah right now.

165
00:25:04,000 --> 00:25:05,000
Who else wants a car?

166
00:25:05,000 --> 00:25:08,000
All right, Zachary, give Zachary some points.

167
00:25:08,000 --> 00:25:11,000
I'm going to give one more book away by the end of it.

168
00:25:11,000 --> 00:25:13,000
Let me know in the chat if you want a book.

169
00:25:13,000 --> 00:25:19,000
And let us get back to the final chat.

170
00:25:19,000 --> 00:25:22,000
And then we're going to have some rebuttals, a round of rebuttals.

171
00:25:22,000 --> 00:25:26,000
So Dan, it's on you, man.

172
00:25:26,000 --> 00:25:28,000
And we're going to bring it home here.

173
00:25:28,000 --> 00:25:32,000
Look, so the answer to this is pretty simple.

174
00:25:32,000 --> 00:25:35,000
You can't have an end-to-end platform because it doesn't actually exist.

175
00:25:35,000 --> 00:25:40,000
It only exists in the mind of marketers and in hype cycles.

176
00:25:40,000 --> 00:25:43,000
So that's the main problem.

177
00:25:43,000 --> 00:25:46,000
For instance, you may desire it to exist.

178
00:25:46,000 --> 00:25:49,000
You may want it to exist.

179
00:25:49,000 --> 00:25:51,000
You may believe that it exists.

180
00:25:51,000 --> 00:25:55,000
But there's a delta between reality and what we think about it.

181
00:25:55,000 --> 00:25:58,000
And you're free to believe whatever you want.

182
00:25:58,000 --> 00:26:01,000
You could believe that gravity doesn't exist.

183
00:26:01,000 --> 00:26:05,000
But if you go up to 100 story window and jump out of it,

184
00:26:05,000 --> 00:26:08,000
you'll find out that gravity has enough defeated record, right?

185
00:26:08,000 --> 00:26:11,000
So I tend to look at things from a practical reality standpoint.

186
00:26:11,000 --> 00:26:15,000
And the fact is when you think about the classic diffusion of innovation curve,

187
00:26:15,000 --> 00:26:18,000
where you have the invention and the early adopters,

188
00:26:18,000 --> 00:26:21,000
and the early majority and the late majority and the lag groups,

189
00:26:21,000 --> 00:26:24,000
we're still very much in the early adopter phase.

190
00:26:24,000 --> 00:26:28,000
And so many of these tools are immature.

191
00:26:28,000 --> 00:26:31,000
And even the ones that are sort of developed,

192
00:26:31,000 --> 00:26:34,000
which we've seen in the infrastructure alliance report,

193
00:26:34,000 --> 00:26:37,000
where we surveyed over 300 different companies,

194
00:26:37,000 --> 00:26:39,000
there's different levels of maturity.

195
00:26:39,000 --> 00:26:43,000
When you think about all of the things that an ML ops platform has to do well

196
00:26:43,000 --> 00:26:46,000
from adjusting the data, cleaning it,

197
00:26:46,000 --> 00:26:50,000
data versioning, providence, lineage, experimentation,

198
00:26:50,000 --> 00:26:54,000
training, distributed training, you know, deployment,

199
00:26:54,000 --> 00:27:00,000
inference, monitoring, management, it's just impossible for any product

200
00:27:00,000 --> 00:27:03,000
at this stage, despite the marketing hype,

201
00:27:03,000 --> 00:27:08,000
to actually be fantastic at all of those things.

202
00:27:08,000 --> 00:27:11,000
Now it is feasible if you have a very limited use case.

203
00:27:11,000 --> 00:27:15,000
You know, if all you're doing is churn prediction and basic analytics,

204
00:27:15,000 --> 00:27:18,000
then you're probably fine with a single solution.

205
00:27:18,000 --> 00:27:22,000
If you've got a single data scientist with a laptop,

206
00:27:22,000 --> 00:27:24,000
trying to figure out your first ML problem, yeah, sure.

207
00:27:24,000 --> 00:27:26,000
One solution's going to do it.

208
00:27:26,000 --> 00:27:28,000
But if you've got hundreds or thousands,

209
00:27:28,000 --> 00:27:31,000
and they're all working together, it simply doesn't exist.

210
00:27:31,000 --> 00:27:36,000
There's no platform on the planet that can do tremendous distributed training

211
00:27:36,000 --> 00:27:39,000
like we do at the stability or open AI.

212
00:27:39,000 --> 00:27:42,000
There's that can also do deployment and inferencing at scale

213
00:27:42,000 --> 00:27:46,000
that has tremendous monitoring and management capabilities,

214
00:27:46,000 --> 00:27:48,000
plus a beautiful experimentation engine.

215
00:27:48,000 --> 00:27:49,000
It just doesn't exist.

216
00:27:49,000 --> 00:27:52,000
And if you look at something like some of the tools that we talked about,

217
00:27:52,000 --> 00:27:56,000
you know, earlier in my illustrious panel stars,

218
00:27:56,000 --> 00:27:58,000
think about something like SageMaker.

219
00:27:58,000 --> 00:28:00,000
You know, when we looked at SageMaker,

220
00:28:00,000 --> 00:28:02,000
the AI infrastructure alliance, you know,

221
00:28:02,000 --> 00:28:06,000
90% of its use cases are structured data.

222
00:28:06,000 --> 00:28:11,000
So, you know, if you're using gigantic video and imagery and audio,

223
00:28:11,000 --> 00:28:13,000
you're mostly out of luck.

224
00:28:13,000 --> 00:28:14,000
And so that's the problem.

225
00:28:14,000 --> 00:28:17,000
If you actually have a diverse set of use cases,

226
00:28:17,000 --> 00:28:19,000
it's simply just not going to fit in there.

227
00:28:19,000 --> 00:28:20,000
So that's the real problem.

228
00:28:20,000 --> 00:28:21,000
If you look at something like Figma,

229
00:28:21,000 --> 00:28:24,000
it's one of the best tools of all time,

230
00:28:24,000 --> 00:28:25,000
like we talked about earlier.

231
00:28:25,000 --> 00:28:29,000
But the reason they bought it is because it's dramatically different

232
00:28:29,000 --> 00:28:30,000
than Photoshop.

233
00:28:30,000 --> 00:28:33,000
And Photoshop does a bunch of things that Figma simply can't do,

234
00:28:33,000 --> 00:28:34,000
it doesn't try to do.

235
00:28:34,000 --> 00:28:37,000
And so if you're trying to do Photoshop level things in Figma,

236
00:28:37,000 --> 00:28:39,000
you're going to find yourself quickly out of luck.

237
00:28:39,000 --> 00:28:40,000
So what you're going to find out,

238
00:28:40,000 --> 00:28:42,000
as you get into any sort of scale,

239
00:28:42,000 --> 00:28:45,000
is exactly what other companies have found,

240
00:28:45,000 --> 00:28:48,000
that you're going to have a platform that you fate does everything.

241
00:28:48,000 --> 00:28:50,000
And then when you start having a diverse set of use cases,

242
00:28:50,000 --> 00:28:53,000
you're going to quickly find out that that falls apart

243
00:28:53,000 --> 00:28:58,000
when faced with actual reality.

244
00:28:58,000 --> 00:29:00,000
Woo!

245
00:29:00,000 --> 00:29:01,000
All right.

246
00:29:01,000 --> 00:29:02,000
Shots fired.

247
00:29:02,000 --> 00:29:03,000
As Virginia said,

248
00:29:03,000 --> 00:29:06,000
Virginia, you get a practical MLObs book.

249
00:29:06,000 --> 00:29:09,000
And I know just the person to give a rebuttal for this.

250
00:29:09,000 --> 00:29:10,000
No, where you at, man.

251
00:29:10,000 --> 00:29:12,000
I want to hear what you got to say,

252
00:29:12,000 --> 00:29:15,000
because that was hot and heavy.

253
00:29:15,000 --> 00:29:16,000
Sure.

254
00:29:16,000 --> 00:29:18,000
I mean, I think it's really the nature of the human mind.

255
00:29:18,000 --> 00:29:21,000
The human mind gravitates towards low probability

256
00:29:21,000 --> 00:29:22,000
but exciting events.

257
00:29:22,000 --> 00:29:24,000
I'll give you two use cases.

258
00:29:24,000 --> 00:29:27,000
So if you can get it to Harvard and then drop out in your first year,

259
00:29:27,000 --> 00:29:28,000
you'll be a billionaire.

260
00:29:28,000 --> 00:29:30,000
But the probability is, you know,

261
00:29:30,000 --> 00:29:33,000
let's say, one hundred million to do that.

262
00:29:33,000 --> 00:29:38,000
Likewise, the majority of people that play high school football

263
00:29:38,000 --> 00:29:40,000
probably think they're going to go to NFL,

264
00:29:40,000 --> 00:29:44,000
but there's a one and five thousand chance you'll even make the roster.

265
00:29:44,000 --> 00:29:49,000
And so we're really thinking about things in an exciting way.

266
00:29:49,000 --> 00:29:53,000
Likewise, anybody with real world experience in the software industry

267
00:29:53,000 --> 00:29:56,000
knows that the bespoke solutions that get created,

268
00:29:56,000 --> 00:29:59,000
what it means is if you actually can create a bespoke solution

269
00:29:59,000 --> 00:30:01,000
for a corporation, most likely,

270
00:30:01,000 --> 00:30:04,000
you're going to leave in a year to get hired by some big company.

271
00:30:04,000 --> 00:30:08,000
Because that company isn't going to take your bespoke solution

272
00:30:08,000 --> 00:30:11,000
and then turn it into a product and then sell it

273
00:30:11,000 --> 00:30:14,000
because they probably do something else like rent cars out.

274
00:30:14,000 --> 00:30:16,000
So I think that's really the biggest issue is it.

275
00:30:16,000 --> 00:30:18,000
It's exciting to think about building your own solutions,

276
00:30:18,000 --> 00:30:20,000
kind of packing it all together.

277
00:30:20,000 --> 00:30:22,000
And we have the history of this, you know,

278
00:30:22,000 --> 00:30:24,000
you know, conspiracy theories,

279
00:30:24,000 --> 00:30:26,000
low probability events that are exciting,

280
00:30:26,000 --> 00:30:30,000
moving to LA to be an accurate actress.

281
00:30:30,000 --> 00:30:33,000
These are all super exciting things, but the reality is,

282
00:30:33,000 --> 00:30:35,000
in the real world,

283
00:30:35,000 --> 00:30:39,000
the boring solutions win a great example is Python.

284
00:30:39,000 --> 00:30:42,000
Python is actually, in some sense, a horrible language, right?

285
00:30:42,000 --> 00:30:43,000
It's super slow.

286
00:30:43,000 --> 00:30:45,000
There's no real concurrency solution,

287
00:30:45,000 --> 00:30:47,000
but it's the most popular language of the world

288
00:30:47,000 --> 00:30:50,000
because things regress to the name

289
00:30:50,000 --> 00:30:52,000
and the boring solutions win.

290
00:30:52,000 --> 00:30:54,000
Likewise, Jira is a great example.

291
00:30:54,000 --> 00:30:56,000
Most people hate Jira,

292
00:30:56,000 --> 00:31:05,000
but companies gravitate towards the boring solutions that work.

293
00:31:05,000 --> 00:31:07,000
Okay.

294
00:31:07,000 --> 00:31:08,000
Yes.

295
00:31:08,000 --> 00:31:09,000
Yes.

296
00:31:09,000 --> 00:31:12,000
Or I'm seeing the chat is starting to light up.

297
00:31:12,000 --> 00:31:13,000
I love this.

298
00:31:13,000 --> 00:31:18,000
So for anybody that wants to add some points that you feel

299
00:31:18,000 --> 00:31:20,000
are not being said,

300
00:31:20,000 --> 00:31:22,000
we're going to go over those at the end.

301
00:31:22,000 --> 00:31:23,000
All right.

302
00:31:23,000 --> 00:31:26,000
So throw in there some of the different talking points

303
00:31:26,000 --> 00:31:29,000
that you feel are not being talked about enough,

304
00:31:29,000 --> 00:31:32,000
and we're going to put them in the end.

305
00:31:32,000 --> 00:31:34,000
We're going to mention all of them.

306
00:31:34,000 --> 00:31:35,000
But next up,

307
00:31:35,000 --> 00:31:36,000
Cheyenne, where are you at?

308
00:31:36,000 --> 00:31:37,000
Where are you at, man?

309
00:31:37,000 --> 00:31:38,000
We're going to get again.

310
00:31:38,000 --> 00:31:40,000
You got to be in the hair.

311
00:31:40,000 --> 00:31:41,000
Oh, yeah.

312
00:31:41,000 --> 00:31:43,000
One minute on the clock.

313
00:31:43,000 --> 00:31:44,000
Let's do it.

314
00:31:44,000 --> 00:31:45,000
Okay.

315
00:31:45,000 --> 00:31:46,000
It's real quick.

316
00:31:46,000 --> 00:31:49,000
I think Python is actually a really great example

317
00:31:49,000 --> 00:31:50,000
for best and breed.

318
00:31:50,000 --> 00:31:52,000
Let me explain.

319
00:31:52,000 --> 00:31:53,000
Yeah.

320
00:31:53,000 --> 00:31:55,000
Everyone tends to use Python.

321
00:31:55,000 --> 00:31:57,000
It's not a great language.

322
00:31:57,000 --> 00:31:58,000
Absolutely.

323
00:31:58,000 --> 00:32:00,000
We regress to the mean totally.

324
00:32:00,000 --> 00:32:03,000
But I'm sure most of the data scientists here

325
00:32:03,000 --> 00:32:05,000
have like pip installed pandas,

326
00:32:05,000 --> 00:32:06,000
pip installed numpy.

327
00:32:06,000 --> 00:32:10,000
All that numerical processing is actually not written in Python.

328
00:32:10,000 --> 00:32:11,000
It's actually written in C.

329
00:32:11,000 --> 00:32:13,000
And you've got like a,

330
00:32:13,000 --> 00:32:16,000
you know, an interface between the Python and the C

331
00:32:16,000 --> 00:32:18,000
to actually do the number crunching.

332
00:32:18,000 --> 00:32:22,000
So in that case, the best tool did win.

333
00:32:22,000 --> 00:32:25,000
Python wasn't used for the things that people typically use

334
00:32:25,000 --> 00:32:26,000
Python for these days.

335
00:32:26,000 --> 00:32:28,000
It's actually C under the hood.

336
00:32:28,000 --> 00:32:31,000
And I also think that it also makes a very good argument

337
00:32:31,000 --> 00:32:33,000
for an integration layer.

338
00:32:33,000 --> 00:32:36,000
My opinion personally and that of my company and I think

339
00:32:36,000 --> 00:32:38,000
several others is that.

340
00:32:38,000 --> 00:32:42,000
What we really want to see is tighter integrations

341
00:32:42,000 --> 00:32:44,000
between lots of different best of breed tools.

342
00:32:44,000 --> 00:32:46,000
And the question is how do we achieve that?

343
00:32:46,000 --> 00:32:49,000
Well, we come up with a common interface for all of these things

344
00:32:49,000 --> 00:32:50,000
to communicate.

345
00:32:50,000 --> 00:32:53,000
Onyx did the same thing for, you know,

346
00:32:53,000 --> 00:32:55,000
PyTorch, TensorFlow, and so on,

347
00:32:55,000 --> 00:32:58,000
recognizing that some folks have just differing opinions about

348
00:32:58,000 --> 00:32:59,000
which platform they'd like to use.

349
00:32:59,000 --> 00:33:00,000
And that's okay.

350
00:33:00,000 --> 00:33:02,000
As long as these things can work together

351
00:33:02,000 --> 00:33:04,000
and have a common interface amongst them,

352
00:33:04,000 --> 00:33:06,000
then it all works just fine.

353
00:33:06,000 --> 00:33:08,000
So I think in terms of

354
00:33:08,000 --> 00:33:10,000
choose the most boring technology,

355
00:33:10,000 --> 00:33:13,000
I don't even think that exists in machine learning today.

356
00:33:13,000 --> 00:33:17,000
It's just so new and NLOps is still a kind of a new concept

357
00:33:17,000 --> 00:33:20,000
that that boring technology still doesn't exist.

358
00:33:20,000 --> 00:33:23,000
What we have are end-to-end platforms that try to do a lot

359
00:33:23,000 --> 00:33:25,000
but don't cover most of, you know,

360
00:33:25,000 --> 00:33:28,000
kind of like the more interesting and more valuable use cases

361
00:33:28,000 --> 00:33:29,000
that lots of companies have.

362
00:33:29,000 --> 00:33:31,000
And you've got best and breed tools

363
00:33:31,000 --> 00:33:34,000
that actually do need those integrations amongst themselves.

364
00:33:34,000 --> 00:33:39,000
And they, frankly, they're doing an okay job at it right now.

365
00:33:39,000 --> 00:33:41,000
But I think what we're going to see over time

366
00:33:41,000 --> 00:33:44,000
we're going to have common interfaces amongst all these tools

367
00:33:44,000 --> 00:33:49,000
emerge as we sort of build up more of a grammar around NLOps.

368
00:33:52,000 --> 00:33:53,000
I like it.

369
00:33:53,000 --> 00:33:54,000
Okay.

370
00:33:54,000 --> 00:33:56,000
And the chat right now is going off.

371
00:33:56,000 --> 00:33:57,000
I love this.

372
00:33:57,000 --> 00:34:00,000
I love seeing everything that we see at James

373
00:34:00,000 --> 00:34:02,000
just brought up an excellent point.

374
00:34:02,000 --> 00:34:04,000
But I don't want to derail you too much.

375
00:34:04,000 --> 00:34:06,000
Bindu, you're up next.

376
00:34:06,000 --> 00:34:08,000
But James, incredible point in the chat.

377
00:34:08,000 --> 00:34:10,000
What's the difference between an integration layer

378
00:34:10,000 --> 00:34:13,000
and a flexible end to end offering?

379
00:34:13,000 --> 00:34:14,000
Oh, right.

380
00:34:14,000 --> 00:34:17,000
Bindu, you don't have to answer that question.

381
00:34:17,000 --> 00:34:19,000
Happy to give us a minute.

382
00:34:19,000 --> 00:34:22,000
Give us a minute on your model.

383
00:34:22,000 --> 00:34:23,000
Here we go.

384
00:34:23,000 --> 00:34:24,000
All right.

385
00:34:24,000 --> 00:34:25,000
All right.

386
00:34:25,000 --> 00:34:26,000
Here we go.

387
00:34:26,000 --> 00:34:28,000
So actually, actually, we're mostly agreeing,

388
00:34:28,000 --> 00:34:30,000
not disagreeing from what I can tell.

389
00:34:30,000 --> 00:34:32,000
Let's start with saying that Dan basically said,

390
00:34:32,000 --> 00:34:35,000
hey, if there is an entry and platform that exists and actually works,

391
00:34:35,000 --> 00:34:36,000
that's great.

392
00:34:36,000 --> 00:34:37,000
It's all marketing out.

393
00:34:37,000 --> 00:34:38,000
There's none that exists.

394
00:34:38,000 --> 00:34:39,000
I actually disagree.

395
00:34:39,000 --> 00:34:41,000
For most common enterprise these cases,

396
00:34:41,000 --> 00:34:44,000
whether it's churn, forecasting, personalization,

397
00:34:44,000 --> 00:34:48,000
NLP, text summarization, vision segmentation,

398
00:34:48,000 --> 00:34:51,000
object identification, which, by the way,

399
00:34:51,000 --> 00:34:56,000
is almost 90 to 100% of these cases for common companies.

400
00:34:56,000 --> 00:34:58,000
These are not AI first companies.

401
00:34:58,000 --> 00:34:59,000
These are not open AI.

402
00:34:59,000 --> 00:35:00,000
This is not stability.

403
00:35:00,000 --> 00:35:02,000
This is someone like Macy's.

404
00:35:02,000 --> 00:35:03,000
This is someone like FedEx.

405
00:35:03,000 --> 00:35:05,000
This is someone like Procter and Gambo.

406
00:35:05,000 --> 00:35:09,000
The entry and demolops does exist.

407
00:35:09,000 --> 00:35:12,000
These use cases are a lot of them are structured data use cases.

408
00:35:12,000 --> 00:35:14,000
Some of them are deep learning use cases.

409
00:35:14,000 --> 00:35:17,000
For those kinds of people where it's not one data scientist,

410
00:35:17,000 --> 00:35:20,000
they have more like, you know, 20 to 100 data scientists.

411
00:35:20,000 --> 00:35:21,000
It exists today.

412
00:35:21,000 --> 00:35:22,000
It works.

413
00:35:22,000 --> 00:35:23,000
It's in production, right?

414
00:35:23,000 --> 00:35:26,000
And I think more or less everybody agrees that for those use cases,

415
00:35:26,000 --> 00:35:28,000
there are platforms that exist.

416
00:35:28,000 --> 00:35:30,000
You know, those platforms are great.

417
00:35:30,000 --> 00:35:32,000
And yes, I also agree with a child.

418
00:35:32,000 --> 00:35:34,000
There's actually innovation required there.

419
00:35:34,000 --> 00:35:37,000
And they should make it so that there is more and more innovation

420
00:35:37,000 --> 00:35:39,000
in those entry and demolopsies cases as well.

421
00:35:39,000 --> 00:35:42,000
And, you know, people, there should be more companies doing that

422
00:35:42,000 --> 00:35:43,000
and that sort of thing.

423
00:35:43,000 --> 00:35:46,000
And to the question, which is like, what's the difference between

424
00:35:46,000 --> 00:35:49,000
an emolops platform and like, you know, actually that integration there?

425
00:35:49,000 --> 00:35:50,000
I actually think there isn't.

426
00:35:50,000 --> 00:35:54,000
Because a lot of enter and emolops platforms do take in open source APIs.

427
00:35:54,000 --> 00:35:56,000
For example, we would be integrating interstability.

428
00:35:56,000 --> 00:35:58,000
So, you know, I think we agree there.

429
00:35:58,000 --> 00:36:01,000
I think in terms of like, you know, AI first companies,

430
00:36:01,000 --> 00:36:03,000
in terms of like open AI, in terms of like stability.

431
00:36:03,000 --> 00:36:06,000
Of course, you can't use an enter and demolops platform today.

432
00:36:06,000 --> 00:36:08,000
Because, you know, they have diverse needs.

433
00:36:08,000 --> 00:36:11,000
And you have to then go see what makes sense for you.

434
00:36:11,000 --> 00:36:13,000
But, you know, fundamentally, the debate though is,

435
00:36:13,000 --> 00:36:16,000
like, what happens for use cases, which are standard,

436
00:36:16,000 --> 00:36:20,000
which are prevalent, not just standard, but there is a problem.

437
00:36:20,000 --> 00:36:21,000
They're universal.

438
00:36:21,000 --> 00:36:24,000
And for companies, which are kind of what I would say,

439
00:36:24,000 --> 00:36:26,000
kind of like brick and mortar companies, right?

440
00:36:26,000 --> 00:36:29,000
Who's actually like, you know, focus is not AI,

441
00:36:29,000 --> 00:36:31,000
but AI is a big enabler.

442
00:36:31,000 --> 00:36:33,000
Think, actually, Shopify.

443
00:36:33,000 --> 00:36:34,000
Classic example.

444
00:36:34,000 --> 00:36:37,000
Shopify is used by tons and tons of e-commerce companies.

445
00:36:37,000 --> 00:36:38,000
And it's one platform.

446
00:36:38,000 --> 00:36:39,000
It does everything.

447
00:36:39,000 --> 00:36:42,000
It does all kinds of stuff from, you know, getting orders,

448
00:36:42,000 --> 00:36:44,000
you know, fulfilling those orders,

449
00:36:44,000 --> 00:36:46,000
making inventory decisions, doing fraud management.

450
00:36:46,000 --> 00:36:48,000
And it's a $50 billion company.

451
00:36:48,000 --> 00:36:50,000
Another example of an end-to-end

452
00:36:50,000 --> 00:36:53,000
a platform, which works for that particular industry.

453
00:36:53,000 --> 00:36:56,000
So, I think in some sense, in this debate,

454
00:36:56,000 --> 00:36:58,000
we're actually converging and agreeing to like,

455
00:36:58,000 --> 00:37:01,000
what makes sense for you based on who you are.

456
00:37:04,000 --> 00:37:05,000
Strong points.

457
00:37:05,000 --> 00:37:07,000
This is good.

458
00:37:07,000 --> 00:37:11,000
Dan, you're going to bring us home, man.

459
00:37:11,000 --> 00:37:12,000
It's all on you.

460
00:37:12,000 --> 00:37:13,000
Last one.

461
00:37:13,000 --> 00:37:16,000
One minute on the clock.

462
00:37:16,000 --> 00:37:20,000
So, I think, look, the simplest thing to say here is that

463
00:37:20,000 --> 00:37:22,000
it exists for simple use cases,

464
00:37:22,000 --> 00:37:25,000
but even in those kinds of use cases,

465
00:37:25,000 --> 00:37:27,000
you know, you're monitoring your management

466
00:37:27,000 --> 00:37:30,000
or bringing in someone like Arise or Y-Labs or Truera,

467
00:37:30,000 --> 00:37:32,000
or any of the kind of companies, for instance,

468
00:37:32,000 --> 00:37:34,000
and the infrastructure alliance,

469
00:37:34,000 --> 00:37:36,000
that monitoring and management is just simply going to be

470
00:37:36,000 --> 00:37:39,000
infinitely more robust than whatever is baked into, you know,

471
00:37:39,000 --> 00:37:40,000
a normal platform.

472
00:37:40,000 --> 00:37:41,000
It's just as simple as that.

473
00:37:41,000 --> 00:37:43,000
So, even for simple use cases,

474
00:37:43,000 --> 00:37:45,000
I would argue that it doesn't really exist at this point.

475
00:37:45,000 --> 00:37:48,000
I think I agree with Bindu that if it does exist,

476
00:37:48,000 --> 00:37:49,000
we want it.

477
00:37:49,000 --> 00:37:52,000
And eventually, an ecosystem does evolve to that.

478
00:37:52,000 --> 00:37:54,000
In other words, you get to the point where you have a VM

479
00:37:54,000 --> 00:37:57,000
where in the space, and then everybody just writes to the

480
00:37:57,000 --> 00:37:58,000
API of that thing.

481
00:37:58,000 --> 00:38:01,000
But we're just, we're in the diffusion of innovation curve

482
00:38:01,000 --> 00:38:03,000
right now, where we're in the early stage.

483
00:38:03,000 --> 00:38:04,000
And it doesn't exist.

484
00:38:04,000 --> 00:38:06,000
We haven't figured out all the best ways to do distributed trading

485
00:38:06,000 --> 00:38:09,000
or experimentation tracking or, you know,

486
00:38:09,000 --> 00:38:11,000
maybe we've done deployment pretty well,

487
00:38:11,000 --> 00:38:13,000
you know, versus some of the other things.

488
00:38:13,000 --> 00:38:15,000
But, you know, it's scaled, inferencing,

489
00:38:15,000 --> 00:38:16,000
all those kinds of things.

490
00:38:16,000 --> 00:38:17,000
We just haven't really nailed it.

491
00:38:17,000 --> 00:38:19,000
And there's still development that's happening.

492
00:38:19,000 --> 00:38:22,000
So, how can we possibly have a finished system

493
00:38:22,000 --> 00:38:24,000
at this point in time?

494
00:38:24,000 --> 00:38:26,000
And certainly, if you have diverse use cases,

495
00:38:26,000 --> 00:38:28,000
it's not enough to say, well, it kind of exists for the,

496
00:38:28,000 --> 00:38:30,000
for the small folks, you know, who are just doing,

497
00:38:30,000 --> 00:38:32,000
you know, one or two use cases,

498
00:38:32,000 --> 00:38:34,000
what happens when they expand into another use case?

499
00:38:34,000 --> 00:38:36,000
What happens is they buy another platform.

500
00:38:36,000 --> 00:38:39,000
And then when you certainly get into more complex use cases,

501
00:38:39,000 --> 00:38:42,000
it's simply, it just simply doesn't exist at this point in time.

502
00:38:42,000 --> 00:38:45,000
And so, again, there's a delta between a desire

503
00:38:45,000 --> 00:38:46,000
for something to exist.

504
00:38:46,000 --> 00:38:48,000
And it actually exists.

505
00:38:48,000 --> 00:38:49,000
And it will happen.

506
00:38:49,000 --> 00:38:52,000
We'll get to the lamp stack of AI.

507
00:38:52,000 --> 00:38:54,000
And then we'll move up the stack to, you know,

508
00:38:54,000 --> 00:38:55,000
higher orders of abstraction.

509
00:38:55,000 --> 00:38:57,000
And just like, you know, people moved up the stack

510
00:38:57,000 --> 00:38:58,000
and went to WordPress.

511
00:38:58,000 --> 00:39:00,000
And then moved up the stack and had Divi

512
00:39:00,000 --> 00:39:01,000
and a drag and drop editor.

513
00:39:01,000 --> 00:39:04,000
So any idiot like me can, who has very limited design skills

514
00:39:04,000 --> 00:39:05,000
can make something.

515
00:39:05,000 --> 00:39:08,000
And we're just not even at that WordPress level of abstraction yet.

516
00:39:08,000 --> 00:39:12,000
And where we've standardized on kind of the best ways to do things

517
00:39:12,000 --> 00:39:16,000
because the best ways are still developing.

518
00:39:16,000 --> 00:39:19,000
Beautiful. Beautiful.

519
00:39:19,000 --> 00:39:23,000
That is it for the debaters.

520
00:39:23,000 --> 00:39:27,000
We're going to bring them all on to the stage right now.

521
00:39:27,000 --> 00:39:31,000
And we're going to get the questions

522
00:39:31,000 --> 00:39:36,000
and all of these chat things that we've got in happening.

523
00:39:36,000 --> 00:39:41,000
But maybe before we do, well, no, no, no.

524
00:39:41,000 --> 00:39:45,000
We're going to throw the pull-up right now.

525
00:39:45,000 --> 00:39:48,000
Again, while we're talking about the different chat questions

526
00:39:48,000 --> 00:39:50,000
that are happening.

527
00:39:50,000 --> 00:39:52,000
So I saw Noah.

528
00:39:52,000 --> 00:39:56,000
Noah was being kind of funny in the chat.

529
00:39:56,000 --> 00:39:59,000
Noah, what were you mentioning there?

530
00:39:59,000 --> 00:40:00,000
Hell is hiring?

531
00:40:00,000 --> 00:40:03,000
Well, I mean, I don't know how many people have managed companies.

532
00:40:03,000 --> 00:40:05,000
I think a few of you have, you know,

533
00:40:05,000 --> 00:40:08,000
the life cycle of a company that is on its success

534
00:40:08,000 --> 00:40:10,000
and then it goes on its way down.

535
00:40:10,000 --> 00:40:12,000
But hiring for multiple languages,

536
00:40:12,000 --> 00:40:15,000
multiple technologies, it has killed organizations.

537
00:40:15,000 --> 00:40:17,000
I've worked at for sure.

538
00:40:17,000 --> 00:40:20,000
And I would say the boring solutions are the ones

539
00:40:20,000 --> 00:40:22,000
that actually you can hire for.

540
00:40:22,000 --> 00:40:26,000
And you're also betting on the technology provider

541
00:40:26,000 --> 00:40:27,000
increasing the future.

542
00:40:27,000 --> 00:40:29,000
So even if they're not there at first,

543
00:40:29,000 --> 00:40:30,000
look at Facebook.

544
00:40:30,000 --> 00:40:32,000
And we have some people that know Facebook.

545
00:40:32,000 --> 00:40:36,000
Facebook just, you know, got in, got in a good relationship

546
00:40:36,000 --> 00:40:40,000
with the Obama administration and then bought all the competitors.

547
00:40:40,000 --> 00:40:43,000
So they just, right, today just steal all the features of everybody,

548
00:40:43,000 --> 00:40:44,000
everybody else.

549
00:40:44,000 --> 00:40:46,000
So that strategy does work.

550
00:40:46,000 --> 00:40:49,000
So, so I, I, that's, that's basically what I'm saying is that

551
00:40:49,000 --> 00:40:52,000
the boring solution where the platform buys up all the competitors

552
00:40:52,000 --> 00:40:55,000
is, is a pretty well known solution.

553
00:40:55,000 --> 00:40:59,000
And that is echoing the sentiment of what Chuck was saying

554
00:40:59,000 --> 00:41:04,000
about Jira versus Salesforce and how they're all just basically

555
00:41:04,000 --> 00:41:08,000
giant collections of extensions and stealing one from another.

556
00:41:08,000 --> 00:41:13,000
There's a few other great things in the chat that I'm seeing

557
00:41:13,000 --> 00:41:14,000
right now.

558
00:41:14,000 --> 00:41:18,000
We've got Williams saying one thing that should be considered,

559
00:41:18,000 --> 00:41:22,000
often an ML team is building in the context of an existing

560
00:41:22,000 --> 00:41:24,000
engineer stack.

561
00:41:24,000 --> 00:41:27,000
In that case, it may be difficult to work with a third party

562
00:41:27,000 --> 00:41:30,000
N10 platform if it's not flexible enough.

563
00:41:30,000 --> 00:41:34,000
So there's another point like anybody want to talk about that?

564
00:41:34,000 --> 00:41:36,000
Are we encounter this all the time?

565
00:41:36,000 --> 00:41:39,000
I think that's a really good solid point.

566
00:41:39,000 --> 00:41:43,000
You know, to the point of like there being existing infrastructure,

567
00:41:43,000 --> 00:41:47,000
you, you want ideally a platform which can integrate at whatever

568
00:41:47,000 --> 00:41:49,000
like part of the Lego break plan, right?

569
00:41:49,000 --> 00:41:52,000
If you, you know, if you have something which does training

570
00:41:52,000 --> 00:41:55,000
and deployment and you want somebody to do a feature store,

571
00:41:55,000 --> 00:41:59,000
you want, ideally, once you want a platform which is easily

572
00:41:59,000 --> 00:42:01,000
extensible, but is also modular.

573
00:42:01,000 --> 00:42:04,000
And to downs point, I feel like, are we there there?

574
00:42:04,000 --> 00:42:07,000
Probably not. This is an innovative field.

575
00:42:07,000 --> 00:42:09,000
We'll get there sooner or later.

576
00:42:09,000 --> 00:42:13,000
But, you know, you also want flexibility and customization.

577
00:42:13,000 --> 00:42:16,000
So you can just switch off whatever you have.

578
00:42:16,000 --> 00:42:17,000
Hmm.

579
00:42:17,000 --> 00:42:24,000
So there's an uncle is giving something that I want to mention

580
00:42:24,000 --> 00:42:29,000
in the chat and he's talking about how ML is supposed to provide

581
00:42:29,000 --> 00:42:32,000
this differentiating factor with the business, right?

582
00:42:32,000 --> 00:42:37,000
And if we're all using the same tool and it's some end to end

583
00:42:37,000 --> 00:42:42,000
tool, doesn't that just make it so that we're all doing the same

584
00:42:42,000 --> 00:42:46,000
thing at the end, kind of like when you're everyone in the

585
00:42:46,000 --> 00:42:48,000
tour de France is doping.

586
00:42:48,000 --> 00:42:52,000
It's just how good of a dope or are you?

587
00:42:52,000 --> 00:42:56,000
I have my thoughts on this, but I don't think it is quite that

588
00:42:56,000 --> 00:42:58,000
simple of a narrative.

589
00:42:58,000 --> 00:43:01,000
Anybody want to jump in on that?

590
00:43:01,000 --> 00:43:03,000
Do you mind if I jump in again on this one?

591
00:43:03,000 --> 00:43:05,000
Yeah, go for it.

592
00:43:05,000 --> 00:43:08,000
So that's the same as saying if I use Figma,

593
00:43:08,000 --> 00:43:10,000
I don't have an advantage if I'm doing design.

594
00:43:10,000 --> 00:43:13,000
Like the ML ops platform is just a tool.

595
00:43:13,000 --> 00:43:15,000
All you're advantage is in your data.

596
00:43:15,000 --> 00:43:18,000
And so if you, I mean fundamentally the data that you have

597
00:43:18,000 --> 00:43:21,000
and extracting signal pattern from that data is what is the

598
00:43:21,000 --> 00:43:22,000
differentiator, right?

599
00:43:22,000 --> 00:43:24,000
That being said, of course, there are techniques.

600
00:43:24,000 --> 00:43:27,000
And ML ops doesn't mean that you can't use techniques.

601
00:43:27,000 --> 00:43:29,000
It doesn't mean that you can't do distributed training.

602
00:43:29,000 --> 00:43:31,000
It doesn't mean that you can't use neural architecture.

603
00:43:31,000 --> 00:43:32,000
A search of different types.

604
00:43:32,000 --> 00:43:36,000
So I think using a tool is very different from like actually

605
00:43:36,000 --> 00:43:38,000
innovating on that tool.

606
00:43:38,000 --> 00:43:41,000
And so you can't just say, hey, just because I'm using the same

607
00:43:41,000 --> 00:43:45,000
platform, me and you know, Macy's and Neiman Marcus are the same.

608
00:43:45,000 --> 00:43:47,000
I want to quickly jump in.

609
00:43:47,000 --> 00:43:52,000
I agree with the sentiment that just because like two people

610
00:43:52,000 --> 00:43:55,000
use a hammer doesn't mean that their work is going to be

611
00:43:55,000 --> 00:43:56,000
exactly the same.

612
00:43:56,000 --> 00:43:59,000
However, I think I disagree with the sentiment that all

613
00:43:59,000 --> 00:44:01,000
of the values and the data.

614
00:44:01,000 --> 00:44:04,000
And this is coming from someone who's company like focuses on

615
00:44:04,000 --> 00:44:05,000
the data side of this.

616
00:44:05,000 --> 00:44:08,000
The data is like naturally very, very important.

617
00:44:08,000 --> 00:44:11,000
But also is the treatment of that data.

618
00:44:11,000 --> 00:44:12,000
And the way you're.

619
00:44:12,000 --> 00:44:13,000
No, I agree.

620
00:44:13,000 --> 00:44:14,000
Sorry.

621
00:44:14,000 --> 00:44:15,000
I mean, the techniques are important to absolutely.

622
00:44:15,000 --> 00:44:16,000
Yeah.

623
00:44:16,000 --> 00:44:18,000
So I guess the point I'm trying to make is that if you're

624
00:44:18,000 --> 00:44:21,000
bounded by like a solution, if you're bounded by a platform

625
00:44:21,000 --> 00:44:24,000
that has a ceiling to the types of techniques that you can

626
00:44:24,000 --> 00:44:26,000
apply to your data.

627
00:44:26,000 --> 00:44:27,000
And that's like fundamentally problematic.

628
00:44:27,000 --> 00:44:28,000
That's terrible.

629
00:44:28,000 --> 00:44:29,000
Yeah.

630
00:44:29,000 --> 00:44:32,000
So.

631
00:44:32,000 --> 00:44:36,000
Cost is asking what are the boring off the shelf tools that

632
00:44:36,000 --> 00:44:38,000
people like and are using.

633
00:44:38,000 --> 00:44:42,000
Maybe there's a top three or top five list.

634
00:44:42,000 --> 00:44:47,000
And it may be doesn't have to be in just ml who's got an answer

635
00:44:47,000 --> 00:44:48,000
or Noah.

636
00:44:48,000 --> 00:44:51,000
Well, I would just say the, I mean, it's just look at the data,

637
00:44:51,000 --> 00:44:52,000
right?

638
00:44:52,000 --> 00:44:56,000
The top vendors of cloud computing and whatever your company is

639
00:44:56,000 --> 00:44:58,000
currently using, just pick what they have.

640
00:44:58,000 --> 00:45:01,000
If it's AWS and you're already on AWS, pick SageMaker.

641
00:45:01,000 --> 00:45:04,000
If you're on Azure, pick Databricks or pick Azure ML Studio.

642
00:45:04,000 --> 00:45:06,000
If you're on Google, pick Vertex AI.

643
00:45:06,000 --> 00:45:07,000
I mean,

644
00:45:07,000 --> 00:45:11,000
that may not be the exciting answer that people want to know.

645
00:45:11,000 --> 00:45:14,000
But that's going to be a pretty safe strategy that.

646
00:45:14,000 --> 00:45:17,000
Yeah, it's funny that you mentioned that because I just

647
00:45:17,000 --> 00:45:20,000
talked to the SageMaker team about stability where we have

648
00:45:20,000 --> 00:45:24,000
a 4,000 node a 100 cluster.

649
00:45:24,000 --> 00:45:28,000
And unfortunately, SageMaker won't work for what we're doing.

650
00:45:28,000 --> 00:45:31,000
In other words, their thing is primarily structured data.

651
00:45:31,000 --> 00:45:34,000
And it can't handle any of the image audio and video that

652
00:45:34,000 --> 00:45:35,000
we're handling.

653
00:45:35,000 --> 00:45:38,000
And in fact, we're even talking to the team on the back end

654
00:45:38,000 --> 00:45:41,000
who is quite wonderful that has a whole series of unreleased

655
00:45:41,000 --> 00:45:44,000
scripts and monitoring and management systems that they use

656
00:45:44,000 --> 00:45:46,000
for supercomputing clusters.

657
00:45:46,000 --> 00:45:49,000
Where they basically have kind of automations for when

658
00:45:49,000 --> 00:45:52,000
the chips start to die down or need to be rebooted in those

659
00:45:52,000 --> 00:45:53,000
kinds of things.

660
00:45:53,000 --> 00:45:55,000
And then you look across the pond.

661
00:45:55,000 --> 00:45:58,000
And there are other groups that have focused entirely on things

662
00:45:58,000 --> 00:45:59,000
like that, like CoreWeaver.

663
00:45:59,000 --> 00:46:01,000
We have a lot of our inferencing.

664
00:46:01,000 --> 00:46:04,000
And they have all these kind of scripts sort of built in

665
00:46:04,000 --> 00:46:07,000
because that's all they do is sort of GPU sort of super computers.

666
00:46:07,000 --> 00:46:08,000
Right?

667
00:46:08,000 --> 00:46:09,000
So again, it sort of.

668
00:46:09,000 --> 00:46:11,000
It boils down to your right.

669
00:46:11,000 --> 00:46:14,000
Like Vindu has said this earlier, like if you have a sort of.

670
00:46:14,000 --> 00:46:15,000
I don't think there's,

671
00:46:15,000 --> 00:46:16,000
it's not a matter of the,

672
00:46:16,000 --> 00:46:19,000
like I disagree with the boring set of tools.

673
00:46:19,000 --> 00:46:21,000
In other words, I'll take the boring so the tool any day.

674
00:46:21,000 --> 00:46:22,000
Back in the day,

675
00:46:22,000 --> 00:46:23,000
I take Vm wherever anything.

676
00:46:23,000 --> 00:46:25,000
I'm not going to go spin up my own,

677
00:46:25,000 --> 00:46:26,000
you know,

678
00:46:26,000 --> 00:46:28,000
visualization cluster just because Zen is cooler.

679
00:46:28,000 --> 00:46:29,000
Right?

680
00:46:29,000 --> 00:46:30,000
But in this case,

681
00:46:30,000 --> 00:46:32,000
I'm saying it doesn't exist for a lot of things.

682
00:46:32,000 --> 00:46:33,000
It exists.

683
00:46:33,000 --> 00:46:34,000
The boring use case exists.

684
00:46:34,000 --> 00:46:35,000
Absolutely.

685
00:46:35,000 --> 00:46:36,000
And, you know,

686
00:46:36,000 --> 00:46:37,000
if you're got,

687
00:46:37,000 --> 00:46:38,000
if you've got,

688
00:46:38,000 --> 00:46:39,000
you know, again,

689
00:46:39,000 --> 00:46:41,000
churn prediction and these kinds of sort of like,

690
00:46:41,000 --> 00:46:42,000
you know,

691
00:46:42,000 --> 00:46:43,000
analysis that everybody's doing,

692
00:46:43,000 --> 00:46:44,000
I think,

693
00:46:44,000 --> 00:46:45,000
you're going to go to date.

694
00:46:45,000 --> 00:46:46,000
You're going to do these kinds of things.

695
00:46:46,000 --> 00:46:47,000
And yeah,

696
00:46:47,000 --> 00:46:48,000
it's going to work absolutely fine.

697
00:46:48,000 --> 00:46:49,000
But I think as soon as you get into like,

698
00:46:49,000 --> 00:46:50,000
you know,

699
00:46:50,000 --> 00:46:51,000
cutting edge of an hour and those kinds of things are,

700
00:46:51,000 --> 00:46:52,000
you're just kind of,

701
00:46:52,000 --> 00:46:53,000
you're just out of luck.

702
00:46:53,000 --> 00:46:54,000
Well, I agree with it.

703
00:46:54,000 --> 00:46:55,000
So we're actually in agreement,

704
00:46:55,000 --> 00:46:57,000
but you're talking about the 1% use case.

705
00:46:57,000 --> 00:46:59,000
And I'm talking about the 99% use case.

706
00:46:59,000 --> 00:47:00,000
It would be like saying,

707
00:47:00,000 --> 00:47:01,000
hey,

708
00:47:01,000 --> 00:47:02,000
look,

709
00:47:02,000 --> 00:47:04,000
if you want to make $10 million and fly.

710
00:47:04,000 --> 00:47:05,000
Private,

711
00:47:05,000 --> 00:47:06,000
private planes each year,

712
00:47:06,000 --> 00:47:08,000
and actually be an NFL quarterback,

713
00:47:08,000 --> 00:47:11,000
there's only one thing you can do is try out for the NFL.

714
00:47:11,000 --> 00:47:12,000
It's like,

715
00:47:12,000 --> 00:47:13,000
you're right.

716
00:47:13,000 --> 00:47:15,000
But the 5,000 other high school athletes

717
00:47:15,000 --> 00:47:16,000
should probably study math,

718
00:47:16,000 --> 00:47:17,000
or they should probably study data science.

719
00:47:17,000 --> 00:47:18,000
So similarly,

720
00:47:18,000 --> 00:47:19,000
the 99% of companies

721
00:47:19,000 --> 00:47:20,000
should use the boring technology.

722
00:47:20,000 --> 00:47:21,000
If it is the case,

723
00:47:21,000 --> 00:47:23,000
if you're the one in the 100 companies

724
00:47:23,000 --> 00:47:25,000
that actually is trying to compete with Google

725
00:47:25,000 --> 00:47:26,000
or hugging face with

726
00:47:26,000 --> 00:47:27,000
taking,

727
00:47:27,000 --> 00:47:28,000
making pre-trained models.

728
00:47:28,000 --> 00:47:29,000
And yeah,

729
00:47:29,000 --> 00:47:30,000
don't,

730
00:47:30,000 --> 00:47:31,000
don't use the platform.

731
00:47:31,000 --> 00:47:32,000
But that's,

732
00:47:32,000 --> 00:47:34,000
but that's one percent of the people.

733
00:47:34,000 --> 00:47:36,000
But I don't know if it's one percent.

734
00:47:36,000 --> 00:47:37,000
I think that's something.

735
00:47:37,000 --> 00:47:38,000
But maybe.

736
00:47:38,000 --> 00:47:39,000
Yeah.

737
00:47:39,000 --> 00:47:43,000
But I don't know if it's one percent or I think that's okay.

738
00:47:43,000 --> 00:47:44,000
But maybe.

739
00:47:44,000 --> 00:47:45,000
Yeah.

740
00:47:45,000 --> 00:47:46,000
I don't know if it's one percent.

741
00:47:46,000 --> 00:47:47,000
But I also like,

742
00:47:47,000 --> 00:47:48,000
let me actually,

743
00:47:48,000 --> 00:47:49,000
you know,

744
00:47:49,000 --> 00:47:50,000
counternova.

745
00:47:50,000 --> 00:47:51,000
A little bit here.

746
00:47:51,000 --> 00:47:52,000
The point is.

747
00:47:52,000 --> 00:47:53,000
Friendly fire.

748
00:47:53,000 --> 00:47:54,000
It's all the same team.

749
00:47:54,000 --> 00:47:55,000
You can't counternova.

750
00:47:55,000 --> 00:47:56,000
Well,

751
00:47:56,000 --> 00:47:57,000
we still agree.

752
00:47:57,000 --> 00:47:58,000
It's gone after all.

753
00:47:58,000 --> 00:47:59,000
Oh,

754
00:47:59,000 --> 00:48:00,000
that's not.

755
00:48:00,000 --> 00:48:01,000
So basically,

756
00:48:01,000 --> 00:48:02,000
the big tech.

757
00:48:02,000 --> 00:48:03,000
I mean,

758
00:48:03,000 --> 00:48:04,000
I see kind of big tech as a Walmart,

759
00:48:04,000 --> 00:48:05,000
right?

760
00:48:05,000 --> 00:48:06,000
And I agree with this thing

761
00:48:06,000 --> 00:48:08,000
that they're actually boring.

762
00:48:08,000 --> 00:48:11,000
And they'll actually even say to make her to be honest or vertex

763
00:48:11,000 --> 00:48:12,000
aren't there there yet.

764
00:48:12,000 --> 00:48:14,000
An example of this is some data warehouses.

765
00:48:14,000 --> 00:48:16,000
The best data warehouse.

766
00:48:16,000 --> 00:48:17,000
There is a snowflake.

767
00:48:17,000 --> 00:48:19,000
And it's something somebody else.

768
00:48:19,000 --> 00:48:22,000
So I think just because it's part of Google,

769
00:48:22,000 --> 00:48:23,000
because of Amazon,

770
00:48:23,000 --> 00:48:24,000
you shouldn't adopt it.

771
00:48:24,000 --> 00:48:26,000
I am for entering an ML platform,

772
00:48:26,000 --> 00:48:27,000
ML ops platforms.

773
00:48:27,000 --> 00:48:28,000
I will.

774
00:48:28,000 --> 00:48:30,000
It makes sense for it to be the best of breed,

775
00:48:30,000 --> 00:48:31,000
enter and ML ops platform.

776
00:48:31,000 --> 00:48:32,000
I don't know which one it is yet.

777
00:48:32,000 --> 00:48:34,000
I think the jury is out there, right?

778
00:48:34,000 --> 00:48:37,000
But I think just adopting the Google tool or the Amazon tool

779
00:48:37,000 --> 00:48:40,000
doesn't make sense because Amazon and Google are just

780
00:48:40,000 --> 00:48:41,000
praying and praying right now.

781
00:48:41,000 --> 00:48:42,000
I mean,

782
00:48:42,000 --> 00:48:44,000
it's every everywhere in every area.

783
00:48:44,000 --> 00:48:46,000
There is some tool that they are developing.

784
00:48:46,000 --> 00:48:48,000
And they have like 10 people on that tool.

785
00:48:48,000 --> 00:48:49,000
And the tool is not great, right?

786
00:48:49,000 --> 00:48:51,000
So I think that shouldn't be the reason to use it,

787
00:48:51,000 --> 00:48:54,000
just because big tech is using it or because it's

788
00:48:54,000 --> 00:48:55,000
foreign technology.

789
00:48:55,000 --> 00:48:57,000
I think we should figure out what is.

790
00:48:57,000 --> 00:48:58,000
If a doesn't exist,

791
00:48:58,000 --> 00:49:02,000
b is it really good for my task for my problems and my models?

792
00:49:02,000 --> 00:49:03,000
And then see,

793
00:49:03,000 --> 00:49:05,000
okay, then let's go to it.

794
00:49:05,000 --> 00:49:06,000
Yeah.

795
00:49:06,000 --> 00:49:07,000
So really quickly,

796
00:49:07,000 --> 00:49:11,000
I think that there's a lot of talk about the distribution

797
00:49:11,000 --> 00:49:14,000
of types of problems in the AI space.

798
00:49:14,000 --> 00:49:15,000
I think like everyone here,

799
00:49:15,000 --> 00:49:17,000
both panelists and attendees,

800
00:49:17,000 --> 00:49:20,000
probably want more proliferation and more penetration of AI

801
00:49:20,000 --> 00:49:23,000
into interesting business use cases.

802
00:49:23,000 --> 00:49:24,000
Even if you're a Macy,

803
00:49:24,000 --> 00:49:26,000
even if you're a Macy's,

804
00:49:26,000 --> 00:49:27,000
even if you're a GM,

805
00:49:27,000 --> 00:49:28,000
even if you're, you know,

806
00:49:28,000 --> 00:49:31,000
whatever insert big incumbent company here,

807
00:49:31,000 --> 00:49:34,000
there exists some set of problems within your organization

808
00:49:34,000 --> 00:49:36,000
that are extremely high value,

809
00:49:36,000 --> 00:49:39,000
that you simply can't build AI for very easily

810
00:49:39,000 --> 00:49:41,000
because you either lack the tooling

811
00:49:41,000 --> 00:49:44,000
or you lack the data or some combination of the two.

812
00:49:44,000 --> 00:49:46,000
Now,

813
00:49:46,000 --> 00:49:49,000
I think everyone here wants those use cases to be solved for,

814
00:49:49,000 --> 00:49:50,000
using AI.

815
00:49:50,000 --> 00:49:51,000
I think that's sort of a given.

816
00:49:51,000 --> 00:49:55,000
So I think really what we should be arguing for is just

817
00:49:55,000 --> 00:49:59,000
deep innovation where we bring kind of like the floor up.

818
00:49:59,000 --> 00:50:01,000
And I think that's really what we're talking about here,

819
00:50:01,000 --> 00:50:03,000
especially like from the camp of Bess of Breed,

820
00:50:03,000 --> 00:50:06,000
because that's sort of like the most linear path towards that.

821
00:50:06,000 --> 00:50:07,000
You know,

822
00:50:07,000 --> 00:50:10,000
if we can focus on these ultra high value use cases,

823
00:50:10,000 --> 00:50:12,000
then it's possible for these companies to basically be priced in

824
00:50:12,000 --> 00:50:16,000
to solving really their hardest problems using AI.

825
00:50:19,000 --> 00:50:20,000
Okay.

826
00:50:20,000 --> 00:50:23,000
So before Dan jumps in

827
00:50:23,000 --> 00:50:25,000
because I know he wants to say something,

828
00:50:25,000 --> 00:50:27,000
everybody just to let you know

829
00:50:27,000 --> 00:50:30,000
we've got the second round of the polls.

830
00:50:30,000 --> 00:50:33,000
So I want to see if there has been any change

831
00:50:33,000 --> 00:50:35,000
in the way that you would vote.

832
00:50:35,000 --> 00:50:38,000
Go answer the polls again.

833
00:50:38,000 --> 00:50:40,000
It's the same question.

834
00:50:40,000 --> 00:50:43,000
But let's have you changed your mind.

835
00:50:43,000 --> 00:50:46,000
Did you get swayed one way or another?

836
00:50:46,000 --> 00:50:48,000
And remember,

837
00:50:48,000 --> 00:50:49,000
I told you,

838
00:50:49,000 --> 00:50:51,000
come into this with an open mind.

839
00:50:51,000 --> 00:50:53,000
Be willing to change your mind.

840
00:50:53,000 --> 00:50:54,000
All right,

841
00:50:54,000 --> 00:50:57,000
because it'd be no fun if everybody just held to their morals

842
00:50:57,000 --> 00:51:02,000
and they said, ah, I don't believe it just because they are

843
00:51:02,000 --> 00:51:05,000
aggressively holding on to it like a belief.

844
00:51:05,000 --> 00:51:06,000
Dan,

845
00:51:06,000 --> 00:51:08,000
what was this score for the best study?

846
00:51:08,000 --> 00:51:09,000
I had a question.

847
00:51:09,000 --> 00:51:12,000
What was the score for the initial poll?

848
00:51:12,000 --> 00:51:13,000
The initial one.

849
00:51:13,000 --> 00:51:18,000
It was 72% for end to end.

850
00:51:18,000 --> 00:51:22,000
And then I think 28% was the way around.

851
00:51:22,000 --> 00:51:23,000
Yeah.

852
00:51:23,000 --> 00:51:24,000
Yeah.

853
00:51:24,000 --> 00:51:25,000
All right.

854
00:51:25,000 --> 00:51:28,000
I was close enough.

855
00:51:28,000 --> 00:51:29,000
I was close enough.

856
00:51:29,000 --> 00:51:31,000
Just like,

857
00:51:31,000 --> 00:51:32,000
thank you.

858
00:51:32,000 --> 00:51:33,000
Thanks.

859
00:51:33,000 --> 00:51:35,000
Just kind of messing it up for everybody.

860
00:51:35,000 --> 00:51:36,000
Look,

861
00:51:36,000 --> 00:51:39,000
I'd like to thank all my worthy, you know, competitors.

862
00:51:39,000 --> 00:51:40,000
When I really competitors,

863
00:51:40,000 --> 00:51:41,000
as Noah said on any given day,

864
00:51:41,000 --> 00:51:42,000
depending on our paycheck,

865
00:51:42,000 --> 00:51:44,000
we could certainly switch to the other side of the debate,

866
00:51:44,000 --> 00:51:45,000
like,

867
00:51:45,000 --> 00:51:46,000
like,

868
00:51:46,000 --> 00:51:47,000
good softness from ancient Greece.

869
00:51:47,000 --> 00:51:48,000
I will say though that,

870
00:51:48,000 --> 00:51:49,000
you know,

871
00:51:49,000 --> 00:51:51,000
best of breed is something that I actually do sort of buy

872
00:51:51,000 --> 00:51:53,000
into and believe in because, you know,

873
00:51:53,000 --> 00:51:54,000
my study with.

874
00:51:54,000 --> 00:51:56,000
I think that's what it does come back to the diffusion of

875
00:51:56,000 --> 00:51:58,000
innovation curve and where we're at in, in the thing.

876
00:51:58,000 --> 00:52:01,000
And eventually you do get to like two or three tools that

877
00:52:01,000 --> 00:52:02,000
just everybody uses a big sense.

878
00:52:02,000 --> 00:52:03,000
The lamp stack is there.

879
00:52:03,000 --> 00:52:04,000
Why are you going to be in the wheel?

880
00:52:04,000 --> 00:52:07,000
But I think actually the perfect example of that to do is,

881
00:52:07,000 --> 00:52:08,000
is snowflake.

882
00:52:08,000 --> 00:52:11,000
And that snowflake is like a very late stage product,

883
00:52:11,000 --> 00:52:12,000
where,

884
00:52:12,000 --> 00:52:14,000
where there were already two every generations that exist.

885
00:52:14,000 --> 00:52:16,000
And the same thing that Kubernetes is like a third stage product.

886
00:52:16,000 --> 00:52:17,000
Yeah.

887
00:52:17,000 --> 00:52:19,000
Versus like two internal systems at Google belt.

888
00:52:19,000 --> 00:52:20,000
So,

889
00:52:20,000 --> 00:52:20,000
and in fact,

890
00:52:20,000 --> 00:52:23,000
I think somebody tweeted the other day that snowflake had

891
00:52:23,000 --> 00:52:24,000
one of the funniest,

892
00:52:24,000 --> 00:52:27,000
like value props in the early days that they said,

893
00:52:27,000 --> 00:52:28,000
they just came in and they're like,

894
00:52:28,000 --> 00:52:29,000
it's,

895
00:52:29,000 --> 00:52:30,000
it's like redshift,

896
00:52:30,000 --> 00:52:32,000
but it works and you don't have to think about it.

897
00:52:32,000 --> 00:52:33,000
And,

898
00:52:33,000 --> 00:52:34,000
and really that was,

899
00:52:34,000 --> 00:52:35,000
that was actually their pitch.

900
00:52:35,000 --> 00:52:36,000
And people were like,

901
00:52:36,000 --> 00:52:39,000
you don't have fancy slides and like a bunch of demos.

902
00:52:39,000 --> 00:52:40,000
And they're like,

903
00:52:40,000 --> 00:52:41,000
no, no, no, no,

904
00:52:41,000 --> 00:52:44,000
we just make it so you never have to think about running out of space

905
00:52:44,000 --> 00:52:45,000
or replication or whatever.

906
00:52:45,000 --> 00:52:47,000
And it took redshift existing.

907
00:52:47,000 --> 00:52:48,000
For snowflake to exist, right?

908
00:52:48,000 --> 00:52:49,000
And in,

909
00:52:49,000 --> 00:52:50,000
by the way,

910
00:52:50,000 --> 00:52:53,000
redshift is still a pretty cool product and continues to improve.

911
00:52:53,000 --> 00:52:55,000
But, you know, snowflake is on an order of magnitude.

912
00:52:55,000 --> 00:52:56,000
It's just awesomeness,

913
00:52:56,000 --> 00:52:58,000
but it's better and it's because it's later in the,

914
00:52:58,000 --> 00:53:00,000
the development stage.

915
00:53:00,000 --> 00:53:01,000
Yeah,

916
00:53:01,000 --> 00:53:04,000
you get to learn from the mistakes people made, right?

917
00:53:04,000 --> 00:53:05,000
Like,

918
00:53:05,000 --> 00:53:07,000
if you look at the data robots and stage makers of the world,

919
00:53:07,000 --> 00:53:08,000
the next generation,

920
00:53:08,000 --> 00:53:09,000
we believe strongly.

921
00:53:09,000 --> 00:53:10,000
I mean,

922
00:53:10,000 --> 00:53:11,000
you're making my case personally,

923
00:53:11,000 --> 00:53:12,000
selfishly for me,

924
00:53:12,000 --> 00:53:14,000
but yes, absolutely agree.

925
00:53:14,000 --> 00:53:15,000
So,

926
00:53:15,000 --> 00:53:19,000
a Troy just said something incredible in the comments.

927
00:53:19,000 --> 00:53:22,000
And I have to relay this to everyone

928
00:53:22,000 --> 00:53:24,000
in case they're not looking at it.

929
00:53:24,000 --> 00:53:26,000
The real debate here is

930
00:53:26,000 --> 00:53:28,000
not quite there yet,

931
00:53:28,000 --> 00:53:30,000
end-to-end platform,

932
00:53:30,000 --> 00:53:32,000
versus box of parts

933
00:53:32,000 --> 00:53:35,000
that don't work that well together.

934
00:53:35,000 --> 00:53:36,000
That's,

935
00:53:36,000 --> 00:53:38,000
that's 100%.

936
00:53:38,000 --> 00:53:39,000
That's real.

937
00:53:39,000 --> 00:53:41,000
That's reality right there.

938
00:53:41,000 --> 00:53:42,000
That is,

939
00:53:42,000 --> 00:53:43,000
I agree.

940
00:53:43,000 --> 00:53:44,000
I agree.

941
00:53:44,000 --> 00:53:45,000
And that's,

942
00:53:45,000 --> 00:53:46,000
but that's the whole point

943
00:53:46,000 --> 00:53:49,000
from a microeconomics standpoint, right?

944
00:53:49,000 --> 00:53:51,000
Is, is, who is going to actually

945
00:53:51,000 --> 00:53:53,000
be the person that's going to be successful?

946
00:53:53,000 --> 00:53:56,000
It's probably the company with the most money.

947
00:53:56,000 --> 00:53:57,000
Ooh.

948
00:53:57,000 --> 00:53:58,000
That's,

949
00:53:58,000 --> 00:53:59,000
that's so interesting.

950
00:53:59,000 --> 00:54:01,000
It is interesting that it's going to end

951
00:54:01,000 --> 00:54:02,000
us on that.

952
00:54:02,000 --> 00:54:03,000
Man, I can't,

953
00:54:03,000 --> 00:54:05,000
we can't end on that.

954
00:54:05,000 --> 00:54:06,000
Actually, what

955
00:54:06,000 --> 00:54:08,000
about the best meritocracy in the best possible?

956
00:54:08,000 --> 00:54:09,000
No, I think,

957
00:54:09,000 --> 00:54:11,000
what about the most positive innovation?

958
00:54:11,000 --> 00:54:12,000
Let's go with what Chai has said.

959
00:54:12,000 --> 00:54:13,000
Innovation.

960
00:54:13,000 --> 00:54:14,000
That's right.

961
00:54:14,000 --> 00:54:15,000
Yeah.

962
00:54:15,000 --> 00:54:16,000
We can dream.

963
00:54:16,000 --> 00:54:17,000
We can dream.

964
00:54:17,000 --> 00:54:18,000
We can dream.

965
00:54:18,000 --> 00:54:20,000
So let's end on the poll.

966
00:54:20,000 --> 00:54:21,000
Let's see.

967
00:54:21,000 --> 00:54:22,000
The results are in.

968
00:54:22,000 --> 00:54:24,000
I think we can throw them up on the screen.

969
00:54:24,000 --> 00:54:26,000
And what do we got here?

970
00:54:26,000 --> 00:54:29,000
What do we have as far as the poll results?

971
00:54:29,000 --> 00:54:31,000
Is it?

972
00:54:31,000 --> 00:54:32,000
Oh.

973
00:54:32,000 --> 00:54:33,000
Oh.

974
00:54:33,000 --> 00:54:37,000
So end to end was swayed.

975
00:54:37,000 --> 00:54:40,000
Now more people are going for end to end.

976
00:54:40,000 --> 00:54:43,000
And I don't know if that's because more people just showed up.

977
00:54:43,000 --> 00:54:46,000
And they voted for end to end or if they actually were swayed.

978
00:54:46,000 --> 00:54:49,000
They do set out a male to all of her followers.

979
00:54:49,000 --> 00:54:50,000
Yeah.

980
00:54:50,000 --> 00:54:52,000
Back in the deck.

981
00:54:52,000 --> 00:54:53,000
Yeah.

982
00:54:53,000 --> 00:54:58,000
Maybe there was some shenanigans going on on the back end here.

983
00:54:58,000 --> 00:54:59,000
I didn't cheat.

984
00:54:59,000 --> 00:55:00,000
I promise.

985
00:55:00,000 --> 00:55:04,000
No, I told all his students if they want extra credit,

986
00:55:04,000 --> 00:55:07,000
get on here and vote for end to end.

987
00:55:07,000 --> 00:55:09,000
Well, this has been awesome, y'all.

988
00:55:09,000 --> 00:55:10,000
This has been super cool.

989
00:55:10,000 --> 00:55:13,000
Hopefully everyone enjoyed it.

990
00:55:13,000 --> 00:55:15,000
I gave away a few different books.

991
00:55:15,000 --> 00:55:17,000
I think I gave away three of them.

992
00:55:17,000 --> 00:55:21,000
But in case I didn't, I'll give away a fourth to, I mean,

993
00:55:21,000 --> 00:55:28,000
I see Chuck here gave an awesome point up above.

994
00:55:28,000 --> 00:55:33,000
So Chuck, where you at, Chuck, you get a book and also William.

995
00:55:33,000 --> 00:55:36,000
You get some points when that was awesome.

996
00:55:36,000 --> 00:55:39,000
Last person, David Palmer, you get some points.

997
00:55:39,000 --> 00:55:42,000
I'll go on away from this with a good time.

998
00:55:42,000 --> 00:55:44,000
Hopefully that was excellent.

999
00:55:44,000 --> 00:55:49,000
It was as good as, no, it was as good for you as it was for me.

1000
00:55:49,000 --> 00:55:52,000
And we're going to finish on that note.

1001
00:55:52,000 --> 00:55:57,000
I think we've got, we've got Thomas from weights and biases coming up.

1002
00:55:57,000 --> 00:56:00,000
Sam, you're going to, you're going to bring in Thomas, right?

1003
00:56:00,000 --> 00:56:01,000
Yeah.

1004
00:56:01,000 --> 00:56:02,000
Yep.

1005
00:56:02,000 --> 00:56:03,000
All right.

1006
00:56:03,000 --> 00:56:04,000
There we go.

1007
00:56:04,000 --> 00:56:05,000
Awesome.

1008
00:56:05,000 --> 00:56:06,000
For you, Sam.

1009
00:56:06,000 --> 00:56:07,000
That was amazing.

1010
00:56:07,000 --> 00:56:11,000
Paul here, also in the chat, which was very lively.

1011
00:56:11,000 --> 00:56:18,000
So big thanks to our audience for engaging and keeping it lively.

1012
00:56:18,000 --> 00:56:27,000
And thank you, Demetrios and all of our debaters this morning for a very lively conversation.

1013
00:56:27,000 --> 00:56:28,000
Thanks for having us.

1014
00:56:28,000 --> 00:56:29,000
This is a lot of fun.

1015
00:56:29,000 --> 00:56:39,000
See you all later.

