Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week's shows are drawn from some of the great conversations I had at the recent Nvidia
GPU technology conference and they're brought to you by Dell.
If you caught my tweets from GTC, you may already know that one of the announcements this
year was a new reference architecture for data science work sessions powered by high
NGPUs and accelerated software such as Nvidia's Rabbids.
Dell was among the key partners showcased during the launch and offers a line of workstations
designed for modern machine learning and AI workloads.
To learn more about Dell precision workstations and some of the ways they're being used by customers
in industries like media and entertainment, engineering and manufacturing, healthcare
and life sciences, oil and gas and financial services, visit Dell EMC.com slash precision.
Alright everyone, I am on the line with Trista Chen.
Trista is the chief scientist of machine learning at Inventek.
Trista, welcome to this week a machine learning and AI.
Thank you Sam, thank you for inviting me.
Absolutely, absolutely.
I'm looking forward to our conversation.
Before we jump into the heart of our talk, which would be focused on a presentation you
did recently at the Nvidia GTC conference on edge AI and smart manufacturing, I'd love
to learn a bit about your background.
How did you get started working in machine learning?
I'm always a visual person to start with as a child.
So naturally I study computer vision and after I finished my PhD at Carnegie Mande University,
I joined Nvidia to be the first video architect of their video processor.
So video processor is their new line of processor in addition to the graphic processor, so that
they can handle all the video encoding, decoding and preprocessing post-processing more smoothly.
After Nvidia, I joined Intel to be their research scientist, I lead a team of researchers
in Russia to do open CV development, as well as working on medical and multi-core processor
research.
So open CV to some of you who know the open source library is probably the biggest open source
library on computer vision.
You have 14 million dollars so far and I'm very happy to be proud of effort to contribute
to the community to help everybody to have a good head start in all the computer vision
applications.
And after Intel, I think most people at Silicon Valley had this dream of studying their
own company.
So I started my own company doing computer vision for other companies.
So basically a B2B effort.
So I help company to annotate their image or video assets so that they can match related
advertisement to their image or video.
So example to that would be if you watch YouTube video array.
So a lot of them are consumer generated and you want to monetize them.
So our company will help them to look at what's inside a video and then what would be
the appropriate related content.
For example, if you are posted a video of writing a bicycle in the wild, then maybe we will
match our AI advertisement for you and same thing with image.
So if you write a blog about your dinner with friends and then you write a blog about
going to somewhere for fun, then we can match that with travel advertisement and whatnot.
And so we got accepted by a wide combinator in the last round.
But eventually I didn't join the wide combinator effort.
And then so because I was a first time entrepreneur, it wasn't successful.
And after that, I get a taste of stuff and I decided to join another stuff which was
called cognitive networks.
So what they do is they do content recognition on TV.
So they have this small device inside the TV that they partner with so that they can
understand exactly what TV content you are watching.
For example, you are watching Big Ben Theory episode A and two minutes of two and 30 seconds
and whatnot.
So by knowing what you are watching at the moment, they can do a lot of analytics, the video,
they can know the advertisement or the campaign effectiveness and whatnot.
So eventually the company was successful and got acquired by Vizio, one of the top TV
manufacturer in the world, and so after a happy acquisition, I kind of semi-retire, per
say.
Yeah, after like 15 years of second value, like how working and craziness.
So I actually moved across Pacific Ocean to Taiwan.
So actually my career has been interesting in that it's been different countries.
It's been different areas as a firm research to industry research from big companies and
eventually start up and now I'm moving back to another big company, which is Inventake
in Taiwan.
So for a lot of you in the US, you might not be aware of the company Inventake.
It's actually OEM or some of you might heard of FACSCOM, right?
So FACSCOM make manufacturer all the Apple devices for Apple.
So Inventake is a similar line of company.
So they manufacture cloud servers, laptops and smart devices for all the famous computer
they ever heard of, even though you never heard of Inventake itself because it's OEM.
As a chief scientist in the company, I was actually I'm helping the company to optimize
their industrial 4.0 effort in the factory as well as looking at future direction in products
so that we can prepare ourselves to be ready to manufacture a new line of AI products for
our customers.
So the presentation that you did at GTC around the application of AI at the edge, was that
for Inventake's own manufacturing facilities or for customers or partners?
Yeah, the answer is yes and no.
So to start the research, we actually focus mostly internally.
So we go to the factory and talk to the people in the production line to understand what
their needs and what their pain point is.
And from there we develop the solutions.
So what you see in our GTC presentation are the real application that we develop for
the factories.
And from there it becomes a solution.
So it's possible that we provide the solution or we customize a solution for other customers
other than Inventake.
But that's another process.
So now we are not talking about that in the presentation.
So the presentation that you saw is for Inventake only.
Okay, great.
And so we're not going to assume that anyone listening has seen the presentation.
So why don't you walk us through the main points of the case study?
What were the main challenges that you are trying to solve?
Let's start there.
Okay.
So manufacturing is actually a very interesting area.
So most people, they had the whole their smartphones, they had their computers, but they
had no knowledge about those factories.
So they imagined something like, oh, there's this big huge factory in China somewhere.
And some like huge robots or many robots working there along with maybe a lot of cheap labor
and all that.
Honestly, it's more advanced than that.
So industry 4.0 is actually an effort to digitize a lot of the physical part of the factory
so that we can seamlessly integrate what we sense from the physical, for example, the
defects or what we see from the production line together with a lot of the knowledge that
we learned from previously for that to optimize the process.
Okay.
So let me give you an example.
So traditionally in like industry of 3.0, so we need a lot of humans to look at, for
example, the PCB board or the motherboard of a computer, one by one, right?
To make sure that the components in the PCB board are not misplaced, they are not having
any defects, they are not missing and whatnot.
But then you can imagine that it's pretty cost ineffective, right?
And then a human can get tired and they can get inconsistent in their result.
So by using a computer region to do this, we can ensure that accuracy of its action is
much higher.
And more importantly, you'll be more consistent, right?
So a lot of people or a lot of labor before a lunchtime, they get so tired, they want
to go to lunch and they will simply just press OK or back to the product, right?
And then they just go to and they finish their job and they can go, right?
But then for computer, they won't have such a problem, they'll just keep working tirelessly.
So this is the very obvious example of application of AI in manufacturing.
But honestly, industry application of AI is actually a very broad area.
So according to McKinsey, a report of McKinsey in 2013, they actually more manufacturing
data than any other sector combined in a year.
So that would be two extra bytes or two and then 10 to the power of 18 bytes of data per
year, according to the report.
And that amount of data can be used to do a lot of interesting things.
In addition to defect detection, so it could be used to do like pretty few maintenance.
It can be used to do operational optimization, supply chain optimization and all that.
So what we talk about in GTC is just a very small portion of how AI can help manufacturing
or factory.
Now, one of the things that I've heard in talking to folks about this space and I'm curious
if you've seen similar is that one of the big challenges here is that while there
is a ton of data, often it's locked in proprietary systems like robotics provider might provide
the robot, but not open up the data to the customer to integrate it in with other things
or they may have data in some SCADA system, but they aren't easy interfaces to accessing
or using that data.
Do you find that in your world as well?
Yeah, that's an excellent question, actually.
So there are two aspects to that.
So one is the device, with the device actually output the data you need in order to do your
AI thing.
So first, for example, in the case of defect detection, right?
So AOI machine stands for automatic optical inspection.
So what they do is they use a camera, press some algorithm to detect defect, but a lot
of times those AOI vendor, they won't output the raw image to the factory people.
So what they're going to output is just a result of a pass or fail of this piece before.
And there's a good reason that they don't want to release their precious data to the customer.
So oftentimes you have to pay extra fee or maintenance fee or service fee in order
to get those data to do further optimization as a factory customer to those device manufacturers.
So that's one aspect to that.
And the other aspect is even the factory has access to all of this confidential or secret
data.
It's usually hard or impossible for the outsider to touch those sensitive manufacturing
data to do further things.
So I will use the example, right?
For example, like landing AI.
So a company that is certified into one of the AI pioneers, right?
So landing AI, they claim to work on this field of industry or AI, but honestly, any outsider
is close to impossible to touch data like what is the EORA of the process.
And then, for example, what is the inventory level and what is the order level or the
older number, the sales order number of this factory or for this side.
So those are so sensitive that it's impossible to reduce to outsiders.
So to answer your question, I think a lot of the thing is happening inside the factory,
inside the wall of the factory.
So the wall could be physical, it could be digital, but whichever, everything is contained.
So I would say it's not a sexy field to work with by design.
So that they can keep all the secret data that can affect their like stock price and
then that can affect a lot of things, transactions that keep everything inside and they work on
those data inside the wall.
And so you mentioned these ALI systems.
When folks are deploying AI to do visual inspection, that's one of the big use case areas for
AI in the industrial context is defect reduction and visual inspection is a goal to replace
the existing ALI systems which tend to be based on more traditional computer vision algorithms
as opposed to machine learning or is it to as you're suggesting you take their data
and optimize but still use those systems.
And there are lots of different types of computer vision based systems on the in a manufacturing
environment.
There are these defect detection systems, there are the sorting types of systems, but they're
all kind of characterized as being these closed systems where you'd want to have the
data to optimize.
Are folks trying to replace these with more open types of systems or just get access to
the data and feed it back to what those systems are doing?
Yeah, I think it depends on the stage of your development in using AI in factory.
So ALI is actually a pretty mature field so you can go out and talk to at least a hundred
different ALI vendors in the market and buy machines that do ALI for you.
So like you said, so currently most of the ALI machine in the market are using the so-called
traditional computer vision methods to find out the defects.
For example, it can detect the lines, it can detect a pattern and then the ALI machine
will realize that this pattern is like 10 degrees different from the position or the
angle that it should be, or the part is missing because the pattern doesn't match.
So those are pretty typical computer vision thing that, for example, OpenCV can do.
But for some of the defects like solder, that is kind of fuzzy or ambiguous.
Human can do pretty good on that.
They can look at the image and say, hey, this is too much solder and this is like too
little solder.
But for computer or for traditional computer vision, what they see is just a bunch of stuff,
right?
They don't know the angle, they don't, it's probably hard for them to calculate the area
and that would be the thing that is most suitable for the did learning base AI agreement to
attack or to solve and so like I said, right?
So ALI machine is capable if you can get the image out of the machine, then you can have
a second stage re-inspection engine to the image so that you can fix the defect detection
after you have fixed already the first level of defect detection.
The first level will be the easier cases that you can detect with traditional computer
vision.
And in the second stage, you can use AI based method to solve the harder to detect kind
of fuzzy and ambiguous case.
And eventually, if the system is smart enough and maybe eventually some of the AI machines
smart enough, it can combine both of the traditional and the new deep learning base method together
so it can either switch between or fuse them together so that you have one super powerful
AI machine that does everything and that can look at different kind of defects that had
different characteristics.
So the way I see that is like it does not one answer to that.
It depends on the stage of your development and sometimes it's two stages and it's like
one big stage and that.
So we've talked about ALI and defect detection quite a bit.
Were there other use cases that you mentioned in your presentation?
Yes.
So this presentation is mostly focused on defect detection even though I talked earlier,
like there's other application of AI in Infectory.
For example, like older prediction, like predictive maintenance, like safety improvement using
computer vision or environmental aspect of the AI.
So there are actually a lot of more different applications but it's not mentioned in my presentation.
Based on the title of your presentation, you also were incorporating the idea of edge
in Twitter.
Where does that fit in?
So there are two kind of scenarios.
One is edge, the other one is cloud.
So cloud in general is the mainstream of most of the AI application right now.
So in terms of image, you capture image or video, you send it to the cloud and the cloud
had usually super powerful machine with graphic processing there and they do all the inference
and the speedouts and results.
For example, pass or fail or they tell you the object type does contain in this image
and does them send it back to the user which is at edge.
So that's one scenario.
The other scenario is edge meaning you don't send your image or your data to the cloud.
You compute everything on site and when I say on site, there are a few benefits to that.
So one obvious benefit is privacy.
So there's no intermediate point that you can hijack the data.
You can keep everything within the locality of this edge device.
So privacy is number one and number two is reliability.
So reliability, what I mean by that is sometimes the network or the communication become reliable.
You can drop off or the bandwidth can be reduced and there could be error in your transmission.
And as I said earlier, they could be interference or somebody can hijack your data or your package.
And that causes a huge issue in the result.
So by having edge AI, then you avoid that problem.
And the last point of edge AI benefit would be network efficiency.
So in terms of a defect detection.
So image data is an input for your AI engine.
And image data are usually quite big, especially if you had like hundreds, thousands, if not
millions of images sending to the cloud, every hour, right?
That would pretty much jam all your infrastructure, the network infrastructure.
So by computing on the edge, then you avoid that problem altogether.
So you don't have that problem.
And so most of the conversation around edge today is focused on edge inference.
And we're seeing a lot of new hardware, actually almost every week, there's folks proposing
new hardware approaches for edge-based inference, low-power inference, et cetera.
But there's also work happening around more advanced scenarios.
So just starting to see like federated learning and ways to combine the edge with the cloud
and to have learning that incorporates the edge.
Are you doing any work in that area or seeing anything interesting in some of these more
advanced scenarios?
Yes.
So when you just mentioned, I believe you something called hybrid AI.
So basically AI computation or inference can happen, like I said, in both cloud and edge.
And edge device traditionally are pretty, they don't have that much power.
So they can emote in the case of image, they can emote capture image.
And it tracks some features like edge, like regional interests, and send the data back
to the cloud to do most of the computation like detection and classification and all that.
But now they more and more AI device like you said is popping up from the market pretty
much every day.
I think in addition to those AI chips, per se, a lot of existing device like cameras,
they start to have more and more terror frops inside their module already.
So I think it's right now it's kind of like a messy situation that everybody is trying
to capture some piece of this AI shape or edge AI market.
So what I observed right now may not be conclusive, but I think this trend is going to continue.
And then everybody is going to expand their territory, so Amazon is going to claim that
a cloud service is more powerful, more accurate, and then it's easier to deploy and all that.
And then I think the edge AI people or the chip people, they're going to say, hey, it's
more private, it's more reliable and what now I think this was going to continue.
And then eventually nobody is going to win or lose.
I think it's going to be win-win.
And then it's depending on your usage scenario and then you will choose the best solution.
I'm sure.
Going back to this idea of hybrid AI, have you seen any use cases for doing some learning
at the edge and some learning in the cloud?
So in particular to learning, so most of the edge devices are not, they only do inference.
So they don't have the capability of adapting or training on the go.
So what usually happened is you had you download a model.
So they call it model, a deep learning model from the cloud.
So those are trained in the cloud.
And in the edge, they simply do the inference or testing.
So in the case of human detection and at edge, they deploy, like say smart camera.
Smart camera with the AI chip inside.
For example, the smart camera will do the detection and happy, right?
But if somehow the population that this camera see starts to change or migrate over time,
so you might need to update your model about how people around this area look like, right?
So in this case, you have to update your model.
And currently all the training is still in the cloud and updating of the model is still
happening in the cloud.
And you still have to periodically or depends on the event that you download the new model
or updating model to the edge device.
So there's no device training currently as far as I can see.
So the hybrid AI that I see is mostly training in the cloud inference in the edge.
Yeah, that's a little bit different from this thing that is you're still a bit undefined
and maybe even still a bit mythical of federated AI where you're trying to do distributed training
across multiple devices.
But I just, I keep asking everyone who might know a little bit about, might have seen
it to get a sense for if anyone's doing it out in the wild yet.
Yeah.
In terms of distributed AI in training, I do see a quite a few applications in there.
So in coming back to the example of human recognition.
So for example, you can place your camera in different locations, different cities,
and different city would have different people looking differently, right?
In New York, people look like they're dressing suits and like very serious and look serious.
So and then you have some camera that's in urban area, suburban area that people dress at
t-shirt and sandals and whatnot.
So collection of data, they happen, distributionally, that's obvious, right?
And in terms of training, I think distributed training, they might have a local training
server that is in the proximity of those sensors and do their local training on how the
human look like in that area and eventually they'll combine them out in the cloud so
that you would have this like very small argument that can understand how human look like
in different areas.
So that's what I see in distributed AI training, but I don't see distributed AI inference
because there's no such need to do inference in multiple locations as far as I see because
inference usually is not as heavy as training, so you don't need distributed inference.
And so this distributed training that you're referring to when the model is combined, is
that a fairly simple combination like some kind of ensembling or where you're running
multiple models and picking the one that's most confident or is it something more elaborate
where you're kind of merging the models in some kind of way?
Yeah, that's a different level of integration or a different level of merging of the model.
So the simplest way is like winner winner take all right.
So whoever whose model performs the best in that case then wins, right?
So the other case is simple waiting.
So you weigh the score from different model in different locations and combined results.
So that's a second approach, still pretty simple.
And eventually you can go through this like super complicated case that you have another
neural network, right?
That neural network is trying to combine all this like child model and combine it together.
And in that you will need training data from more training data so that you can train
the parameters of this like super neural network on top of all these child models.
So there are different complexity in terms of how you combine it together.
Going back to your presentation, one of the things that you mentioned is the need to
redefine the problems in order to get the kind of results in terms of ROI and solution
capability that you're ultimately looking for.
Can you elaborate a bit on that and how that comes up?
Yeah, so when we start talking to the factory people, right, there are there are how working
people they they they focus on one specific area every single day.
So one of them talk to us about the problem that they are facing, they're like, oh, our
inventory is so big that we have to keep building like a warehouse to store all the parts that
we need.
And then the warehouse is getting bigger and bigger is getting harder and harder to manage.
And then so they're like, oh, can AI help us to manage the inventory and also to help
us to build a bigger warehouse?
And we're like, hmm, that's an interesting question to have, let's figure out like how
we do that.
Well, now architect, we don't build warehouses.
So how can we help them?
So in so after digging deeper and talk to them more, they actually had different problem
that caused this warehouse problem, right?
So what they have is they actually don't have accurate forecast or accurate prediction
of their order so that in order to meet or in order to fulfill the requirement of the
customer.
They can prepare a lot of parts in advance so that if the customer wants something, they
can quickly pull out the parts and assemble the computer for the customer.
So eventually this is not a warehouse problem or how to manage a warehouse problem.
This is actually an older position problem.
So this is what I mean by one of the key examples redefine the problem and using AI.
And another thing that you mentioned is this AI thing.
So our AI stands for return on investment.
So this is a typical practice of everybody working in manufacturing or industry.
So in order to buy something or in order to adopt a solution, you have to actually calculate.
So this much cost will take you two years, three years, 80 months, whatever long time that
to recapture the cost that you invest.
And so for a lot of the thing, our AI is really hard to calculate because this is not a typical
stationary or static optimization problem that you had all the parameters laying out in
front of you.
And then you can do your optimization and do your trick.
And then you come up with some savings or you come up with some improvement.
This is actually a dynamic process.
So when you are optimizing the system, you're actually changing the nature of the factory.
And it's similar to start market, right?
So you don't, it's like Warren Buffet, like praying in the start market when he decided
to buy something, actually he changed the S&P index already because the volume is so big.
So similar to that.
So when you actually change something or invest to something into the factory, you actually
change the nature of the problem.
So it's on the gold target.
So our AI is consequently very difficult to calculate.
Can you give a specific example of a change that AI causes in the factory that makes it
difficult to estimate ROI?
And one point of clarification before you do that is the challenge, specifically with
forecasting ROI or measuring it after the fact or both.
I think it's both.
So forecasting is actually harder.
So let's say I decided to buy robots.
Okay.
So buy robots to move all the parts in different size of my factory.
And then you can say, hey, the robot cost me like a million dollars and blah, blah, blah.
And then so the AI is like 18 months, right?
But then this is actually eventually is going to be wrong.
Reason for that is by adopting robots, you actually improve the accuracy of or the time that
arrive at different manufacturing lines.
So the time that you arrive at the line is more precise.
So you can produce your product in a more timing manner, manner.
So your euro will be higher and all that.
So when your euro is higher, actually the overall profit or revenue of the companies is increased.
And how do you factor that into your calculation of our AI, right?
And that's why in prediction or imagery, both the our calculations is often wrong.
Okay.
It sounds like if I could maybe try to restate that.
It sounds like when you automate in the manufacturing facility, you know, there's a specific
thing that you're trying to improve.
Maybe it's reducing defect and then you automate, but the introduction of automation has a
bunch of downstream effects like increasing consistency and reducing wait times and things
like that.
And it's hard to produce an all encompassing ROI study that takes into account all these
different factors.
Trista, were there any other thoughts that you included in your presentation as key takeaways
or any parting thoughts that you'd like to leave with the audience that, you know, might
help them in their path towards industry for data?
Yes.
And industry for that is a seamless integration of physical and digital world.
So the obvious benefit will be accuracy, consistency, and ROI.
And then when I want to stress, it's actually go higher than that.
So if you can actually get all those digital and physical data, IoT data, with you, you
can do much, much more optimization.
You can actually change the whole scope of how you design the manufacturing line.
You can change how you take the order.
You can change how you ship.
You can change the location where you start your inventory.
You can change the whole thing.
I think without the digital and physical integration or people call it digital twin, without
that.
I know where you can do such a ground up change of how the whole manufacturing process.
So in addition to focusing on the obvious benefit, I think everybody working in manufacturing
should look one step up and looking at what you can do to change the whole process.
And then this, again, it's not just 10%, 20%, it could be like exponential, like two times
and it can be 10 times because the whole thing is changed.
Awesome.
Well, Trista, thank you so much for chatting with us.
Thank you.
All right, everyone, that's our show for today.
For more information on any of the shows in our GTC 2019 series, visit twimmaleye.com
slash GTC19.
Thanks again to Dell for sponsoring this series.
Be sure to check them out at dellemc.com slash precision.
As always, thanks so much for listening and catch you next time.
