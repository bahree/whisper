Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
The details are set for the next Twimble Online Meetup.
Mark your calendars.
The September Meetup will be held on Tuesday the 12th from 3 to 4 p.m. Pacific time.
The discussion will be led by Nikola Kuchereva, who will be presenting learning long-term
dependencies with gradient descent is difficult by Joshua Benjiro in company.
This is one of the classic papers on recurrent neural networks so you won't want to miss
it.
For additional details or to join the meetup, head over to twimbleai.com slash meetup.
If you missed the first meetup, the recording is available on that page as well.
My guess this week is Jennifer Prinky.
That name might sound familiar as she was one of the great speakers for my future of
data summit back in May.
At the time, Jennifer was a senior data science manager and principal data scientist at Walmart
Labs, but she's since moved on to become head of data science at Atlassian.
Back at the summit, Jennifer gave an awesome talk on what she calls data mixology.
The slides for which can be found on the show notes page at twimbleai.com slash talk slash
46.
Our conversation this time begins with a recap of that talk, after which we shift our
focus to some of the practices she helped develop and implement at Walmart around the
measurement and management of machine learning models and production.
And more generally, building agile processes and teams for machine learning.
Before we jump in, I want to give a big thank you to our friends at cloud error for sponsoring
this show.
You probably think of cloud error primarily as the Hadoop company, and you're not wrong
for that.
But did you know they also offer software for data science and deep learning?
Yep, they do.
The idea is pretty simple.
If you work for a large enterprise, you probably already have Hadoop in place, and your Hadoop
cluster is filled with lots of data that you want to use in building your models.
But you still need to easily access that data, process it using the latest open source tools
and harness bursts of compute power to train your models.
This is where cloud error's data science workbench comes in.
With the data science workbench, cloud error can help you get up and running with deep
learning without massive new investments by implementing an on demand self service deep
learning platform on existing CDH clusters.
From a tech perspective, data science workbench is pretty neat.
It uses Kubernetes to transparently schedule workloads across the cluster, supporting our
Python and Scala and deep learning frameworks like TensorFlow, Keras, Cafe, and Theano.
And as of last month's 1.1 release, GPUs on the Hadoop cluster are fully supported.
The folks at cloud error are so confident that you're going to like what you see that for
a limited time, they're offering a drone to qualified participants who register for
a demo of the data science workbench.
For your demo and drone, visit twimmaleye.com slash cloud error.
And now on to the show.
All right, everyone, I am on the line with Jennifer Prinky.
Jennifer is a senior data science manager at Wal-Mart Labs, specializing in machine
learning.
And I am super excited to have her on the line with me, Jennifer, welcome to the show.
Hi, Sam.
Nice to be here.
Yeah, nice to have you here.
And it is so nice to speak with you again.
Folks recognize Jennifer's name.
It's because Jennifer was one of the speakers at the future of data summit.
And she so graciously offered to spend some time with us to talk a little bit about what
she's doing at Wal-Mart Labs.
Before we jump into that, Jennifer, why don't we have you spend a little bit of time talking
about your background and how you ended up working in machine learning at Wal-Mart?
Sure.
So actually, when I tell people what my background is, they're a little bit surprise because
I'm actually a particle physicist originally.
And so the reason why it's not as crazy as you might think at first is that I was doing
the type of particle physics where you have lots of data to treat.
And so I was actually working with a huge amounts of data even before the word data science
become as trendy as it is today.
So I mean, the reason why I eventually switched to pure data science and specifically retail
data science is that I was looking for like lots of data, interesting data to work with.
And so it actually turns out that retail has lots of very interesting challenges for somewhat
passionate with data to work with.
So here I am, right?
Fantastic.
Fantastic.
Can you tell us a little bit about the talk that you gave at the summit?
What were your goals for that presentation?
Yes.
So my topic for the summit was something I call data mixology, right?
I mean, so my goal was to try to set society people to the fact that the real challenge with
big data today is not necessarily velocity or volume as people think.
It's really about viability, right?
Because when you start plugging in several data sources, sometimes you have to rethink
your model entirely and you have to deal with all challenges related to data silos and
understanding the quality of the data coming from different sources.
And so, yeah, I mean, I really thought that this was a topic that was not necessarily covered
enough in the different conferences that I had been around recently.
So I thought it was an interesting topic to cover.
It was definitely an interesting topic and it was clear as you were delivering it that
it came from your experience.
How did these issues of silos manifest themselves in your world?
Now, so the way it comes around on my experience is that I recently started a new team where
essentially the goal is to try to use both stores data from Walmart and online data from
Walmart and bring them together.
Okay.
So in the Walmart world, truth is, I mean, the Walmart e-commerce business and the Walmart
stores business are essentially separated.
It's not the same people and even the data lives in separate places.
It's not necessarily trivial for an e-commerce data scientist at Walmart to access the store
sales data, for example.
And so as we were trying to bring these two worlds together, I actually came to discover
first-hands all the different challenges you have from bringing different data sources
together, even when it comes from the same company.
So this is exactly how I came up.
You like to come to speak about this topic.
And a lot of companies are pursuing ideas like data lakes or that idea by various different
names.
Is that something that you guys ended up doing or did you take a different approach to
integrating all this data?
Now, we're absolutely taking that direction, right?
But as you can imagine, right, I mean, the challenge for Walmart is really that you have a Walmart
e-commerce, which is a tech company that there's more recent and really like a typical Silicon
Valley company.
And on the other hand, this huge Walmart company, legacy company that has lots of data.
They actually been gathering data for a long time now.
I think they were one of the first companies actually realized that data was so important.
And so you really have to deal with different types of systems altogether.
We're not necessarily using the same technology.
So we're definitely after the creation of a data lake where all data scientists across
the company would be able to come and look at their, the same data.
But it's a long road, right?
I think every company that is trying to tackle this challenge knows that it is a long road.
And it requires a lot of different skillset and lots of different people and expertise
to actually achieve the goal.
It's funny, I think the way that some of the vendors in the space talk about it is that
you just set up, you know, set up a Hadoop cluster and run some ETL jobs and you'll have
a data lake.
What are some of the challenges that you ran into and what makes it, what makes the road
long?
Now, I mean, so I mean, I'll give you a specific example.
So one of the very interesting data sets that everybody across the company wants to work
with is the online engagement data, right?
I mean, essentially, which items does the customer actually click on and what do they eventually
buy, right?
And so this is a data set that, for example, stores doesn't have access to because they
don't have engagement, they just have their final purchases.
So they don't have any way to measure properly the interest of the customer as long as they
don't purchase something.
And so people have to keep like actually getting this data from us and they actually get
data dump, right, and they don't necessarily create like a exhaustive signal pipelines
to get this real time.
And so there are lots of different versions of these data that live across the company.
And so whenever we Walmart e-commerce make a change to this data, it's not easy to
communicate these changes to other teams.
And so one of the challenges is you don't necessarily know any more which is the original
source of truth.
So in that specific case, it might be easier because you know who the owner is, but in some
other cases, we don't necessarily even know where the data is coming from.
And so everybody's interested in the same data, but this data exists in multiple versions.
And it's actually very hard to come up with a procedure to actually figure out which one
is the best one and which one is the accurate source of truth.
That actually gives us a really interesting segue to one of the main topics that I wanted
to dig in with you here on the podcast.
And that is one of the interesting aspects of your role is leading a team that's focused
on measuring and auditing for the various machine learning models at Walmart.
And you mentioned this source of truth and data providence is kind of one small aspect
of that.
And tell us a little bit about your role and some of the type of work that you're focused
on in that role.
Right.
Definitely.
So I'm actually a part of a group called the search algorithms team.
So we're essentially the group of data scientists and machine learning experts that take care
of all machine learning algorithms that you would see at work on the walmart.com page,
right?
That includes learning to rank algorithms and involves everything related to the understanding
of the customer.
So we actually split down the responsibilities on my team into three different portions.
So there is something called the perceived team, which is essentially in charge of trying
to understand what the customer wants.
Right.
I mean, so query understanding it involves a lot of natural language processing algorithms,
auto completion algorithms, spell checking algorithms would be their responsibilities.
Okay.
Here's the guide team.
So the guide team is about learning to rank and showing the right items once you think
you understand what the customer is looking for.
And then we have this measure team, which is my team that essentially takes care of helping
the others understand their weaknesses, suggest new data sets that they can use, suggest
best practices, make sure that these other algorithms are retrained properly at the proper
frequency, catch problems early on, so we're essentially creating models to take care
of other models, right?
I mean, so we create specific measurement scoring systems that range from data quality
to customer satisfaction.
So we're trying to bring like a essentially what the team that gets a real profound understanding
of the other algorithms in order to help the others understand what they need to do to
make it even better.
And are you primarily focused on helping the search teams or are you, do you also work
with teams outside of search that are doing data science and machine learning?
So that's an interesting question, because my original mission was definitely to help
the search team.
But we actually, it turns out that we are the only measured team within the company.
And so once people started understanding what we're doing, we actually get lots of
requests from other teams to actually help them as well, right?
Search is obviously an area where you have lots of different teams that are involved
with us, right?
And so we're really focusing on search, but you can imagine that the team in charge of
the inventory and the catalog is also a team, teams that we are very close, closely working
with.
So it's pretty natural that we also bring measurements for them.
And so another area where we're also partnering with other teams is that we actually created
an entire process called machine learning lifecycle management, which is essentially a checklist
of things that we believe all machine learning models should, I mean, people who work on machine
learning models should do before pushing something to production.
And so it actually turns out that we have a pretty efficient system now.
So I mean, we are essentially requiring data scientists to provide, you know, like a very
clear view of what the accuracy is, but also what the performance of the algorithm is
in terms of the amount of CPU that their model consume when they're retraining and so
for sets, so on and so on.
We are not trying to expand this process to the entire e-commerce section of Walmart.
And it actually turns out that lots of people are interested by that because the challenging
data science is oftentimes in a company like ours.
You have machine learning engineers who are really like engineering people who don't necessarily
understand the limitation of data science properly speaking, right?
And so they are not necessarily trained to think in terms of evaluating the accuracy
and making the proper checks before sending something to production.
They are the type of people who are really looking forward to see their model in action
and they don't necessarily take the time to evaluate the statistical performance of the
models.
And so creating this, you know, like this process is really making sure that everybody's
on the same page that things are running properly in production.
It's interesting, it makes me think of a few years ago when the software development community
went through this process of like industrializing the delivery of software and that resulted
in ideas like lean and agile methodologies and DevOps and things like that.
And it sounds like you guys are kind of on the, you know, the cutting edge of an industrialization
wave of machine learning, not to be confused at all with the industrial AI line of inquiry
that we've talked about here in the podcast recently.
But I love this idea of a machine learning lifecycle model.
What can you tell us about that model and the, you know, the various steps and stages
and requirements that you've put in place for the teams there?
Right.
So I mean, you definitely write about, you know, like that being like a new wave of agile,
right?
I mean, agile for data science or machine learning, this is exactly what we're after.
So I mean, as we were putting the like the first steps together, I actually came to realize
that it is really a cultural problem, right?
I mean, because if you want to reach a stage where things are done properly, you're really
about trying to fix tech debt, but people usually think of tech debt as code debt, right?
I mean, this is the way that people came to know code debt and truth is tech debt is much
more than this, right?
I mean, there's a, there are actually more pieces to tech debt than just code debt.
There is a definitely data that related to like the quality of your data, but also the
data sets that you may not be using, but your competitors are using, right?
I mean, so if you're actually in a situation where, for example, we know, for example,
that Amazon is using a specific data set that we have, but we are not using, you're actually
aware in the data debt situation, right?
Then there is the notion of system debt.
So the case where you're using legacy systems and you're not improving and getting to use
the latest versions of a specific software or not like a newest cutting-edge software that
is intended into three.
And then you have machine learning that so machine learning that is really when you're using
a machine learning model, not to the best of its ability, right?
I mean, so, for example, if you don't understand at which frequency you should be retraining
a model, you don't understand, you don't monitor the inputs and outputs.
It's definitely also a situation that you have to take care of.
So I mean, the steps that, you know, when somebody asked me, how, what should I do to
actually get started with, you know, like a automation and I'll try to, like, basically
audit my models, what should I do?
So my answer to that is, it's not necessary, something that's very complicated.
It's really about a process and also creating a culture in your company where everybody
understands that making things right is important.
And so it really depends on the kind of model you're dealing with, but like usually one thing
I suggest everybody should do is make sure that you document everything that you're doing,
right?
I mean, so it may sound like a cheesy answer, you know, but it's definitely super important.
We actually turned out that most of the times when we didn't have a model performing well
enough, it wasn't necessary because of the model itself.
It was because we didn't have a clear understanding of what the model was doing, right?
And so we were not able to reproduce the same model.
There was a lack of transparency.
And so for example, you would have a new engineer coming over and trying to take over the
project and they wouldn't even know how the model was built.
So the other thing is you're, it's extremely important that you have a clear understanding
of what your failures and weaknesses were, so that I mean, people tend to forget that
you're like in the concept of machine learning, life cycle management, there is the worst
cycle, right?
I mean, so there is an opportunity for everybody to learn about their weaknesses in order
to make sure that the next iteration of your model is better.
Right.
And you're like, so definitely think about the culture that you have to bring in your
company and make sure that you keep in track of everything you're doing, that it is
very clear the data you're using, it is very clear that you understand the quality of
your data and you understand your challenges.
On the various teams there, can you tell me a little bit about the relationship between
data scientists and people with a statistical orientation and developers and engineers?
Yeah, I can absolutely tell you about that.
So actually my team has a statistical analyst, data scientist and machine learning engineers.
So people sometimes struggle to understand what the difference is.
So really our, in our view, a statistical analyst are people who know how to play with
the data really, really well.
So they essentially like can get you, you know, like a very clear understanding of whether
your data is sufficient entropy and sufficient variance for you to build a model and then
can give you answers very quickly to get started.
The data scientist is actually the person that would, I would say like prototype a model,
right?
And so once you have an understanding that your data is good enough for you to solve a
specific problem, the data scientist will come up with a solution and essentially try
to assess which is the best type of machine learning model for you to solve that problem.
So we don't necessarily expect like the statistical analyst to be someone who's an expert
in machine learning.
I mean, of course, they have some understanding that they are not the persons that be in
charge of creating a model.
And then the machine learning engineer is someone that knows how to optimize this machine
learning model and make it work at scale.
So they're really like focusing on making everything efficient and I mean, they really have
the ability to push that to production.
So having all this skill set together in one team has been really helpful for us because
it really helps us move things to production really quickly.
One of the things I've seen in the past with organizations that have a model similar to
yours, although I think less sophisticated in the way you are managing it and the machine
learning life cycle processes that you've introduced is a little bit of friction in kind
of the interface between the data scientists and the machine learning engineers where you
would have a data scientist, you know, create a model kind of coded up using, you know, maybe
even a set of tools that the are not the set of tools that the ML engineers are working
with kind of throw it over the wall and then have this machine learning engineer who,
you know, maybe less sophisticated in understanding the model, you know, try to implement it often
in, you know, going from, you know, Python, for example, to Java or something like that.
And that both resulting in, you know, creating an opportunity for the introduction of errors
as well as slowing cycle time and iteration time, just because of the back and forth over
this barrier, how have you guys seen that at all and how have you addressed it?
Now, so I definitely saw see how that problem can arise, right?
I mean, so I think like at the very beginning, when this team was still very decent, I mean,
we definitely had that problem.
The way we kind of sold it is that there is actually a very decent overlap between the
data scientists and the machine learning engineer.
So usually the data scientists would actually code something, which is pretty close to what
would end up being in production except that it is not necessarily functioning as scale,
right?
I mean, so usually they use the same language.
So that's for sure.
The other thing is we make sure that I actually like I have my machine learning engineers
and my data scientists working pairs.
Okay.
So the machine learning engineer is actually involved in the early stages as well, but he's
not the tech lead for that portion, right?
I mean, so he actually gets to be involved and immersed with the model like very early
on, which gives him some more sophisticated understanding of the model that makes it easier
for him to him or her to actually push it to production later.
So we don't really have like this French association faces really like the entire pair
is working throughout the process except that the first phase is the phase where the data
scientists is in charge and the last phase is the phase where the machine learning person
is in charge.
Okay.
That's another really adaptation of the agile idea or at least the pair programming notion
of agile to this machine learning lifecycle.
Interesting.
Interesting.
So you develop these models, you get them in production and then you are tasked with
tracking and measuring and auditing their performance, not just when you're putting them
in the production, but over time, tell us a little bit about that cycle.
Yeah, sure, sure.
So I mean, the interesting thing was, you know, like at first when we came up with this
new model of having like an external team kind of measuring things was pretty interesting,
right?
Because our other teams up to that point in time they were actually used to essentially
come up with a success metric that they would use for essentially build a model and they
would actually use the same success metric for measuring and auditing this model themselves,
right?
And so the value proposition was that you're kind of in the situation where you have a
conflict of interest, right?
Right.
Right.
And even if you want to be like a really truthful, I mean, if the same person is actually coming
up with measurements and actually assessing their own models, they don't necessarily see
things in a different light, right?
I mean, so the value proposition here is that you have a different person that doesn't
know or knows very little about the model auditing things and actually come up with their
own definition of what success means for that model, right?
So we had a little bit of tension at the beginning, as you can imagine, right?
Because it's almost like you use the word auditing, right?
And that's definitely what we do, right?
I mean, so you're in a situation where everybody's wondering like, well, what is the status
of my model?
Well, these guys are going to find anything wrong with my model.
So it took some time for us to actually make it very clear that we are not here to actually
judge your work.
We're actually here to help you improve it, right?
Right.
Right.
Right.
But I mean, I think everybody's very comfortable right now that we're actually in charge
of making sure of the quality of the model.
So we have a very good dynamic with the other teams right now.
And it comes to measuring the performance of the models that you guys are using.
Are you focusing on business metrics or technical model performance metrics or a combination
of both?
It's definitely a combination of both.
I mean, so the reason why we believe that there should be an entire team focused on this
is as you can imagine, there is not one single metric per model, right?
Sure.
We actually have like some models actually use like several metrics or several tens of
metrics to actually make sure that we have a comprehensive view of how the model is
performing.
And so it ranges from like a how accurate is the model to how efficient is the model in
terms of your like is it using too much CPU as I mentioned earlier?
And is it is it impacting the customer in a proper way, right?
So our belief is that you should have a specific metric for every single model separately.
So in retail, it is pretty traditional to use typically like the number of add to cards
or the number of clicks or even the revenue as a measurement of your like success when
you, for example, run a bit tests, right?
So our belief is that because you have these two steps, right, understanding the customer
through perceive and guiding the customer through guide, we believe that you should have
metrics specific to each one of these portions, specifically because otherwise you're looking
at all models in terms of add to cards, it doesn't really make sense, right?
Because the perception phase is really about understanding the customer not necessarily.
So for example, if I ever drop in add to cards, it is possible that my new perceived algorithm
is really working well, but because there is a bottleneck with the guide phase, I won't
see that this model is performing well, right?
I mean, so really making sure that you have very narrow and very specific metrics, even
if it means having many of them is definitely working very well for us.
I guess I have mixed feelings about that hearing it.
I wonder about local minima, local maxima, or I guess probably a better way to put it
is unit test versus integration test or system tests, like what if you're creating, you
have a measure that the perceived team is able to maximize, but it doesn't maximize
the overall metric of something like revenue or an add to card.
How do you manage that?
That's a very good question, actually.
We see that problem very often, so basically, you would have a new perceived algorithm
that performs really well, but you'd actually see that it actually causes the guide performance
to drop, right?
I mean, so you definitely have this kind of cannibalization problems.
Let me give you an example, right?
I mean, so we actually figured at some point that when you're actually improving the accuracy
or the efficiency of your autocompletion algorithms, it essentially drops the performance of the
spell check algorithm.
Why?
Because if people can use the autocompletion algorithm, they're not going to finish entering
the queries by end, which means that the spell checking algorithm is not called that
offer, right?
I mean, it's pretty logical, if you think about it.
So I mean, this is exactly the kind of thing you want to observe, because in that specific
scenario that essentially allows us to say, you know what, it's worth investing more
time, making a perfect autocompletion algorithm rather than making a perfect spell check algorithm.
So you actually use these inefficiencies to determine which algorithms you should focus
on.
Interesting.
So a related question that I've had for folks in the retail space is around short
sighted versus long sighted models, and this, an example here might be, you know, as
we talked about, it's pretty common to optimize your models around add to carts or even, you
know, short term, you know, even immediate revenue creation or even something like profitability
to be kind of one level higher in business impact.
But I wonder if, when you're doing that, if it's possible that you are sub optimizing the
broader metric like customer lifetime value or something along those lines, is that something
that you think about there at all?
Now, we definitely have that as a metric.
So you suggested like a customer lifetime value.
This is one of the metric you would monitor against the entire process, which is why I
say that you need to have several metrics for every model, right?
I mean, so we make sure that we keep track of all different aspects and dimensions of
the problem.
But as in always in business, at the end of the day, you have to follow a business decision
as well, right?
And so if the goal of the company is to increase revenue drastically over the next quarter,
I mean, you, at the end of the day, you, you align your decision based on this as well,
right?
I mean, at the end of the day, the final choice of what you're like, which algorithm
you should improve comes down to a business decisions.
Our goal is really to make sure that they have all information in hand and handy to actually
make a decision based on that, right?
I mean, so whatever they decide to do, we make sure that they are aware that if they choose
to do a specific or take a specific decision, it may impact customer lifetime value out
these kind of things.
Are there other instances where you are, where you're working to balance short-term versus
long-term optimization targets?
Well, I mean, obviously for as far as I've seen things that were more so far as really
like this kind of optimization would come down to a business decision, right?
I mean, so I don't think we're already reached a level where we can forecast our predict
the future well enough to actually get to this side to comprehensive knowledge that brings
everybody on the same page, for sure.
Right.
Are you in the process of auditing these various teams?
Do you have a list, either formal or in your head of these are the top end things that
people tend to do wrong or put another way, what's your advice for folks that want to
learn from what you've learned from your teams on how they should approach modeling?
Right.
Definitely.
So I would see three things that I believe are good take-up is for everybody who's trying
to tackle this problem.
So the first one is definitely what I would say before making sure you document everything,
especially in large organizations where the turnover of your employees is really high,
right?
I mean, you want to make sure that if something went wrong with the past model, at least
you know what went wrong and you're at the ability of fixing this in the next situation.
And so make sure that anyone can actually grab that model and reproduce the same results.
That's one thing.
The other thing is I actually noticed that many times when our models are unsuccessful,
it is essentially not due to a performance issue from the model side, it's actually a problem
with the inputs.
So a failure in one of the systems or like typically it retains something you could see happening
is a seasonality pattern, right?
I mean, so basically your model was meant to function well for your inputs to be in a
specific range and you have to make sure that it is still the same range, right?
I mean, so actually monitoring the inputs and the outputs goes a very long way.
It doesn't necessarily mean that you have to monitor things very closely, but you're
essentially get a sense that the number of average number of add to cards you see on
a specific day is still pretty close to what you would expect them and what it was when
you actually trained your model.
Right, right.
The last thing is, I would say that one issue I've seen as well is not necessarily an
issue, but data scientists tend to, I would say that not necessarily overfitting the
way you would think about it, but like use too much data for the models.
So something that we're actually requiring from all our data scientists now is that when
they suggest a specific amount of data for retrain the models, we actually ask them to train
the same model, exact same model with the lesser amount of data and they actually do that
for several data points.
And we actually build this curve of essentially CPU consumption versus accuracy of the model.
And I actually turned out that in our case, many people were using, I would say four times
too much data compared to what was actually needed.
Oh wow.
So essentially that means that you're using four times too much CPU, right?
I mean, so you're sticking you to four times longer to train these models.
So essentially, of course, it's better to use more data, but if you're going to increase
your accuracy by just one percent by throwing four times as much data, it doesn't really
make sense, right?
That means four.
So I'm definitely, I think that data scientists are not trained to think in terms of money
optimization, right?
That means four.
So this is something that we've made sure now that everybody is actually aware of conscious
of the amount of CPU they're using when they're training the models.
And have you developed a set of rules of thumb?
Is there a way to generalize that or is the right way for them to, do they always need
to run the models with four different data points and understand where the kind of that
utility curve and pick the right point on it?
This is the way we're functioning right now, right?
I mean, of course, for the future, I have some hope of coming up with a, I mean, it's
a very iterative process, right?
I mean, so the way we've been thinking about this data, I mean, as we ask people,
to do things, people actually start creating their own scripts and their own tools to actually
perform these tasks.
So when something comes across as being easy to generalize, we try to make sure that this
is also accessible to other team members.
And so over time, we're actually building this data base of tools that everybody can use
for their specific problems.
And so we're moving towards automations, just like it's a very slow process because as
you may guess, like we have very different types of models and not everything can be
used for the models as well.
Do you have some kind of tool or platform in place for deploying and managing the various
models or the individual teams do that themselves for their own services?
I guess, you know, part of the question is thinking about it, like what's happening
on the dev side of things, folks are forming, you know, dev ops teams around microservices
that, you know, have full lifecycle responsibilities for those services, are you doing similar things
around models?
Yes, so we're moving in that direction.
So we're actually developing our own compute platform where essentially all models will
be trained.
And so that platform would actually be talking to the data like DRK, right?
I mean, but again, it's a very slow process because you have to train people to use that
new platform, there are some paradigms that are not necessarily very obvious to everybody.
We try to make sure that, you know, like everybody gets to use their favorite language
in that platform, but essentially we're also loading that compute platform with the tools
that was mentioned before.
So that everything is in one place.
Everybody's aware of, you know, like what tools exist to make your life easier as a data
scientist or a machine learning engineer?
And is this a home-run platform or something that you're...
Yes.
I'm imagine when you talked about the monitoring, the inputs and outputs of the models that
struck me as really interesting, and I imagine some platform that, you know, you would tie
into a monitoring system that when you're, you know, as part of your documentation phase,
you're able to describe the expected bounds of a given model.
And then this thing is monitoring the inputs.
And if it starts, if you start seeing inputs outside of the bound, this thing would shoot
off, you know, red flags and start paging people, have you gotten there yet or is that part
of what you're working for?
Yes.
Yes, absolutely.
Absolutely.
This is exactly what we're trying to do right now, right?
I mean, so it's...
For, I mean, the challenge with this specific is that when you have, like, you're like
supervised models and like a numerical data, it's fairly easy to monitor the inputs, right?
I mean, so...
Right.
I mean, so for some of the models, it's actually already in place, where essentially,
we...
Whenever an input goes outside of, like, minus two sigma plus two sigma boundaries, it's actually
shooting an email to the person in charge of monitoring the bottle and they would actually
know that, you know, something is potentially about to happen, right?
I mean, so we're definitely geared towards this, like, I mean, one thing we definitely
want to achieve in the near future is a model that allows you to understand that your model
is expiring before...
Before it's time, right?
I mean, so right now, I think most companies are thinking of retraining models in terms
of a regular sequence, right?
I mean, basically, I retrain my model every other week.
Right.
So when you're in retail, there may be lots of happenings, there may be, like, holidays
and sometimes you have to retrain things faster, unless you have something in place to let
you know that the bottle is about to change or needs to be updated, you would actually
learn that by your customer complaining about getting the wrong results or something that's
not accurate or relevant to our searches, right?
And so you don't want that to happen because it essentially involves that the customer needs
to have a bad experience for you to be aware that something's wrong with your model.
Right.
So make sure that we can catch these problems early in the process before it actually impacts
the customer.
Mm-hmm.
And so what are some of the methodologies that you use to identify these expiring models?
Well, again, it's about like finding the right metric to actually assess the satisfaction
of the customer, right?
I mean, but I don't think there's like a one-true, only metric that works for all cases,
but I mean, again, it's the mission of the measure team, right?
Right.
And measuring things as well.
So another word with a model.
Capturing the dissatisfaction.
Sorry, sorry for cutting you off with just a paraphrase.
In other words, the model is expiring when it stops performing if there's not some other
dimensions to it.
Yeah.
Right.
Right.
Okay.
Interesting.
As part of this measure team, you also are chartered with specifically looking for weaknesses
in other people's models.
What does that look like and how do you approach that?
And I guess I'm thinking of looking for corner cases or cases in the data that these teams
might not have thought about that based on your experience, you could foresee causing
poor model performance.
How do you approach that part of the role?
Now, there are definitely two components to it, right?
I mean, so there's definitely weaknesses that you would see.
And like a specific model that requires like a free-contrading or is extremely sensitive
to seasonality would be something that we would like to look at and try to figure out
like what is causing this, right?
I mean, so the way we do that is essentially, we essentially keep track of, for example,
the assume that your model is something like a logistic regression model because it's
a easy to explain.
So you would be able to see like what parameters are extremely stable over time and essentially
don't change even when you will train the model.
And which one of these parameters are actually extremely volatile and have a very big error
to it, right?
And so we would actually understand very with precision, but parameters are causing the
model to underperform.
So that's kind of like a reverse engineer, other people's model in order to understand
what the weaknesses are.
So that's one thing we do when we're trying to kind of automate.
The other piece is something which is like something you have an inkling that requires
to be updated, right?
I mean, so an example of something we have tried to do recently is that we were trying
to add the notion of geolocation to which he personalized the results depending on
your location in the country, right?
And so I mean, you know that this needs to be taken into account and you know that you're
going to add that feature in the model, but the question is like, what is your base data
set and your best bet to actually add that to the model, right?
I mean, so this is why we have statistical analyst trying to assess the quality of the
different data sets that we have available.
So this is where our job actually gets interesting because we get to touch to lots of different
data sets across the company, right?
And try to understand what is the data source that we could use to actually improve these
signals and make our search engine better.
So do you have, is this maybe goes back to our platform discussion a moment ago, but
is there a place that has a dashboard of all the models that are running in Walmart?
I guess I'm wondering at the granularity at which you track this like, do you have a
master view of all deployed models and their performance and you can do trend analysis
across this and see, you know, where logistic regression, you know, types of models work
versus other things or are these things managed more on a product by product basis?
No, no, we're definitely geared towards like at least for search, I mean, we're definitely
moving forward to a phase where we get to see a holistic view of all models in product
model.
At one time, right?
I mean, so basically if lots of your models are using the same base model, like it's
fairly easy to do, it gets more complicated if you have many different types of machine
learning models in production, but we definitely believe that you should have a comprehensive
view of everything.
For the reason we mentioned earlier that you have some crosstalk happening across models,
right?
I mean, it's possible that the fact that one model is underperforming is caused by another
one overperforming and so we believe that you cannot keep things segmented and just keep
track of one product at a time.
I mean, I really strongly believe that having a comprehensive view as much as possible
is really important.
Getting to the level where we have a comprehensive view of all the models across the company is
going to be very challenging as you can imagine, but to what extent do you use machine learning
models to manage these models?
And then how do you do that?
That's definitely what we want to do, right?
I mean, I sometimes call my team like the team that creates machine learning models of
machine learning models, right?
And so essentially the way you would do that is essentially using the parameters of the
other models as a feature for another model, right?
I mean, so basically you're kind of, as you mentioned earlier, right?
I mean, you want to manage your things over time.
So essentially like a trend analysis would be something that could, you know, you could
definitely use machine learning for this type of management.
And are you doing this at all today or is it more directional?
Getting started.
Okay.
Interesting.
Interesting.
Yeah, I can imagine if you have all of your model data, all of your parameter data,
all of your performance data, you know, then part of what your measure team is able to
do is someone brings you a model and some data and you can just run your meta model against
it and predict whether their model is going to work or not.
It sounds like a great application.
Yeah.
Awesome.
Is there anything else that your team is focused on that we haven't talked about so far?
Well, I mean, I've covered most of it.
I mean, the one thing is like I mentioned this effort to actually bring the stores data
together with the online data.
So this is an effort we started pretty recently.
One of the challenges we're trying to tackle is the following.
So, uh, the search is actually an interesting problem because where, as you can imagine, we're
using like lots of different data sources to rank the items we're showing to the customer,
right?
And so essentially we're using data related to the content of the items.
So if somebody searches for TV Samsung, you want to show that you're like the right
brand and the right product for sure.
But then the question is among Samsung TVs, which one do you want to show first, right?
And so the answer to that is you're showing the one that is the most popular.
So we sometimes run into a problem because I think, for example, of like a smaller type
of item that you would usually buy in the store, right?
I mean, so sometimes people connect to walmart.com website.
They enter a search and they actually decide to go buy that item in store.
So the reason why they actually search for that item was to check the inventory in their
local, local Walmart store, right?
I mean, so for us as the search team, this is really a problem because if you see someone
click on the item, but they eventually they don't purchase it.
We take that as a bad sign that we didn't show the right item, right?
And so essentially that would cause us to demote that item over time.
And it's very possible that the item that we showed was actually the one that the customer
meant to see, right?
I mean, and so it is very possible that eventually they bought that.
So closing the loop with that and actually like attributing a specific store purchase
to a specific online search is something that we're trying to do now, right?
I mean, so I think people have heard of the new Google attribution, right?
And they actually get to track you when you shop in store as well as online.
I mean, we're essentially trying to do that for essentially mapping the, like essentially
mapping the gap between the stories and the online experience.
Mm-hmm.
And that that's what the data lake enables you to do by pulling all that information into
the one place and allowing folks to build models across it.
Yeah, definitely.
Interesting.
Anna, are you, to what extent are you using external data sources and building your search
models?
So we're, I mean, we do, you know, like I don't know that we're using like a lot of
data sources.
I mean, the external data sources we essentially use is to, I would say for monitoring purposes,
right?
I mean, so for example, we're trying to catch instances where we have a cold stock
problem, right?
I mean, so if something doesn't sell really well at Walmart, when you actually know this
is a very popular item on the marketplace, you would try to do something about it.
But we don't necessarily use that to create and build new models or essentially focusing
on our own data at this point.
Got it.
Got it.
All right.
Well, this has been a really, really interesting conversation and I appreciate you taking
the time out to chat with us about what you're up to.
But I think folks can learn a ton about the machine learning lifecycle management challenge
and, and, and learn a ton from the way you guys have taken it on at, at Walmart.
I really appreciate you taking the time to join us.
No worries.
I mean, I always love talking about this topic.
So my pleasure.
Awesome.
Thanks so much, Jennifer.
All right, everyone.
What's our show for today for the notes for this episode head on over to twimmaleye.com
slash talk slash 46, whether this is your first or 50th show, I want to thank you so much
for listening.
I really want to hear from you.
So please take a moment to comment on the show notes page or on Twitter with your feedback
or questions or just what you found most interesting and useful about this episode.
Also, if you share your favorite quote via a comment or social media, we'll send you
one of our fab laptop stickers.
Another thanks to this week's sponsor, Claudeira, for more information on their data science
workbench or to schedule your demo and get a free drone, visit twimmaleye.com slash Claudeira.
If you subscribe to my newsletter, you already know that I've got a busy month ahead as
far as events go.
The week of September 18th, I'll be in San Francisco for the O'Reilly Artificial Intelligence
Conference.
There's also a chance that on Saturday the 16th, I'll make it to the Scaling Deep Learning
Conference NSF, which looks to be an interesting one.
The following week, I'll be at Strange Loop, a great technical conference held each year
right here in St. Louis.
Now I love meeting up with listeners, so if you're planning to be at any of these events,
please drop me a note via a comment, the contact form, or Twitter.
For more info on any of these events, check out the show notes.
Thanks again for listening and catch you next time.
