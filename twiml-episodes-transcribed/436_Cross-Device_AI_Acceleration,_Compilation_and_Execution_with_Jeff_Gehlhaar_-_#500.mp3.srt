1
00:00:00,000 --> 00:00:23,000
All right, everyone. I am here with Jeff Galhar. Jeff is VP of technology and ahead of AI software platforms at Qualcomm, as well as a great friend of the show. Jeff, welcome back to the Twoma AI podcast.

2
00:00:23,000 --> 00:00:32,000
Thank you very much for having me. It's great to be back. I think for a third visit, give you guys an update on what we've been up to Qualcomm AI software.

3
00:00:32,000 --> 00:00:41,000
Yeah, I think we'll have a great conversation. I've heard a little bit about some of the updates that we will be talking about and lots of good stuff.

4
00:00:41,000 --> 00:00:53,000
It's been coming up on nine months or so, I guess, since the last time we spoke. And since then, we've covered a lot of interesting conversations with folks on the research side.

5
00:00:53,000 --> 00:01:04,000
But before we jump in, I'd love to have you share a little bit about your background, as well as your current role at Qualcomm, which congratulations been expanded a bit.

6
00:01:04,000 --> 00:01:14,000
Yeah, thank you very much. So yeah, to refresh the audience a little bit, rejoining the podcast, Jeff Galhar VP technology.

7
00:01:14,000 --> 00:01:25,000
Look, my role right now is really about expanding and enhancing Qualcomm's investment in software for AI across our whole portfolio.

8
00:01:25,000 --> 00:01:40,000
The expanded role is really to include the cloud AIC 100 product line as part of that. And so we can talk maybe a bit about how we're looking to harmonize our AI snack across our whole portfolio.

9
00:01:40,000 --> 00:02:02,000
A bit of a background. I spent a long period of time in what's now Qualcomm at research, you spoke to one of my colleagues on that side recently, but happy to be partying really closely with Qualcomm at research to take their ideas and put them in the software and working of course with our hardware teams around our complete AI software solutions.

10
00:02:02,000 --> 00:02:20,000
You mentioned the cloud AI 100 and that portfolio of products. We've talked about it a little bit on the show previously, but kind of give us a refresher and share a little bit about the current updates there.

11
00:02:20,000 --> 00:02:38,000
Yeah, great. Thank you very much. So the AIC 100 is a product designed for cloud edge applications. So data center applications, smart cities, you think about videos, surveillance kinds of applications, autonomous robots.

12
00:02:38,000 --> 00:02:56,000
We're using that everywhere from from our automated driving program all the way up into, you know, cloud inference infrastructure kinds of applications to power very high end, you know, hundreds of tops kinds of applications and when put in a rack.

13
00:02:56,000 --> 00:03:14,000
And I think the admitted joke about, you know, petta ops kind of class applications. So we've got quite a range of compute available in that device. And then we're using that in conjunction with our, let's say, more traditional snapdragon processors.

14
00:03:14,000 --> 00:03:23,000
To power a whole range of smart city robotics and autonomous devices include in addition to our cloud edge kinds of use cases.

15
00:03:23,000 --> 00:03:43,000
Awesome. So one of the topics that I wanted us to dig into a bit is on ML compilers is an increasingly kind of interesting and hot topic with projects, the likes of glow out of Facebook and TVM.

16
00:03:43,000 --> 00:03:59,000
Let's start by having you kind of introduce us to the concept broadly and what's the role of ML compilers compared to some of the other types of technologies that, you know, we've seen like on X and others.

17
00:03:59,000 --> 00:04:23,000
So we think when we think of compilers at Qualcomm, we think of it in the broadest set of the words. Let me define that a little bit. We were thinking about the whole problem of compilation. And typically when we think about a compiler, C++ compiler, we think we have a high level language and somebody writes an algorithm in it and we want to reduce that to, you know, machine code.

18
00:04:23,000 --> 00:04:36,000
And of course, we have really good compilers like LVM to do that. When we think about a neural network analogy holes. I've got a neural network. I want to, you know, bring it in wherever I bring it in from.

19
00:04:36,000 --> 00:04:53,000
We'll talk about PyTorch, TensorFlow, we'll talk about Onyx in a second. And I want to compile it, you know, in a sense. Now, in some cases, the hardest part of that problem is figuring out all of the parallelism that's going on inside of that neural network.

20
00:04:53,000 --> 00:05:15,000
Sort of standard to feed forward kind of neural network. There's a lot of things you could do in parallel. And so the area where we're making, we're making a big investment is in that I'll call it scheduling and tiling part of the problem. How do you deploy this compiler technology in a way that your neural network is mapped to hardware in a highly efficient way.

21
00:05:15,000 --> 00:05:30,000
The original code generation piece back end of it is also a challenging piece in its own right generating highly performant code to produce, you know, what we'll call neural network ops.

22
00:05:30,000 --> 00:05:49,000
So fragments of code that represent the actual work being done on a neural network. In addition to getting those tiled and deployed correctly onto the hardware is a pretty tricky operation. And so in the case of the AIC product, we're largely leveraging work that Facebook started with the glow infrastructure.

23
00:05:49,000 --> 00:06:01,000
We've done a lot of customization and optimization, what I'll call the back end of the compiler, the part of the compiler that deals with that scheduling, tiling and code generation piece.

24
00:06:01,000 --> 00:06:24,000
And also more towards our Snapdragon portfolio, working with the TVM community to do a similar kind of thing more on the sort of single core, you know, mobile, IoT kind of devices around whole graph compilation of taking in a graph, solving that tiling and scheduling problem and then deploying it, you know, onto our hardware.

25
00:06:24,000 --> 00:06:39,000
Can you maybe give a little bit more concrete detail that illustrates the difference between the tiling, you know, what kind of operations would be covered by tiling versus the code generation part.

26
00:06:39,000 --> 00:07:03,000
So when I say tiling and there's a lot of different sort of ways to slice this, this problem, I'm really thinking about the problem of how do I do things like take my tensors, break them up into pieces so that I can paralyze parts of the operation, how do I make those tensors fit into blocks of memory that I have in ways where those blocks are packed into memory efficiently.

27
00:07:03,000 --> 00:07:21,000
I'm stored in local memory, you know, a correct amount of time, so I'm using my local memory really efficiently, I'm being efficient about when I go to DDR, when I don't go to DDR, how do I bring in data for the neural network, how do I send the results, you know, to the next stage of processing.

28
00:07:21,000 --> 00:07:34,000
Parallelism, these are highly parallel machines, especially when we talk about a I see 100 it's a highly parallel machine, just solving that problem of parallelism is a big part of the ML compiler job.

29
00:07:34,000 --> 00:07:44,000
The code generation is really about once I've solved that problem like if taking my data and I've kind of chopped it into pieces and I figured out how to paralyze it and I figured out what has to happen first.

30
00:07:44,000 --> 00:07:55,000
Just generating the code to do that is kind of what I'll call the code generation piece so whether it's the code for that, you know, that pipeline I've just kind of figured out.

31
00:07:55,000 --> 00:08:06,000
Or it's the actual code of the kernels I've actually got to make it run on the hardware right so at the end I've got to take that schedule I got to link it with code and I got to deploy that onto.

32
00:08:06,000 --> 00:08:17,000
You know I set a course to execute the workload and that's really in the broadest sense that we think of when we say a qualcomm when we think of when we say compilers got it got it so the.

33
00:08:17,000 --> 00:08:26,000
The thinking about it from the kind of the graph that the developers created down the.

34
00:08:26,000 --> 00:08:37,000
The code generation kind of looks at the graph and turns that into code and the tiling is more the low level data structure and it's close to hardware.

35
00:08:37,000 --> 00:08:57,000
Another way around so so it you think of the tiling as the part where I'm taking the neural network and in the most naive way I would just sort of executed as specified by the you know by the data scientist but to get maximum performance I want to be really.

36
00:08:57,000 --> 00:09:10,000
I want to be really efficient about how we're using memory and how big those memory blocks are and I want to break operations that I can make parallel into smaller parallel operations and do them all at the same time on a parallel machine.

37
00:09:10,000 --> 00:09:23,000
The code generation is within the very last stage like once I've decided that whole recipe if you will for execution what are the actual instructions I have to give my hardware to do that.

38
00:09:23,000 --> 00:09:34,000
Really bad figuring out the puzzle I have a giant neural network how do I efficiently you know order the instructions so that it produces the correct semantic results.

39
00:09:34,000 --> 00:09:46,000
But so that it says optimal on the hardware as possible and every one of the custom accelerators in the market has picked a different architecture so they have a slightly different optimization problem the salt.

40
00:09:46,000 --> 00:10:04,000
Yeah is there on that note is there yeah how do you think about the a c 100 architecture from a you know principles perspective relative to other approaches that are out on the market what's different about it.

41
00:10:04,000 --> 00:10:15,000
Of course I can't share a ton of details but I'll say it's a it's a leverages our traditional focus on high performance and low power.

42
00:10:15,000 --> 00:10:24,000
I think he had shared a lot of metrics at a high level about that when he was on the show with you and your audience and.

43
00:10:24,000 --> 00:10:41,000
The way I think about it is that other devices in the market I think if picked very large arrays to solve their problem and I would say about this is we picked a more kind of finer grain highly parallel architecture right.

44
00:10:41,000 --> 00:10:59,000
Therefore we can take advantage of the parallelism that exists sort of naturally in some of these core and some of these neural networks i'm sorry and deploy them onto the hardware in a really efficient way and so one of the sort of evidences of that we did quite well in the recent ml commons.

45
00:10:59,000 --> 00:11:21,000
And so the audience can go see those results and see that we you know best in class results and a number of categories deploying this exact stack bring in a neural network you know figure out how to tile and deploy to hardware and then get very good power performance out of the solution.

46
00:11:21,000 --> 00:11:45,000
And from a software perspective are there differences in the architecture or approach of glow and TVM that led to glow being the preferred route for the cloud inference solution and TVM being the preferred route for the devices or is that more market based.

47
00:11:45,000 --> 00:12:10,000
I'd say it's a bit of market and a bit of technology to be fair and the glow I think was designed from from the beginning with these large cloud workloads in mind and we had in mind to build as large you know cloud oriented array and I think maybe a little bit later we discovered wow this is a really cool device we can actually apply it to all kinds of things besides.

48
00:12:10,000 --> 00:12:29,000
You know cloud and edge and so we had started on that road and we started at a time when compilers were I think still a little nascent for applications of actually compiling ml kernels a lot of a lot of companies have written a lot of handwritten code.

49
00:12:29,000 --> 00:12:57,000
And so a little bit of the market forces is that there's a you know maybe these are the two big contenders for ml compilers today apart from maybe for some proprietary offerings and so it is a bit of a horse race it's still early I think as we've discussed before in the ml compiler space although it's sort of crystallizing a bit more as people understand the problems better.

50
00:12:57,000 --> 00:13:08,000
And that's a little bit why also Qualcomm kind of is looking at both because because we feel like there's elements of both that are very strong and we want to take advantage of that.

51
00:13:08,000 --> 00:13:23,000
But in the past we've talked about some of your support for tiny ml and work that you've done with Google is that to what degree does that relate to this compiler conversation we've been having.

52
00:13:23,000 --> 00:13:40,000
So it's in a space that that we are invested in we've got a tiny ml work with Google and there is a tiny tv m effort and we're looking at we're looking at that space as well for the super deeply embedded always on kind of use cases.

53
00:13:40,000 --> 00:13:47,000
And it's you know it's a smaller part of our overall portfolio but we're definitely you know looking at that as well.

54
00:13:47,000 --> 00:14:04,000
So the the is the way to think about it is you've got the kind of cloud class large scale infrastructure devices that's a I see 100 and using glow as the compiler there you've got the handheld devices.

55
00:14:04,000 --> 00:14:30,000
There are still you know relatively full feature and powerful and TVM is the approach there and then tiny ml is more for these you know smaller footprint more constrained always on devices and tiny TVM is an offshoot that is going to provide that same set of compiling capability for those devices.

56
00:14:30,000 --> 00:14:34,000
Yeah it's a great way to summarize it absolutely.

57
00:14:34,000 --> 00:14:51,000
And then to close the loop I mentioned Onyx earlier Onyx isn't exactly a compiler is it no no so so we were involved very early in the Onyx you know standards establishment if you will sort of de facto standard.

58
00:14:51,000 --> 00:15:04,000
We chaired part of the onyx edge working group for a good period of time and so onyx is is kind of gone in kind of maybe two ways we still use it very extensively as an interchange format.

59
00:15:04,000 --> 00:15:17,000
So our tools can read onyx files are exported let's say from PyTorch and frequently our customers bring us those kinds of models and we work with them to deploy them onto our silicon.

60
00:15:17,000 --> 00:15:29,000
There's also an onyx runtime right so onyx runtime has been started to be integrated into places like win ml and the Microsoft stack and in some of the data center applications as well.

61
00:15:29,000 --> 00:15:45,000
And so we've got work going on to bring Onyx is a runtime on top of our underlying API is into our portfolio as well so we think of onyx as like you said not a compiler but as the data interchange format.

62
00:15:45,000 --> 00:15:52,000
And it's a sort of execution framework that that can sit sort of on top of our silicon.

63
00:15:52,000 --> 00:16:10,000
Yeah I kind of think of it as like a CSV file for spreadsheets you know yeah TensorFlow might be Excel and PyTorch might be pages and you can still interchange the interchange data using the CSV format similar to that.

64
00:16:10,000 --> 00:16:16,000
So it wasn't familiar with the runtime effort that sounds pretty interesting.

65
00:16:16,000 --> 00:16:29,000
So think about it there's you know when we think about it and talk about it a little bit but we're providing a set of APIs across a whole platform with this you know as we sort of harmonize our offering.

66
00:16:29,000 --> 00:16:51,000
So what we then find is that there are a number different what I'll call sort of execution frameworks that various markets or various customers want to use so in some markets they want to use TF light we think of that like an execution framework TF light can accelerate on our hardware for very good experiences and.

67
00:16:51,000 --> 00:17:16,000
So in some markets that like in the windows market Microsoft is established win ML Android Google is established Android neural networks these are all execution frameworks if you will they can read neural networks can orchestrate their execution on the hardware and do so with our underlying drivers and we can talk a little bit about that that's part of why we did AI engine direct.

68
00:17:16,000 --> 00:17:30,000
So in order to provide a best in class acceleration in a package that can then be exposed to the ecosystem along a number of different routes and on its runtime is one of those kinds of routes.

69
00:17:30,000 --> 00:17:40,000
So on its runtime orchestrates the deployment of the network and we provide libraries that accelerate that on our hardware is the way to think about it got it got it.

70
00:17:40,000 --> 00:17:46,000
So you mentioned the platform what's new in the on the platform front at Qualcomm.

71
00:17:46,000 --> 00:18:04,000
Oh my gosh so much stuff so with a couple different things going on so with the announcement of the Snapdragon 888 we announced the AI engine direct which is a evolution on the hexagon and end direct we hit announced in the previous generation product.

72
00:18:04,000 --> 00:18:19,000
The basic idea there is is we realize that there was this diversity of routes that our customers wanted to use to get on the platform we just talked about TF light and Android neural networks and sell on.

73
00:18:19,000 --> 00:18:28,000
And we realized that the kind of biggest innovation we can provide is linking that compiler discussion we had earlier this tiling and deployment.

74
00:18:28,000 --> 00:18:45,000
The English is very hard to do and and it's unique to our hardware and provide a bridge to these execution frameworks so this AI engine direct is sort of a mid level API that's consistent across it will be consistent across all of our product offerings as we move forward.

75
00:18:45,000 --> 00:19:06,000
But starting with the Snapdragon portfolio of products and that API can then be used above that API you can write an orchestration layer like a TF light or like an on extra on time and below that API we provide best in class hardware acceleration across our IP blocks.

76
00:19:06,000 --> 00:19:23,000
The advantage of the developer is a common API so they can talk to our hardware whether they want to talk to a GPU or they want to talk to HTTP they can talk to our hardware with a common API it's on us to provide best in class scheduling and tiling and acceleration.

77
00:19:23,000 --> 00:19:26,000
And then they can build their application kind of however they want.

78
00:19:26,000 --> 00:19:42,000
So we're going to take that and we're going to extend that concept over to the AIC product so that you will have a kind of horizontal experience regardless of where you want to use a Qualcomm piece of silicon in your product.

79
00:19:42,000 --> 00:20:02,000
You will have a common way of accessing our APIs and if you want to use you know common orchestration like an on X runtime you can then build on top of that and of course we'll partner with you know industry partners to make that you know smooth and seamless so we're really trying to open up our portfolio.

80
00:20:02,000 --> 00:20:14,000
So we're going to provide best in class sort of hardware and software acceleration at the low level and then really provide kind of choice is you go up the stack depending on what your application needs are.

81
00:20:14,000 --> 00:20:35,000
And as you bring more capability to the platform are you seeing a shift in the types of developers that are accessing your API's meaning is it shifting from kind of the device manufacturers and OEMs to the end developers at all.

82
00:20:35,000 --> 00:21:04,000
Yeah it depends a little on market segment right so in in like her traditional smart phone segment we still work very very closely with the big names you'd be familiar with and in those areas you know we you know you and I often talk about what are the use cases you know we work with them I think in the last generation you know well over a hundred different models or we work with them to deploy and optimize obviously different OEMs have different combinations.

83
00:21:04,000 --> 00:21:13,000
But it's not unusual for you know our flagship devices with our partners to have 50 60 70 neural networks maybe in a single device.

84
00:21:13,000 --> 00:21:23,000
And these are models that are provided you know via the operating system for things like facial recognition and you know photography for example lots of things.

85
00:21:23,000 --> 00:21:43,000
Yeah so fingerprint anywhere from unlocking your device so face unlock fingerprint right these are biometric secure secure payments you know camera nighttime photography some of the cinematographic effects you know face detect face tracking subject tracking.

86
00:21:43,000 --> 00:22:08,000
Super resolution right to this idea that you can take like a lower res sensor and create the you know the appearance the effect of a high res sensor and these are all little secrets that your smartphone does for you and they're cool and maybe you're taking from granted but behind there you know one frame might be processed by 10 or 15 different neural networks that are each providing.

87
00:22:08,000 --> 00:22:27,000
You know one layer in that one is going to your move noise another one is going to improve the sharpness and other one is going to you know deal with nighttime lighting and so on and the composite effect of these incredible you know photos that we see coming out of our devices right they say the best camera you have as someone's in your pocket.

88
00:22:27,000 --> 00:22:54,000
And in a lot of cases while I'm a photographer and I like my you know cameras the one I always have is my smartphone right right right and so we but we see that in audio processing you know mic noise suppression and so on right and when you look at segments like compute you and I are using some kind of you know laptop or computer.

89
00:22:54,000 --> 00:23:08,000
You know making like the video conferencing experience better in this you know new work world these are things that we're doing already with our partners using AI on our silicon.

90
00:23:08,000 --> 00:23:21,000
And you know these kind of platform environments you often see the you know the platforms provide higher and higher level of abstraction capability.

91
00:23:21,000 --> 00:23:48,000
First of you can comment on this at all just initially you know the large vendors whose name we all know like they had a distinctive advantage and you know their phones because they they had the ML chops to make their cameras do amazing things I'm imagining a world in which you know as opposed to them just relying on you know great APIs and hardware to make that happen in proprietary models.

92
00:23:48,000 --> 00:24:03,000
This can start to offer you know a night vision model built into the platform and I'm curious you know and that's just an example but I'm curious the extent to which that's happening today the extent to which you you foresee that in the future.

93
00:24:03,000 --> 00:24:05,000
Anything you can share on that.

94
00:24:05,000 --> 00:24:30,000
So it's a complicated question because in the following sense it used to be that the things that made these cameras great were people who were really good at cameras and of course we still have those people but it was a algorithmically solved you know traditionally solve problem and now these neural networks are really the IP that these

95
00:24:30,000 --> 00:24:45,000
camor vendors. You know whatever the application is whether your automotive auto autonomous driving and it's important to do segmentation and you know pedestrian detection and sign detection whatever that

96
00:24:45,000 --> 00:24:54,000
So the model is now the sort of IP that's so valuable, right, to the end, to the end, OEM, whatever you're building, whatever your product is.

97
00:24:54,000 --> 00:24:57,000
And so let me turn that around.

98
00:24:57,000 --> 00:25:06,000
I will say that we use the same tools and the same innovation inside of, let's say, Qualcomm's camera pipeline, Qualcomm's audio pipeline.

99
00:25:06,000 --> 00:25:23,000
So I can't speculate with the audience on, you know, whether we would introduce a, you know, face beautification model or a, you know, relighting model as part of our end and camera offering, except to say that we use the same kind of techniques to improve our camera pipeline.

100
00:25:23,000 --> 00:25:35,000
And some of that stuff is baked in and then some of it, the OEMs come to us and say, no, no, we have a proprietary solution, we think is a differentiator, and we want your help to get it on and get it into our products.

101
00:25:35,000 --> 00:25:49,000
And so we respect the fact that our OEMs, whether we're talking about a hand set OEM or a IoT or an automotive developer that that's part of the value that they're offering to their, to their customer on top of our silicon.

102
00:25:49,000 --> 00:26:04,000
So we want to be mindful of that, respectful of that. And so sure, we'll bring certain innovations to the market through our channels and with partners, but we want to be respectful of, you know, the fact that our customers want to build their own IP and differentiate on it.

103
00:26:04,000 --> 00:26:08,000
So we want to be an accessible platform for that innovation.

104
00:26:08,000 --> 00:26:24,000
Yeah, we talked a little bit about benchmarking in the context of the AIC 100, are there also benchmarking results that you're doing in the context of the platform.

105
00:26:24,000 --> 00:26:40,000
Yeah, so more broadly, and maybe in a more established sort of way, there are a number of AI benchmarks. There's been some early benchmarks and then more and more interest from the traditional benchmark developers.

106
00:26:40,000 --> 00:26:55,000
I'm sure your audience is familiar with the geek benches of the world. And when they last bought a PC, they probably looked at, you know, a spec marker geek bench and looked at specs. And so by similar, some of those traditional benchmark vendors are doing now AI benchmarks.

107
00:26:55,000 --> 00:27:04,000
And there's some new entrance. And so we very actively participate in that on the mobile side, our Snapdragon portfolio.

108
00:27:04,000 --> 00:27:20,000
So whether it's it's ludashi, very popular sort of AI benchmark in China, whether it's benchmarks oriented oriented around like Android neural networks is a few Android neural network benchmarks.

109
00:27:20,000 --> 00:27:30,000
And getting back to ML Perf, we also participate in the ML Perf benchmarks that are mobile oriented. So ML Perf has a whole categorization of benchmarks.

110
00:27:30,000 --> 00:27:40,000
In fact, of course, that a benchmark for a mobile device is not going to be competing with a benchmark in the cloud market to their segments of benchmarks in ML Perf.

111
00:27:40,000 --> 00:27:48,000
And we're very active participants there. Now one thing that I wanted to maybe highlight, get back to when you're asked about the platform.

112
00:27:48,000 --> 00:28:00,000
This was announced at Google IO and it ties in with the kind of idea around benchmarks is we're working with Google have announced that we will be working with them on these updatable drivers for Android.

113
00:28:00,000 --> 00:28:06,000
And so the way that ties with benchmarks is that you know, we release a device and it has hardware in it.

114
00:28:06,000 --> 00:28:22,000
We talked early in the conversation about compilers and about tiling and about optimization. These are not solve problems. I mean, we're doing some really interesting innovative work in the compile or tiling algorithm development area.

115
00:28:22,000 --> 00:28:38,000
So as our customers bring use cases as we continue to innovate, we find that we can make marked improvements in the performance of these platforms through software, you know, we can find better ways to schedule these graphs onto the hardware.

116
00:28:38,000 --> 00:28:54,000
We do that. And in many of our products like our Snapdragon neural processing SDK, I'll call it our oldest most established SDK. We make monthly releases to to the marketplace and to our customers.

117
00:28:54,000 --> 00:29:12,000
But this often requires them to make a product refresh like update the wrong, you know, issue a new app, this kind of thing. The work with Google will actually allow us to make updates working with Google directly to the end device to have sort of continuous improvement on Snapdragon with these updates.

118
00:29:12,000 --> 00:29:35,000
So the tie back to benchmarks is people like to say, Oh, I had this snapshot in time. I did this benchmark and I got this score. I got like 800 and it was amazing. And it's great. And then, you know, three months later, four months later, six months later, that scores 900 and people are like, well, second, you didn't change the hardware. And it's like, yeah, but we're continually updating these algorithms that tile and schedule and make improvements.

119
00:29:35,000 --> 00:29:48,000
And being able to update the marketplace as fast as AI is changing. So you don't need to go get a new phone. You can actually get an update that's going to make your phone work better. That's really where we want to push also, right.

120
00:29:48,000 --> 00:30:07,000
The product cycles kind of match as best we can, of course, the rate of innovation that we see happening in the AI space in a general sense. And so these, you know, improvements that we're doing, lead to improvements and benchmarks improvements and end user experience.

121
00:30:07,000 --> 00:30:24,000
And also, you know, give us a chance to work with partners like Google to, you know, find ways to make that happen. Yeah, yeah, interesting, interesting. Related to the topic of performance and, you know, somewhat related to the topic of compilers.

122
00:30:24,000 --> 00:30:37,000
When we spoke last, we talked about quite a bit about quantization and the aim at toolkit and some of the work that you're doing there. It's one of the areas.

123
00:30:37,000 --> 00:30:52,000
Now often talk to Qualcomm researcher is about compression and and quantization. And it's one of the areas there seems to be a free flowing pipeline of innovation from research into product. Can you give us an update on on that.

124
00:30:52,000 --> 00:31:10,000
Sure, absolutely. So again, we talked a little bit had a short update in our tech summit at the end of the year around Snapdragon 888. As you mentioned, there's a sort of continuous innovation happening. And this is really a good example of one of many, but a good example where

125
00:31:10,000 --> 00:31:23,000
we're mostly partnering with our colleagues in Qualcomm research and trying to, you know, make those ideas that you hear from Amir and from others, maybe on the podcast over into into the product.

126
00:31:23,000 --> 00:31:46,000
The update there is that's part of my expanded role will be taking on the, you know, full commercialization of these tools will be integrating them as part of this. If you will harmonization of our full stack, that will include being able to bring in, you know, quantization results from tools like Amit, it won't be the only tool.

127
00:31:46,000 --> 00:31:58,000
Except, of course, PyTorch and TensorFlow, but to complement that and we think to advance sort of state of the art and quantization, the techniques that are in Amit that are published by the research group.

128
00:31:58,000 --> 00:32:15,000
Many of them are made available in open source. Some of them aren't, we are going to have a sort of pro version of Amit that'll go out to our customers that will be, you know, scaled up and it'll be sort of impedance matched to the road map of the hardware and the software that we're developing.

129
00:32:15,000 --> 00:32:43,000
In the overall product portfolio, so that we can continue to assist our customers in getting best in class performance, power performance on our hardware in a way that that allows them to take advantage of our optimized hardware at these lower bit rates, right, getting to lower bit rates as you've probably heard is not always a straightforward thing and it's an area where we partner with our customers to get right.

130
00:32:43,000 --> 00:32:55,000
But by making sure that we've got tools that can do compression that can do things like eight around, right, this is statistical rounding technique to improve accuracy for these bit rates.

131
00:32:55,000 --> 00:33:02,000
As we do that is we do other things in the future that improve on our tooling.

132
00:33:02,000 --> 00:33:20,000
You'll see more and more of these things, you know, hopefully fit together. And in that area, we are, we're really, you know, we're really trying to harmonize that to again reduce the friction they asked about developers and whether it's just the OAMs reduce the friction so that more and more

133
00:33:20,000 --> 00:33:34,000
less and less, you know, data scientists and more and more developers can do this for themselves and can get the advantage of it. So that's really where we're going with that. So keep stay tuned at the audience to stay tuned as we do that.

134
00:33:34,000 --> 00:33:46,000
We're already engaged with our partners. Many of our partners have already, you know, basically aim at pro and they're using it and they're giving us feedback on it.

135
00:33:46,000 --> 00:33:52,000
So that will become more and more a mainstay of our products as we move forward.

136
00:33:52,000 --> 00:34:13,000
And can you speak to the, you know, maybe the most promising or most recent techniques that have been incorporated into aim at pro for quantization and, you know, what they, what they offer from a relative performance perspective or, you know, how you think about that landscape.

137
00:34:13,000 --> 00:34:23,000
So, so we, so the little secret here is that tricks that are in aim at an aim at pro have been shipping in our products for a while.

138
00:34:23,000 --> 00:34:34,000
So the some of our best quantization techniques we kind of snuck into the product like 18 months ago before we had sort of a toolkit sort of fully identified.

139
00:34:34,000 --> 00:34:45,000
Some of the what you see in the marketplace already takes advantage of those techniques, but techniques the network that's in the product and you've used these quantization techniques in that network.

140
00:34:45,000 --> 00:34:51,000
The tool is to allow other folks are building their own networks to achieve the same things.

141
00:34:51,000 --> 00:35:06,000
Yes, both so both networks we've developed that have used it, but also before it really had a product name, we were already taking these techniques, putting them into tools like our Snapdragon neural processing SDK and they were shipping out to customers.

142
00:35:06,000 --> 00:35:21,000
So your if I can say you are, you know, galaxy device already leverages some of those techniques and it has for a period of time we're now creating a more say well defined product around it and we're adding more techniques.

143
00:35:21,000 --> 00:35:42,000
So exciting techniques are kind of two things in the area of things like quantization this eight around thing we keep coming back to is a very interesting idea of sort of stochastically rounding up or rounding down and playing with that decision turns out to have a really interesting effect on quantization positive effect on quantization.

144
00:35:42,000 --> 00:36:08,000
The other thing I think is important and maybe a little subtle to the audience is when you're doing, for example, quantization of our training right so this is a training technique where you're you're training the network in the face of quantization or quantization noise so that it's becoming if you will familiarized with the effects of quantization so when you actually move to quantize the network.

145
00:36:08,000 --> 00:36:14,000
It's already kind of aware what the consequences are it's been trained in the face of that environment.

146
00:36:14,000 --> 00:36:27,000
The better you can do that in a way that's aware of how the network will actually be deployed on the hardware what we like to call hardware aware quantization aware training that's a mouthful.

147
00:36:27,000 --> 00:36:51,000
Better name but the idea being if I know not just that the network will be quantized and I train it in the face of that but I if I can make those techniques aware of what it'll actually look like on the hardware so the more I can move the training part of the problem closer to the hardware the better my results are going to be.

148
00:36:51,000 --> 00:37:10,000
We're just starting to see some of that and we've got some ideas in the pipeline about how we can do that in a better way so the sort of longer term and maybe in another nine months or whatever I can come back and maybe we'll be ready to share some more is closing that loop as a hardware manufacturer.

149
00:37:10,000 --> 00:37:30,000
As a developer of these you know softer toolkits and as a research innovator I feel like we really have the three main elements here where we can close that loop and the more that our tools are aware of how it goes on to hardware and therefore by consequence longer term how we can change the hardware in ways that it makes it easier to close that loop.

150
00:37:30,000 --> 00:37:41,000
We really have a word to a cycle and you're we're going to start to see some of those loops close and that to me is one of the most exciting things about about that toolkit right is being able to fully close that loop.

151
00:37:41,000 --> 00:37:53,000
Yeah when you talk about closing that loop what what exactly does that mean i'm imagining you know trying to you know taking decades to train a transformer on a device.

152
00:37:53,000 --> 00:38:10,000
Let's go close in that loop is more yet it's a good question closing that loop in my mind is more about for example making sure that the simulated quantization that's happening during the training during let's say quantization that we're training past that that is.

153
00:38:10,000 --> 00:38:25,000
Or call it hardware where it's aware of some of the eccentricities that a particular piece of hardware might you know impact.

154
00:38:25,000 --> 00:38:35,000
Right although we can talk about that too but in this case i'm really talking about that that there's fidelity and that they're complimentary they're aware of the eccentricities if you will.

155
00:38:35,000 --> 00:38:41,000
Of both environments in a way that we can kind of leverage that eccentricity for benefit.

156
00:38:41,000 --> 00:39:04,000
Got it got it and then you mentioned we can talk about training on device that is that in the context of like a federated type of environment we've talked briefly about that in the past and talked about it from a research perspective any any updates there in particular yeah so the you know it's a research group has been busy at it since we talk last and.

157
00:39:04,000 --> 00:39:24,000
I can't really talk about specific applications that we're not ready to unveil that done a lot of research got labs up and running by labs I mean you know to do this you have to simulate real and virtual devices at some scale to understand you know how to federate learning and what the trade offs are.

158
00:39:24,000 --> 00:39:42,000
And of course we're first and foremost a wireless communications company so this becomes really natural we often have to simulate large numbers of users scale that have unreliable radio links and so on so this is perfect we're just now doing it with federated learning in addition to the communication link.

159
00:39:42,000 --> 00:40:01,000
And I think you heard Ziad in a previous podcast and those who didn't listen please go back and listen to my colleague Ziad talk about you know 5G and AI be complimentary and this is a great example where you know we're know a lot about the link we know a lot about the fidelity and in fidelity as you know robustness of the link.

160
00:40:01,000 --> 00:40:13,000
And when you're federated learning of course you have to factor in that you might have somebody in your federation that goes away because their battery guys or the call goes away or they drive into a parking lot or who knows why.

161
00:40:13,000 --> 00:40:30,000
And but statistically you need a certain number of samples and so on and the applications here are really about things that are amenable to crowdsourcing in a general sense so data sets whether it's I think I think Ziad talked about you know Google keyboard but.

162
00:40:30,000 --> 00:40:51,000
Generalizing that data sets where the experience of a large number of users improves the whole for everybody is kind of we have in mind and we don't have in mind start with brass tax and like you said train a transformer you know large language scale transformer on devices that would take an eternity perhaps but.

163
00:40:51,000 --> 00:41:16,000
Can we start with though if you will a lab train cloud train model deploy it to a number of devices right it's in every shipping galaxy device maybe or something right and can the experiences of the users collectively improve because their environment the experiences that they and their device go through are all different or largely different right.

164
00:41:16,000 --> 00:41:29,000
And this can be for sound detection this could be for maybe wake word improvement this could be for you know censor modalities right and so when we talk about the life cycle of a device.

165
00:41:29,000 --> 00:41:45,000
Each of our devices goes to different life cycles but we can you know group those experiences and have a better you know collective outcome and so we're looking at the infrastructure frameworks for that and then also some use cases kind of in that direction where we think that this technology is really

166
00:41:45,000 --> 00:41:55,000
complementary to improve user experiences by if you will fine tuning models around those experiences got it got it awesome awesome.

167
00:41:55,000 --> 00:42:05,000
Well Jeff it is always a pleasure to chat with you and appreciate the updates as always wonderful to have you on the show.

168
00:42:05,000 --> 00:42:17,000
Well thank you very much really enjoy it. Thank you for having me and and a great conversation about all these exciting things we're doing and come back the next time and give you and get another update.

169
00:42:17,000 --> 00:42:35,000
Awesome and I should mention that we have mentioned several other conversations with Ziat and some of the researchers and we'll have related links on the show notes page so folks can check them all out.

170
00:42:35,000 --> 00:42:36,000
Thank you.

171
00:42:36,000 --> 00:42:37,000
Thank you.

172
00:42:37,000 --> 00:42:38,000
You have a good afternoon.

173
00:42:38,000 --> 00:42:48,000
Thank you.

