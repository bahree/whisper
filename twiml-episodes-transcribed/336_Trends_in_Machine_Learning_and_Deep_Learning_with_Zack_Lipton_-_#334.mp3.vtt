WEBVTT

00:00.000 --> 00:26.080
Hey everyone, hope you all had a wonderful holiday.

00:26.080 --> 00:30.840
For the next few weeks we'll be running back the clock with our second annual AI Rewind

00:30.840 --> 00:32.160
series.

00:32.160 --> 00:33.960
Join by a few friends of the show.

00:33.960 --> 00:39.240
We'll be reviewing the papers, tools, use cases, and other developments that made a splash

00:39.240 --> 00:46.720
in 2019 in key fields like machine learning, deep learning, NLP, computer vision, reinforcement

00:46.720 --> 00:49.480
learning, and ethical AI.

00:49.480 --> 00:55.600
Be sure to follow along with the series at twomolai.com slash rewind 19.

00:55.600 --> 00:59.960
As always, we'd love to hear your thoughts on this series, including anything we might

00:59.960 --> 01:01.040
have missed.

01:01.040 --> 01:06.480
Send me your feedback or favorite papers via Twitter, where I'm at Sam Charrington, or via

01:06.480 --> 01:11.400
a comment on the show notes page you can find at twomolai.com.

01:11.400 --> 01:12.400
Happy New Year.

01:12.400 --> 01:14.440
Let's get into the show.

01:14.440 --> 01:17.960
All right, everyone.

01:17.960 --> 01:23.480
Welcome to Twomolai's AI Rewind 2019, where I check in with friends of the show to hear

01:23.480 --> 01:31.920
from them on their favorite papers, perspective, thoughts, reflections on the year 2019 in their

01:31.920 --> 01:33.840
area of work and research.

01:33.840 --> 01:37.320
I've got the pleasure of speaking with Zach Lipton.

01:37.320 --> 01:41.080
Zach is a jointly appointed professor in a Tepper School of Business and the machine

01:41.080 --> 01:46.520
learning department at CMU, where he's also affiliated with the Heinz School of Public

01:46.520 --> 01:47.520
Policy.

01:47.520 --> 01:53.160
Zach was my guest in July, where we talked about fairwashing and the folly of ML solution

01:53.160 --> 01:54.160
mechanism.

01:54.160 --> 01:55.640
I encourage you to check out that show.

01:55.640 --> 02:00.000
If you're not already familiar with Zach in his background, Zach, welcome back to the

02:00.000 --> 02:01.000
Twomolai podcast.

02:01.000 --> 02:02.840
Thanks for having me, Sam.

02:02.840 --> 02:07.640
So I'm really looking forward to digging into some of the papers that you identified for

02:07.640 --> 02:09.840
us to walk through.

02:09.840 --> 02:17.080
But before we do that, I'd love to just start with your general sense of 2019 and what

02:17.080 --> 02:22.720
it meant for you or what you think it means for us kind of for machine learning in general.

02:22.720 --> 02:29.640
I think that we've been for the last few years kind of in this just mad dash, just sort

02:29.640 --> 02:36.200
of just catch up with what's changed following, I think 2010 to 2012, disruption in terms

02:36.200 --> 02:41.200
of the kind of like new set of tools that were made available by kind of line of successful

02:41.200 --> 02:44.160
works that got deep learning actually working.

02:44.160 --> 02:49.760
And so the metaphor I settled on at NURBS is it's sort of like the task of doing research

02:49.760 --> 02:55.160
which is kind of like wandering around in the dark like swinging around for like a pinata.

02:55.160 --> 03:01.200
And somebody smashed it open in 2012 and just suddenly became really easy to do research.

03:01.200 --> 03:04.640
Like it was really easy to collect candy because there was a bunch of it sitting on the

03:04.640 --> 03:05.640
ground.

03:05.640 --> 03:10.680
And I think people have just been trying to pick all the low hanging for 2013.

03:10.680 --> 03:15.920
You could get really smashing successful papers just by saying we played with a number

03:15.920 --> 03:21.400
of layers in the neural network and the results to be gotten by doing that were sufficiently

03:21.400 --> 03:24.920
compelling that you know it didn't matter how much sort of intellectual content there

03:24.920 --> 03:25.920
was.

03:25.920 --> 03:29.640
I think you know over the over the last few years it's gotten a little bit more refined

03:29.640 --> 03:33.320
and it's taking a bit more work to make an interesting paper.

03:33.320 --> 03:37.360
But I think right now more than any other point I think what you're starting to find is

03:37.360 --> 03:42.440
people just kind of coming up against the limitations of the general paradigm of like we collected

03:42.440 --> 03:46.000
the big data said you know we trained it on one partition we evaluated on the whole

03:46.000 --> 03:51.200
doubts that we kind of saw how we did and you know you can come up with a whole bunch

03:51.200 --> 03:57.840
of sophisticated tools to sort of either analyze that process or to try to sort of improve

03:57.840 --> 03:59.440
its efficiency a little bit.

03:59.440 --> 04:04.200
But there's a fundamental gap I think between sort of where people sort of got near dreams

04:04.200 --> 04:08.280
about what they thought was technology would could go and sort of like what we actually

04:08.280 --> 04:09.280
kind of do with it.

04:09.280 --> 04:14.000
I think what you're really seeing now in 2019 is people starting to think stock of the

04:14.000 --> 04:18.800
limitations of the current setup and the current paradigm and I think a lot of the creative

04:18.800 --> 04:22.960
people starting to think more seriously about going beyond just doing supervised learning

04:22.960 --> 04:27.640
and thinking really seriously whether it's about causal inference or robustness under

04:27.640 --> 04:32.360
domain adaptation or starting to think more seriously about the economic aspects of if

04:32.360 --> 04:37.120
you're deploying systems to make decisions then you're you're interacting in an environment

04:37.120 --> 04:41.320
with other agents who are also going to update their behavior and response to whatever

04:41.320 --> 04:46.760
it is that you changed and I think that this is kind of what's going on right now is

04:46.760 --> 04:51.840
that the sort of like rocket fuel from deep learning and starting to in my eyes I think

04:51.840 --> 04:57.280
it's starting to run out but I think the hope is that people are starting to get ambitious

04:57.280 --> 05:02.800
again and starting to get ambitious not just about scale but sort of about the kinds of

05:02.800 --> 05:04.520
problems that they pick on.

05:04.520 --> 05:13.040
Yeah, when I think back on this time of year a couple of years ago post-Nerrips a lot

05:13.040 --> 05:18.920
of people were talking about Ali Rahimi's kind of call for increased rigor in deep learning

05:18.920 --> 05:23.440
strikes me that that's kind of somewhat related to what you're describing and what we're

05:23.440 --> 05:28.880
seeing now but what I'm hearing you saying is less about kind of this shift to really

05:28.880 --> 05:35.320
digging into trying to find rigor but more looking elsewhere incorporating other ideas

05:35.320 --> 05:40.320
I'm hearing a ton about causality as well coming up in conversations do you know are those

05:40.320 --> 05:43.200
related kind of themes do you think?

05:43.200 --> 05:48.320
I think it's an important distinction to be made between doing science in a rigorous

05:48.320 --> 05:53.760
fashion and what are the set of scientific questions that you're working on and I think

05:53.760 --> 06:00.560
that this sort of big discussion that started after Ali's talk about sort of doing this

06:00.560 --> 06:08.560
work more carefully is I think very important and I've participated in that discussion

06:08.560 --> 06:14.920
a bit I think also like many people inspired by Ali's call to attention in 2017 but I

06:14.920 --> 06:19.760
think that's a sort of separate question from this other question about the set of problems

06:19.760 --> 06:24.800
that we start looking to like you could be very rigorous but still just focus entirely

06:24.800 --> 06:29.120
on this sort of trained test mode of machine learning and that's a little bit different

06:29.120 --> 06:35.160
from like you can become rigorous without leaving that world right you could just do just

06:35.160 --> 06:39.320
there's enough there's enough questions to probably still keep a large number of researchers

06:39.320 --> 06:45.920
working for a lifetime just about generalization of from one night samples to some some distribution

06:45.920 --> 06:50.400
but there's a different question about doing work that addresses qualitatively sort of

06:50.400 --> 06:55.440
broader sets of questions I think causality offers that right which is it's not just about

06:55.440 --> 07:00.800
doing work rigorously it's sort of an all-concentration which is what what are the categories

07:00.800 --> 07:06.400
of questions that you can answer and so you've identified some papers that were particularly

07:07.200 --> 07:13.920
meaningful for you in 2019 let's jump into let's jump into those that where would you like

07:13.920 --> 07:19.920
to start yeah so I try to include papers that or groups of papers that that sort of capture

07:20.480 --> 07:24.560
multiple aspects of this paper's papers sort of doing the old thing and interesting why

07:24.560 --> 07:31.680
paper is doing something new I guess to start off I thought two really interesting papers one

07:31.680 --> 07:35.920
of them was 2018 I guess but we could bundle it with a more recent one so don't hold that

07:35.920 --> 07:42.400
against me are these papers from Ben Wreck and Ludwig Schmidt and Becca Roelhoff's call do

07:42.400 --> 07:49.040
see far 10 classifiers generalized see far 10 and do image net classifier generalized to image net

07:49.520 --> 07:55.200
and in both cases these were basically like taking a hard look at this we we've had a whole

07:55.200 --> 08:00.480
community that part of how we put so many people to use is that we've really embraced this leaderboard

08:00.480 --> 08:05.280
in benchmark way of evaluating research or at least empirical research only been able to organize

08:05.280 --> 08:10.160
a large community around doing this work because we had these objective benchmarks and they're

08:10.160 --> 08:15.680
kind of asking this critical question of well after 10 million different researchers have trained

08:15.680 --> 08:21.120
their models on the see far turning set and evaluated on the the same exactly far holdouts that

08:21.120 --> 08:27.120
are we actually getting sort of like faithful sense of how these models generalize or

08:27.120 --> 08:33.280
or are we just overfitting that particular holdouts that basically we tapped out the the

08:33.280 --> 08:37.040
capacity of that holdouts that to tell us anything interesting about which bed models are better

08:37.040 --> 08:42.160
than what's other ones and so they did in both cases they undertook this really incredibly

08:42.160 --> 08:48.080
laborious effort to follow to the tee to the extent possible and you know in a good faith effort

08:48.080 --> 08:54.400
to create new holdouts sets for both both data sets and to basically say okay it's uh

08:54.400 --> 09:00.160
if instead of having 50,000 trained images and 10,000 holdout images if now suddenly in 2019

09:00.160 --> 09:05.600
an additional independent sample you know 10,000 holdout images fell out of the sky how well

09:05.600 --> 09:11.200
with all these models that have been basically all fit in the same kind of rat race of fitting

09:11.200 --> 09:15.680
it gets single as a how well would they generalize to this sort of new fresh data

09:15.680 --> 09:21.280
ostensibly from the same distribution and so a big part of the effort to went into these papers

09:21.280 --> 09:28.560
it sounds like was curating this new data set that to the best of their ability they demonstrated

09:28.560 --> 09:34.160
met you know felt were in line with the the distributions of the the popular data sets

09:34.160 --> 09:40.000
right so so in both cases there isn't actually an additional 10,000 seafarer images sitting

09:40.000 --> 09:44.880
around other aren't also an additional however many image net images sitting around so they

09:44.880 --> 09:50.080
actually had to go and sort of read the papers and say exactly what procedures that they followed

09:50.080 --> 09:55.680
to create this data set right so in the case of image net they first created some kind of ontology

09:55.680 --> 10:00.320
of of nouns based on the word net hierarchy and once they had these categories they used them

10:00.320 --> 10:05.520
to create Google image search these created set of candidate images which then were subjected

10:05.520 --> 10:10.320
to a crowdsourcing protocol that they used to basically you know to do something like identify

10:10.960 --> 10:15.440
several thousand images that were likely to be Persian cats but then you needed to show them

10:15.440 --> 10:20.880
to a crowd worker and say is this a Persian cat and then like this was their way that we're

10:20.880 --> 10:25.040
able to create the data sets so they tried to go through and this included you know creating

10:25.040 --> 10:32.080
sets of candidate images going through the crowdsourcing pipelines it was really you know interesting

10:32.080 --> 10:37.440
work I think sort of on a sort of qualitative sense it they're sort of taking a fresh look at

10:37.440 --> 10:43.440
this data set construction process as they're going through trying to follow in the footsteps of

10:43.440 --> 10:48.880
the original data set creators and they come up with a bunch of interesting observations like what

10:48.880 --> 10:53.920
precisely is you know you get the weird questions that you wouldn't think would be so

10:53.920 --> 10:58.960
vexing like what really is a basketball you know where you'll find some of these images that they

10:59.600 --> 11:04.400
annotators disagree on it's because maybe it's not a real basketball maybe it's a toy basketball

11:04.400 --> 11:10.560
like is it maybe it's a picture of basketball what precisely is going on there and and how

11:10.560 --> 11:16.960
off the thing could be categorized but then the sort of key result right is that besides this

11:16.960 --> 11:21.200
sort of really interesting aside about how the data set were collected some of what they believe

11:21.200 --> 11:25.840
were in the paper is in the number in some really nice talk that the others have given a long

11:25.840 --> 11:30.000
away an event right given I just talked about this work at the Simon's Institute last summer

11:30.560 --> 11:35.520
is that then what they do is they evaluate basically our current leaderboard right to take all

11:35.520 --> 11:40.160
these great models that are sort of the state of the art classifiers for these different data sets

11:40.160 --> 11:45.760
that you know have some kind of ranking among them and they evaluate them all on the new the

11:45.760 --> 11:51.280
new C bar test that right on the new image net test that the funny thing about it is that basically

11:51.280 --> 11:56.800
the model is all performed so here's here's like the like don't get too excited or too angry or

11:56.800 --> 12:03.920
whatever the first step is that the model is all performed of course right and because of that

12:03.920 --> 12:08.320
there's some you know this is sort of like usual crowd on Twitter the guy could be critical on

12:08.320 --> 12:15.920
Twitter but like like like the evidence say you know what it says and not you know the usual like

12:15.920 --> 12:19.760
whether it was like see like I told you deep learning doesn't work and sort of interpreted it

12:19.760 --> 12:24.400
that way like this is like a damning result about deep learning but actually this was a really

12:24.400 --> 12:30.080
positive result and that's because the other side of the result was that it's not yes all the models

12:30.080 --> 12:37.360
did worse but they were still arranged sort of in the same order so basically like the best model

12:37.360 --> 12:43.200
was a certain amount better than you know the fifth best model and that was also true on the new

12:43.200 --> 12:48.160
data right yeah so what it sort of showed was that like if you think of the leaderboard as being

12:48.160 --> 12:54.240
that's like step of like incremental progress towards like ever better models that the models that

12:54.240 --> 12:58.960
we thought were better because they did better on that C bar like the actual C bar holdout set

12:59.600 --> 13:06.480
actually were better on the new C bar holdout on the fresh C bar data or on the fresh

13:06.480 --> 13:12.400
image net data it's just that they were the entire benchmark like all the models in the benchmark

13:12.400 --> 13:17.040
sort of across the board were a bit worse but in the same order so the better models really were

13:17.040 --> 13:24.320
better and we kind of talk about this informally as the industry overfitting on image net the

13:24.320 --> 13:31.280
industry overfitting on these kind of standardized data sets and that this paper is is demonstrating

13:31.280 --> 13:37.200
is that perhaps that's true but at least there's still some order here right this stuff is really

13:37.200 --> 13:42.720
complicated and I think the general problem is that basically we have one way of talking about

13:42.720 --> 13:49.200
generalization normally and supervising which is from a finite sample to the the underlying

13:49.200 --> 13:53.760
distribution and then when people started saying like well what about on some different distribution

13:53.760 --> 13:58.000
does the generalization we start overloading the word generalize in different ways yeah and so

13:58.000 --> 14:01.920
that actually that part of the story also comes in here which is why it's interesting the question

14:01.920 --> 14:07.280
is then okay all the models are appearing the same order which means basically if nothing else

14:07.280 --> 14:12.880
it means we didn't scorch the test set to the point that we got no value out of continuing to

14:12.880 --> 14:18.320
do this leaderboard chasing right right but then the question is why did we do worse on the fresh

14:18.320 --> 14:24.000
data and there's two possible explanations right one possible explanation is that well we sort of

14:24.000 --> 14:31.200
are overfitting the image net cold outside but this is having sort of it somehow like an equal

14:31.200 --> 14:35.280
effect on all the models that they all deteriorate by the same amount or something like that

14:36.160 --> 14:42.080
but the other one which I believe the authors seem to believe is the most likely explanation

14:42.080 --> 14:46.400
and we've done some experiments it suggests that it's plausible the other explanation is that

14:46.400 --> 14:50.960
the reason why they do worse is because they're not actually see far data and they're not actually

14:50.960 --> 14:57.440
image net data so basically like despite the authors very best effort and very best like

14:57.440 --> 15:03.920
intention to in every way like you couldn't expect you couldn't hope for a better better behave

15:03.920 --> 15:09.520
deployment scenario right like imagine that you trained on some data and then you go like this

15:09.520 --> 15:14.720
is from like whatever your business is and you go to deployment time and basically at deployment time

15:14.720 --> 15:20.000
what you do is you face a team of academic researchers an absolute best faith effort to in every

15:20.000 --> 15:25.040
single way create data that statistically is identical to the data you saw during training

15:25.040 --> 15:29.520
like you can never hope for this in a real world scenario right exactly of all the ways you can

15:29.520 --> 15:35.200
go out of domain this is the most friendly right someone really trying hard and extremely

15:35.200 --> 15:40.240
confident president even PhDs who want nothing more than to like make this a valid scientific

15:40.240 --> 15:46.320
experiment do everything possible to make it so that this is i.e. like true holdout data and still

15:46.320 --> 15:50.800
despite their best efforts the distribution of data is slightly different because it's

15:50.800 --> 15:56.080
it's a few years later so maybe photographs look slightly different maybe compression algorithms

15:56.080 --> 16:00.400
using cameras that then get logged in image search or whatever is slightly different somehow

16:00.400 --> 16:05.120
you're getting you're getting new images in that that just are slightly different in distribution

16:05.120 --> 16:09.520
and that's enough to make your classifiers say 7% worse 8% worse something like that if we get

16:09.520 --> 16:13.280
the exact number yeah so that's having the interesting thing is that like there's all these

16:13.280 --> 16:18.560
different notions of generalization here one is that it seems that they actually like the benchmarks

16:18.560 --> 16:22.000
for whatever reason like this process with the only way that we're squeezing juice out of the

16:22.000 --> 16:26.400
holding set is changing our neural network architectures and hyper parameters doesn't leak as

16:26.400 --> 16:32.880
much information as in my fear the models do generalize as well the benchmark like leaderboard

16:32.880 --> 16:39.200
chasing kind of works better than we have any right to hope it would and at the same time also we

16:39.200 --> 16:44.160
sort of see how in the context of the experiments how brittle our classifiers are through distribution

16:44.160 --> 16:49.680
jet so I think the fact that all this is going on in these papers and and I have a fondness for

16:49.680 --> 16:54.080
this kind of paper that they're not introducing a new model they're not claiming a new leaderboard

16:54.080 --> 16:59.520
result but they're asking is a scientific question and I think doing that precise kind of hard work

16:59.520 --> 17:05.200
experiment to kind of produce that knowledge and that whole story kind of comes out of here

17:05.200 --> 17:09.680
are there other papers that come to mind that took similar approaches this year there were

17:09.680 --> 17:14.640
feet of different papers that have various different aspects I think sometimes this this is unique

17:14.640 --> 17:19.200
and then it's sort of saying do these models generalize do the very same distributions that they

17:19.200 --> 17:24.640
were trained on there are also a lot of papers that kind of look at various ways of saying basically

17:24.640 --> 17:30.240
these models were trained on some data set that was meant to sort of capture some kind of real world

17:30.240 --> 17:34.640
task you know like you had some kind of idea of the confidence we thought you were evaluating and

17:34.640 --> 17:39.600
a lot of papers are more like these performance on this data set really indicating that confidence

17:39.600 --> 17:44.720
like as as you might hope it would and there were a number of these over the last year and a half

17:44.720 --> 17:51.120
or so so I had a student did young's project we had a paper at EMNLP last year that basically was

17:51.760 --> 17:56.640
said something like how much reading does reading comprehension require and so we looked at these

17:56.640 --> 18:01.120
reading comprehension data sets and we found that essentially in reading comprehension you're given

18:01.120 --> 18:06.720
a question and a passage you have to produce an answer and so presumably producing the answer

18:06.720 --> 18:11.520
correctly if it's really testing reading comprehension it should require that you've actually read

18:11.520 --> 18:15.520
the question and should require that you've actually read the passage otherwise it's hard to

18:15.520 --> 18:20.240
claim that the what the model is doing is question answer this passive based question answer

18:20.240 --> 18:23.840
and right if it doesn't actually look at the passage or if you can't be sure that it actually

18:23.840 --> 18:27.760
looked at the passage or if you can't actually be sure that it read the question we had a paper

18:27.760 --> 18:32.640
basically just said hey how come nobody's run this baseline on all these data sets just training

18:32.640 --> 18:36.480
the exact same models but looking only at the passage and not the question just get it based

18:36.480 --> 18:41.200
on performance or just looking only at the question and looking at a randomized passage and so

18:41.200 --> 18:46.240
when we did that it turned out that you can match a lot of the best results reported in the literature

18:46.240 --> 18:50.080
either not looking at the question or not looking at the passage so it says something about

18:50.080 --> 18:54.320
does that exactly the same but it's a similar spirit and it's sort of saying what you know asking

18:54.320 --> 18:59.840
out a question about the trying to ask the sort of fundamental question about this data or another

19:00.640 --> 19:06.400
example was there was a English polia at a paper that did a similar thing with natural language

19:06.400 --> 19:11.840
inference this is a task where basically you have two sentences the similar kind of said you have

19:11.840 --> 19:17.040
two sentences one is called a trimis and the other is called a hypothesis and these could be in

19:17.040 --> 19:23.040
a relationship with each other which is either entailment contradiction or like a neutral posture

19:23.040 --> 19:27.920
so it's a three-way classification problem and so the trick is to sort of read the two sentences

19:27.920 --> 19:33.120
and to deduce like which one of the three best describes the relationship of these sentences

19:33.120 --> 19:37.840
to each other and they basically found that if you just the way these data sets had been created

19:37.840 --> 19:43.520
for that task you could often get the same performance as safety art models just by only looking

19:43.520 --> 19:47.440
at the hypothesis and ignoring the premise so you know someone might have thought that what they

19:47.440 --> 19:51.920
saw was this task of entailment but what they really did was made a sentence classifier

19:51.920 --> 19:54.960
yeah they made a sentence classifier and it turned out there were some clues like

19:54.960 --> 19:59.120
the hypothesis tended to have like negation words and it when it was a contradiction or something

19:59.120 --> 20:03.120
like that something so this kind of gets that high-level thing that we're talking about of like

20:03.120 --> 20:08.160
beyond supervised learning of hey from from a pure supervised learning standpoint what's wrong

20:08.160 --> 20:12.560
with what the classifier is doing there's nothing wrong with it right it's getting good predictive

20:12.560 --> 20:17.200
performance the problem is that what we want is something a little bit more than that we want

20:17.200 --> 20:21.200
something that's going to perform well in other environments but we know we don't know how to

20:21.200 --> 20:27.040
you know we're still very immature about how to incorporate that into our kind of learning setup

20:27.040 --> 20:32.160
right so most of what people know how to do is to say here's represented a data fit a model and

20:32.160 --> 20:35.760
and everything you know everything that we have a right to expect about the model is that it'll do

20:35.760 --> 20:40.960
well on new data from the same distribution and now we reveal that actually what we really wanted

20:40.960 --> 20:45.680
or what it really takes to do interesting things in the real world is is something that that other

20:45.680 --> 20:51.440
notion of generalization to different data sets to different environments yeah speaking of

20:52.080 --> 20:59.040
beyond supervised learning the next paper that you identified is the the birth paper technically

20:59.040 --> 21:06.720
I think we first saw that one late 2018 but we certainly came to understand it a lot more in 2019

21:06.720 --> 21:14.320
right so I think you know if we have our different pools of things going on people trying to get

21:14.320 --> 21:19.120
beyond the current paradigm and sort of counterargument that there's a lot of juice yet to be squeezed

21:19.120 --> 21:26.720
maybe births actually that latter category but it is basically the idea more you know the

21:26.720 --> 21:33.600
the highest level idea is just basically semi-supervised learning right and and I want to semi-supervised

21:33.600 --> 21:37.520
learning and machine learning right is the approach we say I have lots of unlabeled data a small

21:37.520 --> 21:42.240
amount of labeled data how can I make magic out of that right how can I how can I basically the

21:42.240 --> 21:46.800
the baseline would be I only have labeled data like ignore the unlabeled data it's just

21:46.800 --> 21:52.000
trying to classify or using the labeled data right or the unlabeled data so what can I do with

21:52.000 --> 21:56.880
that unlabeled data and deep learning gives you a nice kind of answer to it like with a lot of

21:56.880 --> 22:01.920
things which is well use the unlabeled data to learn the representation the earliest forms of this

22:01.920 --> 22:09.280
that I saw where I think around 2015 or so I think quickly in Andrew die had a paper that was doing

22:09.280 --> 22:14.240
as I'm sure I bet your papers that predated that's one that I remember were the they were doing

22:14.240 --> 22:19.440
stuff like just train a language model on a bunch data and then fine tune the language model to

22:20.240 --> 22:26.080
make predictions on your downstream classification task now the models they were using weren't that big

22:26.080 --> 22:31.280
they weren't using tens of TPUs they were using you know probably they were still probably I don't

22:31.280 --> 22:34.640
even know if tenser flows out they might have written their code into the auto like I was at time

22:34.640 --> 22:39.120
and and I don't think they were using a specially enormous data set or whatever but like you know

22:39.120 --> 22:43.040
that key idea has been there for a while we've talked about you know training auto encoder is

22:43.040 --> 22:49.200
fine tuning to you know supervised task here that the idea basically put if the next word

22:49.200 --> 22:55.040
and then Elmo came out basically 2018 it's basically the same exact idea that there's a

22:55.040 --> 22:59.040
slight wrinkles the wrinkles are on the details right like they say we train a forward

22:59.040 --> 23:04.400
language model from left to right backwards language models from right to left you can catenate them

23:04.400 --> 23:08.160
and then you take some mixture of their representations but the qualitative idea is

23:08.160 --> 23:14.000
train a giant language model on more data play around with some some variants but that are really

23:14.000 --> 23:18.880
not conceptually different they're just kind of you know different levers different offsiturn

23:19.440 --> 23:23.920
and basically see what could do the best performance on the number of downstream tasks and

23:23.920 --> 23:28.160
Elmo was this break your moment in that whether or not you found a conceptually interesting

23:28.160 --> 23:34.640
every single like virtually every single NLB task experience some significant bump in accuracy

23:34.640 --> 23:38.640
in those moments are not so common right where you just say oh there's a new state of the art

23:38.640 --> 23:44.560
for every single task effective tomorrow and so Bert it was Elmo didn't have that much time

23:44.560 --> 23:52.240
in the sun before the the mubbid meme took off and Bert which I think has sort of stayed now it's

23:52.240 --> 23:57.440
been there've been enough variations on Bert but not enough that anybody's willing to you know

23:57.440 --> 24:01.520
this is the same thing happens image of rights like there's been slight changes on resonance

24:01.520 --> 24:06.560
or whatever but for the most part everyone still uses resonance now four years later yeah Bert I

24:06.560 --> 24:10.960
think was this moment where you know they made a few more modifications one key modification was

24:10.960 --> 24:17.360
that they use transformers instead of LSDMs like in Elmo another big change is that for their

24:17.360 --> 24:23.520
modeling objective instead of saying that what they're going to do is like auto regress from left

24:23.520 --> 24:27.440
to right and just try to predict the next word given all the previous ones they do that sort of fill

24:27.440 --> 24:32.160
in the blank type of objective where they mask out certain words and then try to predict which words

24:32.160 --> 24:39.040
were masked out but Bert gave us absolutely massive boots to sort of after after that had already

24:39.040 --> 24:44.400
happened shortly before sort of did it again across the board and at this point you basically cannot

24:44.400 --> 24:48.560
publish a paper in natural language processing without building on top of it you know the question

24:48.560 --> 24:52.960
that then arises is sort of what is the sort of two perspectives one is to say if you want to do

24:52.960 --> 24:58.720
interesting work in this field you have to go to Google and get a TPU farm and this is what you

24:58.720 --> 25:03.600
have to do to to move the state of the art the other way to think about it is to say that that's no

25:03.600 --> 25:09.600
longer the interesting part that the architecture is sort of something that someone will come out with

25:10.160 --> 25:14.320
there's Bert on those Roberta someone will come out with yeah I think they've already come out with

25:14.320 --> 25:20.240
every other method but you know right so someone will go out with something did you create a research

25:20.240 --> 25:24.640
do you say that that is the creative research do you say hey that is the tool upon which anything

25:24.640 --> 25:30.400
that I want to do that's just one part of what I do is this function setting and if I'm doing that

25:30.400 --> 25:34.000
for natural language that this is the base model that I use and someone will come up with another

25:34.000 --> 25:38.240
base model but that you know on one hand it takes a massive amount of resources to train Bert on

25:38.240 --> 25:42.880
the other hand it takes very few resources comparatively to you to fine tune Bert to sundown

25:42.880 --> 25:47.920
stream pass it's kind of opening this interesting wedge of life we're sort of now in this position

25:47.920 --> 25:53.600
where sort of the only way you can you can do at least like leaderboard competitive and alp

25:53.600 --> 25:58.320
work is the build on top of one of these models what's next up in your list of tapers

25:59.040 --> 26:05.520
yeah so kind of continuing this theme of going beyond the standard just like I have some offline

26:05.520 --> 26:11.840
data I say the model I evaluate how predictive it is on some holdout that right there's sort of

26:11.840 --> 26:16.000
a number of things that we're interested in going beyond there one is under what settings can you

26:16.000 --> 26:22.560
detect or adapt to distribution shift the other side is I think the social component which is

26:23.200 --> 26:26.560
you know we're always talking about using machine learning and it came up when we were done

26:26.560 --> 26:30.320
of fairness before right we're always when we talk about machine learning technically we just

26:30.320 --> 26:34.480
use the language of prediction I have some data and make a prediction how accurate it isn't like

26:34.480 --> 26:39.440
the notion of accuracy assumes like a fixed reference distribution right accuracy is a self is

26:40.240 --> 26:45.600
is a is a probable statement what fraction of the times are we right assuming that there is some

26:45.600 --> 26:50.400
objective background distribution that's generating our data but the reality is that you've

26:50.400 --> 26:53.520
been look at what people say they're doing with machine learning like we want to build self-driving

26:53.520 --> 26:59.280
cars we want to make your doctor in AI we want to do whatever all of these tasks involve not just

26:59.280 --> 27:04.800
making predictions but actually driving some kind of decisions in some real world process in a lot

27:04.800 --> 27:09.280
of those cases you know you can think of it like like Google search right what happens the moment you

27:09.280 --> 27:13.600
change to Google search algorithm right the very instant you change to Google search algorithm like

27:13.600 --> 27:18.000
the reddit message boards light up and people are going to start adapting they're going to start

27:18.000 --> 27:22.320
chatting about strategies to take advantage of the new algorithm in some way or another right

27:22.320 --> 27:27.520
and so basically you're making decisions and so our predictions are informing decisions we usually

27:27.520 --> 27:32.960
kind of alive that step but once you turn a prediction into a policy and you deploy it and like a

27:32.960 --> 27:37.840
real social setting where you're not the only agent the next thing that happens is everybody else

27:37.840 --> 27:42.400
is going to start updating their behavior which fundamentally shifts the distribution

27:42.400 --> 27:46.800
against what you're trying to make predictions right exactly like the same individual

27:46.800 --> 27:50.880
who may or may not have the same but it's going to start changing their behavior right people

27:50.880 --> 27:54.480
in aggregate are going to change their behavior which is this is going to somehow manifest in the

27:54.480 --> 27:59.120
data that you see in the next round right I don't think there are necessarily cleanly separated

27:59.120 --> 28:04.400
rounds but we could idealize it that way for you know like analytic purposes so there are

28:04.400 --> 28:07.600
couple papers I think I think I think sort of two groups of people have been thinking really

28:07.600 --> 28:12.560
seriously about these kinds of problems I just wanted to highlight their work is one is Lily

28:12.560 --> 28:18.560
who who's a PhD student at Harvard I believe in like something crazy like I think she's like

28:18.560 --> 28:24.240
joined philosophy and applied math but works with computer scientists something like this but anyway

28:24.240 --> 28:28.880
I think she's done really fantastic work for a long time but specifically I've been looking

28:28.880 --> 28:32.640
at problems with this flavor which is sort of like oh especially in a context of fairness which is

28:32.640 --> 28:36.480
okay you're proposing that we make predictions in this particular way or the design classification

28:36.480 --> 28:41.200
this way but then what happens in the next step right well we're always like missing the next part

28:41.200 --> 28:45.920
of the picture is like okay so then then we make those decisions then how do people think their

28:45.920 --> 28:50.800
behavior or how does this influence censorship or like the data that we see in the next round and

28:50.800 --> 28:55.200
then you know basically what are we going to see next and another group of people that have worked

28:55.200 --> 28:59.840
on this is more it's hard with and he's working on it for a long time I think even before he

28:59.840 --> 29:05.520
joined faculty and now is a faculty member with his students Smith and Millie among others

29:05.520 --> 29:10.560
and so they work you know with the student Lydia Lo that Lydia Leo they won the best paper

29:10.560 --> 29:14.960
at ICML a year ago and this was about the delayed impacts of fair machine learning

29:14.960 --> 29:18.640
and so the two papers that I thought you're just seeing which were both from this year at the

29:18.640 --> 29:24.000
FastStar conference so this is the fairness accountability and transparency conference these

29:24.000 --> 29:28.800
were sort of two contemporary works both called one is called by Lily who called the disparate

29:28.800 --> 29:34.240
effects of strategic manipulation one by Smitha called Smith Millie called the social cost of

29:34.240 --> 29:38.720
strategic classification but they're both addressing the setting more basically someone all the

29:38.720 --> 29:44.320
individuals are characterized by some covariates then there's a firm that drives some kind of decisions

29:44.320 --> 29:49.520
by fitting some kind of classifier which assigns people to predictions they've positive or negative

29:49.520 --> 29:56.640
based on their covariates but then in the very next round basically individuals are going to they

29:56.640 --> 30:01.360
have some power to sort of manipulate the value of their covariates so I can motivate an example

30:01.360 --> 30:06.720
in Lily's paper that I think is a very good one is think about like school admissions

30:06.720 --> 30:13.280
and people notice that maybe people who get better SAT scores tend to do you know in somehow

30:13.280 --> 30:18.560
this tends to be predictive of success in college or whatever but if you then lean more heavily

30:18.560 --> 30:22.720
on the SAT scores or you incorporate this as a more prominent category and how you make your decisions

30:22.720 --> 30:27.600
if you do admit the very next thing that happens is that people who have some resources like

30:27.600 --> 30:31.360
as much as the college board wants you to think that these things are not manipulable they are

30:31.360 --> 30:37.040
and what people are going to do is go out and find a way to spend money to prepare for the test

30:37.040 --> 30:41.840
and when they start preparing for the test the same individuals who you know

30:41.840 --> 30:45.360
they give an individual by preparing really hard for the SATs doesn't necessarily become

30:45.360 --> 30:49.600
fundamentally a better student but they do increase their test score right and so this is like the

30:49.600 --> 30:54.800
the key idea is that you have someone making decisions but you have other people who in response

30:54.800 --> 30:59.840
are able to sort of invest in manipulating their their features or their covariance to influence

30:59.840 --> 31:05.440
their prediction so someone right the decision maker has to publish their model then other people

31:05.440 --> 31:10.320
get to sort of respond to it strategically and then both of these papers they they look both

31:10.320 --> 31:15.520
as dynamic but also in the context of fairness so so one way that they look at things is this

31:15.520 --> 31:20.240
context of well what if not everyone has access to the same resources to manipulate their features

31:20.240 --> 31:25.440
and I think that the SAT example is a really good example of that right where you have people

31:25.440 --> 31:29.840
have spent really exorbitant sums on test prep I remember I had friends when I was in

31:29.840 --> 31:33.760
ecolumbia there was some service I don't want to say what they are maybe they'll help my friend

31:33.760 --> 31:37.680
who works with them you know there was some service they go to his fault so I'm like kind of like

31:37.680 --> 31:44.000
posh like Manhattan test prep service the chart I think the tutors made I'm sure the tutors were

31:44.000 --> 31:48.480
getting a small slice of the cut and they were getting like 100 200 bucks an hour something

31:48.480 --> 31:52.480
absolutely ridiculous I didn't think the requirement was you had to have a perfect SAT score

31:52.480 --> 31:56.400
like the tutors so that was like part of their limit but you know you have these these

31:56.400 --> 32:00.880
services and people will really spend enormous amounts of money and then you rate you create

32:00.880 --> 32:05.280
this dynamic database but people who have lots of resources to influence their decisions will do

32:05.280 --> 32:11.520
so people who have less will you know can't really compete and kind of analyzing this dynamic of

32:11.520 --> 32:16.560
sort of what happens depending on how you make your predictions and how people adjust strategically

32:16.560 --> 32:22.880
and they also both came up with interesting scenarios where sort of often everyone could be made

32:22.880 --> 32:27.920
worse off and like the institution making decisions becomes better off like in the setting of

32:27.920 --> 32:32.560
strategic manipulation like a somehow like works against the this might have something to do

32:32.560 --> 32:36.480
the fact that like you're sort of assuming like a monopolistic party on one end and like competitive

32:36.480 --> 32:41.040
parties on the other but you can have these settings where basically lots of people are competing

32:41.040 --> 32:45.440
with each other to manipulate their predictions and net overall like they're incurring some cost

32:45.440 --> 32:50.320
for doing so and are made worse off and the institution on the other side sort of

32:50.320 --> 32:54.720
is made strictly better off. I don't know that either and I think both papers are addressing a

32:54.720 --> 33:00.000
fairly idealized setting and it's not clear that they're giving you a a tool in the algorithmic

33:00.000 --> 33:05.280
sense that you could go out and sort of analyze it and you know directly say a lending decision

33:05.280 --> 33:09.440
or school admissions or not that I would endorse you know making those decisions based on

33:09.440 --> 33:14.240
machine learning classifier in the first place but I think they are giving and I think this is

33:14.240 --> 33:19.920
true of a lot of this stuff as we start going sort of towards causality and for thinking about

33:19.920 --> 33:25.200
economic mechanisms is there at least giving us I think a framework for thinking coherently

33:25.200 --> 33:29.840
about the problem and I think the problem is you know when we don't do that work you wind up with

33:29.840 --> 33:34.480
people pretending that these are just that the more or less we can just continue to think of these

33:34.480 --> 33:41.200
things as prediction and that you know just like there's some kind of really simple trivial fix

33:41.200 --> 33:46.240
that I think it's around us and I think these are you know nice steps towards thinking coherently

33:46.240 --> 33:51.760
about these problems. Yeah it strikes me that part of the issue in the examples you described

33:51.760 --> 33:59.280
is kind of the gap between the real world thing that you're trying to optimize and how you formulate

33:59.280 --> 34:06.720
that as a machine learning problem does the the framework that they provide in their discussion

34:06.720 --> 34:12.880
help provide additional tools or ways to think about that core problem formulation question.

34:12.880 --> 34:20.400
I think what it gives us at least is a way of stepping back and appreciating at least one component

34:20.400 --> 34:24.560
of yeah there's this thing that is always omitted which is like what is our data the what is the

34:24.560 --> 34:29.680
data where does it come from and to some extent you know when the data represents individuals the

34:29.680 --> 34:34.720
data is the data is subject to choices that they make and some of those choices are directed

34:34.720 --> 34:40.480
specifically to to have them classified and treated it in a certain way yeah and I think just like

34:40.480 --> 34:46.560
that very sort of realization starting to model it into starting to create sort of mathematical models

34:46.560 --> 34:51.360
of this interaction as itself you know that the it's a conceptual tool you know again I don't

34:51.360 --> 34:54.480
think it's a practical tool like I don't need to give you an algorithm is it oh just run this

34:54.480 --> 34:59.440
on the bank and then it'll it'll tell you what is you know and in fact you need to know things

34:59.440 --> 35:02.880
going into it right like you need to know what are the cost of manipulating different features

35:02.880 --> 35:08.000
some things that might not be specified in the data but it's at least sort of starting to cast

35:08.000 --> 35:14.160
these problems in a way that is sort of richer in captors these things that you know we're sort of

35:14.160 --> 35:18.400
naÃ¯ve like ignoring like if nothing else is about like what is this conceptual tool to do we see

35:18.400 --> 35:22.560
all these people talking about like just using big data to do all kinds of whatever I have access

35:22.560 --> 35:28.400
to someone's Facebook likes and this and that and I can say whatever I can make whatever kinds

35:28.400 --> 35:32.480
of decisions based on that data this is giving you a reason not to right you know if you start

35:32.480 --> 35:37.680
thinking it if only in just like the very consideration of these interaction dynamics because

35:37.680 --> 35:42.960
well why should you use Facebook likes well because if somebody knows that their career and all

35:42.960 --> 35:48.320
these other sort of choices that are really consequential to them are being driven based on

35:48.320 --> 35:53.600
these like really easily manipulable features they're going to start changing that behavior right

35:53.600 --> 35:59.600
and so there's multiple sort of aspects of this right one is just the very consideration of strategic

35:59.600 --> 36:05.120
behavior and then the other side is a sort of fairness implications when you're making decisions

36:05.120 --> 36:09.680
about people and then you'll have different groups of people with different abilities sort of

36:09.680 --> 36:14.480
disparate ability can manipulate yeah right you know it's like a new craze of joy somewhere you

36:14.480 --> 36:18.480
know maybe just rich people get whatever classification they want or something like that

36:19.360 --> 36:26.080
okay cool the next paper on your list kind of returns to this theme of generalization

36:26.080 --> 36:32.080
and machine learning it's the invariant risk minimization paper tell us why that paper made your

36:32.080 --> 36:37.920
list yeah so this is a paper that I don't know if they've hit the exact solution to these kind

36:37.920 --> 36:43.280
of family problems but I think was a really solid and interesting attempt that I think got a lot

36:43.280 --> 36:48.320
of people thinking and basically you know they're they're just sort of a connection between prediction

36:48.320 --> 36:56.400
out of domain between causality and some of this is sort of well known but one aspect that sort of

36:56.400 --> 37:02.000
hasn't been folded into it is is representation learning so so the way that causality is like

37:02.000 --> 37:06.480
the prediction out of domain is that you say well my source data came from some distribution my

37:06.480 --> 37:11.840
target data came from a different distribution what changed presumably in some ways that these two

37:11.840 --> 37:16.000
different distributions are related to each other right otherwise like why should I expect that I

37:16.000 --> 37:20.320
could apply a class by returning on one to the other causality gives you one way of thinking about it

37:20.320 --> 37:26.000
like there's you know there was an intervention of some sort something you know one one variable

37:26.000 --> 37:30.160
that used to take its data in some organic way has now been intervened upon and so but otherwise

37:30.160 --> 37:36.000
the process is the same but this paper starts getting at is this idea of well thinking about

37:36.000 --> 37:40.720
predicting well on a range of environments in terms of the representation that you learn which is

37:40.720 --> 37:46.320
the idea that came up in some earlier more a classical domain adaptation work like from Shibon David

37:46.320 --> 37:51.600
but it's developed here and some some novel series developed in the context of classical models

37:51.600 --> 37:57.040
but then also some interesting experiments in the context of deep models but the core nugget

37:57.040 --> 38:04.640
here is this idea that what is the ideal representation that you should learn and the intuition the

38:04.640 --> 38:11.760
thing that they roll with is this idea that a good representation is one such that the optimal predictor

38:11.760 --> 38:16.320
built on top of that representation should be the same for all environments so if you've got a

38:16.320 --> 38:20.880
bunch of different environments or a family of different environments you want to learn a representation

38:20.880 --> 38:27.040
such that the predictor that you would then fit on top of that representation with the sort of

38:27.040 --> 38:34.640
environment diagnostic and what is environment in this context is it the the the world that generates

38:34.640 --> 38:43.120
the distribution of your input and targets that's right so this is for example like I might have

38:43.120 --> 38:47.840
to see you had this example of yeah classical example of like if I see fish they're always

38:47.840 --> 38:51.360
against the water in the background and I see something else that's always against certain

38:51.360 --> 38:57.200
back on these things are correlated but they're also you know exist I could I could also create a

38:57.200 --> 39:02.880
data set where I'd have like the fish is not against is you know that you can try for example

39:02.880 --> 39:06.960
but you need to get the idea where that correlation would be broken yeah right and the reason

39:06.960 --> 39:12.240
why is because it is not you know like the the key fundamental difference you know there there

39:12.240 --> 39:16.080
exists some environment where it's maybe even like anti-correlated or something like that so

39:16.080 --> 39:20.640
that even though like if you if you're just being supervised learning you can be very well by just

39:20.640 --> 39:26.000
using the the water background to produce the fish like in the worst case in the environments where

39:26.000 --> 39:30.400
all the fish are on land and all the you know the zebras are in the ocean or something then you're

39:30.400 --> 39:35.680
going to do really badly right right and so this paper kind of simply is saying we want a classifier

39:35.680 --> 39:41.280
that can perform equally well on fish whether there's water in the background or they're in an aquarium

39:41.280 --> 39:46.480
or they're mounted on a wall or whatever yeah I think if this is you know we rounded off a lot of

39:46.480 --> 39:52.800
the mathematical detail but I think we are you're getting at the ideas and in the key ideas like the

39:52.800 --> 39:58.080
deal representation learning that you should produce this representation such that the predictor

39:58.080 --> 40:02.160
on top of that is the same across all environments then there's these questions of well how many

40:02.160 --> 40:07.840
environments do I need to see and what kind of like under what like conditions have I actually

40:07.840 --> 40:15.040
covered adequately you know all the environments that I would like reasonably anticipate and in this case

40:15.040 --> 40:21.440
they sort of have a in the case of linear models have like a very sort of well developed and

40:21.440 --> 40:28.320
kind of involved theory but sort of the questions wide open about when this makes sense and how

40:28.320 --> 40:33.520
many environments we would need to have represented in our sort of training system in their set like

40:33.520 --> 40:37.680
your training set is you have multiple distinct environments and from each environment you have a

40:37.680 --> 40:42.960
data set they could say something that if if each of these environments is related to each other and

40:42.960 --> 40:48.240
some kind of complicated way then you could say that you know for fitting linear models that that

40:48.240 --> 40:54.400
we you know we need however so many environments in order to find this in varying predictor

40:54.400 --> 40:59.360
but things get a little bit more complicated when we start stepping towards you know I think the

40:59.360 --> 41:03.760
kinds of data and kinds of models that we want to work with which are in general not linear

41:03.760 --> 41:07.600
which is why we care about the representation learning at least in context of deep learning

41:07.600 --> 41:14.880
and so is the the contribution of this paper kind of a theoretical framework for these

41:14.880 --> 41:22.720
invariant risk minimization models and you know how to know if you have one or is it something

41:22.720 --> 41:28.080
more concrete where I can go create an invariant risk minimization model given the tools of the

41:28.080 --> 41:35.120
paper the the contribution right is I think there's this principle for how do you create a predictor

41:35.120 --> 41:41.280
there is a set of simple environments where you could argue that in a more like theoretical

41:41.280 --> 41:44.400
way that you're doing the right thing and then a set of empirical experiments and I believe

41:44.400 --> 41:48.960
doing empirical experiments are still very toy it's something like I think they look at something

41:48.960 --> 41:55.200
like like image and ad s phn images things like this us or not in a genesis image I'm like

41:55.200 --> 42:02.000
amnest where where you have some kind of distractor like there's colors is is associated with

42:02.000 --> 42:07.120
the category but this is not necessarily going to you know the degree to which color is associated

42:07.120 --> 42:11.920
but the category is changing and obviously you know the correlation the hope is that you know

42:11.920 --> 42:15.680
you're going to produce a model that depends less on color and then it's going to do better you know

42:15.680 --> 42:20.320
in the test environments where you know those the the the correspondence between color and

42:20.320 --> 42:27.600
category are that you've come to sort of rely on don't actually stand up unhauled out data okay

42:27.600 --> 42:32.480
so maybe you maybe case on price on the data sets where that is a predictive signal for not

42:32.480 --> 42:36.720
relying on it but you produce backup by producing a model that doesn't rely on color then

42:37.280 --> 42:40.880
if you go find a world where you know the correspondence between color and images is

42:40.880 --> 42:44.720
completely flat then you know your model will be robust something like that yeah there's

42:44.720 --> 42:49.840
another interesting feature of this paper that you pointed out that you don't see a lot of

42:49.840 --> 42:54.960
in paper there's a bit of a secratic dialogue in the back of the paper yeah you know I think I'm

42:54.960 --> 43:01.280
just going to have to lead that to the to the listener to read through this there's form their own

43:01.280 --> 43:07.440
opinions it's certainly different you're not it's not the typical section six in a technical

43:07.440 --> 43:13.360
machine learning paper uh-huh uh-huh I think they maybe wanted to keep with the theme of uh

43:13.360 --> 43:18.160
Greek since there's a lot of Greek symbols earlier in the paper yeah you know the dialogue does

43:18.160 --> 43:22.640
jump the shark at that but it's interesting that throughout the paper you know the authors and I

43:22.640 --> 43:27.920
think this is just they they are they are thinking seriously about these problems about causality

43:27.920 --> 43:34.240
and invariance and are in dialogue a little bit more than maybe you usually encounter initially

43:34.240 --> 43:39.520
in learning with the um philosophy of science which is not to say that you would expect to find a

43:40.160 --> 43:45.760
you know a forpage long secratic dialogue in a modern work on philosophy of science either

43:45.760 --> 43:52.400
but you know that there is uh the treatment of of the kind of like core philosophical questions is

43:52.400 --> 43:58.720
I think sincere and serious and and that the authors are unusually broad in their reading which I think

43:58.720 --> 44:04.640
you know in general is really nice to see mm-hmm so the next paper on your list is one that I was

44:04.640 --> 44:11.760
excited that you included and it's one that I've seen pop up quite a bit of late it's the your

44:11.760 --> 44:16.720
classifier is secretly an energy-based model and you should treat it like one paper uh what's that

44:16.720 --> 44:24.320
one about yeah so this is a paper um that I just came across because uh someone I know um left

44:24.320 --> 44:30.560
to comment and pointed me to it but this this paper was I just accepted with an oral presentation

44:30.560 --> 44:40.560
at ICLR and the key idea is basically so an energy-based model basically it is a model that

44:40.560 --> 44:47.280
assigns scores associated with each input and these scores correspond like unnormalized probability

44:47.280 --> 44:52.720
densities and you know there there's a literature on discriminative models where people try to

44:52.720 --> 44:57.600
produce classifiers that do really good at assigning conditional probabilities of predicting labels

44:57.600 --> 45:02.880
given some input uh there's a corresponding literature of people taking on you have to be very

45:02.880 --> 45:07.120
careful with the word generative modeling because there's a set of tasks associated with generative

45:07.120 --> 45:10.480
modeling that I think most of what we're doing almost what we're calling generative modeling in

45:10.480 --> 45:15.040
the context of GANS not really addressing but using it lightly in that sense of like trying to

45:15.040 --> 45:20.640
learn explicitly or implicitly uh you know uh a probability density over over your inputs over

45:20.640 --> 45:25.040
over your x over your images there's this other literature that does that right including

45:25.040 --> 45:31.360
variational auto encoders including GANS and there have been plenty of approaches on that side

45:31.360 --> 45:37.600
that have adopted this sort of energy-based approach to incorporating it either into the GANS

45:37.600 --> 45:43.200
framework or whatever but sort of there's always this kind of tension that sort of it seems that

45:43.200 --> 45:48.000
like when you train a generative model you know a generative models are capable of performing

45:48.000 --> 45:53.520
prediction but it seems like you never do as well as when you just train a proper discriminative

45:53.520 --> 45:57.680
model itself right if you want to do prediction you know sort of like the theme of like if you

45:57.680 --> 46:02.560
want to do prediction the takeaway so far is like you're better off straight up training a

46:02.560 --> 46:07.120
discriminative model not training a generative model and then trying to trying to run it in some

46:07.120 --> 46:12.240
kind of manner to to generate predictions and with this this model this we says that it sort of

46:12.240 --> 46:17.440
just says hey let's just take a discriminative model like a standard image classifier and you can

46:17.440 --> 46:21.040
sort of interpret it you're keeping the architecture and everything as you would for just uh

46:21.040 --> 46:25.600
sort of off the shelf the discriminative model you are changing uh the learning objective and how

46:25.600 --> 46:31.200
you sample from it but the key idea here is to sort of say that you feed an input to a discriminative

46:31.200 --> 46:34.960
model you can get out some logits then you run the softmax function it turns you logits into

46:34.960 --> 46:39.680
predictive probabilities and you also have like an extra degree of freedom there which is that

46:39.680 --> 46:43.760
if you were to like move all your logits up by some fixed amount and move it down by some fixed

46:43.760 --> 46:48.400
amount you would get out the same softmax probabilities and that's just because sort of um

46:49.280 --> 46:54.080
all of your probabilities need to sum up the one so uh there's actually a degree of freedom there

46:54.080 --> 46:57.760
so it says hey we'll just keep the entire architecture we'll keep all the parameters we keep

46:57.760 --> 47:00.720
everything the same degree of discriminative model but what we're going to do is we're going to

47:00.720 --> 47:07.440
basically interpret sort of pre softmax logits as sort of unnormalized probability densities

47:07.440 --> 47:12.560
like probability of x and y and then they're going to train in uh you know the manner of an energy

47:12.560 --> 47:17.840
based model sort of doing a train with like two objectives one is to just maximize the classification

47:17.840 --> 47:22.320
performance using like the standard cross entropy loss and they're also going to train using

47:22.320 --> 47:29.440
this like energy type objective to make it so that basically the sum of the pre softmax logits

47:29.440 --> 47:34.240
has like a sort of high energy for in distribution data and hopefully you know low energy for

47:34.240 --> 47:38.720
out of distribution data and what what it ends up being really interesting about this is that

47:38.720 --> 47:43.520
in this tension between discriminative and generative models you find basically like usually

47:43.520 --> 47:47.360
if you train a generative model you do good at the things that you want the generative model do

47:47.360 --> 47:52.880
you do bad at things uh you do bad at prediction even when you try to you know leverage your generative

47:52.880 --> 47:57.440
model in the predictions and vice versa when there's ways of using it the discriminative models

47:57.440 --> 48:02.320
generative model they're usually not as good as if you just did uh you know use them more familiar

48:02.320 --> 48:07.920
I mean I'm using the word generative model here but like gans or something yeah yeah and so the end

48:07.920 --> 48:12.720
result here which you know if this stands up I think it's super exciting is that they basically

48:12.720 --> 48:19.760
have a single model that they have trained and this model ends up being good at classification

48:19.760 --> 48:25.040
it also um basically you can now sample from it by basically doing something uh calls the

48:25.040 --> 48:29.840
caustic gradient the launch of end dynamics and so you basically you have some initialization

48:29.840 --> 48:34.640
at the input and you take uh gradient steps over the energy function with some amount of noise

48:34.640 --> 48:39.600
injected and this is sort of equivalent to like sort of sampling if you run it for a long time

48:39.600 --> 48:43.040
and so there's other people out there that are much better experts than I am about uh

48:43.040 --> 48:48.720
London dynamics but the the interesting thing is that if you sample from this thing the samples

48:48.720 --> 48:54.320
that come out end up doing better or like as well are better on the generative model type

48:54.320 --> 48:59.200
objectives as a lot of generative models out there the model does you know still does really good

48:59.200 --> 49:05.360
as a discriminative model and then it turns out that the uh probabilities that you get out of this

49:05.360 --> 49:11.760
model tend to be uh pretty well calibrated whereas in general a neural network probabilities

49:11.760 --> 49:15.280
that you know conditional probabilities assigned to class is tend to be way overconfident

49:15.280 --> 49:19.040
the predicted probability of like you know whichever class you know is the art max of the

49:19.040 --> 49:23.840
salt max tends to be overconfident and then this model also turns out that these these scores

49:23.840 --> 49:28.480
is unnormalized densities that basically give higher probability to in some you know things that

49:28.480 --> 49:34.080
you know you think are high density data and lower lower unnormalized probabilities things that are

49:34.080 --> 49:40.560
you know low density data turn out to be good on a range of empirical benchmarks for detecting

49:40.560 --> 49:46.400
shifts like out of distribution and then finally it seems that the models that they get out of it

49:46.400 --> 49:53.840
also do a strangely good job as evaluated against they do slightly worse than the state of the art

49:53.840 --> 49:58.400
for adversarial robustness and so it's a kind of interesting and exciting here is hey this is

49:58.400 --> 50:02.640
the model that's doing well for discriminative modeling it's getting generative performance that

50:02.640 --> 50:08.880
is similar to uh GANS at least by determined by I think like Frichet inception distance or one

50:08.880 --> 50:13.440
of those or inception similarity one of those you know metrics that they use it's doing well

50:13.440 --> 50:18.160
in calibration which I'm a little bit fuzzier on why that should be on in whether that

50:18.160 --> 50:22.640
that there's any principle that this classifier should be calibrated but perhaps interesting that

50:22.640 --> 50:27.840
it tends to be at least empirically on the data sets that they've looked at it's getting adversarial

50:27.840 --> 50:32.480
robustness and it's useful for out of distribution shift detection so there is something kind

50:32.480 --> 50:36.880
of interesting here and it'll be really interesting to see over the next few months do

50:36.880 --> 50:40.720
does it turn out that basically you know that there's a long history of someone sort of

50:40.720 --> 50:44.800
coming up with a new kind of model that appears to be adversarial robust but that's just because

50:44.800 --> 50:50.960
there's some you know as of yet there's some smart way to attack it that they hadn't considered

50:50.960 --> 50:54.880
because they're proposing the new model we hadn't set the adversaries against it yet

50:54.880 --> 51:00.480
right the adversary is being the graduates right exactly so yeah so I think that you know there

51:00.480 --> 51:04.240
is something interesting here and I think that it also captains the maybe a broader current

51:04.240 --> 51:10.240
in the literature so there's these other papers by Alex Majrey's group at MIT where they've shown

51:10.240 --> 51:15.920
things that adversarial robust models have all kinds of properties that you would normally associate

51:15.920 --> 51:21.280
with GANS or like you know basically models performing this if we don't want to use the

51:21.280 --> 51:26.400
abuse or generative any more because that synthesis is high pass right and so you could do in

51:26.400 --> 51:31.200
sailing with an adversarial robust model or you could generate images or when you do a targeted

51:31.200 --> 51:36.240
adversarial attack that turn an image from one class to another so basically if you take a picture

51:36.240 --> 51:41.440
of a kitten and you try to turn it into a picture of a banana a picture that's classified as a

51:41.440 --> 51:46.880
banana right exactly if you do this with a vanilla classifier what you get is a picture that looks

51:46.880 --> 51:51.760
like a slightly noisy kitten that's misclassified as a banana would you do with the adversarial robust

51:51.760 --> 51:57.360
model you update the pixels of a kitten to look like banana so instead it looks to the classifier

51:57.360 --> 52:03.520
like a banana the weird thing is that it actually makes it look like a banana so like the weird

52:03.520 --> 52:08.000
thing is it's sort of of all the things that could have made it look like it could have made it

52:08.000 --> 52:12.720
just look like something so bizarre that it was not an image in which case like we don't have any

52:12.720 --> 52:17.440
right to expect the model classified as a banana or as a not banana right it could have just made

52:17.440 --> 52:22.880
it look like total garbage and that would sort of it's not clear why the adversarial objective

52:22.880 --> 52:26.800
would have a problem with that as long as it's sufficiently different from the input image right

52:26.800 --> 52:30.880
along it doesn't look like a cat the weird thing is that it ends up making it actually look visually

52:30.880 --> 52:35.200
like a kitten and so there's already this evidence that like some of these things that we maybe

52:35.200 --> 52:40.960
didn't have any good reason to believe a related like robustness and this sort of like image synthesis

52:40.960 --> 52:46.240
capability are actually related to each other and in a way that still right now I think it's

52:46.240 --> 52:50.960
floating around at the level of intuition but it'll be interesting to see if we can kind of like

52:50.960 --> 52:55.360
unify why these tasks are related to each other vis-a-vis you know maybe the dynamics of neural

52:55.360 --> 53:04.000
network training all right cool so you've also got a couple of papers or things that your group

53:04.000 --> 53:10.400
has been working on since we last spoke what do you have going there so one paper that I'm really

53:10.400 --> 53:18.160
excited about just got accepted at ICLR 2020 congrats thanks um so this is work with my student

53:18.160 --> 53:23.680
Vivian Kaushik and basically the idea we talked a little bit earlier about this concern and natural

53:23.680 --> 53:30.320
language processing about the models you know it's okay made it's accurate but is it is it really

53:30.320 --> 53:35.840
like learning the right correlations right and the problem is that you it's not so well defined

53:35.840 --> 53:40.640
what makes a correlation like like you know these people starting to say like is it picking is

53:40.640 --> 53:45.040
a really learning the past or is it learning superficial correlations or bad correlations or

53:45.040 --> 53:49.120
spurious ones the problem is those words don't have like a clear formal meeting within statistical

53:49.120 --> 53:53.520
learning like if you've got two sets of features and they're both associated with the output what

53:53.520 --> 53:58.080
is the general principle according to which you should your model should be relying on one first

53:58.080 --> 54:05.280
the other right and so we kind of cast this problem of within sort of a causal framework you know

54:05.280 --> 54:10.480
we don't use the mathematical machinery of causality but we use this sort of thinking that the

54:10.480 --> 54:15.840
relationship here is vis-a-vis intervention so the reason we give you a clear example to make

54:15.840 --> 54:21.040
this like real so we take a movie reviews and if you're trying to classify our movie reviews

54:21.040 --> 54:25.360
you find that the classifier puts a lot of weight you know just a linear classifier on bad words

54:25.360 --> 54:30.320
it puts a lot of weight positive weight on words like fantastic excellent but it also puts positive

54:30.320 --> 54:35.520
weight on words like romance which is a genre not really a sentiment and then if you it puts

54:35.520 --> 54:40.000
negative weight on words like terrible but also on the word horror which is again the genre like

54:40.000 --> 54:43.280
why can't you have a great horror movie or why why should the fact that it's a horror movie

54:43.280 --> 54:48.160
matter and the reason why is because for whatever other reason perhaps like the confounder is

54:48.160 --> 54:52.560
something like low budget that like Hollywood romance movies tend to be better received tend to

54:52.560 --> 54:58.160
have higher reviews and that horror movies tend to be lower rate right yep so you think like why

54:58.160 --> 55:02.880
shouldn't why shouldn't that be the case why shouldn't the model put weight on those and one

55:02.880 --> 55:08.400
reason why is to say it's the formalizing these like through the the concept of an intervention

55:08.400 --> 55:14.240
that if I took a movie and I changed the genre keeping everything else the same it shouldn't

55:14.240 --> 55:18.400
change the sort of applicable label to that movie review whether it's positive or not right even

55:18.400 --> 55:24.720
if it's correlated vis-a-vis this like sort of confounding that actually intervening upon it

55:24.720 --> 55:30.880
should make a difference so the key idea we had here was to use is that what is real the you know

55:30.880 --> 55:36.880
the the actual like substantial the causal connection verse what is not might not even be identifiable

55:36.880 --> 55:41.360
from the observational data alone so the key idea is to use humans in the loop to supply that

55:41.360 --> 55:46.720
information and so what we do is we kind of flip the crowdsourcing paradigm so instead of saying

55:46.720 --> 55:52.000
here's a review and here's a label or so here's a review give me the label instead what we do is we

55:52.000 --> 55:56.640
say here's a review and here's the actual label associated with it like positive or negative

55:57.200 --> 56:02.720
then below that review we have the same exact review like pre-populating an editable text box

56:02.720 --> 56:06.960
and then to the right of it we have a counterfactual label which is like the opposite you know like

56:06.960 --> 56:11.360
this was a positive review now make it negative it was a negative review now make it positive

56:11.360 --> 56:16.880
and so their goal is to edit the review such that it accords with a counterfactual label right

56:17.680 --> 56:21.760
so then what we're basically told to say editor review said it accords with a kind of actual label

56:21.760 --> 56:26.640
basically leave it in a in a internally consistent state so don't just like edit one sentence you

56:26.640 --> 56:32.400
basically have to edit it so that it's fully coherent yeah however three don't make

56:32.400 --> 56:36.640
gratuitous changes so basically don't change any facts that don't need to change to flip the

56:36.640 --> 56:40.960
applicability of the label and so that's what's happening is that we now for every single original

56:40.960 --> 56:45.440
image every single original review that was negative we have a mirror image review that's positive

56:45.440 --> 56:49.520
for every original review positive we have a mirror image review that's negative it's like you

56:49.520 --> 56:54.320
know evil twin yeah and in these ones what's happened is that you know in that original

56:54.320 --> 56:58.240
data set horror occurred in all these negative reviews but in the new data set those are all

56:58.240 --> 57:02.640
positive reviews but they all still have the genre horror and that's because the human knows that

57:02.640 --> 57:06.400
horror is not the thing that needs to be flitted to change the applicability of labels so they

57:06.400 --> 57:11.280
don't intervene on it so the humans are actually like communicating some information to us that

57:11.280 --> 57:16.240
wasn't even necessarily in that sort of an argument we make it's not clear that information is

57:16.240 --> 57:21.280
even identifiable from the original data set alone but the humans by virtue of not intervening

57:21.280 --> 57:26.080
on that data has sort of revealed to us that structure and so we see interesting things like if you

57:26.080 --> 57:31.440
basically if you train on the original data and evaluate on the holdouts that from the counter

57:31.440 --> 57:37.200
factually revised data you go from 90% accuracy to like 55% accuracy and vice versa train on

57:37.200 --> 57:41.440
the revised data you get like 90% accuracy but evaluate it on the original data and you go down

57:41.440 --> 57:46.640
like 55% accuracy if you train on both data sets together so we call the counter factually

57:46.640 --> 57:52.480
augmented data you end up with a model that gets like 88% accuracy on both which is pretty good

57:52.480 --> 57:57.840
it basically means you know you pay a small price for not relying on these sort of like spurious

57:57.840 --> 58:02.720
correlates but surprisingly small you know you're getting like just you're just paying a couple

58:02.720 --> 58:06.800
percentage points in accuracy and you're basically getting good performance on both the original

58:06.800 --> 58:11.200
data and the counter factually augmented data so the name of the paper is learning the difference

58:11.200 --> 58:15.680
that makes a difference with counter factually augmented data and I think that this sort of you

58:15.680 --> 58:20.400
know leads us towards what I'm excited about is that I think the conversation about these data set

58:20.400 --> 58:25.360
artifacts, spuriousness, whatever has been a little bit derailed by a sort of failure to recognize

58:25.360 --> 58:30.880
that one this this this is not just that sort of IIDML problem it's asking us about something

58:31.680 --> 58:37.920
beyond predicted accuracy and perhaps even beyond what's identified in the in the observational data

58:37.920 --> 58:43.200
but by soliciting this this intervened upon data we're we're actually able to tease apart

58:43.200 --> 58:47.280
and so we actually see some interesting things so we can go empirically places that we can't go

58:47.280 --> 58:51.760
theoretically and one thing that we see is that if you train the model on the counter factually

58:51.760 --> 58:55.840
augmented data not only do you do well on both but all those words that you thought should not be

58:55.840 --> 59:01.840
related like horror romance both live some like a lot of these words that just like why is that

59:01.840 --> 59:06.560
associated with sentiment yeah they actually fall out of the model so they they've ceased to become

59:06.560 --> 59:12.160
like high high co-efficient features and like the linear model but then we see for both linear models

59:12.160 --> 59:17.600
LSTMs birth models if we train on the counter factually augmented data we also do better out of

59:17.600 --> 59:22.240
domain so we go outside movie reviews and we train those models and we run the value of those

59:22.240 --> 59:27.120
models on on a different sentiment task like Yelp restaurant reviews or something like that

59:27.120 --> 59:31.120
that are not movies and what is romance or horror mean in the context of a restaurant review

59:31.120 --> 59:35.920
it's not clear right but those the models that were trained on the counter factually augmented data

59:35.920 --> 59:41.840
empirically generalize significantly better not you know in the iid notion of generalization

59:41.840 --> 59:46.000
but in that across domain sense that we're talking about earlier right so it gives us some

59:46.000 --> 59:51.120
suggestion that there you know there's something substantial here and you know I think the next big

59:51.120 --> 59:55.920
leap is to sort of figure out how can we just like on one hand I think we've learned something

59:55.920 --> 01:00:00.080
really interesting conceptually and we've created a data set and resource that we've released

01:00:00.080 --> 01:00:04.080
in the public that hopefully other people can do interesting things with on the other hand is

01:00:04.080 --> 01:00:09.360
an extremely laborious process so so the huge part of the work that makes possible was my student

01:00:09.360 --> 01:00:15.600
had to basically single handedly wrangle a workforce of like 800 crowd workers in order to

01:00:16.160 --> 01:00:21.120
and then actually manually inspect them to make sure that all of these revisions were coherent and

01:00:21.120 --> 01:00:24.880
made sense and that people were actually doing the past we were asking a lot more from them and

01:00:24.880 --> 01:00:30.000
just like check the right radio button we're saying revise a paragraph of prose which is usually

01:00:30.000 --> 01:00:35.840
hard with crowd crowdsource crowd workers right you know but I think we learned a lot of things one

01:00:35.840 --> 01:00:43.520
is that I think you get a lot of benefit from really actively monitoring the process you get a

01:00:43.520 --> 01:00:50.240
lot of benefit from paying them well and that there's a huge like really significant factor that's

01:00:50.240 --> 01:00:55.600
determined by the quality of the instructions that you give them so we we kind of recognize over

01:00:55.600 --> 01:01:00.720
that and I think getting the instructions right and reaching them crystal clear was a big factor that

01:01:00.720 --> 01:01:07.120
made this the results better than if we had given them some hard work of ours instructions all right

01:01:07.120 --> 01:01:13.200
we're getting short on time so before we move on to your predictions you also mentioned another

01:01:13.200 --> 01:01:17.680
paper that you're excited about can you give us a quick summary of the fair ML from a non ideal

01:01:17.680 --> 01:01:24.400
perspective paper sure so this is and I'm going to ruin his name in a horrible way such that

01:01:24.400 --> 01:01:29.200
you know I'll never talk to you again but this is work with a postdoc student is got a Persian

01:01:29.200 --> 01:01:36.800
surname Sina Fuzzleport so sorry Sina for your name but this is work that we've done together

01:01:36.800 --> 01:01:42.160
that basically we've been looking at a lot of the work in fair machine learning both from the

01:01:42.160 --> 01:01:46.720
sort of physical approach and the causal approach and coming up against a certain set of frustrations

01:01:46.720 --> 01:01:50.640
and the main set of frustrations is sort of that and we this is sort of what we talked about last

01:01:50.640 --> 01:01:55.040
time is that what what they're sort of missing is the right set of ingredients to even make a

01:01:55.040 --> 01:01:59.360
determination about what is the just thing to do that you could very easily for a lot of these

01:01:59.360 --> 01:02:04.000
things construct two different scenarios where you could describe them both in terms of you know

01:02:04.000 --> 01:02:08.160
choose your covariance here's your label here's your prediction and here's a sensitive feature

01:02:08.640 --> 01:02:13.280
and you know what seems like the Justin to do is very different you know if we're say on

01:02:14.160 --> 01:02:20.400
hiring and like one case is why do we think that there's something different about the way you

01:02:20.400 --> 01:02:24.720
should approach incorporating demographic in the hiring when you're considering say whites and

01:02:24.720 --> 01:02:30.640
Asian applicants versus white and black applicants in the United States and the reason why is because

01:02:30.640 --> 01:02:36.400
that there's a very different question of justice is because in one of them the current situation

01:02:36.400 --> 01:02:42.400
is a product of very well documented very well understood history of discrimination in justice

01:02:42.400 --> 01:02:47.520
and the other case you have a sort of over representation that sort of occurs not because of

01:02:47.520 --> 01:02:52.960
discrimination but maybe in spite of and so that absent this coherent causal story of the data

01:02:52.960 --> 01:02:59.440
that you're addressing and also maybe a coherent causal story of what is the impact of a proposed

01:02:59.440 --> 01:03:04.240
policy intervention that you know you don't really have the right ingredients to offer someone

01:03:04.240 --> 01:03:11.200
a off-the-shelf solution to say oh this is the way to be fair to be just so basically in this

01:03:11.200 --> 01:03:16.880
context of formulating these this sort of argument about so we can't just sort of hand people

01:03:16.880 --> 01:03:21.360
tools and tell them that they are these like oh like plug your classifier into this and it'll

01:03:21.360 --> 01:03:25.680
magically become fair that it doesn't have the right set of inputs scene I made a really nice

01:03:25.680 --> 01:03:30.160
connection to a lot of the literature that arose in the context of political philosophy

01:03:30.160 --> 01:03:34.960
in the context of segregation and integration and there it was actually were especially influenced

01:03:34.960 --> 01:03:39.520
by a work called the imperative of integration by a political philosopher named Elizabeth Anderson

01:03:39.520 --> 01:03:43.920
and you see throughout this book she makes this distinction really clear between the ideal and

01:03:43.920 --> 01:03:49.760
the non-ideal approach in philosophy and basically the ideal approach sort of asserts what does the

01:03:49.760 --> 01:03:54.160
like in the most naive form in the ideal approach you sort like what does the perfect world look like

01:03:54.160 --> 01:03:58.640
and you sort of identify some discrepancies between the world you live in and the ideal approach

01:03:58.640 --> 01:04:03.840
and this is your justification and so historically you know a sort of naive of

01:04:03.840 --> 01:04:08.160
application of ideal approach might be something like saying that we must have a colorblind policy

01:04:08.160 --> 01:04:13.040
for everything because in the perfect world everyone would be colorblind and the non-ideal approach

01:04:13.040 --> 01:04:17.520
is more concerned with well what are we don't live in a perfect world so what is the landscape

01:04:17.520 --> 01:04:22.880
of injustices and and basically what you know why are we in a situation where in the who are the

01:04:22.880 --> 01:04:28.000
agents what the moral responsibility to do something about it and then what are the strategies

01:04:28.000 --> 01:04:32.400
that are actually going to be effective given the dynamics of the kind of messed up world that we

01:04:32.400 --> 01:04:40.240
live in and so you know in the context of like say the naive ideal approach for a raceblind policy

01:04:40.240 --> 01:04:46.240
the non-ideal approach would say something like well given we don't live in that ideal world so

01:04:46.240 --> 01:04:50.400
given that people have whether or not we even believe that race is real given that on the count

01:04:50.400 --> 01:04:54.880
of perceived race people have been discriminated against in their these mechanisms in place

01:04:54.880 --> 01:05:01.120
what then is sort of your responsibility to act and so the I think that you know sort of maybe

01:05:01.120 --> 01:05:04.880
argument against this casting of like ideal approach is that it's almost like so

01:05:04.880 --> 01:05:10.640
uh it's also naive that like you'd say well maybe no one actually believes it so literally and

01:05:10.640 --> 01:05:16.400
that actually you know in some ways it's you know besides maybe like someone reaching for like

01:05:16.400 --> 01:05:22.080
us like a like a naive argument against affirmative action or something like that but actually in a

01:05:22.080 --> 01:05:27.520
lot of ways it's actually precisely in uh the fair machine learning approaches that we're seeing

01:05:28.160 --> 01:05:33.600
precisely like this kind of you know like like each of these parodies is something that would hold

01:05:33.600 --> 01:05:39.600
in our ideal world where um whatever is the demographic didn't matter and the proposed approach

01:05:39.600 --> 01:05:45.440
you just latch on to one of them and then minimize the disparity. So is the idea is is this

01:05:45.440 --> 01:05:53.520
principally a critique of the fair ML kind of the state of the the world in fair ML or

01:05:53.520 --> 01:06:00.880
is are you more trying to point to tools and that have been discovered or presented in the

01:06:00.880 --> 01:06:07.520
political philosophy world and kind of getting to non-ideal approaches and drawing analogies for

01:06:07.520 --> 01:06:14.160
fair ML. Yeah so I wouldn't say so much that the purpose of the paper is to offer a critique. I

01:06:14.160 --> 01:06:21.200
think there are some sort of well-known problems with any of these individual parity metrics but

01:06:21.200 --> 01:06:28.640
I think that's really more to one maybe present something of a more unifying view and maybe

01:06:28.640 --> 01:06:34.560
most importantly just to make a connection to a large existing body of work that I think a lot of

01:06:34.560 --> 01:06:39.600
people there are people in the space are aware of but I think a lot of people in the space are

01:06:39.600 --> 01:06:45.280
not and I think this is something that maybe you know by by by stepping back from the like

01:06:45.280 --> 01:06:50.960
fair ML kind of I've utilized classification context and looking at actually cases that have been

01:06:50.960 --> 01:06:56.160
probed you know really much more critically and I think that like cases like segregation

01:06:56.160 --> 01:07:01.280
integration offer a sort of policy context where people have really thought deeply about the

01:07:01.280 --> 01:07:07.600
problem and by reading a book like I think Anderson's imperative of integration you see sort of what

01:07:07.600 --> 01:07:14.720
what is the actual work that goes into making a coherent policy argument the fact that you know

01:07:14.720 --> 01:07:19.920
that this includes among other things a sort of an account for the disparity a sort of like

01:07:20.480 --> 01:07:25.840
normative framework that tells you who's responsible for intervening which is not necessarily

01:07:25.840 --> 01:07:29.440
congruent with a set of people that are responsible for the problem in the first place

01:07:29.440 --> 01:07:34.880
that actually is a pragmatic one that takes into account the best evidence for for what the

01:07:34.880 --> 01:07:40.960
actual impact of proposed interventions would be and is it you know are they actually not just

01:07:40.960 --> 01:07:46.160
beneficial in a sense of like the entries to your confusion matrix then you're like idealized

01:07:46.160 --> 01:07:50.800
view of the world but actually do they sort of lead to the kind of social change that we sort of

01:07:50.800 --> 01:07:57.760
are are seeking I think that hopefully by making this connection between this sort of previous

01:07:57.760 --> 01:08:03.600
body of work and philosophy and what's happening right now in Fair ML I think if anything maybe we

01:08:03.600 --> 01:08:08.800
allow the hope is made to give people an opportunity that sort of learn from sort of people's

01:08:08.800 --> 01:08:15.360
past mistakes rather than having to make them all ourselves got it got it so maybe let's switch gears

01:08:15.360 --> 01:08:23.600
and talk a little bit about your predictions for 2020 or if you dare the next decade since we're

01:08:23.600 --> 01:08:31.760
entering a new one number one prediction and I think this is already well underway so maybe

01:08:31.760 --> 01:08:36.960
it's a conservative prediction but I think that what we're seeing and we'll continue to see and

01:08:36.960 --> 01:08:45.520
become more obvious is is a kind of great commodification that the technology probably talk about like

01:08:45.520 --> 01:08:50.800
exponential progress and technology so I think actually in a lot of ways certain aspects of ML

01:08:50.800 --> 01:08:55.600
progress have started to stagnate a little bit because that's how things go we don't we don't just

01:08:55.600 --> 01:08:59.680
blow up exponentially we we make a little progress we have a little bit of a rupture and then things

01:08:59.680 --> 01:09:05.280
slow down again and tell the next big idea where things are growing it sort of horizontally is that

01:09:05.280 --> 01:09:10.400
I think we used to be you went to nerfs or ICML and you saw the same four companies that were

01:09:10.400 --> 01:09:17.040
basically massively represented it which is you know Google Microsoft Facebook you know some Amazon

01:09:17.760 --> 01:09:22.800
some Apple maybe and now what you're starting to see is a lot of like boring companies

01:09:22.800 --> 01:09:27.760
showing up the nerfs and sending you know and I say it's not as like a put down but more like this

01:09:27.760 --> 01:09:36.640
isn't like just like sole province of the extremely like tech elite but that this technology I think

01:09:36.640 --> 01:09:41.520
is going to start to become boring technology in the way that like any successful technology does

01:09:41.520 --> 01:09:46.800
right so I think there was a time when you built the first object oriented program you were doing

01:09:46.800 --> 01:09:52.080
something riveting that was you know academic kind of happening and there's a time now where

01:09:52.080 --> 01:09:57.280
basically every single person building a website is building an object oriented program and I think

01:09:57.280 --> 01:10:04.480
that we're seeing a lot more of this of sort of banks government just sort of all throughout the

01:10:04.480 --> 01:10:10.320
economy that there's sort of a larger sort of base of people that are sort of participating in

01:10:10.320 --> 01:10:15.360
these technologies and these jobs and it's becoming much more widely diffused at the same time I think

01:10:15.360 --> 01:10:19.440
you know a lot of this there was this moment right where if you could train a neural network you

01:10:19.440 --> 01:10:25.120
had a route to possibly like a 400k year job or something absolutely ludicrous you know that

01:10:25.120 --> 01:10:31.440
a number of companies and I think that now at the very top you know I think there's not as many

01:10:31.440 --> 01:10:36.880
people that could could run a research lab that could you know I think they're still there's

01:10:36.880 --> 01:10:42.480
still parts of the economy that are as hot or hotter than ever but I think that a lot of this

01:10:42.480 --> 01:10:49.520
just kind of can train neural network to do X is getting commodified and it's just going to you

01:10:49.520 --> 01:10:55.760
know the there's a lot more people could do it and a lot more companies will be doing it and I

01:10:55.760 --> 01:11:01.680
think it's going to start becoming just more kind of commonplace and obvious I think at the same

01:11:01.680 --> 01:11:06.480
time hand in hand with that and this ties into the maybe like theme of the conversation is that I

01:11:06.480 --> 01:11:11.920
think that that's sort of like that's the direction I think for this established sort of just

01:11:11.920 --> 01:11:20.320
predictive models as on one hand yeah get boring become widely diffused at the same time maybe

01:11:20.320 --> 01:11:25.120
a little bit of a stagnation on progress on the other hand I think a lot of the creative people

01:11:25.120 --> 01:11:30.320
are going to be pushing more and more sort of beyond the limits of just this sort of train test

01:11:30.320 --> 01:11:35.360
prediction and I think that'll you know one direction is people thinking more seriously about

01:11:35.360 --> 01:11:40.480
generative models I think one direction is people thinking much more seriously about causality

01:11:40.480 --> 01:11:45.920
people trying to come up with more expansive or ambitious ideas about robustness so we've been

01:11:45.920 --> 01:11:51.040
sort of my authentically fixated on this idea of perturbations within the L2 ball or within the

01:11:51.040 --> 01:11:56.240
L infinity ball like for adversarial examples but starting to get a bit more ambitious with

01:11:56.240 --> 01:12:02.160
kinds of invariances and kinds of robustness that we want to build into models and I think

01:12:02.160 --> 01:12:06.240
hand in hand with causality and the papers we talked about earlier I think is also starting to

01:12:06.240 --> 01:12:13.760
ask research questions that situate these models in the context of the like wider decision-making

01:12:13.760 --> 01:12:18.640
process that they're actually part of so I think this kind of integration of machine learning

01:12:18.640 --> 01:12:24.400
and economics is sort of going to be an exciting area and I think that's really if I have to look

01:12:24.400 --> 01:12:30.000
you know over the next five ten years what I think is going to blossom I think that is that sort of

01:12:30.000 --> 01:12:34.880
I can't say that I see completely the technical router that I have all the right tools to make it

01:12:34.880 --> 01:12:39.360
blossom but I think in terms of like what needs to in order for this technology actually to be

01:12:39.360 --> 01:12:44.640
deployed in the kinds of ways that we imagine it should be that's the kind of research that needs

01:12:44.640 --> 01:12:49.360
to start happening and so I think we're going to start seeing the field kind of setting it

01:12:49.360 --> 01:12:54.560
sides a little bit a little bit wider yeah do you have a sense for you you mentioned not being

01:12:54.560 --> 01:12:59.440
able to see clearly what the technical pieces of that but do you have any kind of sense for what

01:12:59.440 --> 01:13:06.000
that needs to look like certainly some of the things we've talked about in the context of fairness

01:13:06.000 --> 01:13:12.560
some of these fat star papers feedback loop papers are in this vein is there kind of a broader way

01:13:12.560 --> 01:13:18.160
to characterize what happens when these two fields start to you know collide more frequently

01:13:18.160 --> 01:13:23.280
I think what really needs to happen is that where I think where we're sort of in trouble right now

01:13:23.280 --> 01:13:28.320
is that we have the pure predictive modeling world which is sort of conceptually involved

01:13:28.320 --> 01:13:35.040
rich but is able to deal with really rich real world better so if we pretend all we care about

01:13:35.040 --> 01:13:42.000
is prediction then we're like limited to a very kind of you know really flat set of conceptual

01:13:42.000 --> 01:13:48.080
questions we can ask but we're able to address them concerning really rich spaces of interesting

01:13:48.080 --> 01:13:54.160
data then on the other hand I think we have a much richer conceptual worlds that are offered by

01:13:54.160 --> 01:14:00.560
the language of causality the language of the economic modeling that get in towards a much more

01:14:00.560 --> 01:14:06.080
you know deeper and critical consideration of these multi-agent environments or or even just

01:14:06.080 --> 01:14:11.840
you know the causal structure of the world that are really allow us to frame like philosophically

01:14:11.840 --> 01:14:17.440
coherent questions that are much more expansive than what we could say and to sort of supervise

01:14:17.440 --> 01:14:22.320
learning business as usual but the downside there is that we don't have tools that we can take

01:14:22.320 --> 01:14:27.440
to real data so it's like do we want the sort of impoverished tools that we could really take the

01:14:27.440 --> 01:14:32.800
data or do we want the really rich tools that we can just use to run thought experiments or

01:14:33.440 --> 01:14:40.240
you know even toy data experiments and and I think bridging this gap and if I you know if I

01:14:40.240 --> 01:14:43.280
had all the answers I certainly wouldn't be telling you you know I'd be

01:14:43.280 --> 01:14:52.000
be writing this family the archive of everyone else yeah no but I don't I don't know I don't

01:14:52.000 --> 01:14:58.400
want to pretend to to know what that looks like I don't think it's sufficiently like respectful

01:14:58.400 --> 01:15:03.680
to the difficulty of the task but that's that's that's what I try to look for right now

01:15:03.680 --> 01:15:10.720
it one of the things that I try to do coming back from noreps and this time of year is try to

01:15:10.720 --> 01:15:21.920
identify a few kind of key ideas or thoughts that were notable out of that event but also kind of

01:15:21.920 --> 01:15:27.680
broadly you know gaining traction over the year and a couple that come to mind for me this year

01:15:28.400 --> 01:15:35.760
were causality and generative models and they certainly came up quite a bit in our conversation

01:15:35.760 --> 01:15:41.600
today you know do you think similarly in terms of those those particular do you have others

01:15:41.600 --> 01:15:47.680
you know if if those are at the top of your list like why do you think that is the case now

01:15:47.680 --> 01:15:52.640
well I don't know about you know I think those those two are a bit different like I mean there's

01:15:52.640 --> 01:15:57.200
other contexts amongst people talking about generative models that that aren't you know the

01:15:57.200 --> 01:16:01.360
the sort of graphical models we talked about in the context of causality of those those are

01:16:01.360 --> 01:16:07.600
our generative models but I think the reason why they're pressing now is just because we're

01:16:07.600 --> 01:16:13.360
actually using this technology right so it's like we sort of have a technology that addresses

01:16:13.360 --> 01:16:18.560
a narrow set of concerns that it produces sort of like artifacts that are enough to get us excited

01:16:18.560 --> 01:16:24.480
enough to get us excited about deploying the technology but not enough to actually really

01:16:24.480 --> 01:16:30.320
address the needs of like the deployment environment if that makes sense we we basically like we

01:16:30.320 --> 01:16:36.080
were running this stuff in the lab forever we have these tools that that do well at this sort of like

01:16:36.080 --> 01:16:42.480
guess the answers on the test set and now we're deploying tools based on it we're not actually like

01:16:43.200 --> 01:16:47.920
ready for prime time in terms of being able to address address the needs of those real world

01:16:48.480 --> 01:16:52.400
deployment environments I think what's happening is people are starting to go to those stakeholders

01:16:52.400 --> 01:16:58.320
this starting to come up against the limits of like what's wrong like why why it's insufficient to

01:16:58.320 --> 01:17:05.680
use neural network like like why hold out test performance isn't enough to make decisions in a

01:17:05.680 --> 01:17:10.000
medical decision-making scenario like why it's not enough to make clinical decisions just to have

01:17:10.000 --> 01:17:13.920
good prediction accuracy and so I think the more people start using this technology like those

01:17:13.920 --> 01:17:17.680
issues have to come up because they were there in the first place we just weren't thinking

01:17:17.680 --> 01:17:21.840
seriously about it but we sort of like put ourselves in this bind like I think like that that's I

01:17:21.840 --> 01:17:27.120
think the drivers that we suddenly are I think it's some amounts of sobriety as we start coming up

01:17:27.120 --> 01:17:32.160
again sort of like failure after failure I think same for just like in the problem robustness

01:17:32.160 --> 01:17:35.920
out of distribution and causality are quite quite related to each other and I think a lot of the

01:17:35.920 --> 01:17:40.720
conditions under which we try to ensure robustness correspond to causal stories like we talked about

01:17:40.720 --> 01:17:45.120
before but I think you know when we start many people you look at what are people spending billions

01:17:45.120 --> 01:17:52.080
of dollars on in a machine learning space one is you know medical decision-making one is self-driving

01:17:52.080 --> 01:17:58.160
cars right so if you're building self-driving cars and you have no assurance that given training

01:17:58.160 --> 01:18:03.600
on the enormous statistic like through 2019 or something like that you have no assurance that

01:18:03.600 --> 01:18:08.960
they're gonna not crash in 2020 and that any you know small things like Mazda comes out with a new

01:18:08.960 --> 01:18:14.800
paint job that your cars are gonna start killing people that's you know obviously a problem so I

01:18:14.800 --> 01:18:18.160
think we've already sort of you know I think I think the reason why they're coming out is to be

01:18:18.160 --> 01:18:22.000
already like sort of sign the contracts like we've already like hitched our reputations towards

01:18:22.000 --> 01:18:27.200
delivering these products and and we're discovering a little bit too like that that these other

01:18:28.000 --> 01:18:33.120
sort of things that are that you know these other competencies are that that our machinery

01:18:33.120 --> 01:18:37.520
doesn't provide or actually necessary to do the things we told people we would deliver for them

01:18:37.520 --> 01:18:44.080
I think like the real driver is just coming up against nature also awesome well Zach all good

01:18:44.080 --> 01:18:50.800
things must come to an end so it goes for 2019 as well as this wonderful conversation reflecting

01:18:50.800 --> 01:18:59.360
on 2019 thanks so much for taking the time to chat with us your perspective on these papers and

01:18:59.360 --> 01:19:07.280
the field in general really appreciate it thanks for having me Sam awesome thank you all right

01:19:07.280 --> 01:19:12.800
everyone that's our show for today for more information on today's guest or for links to

01:19:12.800 --> 01:19:20.160
any of the materials mentioned check out twimmol ai.com slash rewind 19 be sure to leave us a

01:19:20.160 --> 01:19:24.720
five star rating and a glowing review after you hit that subscribe button on your favorite

01:19:24.720 --> 01:19:40.400
podcast catcher thanks so much for listening and catch you next time

