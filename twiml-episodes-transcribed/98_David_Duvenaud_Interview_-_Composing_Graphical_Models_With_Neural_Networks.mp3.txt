Hello and welcome to another episode of Twimble Talk, the podcast where I interview interesting
people doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Last week I spent some time at CES in Las Vegas exploring the vast sea of drones, cameras,
paper thin TVs, robots, laundry folding closets and other smart devices.
You name it, it was there.
Of course, I was also able to sit down with some really interesting people working on some
pretty cool AI-enabled products.
Head on over to our YouTube channel to check out some behind-the-scenes footage from my
interviews and other quicktakes from the show.
And of course, be on the lookout for our AI and consumer electronic series right here
on the podcast coming soon.
A quick note, this is your final reminder about tomorrow's Twimble Online Meetup.
At 3pm Pacific, we'll be joined by Microsoft Research's Timnett Gebru.
Timnett will be joining us to discuss her paper using deep learning and Google Street
View to estimate the demographic makeup of neighborhoods across the United States.
And I'm especially looking forward to her going into more detail about the pipeline she
used to identify 22 million cars and 50 million street-view images.
As usual, we'll get the meetup kicked off with the discussion segment in which we'll
be exploring your AI resolutions and predictions for 2018.
For links to the paper, to register for the meetup or to check out previous meetups,
visit twimbleai.com slash meetup.
The show you're about to hear is part of a series of shows recorded at the rework,
deep learning summit in Montreal, back in October.
This was a great event, and in fact, their next event, the deep learning summit San Francisco
is right around the corner on January 25th and 26th.
The summit will feature more leading researchers and technologists like the ones you'll hear
here on the show this week, including Ian Goodfellow of Google Brain and Daphne Kohler
of Calico Labs and more.
Definitely check it out and use the code Twimbleai for 20% off of registration.
In this episode, we hear from David Duveno, Assistant Professor in the Computer Science
and Statistics Department at the University of Toronto.
David joined me after his talk at the deep learning summit on composing graphical models
with neural networks for structured representations and fast inference.
In our conversation, we discuss the generalized modeling and inference framework that David
and his team have created, which combines the strengths of both probabilistic graphical
models and deep learning methods.
He gives us a walkthrough of his use case, which is to automatically segment and categorize
mouse behavior from raw video, and we discuss how the framework is applied to this and other
use cases.
We also discuss some of the differences between the frequentist and Bayesian statistical approaches.
I had a great time with this interview, and I think you will too.
And now on to the show.
Alright everyone, so I am here at the rework deep learning conference in Montreal, and
I am with David Duveno, David's a professor at the University of Toronto, and he actually
just got off stage, and I'm excited to be sitting here with him, or here with you.
I should say David, welcome to the podcast.
Thank you, Sam.
So, tell me a little bit about your background and how you got interested in machine learning
in AI.
It sounds like you come a little bit from the statistics side of the world.
No, I've actually been sort of maybe pushed into this.
That's right, man.
I'm half-appointed since that's the University of Toronto.
And it's really nice.
No, actually the way I got into this was actually reading JÃ¼rgen Spadeuber's web page when
I was in undergrad, and he had, I think, you know, to his credit, he really was thinking
about a lot of things that people, you know, later also found interesting, and he has
a way of presenting his ideas as sort of like the last word.
I mean, everyone sort of does, but at the time I didn't really know how to evaluate these
things, and I thought, oh man, I need to go to grad school, because this guy is going
to solve AI in like two years or so.
So, yeah.
I mean, of course, now I have a much broader view, and I feel like if you go to grad school
in Switzerland?
No, no.
Although I certainly, like, actually went to grad school at the University of British Columbia.
Okay.
Yeah.
Initially, all my friends went to grad school, and my first time I applied for scholarships
I got rejected, and so I had to be chip on my shoulder, and I was like grad school
is for jerks.
But yeah, then eventually I applied again and got it, and yeah, I went to work with Kevin
Murphy back when graphical models were still cool, and I did a bit of machine vision and
things like that.
Okay.
Yeah.
And then he actually set me up with an internship at Google.
It wasn't called Google Brain Back then.
It was actually the video content analysis team, but it was all the same people that ended
up forming it.
And we just sort of tried to get confidence to predict whether YouTube videos contained
people dancing for this content, so that YouTube was running at the time.
Interesting.
Interesting.
And so you're now at University of Toronto?
Yes.
How long have you been at the University?
Just a year actually.
Okay.
And you said it, you've kind of been pushed into this joint appointment.
Well, okay.
Wow.
I'm not.
I mean, I started off being interested in machine learning, and then went and did my PhD on
Beijing Non-Priorometrics, and I realized that sort of math was a missing component.
You know, like everyone in machine learning has to be able to code and do math, but you
really can't get away without knowing the math.
And really a little bit of formalism can go a long way.
And especially recently, I've been working on building gradient estimators for discrete
latent variable models.
And this is also shows up in reinforcement learning where having unbiased gradient estimators
is actually really important.
And unbiased and it sounds like this boring horrible thing that frequentes statisticians
care about.
But it actually now, I think, is going to turn out to be sort of a central thing that
people are going to be worrying about over the next few years.
And so you just did your talk, and your talk was on combining graphical models with deep
learning models.
Yeah.
Tell us a little bit about the talk as an overview, and we can kind of dive into the different
elements of it.
Sure.
Well, the way that paper that came about was actually pretty fun.
So I just did a postdoc at Harvard with this guy, Matt Johnson, who is now at Google Brain.
And when I showed up, I had just sort of de-converted from Bayesian non-parametrics where I was
trying to fit giant, infinite dimensional graphical models to everything.
And sort of I had sort of realized the limitations of these approaches.
And I was really excited about variational autoencoders where we learn a generative model
using a neural network.
And we also use a neural network to learn to do inference in that model.
And sort of as I said in the talk, people have been able to write down really rich generative
models for a long time.
But they haven't been able to do inference in them, and that's sort of the limiting factor.
Okay.
With regards to this paper, Matt was coming from a sort of also old school background where
he was saying, oh, I want to use graphical models, linear dynamic systems, like nice models
that we can analyze and understand.
And I was telling him, no, no, no, you've got to forget all of that.
You have to just use neural networks to do inference.
We can fit everything with a variational autoencoder.
And we had this long back and forth over the course of about a year.
In the end, it was kind of like my chocolate and your peanut butter.
And we had this nice synthesis of ideas.
And we sort of said, oh, maybe we can combine these and get sort of the best of both worlds.
Okay.
So when you say graphical model, what do we mean with that?
Yeah.
Let me mean by that.
So that's a pretty broad phrase.
It basically just means it doesn't mean graphics as we traditionally talk about graphics.
That's a starting place.
So the graphical model means like graphs.
And the idea is that initially when people started writing down models, they just wrote
down equations.
I would say what the probability of different things were.
And it was pretty hard to analyze these models.
And so, you know, I think Kevin Murphy, Daphne Collar, near Friedman, these guys said,
what if we actually represent the dependence of different variables as a graph?
And you know, the idea is that if I say that, you know, like smoking causes cancer, then
I would have an arrow going from smoking to cancer.
And this says something about how the probability of these things change together.
So if I get cancer, that doesn't make me smoke, but if I get smoke, I make me get cancer.
And once we start to have, you know, ten or a hundred different variables, keeping
track of all these arrows or these relationships becomes pretty tricky.
But when we express these models as a graph, we can automatically analyze them and ask, you
know, whether we think that like, you know, drinking wine causes, you know, better like test
scores through some complicated mechanism.
Or we can also ask, you know, what information would we need to learn in order to tell us
what the answer is?
Yeah.
And so this is sort of this, it's not quite old fashioned AI, but it's sort of this, like,
we're going to have these rational, understandable models where every sort of human concept has
a little box that we put it in and we try to understand everything that's happening in
our models.
Okay.
Okay.
So when we talk about graphs and applying graphs to these types of models, so one of the
things that I think of at least is the, some of the deep learning frameworks like TensorFlow
allow you to create neural network architectures using graphs, but that's like at a higher level
than what you're talking about, right?
Yeah.
So that's kind of a confusingly similar idea because, you know, we write down these
computation graphs where we have these arrows that mean A is a function of B. And that's
also the same sort of relationship we're talking about when we write down graphical models.
But in graphical models, the relationships are all probabilistic, like we're not saying
that A determines B, we're saying the probability of B depends on A.
Okay.
And then the thing we can do with graphical models that we can't do with neural networks
so easily is to go backwards and say, you know, given that someone has cancer, what is
the chance that they smoked, which is something, you know, you can't, well, it's not as easy
to sort of run a neural network backwards.
But when we run a graphical model backwards, we're sort of asking what are the hidden causes
of the things that we observed?
And this is this, this is what we tend to refer to as like the inference problem.
Okay.
How do we figure out what, like, how does what we saw change what we believe about what
we didn't see?
And you know, this, as an example, you can even view, for instance, like learning grammar,
like when babies learn language, they hear all these sentences and there's this hidden
thing, which is, you know, grammar and vocabulary and all these rules about how language goes
together.
And so the problem of hearing a bunch of sentences and then trying to figure out which
is the likely rules of that language is an inference problem.
And so this is kind of why people have been really excited about these generative models
are like also called latent variable models for a long time.
Yeah.
Like the idea is the grammar is this latent unseen variable that we have to infer.
And for instance, and also this motivates why people have been so excited by inference.
So if you go to nips, there's like the advances in approximate vision inference workshop
and, you know, variance of it.
And it's one of my favorite parts of nips because it sounds really boring and dry.
But, you know, this, the inference problem is kind of the bottleneck for doing all the
cool things that babies can do that we can't do with, at least, that's at least one bottleneck.
You know, even if we solve inference, there's probably more problems that we need to solve.
But for instance, people like Josh Tenenbaum and MIT for, you know, now like 20 years or
so heavy to get him on the show, his name has come up probably like three times just
today.
Yeah.
And he's just a really inspiring person to talk to you because he really saw this vision,
you know, a long time ago that, guys, guys, guys, if we could just, you know, figure out
how to do inference, we would be able to not only explain like how humans do all these
things, but also get machines to do them ourselves.
So he's been, you know, looking into these for a long time.
And I think actually all that I would be sure to keep an eye on the stuff that's coming
out of his, his lab because inference methods have just been making, like these major leaps
and bounds in the last three or four years.
You know, I think inference methods is maybe another, like name collision because we refer
to inference as kind of using models generally, but again, we're talking about something different
here.
We're talking about, yeah, maybe I would say probabilistic inference.
Yeah.
I agree that this word is completely overloaded.
And also in frequent statistics, it has another meaning in frequentistic, what is frequentist
statistics?
Oh, well, I can even say that a lot of long back, so I would call myself a vision.
So it's kind of like asking, you know, like a liberal to describe a Republican or like
a Catholic described a Protestant or something.
But you know, they're interested in like worst case guarantees, computing p-values, doing
hypothesis testing, and basically making procedures that can tell us, you know, what does the
data say about this particular question, regardless of what we happen to think before.
And then the division side says, well, let's actually ask how to combine what we saw today
with what we knew before.
I mean, actually, that's the standard answer.
To me, the real answer is, visions consider all possibilities, and they just keep all them
around and weight them all equally, or not equally, weight them according to how well
they fit the data.
And frequentists try to identify like the best possible hypothesis.
And these are like good reasons to do both, these are very different schools of thought.
And you know, there's been this unfortunate bit of tribalism, I think, where people naturally
tend to like to form groups, and then, yeah, so that has definitely happened, and that's
been a thing for like 70 years, since it's six now.
So yeah, maybe a digression, are there a set of things that you think of about statistics
that you wish more people doing, machine learning, or deep learning, knew or understood better?
Okay, well, I'm going to take that as a yes.
There's like a little rant that I've been wanting to go on for a long time, which even
like Princess at Harvard, when they were teaching machine learning, someone said, what's
the difference between a frequentist and a vision?
And then they said, oh, the frequentist treats the data as a random variable, and the
vision treats the truth as a random variable.
That is sort of technically what is happening, but it's just like a bizarre mis-framing
of the entire discussion.
I mean, like a random variable is sort of not necessarily random, and it's not necessarily
a variable, it's like a very bad name, but the idea is that, you know, the frequentist
is going to say, you know, if this thing was the case, how likely would I have been to
see the data that I saw?
That's a sort of analysis that is often done, I mean, there's all sorts of methods, and
the vision will say, given that I did see this data, what do I believe about the truth?
Even though I think the truth is fixed, and it's not random, because I don't know it,
I can use probability to describe my state of uncertainty, and that doesn't mean that
I think it's random.
That just means I am using probability to describe my uncertainty.
Right.
Okay.
Yeah, so that's my public service announcement.
Okay.
And so is there, you know, so that's kind of describing these two tribes.
Are there ways that you see those two kind of schools of thought influencing the way
folks approach, you know, machine learning and AI that, you know, particularly that you
think, you know, a little bit more kind of commonality or something would kind of advance
us as a community?
Well, it's kind of funny, because deep learning has represented a sort of de facto sort
of third direction, or maybe synthesis, and these debates about vision and frequentist
methods, I think, really took a backseat to saying, let's just define a probabilistic model
that will give us a continuous loss function that we can optimize and use gradient-based
optimization to do maximum likelihood estimation.
And deep learning in a nutshell, and this is sort of like takes elements from both.
Now, of course, that people are saying, how do we, you know, train deep learning models
with less data?
People are looking more into vision, deep learning, which actually can be made to look
a lot like standard deep learning, where you just add a little bit of noise to everything,
which is really nice.
Yeah, I guess the thing is that most of the classic, like, frequentist methods are based
on taking really simple methods that are easy to analyze and prove things about, just
sort of developing those, whereas for neural networks, you can't really say much about them
in these sort of like hard-bound proving asymptotic sense that you could with like maximum likelihood
estimation or, well, okay, sounds.
Let's say with, like, the sort of simple estimators that people like to use in science.
Yeah, so people used to love these frequentist estimators because they were simple and fast.
And now we sort of accepted that if you pay this price of having a little bit more complex
models and like maybe a GPU or something, you're going to get better enough performance
that you'll just forget about any asymptotic guarantees that the other models might have
had.
What are some examples of frequentist models?
Oh, frequentist methods, let's see.
So there was this whole cottage industry of frequentist kernel methods, let's define
a non-parametric estimator by considering all possible functions in some infinite dimensional
Hilbert space that could possibly separate our data or explain it or model its density.
And you know, these methods still have a place and I think one of the, like, oral presentations
that nips the series is on these methods, so they're not like gone, but they're sort of
definitely in a little bit of, there's definitely like a kernel winter happening right now.
Okay.
So then the, in fact, your presentation was talking about in some sense how to combine elements
of both of these schools of thought?
Yeah, exactly.
So I think when I got to Harvard, I was sort of like one of these people who recently
de-converts from a religion and they just had nothing about the fantasy about it and Matt
was sort of, you know, saying, well, are you sure you want to, you know, really throw
this all out?
And yeah, and we really did have this problem of analyzing this, this most data and they
had fit just like the pure graphical model sort of standard approach to the data and it
had a, had a bunch of problems with, you know, well, let's hit pause there and talk about
the data.
Sure.
So that we're all on the same page.
Sure.
So the data is a bunch of connect video of mice running around in the dark and the idea
is that when biologists want to measure what happens when we, you know, change the genes
of a mouse or give it a drug or, you know, expose it to like the odor of a fox or whatever,
they need to quantify how it's behavior has changed.
So they can write about how, you know, our model of autistic mice do this more often, but
then when we give them the drug, they act more normally or something like that.
And I couldn't tell from the video whether that was kind of top down or like through a
glass floor looking up or something like a top down, okay.
So the idea is that right now they have a army of grad students who spend thousands of
hours watching this video and then saying, okay, and now the mouse, you know, ate something
and now he ran over there and now he stood up and now he went over to his buddy.
And you know, this is cruel both to these students and it's also introduces sort of variability
between different people who might, you know, labeling error or just differences, right?
It's hard to like canonically say like, okay, this is a mouse that is, you know, grooming
or not, right?
And also even the labeling noise more so than error, maybe, it's different interpretations
of what the mouse is doing at a given time.
Yeah, exactly.
And really, you know, we can imagine this changing systematically across labs, maybe in
different countries, maybe it's hard, there's like a language barrier, maybe eat something
so they're different, right?
So we'd really like to automate this part of the scientific pipeline, both just because
it will save the grad students time, but also because it'll, you know, help us do better
science by removing one or by standardizing one part of the entire pipeline.
Mm-hmm.
So you have this data, was there, were there a fixed number of classes or activities that
you were trying to capture or was part of the challenge trying to identify, you know,
how many fixed sets of activities there were?
Yeah.
Great question.
So we really wanted to make sure that we didn't have to tell it exactly how many different
classes of activity that were, because that we kind of defeat the purpose and it would
also sort of make you wonder, well, what if I had given it one more class or one less,
what would it, the results have been totally different?
So this is kind of one of the benefits of being patient is that you can compare the model
fit in a systematic way between different models.
So we, what we did was we said, okay, there are up to, I think we chose like 40 different
clusters.
And the idea was that we, 40 clusters are 40 classes within those, well, classes and
clusters are the same thing in the way that I'm talking about this.
Okay.
Sorry.
And then the idea is that we could let the model choose how often different activities
appeared and some of them it would just never use.
And the idea was that we tell there are at least 40 clusters and it will say, I can explain
this, this data with only 20.
And so, you know, the other 20 just stay off forever.
So we're automatically learning the number of clusters.
We just have to make sure we have a good upper bound on how many there could possibly be.
And I'm sorry.
So you make clusters in a sense of cluster data points is that that will define a class.
Is that the right way to think about this or that's a really good point.
So I guess I mean clusters in a more abstract sense, where we're clustering the dynamics
of the mouse movement.
The idea of being that a behavior is not, you know, a particular pose and the most
be it.
It's not a particular pose that the mouse might be in.
But it's the way that he moves from one pose to another or, you know, when he's grooming
he's like, you know, moving in this sort of circular motion or something like that.
So I mean a cluster in the dynamics is one behavior that he could be doing.
Okay.
So how many, how many of these clusters ended up being identified?
To be honest, I forget and I think it was around 20, but so I just have to say Matt was
the first author and he's the one who really spent a lot of time with the data.
Okay.
Okay.
And so how did, how did the graph, the graphical analysis or the graphical element of this
play in?
Yeah.
So the alternative, the baseline that we could have done would just be to say that there's
some recurrent neural network that defines how the mouse sort of changes through time.
And then we'll have some continuous factor that is changing and we don't necessarily really
know what that can you expect our means.
And so that would have probably fit the data pretty well.
We would have been able to like predict the mouse's future movements.
I think about as well.
But we wouldn't have been able to look in and say, oh, there's these distinct clusters.
So that was sort of the home motivation was the interpretability of this model.
So what is the, what's the process for building a graphical model?
Like are you literally identifying states and transition vectors and things, what's things
like that?
Or is it more abstract and mathematical?
Well, we try to make it as abstract as possible because we want to let the data speak for
itself as much as possible.
So all we did was we said, you know, there's 40 different states, the musket being, and
there's some probability of transitioning from each one to each other one.
And we don't know what that is.
So the model has to learn that.
It also has to learn how those states influence the dynamics and it also has to learn how those,
like the mouse's body state corresponds to actual video frames.
So the idea is that all we basically said is there's some discrete stuff that controls
some continuous stuff, that controls some video stuff.
And all the connections between those things and the connections through time had to be
learned automatically.
And did you end up finding that I'm thinking about like the density or sparsity of the
connection graph, like does that play a significant role in this or what did it end up kind
of looking like?
Great question.
So I do think that mouse's behavior transition is probably our sparse.
Like maybe he never goes from eating to like standing up right away or something like that.
We didn't actually put any capacity for the model to, or we didn't sort of put any prime
information about whether the transition matrix was sparse or not.
To be honest, we didn't look at how sparse the learned matrix was, but I bet it was sparse.
Yeah.
So these are the sorts of like refinements that we would generally like to make.
And actually I want to say these are the sorts of refinements that we would like the,
our learning algorithm to be able to propose on its own.
So at the end of the talk, I sort of said, so right now we built.
The model, but you know, what if we got it wrong?
What if mouse behavior isn't discrete or what if, you know, when there's two mice involved,
there's some more complicated structure that, you know, we just don't understand most
sociology so we don't even know how to write it down.
So what we'd really like to do is try learning both all the parameters of these models and
which types of structure they should have as well.
And there's no sort of technical reason why we can't do this.
It's just that it requires searching over this discrete space, which is sort of always
a big pain.
And are there things that, are there things kind of happening in the field that you think
will enable you to do that?
Like is it just going to be brute force, you know, a better compute or are there, you
know, folks doing research that you think will lend themselves to, or what are the research
areas that will lend themselves to figuring this out?
Right.
So I'm glad you asked.
So I am perfectly think that in the next few years we're going to see a lot of progress
in models of how to fit discrete models or methods of discrete, sorry, I think we're going
to see a lot of progress on methods to fit discrete models.
And sort of the deep learning revolution has basically been all about continuous everything.
So we have continuous parameters, you know, continuous predictions, we have a latent
variable model.
The latent variables are mostly continuous.
The stuff that I talked about today was just like a tiny step we added like one sort
of easy to handle discrete latent variable.
But really the stuff that I think is more interesting is going back to the grammar example, like
learning an entire grammar on a language or even learning a parse tree for a given sentence
is this complicated discrete object that, you know, you can't even say it's like one
out of a hundred possibilities is actually like these entire trees of rules.
And so there still isn't a much better way to handle these sorts of models than we had,
you know, 10 years ago.
So the thing, so one thing that I've been really excited about and getting my students to
work on is how do we find sort of continuous relaxations or gradient estimators for models
with discrete latent structure?
And this is going to let us, I mean, if it works, do all sorts of things like learn hard
attention models, train gans to generate text, you know, let's see.
Yeah, as I said, learn grammars, learn models with these interpretable latent structures,
learn to do things like produce programs.
I mean, obviously none of this stuff is going to work out of the box.
But again, gradient based estimation is sort of a really, really great method because it
scales to, you know, millions of parameters in a way that something like evolutionary
algorithms, I think, never will.
So there might be another way forward, but the way forward I'm excited about is trying
to get good gradient estimators for models with the discrete structure.
Okay.
And just so I can make sure I understand the single latent variable in the example that
you presented is the cluster that the mouse is in at a given time, and it's discreet
because, you know, it's a cluster you kind of quantized it inherently, exactly.
Are there other formulations of that same, I mean, I, there are lots of formulations
of that problem that aren't necessarily discreet.
And so you went down this particular path, why again, just for interpretability.
So the idea is, yeah, we could have said that there's, you know, 10 actions that he can
be doing to some degree, you know, maybe he's eating a little bit, maybe he's running
a little bit.
And then the point is that we think it would have been really a lot harder to interpret
what these, these variables meant.
And we would, if we wanted to say what he was doing at a given time, we'd have to say
these 10 numbers instead of just like this nice one.
Right.
And that's the kind of thing we typically see, like in image interpretation, like, you
know, in this image, there's a umbrella with whatever probability and a girl with whatever
probability.
And so you've got this kind of continuous probability distribution of the things that
are in the image and you could do similar with, with a video clip, you know, the mouse
is doing x, y, z with some probability and, you know, eating with some probability and
grooming with another probability.
Right.
But the idea that the, so the probabilities are continuous, but there's still a big difference
between having a model where the variables are continuous and we're certain about them
or where the variables are discrete and we're uncertain about them.
This, this is an interesting point, which is that when you look at reality, sort of reality
is always continuous.
So why do we even have this discrete structure?
And sort of one reason that it arises naturally is because when we have to describe stuff to
each other in language, we have to choose, you know, which word to use, right?
Like, am I hungry or am I sleepy?
We don't have a whole, yeah.
And we can modify this with verbs, but again, we don't have like some continuous signal
that we can send to each other to just, you know, we can't just give each other high
dimensional vectors.
That would be amazing.
Then, you know, language learning would be a lot easier.
And in fact, there's been some work recently by like OpenAI and some other groups on
like how to teach agents to communicate and they come across this exact same problem,
which is, you know, the agents have to choose a discrete word to say to the other agent,
but there's no way to backprop through that.
There's no gradient signal that says, oh, you should have said this word a little bit less,
right?
It's not clear what that would mean.
Have you seen the movie, her?
Right.
Yeah.
Love that movie.
People have been telling me forever to watch it.
And I just started watching it on this trip and I'm not all the way through with it.
But there was this one part where the, for those who haven't seen it, maybe I shouldn't
give any spoilers on the podcast, that wouldn't be right.
But there's an interesting point where, where this one AI talks about is talking to a human
and says, hey, can I go offline with this other AI and communicate post verbally, which
is exactly what you're talking about, exactly.
So, I mean, it does raise the question, why do we want these artificial agents to even
use discrete words to talk to each other when they can just communicate post verbally, as
you say?
Right.
And I think maybe the answer is, well, we still want them to talk to us and they're going
to have to probably use words to do that.
That's one way, place where this comes up.
Okay.
So what other kinds of things are you working on?
So I'm working a little bit on meta learning and just recently, I've sort of decided that
the way that I was approaching this and that, you know, I think to sort of the mainstream
way now, I think there might be another way forward.
So there's been a lot of work recently where people have said, okay, I want to have like
a robot or a little agent that's going to be able to learn really quickly.
So I want to put it in a new environment and it's going to, in a few seconds, you know,
figure out what's going on and then have a good policy of how to act or something like
that.
The sort of brute force way to do this, no one really did very much until a couple years
ago was to back propagate through the entire learning procedure of, you know, the robot.
So take those, you know, three seconds of him learning his way around the world.
And if everything he did is continuous, we can actually just ask, you know, if I had changed
his learning rules a little bit, how much better would he have done on the task?
And this is fine.
Compute this automatically with automatic differentiation and this also shows up when we have to tune
the hyper parameters of our model, right?
We want to fit an entire neural network, but we have like a learning rate or like a regularization
parameter that we said at once at the beginning, and then it totally changes the outcome at the
end.
So a couple of years ago, me and my colleague, Dougal McClaren wrote a paper where we actually
did back propagate through the entire training procedure of training a neural network for
hundreds of iterations.
And we got, you know, exact gradients and we could use gradient based optimization to tune
our hyper parameters.
And this is like, you know, really exciting because before that, everyone had to use these
black box methods like random search or vision optimization that is kind of like Harry.
And then after that, there was like this paper learning to learn by gradient descent,
by gradient descent.
It's like an amazing title.
I wish that I had thought of that title for our paper, which is like much more boring
title.
Yeah.
Doing the same sort of ideas.
And you know, reinforcement learning people are really excited about this right now.
So, but there's another way forward, right, which is to train a neural network to look
at a problem, maybe, you know, take in the hyper parameters and then just directly output
the optimal weights of a neural network.
So skip the entire training procedure and just have the hyper parameters, the weights.
Yeah, exactly.
Yeah.
We're still going to have to tune some hyper parameters, but we can train a neural network
to just directly output the optimal weights.
And that sounds maybe done because if you think, well, if you think what I'm going to do
is now train a whole bunch of neural networks like I did before and then, you know, learn
to predict the final outputs, then that would be slow and that would be a waste of time.
But it turns out we can train a neural network to produce, like, I'll call this like a
hyper network because it produces the weights of another neural network.
I can train a hyper neural network to produce an optimal neural network without ever having
seen an optimal neural network.
I can just have it, you know, start off produce a bad neural network and then ask, you
know, use back prop to determine how should I have adjusted the parameters of my hyper network
so that it would have given me a better actual network.
And then this gets very hard to talk about and everything is very good and confusing.
I thought you were going to go in a direction of something like a GAN or something like
that.
Maybe.
I mean, so the GAN does have this generator and I mean, I guess, yeah, I didn't really
thought about that.
We could train again to produce a network, but I guess the discriminator would have to
see sort of the, our train networks and sort of real optimal networks and distinguish between
them.
But the whole point is I want to avoid ever having to start with a bunch of like optimal
networks.
Right.
So I guess we can call this amortized optimization where amortized optimization is just
like, okay, we learn to do optimization by sort of practicing, producing the optimal
thing.
You know, this idea has been sort of staring us in the face because this is what variational
autoencoders do.
They do amortized inference and that's sort of like a name of this little subfield where
they say we're going to learn to look at the data and train a neural network to produce
the optimal posterior, like the optimal probability of the latent variables.
And again, these are trained in the same way where we never see the optimal posterior directly.
We just can use gradient signals to tell us how to get better and better.
And then after we train for a while, we sort of hope that we're almost optimal.
Cool.
Well, one thing I want to say is that the second idea of the using the hyper network to
avoid training is due to my student John Lorraine.
This is the beauty of this job is now I get to look good because all these brilliant
students are coming up with ideas and then I get asked about them.
So, but I have to give credit, we're credit to Steve.
Awesome.
Awesome.
Well, thank you very much.
This was a really interesting conversation and certainly has a lot of my neurons firing
trying to figure out all the stuff that we talked about.
And there's a lot of interesting stuff to dig into here.
I appreciate you taking the time.
Oh, my foot.
Awesome.
Thank you.
All right, everyone, that's our show for today.
Thanks so much for listening and for your continued feedback and support.
Thanks to your support, this podcast finished the year as a top 20 technology podcast on
Apple Podcasts.
My producer says that one of his goals this year is to crack the top 10, but we definitely
cannot do that without your support.
What we need you to do is to head on over to your podcast app, rate the show, hopefully
we've earned five stars, leave us a glowing review and share it with your friends, family,
co-workers, Starbucks baristas, Uber drivers, everyone.
Every review and rating goes a long way, so thank you so much in advance.
For more information on David or any of the topics covered in this episode, head on over
to twimmolai.com slash talk slash 96.
Of course, we'd be delighted to hear from you, either via a comment on the show notes page
or via Twitter at twimmolai.
Thanks once again for listening and catch you next time.
