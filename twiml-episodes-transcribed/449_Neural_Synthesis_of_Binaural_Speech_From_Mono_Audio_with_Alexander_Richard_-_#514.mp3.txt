All right, everyone. I am here with Alexander Richard. Alexander is a research
scientist with Facebook reality labs. Alexander, welcome to the Twomol AI
podcast. Hi, thanks for having me. Absolutely. Really looking forward to our
conversation. Let's get started by having you share a little bit about your
background. How'd you come to work in machine learning? That is basically just a
chain of coincidences. I was never planning to do it. I was planning to study
music, but you know, in order to have a good career. Yeah, but in order to have a
good career, you know, you need to be extraordinary, which I am not, that's
basic. So it was like, that was my backup plan. And like in the first semester at
university, it was in Germany at university, where they had a big institute for
speech recognition. And there was before smartphones were big, before there was
Alexa, before there was Siri. And they had like this open house one day, and you
know, free food, free drinks, as a poor student, of course, you go there. And they
had this demo where you could talk into a microphone at the words that you said
would just magically appear on the screen. And it was like, wow, I was fleshed by
that. That was amazing. So that was probably the point when I decided I really
want to go into machine learning and this exactly what I want to do. So I
basically stayed at this institute until I got my masters for like four or five
years and then changed to computer vision for the PhD and randomly ran to some
guy I cited a lot because he was doing the PhD in the same topic that I did. And
he was like, you know, I graduated, I'm at Facebook now. We're looking for interns.
It's a slightly different field than what you're doing. But don't you want to
join us? I'm never going to leave Europe or Germany. But well, for an internship,
why not? So I came to Pittsburgh. I joined this lab and it was, again, an
amazing experience, like the first time that I could really see that with my
education, I can work on something that can potentially change the way we
communicate and really have a big impact on people's lives. And there was so
exciting that I completely made up my mind. And half a year later was like, okay,
I graduate and I want to go back to Facebook. There was a great project, a great
time and you work with great people. So that's how I ended up here. Super random.
Awesome. Well, tell us a little bit about Facebook reality labs. I'm assuming
there's some kind of CMU connection given. I mean, there are a lot of
industry labs in Pittsburgh, probably due to CMU. Our director, yes,
I used to be a CMU professor. Okay. I think that's the origin of it all. And
that's also the origin how Facebook reality labs came to the city. Yeah, I mean,
the lab is all about social talent presence, right? So our mission, so to
say, is, well, if we would have succeeded already, then we wouldn't have this
conversation over video conferencing, right? You wouldn't be a small rectangle in
my big room. I wouldn't be a small rectangle in your big room. But we would
both be wearing a virtual reality headset. And we would stand right next to each
other and having this conversation in 3D in virtual reality in a shared space.
And that is really the promise and the mission of this lab that we try to accomplish,
which I believe is super exciting. And as I said, which is like this one thing that
can really be a big leap forward in the way how we connect over the
distance, which in my view, makes it so exciting to be part of that. Nice. And as
the lab, what's the relationship between the Facebook reality lab and
the Oculus? Yeah, I mean, it emerged from Oculus. It used to be Oculus research,
actually, at the time when I interned there. And then transitioned into Facebook
reality lab, which is now the big branch, which is looking into VR and AR
at Facebook. Okay. Awesome. Awesome. So tell us a little bit about your
research interests. We're going to be talking in particular about one of your
papers, Neural Synthesis of Binaural Speech from Mano Audio, which was an
ICLR best paper award winner. But you know, more broadly, I'd love to hear
your general research interests. Right. I'm particularly interested in
anything audio visual. And particularly, you know, in generative modeling from
audio and visual input. And that is like, I believe one field that has been
largely forgotten in early years of computer vision, that the visual modality
always just gives you very limited information, particularly when you think
about a webcam, where the resolution is low, or when you think about a virtual
reality headset, where you just have very limited sensory data. And audio is
like one of these modalities that gives you a lot of cues that can fill in
the gaps. So it holds like a big promise on improving whatever we developed
over the last years in computer vision. And really filling in what visual
sensors in specific circumstances cannot give you. And I found that super
exciting. So I was really starting to focus looking particularly onto how can
we combine audio and vision in a generative modeling setting where we really
want to generate something like realistic looking faces in the end. And you can
imagine that this is difficult if in this specific application of virtual
reality, if you wear a headset, your face is partly occluded, you just cannot
observe anything. Your data is always lossy. And occluding audio is a much
hard task or put in other words, it's much easier to get the complete audio
information that is surrounding you than to get the complete visual
information. Yeah, can you can you give an example that you think really
illustrates the, you know, the ultimate power of combining audio and video?
Yes, I think we should talk about two things there. One is clearly the
sensing side. So what is the input, right? And let us take the example of a virtual
reality headset. We might have a camera that is kind of facing your mouth, but
it's very hard to really get good illumination of the interior of your mouth
and have some accurate tongue modeling. It is very hard to get all the mouth
all the mouthclosures correct. Like I have a beard, right? There is a camera
facing towards me with a skewed angle. It is extremely hard to see every detail
of my lips. But with audio, we get these details. I can't produce the P sound
without closing my lips. I can't use the M sound without closing my lips.
All these correlations are clearly some that we need to explore. That is like
on the sensing side, but then we also work obviously on the on the output
side where it comes to putting things into virtual reality. And we have
this correlations between audio and vision there as well, right? I mean,
I just have a single microphone here, but if we had multiple microphones
and I would be snapping my finger, you should hear this from two different
directions. And that needs to be some audio visual correspondence, which
is frequently forgotten. It's freaking like, yeah, okay, we generate something
where I am talking and I can do things with my hand, but we have mono audio
and the systems. And that seems like leaving out half of the signal that
is there. Yeah, I think, you know, thinking about video games that are
kind of purely, you know, generated, you know, in many cases, like, you know,
it's as much as much more like a movie production than like a generated
something that a model generates. You know, I used to hearing kind of stereo
sounds and, but I'm imagining that, you know, that's a lot more difficult
when you're creating generative scenes from a model of some sort.
Yeah, absolutely. I think one of the big problems here is that in video games
it's usually fine if you have something that is just plausible, meaning
the sound is coming from the right direction and kind of fits the environment
that's all good. Same for the visual part, right? You have a synthetic
video character that doesn't necessarily authentically look like you.
So plausibility is fine in video games, but for the mission that we are pursuing
plausibility is not enough. We really want to have accuracy. We really
want to represent yourself in virtual reality that your loved ones, your
friends, people who know you are close to you can really recognize you and
recognize all the subtleties in your voice, in your facial experience.
But all the expressions, this micro expressions that you do, right?
That needs to be there if you want to have a real social experience.
And that is the big difference to movies and games where you can just post
process and just need to produce something that is plausible.
Right. Right. Yeah. For a long time now, the kind of holy grill and ARVR is
this idea of the uncanny valley, like there's something, there's just
something wrong. I think for a long time it's been pretty basic things,
you know, the resolution of the visor isn't high enough. I think you're
pointing to, you know, maybe we've overcome some of those challenges.
And now the uncanny valley frontier is a lot more subtle. Is that part of
it? Absolutely. Absolutely. It is. Whenever we have social
interaction, there is so much that we are trained as humans, right?
So I have like decades of training talking to people. You have a lot of
training talking to people and we really know how a social conversation
between humans works and what these subtleties are and how to interpret
them. Well, let's say our mind knows, but we do not know this actively.
Yeah. So it's very hard to quantify. So how do we teach a machine to do
this to do something we cannot even quantify ourselves? And that is the
massive challenge now to go beyond the uncanny valley with the
subtleties of communication. And our approach there is to argue that
everything needs to be metric, meaning whatever we can measure in reality
should be transferred like that into virtual reality. If we do that
right, then automatically we will have all these subtle social cues and
virtual reality as well. However, if we would resort to a solution that
is only plausible, we might transfer something into virtual reality
that substantially changes the meaning of what you are saying.
Like, I don't know, it's my smile, a generally happy smile, or am I
being sarcastic? This is very important for our conversation.
But if we misinterpret that because we generate something that is
just plausible, but not accurate, not metric, then we have a problem
because we changed the meaning of conversation.
I'd love to have you dig a little bit into that comment a bit more
about metrics and having everything that you can measure in a
physical world, be measurable in virtual reality. The thing that
the thought that it prompts is that there are an infinite many things
that we could measure in the physical world and to treat all of
those as discrete metrics in the virtual world. And then, you
know, try to train a model, for example, that's looking at, you
know, so many metrics. You know, that's a stark contrast to the
way things that the way things have generally been trending,
which is, you know, let's just take pixels and try to, you know,
focus on pixels and then the networks, if we can throw enough
compute at them, they'll figure everything out. Can you elaborate on
kind of the way you think about that?
I mean, absolutely. So when we talk about training a network,
we still want to optimize a metric loss. Like in standard L2 loss,
what everyone else is doing in the vision community as well,
when you have generative models, it's, you see that frequently
when you have supervision, you optimize your L2 loss against
your supervised signal. So we do not really try to reinvent that.
But what we are saying, when we say we want to be metric,
means our systems have to learn from the best possible measurements,
meaning if I only give you data where you see like a snippet of
my mouth and maybe images of my eyes and a mono audio signal,
something we can maybe get from a headset, from a virtual reality
headset, we cannot expect that we can make a realistic representation
of that in virtual reality, if we do not have better measurements,
meaning it all starts with accurate measurements, highly accurate
measurements. So the research that we do is like you would come to
Pittsburgh, you get a 3D scan of your face, and that is prior
information that is accurate measurements about how your face moves.
And then the next step is to take this metric information
of this priors that the models have learned and take the VR headset
input. And you know, we can take these parts that are visible and
transfer them into virtual reality. But there are parts that are
not visible that we cannot measure while you're wearing a headset.
And for this, we need the strong priors. And the strong priors
is what we get from very accurate measurements in the sense that
we need a kind of set of people where we have high fidelity,
3D scans of facial motion, facial expressions, so that we can
fill in the gaps. So, you know, whenever information is not
present, how can we get the closest to reality? Well, we need
some kind of prior in whatever way. So you mentioned, you know,
going to the lab and getting scanned the paper that we'll be
talking about falls under the context of this broader effort
called codec avatars. Is that what that is? That scanning
process? And exactly. It's also a little bit more about the
broader effort. Absolutely. The idea of codec avatars is that
you can have an avatar of yourself in virtual reality that
looks like you, sounds like you, moves like you, of course,
not autonomously, but driven by you while you wear the headset
with this lossy sensory input. And this whole problem looks like
you were not talking about the forget the name of the apple
avatars cartoons. We're talking. No, no, we are realistic
avatars. Exactly. Ideally, indistinguishable from reality,
of course, this is a very high bar that we said for ourselves
there, but we are talking about photo realistic avatars.
Because again, the same point that I made before, if we really
want to transfer and transmit all these subtleties of
communication, a comic style avatars just not enough. There is
not this very personalized specific smirk that you might
have when you smile and your very specific facial expression.
So we need something that is photo realistic. And well, the
way we approach this is essentially, well, it all starts with
like accurate data measurements. So we need enough facial
data to be able to make a high quality 3D reconstruction
in VR. We usually talk codec avatars, since for like
encoder decoder. So when we talk about the decoder side, that
is what would generate your highly realistic 3D representation
in virtual reality. And that is what we tried to learn from lots
of data where we have lots of cameras capturing your face.
And then we are able to reconstruct your face in 3D. Of course,
while you are being captured, you cannot do every kind of
expression that you can do in reality. But what we can ask you
to do is some peak expressions, some example expressions. And
we know that neural networks are magnificent interpolation
machines. So we feed this data into neural networks. And then
if you have some facial expression that falls in between two
different peaks, we can generate a very accurate and faithful
representation of that specific expression just from what
the network has learned from the peak expressions that you
can be doing. So this is the decoder side of codec avatars. On
the encoder side, we are talking more about how can we take the
sensory input from the headset, which is obviously lossy, we
cannot place hundreds of cameras around here, right? How can
we take this very limited data and map this to a representation
where the decoder can interpret enough to reconstruct your
face how it looks like. And as you can imagine, this is a very
challenging problem that we have on the visual and on the
audio side, because the data that we can collect for generating
this 3D representation, well, we can have multiple cameras
et cetera, of your face, but we can never have paired data
with whenever you're wearing your headset, because your face
is occluded by the headset, your sound might be different
because you know, you're in a massive capture stage that is
loud because there is a sea and stuff like that for visual
captures, but then for audio captures, you might be in another
room, I have high ceilings that might be reverberation. So all
these differences in the domains that we have on the input
side and on what we want to generate, this poses the big
problem of how can we actually connect these two parts, the
encoder and the decoder. And that is one of the big challenges
that's probably a bit deep if we want to dig into the details
here for codec avatars in general, but I'm happy to answer any
questions in specific if you want to dig deeper on that.
Since you're inviting us to elaborate a little bit on the
challenges there, what makes the connection between these two
the key source of challenge in this problem domain?
Right. So what people have shown is that, you know, you have
variational auto encoder, you have generative for zero networks.
So if you learn a representation of data that you captured,
we have shown that we can generate faithful and highly accurate
representations of, for example, your face or even of sound,
if you look at what's going on in the Texas Beach community,
that is extremely realistic and works extremely well.
What is a bit lacking is the question if you have a noisy
input for audio that could be speech with like a lot of background
noise with kids running around with car driving with an AC
running for vision that is you wear the headset, you have
difficult illumination, you have varying background because
you move around. And the question is, how can we use this
information to really condition our models that can generate
this faithful representations and still match exactly what we
have seen on the sensory input? And yeah, so one way we approach
this is by essentially looking at the renders in VR.
So if you think about the whole pipeline, you have headset
inputs, we want to decode this to some numeric representation
that can be transmitted from the transmitted to the receiver.
And then on the receiver side, we want to have this big network
that creates this code and generates your face out of that.
The question is, how do we match the code? And yeah, for that
we basically apply some standards, domain transfer techniques
it's based on guns based on some other techniques, it is
always difficult to evaluate this because a major problem is
that we do not have correspondences. So how do we really
know that what we represent is absolutely faithful and 100
percent faithful. And yeah, that is where one of the big
research challenges lies for the future.
And is the idea that you've got the headset and the headset
has cameras looking in at the eyes? And is there a, is there
a research foundation that comes out of, I don't know where
we'll come out of, you know, biology, psychology, neuropsychology
that says that the eyes are robust enough to tell you
everything that might be happening with the face?
We try to stay away from these kind of priors because
as I said, there are several things in communication that we
just cannot quantify and even psychology on neuroscience
cannot really tell us which parts of the face are important
for which parts of nonverbal communication specifically.
So what we try is really to get the most amount of data we
can get from a headset, whatever restricts us in terms of
hardware, but that is our approach to this and not relying on
something where we introduce human knowledge, which in the
end might be wrong or might be biased. And it's just not
data driven anymore in that case, but there's an assumption
that there is that relationship, the eyes, you know, we're
going to look at the eyes and we're going to try to predict
the face and with enough data, we hope that that relationship
holds.
Absolutely. I mean, if you just look at the anatomy of your
face, you see that some parts of your face, just if more
muscles can expose much more motion and that is like areas
around the lips, which are certainly, you know, harder or like
more important to actually observe than your ears, which barely
move at all and which are very easy to just plug in from
prior information that we have from you.
Yeah. Yeah. Awesome. Awesome. So tell us about the specific
paper, the neural synthesis of the neural speech from
Mono audio. What's the kind of what's the setting and
motivation? Yeah. Let's draw a quick line to computer vision
or computer graphics. 3D neural rendering is big in that field
for years and has led to massive improvements. And for some
reason, audio was kind of lacking behind that, right? So when
you look at like, how do you do specialist audio in computer
games and movies, it is all breaking down to linear time and
variant systems. You have some filters that you measure in an
idealized anicoic setup. And then you just like stack a bunch
of linear filters on top of each other and get some plausible
reconstruction. And we wondered why this is the case and why we
do not follow the same route that computer graphics is going
where you have some 3D renderers that you can train and to end
from data. Because that is ultimately what we want, right? If
we want to represent reality, we do not want to have a linear
function that is as close as possible. But we want to have
data that we can measure and that we can optimize against.
And that was basically the premise we started this work with.
And it was kind of interesting to see, first of all, to see
how much better traditional signal processing and audio
processing is compared to how good traditional computer
graphics methods were, right? So if you look at a traditional
render of a computer graphics method, traditional face renders,
you had facial models that were crude approximations and we're
really looking on Kenny. And if you look at results like
recent progress in neuro rendering, that looks super realistic.
In audio, it seems that this gap was much smaller probably
because audio is well understood on a mathematical way has
less uncertainty in it inherently. But we were still figuring
that there is there is the stack of linear transformations
that you do in signal processing that can gradually introduce
more and more errors. And you always make assumptions, you always
make an assumption that your room behaves in a very specific
way. You make assumptions that your ears behave in a very
specific way, in the way, you know, how they modify sound.
And that is not really true. Let me tell you about a funny
experiment we did initially. We used by neural microphones
that we stuck into the ears of a colleague of mine. And he was
just walking around and recording sound. And I was putting on
my headphone and listening to that recording. And because our
two ear shapes were so different, to me, it always sounded as
it was coming from behind my head. And that was clearly not
the case in reality. So there is clearly something where this
traditional signal processing based approaches reach their
limits. And if we have a data driven approach, we can really
learn something from evidence from data and do not have to make
modeling assumptions. And we could show in this paper that we
can really improve the quality of audio in this case. And I
think it's a very exciting direction to think about 3D
modeling of audio in the way that we think about 3D modeling
of graphics. And so when you talk about improving the quality
of audio, you know, drill down into the specific metric that
you're looking at there.
Yeah, also super difficult. We of course went went down the
first way that you always do in computer vision. You
optimize an else who lost on the raw waveform. Why shouldn't
it work? If you reach zero loss, you are perfect. So it's all
good. The problem is in reality, of course, you never reach a
zero loss. And then the question is, what kind of errors do
you have? And it seems that particularly in audio, this
discrepancy between the value of your L2 loss and the
perceptual quality of the sound that you generate is pretty
massive. So you can easily imagine that more so than a video,
you can easily imagine that you have like tiny deviations
that just imagine you have a sine wave in audio. And you
have like a tiny flickering, you always say pretty close, but
you have like a high frequency signal that flickers. All of
a sudden, you have the super annoying high frequency noise
in your signal that we perceptually find really disturbing
and can immediately say, well, this is not real. So that was
really challenging. And we were diving deeper into this
looking into the loss functions and how to train audio. And
one of the observations is that the L2 loss has a very
undesirable property audio is basically a composition of
amplitude information and face information amplitude, meaning
how high are the peaks of the sine waves and face information
meaning, where does the sine wave start? How much is it
shifted, right? And then we can add up potentially infinitely
many sine waves. And this is the audio signal that you can
hear, essentially a four year decomposition. And what we
figured out is that the L2 loss, you can actually show the L
2 loss optimizes the amplitudes aggressively in the
beginning of the training process of a neural network, but
completely neglects the face information. That means you
try to fit amplitudes of signals where you shouldn't even
fit the amplitude because if you just would shift the signals
accordingly, it would solve all your problems. The L2 loss is
not doing this inherently, it is keeping, it is trying to
match the amplitudes, but it's not trying to match the face
and it's not trying to match the face at a very late time
in training, which turns out to be a problem. So the very simple
solution on the loss side is to explicitly optimize for this
face information. However, face information is highly
difficult because as soon as you have noise and in every real
life measurement, you do have noise as soon as you have
noise, this is not really reliable data. So we went a step
further and tried to implement something about noise in both
the amplitude direction and the face direction. Yeah,
absolutely. I mean, we tried to record audio in a room and
we tried to get something that is as clean as possible, but
you might have electrical noise because of your setup. You
might have some airflow that is, I don't know, going into the
microphones and that is that is air pressure, right, which is
interpreted as sound. So all these subtleties really corrupt
your data constantly when you have audio, and you have to
account for that. So yeah, we kind of shot for a solution that
is on the modeling side. And I believe that is something that
has been highly successful in all of deep learning that you
take some component of traditional processing and incorporated
in your networks, the most prominent success stories, certainly
the convolution operation, right, bringing convolution
into your networks has boost computer vision. In audio
dynamic time, warping is a very old traditional technique
where you want to align signals in some way. And we essentially
incorporate that as a neural network layer, so fully
differentiable neural network layer to solve this face
problem because this time warping allows you to shift
components of the audio signal left or right depending on where
you want to have them. And yeah, it turned out that once
more incorporating one of these traditional methods can have
a big impact on how well neural networks perform.
And so in the problem that you've formulated, what are the
the inputs is it, you know, clearly there's a mono audio
and the output is going to be some stereo audio. But are
you also, you know, you localized as their location inputs
like how what are the very inputs? Yeah, let's let's talk
about this very specific application where we want to
generate by a neural sound, meaning that is spatialized where
it can hear where it's coming from, but that also sounds as if
you perceive it with your ears, you know, all the transformations
that your ears do to the sound. The input clearly, as you said,
is mono audio. I might have a microphone very close to my lips
and I just speak into that and that is all I need as input
signal, but I also need to know is the relative position
between you and me. If we are in a virtual environment and you
are standing two meters apart to the right of me, your
voice will sound different as if you're like super close to my
face and on the left of me. So that is like the second input.
Where are we in the virtual space in relation to each other?
And given this input, we want to modify your mono signal
based on these positions so that you receive a stereo
signal on your headphones that is, you know, simulating this
effect of you hear it with your own ears, meaning the
spatialization is correct and the transformations, the signal
undergoes with your ears is correct as well. So that is
essentially the problem setting of this particular problem.
What we were restricting ourselves to is one set of ears
because you can imagine ears are like fingerprints like
everyone has a different ear shape and it's a really unsolved
problem how to generalize to different ear shapes. So we use
like a kind of medium ear shape, a generic ear shape that
works kind of well for everyone. But apart from that, it's
really this full setting of having mono signal and just the
positions and getting a complete manner of signal synthesized
on your headphones.
Okay. And what is the the data set that you use for the
modeling? How did you collect it? What challenges did you find
there?
Yeah, interesting question. Glad you asked for that. It was a
long process to actually get a successful capture. We started
trying something where you can buy mannequins with silicon
ears that behave similar to you flesh of similar properties.
And we started with that and placed just such a mannequin
into a room and then had a tracking system that we can put
markers on top, basically a motion tracking system,
markers on top of the mannequin, markers on top of the
participant who is walking around and talking to the mannequin.
There were ears and microphones in the ears of the mannequin
and we get sound so far so good. We have tracked everything
so we get all the information we need, right? The position
between mannequin and speaker and also the binarital sound.
The initial round of this was completely useless and without
any success. We had an AC running in that room and if you
spend like, I don't know, two hours in the room, it gets
hot at the AC turns on and off and on and off. And then you
could hear footsteps of people and you could hear people
breathing as they came too close to the microphone. So you
can imagine all these things, all these lessons that you
that you learned when you do a data collection that you never
did before.
We can look at that as some kind of regularization or
the main adaptation or something.
That is true. That is true. But with the premise that we want
to learn metric information that we really want to get out
with reality sounds like regularization is bad, right?
Regularization is just like another prior that we impose
another assumption that we make on the signal and ideally we
want to learn from a signal that is as clean as possible.
So it was really worth going all the way of making a room
acoustically treated that it's silent enough and that you
really just have the default room reverberations and these
audio effects that you have when you talk in a room, but no
noise floor anymore.
We basically went down all this way, installed some
acoustic paneling and improved the capture setup
significantly, which in the end gave us successful data.
And it should also be mentioned that like, if you compare
to how this has been solved before, you need to build an
unequake chamber and then you would record just the
transformations that are here that your ear are doing in an
unequake chamber because you do not have room reverberations
there, right?
So you just capture that as a bunch of fixed spatial
positions and you have a model for that.
And then you have another model where you measure room in
pulse responses at different places in a room.
And then again, you try to stack this together.
That is what traditional processing is doing.
And one of the big disadvantages there was you always measure
as discrete spaces in time.
And if you can imagine you walk around, this is how sound
changes, right?
Doppler effect.
If I get closer to some sound source or closer to a
microphone, the sound waves basically compress and you get
like these, yeah, these motion effects.
And you can never capture this in a static setup.
So we were really shooting for something where we have this
dynamic setup where we track a person in real time and we
have moving trajectories of sound that we can model.
And I think, yeah, that is probably one of the reasons that
the system in the end turned out to work so well.
That this is like the first time that you have this realistic
scenario of moving sound sources.
Now, I can even with that said, I can imagine a pretty broad
spectrum of complexity.
Like I can imagine looking at the relative position of the
participant in the mannequin in 2D space.
Can imagine then looking at them in 3D space.
You know, I can imagine then, you know, extending that to
include like radio direction for each other part.
Like how did you manage that complexity factor?
Absolutely.
I mean, we tried the simplest thing which is restricting
ourselves to a kind of donut of social interaction.
People usually tend to feel uncomfortable if I can very
close them and conversations.
Or if I'm very far, I wouldn't have a conversation but I
would move close efforts.
So we restricted ourself to the circle around all like basically
this donut around people where social interactions typically
happen.
So we had a restricted input space which can be helped.
One thing that we completely misjudged is a mannequin is
always standing still.
If you have a user in virtual reality, putting on a virtual
reality has seen something here and something.
First thing that you do is look around in all the directions.
So we tried to imply our first model and it was all
breaking because this was all movement that we didn't
see and that we didn't account for in this simplified setting.
So we actually moved forward from this donut allowed
near field captures where people get really close to the
mannequin where people move around to talk from the top
and from below into the ears of the mannequin to cover
much denser spatial coverage.
And then in the end, there are other effects that you have
to have to think of not just positions but even speech
direction.
I have a head which is modifying the way I sound.
So if I look in this direction, I sound different to you as
if I look straight to your face.
And these were like effects that we didn't model at all.
Well, yeah, this is neglectable but it is honestly not.
If you want to learn from the raw waveforms and that is such
sensitive data, then this is not neglectable and you really
need all these additional information that you can get.
So talk a little bit about the method or approach
that you came up with for solving the problem.
Yeah, absolutely.
I'd love to.
So the massive advancement in deep learning for audio is
certainly wave net, which was this 2016 paper by DeepMind.
And it has ever since revolutionized text to speech and
vocoding, meaning transferring mouse pectograms of frequency
information into actual waveforms of speech.
So this is the logical starting point whenever you do
something where you have generative audio and you want to
generate a raw waveform.
And that is where we also started from a standard wave net.
And we just used a conditioning on this position information
between transmitter and receiver.
We were hoping that this would do the job.
This wouldn't be like a exciting research inside,
but in terms of getting the job done that would have been
amazing.
But as you can expect, you get a lot of distortions.
You get into a lot of problems for the reasons that
I told you before that, you know, it is hard to match
the face.
Also, this original wave net architecture has been designed
to produce something that is plausible, but not necessarily
matrically accurate.
So we were facing all these problems.
And from there on, extending the architecture.
One of the major components is that, well, if you have
a BINORAL audio, this sound takes some time to travel
from me to you.
And we can clearly geometrically map that.
But this is never correct because what does it mean
geometrically?
If I look in this direction, it travels in a slightly
different way.
If you look away, sound has to travel around your head.
So just going by distances is not enough.
But what we do want to do is having some warping that
tells us, OK, we are at time T when I emit the sound.
What is time T prime when this specific part of sound
arrives at your ears?
And this is what we implemented as a form of neural
time warping.
Basically taking dynamic time warping, this very traditional
approach to align two time sequences, which is not
differentiable and putting it into a differentiable setting
while still maintaining physical properties.
Meaning, sound has to be causal, right?
I mean, if I say something, it cannot arrive at your ears
before I actually said it.
It has to be monotonous.
So what I say first arrives at your ears first.
Unless I move faster than speed of sound, which unfortunately
I can't.
And we have to incorporate these physical properties
into the model.
And this is what we essentially did.
We designed this differentiable version of dynamic
time warping as a neural network layer.
And this enables us to maintain all the physical properties
of sound, but at the same time still aligning
exactly between the transmitter and the receiver.
And from that point, if you have an exact alignment,
standard deep learning approaches, convolutional
time domain convolutional architectures, do a very good job
in changing the amplitudes of the signal.
But the face of the signal has already been accounted for
to a large amount because you already
want everything in the time domain on the right position.
So you solve one of the major problems, one of the major
issues that L2 lost a struggle with doing.
And if you start from there, essentially, was a piece of cake.
And so what assumptions did you need to make to make that
differentiable?
Not too many assumptions, actually, because you can formulate
most of these, most of these components in a quite straightforward
way.
One assumption is clear that you do not move faster than
speed of sound, but for any kind of human.
It's reasonable.
Opposition that is absolutely reasonable.
Absolutely.
One maybe slightly bigger limitation or assumption that
we had to make in this whole architecture and this whole
network is that the acoustic properties of your environment
are the same.
So what we cannot model is changing acoustic properties
because, I don't know, you stand super close to a wall
and you would have different diffractions.
That would require you to really have a prior on rooms and to
really have a good representation of the 3D scene you are in.
So what the model learns is a kind of global representation
of the room acoustics you are in.
But it does not learn that you are standing in the corner of
the room and it sounds significantly different.
This is something we need to solve if we want to bring this
technology into virtual reality and want to have a realistic
impression.
But yeah, in this initial work, this is something we had to
abstract from and really focus on only like the global
appearance of sound and getting the spatialization and
banalization correct.
I'm trying to remember if we've talked about this already
in a slightly different context, but the relationship
between the metrics that you're training on and like
perceptual metrics.
You know, from everything we've said thus far,
you know, you've got a room that's kind of a fixed
size and fixed geometry.
You've got the mannequin ears and you've talked at length
about how sensitive the models are to those specifics.
It seems like you can do very well on your metrics,
but still from a perception perspective.
And maybe this is more generalization than from person
to person than perception, still have issues.
Can you talk a little bit about that?
Yeah, that's a very good question.
We need to make a distinction here where we have good
generalization properties and where we don't.
Because in the setup, you always have a transmitter and
a receiver.
That is fundamentally different from vision where we kind
of assume if you render an image, we all see it in the same
way.
We do not care that maybe your eyes are slightly different
than my eyes.
For audio, we do care because our ears are fundamentally
different.
And we generalize really well on the transmitter side.
Meaning it doesn't really matter which kind of person is
speaking.
We can train on a very small amount of people.
A few dozen is enough.
And generalize to any kind of arbitrary voice and get
an accurate representation and reconstruction of this
arbitrary transmitter voice.
When it comes to the receiver, of course, the generalization
is well, we have one fixed ear pair.
So if you have an extraordinary ear shape,
you might have something that is typically called front back
confusion that you cannot tell if the sound is coming
from the front or from the back of you.
And these are issues we can really only solve if we can
generalize towards ears.
Which again gives us this close connection to codec
avatars because if we do have a realistic representation
of your head and of your ear shape of your, you know,
of your head geometry in the end, what we hope is that we
can personalize the receiver side based on this ear
geometries that we know from codec avatars.
And how about the room geometry?
Does that generalize well across different types of
environments?
Yes.
Quite frankly, no, it does not.
If you record audio in this kind of setting,
or you were a virtual reality headset, your microphone
would be super close to your mouth.
So you do not really pick up a lot of these room acoustics
anyway.
And the signal you have is almost unequally.
It doesn't really have a lot of the information of the room
you are in if it's very close to your mouth anyway.
So from that point of view, that is fine.
The question is, how can we, you know,
transfer the system into different virtual rooms?
At the moment, the way it sounds is the sound of the room
where we collected the data.
This might not necessarily be the only room in virtual reality
you want to be.
And honestly, you probably want to be in a variety of rooms,
which means we have to put some work into disentangling
this part, disentangling the binarialization part
from the room acoustics part.
And one way to go there is to have a better capture setup
where the room where you record all this specialized audio data,
all this binarial audio data is quasi-anicoic.
So basically doesn't really have room responses at all.
And if you have that, and then if you have at the other time,
captures in multiple rooms where you can learn
characteristics of the room responses,
you can disentangle these two effects
and model them sequentially.
But again, in a data driven way, this is very hard
because then the question is again,
how do we get correspondences between the data we capture
in rooms and the data we capture in this binarial capture stage?
It's something we are looking into,
but we do not have a solution to that yet.
Okay.
So we've talked about the setup.
We've talked about the approach.
We've talked about metrics and evaluation a bit.
We've also talked about this big picture
that you're trying to get to.
What are the next steps from where you are
to get you towards the bigger picture?
Where does this research direction take you?
Oh, yeah, absolutely.
As I said initially, my interest is working on audio visual topics
and this binarialization, the sound binarialization paper
is basically an approach to do 3D rendering of sound.
So where's the visual component here, right?
And again, if we talk about codec avatars, encoders,
and decoders, there are these two elements
where we need to fuse the vision in.
On the decoder side very clearly, we talked about ear shapes.
So we need to make sure that the visual information
can influence the way you perceive the sound based on your ear geometry,
based on your hatch geometry.
And on the side of the encoder data,
we talked about how we kind of rely on a quasi-anechoic mono input, right?
But what if you are in a noisy environment?
What if there are kids running around?
We don't want to binarialize that sound, right?
If I'm in VR and your dog, your kids,
whatever is making noise in the background,
it's not present in the virtual environment here.
And so we need to get rid of that.
So that is like another big direction where we have to think about
how can we take the sensory data from the headset,
use the audio sensors, the microphone that we have,
but also use the visual sensors to make sure
that only what you actually say,
only your actual speech is transferred into virtual reality.
And we get rid of all of these other noises
and all of these other background signals
that we are not interested in, you know,
transferring into virtual reality.
Well, Alexander, thanks so much for taking the time to share with us
a bit about what you're up to.
Congrats on the best paper award.
I guess I should ask you, you know, what is your...
What do you think were the factors that
led to the judges picking out this paper for that award?
I mean, the judges are probably the best people
to talk to for that question, but if you ask me,
I believe it's probably this novelty of the field,
you know, moving for audio,
which is still predominantly signal processing
and linear time invariant systems
to what data-driven deep approaches,
having something that is, you know,
a 3D rendering equivalent for audio.
And I think this is just like something
that has not been done before.
And it is probably kind of unclear why audio
is like lagging behind the computer graphics community
in that sense.
And I believe that is probably the big impact of the paper
that we are able to make this step into 3D audio rendering
that is data-driven and deep.
That's awesome. Well, congrats again on that.
I'm looking forward to the follow-on, I think.
A lot of interest in the community now
about kind of multi-modal, multi-channel,
combining audio, video, and other modalities.
Oh, absolutely. Sure.
Yeah, I mean, we are always happy to host interns
and have people over who are interested in this work.
So super happy if someone here is interested
in this direction of audio-visual.
We are always looking for interested.
Mission reach out to you.
Oh, of course.
To our recruiters, whatever you want.
No, it's always welcome.
I feel it's a community that needs to grow.
I feel the community really looking at this multi-modal
problems, this audio vision problems, is too small.
I skimmed over the nearest publications
and there were like nine papers that had sound in that title,
which seems to be too low for a conference
of the size of the nearest.
So I really feel it's a field that needs to grow.
Yeah, absolutely.
Well, once again, Alexander, thank you so much for joining us.
Thank you for having me.
