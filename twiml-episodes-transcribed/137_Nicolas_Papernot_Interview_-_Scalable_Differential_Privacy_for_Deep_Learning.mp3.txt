Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
This week on the podcast, I'm excited to present a series of interviews exploring the emerging
field of differential privacy.
Over the course of the week, we'll dig into some of the very exciting research and application
work happening right now in this field.
In this episode, I'm joined by Nicola Paparano, Google PhD Fellow in Security, and graduate
student in the Department of Computer Science at Penn State University.
Nicola and I continue this week's look into differential privacy and machine learning
with the discussion of his recent paper, semi-supervised knowledge transfer for deep learning
from private training data.
In our conversation, Nicola describes the private aggregation of teacher ensembles or
Patee model proposed in this paper and how it ensures differential privacy and a scalable
manner that can be applied to deep neural networks.
We also explore one of the interesting side effects of applying differential privacy to machine
learning, namely that it inherently resists overfitting, leading to more generalized models.
Thanks once again to Georgian Partners for their continued support of the podcast and
for sponsoring this series.
Georgian Partners is a venture capital firm that invests in growth stage, business software
companies, and the US and Canada.
Most investment, Georgian works closely with portfolio companies to accelerate adoption
of key technologies, including machine learning and differential privacy.
To help portfolio companies provide privacy guarantees to their customers, Georgian recently
launched its first software product, Epsilon, which is a differentially private machine
learning solution.
You'll learn more about Epsilon in my interview with Georgian Chang Liu later this week.
But if you find this field interesting, I'd encourage you to visit the differential privacy
resource center.
They've set up at gptrs.vc slash twimmelai.
And now on to the show.
All right everyone, I am on the line with Nicola Papreno.
Nicola is a Google PhD fellow in the Department of Computer Science and Engineering at Penn
State University.
Nicola, welcome to this week in machine learning and AI.
Thank you so much.
I'm very happy to be here.
I'm very happy that we have finally connected.
We have been trying to get this conversation going for a while now.
So I'm also really interested in digging into differential privacy, which is, I think,
in an area of very interesting promise with regards to machine learning and AI in the
intersection with privacy.
But before we jump into that, why don't we take a few minutes and have you introduce
yourself to the audience and tell us how you got involved in machine learning and AI?
For sure.
Yeah.
So I joined Penn State about four years ago just coming in from France where I'm from.
I started finishing up my undergrad and I joined this lab here at Penn State, headed
by Professor Patrick McDaniel, who is a security researcher.
And at that time, it had been very recently demonstrated that state-of-the-art vision
models were vulnerable to small perturbations of their inputs, which researchers called
adversarial examples.
And so that was something very interesting to us because from the machine learning perspective
adversarial examples have a lot of implications because they mean that models don't generalize
as well.
As we expected them, and at that point in about 2014, machine learning models were starting
to get pretty good sometimes upperforming humans at some of the tasks.
And so it was already a surprising fact from the machine learning perspective that models
are so susceptible to these small perturbations.
But for us, from the security perspective, the implications were much more serious and
that sort of attracted my interest in machine learning, especially when you see applications
to transportation, to the energy sector with machine learning being able to sort of optimize
some of the decision-making that's traditionally done by humans.
Or with healthcare, all of these domains, it is clear that machine learning is being applied
more and more widely in these critical applications.
And so as a security researcher, when the technology becomes so pervasive, you of course
consider all the potential implications as it becomes a target for adversaries and try
to envision how adversaries will try to manipulate these systems.
And so adversarial examples were sort of the first thread vector that I've considered
against machine learning.
And from there, branched a lot of interests more widely with anything related to machine
learning and including a little later a few, about two years later in differential privacy.
All right.
Awesome.
Maybe a great place to start is for you to talk a little bit about differential privacy,
what it means and what some of the objectives are.
So differential privacy is essentially a framework for understanding where the privacy information
may leak in algorithms and it's also a framework for preventing this information from leaking
from the algorithms manipulating the data.
So at a very high level, you can think of differential privacy as defining two worlds.
So you have assumed that your application is collecting data about a particular population.
And so you would have two worlds, one world where you would have all of the people in the
population in a second world where you would have just one person missing from this population.
And so what differential privacy requires is that this, the algorithm that you're going
to run on the data should not have a statistically different behavior, whether you are in the
first world or in the second world, and what that means for all of the users in the population
that you're learning from is that the behavior of the algorithm is not going to reflect
any specific information about a particular user, but whether it's going to only reflect
general patterns that are found across the population.
And so this is why differential privacy has become one of the gold standards in privacy
and probably the most widely applied is because this definition does not make any assumptions
about the adversary.
So essentially that means that regardless of the knowledge that the adversary has, regardless
of what attack techniques the adversary will think of, the guarantee that you have provided
that the behavior of the model does not reflect a specific people in the population will
stand in the future.
And so differential privacy has been applied to many algorithms.
For instance, it was applied to databases to answer queries for a SQL databases while
protecting privacy.
And more recently, it has been applied a lot to machine learning and especially deep learning
in the last couple of years.
And there is a really natural, I would say, synergy between machine learning and differential
privacy and the way I like to think about it is that differential privacy is a way to give
a cost to put a value on some of the failures that machine learning may have.
So when you think about it, overfitting to a particular training point is likely to
violate privacy.
So if you memorize the information about a particular training point that you have in your
set, then whoever contributed this training point may not be able to retain its privacy
because the model's behavior will very likely be largely influenced by this specific
training point.
And so with differential privacy, you're essentially saying during the learning process, if you
overfit to this particular point, you will have to pay this particular amount of privacy.
And so you have this connection between privacy and achieving generalization, where in a way
privacy is a worst case guarantee and generalization is more of a mean average case metric.
But when you have privacy, you are helping the model in a way to behave better.
Does it apply equally well across different types of models and algorithms, meaning traditional
machine learning algorithms relative to deep learning algorithms and neural networks?
Yeah, differential privacy has been applied to all sorts of machine learning models,
starting with very simple things like logistic regression and all the way to deep learning
more recently.
What happens is that you can make most algorithms differentially private by randomizing their
behavior.
So for instance, if you're learning a neural network, you have several ways that you can introduce,
you can guarantee that you're providing differential privacy, you can perturb the inputs.
So if you have a data set, you can randomly flip some of the labels, randomly perturb some
of the inputs in a way that would provide differential privacy.
You can also perturb the models parameters.
So that would imply, for instance, when you're applying stochastic gradient descent to
learn a model, you would take the gradients of the model with respect to using the training
data and you would add some noise to these gradients before applying them.
And that would result in a model whose parameters provide differential privacy.
And then you have a third way of achieving differential privacy, which is by perturbing
the output of the model itself.
And so essentially, you can adapt all learning algorithms to introduce privacy by randomizing
either their input, the model itself or their output.
And the question is whether you can calibrate this random behavior to be able to provide
on one hand a strong privacy guarantee so that requires that you analyze in the worst-case
settings how the training data will influence your predictions.
And on the other hand, so if you once you have this guarantee of privacy, you also want
to make sure that the model is going to perform well.
So you want to calibrate the noise so that it is large enough to protect the privacy
of the training data, but also small enough that it will not harm your performance.
And I guess what has been very exciting in the, I would say, the last year or two is
that we are seeing different scenarios where we can calibrate the noise in a way that
it doesn't harm the performance.
And actually, it providing privacy can allow us to also achieve at the same time very strong
performance.
And again, that it is because there is this synergy that you can exploit between privacy
and utility.
If you think about it, if a model is making a prediction that is very likely to be correct,
it means that this prediction is supported by patterns found in the training data that
are patterns that are widely found in this training data.
And so it also means that this prediction does not depend on specific points in the
training set.
And as a consequence, you can provide strong privacy for that prediction.
And so, yeah, this is something that I'm very excited about because that means that we
would be able to switch from a setting where privacy is often presented as something that
you need to trade off with utility to a setting where you can achieve privacy with very limited
or no impact on performance.
So I would say we're sort of in between these two right now.
We're, for most algorithms, we're still in the first scenario where we have to trade
off some utility to get privacy, but in some cases, we are moving to the second scenario.
And that's extremely exciting for the field because it means that differential privacy
would be more likely to be applied in all sorts of algorithms.
Right.
Right.
And so just to capture that last point, the idea there is that traditionally we've got
this trade off between model generalization overfitting and creating models that overfit
is one of the biggest challenges in employing these models beyond the training date against
real world data, but differential privacy because its goal is highly aligned with generalization,
it also serves to kind of fend off overfitting.
Right. So just to be clear, again, generalization and privacy are different things.
So generalization is a metric that looks at the average performance and privacy is a metric
that looks at the worst case performance.
So you do have this direction where when you achieve privacy, you help towards generalization,
but the other way around doesn't hold.
So if you have generalization, that doesn't guarantee privacy because, again, privacy is
a worst case setting, but it is true that providing privacy is eventually one direction
for proving the generalization of learning algorithms that is definitely true.
Okay.
So maybe can you tell us a little bit about your specific research in this area?
Do you have any recent papers on this that we can maybe talk a little bit about?
Yeah.
So my research in differential privacy and machine learning has focused on a particular
approach, which is called Pate, so the name is French, even though it's hard to believe
I did come up with the name on my own, but it stands for private aggregation of teacher
on samples, so the idea for this approach is that we'd like to achieve privacy and a rigorous
form of privacy, so differential privacy, but also at the same time, be able to explain
why we achieve this privacy in a very intuitive way, and the idea is that rather than having
this mysterious guarantee that you provide the privacy, you're able to convey an intuition
why the approach protects the privacy of the training data that is very useful, because
again, differential privacy often comes with its set of theorems you need to prove things
in a language that's not necessarily easy to understand for a lot of people, unless they
have lots of experience in that area.
So basically the approach looks at the training data and begins by partitioning the training
data.
So if you have one set of data, you'll create end partitions from this set of data, and
the only constraint is that these have to be real partitions so that there is no overlap
between the subsets of data.
And so from each of these subsets, what you do then is that you learn a machine learning
model independently on each of these subsets, and the nice thing is that you can learn
these models using any learning algorithm.
So if you want to use a decision tree, if you want to use logistic regression or even
deep learning, that's okay.
And so at this stage, you have learned n different models on your training data in a completely
independent way.
So the models are looking at this trend subsets of the training data, and when they all
make a prediction on a particular input, if they all agree on this prediction, you know
that this prediction was made from a pattern that is general across the data, and not
about a specific partition of the data.
And so this is where the intuitive privacy guarantee comes from, because again, when we
ask all of these models and we call these models teachers, so when we ask all of the teachers
to make a prediction, if 95% of them all make the same prediction, we know that this
is a prediction that results from a general pattern from the data.
And so if we take as the common answer of the teachers, the prediction that is assigned
the most number of votes, so the one that the majority vote among the outputs of teachers,
then we have this intuitive privacy guarantee that again, this is a prediction that results
from a consensus.
The question now is how do we add noise to this mechanism in order to be able to prove
that it provides a rigorous guarantee in a sense that it provides differential privacy.
And so what happens there is that you introduce random noise at the outputs of the teachers.
So you randomly perturb a subset of the labels predicted by the teachers, and then afterwards
you make the aggregation.
So what this means is that the aggregation is not performed on the true votes of the teachers,
but on the randomized votes of the teachers, and using the noise and by calibrating this
noise according to the privacy guarantee strength that we want to achieve, then we are able
to guarantee that essentially every time the teachers make a prediction, this prediction
is made with a certain level of privacy, and so in privacy, in differential privacy,
there is a particular parameter called epsilon, which essentially measures how indistinguishable
the two worlds that I mentioned earlier are.
And so the smaller the value of epsilon is, the stronger the privacy guarantee is, and
essentially, using this mechanism, we can provide a specific epsilon value for each of the
queries that we make to the teachers.
So then we have another step in the approach, because at this point, every time the teacher
is respond, they are going to reveal very little private information, but over time, this
private information is going to accumulate, and so you would have to bound to fix the
number of queries that the teacher is answer.
And so what we do is that we use the teachers to supervise the training of an additional
model called the student model.
And so this student model is going to learn from the teachers by sending them unlabeled
inputs and training on the private label that the teachers are returning as an aggregate.
And so essentially what this does is that it transfers the knowledge that the ensemble
of teachers learned from the sensitive data in a privacy preserving way into the student
model.
And so once the student model is trained, and we only need a fixed number of labels to
train the student, we can release the student as the model that makes predictions, and
that student model can answer as many predictions as we want.
The cost in terms of privacy is fixed.
And what that means as well is that if an adversary tries to inspect the parameters of the
student model or to look at the predictions of the student model, in the worst case, what
it can recover is the private labels that the student learned from, that it received from
the teachers.
And because we provided these labels with differential privacy, we can guarantee that there
will be no more private privacy leakage than what is allowed by the guarantee that we
were able to prove mathematically.
And so once we have this student, this is essentially the end product.
This is what we want to release to deploy in our application.
And then we can also simply just discard all of the teachers, all of the training data.
We don't need it anymore to use the student.
And so that's the approach that we've been working on.
And this is where sort of the idea of having a synergy between privacy and utility came
and that I was describing earlier in our conversation is that you can see here clearly that the
utility of the label is reflected by the number of teachers that agree on that prediction.
So when you have lots of teachers, almost all of them agree on the prediction.
It's very likely to be a correct prediction.
And because they all agree, that means that you can perturb their answers a lot.
You can introduce lots of random notes and still provide, and that will result in a very
strong privacy guarantee.
And so that's how we're able to make the synergy between privacy and utility explicit
with the Pate approach.
And that's extremely exciting, and we've tested it on a few data sets and it really gives
you some very high utility at very strong privacy guarantee.
So I'm very excited to see where we can push that technique in the future.
You may have just answered one of the questions that I had, but to kind of take a step back,
you've got this three-step approach to creating privacy guarantees using this model, this
approach.
One is the first is you partition the data set and you train an ensemble of these teacher
models, and these can be mixed models so they don't have to be uniform.
You then use the predictions of these teacher models in kind of a consensus manner to determine
an intermediate prediction, which is then used to train a student.
The student model that you're creating is kind of your ultimate model that you deploy.
And because of the process that you have gone through, there are some privacy guarantees
that you have made around the student, and it's also impervious to long-term data leakage.
Is that correct in a nutshell?
Right.
Before I go into my question.
That's exactly it.
Okay, so one question that I had was at the second step where you're perturbing the outputs
of your teacher models, would you say that that perturbation increases the privacy that
is attained or is it simply required to make the privacy guarantees?
Does that question make sense?
It makes a lot of sense.
It is required that you perturb the output of the teachers.
It is not possible to achieve differential privacy without introducing some randomness
and the algorithm's behavior at some point.
And that's because differential privacy, basically, if you don't have this randomness, you
would not be able to learn anything meaningful because the definition would prevent you from
learning anything from any point.
And so this randomness is what a lot of it is.
Can you elaborate on that, meaning the definition of differential privacy?
Right.
Because if you want to have this guarantee that the behavior of the algorithm is identical
on the two worlds, this guarantee has to be statistical, if it were deterministic, you
would not be able to learn anything because the algorithm could not learn anything from
any of the training points.
And so that's why you have this randomness because it introduces this ambiguity when
the adversary sees the same prediction or a different prediction from the algorithm,
it is not able to know whether that change in the prediction resulted from a change in
the training data or a change in the outcome of the randomness that was sampled.
Right.
And so I'm, I guess I'm asking more is maybe like a hair splitting or semantic kind of
question, but I'm really asking it as a path to truly understand what's happening here.
And the, you know, the goal is, you could argue that the goal is privacy as opposed to differential
privacy.
Right.
Differential privacy provides a guarantee of privacy, but what I ultimately want is privacy.
And what I'm trying to get at is if we didn't do steps two and three, it strikes me that
there's an argument that we've already, you know, created some degree of privacy just
by doing the partitioning of the data and this kind of ensemble consensus approach.
And I'm wondering if that is in fact the case or, you know, is it perhaps the case that
no, not really we haven't even achieved any measure of privacy with doing that or it's
very little.
Is there anything that you can say about the degree of privacy that we've achieved
just in that first step as opposed to, you know, the differential privacy specific pieces
that the second and third steps add?
Yeah.
For sure.
I mean, you're basically coming at one of the design goals of this approach is that we
want to be able to provide an intuitive notion of privacy in addition to providing this rigorous
definition of differential privacy. And so you are right that if we only perform the
unsombling and output in the aggregate answer of this ensemble, that would provide some
notion of privacy that is not differential privacy, but you could perceive it as a form
of privacy.
And that really drives at the fact that privacy is an extremely subjective notion.
And so different people will have different expectations in terms of privacy, but it
is true that the approach, even if you don't add noise or train the student, provides you
with this intuitive definition of privacy.
However, I want to be careful with that because I would say that our intuition when we design
algorithms to provide privacy is very, very often wrong and it's hard to capture all
of the ways which could result in a privacy leakage.
Is there an example that comes to mind of how the ensemble approach alone doesn't create
the kind of privacy that we want?
Yeah, I'll give you an example of the ensemble and then with a completely different application.
Okay.
With the ensemble, let's assume that you're sending a query to the ensemble and exactly
half of the teachers assign the label cat and half of the teachers assign the label dog.
If you then change the training data of one of the teachers which the adversary could do
and you run the same algorithm, so you train the teachers and answer the same prediction,
then because you change the training data of one of the teachers, in the worst case, you
should assume that that teacher will change its prediction.
In cases where you have exactly the same number of votes for two classes, changing the predictions
of one teacher might change the outcome of the aggregation because it might make one class
more likely than the other or just invert the order of the classes.
If you don't have this noise, the prediction of the ensemble can depend on the predictions
of a single teacher and so you don't have this consensus part where you have this large
overwhelming consensus among the different models which provide you this intuitive notion
that you're having a very large agreement that reflects the fact that the pattern is general
across the training data.
So that's why the noise provides privacy because it makes these scenarios ambiguous and
prevents the adversary from understanding whether the change in the prediction results from
the change in the training data or from the noise.
Another example to sort of motivate differential privacy is that there are lots of ways that
you can achieve intuitive notions of privacy and one very common way is to anonymize the
data.
So to remove any part of the data that could be used to infer the identity of the people
who contributed this data.
And this has happened in the past.
So there was a data set which was released by Netflix where they had anonymized the data
which was essentially ratings for movies and ratings from a large pool of users.
And so they removed all of what they thought would be something useful to infer the identity
of the persons who assigned these ratings.
But then some researchers found later that if they perform what is called a linkage
attack so they use a second database which in this case was the IMDB database to sort
of cross the records from the Netflix database with records from the IMDB database because
the ratings were unique enough in both databases they were able to link the records from the
Netflix database to the records from the IMDB database which is public.
And so they were able to recover the identity of a lot of users in the Netflix database
which was supposed to be anonymous.
And so that's just an example of why providing these notions of privacy that are intuitive
is a good start but it's not enough to claim success.
And I'm not saying that we should not be anonymizing the data, it is a good practice to
anonymize the data but we shouldn't rely on it as the ultimate way to provide privacy.
And so what is nice again about different privacy is that the definition is robust to
all these attacks that use auxiliary information like what's the case with the IMDB database
anonymization did not take into account that the adversary would have access to this public
information which helps mount the attack.
And so with different privacy you don't have this problem so the adversary can have all
this knowledge, it's not going to impact the strength of your guarantee.
And so it makes it, it makes it, the definition is constraining in the sense that the analysis
that you have to perform is extremely worst case, it's very constraining.
But once you've managed to provide differential privacy you're really providing a guarantee
that is robust in the face of a lot of future attacks that people may come up with.
Can you elaborate on how differential privacy specifically applies in the case of this
Netflix example you gave?
In particular the differential privacy we've talked about thus far has to deal with a model
that I've created on some training data set and the kind of result or product of this
differential privacy process is a you know call it a privacy robust model.
In the case of the Netflix example you described what Netflix provided that you know potentially
got them in the trouble was this data set even though it was somewhat randomized.
Is there a way that how does differential privacy play here could differential privacy
have been used to create a better anonymized data set for them to share or is differential
privacy not applicable if you need to actually share your training data?
That's a very good question so as I mentioned before there are really three places where
you can think of providing the privacy it's at the input of the algorithm and at the output
of the algorithm.
So there's several techniques to provide differential privacy at the input one of the techniques
that is easiest to get an intuition for is called local differential privacy where the
idea is that you're going to perturb the data itself and so one easy example to think
about it is let's assume you're collecting data from a pool of users and so you're going
to ask each of these users to flip a coin and depending on the outcome of that coin flip
they will do if it's head state they will respond with the true answer to your question
and if it's tails they'll respond with a completely random answer and so if you do this
across a very large pool of users because you know with what probability people will respond
with the random answer or with the correct answer you can still collect data that will
be useful for you to extract statistics or to perform analysis on but each user that
participated in this process can still have what is called plausible deniability that
they did not provide the correct answer so if you ask any particular user they could
just tell you I responded the random answer and you have no way to verify that because
you don't have access to the coin flip that outcome that they had and so that's a particular
way where you can collect data from users and achieve differential privacy without training
the model on top of that data first meaning so netflix and providing this data set in addition
to eliminating any personally identifiable information they could have also randomized some
number of the labels on the data set and it sounds like doing so in such a way that
didn't inherently change the statistics of the data set right and yeah so for instance
the yeah they could have for each rating flip the coin and depending on that output the
real rating or a random rating for just as an example and if I'm building a model using
this data and I and I know that this is the case or I suspect that this is the case can
I can I take advantage of that what do you mean by taking advantage of that meaning if
I'm you know if I if you know this is the netflix prize and you know their netflix has published
this data set and I suspect that they've done this or they've told me that they've done this
can I use that knowledge to increase the chance that I'll you know win the prize but you know
make my model better performing I don't think that would the having knowledge about the fact that
they they collected the data in a privacy preserving way would provide you in any advantage in
the competition but what what it does point on is it's a very nice property of differential privacy
is that once you've achieved differential privacy and you release statistics that are
resulting from this differentially private process any post processing of the data
maintains the differential privacy so once you've achieved differential privacy you can
analyze the data as as much as you want you can train another model on the data and the the
guarantees is is still provided so it holds in the face of post processing which is
another very very nice property of this definition okay and so then just to to take a step back and
and be clear the this this coin flip and substituting random data is that in and of itself enough to
give me a differential privacy guarantee for this data set yes so it's called local differential
privacy there are some variants of event to to improve the utility of the process but yes it
and in short and at a very high level that that is the idea that is sufficient to provide differential
privacy ah great great okay awesome and so then the the third phase of the pate model that we
talked about was training the student and the idea with the with training the student model
was that it the student basically is trained on this aggregate or ensemble parent model and the
ideas that it provide using the student provides you with a further a set of guarantees because the
student never had any access to the underlying training data is that the right way to think about
it or is there I feel like I may be missing a nuance here yeah it is mostly the right way to
think about it there are two components so the one that you mentioned that it did not have access
to the underlying data provides robustness to to adversaries who attempt to to inspect the
parameters of the model the nice thing about training the student is is mostly that it fixes the
number of questions the number of predictions that the teachers will answer so if you could in
practice if you were able to guarantee that no one will be able to inspect the the teachers
parameters or to to extract the teacher models you could use the the ensemble of teachers as
sort of a differentially private API which would respond to user queries and it would each query
would be provided with a certain privacy guarantee but again over time as you answer more and more
and more queries you would accumulate a privacy budget that would become unreasonable and eventually
you would not be able to provide a meaningful guarantee with respect to your training data and
again if as as adversaries and we're seeing this more and more were attacks are able to extract
some of the training data or some of the parameters of the model by only having access to its
predictions so if that is also possible then we have to be worried that the adversary would be
able to recover some information about the training data that the teachers had access to and that's
why training the student is very nice because the student only has access to this
limited set of labels from the teachers we're able to to guarantee first that the overall
budget that we spent in terms of differential privacy is fixed so once we've trained the student
we don't access the teachers anymore so we don't perform any computation that depends on
the sensitive data and and again because the student was only trained on this this private data
even if the adversary is very strong and able to recover the training data of the student
that data is is not the sensitive data that we were protecting so so we have this this strong
guarantee in this case as well okay sounds like a way to think about that is that we use the
student for the same reason why your computer or your iPhone will start injecting delays if you
get your password wrong for you know three times it's to prevent someone from using a large number
or infinite number of brute force attacks to you know attack the model or attack the password
yeah I guess that's that's that's a way to think about it is that once the adversary has access
to to the system and is able to make lots and lots of queries to it so like in your example
trying to enter as many passwords to eventually figure out if that is the correct password then yes
as the as the adversary makes a very large number of queries eventually it is able to
infer some information about the system and and so that's that's what the student protects
against because it disconnects the model that is predicting from all of the teachers that were
that had access to to this to the sensitive training data and and the the reason that it's
it's not as easy as preventing against someone using your phone to to enter the password and
eventually figuring out is so in that case you you can introduce this delay and that makes the
attack much more impractical but often what happens is that machine learning models are exposed
through an API through online or or or or a local network and so essentially you could envision
that the adversary would distribute its queries and so that would make it very hard for the owner
of the model to to know whether a particular sequence of queries is an attack trying to find
information about the model or if it's just a set of legitimate users who are actually using
the model as intended and so you have this point where after you hit sweet spot the the utility
and the privacy of the model will will start degrading and so it's it's hard to give numbers
like guidelines for dollars it's a really is a question of both the complexity of your models
which sort of indicates how much data you will need to train them and also the complexity of the
task how many outputs there is in the task and in our research we've we've sort of
tried different applications different models with smaller smaller tasks and task with hundreds of
outputs and the number of teacher will vary in our case between a hundred and five thousand so you
so it really really depends if you have the data to support more teachers than that makes your
life easier but but again the the nice thing is that you're able to use any machine learning
model so once you have a model that works without privacy if you have a lot of data then you can
sort of apply this framework quite easily by partitioning the data and just training your model
several times on these on these different datasets so that's that's the nice advantage
about the approach okay and I don't recall off the top of my head in the case of you know let's
say the case of deep learning you know I don't recall what the you know if training time is
you know linear in the you know linear sublinear super linear in the number of training examples
but I'm wondering if there if you've done any work to look at like theoretical bounds on the
relative training time using a partitioned model versus a model trained on the entire data set
I'm sure other people have done this looking at it from you know just the scalability perspective
but it seems like if there's some advantage there just from a scale perspective and now you're
overlaying the privacy piece that's kind of further supports doing this yeah no that's that's
very good point so and we haven't looked at the theoretical aspects of this of this trade off
but in practice what we found is obviously if you have the resources you the approaches by
by definition very parallelizable so you you can train all of the teachers simultaneously
and so you if the resources are there you don't see an overhead on in terms of training time
compared to just training one model one thing is even if you have limited resources because each
model gets less data to train from it also it also trains faster in general and so the I would say
the computational overhead is not a main limitation it just it's it's kind of like the rest of
deep learning if you have lots of resources that makes your life easier but but in that case
because you're able to sort of prototype your model without privacy and then once you're ready to
once your model is properly fine-tuned then you you have all the right number of players right
I prepared matters you can sort of apply the pate approach you you know what what architecture
you're going to go for and so you just parallelize the training overall all of the subsets of data
and so that's relatively straightforward is the implication of what you just said that in practice
you're going to want to design your model by training against the entire data set as you usually
would and then apply differential privacy as a kind of a step further towards production as opposed
to designing you know from the beginning with differential privacy in mind and a you know such
that you may never train against all of your data but always do this approach yeah so it really
depends what data you're handling and how sensitive it is I'm I'm sure that in some cases it's
just not possible to to train maybe for illegal reasons on the whole entire data set if you're
not involved to provide privacy but what I wanted to basically to say is that if you have
about the same amount of data that you would use in a partition then you can prototype your
model and then replicate it over the other partitions and so that that is a very easy and practical
way to to limit the overhead of implementing the privacy because you only implement privacy once
you have the model that you're confident with that is a good fit for this this particular task
yeah can you say that again I don't think I followed it sure so so let so let's say you're going to
train on 50,000 inputs and basically once you have you you can prototype your model on a subset
of the data and once you have a good model then at this point only you you have to replicate it
over the different subsets of data to to train the unsolvable and then achieve the privacy but
you can sort of do the prototyping for a single teacher before you train the entire ensemble I guess
that that was my point okay and so the then the the data sets that you train the teachers on they
don't need to be it sounds like strict partitions of your data they can be overlapping no they
they have to be non overlapping okay if if they are overlapping then this changes the way that you
analyze the privacy guarantees because essentially when they are non overlapping changing one point
of the training data has an impact on multiple teachers right right so if you have overlaps then
you have to if there is for instance a point in three partitions changing that point would change
three teachers in the worst case right and so you have to take that into account in the privacy
analysis so in our case we always considered non overlapping partitions and if if you wanted to
use our approach and the the guarantees that come with it out of the box you would have to to
use non overlapping partitions you could adapt the privacy analysis to take into account the fact
that the partitions are overlapping but that would probably require more work than the benefit
that you would you get from having overlapping partitions okay okay I thought that what you
were saying previously implied duplicating the data that the teacher sees or overlapping in some
way oh no but I guess I may have said something ambiguous what I meant is that you you you can
perform sort of the parameters search of your model for only one of the teachers and then apply
that search to all of the other models in the ensemble which which reduces the amount of time
that you have to spend training the training the models because so in other words instead of
training the 500 teachers you train one teacher and use the parameters for all 500 teachers so you
use the hyper parameters like the hyper parameters right the number of layers so once you found
an architecture that works well for this particular data got okay sorry about that
no that's that's fine it's it's very subtle because even you you have to take into account that
although every time you use the the data that is sensitive you have to take that into account
if you want to provide the the guarantee of privacy of the overall approach so it can be it can be
very tricky okay any parting words or thoughts on what's next for you in this line of research
sure so I I'm extremely excited about this synergy between privacy and and utility again I think
that's that's really a deal breaker and in terms of the next steps I think the the most compelling
would be to to apply these techniques to to datasets that that I've traditionally been very hard
to tackle with with privacy preserving techniques or to get good performance at to sort of show the
implications of these techniques to real world applications and there is also one thing that
I'm interested in before we go there what are some examples of those datasets that are
traditionally difficult I mean there there's a lot of progress being made in healthcare for
instance and obviously these approaches to provide different for privacy are are a good way to
address some of the concerns that users contributing these datasets may have there are many many
many examples even in in applications related to justice or anywhere where the data is sensitive
and we're at the same time making progress with applying neural networks or other more complicated
machine learning because differential privacy used to used to be limited to to more to more simple
machine learning techniques like logistic regression and and so now that we are able to provide
differential privacy with things like that deep neural networks that it's very exciting because
that means we can look at tasks that are much more complicated to solve and that that means in
term in turn that will have a very beneficial impact on on society at large. Great. Well,
Nikola thank you so much for taking the time to walk us through this it is really interesting
and important work and I'm looking forward to kind of tracking it as as you and others in the
space progress it. Yes thanks thanks a lot Sam for for having me I really enjoyed our conversation
around privacy. All right everyone that's our show for today for more information on Nikola or any
of the topics covered in this episode head on over to twimmalei.com slash talk slash 134 thanks
again to our friends at Georgian partners for sponsoring this series and be sure to visit their
differential privacy resource center at gptrs.vc slash twimmalei for more information on the field
and what they're up to. Thanks so much for listening and catch you next time.
