WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.600
I'm your host Sam Charrington.

00:32.600 --> 00:36.800
We'd like to send a huge shout out to our friends at Waits and Biosys for their sponsorship

00:36.800 --> 00:43.000
and support not only for this podcast, but our upcoming event, Twimblecon AI Platforms.

00:43.000 --> 00:47.200
As you'll hear in my conversation with Lucas, Waits and Biosys believes that the more accessible

00:47.200 --> 00:52.280
transparent and collaborative the process of model training becomes, the safer and better

00:52.280 --> 00:54.480
the models we build will be.

00:54.480 --> 00:58.640
To enable this, they offer an experiment tracking platform for deep learning researchers

00:58.640 --> 01:00.040
and practitioners.

01:00.040 --> 01:04.640
To learn more about Waits and Biosys and get started tracking your modeling experiments,

01:04.640 --> 01:13.280
visit www.wndb.com, w-a-n-d-b.com.

01:13.280 --> 01:24.480
And now on to the show.

01:24.480 --> 01:53.600
Thank you for joining us for this week in machine learning and AI.

01:53.600 --> 01:57.960
You know, so with that in mind, probably a little bit of history and your background

01:57.960 --> 02:03.680
is probably a good place to start so we can start to kind of get the decoder ring in place.

02:03.680 --> 02:04.680
Totally.

02:04.680 --> 02:09.720
So, yeah, you know, I started my professional career at a company called Yahoo.

02:09.720 --> 02:14.280
I don't know if you remember them, but back in Santa Clara.

02:14.280 --> 02:15.280
Yeah, yeah, exactly.

02:15.280 --> 02:21.520
Back in 2004, 2005, I was actually working to convert their kind of rule-based system

02:21.520 --> 02:25.360
into a machine learning ranking system.

02:25.360 --> 02:30.360
So you know, ranking search results is one of the first real applications of ML.

02:30.360 --> 02:34.800
And then I went to a startup called PowerSet to do kind of the same thing that became Microsoft

02:34.800 --> 02:35.800
Bing.

02:35.800 --> 02:41.360
And yeah, this is back in maybe 2007, 2008 and power set initially based in Atlanta.

02:41.360 --> 02:43.360
Am I thinking of the right company?

02:43.360 --> 02:46.200
No, I don't think it was based in San Francisco.

02:46.200 --> 02:47.200
Okay.

02:47.200 --> 02:51.520
I mean, it was a search kind of a natural language search company.

02:51.520 --> 02:52.520
It had a lot of great ideas.

02:52.520 --> 02:56.840
Actually, a lot of the folks working there have gone on to do impressive stuff like the,

02:56.840 --> 03:01.520
you know, the GitHub founders were working there and Descartes Labs is another big kind

03:01.520 --> 03:02.520
of aerial imaging company.

03:02.520 --> 03:04.960
It was kind of, it was a fun place to work.

03:04.960 --> 03:08.320
There were a lot of super smart people there.

03:08.320 --> 03:13.000
And it was definitely way ahead of its time, right, trying to do deep natural language processing

03:13.000 --> 03:14.520
applied to search.

03:14.520 --> 03:15.520
Yeah.

03:15.520 --> 03:20.520
And kind of what I was seeing at both the bigger company and the startup was machine learning

03:20.520 --> 03:25.760
had tons of promise, but tons of obstacles to kind of make it work in the real world.

03:25.760 --> 03:30.120
And that's always kind of what's driven me is the applications of machine learning.

03:30.120 --> 03:34.680
And so I founded a company originally called Crowdflower that became Figure 8, which was

03:34.680 --> 03:40.040
all about getting high quality training data because that's such a bottleneck for building

03:40.040 --> 03:42.800
and deploying machine learning models.

03:42.800 --> 03:48.400
But one of the things that I saw when I was running Crowdflower and later Figure 8 was that

03:48.400 --> 03:52.000
there's a lot of problems that happened downstream, right, as people tried to, you know, take

03:52.000 --> 03:54.560
the training data and turn it into live production models.

03:54.560 --> 03:57.640
And so, you know, I think the next problem that people run into once you have the training

03:57.640 --> 04:03.640
data is figuring out a sane way to manage all the experiments that you do when you train

04:03.640 --> 04:04.640
your models.

04:04.640 --> 04:08.000
And so that was the inspiration behind starting weights and biases.

04:08.000 --> 04:15.720
You know, I think every decade, maybe I'd do another step in the machine learning.

04:15.720 --> 04:24.240
So I'm curious, what is the craziest thing you've seen in terms of the way people are managing

04:24.240 --> 04:25.240
experiments?

04:25.240 --> 04:30.640
Like, you know, I tell people all the time is I'm kind of laying out a, you know, landscape

04:30.640 --> 04:37.680
of, you know, machine learning and the process and how experiment management happens.

04:37.680 --> 04:41.520
Like, you know, sometimes you're better off if it's not even happening.

04:41.520 --> 04:47.320
Like, I've seen like crazy foul names with like hyperparameters in them and stuff like

04:47.320 --> 04:48.320
that.

04:48.320 --> 04:49.800
Like, do you see that kind of stuff?

04:49.800 --> 04:50.800
Oh, yeah.

04:50.800 --> 04:55.440
I mean, I think back to, you know, I remember in grad school, actually, I was, I was fairly

04:55.440 --> 05:00.160
sloppy at managing experiments, which is maybe why I am so passionate about this space.

05:00.160 --> 05:06.120
But I had a huge single file that I added in EMAX and yes, sporadically put notes in

05:06.120 --> 05:12.080
for all the things that I tried and I had, you know, kind of a crazy file naming scheme,

05:12.080 --> 05:16.040
you know, that would kind of evolve over time, if you will.

05:16.040 --> 05:20.440
But you know, actually when I was starting with advices, I, I spent a lot of time with

05:20.440 --> 05:24.400
my friends kind of studying the different ways that people do experiment management and

05:24.400 --> 05:29.080
actually think the most common approach is to have, you know, sets of directors, sets

05:29.080 --> 05:30.080
of files.

05:30.080 --> 05:34.600
And then these days, I think people typically use a Google doc and put notes for, for

05:34.600 --> 05:35.600
each of their runs.

05:35.600 --> 05:39.000
You know, I think everybody, you know, I've looked at lots of different people's, you know,

05:39.000 --> 05:40.800
Google Docs or whatever they use.

05:40.800 --> 05:45.320
And, you know, I mean, these are some of the most incomprehensible pieces of text.

05:45.320 --> 05:49.080
Like, I always wondered if people could actually go back and figure out what they were doing

05:49.080 --> 05:51.880
even, you know, a few weeks before.

05:51.880 --> 05:52.880
Right, right.

05:52.880 --> 06:00.600
So, yeah, I mean, I've come across everything from post-it notes to lab notebooks, to Google

06:00.600 --> 06:08.360
Docs, to spreadsheets, to, you know, the crazy thousand character long founding with all

06:08.360 --> 06:12.440
of the parameters dot pickle, you know, it's like, yeah, yeah, yeah, exactly.

06:12.440 --> 06:17.840
And then you start calling things like dash fixed and then like, dash fixed, dash fixed again.

06:17.840 --> 06:18.840
Right, right.

06:18.840 --> 06:19.840
Dash final, dash final, dash final.

06:19.840 --> 06:20.840
Yeah, yeah, exactly.

06:20.840 --> 06:24.840
That's the real way that they pattern.

06:24.840 --> 06:25.840
Right.

06:25.840 --> 06:26.840
Right.

06:26.840 --> 06:30.680
Hey, everyone.

06:30.680 --> 06:31.680
Sam here.

06:31.680 --> 06:35.880
Our conference, Chumul Khan AI platforms is right around the corner.

06:35.880 --> 06:41.800
So I want to take a minute here to share a bit about the now supersized to track agenda.

06:41.800 --> 06:46.360
On top of our great keynote interviews, including the one with Andrew Ng that I am so looking

06:46.360 --> 06:51.080
forward to, our technical case studies track will feature speakers from companies like

06:51.080 --> 06:56.800
Capital One, Comcast, Levi's and Zappos, all sharing about the architectures and approaches

06:56.800 --> 07:00.760
they've developed to support their machine learning and deep learning workflows.

07:00.760 --> 07:05.360
And the enabling technologies track will focus on tools and solutions that can help organizations

07:05.360 --> 07:11.200
like yours automate and scale various aspects of your machine learning pipelines.

07:11.200 --> 07:13.440
We have a ton of awesome speakers lined up for you.

07:13.440 --> 07:19.080
So head on over to twomulcon.com slash speakers to check out the agenda.

07:19.080 --> 07:22.720
The conference will be held in San Francisco on October 1st and 2nd.

07:22.720 --> 07:27.880
I encourage you to register now as tickets are going fast. The early rate is ending soon

07:27.880 --> 07:31.200
and you definitely don't want to miss out on this event.

07:31.200 --> 07:35.920
PS, stick around to the end of the interview with Lucas and you'll get a discount code

07:35.920 --> 07:39.000
good for 20% off of registration.

07:39.000 --> 07:41.240
Hope to see you there.

07:41.240 --> 07:47.160
So you kind of did some investigation as you're starting the company and found that people

07:47.160 --> 07:56.600
were all over the map kind of tending towards file and directory based experiment management.

07:56.600 --> 07:58.640
What issues did that cause for them?

07:58.640 --> 08:01.360
Or was it just fine but not pretty?

08:01.360 --> 08:06.360
Well, I think the biggest issue is just remembering what you did, right?

08:06.360 --> 08:14.000
So it actually reminds me a lot of, I remember my first job when I was in college at a summer

08:14.000 --> 08:18.160
job programming and honestly, I didn't really know about version control at the time, didn't

08:18.160 --> 08:19.160
really trust it.

08:19.160 --> 08:25.440
You know, not the best and most organized programmer and I remember I would have all these files

08:25.440 --> 08:26.440
and anything.

08:26.440 --> 08:33.200
What was the version control scheme of choice in your day?

08:33.200 --> 08:34.200
Oh, yeah, what was it?

08:34.200 --> 08:35.200
It was before SVN.

08:35.200 --> 08:36.200
It was CVS.

08:36.200 --> 08:37.200
CVS?

08:37.200 --> 08:38.200
CVS, yes, CVS.

08:38.200 --> 08:39.200
That was the first one.

08:39.200 --> 08:40.200
Yep.

08:40.200 --> 08:47.200
I remember when I first discovered CVS, it was just like, oh, man, this is awesome.

08:47.200 --> 08:54.440
Actually, keep track of this stuff and kind of roll back to something I did before.

08:54.440 --> 09:01.280
But the initial pain, I remember this originally trying to write papers, you collect a bunch

09:01.280 --> 09:04.280
of metrics on the run that you want to put in your paper and then you realize, oh, there's

09:04.280 --> 09:09.000
actually another metric that I'd like to include in my table and it's really hard to go

09:09.000 --> 09:13.520
back and recreate all the past experiments that you did.

09:13.520 --> 09:19.280
Even if you snapchat the code, the problem is with machine learning, there's more than

09:19.280 --> 09:20.280
just the code.

09:20.280 --> 09:27.440
There's the code and there's the hyper parameters that you used and there's the data set

09:27.440 --> 09:29.680
that you input into your run.

09:29.680 --> 09:35.720
So if you're not careful, it can be really tricky to even, people talk about the reproducibility

09:35.720 --> 09:41.400
crisis in machine learning, right, because it's hard to reproduce other people's runs.

09:41.400 --> 09:45.880
But forget about reproducing other people's runs, try to reproduce in a year, your past

09:45.880 --> 09:50.240
self, your one month ago self set, runs, you know, I don't know how many people can actually

09:50.240 --> 09:51.240
do that.

09:51.240 --> 09:52.240
Yeah.

09:52.240 --> 10:00.360
And so the big part of that is understanding the hyper parameters and settings associated

10:00.360 --> 10:07.840
with a given run, but you also mentioned the data, are you doing anything there on the

10:07.840 --> 10:12.960
weights and biases side or what are you seeing there?

10:12.960 --> 10:17.000
You know, we actually, in weights and biases, we don't yet like snapchat the data for

10:17.000 --> 10:18.000
your version, the data.

10:18.000 --> 10:20.920
There are some really interesting technologies out there.

10:20.920 --> 10:24.880
We just haven't seen a level of adoption yet that we're sure that something's kind of

10:24.880 --> 10:27.800
becoming the standard, but we're watching that.

10:27.800 --> 10:34.000
So we actually just, we snapchat your, the status of your code.

10:34.000 --> 10:39.400
So we'll take not just your latest Git commit, but any diff from your commit, because, you

10:39.400 --> 10:43.200
know, it flows a little different with ML where I think people are running lots and lots

10:43.200 --> 10:46.360
of experiments and you kind of want to snapchat every point, you don't necessarily want

10:46.360 --> 10:51.840
to have a Git commit or you can't necessarily rely on a user to do that between each run

10:51.840 --> 10:52.840
that you do.

10:52.840 --> 10:57.200
So we, we snapchat with just your latest commit and then a patch.

10:57.200 --> 11:00.640
And then we also, you know, capture hyper parameters.

11:00.640 --> 11:05.480
So I think the, the last step is the, the data, but we don't, we don't actually do that

11:05.480 --> 11:06.480
yet.

11:06.480 --> 11:09.480
Although, of course, if you're using some system where you have some pointer to the,

11:09.480 --> 11:13.880
the data you have, you could input into our system, you know, like a path to your data

11:13.880 --> 11:17.120
or some output of a, a data versioning system.

11:17.120 --> 11:22.640
And I should mention that by the time this podcast is published, the, my ebook will be

11:22.640 --> 11:28.760
out on machine learning platforms and there is a section where I talk about some of the

11:28.760 --> 11:32.360
technologies that are out there for data management and versioning.

11:32.360 --> 11:33.560
Oh, what are you seeing?

11:33.560 --> 11:41.760
I'm curious what, what do you think the ones that I've seen out there are, are the ones

11:41.760 --> 11:47.560
that come to mind at least are, there's a starter called DVC, which is like, I guess

11:47.560 --> 11:51.680
the sense for a data version control, but they also do versioning of models and stuff

11:51.680 --> 11:52.680
like that.

11:52.680 --> 12:00.440
And there's packet arm, I don't have a lot of data on adoption of either.

12:00.440 --> 12:07.560
But from a dedicated to solving this kind of problem, you know, a machine learning problem

12:07.560 --> 12:15.120
from a data perspective, both of those are probably the, the, the pure plays that come

12:15.120 --> 12:24.080
to mind, but they're also a number of kind of these end to end machine learning platforms

12:24.080 --> 12:32.280
that come at it from a data perspective or have like some kind of data versioning capability

12:32.280 --> 12:38.560
that kind of underlies the way that they handle data in their platforms.

12:38.560 --> 12:42.360
And we actually had an interesting conversation the last time.

12:42.360 --> 12:48.600
We spoke about the whole broad structure of this machine learning platform market and

12:48.600 --> 12:53.800
how you have all these kind of specialists and then you have all these end or or kind

12:53.800 --> 13:03.720
of wide folks that are trying to address like all of the problems in machine learning.

13:03.720 --> 13:08.600
And they all kind of start from somewhere or come at it from some angle and for some

13:08.600 --> 13:13.320
of them, the angle that they come at it from is data management, totally.

13:13.320 --> 13:19.800
But so it sounds like your perspective is that you don't need to fully solve that problem

13:19.800 --> 13:28.080
to get to some level of reproducibility or at least kind of useful utility.

13:28.080 --> 13:29.080
Yeah, exactly.

13:29.080 --> 13:33.560
I mean, it's just that back and sort of say that the sort of value prop of the experiment

13:33.560 --> 13:37.200
tracking tool we have and there's sort of three parts to it, right?

13:37.200 --> 13:44.320
So one is this kind of versioning and reproducibility and we want to make it as lightweight as possible.

13:44.320 --> 13:49.600
So we don't to have true reproducibility, we'd have to put a lot of constraints around

13:49.600 --> 13:56.520
you and so we just try to track as much as we can simply and you can add more things

13:56.520 --> 13:57.520
to track.

13:57.520 --> 14:02.680
But that's just one piece of I think experiment tracking is the sort of the versioning.

14:02.680 --> 14:08.160
I think the sort of second piece of experiment tracking is around visualizing what's

14:08.160 --> 14:09.160
happening, right?

14:09.160 --> 14:14.560
So, as your model's training, you want to see typically lots of different accuracies

14:14.560 --> 14:21.200
and lost curves kind of over time and over other axes and you also want to these days

14:21.200 --> 14:27.720
compare across tens, hundreds or we even see thousands or tens of thousands of experiments

14:27.720 --> 14:32.480
and there may be many different ways to look at kind of what's the best model or what's

14:32.480 --> 14:35.320
the best set of input hyper parameters.

14:35.320 --> 14:39.720
That's sort of a second value proposition that you get really well out of our tool and

14:39.720 --> 14:42.720
then the third thing is collaboration, right?

14:42.720 --> 14:48.040
So being able to kind of share the work that you did with a colleague or with someone at

14:48.040 --> 14:54.240
a different organization or even with your future self in a way that they can understand.

14:54.240 --> 14:59.080
So you kind of get all of those things with our tool and you sort of get them to the extent

14:59.080 --> 15:00.080
that you buy in.

15:00.080 --> 15:05.800
So a big thing for me is actually not to build a kind of end-to-end platform that's super

15:05.800 --> 15:10.200
heavy weight that requires a lot of upfront cost.

15:10.200 --> 15:15.920
So we try to make it just two or three lines of code that you add to your working thing

15:15.920 --> 15:20.560
and it can run anywhere, you can run an Amazon, it can run on an Jupyter notebook, it

15:20.560 --> 15:25.160
can run on prem, all these things kind of work and then you get reasonable defaults.

15:25.160 --> 15:30.640
And as you kind of add things like hooking us up to your Git state, hooking us up to your

15:30.640 --> 15:35.080
data version control, hooking us up to your underlying platforms, then we're able to kind

15:35.080 --> 15:36.080
of give you more things.

15:36.080 --> 15:40.280
And sort of the more information that you give and the more that you tell us about your

15:40.280 --> 15:44.440
specific state, the more we can give you full reproducibility.

15:44.440 --> 15:50.080
What are the couple of lines of code that you're inserting to get started doing?

15:50.080 --> 15:57.320
So basically on your client side, we have a Python library, so you basically do a pip install

15:57.320 --> 16:00.640
WNB and then you import a library.

16:00.640 --> 16:05.920
And then actually if you're using Keras, we have a one-liner that will instrument your

16:05.920 --> 16:09.800
code so you'll actually get a lot of value in a single line.

16:09.800 --> 16:16.520
If you're using PyTorch or TensorFlow, it's maybe two or three lines you call WNB.init

16:16.520 --> 16:22.400
and you pass in your configuration and then in PyTorch you call WNB watch on your model.

16:22.400 --> 16:29.440
In TensorFlow, we use a hook so you'll basically use a WNB hook so that during your training

16:29.440 --> 16:31.680
it keeps reporting to our model.

16:31.680 --> 16:35.000
And also if you're using TensorFlow, we have a different way of integrating.

16:35.000 --> 16:40.520
So we basically kind of worked with the sort of status quo of how people monitor things

16:40.520 --> 16:44.680
today and kind of made a really lightweight way to connect to that.

16:44.680 --> 16:50.280
And so you're using whatever native model introspection tools are available through these

16:50.280 --> 16:56.440
frameworks to figure out things like the model type and parameters and all that kind

16:56.440 --> 16:57.440
of stuff.

16:57.440 --> 17:00.120
If you don't have to pass that in explicitly to the library.

17:00.120 --> 17:01.120
Yeah, exactly.

17:01.120 --> 17:02.800
So we figure out what we can.

17:02.800 --> 17:08.160
So Keras and TensorFlow and PyTorch all have different ways of doing this.

17:08.160 --> 17:16.720
So we'll take your TensorFlow flags or your Keras config parameters and we'll save those.

17:16.720 --> 17:19.880
But if you want to save extra stuff, we make that really easy.

17:19.880 --> 17:25.680
It's just a single line of essentially it's a dictionary of inputs and you can add to

17:25.680 --> 17:26.680
that dictionary.

17:26.680 --> 17:30.800
You mentioned PyTorch and TensorFlow and Keras.

17:30.800 --> 17:35.600
These are all deep learning frameworks.

17:35.600 --> 17:39.920
Is deep learning the only use case for weights and biases?

17:39.920 --> 17:46.920
Do you tell folks to look elsewhere if they're trying to experiment tracking for more traditional

17:46.920 --> 17:50.960
models or not using one of these frameworks?

17:50.960 --> 17:51.960
Mostly.

17:51.960 --> 17:56.360
So our application is framework agnostic in the sense that you can use it with any framework.

17:56.360 --> 18:01.120
So you can do the configuration tracking and the logging with, you know, with scikit

18:01.120 --> 18:05.920
learn or XG boost or anything like that. I think that where experiment tracking becomes

18:05.920 --> 18:11.880
more valuable is when your experiments take longer or you want to do complicated hyper

18:11.880 --> 18:13.520
parameter searching.

18:13.520 --> 18:15.960
And we see more of that with deep learning.

18:15.960 --> 18:21.120
So just as a company, we've really focused on deep learning and these frameworks to kind

18:21.120 --> 18:23.960
of give you these magical installs.

18:23.960 --> 18:27.240
If you're using a different framework, it's going to take you some more lines.

18:27.240 --> 18:32.200
Although, you know, we've had people, we've had the community basically submit a integration

18:32.200 --> 18:34.320
for fast.ai.

18:34.320 --> 18:37.680
So we now have a community provided fast.ai integration.

18:37.680 --> 18:42.040
And then we had an enthusiastic employee build a Jack's integration.

18:42.040 --> 18:44.280
I don't know if anyone, do you know Jack's?

18:44.280 --> 18:45.280
What's Jack's?

18:45.280 --> 18:48.640
It's like a newer kind of even lighter weight.

18:48.640 --> 18:51.440
I guess you might call it deep learning framework.

18:51.440 --> 18:57.320
And then we have started to see people use us with XG boost and scikit learn.

18:57.320 --> 19:00.280
So we're working on kind of making that more native.

19:00.280 --> 19:04.000
And we did a study group.

19:04.000 --> 19:10.880
One of our study groups associated with the Twoma Meetup was studying or working through

19:10.880 --> 19:17.640
the full stack deep learning course by Peter Abil and others.

19:17.640 --> 19:25.000
And I guess weight's and biases is like a standard part of that course or something

19:25.000 --> 19:29.720
that you're told about and told to install in the course because we had a bunch of chatter

19:29.720 --> 19:34.480
in our slack about weight's and biases and people sharing screenshots and stuff like

19:34.480 --> 19:35.480
that.

19:35.480 --> 19:36.480
Yeah.

19:36.480 --> 19:40.680
And we saw a bunch of, of Tumofux come in from that, which is, which is fun for us.

19:40.680 --> 19:41.680
Oh, awesome.

19:41.680 --> 19:44.560
And by the way, anyone listening, you should, you should just know if you reach out to

19:44.560 --> 19:47.040
our, our little chat in the bottom right.

19:47.040 --> 19:50.440
It's not, it doesn't go to like some sales rep, it mainly goes to me.

19:50.440 --> 19:57.080
So I'm happy to give you tech support and please reach out and tell us, you know, kind

19:57.080 --> 19:58.080
of who you are.

19:58.080 --> 20:02.040
We're not so big that we don't, you know, kind of want to know what people's issues are.

20:02.040 --> 20:10.040
So the first of these kind of main value propses versioning and we've, we're primarily

20:10.040 --> 20:16.840
talking about keeping track of your model parameters, your models as well.

20:16.840 --> 20:22.200
You're not tracking those, you're just connecting or kind of tracking, uh, get commits of the

20:22.200 --> 20:23.200
models themselves.

20:23.200 --> 20:24.200
Is that right?

20:24.200 --> 20:27.000
So again, everything is sort of, you know, opt in, right?

20:27.000 --> 20:30.640
You know, so, so we work with people that, you know, have various levels of sensitivity.

20:30.640 --> 20:34.160
So, um, and, and we really, you know, we have a strong point of view here where I really

20:34.160 --> 20:38.960
don't want to be a end to end framework where you have to buy into everything to get value.

20:38.960 --> 20:39.960
Yeah.

20:39.960 --> 20:43.960
Um, so, you know, if you want us to, we'll keep track of your, your get shot and we'll

20:43.960 --> 20:47.600
keep a patch against the get, um, your latest get commits.

20:47.600 --> 20:51.160
So we'll know the state of your code if you want us to also, if you want us to, we'll

20:51.160 --> 20:54.960
save your, um, model files either during training or at the end of training.

20:54.960 --> 20:58.840
So, or, or any actually any other artifact that is important for running your model.

20:58.840 --> 21:03.400
So there is a saving component to this, which is important to a lot of our, our users.

21:03.400 --> 21:09.080
If you don't already have a pipeline built out that programmatically saves your model

21:09.080 --> 21:18.160
parameter someplace and commits your code someplace all in the context of a run, you could do that

21:18.160 --> 21:19.160
all through way to buy.

21:19.160 --> 21:20.160
So it sounds like.

21:20.160 --> 21:21.160
Yeah, exactly.

21:21.160 --> 21:25.160
And if you already do have some kind of, um, pipeline where it's somewhere saved, then

21:25.160 --> 21:28.760
maybe the best thing to do is just actually save a link to it, um, in our application.

21:28.760 --> 21:33.640
You know, the important thing is that, um, the run gets associated with all the, the files

21:33.640 --> 21:35.040
you'd need to reproduce it.

21:35.040 --> 21:40.040
Okay. And so the next thing that comes up is, uh, visualization.

21:40.040 --> 21:45.080
And when I think about like versioning and, and like saving model parameters and visualization,

21:45.080 --> 21:48.000
the first thing that comes to mind for me is tensor board.

21:48.000 --> 21:53.960
Are, are you trying to compete with that or replace it or do they complement each other

21:53.960 --> 21:54.960
somehow?

21:54.960 --> 21:55.960
Yeah.

21:55.960 --> 21:57.960
I mean, I think it's, it's super complimentary.

21:57.960 --> 22:02.960
So, you know, one thing that we actually do, which people find useful is we will host

22:02.960 --> 22:03.960
your tensor board.

22:03.960 --> 22:09.240
We send us a TF events files and artifact, um, you know, we recognize that and we'll actually

22:09.240 --> 22:12.960
pop open a hosted tensor board for you in the cloud.

22:12.960 --> 22:18.680
Um, you know, my big issue, there's sort of two things, two ways that we improve on, um,

22:18.680 --> 22:19.680
tensor board.

22:19.680 --> 22:23.520
Which I actually think is an excellent tool, um, you know, the, the, the, the, the first

22:23.520 --> 22:27.760
thing that we improve on is that, um, tensor board tends to be a femoral.

22:27.760 --> 22:30.280
So, you know, you, you typically run it locally.

22:30.280 --> 22:34.440
And you know, one thing we, I saw a lot when I went around, um, and talked to folks about

22:34.440 --> 22:38.480
how they were doing their experiment tracking today is they were literally taking screenshots

22:38.480 --> 22:41.880
of their, um, tensor board and, and posting it into Slack.

22:41.880 --> 22:46.240
And that, that seems a little, um, you know, that seems like a little crazy to me, or,

22:46.240 --> 22:51.200
I guess it seems like there's an opportunity for, um, that to be hosted forever, right?

22:51.200 --> 22:57.840
So if you, um, if you use weights and biases, then all this stuff, all these graphs that

22:57.840 --> 23:03.280
you make and all the, um, all the runs that you do, they're hosted forever, or as until

23:03.280 --> 23:05.360
you delete them, um, in the cloud.

23:05.360 --> 23:07.920
So I think that's a much better practice, right?

23:07.920 --> 23:12.920
Because, you know, if you shut down your tensor board server, um, you know, your colleagues

23:12.920 --> 23:14.360
may still want to look at what you're doing, right?

23:14.360 --> 23:19.400
So that, the, the, the sort of like static, permanent URL is kind of the first improvement.

23:19.400 --> 23:21.240
I think the weights and biases has.

23:21.240 --> 23:22.240
Mm-hmm.

23:22.240 --> 23:27.000
And then the second thing, um, is I think that tensor board starts to struggle when you're

23:27.000 --> 23:31.600
comparing lots and lots of models with lots and lots of data points, uh, elaborate on

23:31.600 --> 23:32.600
that.

23:32.600 --> 23:34.600
Where in particular does it struggle?

23:34.600 --> 23:36.840
So there's kind of two ways that it can struggle, right?

23:36.840 --> 23:43.760
So one is if you, if you run a model over millions and millions of, um, data points, it

23:43.760 --> 23:46.200
doesn't, um, ever really start to sample, right?

23:46.200 --> 23:49.440
So you know, if, if you have a run that, that, you know, you run it for like a couple

23:49.440 --> 23:55.440
of weeks, um, you know, it, uh, it can just be actually literally slow, um, super slow,

23:55.440 --> 24:01.000
right to, to run it even in your browser, um, and the second thing is that if you want

24:01.000 --> 24:05.360
to compare lots of runs, I don't think that tensor board was kind of originally designed

24:05.360 --> 24:06.360
for that.

24:06.360 --> 24:11.360
So there is ways to compare, you know, three, four, five, um, runs, but what we typically

24:11.360 --> 24:16.160
see is people will do, you know, hundreds or thousands of runs with different, um, you

24:16.160 --> 24:19.920
know, kind of hyper parameters and configurations and they want to mark some is, hey, you know,

24:19.920 --> 24:22.600
these were bass lines and, you know, here's what I was doing here and here's what I was

24:22.600 --> 24:23.600
doing here.

24:23.600 --> 24:27.160
And, you know, the typical way people do it in tensor board is they start to do that crazy

24:27.160 --> 24:32.160
long, um, file name thing and then kind of search over them with regular expressions.

24:32.160 --> 24:34.840
And, um, you know, that just, it doesn't scale, right?

24:34.840 --> 24:39.960
So when you do, um, you know, when you, when you're really doing like serious, um, evaluate

24:39.960 --> 24:43.400
equations, I mean, I, I don't want to knock on, um, tensor board, I think it's a, it's

24:43.400 --> 24:47.240
a great tool for regular expressions, or regular expressions, that's a lot of regular

24:47.240 --> 24:48.240
expressions.

24:48.240 --> 24:49.240
Yeah.

24:49.240 --> 24:50.240
Totally, totally.

24:50.240 --> 24:51.240
Um, you know, yeah, good point.

24:51.240 --> 24:57.520
I definitely don't want to, um, insult regular expressions, um, but, um, I think that, uh,

24:57.520 --> 25:02.280
we have a tool that's more designed for a, kind of where you get to down the road when

25:02.280 --> 25:07.920
you have, you know, hundreds of runs and you want to kind of filter and group things, um,

25:07.920 --> 25:08.920
in different ways.

25:08.920 --> 25:13.480
And I should say it is a, it can be a little tricky to monitor things on kind of big distributed

25:13.480 --> 25:14.480
runs.

25:14.480 --> 25:17.200
So when you're running across like multiple machines, um, that's also a case that we

25:17.200 --> 25:20.240
really focused on at, at weights and biases.

25:20.240 --> 25:25.440
Kind of a distributed training scenario where you're, you've got multiple machines training

25:25.440 --> 25:26.440
a single model.

25:26.440 --> 25:27.440
Yeah, exactly.

25:27.440 --> 25:31.160
I mean, it's kind of makes sense if you're running on multiple machines or a single model,

25:31.160 --> 25:36.280
um, to have everybody kind of reporting to like a single centralized place, um, universes

25:36.280 --> 25:41.080
everybody writing out, you know, kind of files locally, you know, I think it's just different,

25:41.080 --> 25:42.400
different design goals.

25:42.400 --> 25:46.560
And so, um, I would say, I would say weights and biases is complimentary with, um, tensor

25:46.560 --> 25:50.200
board, but it is kind of the closest, I'd say it's the most common experiment track

25:50.200 --> 25:52.000
and thing that we, that we see today.

25:52.000 --> 25:55.000
So it is the right place to compare weights and biases.

25:55.000 --> 26:00.920
Uh, and so then the third, uh, element that you mentioned is collaboration.

26:00.920 --> 26:06.800
Uh, I imagine just having that static URL is, uh, not having to screenshot and send it

26:06.800 --> 26:08.120
in Slack.

26:08.120 --> 26:13.680
It probably, uh, is a starting point for like collaboration, but the, are you doing something

26:13.680 --> 26:16.240
explicit, uh, around collaboration?

26:16.240 --> 26:17.240
Yeah, totally.

26:17.240 --> 26:22.080
I mean, I think, um, so I think Adrian, uh, guidance on your podcast from TRI and talked

26:22.080 --> 26:26.480
a little bit about, um, using our tool for collaboration, but, um, you know, they've,

26:26.480 --> 26:29.880
they've sort of talked about it publicly and, you know, opening up is kind of done a case

26:29.880 --> 26:33.480
study with us and how they use us to, um, collaborate.

26:33.480 --> 26:38.400
But I think, I think a lot of it is just around, um, helping people get more systematic

26:38.400 --> 26:42.080
about their training so that it's possible for other people to pick up your work.

26:42.080 --> 26:44.760
And, you know, the regular fashion thing we talked about is a great example, right?

26:44.760 --> 26:50.280
Like if you put in little notes in your run names for your hyper parameters, that might

26:50.280 --> 26:54.840
make sense to you, but it could be really hard for your colleague, um, to come in and do

26:54.840 --> 26:59.360
a similar analysis if they don't know, um, you know, what's your, you know, exactly what

26:59.360 --> 27:01.600
your run names mean and, and what you were doing.

27:01.600 --> 27:05.320
So, you know, we make it a lot easier to have human readable names and then also share

27:05.320 --> 27:07.440
your projects with your colleagues.

27:07.440 --> 27:11.520
So what happens is, you know, you can, you can basically set up a report and you can, you

27:11.520 --> 27:14.720
can put notes in that report and like literally type out, okay, here's what I was doing.

27:14.720 --> 27:18.720
Here, here's the different runs and then a colleague can, you know, go and look at any of

27:18.720 --> 27:23.600
those individual runs and see what happened and then also look at the aggregate, um, statistics

27:23.600 --> 27:26.320
and then also kind of build their own, um, analysis.

27:26.320 --> 27:32.200
So I think where, where waits and biases becomes this like really beloved tool, um, is

27:32.200 --> 27:36.240
when our, our customers start to use it for collaboration because, because that's just

27:36.240 --> 27:39.840
something you kind of can't get out of anything else at, at least right now.

27:39.840 --> 27:46.520
It doesn't sound like though that you're necessarily trying to build the, I don't know what you

27:46.520 --> 27:51.760
would call this thing, the enterprise Facebook for models or something like that where like

27:51.760 --> 27:56.560
every user has a feed and all of their runs going their feed and people are commenting

27:56.560 --> 27:58.920
on each other's runs and that kind of thing.

27:58.920 --> 28:03.920
No, I mean, I think, um, I think we want to do support, you know, discussions outside

28:03.920 --> 28:04.920
of our tool.

28:04.920 --> 28:07.320
I mean, I think there's lots of good ways to, you know, so you have a Slack integration,

28:07.320 --> 28:10.000
you know, so you can, you can post this stuff into Slack.

28:10.000 --> 28:15.280
Um, but I don't think that, that machine learning has necessarily that different of a workflow

28:15.280 --> 28:20.560
that we want to, you know, try to, try to make our own version of a, of a feed, um, although

28:20.560 --> 28:24.440
I would say, you know, we are experimenting with a thing that's been, I'm really excited

28:24.440 --> 28:29.520
about called benchmarks where people can collaborate across, um, organizations.

28:29.520 --> 28:36.680
So we can take an open source, um, you know, machine learning project, um, and then people

28:36.680 --> 28:38.920
around the world can submit their results on it, right?

28:38.920 --> 28:44.280
So they can, they can modify the code and show their accuracy on, you know, their own

28:44.280 --> 28:46.040
data sets or their own setups.

28:46.040 --> 28:47.600
And so, have you done any of those?

28:47.600 --> 28:53.760
What's an example of, of a project that, and some, you know, folks submitting these benchmarks?

28:53.760 --> 28:54.760
Yeah.

28:54.760 --> 28:58.040
So you can find them on our website, but I think I'm one that was kind of fun was, you

28:58.040 --> 29:02.960
know, Giffy, the company gave us a whole ton of gifts of cats.

29:02.960 --> 29:07.520
And so we did kind of a video frame, um, prediction benchmarks.

29:07.520 --> 29:11.840
So, you know, you get the first five frames, I believe, and then you predict the next five

29:11.840 --> 29:12.840
frames.

29:12.840 --> 29:16.600
And so, you know, what we actually saw was all the kind of different strategies that people

29:16.600 --> 29:21.280
use for, um, video frame prediction, how well they work on this data set, but I thought

29:21.280 --> 29:24.480
it was particularly cool about the way the benchmark was set up is that you can actually

29:24.480 --> 29:31.160
go in and look at all the different submissions, um, models and all of their kind of, um, data

29:31.160 --> 29:36.080
in a standard as way, um, or even kind of sort the submissions based on, you know, different

29:36.080 --> 29:37.960
metrics and then pixel distance.

29:37.960 --> 29:41.800
So, you know, different algorithms might work better depending on your, and metric, and,

29:41.800 --> 29:45.520
you know, we have all the different models of people submitted, and then, you know, we're

29:45.520 --> 29:50.440
doing one now on, um, you know, drought prediction, you know, to kind of help, um, to help

29:50.440 --> 29:52.760
folks, you know, figure out where, where droughts are happening.

29:52.760 --> 29:56.280
It's actually sort of a, apparently like a cactus identifier, because, you know, they,

29:56.280 --> 29:59.480
they, the state of the Arctic, it's just like using the amount of green, but the problem

29:59.480 --> 30:03.760
is, you know, cactus is our green, and, you know, that can be like a challenge to, um,

30:03.760 --> 30:07.080
to kind of know where, where, where, you know, droughts are happening on like a small

30:07.080 --> 30:08.080
scale.

30:08.080 --> 30:10.440
And, you know, we have a whole bunch of other benchmarks, it's kind of a new feature,

30:10.440 --> 30:13.840
so it's still, um, we're kind of still seeing how people use it.

30:13.840 --> 30:19.120
But, um, what I like about it is I think there is a lot of room for collaboration across

30:19.120 --> 30:22.360
teams, and there's sort of a rich, kind of culture of it, and machine learning because

30:22.360 --> 30:24.760
so much of it comes out of academia, right?

30:24.760 --> 30:28.680
But, um, you know, I think one of the challenges with, when you get a research paper is that,

30:28.680 --> 30:32.080
you know, you get a small table of results, but you don't get to see all the different

30:32.080 --> 30:35.280
things that the researcher tried and all the, you know, kind of all the paths that they

30:35.280 --> 30:39.240
went down that, that didn't work, but, um, you know, if, if their stuff is instrumented

30:39.240 --> 30:44.200
with weights and biases, you can actually have this really, um, detailed record of, of

30:44.200 --> 30:48.640
lots and lots of different things, and, and kind of start from any point in the process.

30:48.640 --> 30:50.000
Has anyone done that?

30:50.000 --> 30:56.360
Have you seen a paper that cited one of these static URLs with, uh, all of their experimental

30:56.360 --> 30:57.360
results?

30:57.360 --> 31:01.520
Yeah, we just had our first one, actually, so, uh, really knew the, you know, the paper

31:01.520 --> 31:05.240
publishing process is long, but yeah, we actually just, we just had our first one.

31:05.240 --> 31:06.240
What was the paper?

31:06.240 --> 31:10.520
I mean, I think we'll see a lot more, uh, down the road because we did make our, we

31:10.520 --> 31:14.200
make our product, um, free, academics, because we really liked the Seuce case, but this

31:14.200 --> 31:18.480
was called a machine learning techniques for detecting, identifying linguistic patterns

31:18.480 --> 31:20.600
in, um, news media.

31:20.600 --> 31:21.760
Is that literally what they did?

31:21.760 --> 31:25.760
They kind of gave the static weights and biases, Lincoln, you can go in and look at all

31:25.760 --> 31:29.040
of their various experiment runs or, yeah, yeah, exactly.

31:29.040 --> 31:34.000
I mean, we probably should make a more systematic way to do this, but, um, for the emancipation,

31:34.000 --> 31:37.560
um, you know, for their reports, you can go in and look at their, you know, look at

31:37.560 --> 31:40.200
their report and, and get more detail.

31:40.200 --> 31:47.360
When you're talking to folks that are, uh, outside of academia on the enterprise side,

31:47.360 --> 31:52.680
and they're, you know, getting serious with deep learning and starting to figure out

31:52.680 --> 31:59.000
how they can build some more structure around their approach.

31:59.000 --> 32:04.240
What are the, I guess I have a bunch of questions around your experiences there.

32:04.240 --> 32:09.880
Are there, are there, do you find cultural issues, quote, unquote, around, you know, them

32:09.880 --> 32:15.840
adopting a tool like this or some, are there patterns around which teams are more likely

32:15.840 --> 32:23.720
to kind of get it and, you know, want it or, um, is it, is it kind of random?

32:23.720 --> 32:28.120
No, no, I mean, we've, we've designed this tool with a really particular end user in

32:28.120 --> 32:29.120
mind.

32:29.120 --> 32:34.240
Um, so this is a tool for researchers that are working on deep learning.

32:34.240 --> 32:39.800
So we work with companies that have, um, researchers that are training deep learning models.

32:39.800 --> 32:43.720
And we tend to work with, you know, companies that are making a bigger investment in that

32:43.720 --> 32:44.720
today.

32:44.720 --> 32:49.040
You know, you work with folks like, um, you know, GitHub and, and, you know, Blue River,

32:49.040 --> 32:54.160
which is bought by John Deere, um, and, you know, a whole bunch of kind of robotics and,

32:54.160 --> 32:57.680
and aerial imaging companies and, and that's, that's because those are the companies that

32:57.680 --> 33:02.080
are right now making the biggest investments in deep learning.

33:02.080 --> 33:06.720
And so, you know, our strategy has been, you know, rather than kind of focus on the sort

33:06.720 --> 33:12.040
of mass democratization of, of AI to focus on, let's, let's look at what the companies

33:12.040 --> 33:17.560
that are, um, most advanced doing and make the bet that other companies are going to follow

33:17.560 --> 33:20.280
along and do kind of similar techniques.

33:20.280 --> 33:23.880
So, you know, a lot of people disagree with that strategy and they say, well, you know,

33:23.880 --> 33:27.640
you should build, you know, the, the tools that, um, you know, proctor and gamble needs

33:27.640 --> 33:32.880
is very different than the tools that, you know, maybe an open area or a, a Google needs.

33:32.880 --> 33:36.480
Um, but I guess my perspective is, is a little different.

33:36.480 --> 33:40.800
I think that, you know, I think that, that proctor and gamble may not be training lots and

33:40.800 --> 33:43.200
lots of deep learning model sales, they are training some.

33:43.200 --> 33:46.440
I think that over time, they're going to train more and more and they're going to want

33:46.440 --> 33:49.640
to bring that expertise in house.

33:49.640 --> 33:54.160
So, I want my tool to make things, you know, easier for, for machine learning researchers,

33:54.160 --> 33:58.160
but I don't necessarily want to make the machine learning research drops lead or kind

33:58.160 --> 34:02.240
of, um, you know, automate every, every part of the process like, you know, some of these

34:02.240 --> 34:06.000
like, um, you know, auto email types of, of projects.

34:06.000 --> 34:10.840
So we typically go in, we're typically get adopted by researchers inside of companies

34:10.840 --> 34:15.480
and then end up selling a larger license as the, um, business aside, they want to standard

34:15.480 --> 34:16.840
eyes on our tool.

34:16.840 --> 34:20.520
And you, you're describing this target user as a researcher.

34:20.520 --> 34:28.320
Is that, uh, universal or do you find that the user is split across folks that are

34:28.320 --> 34:33.880
formally called researchers and data scientists, machine learning engineers and other things

34:33.880 --> 34:34.880
we see?

34:34.880 --> 34:39.240
Well, of course, I mean, Sam, you know, this market, well, also, I mean, the titles

34:39.240 --> 34:40.640
is total chaos, right?

34:40.640 --> 34:44.240
So, I was kind of getting it there.

34:44.240 --> 34:49.480
Yeah, I mean, you know, as, as you know, right, it's, it's hard to tell, um, you know,

34:49.480 --> 34:53.640
these days from a title, what someone's actually doing, the important thing to us really,

34:53.640 --> 34:58.240
like our, our kind of qualifier is that they're actually training, um, you know, machine learning

34:58.240 --> 35:03.400
models and, and generally training, at least some of the models in one of the frameworks

35:03.400 --> 35:05.240
that we have kind of first order support for.

35:05.240 --> 35:09.640
So that's, you know, as I said, Cara's center flow, pie torture, you know, maybe fast AI.

35:09.640 --> 35:15.400
So those are the folks that are most likely to benefit, um, from our tool.

35:15.400 --> 35:23.560
Have you come across folks that are using fast AI, I don't know if commercially is the

35:23.560 --> 35:29.200
right word or organizationally, like, you know, at a company or research organization,

35:29.200 --> 35:34.080
um, meaning outside of the kind of educational context?

35:34.080 --> 35:39.720
Yeah, I would say yes, it's not, I would say it's not, um, it's mostly in an educational

35:39.720 --> 35:40.720
context.

35:40.720 --> 35:43.360
And sometimes it's a little, um, hard to tell.

35:43.360 --> 35:48.080
I think educational products and some of these orgs can bleed into production products,

35:48.080 --> 35:49.080
you know, yeah, yeah.

35:49.080 --> 35:50.080
Yeah.

35:50.080 --> 35:56.880
You know, gradually, um, so, um, we, we actually honestly, we do see some fast AI and

35:56.880 --> 35:59.400
stuff that looks like it's head and towards production.

35:59.400 --> 36:03.600
I don't know that I could point to something where it's actually going to get deployed,

36:03.600 --> 36:06.520
but, you know, it is really pie torch, um, you know, under the hood.

36:06.520 --> 36:12.760
And I would say one insight that I have is we do see pie torch in a production context

36:12.760 --> 36:16.600
a lot more than I think, um, you know, it's reputation to have you believe.

36:16.600 --> 36:22.320
So, um, people are definitely deploying pie torch successfully into real world applications.

36:22.320 --> 36:25.240
So I don't see why fast AI wouldn't allow you to do that.

36:25.240 --> 36:28.640
I mean, you know, folks have different opinions about fast AI, you know, some, some seem

36:28.640 --> 36:30.320
to love it, some seem to hate it.

36:30.320 --> 36:33.840
But I think it, you know, I think if somebody really loved it, it shouldn't be out of the

36:33.840 --> 36:34.840
question to deploy it.

36:34.840 --> 36:35.840
Yeah.

36:35.840 --> 36:43.480
Um, do you, is there any interaction with, uh, tools like Onyx here?

36:43.480 --> 36:45.280
Um, not really.

36:45.280 --> 36:50.480
We haven't seen a lot of Onyx and we've seen that file format used to save, um, models

36:50.480 --> 36:55.200
occasionally, um, but, um, you know, for whatever reason, we, we, I haven't known

36:55.200 --> 37:02.000
it's a lot of that, you know, if you think about the machine learning pipeline or the,

37:02.000 --> 37:06.720
what are the things that are, you know, I guess we've talked about at some of the things

37:06.720 --> 37:11.840
that are like immediately upstream or downshing from you, you know, there's get repositories

37:11.840 --> 37:18.600
and collaboration and things like that are there, uh, things that, you know, folks really

37:18.600 --> 37:24.360
need to have in place in order to take advantage of experiment management or things that,

37:24.360 --> 37:28.080
you know, once you have experiment management, oh, wow, this whole new world opens up

37:28.080 --> 37:33.880
for you and you can do X, Y, Z, yeah, totally, um, you know, it's funny, it's funny to

37:33.880 --> 37:36.800
say this, but I think some folks, you know, come and don't realize this.

37:36.800 --> 37:41.040
I mean, you do need to actually be doing experiments to take advantage of experiment,

37:41.040 --> 37:48.720
it's not, um, you know, to be fair, it's, it's not totally trivial, right?

37:48.720 --> 37:53.120
So, you know, to actually start training, you know, deep learning models, you need, uh,

37:53.120 --> 37:56.600
you need some folks with experience in that, you know, you need machines that can do that

37:56.600 --> 38:01.240
and, and a lot of people want kind of a software layer that helps them with that.

38:01.240 --> 38:05.880
So we've made a fair amount of effort to integrate, um, with, with, um, the different stuff

38:05.880 --> 38:06.880
that we see.

38:06.880 --> 38:10.280
So, you know, we see a fair amount of number of folks using SageMaker.

38:10.280 --> 38:12.760
So, you know, we built the first class integration with that.

38:12.760 --> 38:17.400
We see a lot of excitement around ML flow and cube flow, so we built integrations with

38:17.400 --> 38:18.400
those tools.

38:18.400 --> 38:23.360
And our, our vision is to just run on top of, um, anywhere the people want to train their

38:23.360 --> 38:24.360
models.

38:24.360 --> 38:29.440
ML flow and cube flow sound so similar when you say them like that, but they're totally

38:29.440 --> 38:30.440
different things.

38:30.440 --> 38:36.480
I would have thought of ML flow, or I, I tend to think of ML flow as more of, uh, alternative

38:36.480 --> 38:40.440
to what you're doing, whereas cube flows like more infrastructure and orchestration.

38:40.440 --> 38:42.760
Yeah, but they've kind of bleed into each other.

38:42.760 --> 38:45.800
It's a cube flow, I think of as more infrastructure and orchestration.

38:45.800 --> 38:49.520
I think ML flow, um, you know, at least what they tell me is that they're kind of trying

38:49.520 --> 38:54.720
to be more of a sort of standard, um, set of APIs around, um, training.

38:54.720 --> 39:00.800
Um, as of course as they become kind of a standard way that different, um, you know, ML services

39:00.800 --> 39:01.800
can talk to each other.

39:01.800 --> 39:03.440
We want to, we want to work with them.

39:03.440 --> 39:04.440
Yeah.

39:04.440 --> 39:05.440
But yeah, good point.

39:05.440 --> 39:06.440
It's funny.

39:06.440 --> 39:07.960
It's like, as I say all these things, I'm thinking this podcast is going to be so out

39:07.960 --> 39:08.960
of date.

39:08.960 --> 39:09.960
Six months.

39:09.960 --> 39:16.440
Well, you know, that is a challenge, I, when I did the Kubernetes ebook in, that was

39:16.440 --> 39:19.560
published last November.

39:19.560 --> 39:28.000
And you know, on the one hand, I was racing to get it ready for CubeCon in, uh, in Seattle.

39:28.000 --> 39:32.600
But then there was a whole bunch of stuff that was announced that totally, uh, made a good

39:32.600 --> 39:37.840
chunk of it obsolete, you know, and there was a bunch of updating that needs to be done.

39:37.840 --> 39:45.440
And this, uh, the ML platforms paper, it probably mentions, I don't know, there's probably

39:45.440 --> 39:50.440
like 30, 50 tools or something that are mentioned in this thing.

39:50.440 --> 39:55.640
And I'm sure, you know, within a month of being published, some of them will be acquired,

39:55.640 --> 40:00.320
some of them will be, will disappear, some of them will evolve as kind of, you know,

40:00.320 --> 40:07.080
defective standards or market leaders or whatever it's, it is such a chaotic, um, and

40:07.080 --> 40:12.520
therefore exciting marketplace and time to be, you know, in and following this marketplace.

40:12.520 --> 40:13.520
Yeah.

40:13.520 --> 40:14.520
Absolutely.

40:14.520 --> 40:18.760
Yeah, I guess you have, you have the hardest job of trying to keep track of all this.

40:18.760 --> 40:20.920
It is crazy.

40:20.920 --> 40:26.360
It is crazy how much activity is happening in this space.

40:26.360 --> 40:27.360
Yeah.

40:27.360 --> 40:31.080
So anyway, you know, I think upstream, you know, we, we try to integrate with people

40:31.080 --> 40:37.960
just as, as folks, you know, ask for us and people typically have, you know, some kind

40:37.960 --> 40:40.800
of platform to do their runs, some kind of, sometimes some kind of, um, pipeline management

40:40.800 --> 40:41.800
or workflow management.

40:41.800 --> 40:46.000
Um, you know, they often have kind of a training data solution, um, I strongly recommend

40:46.000 --> 40:47.000
to figure it.

40:47.000 --> 40:48.000
And I'm totally unbiased.

40:48.000 --> 40:58.040
And then, um, you know, downstream from us, um, you know, there's, um, production deployment,

40:58.040 --> 41:01.680
I think it's kind of the next, um, the next step.

41:01.680 --> 41:07.120
And, um, you know, we mostly see folks doing that, um, on an ad hoc basis, although, you

41:07.120 --> 41:09.000
know, we've started to see a whole bunch of companies around that.

41:09.000 --> 41:13.640
So I'm sure that, um, you know, that's going to start, um, start picking up, you know,

41:13.640 --> 41:17.960
as, as the stuff becomes more real and, and, and people started to care about that more.

41:17.960 --> 41:22.280
Is, is there anything else we should cover before we wrap up or any kind of parting thoughts?

41:22.280 --> 41:26.600
No, you know, I just want to say, you know, the, it's a big goal for me is to make, make

41:26.600 --> 41:31.320
our software really, really easy to try and really low, kind of lock in.

41:31.320 --> 41:34.960
So, you know, I guess my, my asked the folks, you know, if you've listened this far, you

41:34.960 --> 41:39.960
probably get more out of spending five minutes, um, integrating weights and biases, um, into,

41:39.960 --> 41:40.960
into your tool.

41:40.960 --> 41:45.840
So, you know, it, it really is like a, kind of a five minute install, um, and we make the

41:45.840 --> 41:52.240
product free for individuals and free for, you know, academics and, um, easy to export

41:52.240 --> 41:53.240
your data.

41:53.240 --> 41:57.080
So, hopefully it's, it's pretty low commitment because, you know, my goal here is that, you

41:57.080 --> 42:01.160
know, everyone, you know, gives our experiment tracking tool just to try and, and, you know,

42:01.160 --> 42:04.640
if they have any questions or concerns, they, you know, write into our little intercom bubble

42:04.640 --> 42:06.200
and, um, ask for help.

42:06.200 --> 42:13.600
Well, we will be sure to point folks to a, uh, a place where they can find weights and

42:13.600 --> 42:16.400
biases and do that, explore it more.

42:16.400 --> 42:21.400
You, you refer to it as software, but it is software plus, like software as a service,

42:21.400 --> 42:22.400
right?

42:22.400 --> 42:23.400
Yeah.

42:23.400 --> 42:29.160
You're accessing the, the dashboards and the visualizations through your website.

42:29.160 --> 42:33.840
Do you have folks, uh, asking you to make it available, kind of behind the firewall

42:33.840 --> 42:34.840
are on premises?

42:34.840 --> 42:35.840
Yeah.

42:35.840 --> 42:36.840
We're selling that today.

42:36.840 --> 42:37.840
Oh, you are.

42:37.840 --> 42:38.840
Okay.

42:38.840 --> 42:39.840
So, you have, for example, to use it in that way.

42:39.840 --> 42:40.840
Okay.

42:40.840 --> 42:41.840
And we're doing lots of installs like that.

42:41.840 --> 42:42.840
Yeah.

42:42.840 --> 42:43.840
Cool.

42:43.840 --> 42:48.080
Uh, well, Lucas, thanks so much for taking the time to chat very, uh, very fun conversation.

42:48.080 --> 42:49.880
Uh, always learn a ton.

42:49.880 --> 42:50.880
Likewise.

42:50.880 --> 42:58.640
All right, everyone, that's our show for today.

42:58.640 --> 43:05.560
For more information on Lucas or for today's show notes, visit twomalai.com slash shows.

43:05.560 --> 43:10.000
Thanks again to weights and biases for their sponsorship of today's show and twomalcon

43:10.000 --> 43:11.480
AI platforms.

43:11.480 --> 43:17.360
Visit WNB.com to learn more for 20% off of your twomalcon registration.

43:17.360 --> 43:23.040
Make sure to use the code WNB20, W-A-N-D-B20.

43:23.040 --> 43:49.800
As always, thanks so much for listening and catch you next time.

