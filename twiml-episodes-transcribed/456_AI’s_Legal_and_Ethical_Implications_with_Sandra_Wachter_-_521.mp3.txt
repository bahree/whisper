All right, everyone. I'm here with Sandra Vockler. Sandra is a professor at Oxford. Sandra,
welcome to the Twomol AI podcast. Thank you so much for the invitation. I am looking forward to
digging into our chat. Of course, to get things started, I'd love to have you share a little bit
about your background and how you came to work in the field of artificial intelligence as a lawyer.
I guess it started with the fact that I always was very, very excited about technology and I grew
up with an understanding that technology could actually help us bring the world closer together.
I'm just started off in the field of health tech. Actually, that was the first area that I looked
at because medical devices that makes immediate sense that people understand that this can be helpful
for a lot of people in our society. And then over time, and especially in my PhD, I branched out
much broader and got interested in all types of technologies and how they can be used for
good or bad and got interested in the question of how it affects laws, how it affects society,
and what it is that we can do to reap the benefits and mitigate the risks.
Nice. Were you ever been a practicing lawyer? Did you go through law school and all that?
Yes, I did go for law school. I have a special degree in medical law. I do have a PhD in law.
So, yes, full-fledged lawyer in dead regard, yes, in state and academia.
Nice. Nice. What's your take on the... Actually, before we jump into that,
tell us about your research. I was going to ask what's your take on the state of AI and the
law, but that is a very broad topic. But it's not one that I've talked about extensively here
on the podcast. So, there's lots to dig in there, but maybe we'll be a bit more focused and I'll
ask you to talk a little bit about your research interests, which you spend a lot of time thinking
about specific areas that AI intersects law. What are those areas? Yes. So, in general, I focus on
the legal and ethical aspects of emerging technologies, and at the moment, I have free, very
distinct research interests that have to do with AI. There is much more, so I don't want to say
those are the only three people have to care about, but those are the only three I have time to
care about at the moment. And there are free, free aspects that I think we all need to think about
whenever we use algorithms for decision making. One definitely has to do with the Blackpops problem,
the other one has to do with the protection issues, and the third one has to do with bias. So,
those three areas, regardless of where you deploy an algorithm, whether this is in the US,
whether this is in Europe, whether this is in New Zealand, and whether this is in banking or
education, or in the health sector, those three areas will be important, because you always
going to want to understand how a decision was made, how did the Blackpops come to the conclusion
that you shouldn't get insurance, or that you have to go to prison, for example,
always going to be a Blackpops problem. There is always going to be a data problem,
because an algorithm is useless without data. So, whenever you talk about algorithms,
you have to talk about data, and whenever you talk about data, you have to talk about algorithms,
because data is only worth anything if you can analyze it in a way. And the genius thing about
inferential analytics and AI is that you can learn so much about people, but the scary thing about
the AI is that it can learn so much about people. So, the question is how can we navigate through
that, that I have a powerful tool that is able to learn a lot of things about me that can be helpful,
right, and be good, can help me diagnose cancer, but can also infer whether I'm gay or not,
or whether I've voted in the last election, or whether I'm a woman, or whether what my
sexual orientation is, right. So, the question is what needs to be done from a data protection issue,
and then the last one I have to deal with both bias and discrimination, and again,
this is prevalent regardless of country and regardless of sector where I deployed,
because the data that we collect is, unfortunately, most instances not fair. Reason being because
the world is not fair. It's not fair in any part of the world, fully at least, and it's usually not
fair, fully fair in any of the sectors where AI is being used. So, the bias will be inherited
when you train the algorithms on it, and just think about all the different types of
sectors where we use them, right. We use them in education, we use them in traditional sector,
we use them in employment, those are all areas where we know that biases exist, so it is no
surprise that we're just going to transport human biases into algorithmic biases. So, regardless
of where you are in the world, regardless of what you're using the technology for, you need to
think about those things, and those are the three areas that I am trying to make an effort to come
up with a contribution at least. Nice, nice. Well, let's maybe explore those in turn
in talking about the black box challenge. We talked about that quite a bit on the podcast,
the need for transparency to some degree, or explainability to some degree, often
from the perspective of, yeah, I'm a business person, I'm relying on this algorithm to help me
make a decision. I kind of, in order to develop a level of trust, I kind of want to know why it's
recommending the thing that it's recommending. When you bring in the element of law and maybe
more broadly regulation, how do the ways I need to think about that problem change?
Right, so, I mean, you can do it with a brute force instrument and just say,
I, as a regulator, I'm only going to allow algorithms that are explainable, period,
exclamation point, and just live with those consequences, and that is something that has been
discussed. And whenever this discussion comes up, then people will immediately say, hold on,
hold on, hold on, you can do this because you cannot explain algorithms. So if you do that,
then you let out banning them. That was the research, you know, there was a wisdom of the day when
I entered the stage to get interested in this topic that it's just impossible. So when I thought
about this a bit more, I came to the conclusion was, yes, there are probably two reasons why people
don't want to give you an explanation. Right, the first is because they can't, and the second is
because they don't want to. And those are two different topics. One is definitely because they
don't want you, one to, and that has to do with trade secrets that they could tell you how
an algorithm works because it's not so complex that you wouldn't know. So you could actually tell
somebody why somebody has to go to prison because the algorithm is not that complex. So that's
something where the law can come in and say, well, you know, just open up that black box
because we know the answer. It's just a question of trying to balance the interests of business
and the interests of the wider community in our society. So there is something that the law could do.
And I recommend that for example, to have like a trusted third party that could have access to
that doesn't have to be spread everywhere. The more challenging, I don't want to give you an
explanation part is there because I can't part, which is not even the person writing a code
allegedly does fully understand what's going on. So even if they want it to, they can't. And
that's the more challenging problem because the law doesn't really have an answer there yet
because some things are unfortunately not explainable, not even to an full extent to the people
writing a code. However, I don't usually take no for an answer. So I figured I tried to
explore if that's actually fully true. And I teamed up with two other people. One of them is
Brent Middlestedt was an ethicist and Chris Russell works in machine learning and we wrote a paper
that is called counterfactual explanations without opening the black box. So what we try to do there
is try to take all those concerns on board and say, okay, is there a way to understand what's
going on inside of a black box without fully understanding the black box, right? And we came
up with counterfactual explanations as a way to do that because we saw it from the view of the
person who wants to have an explanation, right? I have to go to prison and I didn't get the job,
I wasn't promoted. The thing that I want is not a full-fledged code explanation. What I want to
know is why the hell didn't I get the promotion and what do I need to do different to get the
promotion? That's the thing I'm actually after, right? If you fire me and you give me a piece of
code in my hand, I'm going to be very angry at you actually and feel like you haven't listened to me.
And that's exactly how explanation usually works in human settings. I want to know the criteria,
the reasons why I didn't happen and what I need to do differently. And luckily, that type of
reasoning is something that you can code. So we call it counterfactual explanations in human
settings, but in code, you can also generate a counterfactual. So where you just have a very
complex system, one that you might not be fully to understand why, but I can tell you why in this
particular case the decision was made in a certain way. So if you apply for a loan, for example,
a counterfactual will tell you you were denied the loan because your income was 30,000 pounds.
If it had been 35,000 pounds would have given you the loan, right? So you get the most important
criteria and it tells you something what needs to be done to change the result. It gives you
grounds to contestation, all that good stuff, right? Without having to understand the complexity of
the full code. And that was a way to find a middle ground there where we can do something
in a way. And that's done something that the law could require you to do. And that was actually
quite exciting. So we wrote that paper a couple of years back. And Google came across our work
interestingly enough. And they implemented an intensive flow and later on they implemented it
in Google Cloud as well. And then many other companies have followed suits such as
Vodafone for example, which is amazing to see. So the things that we cooked up in our ivory tower
basically were actually something that had a positive meaning for people working on a ground
which is exciting. But anybody interested in the topic can can look at our paper and the code
is free available. Everybody can can use the type of explanation if they wanted to. So it's
freely accessible. But yeah, that is definitely a way to think about opening the black box
in a meaningful way. That's awesome. Can you give us an overview of how those counterfactual
explanations are created or generated? Yes. What you do is you try to find the closest
and minimal changes to a current decision model that need to be taken in order to get the
thing that you want. So you're not actually just asking how does the rationale of the algorithm
work? What you're doing is, okay, what are the smallest possible changes that I need to do
to get from point one to be. And the interesting thing is that I could give you multiple
counterfactuals, right? I could tell you you didn't get admitted to law school because your reference
letters were bad and or because your grades were too low or because you had typos in whatever,
right? And then give you a ranking of that and then you can figure out what would be the most helpful
for you because for some of us it's easier to change the spelling on the covered letter
for others it would be easier to find better reference letters, right? So to give them whatever
set of possible grounds to improve your current situation is a very, very good benefit of counterfactuals.
So you're you have some data point that you want to
generate these explanations for and so then you permute it in different dimensions to try to
understand how the what different decisions the model might take and then you use that to create
the explanations. Yes, exactly. Awesome, awesome. And then the second focus area for you is around
data protection and it sounds like your work is exploring a fundamental question of, you know,
is privacy still viable in this fully connected fully connected world with, you know,
machines making predictions and accessing consumer data and the degree to which, you know,
what our expectations should be and how we can protect data. And tell us a little bit more about
that area and some of your work there. Yes, certainly. That is a topic very close to my heart.
And when we when why do we need to care about I said it already a little bit, but just to give
people a little bit of a scary landscape of what's actually out there is that I don't think we
really fully understand how good those algorithms are to protect very, very sensitive and intimate
information about us that we might not want to share with anybody at all. And they can do that
from very seemingly neutral data. So if I use my search engine, for example, Bing,
they are able to infer based on how I move my mouse what I have Alzheimer's disease. Are you aware
of that? Are you aware that you are giving that health information away to the outside world?
Are you aware that Twitter can infer whether you have depression just based on what you treat online?
Are you aware that to be clear when you give these examples, are you posing them as hypotheticals
Bing could? No, no, they have published a paper that they can do that. So that's what I'm trying
to say. This is not a hypothetical something that I cook up in the ivory tower. That's
fact they can do that. One of the most problematic ones is definitely what happened with Facebook
where they're able to. And again, there's research that shows that able to infer
sexual orientation, ethnicity, gender, ability, without any of their users identifying with any
of those classes and groups just based on what they click on, what they like, what they post,
and who their friends are. They can have that information and then use it in a way where they
for example allow advertisers to exclude them from seeing certain products. And this is something
that they did for example. So they would in the US infer that somebody was for example black
and would allow advertisers to exclude them from seeing job offers and ads for housing
and financial services. So it's not just a theoretical problem that you should not know those
things about me unless I say it's okay, but it also actually has negative consequences
because it can be used against you in a way where you don't know about it. So we've all
of that. And again, so it's just three examples that show how powerful those algorithms are.
The question is, well, I mean, we have the protection law. Why not just apply data protection
law to all of those problems. And the issue here is at least the one that I, you know, I've
been currently working on in a, or I've been working on a research project in a paper that I wrote,
which is called a right to reasonable inferences, is that I show that the current data protection law
was designed in a way without fully anticipating the power of AI. And therefore it was designed
in a way in a very almost 20 century way of thinking about privacy. In the sense that, you know,
20 century privacy is I'm in my home and I don't like my neighbor. He's very nosy and he keeps,
you know, coming over to the fence and he's looking at me and he says, oh, Sandra is again
eating ice cream before noon. Oh, my God, right. She has no self control. Yeah. So she has no
self control whatsoever, which is true. But that's the idea. So what is it that the law wants to
do is that I want to prevent the noisy, the nosy neighbor from collecting information about me.
So I give all the power to me. So you have to ask me first, right? And that is how the protection law
works is a lot of like it's based on consent or transparency where somebody needs to tell you,
be aware, be aware, I'm taking this data from you. Because in a human setting, that's a dangerous
part, you know, they are seeing something and I can anticipate what they know, right? I'm sitting
in my yard eating ice cream. That's the information that the neighbor now has about me, right?
But this is why all of the data protection law focuses on. With algorithm, collecting information
about me being ice cream is the first step, right? That's not the thing that they actually after.
They're interested in what they can infer based on my eating habits, right? And that's the question.
That's something that my neighbor couldn't do. An algorithm now might be able to infer that I
might have a higher chance of getting diabetes at some point, which is something my neighbor couldn't
do, right? But an algorithm can. So the interesting or more interesting, more dangerous being actually
happens after data is being collected. So everything that is being inferred, but the law doesn't really
care about that so much because the law still fought the data collection part is the most dangerous
thing. So I've wrote a very, very long paper. I don't know, 150 pages just to show that that's
a problem because the data protection law focuses too much on the input side of things, collecting
the data, taking data from you and not so much on the output stage, which is what can I learn about
you and that new laws actually need to govern the outputs rather than just the inputs.
I'm curious about the length of the paper. It sounds obvious and clear when you,
you know, you made a very simple and clear argument for this and, you know, I'm totally
bought in. Why do you need 150 papers to, who are you trying to convince and what is it that they
needed that you had to build out this 150 page argument? Yes, because it's really, I don't think
people have looked in complete detail how it's actually being regulated. And I just went through
with a magnifying glass to show that inferential analytics, unfortunately, or inferential data
has almost to no protection. And I showed that in the case law as well, which is something that
wasn't really looked at. Also, I could do with the fact that I was talking about the generative
protection regulation, which at that point was really new too, right? So there, you know, I,
and your framework came out and explaining this new thing that no one understood.
Yes, yes, right. So that was important because it was very, I think,
important to get the message across how really, really
propriomatic that is, which I think up until this point wasn't really clear.
So yes, it's, and I just want to, that's what I usually do when I see there's a problem
that I'm not going to give you just one example. I'm giving you like all the examples. So you
with me and, and can understand that you really need to care at this point and something needs to
be done. So I know GDPR expert, but my sense is that GDPR didn't really, this wasn't really one of
the issues that GDPR was addressing or trying to address. Is that right? Yes, that's definitely
also one of the problems. And this is why data protection law was so much designed in the way to
just keep the noisy, the noisy neighbor out, right? The, the, what actually was capable of doing
happened so much later, right? If we're very honest, the new GDPR that we have is to 80, 90 percent
the same stuff that we had in the data protection directive and that framework is from the 90s,
right? So there are updates there. Yes, but the core mechanisms and the core assumptions and
the thing that it's supposed to be doing hasn't really changed. This is not to say that it should
necessarily regulate AI, but it's just to say that it was never designed to regulate AI
in a way. And therefore it's failing. And I think a lot of people had hope that this new framework
will be visible to all the problems. And I just wanted to say, no, I think there's still a lot of work
to be done and we need to be very, very careful to do this right because there's so much on the line.
You mentioned regulating AI. It's not even regulating AI as much as regulating data in the
age of AI. Yes, definitely, definitely. Yes. So even, even for it to remove from that.
Yeah. Is there a, is there a locale or a regulatory framework that you think
does a good job with the issues that you've described? Are we there yet? No, I don't think we're there yet
because I think it touches so many things at the same time that if you want to come up with one
framework, it will need to do a lot of work. I think what probably would be more helpful is that
to rethink our regulation should work in the future anyway, that you just don't
silo data protection in one corner without thinking about competition law and without thinking
about non-discrimination law and without thinking about protection law because all those things
are very much interconnected. So one law that governs everything, I'm not sure if that's useful
or necessary, I think what's very useful and very necessary is that different types of regulators
start to collaborate more closely because AI kind of puts them in the same room anyway,
right? And the data flow does that anyway. So I think that's a better way of thinking about
regulation is that almost every regulatory aspect or every sectoral law that we have will be
touched by some type of technology at some point. So regulation is only one approach to
ensuring good behavior. There's also kind of self-regulation or industry consortia
or the like. Are there any examples of folks that you can point to that have taken a responsible
approach to these issues or are particularly transparent? I'm just wondering if there's a good
example or reference model that has emerged for say, I guess I'm envisioning an enhanced
data use statement or something like that that talks not only about these are the organizations
with which we share data but these are the derivative products that are created and this is how
we use and or share that information. Is anyone doing that? Yes, I think everyone is doing that
and I think that's a little bit also after the problem that everybody is doing that.
Doing the creating of the derivative products and sharing them or
not creating codes of conduct and guidelines and best practices and standards and all of that
is being created everywhere. It's created by governments, by industry, by NGOs everywhere and
anywhere. My colleague, Brent Smith, said he wrote a paper in nature I think which was called
white principles alone and not enough for something along those lines and I think at the point
when he wrote that piece and that's also I think for probably two years old at this point,
he said that there are 150 different guidelines best practices and standards out there
and they're all roughly the same and he goes through for them all and just points out how
they are still lacking the thing which is being applied in practice with having a good feedback
look how well they actually work in practice. I'm all for responsible innovation and research
and trying to come up with best practices and I think that's absolutely needed but I think that
part is now over. I think the interesting part is now to figure out is anybody actually deploying
them in practice, how good are they, what kind of oversight mechanisms are there. It's great to
have five wonderful ethical principles if you don't tell me how you actually operationalizing them
then I don't necessarily think the job's done yet. From the perspective of concerned parties,
academia, what does the landscape look like in trying to address these issues? There's certainly
papers like yours where you identify the issue and show that it's not being addressed,
what are the steps that we can build upon that to get to something that is a better place?
Yes, I think that I mean that's obviously a biased view here but I think that academia actually
does have a very big role to play and I'm very happy to say that academia has played a very vocal
and important role of the last years. I think if academia hadn't been as loud and persistent,
a lot of things would have not have changed. So the questions around a river of my
accountability and privacy protections have been front and center on a lot of agenda of people
in the field and have they have not been so persistently. I don't think that so much would change
at the moment so definitely that. And in that I think one of the reasons again is why
many of them have been so powerful and influential is because they very often work with people
that are not from the same discipline than they are and I think that's the key thing here because
I can just speak for myself. I think none of my papers
I could have not done it by myself and I was very important to have an emphasis in a machine
learning person on there to teach me and have them also endure my teachings and I think that
made the whole work stronger and I think that's really what if you are thinking about how to
govern things I think you owe it that you try to look at the issue from as many as perspective as
possible. And then the third pillar of your research is focused on bias fairness and discrimination
with regard to the law. It sounds like this is your most recent work and what you're most excited
about right now. Tell us a little bit more about that area. Yes definitely definitely extremely
excited about that. I think everybody will know as I said that you know bias and unfairness is
always an issue when we think about data and algorithms because unless you collecting them in
utopia she said fair fair chance that the data would bias in some way and that's the reality that
we have to deal with. I think this being about kind of the fundamental premise of machine learning
that you know we're training based on you know information we've collected about the past and
this decisions that were made in the past and the mechanism kind of fundamentally carries
forth biases from the past and to the future if there's not extreme care taken. Yes absolutely
I should have said that yes I mean that's as you said that's basically how all machine learning
works is looking at the past trying to predict the future right you feed the algorithm with a bunch
of historical data for example who has been hired who has gotten insurance who was sent to prison
who did reoffend who did get sick right and you train the model based on that because you think
that you have some ground truth because you have historical data you know if somebody actually
got sick right you know if somebody did well in law school you know if they're reoffended and if
somebody that looks similar like the person right is now applying for the job is now applying to
be left out on parole is now wanting to be promoted if they look like similar people that you know
reoffended or dig well on the job then you give them the same chance because you assume
that similar patterns will emerge right and that's all great again if we tend to make good decisions
fair and just decisions that are accurate but if you're very honest very often that is not the case
so unless you're very very careful you will just reinforce the biases and injustices that we had
in human decision making but at much greater speed and much less detectable and so I got interested
in this topic as well and the first question that I always ask myself is like well is the law
sleeping why not just use the law to solve that problem and I got interested in the question of
honest remediation law because that's the closest the most sensible law to look at and
give you wrote in two papers in the past one is called why fairness cannot be automated which
whole what he tells you how I felt about whether this is possible and and what we what we did
there was quite similar to to previous work where we showed that the law just wasn't designed in
a way to govern algorithms it was designed to govern people right so if you think about a
discrimination setting a discrimination setting a traditional one is where you know somebody
indirectly directly is not giving you the job because you're a woman or they harass you because
of your religious beliefs or you are in a hostile environment where other things are going on that
prevent you from succeeding the basic point is you know that something's off right so you bring
a complaint and the discrimination law will will help you with that so with algorithms again it's
different because they discriminate behind your back without you actually being aware so a complaint
based system such as non discrimination law is completely powerless if the person doesn't know
that they have been wronged and again that's not a failing of the law per se and sort of failing
of the algorithm it's just a very unhappy mismatch of the two because again the law was designed for
people not for for algorithms so but discrimination still occurs so what is it that I need to do
which means we have to test and test and test because if I don't know somebody has to know
and the problem is you can only know if you test for it because and that's the second problem very
often you might not even know that the data that you have collected is biased or unfair towards certain
people because again here the intuition kind of rakes down and on discrimination law was very much
based on intuition a judge looks at the case and says oh what you bent head scars from the workplace
that's a problem if we're religion you don't need much data to make that point because there is
a clear understanding of the social reality or the social symbolism of head scarves and religion
that's not much you need to do that right what but what if I you know got fired because I don't have
a dog right that sounds maybe odd but do I know that that correlates with ethnicity or gender
sexual orientation versus belief I really don't know anymore right so how am I supposed to bring a
case in court if you're using data where my social gut doesn't ring alarm bells anymore right
so those two things that I might not know about it and that even if I know about it that
I have almost no way of proving it means that fairness cannot be optimal to my defensive
favorite title so we thought okay then you need to test you need to test test somebody needs to
test because otherwise you wouldn't know so we we came up with with a biased test that lets you
do that so the test is called um demographic conditional demographic disparity CDD
and we chose that test because it aligns the most with European nondiscrimination law in a way
that other tests do not and yeah this year in January of February Amazon has came across our work
um and that bias test and they found interesting and they have decided to implement it in their own
bias toolkit um so say to make a clarify so now customers of Amazon can use that test as well
but again as with all of our research it's publicly available so if anybody's interested in that
having a closer look or in the court or whatever it's free and publicly available as well
and so can you talk a little bit about what differentiates this conditional demographic disparity CDD
tests with the tens or potentially hundreds of other statistical tests that have been used as
metrics of bias in data sets yes certainly so um that's actually quite exciting and that's
the the second paper that complements the the first one uh we also wrote a paper which is called
bias preservation and machine learning and the legality of fairness metrics nondiscrimination law
so exactly uh the question that you uh just asked me um so what we did there is we looked at
uh 20 different fairness tests and um we came up with the classification system
on how they how they make decisions um the one category is um called bias preserving bias tests
and the other one is called bias preserving uh bias transforming fairness tests so what we looked
at is that um the majority of them so 13 out of 20 uh bias preserving so what they do is they look
at the error rates to measure fairness so they want to make sure that um whatever type of decision
has been made and is now being made has the same uh base rate for errors the other ones the other
seven are more looking at decision rates when they're looking at how the outcome is distributed
across certain groups right so that's that is the main contribution coming up with that distinction
and trying to tease out the underlying assumption of this right one bucket says um as long
as we're not making things worse than they used to be i give my fairness check and it's okay
that bias transforming metrics that look at the decision rates say i'm only happy if equal outcomes
are happening across groups right so this is the underlying assumption this is fine
unless you look at what european on the scrimination law wants to do non-scrimination law in europe
is not just about formal equality as in do not actively treat somebody differently because
of their race or gender or sexual orientation right which is more like a negative form as
in passive form of discrimination law um non-scrimination line europe is much more about
substantive equality which is more about actively dismantling inequality keeping things as they are
is not good enough in europe so you're supposed to take an active role as much as you can both the
pride of the public sector to actually make the world a fairer place right and the majority of
those fairness tests don't do that because they condition on an unequal status quo and they freeze
that and that is the main problem and with our fairness test and with other data bias transforming
you could counterbeam that at least would give the opportunity to actually uh make it better
got it got it and so at least for those whose problem is uh specifically covered under the regime
of the european on discrimination tests this cdds the only test the only fairness test that
uh is adequate not not the only one like anything that is bias transforming and there are seven
others that are also uh bias transforming um that you can use you could use those definitely
those are absolutely fine to use and we pointed out in the papers and we listed exactly so
enough other tests that you could use and also it is not to say that you cannot use bias preserving
metrics completely in europe right it just depends on what the context is if you're using bias
if you're making decisions life changing decisions about people in europe in a sector that is
a protected and that is be known to exhibit bias then you should use a bias transforming metrics
right um because it at least gives you the ability to make something better than it used to be
it doesn't have to be you don't have to make it better but you need to at least justify it in a way
however you could still use a bias preserving metrics um and either justify that as well but I think
it's difficult but there are legitimate areas where you can use it but you don't have a problem so
if you use it for research only where it doesn't affect people absolutely fine to use bias preserving
in europe as well if you are unsure of what a good outcome would actually look like it's much
better to keep things as they are than making them worse right if you don't have a normative idea
of how things ought to be then you can use them as well you can also use them if there are situation
where we have you know justified bias or even desired bias if we wanted that no one to preserve
that that is fine to use as well and then areas where you do actually have ground truth where there
is no bias in the data set you can also use bias preserving metrics so there's a whole range
where you can justify we use it the only problem is that if you're making life changing decisions
about people in a protected sector and that sector is known to exhibit certain biases
then the preference would be it be easier for you from a legal perspective to use bias transforming
ones because at least they offer the possibility of making things better they used to be
so the the first contribution of this paper is this drawing the distinction between bias preserving
and bias transforming and correlating those two classes of fairness metrics or bias metrics to
the European non discrimination law but then you're also proposing this new test how does
how is that new test advantage relative to the class of bias transforming metrics that you know
can work under the European law what you really need to do and what a bias test should help you do
is reflect on what you are able to do in order to make a positive contribution the bias test is
not supposed to tell you what's right or wrong it just tell it should just tell you that something
might be a problem it's supposed to act as an alarm system so what you should be doing is that
you run let's say you you you thinking about loan decisions that you run your algorithm that
is distributing loan decisions and it will tell you or did you know that your current deployment
doesn't give black people any loans right and then it will tell you what kind of conditions
criteria and variables were conditioned on and what that helps to do is to have an informed
discussion of whether or not certain biases are justified or not because again in Europe on
the non discrimination law not everything needs to be fair what needs to happen is if something is
unfair you need to tell me why yeah and that is the thing that the test helps you to do because
let's go back to the bank example you could say for example what we use income to make decisions
on what does somebody should get a loan which makes intuitive little sense right and you could
put those criteria conditioning on input in there and it will tell you oh
hardly any women are getting loans is that on purpose right and then you could ask the question
okay I'm conditioning on on salary and it has a negative effect on gender is that justified
because you could say well how can we even use income since we know about the race and gender
pay gap right that's a horrible criteria to use in the first place right and say oh we're not
gonna use that we're not gonna do that or you could say well yes we know about those inequalities
but it's a very effective and defendable proxy um but are not somebody will be repairing
the loan and actually putting people in a situation where they have to default will default
and in depth a minute further it's also irresponsible so maybe we should be using income right
or you could say well um okay income isn't great but how about those other criteria additional
criteria that are also very good at predicting something but are not as disadvantaged
um that's for example income that's the dialogue you need to have right and the test can help
you to do that because it would lay open how your current decision system is affecting or is
making decisions across groups where you could see does it actually make equal decisions across
you know gender lines or lines with ethnicity or age or whatever and it tells you what kind of
criteria we're conditioned on and then you can justify to yourself and ideally actually
to the general public as to why those criteria are acceptable to use um even if they
maybe end up in an unequal distribution if it is the only way to go about it right because
that's the very inconvenient and unhappy discussion we need to have is what kind of disparity is
acceptable and which is not and that biased test will let you do that. It sounds like
and this is maybe a parent from the name that the the key thing that you the key innovation
or contribution of this new test is that it makes explicit this conditioning on other factors
something that's maybe you don't have the same degree of flexibility or the same mechanism to do
that conditioning with some of the other tests. Yes exactly. Got it got it um and then uh you know
so you've you've now got this test you've got this um you've identified the you know the relationship
between these tests and and this particular set of regulations in Europe you know what are the
next steps for you in pursuing this researcher or more broadly your interest in bias fairness
and discrimination and law. Yes yeah fantastic question um I I'm definitely going to maintain
an interest in in all those free areas going forward um I think one of the next topics that I
will be um diving a bit more into is the new regulation that is looming at the horizon here in
Europe because um there's a new draft that came out by the European Commission um the AI Act that
is the first ever first attempt at a regulatory comprehensive regulatory framework to to govern that
so there will be um I think a lot of work that not just me I think a lot of people will try to dive
their teeth into to figure out um you know whether whether this is a good attempt and and what can be
done there so I think that will remain a very active research area and the other ones that I'm
definitely going to are areas of of health that I'm particularly interested in as well as um
education and financial services. And those um those latter two areas health and well three health
education and financial services uh still looking at the intersections of those with AI and the law
or uh I think that I think those free areas will accompany definitely but um I will probably look
at it more from like a power perspective and then and a and a broader regulatory landscape and
oversight um landscape of that makes sense. Got it got it got it. Well Sandra thanks so much for
taking the time to share a bit about what you are working on very interesting stuff and uh
certainly enjoyed the conversation. Thank you so much it was great to be here thank you for the
invitation.
