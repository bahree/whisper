WEBVTT

00:00.000 --> 00:13.400
Welcome to the Twimal AI Podcast.

00:13.400 --> 00:23.040
I'm your host Sam Charrington.

00:23.040 --> 00:29.000
Today we close out our TwimalCon coverage with a panel on operationalizing responsible

00:29.000 --> 00:35.040
AI, but first a quick announcement.

00:35.040 --> 00:39.160
Have you been enjoying our TwimalCon coverage, but want more?

00:39.160 --> 00:44.480
TwimalCon video packages are now available for advanced purchase over at twimalcon.com

00:44.480 --> 00:46.040
slash videos.

00:46.040 --> 00:51.440
The package features over 13 hours of content, including all the awesome keynotes and panels

00:51.440 --> 00:57.640
you've heard on the podcast, all of the breakout sessions, including 9K study and 8 Tech

00:57.640 --> 01:03.920
track sessions, as well as the highly regarded team tear down panels with Airbnb and serving

01:03.920 --> 01:04.920
monkey.

01:04.920 --> 01:15.560
Again, visit twimalcon.com slash videos for more info and now on to the show.

01:15.560 --> 01:20.320
I am really excited to welcome up a panel that's going to talk through a really important

01:20.320 --> 01:27.320
topic, operationalizing how we do machine learning and AI ethically and to lead us through

01:27.320 --> 01:28.320
that.

01:28.320 --> 01:33.720
I'm excited to have our moderator for the panel, Kari Johnson from Benchabee, and he will

01:33.720 --> 01:36.520
introduce the rest of the panel.

01:36.520 --> 01:42.000
Kari and panel, welcome to TwimalCon.

01:42.000 --> 01:49.400
My name is Kari Johnson and I'm senior AI staff writer at Benchabee, and I'm really excited

01:49.400 --> 01:51.200
to be here with our panel.

01:51.200 --> 01:56.720
I could introduce everyone, but I think everyone will do a better job if they do it themselves.

01:56.720 --> 02:03.720
So we'll just quickly go down the row, but we can begin with a quick icebreaker, I think,

02:03.720 --> 02:05.120
that's all right.

02:05.120 --> 02:12.920
I was thinking icebreakers are, they can be corny, that's possible, but sometimes they also

02:12.920 --> 02:16.520
let you know a little bit about the other people too.

02:16.520 --> 02:18.920
When you were a kid, what did you want to be when you grew up?

02:18.920 --> 02:19.920
I'll go first.

02:19.920 --> 02:25.720
I wanted to be a firefighter, but I also asked my mom to do an op-ed on local television

02:25.720 --> 02:31.440
about why kids don't need to take showers, so it could be that I'm doing what I always

02:31.440 --> 02:32.440
wanted to be.

02:32.440 --> 02:37.320
I wanted to be a politician and ultimately a president.

02:37.320 --> 02:38.320
Yeah.

02:38.320 --> 02:39.320
That's nice.

02:39.320 --> 02:43.800
I wanted to be a writer, I actually wanted to write novels, and I write Scalicote, so

02:43.800 --> 02:45.520
yeah, here we are, yeah.

02:45.520 --> 02:51.360
At various points, I wanted to be a nurse, marine biologist, archaeologist, and writer.

02:51.360 --> 02:54.520
These are all good professions, I think we're doing good stuff too.

02:54.520 --> 03:01.600
But again, down the line, if we could start with Perry, and can you please correct me on

03:01.600 --> 03:05.320
the correct pronunciation of your name and tell us a little about the current work that

03:05.320 --> 03:06.320
you do?

03:06.320 --> 03:07.320
Sure.

03:07.320 --> 03:08.320
I'm Perry Nas.

03:08.320 --> 03:09.320
I'm coming from Georgian part.

03:09.320 --> 03:11.920
I work for Georgian partners right now.

03:11.920 --> 03:15.320
For those of you who are not familiar with Georgian partners, we are a venture capital

03:15.320 --> 03:21.840
based in Toronto, biggest private funding in Canada, investing in grossest-stage companies

03:21.840 --> 03:28.120
as software services, and our business plan and model is a little bit different.

03:28.120 --> 03:32.760
We have impact in where I'm coming from in addition to the investment team.

03:32.760 --> 03:37.640
We are like a applied research lab for our portfolio companies, to risking research and

03:37.640 --> 03:44.280
innovation for them, and accelerating the adoption of cutting edge disruptive technologies such

03:44.280 --> 03:50.000
as machine learning and AI, so we are a team of practitioners working on R&D projects

03:50.000 --> 03:52.160
with our companies.

03:52.160 --> 03:53.160
Awesome.

03:53.160 --> 03:54.920
I'm Guillaume, which is French for William.

03:54.920 --> 03:58.520
If my first name is too hard to pronounce, if you're coming to William, or even Bill, if

03:58.520 --> 04:03.080
you must, essentially, I lead computational science at LinkedIn, which is a team that's

04:03.080 --> 04:05.440
heavily invested in fairness.

04:05.440 --> 04:08.440
I assume I don't need to introduce LinkedIn, but if you don't know what it is, you should

04:08.440 --> 04:09.440
check it out.

04:09.440 --> 04:11.240
It's a pretty cool network.

04:11.240 --> 04:17.200
Essentially, what I've primarily worked on lately is specifically about the experimentation

04:17.200 --> 04:20.640
side of fairness and responsible design.

04:20.640 --> 04:24.680
I'm Rachel Thomas, and I'm Director of the Center for Applied Data Ethics at the University

04:24.680 --> 04:29.320
of San Francisco, which we just launched a few months ago, so it's still very new.

04:29.320 --> 04:36.520
I'm focused on harms that are happening now, so that includes unjust bias, surveillance

04:36.520 --> 04:41.840
and the erosion of privacy and disinformation, and at the center, we'll be working on a

04:41.840 --> 04:47.520
research, a mix of research, education and policy.

04:47.520 --> 04:52.840
Co-founder of FastAI, where we're interested in getting people from more diverse backgrounds

04:52.840 --> 04:55.840
into AI.

04:55.840 --> 05:03.320
Just to get us started, how do you get started with operationalizing AI ethics?

05:03.320 --> 05:07.640
What do you feel like are the first steps that an organization should be taking to set

05:07.640 --> 05:08.640
the table?

05:08.640 --> 05:09.640
That's a very good question.

05:09.640 --> 05:14.680
I believe you have to start with the vision, your vision of the company, and that's what

05:14.680 --> 05:19.360
we always value a lot when we are doing the due diligence.

05:19.360 --> 05:22.160
Then, of course, the culture is also very important.

05:22.160 --> 05:26.120
Teams, do you have the right team in place?

05:26.120 --> 05:32.040
You have to understand that, of course, you are building softwares, but most of these issues

05:32.040 --> 05:37.960
started from not having diverse teams working on these software products, so it's really

05:37.960 --> 05:44.560
important to start by building a diverse team, not only in terms of the gender or ethnicity,

05:44.560 --> 05:50.400
but maybe we need to have new roles in our machine learning or data science teams.

05:50.400 --> 05:55.600
Maybe we need sociologists, maybe we need somebody with legal and compliance background, so

05:55.600 --> 06:01.440
it's not only about, okay, let's have equal number of women and men, or from different

06:01.440 --> 06:07.080
ethnicity, but it's also about maybe thinking about diversity in the background of the team,

06:07.080 --> 06:12.760
and at the end, there is the question of what kind of processes do we need in place to

06:12.760 --> 06:19.000
be able and enable the team to build responsible, ethical AI products?

06:19.000 --> 06:20.920
Yeah, I think that makes a lot of sense.

06:20.920 --> 06:23.560
I think values and vision are very important.

06:23.560 --> 06:27.040
We found that we can't really have just one central team that writes one piece of code

06:27.040 --> 06:30.560
and fixes fairness, for example, for the whole company, and so it's really everyone's

06:30.560 --> 06:31.560
problem.

06:31.560 --> 06:34.760
Everyone is involved in this, and so what that means is that you really have to have

06:34.760 --> 06:37.280
the right alignment in terms of values and culture.

06:37.280 --> 06:40.440
So when you join LinkedIn, first thing they tell you before you even sit down is members

06:40.440 --> 06:41.440
first, right?

06:41.440 --> 06:45.080
So if you have a doubt that you always put the member first, and also act like an owner.

06:45.080 --> 06:49.280
And I think once everyone kind of feels like, okay, being fair, treating the members fairly

06:49.280 --> 06:52.880
and the way I would want to be treated myself, when everyone is kind of behind that, it

06:52.880 --> 06:53.880
makes it a lot easier.

06:53.880 --> 06:57.120
And of course, to get started practically, you also want to have the right tools and the

06:57.120 --> 07:01.520
ways to measure and the right processes, but really without culture and without the

07:01.520 --> 07:04.520
values, it's actually, I think, very hard to do anything, given the fact that it's really

07:04.520 --> 07:06.840
kind of a broad effort that everyone should take ownership of.

07:06.840 --> 07:07.840
Yeah.

07:07.840 --> 07:12.280
And for concrete resources, one resource I highly recommend is the Marcoola Center has

07:12.280 --> 07:18.200
a toolkit for tech ethics, and they recommend kind of a number of processes you can implement,

07:18.200 --> 07:21.240
but a key one is ethical risk sweeps.

07:21.240 --> 07:25.760
And so this is kind of periodically scheduling times to really go through kind of what could

07:25.760 --> 07:29.840
go wrong, and like what are the ethical risks, because I think a big, big part of ethics

07:29.840 --> 07:34.640
is also kind of, you know, thinking through what can go wrong before it does and having processes

07:34.640 --> 07:39.480
in place around what happens when there are mistakes or errors.

07:39.480 --> 07:44.520
Diving a deeper into that, what does the panel think about in terms of favorite frameworks

07:44.520 --> 07:50.760
or approaches or different tools that are available and out there now that teams can use?

07:50.760 --> 07:58.440
So let's start with what are under the umbrella of responsible AI, what are main concerns,

07:58.440 --> 08:04.200
privacy is one of them, and not only about the tools, but also about the technology, I

08:04.200 --> 08:10.200
really believe in the financial privacy and federated learning to build privacy preserving

08:10.200 --> 08:14.960
machine learning products and systems.

08:14.960 --> 08:23.160
In terms of explainability and communication with the end users or actually deboking, enabling

08:23.160 --> 08:31.240
developers to debok the systems, I really like approaches like Lime or Alime or Anchor

08:31.240 --> 08:32.320
Lime.

08:32.320 --> 08:40.160
In terms of fairness and bias, I believe it's really important to root out bias.

08:40.160 --> 08:48.240
So I personally use the tool developed by EPF all university and I guess Columbia University,

08:48.240 --> 08:55.120
its name is FairTest, discovering un-valented associations in data-driven applications

08:55.120 --> 09:02.920
and Google Wattif tool, they are also my favorites, and I guess that's it.

09:02.920 --> 09:09.760
And we recently also open sourced our TensorFlow tool for building privacy preserving machine

09:09.760 --> 09:11.160
learning systems.

09:11.160 --> 09:16.240
Yeah, so we have similar kind of areas that we care about.

09:16.240 --> 09:19.640
I would say there's a lot of internal tools that we're also building.

09:19.640 --> 09:23.080
Some of it is that there are some great tools that don't always scale, and we have a lot

09:23.080 --> 09:25.600
of data that we have to process.

09:25.600 --> 09:29.040
And we also have some tools that we're building completely our own.

09:29.040 --> 09:32.920
So for example, we have a lot of fairness built into experimentation.

09:32.920 --> 09:37.360
The idea is it's not just about the algorithm, it's not just about are you fair from an

09:37.360 --> 09:43.160
algorithmic perspective, once your output is put in front of humans, what kind of outcomes

09:43.160 --> 09:45.920
are we seeing between treatment and control, things like this.

09:45.920 --> 09:49.360
So that's also the kinds of, we also care a lot about that, which is try to think about

09:49.360 --> 09:53.040
it when you train your data, when you first put together your training data, when you train

09:53.040 --> 09:57.240
your model, and then even in an ongoing basis, how does it interact with people, how does

09:57.240 --> 09:58.240
it interact with users?

09:58.240 --> 10:02.480
And so we try to cover all of that, at least the whole kind of machine learning and AI

10:02.480 --> 10:03.480
life cycle.

10:03.480 --> 10:07.400
Yeah, going along with that, I think that some of the issues that arise from not thinking

10:07.400 --> 10:12.200
about the whole system, and so kind of how the different parts interact, I think a lot

10:12.200 --> 10:16.440
of tech kind of encourages us to kind of hyper specialize, and it's really important

10:16.440 --> 10:20.000
for people across different groups to be talking.

10:20.000 --> 10:25.320
One great idea I've heard from Alex Fierst is having, like having trust and safety embedded

10:25.320 --> 10:29.840
with engineering, product and design, because they have, you know, trust and safety is kind

10:29.840 --> 10:34.640
of seeing what can go wrong and what happens when it does, and that engineering, product

10:34.640 --> 10:39.160
and design tend to live in a bit more optimistic world, but I really think having kind of those

10:39.160 --> 10:43.440
communication channels open between groups, and also having kind of all the necessary

10:43.440 --> 10:47.880
stakeholders, like everyone's that's going to be impacted downstream involved.

10:47.880 --> 10:51.880
What do you feel like are some of your biggest concerns as it relates to responsible AI

10:51.880 --> 10:56.160
today, or to put it another way, if you could change one thing about the AI ethics debate

10:56.160 --> 10:57.960
today, what would it be?

10:57.960 --> 11:02.320
I think I might actually say, like, you know, focusing a bit more on harm and less on bias.

11:02.320 --> 11:04.960
I think sometimes we get focused on the role algorithms, and you're like, oh, there's

11:04.960 --> 11:08.640
bias, and I think there's a sense in which like, you know, every algorithm is going to

11:08.640 --> 11:11.520
be biased somewhat, but you might see very different types of harm.

11:11.520 --> 11:15.520
But so Edlington, we're about people's careers, and so we actually, very mindful of like,

11:15.520 --> 11:18.360
are we helping everyone, you know, kind of like get ahead in their careers?

11:18.360 --> 11:21.840
And so in terms of kind of shifting the debate, I think there's a sense in which it's sometimes

11:21.840 --> 11:25.240
really too focused on kind of the algorithm itself, and kind of the bias of the algorithm,

11:25.240 --> 11:28.280
versus kind of like taking a little step back and kind of talking, thinking about harm

11:28.280 --> 11:29.280
more globally.

11:29.280 --> 11:31.160
Yeah, I completely agree with that.

11:31.160 --> 11:34.720
Like, I think bias is an important issue, but it's just one of many, and kind of looking

11:34.720 --> 11:39.400
at harms, and particularly aren't harms that are already happening when there are mistakes,

11:39.400 --> 11:45.520
you know, we've seen mistakes of an algorithm cutting off health care incorrectly, for Medicaid

11:45.520 --> 11:50.360
benefits, of teachers wrongly being fired, that kind of, there are things already happening

11:50.360 --> 11:51.880
and understanding those.

11:51.880 --> 11:58.520
And I think one of the dynamics that comes into play is that algorithmic systems can make

11:58.520 --> 12:03.720
it harder for anybody to feel responsible, and you know, bureaucracy does this as well,

12:03.720 --> 12:08.200
but often algorithms are kind of extending bureaucracy, and this is an idea Dana Boyd has

12:08.200 --> 12:12.840
talked about, but that's something that alarms me and we need to talk more about of kind

12:12.840 --> 12:17.200
of how we build systems where we can feel responsible for the outcomes, because that's important.

12:17.200 --> 12:23.320
Yeah, I believe also I do have a few of you, but I also believe that we need to have

12:23.320 --> 12:30.440
machine learning specific quality assurance processes, and also we have to have like a

12:30.440 --> 12:38.040
guard-lay, guard-rails fault tolerance, and we have to have a plan for when things can

12:38.040 --> 12:43.760
go wrong, and we have to reduce the impact of models errors, especially for more critical

12:43.760 --> 12:46.760
decision-making or applications.

12:46.760 --> 12:51.520
One of the problems I had so far is like some people believe maybe that these systems

12:51.520 --> 12:55.960
are perfect or they can be perfect down the road, but I personally believe because they

12:55.960 --> 13:01.600
are probabilistic systems that we are using them for deterministic decision-making, we

13:01.600 --> 13:05.920
can be might never reach to 100 percent performance.

13:05.920 --> 13:08.760
So it's really important to have guard-rails.

13:08.760 --> 13:10.680
I totally agree with that.

13:10.680 --> 13:15.000
No doubt, one other point also is to ask about just what are the things we shouldn't be

13:15.000 --> 13:16.000
doing at all?

13:16.000 --> 13:20.000
I think sometimes people skip to this, like, oh, how do we de-biase the data?

13:20.000 --> 13:22.800
You know, and we're seeing this a lot with facial recognition of like, oh, you know,

13:22.800 --> 13:27.440
we need to de-biase it and have more faces of people of color, but we also need to ask,

13:27.440 --> 13:32.520
like, you know, should we be using facial recognition this much at all, and in these use cases,

13:32.520 --> 13:35.440
and really kind of pausing at the start, too?

13:35.440 --> 13:41.680
It comes to that idea of like public perception, you know, you were talking about people believing

13:41.680 --> 13:43.320
that it can be a perfect system.

13:43.320 --> 13:49.080
It seems like maybe part of the process, as well as teaching or at least having public

13:49.080 --> 13:53.120
education stuff out there so that people understand that these are probabilistic systems and they're

13:53.120 --> 13:55.600
making predictions based on data.

13:55.600 --> 13:56.600
Exactly.

13:56.600 --> 14:03.120
Education is a key for both not only for the end users, but also for the developers, product

14:03.120 --> 14:06.240
leads, or the executive teams in general.

14:06.240 --> 14:07.240
Yeah.

14:07.240 --> 14:08.240
Yeah.

14:08.240 --> 14:09.240
I agree with that.

14:09.240 --> 14:13.320
I would also add on that I think in some cases tech companies bear some responsibility

14:13.320 --> 14:17.720
for overhyping what they're selling, and then are kind of, you know, praying on that

14:17.720 --> 14:23.080
often people purchasing these products don't have the understanding of probability that

14:23.080 --> 14:30.080
they need, or have these misconceptions that AI is 100% accurate, but I think we also have

14:30.080 --> 14:34.360
a real responsibility to not over promise or over sell the capabilities of what we're

14:34.360 --> 14:35.360
doing.

14:35.360 --> 14:36.360
Yeah.

14:36.360 --> 14:41.160
There's definitely a lot of marketing injected into the AI space.

14:41.160 --> 14:44.840
Guillaume, I was curious if you could talk a little bit about sort of the practical challenges

14:44.840 --> 14:50.920
of scaling a responsible AI within a large organization, and as an aside to that, if you

14:50.920 --> 14:56.600
think of sort of those rules as different for the organization-wide approach as opposed

14:56.600 --> 14:59.080
to like specific teams within the company.

14:59.080 --> 15:00.080
Yeah.

15:00.080 --> 15:04.880
So as I briefly mentioned, it really starts with the values kind of in the culture.

15:04.880 --> 15:08.240
And then you know, it starts with essentially we also found that it's hard to just have

15:08.240 --> 15:10.080
one role that applies to everybody.

15:10.080 --> 15:12.840
And so what we try to do is kind of give ourselves the tools, the measurement tool, the

15:12.840 --> 15:16.400
analysis tool, the experimentation tools that we can then kind of like deliver to specific

15:16.400 --> 15:21.200
teams, and we work with them to figure out like what makes sense for their specific vertical.

15:21.200 --> 15:25.920
And importantly, we also care about kind of the final outpost once there's an experiment

15:25.920 --> 15:27.360
that people are interacting with it.

15:27.360 --> 15:31.720
And so you know, we really try to have like a 360 degree view of the whole process and

15:31.720 --> 15:33.720
make sure that kind of every part of the process is covered.

15:33.720 --> 15:38.000
So say you train an algorithm, for example, you know, we'd like you to be a wire like

15:38.000 --> 15:40.760
whether it's let's say a general representative or something like that.

15:40.760 --> 15:43.760
And so you know, we try to make sure that people have the information in front of their

15:43.760 --> 15:45.680
eyes when it's available.

15:45.680 --> 15:48.600
And we also have, you know, we just kind of have a lot of meetings where we just kind

15:48.600 --> 15:52.080
of discuss whether we this is the right thing or not the right thing.

15:52.080 --> 15:56.400
You know, it my sense that it has to be decentralized to an extent.

15:56.400 --> 15:59.920
And so this is also why, you know, when we so we analyze all the experiments and we look

15:59.920 --> 16:03.360
at what's happening with the fairness metrics and sometimes people have like really fairness

16:03.360 --> 16:05.280
and enhancing things, but they didn't know they did.

16:05.280 --> 16:07.320
It's just it's just a really happy accident.

16:07.320 --> 16:11.080
And we also want to be able to like see that and reward that and kind of learn from what

16:11.080 --> 16:12.080
they did that worked.

16:12.080 --> 16:13.080
Right.

16:13.080 --> 16:15.360
And so really we're trying to have this kind of like decentralized approach where everyone's

16:15.360 --> 16:16.360
an owner.

16:16.360 --> 16:20.280
What are some of the sort of major AI systems that are used at LinkedIn?

16:20.280 --> 16:24.280
So essentially it's, so we have like, we have the feed, we have the recommendations.

16:24.280 --> 16:25.280
Yeah.

16:25.280 --> 16:28.600
So essentially, you know, these are built into potentially they can be used by everything

16:28.600 --> 16:29.600
basically.

16:29.600 --> 16:33.800
So to give you a couple of examples, one that uses this heavily is the when we rank candidates.

16:33.800 --> 16:34.800
So job candidates.

16:34.800 --> 16:37.480
You like until LinkedIn, you see potential candidates, right.

16:37.480 --> 16:39.640
And so those we make sure that they're representative.

16:39.640 --> 16:43.080
And so there, you know, we actually make sure that when you see, let's say your first

16:43.080 --> 16:46.240
list of candidates, it's actually representative of the, you know, population distribution

16:46.240 --> 16:49.800
of the underlying talent pool and this, you know, and the bias there is like we're being

16:49.800 --> 16:52.320
very, very careful that you're seeing something as representative.

16:52.320 --> 16:53.320
So that's that's one example.

16:53.320 --> 16:54.320
Yeah.

16:54.320 --> 16:55.320
Yeah.

16:55.320 --> 16:59.400
And for the panel, just curious what you think about in this age of AI, how this ethos

16:59.400 --> 17:05.120
of move fast and break things with startups has evolved or changed, if at all?

17:05.120 --> 17:06.120
Yeah.

17:06.120 --> 17:12.960
I believe generally it's much harder for startups to proactively prioritize responsible

17:12.960 --> 17:14.680
and ethical use of AI.

17:14.680 --> 17:21.440
It's been kind of overwhelming for it and endeavor for them.

17:21.440 --> 17:24.680
And that's why we gave them a framework in our team.

17:24.680 --> 17:29.520
We provided the principles and framework for them and we, we tried to kind of educate

17:29.520 --> 17:35.120
them that trust and responsible AI is a, you can think of it in a two in the, in terms

17:35.120 --> 17:41.560
of two dimensional space where one where to, when one dimension is the value, you deliver

17:41.560 --> 17:46.040
to your customers and the other dimension is the level of comfort.

17:46.040 --> 17:51.920
So of course, the value is like the actual product you are building.

17:51.920 --> 17:59.000
And then level of comfort can be a, but it's mainly about data ownership and do customers

17:59.000 --> 18:04.840
have any opinion in your product roadmap or not privacy and security.

18:04.840 --> 18:10.960
One of them most important ones and explainability and transparency.

18:10.960 --> 18:19.000
Do your customers know how their data is used or end users and for what kind of purposes

18:19.000 --> 18:24.480
you are using that data and do they have based on this new compliance, for example, GDPR

18:24.480 --> 18:30.640
right to be, to delete their data from your existing databases or even fingerprints from

18:30.640 --> 18:32.160
your Michelle and X systems.

18:32.160 --> 18:35.720
Is this something you ran across at the Center for Applied Data?

18:35.720 --> 18:37.760
I think Senator, you guys are just getting started, but yeah.

18:37.760 --> 18:38.760
Oh, yeah.

18:38.760 --> 18:39.760
That's good to say.

18:39.760 --> 18:43.880
I think that kind of looking at like a, on a long time horizon, I think being ethical

18:43.880 --> 18:49.560
is a kind of like a profitable decision and I think there is this real conflict of interest

18:49.560 --> 18:54.040
though with, there's so many short term pressures and pressures to focus on the short term.

18:54.040 --> 18:58.200
And that's something that is tough, tough to crack.

18:58.200 --> 19:01.840
And I think that it's great to hear about like a venture firm working on that because

19:01.840 --> 19:07.000
I think often the incentives around venture capital and kind of just our current corporate

19:07.000 --> 19:12.000
ecosystem push people on short term and that it does take a longer time horizon to think

19:12.000 --> 19:17.680
about ethical behavior and the financial benefits of ethical behavior.

19:17.680 --> 19:22.360
I think that there are deeper benefits to ethical behavior in the short term as well.

19:22.360 --> 19:23.360
Yeah.

19:23.360 --> 19:27.280
And I think also like if there's also the risk that like if you're very biased, you might

19:27.280 --> 19:29.440
only kind of cater to one population, right?

19:29.440 --> 19:31.720
And so eventually that limits the growth of a user base.

19:31.720 --> 19:35.600
So even from a business perspective, you actually want to have everyone come on board, right?

19:35.600 --> 19:37.440
And so it's actually, I think it's actually, I agree with you, I think it's actually

19:37.440 --> 19:40.760
good business in the long run to actually be responsible and worry about this.

19:40.760 --> 19:43.920
I would say though, I don't think LinkedIn ever really was kind of move fast and break

19:43.920 --> 19:45.440
things as far as I can tell.

19:45.440 --> 19:48.520
It's, you know, I think it's, if you're an ML engineer, sometimes you can kind of get

19:48.520 --> 19:52.160
carried away and maybe you forget why you're working on, but at least when you're working

19:52.160 --> 19:55.600
on something that's about jobs, it kind of puts this, you know, you kind of realize,

19:55.600 --> 19:59.880
okay, like this is important and how would I want people to treat my resume and my own

19:59.880 --> 20:00.880
job?

20:00.880 --> 20:03.440
And so my sense is like, and of course I wasn't around when the company was created,

20:03.440 --> 20:07.240
but it seems like it seems like for a very long time, at least it's been about responsibility

20:07.240 --> 20:09.800
and kind of like putting members first.

20:09.800 --> 20:12.920
So I've got two more questions and then I'm going to open it up and see if there's any

20:12.920 --> 20:14.760
questions from the audience.

20:14.760 --> 20:18.400
One of them was, you know, Rachel and I were talking about this before we got started

20:18.400 --> 20:24.280
today and it's like, so the Pope was talking about AI ethics effectively at a tech conference

20:24.280 --> 20:30.880
at Envatican City a couple days ago, which is something I want to see pictures of or something

20:30.880 --> 20:32.880
that must be interesting.

20:32.880 --> 20:38.000
But I mean, and then, you know, they're essentially talking about a need for protection of the

20:38.000 --> 20:42.800
common, of the common good, from AI perspective as well as tech.

20:42.800 --> 20:49.840
And you know, other ones, examples of, yeah, ethics coming up in the context of power

20:49.840 --> 20:55.560
in AI, it feels like a word that's been missing from a lot of conversations around artificial

20:55.560 --> 21:01.960
intelligence, Alexandria, Casio, Cortez, and someone else, Joey Blumwinny having a long

21:01.960 --> 21:09.160
exchange about the systems working best on white men and not as well on a women of color,

21:09.160 --> 21:11.880
for example, it seems present in a lot of these.

21:11.880 --> 21:19.040
And this is a very long-witted way of asking how the panel, if there are any specific ways

21:19.040 --> 21:23.240
that you see this power dynamic playing out in the AI ethics conversation today.

21:23.240 --> 21:27.080
I think, you know, this is also one place where like diversity is like super important,

21:27.080 --> 21:28.080
right?

21:28.080 --> 21:30.280
Because essentially the people are in the room making the decision about like what should

21:30.280 --> 21:32.600
be fair will kind of refer to their own experience.

21:32.600 --> 21:35.600
And so that's why, you know, it's very important to like make sure that both in the company

21:35.600 --> 21:40.880
and also kind of like inside, you know, your development teams, like there's actually as

21:40.880 --> 21:45.720
much diversity as possible, so that the decision kind of is made in a way that's kind of like

21:45.720 --> 21:49.600
kind of conscious of everyone's kind of, you know, approaches.

21:49.600 --> 21:53.560
I think, you know, it's, and really diversity is probably one of the best way to get, at

21:53.560 --> 21:54.560
least get started on that.

21:54.560 --> 21:58.400
Yeah, I think it's a really important point you bring up, I would say one area, I think

21:58.400 --> 22:03.680
we see it a lot in is surveillance technology, which tends to kind of disproportionately

22:03.680 --> 22:07.160
be applied against, against poor people.

22:07.160 --> 22:12.360
We've also seen instances of, you know, Baltimore police, I, using facial recognition to identify

22:12.360 --> 22:17.280
protesters protesting the death of Freddie Gray, and those things definitely have this

22:17.280 --> 22:21.640
kind of clear power differential and, you know, even looking at history kind of how surveillance

22:21.640 --> 22:28.640
technology has been used to suppress dissent and moves for social, positive social change

22:28.640 --> 22:31.280
powers a very important dynamic.

22:31.280 --> 22:39.840
And then one last thing before asking the audience if you have any questions to NIA positive

22:39.840 --> 22:46.680
note, do you have any favorite examples of AI ethics success stories where an organization

22:46.680 --> 22:53.000
or team was able to resolve some, some, some challenges or confront them?

22:53.000 --> 22:58.640
I was working with one of our companies, turn it in, they are also based on Valley, and

22:58.640 --> 23:06.480
we were building a plagiarism detection tool mainly for investigators and teachers to mostly

23:06.480 --> 23:13.480
for universities and K-12 institutes, and so it's basically using NLP, all your kind

23:13.480 --> 23:20.520
of authorship style, not using NEPIO, not using any student, performance, grades, anything.

23:20.520 --> 23:27.120
But at the end of like development process, we get together and because I'm not a native

23:27.120 --> 23:35.120
speaker, I ask a question, like, is it possible that our model has, for example, more false

23:35.120 --> 23:39.560
positive rate against non-native speakers?

23:39.560 --> 23:44.520
And because, you know, like, we, historically, there might be more instances of like a

23:44.520 --> 23:50.400
plagiarism for non-native speakers and the model in humano-mission learning is all about

23:50.400 --> 23:55.280
correlation, not necessarily causation, and the model might have picked up those kinds

23:55.280 --> 24:01.600
of, for example, grammatical errors as the signs for, for example, plagiarism.

24:01.600 --> 24:06.600
And we started testing our system and we used the first test and we realized, yes, there

24:06.600 --> 24:13.440
is a potential problem and we actually could mitigate and address such bias in the system.

24:13.440 --> 24:18.120
Oh, that's, yeah, so I have two, two small examples and the, so one that I briefly mentioned

24:18.120 --> 24:22.320
is the representative ranker, so when, you know, when you look for candidates, you essentially

24:22.320 --> 24:25.480
see something else representative, the second one is something that we actually discovered

24:25.480 --> 24:28.720
when we analyzed the experiments, so it was kind of a surprise, but it turns out, so

24:28.720 --> 24:32.560
we have this feature that sends like job notifications instantly, so, you know, when

24:32.560 --> 24:35.720
there's a job that's good for you, you instantly get notified, and we looked at the metrics

24:35.720 --> 24:37.640
and we're like, wow, this is like really equalizing.

24:37.640 --> 24:41.040
So, you know, it seems like people who were not too engaged or like didn't have like

24:41.040 --> 24:44.960
that great networks, you know, were kind of categories that, you know, we thought were

24:44.960 --> 24:46.360
a bit disengaged.

24:46.360 --> 24:49.640
These people just, we see the metrics go way up and we just, we wonder why, and it turns

24:49.640 --> 24:53.520
out, you know, like people tend to self-sensor when they're playing to jobs, they're like,

24:53.520 --> 24:56.280
oh, maybe I'm not qualified enough, maybe I've done good enough, or I can see there's

24:56.280 --> 24:58.000
already people applied, I don't want to do it.

24:58.000 --> 25:01.400
And so it turns out that like when an algorithm tells you, hey, we think you should apply

25:01.400 --> 25:04.080
to this, you'd be good for this, that can also be empowering.

25:04.080 --> 25:07.120
So that's a success story there, which is, you know, and, you know, we just found out

25:07.120 --> 25:10.640
that like some of these algorithms actually help people and actually reduce the gap between

25:10.640 --> 25:12.280
the categories that we might care about.

25:12.280 --> 25:15.560
And so, yeah, these are two, two stories that I was, I was pretty happy about.

25:15.560 --> 25:16.560
Yeah.

25:16.560 --> 25:20.840
Yeah, a story I like is from a meetup, Evan Estola, the lead machine learning engineer

25:20.840 --> 25:24.880
shared that when they were building their recommendation system to recommend meetups

25:24.880 --> 25:29.960
to people, they realized there was the potential for a feedback loop of if you're a woman

25:29.960 --> 25:34.520
or interested in technical meetups, so then the algorithm might recommend even fewer

25:34.520 --> 25:38.920
meetups to women, tech meetups, so you would have fewer women attending and kind of create

25:38.920 --> 25:41.240
this feedback loop pretty quickly.

25:41.240 --> 25:43.680
And so they decided to short circuit that from the start.

25:43.680 --> 25:48.520
And I really like that story because it's also an instance of kind of not just unthinkingly

25:48.520 --> 25:53.480
trying to optimize a metric, but thinking about, yeah, thinking about the outcome and taking

25:53.480 --> 25:56.040
the responsibility around feedback loop seriously.

25:56.040 --> 25:58.040
Nice.

25:58.040 --> 26:01.080
And any questions from the audience?

26:01.080 --> 26:02.080
There we go.

26:02.080 --> 26:04.560
I use this.

26:04.560 --> 26:07.760
Speaking to Lola Pal.

26:07.760 --> 26:09.800
Alrighty.

26:09.800 --> 26:22.000
So with the rise of deep learning, a lot of more principled approaches have fallen to

26:22.000 --> 26:24.000
the wayside in favor of empiricism.

26:24.000 --> 26:25.000
It's going to be a long question.

26:25.000 --> 26:26.000
It's not very long.

26:26.000 --> 26:30.560
I see that a lot in natural language processing.

26:30.560 --> 26:37.280
But in the case of ethics and AI, how do you think principled approaches fit in?

26:37.280 --> 26:40.240
Can you give an example of what you mean by principled approaches?

26:40.240 --> 26:48.480
So I work on machine translation and we've seen of shift towards bite pair encodings using

26:48.480 --> 26:54.760
sub words that don't necessarily have any meaning by themselves, as opposed to using

26:54.760 --> 26:57.120
a large vocabulary of whole words.

26:57.120 --> 27:03.960
So we can't inspect why a model attributes the decision to the sub words themselves as

27:03.960 --> 27:06.680
intelligently as before.

27:06.680 --> 27:12.960
It's kind of in conflict with deep learning's empirical approach to AI.

27:12.960 --> 27:15.960
But I'm wondering, do you think it can come back?

27:15.960 --> 27:16.960
So kind of.

27:16.960 --> 27:21.440
What do you do when your techniques abstract away from your ability to be fair?

27:21.440 --> 27:22.440
In a sense?

27:22.440 --> 27:23.440
A little bit.

27:23.440 --> 27:24.440
A little bit.

27:24.440 --> 27:29.480
Yeah, so I would say I think the Y is generally always a hard question.

27:29.480 --> 27:33.720
So we try to focus kind of at first as like we're seeing some metrics about is a performance

27:33.720 --> 27:37.480
the same for groups that we care about, how people interact with it.

27:37.480 --> 27:41.920
And even with this, we even when the techniques are simple, the Y is always kind of a difficult

27:41.920 --> 27:42.920
question.

27:42.920 --> 27:44.240
Like, why is it that we're seeing this?

27:44.240 --> 27:45.240
What is it about the algorithm?

27:45.240 --> 27:47.040
And sometimes it's not the arguments about the people.

27:47.040 --> 27:50.680
And so when it's about people like figuring the Y is very, very complicated.

27:50.680 --> 27:56.880
So I think what a first step is to try and at least to an extent try to have some idea

27:56.880 --> 28:00.680
about the metrics that we care about or the principles that we care about and maybe

28:00.680 --> 28:04.680
comfortable kind of either delaying something or thinking more deeply about something until

28:04.680 --> 28:05.680
we've understood the Y.

28:05.680 --> 28:10.520
And I agree that as things get more complex or as increasingly one algorithm, the output

28:10.520 --> 28:12.080
of one is the input of the other one.

28:12.080 --> 28:15.080
The Y gets really, really hard and that's no exception there, if that makes sense.

28:15.080 --> 28:16.640
But that's been my experience at least.

28:16.640 --> 28:24.000
I also believe that it's so hard, like the root cause analysis is always hard as we use

28:24.000 --> 28:29.240
more complex machine learning models, it's even harder, but I'm also very encouraged

28:29.240 --> 28:35.520
to see if we can use the same optimization techniques because it's like like deep learning

28:35.520 --> 28:38.920
any of those machine learning, they are like optimization tools for us.

28:38.920 --> 28:44.200
Can we use the same optimization tools to reach fairness?

28:44.200 --> 28:48.320
And there are some papers out there in the recent conferences.

28:48.320 --> 28:54.960
They talk about, for example, rather than optimizing over the global population.

28:54.960 --> 29:00.360
For example, optimizing for micro segments of the populations and getting to, for example

29:00.360 --> 29:05.560
demographic parity, but of course for products like machine translation is much more complex

29:05.560 --> 29:11.680
because the word embeddings, they can't even be racist and we know about all these problems.

29:11.680 --> 29:19.720
And it's really harder to kind of modify the objective functions to reach to the fairness

29:19.720 --> 29:25.800
in the context of, for example, machine translation, but I'm pretty optimistic that we can use

29:25.800 --> 29:28.240
the same technology to get there.

29:28.240 --> 29:32.560
And all I would add is I think it can be useful to just even simple techniques of altering

29:32.560 --> 29:37.760
your input to see how that impacts the output, can give you a lot of insight into what

29:37.760 --> 29:43.200
might be happening, as well as to remember that with humans we're bad at knowing why

29:43.200 --> 29:44.600
we make the decisions we do.

29:44.600 --> 29:49.920
We do a lot of post-talk justification of our decisions and so the causality is often

29:49.920 --> 29:52.920
not well understood there either.

29:52.920 --> 29:54.920
Thank you to our great panel.

29:54.920 --> 29:56.920
Not on pause.

29:56.920 --> 30:05.920
All right, that's our show for today.

30:05.920 --> 30:13.200
To learn more about today's show or any of our panelists visit twomolai.com slash shows.

30:13.200 --> 30:19.200
Make sure you visit twomolcon.com slash videos to secure your access to twomolcon video content

30:19.200 --> 30:20.200
now.

30:20.200 --> 30:36.200
Peace.

