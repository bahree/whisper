WEBVTT

00:00.000 --> 00:04.560
you need to find a way to translate, you know, the computational insight into the

00:04.560 --> 00:08.880
language of chemistry or biology, which kind of means you need to know chemistry and biology

00:08.880 --> 00:10.880
and draw discovery in addition to knowing the AI.

00:16.240 --> 00:21.920
All right, everyone. I am here with Barat Rumsunder. Barat is founder and CEO at DeepFar

00:21.920 --> 00:25.280
Sciences. Barat, welcome to the Twomo AI podcast.

00:25.280 --> 00:27.520
Thank you for having me on, Sam. Excited to be here.

00:27.520 --> 00:32.720
It's great to have you on the show and I'm looking forward to digging in and talking a little bit

00:32.720 --> 00:38.640
about your work on deep learning for molecular design to get us started once you share a little

00:38.640 --> 00:42.720
bit about your background and what kind of caught your interest about that field.

00:42.720 --> 00:48.080
Yeah, for sure. So a little bit of background about me, I did my PhD at Stanford a few years ago,

00:48.080 --> 00:53.040
worked with VJ Pandey's group at the time. So my, you know, training before then I had been as

00:53.040 --> 00:59.920
a software engineer and pure math person. So I was really, you know, when I started at Stanford,

00:59.920 --> 01:04.800
I kept passing these posters and all that had all these crazy pictures and molecules on them outside

01:04.800 --> 01:09.360
VJ's lab. So I kind of went in one day and was like, oh, let me just talk to these people and

01:09.360 --> 01:14.480
ended up clicking. So we decided to work together. A lot of the work I did there was in applying to say

01:14.480 --> 01:20.000
my previous background, working with software engineering and mathematics to the problem of

01:20.000 --> 01:26.720
molecular design and drug discovery. So I started an open source framework called DeepKem that has

01:26.720 --> 01:31.520
grown since I think we're now probably one of the most popular open source platforms for drug

01:31.520 --> 01:38.240
discovery out there. I also started a benchmark suite called molecule net which helped establish

01:38.240 --> 01:44.160
standard benchmarks for designing new molecular algorithms. That's been widely used in academia and

01:44.160 --> 01:49.680
in the industry. Since then after leaving Stanford, I co-founded a startup in the crypto space.

01:49.680 --> 01:55.600
I worked on that for a couple years but then decided to come back to working on biotech and medicine

01:55.600 --> 02:00.880
discoveries. So the last few years I've been working with different biotech and pharma companies

02:00.880 --> 02:06.480
on getting AI into their drug discovery process. So that's kind of the the heart of my new company

02:06.480 --> 02:12.240
Deep4 sciences. Awesome. Awesome. What do you see as some of the big challenges that the drug discovery

02:12.240 --> 02:18.320
companies are having as they try to incorporate AI into that process? That's it's a great question.

02:18.320 --> 02:23.840
I think there's a whole range of issues like starting at the very basic. Oftentimes IT setup

02:23.840 --> 02:27.920
is not, you know, unable to handle this. You have to think about, all right, do we get up on

02:27.920 --> 02:33.040
cloud? Many companies are transitioning to cloud so that partly is starting to be taken care of

02:33.040 --> 02:38.400
but it's not a core competency. I think in many of these firms, you often see like external

02:38.400 --> 02:44.800
consultants who are running the IT. So it can be an early situation. Other things I would say,

02:44.800 --> 02:51.280
there's just I think things like pay scale and miscaps between, you'll see very talented senior

02:51.280 --> 02:56.400
biologists getting paid less than a fairly junior developer would make at a tech company,

02:57.040 --> 03:03.280
which it's a it's a hard problem. So I think attracting talent, there's a good sound that you're

03:03.280 --> 03:07.840
working on what you could say are arguably more meaningful challenges trying to find medicine,

03:07.840 --> 03:12.480
but you also have to make that make greater sense for people. So I think a lot of these firms

03:12.480 --> 03:19.200
are still struggling to hire and build out their top tier AI talent. And I think that this will

03:19.200 --> 03:24.720
eventually begin to find a medium ground, but for now I have several friends who are all looking

03:24.720 --> 03:30.320
for basically the same type of candidate and they're also looking as far as I know. So it's

03:30.320 --> 03:34.560
definitely a challenge there, but university programs are starting to pick up the slack. I'd say

03:34.560 --> 03:39.360
other challenges. It's, you know, how do you mesh in these systems in well with human scientists?

03:39.360 --> 03:45.200
So I would say the AI is not a replacement for an expert human team by any means, but you need

03:45.200 --> 03:51.360
to find ways to have the human teams and the scientific teams work together in a complementary

03:51.360 --> 03:58.320
fashion. This can be really hard. Like as I'm sure you've heard from your other data scientist,

03:58.320 --> 04:03.520
a guest like it's a state communication problem, you know, the computational insight into the

04:03.520 --> 04:07.920
language of chemistry or biology, which kind of means you need to know chemistry and biology

04:07.920 --> 04:13.840
and drug discovery in addition to knowing the AI, which makes us a bit of a hard feel to get into.

04:13.840 --> 04:16.240
But I think it's worth it. I think it's been a lot of fun.

04:16.880 --> 04:25.360
At the highest level, the promise of AI for drug discovery is just that that some AI will be

04:25.360 --> 04:32.560
able to spit out drugs for us that are effective at solving medical challenges. Tell us a little bit

04:32.560 --> 04:38.160
about, you know, at a lower level of granularity. Like where is the innovation frontier now? And

04:39.440 --> 04:47.120
if all goes right in efforts like yours, you know, what's the promise for AI in the near term

04:47.120 --> 04:52.240
in this field? It's a great question. And I would say divide this field into two parts.

04:52.240 --> 04:56.240
The first I would say is machine learning for chemistry. And the second is machine learning

04:56.240 --> 05:01.920
for biology. I think machine learning for chemistry has made a lot of strides. Like you see,

05:01.920 --> 05:05.760
like a lot of top-tier conferences, like I clear recently announced they had a

05:06.320 --> 05:11.840
ML for drug discovery and molecules. Track on Europe's has had a standalone workshop for a few years.

05:12.720 --> 05:17.920
I think that there's a lot of open packages like DeepCam. There's DGL Life Sciences from Amazon.

05:18.560 --> 05:24.160
And a few others that really have started to support this field. I would say there's still

05:24.160 --> 05:30.480
challenges in generating molecules effectively, but say predicting properties in molecules,

05:30.480 --> 05:34.880
say analogous to predicting properties of images. Is there all to solve task? But it's an

05:34.880 --> 05:42.880
increasingly straightforward task. The challenge I think is still working at the low data setting.

05:42.880 --> 05:48.960
So unlike other parts of the tech industry, you can't have human annotators create data.

05:48.960 --> 05:53.760
You need to have some type of experimental output actually coming from either a trained lab tech

05:53.760 --> 05:58.400
or robot that's been very properly configured. So we're figuring out how to get

05:58.400 --> 06:04.000
transfer learning working, how to get low data methods, meta-learning. Working I think is

06:04.000 --> 06:08.880
the real frontier. Lots of cool papers like people learning things like contrasts of learning,

06:08.880 --> 06:15.280
looking at the 3D structure, using transformers. There's a whole range of papers that are exploring

06:15.280 --> 06:19.760
this. But I'd say that's the frontier right now. The basics work, but how do we make this work in

06:19.760 --> 06:24.000
like practical settings where there's a lot much data. The secondary machine learning for biology,

06:24.000 --> 06:29.840
I think, is much more mysterious. I would have said that there weren't really any successes until

06:29.840 --> 06:39.600
deep-mind announced alpha-folds too. Alpha-fold 2 is really, I think, made a dramatic difference

06:39.600 --> 06:47.440
in this field where you've taken, I'd say, a fundamental problem of biology, you know,

06:47.440 --> 06:51.760
predicting the structure of proteins. And this has been open for about 50, 60 years now.

06:51.760 --> 06:59.760
And been able to push that into a place where you could argue it's even solved. I think this

06:59.760 --> 07:04.880
just really had a sea change in how scientists and biologists see AI methods. Whereas before,

07:04.880 --> 07:08.640
it was something that was kind of exotic and, well, that doesn't really work. Now it's like,

07:08.640 --> 07:12.480
oh, okay, they just whacked this problem out of the ballpark. We need to pay attention.

07:13.200 --> 07:19.520
But besides that one place, I would say that AI and bios still very early stage. To give an

07:19.520 --> 07:25.920
example of the fundamental challenge, like take a disease like Alzheimer's. The what you hope

07:25.920 --> 07:31.680
the AI would tell you is that, okay, the root cause of Alzheimer's is X. So you just need to

07:31.680 --> 07:37.920
design a molecule that modulates X somehow. Then bam, you'll cure Alzheimer's. We wish we knew that,

07:37.920 --> 07:41.920
but then we see all these failed clinical trials for Alzheimer's because that's an extraordinarily

07:41.920 --> 07:45.840
difficult problem. You're trying to untangle, you know, the history of evolution itself in the human

07:45.840 --> 07:51.920
body. So it's going very deep to the heart of medicine, I would say. And that's beyond today's AI.

07:51.920 --> 07:57.520
I think the best we can hope is to create better tools that make our best doctors and biologists

07:57.520 --> 08:03.040
more capable of handling these problems. So I'd say ML for chemistry much further along,

08:03.040 --> 08:07.440
closer to where, say, things like images are, ML for biology, fundamental science. I think

08:07.440 --> 08:14.240
it's a open field and just beginning to be explored. Got it. Got it. Tell us about the origins of

08:14.240 --> 08:21.120
deep chem. Yeah, absolutely. So back during my PhD, I was fortunate enough to do an internship

08:21.120 --> 08:27.040
with the Google accelerated scientists team at Google. So we use a early version of their

08:27.040 --> 08:32.000
internal AI system called disbelief. This is a pre-tensor flow system. And we built I think a

08:32.000 --> 08:37.840
really cool model and explored, say, large scale training on a bunch of data we gathered to do a

08:37.840 --> 08:44.320
multitask deep learning in the system. And this worked pretty well. And I was really excited. We got

08:44.320 --> 08:50.240
a paper out. We had it written up in the Google blog. A lot of people are excited. But as with all

08:50.240 --> 08:54.400
good things and the internship ended, I came back to grad school and it's like, oh, well, what's I

08:54.400 --> 09:00.080
don't have my core research discovery to work with. So at the time, Kyrsten Tionno just really

09:00.080 --> 09:06.160
started to come online. So I hacked together a few scripts to try to replicate the original paper.

09:06.160 --> 09:11.200
I got most of the way and wanted to share it with some friends. We just put it up on I just put

09:11.200 --> 09:16.160
it up on GitHub. And from there, it's sort of grown organically. So we attracted a few talented

09:16.160 --> 09:23.120
early contributors. We made the licensing permissive MIT rather than a more restrictive license.

09:23.120 --> 09:28.160
So we had some early corporate contributors come in really help out a number of grad students

09:28.160 --> 09:34.240
put projects and papers into there. We're also I think fortunate enough to be able to participate

09:34.240 --> 09:40.480
in Google Summer of Code through the Open Chemistry collaboration. We've been participating

09:40.480 --> 09:44.320
in Google Summer of Code. I think for three years now, had a number of excellent students.

09:45.040 --> 09:49.520
One of the big things that we've done is program allows students to come work on your open source

09:49.520 --> 09:54.720
project. Yes, that's correct. And Google will pay them for their time, which I think is

09:54.720 --> 09:59.440
a tremendous advance. You see all these kids who couldn't otherwise like participating in open

09:59.440 --> 10:05.600
source. One of the nice things we've grown is really a community of, you know, teaching and

10:05.600 --> 10:10.000
documentation. So we have about 30 tutorials really laying out the basics of the science.

10:11.520 --> 10:16.240
Also on the way, we ended up writing a book with O'Reilly deep learning for the life sciences,

10:16.240 --> 10:21.280
which introduces a lot of the techniques in this field and how to use deep Ken with them.

10:21.280 --> 10:25.600
So I think we've tried to put a lot of effort into growing a friendly collaborative open

10:25.600 --> 10:31.680
community. And I think that's really drawn people to work with us. A lot of the people who do AI

10:31.680 --> 10:36.880
in this industry, they often get, I think, their first taste of AI in this field through deep

10:36.880 --> 10:41.280
Ken. So I think that's something we want to do more of. So we continue working on it. We just had

10:41.280 --> 10:46.800
a new release come out last week. We had 29 contributors on this release. So we've got a,

10:46.800 --> 10:51.840
I think a vibrant open source community that's working to really build out better tools.

10:51.840 --> 10:57.360
And what are the specific problems that deep Ken solves for practitioners working in the field?

10:57.360 --> 11:02.080
So I think one of the big things that deep Ken does is we effectively serve as a model

11:02.720 --> 11:07.360
zoo slash repository. So we take a lot of these models that come out in the literature,

11:08.000 --> 11:13.360
you know, academics do great work. They invent a new technique. They put it up on GitHub. It's

11:13.360 --> 11:18.160
pretty clean code. But you're not incentivized really to go further beyond that. That's sort of where

11:18.160 --> 11:22.560
we come in. We say, okay, well, we need unit tests. We need to stabilize this. We need to make

11:22.560 --> 11:28.560
sure that this can be maintained. So we wire it into our CI system. We get test put up. We get

11:28.560 --> 11:34.080
now type checks. We get to everything linted. And at the end, we have a stable implementation of

11:34.080 --> 11:39.760
the model. And we have an extensive CI that tests, you know, Windows, Mac, Linux, there's about

11:39.760 --> 11:46.720
600 700 unit tests, a lot of testing on machine learning. How to stabilize machine learning methods?

11:46.720 --> 11:52.720
So we have a robust production grade implementation that can then be used by downstream industry

11:52.720 --> 11:57.840
and also academic practitioners. So we see ourselves as, you know, picking up from where

11:58.560 --> 12:03.440
academia often leaves off. We do have a number of students also interested in designing new models

12:03.440 --> 12:08.880
of their own. So we do have research collaborations with academics. But I think our mission really is

12:08.880 --> 12:15.840
service a high quality repository of good good models and techniques. What are some of the most

12:15.840 --> 12:22.160
popular models today on DeepCam? I would say the most popular historically has been the Graph

12:22.160 --> 12:27.520
Convolution Network. So we had, I think, one of the first quality implementations of a Graph Convolution

12:27.520 --> 12:32.960
Network for molecules. This was before PyTorch Geometric and DGL. In the last few years,

12:32.960 --> 12:38.320
we've increasingly shifted to using DGL and and PyTorch Geometric to underpin our Graph Convolution.

12:38.320 --> 12:43.120
So we have an extensive collection still, but a lot of them are, say, rappers or extensions

12:43.120 --> 12:50.240
around these other frameworks. More recently, I think we've been expanding into, we have a number

12:50.240 --> 12:54.880
of new models, say, at the frontier and material science. People there are just really beginning to

12:54.880 --> 12:59.600
embrace the deep learning mindset. So some newer models there, as I was talking to students this

12:59.600 --> 13:05.840
morning, who's adding a Graph Convolution variant for predicting material structure this morning.

13:06.400 --> 13:11.840
Another frontier, I think, is actually solving partial differential equations. So the PDEs,

13:11.840 --> 13:16.960
as I recall, they're just critical in many fields of engineering and physics. Let's you solve

13:16.960 --> 13:21.600
these complicated systems. So we had a very talented Google Summer of Code student this summer.

13:22.160 --> 13:27.760
Add in a first deep learning PDE solver. It's called the Physics Inspire Neural Network.

13:28.400 --> 13:32.800
It's still very alpha. I think the API needs a lot of work, but it's a great proof of concept.

13:33.360 --> 13:37.680
And the dream of this entire field is, can you solve these high-dimensional PDEs,

13:37.680 --> 13:43.680
I was talking to some engineers at GM a while back. And it takes them, I think, 18 hours to be

13:43.680 --> 13:49.200
able to solve the fluid flow equations around like a car structure. And they can't really do iterative

13:49.200 --> 13:54.320
design. Whereas if you had a fast approximate solver, you could potentially try out these radical

13:54.320 --> 13:59.760
designs. Click a button and have the model give you approximate answer in 30 seconds and then

13:59.760 --> 14:06.000
just keep designing. When these tools mature, I think they'll enable these types of radical

14:06.000 --> 14:10.880
innovations, but that's probably several years off. There's a lot of fundamental work. So

14:10.880 --> 14:16.000
we see our role as continuing to maintain the core, keep it stable for production use,

14:16.000 --> 14:20.240
but also grow out at the frontiers, adding new scientific models and areas.

14:20.240 --> 14:29.680
What's the relationship between DeepCam as a repository and DeepCam as an API? When describing DeepCam,

14:29.680 --> 14:38.240
you describe primarily this model repository, but those models are built using a DeepCam library.

14:39.120 --> 14:43.920
Can you talk a little bit about the library and the capabilities that enables and

14:43.920 --> 14:47.920
my folks use it as opposed to other lower level libraries?

14:47.920 --> 14:52.400
Oh, yes, that's an excellent question. So the DeepCam library itself is, say,

14:53.200 --> 14:59.440
designed to be one level higher up than, say, the underlying tools like PyTorch or TensorFlow.

14:59.440 --> 15:04.960
So for PyTorch or TensorFlow, you often, say, don't know about scientific file formats.

15:04.960 --> 15:09.280
You don't know about how to featureize scientific data sets in a meaningful fashion.

15:09.280 --> 15:16.000
Molecular data sets come in a hole. The prominent open source package that loads these

15:16.000 --> 15:21.760
file formats is called Bable. That should give you some idea of the diversity of formats that

15:21.760 --> 15:26.320
just kind of rest out there. You need to be able to slurp in all these formats. You need to be able

15:26.320 --> 15:31.520
to transform them into either vectors or graphs that you can use in machine learning.

15:31.520 --> 15:35.520
Oftentimes, there are various scientific transformations. You need to apply maybe

15:35.520 --> 15:40.800
types of normalization. There are metrics that you often use to measure these models that,

15:40.800 --> 15:46.320
again, require you to be scientifically aware rather than using it out of the box metric.

15:46.320 --> 15:51.040
So I think where we see a role is coming in is trying to provide these scientifically aware

15:51.040 --> 15:55.520
layer on top. So our goal really is not to try to reinvent the wheel. If something's in

15:55.520 --> 15:59.280
PyTorch geometric or PyTorch or what have you, we just kind of use that very happily.

16:00.000 --> 16:05.440
And part of our extensive collection of tutorials is walking through how do you use DeepCam

16:05.440 --> 16:10.800
with X other library. But there are just cases where something doesn't make sense for the broader.

16:10.800 --> 16:15.360
There's some, what is relatively niche to the broader world way of processing molecules,

16:15.360 --> 16:19.760
but for us is really interesting. That goes in DeepCam. Because that's part of our core mission

16:19.760 --> 16:24.400
or the same for a material. So it really is like, I think the dividing line is,

16:24.400 --> 16:33.040
this is scientific AI tool. And second, is there, if there is already a high-quality open-source

16:33.040 --> 16:37.680
package and community that maintains this, can we just use them? And if the answer to both of

16:37.680 --> 16:42.160
these questions comes out, then we added to DeepCam. Otherwise, we say, hey, why not add a

16:42.160 --> 16:48.000
tutorial that teaches you how to use DeepCam with this other tool to be able to do this right way?

16:48.000 --> 16:51.600
One of the interesting conversations we've been having on the podcast recently,

16:51.600 --> 17:00.160
primarily in the context of reflections on the past year in different areas within machine learning

17:00.160 --> 17:06.080
is kind of this maturing of the field. For example, in natural language processing,

17:06.720 --> 17:13.760
there are so many models off the shelf that a practitioner can take without having to

17:13.760 --> 17:24.080
create a new architecture, for example. And I'm curious about that the state of the field with

17:24.080 --> 17:30.080
regard to molecular design, it sounds like by virtue of the fact that you are primarily offering

17:30.080 --> 17:37.680
a repository, there's a great opportunity for reuse. But I'm wondering if you can elaborate on

17:37.680 --> 17:47.360
kind of this innovation frontier or the extent to which new problems that people want to solve

17:47.360 --> 17:54.160
are solved best by kind of fundamentally pushing machine learning or taking things off the shelf

17:54.160 --> 18:00.160
and implementing them and applying them to their data. How do you see that evolving now for

18:00.880 --> 18:05.920
DeepCam and the molecular design field? This is a great question and something we've been actively

18:05.920 --> 18:11.840
researching for a few years now. So we have this open source consortium within DeepCam,

18:11.840 --> 18:17.680
like a subgroup that works on a series of models called Converta. So Converta is, as you might

18:17.680 --> 18:26.560
guess, a variant. It is a type of transformer. We trained a model on a number of smile strings,

18:26.560 --> 18:33.360
which is the textual representation of a molecule. And you just kind of use a standard NLP style

18:33.360 --> 18:39.520
technique. So Converta one was intriguing, but not really performant, didn't really match the

18:39.520 --> 18:44.240
standard methods out there. We just very recently at the Alice machine learning from molecules

18:44.240 --> 18:51.520
workshop in December put out Converta 2. Converta 2 is now, I would say comparable with the latest

18:51.520 --> 18:56.560
graph convolutional methods, but it's a lot of work and it gets close to the same results,

18:56.560 --> 19:00.560
so I think we have a ways to go. So all the Converta models, we've actually put them up on the

19:00.560 --> 19:05.680
Hugging Face model hub, so you can download those, I think they've been used by actually a

19:05.680 --> 19:10.560
pretty large community people. This is all in the DeepCam org within Hugging Face.

19:12.240 --> 19:16.880
So we do think this is an area where there's going to be major growth. So far, what I would say is

19:16.880 --> 19:22.480
that we are less mature than, say, for natural language processing, things don't work out at the box.

19:23.600 --> 19:29.280
We hope that when we knock on what get Converta 3 out there, maybe that'll take one more step

19:29.280 --> 19:34.400
towards making this a general purpose technology, but as of now, I think it's a cool research

19:34.400 --> 19:40.720
direction. We spent a lot of time thinking about it. We have a very nice team with kind of researchers

19:41.840 --> 19:46.640
who work with DeepCam and also Revery Labs and UC Berkeley and a couple other places

19:47.360 --> 19:53.920
who've been partnering with us. But for now, we hope to see the Hugging Face style off the shelf

19:53.920 --> 19:59.200
models, but we've not yet gotten that to work, where we were able to recommend that. Whereas the

19:59.200 --> 20:04.080
GraphCon, Lusional Methods are always in this field, the random forest, you know, just sometimes

20:04.080 --> 20:08.800
you pull it off, it just works great. I think we'll get there a few more years.

20:09.440 --> 20:16.160
In addition to DeepCam, you've also worked on a molecule net, which is a data set and a benchmark

20:16.160 --> 20:21.920
suite around molecular design. Can you talk a little bit about that in its origins?

20:21.920 --> 20:26.480
Yep, for sure. So, you know, going back to when we started this DeepCam project,

20:27.120 --> 20:31.920
we put kind of these methods out there, and there are a bunch of new methods starting to come

20:31.920 --> 20:38.160
online. And the question naturally was, you know, how do we compare these methods against each other?

20:38.160 --> 20:44.160
So for a long time, I'd been, you know, inspired really by ImageNet. So I was at Stanford at the time,

20:44.160 --> 20:48.800
I kind of joined, right as the ImageNet paper came out. And I could just see the transformative

20:48.800 --> 20:55.600
impact it had on image processing algorithms. So I'd had this vision of, you know, let's try to

20:55.600 --> 21:00.320
do something similar for molecules. And DeepCam provided a platform. So I partnered with

21:00.320 --> 21:07.360
a kind of very talented other student, Michael. He and I put together the kind of core of molecule

21:07.360 --> 21:13.840
net. So we implemented about, say, 15 algorithms in DeepCam that were prominent in the community

21:13.840 --> 21:20.160
at the time. We added about 15 data sets. We ran that matrix roughly of a lot of experiments

21:20.160 --> 21:25.120
right there. We burned up some of the clusters for a while. And yeah, we tried to put out, I think,

21:25.120 --> 21:30.080
quality benchmarks. And we had recommendations on how do you do a trained valid test split,

21:30.080 --> 21:34.720
and that's chemically aware. And I think it's seen a lot of uptake by the community. I'd say it's

21:34.720 --> 21:38.720
also, you know, it's been a few years, like four years at this point, I think, since we put out

21:38.720 --> 21:42.560
the paper. So starting to show its age a little bit, there's a couple of really cool projects.

21:42.560 --> 21:48.400
There's a therapeutic data commons now that MIT has just put out that extends molecule net with

21:48.400 --> 21:56.000
a number of new new data sets that they've gathered and curated. We are working to extend molecule

21:56.000 --> 22:01.280
net ourselves. We have a successor project that's slowly winding its way out the door and that

22:01.920 --> 22:06.400
knock on wood. Maybe this year we'll get out the door for lucky. But we have integrated

22:06.400 --> 22:12.080
molecule net from the earliest days into DeepCam. So the way a lot of people get access to the

22:12.080 --> 22:18.800
data that they do, you know, input DeepCam and the DeepCam.MoleCulnet.loadX4X is whatever data set.

22:19.440 --> 22:26.320
This is kind of modeled on the psychic learn integrated data sets. So I think that we continue

22:26.320 --> 22:31.040
to support this. We continue to add new data sets. We just added a couple, I think, a couple

22:31.040 --> 22:36.640
months ago. But it's again, it's a long-term effort to try to build out a critical mass of data

22:36.640 --> 22:43.280
that can further the science here. Can you talk a little bit more about developing what you

22:43.280 --> 22:51.440
described as a chemistry-aware validation process? So this is a great question. So oftentimes in

22:53.120 --> 22:57.520
well, at least in the earlier papers on machine learning, you do something like a random split

22:58.160 --> 23:05.360
of your data. And this is mostly fine when you're building kind of toy models. But the downside is

23:05.360 --> 23:10.320
in the real world this often doesn't generalize. To use an example from self-driving car world,

23:10.320 --> 23:16.960
it's really the long tail of a squadron of geese fly across the road. Was that in the training

23:16.960 --> 23:21.760
data? Probably not. But you want to test really on the crazy examples. It's something similar

23:21.760 --> 23:26.880
molecules. You're most interested in predicting the structure of molecules that you haven't seen.

23:26.880 --> 23:32.480
So what you want to do is when doing train valid tests, you often want to do a splitting of the data

23:32.480 --> 23:37.520
where the validation and test sets are drawn from chemical scaffolds, as the term is,

23:37.520 --> 23:41.840
that are far away from the scaffolds that you saw in training. And this gives you a better

23:41.840 --> 23:47.200
estimation of the true generalizability of the model. So one of the things we did in molecules

23:47.200 --> 23:51.040
is probably one of the most used parts of deep chemists. We introduced something called the scaffolds

23:51.040 --> 23:59.120
butter. This wrapped the an existing algorithm by Beemis and Mirko. And that was in the Ardekin

23:59.120 --> 24:05.680
library, put it in a nice usable format for people. So I think this is one of our, you know,

24:05.680 --> 24:10.160
little bit, a little part of the project, but maybe one of the most used pieces actually.

24:11.520 --> 24:14.480
So there was an existing library that allowed you to

24:15.760 --> 24:21.360
compare the scaffolds for different molecules and you were able to use that as part of

24:21.920 --> 24:27.200
creating test transplants. Yep. And I think I would definitely give a shout out to Ardekin,

24:27.200 --> 24:32.080
which I think is probably the foundational open project in chemo and formatics. So they kind

24:32.080 --> 24:37.040
of established the machinery that made it possible for us to build on them. They're

24:38.480 --> 24:42.000
probably one of the most important scientific projects that you may or may not have heard of.

24:42.640 --> 24:45.200
So they're a really cool project. I recommend checking them out.

24:45.200 --> 24:53.360
Nice. Nice. Yeah. As deep learning is maturing, you know, a few years ago,

24:53.360 --> 24:57.680
a lot of scientific papers were, hey, I heard about this machine learning thing. I'm going to

24:57.680 --> 25:03.040
throw it at my problem and see what happens. You know, the field has been maturing quite a bit.

25:04.400 --> 25:10.720
And we've got libraries like yours that are able to aid researchers and practitioners.

25:11.440 --> 25:18.320
I'm wondering if you can talk a little bit about, you know, broadly how you see machine learning

25:18.320 --> 25:23.360
and the traditional sciences and what you see the future is there. How does it evolve?

25:24.080 --> 25:28.720
I would say that, you know, probably the biggest shift is going to be that most scientists will

25:28.720 --> 25:34.480
probably want to do some machine learning coursework as part of their core training.

25:35.120 --> 25:39.840
So I think as these tools become more and more established, it'll be just a core part of the

25:39.840 --> 25:46.960
scientific toolkit that new scientists have to learn. I'd say increasingly, what I see now is that

25:46.960 --> 25:53.280
there's a lot more creativity in terms of like, okay, we've got the basics down. So the paper isn't

25:53.280 --> 25:57.120
just, you know, take off the shelf thing and apply it here and see what happens, which is a lot of fun.

25:57.120 --> 26:02.160
I think you could sometimes get some real surprises that way, but now you know the basics,

26:02.160 --> 26:06.080
people have written about it. So what do you do that's creative? So I think at times you actually

26:06.080 --> 26:14.800
see these very innovative applications. You know, I would say that one of the areas I'm most interested

26:14.800 --> 26:20.000
in right now as a researcher is the application of deep learning to solving partial differential

26:20.000 --> 26:26.080
equations. And these are, you know, foundational mathematical tools, I think from the 1800s or

26:26.080 --> 26:31.280
even earlier, you could argue. And but the challenges have been we've only really been able to solve

26:31.280 --> 26:38.000
them in relatively restricted cases. Like in the 70s, under these class of algorithms called finite

26:38.000 --> 26:43.040
element methods, that this is what really underpins I think a lot of CAD and other modeling tools and

26:43.040 --> 26:48.560
really pushed the field forward. But we might now be seeing the second revolution where through

26:48.560 --> 26:54.480
these deep learning methods. And the big shift I think is that these earlier class of techniques,

26:54.480 --> 26:58.960
you often had to put down what's called a mesh. So let's say I have a complicated car, you need

26:58.960 --> 27:04.160
to basically, you know, drop grid points on your car and your computer model. So you can like model,

27:04.160 --> 27:08.720
you know, the airflow at that point. Whereas with the deep net, you can now do what's called a mesh

27:08.720 --> 27:12.640
free method. You can just say, Oh, well, I'm not going to worry about that. I'll just have my deep net

27:12.640 --> 27:17.360
approximated. I'll give me your data. Now, there's still a lot of caveats here. I think this is a

27:17.360 --> 27:23.520
new technique. But I think that it's one of the more exciting things. And it's because you have to

27:23.520 --> 27:29.040
really understand the math and the physics of these systems in addition to the deep learning.

27:29.040 --> 27:34.960
That's where I think you begin to see real scientific creativity. I think so. I think there's like,

27:34.960 --> 27:39.760
you know, a long tail of like really cool innovations. But you, it requires people to understand

27:39.760 --> 27:44.560
both sides, you know, it's all enough to just be a, you know, numerical analyst or a machine learning

27:44.560 --> 27:49.600
person or, you know, a car designer, you kind of have to have someone in the room who does all,

27:49.600 --> 27:56.080
uh, each of these things are all of these things. So your company deep forest sciences started out as

27:57.120 --> 28:01.680
vehicle for doing some consulting with the drug discovery companies, but you're moving in the

28:01.680 --> 28:07.440
direction of productization. Can you talk a little bit more about your vision for the company

28:07.440 --> 28:13.920
and your offerings? Yeah, for sure. So, uh, basically a few years ago, I started almost my

28:13.920 --> 28:21.520
happenstance consulting for a few friends who were working. I was like, well, there's all these

28:21.520 --> 28:25.040
open tools. You really need me to come in and help you. And they're like, yes, I was like, okay,

28:25.040 --> 28:30.400
sure. Why not? I've just left my previous company. Then over time, I think it's grown into a

28:30.400 --> 28:35.840
realization there are the systematic challenges, I think, in applying AI to these problems.

28:36.480 --> 28:40.880
One of the biggest challenges I think is that in drug discovery at the end of the day, like,

28:41.680 --> 28:46.720
you as a company in response over putting out a medicine. That's what your stockholders are

28:46.720 --> 28:52.720
therefore. That's what the market rewards or punishes you for. So the amount of time you can

28:52.720 --> 28:57.760
afford as an organization to really put into building world quality software, world class

28:57.760 --> 29:04.720
software is limited. And it should be because that's not what your market is. So I think there's

29:04.720 --> 29:10.800
a niche to really build out quality AI software and partner with companies and let them do what they

29:10.800 --> 29:16.480
are the best at, which is trying to find new medicine and, you know, work with us to help solve

29:16.480 --> 29:23.280
the hard AI problems we have. So we have built a system that we call internally Kyron. It builds

29:23.280 --> 29:29.920
on deep chem. And, you know, we continue to support and help grow the deep chem community.

29:30.720 --> 29:35.840
But we've also, I think, made some extensions to it that I think make it more useful for our

29:35.840 --> 29:41.040
partners. And we're able to take this technology and work with a drug discovery company who's

29:41.040 --> 29:47.040
say, got a therapeutic hypothesis to really passionate about. But maybe they don't really have

29:47.040 --> 29:51.600
the expertise in house to really throw the kitchen sink at it and figure out what all these AI

29:51.600 --> 29:55.520
techniques can do. There's just a dizzying area of things you can do these days, especially

29:55.520 --> 30:00.720
to feel this grown. That's where we come in. Like we have kind of the extensive capabilities of

30:00.720 --> 30:05.680
deep chem. We have Kyron built on top of that. We've worked with many different companies at this

30:05.680 --> 30:11.680
point. We bring that expertise. So you can focus on the biology, the hard problems of human medicine,

30:12.400 --> 30:16.880
discovery, and we can focus on the AI and we can help you get where you need to be. So that's

30:16.880 --> 30:21.200
the vision of the company in our shell. Well, I wish you the best of luck with the company and

30:21.200 --> 30:26.240
thanks so much for taking the time to share a little bit about what you've been up to. Any time,

30:26.240 --> 30:54.640
that's my pleasure and I had a lot of fun.

