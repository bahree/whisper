1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,480
I'm your host Sam Charrington.

4
00:00:31,480 --> 00:00:36,720
Today's show continues our TensorFlow Developer Summit series, which I recorded at the event

5
00:00:36,720 --> 00:00:39,960
on the Google campus just a few weeks back.

6
00:00:39,960 --> 00:00:44,160
The summit was great, I met a bunch of Twimble Talk listeners and friends of the show and

7
00:00:44,160 --> 00:00:49,160
got caught up on the new features released as part of the TensorFlow 2.0 Alpha.

8
00:00:49,160 --> 00:00:54,040
This time around, we're joined by Elena Niedu, PhD student at Roma Trey University, who

9
00:00:54,040 --> 00:00:58,720
presented her team's project in Kudice Ratsio at the summit.

10
00:00:58,720 --> 00:01:03,400
In our conversation, Elena provides an overview of the project, which aims to annotate and

11
00:01:03,400 --> 00:01:08,480
transcribe Vatican secret archive documents via machine learning.

12
00:01:08,480 --> 00:01:13,240
We discussed the many challenges associated with transcribing this vast archive of handwritten

13
00:01:13,240 --> 00:01:18,040
documents, including overcoming the high cost of data annotation.

14
00:01:18,040 --> 00:01:23,360
I think you'll agree that her team's approach to that challenge was particularly creative.

15
00:01:23,360 --> 00:01:27,800
If you're interested in TensorFlow in deep learning, you'll probably also be interested

16
00:01:27,800 --> 00:01:31,800
in the awesome swag box that Google gave to summit attendees.

17
00:01:31,800 --> 00:01:37,480
They included the new Coral Edge TPU device and the Spark Fund Edge development board,

18
00:01:37,480 --> 00:01:39,760
as well as some other fun goodies.

19
00:01:39,760 --> 00:01:45,240
And I'm excited to give you an opportunity to win one with our latest giveaway.

20
00:01:45,240 --> 00:01:50,400
To enter, just visit twimmolai.com slash TF giveaway, and let us know what you would

21
00:01:50,400 --> 00:01:53,360
do with this kit if you got your hands on it.

22
00:01:53,360 --> 00:01:57,680
Of course, we owe a huge thanks to the TensorFlow team for helping us bring you this podcast

23
00:01:57,680 --> 00:01:59,480
series and giveaway.

24
00:01:59,480 --> 00:02:03,320
With all the great announcements coming out of the TensorFlow Dev Summit, including the

25
00:02:03,320 --> 00:02:09,280
2.0 Alpha, you should definitely check out the latest and greatest over at TensorFlow.org,

26
00:02:09,280 --> 00:02:13,600
where you can also download and start building with the framework.

27
00:02:13,600 --> 00:02:16,320
And now on to the show.

28
00:02:16,320 --> 00:02:20,000
All right, everyone.

29
00:02:20,000 --> 00:02:22,080
I am here with Elena Nierdou.

30
00:02:22,080 --> 00:02:26,320
Elena is a PhD student at Roma Kray University.

31
00:02:26,320 --> 00:02:31,200
And we're together at the TensorFlow Developer Summit, where she gave a talk yesterday.

32
00:02:31,200 --> 00:02:33,680
Elena, welcome to this week in machine learning and AI.

33
00:02:33,680 --> 00:02:34,680
Thank you, Sam.

34
00:02:34,680 --> 00:02:35,680
Awesome.

35
00:02:35,680 --> 00:02:39,880
I'm really looking forward to diving into this conversation with you.

36
00:02:39,880 --> 00:02:44,880
I had a chance to catch your presentation yesterday, which was about the project you worked

37
00:02:44,880 --> 00:02:45,880
on.

38
00:02:45,880 --> 00:02:46,880
Incodice Ratio.

39
00:02:46,880 --> 00:02:47,880
Yes.

40
00:02:47,880 --> 00:02:48,880
Incodice Ratio.

41
00:02:48,880 --> 00:02:49,880
Incodice Ratio.

42
00:02:49,880 --> 00:02:52,880
My Italian is very rusty.

43
00:02:52,880 --> 00:02:55,880
It doesn't matter.

44
00:02:55,880 --> 00:02:57,520
It's actually late in, so.

45
00:02:57,520 --> 00:02:58,520
Okay.

46
00:02:58,520 --> 00:03:02,560
So my Latin is not existing.

47
00:03:02,560 --> 00:03:03,560
Non-existent at all.

48
00:03:03,560 --> 00:03:08,880
We'll be diving into that, but to kind of get us started, I'd really love to hear about

49
00:03:08,880 --> 00:03:13,200
your background and what got you interested in starting to work in machine learning and

50
00:03:13,200 --> 00:03:14,200
AI.

51
00:03:14,200 --> 00:03:20,680
I began working with machine learning exactly when I started working at the project, because

52
00:03:20,680 --> 00:03:29,600
I have studied computer science at university, but I was more like operations oriented, systems

53
00:03:29,600 --> 00:03:37,040
oriented, and very much I didn't think I would join a PhD or something like that.

54
00:03:37,040 --> 00:03:43,400
But then, when I was searching for thesis for my master's thesis, then this project came

55
00:03:43,400 --> 00:03:44,560
up.

56
00:03:44,560 --> 00:03:49,960
It was very interesting to me, because since it's about applying machine learning to the

57
00:03:49,960 --> 00:03:55,600
context of historical documents, and I had somewhat a background in classical studies

58
00:03:55,600 --> 00:04:02,600
in high school, then it seemed to me like it was the perfect match for me, because it

59
00:04:02,600 --> 00:04:09,600
joined two of my passions, two of my interests, and also it might have been, for me, the very

60
00:04:09,600 --> 00:04:15,760
last chance, or so I thought, to learn something about machine learning, because then I suppose

61
00:04:15,760 --> 00:04:20,880
I was supposed to go work somewhere, and then I just said to me, well, let's give it a

62
00:04:20,880 --> 00:04:21,880
shot.

63
00:04:21,880 --> 00:04:22,880
Okay.

64
00:04:22,880 --> 00:04:23,880
Awesome.

65
00:04:23,880 --> 00:04:24,880
Awesome.

66
00:04:24,880 --> 00:04:31,040
So the project that you're working on, describe the project for us, what's the ultimate

67
00:04:31,040 --> 00:04:32,360
challenge?

68
00:04:32,360 --> 00:04:38,840
So we're used to think of big data as something that happens in the web.

69
00:04:38,840 --> 00:04:44,240
So very large amounts of data we produce every day, it's told everyone of us has heard

70
00:04:44,240 --> 00:04:49,600
at least one time in their life, but something that we sometimes fail to consider is that

71
00:04:49,600 --> 00:04:56,360
there is lots of historical precious information that is stored in physical archives, like

72
00:04:56,360 --> 00:04:59,640
in literal paper and something like that.

73
00:04:59,640 --> 00:05:03,960
And in Rome and in the whole of Italy and the whole of the world really, there is lots

74
00:05:03,960 --> 00:05:10,920
of huge archives that mostly have are still undiscovered because of their sites and because

75
00:05:10,920 --> 00:05:11,920
they're physical.

76
00:05:11,920 --> 00:05:13,640
They're not on a web.

77
00:05:13,640 --> 00:05:19,680
And so the goal of the Encoli-Shirazio project is to make these archives accessible, and

78
00:05:19,680 --> 00:05:25,240
in particular the Vatican, we're working on manuscripts inside of a Vatican secret archive.

79
00:05:25,240 --> 00:05:26,240
Now, what?

80
00:05:26,240 --> 00:05:30,960
I feel like if it's called the secret archive, I should be working on it.

81
00:05:30,960 --> 00:05:38,000
No, this is, I'm glad you're saying this because this is a common misconception even with

82
00:05:38,000 --> 00:05:44,760
Italians, because the secret in Vatican secret archive does not stand for confidential, but

83
00:05:44,760 --> 00:05:52,040
it comes from the late in origin of the word, secretum, which does not mean confidential,

84
00:05:52,040 --> 00:05:57,960
but the brother means private or separate, because the Vatican secret archive is a private

85
00:05:57,960 --> 00:05:59,640
property of the Pope.

86
00:05:59,640 --> 00:06:00,640
Okay.

87
00:06:00,640 --> 00:06:06,400
So each Pope inherits the archive, that is the center of all the administration, of all

88
00:06:06,400 --> 00:06:09,800
the archived administration of the church.

89
00:06:09,800 --> 00:06:16,120
So the challenge is to do what with the documents in this archive?

90
00:06:16,120 --> 00:06:21,920
Well, the leader of the project, Professor Marie Aldo, who is also my PhD advisor, he's

91
00:06:21,920 --> 00:06:26,400
an expert in knowledge extraction from web sources.

92
00:06:26,400 --> 00:06:30,360
And so the project precisely started because he was wondering whether we could apply the

93
00:06:30,360 --> 00:06:34,360
same techniques to the domain of historical documents.

94
00:06:34,360 --> 00:06:40,720
And so, yeah, the goal would be to build a knowledge base, a historical knowledge base

95
00:06:40,720 --> 00:06:47,080
out of the documents, but the very first challenge you're facing when you want to do that is

96
00:06:47,080 --> 00:06:54,640
that most of these documents aren't digitized, and if they are digitized, their digitized

97
00:06:54,640 --> 00:06:57,560
images, their scans.

98
00:06:57,560 --> 00:07:06,000
And for those that are printed, you can use traditional OCR and then it usually just works.

99
00:07:06,000 --> 00:07:11,160
But then there is more ancient documents that pre-exist the invention of the printing

100
00:07:11,160 --> 00:07:16,480
press, for example, and they are by necessity handwritten.

101
00:07:16,480 --> 00:07:25,040
And so that's where the classical OCR fails, and you have to think of something more like

102
00:07:25,040 --> 00:07:32,600
more smarter, I guess, or it's a greater challenge than simple OCR.

103
00:07:32,600 --> 00:07:38,800
And so was this project already in existence and ongoing when you joined it or?

104
00:07:38,800 --> 00:07:46,120
It was, it was starting, but there was, I was actually dragged into the project because

105
00:07:46,120 --> 00:07:51,640
one friend of mine had already done his master thesis, and he was the very first person to

106
00:07:51,640 --> 00:07:57,800
begin the code base of Encadyceratio, Chalandrea.

107
00:07:57,800 --> 00:08:02,400
And he told me, look, this project is great, you should join it.

108
00:08:02,400 --> 00:08:11,240
And so it was just starting, and then I followed up on his work, but yeah, it was about

109
00:08:11,240 --> 00:08:17,520
two years ago now, and the project had started for like six months or so.

110
00:08:17,520 --> 00:08:24,560
And so to provide some context on the size of the secret, but not secret archive, I've

111
00:08:24,560 --> 00:08:29,480
seen a couple of comparisons, one sort of Panama Canal, the other to Mount Everest.

112
00:08:29,480 --> 00:08:30,480
Yeah.

113
00:08:30,480 --> 00:08:31,920
Who, what are those?

114
00:08:31,920 --> 00:08:39,600
Well, that's, that's to give a hint of the scale, and that's like Professor Mallorino

115
00:08:39,600 --> 00:08:41,480
who works at the Vatican Secret Archive.

116
00:08:41,480 --> 00:08:47,200
He likes to say that if we were, if we were to take each shelving of a Vatican secret archive

117
00:08:47,200 --> 00:08:55,080
and put it one next to the other, we could cover the, the length of the Panama Canal.

118
00:08:55,080 --> 00:08:59,480
It's 85 kilometers of linear shelving.

119
00:08:59,480 --> 00:09:04,280
So it's a very large collection, and it's a very, very varied collection, like you have

120
00:09:04,280 --> 00:09:11,120
documents from, from China, from, and Asia in general, from, of course, from, he, from

121
00:09:11,120 --> 00:09:16,040
Europe, from the Americas, and Africa, and so on.

122
00:09:16,040 --> 00:09:24,280
And they spend centuries, like there is documents as old as the nymph century.

123
00:09:24,280 --> 00:09:29,040
And up to this day, really, because the, the archive is, is still active, they are still

124
00:09:29,040 --> 00:09:37,840
collecting documents that other institutions inside of the Holy See, don't need anymore.

125
00:09:37,840 --> 00:09:42,280
Because that's how it works, or for example, suppose there is a chance series somewhere

126
00:09:42,280 --> 00:09:45,160
in the world that belongs to the Holy See.

127
00:09:45,160 --> 00:09:50,320
And when they have to archive their files, because they are not using them anymore, they

128
00:09:50,320 --> 00:09:56,280
send that to the, they send them to the Vatican Secret Archive, so not only it's 85 kilometers,

129
00:09:56,280 --> 00:09:57,280
it's growing.

130
00:09:57,280 --> 00:09:58,280
Okay.

131
00:09:58,280 --> 00:10:04,040
Interesting, interesting, and so are you, is this project focused on a particular subset

132
00:10:04,040 --> 00:10:10,480
of the documents, or are you kind of randomly picking them, or, right now we're focusing

133
00:10:10,480 --> 00:10:16,040
on medieval manuscripts, because, which sounds like a pretty broad subset.

134
00:10:16,040 --> 00:10:22,080
Yes, it's still a pre-broad subset, but it allows for some consistency.

135
00:10:22,080 --> 00:10:27,840
And in particular, we're working on a collection that is called the Vatican registers, and the

136
00:10:27,840 --> 00:10:34,800
Vatican registers, they record every letter that the Pope sent in an official way.

137
00:10:34,800 --> 00:10:39,920
So every time the Pope was to send a letter, of course he would write it, and before it

138
00:10:39,920 --> 00:10:45,760
was sent, it was copied inside of the Vatican registers, so that there was a proof that

139
00:10:45,760 --> 00:10:51,640
that letter was sent, and also people could read it if they needed to, because before there

140
00:10:51,640 --> 00:10:59,520
was canon law, what the Pope actually said in letters was the law, act as law itself.

141
00:10:59,520 --> 00:11:05,800
And so it was important to scholars, to law scholars, back then, like in the Middle Ages.

142
00:11:05,800 --> 00:11:13,720
And that's the subset of documents we're working with, but our goal is not just to, not

143
00:11:13,720 --> 00:11:19,600
just to transcribe that, but to create a methodology and to create a way that, so that if tomorrow

144
00:11:19,600 --> 00:11:24,440
we were to change documents, that the documents we're focusing on, we could just reapply the

145
00:11:24,440 --> 00:11:27,240
same techniques and get similar results.

146
00:11:27,240 --> 00:11:31,560
So you said that you're, this project is really your first introduction to machine learning,

147
00:11:31,560 --> 00:11:36,520
and also the project was just getting started as you joined it.

148
00:11:36,520 --> 00:11:43,120
It sounds like a lot to figure out for some of the machine learning.

149
00:11:43,120 --> 00:11:50,960
Maybe can you share a little bit about how you got started and how the project has evolved?

150
00:11:50,960 --> 00:11:58,640
Well, as I mentioned, the project didn't start as necessarily as a machine learning project.

151
00:11:58,640 --> 00:12:03,560
It started as a knowledge discovery project in general.

152
00:12:03,560 --> 00:12:08,080
And in the beginning, transcription wasn't even the main focus.

153
00:12:08,080 --> 00:12:09,080
Okay.

154
00:12:09,080 --> 00:12:14,400
So we thought that since OCR is a solved problem, well, we will just OCR the documents and

155
00:12:14,400 --> 00:12:16,440
then, and then we'll see.

156
00:12:16,440 --> 00:12:19,200
And then we found out that that was just not the case.

157
00:12:19,200 --> 00:12:26,040
And we needed something that was smarter that helped us really transcribe what was there

158
00:12:26,040 --> 00:12:28,920
because it was not trivial at all.

159
00:12:28,920 --> 00:12:35,640
And in the beginning then, so what we did was we went to experts in reading and ancient

160
00:12:35,640 --> 00:12:38,480
writing, they're called polyographers.

161
00:12:38,480 --> 00:12:45,200
And they, we asked them, like, do you think this is possible and they were like, no, no way.

162
00:12:45,200 --> 00:12:51,480
I have studied years to do that, no way I can do that, can do that.

163
00:12:51,480 --> 00:12:58,400
And then the second problem was that we, then so we were searching for how other people

164
00:12:58,400 --> 00:13:02,800
solved our own problem because we didn't think we were the first to, you know, to find

165
00:13:02,800 --> 00:13:05,360
this, this kind of issue.

166
00:13:05,360 --> 00:13:10,880
And what we found out is that many systems that work also very well, but they require a

167
00:13:10,880 --> 00:13:13,920
massive amount of annotations.

168
00:13:13,920 --> 00:13:20,560
So for example, say you want to have a good transcription for a page, then you have to

169
00:13:20,560 --> 00:13:28,360
provide like at least 1,000, 3,000, 10,000 lines carefully annotated.

170
00:13:28,360 --> 00:13:33,400
And since this, this kind of writing can be very hard to read, even when polyographers

171
00:13:33,400 --> 00:13:39,240
know of these tools, they get very discouraged because they say, okay, it's not cost-effective

172
00:13:39,240 --> 00:13:40,240
for me.

173
00:13:40,240 --> 00:13:45,680
Why do I have to give you like 1,000 lines that is months of my work, even years, if it's

174
00:13:45,680 --> 00:13:51,920
very hard to do, just to have a transcription or a particular document, it's not scalable.

175
00:13:51,920 --> 00:13:57,320
So the first thing we found out, it was that the main issue was scalability.

176
00:13:57,320 --> 00:14:01,840
And so that's one of our greatest concerns.

177
00:14:01,840 --> 00:14:09,360
So what is an example of one of these more classical techniques that requires that much

178
00:14:09,360 --> 00:14:10,360
annotation?

179
00:14:10,360 --> 00:14:15,680
And is it the same kind of annotation that we do on a machine learning type of a context?

180
00:14:15,680 --> 00:14:21,040
Yes, but usually you can use unskilled workers to do that.

181
00:14:21,040 --> 00:14:27,360
For example, you put work on Amazon Mechanical Turk, Crowdflower, you name it.

182
00:14:27,360 --> 00:14:31,560
It's usually easy tasks, for example, the one that you're solving when you're doing

183
00:14:31,560 --> 00:14:32,720
captures.

184
00:14:32,720 --> 00:14:37,080
So like mark the boxes that have stop signs in it.

185
00:14:37,080 --> 00:14:44,600
Or mark the boxes that have pedestrian crossing or something like that.

186
00:14:44,600 --> 00:14:49,440
And then it's easy, and everybody can solve it because it's trivial or is there a card

187
00:14:49,440 --> 00:14:50,440
in here?

188
00:14:50,440 --> 00:14:51,440
Yes, no.

189
00:14:51,440 --> 00:14:58,200
But when we're talking these kind of manuscripts, like I said, you need an expert.

190
00:14:58,200 --> 00:15:03,080
An expert are expensive both in the literal sense.

191
00:15:03,080 --> 00:15:06,920
That's time-wise because you need time to get it done.

192
00:15:06,920 --> 00:15:08,640
And there is people who do that.

193
00:15:08,640 --> 00:15:13,280
For example, there is the read project that you allow you.

194
00:15:13,280 --> 00:15:18,440
If you send them in a annotation, they will train their own system and let you have the

195
00:15:18,440 --> 00:15:23,920
transcription for following parts and then iteratively improve on that.

196
00:15:23,920 --> 00:15:31,400
But even when we talked about that, for polyographers, they told us, yes, we know of that system,

197
00:15:31,400 --> 00:15:35,840
but we just don't use it because sometimes it's not worth our time.

198
00:15:35,840 --> 00:15:40,440
We will just do it ourselves if that's the amount of work we're supposed to do.

199
00:15:40,440 --> 00:15:45,600
And I don't mean to say it's not useful because it can be useful, for example, for the

200
00:15:45,600 --> 00:15:49,520
end goal of transcribing entire collections.

201
00:15:49,520 --> 00:15:55,320
But you have to be invested in doing that, and more often than not, maybe historians

202
00:15:55,320 --> 00:16:02,080
are focused on finding a particular occurrence and they don't want all the rest of the hassle.

203
00:16:02,080 --> 00:16:09,360
And so in this project, when I'm imagining one of the big challenges is still annotations

204
00:16:09,360 --> 00:16:15,680
and having some kind of labeled data set, how did you go about that?

205
00:16:15,680 --> 00:16:21,720
I feel it's a challenge for many machine learning practitioners because, of course, when it's

206
00:16:21,720 --> 00:16:31,080
about business data, maybe your industry and you already have that data and it's there

207
00:16:31,080 --> 00:16:32,720
and you just have to use it.

208
00:16:32,720 --> 00:16:37,720
But for many other fields and especially in the humanities, there is no such thing.

209
00:16:37,720 --> 00:16:44,760
And so what we decided to do was not to annotate by line or by word, but to annotate single

210
00:16:44,760 --> 00:16:49,800
characters because, of course, the size of the alphabet is way smaller than the size

211
00:16:49,800 --> 00:16:51,080
of a vocabulary.

212
00:16:51,080 --> 00:16:59,520
And also, it suffers way less of the skew of the distribution.

213
00:16:59,520 --> 00:17:07,380
What I mean by that is that if you consider the whole 100 pages of text, it could be

214
00:17:07,380 --> 00:17:09,320
latent, but it could also be English.

215
00:17:09,320 --> 00:17:15,160
What you will find out is that there is a very few words that occur an enormous amount

216
00:17:15,160 --> 00:17:18,240
of times.

217
00:17:18,240 --> 00:17:23,880
And then most of the words, they just occur like 10 to 30 times.

218
00:17:23,880 --> 00:17:29,920
And they are the same words you would be more interested in recognizing because maybe

219
00:17:29,920 --> 00:17:34,360
there are verbs, they are names and so on.

220
00:17:34,360 --> 00:17:40,080
Like you don't care about conjunctions or V and so on.

221
00:17:40,080 --> 00:17:44,280
And that is why the systems require lots of data.

222
00:17:44,280 --> 00:17:50,440
Because you need a good amount of each relevant example.

223
00:17:50,440 --> 00:17:55,400
But letters on other hand are more distributed.

224
00:17:55,400 --> 00:18:01,360
Of course, for example, vowel in Italian and Latin as well, vowels will occur more frequently

225
00:18:01,360 --> 00:18:02,920
than consonants.

226
00:18:02,920 --> 00:18:05,920
But still, it's something you can get over with.

227
00:18:05,920 --> 00:18:14,120
And also that enabled us to create a system that was very much like the Capture Solving

228
00:18:14,120 --> 00:18:15,640
system.

229
00:18:15,640 --> 00:18:22,520
So we made our own custom crowdsourcing platform where the workers were shown an image

230
00:18:22,520 --> 00:18:29,480
of a word and then on the upper side they were shown some positive examples of symbols

231
00:18:29,480 --> 00:18:32,560
that they were supposed to recognize inside of that word.

232
00:18:32,560 --> 00:18:38,360
Without even having knowledge of the symbol they were shown for.

233
00:18:38,360 --> 00:18:42,680
And then they were asked to mark by clicking inside of the image.

234
00:18:42,680 --> 00:18:46,560
The areas that corresponded to the example they were provided.

235
00:18:46,560 --> 00:18:52,800
And one question that I had was were they annotating on a letter by letter basis or stroke

236
00:18:52,800 --> 00:18:54,720
or subset of a letter basis?

237
00:18:54,720 --> 00:18:57,960
I thought I saw something that suggested it was done at the stroke level and then you

238
00:18:57,960 --> 00:18:59,880
were combining strokes to form letters.

239
00:18:59,880 --> 00:19:06,400
Yes, of course, since we couldn't know before which parts of the word would correspond

240
00:19:06,400 --> 00:19:14,080
to which letter because otherwise we would already have transcription by definition.

241
00:19:14,080 --> 00:19:18,800
Then we use a system that can over segment a word in its strokes.

242
00:19:18,800 --> 00:19:25,600
So for example, maybe you have an M and the system was segmented into a free sticks.

243
00:19:25,600 --> 00:19:32,360
And so if workers were asked to mark an M they would just click on the free sticks and

244
00:19:32,360 --> 00:19:34,320
then submit their answer.

245
00:19:34,320 --> 00:19:40,360
A bit like what you do when you have an image segmented into small squares and you click

246
00:19:40,360 --> 00:19:41,680
on the squares.

247
00:19:41,680 --> 00:19:44,720
Pretty much the same principle, a bit more fine grade.

248
00:19:44,720 --> 00:19:50,800
And so in this case you created this labeling interface and as opposed to using our mechanical

249
00:19:50,800 --> 00:19:56,480
turkeys you use what resource.

250
00:19:56,480 --> 00:20:02,920
So in Italy it's not that easy to access to mechanical turk or crowdsourcing platform

251
00:20:02,920 --> 00:20:05,200
in general.

252
00:20:05,200 --> 00:20:11,520
And also high school students have to work a certain amount of time in the last three

253
00:20:11,520 --> 00:20:15,240
years of their high school.

254
00:20:15,240 --> 00:20:21,000
And so they join projects where they are supposed to do works that are related to their kind

255
00:20:21,000 --> 00:20:22,000
of studies.

256
00:20:22,000 --> 00:20:29,400
For example, high school students like I was myself in classical studies, maybe they

257
00:20:29,400 --> 00:20:35,800
will go to museum and help tourists or something like that or those that are more science-oriented.

258
00:20:35,800 --> 00:20:41,120
They might go somewhere else or to laboratories or industry, something like that.

259
00:20:41,120 --> 00:20:48,600
And so thanks to high school teacher Marika Shone, we were able to present our project,

260
00:20:48,600 --> 00:20:54,480
our crowdsourcing platform as a working platform for high school students.

261
00:20:54,480 --> 00:21:01,440
And also they were given a few lessons about image processing, machine learning and also

262
00:21:01,440 --> 00:21:03,040
polyography.

263
00:21:03,040 --> 00:21:05,160
And they were able to visit the university.

264
00:21:05,160 --> 00:21:09,160
Sometimes they worked for us at the university at our labs.

265
00:21:09,160 --> 00:21:15,760
And some but the good thing about that was that they could also work from home.

266
00:21:15,760 --> 00:21:20,880
And so they were very flexible on when and how to work and that was an advantage that

267
00:21:20,880 --> 00:21:27,240
also the professors, their teachers liked because sometimes to do the work part you have

268
00:21:27,240 --> 00:21:29,040
to skip the school part.

269
00:21:29,040 --> 00:21:32,880
And so of course teachers are not very happy about that.

270
00:21:32,880 --> 00:21:38,880
And in that way they could like manage their own time and see a different way of working

271
00:21:38,880 --> 00:21:39,880
in the morning.

272
00:21:39,880 --> 00:21:46,560
So you have these crowdsourced annotations that you collected from the high school students.

273
00:21:46,560 --> 00:21:52,320
How many or how was the volume of annotations?

274
00:21:52,320 --> 00:21:59,960
The annotations there are more than 40,000 annotations over about 30 classes.

275
00:21:59,960 --> 00:22:00,960
30 classes.

276
00:22:00,960 --> 00:22:03,360
Okay, so the 30 classes are letters approximately.

277
00:22:03,360 --> 00:22:04,360
Yes.

278
00:22:04,360 --> 00:22:07,200
And so you 40,000 letter examples of letters.

279
00:22:07,200 --> 00:22:08,200
Yes.

280
00:22:08,200 --> 00:22:13,160
Not by character to each single, each we have about depending on, of course, depending

281
00:22:13,160 --> 00:22:23,160
on distribution of the characters, but we have about 1,000, 1,500 examples per character.

282
00:22:23,160 --> 00:22:26,000
So that's where the 40,000 comes from.

283
00:22:26,000 --> 00:22:31,560
Often when you're working with crowdsourced data, you'll do things like you'll have

284
00:22:31,560 --> 00:22:37,600
multiple workers annotate the same thing and do like forums or things like that.

285
00:22:37,600 --> 00:22:41,680
Did you do anything like that or was there a higher level of trust that you established

286
00:22:41,680 --> 00:22:45,240
with this group that allowed you to skip that kind of thing?

287
00:22:45,240 --> 00:22:51,720
We experimented with different levels of like we just had a threshold.

288
00:22:51,720 --> 00:22:59,120
So for example, we considered good things that were voted more than three times or five

289
00:22:59,120 --> 00:23:00,120
times.

290
00:23:00,120 --> 00:23:05,920
We experimented with different thresholds and or for example, we realized that some

291
00:23:05,920 --> 00:23:10,360
characters were more challenging than others and we didn't want the students to spend too

292
00:23:10,360 --> 00:23:15,720
much time on two easy characters, but just or maybe there were characters that occurred

293
00:23:15,720 --> 00:23:17,120
less.

294
00:23:17,120 --> 00:23:23,240
And so in the case of character that occurred less, maybe the threshold was lower or characters

295
00:23:23,240 --> 00:23:28,400
that were very easy that we saw the student didn't get them wrong so often.

296
00:23:28,400 --> 00:23:33,480
And in some other cases, we it was it was not as clear.

297
00:23:33,480 --> 00:23:38,040
It was it was harder for them to to find them instead of a word and so on and so that

298
00:23:38,040 --> 00:23:39,960
threshold was higher.

299
00:23:39,960 --> 00:23:44,360
And so you collected this training data presumably to train some kind of model.

300
00:23:44,360 --> 00:23:49,960
What kind of model did you train and how did you arrive at that model?

301
00:23:49,960 --> 00:23:56,000
So we in the end settled for a convolutional neural network, which is pretty standard considering

302
00:23:56,000 --> 00:24:02,920
image processing tasks, but we wanted to like build incrementally because we didn't

303
00:24:02,920 --> 00:24:07,720
want to like we could have just downloaded one one big model.

304
00:24:07,720 --> 00:24:14,480
But we knew our dataset wasn't that big and it wasn't as hard as image net, like recognizing

305
00:24:14,480 --> 00:24:20,400
single characters is more like NIST than this image net.

306
00:24:20,400 --> 00:24:23,720
But at the same time, we wanted something that really fitted our data.

307
00:24:23,720 --> 00:24:29,640
So we just started simple, like very basic logistic regression.

308
00:24:29,640 --> 00:24:36,200
And then it improved from that to see at each step how better we were doing and why.

309
00:24:36,200 --> 00:24:42,000
And so you say at each step, meaning at each kind of iteration of the modeling step or?

310
00:24:42,000 --> 00:24:48,840
Yes, we would go about a cycle of, for example, choosing, first of all, choosing a type

311
00:24:48,840 --> 00:24:53,560
of a network of architecture in general.

312
00:24:53,560 --> 00:25:00,080
And then seeing, seeing just as it was, seeing how it, how it did.

313
00:25:00,080 --> 00:25:05,240
And then, for example, trying to change hyper parameters inside of that model.

314
00:25:05,240 --> 00:25:08,600
And then we would try to make it more complex.

315
00:25:08,600 --> 00:25:15,760
So for example, if it was a fit forward network, then we would just start easy one true layers.

316
00:25:15,760 --> 00:25:19,320
And then, and then tune the hyper parameter inside of those.

317
00:25:19,320 --> 00:25:24,880
And then when we were sure that we wouldn't get, we couldn't get better.

318
00:25:24,880 --> 00:25:29,320
We would choose the best configuration and then maybe add the layer, remove a layer and

319
00:25:29,320 --> 00:25:36,800
see, for example, because there is a, like, at some point, you won't improve anymore

320
00:25:36,800 --> 00:25:39,040
because the model has limitations.

321
00:25:39,040 --> 00:25:46,560
And so we did that for a certain number of times, so starting with the fit forward and

322
00:25:46,560 --> 00:25:49,800
then moving to a convolutional.

323
00:25:49,800 --> 00:25:54,360
So we're here at the TensorFlow event, presumably using TensorFlow to develop this.

324
00:25:54,360 --> 00:25:56,000
What was that experience like?

325
00:25:56,000 --> 00:26:04,840
Well, it could be challenging because there were like, there were so many things, so many

326
00:26:04,840 --> 00:26:07,760
ways you could do the same thing.

327
00:26:07,760 --> 00:26:11,280
And no one was necessarily better than the other.

328
00:26:11,280 --> 00:26:17,040
And for a beginner that can be, like, a bit scary, but of course, I wasn't alone because

329
00:26:17,040 --> 00:26:22,200
I was a beginner, but there was somebody who was not that also directed the whole process

330
00:26:22,200 --> 00:26:27,840
of building the network that was Simones Cardapani, who is also a Google developer expert

331
00:26:27,840 --> 00:26:30,320
in machine learning.

332
00:26:30,320 --> 00:26:35,600
And so we were together in this and I was guided, like, I didn't do it all by myself.

333
00:26:35,600 --> 00:26:40,120
Of course, I did my fair share of studying and of working on it.

334
00:26:40,120 --> 00:26:42,880
So which APIs did you use for?

335
00:26:42,880 --> 00:26:51,120
OK, so the thing is, for example, we were very torn apart between Keras and PureTensor

336
00:26:51,120 --> 00:26:52,120
Flow.

337
00:26:52,120 --> 00:27:00,920
Because we loved Keras for prototyping because it kept the code small and very readable,

338
00:27:00,920 --> 00:27:08,120
understandable, much object-like, something you would like, you're familiar with, maybe,

339
00:27:08,120 --> 00:27:11,560
more familiar than other approaches.

340
00:27:11,560 --> 00:27:17,280
And but then at some point, for example, maybe TensorFlow had a new feature and Keras had

341
00:27:17,280 --> 00:27:22,560
to implement it inside of it, and it was constantly lagging behind.

342
00:27:22,560 --> 00:27:31,040
And so, well, before the integration, at least, or maybe they said, we're now supporting

343
00:27:31,040 --> 00:27:32,320
a tensor board.

344
00:27:32,320 --> 00:27:37,120
And then, for example, it was true, but it was true only for a very restricted set of

345
00:27:37,120 --> 00:27:39,280
operations.

346
00:27:39,280 --> 00:27:45,800
And so since we sort of wanted to have it all.

347
00:27:45,800 --> 00:27:50,280
And so maybe sometimes we would just, for example, prototype fast in Keras.

348
00:27:50,280 --> 00:27:54,640
And then when we were pretty sure about the model that it wouldn't change much anymore,

349
00:27:54,640 --> 00:27:56,440
we would move to PureTensor Flow.

350
00:27:56,440 --> 00:27:57,440
OK.

351
00:27:57,440 --> 00:28:02,960
And was that was that process difficult or just tedious?

352
00:28:02,960 --> 00:28:08,680
Well, at some point, TensorFlow introduces the high-level API layers.

353
00:28:08,680 --> 00:28:14,880
And so it was pretty much straightforward because it was one-to-one, like syntax change

354
00:28:14,880 --> 00:28:17,920
a little bit, but not so much.

355
00:28:17,920 --> 00:28:25,800
And but of course, it was a hassle, like you constantly have to do back-and-forth.

356
00:28:25,800 --> 00:28:32,800
And yeah, so the last model we trained, the one that we are using right now, but we're

357
00:28:32,800 --> 00:28:33,800
trying to improve on.

358
00:28:33,800 --> 00:28:39,400
But so far, it was implemented in layers and estimators.

359
00:28:39,400 --> 00:28:44,320
It was actually implemented in layers estimators so that we could take full advantage of tensor

360
00:28:44,320 --> 00:28:45,320
board.

361
00:28:45,320 --> 00:28:52,600
And for example, at some point, we wanted to try the deep-dream technique to generate

362
00:28:52,600 --> 00:28:57,920
images at each layer of the network to understand what was learned.

363
00:28:57,920 --> 00:29:01,040
And that would, we just couldn't do that in Keras.

364
00:29:01,040 --> 00:29:03,920
So they hid too much of the internals from you.

365
00:29:03,920 --> 00:29:11,440
And so this model, you've got this CNN that's basically classifying these characters within

366
00:29:11,440 --> 00:29:15,360
the words, within these documents.

367
00:29:15,360 --> 00:29:19,080
What's the broader kind of pipeline that that fits into?

368
00:29:19,080 --> 00:29:20,080
OK.

369
00:29:20,080 --> 00:29:26,600
Of course, since we have this, we know we recognize that recognizing character is a limitation

370
00:29:26,600 --> 00:29:30,160
because of course, then you have to provide the network with characters.

371
00:29:30,160 --> 00:29:36,800
So there is a whole image processing step before the actual classification so that we can

372
00:29:36,800 --> 00:29:39,680
provide the network with actual characters.

373
00:29:39,680 --> 00:29:41,880
Or maybe not, because that's the point.

374
00:29:41,880 --> 00:29:45,960
When you get to a word, it's pretty easy to segment into words, then you get to a word.

375
00:29:45,960 --> 00:29:49,080
And then everything is written all together.

376
00:29:49,080 --> 00:29:51,760
It's very like its cursive script.

377
00:29:51,760 --> 00:29:56,800
So each word starts when each character starts when the other ends.

378
00:29:56,800 --> 00:30:01,680
And it can be very tricky to understand where character starts and where character ends.

379
00:30:01,680 --> 00:30:10,160
So we have a few old-school computer visual heuristics that help us cut these whole words

380
00:30:10,160 --> 00:30:17,600
into sub-shapes, or as if they were puzzle pieces in a way.

381
00:30:17,600 --> 00:30:21,440
And then we combine these puzzle pieces together.

382
00:30:21,440 --> 00:30:27,280
We make the network classify them in isolation and in combination.

383
00:30:27,280 --> 00:30:36,080
So the network has some confidence on those classification and can also have a special

384
00:30:36,080 --> 00:30:41,320
class that's supposed to represent our wrong segmentation.

385
00:30:41,320 --> 00:30:47,160
And so the network can both tell you, according to my knowledge, if this is an A or it can

386
00:30:47,160 --> 00:30:51,120
tell you, no, that's not a character, drop it.

387
00:30:51,120 --> 00:30:57,720
So that helps us prune the whole space of the possibility which otherwise would be very

388
00:30:57,720 --> 00:31:03,200
large, because we're constantly combining adjacent pieces of the puzzle that are parts

389
00:31:03,200 --> 00:31:04,960
of the word.

390
00:31:04,960 --> 00:31:11,360
Is it like the segmentation is kind of like a sliding window kind of thing, or is it more

391
00:31:11,360 --> 00:31:17,800
the algorithm is producing distinct segments, but it's noisy?

392
00:31:17,800 --> 00:31:21,240
Yeah, more like the second one.

393
00:31:21,240 --> 00:31:26,120
In the beginning we used a sliding window, but it produced way too much noise.

394
00:31:26,120 --> 00:31:31,760
And then you're very dependent on, for example, the stride you're moving at.

395
00:31:31,760 --> 00:31:39,040
And you may have noise from other characters in the background, in the sound of the sides.

396
00:31:39,040 --> 00:31:49,080
So we have this series that basically computes the higher contour and the lower contour of

397
00:31:49,080 --> 00:31:51,080
the characters.

398
00:31:51,080 --> 00:31:57,240
And wherever there is a local minimum or a local maximum, because that's how the hand

399
00:31:57,240 --> 00:31:58,720
writing goes.

400
00:31:58,720 --> 00:31:59,720
That's interesting.

401
00:31:59,720 --> 00:32:05,800
And then we're able to connect them and we create strokes from the characters.

402
00:32:05,800 --> 00:32:12,240
So each segment could be either an entire character or a part of a character.

403
00:32:12,240 --> 00:32:17,600
So then you have these, you kind of pass through the first step in this pipeline is passing

404
00:32:17,600 --> 00:32:19,000
through the segmentation step.

405
00:32:19,000 --> 00:32:20,000
Yes.

406
00:32:20,000 --> 00:32:25,640
And then for each of these segments, you're passing it on to your CNN to tell you, what

407
00:32:25,640 --> 00:32:29,120
are the 30 characters, is it, or is it not a character?

408
00:32:29,120 --> 00:32:30,120
Exactly.

409
00:32:30,120 --> 00:32:37,000
And then at this point, you still may have some ambiguity, because for example, there

410
00:32:37,000 --> 00:32:45,160
is things you might not be able to solve, consider, oh well, this looks so much better if

411
00:32:45,160 --> 00:32:53,920
I can draw, but okay, consider when you have, for example, one word we often use as

412
00:32:53,920 --> 00:33:00,280
an example, let's try and make this, this, this imagination challenge, okay, it's unknown

413
00:33:00,280 --> 00:33:12,120
A and N, O, now in this hand writing, the eyes have no dots on them, okay?

414
00:33:12,120 --> 00:33:17,760
So even though, so you, you broke unknown into pieces with our segmentation.

415
00:33:17,760 --> 00:33:22,760
And so probably what you will get is A and then a series of sticks, because they could

416
00:33:22,760 --> 00:33:31,080
either be eyes or ends or amps, and then you get the O, but then again, for example, each

417
00:33:31,080 --> 00:33:38,760
of the sticks of the ends could be either eyes, they could be all from the, from the perspective

418
00:33:38,760 --> 00:33:45,640
of a character recognition without having further context, they could either be eyes, they

419
00:33:45,640 --> 00:33:50,680
could be ends, they could be amps combined with eyes.

420
00:33:50,680 --> 00:33:56,600
And all of these combinations are valid from the point of view of the optical recognition

421
00:33:56,600 --> 00:34:02,480
of the visual features, but then again, of course, only one of them is correct, which is

422
00:34:02,480 --> 00:34:08,920
the double N. And so to know that, you need more context and that context is the knowledge

423
00:34:08,920 --> 00:34:17,000
of the leading language, so that is why even though we have classification, then we need

424
00:34:17,000 --> 00:34:22,640
to rank, we could have ambiguity on some words. And so we have to rank them, and we rank

425
00:34:22,640 --> 00:34:26,720
them according to a language model we built out of the latent text.

426
00:34:26,720 --> 00:34:27,720
Okay.

427
00:34:27,720 --> 00:34:33,080
So they, of course, language model picks the one that sounds more latent-ish.

428
00:34:33,080 --> 00:34:39,200
From your classifier, you produce kind of a list of the top N candidates, and then you

429
00:34:39,200 --> 00:34:45,960
pass that to your language model to basically re-order, re-rank them based on the semantics

430
00:34:45,960 --> 00:34:47,640
or the context of the language.

431
00:34:47,640 --> 00:34:48,640
Exactly.

432
00:34:48,640 --> 00:34:53,160
And then that gives you your transcription, your transcribed word.

433
00:34:53,160 --> 00:34:54,160
Yes.

434
00:34:54,160 --> 00:34:58,920
It gives us, well, it gives us a ranking of transcriptions, and so, yeah, the metrics

435
00:34:58,920 --> 00:35:05,720
we were using are not just correct or not correct, but also how good were they ranked?

436
00:35:05,720 --> 00:35:10,960
On that note, how do you kind of evaluate the performance of the network?

437
00:35:10,960 --> 00:35:16,320
So of course, we evaluated the performance of the network on the single character recognition,

438
00:35:16,320 --> 00:35:22,240
but also we evaluated the performance of the pipeline as a whole on a word-at-word level.

439
00:35:22,240 --> 00:35:32,640
So the network by itself has 94% average accuracy over 33 classes, and then, of course, since

440
00:35:32,640 --> 00:35:39,280
there is ambiguity or maybe sometimes the segmentation can go wrong, or since it's a very

441
00:35:39,280 --> 00:35:45,480
pipeline process at each step you could introduce error, or also the network might be wrong.

442
00:35:45,480 --> 00:35:49,720
If the segmentation were perfect, so for example, if we didn't have that kind of ambiguity,

443
00:35:49,720 --> 00:35:59,720
we could get up to 80%, 85% accuracy, and what we actually have is about 65% perfect transcription,

444
00:35:59,720 --> 00:36:03,600
and then we get up to 80% if we consider minor spelling errors.

445
00:36:03,600 --> 00:36:04,600
Okay.

446
00:36:04,600 --> 00:36:12,160
For example, maybe E was mistaken for a C, or sometimes since convolutional neural networks

447
00:36:12,160 --> 00:36:20,160
are terrible at counting, you have like a double C or a double S, and so double letters,

448
00:36:20,160 --> 00:36:24,960
which occur pretty often, they might be conflated into one single error.

449
00:36:24,960 --> 00:36:31,280
So it sees like two S, and then it just say one S, because it's twice the same symbol,

450
00:36:31,280 --> 00:36:37,000
but it's just, you know, it's when you have an image of, you want the classic network

451
00:36:37,000 --> 00:36:43,360
that tells you cat or dog, and it doesn't matter how many cats or dogs are in the picture.

452
00:36:43,360 --> 00:36:46,160
It will not tell you it's not a cat if there is two cats.

453
00:36:46,160 --> 00:36:52,960
So the language model is only really ranking the candidates that you give it.

454
00:36:52,960 --> 00:36:58,920
It's not doing anything like, it's not like an embedding that's looking at possible

455
00:36:58,920 --> 00:37:04,160
words around the ones that you give it that would be more likely, so as to like correct

456
00:37:04,160 --> 00:37:07,520
for these single letter spelling errors.

457
00:37:07,520 --> 00:37:08,520
Okay.

458
00:37:08,520 --> 00:37:18,040
So far, we experimented with error correction, but only in the form of Viterbi, like

459
00:37:18,040 --> 00:37:19,040
of what?

460
00:37:19,040 --> 00:37:21,840
And I hit the mark of models, something like that.

461
00:37:21,840 --> 00:37:28,200
And so using the Viterbi algorithm, but the problem we have with that is that sometimes

462
00:37:28,200 --> 00:37:33,040
it produces like, it produces more candidate transcription and more noise.

463
00:37:33,040 --> 00:37:34,040
Okay.

464
00:37:34,040 --> 00:37:42,880
And so maybe we can get a few words corrected, but then we greatly, like our ranking gets

465
00:37:42,880 --> 00:37:47,520
worse because there is so many things, there is more things that it has to consider.

466
00:37:47,520 --> 00:37:53,160
And the more alternatives you produce, the more likely it is, they sound like latent.

467
00:37:53,160 --> 00:37:57,600
And so that language model would put them like on top even though maybe they weren't

468
00:37:57,600 --> 00:37:58,600
correct.

469
00:37:58,600 --> 00:37:59,600
Okay.

470
00:37:59,600 --> 00:38:06,480
So, you mentioned that the character classifier is performing it like 94%, the pipeline

471
00:38:06,480 --> 00:38:13,160
at 64 or 80 depending on whether you're counting these spelling errors.

472
00:38:13,160 --> 00:38:18,680
How does that compare to the payliographers and their performance?

473
00:38:18,680 --> 00:38:26,360
Well, of course, you would have the ground truth we're testing against was written by

474
00:38:26,360 --> 00:38:27,360
callographers.

475
00:38:27,360 --> 00:38:36,640
So you have to assume that, like, they are 100%, that's a limit we do not expect, of

476
00:38:36,640 --> 00:38:37,960
course, to do better.

477
00:38:37,960 --> 00:38:45,120
Even though sometimes it actually occurred that maybe the polygrapher has made up very

478
00:38:45,120 --> 00:38:50,400
small spelling error, especially when they were dealing with many sticks, because suppose

479
00:38:50,400 --> 00:38:57,920
you have, like, UM and I, you and et cetera, that's very, very common sequences that can

480
00:38:57,920 --> 00:39:05,480
occur, considering like numerals, it's like, it's all sticks and sometimes they get the

481
00:39:05,480 --> 00:39:06,480
wrong count.

482
00:39:06,480 --> 00:39:07,480
Okay.

483
00:39:07,480 --> 00:39:14,280
And so they also may get wrong and sometimes it happens, but it's like, I guess they get

484
00:39:14,280 --> 00:39:22,440
5% wrong maybe, at most, maybe to kind of wrap things up, what do you plan for the future

485
00:39:22,440 --> 00:39:25,000
with this project, what are the next steps?

486
00:39:25,000 --> 00:39:26,000
Okay.

487
00:39:26,000 --> 00:39:33,600
So, of course, we have collected some more data, since then, but always at character level,

488
00:39:33,600 --> 00:39:40,600
just it changed the way we did it, so that it enabled us to keep, to be aware of the context

489
00:39:40,600 --> 00:39:42,640
characters we're put in.

490
00:39:42,640 --> 00:39:51,520
And so one very first step I would like to go into is trying to create a system that both

491
00:39:51,520 --> 00:39:55,000
performs segmentation and recognition.

492
00:39:55,000 --> 00:40:02,160
So we have done recently a few tests and it turns out that this part of, you know, handmade

493
00:40:02,160 --> 00:40:05,680
segmentation is one of the major bottlenecks we have.

494
00:40:05,680 --> 00:40:12,560
So we, so right now we depend highly on how much good, how good we can do.

495
00:40:12,560 --> 00:40:15,120
With the segmentation.

496
00:40:15,120 --> 00:40:21,120
And so maybe to train a segmentator could be interesting and see, and see what happens,

497
00:40:21,120 --> 00:40:24,800
maybe it doesn't work, but it's worth a try.

498
00:40:24,800 --> 00:40:33,480
And then, of course, our end goal is to move to sequence models, because there is things

499
00:40:33,480 --> 00:40:36,240
that otherwise we wouldn't be able to solve.

500
00:40:36,240 --> 00:40:40,080
Like other DMs, RNNs, that kind of thing.

501
00:40:40,080 --> 00:40:47,000
Exactly, or anyways, for example, models that use attention, I don't know, like the captioning

502
00:40:47,000 --> 00:40:48,000
ones.

503
00:40:48,000 --> 00:40:49,000
Okay.

504
00:40:49,000 --> 00:40:53,200
To see the problem of transcription as one of, of, of captioning somehow.

505
00:40:53,200 --> 00:40:54,200
Okay.

506
00:40:54,200 --> 00:40:55,200
Interesting.

507
00:40:55,200 --> 00:40:56,200
Yeah.

508
00:40:56,200 --> 00:40:57,200
Yeah.

509
00:40:57,200 --> 00:41:03,440
And but for that, of course, you, you need the line dataset and, and our, our, our focus,

510
00:41:03,440 --> 00:41:09,240
you know, is on scalability and that's why we would like to try generative models to

511
00:41:09,240 --> 00:41:13,640
help us improve our, the dataset that we have.

512
00:41:13,640 --> 00:41:20,800
So, for example, we are able to reconstruct words from the annotation of the students.

513
00:41:20,800 --> 00:41:25,280
They are a bit noisy, but they still look pretty good, like, they look like words.

514
00:41:25,280 --> 00:41:30,480
And then, if we were able to feed, like, a generative model and to explore latent space

515
00:41:30,480 --> 00:41:36,720
and create slightly different versions of the same words, that's so that we could augment

516
00:41:36,720 --> 00:41:37,720
our dataset.

517
00:41:37,720 --> 00:41:38,720
Okay.

518
00:41:38,720 --> 00:41:41,640
And the character dataset.

519
00:41:41,640 --> 00:41:46,320
Combining, actually combining the character dataset into creating words and then augmenting

520
00:41:46,320 --> 00:41:47,320
those words.

521
00:41:47,320 --> 00:41:48,960
Okay.

522
00:41:48,960 --> 00:41:52,800
And so creating very, very realistic synthetic dataset.

523
00:41:52,800 --> 00:42:02,560
So as, as always, you could get a polyographer to do that, but we want to involve them, just,

524
00:42:02,560 --> 00:42:11,240
what we, like, in the, oh, as always, you could, like, you could ask an expert to do that,

525
00:42:11,240 --> 00:42:17,440
but where our concern is scalability, so we want to be as unsupervised as possible

526
00:42:17,440 --> 00:42:18,440
in this.

527
00:42:18,440 --> 00:42:23,200
It is part of the way this could play out is you build the, you have the model that

528
00:42:23,200 --> 00:42:28,680
does it at the character level and kind of gets good enough at that, that you can generate

529
00:42:28,680 --> 00:42:32,120
some transcriptions and then you can use those to train the line level.

530
00:42:32,120 --> 00:42:33,120
Exactly.

531
00:42:33,120 --> 00:42:34,120
Exactly.

532
00:42:34,120 --> 00:42:35,120
Okay.

533
00:42:35,120 --> 00:42:39,920
Or maybe integrate, so maybe we generate some transcriptions and then so the polyographer

534
00:42:39,920 --> 00:42:44,720
just have to say, okay, this is wrong, but this is right, and then a bit like translate

535
00:42:44,720 --> 00:42:49,040
or do right now with Google Translate, so they give it to Google Translate.

536
00:42:49,040 --> 00:42:56,440
It does, so and so translation, and then they just correct it and they go faster this

537
00:42:56,440 --> 00:42:57,440
way.

538
00:42:57,440 --> 00:43:03,880
Any advice for folks that are kind of embarking on projects like this, or just getting started

539
00:43:03,880 --> 00:43:04,880
and...

540
00:43:04,880 --> 00:43:16,240
Well, just don't be scared, because you can do it and it's, like, don't expect yourself

541
00:43:16,240 --> 00:43:25,320
to be able to do that in one day, but eventually, like, going step by step and it's at some

542
00:43:25,320 --> 00:43:32,160
point, like, if somebody asked me, where will you be one year from now, one year ago, and

543
00:43:32,160 --> 00:43:34,680
I wouldn't answer, I would be here, so...

544
00:43:34,680 --> 00:43:35,680
Right.

545
00:43:35,680 --> 00:43:36,680
Awesome.

546
00:43:36,680 --> 00:43:37,680
Awesome.

547
00:43:37,680 --> 00:43:39,280
Well, I'd like to thank you so much.

548
00:43:39,280 --> 00:43:40,280
Thank you.

549
00:43:40,280 --> 00:43:46,160
Alright, everyone, that's our show for today.

550
00:43:46,160 --> 00:43:52,160
For more information on Elena or any of the topics covered in this episode, visit twomolai.com

551
00:43:52,160 --> 00:43:55,640
slash talk slash 243.

552
00:43:55,640 --> 00:44:25,200
As always, thanks so much for listening and catch you next time.

