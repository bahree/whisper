1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,120
I'm your host, Sam Charrington.

4
00:00:32,120 --> 00:00:37,480
Today we're joined by Ida Wan-Euzjin, professor of electrical and computer engineering at UCLA,

5
00:00:37,480 --> 00:00:43,600
where his research group focuses on photonics and its applications to nano and biotechnology.

6
00:00:43,600 --> 00:00:48,000
In our conversation, we explore his group's research into the intersection of deep learning

7
00:00:48,000 --> 00:00:51,640
and optics, holography, and computational imaging.

8
00:00:51,640 --> 00:00:56,640
We specifically look at a really interesting project to create all optical neural networks,

9
00:00:56,640 --> 00:01:00,640
which work based on diffraction, where the printed pixels of the network are analogous

10
00:01:00,640 --> 00:01:02,320
to neurons.

11
00:01:02,320 --> 00:01:07,080
We also explore some of the practical applications for their research and other areas of interest

12
00:01:07,080 --> 00:01:10,240
for their group, and now on to the show.

13
00:01:10,240 --> 00:01:13,520
Alright everyone, I am on the line with Ida Wan-Euzjin.

14
00:01:13,520 --> 00:01:18,960
Ida Wan is a professor in electrical and computer engineering department at UCLA.

15
00:01:18,960 --> 00:01:22,240
Ida Wan, welcome to this week in machine learning and AI.

16
00:01:22,240 --> 00:01:24,600
Well thanks for having me, Sam.

17
00:01:24,600 --> 00:01:31,440
So you are currently working at the intersection point between optics and deep learning, and

18
00:01:31,440 --> 00:01:35,520
I'm really interested in jumping into this and learning more about the work that's coming

19
00:01:35,520 --> 00:01:40,640
out of your research group, but before we do, tell us a little bit about your background

20
00:01:40,640 --> 00:01:44,320
and how you started working with deep learning.

21
00:01:44,320 --> 00:01:46,320
Sure, sure.

22
00:01:46,320 --> 00:01:53,000
Yes, I work in optics, applications of optics, specifically computational imaging and sensing

23
00:01:53,000 --> 00:01:58,680
techniques, where we're creating new types of imaging systems, new types of microscopes

24
00:01:58,680 --> 00:02:07,040
and sensors, with heavily focusing on applications in biomedical space.

25
00:02:07,040 --> 00:02:11,640
For example, pathology, creating new types of microscopes in pathology, at the same time

26
00:02:11,640 --> 00:02:17,440
different types of sensors for telemedicine, mobile health related applications, and more

27
00:02:17,440 --> 00:02:22,120
and more recently we're interested in the environmental monitoring looking into water

28
00:02:22,120 --> 00:02:23,800
and air quality.

29
00:02:23,800 --> 00:02:30,080
And there's a huge opportunity ahead of us at the intersection of optics and machine

30
00:02:30,080 --> 00:02:36,240
learning that we've been exploiting for the last several years.

31
00:02:36,240 --> 00:02:44,440
Let's first start with essentially using machine learning to look at the images that we

32
00:02:44,440 --> 00:02:48,240
reconstruct and finding specific targets of interest.

33
00:02:48,240 --> 00:02:54,240
Let's say you're looking at certain parasites in drinking water and you create a mobile

34
00:02:54,240 --> 00:03:02,040
microscope and create images of particles inside your observation volume chamber, disposable

35
00:03:02,040 --> 00:03:03,440
cartridge.

36
00:03:03,440 --> 00:03:09,320
And then of course you need some sort of machine learning tool to look at what's captured

37
00:03:09,320 --> 00:03:14,000
there and specifically label, let's say, certain parasites of interest.

38
00:03:14,000 --> 00:03:20,120
Say it starts from that direction of annotating images and looking for specific signatures

39
00:03:20,120 --> 00:03:22,520
that you're looking that you're after.

40
00:03:22,520 --> 00:03:29,320
But more and more recently we're realizing the power of specially deep learning to design

41
00:03:29,320 --> 00:03:35,240
instruments, the design sensors from image labeling, image classification, we're moving

42
00:03:35,240 --> 00:03:42,120
toward data driven designs for optical instrumentation for optical sensors.

43
00:03:42,120 --> 00:03:49,960
In a sense, I've spent easily the last 10 years creating new types of computational reconstruction

44
00:03:49,960 --> 00:03:55,480
methods for different kinds of microscopes, holographic microscopes, for example.

45
00:03:55,480 --> 00:03:59,880
And in the last few years, we're realizing that we're moving from physics driven solutions

46
00:03:59,880 --> 00:04:07,320
to data driven solutions, how data and the framework around deep learning, holistically they're

47
00:04:07,320 --> 00:04:13,160
helping us to essentially better reconstruct images, especially for holography, we've

48
00:04:13,160 --> 00:04:20,800
seen some great examples of that where without any iterations, training purely a neural net

49
00:04:20,800 --> 00:04:27,560
based on input output labels, you can actually reconstruct holograms much better than traditional

50
00:04:27,560 --> 00:04:32,240
methods of solving an inverse problem and that's very, very exciting for actually optics

51
00:04:32,240 --> 00:04:35,480
community at large as a whole.

52
00:04:35,480 --> 00:04:37,560
What does it mean to reconstruct the hologram?

53
00:04:37,560 --> 00:04:40,120
What specifically are we trying to do there?

54
00:04:40,120 --> 00:04:47,160
So one class of mobile microscopes that we've created is actually using diffraction of light.

55
00:04:47,160 --> 00:04:52,200
In fact, holographic shadows of specimen to reconstruct images.

56
00:04:52,200 --> 00:04:58,000
So in this line of work, you're taking, for example, a CMOS image or like the SCCD or

57
00:04:58,000 --> 00:05:01,920
a CMOS image or the same thing that you have at the back of your cell phone and place

58
00:05:01,920 --> 00:05:08,520
specimen right in front of it without any lenses where the specimen can be, let's say, pathologist

59
00:05:08,520 --> 00:05:16,200
sample or a smear and you shine light through it and you capture with that simple configuration

60
00:05:16,200 --> 00:05:21,920
with a simple 510 megapixel imager, the diffraction hologram of a specimen.

61
00:05:21,920 --> 00:05:28,000
So that contains all the three-dimensional information of the specimen, but it needs

62
00:05:28,000 --> 00:05:35,880
to be reconstructed because it doesn't immediately give you the micro scale information of the sample.

63
00:05:35,880 --> 00:05:39,280
It gives you a hologram which is light interference.

64
00:05:39,280 --> 00:05:44,800
The light scattered by the object interferes with the background light giving you some fringes

65
00:05:44,800 --> 00:05:47,280
and a hologram.

66
00:05:47,280 --> 00:05:49,360
It needs to be reconstructed.

67
00:05:49,360 --> 00:05:57,040
So physics is great at providing us tools to do this reconstruction.

68
00:05:57,040 --> 00:06:04,880
But nowadays, one alternative method that we've been exploring and being very successful

69
00:06:04,880 --> 00:06:11,280
at is to use actually holograms as inputs to a neural net that has been trained with

70
00:06:11,280 --> 00:06:17,200
reconstructions, gold standard labels coming from physical reconstruction methods which

71
00:06:17,200 --> 00:06:23,280
are relatively speaking cumbersome in the sense that they're typically using some iterations

72
00:06:23,280 --> 00:06:31,120
to clean up the image and give you something that with reasonable resolution shows you

73
00:06:31,120 --> 00:06:33,840
what's going on in your sample.

74
00:06:33,840 --> 00:06:38,080
Now we're seeing that through data you can actually teach a neural net to reconstruct

75
00:06:38,080 --> 00:06:45,080
the hologram from those interference patterns, from those fringes, to go back to the sample

76
00:06:45,080 --> 00:06:51,920
plane, sample volume to give you all these signatures of the samples.

77
00:06:51,920 --> 00:06:57,080
Better than before, in the sense that this reconstruction is now extremely fast, and

78
00:06:57,080 --> 00:07:02,800
it can also provide some better rejection of some of these interference artifacts that

79
00:07:02,800 --> 00:07:04,720
are coming to holography.

80
00:07:04,720 --> 00:07:12,720
It's just one example where data-driven approaches are providing alternatives to intuition driven

81
00:07:12,720 --> 00:07:16,280
by physics to do every construction.

82
00:07:16,280 --> 00:07:25,160
And so in this case is the your ground truth and image that is image like traditionally

83
00:07:25,160 --> 00:07:31,120
like with a lens in front of the same type of CCD, and then you take an image of the

84
00:07:31,120 --> 00:07:38,000
same subjects without the lens just using the diffractive patterns, and that's how you

85
00:07:38,000 --> 00:07:43,400
produce your labels or is it more involved in that?

86
00:07:43,400 --> 00:07:50,720
You can definitely take that approach and use standard microscopes to create labels.

87
00:07:50,720 --> 00:07:57,320
A better approach is actually to use traditional physical reconstruction methods that take

88
00:07:57,320 --> 00:08:03,400
maybe a bunch of holograms to reconstruct the sample's information.

89
00:08:03,400 --> 00:08:07,840
Different than a traditional lens-based bright field microscope, holography has two different

90
00:08:07,840 --> 00:08:09,480
channels of information.

91
00:08:09,480 --> 00:08:16,000
One of them relates to the amplitude of the sample, amplitude of how the light is scattered,

92
00:08:16,000 --> 00:08:19,000
and the other one is the phase information of the sample.

93
00:08:19,000 --> 00:08:24,480
And that's where holography is very powerful as a coherent imaging modality.

94
00:08:24,480 --> 00:08:30,080
And that's why a better approach to use as gold standard ground truth images is actually

95
00:08:30,080 --> 00:08:35,040
coming from traditional holographic reconstruction solutions.

96
00:08:35,040 --> 00:08:39,000
They provide essentially the physics enabled ground truth.

97
00:08:39,000 --> 00:08:44,560
And after we've done that, we see some very interesting results where it solves the

98
00:08:44,560 --> 00:08:51,320
inverse problem in a different path, but very robust in terms of what you need to see,

99
00:08:51,320 --> 00:08:55,560
what you need to reconstruct at the microscope scale.

100
00:08:55,560 --> 00:08:58,800
For both of these channels that I mentioned, phase and amplitude.

101
00:08:58,800 --> 00:09:03,720
The phase channel is extremely important, especially for transparent samples like jellyfish,

102
00:09:03,720 --> 00:09:08,920
for example, thin samples that do not scatter light as much.

103
00:09:08,920 --> 00:09:13,000
And is the reconstruction that you're targeting?

104
00:09:13,000 --> 00:09:20,800
Is it a visual reconstruction that you might get if you had a traditional microscope

105
00:09:20,800 --> 00:09:27,520
or is it something different that better captures the two types of information that you

106
00:09:27,520 --> 00:09:30,520
have access to with holographic images?

107
00:09:30,520 --> 00:09:36,280
It bears the advantages of holography in the sense that it can look at these two different

108
00:09:36,280 --> 00:09:42,160
information channels that I mentioned, amplitude and phase, and at the same time over a larger

109
00:09:42,160 --> 00:09:43,320
volume.

110
00:09:43,320 --> 00:09:50,200
One advantage of holography is it can see objects at different depths without a physical

111
00:09:50,200 --> 00:09:51,200
focusing mechanism.

112
00:09:51,200 --> 00:09:57,400
You can time reverse the optical field and go to different depths within your sample.

113
00:09:57,400 --> 00:10:03,200
And that gives an advantage to a holographic reconstruction, and it essentially gives

114
00:10:03,200 --> 00:10:06,920
you microscopic features at different planes of your sample.

115
00:10:06,920 --> 00:10:10,200
And that's what the network output gives you.

116
00:10:10,200 --> 00:10:16,160
And we've shown that actually if you trade the neural net with different holograms acquired

117
00:10:16,160 --> 00:10:22,440
at different sample to sensor distances, you can extend a depth of field.

118
00:10:22,440 --> 00:10:28,480
In other words, the neural net can be taught to not only holographically reconstruct objects,

119
00:10:28,480 --> 00:10:30,480
but autofocus them as well.

120
00:10:30,480 --> 00:10:36,440
Which is being very interesting, imagine for example, as a volume of a sample, which contains

121
00:10:36,440 --> 00:10:41,960
several different objects, cells scattered for example in a volume, you can actually

122
00:10:41,960 --> 00:10:46,440
break them all into focus within the reconstruction.

123
00:10:46,440 --> 00:10:52,600
And that's something that we recently shown as a holographic reconstruction doing both standard

124
00:10:52,600 --> 00:10:59,040
hologram to image transformation that physics has been very powerful doing for decades.

125
00:10:59,040 --> 00:11:04,280
But at the same time, merging it with autofocusing so that different parts of the sample come

126
00:11:04,280 --> 00:11:07,360
into focus digitally at the output of the network.

127
00:11:07,360 --> 00:11:14,160
And what do these networks tend to look like? Are they convolutional neural nets with

128
00:11:14,160 --> 00:11:15,760
traditional architectures?

129
00:11:15,760 --> 00:11:22,520
Or are you doing very domain specific things in the various layers of the network?

130
00:11:22,520 --> 00:11:26,280
Well, these are powered by convolutional neural nets.

131
00:11:26,280 --> 00:11:32,360
And we're taking essentially standard architectures, but of course fine tuning them without making

132
00:11:32,360 --> 00:11:35,760
them unnecessarily complicated at the same time.

133
00:11:35,760 --> 00:11:38,960
This is a microscopic imaging modality.

134
00:11:38,960 --> 00:11:47,240
We're also fine tuning some of the parameters in this space to make it learn this transformation

135
00:11:47,240 --> 00:11:53,440
across different spatial features so that it can actually reconstruct larger features at

136
00:11:53,440 --> 00:11:58,360
the same time subcellular features, for example, if you're living into a cell.

137
00:11:58,360 --> 00:12:07,280
And when you're building these networks that essentially map from this kind of raw data

138
00:12:07,280 --> 00:12:16,960
from the imager to the result of something that's trained on the physics-based reconstructions,

139
00:12:16,960 --> 00:12:23,080
is there anything about the networks that you've seen produced and their function that

140
00:12:23,080 --> 00:12:31,760
suggests that the network is learning structure that's analogous to the rules of a physics

141
00:12:31,760 --> 00:12:37,920
that a, you know, the govern the system or is it taking shortcuts that don't necessarily

142
00:12:37,920 --> 00:12:41,520
relate to the physical representation?

143
00:12:41,520 --> 00:12:43,440
Yeah, a fantastic question.

144
00:12:43,440 --> 00:12:50,200
So it learns whatever you teach it to do and it rejects anything else.

145
00:12:50,200 --> 00:12:54,840
Even though that something is physical, I'll give you an example.

146
00:12:54,840 --> 00:13:00,640
We've taught a neural net to do reconstructions of holograms, but these holograms were

147
00:13:00,640 --> 00:13:06,000
planar objects like we were interested in tissues used in pathology, right?

148
00:13:06,000 --> 00:13:11,240
There are thin sections of tissue and our goal was to reconstruct those tissue sections

149
00:13:11,240 --> 00:13:13,320
from their holograms.

150
00:13:13,320 --> 00:13:18,160
This was a plane to plane transformation in a sense that the sample is planar, its hologram

151
00:13:18,160 --> 00:13:24,880
is planar, and we were taking the raw holograms and transforming them back to how the sample

152
00:13:24,880 --> 00:13:27,920
should look like in its phase and amplitude.

153
00:13:27,920 --> 00:13:32,440
And the network was very successful learning kind of like, you know, without understanding

154
00:13:32,440 --> 00:13:37,760
physics, learning the physical principles of this transformation.

155
00:13:37,760 --> 00:13:40,720
However, it's rejecting anything else.

156
00:13:40,720 --> 00:13:47,120
For example, in our experiments, we usually have some dust particles that are seated somewhere

157
00:13:47,120 --> 00:13:52,760
in our optical path above or below the sample, but not on the sample plane.

158
00:13:52,760 --> 00:13:58,000
These dust particles are unavoidable, especially, you know, if you're not having a vacuum type

159
00:13:58,000 --> 00:13:59,520
of an environment.

160
00:13:59,520 --> 00:14:06,520
So in a physical reconstruction, those dust particles appear as some sort of artifact

161
00:14:06,520 --> 00:14:07,520
in the image.

162
00:14:07,520 --> 00:14:08,520
And we understand it.

163
00:14:08,520 --> 00:14:09,520
That's a physical solution.

164
00:14:09,520 --> 00:14:13,680
That's a physical particle creating some sort of an interference pattern, superimpose

165
00:14:13,680 --> 00:14:16,480
on the image of the sample, right?

166
00:14:16,480 --> 00:14:17,480
It's not part of the sample.

167
00:14:17,480 --> 00:14:20,480
It's somewhere else, but it's a physical particle.

168
00:14:20,480 --> 00:14:25,520
The network rejects that, although it appears in the physical reconstruction.

169
00:14:25,520 --> 00:14:29,800
The reason is, the network thinks it's an artifact of holography.

170
00:14:29,800 --> 00:14:33,480
It doesn't seem like it's in focus.

171
00:14:33,480 --> 00:14:38,800
It's an out of focus, you know, dust particle or artifact related to that.

172
00:14:38,800 --> 00:14:44,600
And actually, it's rejecting those kinds of physical, but outside the solution domain

173
00:14:44,600 --> 00:14:47,000
type of pixels.

174
00:14:47,000 --> 00:14:50,760
And that's very powerful in the sense that you see in the physical reconstruction some

175
00:14:50,760 --> 00:14:55,880
artifacts that must be there, but they're actually attacked by the network.

176
00:14:55,880 --> 00:15:00,800
Similar things that you can mention for particles or objects at different depths.

177
00:15:00,800 --> 00:15:07,040
Physics give you a certain solution, and the network sometimes violates that.

178
00:15:07,040 --> 00:15:11,160
But if only it is part of what you've trained it for.

179
00:15:11,160 --> 00:15:20,120
So you said that the physical reconstructions do contain these artifacts from the dust

180
00:15:20,120 --> 00:15:25,360
particles, and these are your ground truth, how then is the network learning to reject

181
00:15:25,360 --> 00:15:26,360
them?

182
00:15:26,360 --> 00:15:28,320
Well, they're rare, right?

183
00:15:28,320 --> 00:15:29,720
So that's the good thing.

184
00:15:29,720 --> 00:15:34,040
The physical reconstruct that, the physics, I mean, the dust particles are obviously happening

185
00:15:34,040 --> 00:15:35,040
rarely.

186
00:15:35,040 --> 00:15:40,080
Like over a large field of view, maybe you have a few places, and by and large, everything

187
00:15:40,080 --> 00:15:42,160
else is plain to plain.

188
00:15:42,160 --> 00:15:48,440
That's why it generalizes to that transformation and attacks anything of that nature.

189
00:15:48,440 --> 00:15:52,560
You're creating these new types of reconstruction methods.

190
00:15:52,560 --> 00:15:54,920
How also you applying deep learning?

191
00:15:54,920 --> 00:16:01,360
Well, I mean, a recent work that we've done on optics deep learning intersection is

192
00:16:01,360 --> 00:16:09,320
actually creation of an optical network, optical neural net that's based on diffraction.

193
00:16:09,320 --> 00:16:16,840
So it's not a traditional deep neural net in the sense of standard convolutional neural

194
00:16:16,840 --> 00:16:22,280
nets with non-denarities like rectified linear units, et cetera, that you find in electronic

195
00:16:22,280 --> 00:16:23,280
neural nets.

196
00:16:23,280 --> 00:16:25,080
This is actually an analogous to that.

197
00:16:25,080 --> 00:16:34,680
It's an optical analog of it where, imagine the input is an object where you shine light,

198
00:16:34,680 --> 00:16:38,240
and behind the input, there is an optical construct.

199
00:16:38,240 --> 00:16:45,400
It's a passive system, meaning you design this optical hardware using a computer, using

200
00:16:45,400 --> 00:16:47,960
deep learning principles.

201
00:16:47,960 --> 00:16:54,560
And once you optimize it, you fabricate it using a 3D printer, if your wavelength is

202
00:16:54,560 --> 00:16:57,640
of interest to that, or using lithography.

203
00:16:57,640 --> 00:17:02,800
You fabricate features to craft essentially a volume.

204
00:17:02,800 --> 00:17:09,000
And that volume as light is penetrating from the input plane into the diffractive layers

205
00:17:09,000 --> 00:17:14,880
is solving essentially a problem, let's say a classification problem.

206
00:17:14,880 --> 00:17:21,520
And that volume is composed of different layers, where every layer is composed of several

207
00:17:21,520 --> 00:17:24,160
pixels, thousands of pixels.

208
00:17:24,160 --> 00:17:28,480
And these pixels are representing what we call as neurons.

209
00:17:28,480 --> 00:17:34,800
The transmittance phase and amplitude of the transmittance of each neuron is a trainable

210
00:17:34,800 --> 00:17:36,480
learnable parameter.

211
00:17:36,480 --> 00:17:39,680
And at the end of the network, there's a light pattern.

212
00:17:39,680 --> 00:17:46,280
And that light pattern is the output of your network that you're trying to solve a problem.

213
00:17:46,280 --> 00:17:48,680
Let's look at, for example, a classification problem.

214
00:17:48,680 --> 00:17:53,280
A simple one, let's look at classification of handwritten digits.

215
00:17:53,280 --> 00:18:01,720
If I input to this network, zeros, like handwritten zero digits, the light that is transmitted

216
00:18:01,720 --> 00:18:07,520
from that object is diffracting through these different layers that are all optimized to

217
00:18:07,520 --> 00:18:15,080
guide the light to a certain detector at the output plane that is assigned to zero.

218
00:18:15,080 --> 00:18:20,920
If I bring a new zero handwritten, it's, if it's correctly built, it's going to actually

219
00:18:20,920 --> 00:18:25,400
channel most of the photons at the output to the correct detector, saying that it's all

220
00:18:25,400 --> 00:18:29,000
optically in fearing that it's a zero.

221
00:18:29,000 --> 00:18:32,000
Same idea applies for other classes.

222
00:18:32,000 --> 00:18:38,240
And it's all optically, essentially, using diffraction, solving this problem that you

223
00:18:38,240 --> 00:18:40,040
asked the network to solve.

224
00:18:40,040 --> 00:18:47,040
This is really fascinating to think of that you can implement a deep neuron that all

225
00:18:47,040 --> 00:18:48,960
in optics.

226
00:18:48,960 --> 00:19:00,200
You mentioned that the pixels are analogous to neurons in a traditional deep neuron network

227
00:19:00,200 --> 00:19:03,560
and that the parameters can be learned.

228
00:19:03,560 --> 00:19:08,240
Are you learning those parameters offline, so to speak, meaning before you're printing

229
00:19:08,240 --> 00:19:16,240
these, this network, or is there somehow, these networks are not, they're mutable,

230
00:19:16,240 --> 00:19:17,240
posts printing?

231
00:19:17,240 --> 00:19:18,240
Great question.

232
00:19:18,240 --> 00:19:26,320
So yes, let's think first, an entirely passive system where it's fabricated and fixed.

233
00:19:26,320 --> 00:19:32,000
Before that fabrication process, you use a computer to finalize the, through deep learning,

234
00:19:32,000 --> 00:19:36,880
through backpropagation, error backpropagation, you finalize the transmittance values of

235
00:19:36,880 --> 00:19:38,800
each one of these neurons.

236
00:19:38,800 --> 00:19:43,560
And then you fabricate it, so it's essentially fixed, but you can also create a hybrid system

237
00:19:43,560 --> 00:19:51,600
where some of these layers are composed of dynamic light modulators, like spatial light

238
00:19:51,600 --> 00:19:57,000
modulators where you can actually change them and make them as part of a learning scheme

239
00:19:57,000 --> 00:20:01,040
or kind of like mend the network's function.

240
00:20:01,040 --> 00:20:06,920
But in the implementation that we had, experimentally, these were trained using a computer in

241
00:20:06,920 --> 00:20:14,520
TensorFlow and then fabricated and then all optically tested with light as input passing

242
00:20:14,520 --> 00:20:16,440
through some input.

243
00:20:16,440 --> 00:20:23,440
One thing that's very important here is that this framework is, it has two branches to

244
00:20:23,440 --> 00:20:24,440
it.

245
00:20:24,440 --> 00:20:31,160
One, you can create something with this kind of layer-to-layer design using linear materials,

246
00:20:31,160 --> 00:20:38,720
linear optical materials, meaning that they are essentially not including any non-linearities.

247
00:20:38,720 --> 00:20:46,560
As a result, if you use no non-linearity in your diffractive layers, this becomes a linear

248
00:20:46,560 --> 00:20:53,720
classifier, meaning that you can have all these different layers of free space propagation

249
00:20:53,720 --> 00:21:00,760
of light and a diffractive layer, some more free space diffraction, some more layer coming

250
00:21:00,760 --> 00:21:03,920
after it and repeating the same process.

251
00:21:03,920 --> 00:21:11,240
All of this can be mathematically squeezed into a single matrix operation, but even using

252
00:21:11,240 --> 00:21:16,960
these linear materials, the network has depth to it and that's where the diffractive

253
00:21:16,960 --> 00:21:20,880
deep network, optical network analogy comes from.

254
00:21:20,880 --> 00:21:25,040
The depth of the network is because of this.

255
00:21:25,040 --> 00:21:31,240
You cannot take a single layer between the input and output planes and generalize to any

256
00:21:31,240 --> 00:21:36,480
function that multiple diffractive layers collectively can produce.

257
00:21:36,480 --> 00:21:41,920
In fact, we've shown that as you add more layers to this system, one after another, all

258
00:21:41,920 --> 00:21:49,600
trainable, your network gets better in its classification and its power efficiency, its

259
00:21:49,600 --> 00:21:56,600
output contrast, per detector, per class improves as the number of layers increases.

260
00:21:56,600 --> 00:21:58,680
That's one aspect that I would like to emphasize.

261
00:21:58,680 --> 00:22:03,800
The other aspect is, before you move on, just to make sure I understand this, are you saying

262
00:22:03,800 --> 00:22:12,000
that adding additional layers inherently introduces non-linearity or that you're using non-linear

263
00:22:12,000 --> 00:22:18,600
materials with non-linear reflective and refractive characteristics to introduce the non-linearity?

264
00:22:18,600 --> 00:22:26,000
Certainly, we can use non-linear materials as part of the diffractive layers to introduce

265
00:22:26,000 --> 00:22:35,120
non-linearity in the system and then it becomes a more sophisticated tool in terms of how

266
00:22:35,120 --> 00:22:38,600
different kinds of functions it can generalize.

267
00:22:38,600 --> 00:22:43,640
You can include non-linearities like non-linear materials, crystals, polymers, semiconductors,

268
00:22:43,640 --> 00:22:49,160
as you fabricate these things as part of the network to introduce non-linearity.

269
00:22:49,160 --> 00:22:55,280
What I was referring to is the fact that even though you don't have any non-linear material

270
00:22:55,280 --> 00:23:03,080
in this system, there is depth to it in the sense that multiple layers perform or have

271
00:23:03,080 --> 00:23:09,880
more degrees of freedom to perform a more general function better than a single diffractive

272
00:23:09,880 --> 00:23:16,960
layer, and that's where, even with the linearity of the material, there is depth and deepness

273
00:23:16,960 --> 00:23:19,640
of the network in its performance.

274
00:23:19,640 --> 00:23:27,440
You've got this essentially a classifier that you've printed just to make this very concrete.

275
00:23:27,440 --> 00:23:32,280
What type of scale are we talking about physically?

276
00:23:32,280 --> 00:23:33,280
Great question.

277
00:23:33,280 --> 00:23:40,360
We tested these experimentally using Terahertz wavelengths. The weigh length of which in

278
00:23:40,360 --> 00:23:46,800
air was about 0.75 millimeters, so it's a big weight length, it's Terahertz.

279
00:23:46,800 --> 00:23:53,280
That's why we were able to use luckily a standard 3D printer, which is transparent at

280
00:23:53,280 --> 00:23:54,280
those weigh length.

281
00:23:54,280 --> 00:24:00,360
We could put many layers, like five layers, one after another, forming a diffractive optical

282
00:24:00,360 --> 00:24:05,760
network made out of, literally, plastic coming from a 3D printer.

283
00:24:05,760 --> 00:24:10,560
The size of this network was on the order of 8 to 9 centimeters.

284
00:24:10,560 --> 00:24:15,880
Obviously, a divisible weight length at shorter weight lengths.

285
00:24:15,880 --> 00:24:22,040
What you're looking at is maybe half a centimeter by half a centimeter type of a network in terms

286
00:24:22,040 --> 00:24:26,320
of width, which would be sufficient in terms of number of neurons, et cetera, that you

287
00:24:26,320 --> 00:24:28,840
can fabricate there with lithography.

288
00:24:28,840 --> 00:24:38,880
So 8 to 9 centimeters sounds very large relative to what I envisioned for this.

289
00:24:38,880 --> 00:24:42,520
That was the weigh length is very large.

290
00:24:42,520 --> 00:24:49,280
The weigh length, as I said, is about 0.7 to 0.8 millimeters.

291
00:24:49,280 --> 00:24:53,600
That's almost about 1,800 microns.

292
00:24:53,600 --> 00:25:00,960
If you go to visible weigh lengths, sub-micron weigh lengths, you're looking at a drastic reduction

293
00:25:00,960 --> 00:25:01,960
in the size.

294
00:25:01,960 --> 00:25:08,840
That's why a few millimeters by a few millimeters would be the size of this kind of a network working

295
00:25:08,840 --> 00:25:15,040
in visible weigh length, let's say in green weigh length, like half a micron weigh length.

296
00:25:15,040 --> 00:25:18,080
That's why everything will be scalable to the weigh length.

297
00:25:18,080 --> 00:25:28,840
In this Terahertz weigh length that you fabricated, the 8 to 9 centimeters is that 8 to 9 centimeters

298
00:25:28,840 --> 00:25:37,720
square for the plane that the light initially hits, and then is there a separate depth for

299
00:25:37,720 --> 00:25:40,720
this or is it 8 to 9?

300
00:25:40,720 --> 00:25:43,720
What dimension is that 8 to 9 millimeters referred to?

301
00:25:43,720 --> 00:25:44,720
Great.

302
00:25:44,720 --> 00:25:46,640
The 8 to 9 centimeters.

303
00:25:46,640 --> 00:25:49,360
Right, right, sorry.

304
00:25:49,360 --> 00:25:59,480
Is the width and the depth of this system is actually layered to layer spacing is only

305
00:25:59,480 --> 00:26:00,480
3 centimeters.

306
00:26:00,480 --> 00:26:12,280
Essentially, you're looking at maybe a cube of 8 to 8 centimeters by maybe about 15 centimeters

307
00:26:12,280 --> 00:26:14,080
in total depth.

308
00:26:14,080 --> 00:26:21,200
The spacing between the layers being 3 centimeters, we also had another network which was much

309
00:26:21,200 --> 00:26:27,520
more compact with a 4 millimeter gap, but you should all convert these to weigh lengths

310
00:26:27,520 --> 00:26:34,200
because that's what is more relevant to discuss because here we use a specific weigh length.

311
00:26:34,200 --> 00:26:40,480
In terms of weigh length, we're looking at layer to layer spacing of less than 50 weigh

312
00:26:40,480 --> 00:26:46,320
lengths, so if your weigh length is let's say a micron, you're talking about 50 micron

313
00:26:46,320 --> 00:26:50,240
at the most layer to layer spacing, sub-micron.

314
00:26:50,240 --> 00:26:57,160
In terms of the width, we're talking about maybe 100 weigh lengths in terms of width and

315
00:26:57,160 --> 00:27:03,520
height of the network, so in that regard, these are really very compact networks that will

316
00:27:03,520 --> 00:27:11,320
scale down with weigh length, obviously, in this example, we've used off the shelf 3D printers

317
00:27:11,320 --> 00:27:17,120
and that's why Terahertz was used to show the fidelity of this thing, this thing with relatively

318
00:27:17,120 --> 00:27:19,920
very inexpensive fabrication methods.

319
00:27:19,920 --> 00:27:24,840
It costs us only a few dollars to print one of these things with 3D printers.

320
00:27:24,840 --> 00:27:29,680
The target of he would be more expensive, but with economies of scale, obviously, the

321
00:27:29,680 --> 00:27:36,160
fabrication of something like this using gloss and using photolithography would be no

322
00:27:36,160 --> 00:27:42,880
different than fabricating, for example, your CMOS imager at the back of your cell phone.

323
00:27:42,880 --> 00:27:47,760
In that regard, it will be also pennies at large scale fabrication.

324
00:27:47,760 --> 00:27:55,440
In terms of visualizing what the layers might look like, should we or could we think of

325
00:27:55,440 --> 00:28:01,800
them kind of as like Fresno lenses or maybe more digital patterns, like QR codes or

326
00:28:01,800 --> 00:28:05,720
is there a way that you can articulate what these things might look like if we were looking

327
00:28:05,720 --> 00:28:08,560
at a layer individually?

328
00:28:08,560 --> 00:28:13,000
Each layer would look like speckle pattern to you.

329
00:28:13,000 --> 00:28:18,920
It wouldn't mean much, but as you come toward the output layer, you will see emergence

330
00:28:18,920 --> 00:28:22,640
of some patterns depending on where you put your detectors.

331
00:28:22,640 --> 00:28:29,400
So essentially, it's a gradual shaping of statistical waveforms coming from different

332
00:28:29,400 --> 00:28:33,720
objects that you've learned to classify.

333
00:28:33,720 --> 00:28:40,880
So, it's very difficult to exactly understand how they work together because there are

334
00:28:40,880 --> 00:28:48,680
multiple layers and your signal is essentially a stochastic signal in the sense that it

335
00:28:48,680 --> 00:28:52,040
can come in different forms.

336
00:28:52,040 --> 00:28:57,840
But what you can see from each layer is that there is a face pattern and it's slowly

337
00:28:57,840 --> 00:29:06,320
emerging at the output to shape itself as if it's facing a bunch of detectors that each

338
00:29:06,320 --> 00:29:09,680
of which is assigned to a class.

339
00:29:09,680 --> 00:29:13,960
If you have 10 classes, then you're looking at 10 detectors at the output so that the

340
00:29:13,960 --> 00:29:19,800
inferences is all optical except the final detection part per detector.

341
00:29:19,800 --> 00:29:25,120
So obviously, you can have more detectors to be on the class.

342
00:29:25,120 --> 00:29:31,800
In fact, in our recent work as a follow-up on this, we've shown the merge of these diffractive

343
00:29:31,800 --> 00:29:37,800
optical layers forming the all optical network with electronic neural nets.

344
00:29:37,800 --> 00:29:43,880
So formed kind of like a hybrid system where the front end was all optical and the backhand

345
00:29:43,880 --> 00:29:49,720
was standard neural nets with the standard non-linearities, et cetera.

346
00:29:49,720 --> 00:29:54,200
So that obviously has some very interesting and appealing features.

347
00:29:54,200 --> 00:30:01,520
And one thing that we've shown is that the input pixels to the electronic neural net can

348
00:30:01,520 --> 00:30:07,360
be compressed by the all optical network if they're both optimized at the same time.

349
00:30:07,360 --> 00:30:14,160
The optical neural net and the electronic neural net, if they are optimized jointly, we

350
00:30:14,160 --> 00:30:17,000
see some very interesting advantages.

351
00:30:17,000 --> 00:30:19,680
But this is something that's unpublished.

352
00:30:19,680 --> 00:30:22,880
We just put it into archives.

353
00:30:22,880 --> 00:30:28,800
And we're seeing some very interesting hybrid systems that can emerge from the same diffractive

354
00:30:28,800 --> 00:30:30,440
neural net concept.

355
00:30:30,440 --> 00:30:35,880
And how is that physically implemented, this mixed mode system?

356
00:30:35,880 --> 00:30:44,720
So this was an analysis, but in an experiment, imagine you take an optical electronic sensor,

357
00:30:44,720 --> 00:30:50,920
let's say 100 pixel by 100 pixel or a few hundred pixel by a few hundred pixel type of design.

358
00:30:50,920 --> 00:30:55,840
In front of it, as if you're putting a lens, you're going to be putting a bunch of diffractive

359
00:30:55,840 --> 00:31:01,400
layers that have been optimized for a certain inference task.

360
00:31:01,400 --> 00:31:09,960
Assuming that there is a very simple neural net that is running beyond the CMOS.

361
00:31:09,960 --> 00:31:16,600
And that's the CMOS or the CCD, the low pixel count CCD or CMOS imager is kind of like

362
00:31:16,600 --> 00:31:22,160
the layer between the optics and the electronic neural net.

363
00:31:22,160 --> 00:31:30,040
And are you using it as an electronic or a dynamic detector?

364
00:31:30,040 --> 00:31:31,040
Is that its role?

365
00:31:31,040 --> 00:31:39,440
Or are you also implementing layers beyond that in the analysis that you did?

366
00:31:39,440 --> 00:31:47,760
We've shown it works as a classifier, just like the all optical network, but a better

367
00:31:47,760 --> 00:31:49,080
classifier.

368
00:31:49,080 --> 00:31:56,360
And it also takes a very primitive neural net that the inference performance, not very deep,

369
00:31:56,360 --> 00:32:02,560
not a lot of trainable parameters, very low power requirement type of a neural net.

370
00:32:02,560 --> 00:32:06,080
You can take something like that and make it work very good.

371
00:32:06,080 --> 00:32:14,040
So in a sense, it will be very useful, especially for low power and mobile systems that need

372
00:32:14,040 --> 00:32:20,560
to frequently, with a very high frame rate, look at some scene or some data.

373
00:32:20,560 --> 00:32:28,280
That's where you don't have the luxury of working with very sophisticated neural nets.

374
00:32:28,280 --> 00:32:31,440
For power reasons, for frame rate reasons, et cetera.

375
00:32:31,440 --> 00:32:38,560
That's where optics can help, because of its speed and its front end, making the rest

376
00:32:38,560 --> 00:32:43,680
after the digitization step to be compact in terms of number of pixels, in terms of

377
00:32:43,680 --> 00:32:48,960
frame rate being fast, and also the depth of the electronic neural net or the complexity

378
00:32:48,960 --> 00:32:56,880
of the electronic neural net becoming kind of much less advanced compared to purely electronic

379
00:32:56,880 --> 00:32:58,120
neural net?

380
00:32:58,120 --> 00:33:06,800
Other experiments were based on an M-ness style database, handwritten digits.

381
00:33:06,800 --> 00:33:17,120
Do you have a sense for the way the physical characteristics of the layers change with the

382
00:33:17,120 --> 00:33:22,000
complexity of the underlying data set, the input data?

383
00:33:22,000 --> 00:33:26,440
We've also tried another data set, which is going to be more complicated.

384
00:33:26,440 --> 00:33:28,440
It's called the fashion M-ness.

385
00:33:28,440 --> 00:33:36,440
We had 10 different classes of fashion products, like sneakers, bags, t-shirts, trousers, et cetera,

386
00:33:36,440 --> 00:33:37,440
et cetera.

387
00:33:37,440 --> 00:33:47,160
It's not as challenging as C-Far-10, for example, but it's more complicated than a fashion

388
00:33:47,160 --> 00:33:48,680
M-ness.

389
00:33:48,680 --> 00:33:55,920
We've also tried that and shown that another network trained for that data set 3D printed

390
00:33:55,920 --> 00:34:03,160
and tested experimentally works nicely to match our predictions or numerical analysis.

391
00:34:03,160 --> 00:34:12,240
In terms of complexity, hard for us to understand what changed from M-ness layers to the fashion

392
00:34:12,240 --> 00:34:14,240
M-ness layers.

393
00:34:14,240 --> 00:34:22,400
As I look at them in front of me, they look similar to me, but of course, all the details

394
00:34:22,400 --> 00:34:28,400
are hidden there in terms of the face profiles of each layer, so it's very difficult to obviously

395
00:34:28,400 --> 00:34:29,400
understand.

396
00:34:29,400 --> 00:34:30,400
Sure.

397
00:34:30,400 --> 00:34:37,400
But there wasn't anything obvious like you needed to go wider or deeper to achieve reasonable

398
00:34:37,400 --> 00:34:38,400
performance.

399
00:34:38,400 --> 00:34:39,400
No.

400
00:34:39,400 --> 00:34:43,720
Of course, there's a huge parameter space that we need to optimize here, and maybe there's

401
00:34:43,720 --> 00:34:49,640
more room to optimize to improve the inference performance for more complicated data sets.

402
00:34:49,640 --> 00:34:56,320
It's still research ongoing in that regard, but what we've shown is that the same 5 layers

403
00:34:56,320 --> 00:35:01,560
with the same number of learnable, trainable parameters worked.

404
00:35:01,560 --> 00:35:06,600
Of course, lower accuracy compared to M-ness because fashion is more difficult.

405
00:35:06,600 --> 00:35:12,320
But essentially, it's the same framework that nicely generalizes for another task.

406
00:35:12,320 --> 00:35:14,640
But how well you can push it further?

407
00:35:14,640 --> 00:35:16,160
It's still a research question.

408
00:35:16,160 --> 00:35:19,760
In fact, in one of our recent work, we have improved.

409
00:35:19,760 --> 00:35:28,720
For example, the paper that the science paper report something like about 92% for M-ness.

410
00:35:28,720 --> 00:35:37,800
We've pushed it to now 98% without using any nonlinear optical materials with some changes

411
00:35:37,800 --> 00:35:42,920
in the way that we optimize the neural net, optical neural net.

412
00:35:42,920 --> 00:35:47,280
So essentially, we've already optimized and improved some of the parameters that we've

413
00:35:47,280 --> 00:35:53,760
been using in designing this diffractive layer set to push it by a good margin from, as

414
00:35:53,760 --> 00:35:58,800
I said, for M-ness, for example, from about 92% with 5 layers now.

415
00:35:58,800 --> 00:36:05,560
We're approaching 98% with also 5 layers, same number of trainable parameters.

416
00:36:05,560 --> 00:36:10,280
It's a lot of different things that you can do to improve the performance of something

417
00:36:10,280 --> 00:36:11,280
like this.

418
00:36:11,280 --> 00:36:15,120
One unique aspect of diffractive layers is it's passive, right?

419
00:36:15,120 --> 00:36:19,560
I mean, once you've fabricated, it doesn't consume any power except the illumination

420
00:36:19,560 --> 00:36:23,600
power and maybe the detectors that you have at the output plane.

421
00:36:23,600 --> 00:36:26,120
Everything else in thin is just material.

422
00:36:26,120 --> 00:36:28,480
It's just you fabricated and it stays.

423
00:36:28,480 --> 00:36:29,480
It's good.

424
00:36:29,480 --> 00:36:30,720
It doesn't consume any power.

425
00:36:30,720 --> 00:36:31,720
But it's bad.

426
00:36:31,720 --> 00:36:32,720
It's static.

427
00:36:32,720 --> 00:36:36,920
If your data changes, you need to reprint it.

428
00:36:36,920 --> 00:36:43,160
The one thing that we've shown is actually you can take a neural net with several layers

429
00:36:43,160 --> 00:36:49,280
and peel off some layers and add new layers, trainable, with respect to the rest of the

430
00:36:49,280 --> 00:36:55,960
static ones and kind of mend and improve the performance of the entire system.

431
00:36:55,960 --> 00:36:58,920
So it's in that sense, it's like a Lego piece.

432
00:36:58,920 --> 00:37:02,160
Kind of like fine tuning applied to a physical network.

433
00:37:02,160 --> 00:37:03,160
Right.

434
00:37:03,160 --> 00:37:04,160
Exactly.

435
00:37:04,160 --> 00:37:09,680
Different layers, you take out some of them or you patch additional layers onto an existing

436
00:37:09,680 --> 00:37:10,680
one.

437
00:37:10,680 --> 00:37:15,560
That's one way of bringing some reconfigurability to the system.

438
00:37:15,560 --> 00:37:20,960
Another way of bringing reconfigurability to the system is replacing some of these layers

439
00:37:20,960 --> 00:37:24,800
with dynamic, electrooptic modulators.

440
00:37:24,800 --> 00:37:31,120
So rather than printing and having the code as part of gloss, permanently, you have a

441
00:37:31,120 --> 00:37:33,640
layer that you can actually change the pixels.

442
00:37:33,640 --> 00:37:35,840
Specialized modulator type of systems.

443
00:37:35,840 --> 00:37:41,320
Of course, you can create a hybrid system where some layers are static, some layers are

444
00:37:41,320 --> 00:37:48,280
dynamic and create some sort of a trade-off between complexity and reconfigurability of

445
00:37:48,280 --> 00:37:49,800
active layers.

446
00:37:49,800 --> 00:37:51,720
This is really fascinating.

447
00:37:51,720 --> 00:37:54,920
How do you see this playing out?

448
00:37:54,920 --> 00:37:58,760
Do you see practical applications for this?

449
00:37:58,760 --> 00:38:06,080
You know, some arbitrary time frame and what might those be or is this kind of a research

450
00:38:06,080 --> 00:38:13,480
direction that's driving you on a broader path that you're not necessarily trying to see

451
00:38:13,480 --> 00:38:15,760
this use practically?

452
00:38:15,760 --> 00:38:19,440
First of all, we're enjoying playing with it.

453
00:38:19,440 --> 00:38:21,200
It's a toy.

454
00:38:21,200 --> 00:38:24,440
And it's making us happy.

455
00:38:24,440 --> 00:38:28,080
So it sounds really fun.

456
00:38:28,080 --> 00:38:30,280
I can definitely see that part of it.

457
00:38:30,280 --> 00:38:34,440
Yeah, it's certainly a good toy that keeps you awake.

458
00:38:34,440 --> 00:38:35,640
It's like a good puzzle, right?

459
00:38:35,640 --> 00:38:40,640
I mean, so many, so many things that you can do with it and try different things.

460
00:38:40,640 --> 00:38:42,560
And it's just at its infancy.

461
00:38:42,560 --> 00:38:46,720
So it's our baby and we're trying to play with it and be happy.

462
00:38:46,720 --> 00:38:50,760
But at the same time, there are so many interesting applications that we foresee.

463
00:38:50,760 --> 00:38:57,880
I believe for defense and security, it has tremendous applications, especially at

464
00:38:57,880 --> 00:39:04,960
longer wavelengths, infrared wavelengths, mid-inferred wavelengths, where some of these

465
00:39:04,960 --> 00:39:11,640
thermal cameras or other types of focal plane arrays, I think those will benefit tremendously

466
00:39:11,640 --> 00:39:18,980
from an optical front end because omega pixel at those wavelengths, omega pixel image

467
00:39:18,980 --> 00:39:25,600
or at those wavelengths is very expensive and not every country has the no-have to fabricate

468
00:39:25,600 --> 00:39:27,000
something like that.

469
00:39:27,000 --> 00:39:31,080
So I think for defense, the applications are enormous.

470
00:39:31,080 --> 00:39:35,920
Overall, I'm very excited about one direction that I've been constantly thinking and that

471
00:39:35,920 --> 00:39:43,680
is to create low power and mobile hybrid systems that are powered at their front end with

472
00:39:43,680 --> 00:39:53,080
some all optical machine learning front end at the backside and inexpensive CMOS or CCD

473
00:39:53,080 --> 00:39:57,360
followed by a very modest low power neural net implementation.

474
00:39:57,360 --> 00:40:02,160
It can go very low power this way, it can be extremely fast this way, frame rates can

475
00:40:02,160 --> 00:40:07,240
be very high and at the same time we're talking about very modest form factor.

476
00:40:07,240 --> 00:40:13,160
So that obviously has to me some very interesting security applications at different parts

477
00:40:13,160 --> 00:40:15,480
of the electromagnetic spectrum.

478
00:40:15,480 --> 00:40:21,160
And I can obviously, because of my background, think enormous amount of biomedical problems

479
00:40:21,160 --> 00:40:27,240
that one can tackle with this kind of front end integrated with optoelectronic sensors

480
00:40:27,240 --> 00:40:28,840
and electronic neural nets.

481
00:40:28,840 --> 00:40:32,800
That's one area and the other area that I'm very excited about is actually nonlinear

482
00:40:32,800 --> 00:40:38,960
materials, meta materials, plasmonics, exotic material systems.

483
00:40:38,960 --> 00:40:44,880
As we discussed this in our paper that we published in science, the nonlinearity aspect

484
00:40:44,880 --> 00:40:48,400
of material science, optical nonlinearity is in material science.

485
00:40:48,400 --> 00:40:57,280
So I think opens up a huge platter of opportunities for enhancing the function of something like

486
00:40:57,280 --> 00:40:58,280
this.

487
00:40:58,280 --> 00:41:03,720
And that's where I think some of these metamaterial basic, exotic structures, plane by plane by

488
00:41:03,720 --> 00:41:09,960
plane, following each other, would really generalize some very sophisticated functions,

489
00:41:09,960 --> 00:41:17,200
maybe coming close to electronic neural nets with favorite, favorite choices of nonlinearities.

490
00:41:17,200 --> 00:41:20,560
We'll see how it goes, but this is another area where I'm very excited about it.

491
00:41:20,560 --> 00:41:24,680
Well, I don't want to thank so much for taking the time to walk us through what you're

492
00:41:24,680 --> 00:41:25,680
up to.

493
00:41:25,680 --> 00:41:29,040
This is really fascinating and it sounds very fun.

494
00:41:29,040 --> 00:41:32,480
So your group must have a good time playing with the stuff.

495
00:41:32,480 --> 00:41:33,480
Thank you.

496
00:41:33,480 --> 00:41:34,480
Really?

497
00:41:34,480 --> 00:41:35,480
Yeah.

498
00:41:35,480 --> 00:41:36,480
We're enjoying ourselves.

499
00:41:36,480 --> 00:41:37,480
I'm enjoying the time.

500
00:41:37,480 --> 00:41:38,480
Yeah.

501
00:41:38,480 --> 00:41:42,680
Thank you so much for having me.

502
00:41:42,680 --> 00:41:47,520
All right, everyone, that's our show for today for more information on I'dawan or any of

503
00:41:47,520 --> 00:41:53,920
the topics covered in this show, visit twomalai.com slash talk slash 237.

504
00:41:53,920 --> 00:42:21,360
As always, thanks so much for listening and catch you next time.

