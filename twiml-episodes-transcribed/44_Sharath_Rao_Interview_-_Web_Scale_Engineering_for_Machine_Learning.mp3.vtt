WEBVTT

00:00.000 --> 00:16.000
Hello and welcome to another episode of Twimal Talk, the podcast where I interview interesting

00:16.000 --> 00:20.920
people, doing interesting things in machine learning and artificial intelligence.

00:20.920 --> 00:23.520
I'm your host, Sam Charrington.

00:23.520 --> 00:28.120
This is the second show in our series of podcasts from the recent Wrangle Conference.

00:28.120 --> 00:32.680
As you might know, a few weeks ago I was in San Francisco for Wrangle, which is a great

00:32.680 --> 00:36.600
little conference brought to you by our friends over at Claudeira.

00:36.600 --> 00:38.960
Wrangle is such a fun event each year.

00:38.960 --> 00:43.720
It brings an interesting and diverse community of data scientists to an intimate and informal

00:43.720 --> 00:48.960
setting for great talks on real data science projects and issues, not to mention Cowboy

00:48.960 --> 00:50.560
Hatson BBQ.

00:50.560 --> 00:55.560
If you haven't caught the first episode in our Wrangle series, Twimal Talk No. 39 with

00:55.560 --> 00:57.080
Drew Conway.

00:57.080 --> 00:59.360
You'll want to be sure to check that out.

00:59.360 --> 01:03.800
It's a great interview and the intro includes important announcements about this series

01:03.800 --> 01:08.960
as well as our latest ticket giveaway, our online research paper discussion group, and

01:08.960 --> 01:10.600
my email newsletter.

01:10.600 --> 01:14.600
The show you're listening to now features my interview with Sharath Rao.

01:14.600 --> 01:19.040
I reached out to Sharath about being on the show and was blown away when he replied that

01:19.040 --> 01:24.760
not only had he heard about the show, but that he was a fan and an avid listener.

01:24.760 --> 01:29.160
My conversation with him digs into some of the practical lessons and patterns he's learned

01:29.160 --> 01:34.960
by building production ready, web scale data products based on machine learning models,

01:34.960 --> 01:38.640
including the search and recommendation systems at Instacart.

01:38.640 --> 01:43.360
A quick note before we dive in, as is the case with my other field recordings, there's

01:43.360 --> 01:46.400
a bit of unavoidable background noise in this interview.

01:46.400 --> 01:57.880
Sorry about that, and now on to the show.

01:57.880 --> 02:01.040
All right everyone, I am here with Sharath Rao.

02:01.040 --> 02:08.560
He's an engineering manager at Instacart, and we are on location at the Wrangle conference,

02:08.560 --> 02:14.680
and Sharath has a talk later on, and I'm fortunate enough to have him here to tell us a little

02:14.680 --> 02:18.360
bit about what he's going to tell the Wrangle audience about.

02:18.360 --> 02:21.440
So Sharath, welcome to the show, it's great to have you.

02:21.440 --> 02:27.000
Great, thanks for having me Sam, I'm a big fan of the show from having heard earlier episodes.

02:27.000 --> 02:29.680
Nice, nice, well I really appreciate that.

02:29.680 --> 02:33.600
As you know, one of the places I like to get started then is to have folks introduce

02:33.600 --> 02:38.240
themselves, tell us a little bit about what you're up to, and how you got there.

02:38.240 --> 02:39.720
Yeah, yeah definitely.

02:39.720 --> 02:45.640
So I'm at Instacart, I've been here for a couple of years, Instacart is an on-demand grocery

02:45.640 --> 02:52.760
delivery service, and my role here is, I started off as the data scientist or machine learning

02:52.760 --> 02:59.400
engineer focused on search personalization and recommendations, and now I do that, but

02:59.400 --> 03:03.360
also lead a team that is working on that effort.

03:03.360 --> 03:09.440
So prior to this, I've spent time at a couple of companies doing search, advertising,

03:09.440 --> 03:13.840
and auctions, that sort of stuff over the last 10 years.

03:13.840 --> 03:19.360
And even going back further, grad school, I worked on speech recognition and speech translation

03:19.360 --> 03:22.760
before it was practically useful in the field.

03:22.760 --> 03:28.560
That's part of the reason why I started working on other things after grad school.

03:28.560 --> 03:31.720
So yeah, so that's probably captures the range of things.

03:31.720 --> 03:32.720
Awesome, awesome.

03:32.720 --> 03:37.000
I hear a lot of stories from folks who, you know, I was working on this stuff in grad school

03:37.000 --> 03:42.000
and it just wasn't ready, or we were in the middle of the AI winter, and it kind of went

03:42.000 --> 03:43.000
into hibernation.

03:43.000 --> 03:49.240
And now that we're all so focused on doing this stuff, it's like time to dust off those

03:49.240 --> 03:50.240
skills.

03:50.240 --> 03:51.240
Absolutely.

03:51.240 --> 03:52.240
Yeah.

03:52.240 --> 04:00.480
So in your description, you said data science, slash machine learning engineering, how

04:00.480 --> 04:05.640
evolved is the distinction between the two of those at Instacart, and do you consider

04:05.640 --> 04:07.360
yourself as spanning both?

04:07.360 --> 04:08.800
Yeah, let me take the first one first.

04:08.800 --> 04:15.000
I think at Instacart, well, or was industry, I think we are moving better, you know, towards

04:15.000 --> 04:17.040
understanding these two roles.

04:17.040 --> 04:22.240
And so at least we are sort of settling around maybe three roles, actually data analysts,

04:22.240 --> 04:25.320
data scientists, and data engineers.

04:25.320 --> 04:29.080
Even if they're slightly called separately, I think maybe their companies like maybe LinkedIn

04:29.080 --> 04:35.600
or maybe NB where probably have data scientists and machine learning engineers and data engineers.

04:35.600 --> 04:41.640
But broadly, the difference is around, you know, people working on data products, building

04:41.640 --> 04:46.520
systems and algorithms, essentially things that are consumed by other algorithms and systems

04:46.520 --> 04:50.800
who might be like machine learning engineers, people working on things, the output of which

04:50.800 --> 04:55.840
is used for, you know, decision making, you know, by the team that they serve or the executive

04:55.840 --> 04:56.840
leadership.

04:56.840 --> 05:00.840
We call them data analysts at Instacart, but data scientists elsewhere, and then finally

05:00.840 --> 05:06.280
data engineers who are working on like platform infrastructure typically comes in, you know,

05:06.280 --> 05:07.880
maybe later in the company stage.

05:07.880 --> 05:08.880
Right.

05:08.880 --> 05:09.880
Right.

05:09.880 --> 05:15.200
I think it's a lot of progress we've made from just a few years ago, and, you know, data

05:15.200 --> 05:19.320
scientists was this unicorn that no one could really hire because we've defined it in

05:19.320 --> 05:24.760
such a way that, you know, it requires these disparate skills that, you know, aren't traditionally

05:24.760 --> 05:25.760
paired.

05:25.760 --> 05:31.920
Yeah, I remember back in maybe 2012, 2013, the term of unicorn data scientist was probably

05:31.920 --> 05:35.560
one that captures what he just described, but I don't hear that too often.

05:35.560 --> 05:39.240
There's more like specialization and people understand that over time, most professions

05:39.240 --> 05:43.120
end up specializing, although I'd imagine, in depends on the context of the companies,

05:43.120 --> 05:46.720
I would imagine say startups, if they're hiring their first data scientist, they probably

05:46.720 --> 05:51.840
want somebody who can also do like the other two things that are reasonable level of competence.

05:51.840 --> 05:56.320
Overall, in a macro view, I think, the roles have sort of specialized over time.

05:56.320 --> 05:57.320
Yeah.

05:57.320 --> 06:03.160
And I remember back in that same time period, 2012, I think as a community, we thought

06:03.160 --> 06:07.880
that it would take way longer for us to reach this level of maturity, because like you'd

06:07.880 --> 06:12.380
see these stats that say, you know, we're going to have, you know, we're going to be under

06:12.380 --> 06:17.320
resource in terms of the number of data scientists until like the year 2050 or something like

06:17.320 --> 06:18.320
that.

06:18.320 --> 06:26.080
I think this specialization has been part of the key for alleviating that stress, although

06:26.080 --> 06:32.000
we're far from having enough, you know, people with data competencies to meet all the

06:32.000 --> 06:33.000
demand.

06:33.000 --> 06:34.000
Right.

06:34.000 --> 06:37.360
I think every speaker at an event like this gets up and says, oh, and by the way, we're hiring.

06:37.360 --> 06:38.360
Yes.

06:38.360 --> 06:39.360
Right.

06:39.360 --> 06:40.360
Yeah.

06:40.360 --> 06:42.400
So I've heard that and also more recently I've heard about and maybe even seen it in

06:42.400 --> 06:47.240
the field where there's, you know, with, you know, obviously the market has responded to

06:47.240 --> 06:50.960
at least a perceived lack of, you know, strong talent.

06:50.960 --> 06:55.880
And so we have quick, like smaller one of your masters or bootcamps that have served the

06:55.880 --> 06:56.880
need.

06:56.880 --> 07:00.320
So as a result, I think in maybe in some part of the market, they might, maybe at like

07:00.320 --> 07:05.600
optimum or even oversupply of, you know, data scientists at arguably at starting level.

07:05.600 --> 07:06.600
Really?

07:06.600 --> 07:08.240
Well, that's happening quickly.

07:08.240 --> 07:13.960
Maybe I mean, just generally that I guess everything is moving faster and now even for

07:13.960 --> 07:19.840
example, with newer areas around, say, deep learning and AI, there is, you know, there

07:19.840 --> 07:24.760
are more research institutes, there are more, again, training, you know, workshops and

07:24.760 --> 07:30.240
so on that people respond faster to needs than maybe they used to.

07:30.240 --> 07:31.240
Mm-hmm.

07:31.240 --> 07:32.240
Interesting.

07:32.240 --> 07:33.240
All right.

07:33.240 --> 07:34.240
So you talk.

07:34.240 --> 07:35.240
Yes.

07:35.240 --> 07:36.240
You've got to talk in an hour.

07:36.240 --> 07:37.240
Yeah.

07:37.240 --> 07:38.240
What's your talk about?

07:38.240 --> 07:39.240
Sure.

07:39.240 --> 07:43.240
So my talk is title lessons from integrating data, machine learning models into data products.

07:43.240 --> 07:47.000
So initially I had a two part talk, but you know, today I'm talking about like the part

07:47.000 --> 07:48.200
one of that.

07:48.200 --> 07:54.960
But really the, the genesis of that comes from having worked at first of all, you know,

07:54.960 --> 07:58.800
relatively a smaller company as opposed to some of the bigger companies that were in

07:58.800 --> 08:04.880
we have, there is a product per se and there are machine learning models that are integrated

08:04.880 --> 08:09.920
into various parts of the product that are making different decisions for the customers.

08:09.920 --> 08:16.520
So there's that and then there is aspects about like data scientists building models, model

08:16.520 --> 08:22.360
prototypes and even productionizing them, but also interfacing with product engineers

08:22.360 --> 08:24.320
within a single team.

08:24.320 --> 08:28.920
And the fact that what I call like all model, all model prototypes look familiar, but every

08:28.920 --> 08:32.360
model and production is unique in its own way.

08:32.360 --> 08:37.480
So I'm trying to understand, try to frame a conversation about what does it mean to have

08:37.480 --> 08:38.480
a model and production?

08:38.480 --> 08:44.360
You know, what are the questions to ask yourself and how do we communicate as data scientists

08:44.360 --> 08:50.480
with in a product managers and product engineers about what this model does, what are its requirements,

08:50.480 --> 08:52.240
what are the constraints?

08:52.240 --> 08:54.560
And so that's, that's what I'll be talking about.

08:54.560 --> 08:58.840
Okay, so how do you, how do you structure walking people through that?

08:58.840 --> 09:04.000
Yeah, I try to keep it, try to find, you know, going back to conversations within the

09:04.000 --> 09:08.440
team, for example, each time we, we have one team that has, you know, three or four product

09:08.440 --> 09:13.000
engineers, you know, a couple of data scientists in a designer and a PM, right?

09:13.000 --> 09:16.280
And largely most of the work happens, you know, within the team.

09:16.280 --> 09:22.000
And each time we work on an experiment or a feature that that data's data scientists build,

09:22.000 --> 09:25.600
we have this conversation about how will this go into production?

09:25.600 --> 09:29.960
And typically the conversations, you know, if you think about it like the process of building

09:29.960 --> 09:34.920
a model, thinking about a problem, building a model prototype, largely happens, you know,

09:34.920 --> 09:39.280
within the domain of data science and maybe, you know, with some interaction at the PM.

09:39.280 --> 09:42.240
But the moment it goes into production, it's now touching like so many different parts

09:42.240 --> 09:43.240
of the system.

09:43.240 --> 09:47.680
And, you know, a couple of things come in like, how much time do we have to make, make

09:47.680 --> 09:51.920
a decision in terms of like the model, you know, running in production, you know, many

09:51.920 --> 09:55.800
if you're serving up pages live to the web, you know, you've got to serve, yeah, serving

09:55.800 --> 09:58.200
latency, for example, yeah, address right?

09:58.200 --> 10:02.000
Yeah, so there's a constraint that very often gets dictated by the product itself and

10:02.000 --> 10:03.480
rightly so.

10:03.480 --> 10:07.920
And then there's this question of, you know, for the model to be successful, how much information

10:07.920 --> 10:09.400
does it need?

10:09.400 --> 10:10.760
And how can I get that information?

10:10.760 --> 10:15.200
By information, I mean like, for example, let's say you're trying to recommend a product.

10:15.200 --> 10:18.880
Maybe you definitely need to know if you want to, you know, it's a personalized recommendation,

10:18.880 --> 10:24.040
you need to know the user, you know, user identity of their past data, you probably need

10:24.040 --> 10:26.640
to know like what page they're on and so on.

10:26.640 --> 10:30.920
But do you really need to know their recent searches and, you know, recent activity, like

10:30.920 --> 10:32.840
that certainly is like short-term context.

10:32.840 --> 10:39.040
So how much context do we need, does the model need rather to operate successfully?

10:39.040 --> 10:44.040
Both of these sort of give you a way to think about, can I cash my recommendations?

10:44.040 --> 10:48.120
You know, what can I cash versus what needs to be served in real time?

10:48.120 --> 10:53.480
If it's real time, can I do a reasonably well if, you know, let's say, let's say I'm making

10:53.480 --> 11:00.000
a decision about what product to show based on things that you've added to the cart recently.

11:00.000 --> 11:05.600
So think of a model that is continuously producing a, you know, prediction of what, what part

11:05.600 --> 11:07.400
is a good recommendation?

11:07.400 --> 11:08.800
But it doesn't necessarily surface that.

11:08.800 --> 11:13.200
It's always, you know, continuing to score and maintain a best estimate of a recommendation

11:13.200 --> 11:14.200
in the background.

11:14.200 --> 11:18.000
But, you know, at any point it's available with a best effort response.

11:18.000 --> 11:24.240
So this is an example where you don't, you know, you have a good response whenever there's

11:24.240 --> 11:25.240
a need for it.

11:25.240 --> 11:28.000
We can wait until the last moment to actually serve that recommendation.

11:28.000 --> 11:30.520
So those things that happen in the background.

11:30.520 --> 11:36.480
Are you describing a system that is doing like online learning or active learning, which

11:36.480 --> 11:40.280
is where you're updating your model kind of in place?

11:40.280 --> 11:41.280
Well, yeah.

11:41.280 --> 11:46.080
So the model could be static, model need not be updated, but the model predictions could

11:46.080 --> 11:51.520
be happening continuously in the background based on, based on whatever data it sees.

11:51.520 --> 11:55.480
It doesn't need to score in immediately reporting, it can keep scoring as it gets more data

11:55.480 --> 11:59.000
and then always be ready to serve a recommendation when necessary.

11:59.000 --> 12:00.000
Okay.

12:00.000 --> 12:03.760
So that gives us like, you know, so we have this, you know, four quadrant because I'm considering

12:03.760 --> 12:07.680
latency and context sensitivity of the model behavior.

12:07.680 --> 12:12.360
And we get these four quadrants where, you know, you obviously have like high latency is

12:12.360 --> 12:16.600
okay, but you have context sensitive and the other, basically, the other four possibilities,

12:16.600 --> 12:18.600
other three possibilities.

12:18.600 --> 12:25.720
And so it's part of what you've observed, then, you know, are we at the point of maturity

12:25.720 --> 12:30.120
and thinking about this that, you know, for each of these quadrants, we've got like design

12:30.120 --> 12:35.520
patterns or best practices that teams, you know, either at InstaCard or in the industry

12:35.520 --> 12:36.520
are following?

12:36.520 --> 12:40.520
Certainly, InstaCard, we try to place ourselves and don't like, where does this model sort

12:40.520 --> 12:42.960
of lie, you know, just mentally and internally.

12:42.960 --> 12:48.240
And that's sort of all, given that we've built a few experiences, few models over the past

12:48.240 --> 12:54.080
a couple of years, we have patterns, you know, we know, we sort of know 90% of, you know,

12:54.080 --> 12:57.880
the way the model is integrated in the rest of the product, when we are thinking about

12:57.880 --> 13:01.200
the model, we don't have to like start from scratch each time.

13:01.200 --> 13:06.640
So for example, search, search ranking, you know, we latency needs to be really low.

13:06.640 --> 13:10.800
And context sensitivity, we can start off with, you know, low context sensitivity, I mean,

13:10.800 --> 13:14.920
let's say you're simply matching the search query and the bunch of products without like

13:14.920 --> 13:15.920
recent context.

13:15.920 --> 13:19.760
If you could start there, then as we, the way to improve the model would be to, well,

13:19.760 --> 13:25.240
continue to have latency requirement being low, but make it more context sensitive, obviously

13:25.240 --> 13:29.360
there's a lot of work that might involve a model that is ranking, re-ranking products

13:29.360 --> 13:32.000
based on, you know, recent activity.

13:32.000 --> 13:33.000
So that's one example.

13:33.000 --> 13:37.160
So with search, we start, I call it we start from quadrant four and go to quadrant one.

13:37.160 --> 13:43.240
Sort of will be clear from the slides, I guess, but with the recommendations, it's almost

13:43.240 --> 13:47.400
always the case that you start with, you know, in start in quadrant three, where you cache

13:47.400 --> 13:51.920
as much as possible, you score them, you know, in batch mode, you cache them and then you

13:51.920 --> 13:53.000
serve.

13:53.000 --> 13:57.840
So that keeps, that gives you, you know, you can use as much information as you want,

13:57.840 --> 14:02.760
and you know, gives you the latitude to use more data and have like recommendations.

14:02.760 --> 14:07.160
The way to improve that would be to go from quadrant four to, or quadrant three to two,

14:07.160 --> 14:12.040
I guess, where, you know, you still, you know, latency is still not a consideration.

14:12.040 --> 14:17.800
You're re-ranking your products, your, you know, candidate products, and, you know,

14:17.800 --> 14:20.000
in the background, be very, very, to serve.

14:20.000 --> 14:22.960
And then finally, you might have like, you know, contextual recommendations wherein you

14:22.960 --> 14:28.920
have to recommend something, you know, immediately, and so that becomes like a real-time recommendation,

14:28.920 --> 14:34.440
which is, you know, a model that, you know, re-ranks a set of products that you've cached,

14:34.440 --> 14:35.720
you know, based on the context.

14:35.720 --> 14:39.160
So the set of products you've cached may be personalized or maybe not personalized, but

14:39.160 --> 14:44.320
then when you serve, you re-rank it based on user activity and make it personalized.

14:44.320 --> 14:45.320
Yeah.

14:45.320 --> 14:53.120
So how much of this machine learning engineering is machine learning and how much is engineering,

14:53.120 --> 15:00.480
meaning a lot of what we're talking about is, you know, system level, scalability, like,

15:00.480 --> 15:05.320
you know, just hardcore software engineering that, you know, in a lot of ways is orthogonal

15:05.320 --> 15:07.120
to the machine learning.

15:07.120 --> 15:11.400
Well, I guess the model stuff isn't, but in a lot of ways it seems orthogonal.

15:11.400 --> 15:15.960
There are principles at least that seem orthogonal to the thing that you're scaling, right?

15:15.960 --> 15:19.920
It's, it's web-scale engineering, yeah, applied to M-O.

15:19.920 --> 15:25.240
So like fine tune that statement, how much of it, you know, a specific is domain specific

15:25.240 --> 15:28.920
and where are those points that it gets super domain specific?

15:28.920 --> 15:31.160
That's a really good question.

15:31.160 --> 15:35.960
And I think the point at which it gets domain specific is that, as a data scientist,

15:35.960 --> 15:40.400
you understand the model, its assumptions, its requirements.

15:40.400 --> 15:43.080
At what point, you know, it'll fail to perform.

15:43.080 --> 15:44.320
What does it absolutely need?

15:44.320 --> 15:47.880
What is table stakes versus, you know, what is a cherry on the top?

15:47.880 --> 15:51.160
You understand that and you, well, you need to understand that and then articulate it

15:51.160 --> 15:55.840
to, you know, product engineers and to others on the team.

15:55.840 --> 15:58.920
Obviously, you don't want to do that right at the end when you've already built the model

15:58.920 --> 16:02.040
but you want to do it right at the beginning so that you can know up front if there are

16:02.040 --> 16:04.640
any constraints on the engineering side.

16:04.640 --> 16:05.640
Right.

16:05.640 --> 16:10.600
So it is sort of good mix of machine learning, you know, and software engineering.

16:10.600 --> 16:14.760
And even product design, frankly, you know, I'd imagine we've one of one of the things

16:14.760 --> 16:22.080
you found, for example, is we may have some latitude in how we change the product experience

16:22.080 --> 16:26.760
for a user based on some of the constraints on the engineering and machine learning

16:26.760 --> 16:27.760
side.

16:27.760 --> 16:32.320
For example, if you want to buy yourself time to be able to score, what would that interface?

16:32.320 --> 16:34.520
What will we do with the customer at that time?

16:34.520 --> 16:37.000
How do we design the product experience?

16:37.000 --> 16:38.000
Yeah.

16:38.000 --> 16:45.000
Interesting, I've been, I've talked to a few people about this, like, I think there's

16:45.000 --> 16:50.820
an evolving field of like, I always call it intelligent design but that's like way over

16:50.820 --> 16:58.560
loaded but there needs to be, and I think there will be, you know, some, you know, thought

16:58.560 --> 17:04.240
best practices for lack of a better term, like just a way of thinking about designing

17:04.240 --> 17:09.800
kind of in light of machine learning and intelligence and things like that, that, you know, I don't

17:09.800 --> 17:13.040
see a lot of people talking about that kind of stuff yet.

17:13.040 --> 17:16.600
And it's not necessarily exactly what you're describing here, what you're describing is

17:16.600 --> 17:21.680
even, you know, kind of going back to the previous question, it's really more the interaction

17:21.680 --> 17:26.800
between, you know, design and product engineering and like performance engineering and things

17:26.800 --> 17:31.600
like that, like how does our, how does the system, how does a user experience change because

17:31.600 --> 17:36.800
of the limitations, you know, in, you know, performance and product engineering and stuff

17:36.800 --> 17:37.800
like that?

17:37.800 --> 17:44.400
Yeah, I definitely think it's definitely controversial to suggest that we have to change user experiences

17:44.400 --> 17:51.360
to work with the limitations of, you know, engineering or machine learning, right?

17:51.360 --> 17:57.000
And it may not be common to do that but I wish I had like more examples where it's

17:57.000 --> 18:01.840
possible to do maybe, maybe it's not something that we generally talk about, if indeed it happens

18:01.840 --> 18:02.840
in the field.

18:02.840 --> 18:06.640
And that's the other thing, which is like, it's hard to talk like about machine learning

18:06.640 --> 18:11.080
models and production, widely, simply because there are a lot of details that are probably

18:11.080 --> 18:14.320
very specific to the product and, you know, one just assumes that it's not interesting

18:14.320 --> 18:17.880
to somebody else or that these details don't really generalize.

18:17.880 --> 18:22.280
So there is no, there are definitely basic principles, you know, software engineering,

18:22.280 --> 18:27.960
but beyond that are there other principles that are specific or that are more important

18:27.960 --> 18:33.000
when it comes to like software engineering with like machine learning systems or data products.

18:33.000 --> 18:37.720
So I called out a couple of examples of some work in the community like recently, maybe

18:37.720 --> 18:42.360
about early this year, there was a document from one of the machine learning engineers

18:42.360 --> 18:46.360
at Google about, I think it's called rules of machine learning or so.

18:46.360 --> 18:50.760
It talks about a really nice, what I think is it quite reading for data scientists building

18:50.760 --> 18:52.080
data products.

18:52.080 --> 18:55.960
And what are the different things, what are the different trade-offs, what are different

18:55.960 --> 18:59.360
stages of building a data product?

18:59.360 --> 19:06.240
And talks a lot about the engineering aspects, the training, the data management, metrics,

19:06.240 --> 19:11.440
the interface between like metrics and our model formulations and so on.

19:11.440 --> 19:16.360
It's kind of like, you know, it's a four-page PDF with such a density of knowledge.

19:16.360 --> 19:20.240
Like, at Instacard, we had like a reading group session, we spent like, I let a group

19:20.240 --> 19:25.200
with like two hours of going through that and they're talking through like salient ideas

19:25.200 --> 19:26.200
from that paper.

19:26.200 --> 19:27.200
Okay.

19:27.200 --> 19:30.360
Well, I have to make sure I get a link for that one and post it up in the show.

19:30.360 --> 19:31.360
Definitely, yeah.

19:31.360 --> 19:34.560
There's one more people that are like, quickly, which from Pinterest, where they talk about

19:34.560 --> 19:38.200
the evolution of the recommendation systems over three years.

19:38.200 --> 19:40.800
And this is, it's a really good one, I think.

19:40.800 --> 19:45.920
It talks about, incidentally, it talks about how they started it off with a simple system

19:45.920 --> 19:50.000
that was like caching recommendations of the Quadrant 3 world.

19:50.000 --> 19:55.240
And then from the how they progress from that to, you know, Quadrant 1, where they use

19:55.240 --> 19:58.320
what you get, what you cash, and then you re-rank it with a model.

19:58.320 --> 20:03.480
It's helpful to have this, you know, view of how this really massive, you know, at-scale

20:03.480 --> 20:07.960
recommendation system evolved so that, you know, people who haven't done that before

20:07.960 --> 20:11.880
don't need to believe that we need to start with what Pinterest has in production after

20:11.880 --> 20:12.880
three years of work.

20:12.880 --> 20:13.880
Right.

20:13.880 --> 20:14.880
Right.

20:14.880 --> 20:17.280
So, it's sort of underappreciated part of literature, I think.

20:17.280 --> 20:18.280
Yeah.

20:18.280 --> 20:22.920
So, this is maybe a little bit of a tangent, but I don't know if you've heard on some

20:22.920 --> 20:28.360
recent podcasts I've mentioned that I'm going to be starting a, like, an online paper

20:28.360 --> 20:31.160
reading group of podcast listeners.

20:31.160 --> 20:36.200
I mentioned it randomly in a podcast, and like, there were a bunch of people who expressed

20:36.200 --> 20:37.200
interest in it.

20:37.200 --> 20:41.480
So, I'm going to be getting that kicked off, but since you mentioned that you are involved

20:41.480 --> 20:46.840
in one at Instacart, any tips for running a paper reading group?

20:46.840 --> 20:47.840
Great.

20:47.840 --> 20:48.840
So, yeah, let me see.

20:48.840 --> 20:52.960
We have, like, two different sessions where all the data scientists come together, you

20:52.960 --> 20:57.280
know, sort of alternate weeks, and one of them is, or the one that I was talking about,

20:57.280 --> 21:01.320
we call that lunch and learn, we have, we do it over lunch every Tuesday.

21:01.320 --> 21:06.680
And the bar, in a sense, is that, you know, I had a time we talk about, like, what paper

21:06.680 --> 21:09.080
we are, what somebody is going to lead a discussion.

21:09.080 --> 21:13.560
That person doesn't need to have completely understood, but it's helpful for them to be

21:13.560 --> 21:16.200
able to know enough to drive the discussion.

21:16.200 --> 21:20.200
It's obviously optional for, you know, people who want to attend, because, you know, we

21:20.200 --> 21:24.040
have, we touch different domains in Instacart so somebody may not be particularly, you know,

21:24.040 --> 21:26.200
keen on a paper.

21:26.200 --> 21:29.960
And, but then it helps to have, like, discussion, you know, if you come in, make sure you've

21:29.960 --> 21:33.560
skimmed the paper and come in for discussion.

21:33.560 --> 21:37.520
We talk about what the paper, some of the, you know, what it takes away from the paper,

21:37.520 --> 21:41.000
and also, like, how that immediately might apply to what we're working on, you know, in

21:41.000 --> 21:42.000
our regular work.

21:42.000 --> 21:44.040
How long did they turn around?

21:44.040 --> 21:48.320
It's a one-hour, and we typically have anything from, like, eight to twelve people and

21:48.320 --> 21:53.040
odd, you know, group, you know, I would say maybe, like, four to five of them will live,

21:53.040 --> 21:54.680
like, let's put the paper a little bit.

21:54.680 --> 21:55.680
Yeah.

21:55.680 --> 22:01.880
But I think these are opportunities for people to not just learn, but also lead a discussion

22:01.880 --> 22:05.320
and people get better at it as they go.

22:05.320 --> 22:06.320
Yeah.

22:06.320 --> 22:07.320
Yeah.

22:07.320 --> 22:08.320
Okay.

22:08.320 --> 22:09.320
Well, I'm really looking forward to it.

22:09.320 --> 22:12.120
And, like I said, a lot of people, a lot of other people seem to be looking forward to

22:12.120 --> 22:13.120
it as well.

22:13.120 --> 22:14.120
Yeah.

22:14.120 --> 22:17.840
Also, have you joined us and then walk us to a paper over here of your choice?

22:17.840 --> 22:18.840
Really, yeah, definitely.

22:18.840 --> 22:22.480
Yeah, I think it's very common in, like, grad school and, like, research labs to be,

22:22.480 --> 22:26.880
like, doing this, like, reading the paper is an art and understanding, like, what is

22:26.880 --> 22:30.760
the next, what are the assumptions, what are the achieved, what are the opportunities

22:30.760 --> 22:37.040
from here, like, how can we build on that, what is relevant to us, how hard is it to reproduce,

22:37.040 --> 22:41.160
you know, in our environment, right, that, you know, what are the generalizable lessons,

22:41.160 --> 22:42.160
so to speak.

22:42.160 --> 22:43.160
Yeah.

22:43.160 --> 22:44.160
Those are sort of the questions we are thinking about.

22:44.160 --> 22:45.160
Yeah.

22:45.160 --> 22:46.160
Yeah.

22:46.160 --> 22:50.920
So, kind of going back to your presentation, what are the generalizable lessons from your

22:50.920 --> 22:51.920
presentation?

22:51.920 --> 22:52.920
Sure.

22:52.920 --> 23:00.760
So, one of them is to think deeply about how the model integrates into your product.

23:00.760 --> 23:06.120
Have the discussion ahead of time with people outside, you know, your own group, like, off

23:06.120 --> 23:10.040
data scientists, maybe, you know, product managers, product engineers, like, the kickoff

23:10.040 --> 23:12.360
meetings should probably talk about that.

23:12.360 --> 23:16.040
And at what level are you having that conversation, because there's integration from a perspective

23:16.040 --> 23:20.400
of data, there's integration from a perspective of data sources, APIs.

23:20.400 --> 23:21.400
Yeah.

23:21.400 --> 23:23.800
Generally, at the level of APIs.

23:23.800 --> 23:24.800
Okay.

23:24.800 --> 23:30.960
And it's also a way to sort of get a buy-in from product engineers, build that relationship

23:30.960 --> 23:35.440
so that, you know, think shouldn't look like you're just throwing things or the fence

23:35.440 --> 23:38.040
to the engineer and asking them to, like, productionize it.

23:38.040 --> 23:44.680
I think, for them to understand it in a way that is at a level that they can even provide

23:44.680 --> 23:48.880
an input to and, you know, contribute to improve the design, like, the technical implementation

23:48.880 --> 23:50.560
would be helpful.

23:50.560 --> 23:54.440
And I picked out, like, two different aspects of, like, latency and context sensitivity.

23:54.440 --> 23:55.440
There might be others.

23:55.440 --> 23:57.840
I mean, I picked up two and I got a four quadrant hub.

23:57.840 --> 23:58.840
Yeah.

23:58.840 --> 24:04.360
We can pick, look at, you know, more aspects of it that are generalizable across models.

24:04.360 --> 24:09.800
And, yeah, this, I hope it sort of evolves into a discussion about how we can, you know,

24:09.800 --> 24:16.000
we have ways to talk about model prototypes, you know, how a problem is set up and how

24:16.000 --> 24:20.400
training data is generated, you know, at someone, you know, the model is fit and we persist

24:20.400 --> 24:26.480
the model we have, you know, abstractions about what happens, you know, in training phases.

24:26.480 --> 24:32.800
But abstractions that help us understand how a model is, you know, being served or the

24:32.800 --> 24:38.280
model predictions are being served and, you know, how it's actually executed and implemented

24:38.280 --> 24:39.600
might be helpful.

24:39.600 --> 24:40.600
Mm-hmm.

24:40.600 --> 24:45.800
Yeah, I keep coming back to this idea of design patterns, like, you know, you may be familiar

24:45.800 --> 24:50.400
with the gang of four design patterns book where there's, you know, they've cataloged,

24:50.400 --> 24:54.720
I don't remember the year, but, you know, cataloged a lot of, I've been oriented design

24:54.720 --> 24:55.720
principles.

24:55.720 --> 24:58.280
Like, are we there yet with machine learning or?

24:58.280 --> 24:59.280
Great question.

24:59.280 --> 25:04.600
And I think I probably had a, I tweeted this question a few, maybe, maybe a year ago,

25:04.600 --> 25:09.800
like, what sort of abstractions do we have for, you know, serving machine learning models

25:09.800 --> 25:10.800
in production?

25:10.800 --> 25:11.800
Right.

25:11.800 --> 25:12.800
Right.

25:12.800 --> 25:19.120
I don't know if any that is, that substantially builds on, like, you know, software engineering

25:19.120 --> 25:22.360
patterns, like, or I don't know if anything that the community has agreed on.

25:22.360 --> 25:24.760
I'm sure there are a lot of things that in people's heads.

25:24.760 --> 25:25.760
Yeah.

25:25.760 --> 25:28.520
And a little bit of that I'm talking about today will be about, like, what I, you know,

25:28.520 --> 25:31.400
we've seen and working in our team.

25:31.400 --> 25:34.880
But going back to the Google doc, I think that, again, although it talks about more than

25:34.880 --> 25:39.040
just serving, I think we need more of that in the community, yeah.

25:39.040 --> 25:44.480
To mention that there isn't enough generalizable because things are just too coupled with the,

25:44.480 --> 25:48.360
with the product is probably not, not true.

25:48.360 --> 25:49.360
Hmm.

25:49.360 --> 25:53.760
Yeah, you made an interesting comment early in the conversation about how you have a lot

25:53.760 --> 25:58.040
of these machine learning models in production, but they, you know, at some level, they all

25:58.040 --> 26:01.360
seem like snowflakes, I guess, is what I took from that.

26:01.360 --> 26:05.800
And that, you know, it's been, it's been an effort and a challenge to kind of extract

26:05.800 --> 26:10.480
the general principles from across these different environments, is that kind of what you

26:10.480 --> 26:11.480
were getting at?

26:11.480 --> 26:12.480
Yeah.

26:12.480 --> 26:14.480
Insofar as we're talking about, like, implementation.

26:14.480 --> 26:15.480
Right.

26:15.480 --> 26:16.480
Yeah.

26:16.480 --> 26:17.480
Right.

26:17.480 --> 26:18.720
That's all in different problems, but there might be, like, in terms of, like, software

26:18.720 --> 26:23.960
engineering patterns, they share, like, some of these aspects that they then, in one

26:23.960 --> 26:25.360
of these quadrants.

26:25.360 --> 26:32.720
And are there common implementation slash architectural patterns that, you know, that are kind

26:32.720 --> 26:37.840
of assumptions for you guys that, you know, you're doing across all, at least all new

26:37.840 --> 26:38.840
efforts.

26:38.840 --> 26:42.960
Like, for example, microservices, are you doing microservices, containers, are you doing

26:42.960 --> 26:43.960
containers?

26:43.960 --> 26:49.360
Like, does any of that, where does the kind of evolution of the state of the art and, you

26:49.360 --> 26:54.000
know, software engineering for non-ML stuff intersect with ML stuff?

26:54.000 --> 26:55.000
Good question.

26:55.000 --> 27:01.640
And, well, at the early point, yeah, we have services that are, well, we can call them

27:01.640 --> 27:06.080
microservices, I guess, but, yeah, we have, you know, engineers, product engineers and

27:06.080 --> 27:10.960
mission engineers agree on, you know, certain contracts.

27:10.960 --> 27:14.360
And so that is, like, table stakes.

27:14.360 --> 27:19.480
There are questions about, like, what can we cache, what can we not cache?

27:19.480 --> 27:23.920
There are questions about, well, yeah, how long can we cache something, you know, what

27:23.920 --> 27:30.320
sort of, what sort of data stores we might use, and what is configurable, what is not, what

27:30.320 --> 27:34.120
is, yeah, what is an experiment versus what is not.

27:34.120 --> 27:39.920
Most things are experiments, how much do we expect to iterate on this feature before,

27:39.920 --> 27:45.400
you know, we think we're done for a while, which is, I think, for a company like Instacart,

27:45.400 --> 27:48.160
where there are a lot of different places we can invest in.

27:48.160 --> 27:51.800
It might be, like, quite a few months before we come back to, you know, we too of something

27:51.800 --> 27:52.800
that we built.

27:52.800 --> 27:53.800
Yeah.

27:53.800 --> 27:57.280
Because, you know, we're still exploring the space of areas where machine learning can

27:57.280 --> 28:01.200
help, and, you know, where help the most, I guess, yeah.

28:01.200 --> 28:06.160
But do you, to a point about, like, beyond software editing, what is new, yeah, I don't

28:06.160 --> 28:07.640
think I have an answer right now.

28:07.640 --> 28:08.640
Okay.

28:08.640 --> 28:09.640
Okay.

28:09.640 --> 28:16.000
To follow up on your last comment on just the opportunity prioritization, at Instacart

28:16.000 --> 28:23.000
are you generally taking an approach of, like, trying to take on, you know, big moonshot

28:23.000 --> 28:28.560
types of problems and, you know, make a few small big bets to kind of, you know, ensure

28:28.560 --> 28:32.360
that your resources are focused on, you know, these things that could have outsized

28:32.360 --> 28:36.840
an impact, or is it more, you know, we're going to try to, you know, we're going to try

28:36.840 --> 28:43.320
to touch, you know, broadly, you know, in, you know, high impact, but more concentrated

28:43.320 --> 28:48.800
ways to kind of spread that impact around, you know, the systems in the various business

28:48.800 --> 28:49.800
teams and processes.

28:49.800 --> 28:50.800
Yeah.

28:50.800 --> 28:57.480
I think there's probably the, there's a top down and bottom up in terms of what sort of

28:57.480 --> 29:03.080
projects get worked on and explored, you know, within, you know, within, say, data

29:03.080 --> 29:06.880
science for, just to stay in data science for a moment.

29:06.880 --> 29:11.200
So we have company goals about, like, what are we, quarterly goals, long-term goals,

29:11.200 --> 29:16.800
translate the company goals, which typically talk, you know, are aligned to some metrics,

29:16.800 --> 29:21.280
and, you know, different teams should know how they can move certain metrics.

29:21.280 --> 29:27.280
And, you know, there is, and then we think about, like, what sort of a data science effort

29:27.280 --> 29:28.440
could serve that metric.

29:28.440 --> 29:29.760
So there's that.

29:29.760 --> 29:35.840
What I call the bottom up is, well, you know, in the end, we are still, we are e-commerce,

29:35.840 --> 29:40.040
but also last mile, you know, logistics operation.

29:40.040 --> 29:44.840
So naturally with that domain, some problems just are there.

29:44.840 --> 29:49.120
Like if you have e-commerce operation, you have a search engine, and if you have a search

29:49.120 --> 29:52.800
engine, you're probably working on, like, you know, spell correction and autocomplete

29:52.800 --> 29:57.880
and, you know, search ranking, matching, business understanding, the queries better,

29:57.880 --> 29:59.880
and recommendation systems and so on.

29:59.880 --> 30:07.760
So we already know, given the kind of data we have, and we collect, what sort of problems

30:07.760 --> 30:13.360
have been addressed, like, what sort of product features have been useful, in that sense.

30:13.360 --> 30:18.760
So between the top down and bottom up, I think we generally find, you know, tactically projects

30:18.760 --> 30:23.400
to work on, you know, given, you know, current goals, yeah.

30:23.400 --> 30:29.720
And oftentimes, you know, we hit, like, dimension utilities, soon enough for plateau,

30:29.720 --> 30:34.520
soon enough, maybe in the first version, we might wait for, like, a while before we come

30:34.520 --> 30:35.520
back to it.

30:35.520 --> 30:36.520
Okay.

30:36.520 --> 30:37.520
Interesting.

30:37.520 --> 30:41.280
Well, I think you've got a presentation to get ready for, but I really appreciate

30:41.280 --> 30:44.280
you taking the time to chat with us, and I'm looking forward to your talk.

30:44.280 --> 30:45.280
Sure.

30:45.280 --> 30:46.280
Thank you, Sam.

30:46.280 --> 30:47.280
Thanks, sure.

30:47.280 --> 30:53.200
All right, everyone, that's our show for today.

30:53.200 --> 30:58.480
Thanks so much for listening, and for your continued support of this podcast.

30:58.480 --> 31:03.480
For the notes for this episode, to ask any questions, or to let us know how you like

31:03.480 --> 31:11.320
the show, leave a comment on the show notes page at twimmelai.com slash talk slash 39.

31:11.320 --> 31:16.120
Thanks again to our sponsor for the Wrangle Conference series, Cloud era, to learn more

31:16.120 --> 31:21.480
about Cloud era and the company's data science workbench family of products, visit them at

31:21.480 --> 31:26.320
cloudera.com, and be sure to let them know how much you appreciate their support of the

31:26.320 --> 31:30.160
podcast by tweeting to them at at cloud era.

31:30.160 --> 31:34.360
If you're interested in joining our first Twimmel online meetup, where we'll discuss

31:34.360 --> 31:39.400
Apple's recent research paper on generative adversarial networks, you can register for

31:39.400 --> 31:42.600
that at twimmelai.com slash meetup.

31:42.600 --> 31:49.320
And don't forget to sign up for our email newsletter at twimmelai.com slash newsletter.

31:49.320 --> 32:00.800
Thanks again for listening, and catch you next time.

