Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Alright everyone, I am on the line with Genevara Allen, Genevara is an associate professor
of statistics in the Department of Statistics, Computer Science and Electrical and Computer
Engineering at Rice University, as well as Founder and Director of the Rice Center for
Transforming Data to Knowledge and an Investigator with the Neurological Research Institute
with the Baylor College of Medicine.
Genevara, welcome to this week in machine learning and AI.
Thank you, thanks for having me.
Why don't we jump right in and have you introduce yourself to our audience, you work kind
of at the intersection of stats, as well as medicine, how do those connect for you and
how did you arrive at this, your research interests?
Sure, so my work, my research interests are to build statistical and machine learning
tools to help scientists make discoveries from their big and large complex data sets.
I particularly like working in neuroscience and genetics, but I'm interested in lots
of scientific problems.
And I took a little bit of a roundabout track to get here.
I went to undergrad interested in music, actually, but took a statistics class and really liked
it.
It felt like it made sense, it's kind of the, to me, it's kind of the mathematical basis
of making decisions, right, kind of data-driven decisions.
And I really liked that a lot, got into statistics and as an undergrad, I actually worked
at the Baylor College of Medicine, which is across the street from Rice University, where
I was doing my undergrad as a data analyst in a biomedical research lab, but also did
some research in statistics and then went to grad school at Stanford and statistics,
which that department is heavily influenced by machine learning and computer science.
So while in grad school, I took quite a bit of classes in machine learning and applied
math and optimization, and that kind of launched my research at the intersection.
It's interesting.
I'm pretty interdisciplinary using kind of statistics, machine learning, optimization techniques,
but I'll apply the solve scientific problems.
You gave a talk at the AAAS conference, was it last month?
It was last month.
Okay, AAAS being the American Association for the Advancement of Science.
On the topic of reproducibility in machine learning, that sounds like from what I read,
the reaction to your talk caught you a little bit by a surprise.
Maybe we'll start by talking about the reaction and really dig into kind of your thoughts
on the topic.
Absolutely.
So I gave a talk at the AAAS meeting and a press briefing.
And I titled my talk, can we trust data driven discoveries, or more specifically, can we
trust discoveries made using machine learning techniques from large scientific data sets?
And I really ask this question as to be a bit provocative, but my core interest and my
research is actually on how to answer these questions, kind of what are some of the new
research directions and directions we can go to help make scientists give scientists tools
so that they can better make inferences from their really big data using machine learning
techniques.
So I gave a talk and outlined some of these new research approaches, some general approaches
and some new research from my own group as well.
So I was certainly a surprise to say the least at some of the response online.
And I'd love to take the opportunity to dig into this deeper.
I think that there are a lot of people that were very excited about what I said and said,
hey, these are good points or at least these are things that we should have a conversation
about or start a discussion on.
Those were quite upset and said I was machine learning public enemy number one, which I would
disagree with slightly because I love machine learning.
It's what I do.
I think machine learning is awesome and want to promote machine learning.
And that's certainly not what I was doing.
But I would love to raise the point of that there's a lot of research directions in
this area.
And perhaps today I can highlight a couple of those and perhaps sparks some ideas for other
machine learning researchers.
Let's do that.
But where I like to start having seen your slides from that presentation is where you
started.
And in particular, you gave a lot of examples, very focused on medicine and biological
science.
We've talked here on the podcast about the reproducibility problems in ML broadly.
But I'd love to understand the way you contextualize them in the medicine domain.
Absolutely.
So I should say biomedical data is really broad.
There's lots of different types of data out there and machine learning techniques that
people apply to it.
So I can give a couple of examples, but there's lots of situations where there's pretty large
and complex data sets over the past, say, five, 10 years, 20 years.
There's been a lot of new technologies developed to measure, say, nearly every aspect of our genome
or to image brains and record from neurons at scales larger than we could ever have imagined
five years ago.
And so all of this is really exciting, but the data is quite big to analyze this data and
really kind of make discoveries from this data.
You need to use, it's, you can't necessarily just use classical statistics and classical
techniques.
A lot of machine learning techniques are used to really dive into this data and understand
the data.
And the question I would ask is, are the discoveries that we make from using techniques
in this data, are they reproducible?
So a specific example is, and this has actually been a well studied example.
So in cancer genomics, people are very interested in finding subgroups of patients that have
are genomically similar, have genomically similar tumor profiles, but and also have clinical
differences.
Cancer, cancer subtypes.
And these are very promising in precision medicine because they're useful in precision
medicine, where say we want to find groups of patients that are genomically similar and
develop jug targets that can specifically target their genomic signatures.
So it's important to find these groups of patients that have these similar genomic
signatures and one very successful, very well-known case study of this is in breast cancer, where
there's very famous breast cancer subtypes that have been very well validated.
These were found using clustering techniques, pretty standard clustering techniques actually
just hierarchical clustering.
And the, there's a very strong signal and separation between groups of patients based
on their genomic profiles, and these subgroups have been very well validated by many studies
in many clinical settings, and now they're actually developing drugs that are targeting
specific for different subgroups of breast cancer right now.
So that's a success story of using machine learning in biomedicine, and after that a lot
of people for various different cancer types say lung cancer, colorectal cancer or ovarian
cancer have asked the same questions.
Can we find these subgroups that have where their tumors are genomically similar so that
we can treat them in a similar manner with similar drugs?
And the problem is that one, you know, one paper will come out and it'll make a big splash
and say here's a definitive, you know, subtyping and clustering of, you know, this particular
cancer type, and then another paper comes out maybe two years later on another cohort
of patients that has a different clustering and maybe even totally different numbers of
groups anywhere from three subtypes or groups to seven subtypes or groups.
And they haven't necessarily been consistent and reproduced, and several of the people
that have tried to do meta-analyses on these studies have said, look, you know, some
of these clustering results that we found, these proposed subtypes, don't in fact, are
not reproducible, do not necessarily hold up.
So this is just one example from cancer genomics, but I think it's reminiscent of kind of a
larger problem.
Now I should caution that my research is not necessarily on, I don't study the extent
of the reproducibility crisis, I'm focused more on developing methods to solve problems
related to this, but I do think this is an open area for research.
I do think it's a problem in biomedical studies, and I've certainly heard lots of anecdotal
evidence that there's a problem from both scientists who are reading these papers and say,
look, I tried to reproduce this lab's finding from their big data set, I have data just
like it, I tried to reproduce it, I, you know, couldn't.
I hear this actually quite a bit a lot from scientists that I work with.
I hear this also from other machine learning and data science researchers in the biomedical
domain, who seem well aware of the problem, and I do think it warrants further investigation.
Okay, so a couple of clarifying questions.
The first is you talked about the breast cancer studies that have been validated, that
originated from using statistical clustering techniques, and then some other results that
have not been validated and were contradictory results have found, just to make sure the
statistical techniques in both, I guess there are a couple of ways to ask this question
A, are you kind of categorizing the clustering techniques and using the first example as
machine learning, you know, and or, you know, are the clustering, are the techniques used
in some of these other studies, are they the same, and the issue is that, you know, what
worked here is trying to be used in other places and it's not working, or, you know, are
there different types of techniques being used?
Okay, so that's a great question.
So first, I don't like to necessarily call techniques, statistics techniques, machine learning
techniques, data science techniques, I actually think there are a lot of the same toolkits.
It's just a data scientist toolkits, and we should probably be using all of these techniques.
But most of the techniques that discovered the breast cancer subtypes are very, very, very
simple algorithms, hierarchical clustering and k-means, very standard clustering algorithms.
And I characterize the reproducibility issue here as not just, you know, a scientist, you
know, took a new data set and applied the exact same pipeline and the exact same techniques
and couldn't reproduce the results.
But it's also, if something is truly, I think our goal, especially when it comes to analyzing
scientific data, our goal is to make some type of scientific discovery that actually has
underlying principles, in this case, that actually if we're trying to find subgroups based
on similarity of genomic profiles, there should be some actual underlying biology that links
these groups.
So there's some type of scientific truth that we're trying to discover, and we don't want
necessarily, or our goal should be to have techniques that we can apply, that we understand
but whether it's likely to be a finding is likely to be reproducible on a future data
set, not just when applying the exact same kind of technique and pipeline, but is going
to be lead to kind of a reproducible, true discovery, perhaps if you alter the pipeline
a little bit, apply a slightly different clustering technique.
So it's really kind of a robustness to like the particular choices of the data analyst,
taking with the data.
So it's a little bit more than just exactly, I think stage one is you should be able to
exactly apply someone else's pipeline on a new data set and hopefully get similar results,
kind of the training to test that type thing.
But a step beyond that is if you change the pipeline just a little bit, if another analyst
applies a very reasonable clustering technique or dementia reduction technique, can you still
recover the same type of scientific result under reasonable perturbations to your methods?
Do you contend that kind of reproducibility by definition means that you should be able
to apply the same pipeline to different data sets that are exploring different diseases?
Why are those, why do those need to go hand in hand?
Ah, okay.
So I'm not necessarily saying apply the same pipeline to data sets on different diseases.
Let's think about this and we're talking about data set, you've got the same fundamental
type of data.
The same.
Imagine you've got a training data set and say if you're doing, if your goal in machine
learning is doing prediction and prediction has been super well studied and we know or
a lot of people are doing research on how to do reproducible predictive models, right?
We've got to, in the fundamental premises, we've got a training data set and we've got
a test data set, right?
We've got prediction error that measures how close our predictions on the test data
set are to the true labels or outcome on that data set, right?
So we've got very well understood principles for measuring kind of the reproducibility or
the more broadly, like the generalizability of our findings when it comes to prediction
as the goal.
And what I think the other kind of big goal of machine learning techniques is data driven
discovery or inference.
And I would define this as, say, trying to find insights from big data, right?
So we're not interested necessarily in, and this isn't a distinction between unsupervised
learning and supervised learning.
It's more of a distinction of, in prediction, are you interested in just the predicted labels
or outcomes and it's kind of a black box that got you there and you don't care what got
you there?
Or are you interested in what's underneath that black box that insights from the data?
So this could be, say, if you're doing supervised learning, say you're interested in which features
are associated with outcome and your inference question is what are the important features
associated with my outcome or it could be this fine groups or structure in my data set,
fine patterns, fine interactions between features and so forth.
So those insights from data, we have a lot of fantastic machine learning methods that
have been developed to find all sorts of really great insights from big data.
It's really awesome.
There's been so much development in this area.
This is actually one of my primary areas of research.
But there's been less research until recently on asking, can we get that type of same generalized
ability and reproducibility of our data-driven discoveries as we can for prediction?
So for example, in prediction, we assume that our training and our test data come from
the same population.
It's the same data set, right?
And our goal is to learn some type of machine learning model on a training data set that's
going to generalize well on future data set, which is the test data set that comes from
the same population.
And if you over train to your training data set, you of course over fit and your model will
not generalize well and you're going to have poor predictions on your test data.
So imagine even you can do the same thing with data-driven discovery if that's your primary
goal and say, OK, we've got a test, we've got a training data set, and we've got a test
data set.
And what if we use our training data set to find a particular type of pattern or group
or important features?
If you, there's a lot of potential to say over fit and I'm saying over fit and quotes
to your training data and find particular kind of discoveries that are inherent in your
training data that are probably not going to generalize to a test data set that comes
from the same population.
And so the question is, is how can we build new data-driven discovery techniques or perhaps
kind of other techniques to make sure that the discoveries we make are generalizable
to kind of future test data sets in the context of data-driven discoveries?
You actually hit on my second question, which was I was kind of keying off of your use
of the term scientific discovery here and really wondering about like what's special about
that relative to other uses of machine learning in other places that this reproducibility
problem comes up.
And I think that the idea of prediction versus inference really captured that, captured
that really nicely.
So we're talking about, you know, the way we typically think about kind of building these
models, you've got some data set and you kind of split it into your training data and
your test data and you build a model.
And in some ways, you want to apply that to, you know, another data set, you know, totally
different data set, meaning the same kind of data, obviously, the same, you know, underlying
population, but a data that's been collected, you know, maybe differently and that you can,
you kind of massage to kind of fit the need of your original pipeline.
We look at kind of the way we typically report these results like in academic papers where
almost always using the same data set, oftentimes it's kind of the same data set that everyone
else uses.
And so, you know, there's this reproducibility across different data sets.
Yeah, I'm, I'm perhaps talking a little bit less about reproducibility across different
data sets of different types, but more asking with, if your goal is to do inference or data
driven discovery instead of prediction, are there good ways of assessing how generalizable
your results are from an analysis of a particular data set?
So in particular, let's take an example of just imagine you've got a data set that someone
gives you a big, could be scientific data, could be any data, someone gives you a big data
set and you split it into two-thirds training and one-third test data.
So if you were doing prediction, there's great metrics on you build, you build a model
that predicts some labels on your test data and there's great metrics to measure the success
of your labels, misclassification error, all sorts of other things.
We have kind of metrics to measure prediction error.
If we're doing data driven discovery, for example, suppose you find clusters or groups
on your training data set.
How do you measure on your test data set, whether you found those same groups, do you apply
your same cost-oring pipeline on the test data and group those observations and then see
how they overlap, there's many different ways to define this and some people have proposed
this in the literature, but it's not even clear what metrics are to measure different
types of reproducibility of different types of data driven discoveries.
So I think one of the first research questions in this area or an open question would, if
you do use the training and test set split, what are possible metrics to measure kind
of the accuracy of, say, clustering results or pattern recognition results or interactions,
important interactions between features or things like this.
We have very clear metrics when it comes to prediction.
We've got great stuff in the prediction realm.
What about these kind of inference on data or data driven discovery realm?
So you're not even trying to address this broader issue of reproducibility and kind of
that disconnect between what you're specifically looking at and this kind of where I was taking
it is leading me to ask, do we even have good names for like these different types of reproducibility
issues yet?
Yeah, that's actually a really great question.
I don't think we do.
I think we need to somehow expand our terminology because I don't know the right words to always
talk about this.
Everybody, when you say computational reproducibility, everybody knows what that is, that we should
document our code and all of the steps so that you can fully computationally reproduce
the results on your given training data set that you analyzed, right?
We know what computational reproducibility is.
We know what generalizability of prediction results are, right?
We've studied that very well.
I'm not really sure how to, I don't know that there's a good definition or good terminology
to talk about how we generalize results for data driven discoveries or make sure, I like
to call this, you know, make sure they're reproducible.
Generalization is really making sure that the results are reproducible.
All right, so some of the techniques that we do have for doing this, walk us through
that.
Absolutely.
So, the first and easiest one, you can split your data into a training and test data split,
and you can do kind of, you can build a pipeline.
I like to say your data analysis pipeline needs to be automated in some way so that you
can apply it without human interaction on your test data set and see, do the, quote, discoveries
that I made or inferences that I've drawn from my training data set, are they the same
on my test data set?
So, that's one check.
It's not a great check because you really only have one data set, and if they don't exactly
match the question as well, is it because of the random split that I made in my training
test data set or is there something wrong with my pipeline and my methods, or is the
discovery not there?
So you can repeatedly split the data into training and test data sets and do this over and
over again.
The challenge is you might get slightly different discoveries, right?
If your goal is say, feature selection, every time you re-splip your data, you might select
slightly different features.
So in the end, what is the true discovery from this method?
So, there has been some work on this, there are some challenges with that approach, but
an approach that's an extension of this that I think is perhaps one of the easiest approaches
for practitioners to apply right now is the stability principle.
And the idea of the stability principle is very simple, and it's a stable discovery is
likely to be a reproducible discovery.
The idea is to take your training data set and randomize your training data set in a way
that would mimic if you had future test data sets.
So for example, if you have IID observations in a training data set, you can perhaps bootstrap
your data or subsample your data as a specific example.
In other situations, you might want to add a slight noise, maybe even the plus noise in
certain situations.
So you're randomizing your data and you do this repeatedly for say 500 a thousand times
and you apply your data analysis pipeline to draw your inferences and make your discoveries
on every one of the randomized data sets.
And then you aggregate the results over all of the randomizations and the discoveries
that are stable, meaning those discoveries that appeared in all of the different randomized
data sets or a high proportion of them, are likely to be reproducible discoveries.
So why is it that doing this type of analysis using randomized noise as opposed to randomly
generating your trained test split, why is the former more efficient, or if efficiency
is not the right word, why is it easier to do in practice?
I think the training and test sets are certainly doable in practice.
There's a challenge of how you aggregate all the results over all of those different
training and test sets splits at the end.
So maybe you split your data and every time you split your data, you get slightly different
discoveries that have different generalization properties on the test data set.
The stability principle, I actually think, takes that randomly splitting your data over
and over again.
And it's very similar because you can, for example, subset your data many times.
You can just repeatedly, randomly sample two-thirds of your data over and over again.
It's very similar to data splitting, but the goal is just to aggregate all of the discoveries
you made over the different randomization, which ones kind of always appeared.
It's kind of a robustness measure that if we kind of randomize and shake up our data
over and over again and the discovery, whatever that discovery is, if it's always there,
it's probably got a pretty strong signal and it's probably going to be generalizable.
I still don't understand why you can't do that with the data splitting.
Like in both cases, you ultimately have a bunch of test and train or a bunch of train
data sets and you're training a model and then you're looking at the features and you're
trying in the case of stability principle, you're looking at the features and looking
at maybe taking a majority or which one, which features rank highest on the importance
over a bunch of different runs.
Why couldn't you do the same thing on the data splitting side?
You absolutely can.
So absolutely and again, subsampling for stability principle is essentially the same as data
splitting.
Yeah, so they're essentially the same thing if you're aggregating the results over them.
The challenge with data splitting is again, a matter of, do we have a metric for when
we apply our discovery technique to the test data set?
Do we have a metric to understand whether it's a good discovery or it's generalizable or
not and we don't, like we do for prediction error if our goal is prediction.
But with the stability principle, we do have firmer metrics.
I'm not sure they're firmer metrics, it's just an indication if you aggregate overall
the different randomizations, you just, it's some measure of kind of the uncertainty in
your discovery or the robustness of your discovery.
So I'm not sure it's a, I mean, you can always come up with metrics, you could probably
do the same for data splitting.
So both of these techniques are very similar to each other.
Okay.
Yeah.
So there are, I did want to highlight, there are several other techniques that some machine
learning and statistical researchers have started to look at, but are potentially really
interesting directions to go.
And the first is an area called post-selection inference.
And the goal here is to use your training data set to make some type of discovery for
some type of machine learning procedure.
And then say, can you do classical statistical inference on that discovery to say, you know,
whether it's true or not?
I say classical statistical inference.
I mean, you know, hypothesis testing, confidence regions and so forth.
And the idea behind this approach is that if we say, mine a big data set using feature
selection, clustering, pattern recognition, any, any kind of techniques we would use
to analyze a big data set, we're actually exploring that data set to find our model,
to find our hypothesis.
So it's essentially an exploratory analysis situation in which we don't have an a priori
defined hypothesis in advance.
We let our machine learning method find our hypothesis for us.
And then since our machine learning method found our hypothesis, it's a big challenge to
say, then say, now that we've used machine learning to find the hypothesis, we now want
to conduct traditional statistical inference on this hypothesis.
The challenge is all of the classical inference techniques that we know and love don't quite
apply because they assume that you have an a priori hypothesis in advance and that data
was collected specifically to address that a priori hypothesis.
So new techniques need to be developed in this area.
And so far, there have been some techniques developed for feature selection in this area
that say allow you to apply a feature selection method to a training data set and on the same
training data set, then say test the coefficients from a model of the features that you selected.
So this is an area again called post-selection inference and it's potentially exciting
because it's especially exciting for scientific researchers because in science to get published
in a lot of the big journals, you definitely need p-values and confidence regions.
Those are very important.
And so I do see this as a potential potentially big contribution to science that kind of
marry traditional statistical inference with machine learning methods.
You made a comment about features of your training data as opposed to features of your model.
Can you elaborate on that?
Yeah.
And in the post-selection inference paradigm, the goal would be to not necessarily need
to have a test, a training and test set split.
But instead, use your training data just as both the data for which you're going to find
your model or select your model, meaning you're going to find the hypothesis you want to
test and then test that hypothesis on the same data.
So the advantage of this approach over say stability and data splitting is it tends to
be more efficient in terms of using the available data and using the available samples.
The downside is the math is a bit harder, but that gives math and stats people plenty
to do.
And so is there something about the way that the math is done that you can elaborate on
that gets us beyond like when we typically think about using the same data for training
and evaluation.
It's kind of double dipping and you tend to have overfitting problems.
What about this particular technique allows us to avoid that?
Yeah.
So a core part of this technique says, can you take say whatever procedure you applied?
And you have some statistical tests that you want to conduct and you have a test statistic.
The question is can you take that test statistic and can you decompose it into the part of
the test statistic that is depending on your machine learning method or your method you
use to select your model or select your hypothesis and the part that's not dependent on the machine
learning method that you use.
And then you condition on, you basically conduct conditional inference.
So you basically say, can I derive the null distribution of my test statistic conditional
on the results of the machine learning procedure?
So that's essentially what's going on behind the scenes and you need to calculate these
conditional distributions, sometimes conditional and very complicated machine learning procedures
which is the mathematical challenge.
And so when you talk about this mathematical challenge, are we talking about computational
challenge or an analytical challenge?
More analytical challenge.
Okay.
So that kind of suggests to me that this particular approach is maybe harder to scale across
a variety of different problems or it may be even a variety of different data sets, is
that case?
Yeah.
So you're exactly right here, stability and data splitting, the first two I talked about
are pretty generalizable to any situation.
This post-selection inference paradigm, so far is only applicable in limited settings
and it might, it's not even clear that you're going to be able to generalize this to say
all machine learning methods, that's currently unclear, but it is an active area of research.
And so maybe going back to where we started your talk and the reaction to your talk, what
did all that say to you about where kind of your community, the community that you are
speaking to of science as opposed to machine learning practitioners, where they are in terms
of their awareness of these issues and where they need to be?
Yeah, so it seems like there are sometimes machine learning researchers, there's a lot
of great techniques being developed that are really cool, that can do just super fascinating
things with data.
At the same time, sometimes the way machine learning is used by practitioners is very
different from the latest and state of the art machine learning techniques that are being
developed by machine learning researchers.
And I think there is a bit of a disconnect between how kind of machine learning researchers
view how machine learning should be used and what it should be used for and how people
use it in practice and particularly how scientists are using machine learning techniques to
make inferences and discoveries from their very large complex data sets.
And so I do think it's important to pay attention to how people are using machine learning
techniques and science and particularly and part of my goal in my research is how can
we improve machine learning techniques to make them easier for scientists to use or describe
kind of underlying principles to scientists in very clear terms that they can understand.
For example, scientists are now very aware and practitioners of machine learning all
know that if you're doing prediction, you have to have a separate test set.
It's not okay to just train a model on a training data set and report your training error.
So that's very well understood amongst practitioners.
I think we can definitely do a better job when it comes to inference and data driven discovery
on perhaps developing the best ways to go about this and also making sure that those methods
get turned into practical tools that everybody can use to help analyze their data.
So a lot of the work that's happening in the broader machine learning community and maybe
this is just the kind of with a commercial lens on, but it's focused on, I guess not
even commercial, you know, the open source community, things like TensorFlow and PyTorge
on the deep learning side and the toolkits like psychic learning Python, like a lot of
this activity is, you know, we talk about this idea of democratizing machine learning,
like making it more accessible.
Do you're finding suggests that this is wrong headed or, you know, we're not ready for
that yet?
Oh, no.
I mean, that's fabulous.
That's what we should do with research is make it open source and everybody have access
to it.
At the same time, I do think it's important that we're educating users and practitioners
about how best to apply these techniques in practice and what are really, what are
good underlying principles and simple principles that they can use and follow to make sure they're
getting robust discoveries.
And again, I think in a lot of comments too in the news articles following my talk alluded
to this, the problem isn't necessarily that the machine learning techniques are bad
or not correct and it's not at all.
The machine learning techniques are great.
It's often in how they're applied.
And so I do think we have a responsibility to think carefully about how we're educating
people on how to apply these techniques and what the principles are for doing so.
Got it.
So you see the first and foremost, there's a problem of education in particular around
the appropriateness of these techniques.
Yeah.
And it was like you said earlier, you asked, you know, is there even a terminology to talk
about reproducibility when it comes to inference or data to other discoveries?
And I don't think there is.
We don't even have terms to necessarily talk about this yet.
So how can we expect practitioners and users to understand how to appropriately apply
these techniques in these situations?
So I would love to see more machine learning and statistical researchers get into this
area and hash out what's a robust terminology to talk about this, what's the correct mathematical
paradigm to write down for these types of problems and let's start studying them and
making sure we get the word out to practitioners to use the machine learning techniques appropriately
so that they can do more generalizable discoveries and hence do better science.
Awesome.
Awesome.
Well, Jennifer, thank you so much for taking the time to chat with us about this really
interesting work and conversation and I appreciated, you know, I obviously struggled with that
terminology myself in trying to kind of narrow in on what we were talking about here.
But I think there is definitely a lot of work to be done in this space and thank you for
doing it.
Well, thank you very much for having me on.
Alright everyone, that's our show for today.
If you like what you've heard here, please do us a huge favor and tell your friends about
the show.
And if you haven't already hit that subscribe button yourself, make sure you do so you
don't miss any of the great episodes we've got in store for you.
As always, thanks so much for listening and catch you next time.
