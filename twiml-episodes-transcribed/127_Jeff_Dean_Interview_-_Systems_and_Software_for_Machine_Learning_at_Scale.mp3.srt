1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:32,040
I'm your host Sam Charrington.

4
00:00:32,040 --> 00:00:36,160
Alright you are in for a major treat this time.

5
00:00:36,160 --> 00:00:40,720
Last week I had a chance to stop by the Googleplex in Mountain View and sit down with none other

6
00:00:40,720 --> 00:00:45,760
than Jeff Dean, Google senior fellow and head of the company's deep learning research

7
00:00:45,760 --> 00:00:48,600
team, Google Brain.

8
00:00:48,600 --> 00:00:53,960
As you hear I was very excited for this interview because so many of Jeff's contributions since

9
00:00:53,960 --> 00:00:59,240
he started a Google in 1999 have touched my life and work.

10
00:00:59,240 --> 00:01:03,800
In our conversation Jeff and I dig into a bunch of the core machine learning innovations

11
00:01:03,800 --> 00:01:05,600
we've seen from Google.

12
00:01:05,600 --> 00:01:10,560
Of course we discuss TensorFlow and its origins and evolutions at the company.

13
00:01:10,560 --> 00:01:18,000
We also explore AI acceleration hardware including the TPU, versions 1 and 2 and future directions

14
00:01:18,000 --> 00:01:21,720
from Google and the broader market in this area.

15
00:01:21,720 --> 00:01:25,240
We talk through the machine learning tool chain including some of the things that Googlers

16
00:01:25,240 --> 00:01:30,880
might take for granted and where the recently announced cloud AutoML fits in.

17
00:01:30,880 --> 00:01:36,200
And we discuss Google's process for mapping problems across a variety of domains too deep

18
00:01:36,200 --> 00:01:38,920
learning and much much more.

19
00:01:38,920 --> 00:01:47,280
This was definitely one of my favorite conversations and I'm pumped to be able to share it with you.

20
00:01:47,280 --> 00:01:51,280
Before we jump into the show though, a few quick questions for you.

21
00:01:51,280 --> 00:01:55,880
Are you an IT technology or business leader who needs to get smart on the broad spectrum

22
00:01:55,880 --> 00:01:59,800
of machine learning and AI opportunities in the enterprise?

23
00:01:59,800 --> 00:02:04,440
Or perhaps someone in your organization could benefit from a level up in this area?

24
00:02:04,440 --> 00:02:07,960
Or maybe you could benefit from them leveling up?

25
00:02:07,960 --> 00:02:13,080
If this sounds like you or someone you know, you'll probably be interested in my upcoming

26
00:02:13,080 --> 00:02:15,280
AI Summit event.

27
00:02:15,280 --> 00:02:20,520
Think of the event as a two-day no-fluff technology plus strategy MBA and machine learning

28
00:02:20,520 --> 00:02:21,840
in AI.

29
00:02:21,840 --> 00:02:26,080
You'll leave with a clear understanding of how machine learning and deep learning work,

30
00:02:26,080 --> 00:02:30,800
no math required, how to identify machine learning and deep learning opportunities within

31
00:02:30,800 --> 00:02:35,720
your organization, how to understand and take advantage of technologies like computer

32
00:02:35,720 --> 00:02:41,800
vision and natural language processing, how to manage and label data for ML and AI,

33
00:02:41,800 --> 00:02:47,600
how to build an AI first culture and operationalize AI in your business.

34
00:02:47,600 --> 00:02:51,040
You'll have an informed perspective of what's going on across the machine learning and

35
00:02:51,040 --> 00:02:55,520
AI landscape and we'll be able to engage confidently in discussions about machine learning

36
00:02:55,520 --> 00:02:58,920
and AI with your colleagues, customers and partners.

37
00:02:58,920 --> 00:03:02,760
I'm very excited about this event and the expert speakers you'll get a chance to learn

38
00:03:02,760 --> 00:03:03,760
from.

39
00:03:03,760 --> 00:03:09,840
For more information, visit twimmalai.com slash AI Summit and feel free to contact me

40
00:03:09,840 --> 00:03:11,440
with your questions.

41
00:03:11,440 --> 00:03:17,800
You can reach me at at Sam Charrington on Twitter or shoot me an email via twimmalai.com slash

42
00:03:17,800 --> 00:03:19,200
contact.

43
00:03:19,200 --> 00:03:21,440
We hope to see you there.

44
00:03:21,440 --> 00:03:23,920
And now on to the show.

45
00:03:23,920 --> 00:03:27,440
All right, everyone.

46
00:03:27,440 --> 00:03:30,080
I am here with Jeff Dean.

47
00:03:30,080 --> 00:03:35,080
Jeff is a Google senior fellow and head of Google Brain.

48
00:03:35,080 --> 00:03:37,520
Welcome to this week in machine learning and AI, Jeff.

49
00:03:37,520 --> 00:03:38,520
Thanks for having me.

50
00:03:38,520 --> 00:03:44,040
I've got to say, I am quite star struck to be sitting here on the Google campus with

51
00:03:44,040 --> 00:03:45,040
you.

52
00:03:45,040 --> 00:03:50,600
You know, I was at the scaled machine learning conference Saturday where you had a

53
00:03:50,600 --> 00:03:55,840
chance to speak and I didn't realize this until Resa mentioned it in your intro.

54
00:03:55,840 --> 00:04:02,320
But there's a whole quora page of Jeff Dean facts, including things like compilers don't

55
00:04:02,320 --> 00:04:03,320
warn Jeff.

56
00:04:03,320 --> 00:04:04,320
Jeff warns compilers.

57
00:04:04,320 --> 00:04:11,200
You know, Jeff occasionally compiles his code just to see if there are bugs in the compiler.

58
00:04:11,200 --> 00:04:14,640
And my favorite, Jeff Dean doesn't actually exist.

59
00:04:14,640 --> 00:04:18,640
He's really an AI that was created by Jeff Dean.

60
00:04:18,640 --> 00:04:23,080
So you have to excuse me if I ramble because I am star struck.

61
00:04:23,080 --> 00:04:26,920
But thanks so much for taking the time to be on the podcast.

62
00:04:26,920 --> 00:04:27,920
Oh, it's my pleasure.

63
00:04:27,920 --> 00:04:30,960
Why don't we get started by having you tell us a little bit about your background and

64
00:04:30,960 --> 00:04:34,560
how you got involved and interested in ML and AI?

65
00:04:34,560 --> 00:04:35,560
Sure.

66
00:04:35,560 --> 00:04:41,600
So, you know, I've always been interested in computing and I moved around a lot as a child.

67
00:04:41,600 --> 00:04:43,320
I went to 11 schools in 12 years.

68
00:04:43,320 --> 00:04:44,320
Oh my goodness.

69
00:04:44,320 --> 00:04:45,320
Oh my goodness.

70
00:04:45,320 --> 00:04:49,880
Several different continents lived in Hawaii, Boston, Uganda, Boston, Arkansas, Hawaii,

71
00:04:49,880 --> 00:04:54,960
Minnesota, Somalia, Minnesota, Atlanta, and then back to Minnesota for college.

72
00:04:54,960 --> 00:04:58,600
And then was this like were you an army brat or something like that?

73
00:04:58,600 --> 00:05:01,720
My dad did tropical disease research and epidemiology.

74
00:05:01,720 --> 00:05:02,720
Okay.

75
00:05:02,720 --> 00:05:04,040
Mom was a medical anthropologist.

76
00:05:04,040 --> 00:05:05,040
Okay.

77
00:05:05,040 --> 00:05:06,040
And they liked to move.

78
00:05:06,040 --> 00:05:07,040
So I moved.

79
00:05:07,040 --> 00:05:08,040
Nice.

80
00:05:08,040 --> 00:05:09,040
Nice.

81
00:05:09,040 --> 00:05:12,720
And then when I arrived at University of Minnesota as an undergrad, just before doing

82
00:05:12,720 --> 00:05:16,200
its senior thesis, I took a course on parallel and distributed computing.

83
00:05:16,200 --> 00:05:17,200
Okay.

84
00:05:17,200 --> 00:05:21,440
And then I worked with the professor who taught me that to do a senior thesis on parallel

85
00:05:21,440 --> 00:05:27,000
training of neural nets, because I sort of at that time people were very interested

86
00:05:27,000 --> 00:05:32,720
in sort of neural nets as a interesting abstraction for solving problems.

87
00:05:32,720 --> 00:05:37,040
And at that time, it seemed like they could do cool things on toy problems, but not much

88
00:05:37,040 --> 00:05:39,720
on very large scale problems that people cared about.

89
00:05:39,720 --> 00:05:40,720
Okay.

90
00:05:40,720 --> 00:05:44,960
And I thought maybe if we could get like a 60X speed up on a 64 processor machine, we

91
00:05:44,960 --> 00:05:46,400
could do much bigger problems.

92
00:05:46,400 --> 00:05:49,320
And so I worked on some algorithms for that.

93
00:05:49,320 --> 00:05:53,200
But as it turned out, we needed like a million times more compute, not 60X.

94
00:05:53,200 --> 00:05:54,920
So you have to wait that one.

95
00:05:54,920 --> 00:06:00,040
And so I sort of set a file neural nets away as kind of an interesting thing, but then

96
00:06:00,040 --> 00:06:01,840
went off and did a lot of other work.

97
00:06:01,840 --> 00:06:07,080
I eventually went on to grad school, did compiler optimization research, then came to digital

98
00:06:07,080 --> 00:06:12,040
equipments, research labs in Palo Alto and worked on a variety of things, including information

99
00:06:12,040 --> 00:06:17,760
retrieval on the web, profiling systems, various things like that.

100
00:06:17,760 --> 00:06:23,240
And eventually came to Google in 1999, when we were a fairly small company, we were all

101
00:06:23,240 --> 00:06:30,720
kind of wedged in a small office on University Avenue above what's now the demobile store.

102
00:06:30,720 --> 00:06:36,080
And started working on sort of large scale distributed systems for sort of core Google

103
00:06:36,080 --> 00:06:42,320
products and sort of worked first on our first advertising system and then spent many years

104
00:06:42,320 --> 00:06:49,160
working on sort of Google's distributed search systems, including the ranking system and

105
00:06:49,160 --> 00:06:52,160
also the crawling indexing inquiry serving systems.

106
00:06:52,160 --> 00:06:53,160
Okay.

107
00:06:53,160 --> 00:06:56,120
And a ton of other stuff, MapReduce.

108
00:06:56,120 --> 00:06:57,120
Yes.

109
00:06:57,120 --> 00:07:01,840
So then kind of towards the end of working on the search system, my colleague, Sanjay

110
00:07:01,840 --> 00:07:10,480
Gamawatt and I and some other people have started working on sort of core infrastructure

111
00:07:10,480 --> 00:07:15,120
systems for dealing with large amounts of data, how can you efficiently process large amounts

112
00:07:15,120 --> 00:07:20,080
of data on sort of a collection of unreliable computers.

113
00:07:20,080 --> 00:07:24,560
Sanjay and I came up with this MapReduce abstraction in the context of sort of rewriting

114
00:07:24,560 --> 00:07:29,040
or crawling an indexing system where you want to go from having a bunch of raw pages

115
00:07:29,040 --> 00:07:34,920
on disk that you've crawled to actually having all the data structures built for serving

116
00:07:34,920 --> 00:07:36,560
Google queries.

117
00:07:36,560 --> 00:07:40,800
And there's a whole bunch of stages there that involve processing the pages and like doing

118
00:07:40,800 --> 00:07:45,600
things like extracting all the links from the pages and building a link graph and identifying

119
00:07:45,600 --> 00:07:47,840
the language of each page.

120
00:07:47,840 --> 00:07:52,880
And conceptually they're all pretty simple, but when you're trying to deal with making

121
00:07:52,880 --> 00:07:58,480
them reliable computations across lots of machines, if you sort of individually conflate

122
00:07:58,480 --> 00:08:03,440
the sort of low level mechanisms for reliability and the high level thing you're trying to do,

123
00:08:03,440 --> 00:08:08,720
which is figure out what language each page is in or extract links, it becomes kind of

124
00:08:08,720 --> 00:08:10,520
complicated to do.

125
00:08:10,520 --> 00:08:15,600
But if you have this nice abstraction which MapReduce seemed to provide, you can actually separate

126
00:08:15,600 --> 00:08:21,840
those and build an implementation of the MapReduce abstraction that allows you to sort of deal

127
00:08:21,840 --> 00:08:27,120
with lots of things in there like reliability and automatic parallelization across however

128
00:08:27,120 --> 00:08:31,720
many machines you want to throw at the problem, dealing with stragglers so even if a machine

129
00:08:31,720 --> 00:08:38,520
doesn't die, it might be running slowly because it's doing other stuff on a shared environment.

130
00:08:38,520 --> 00:08:44,120
And then you can express a lot of these fairly simple computations using fairly simple abstractions

131
00:08:44,120 --> 00:08:46,840
of MapReduce.

132
00:08:46,840 --> 00:08:53,000
One thing I'm curious about is a little bit of a history of machine learning and AI at

133
00:08:53,000 --> 00:09:00,080
Google, leading up to TensorFlow and some of the things you're doing around the TPU, but

134
00:09:00,080 --> 00:09:06,520
what's your earliest exposure, what was your earliest exposure to ML here at Google?

135
00:09:06,520 --> 00:09:12,720
So we've been using collectively machine learning at Google for quite a long time.

136
00:09:12,720 --> 00:09:18,640
And you can argue that the page rank, the beginning of Google's machine learning, right?

137
00:09:18,640 --> 00:09:25,600
So across lots and lots of our products, we use machine learning in various ways until

138
00:09:25,600 --> 00:09:30,560
maybe about six or seven years ago, typically not deep neural networks, but simpler methods

139
00:09:30,560 --> 00:09:41,320
like logistic regression or sometimes simple counting based statistics methods for estimating

140
00:09:41,320 --> 00:09:44,600
probabilities of various things.

141
00:09:44,600 --> 00:09:49,000
And so we've done, had a long history of that and built systems around how do we actually

142
00:09:49,000 --> 00:09:53,560
scale those kinds of systems to very large amounts of data.

143
00:09:53,560 --> 00:09:58,240
Some of them are embedded in core products that are pretty important.

144
00:09:58,240 --> 00:10:07,560
And then about seven years ago or so, we started to look into how to use deep neural nets

145
00:10:07,560 --> 00:10:13,560
and see if we could make those scale to problems we cared about, you know, very large scale

146
00:10:13,560 --> 00:10:21,200
problems, because there were signs of various successes in using neural nets for things

147
00:10:21,200 --> 00:10:27,960
like speech and vision on a small scale that were starting to appear in the academic literature

148
00:10:27,960 --> 00:10:33,560
as sort of the beginning of the revival of neural nets happening in kind of 2007, 2008.

149
00:10:33,560 --> 00:10:34,560
Right.

150
00:10:34,560 --> 00:10:41,520
And so in 2011, we started to really put, look at this in earnest within Google, Andrew

151
00:10:41,520 --> 00:10:45,320
Eng was spending a day a week at Google sort of consulting and I happened to bump into

152
00:10:45,320 --> 00:10:48,560
him in the micro kitchen and said, oh, what are you up to?

153
00:10:48,560 --> 00:10:53,360
And sort of he said, oh, I'm kind of figuring out what I'm going to work on.

154
00:10:53,360 --> 00:10:57,680
And but at Stanford, you know, I'm starting to look at how neural nets can solve different

155
00:10:57,680 --> 00:10:59,680
kinds of problems and they're starting to be successful.

156
00:10:59,680 --> 00:11:00,680
And I'm like, oh, really?

157
00:11:00,680 --> 00:11:03,560
I did my undergrad thesis years ago on those.

158
00:11:03,560 --> 00:11:04,560
Oh, wow.

159
00:11:04,560 --> 00:11:12,360
And so we kind of got excited about trying to put together a small effort to scale neural

160
00:11:12,360 --> 00:11:18,480
net training to build very big neural nets and see what we could do with them simply because,

161
00:11:18,480 --> 00:11:22,800
you know, we felt like if they're having success on small scale, then building bigger ones

162
00:11:22,800 --> 00:11:23,800
would be even better.

163
00:11:23,800 --> 00:11:25,120
Nice.

164
00:11:25,120 --> 00:11:30,280
And did the work on TensorFlow lead or follow directly from that?

165
00:11:30,280 --> 00:11:31,800
Where did, how did that evolve?

166
00:11:31,800 --> 00:11:37,880
Yeah, so the way this happened was we actually started building a different software system

167
00:11:37,880 --> 00:11:44,680
called disc belief or distributed belief and also as a joke because people didn't think

168
00:11:44,680 --> 00:11:46,880
this would really work out.

169
00:11:46,880 --> 00:11:50,720
So I called it disc belief.

170
00:11:50,720 --> 00:11:59,960
And basically that was a sort of programming abstraction for neural nets that wasn't in

171
00:11:59,960 --> 00:12:03,320
retrospect, it wasn't as flexible as we wanted it to be.

172
00:12:03,320 --> 00:12:09,160
But it would allow us to express kind of in the same way that MapReduce was a programming

173
00:12:09,160 --> 00:12:10,160
API.

174
00:12:10,160 --> 00:12:12,480
This was an API for neural nets.

175
00:12:12,480 --> 00:12:17,280
It was sort of layer based implementations where you'd have a forward method and a backwards

176
00:12:17,280 --> 00:12:18,520
method.

177
00:12:18,520 --> 00:12:24,280
And so you could compose together layers of neural nets with different kinds of computations

178
00:12:24,280 --> 00:12:26,280
in each layer.

179
00:12:26,280 --> 00:12:31,280
And it was good for expressing things like convolutional neural nets or LSTMs or feed

180
00:12:31,280 --> 00:12:34,000
forward neural nets, those kinds of things.

181
00:12:34,000 --> 00:12:36,400
And we started applying them to computer visions.

182
00:12:36,400 --> 00:12:44,200
We did some work on unsupervised learning with auto encoders for vision problems.

183
00:12:44,200 --> 00:12:48,960
And we actually didn't have GPUs in our data center at that time.

184
00:12:48,960 --> 00:12:53,640
So to get enough computation, we ended up parallelizing these computations across large numbers

185
00:12:53,640 --> 00:12:57,640
of CPUs because that's what we had in our data center at that time.

186
00:12:57,640 --> 00:13:02,720
And so we trained a neural net with like two billion parameters at that time, which was

187
00:13:02,720 --> 00:13:11,320
quite a lot and used kind of 16,000 cores for a week to train a pretty large scale auto

188
00:13:11,320 --> 00:13:16,520
encoder and found that it could do interesting things like despite being trained on completely

189
00:13:16,520 --> 00:13:23,600
un-mable data, some of the neurons at the top levels of this large neural net.

190
00:13:23,600 --> 00:13:31,280
It started to be responsive to things that resemble sort of high level human explainable

191
00:13:31,280 --> 00:13:39,520
features, things like kind of cat-like face or the back of a person kind of thing.

192
00:13:39,520 --> 00:13:44,040
And so it's clearly learning interesting high level concepts despite not having any

193
00:13:44,040 --> 00:13:45,960
labeled data.

194
00:13:45,960 --> 00:13:47,960
So you kind of started with disbelief.

195
00:13:47,960 --> 00:13:53,480
What were the challenges or issues with disbelief that led you to say we need something

196
00:13:53,480 --> 00:13:54,480
new and better?

197
00:13:54,480 --> 00:14:04,360
I mean, I think we realized that this sort of forward backward computation fit some kinds

198
00:14:04,360 --> 00:14:10,160
of machine learning model we wanted to be able to express, but it wasn't as general

199
00:14:10,160 --> 00:14:11,320
purpose as we would like.

200
00:14:11,320 --> 00:14:17,240
And so we looked at things like theano, which was an open source package from University

201
00:14:17,240 --> 00:14:22,600
of Montreal that had a more of a sort of auto differentiating graph-based interpretation

202
00:14:22,600 --> 00:14:25,680
of how to express these computations.

203
00:14:25,680 --> 00:14:34,120
And we sort of used that as a rough programming model, but focused on making this sort of

204
00:14:34,120 --> 00:14:36,360
a high-performance scalable implementation.

205
00:14:36,360 --> 00:14:41,040
And also we wanted to be able to run this in a variety of different environments.

206
00:14:41,040 --> 00:14:47,040
So we've always wanted the TensorFlow system to support running machine learning wherever

207
00:14:47,040 --> 00:14:50,440
machine learning wants to run, which is a lot of places these days.

208
00:14:50,440 --> 00:14:52,880
So you want to run it on things like mobile phones.

209
00:14:52,880 --> 00:14:57,800
You wanted to run it on a desktop machine with, you know, with or without a few GPU cards.

210
00:14:57,800 --> 00:15:01,920
You want to run in a data center environment where you have lots of machines each with

211
00:15:01,920 --> 00:15:04,440
maybe perhaps several GPU cards.

212
00:15:04,440 --> 00:15:11,320
You want to support kind of new and emerging hardware accelerators for machine learning

213
00:15:11,320 --> 00:15:16,200
that are appearing both on the data center side and high power environments, but also

214
00:15:16,200 --> 00:15:21,200
in sort of things like mobile phones or starting to get kind of these machine learning accelerators

215
00:15:21,200 --> 00:15:23,640
you want to make sure you take advantage of those as well.

216
00:15:23,640 --> 00:15:24,640
Yeah.

217
00:15:24,640 --> 00:15:31,000
On the topic of machine learning accelerators, Google's now in the second version of

218
00:15:31,000 --> 00:15:36,240
the TPU, the first version was focused on inference, the second version added, you know,

219
00:15:36,240 --> 00:15:37,240
training capability.

220
00:15:37,240 --> 00:15:44,280
I'm curious what your general thoughts are on the acceleration space.

221
00:15:44,280 --> 00:15:49,320
I mean, you've noted a couple of times now at nips and this presentation, you were at

222
00:15:49,320 --> 00:15:54,760
I'm sure other places that, you know, when you look into your crystal ball, you, you

223
00:15:54,760 --> 00:15:59,240
see that it's foggy in terms of what we're going to need from accelerators.

224
00:15:59,240 --> 00:16:04,120
And so that leads you to some specific, um, that leads you to think that we should be

225
00:16:04,120 --> 00:16:10,040
a little bit more open ended in terms of where we're going with hardware accelerators.

226
00:16:10,040 --> 00:16:14,840
You can do a better job of, you know, clarifying your position on these.

227
00:16:14,840 --> 00:16:15,840
Yeah.

228
00:16:15,840 --> 00:16:21,200
So I would say about four years ago, we realized that neural nets were going to be a really

229
00:16:21,200 --> 00:16:26,280
big sort of feature in lots of our computation at Google.

230
00:16:26,280 --> 00:16:27,280
Yeah.

231
00:16:27,280 --> 00:16:32,600
And if you look at trying to run all the inference, we wanted on the existing data center

232
00:16:32,600 --> 00:16:38,040
machines with CPUs and them, that was probably not going to cut it because we wanted to,

233
00:16:38,040 --> 00:16:42,920
for example, I did some very rough back at the envelope calculations and said, okay, if

234
00:16:42,920 --> 00:16:46,760
every user talks to their phone for three minutes a day and we need to do speech recognition

235
00:16:46,760 --> 00:16:52,320
on that, then that alone would mean we need to double the number of data centers we had

236
00:16:52,320 --> 00:16:54,320
at a very rough level.

237
00:16:54,320 --> 00:16:57,200
And that sounded kind of untenable.

238
00:16:57,200 --> 00:17:02,640
So we realized that inference was going to be the first important thing to tackle from

239
00:17:02,640 --> 00:17:09,720
an acceleration standpoint because any services where you have a growing number of users or

240
00:17:09,720 --> 00:17:14,720
a lot of users already inference is a much bigger cost than training typically.

241
00:17:14,720 --> 00:17:18,600
And inference is also a much simpler problem to tackle from an acceleration standpoint

242
00:17:18,600 --> 00:17:24,840
because typically you can get a single chip to do inference for a whole model.

243
00:17:24,840 --> 00:17:28,840
And then if you need more capacity, you can just stamp out more copies of that chip or

244
00:17:28,840 --> 00:17:35,200
more cards that you insert in a machine or a bunch of machines kind of embarrassingly parallel.

245
00:17:35,200 --> 00:17:36,720
It's embarrassingly parallel.

246
00:17:36,720 --> 00:17:41,920
Each chip can handle inference for the model in some modest batch size and you just kind

247
00:17:41,920 --> 00:17:43,360
of go from there.

248
00:17:43,360 --> 00:17:49,240
And so the first version of the TPUs that we built was an inference only accelerator, really.

249
00:17:49,240 --> 00:17:53,360
It supports 8-bit quantized integer arithmetic.

250
00:17:53,360 --> 00:17:57,200
There's a paper in Iska, which is a big computer architecture conference that describes

251
00:17:57,200 --> 00:18:02,520
that and more detail of readers or listeners are interested in the details.

252
00:18:02,520 --> 00:18:08,680
But we tackled that first because that would help us with scaling and using neural nets

253
00:18:08,680 --> 00:18:10,920
in many more places in our products.

254
00:18:10,920 --> 00:18:20,680
So now whenever you do a query or you're touching inference accelerators in many ways.

255
00:18:20,680 --> 00:18:27,160
So for search ranking for various pieces of other aspects of dealing with forming the search

256
00:18:27,160 --> 00:18:33,760
page, for Google Translate, it's used whenever you speak into your phone and that recognition

257
00:18:33,760 --> 00:18:34,760
is done in the data center.

258
00:18:34,760 --> 00:18:37,120
There's inference accelerators there helping with that.

259
00:18:37,120 --> 00:18:42,120
Is the implication then that you're using inference accelerators beyond deep neural nets

260
00:18:42,120 --> 00:18:46,440
and to, you know, for traditional machine learning as well?

261
00:18:46,440 --> 00:18:49,120
No, all those are deep neural.

262
00:18:49,120 --> 00:18:55,840
Yes, that's another wave that's been sweeping across many, many Google products and features

263
00:18:55,840 --> 00:18:59,680
in Google products and so on is we're using deep learning.

264
00:18:59,680 --> 00:19:05,680
You know, initially when we started looking at neural nets in our group in 2011, you know,

265
00:19:05,680 --> 00:19:10,680
a handful of production groups started, product groups started looking at, you know, how

266
00:19:10,680 --> 00:19:16,680
can we use them so we work closely with the speech team, the image search team.

267
00:19:16,680 --> 00:19:23,760
And with disbelief, our first software system, that made it easier for teams to sort of

268
00:19:23,760 --> 00:19:29,000
start training neural nets for their own problems they cared about.

269
00:19:29,000 --> 00:19:33,880
And so, you know, eventually 50 teams or 100 teams were using deep neural nets with the

270
00:19:33,880 --> 00:19:37,680
first system disbelief that we put together.

271
00:19:37,680 --> 00:19:43,120
And like many things, it kind of spreads organically throughout the organization because as soon

272
00:19:43,120 --> 00:19:48,360
as one team uses deep neural nets to solve a problem and another team kind of hears about

273
00:19:48,360 --> 00:19:50,560
it and says, Hey, that sounds a lot like our problem.

274
00:19:50,560 --> 00:19:53,880
I wonder if we could try it for our problem as well.

275
00:19:53,880 --> 00:19:57,760
Then and there's, you know, mailing lists where people can post questions like, Hey, I'm

276
00:19:57,760 --> 00:20:00,880
having trouble with this and people will chime in.

277
00:20:00,880 --> 00:20:05,840
That's generally the way these kinds of things spread throughout Google.

278
00:20:05,840 --> 00:20:10,520
And so we have this kind of nice exponential looking curve of how many different deep learning

279
00:20:10,520 --> 00:20:14,360
models are used in production across lots of Google products.

280
00:20:14,360 --> 00:20:18,880
You have a sense for on a percentage basis.

281
00:20:18,880 --> 00:20:25,320
For example, how much of that usage is, you know, what we traditionally associate with

282
00:20:25,320 --> 00:20:32,120
deep learning, you know, audio, video, image versus other types of use cases.

283
00:20:32,120 --> 00:20:38,720
Um, I mean, quite a lot of it is in sort of more textual oriented applications, including

284
00:20:38,720 --> 00:20:39,720
text in that.

285
00:20:39,720 --> 00:20:47,640
Yeah, I guess I'm curious, um, more specifically if, if Google is, you know, in this time working

286
00:20:47,640 --> 00:20:54,240
with deep learning, identified, um, and, you know, productize, I guess, thinking about

287
00:20:54,240 --> 00:20:59,520
mapping, you know, non-traditional deep learning problems to the deep learning domain.

288
00:20:59,520 --> 00:21:07,280
I mean, I think our experience has been that for problems where you have sufficient data

289
00:21:07,280 --> 00:21:11,400
and sufficient computation, you can apply deep learning to a wide variety of problems,

290
00:21:11,400 --> 00:21:17,440
both kind of more perceptual kinds of things like speech and vision, but also natural language

291
00:21:17,440 --> 00:21:22,240
processing problems, kind of more structured prediction problems.

292
00:21:22,240 --> 00:21:28,280
Um, and in general, you know, it doesn't work perfectly on every problem, but the vast

293
00:21:28,280 --> 00:21:32,520
majority of problems we try these approaches on tend to work pretty well.

294
00:21:32,520 --> 00:21:38,200
So, uh, is there ever a problem at Google for which you don't have enough data?

295
00:21:38,200 --> 00:21:39,200
Sure.

296
00:21:39,200 --> 00:21:46,080
I mean, we're, there are definitely some, like if you want to predict if, uh, a machine

297
00:21:46,080 --> 00:21:50,800
is failing, that sometimes, you know, you don't have that many examples of failed machines.

298
00:21:50,800 --> 00:21:56,160
So, uh, that's the kind of thing where you might have, you know, less data than other kinds

299
00:21:56,160 --> 00:21:57,160
of problems.

300
00:21:57,160 --> 00:21:59,480
Kind of the anomaly oriented cases.

301
00:21:59,480 --> 00:22:04,960
One of the things that is a pretty useful general technique for what you might think of

302
00:22:04,960 --> 00:22:13,640
as non-traditional, uh, things is to take kind of discrete features, uh, like maybe you

303
00:22:13,640 --> 00:22:20,280
have some model that wants to use the country where a user is located.

304
00:22:20,280 --> 00:22:25,960
And you can actually put, uh, discrete features into an embedding space.

305
00:22:25,960 --> 00:22:30,760
So similar to like the word-to-vec model where you have discrete words and then you represent

306
00:22:30,760 --> 00:22:35,280
each one of those words by say a hundred dimensional or a thousand dimensional vector.

307
00:22:35,280 --> 00:22:39,880
And through the optimization process, you kind of nudge these vectors around in a super

308
00:22:39,880 --> 00:22:41,600
high dimensional space.

309
00:22:41,600 --> 00:22:45,680
So that similar things are kind of nearer each other in this high dimensional space.

310
00:22:45,680 --> 00:22:51,400
You can actually do that with more discrete things like the country where a user is located

311
00:22:51,400 --> 00:22:57,640
or whether they're on a, uh, what kind of model phone they're on, uh, because maybe some

312
00:22:57,640 --> 00:23:02,760
phones have very big screens and people behave differently in how they use those than

313
00:23:02,760 --> 00:23:03,760
other kinds of phones.

314
00:23:03,760 --> 00:23:11,200
And so you can put an embedding vector on the model of phone and then kind of nudge, uh,

315
00:23:11,200 --> 00:23:18,080
the point in this high dimensional phone or device space, uh, nearer to ones that seem

316
00:23:18,080 --> 00:23:24,000
to behave similarly and, and other ones maybe form other kinds of, of, of clusters in,

317
00:23:24,000 --> 00:23:25,400
in this high dimensional space.

318
00:23:25,400 --> 00:23:26,400
Okay.

319
00:23:26,400 --> 00:23:30,560
And that's a pretty common technique we use across lots of different problems here that

320
00:23:30,560 --> 00:23:34,560
is not really a research thing.

321
00:23:34,560 --> 00:23:38,400
It's kind of a more of a practitioner like if I'm trying to solve this kind of problem

322
00:23:38,400 --> 00:23:44,080
with a bunch of discrete features, that's something you might try, uh, using, but it's not

323
00:23:44,080 --> 00:23:49,520
like there's really a good single paper you go to to discuss this except word to veck

324
00:23:49,520 --> 00:23:54,760
kind of is, or things like that, uh, is representative of that, but it's not necessarily

325
00:23:54,760 --> 00:24:00,040
obvious that you can do this for non words and non language things, but for all kinds

326
00:24:00,040 --> 00:24:01,040
of things.

327
00:24:01,040 --> 00:24:02,040
Okay.

328
00:24:02,040 --> 00:24:04,760
You know, if you had videos you're trying to recommend into my dimensional space, you

329
00:24:04,760 --> 00:24:08,760
can represent each video as a high dimensional vector, right, right.

330
00:24:08,760 --> 00:24:14,920
So we're talking about the TPU and, uh, hardware accelerations.

331
00:24:14,920 --> 00:24:19,720
Generally, do you have a sense for, you know, when you did your back at the envelope calculation,

332
00:24:19,720 --> 00:24:24,960
did you project out, you know, some number of years and, and kind of get a sense for the

333
00:24:24,960 --> 00:24:29,320
percent of, you know, total, compute at Google that will be dedicated to inference.

334
00:24:29,320 --> 00:24:31,520
Do you have a feeling for that?

335
00:24:31,520 --> 00:24:38,000
Um, I don't have a super training feeling because we're still discovering, uh, training

336
00:24:38,000 --> 00:24:39,440
is certainly a big deal as well.

337
00:24:39,440 --> 00:24:45,760
So, um, our first generation TPU was for inference because that's both, uh, was a more pressing

338
00:24:45,760 --> 00:24:49,560
need in 2013, 2014, but also it's a simpler problem.

339
00:24:49,560 --> 00:24:55,160
You design a chip on a board and that's pretty much the extent of it for training.

340
00:24:55,160 --> 00:25:00,120
That's a much sort of broader scale problem as a, at a systems level because you need

341
00:25:00,120 --> 00:25:06,080
to design both accelerator chips, but also a much bigger scale system that connects those

342
00:25:06,080 --> 00:25:11,960
chips together because unlike inference, it doesn't sort of scale just embarrassingly

343
00:25:11,960 --> 00:25:14,200
parallel where you just plunk in a bunch more cards.

344
00:25:14,200 --> 00:25:19,240
You actually need more compute than you can fit in a single chip for most training, for

345
00:25:19,240 --> 00:25:21,680
large scale training problems.

346
00:25:21,680 --> 00:25:26,720
And then you need, therefore to take a much more holistic systems view, how are we going

347
00:25:26,720 --> 00:25:31,240
to build sort of a much larger scale system that connects together a bunch of these chips

348
00:25:31,240 --> 00:25:38,320
with very high speed interconnects, how can I, you know, have a software story that's easy

349
00:25:38,320 --> 00:25:47,360
to take a computation I care about and map it onto, say, a system with 64 or 256 chips

350
00:25:47,360 --> 00:25:52,720
and have that be sort of easy to use for machine learning researchers and practitioners without

351
00:25:52,720 --> 00:25:58,120
sort of getting into the guts of, you know, them worrying too much about how to parallelize

352
00:25:58,120 --> 00:26:01,760
things across these, these different devices.

353
00:26:01,760 --> 00:26:04,160
And so it's just a much bigger scale problem.

354
00:26:04,160 --> 00:26:09,680
And so the second generation that we tackled was a system for both training and inference

355
00:26:09,680 --> 00:26:15,160
and was much more holistic than a single chip, it's like these large scale TPU devices,

356
00:26:15,160 --> 00:26:17,000
which are four chips on a board.

357
00:26:17,000 --> 00:26:20,600
And then those are designed to be connected together into much larger configurations

358
00:26:20,600 --> 00:26:29,200
of 64 devices, 256 chips, there's a custom 16 by 16 mesh network that, uh, or torridal

359
00:26:29,200 --> 00:26:34,280
mesh, actually with wraparound links on either end on the device or on the whole broader

360
00:26:34,280 --> 00:26:35,280
system.

361
00:26:35,280 --> 00:26:39,920
So the whole system is like four racks in size, looks really like a machine learning

362
00:26:39,920 --> 00:26:41,520
supercomputer.

363
00:26:41,520 --> 00:26:42,520
Uh-huh.

364
00:26:42,520 --> 00:26:47,600
And those are the kinds of things that we're now applying to much larger scale problems.

365
00:26:47,600 --> 00:26:54,240
And the reason we're doing that is because we see, uh, continuing gains as you're able

366
00:26:54,240 --> 00:27:01,000
to tackle larger and larger problems, um, so for example, the deep learning work that

367
00:27:01,000 --> 00:27:07,920
we rolled out to sort of completely replace the old style phrase based translation system

368
00:27:07,920 --> 00:27:15,000
and Google translate with a neural net, um, is, uh, impactful in many ways.

369
00:27:15,000 --> 00:27:22,000
So one is that system, uh, uh, had been around for a decade, I actually helped do some of

370
00:27:22,000 --> 00:27:29,520
the early low level software for some parts of that system, um, and it was a very complicated

371
00:27:29,520 --> 00:27:37,440
system in software terms that had like five or six, uh, different sort of, uh, distributed

372
00:27:37,440 --> 00:27:38,960
systems in it.

373
00:27:38,960 --> 00:27:44,320
And then, uh, about, you know, there's like a, uh, uh, target language model that is spread

374
00:27:44,320 --> 00:27:49,320
across a hundred machines where you do lookups of how often a bunch of five word phrases

375
00:27:49,320 --> 00:27:54,480
occur and that language model can tell you how often every five word phrase in English

376
00:27:54,480 --> 00:27:55,480
occurs.

377
00:27:55,480 --> 00:27:59,560
There's a alignment model for how words in English and French align.

378
00:27:59,560 --> 00:28:00,560
Okay.

379
00:28:00,560 --> 00:28:05,000
And then there's like phrase tables, dictionaries and then 500,000 lines of code to glue all

380
00:28:05,000 --> 00:28:06,000
this together.

381
00:28:06,000 --> 00:28:14,000
Um, so the new system is literally a TensorFlow model, uh, with a neural machine translation

382
00:28:14,000 --> 00:28:21,200
model that we've published a paper about and is like 500 lines of TensorFlow, and just

383
00:28:21,200 --> 00:28:27,360
learn from lots and lots of data and not only is the system much simpler, but the translation

384
00:28:27,360 --> 00:28:34,200
quality is much, much higher, uh, the gains we saw from rolling, moving from the old system

385
00:28:34,200 --> 00:28:40,640
to this new system were equivalent or larger than the gains in quality improvements in

386
00:28:40,640 --> 00:28:44,160
the previous decade of working on improving the old system.

387
00:28:44,160 --> 00:28:49,720
And so that's just a sign of something could be dramatically simpler and because it's

388
00:28:49,720 --> 00:28:55,040
learning from lots of observations, it can actually do a better job of the task you

389
00:28:55,040 --> 00:28:56,040
care about.

390
00:28:56,040 --> 00:29:03,440
And, uh, the, the way this gets back to TPUs is for some of the language pairs that we

391
00:29:03,440 --> 00:29:07,280
rolled this out on, we actually have a lot of data and we can only train on one sixth

392
00:29:07,280 --> 00:29:11,760
of the data we have, we can only get through one sixth of the data once in the sort of

393
00:29:11,760 --> 00:29:15,040
time budget we'd set for each language pair.

394
00:29:15,040 --> 00:29:16,040
Okay.

395
00:29:16,040 --> 00:29:19,800
And that system supports, you know, roughly a hundred to a hundred other languages.

396
00:29:19,800 --> 00:29:22,520
So there's like 10,000 language pairs you need to support.

397
00:29:22,520 --> 00:29:23,520
Okay.

398
00:29:23,520 --> 00:29:29,040
And so that's now on TPU 2 or was that before, uh, that's now, uh, training on, on TPUs

399
00:29:29,040 --> 00:29:32,800
and it's actually, uh, running inference on TPU V1.

400
00:29:32,800 --> 00:29:33,800
Okay.

401
00:29:33,800 --> 00:29:40,880
So, um, but, but, uh, before we had TPU V2 rolled out, it was training on lots of GPU cards.

402
00:29:40,880 --> 00:29:41,880
Okay.

403
00:29:41,880 --> 00:29:49,880
Do you see the TPU effort broadly kind of replacing GPUs at Google and kind of decoupling

404
00:29:49,880 --> 00:29:53,120
Google from a dependency on GPU suppliers?

405
00:29:53,120 --> 00:29:56,400
Um, I wouldn't frame it quite like that.

406
00:29:56,400 --> 00:30:05,040
I would say, you know, because we're designing both the TPU and we have a lot of machine learning

407
00:30:05,040 --> 00:30:13,280
researchers and also our building software to map these, uh, sort of style models onto

408
00:30:13,280 --> 00:30:20,640
the TPU, we can have, uh, sort of much tighter feedback loops in sort of doing vertically

409
00:30:20,640 --> 00:30:25,920
integrated decision making for, for what the future of TPUs should look like.

410
00:30:25,920 --> 00:30:30,920
But there's some things that TPUs don't necessarily do well, that GPUs do extremely well.

411
00:30:30,920 --> 00:30:35,200
Uh, but the kinds of things that do run on TPUs, you know, we think are pretty interesting

412
00:30:35,200 --> 00:30:39,600
in compelling, uh, for my, you know, performance standpoint.

413
00:30:39,600 --> 00:30:45,040
One of the things we really crave is to make turnaround time for machine learning research

414
00:30:45,040 --> 00:30:47,840
experiments much faster.

415
00:30:47,840 --> 00:30:53,640
And so, uh, you know, there's a qualitative difference as a researcher if your fundamental

416
00:30:53,640 --> 00:30:59,800
experimental turnaround time is measured in, you know, an hour rather than a week, right?

417
00:30:59,800 --> 00:31:02,400
You just, it just feels very different.

418
00:31:02,400 --> 00:31:03,600
You can be much more productive.

419
00:31:03,600 --> 00:31:04,960
You can try more things out.

420
00:31:04,960 --> 00:31:09,520
It's not a big, you know, effort where you start up an experiment and then you've like

421
00:31:09,520 --> 00:31:13,720
forgotten what you've actually started by the time it finishes a week instead, you

422
00:31:13,720 --> 00:31:15,200
try many more things.

423
00:31:15,200 --> 00:31:20,600
Uh, and so getting that time down just improves the productivity and the rate at which we

424
00:31:20,600 --> 00:31:24,920
can try out new ideas, which is really super important in this kind of new and emerging

425
00:31:24,920 --> 00:31:25,920
space.

426
00:31:25,920 --> 00:31:30,680
And is that an exercise in driving up tariff laps as quickly as possible or is it more

427
00:31:30,680 --> 00:31:31,680
nuanced in that?

428
00:31:31,680 --> 00:31:34,280
Uh, so it's a few things, right?

429
00:31:34,280 --> 00:31:41,360
One is obviously you want more raw tariff laps, but also you want software layers that

430
00:31:41,360 --> 00:31:43,400
make it easy to use those tariff laps, right?

431
00:31:43,400 --> 00:31:48,560
You don't want to have to write complicated code in order to have a research idea you

432
00:31:48,560 --> 00:31:51,120
just thought of, be mapped onto those tariff laps.

433
00:31:51,120 --> 00:31:52,120
Right.

434
00:31:52,120 --> 00:31:55,800
So if you can make software tools that make it easy to express ideas that then can take

435
00:31:55,800 --> 00:32:02,600
advantage of say a whole TPU pod relatively quickly, um, that's just generally going to

436
00:32:02,600 --> 00:32:03,600
be better.

437
00:32:03,600 --> 00:32:08,800
And so it's really requires work at all these different levels of the sort of hardware

438
00:32:08,800 --> 00:32:17,080
design stack, the software stack, the ease of expression for different kinds of APIs

439
00:32:17,080 --> 00:32:20,320
that you might have added in using for expressing machine learning computations.

440
00:32:20,320 --> 00:32:21,320
Hmm.

441
00:32:21,320 --> 00:32:27,840
There are a set of things that, um, a set of tooling that Googlers take advantage or that

442
00:32:27,840 --> 00:32:35,640
Googlers kind of take for granted that, um, that you think that need to evolve, I guess

443
00:32:35,640 --> 00:32:41,160
more broadly and be, um, you know, someone needs to provide in order for, you know, folks

444
00:32:41,160 --> 00:32:47,440
to take advantage of deep learning and be as, uh, be able to erase quickly as Google does.

445
00:32:47,440 --> 00:32:55,000
Well, I think one of the things that we're, uh, trying to do is to, uh, and, and part of

446
00:32:55,000 --> 00:32:57,240
the reason we decided we would open source TensorFlow.

447
00:32:57,240 --> 00:32:58,240
Sure.

448
00:32:58,240 --> 00:33:03,000
So we open source TensorFlow at the end of 2015 was we wanted the tools that we used to

449
00:33:03,000 --> 00:33:08,640
also be sort of more broadly available to the community and then have the community collectively

450
00:33:08,640 --> 00:33:11,120
work with us to improve those tools.

451
00:33:11,120 --> 00:33:14,000
And I think that has actually played out reasonably well.

452
00:33:14,000 --> 00:33:16,640
We now have tons and tons of people using TensorFlow.

453
00:33:16,640 --> 00:33:22,040
We have, uh, you know, obviously a smaller set of people working to improve the core set

454
00:33:22,040 --> 00:33:28,360
of TensorFlow APIs and algorithms and implementations, uh, but lots of contributions from across

455
00:33:28,360 --> 00:33:33,440
lots of other big companies, you know, lots of, uh, smaller companies, people working on

456
00:33:33,440 --> 00:33:38,520
their own as hobbyists, um, really are, are working collectively to improve those tools.

457
00:33:38,520 --> 00:33:45,280
And I think that's, that's really good because it gives you a common language for expressing

458
00:33:45,280 --> 00:33:51,080
machine learning computations that is really useful to have ideas spread more rapidly through

459
00:33:51,080 --> 00:33:57,360
a community than they would if you relied on just publishing a paper and describing something.

460
00:33:57,360 --> 00:34:02,440
And when you do that, you necessarily, by necessity, leave out a lot of details, right?

461
00:34:02,440 --> 00:34:06,840
English is kind of a poor way, especially exactly what to do.

462
00:34:06,840 --> 00:34:13,120
And so by allowing people to share, sort of working implementations or open source models

463
00:34:13,120 --> 00:34:19,400
that they care about, uh, I think you can get that spread that was happening already within

464
00:34:19,400 --> 00:34:25,360
Google of one team picking up what another team has done and trying out in their problem.

465
00:34:25,360 --> 00:34:31,120
I guess I'm not a virus about, like, you know, so TensorFlow, you know, today is, you

466
00:34:31,120 --> 00:34:34,520
know, in some ways, you know, look at what the way Google was thinking about this five

467
00:34:34,520 --> 00:34:37,080
years ago and what was missing five years ago, right?

468
00:34:37,080 --> 00:34:43,160
If you look at, you know, the way Google's thinking about it today and, and, and, but,

469
00:34:43,160 --> 00:34:46,840
you know, we'll be still missing for the industry in five years.

470
00:34:46,840 --> 00:34:51,320
You know, what are, you know, what's the, kind of the future of that software stack out?

471
00:34:51,320 --> 00:34:56,640
This is kind of a clumsy way of asking the question, but, you know, Google had Borg, you

472
00:34:56,640 --> 00:35:00,680
know, and then that turned into Kubernetes, you know, there was a disbelief, which, you

473
00:35:00,680 --> 00:35:07,440
know, became kind of, uh, publicized as TensorFlow or evolved into TensorFlow in some ways.

474
00:35:07,440 --> 00:35:12,120
Uh, I'm imagining there's actually in the conversation with Ryan Poplin, you know, we talked

475
00:35:12,120 --> 00:35:18,440
about, um, a lot of the data augmentation pipeline that, you know, as a researcher, he

476
00:35:18,440 --> 00:35:19,440
just takes for granted.

477
00:35:19,440 --> 00:35:20,440
It's just there.

478
00:35:20,440 --> 00:35:27,680
Um, I can imagine that being publicized or turned into open source code by Google or,

479
00:35:27,680 --> 00:35:28,680
you know, someone else.

480
00:35:28,680 --> 00:35:34,920
I'm just wondering if, if, um, you know, we can kind of tap into your today as a crystal

481
00:35:34,920 --> 00:35:39,160
ball for, you know, what some of the gaps are in the software pipeline, uh, that need to

482
00:35:39,160 --> 00:35:44,720
be figured out over the next few years to really make deep learning, uh, a lot of folks

483
00:35:44,720 --> 00:35:46,160
to iterate more quickly.

484
00:35:46,160 --> 00:35:47,160
Right.

485
00:35:47,160 --> 00:35:53,880
So, uh, I mean, I think one of the things that's happening is people are open sourcing

486
00:35:53,880 --> 00:36:00,360
of wide variety of different tools built on top of TensorFlow or that sort of run in phases

487
00:36:00,360 --> 00:36:05,520
and then you sort of run TensorFlow as, as sort of the core computation engine.

488
00:36:05,520 --> 00:36:12,040
And I think that's really powerful and most of them are sort of particular things, rather

489
00:36:12,040 --> 00:36:18,120
than a general framework that makes something different, uh, but they all add together because

490
00:36:18,120 --> 00:36:26,040
now there's this, uh, really large community of different pieces and, uh, building blocks

491
00:36:26,040 --> 00:36:28,840
that you can choose from when you're solving your problems.

492
00:36:28,840 --> 00:36:33,800
Um, one of the things I'm pretty excited about in terms of the research our group is doing

493
00:36:33,800 --> 00:36:37,720
is this notion of automating machine learning, right?

494
00:36:37,720 --> 00:36:42,760
I mean, I think even today with the explosion of interest in machine learning and more and

495
00:36:42,760 --> 00:36:48,680
more people are entering the field, um, there's still an incredible shortage of people who

496
00:36:48,680 --> 00:36:56,160
actually know how to take, uh, data and computation and have the expertise to then make a solution

497
00:36:56,160 --> 00:36:58,040
to a machine learning problem.

498
00:36:58,040 --> 00:36:59,040
Yeah.

499
00:36:59,040 --> 00:37:03,200
Um, the way I like to phrase this is there's, you know, maybe 10,000 organizations in the

500
00:37:03,200 --> 00:37:08,600
world who are actually really practically applying machine learning in a production way

501
00:37:08,600 --> 00:37:13,720
to their problems and their, their settings, and have hired people with strong machine

502
00:37:13,720 --> 00:37:15,320
learning expertise.

503
00:37:15,320 --> 00:37:19,720
But there's probably 10 million organizations in the world that have data in electronic

504
00:37:19,720 --> 00:37:25,920
form that could be used for machine learning and have problems that would be amenable to

505
00:37:25,920 --> 00:37:26,920
machine learning.

506
00:37:26,920 --> 00:37:32,800
Uh, one example is, um, every city in the world should be setting their stoplight timing

507
00:37:32,800 --> 00:37:34,400
by machine learning, right?

508
00:37:34,400 --> 00:37:38,360
Like right now, they probably have two lookup tables, they have rush hour and not rush

509
00:37:38,360 --> 00:37:39,360
hour.

510
00:37:39,360 --> 00:37:40,360
Right.

511
00:37:40,360 --> 00:37:42,200
And you select among them via the time of day, yeah.

512
00:37:42,200 --> 00:37:47,400
But you can imagine with a computer vision based stoplight, you could just figure out,

513
00:37:47,400 --> 00:37:54,760
you know, are there cars coming in which direction and, and, uh, if not turn the light, uh, appropriately,

514
00:37:54,760 --> 00:37:59,520
and maximize throughput of, uh, cars through an intersection or through the collective set

515
00:37:59,520 --> 00:38:03,320
of intersections in a city, that's not something most cities are probably doing today.

516
00:38:03,320 --> 00:38:04,320
Right.

517
00:38:04,320 --> 00:38:06,360
Some are, but most of them are probably not.

518
00:38:06,360 --> 00:38:14,040
Um, and so I think we'd like, so the idea behind auto ML or metal learning is that you

519
00:38:14,040 --> 00:38:20,280
want to be able to solve new machine learning problems, um, automatically without a human

520
00:38:20,280 --> 00:38:24,120
machine learning expert sitting down and saying, okay, I'm going to solve that in this particular

521
00:38:24,120 --> 00:38:25,120
way.

522
00:38:25,120 --> 00:38:30,440
And I think that's going to be a really promising and, and fruitful way to get machine,

523
00:38:30,440 --> 00:38:36,720
more machine learning used in the world to solve problems that are, are important.

524
00:38:36,720 --> 00:38:44,360
Uh, what do you project the timeline or how do you see, how do you see auto ML evolving

525
00:38:44,360 --> 00:38:45,360
over time?

526
00:38:45,360 --> 00:38:46,360
Yeah.

527
00:38:46,360 --> 00:38:50,480
So, you know, we're continuing to push on the research aspect of it.

528
00:38:50,480 --> 00:38:55,960
It's obviously an ambitious problem to be able to take any problem and solve it automatically.

529
00:38:55,960 --> 00:39:03,600
Uh, I would say what we have now is we can now solve problems in a set of restricted

530
00:39:03,600 --> 00:39:08,920
domains, uh, automatically, and be able to take a new problem and automatically solve it

531
00:39:08,920 --> 00:39:15,720
in one of those domains and, uh, yeah, so, uh, we actually have a launched, uh, product

532
00:39:15,720 --> 00:39:20,880
that we've jointly worked on with our cloud organization, cloud auto ML, that initially

533
00:39:20,880 --> 00:39:25,960
can solve computer vision problems, and there's tons and tons of organizations that fall

534
00:39:25,960 --> 00:39:29,920
into the category I was saying where they have, you know, a bunch of images of things

535
00:39:29,920 --> 00:39:35,560
to care about, um, the, the sort of default image net model isn't exactly right for their

536
00:39:35,560 --> 00:39:36,560
problem.

537
00:39:36,560 --> 00:39:37,560
Okay.

538
00:39:37,560 --> 00:39:41,600
Uh, like they want to be able to identify as this broken airplane part on their assembly

539
00:39:41,600 --> 00:39:46,000
line or, uh, not broken one, so they have a bunch of images of that.

540
00:39:46,000 --> 00:39:49,200
Some of which are labeled as, you know, broken propeller and some of which are labeled

541
00:39:49,200 --> 00:39:54,520
as, you know, not broken flange of something.

542
00:39:54,520 --> 00:40:00,280
And so you can take those image datasets and now automatically solve classification problems

543
00:40:00,280 --> 00:40:05,520
to high level of accuracy, much more than you can get just by doing transfer learning from

544
00:40:05,520 --> 00:40:08,440
say an image net model that you've already trained.

545
00:40:08,440 --> 00:40:13,800
And that's already going to be useful for a pretty wide variety of, of companies and

546
00:40:13,800 --> 00:40:20,720
organizations and broadening out the set of domains in which that approach works is something

547
00:40:20,720 --> 00:40:21,720
reactively working out.

548
00:40:21,720 --> 00:40:26,160
You'd like to do this for language problems for, you know, recommendation problems, speech

549
00:40:26,160 --> 00:40:29,480
problems, uh, a wide variety of these things.

550
00:40:29,480 --> 00:40:30,480
Hmm.

551
00:40:30,480 --> 00:40:34,480
Do you have a, kind of a research taxonomy of auto ML?

552
00:40:34,480 --> 00:40:37,920
There's, um, you know, the neural architecture search.

553
00:40:37,920 --> 00:40:39,480
There's kind of meta learning stuff.

554
00:40:39,480 --> 00:40:42,880
How do you think about the way the different pieces and the way they fit together?

555
00:40:42,880 --> 00:40:43,880
Yeah.

556
00:40:43,880 --> 00:40:49,200
I think like we started this investigation in, in our group, uh, with the neural architecture

557
00:40:49,200 --> 00:40:54,280
search work, we've also been working on evolutionary approaches for, uh, those same kinds

558
00:40:54,280 --> 00:40:59,120
of, of problems, just as a way of, of comparing several different approaches for tackling the

559
00:40:59,120 --> 00:41:00,120
same problem.

560
00:41:00,120 --> 00:41:05,720
I think that's often useful to ground work is what you're doing the right direction by,

561
00:41:05,720 --> 00:41:07,960
you know, trying out several approaches.

562
00:41:07,960 --> 00:41:11,520
There's a bunch of other work we've been doing on learning things that are not just

563
00:41:11,520 --> 00:41:17,240
the architecture, but, uh, things like learning new optimization update rules, you know,

564
00:41:17,240 --> 00:41:20,880
traditionally, you have SGD, which is a very simple update rule.

565
00:41:20,880 --> 00:41:25,120
You take learning rate times the gradient and you add that to the parameters, uh, but,

566
00:41:25,120 --> 00:41:29,760
uh, and then there's things like SGD with momentum and atom and add a grad that have been

567
00:41:29,760 --> 00:41:35,800
developed, uh, by humans, uh, and seem to be effective for a variety of problems.

568
00:41:35,800 --> 00:41:43,760
Uh, we actually, we actually published a paper in our group on, um, trying to learn optimization

569
00:41:43,760 --> 00:41:49,360
update rules, symbolic optimization update rules and just gave it kind of the raw primitive

570
00:41:49,360 --> 00:41:56,280
things like the things that occur in the expressions for SGD and SGD with momentum and a bunch

571
00:41:56,280 --> 00:41:58,400
of other kinds of symbolic updates.

572
00:41:58,400 --> 00:42:02,920
And then it learned to stitch them together and it found sort of 15 different optimization

573
00:42:02,920 --> 00:42:07,200
update rules that are all better than all four of those, really, sort of things.

574
00:42:07,200 --> 00:42:08,200
Interesting.

575
00:42:08,200 --> 00:42:09,560
Um, which is pretty interesting.

576
00:42:09,560 --> 00:42:15,000
And some of them had sort of common characteristics that you can kind of, uh, get inside from.

577
00:42:15,000 --> 00:42:19,720
So that's kind of, um, and how did, how did you, how did you represent that problem so

578
00:42:19,720 --> 00:42:21,960
that you could do it in a symbolic domain?

579
00:42:21,960 --> 00:42:27,680
Oh, so you just, uh, uh, it's sort of similar to neural architecture search, except what

580
00:42:27,680 --> 00:42:32,880
you're spitting out is not an architecture, but a symbolic update expression for optimization.

581
00:42:32,880 --> 00:42:33,880
Okay.

582
00:42:33,880 --> 00:42:38,840
And so you give it a bunch of primitives and say, please spit me out a, a symbolic update

583
00:42:38,840 --> 00:42:45,200
rule that has, you know, no more than a depth of three and a symbolic expression for

584
00:42:45,200 --> 00:42:46,200
you or something.

585
00:42:46,200 --> 00:42:47,200
Okay.

586
00:42:47,200 --> 00:42:51,840
And then you can search over plausible, uh, symbolic update rules, including things like,

587
00:42:51,840 --> 00:42:55,640
you know, you accumulate the running average of the gradients or you, you multiply by

588
00:42:55,640 --> 00:43:01,880
the learning rate or you, you know, divide by the, uh, recent parameter updates or something.

589
00:43:01,880 --> 00:43:11,160
Um, and so one thing it found was, uh, an interesting rule of, I'm trying to remember the exact

590
00:43:11,160 --> 00:43:12,160
rule.

591
00:43:12,160 --> 00:43:17,800
It's like e to the sign of the current gradient times the sign of the recent running

592
00:43:17,800 --> 00:43:19,520
average of gradients.

593
00:43:19,520 --> 00:43:24,600
And so if those are the same, it's going to scale things by, by that.

594
00:43:24,600 --> 00:43:27,600
And if it, they're different, then it's going to slow way down.

595
00:43:27,600 --> 00:43:34,680
Um, just think basically just spits out your next 15 papers or something like that.

596
00:43:34,680 --> 00:43:38,400
Something like that, although probably you take the best one and, and do that.

597
00:43:38,400 --> 00:43:46,120
But, uh, you know, it is interesting that the kinds of things that are, if you have a problem

598
00:43:46,120 --> 00:43:51,320
that is automatable in this sort of searchable way where you have a clear reward function and

599
00:43:51,320 --> 00:43:57,000
some space to search over, I think this, this is more general than just how do you find

600
00:43:57,000 --> 00:44:03,000
machine learning sort of ideas, but scientific ideas more generally.

601
00:44:03,000 --> 00:44:07,000
If you can automate that process because what, what a scientist does is they run a bunch

602
00:44:07,000 --> 00:44:11,040
of experiments and then they look at the results of those experiments and then they figure

603
00:44:11,040 --> 00:44:13,520
out what are the next experiments to run, right?

604
00:44:13,520 --> 00:44:19,960
And so integrating the results of the previous experiment back into deciding what the next

605
00:44:19,960 --> 00:44:26,400
things are to run is generally a pretty slow process if there's a human in that interpretation

606
00:44:26,400 --> 00:44:27,400
loop.

607
00:44:27,400 --> 00:44:33,680
And so you can actually get a lot of benefit for real world problems if you have a just

608
00:44:33,680 --> 00:44:40,400
sort of a clear reward signal by essentially using reinforcement learning or perhaps evolutionary

609
00:44:40,400 --> 00:44:43,640
search to optimize that reward.

610
00:44:43,640 --> 00:44:46,760
Are you doing much with, uh, with program learning?

611
00:44:46,760 --> 00:44:49,120
Have you published on program learning?

612
00:44:49,120 --> 00:44:53,360
We, we do have a, I mean, you can think of neural architecture search as a very restricted

613
00:44:53,360 --> 00:45:00,720
form of program learning and we're also doing some more kind of more traditional symbolic

614
00:45:00,720 --> 00:45:05,440
computer programming language synthesis work.

615
00:45:05,440 --> 00:45:11,520
We've published a little bit of that, but not, it's sort of an early emerging research

616
00:45:11,520 --> 00:45:12,520
area for us.

617
00:45:12,520 --> 00:45:16,320
We're pretty excited about it because we think it's got a lot of potential benefit to sort

618
00:45:16,320 --> 00:45:23,760
of provide tools for programmers to sort of take some of the work that is traditionally

619
00:45:23,760 --> 00:45:27,400
done by programmers that could be done in a more automated way and a lot of programmers

620
00:45:27,400 --> 00:45:32,880
to spend more of their time on the things that are sort of higher level, more difficult

621
00:45:32,880 --> 00:45:38,600
to, to do and take away some of the drudgery work that, uh, programmers currently sometimes

622
00:45:38,600 --> 00:45:39,600
do.

623
00:45:39,600 --> 00:45:43,080
So we're, where do you think, uh, how would you characterize, you know, where we are

624
00:45:43,080 --> 00:45:45,880
and how that work evolves over time?

625
00:45:45,880 --> 00:45:46,880
Yeah.

626
00:45:46,880 --> 00:45:50,560
I mean, I would say we're pretty early in that work really having real world impact,

627
00:45:50,560 --> 00:45:55,400
but I think it's, it's definitely an interesting direction because obviously if you can, if

628
00:45:55,400 --> 00:46:00,440
you can look at a problem and come up with an algorithmic expression of how to solve

629
00:46:00,440 --> 00:46:04,960
that problem, that's a pretty important and impactful thing.

630
00:46:04,960 --> 00:46:07,680
But we're very far from being able to do that in general.

631
00:46:07,680 --> 00:46:08,680
Yeah.

632
00:46:08,680 --> 00:46:13,200
Maybe circling back to TPUs, you know, what can you say about TPU V3?

633
00:46:13,200 --> 00:46:17,320
Like what, what needs to be, you know, what, where does the work need to continue there

634
00:46:17,320 --> 00:46:20,440
and, and what do you expect that the future holds in that direction?

635
00:46:20,440 --> 00:46:21,440
Right.

636
00:46:21,440 --> 00:46:27,600
I mean, I think one of the things about machine learning accelerator hardware is it's hard

637
00:46:27,600 --> 00:46:33,080
to really read the future out far enough in this very fast moving field.

638
00:46:33,080 --> 00:46:38,880
Um, you know, if you look at the, the given indication of how fast this field is moving,

639
00:46:38,880 --> 00:46:44,800
uses great exponential growth chart of the number of machine learning papers on archives

640
00:46:44,800 --> 00:46:50,520
since 2009 or 10 or something, it's gone from like a thousand per year, up to 20,000

641
00:46:50,520 --> 00:46:51,520
papers per year.

642
00:46:51,520 --> 00:46:52,520
Right.

643
00:46:52,520 --> 00:46:53,520
Right.

644
00:46:53,520 --> 00:46:56,960
So that's like almost a hundred papers per day being published in the machine learning

645
00:46:56,960 --> 00:47:01,040
field, you know, on archive, which is sort of crazy, right?

646
00:47:01,040 --> 00:47:06,560
It's like an entire conference every day, um, you know, obviously they're not all peer

647
00:47:06,560 --> 00:47:07,560
reviewed and so on.

648
00:47:07,560 --> 00:47:12,480
But it's a very fast moving field with lots of people with different kinds of ideas

649
00:47:12,480 --> 00:47:16,920
and backgrounds coming into the field and bringing their perspective on how to solve problems

650
00:47:16,920 --> 00:47:17,920
there.

651
00:47:17,920 --> 00:47:18,920
Mm-hmm.

652
00:47:18,920 --> 00:47:23,000
And if you develop yet the, the deep neural net to summarize all those papers off of archives

653
00:47:23,000 --> 00:47:25,200
so that a researcher could actually keep up.

654
00:47:25,200 --> 00:47:29,800
We are working on some words, but sadly it is not yet at the level where we can understand

655
00:47:29,800 --> 00:47:33,440
all the archives papers published today and tell you which ones to look at, but that would

656
00:47:33,440 --> 00:47:34,640
be cool.

657
00:47:34,640 --> 00:47:40,080
Um, but I think that's an example of, you know, if you're starting to design a chip or

658
00:47:40,080 --> 00:47:45,320
a system for doing machine learning acceleration, you know, currently what we focused on is trying

659
00:47:45,320 --> 00:47:52,000
to make, um, low precision linear algebra super fast, right, because that seems to be a basic

660
00:47:52,000 --> 00:47:57,280
building block that applies to nearly all the deep learning, uh, sort of algorithms and

661
00:47:57,280 --> 00:47:59,240
systems we've looked at.

662
00:47:59,240 --> 00:48:00,240
Mm-hmm.

663
00:48:00,240 --> 00:48:06,040
A bunch of kind of interesting exploratory work in very low precision kinds of work that

664
00:48:06,040 --> 00:48:12,480
have been sort of proven out on small scale problems in software simulation to show that

665
00:48:12,480 --> 00:48:18,800
in some cases you can make, uh, four bit weights and four bit activations work.

666
00:48:18,800 --> 00:48:23,280
And obviously if that worked in general across all problems you cared about, that would be

667
00:48:23,280 --> 00:48:24,280
pretty cool.

668
00:48:24,280 --> 00:48:25,280
Mm-hmm.

669
00:48:25,280 --> 00:48:29,600
And you might really sort of, uh, bet on that in your accelerator hardware.

670
00:48:29,600 --> 00:48:37,080
I think the state of this is such that it's been shown to work on some kinds of problems

671
00:48:37,080 --> 00:48:42,800
on modest data set sizes, some more experiences needed before you really bet on putting it

672
00:48:42,800 --> 00:48:46,240
in as the only thing in your accelerator, right?

673
00:48:46,240 --> 00:48:47,240
Okay.

674
00:48:47,240 --> 00:48:51,480
Because your accelerator, if you design it today, sort of pops out of the fab and into

675
00:48:51,480 --> 00:48:55,560
your data centers a year and a half or two years from now and it has to live for three

676
00:48:55,560 --> 00:48:56,560
years.

677
00:48:56,560 --> 00:48:57,560
Right.

678
00:48:57,560 --> 00:49:03,040
So wanting to bet on things that you're pretty comfortable are going to be important

679
00:49:03,040 --> 00:49:06,880
things for the two to five year time frame from now.

680
00:49:06,880 --> 00:49:13,120
And so that means, uh, you can have some experimental things in there, but not sort of the whole

681
00:49:13,120 --> 00:49:15,000
chip being experimental.

682
00:49:15,000 --> 00:49:21,480
Although I will say there's a bunch of startups now in this space that are, you know, essentially

683
00:49:21,480 --> 00:49:25,480
being funded by, by the VC community to try out ideas.

684
00:49:25,480 --> 00:49:30,160
And I think that's actually kind of cool because there's a lot of ideas being tried out in

685
00:49:30,160 --> 00:49:36,840
the startups space, some of which are likely not to be the best ideas, but are the experiments

686
00:49:36,840 --> 00:49:37,840
are being run.

687
00:49:37,840 --> 00:49:38,840
Yeah.

688
00:49:38,840 --> 00:49:43,400
And some of which may turn out to be influential and interesting and exciting and inform

689
00:49:43,400 --> 00:49:48,200
what we should do in terms of building sort of next generations of ML accelerators.

690
00:49:48,200 --> 00:49:49,200
Yeah.

691
00:49:49,200 --> 00:49:54,960
I've kind of positive that in a world where we're deep learning, machine learning, enterprise

692
00:49:54,960 --> 00:49:59,760
is general workloads are kind of shifting to the cloud over time.

693
00:49:59,760 --> 00:50:05,560
And all of the cloud vendors are investing heavily in, you know, their own hardware to accelerate

694
00:50:05,560 --> 00:50:11,320
their, uh, their workloads in part, I propose or supposed to kind of make sure they're not

695
00:50:11,320 --> 00:50:18,120
dependent on any particular, uh, accelerator, uh, vendor, uh, in their supply chain.

696
00:50:18,120 --> 00:50:22,600
Uh, I kind of wonder what the, you know, what the future holds for, you know, these startups

697
00:50:22,600 --> 00:50:25,960
that are kind of going off on their own and building out these new accelerators, like

698
00:50:25,960 --> 00:50:27,560
who are their customers going to be?

699
00:50:27,560 --> 00:50:31,440
You know, what's your, what's your take on that market and kind of how it shakes out?

700
00:50:31,440 --> 00:50:37,080
Well, I think, uh, you know, it's hard to say and, and I'm not in the business of picking

701
00:50:37,080 --> 00:50:40,640
winners in the chip startup space, which is, which is fairly good.

702
00:50:40,640 --> 00:50:44,320
Uh, but I think, you know, obviously some of them might get acquired.

703
00:50:44,320 --> 00:50:49,200
Uh, some of them will sell equipment to cloud providers, probably cloud providers or to

704
00:50:49,200 --> 00:50:53,480
people who want on-premise machine learning and not running their, their computer in the

705
00:50:53,480 --> 00:50:54,480
cloud.

706
00:50:54,480 --> 00:51:01,120
Um, so I think there, there's definitely avenues for them to be successful and, and it'll,

707
00:51:01,120 --> 00:51:05,560
it's just a little unclear exactly which of the ideas that are being tried out will be

708
00:51:05,560 --> 00:51:10,920
sort of good in terms of, you know, providing really high performance for general things

709
00:51:10,920 --> 00:51:15,880
or maybe there's some restricted set of models or one particular hardware vendors thing

710
00:51:15,880 --> 00:51:19,560
is super fast, but it doesn't really run other things very fast.

711
00:51:19,560 --> 00:51:25,520
Um, I do think part of this is driven by the fact that Moore's law speeding up general

712
00:51:25,520 --> 00:51:31,440
compute has basically dramatically flattened off in the last six or seven years.

713
00:51:31,440 --> 00:51:35,840
And so now we're at the point where a lot of the innovation is going to be on the computer

714
00:51:35,840 --> 00:51:41,240
architecture side rather than the gesture lying on the fab people to give us like smaller

715
00:51:41,240 --> 00:51:47,400
features and faster transistors every two years like they had been.

716
00:51:47,400 --> 00:51:51,760
And so that will mean there's now this really wide open space.

717
00:51:51,760 --> 00:51:56,440
And the other thing is that now that we know machine learning computations, deep learning

718
00:51:56,440 --> 00:52:02,280
models in particular are so applicable to so many problems that previously when you

719
00:52:02,280 --> 00:52:06,600
were building like specialized hardware for a particular thing, you know, it applied

720
00:52:06,600 --> 00:52:11,200
to relatively modest part of all the computing you want to do, right, right, you might speed

721
00:52:11,200 --> 00:52:15,560
up compression algorithm by having custom compressions or goods or something, but that's

722
00:52:15,560 --> 00:52:18,400
not actually that much of what you want to do, right, right.

723
00:52:18,400 --> 00:52:23,840
But now you have machine learning computations where it seems plausible that might be a lot

724
00:52:23,840 --> 00:52:28,200
of what we actually want to run on computers five years, 10 years from now.

725
00:52:28,200 --> 00:52:33,240
And so you can now build specialized hardware that doesn't just apply to like a little bit

726
00:52:33,240 --> 00:52:36,600
of your computation applies to everything a lot of it.

727
00:52:36,600 --> 00:52:42,720
And so that I think is pretty exciting because now general purpose accelerators apply to

728
00:52:42,720 --> 00:52:48,040
such a wide variety of things we care about, vision, language, you know, all the things

729
00:52:48,040 --> 00:52:53,400
you can imagine computers really wanted to do, you know, most of them you can imagine machine

730
00:52:53,400 --> 00:52:58,200
learning tackling and you can imagine machine learning acceleration hardware being an important

731
00:52:58,200 --> 00:53:02,080
part of how you make that scale and be fast.

732
00:53:02,080 --> 00:53:05,720
So your kind of point is then that is just that the market is going to be so massive or

733
00:53:05,720 --> 00:53:11,400
is so massive that that there's potential opportunity for different approaches.

734
00:53:11,400 --> 00:53:16,120
I think so and I think I would liken it to this like a big Cambrian explosion of computer

735
00:53:16,120 --> 00:53:18,120
architecture again.

736
00:53:18,120 --> 00:53:24,840
And now lots of ideas are being tried out and it's unclear which ones will sort of survive

737
00:53:24,840 --> 00:53:29,760
this Cambrian explosion, but many of them probably will and that'll be kind of cool.

738
00:53:29,760 --> 00:53:30,760
Awesome.

739
00:53:30,760 --> 00:53:31,760
Awesome.

740
00:53:31,760 --> 00:53:38,760
So I'm wondering if there are, you know, one, two, three things that are, you know, core

741
00:53:38,760 --> 00:53:44,240
to the Google way of thinking about, you know, deep learning, machine learning AI or even

742
00:53:44,240 --> 00:53:48,960
the Jeff Dean way of thinking about machine learning and AI that, you know, a lot of people

743
00:53:48,960 --> 00:53:53,760
don't know, but they should, you know, even better if they're not things that you talk

744
00:53:53,760 --> 00:53:54,760
about all the time.

745
00:53:54,760 --> 00:53:58,560
Like, you know, what are those, you know, what are those kind of hard fought, you know,

746
00:53:58,560 --> 00:54:03,000
lessons that, you know, folks need to know?

747
00:54:03,000 --> 00:54:10,200
I mean, I think one of the things that has struck me a little is I was actually, I was

748
00:54:10,200 --> 00:54:15,200
preparing to give a talk in the National Academy of Engineering last summer and turns out

749
00:54:15,200 --> 00:54:20,920
the National Academy of Engineering put out a list of 14 sort of grand challenges, engineering

750
00:54:20,920 --> 00:54:22,960
challenges for the 21st century.

751
00:54:22,960 --> 00:54:30,080
And it's pretty good list, it's like, you know, make solar energy, efficient and affordable,

752
00:54:30,080 --> 00:54:34,680
you know, advanced health informatics, a bunch of things like that.

753
00:54:34,680 --> 00:54:42,160
And I was struck by how many of those things seem amenable to machine learning being either

754
00:54:42,160 --> 00:54:45,760
key to solving or an important part of solving some of these problems.

755
00:54:45,760 --> 00:54:51,080
Even things that seem kind of somewhat of a field like more like chemical understanding

756
00:54:51,080 --> 00:54:52,760
or better chemistry.

757
00:54:52,760 --> 00:54:57,120
And I think actually machine learning in the last few years is shown that can actually

758
00:54:57,120 --> 00:55:03,880
be applicable to a lot of scientific problems, including better modeling of chemical dynamics

759
00:55:03,880 --> 00:55:09,800
and these kinds of things and more rapid evaluation of materials and things like that.

760
00:55:09,800 --> 00:55:15,640
So, and we've been doing a lot of work on healthcare related problems using machine learning.

761
00:55:15,640 --> 00:55:21,880
And I think I'm struck by the fact that a lot of these sort of core machine learning

762
00:55:21,880 --> 00:55:27,800
algorithms are going to have impact not just in kind of computer science and machine learning,

763
00:55:27,800 --> 00:55:33,360
but because they're applicable to so many things across, you know, a really broad set of problems

764
00:55:33,360 --> 00:55:42,080
in society, things like healthcare, self-driving cars, obviously, you know, material design.

765
00:55:42,080 --> 00:55:48,160
All these things I think are pretty impacted by better machine learning and better machine

766
00:55:48,160 --> 00:55:50,680
learning hardware and advances in this field.

767
00:55:50,680 --> 00:55:51,680
That's pretty cool.

768
00:55:51,680 --> 00:55:55,400
That's why so many people in the world are interested in this field, because I think

769
00:55:55,400 --> 00:55:57,640
it's going to do amazing things for the world.

770
00:55:57,640 --> 00:55:58,640
Awesome.

771
00:55:58,640 --> 00:55:59,640
Well, Jeff, thank you so much.

772
00:55:59,640 --> 00:56:00,640
Cool.

773
00:56:00,640 --> 00:56:01,640
Thank you.

774
00:56:01,640 --> 00:56:05,640
All right, everyone.

775
00:56:05,640 --> 00:56:07,680
That's our show for today.

776
00:56:07,680 --> 00:56:12,720
For more information on Jeff or any of the topics covered in this episode, you'll find

777
00:56:12,720 --> 00:56:18,280
the show notes at twemolei.com slash talk slash one, two, four.

778
00:56:18,280 --> 00:56:23,440
As you know, we love to receive your questions and feedback about the show, so don't hesitate

779
00:56:23,440 --> 00:56:25,400
to comment there.

780
00:56:25,400 --> 00:56:52,160
Thanks so much for listening and catch you next time.

