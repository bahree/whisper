All right, everyone, I am here with Anshia Bart and Chris
Freggly. Anshia is a senior developer advocate at AWS and Chris is a
principal developer advocate at AWS. Anshia and Chris, welcome to
the Twomo AI podcast.
Thanks so much, Sam. It's great to be here.
I am really looking forward to digging into our conversation. I've
known the two of you for quite some time now. I'd say let's
put it like this before Anshia's hair was blue and before Chris
was wearing button-down shirts.
Oh my God. That is true.
Indeed.
And, you know, in fact, we'll talk a little bit about, you know,
the work that you have both done kind of building communities
around data science.
But to maybe get us there, I'd love to have both of you walk us through
a little bit of your background and bios and just how you came to work in
ML. Anshia, want to get us started?
Sure. Happy to.
So, yeah, I had a quite traditional start. So I studied computer
science here in Germany, actually at the University of
Tupingen. And after that, I really got excited to start right away in
the tech industry. I joined Cisco, worked many years in the data
center space. And then the next big thing that I always was interested
in is data. So I also worked a couple of years in the big data space
at MEPR. And, you know, how it goes, right? I mean, the technology
innovation keeps going. And after big data, I really got super, super
interested in learning more about data science, AI and machine
learning. So I started to do more talks and that space to take
courses, to build fun demos and got excited. And then I joined AWS
as a developer advocate for AI and machine learning.
And I'm super excited right now on this role to be able to help
developers build on AWS and show them how they can use the services
and also enjoy writing books.
Nice. Nice. Chris, how about you?
Yeah, similar type of thing. You know, I moved to the Bay
area about 10 years ago and started working for Netflix.
And they do so much machine learning, so much AI for, you know,
foundations and even, you know, some of the script writing and
script analysis and really caught the bug there, caught the
machine learning bug there. And then moved on to Databricks
to really scale out and, you know, learn all of the, like, depth
and breadth of Spark, of Apache Spark. And then the two came
together. And so coming from Netflix where we use AWS very,
very heavily from day one, my first day, I spun up, like,
EC2 instance, I actually deployed some code.
What's super fun to do that. And then I've kind of come full
circle almost. And now that I'm working for AWS and I'm
specifically focused on big data AI and machine learning.
So that's why I'm here.
That's awesome. And I think you are one of the first people
that I knew that was out kind of evangelizing Kubernetes for
ML and AI. That's right.
You had a course. I think both you and Anjah were working on
the, that's right together.
Talk about some of that work that you have done the community
building and the courses. You have a had or have a meetup
that you have hosted for quite some time in SF.
Yeah, exactly. So right after Databricks, I actually started
what was back then the advanced TensorFlow and Spark meetup.
So TensorFlow had just hit the scene. It was 2016.
And I focused on the advanced part of it because there were
plenty of Spark meetups, there were plenty of, you know,
TensorFlow meetups. But, you know, I would go to lots of
these meetups. And I would come away not really knowing
anything more than I, you know, could have gotten, for example,
from just simple documentation. So because I had so much
Spark knowledge, I actually set up, I think the first,
I think eight or nine meetups was just me speaking.
And because, right, like no one really knew me.
I didn't have that large of a network back then.
So I was able to explain what was going on within Spark.
And each event, which like we ran monthly, we would dive into
the details of this code of the TensorFlow source code itself.
You know, fortunately these projects are open source,
Kubernetes open source. And so once Kubernetes came out,
there was a lot of interest from my community to say,
hey, how do I get Spark running on, on like Kubernetes?
And then also how do I scale out TensorFlow? And then eventually
Kubeflow came out and we sort of pivoted to that. And now that I'm
with AWS, I cover all of those open source and SageMaker.
And yeah, so all of those technologies pie towards TensorFlow.
And until you cover the same set of technologies.
Yeah, exactly. And it's actually fun. So in parallel to Chris
efforts, I was also starting to build up the community here
across Europe. And while I was working in the big data space,
it also started to containerize big data workloads.
Right. So I was also being invited to give presentations and talks
at events, conferences to talk about how to containerize big data
environments. So I was also starting to get into the Kubernetes
and cloud native ecosystem. And yeah, and from there it was
literally a few more steps and then into the machine learning space.
And yeah, Chris and I have been developing a couple of workshops
and courses to help developers really grasp the idea and the
technology, how to be successful working with those open source
technologies, but also now in the role at AWS, how to successfully
build with the services and sometimes even in combination,
right, to leverage both the innovation of the cloud, but also
maybe some exciting open source technologies.
Nice. And are your roles primarily working one to many
with producing materials like workshops and courses or do you
work with individual customers? What's the specific role
that you're or what are the specific focus areas of the role?
Yeah, I think primarily we are community focused.
Yeah, so obviously AWS is very, very customer focused.
And so we do get lots of requests to work with specific
customers. We're super happy to do that. Fortunately, we can reuse
the same like workshop. And so that's even better because we can
present something that's like totally open to both the customer
internally for their like training or just sort of educational
needs, but then they can also go away with it and share it with
the rest of the group or their friends at like other companies.
So yeah, it's a mix of both our, you know, sort of primary
goal is, is, you know, reach and spreading this knowledge and
write like really diving deep into these things. That's why
Auncha and I really focus on sort of end to end type use cases,
where we're not really focused on a single product or a single
service or a single open source tool, we show how to use
them all together and then provide different options.
And so we've been talking kind of generally about workshops
and you've been doing a end to end ML workshop for quite
some time, but recently, in fact, with this yesterday, the
ML summit, you announced your latest effort.
Auncha, do you want to do the grand unveil?
Sure, I'm happy to. So yeah, we've been working for a lot
of different formats. So we've been doing workshops.
We just recently published the book.
And another project, Chris and I have been working on over
the last couple of months, which we launched yesterday during
the ML summit keynote AWS is a project in collaboration with
Deep Learning AI and Coursera.
We worked on a new Coursera course, practical data science,
which is open for enrollments right now.
And the idea was really kind of the same thing.
We wanted to help democratize data science, machine learning
and give people also really this hands-on part, right?
Both Chris and I are very passionate when we develop content to
make it really a hands-on experience.
So developers and builders can really work with the tools
and get this learning right away.
So this is a three course, 10 weeks specialization for Deep
Learning AI.
And we're going to show the learners really kind of how to get
started the easy way that is doing exploratory data analysis
and then also how to analyze the data for statistical
imbalances and to leverage also automated machine learning.
And this is kind of the first course, the easy start.
And then we kind of build up a little bit the knowledge
and the depth.
So in the second course, we're diving deeper into building
your custom models.
So for example, running the feature engineering model training,
tuning, and also starting building your pipelines.
And then the third course, which is about to be released soon,
will then talk about and show the advanced concepts
of model training, distributed training, advanced model
deployments, strategies, monitoring.
And then also wrapping it up with how to build human
in the loop pipelines.
And we're super excited for this course.
And also hope there's a lot of interest out there.
Awesome.
And Chris, who's the target audience for it?
Is it someone who's just getting started or someone
who's more advanced on their journey with building these pipelines?
Yeah, the specialization, the prerex, really, you know,
people who have been doing maybe local machine learning, right?
Like local exploratory data analytics on their laptop.
And now have reached a point where they need to scale out
or they are starting to collect more and more data that either
can't fit on their laptop or they're shared,
like development server.
And now they need to scale it out to the cloud.
So really, it's, it is designed to show the benefits
of doing machine learning in the cloud.
And of course, there's a heavy focus on SageMaker pipelines,
which was released December 2020 reinvents.
We cover all the latest features up till now.
And yeah, so really, you know,
it's really sort of emphasizing scale and then ML ops automation.
And what you'll find is if you're doing your own pipelines,
there's quite a lot of Python scripts and bash scripts
and a lot of things.
And so the ability to really centralize that and to like integrate
with the experiments part of SageMaker as well.
All those pieces come together, lineage tracking, artifact tracking.
And so that's really what it's for is, you know, people wanting
to sort of leave the laptop or leave the sort of local environment.
Yes, I'd love to hear from both of you.
A bit on just how your perspective went into the course,
or they are there specific, you know, things that you can think of
or point to that were like hard one lessons out in the field
that you're trying to give people a head start on
by building them into the course.
Does anything like that come to mind?
Yeah, so I think a personal learning was,
it's quite different to develop like a massive online course
compared to, you know, just running a workshop for developers.
So that was a personal learning for me, but really exciting,
same with writing a book.
It's just those experiences that you tap into and learn
how to do things in a completely different way.
I think for all the learners and machine learning practitioners
out there, we really wanted to give them perspectives
on the different ways to achieve the same goal.
So what we're doing, for example, is we picked a use case,
natural language processing and classifying product reviews,
for example, into sentiment classes.
And we've seen a lot of interest for our communities
in natural language processing, in particular.
So we decided to go for that use case
and sticking to the same use case and then showing them
how they can use the different tools.
It doesn't always have to be the most complicated custom
developed and trained model, right?
Sometimes if it's a problem, you're trying to solve
that has already been solved.
There's probably easier ways you could use a pre-trained model
for example, then just find you in it
or even use pre-built algorithms to do it.
And our goal was to really show the different options
so people can build up this intuition
how easy it can be to do AI and ML these days.
And then also if you need to customize
and go deeper and build your custom code,
that there's also tooling available
that helps you perform the tasks.
That was a little bit what we hoped to get across
and which was also fun in building the content
to see how easy it is sometimes to achieve
what you're trying to build.
Nice.
Yeah, I learned, I sort of have a new appreciation
for these MOOCs, right?
The massive online courses.
I had actually built one back at Databricks.
I was part of that effort.
And I remember thinking back then
that we completely underestimated
how much time these things take, right?
And so working with someone like deep learning, by the way,
they completely raised the bar on this sort of overarching
from course one to course two to course three
and making sure that it's coherent
and trying to make the slides and the script
and the videos and the labs,
all like coherence and down to,
we had cases where some of our slides
actually gave the solution that was being used in the lab.
And so just all these little details have to be worked out.
And we actually had two other people
that were solution architects with AWS still there,
of course, that were part of the launch.
And so coordinating across just four of us,
but then also a few curriculum developers on the deep learning
AI side and some of the folks,
like alpha testers beta testers from Coursera,
it's a massive effort.
And if you ever do decide to do one
and want to do it with very high quality,
you know, it takes a lot longer.
I'd say probably three to four times longer
than you might think.
Oh, wow.
Wow.
We mentioned the book a few times as well.
Chris, can you talk us through the book
and how that came about in the relationship
between the course and the book
and some of the other things you're doing?
Yeah.
We call it the flywheel content strategy, you know?
So we basically started with a GitHub repo.
Yeah, Anchanai were, you know,
like really trying to sort of start off
with an end-to-end example.
We then got the book deal, which, you know,
so I've been trying to get a book deal
with O'Reilly for about seven years, right?
And yeah, finally got it.
Yeah, so thanks to Anchan, right?
And one thing that was really interesting
about the book proposal process I found was
it's very similar to a startup
and to pitching your startup, right?
Because keep in mind that when you approach a publisher,
they're basically the investor.
And they are, so they're actually putting up money
because they have to pay for the editors
and the, you know, figure drawers and all that.
So they make the investment and then
were the founders or the co-authors.
And so, and so, fortunately,
I had just had a startup right like before joining AWS.
So I had already gone through the pitch process
and fundraising process.
And I literally used the 13, you know,
slide deck like template and,
and, you know, converted that into our proposal.
Yeah, so it had total addressable market,
which we had to look at, you know,
SageMaker users and projected users
and AI, ML in general.
We had to look at like competitors,
which in this case was competing books
that were, were either written recently or, you know,
being written that we knew about.
We had to talk about us, the founders slash,
like co-authors, you know, how we're backgrounds.
So what is that competitive positioning of this book
versus the others that are available out there?
There are a lot of books on data science and AI out there.
Sure.
Sure.
Yes.
Aren't you do want to talk about that?
Sure.
So, yeah, we, and we thought a lot about rights.
So, so where does, does the book fit in?
And, and where do we want it to fit in?
And what, what Chris mentioned is we wanted to show.
Practitioners really this end to end approach right there.
There's a lot of books that focus maybe on a specific implementation,
a specific use case, a specific tool.
And we wanted to give the ML practitioners kind of this end to end guidance.
Really starting with discussions about in general,
about use cases, how data science AI could maybe, you know,
spark innovation and companies,
sharing a couple of the use cases we see across the industry.
And then where we're diving into this, how to get started, right?
And again, kind of the easy way, maybe using available AI services
that you can literally choose with it with an API call,
integrate in your applications.
And then, but if you need to build something custom,
walking everyone through the individual steps.
So we're starting with a discussion how to get the data into your environment,
right into the AWS cloud in this example.
And how to build maybe a data lake, what are the considerations?
And then how to explore the data, prepare the data.
And really this end the end approach.
And we didn't really stop with deploying the model or building the pipelines.
So we also decided to add another chapter specifically on how to work with streaming data.
We got a lot of feedback from the developers in our communities,
always asking us in the workshops, how do I do this with streaming data, right?
Real-time data coming into my systems.
What do I have to keep in mind there?
So we decided to actually add another chapter on this.
And then obviously we all have to build secure applications.
So we also decided to add another chapter on how to build secure data science project.
So I think back to your point, what sets this book apart is really this.
And to end approach to showing how to get started discussing ideas, use cases,
and then going into the how to and also discussing the concepts, right?
So it's not just important to know how to to work with the tools,
but obviously to to give it understanding of the overall concepts.
For example, in the feature engineering section, we also talk about the idea of a feature store,
which maybe some people haven't heard of yet or thought about implementing it.
And then also as we evolve through the individual chapters,
we talk about those great new concepts that are available and then show the practitioners
how they can actually use the tools and implement them.
Nice. And is the scenario that you discuss in the book, the same NLP scenario that you discuss in the course?
Yeah, yeah, yeah.
It's similar.
We use a different data set and we used a slightly different NLP task.
And we use a different variant of birth, I think, too, right?
Yeah, yeah. And we use PyTorch instead of TensorFlow.
So there's plenty of differences to if you have both that will cover quite a lot of ground.
We don't cover streaming in the in the MOOC.
So there's some differences there.
We were able to go deeper with 500 pages.
There's been a book, yes, versus just 10 weeks.
But yeah, I should also, yes, I should also mention to that we actually ran a survey while we were writing the book.
This is something else that like me and Anisha decided to do because like AWS is very customer focused.
We actually wanted to hear, you know, from customers, what would you want to see in a book called Data Science on AWS?
And we had, you know, six or seven questions.
And it was everything from, you know, which technologies, which AI frameworks do, you know, like TensorFlow, PyTorch and MaxNet to streaming.
Do you want use cases, which type of use cases? Do you want recommendations? Do you want time series?
And that literally drove our outline. And so the original proposal actually that we gave to the O'Reilly folks changed quite a bit.
And yeah, so they were great working with this trying to slot it in.
But there's chapter two is like all about use cases.
And that came directly from the community, right? They said, okay, that's great that you're doing this like NLP use case.
But I want to learn more about computer vision. I want to learn more about recommendations. And so there's a whole chapter dedicated to.
Yeah, it's all those top like use cases.
Anisha, for the main scenario, talk us through a little bit more of the details there. Like what is the data set?
What is the kind of the process that you're using the models that you're working with?
Sure. So for the book, we decided as a framework to go with TensorFlow.
And for the NLP use case, we use the product reviews data set to classify into into rating classes.
And what we decided also is showing the practitioners different ways as I mentioned before.
So we're using tools like automated machine learning to give practitioners like a way to get an easy start.
And show them. You can just provide the data set and you know automated machine learning.
We'll run analysis on the data, decide for framework and the model to use, train the model.
And then percent with you, send you the best model candidates right away.
So we found that is really, really interesting to see how to use those technologies.
And then we're walking everyone through the custom step-by-step process.
And for the custom model building, we actually decided to also include hacking space.
Having space is a very popular open source NLP library, specifically built around the transformer based models.
And this was actually one of the feedbacks also coming from the community being interested in the end of the space.
So we decided to use a variant of the bird model.
And in the book, we're using that is still bird variant that is optimized on this field.
And we're showing everyone how to start working with that pretty trained model also to show them how powerful business in the meantime.
And then we find unit to our data set.
We also talk them obviously through the whole feature engineering process, how to create bird embeddings.
We also learned quite a lot background on NLP models, bird specifically, how that got to the popularity in the industry really with the attention mechanisms and then show how to prepare the data around the feature engineering, create the embeddings, then find unit.
We also show people how they can profile the model training in that process. So that's another exciting new concept and tooling that is available where you're able to monitor and profile while the training job is running and see how the underlying resources are utilized.
For example, if you're running into any bottlenecks, it might be CPU bottlenecks or GPU bottlenecks. And this can help you right to to right size your infrastructure as well, specifically in the training process.
And also you can capture tensors during the model training and do further analysis. You could visualize, for example, the attention that is building up through the model.
I'm optimizing. So we're showing hyperparameter tuning how to optimize the model to you in the model. And then also we discuss a couple of different deployment strategies in the book. And yeah, this is one of my favorite ones, a Chris worked on the multi on band it on strategy.
And I'm always excited every time I hear a talk about multi on bandits. I always learn something new. So that's a really exciting topic. I think personally. And then we wrap it up in the pipeline section where we show how to orchestrate all of those individual steps and and help to implement automation, whether it's get ups automation or other techniques.
Chris, can you talk a little bit about that multi on band it piece as well as the ML ops chapter in the book.
Yeah. So, so yeah, chapter nine was was all about deployment. And we cover AB tests, cover multi on band it. So, yeah, the ability to dynamically shift traffic.
And so there's a reference architecture that is proposed in the book that is used by customers. There's a blog post on this as well, where we're using dynamo DB very, very lightweight to sort of track the
the band it experiment results. And so as we're shifting traffic, we are figuring out which model is performing better live. That data is also being streamed with kinesis. And yes, all these pieces can be replaced with like Kafka manage Kafka anything.
But a nice simple clean like architecture to do these multi on band it tests. The use case there was we had a model, a birth model that was trained one way to predict the class or the like multi class classification.
If the review was positive neutral negative something like that. And then we use different hyper parameters for a slightly different model for the same use case and sort of pin them up against each other. And why you would want to use multi on band it's of course is you don't want to
use a lot everybody into the lay a bucket. And so you'll find the sort of early winner. And then if that one's winning, you can send more traffic to that model variant.
And then, but you're still exploring other options right like other models. And just in case there is a variant that is better than the like early variants we can actually continue to explore. And then if suddenly B is better than we could shift traffic over to B. So it's sort of an optimization for your AB test that minimizes any negative aspects of this overall test that you're doing in live production.
Nice.
You've mentioned a lot of technologies, a lot of AWS services. I've seen some of the architecture diagrams associated with your your previous workshop and you've got a lot of those services involved that you're orchestrating the build out, you know, one of these and pipelines.
I'm wondering if, you know, how do you recommend folks manage the, you know, the complexity that comes with building real world projects and all of the different parts that you might want to use anything in particular that you've learned about doing that.
You want to get to start it? Sure. So what I always recommend people is to think about what's the easiest way to achieve the goal. Right. And at AWS, we believe to let builders decide the right tool for the right job. Right.
So depending on really your use case, your experience, you might go one path or the other. And I would recommend always using the easiest one, right. So as I explained earlier, if you're working on the problem that has been solved before, you might just need to translate language right from maybe English to German to Spanish.
There are services available and models built that can do this. So the easiest way is maybe to look at is, is there any service that I can literally embed into my application with an API call, then I would definitely recommend starting there.
If you need to build a little bit more custom models and you need a little bit more freedom, then I would definitely recommend looking at the Amazon Sage maker family of services and functionalities. So there have been a lot of additions to this managed service that is giving you basically the tools to build to train and to deploy models easily.
And at the same time, it's taking care of the infrastructure for you. So it does the heavy lifting of managing individual instances, right. So you can really focus on on your tasks to to build models to train the models and to deploy them.
And then for all of the expert practitioners that really want you to build neural networks themselves and have all of the freedom and flexibility. There's a lot of options also available on an infrastructure on framework basis.
They could pick and choose, you know, maybe the latest GPU instances, if they need CPU architectures and then start building as they need. But again, depending on your use case and experience, always start looking for the easiest way to achieve it, because you want to focus on the business problem and normally less on on spending hours of hours of training and model, right.
And as you go and customize, you can leverage additional services and additional tooling that is available.
Anything you'd add to that, Chris. Yeah, Sam, you know, we work with a lot of customers and more recently, where, so back to the sort of a B testing multi-armed and a testing, you know, putting out one model, two models.
You don't have a good automation strategy in place. That actually takes quite a while. And what you want to get to is the ability to try out, you know, many models, like hundreds of models, right. So typically we see people go from one single model to two and that's a big, big step.
And then two to five. And what we're really trying to get people to try out is, you know, five to 10 and then 10 to 100. And people have enough variations, you know, for example, geo variations for your recommendations.
Or, you know, different product categories will have different models. And so right now, people seem to be limited by the ability to get these models out there in an automated way and to auto scale.
And like shift traffic dynamically. So seeing, seeing more and more models going out per customer is super exciting to us. We love that. And then seeing all of this automated and all the experiments being tracked.
And I think I hear you saying that one of the things that you do is encourage customers to build multiple models because it's kind of a forcing function to building that automation that will, it's kind of a virtuous loop or cycle or something like that.
And it kind of forces them to build the automation that will allow them to build more models.
Right. Because it's often not easy to, or to value that automation when you just have one or two, which is why getting to five is kind of a big deal. And then, you know, trying to get from five to 50 and, you know, 50 to 500.
Yeah. So trying to make that investment, which is something we've been trying to do with Sage maker pipelines, making that, that like investment much smaller, much easier, taking the exact same scripts that you're using Python script, C scripts, psychic learn, TensorFlow, PyTorch, whatever.
And then getting those into production quickly, because you, you can't, or yes, oftentimes it's difficult to just evaluate the model in the research lab or, or on a single, you know, sort of enclosed environment.
So like one thing we did at Netflix quite a bit was push these models into production live and that was our platform for experimentation was actually on live traffic, because you could do all the experimentation in with like historical data and get, you know, really, really close, but not until that models in production and you can look at the
like prediction latencies and, you know, real real live traffic as data is changing, right, because the world's always changing. So the ability to just keep putting out new models and, you know, trying different experiments is very, very powerful.
Nice, nice.
We've also mentioned the ML summit recently, it was just yesterday as we're recording this, the videos will be available. I'm wondering if you have any recommendations for folks that are hearing this and want to dig in a little bit deeper into the summit content.
Any any favorites from you? Yeah, I don't want to play favorites right. We had an action fact agenda yesterday. So we had a track dedicated to the signs of machine learning that is super, super interesting with a lot of very bright and smart people
understanding on the different research science topics. And we also had a track that is focused more on how machine learning is done. And actually a couple of our colleagues also presented there.
So if people want to learn more what I mentioned earlier, how to, for example, profile and debug your models. There was a great talk going into details how you can achieve this with a Sage maker debugger, for example.
Also had a talk around NLP specifically. So how do I celebrate NLP training with Sage maker. And yeah, those are think the topics that I found very interesting. And then again, also the research trick, I think, had a lot of great insight.
And yeah, Chris, what are your favorite topics?
I'm trying to learn more and more about football, the soccer version of football. So there was a good talk on Bundesliga. So I pay very close attention to like the NFL talks because that's what I grew up with.
But trying to keep an open mind, trying to be global, trying to learn why everyone likes soccer, still still haven't quite gotten there yet.
And then my favorite track actually is called the how machine learning is done. So to me, this was a more, more on the, the like practical side.
So talking about Jupiter notebooks and Sage maker studio and you know, some of the advancements happening there.
The right algorithm for your use case. There's a really good talk about automated model tuning. And also one of our colleagues has this really good talk about computer vision and how to visualize what's happening.
And you know, the attention that's happening and the like convolutional, like characteristics and what which kernels are being learned. And so using Sage maker debugger in more of a visual way, you know, versus just looking at the actual GPU metrics.
Yes, I should also mention to Sage maker debugger has this, this new feature that is used for profiling. And so this falls under the like debugger service, but it's a feature that does really make recommendations and say, we think you should use a smaller instance type.
Right. And like this blows my mind, right, that that we AWS would be suggesting to use a smaller instance, right. And really earns a lot of trust, I think with, with our customers, when you see those kind of recommendations, or saying that you are using too big of a GPU.
Either, yes, increase batch size, maybe change the algorithm, make some small adjustments, because you're not fully utilizing the hardware that you've chosen. So yeah, really good talks.
Nice, nice.
Yeah, maybe kind of taking a step back from the books and the courses and the conference and all that stuff and thinking, you know, a little bit more broadly about kind of what you're seeing in the space with the customers you're working with and the communities you're involved in.
I'm really curious, like, just what you're finding, you know, most interesting and exciting and where you think things are headed, aren't you anything jump out of you.
Definitely, yeah. So I think I see two really exciting areas right now. So one is definitely the whole orchestration and automation piece. So how can I, you know, get from manually training, tuning, deploying the models to this more of an ML ops strategy, orchestration strategy automation.
I think that's a very exciting field where many companies will will build their solutions.
And the other area I think is also super exciting is how to make AI and ML even more accessible.
And with that, I mean through and to end solutions, right.
So, I mean, I'm a person I always get excited when there's a new tool I want to try out the tool.
But ultimately, we need to solve, you know, our businesses use cases. And I think with AI services and ML services that are more targeted at a solution.
Like, for example, AWS launched the new industrial machine learning services that are helping customers to implement, let's say predictive maintenance as a solution right instead of you have to go and train your individual model to recognize maybe product effects.
And I think as SAI and data science evolves over the next years, it will be more and more of those solutions, hopefully coming and helping customers to even have a faster start to implement their own business use cases. So I'm super excited to see how that space evolves over the next years.
What about you Chris?
Yeah, I like the hardware aspects with with lookouts and I think it's called monotron.
Some of the industrial stuff, like you mentioned, we cover some of that in the book that that came from the community.
Other areas I've been looking at, you know, just more, more on the massive scale and just being able to point something to your data and, you know, this is something I really like about auto pilot our, like auto, our, like auto ML solution.
You can really derive quite a lot and, you know, get pretty far right by just pointing to your data. So seeing some of that working closer with, like natural language data and some of the pre train models for computer vision.
Just, you know, really not even thinking about like machine learning much anymore, you know, just like we don't really think much when we're building microservices, there's like tons of frameworks and very, very easy to do these days, you know, versus 10 years ago.
At like Netflix 20 years ago at like Amazon, right, like 20, 25 years ago. So really just these things being, if you think of it like a drag and drop kind of widget or just, just like a right click and, you know, add recommendations here kind of thing.
Awesome. Awesome. Well, Ancha and Chris, thanks so much for taking the time to share a bit about what you're up to. You've both been really busy.
Looking forward to checking out the course and the book and more detail, but thanks so much for jumping on and sharing.
It's really a pleasure. Yeah, this has been a lifelong dream for me was to be on this podcast. And yeah, there is a signed book that will be, I think it's going to arrive today.
Awesome. Awesome. Thanks so much.
Take care.
