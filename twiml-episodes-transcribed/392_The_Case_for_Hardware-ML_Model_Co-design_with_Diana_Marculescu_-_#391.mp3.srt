1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:16,600
I'm your host, Sam Charrington.

3
00:00:16,600 --> 00:00:29,760
Alright everyone, I am on the line with Deanna, Marcalescu.

4
00:00:29,760 --> 00:00:35,240
Deanna is department chair and professor of electrical and computer engineering at the

5
00:00:35,240 --> 00:00:38,240
University of Texas at Austin.

6
00:00:38,240 --> 00:00:41,000
Deanna, welcome to the Twimal AI Podcast.

7
00:00:41,000 --> 00:00:42,160
Thank you for having me, Sam.

8
00:00:42,160 --> 00:00:43,400
I'm happy to be here.

9
00:00:43,400 --> 00:00:49,680
I am super excited to have you on the show, especially since just before we started recording

10
00:00:49,680 --> 00:00:51,960
you mentioned that you're a long time listener.

11
00:00:51,960 --> 00:00:57,800
And I always love talking to listeners on the show and having the opportunity to host

12
00:00:57,800 --> 00:00:58,800
them on the show.

13
00:00:58,800 --> 00:01:01,360
So thank you for that.

14
00:01:01,360 --> 00:01:04,160
Being a long time listener you know the routine, I'm going to ask you to share a little bit

15
00:01:04,160 --> 00:01:08,920
about your background and how you came to work in machine learning.

16
00:01:08,920 --> 00:01:09,920
Sure.

17
00:01:09,920 --> 00:01:11,280
I'm happy to do so.

18
00:01:11,280 --> 00:01:16,880
Yeah, actually I'd be listening to the podcast for quite some time, especially on my

19
00:01:16,880 --> 00:01:23,800
trips to conferences and more than likely currently we're not traveling that much.

20
00:01:23,800 --> 00:01:28,520
However, we try to do our best to stay up to date by listening to podcasts, especially

21
00:01:28,520 --> 00:01:29,760
in this domain.

22
00:01:29,760 --> 00:01:34,960
My trajectory to working in this field is a little bit less traditional, I come from

23
00:01:34,960 --> 00:01:37,000
the computer hardware systems area.

24
00:01:37,000 --> 00:01:40,800
I've worked for more than 20 years on making computer hardware systems more efficient,

25
00:01:40,800 --> 00:01:43,680
especially more power efficient.

26
00:01:43,680 --> 00:01:50,320
And the way I got to work in machine learning in particular came from the applications side.

27
00:01:50,320 --> 00:01:57,320
So we've used machine learning to make or to perform power management for multicore systems

28
00:01:57,320 --> 00:01:59,600
in the last decade or so.

29
00:01:59,600 --> 00:02:04,640
But more recently in the last three to four years my students and I decided to turn it

30
00:02:04,640 --> 00:02:09,400
around and instead of using machine learning as an application to make computer systems

31
00:02:09,400 --> 00:02:15,640
more efficient, instead we turned our focus to making machine learning models more efficient

32
00:02:15,640 --> 00:02:21,840
such that they run better on existing computer systems or trying to find ways to build computer

33
00:02:21,840 --> 00:02:25,520
systems that are more amenable to running machine learning models and applications.

34
00:02:25,520 --> 00:02:30,680
So that's where the whole idea of the code design of both the machine learning model

35
00:02:30,680 --> 00:02:33,640
and the computer hardware system came into play.

36
00:02:33,640 --> 00:02:39,680
So our first publication in the field was three years ago and ever since we continue

37
00:02:39,680 --> 00:02:41,320
to work in in this field.

38
00:02:41,320 --> 00:02:42,320
Great, great.

39
00:02:42,320 --> 00:02:47,080
And you recently had an opportunity to share some of what you're doing in the field at

40
00:02:47,080 --> 00:02:53,440
the efficient deep learning in computer vision workshop at the recent CVPR conference

41
00:02:53,440 --> 00:03:01,800
where you gave a keynote and working in this field and talking about issues like the

42
00:03:01,800 --> 00:03:10,240
efficiency of machine learning models, often questions about the edge versus cloud come

43
00:03:10,240 --> 00:03:11,240
up.

44
00:03:11,240 --> 00:03:17,000
Did your research focus on one or the other of these settings?

45
00:03:17,000 --> 00:03:20,720
Sure, actually both are really important.

46
00:03:20,720 --> 00:03:26,320
So thinking about where we are now, much of the machine learning applications actually

47
00:03:26,320 --> 00:03:28,680
do run in the cloud.

48
00:03:28,680 --> 00:03:33,320
So for both training, we train them in the cloud, much of the inference also happens in

49
00:03:33,320 --> 00:03:39,520
the cloud if you're trying to run a speech recognition, simple app on your phone or smart

50
00:03:39,520 --> 00:03:43,920
watch, if it's not connected to the network, it's not going to be able to accomplish its

51
00:03:43,920 --> 00:03:47,200
task because everything does happen in the cloud.

52
00:03:47,200 --> 00:03:52,960
On the other hand, our work tries to democratize access to machine learning as an application

53
00:03:52,960 --> 00:03:58,720
such that it's pushing both inference as well as training to edge devices.

54
00:03:58,720 --> 00:04:03,040
So this is where the idea of making machine learning models in particular neural networks

55
00:04:03,040 --> 00:04:09,320
more efficient and more amenable to be run on tiny devices where constraints are really

56
00:04:09,320 --> 00:04:10,640
important.

57
00:04:10,640 --> 00:04:16,080
So yeah, our focus is mostly on edge devices, although looking at the entire continuum

58
00:04:16,080 --> 00:04:20,280
from edge all the way to the cloud is just as important.

59
00:04:20,280 --> 00:04:25,280
So your keynote at the workshop is titled putting the machine back in machine learning,

60
00:04:25,280 --> 00:04:29,680
the case for hardware, ML model co-design.

61
00:04:29,680 --> 00:04:35,640
And it sounds like co-design is a key idea in your research.

62
00:04:35,640 --> 00:04:44,080
What it can know for me is that often we hold either one, the machine or the algorithm

63
00:04:44,080 --> 00:04:50,120
as fixed and try to optimize the other, you know, with the constraint of the thing that

64
00:04:50,120 --> 00:04:51,320
we hold as fixed.

65
00:04:51,320 --> 00:04:58,640
And what I'm guessing here or imagining is that you're treating both of these as things

66
00:04:58,640 --> 00:05:00,760
that we can kind of co-optimize together.

67
00:05:00,760 --> 00:05:03,520
Is that the general idea of your research?

68
00:05:03,520 --> 00:05:05,240
That's correct.

69
00:05:05,240 --> 00:05:10,800
And actually the co-design idea has been used for quite some time in embedded systems.

70
00:05:10,800 --> 00:05:15,440
For a long time, the idea that you want to customize a hardware that runs the embedded

71
00:05:15,440 --> 00:05:21,760
software has been at the forefront for the last 15 to 20 years in the field of embedded

72
00:05:21,760 --> 00:05:23,320
system design.

73
00:05:23,320 --> 00:05:24,840
So it's not a new concept.

74
00:05:24,840 --> 00:05:30,120
What's new is actually putting the focus on the machine in the machine learning domain.

75
00:05:30,120 --> 00:05:36,400
And machine has never left to be honest, the machine learning space, but we're just trying

76
00:05:36,400 --> 00:05:43,400
to put the focus back on the hardware because a lot of the focus that has been happening

77
00:05:43,400 --> 00:05:48,120
recently has been on the performance of machine learning applications, meaning accuracy or

78
00:05:48,120 --> 00:05:54,480
classification error, or things that actually tell us whether it does its job in the proper

79
00:05:54,480 --> 00:05:55,760
way.

80
00:05:55,760 --> 00:06:01,200
Putting things in a historical perspective, machine learning and in particular neural network

81
00:06:01,200 --> 00:06:07,000
research has been going on for quite some time, but it did not get at the level as we see

82
00:06:07,000 --> 00:06:13,680
today without the progress that we've seen in hardware design and everything we've seen

83
00:06:13,680 --> 00:06:17,400
from the single core to multi-core to GPU progress.

84
00:06:17,400 --> 00:06:24,160
So it's important to understand that the reason why machine learning applications have received

85
00:06:24,160 --> 00:06:28,480
so much attention and widespread usage is because of the hardware.

86
00:06:28,480 --> 00:06:34,520
But we're not going to be able to write that wave of dissemination without support from

87
00:06:34,520 --> 00:06:35,520
the hardware.

88
00:06:35,520 --> 00:06:43,800
So our focus is to put the machine specifics and machine hardware metrics into the design

89
00:06:43,800 --> 00:06:46,440
process for neural networks.

90
00:06:46,440 --> 00:06:50,160
And when you do that, you have one half of the problem you mentioned, right?

91
00:06:50,160 --> 00:06:53,240
When you think about code design, you have to add the other half of the problem.

92
00:06:53,240 --> 00:06:57,440
It's not just that the neural network has to be designed with a hardware in mind, but

93
00:06:57,440 --> 00:07:01,840
at the same time, we need to design the hardware with the application in mind in this particular

94
00:07:01,840 --> 00:07:03,080
case machine learning.

95
00:07:03,080 --> 00:07:09,200
So yes, code design is the end goal and we are, we are not quite there yet.

96
00:07:09,200 --> 00:07:13,440
There's quite a few researchers working in this field, but there are a few components

97
00:07:13,440 --> 00:07:17,000
that are needed before we get to true code design.

98
00:07:17,000 --> 00:07:21,320
And one is understanding what are the hardware metrics we need to look at?

99
00:07:21,320 --> 00:07:25,080
How do we expose them to the machine learning design process?

100
00:07:25,080 --> 00:07:30,280
And then how do we design neural network models that take that into account such that they

101
00:07:30,280 --> 00:07:36,200
fit the hardware that we might be able to have or we might be able to design for them?

102
00:07:36,200 --> 00:07:42,360
So it's actually a true cycle, iterative cycle of optimization.

103
00:07:42,360 --> 00:07:50,200
You say that we need to focus on hardware and putting the hardware back into the process.

104
00:07:50,200 --> 00:07:54,960
And maybe I'm asking the same question again, or maybe I'm just going to provide the

105
00:07:54,960 --> 00:07:59,840
answer, but my sense is that, you know, there has been a lot of focus on hardware in

106
00:07:59,840 --> 00:08:04,480
the fields, you know, clearly, you know, different types of accelerators.

107
00:08:04,480 --> 00:08:13,480
What you're saying here is we've focused on hardware for optimizing hardware for a specific

108
00:08:13,480 --> 00:08:14,480
class of problems.

109
00:08:14,480 --> 00:08:19,080
This is going to be an accelerator that works well at, you know, deep neural networks,

110
00:08:19,080 --> 00:08:26,720
as opposed to what I'm seeing in your research is a specific neural network architecture.

111
00:08:26,720 --> 00:08:31,040
We're going to kind of design the hardware for that, kind of design the neural network

112
00:08:31,040 --> 00:08:32,600
to work well with that hardware.

113
00:08:32,600 --> 00:08:35,080
Do all of this at the same time.

114
00:08:35,080 --> 00:08:40,480
And overall, we're going to end up with a system that, you know, performs better than

115
00:08:40,480 --> 00:08:46,080
hopefully the, you know, things that aren't designed, you know, so in such an integrated

116
00:08:46,080 --> 00:08:47,080
manner.

117
00:08:47,080 --> 00:08:50,840
Yes, I mean, that's exactly the point, right?

118
00:08:50,840 --> 00:08:56,360
So any or right, there has been a lot of hardware focus lately.

119
00:08:56,360 --> 00:09:00,440
Maybe it started a little bit, you know, with proxies, right?

120
00:09:00,440 --> 00:09:01,880
They weren't really hard on metrics.

121
00:09:01,880 --> 00:09:06,120
There were maybe flops, flop count, floating point operations.

122
00:09:06,120 --> 00:09:10,320
Maybe it was model size.

123
00:09:10,320 --> 00:09:13,880
Maybe it was some sort of proxy for latency.

124
00:09:13,880 --> 00:09:20,360
What we're trying to do is characterize or provide ways or methodologies to characterize neural

125
00:09:20,360 --> 00:09:26,640
networks from their latency, power, energy perspective on a given hardware.

126
00:09:26,640 --> 00:09:32,640
But at the same time, identify ways to build hardware that minimizes those metrics.

127
00:09:32,640 --> 00:09:34,040
If it makes sense, right?

128
00:09:34,040 --> 00:09:41,000
So the mentioning, you mentioning the talk I gave at the workshop at CVPR.

129
00:09:41,000 --> 00:09:47,760
So one piece that is required to make this code design process possible is the ability

130
00:09:47,760 --> 00:09:54,440
to characterize neural network architectures or perhaps components thereof in terms of

131
00:09:54,440 --> 00:10:00,400
their, the time it takes to process those things, the energy or power it takes and maybe

132
00:10:00,400 --> 00:10:05,720
come up with a joint metric that characterizes their energy efficiency or latency per correct

133
00:10:05,720 --> 00:10:07,440
inference if you want.

134
00:10:07,440 --> 00:10:14,200
In other words, come up with combined metrics that capture both hardware as well as accuracy

135
00:10:14,200 --> 00:10:15,280
in some sense.

136
00:10:15,280 --> 00:10:23,280
So providing that methodology for estimating power, energy and latency for these, it's

137
00:10:23,280 --> 00:10:28,680
actually a piece of software, right, that you write is one of the steps that allows us

138
00:10:28,680 --> 00:10:34,680
to identify the right configuration or architecture for the neural network because you can't optimize

139
00:10:34,680 --> 00:10:38,440
what you cannot model or characterize, right?

140
00:10:38,440 --> 00:10:40,640
And actually, it's easier.

141
00:10:40,640 --> 00:10:43,640
It's actually pretty easy to do that.

142
00:10:43,640 --> 00:10:49,960
We've done this for quite some time in the computer or hardware embedded system domain.

143
00:10:49,960 --> 00:10:55,360
It's much harder to have the same similar kind of model for accuracy.

144
00:10:55,360 --> 00:11:00,160
We can tell how accurate is a neural network until we actually train it.

145
00:11:00,160 --> 00:11:06,200
On the other hand, for a given architecture, we can easily tell what the power or latency

146
00:11:06,200 --> 00:11:10,280
will be for a given hardware platform once you characterize it, right?

147
00:11:10,280 --> 00:11:15,880
So it's a very low-hanging fruit that we should take advantage of.

148
00:11:15,880 --> 00:11:20,240
The idea there is expressed in some research that you've done called neural power.

149
00:11:20,240 --> 00:11:27,840
Is that the one and you're, you know, we've previously talked about, you know, characterizing

150
00:11:27,840 --> 00:11:34,880
neural networks in terms of, you know, things like depth and parameters and layers, and we

151
00:11:34,880 --> 00:11:41,400
talk about hardware in this totally different language, which is flops and power and things

152
00:11:41,400 --> 00:11:42,400
like that.

153
00:11:42,400 --> 00:11:49,320
And you're trying to kind of unify the way that we're able to talk about this.

154
00:11:49,320 --> 00:11:52,080
And you're doing it using a neural network.

155
00:11:52,080 --> 00:11:53,080
Is that right?

156
00:11:53,080 --> 00:11:57,720
Actually, we use machine learning for that.

157
00:11:57,720 --> 00:12:07,400
So we build models for latency and power that we fit using data that we collect from a

158
00:12:07,400 --> 00:12:11,400
lot of synthetically generating neural networks and constructs, right?

159
00:12:11,400 --> 00:12:18,680
Because we look at different types of convolutional layers, fully connected layers, pulling layers,

160
00:12:18,680 --> 00:12:23,440
things that you typically find in neural networks, deep neural networks.

161
00:12:23,440 --> 00:12:28,760
So we try to build a power and a latency model and, of course, also an energy model that

162
00:12:28,760 --> 00:12:31,080
captures all those components.

163
00:12:31,080 --> 00:12:36,240
And then in a composable fashion, you're able to say, once you've done this characterization

164
00:12:36,240 --> 00:12:42,360
and you fit the model once, you can tell just by looking at the configuration of the network,

165
00:12:42,360 --> 00:12:46,680
the latency and to end for an inference will be this much and the power consumption will

166
00:12:46,680 --> 00:12:47,680
be this much.

167
00:12:47,680 --> 00:12:49,920
You don't even have to run it, right?

168
00:12:49,920 --> 00:12:56,560
So you collect the data, you fit the model once, and then you use it over and over again.

169
00:12:56,560 --> 00:13:02,240
And the good thing about this is that you can use it in the optimization process when you

170
00:13:02,240 --> 00:13:04,400
do say architecture search.

171
00:13:04,400 --> 00:13:07,080
Architecture search is very time consuming.

172
00:13:07,080 --> 00:13:13,080
But the fact that you can characterize every single component or block of this architecture

173
00:13:13,080 --> 00:13:21,240
space in a zero cost of one kind of complexity, it's really appealing because it allows you

174
00:13:21,240 --> 00:13:24,720
to put it in this iterative optimization process.

175
00:13:24,720 --> 00:13:32,200
So that's where we took this expertise in characterizing hardware metrics and used it

176
00:13:32,200 --> 00:13:35,840
to identify neural architectures that satisfy hardware constraints.

177
00:13:35,840 --> 00:13:40,960
So this is the second part of the talk that I had that talked about neural architecture

178
00:13:40,960 --> 00:13:43,240
search using a single path approach.

179
00:13:43,240 --> 00:13:50,120
And inside that iterative optimization loop, we did have this kind of latency model.

180
00:13:50,120 --> 00:13:54,600
In that particular case, we were looking at the latency on a very specific hardware platform

181
00:13:54,600 --> 00:13:56,400
Pixel 1.

182
00:13:56,400 --> 00:14:03,560
And so is the idea that if you're applying neural architecture search without the metrics

183
00:14:03,560 --> 00:14:10,680
that you have available via neural power, you would have to run as long an expensive process

184
00:14:10,680 --> 00:14:15,560
and come up with a bunch of candidate models that after the fact you have to see if they

185
00:14:15,560 --> 00:14:20,040
satisfy your hardware constraints, but here you're able to integrate it in and in doing

186
00:14:20,040 --> 00:14:27,120
so it's kind of cut off some dead ends that your search process might need to run through.

187
00:14:27,120 --> 00:14:28,680
Yes, that's right.

188
00:14:28,680 --> 00:14:34,960
So you can prune the search process by removing candidates that do not meet maybe your latency

189
00:14:34,960 --> 00:14:35,960
constraints.

190
00:14:35,960 --> 00:14:40,920
You want to have each inference take less than a few tens of milliseconds, right?

191
00:14:40,920 --> 00:14:45,480
By having this characterization done once and reuse it over and over, you can prune the

192
00:14:45,480 --> 00:14:46,640
search process.

193
00:14:46,640 --> 00:14:53,240
You can also, the way we did it, we actually incorporated in our loss function a term that

194
00:14:53,240 --> 00:14:56,920
captures the cost in terms of latency.

195
00:14:56,920 --> 00:15:03,440
And by doing so, we're able to basically make the neural architecture search process hardware

196
00:15:03,440 --> 00:15:05,240
of aware.

197
00:15:05,240 --> 00:15:10,320
And there are other components to the approach that we've had because it relies on the concept

198
00:15:10,320 --> 00:15:15,960
of a super network that actually shares, it shares its weights across multiple different

199
00:15:15,960 --> 00:15:17,720
types of structures.

200
00:15:17,720 --> 00:15:24,240
But I mean, that's one component, but the reason why it's actually pretty powerful is because

201
00:15:24,240 --> 00:15:32,440
it identifies networks that satisfy these hardware constraints in this case latency while

202
00:15:32,440 --> 00:15:35,640
at the same time doing this in a very efficient fashion.

203
00:15:35,640 --> 00:15:40,720
So like you said, instead of identifying a bunch of candidates that satisfy accuracy,

204
00:15:40,720 --> 00:15:47,080
and then we check does it satisfy latency constraints, we're able to do this through just a single

205
00:15:47,080 --> 00:15:48,080
process.

206
00:15:48,080 --> 00:15:50,080
So it's more efficient.

207
00:15:50,080 --> 00:15:57,480
There's some property of the search space that tells you that if you are looking at

208
00:15:57,480 --> 00:16:03,840
a particular model and it doesn't meet your power constraints, some evolution of that,

209
00:16:03,840 --> 00:16:09,600
not to assume that you're using evolutionary models in here, won't then be better than

210
00:16:09,600 --> 00:16:13,240
the thing that you're looking at, like a convexity property or something like that.

211
00:16:13,240 --> 00:16:20,240
Is that an assumption or is that something that is just kind of demonstrably true about

212
00:16:20,240 --> 00:16:23,760
these search bases for some set of models?

213
00:16:23,760 --> 00:16:24,760
Yeah.

214
00:16:24,760 --> 00:16:27,760
Well, I think it's an excellent question.

215
00:16:27,760 --> 00:16:33,640
We just started to scratch the surface on that our particular approach is a differentiable

216
00:16:33,640 --> 00:16:39,560
search approach, meaning we cast the problem as a differentiable function and then you

217
00:16:39,560 --> 00:16:45,160
just use anything that works on that, like a typically stochastic gradient is and will

218
00:16:45,160 --> 00:16:50,280
make convexity assumptions, although they might not be, they might hold in reality, but

219
00:16:50,280 --> 00:16:53,920
it works pretty well in practice.

220
00:16:53,920 --> 00:16:58,000
And in our case, you're absolutely right, the efficiency of architecture search comes

221
00:16:58,000 --> 00:16:59,000
from two components.

222
00:16:59,000 --> 00:17:00,400
One is the search space.

223
00:17:00,400 --> 00:17:05,960
So what are the components that we actually allow to be the building blocks for our neural

224
00:17:05,960 --> 00:17:06,960
networks?

225
00:17:06,960 --> 00:17:07,960
So that's one.

226
00:17:07,960 --> 00:17:13,600
And the second is how do we do the search and you did allude to say evolutionary algorithms.

227
00:17:13,600 --> 00:17:16,480
There's also reinforcement learning that you can use.

228
00:17:16,480 --> 00:17:22,400
We use a differentiable approach in which all the choices that we make in terms of the

229
00:17:22,400 --> 00:17:29,360
architecture, although they are discrete choices, we perform a transformation that allows

230
00:17:29,360 --> 00:17:32,960
us to do a differentiable search on them.

231
00:17:32,960 --> 00:17:39,560
So apply optimization algorithms that typically are used on those kinds of objective functions.

232
00:17:39,560 --> 00:17:44,640
So the closest or the earliest work that is in the same kind of vein as us is darts,

233
00:17:44,640 --> 00:17:48,880
which does differentiable architecture search for neural networks.

234
00:17:48,880 --> 00:17:51,840
The difference for in our case is the search space.

235
00:17:51,840 --> 00:17:57,160
So the basic components that we consider is broader.

236
00:17:57,160 --> 00:18:01,400
But at the same time, the way we identify the different selections that take us to the

237
00:18:01,400 --> 00:18:10,040
final architecture is done such that we share much of the, for example, for a three by three

238
00:18:10,040 --> 00:18:15,720
five by five seven by seven convolutional kernel, we basically share all the weights.

239
00:18:15,720 --> 00:18:20,080
So the only thing we need to know for a five by five is that we use a three by three plus

240
00:18:20,080 --> 00:18:22,240
the outer layer that takes us to five by five.

241
00:18:22,240 --> 00:18:29,640
So in some sense, it's less for us to identify, you get, it puts more constraints on the

242
00:18:29,640 --> 00:18:34,400
search process, but it's more efficient in the kinds of things that you can't find out.

243
00:18:34,400 --> 00:18:43,120
So, and it is able, so our word that was published last at the ECML PKDD conference last September

244
00:18:43,120 --> 00:18:48,480
in Germany, and we extended it for for a journal paper this year.

245
00:18:48,480 --> 00:18:55,120
It allows us this more efficient search process allowed us to find a more accurate neural

246
00:18:55,120 --> 00:19:00,360
architecture than in the case where you would have more freedom to search.

247
00:19:00,360 --> 00:19:05,440
So I guess it's a fine balance between the exploitation and exploration.

248
00:19:05,440 --> 00:19:12,560
And we were able to find that somehow at the same time satisfying this hardware constraints

249
00:19:12,560 --> 00:19:14,480
in terms of latency.

250
00:19:14,480 --> 00:19:19,280
And interestingly, at the time when we did our comparisons, we were able to be better in

251
00:19:19,280 --> 00:19:21,440
terms of accuracy compared to the manuals.

252
00:19:21,440 --> 00:19:27,960
So to speak designs like mobile net V2, V3 that, I mean, their manual, their zero cost

253
00:19:27,960 --> 00:19:33,880
so to speak, but there was a lot of engineering time, a lot of hours spent by engineers behind

254
00:19:33,880 --> 00:19:35,200
those efforts.

255
00:19:35,200 --> 00:19:39,680
So I don't, I wouldn't want to call them zero time search costs, but, you know, their

256
00:19:39,680 --> 00:19:44,280
manual, single designs, and then, you know, we did even better in terms of the search

257
00:19:44,280 --> 00:19:48,360
cost and accuracy compared to other types of neural architecture approaches.

258
00:19:48,360 --> 00:19:53,640
So because of that, we were able to, because of the efficient search space and the way

259
00:19:53,640 --> 00:19:58,720
we searched, so the two components put together, we were able to push the search time to hours

260
00:19:58,720 --> 00:19:59,880
as opposed to days.

261
00:19:59,880 --> 00:20:04,320
So I think that's where the strength came in only a few epochs.

262
00:20:04,320 --> 00:20:07,000
We were able to find this kind of architecture.

263
00:20:07,000 --> 00:20:13,600
So those are the two strengths are one ability to characterize hardware metrics inside the

264
00:20:13,600 --> 00:20:19,760
optimization process as well as the second component, which is the more efficient search

265
00:20:19,760 --> 00:20:21,520
process overall.

266
00:20:21,520 --> 00:20:30,440
Is the resulting architecture that you are comparing to whatever your baseline is, the

267
00:20:30,440 --> 00:20:39,120
output of the single path network architecture search process, or do you also, you know, apply

268
00:20:39,120 --> 00:20:45,040
other processes to that like quantization or pruning or, you know, compression, other

269
00:20:45,040 --> 00:20:46,560
compression techniques?

270
00:20:46,560 --> 00:20:47,560
Sure.

271
00:20:47,560 --> 00:20:51,920
I mean, single path NASS does just that, the former.

272
00:20:51,920 --> 00:20:57,800
So whatever you said, it identifies the architecture, but of course, on top of that, you can perform

273
00:20:57,800 --> 00:21:01,400
other things like quantization, compression.

274
00:21:01,400 --> 00:21:05,680
There's an interesting question as to whether doing this equationally.

275
00:21:05,680 --> 00:21:10,680
So first, identifying the architecture and then performing compression and or quantization

276
00:21:10,680 --> 00:21:14,760
leads us to a better result rather than doing it together.

277
00:21:14,760 --> 00:21:19,920
I think there's a very interesting, there's some emerging work in that space.

278
00:21:19,920 --> 00:21:26,560
We're also looking at ways to identify how quantization or precision might play a role

279
00:21:26,560 --> 00:21:29,000
in this architecture search process.

280
00:21:29,000 --> 00:21:32,160
But to answer your question, you can do this.

281
00:21:32,160 --> 00:21:36,800
So once you identify a neural architecture, you can apply quantization.

282
00:21:36,800 --> 00:21:40,520
You can apply model compression or pruning, right?

283
00:21:40,520 --> 00:21:45,800
Those are techniques that exist, you know, going back to the presentation I had at the

284
00:21:45,800 --> 00:21:51,400
CFPR workshop, we did present some of the ideas that we had in that space, especially

285
00:21:51,400 --> 00:21:57,400
as they relate to quantization as well as to pruning, channel pruning.

286
00:21:57,400 --> 00:22:03,560
And so in the quantization space, the idea, I mean, there's been quite a bit of work, you

287
00:22:03,560 --> 00:22:09,320
know, ranging from binarized neural networks all the way to using limited precision, fixed

288
00:22:09,320 --> 00:22:13,920
point, limiting the number of bits for representation.

289
00:22:13,920 --> 00:22:19,480
Our idea was actually pretty simple, so that we published first on this in 2017 and then

290
00:22:19,480 --> 00:22:25,520
ever since we refined that further, we wanted to limit the number of bits representing

291
00:22:25,520 --> 00:22:28,600
weights to a fixed number.

292
00:22:28,600 --> 00:22:33,400
And it turned out that in practice, using just one or two bits is sufficient to get the

293
00:22:33,400 --> 00:22:38,680
best in terms of efficiency, but still not miss much in terms of accuracy.

294
00:22:38,680 --> 00:22:45,040
So the idea is that for each weight, we identify the, say, the combination of two powers of

295
00:22:45,040 --> 00:22:50,800
two that, when stochastically rounded, give you the best accuracy.

296
00:22:50,800 --> 00:22:58,640
So we have a few examples in the original paper where we show how using stochastic rounding

297
00:22:58,640 --> 00:23:02,960
actually on average will give you, you know, in theory, will give you the same accuracy

298
00:23:02,960 --> 00:23:07,440
as the original, unquantized neural network.

299
00:23:07,440 --> 00:23:12,200
So we call this light an N, so light weight neural networks.

300
00:23:12,200 --> 00:23:16,720
And light an NK uses K bits to represent the weights.

301
00:23:16,720 --> 00:23:21,760
We still maintain the activations in the original precision.

302
00:23:21,760 --> 00:23:27,800
And it turns out that you can get to up to two orders of magnitude, reduction in the

303
00:23:27,800 --> 00:23:34,680
either power, memory, storage, and still maintain the original accuracy.

304
00:23:34,680 --> 00:23:35,760
So this is pretty powerful.

305
00:23:35,760 --> 00:23:41,640
This was for Cypher 10 data set and then we extended it for ImageNet.

306
00:23:41,640 --> 00:23:46,440
Of course, the downside for using a quantized model is that to actually get these benefits

307
00:23:46,440 --> 00:23:52,280
you will need to build an accelerator that relies on replacing every multiply, accumulate,

308
00:23:52,280 --> 00:23:58,600
or MAC operation with these logic operators, operators that just do shifts and adds, right?

309
00:23:58,600 --> 00:24:02,280
And GPUs are not readily doing that.

310
00:24:02,280 --> 00:24:06,000
You could program CPUs to do that, but might take some effort.

311
00:24:06,000 --> 00:24:10,720
So I think the, what is out there right now is that if you're going to use this type of

312
00:24:10,720 --> 00:24:15,480
quantization, you will need to build a specialized piece of hardware to do it.

313
00:24:15,480 --> 00:24:20,840
So there will be true accelerators, but I think the results are pretty encouraging for

314
00:24:20,840 --> 00:24:26,120
us to continue to do so because it does achieve quite a bit of energy and hardware efficiency

315
00:24:26,120 --> 00:24:30,000
in general for not much of a loss in accuracy.

316
00:24:30,000 --> 00:24:33,400
From the other perspective, you could also do pruning, right?

317
00:24:33,400 --> 00:24:37,440
So you could consider how you want to prune the network.

318
00:24:37,440 --> 00:24:42,640
You don't want to lose much in the accuracy, but you want to simplify the model even further.

319
00:24:42,640 --> 00:24:46,760
So our work in that space, and actually that work was just published in the conference

320
00:24:46,760 --> 00:24:50,080
in CVPR as an oral presentation.

321
00:24:50,080 --> 00:24:55,440
The idea is, so the name we used is ledger.

322
00:24:55,440 --> 00:25:02,960
Basically is using a layer-based global ranking to identify where do we want to do pruning?

323
00:25:02,960 --> 00:25:05,320
So we do pruning based on importance.

324
00:25:05,320 --> 00:25:07,800
So we don't want to prune uniformly across the board.

325
00:25:07,800 --> 00:25:12,760
We want to prune only where accuracy is not going to suffer the most.

326
00:25:12,760 --> 00:25:17,280
And by doing so, you're able to achieve additional efficiency on the model site.

327
00:25:17,280 --> 00:25:20,720
And of course, that translates into hardware efficiency as well.

328
00:25:20,720 --> 00:25:24,880
So was the name of that paper ledger and intentional head fake to get people excited about

329
00:25:24,880 --> 00:25:28,360
some Bitcoin quantization approach?

330
00:25:28,360 --> 00:25:30,720
Hopefully.

331
00:25:30,720 --> 00:25:34,240
I think we need to ask my student why he chose that.

332
00:25:34,240 --> 00:25:36,520
I like the name.

333
00:25:36,520 --> 00:25:44,800
Going back to NeuroPower and this model for the metrics.

334
00:25:44,800 --> 00:25:48,600
You touched on the data source, but can you elaborate on that a little bit?

335
00:25:48,600 --> 00:25:51,560
Did you run a bunch of simulations?

336
00:25:51,560 --> 00:25:58,880
Did you pull power data off of actual hardware running some suite of models?

337
00:25:58,880 --> 00:26:01,120
That sounds very expensive and hard.

338
00:26:01,120 --> 00:26:03,600
How did you collect data to fit a model?

339
00:26:03,600 --> 00:26:04,600
Yeah.

340
00:26:04,600 --> 00:26:05,960
So that's an excellent question.

341
00:26:05,960 --> 00:26:08,680
We actually did a lot of profiling.

342
00:26:08,680 --> 00:26:14,440
So we started with one GPU platform and that we added more GPU platforms.

343
00:26:14,440 --> 00:26:18,240
And like I said, we as machine learning to build the power and latency models.

344
00:26:18,240 --> 00:26:21,200
So we needed a lot of data to train those models.

345
00:26:21,200 --> 00:26:26,160
So we relied on profiling on several GPU platforms.

346
00:26:26,160 --> 00:26:28,360
We profiled power.

347
00:26:28,360 --> 00:26:30,400
We profile latency.

348
00:26:30,400 --> 00:26:36,480
We profiled also memory usage, although we did not use it explicitly, except for a component

349
00:26:36,480 --> 00:26:39,040
in one of our models.

350
00:26:39,040 --> 00:26:43,880
And also at the same time, we had to profile a lot of these synthetic neural networks that

351
00:26:43,880 --> 00:26:48,880
exhibited different types of combinations of convolutional and fully connected and pulling

352
00:26:48,880 --> 00:26:49,880
layers.

353
00:26:49,880 --> 00:26:52,080
So we took a lot of effort.

354
00:26:52,080 --> 00:26:53,560
That's true.

355
00:26:53,560 --> 00:26:59,840
The idea though is if you are using any of those platforms, you can use data that we

356
00:26:59,840 --> 00:27:01,200
collected.

357
00:27:01,200 --> 00:27:06,760
But of course, if you want to use your own platform, you can use our methodology.

358
00:27:06,760 --> 00:27:09,080
So our code is available.

359
00:27:09,080 --> 00:27:14,920
You can use your own platform, perform the same type of profiling and then fit your own

360
00:27:14,920 --> 00:27:17,480
power and if latency model.

361
00:27:17,480 --> 00:27:24,120
So it's a methodology that allows us and others allows everyone to apply on different types

362
00:27:24,120 --> 00:27:25,120
of platforms.

363
00:27:25,120 --> 00:27:26,880
But it is data intensive.

364
00:27:26,880 --> 00:27:31,200
As you collect the data, though, it's just a one time kind of thing.

365
00:27:31,200 --> 00:27:32,200
Great.

366
00:27:32,200 --> 00:27:40,760
And you did that for, is it a handful of GPU architectures or, I guess, how specific

367
00:27:40,760 --> 00:27:47,520
is a particular model that you develop to a hardware platform?

368
00:27:47,520 --> 00:27:53,560
Is it kind of the architecture family or is it a specific model or generation of a platform?

369
00:27:53,560 --> 00:27:57,120
How what is it generalized as the hardware evolves?

370
00:27:57,120 --> 00:27:58,120
Right.

371
00:27:58,120 --> 00:28:00,200
I think that's an excellent question.

372
00:28:00,200 --> 00:28:04,480
So a methodology is as good as its potential use, right?

373
00:28:04,480 --> 00:28:07,480
So and I always get this question.

374
00:28:07,480 --> 00:28:11,920
So okay, neural power is good, but if I can't use it for my own platform, if I can't

375
00:28:11,920 --> 00:28:17,520
generalize to other types of computer hardware platforms that don't even exist, how is this

376
00:28:17,520 --> 00:28:20,800
going to be used or how is it going to be usable?

377
00:28:20,800 --> 00:28:27,400
And the answer is, I think this is where the co-design comes into the picture, right?

378
00:28:27,400 --> 00:28:33,360
So we've done, we've built this methodology on an existing platform.

379
00:28:33,360 --> 00:28:40,760
However, if you are in the business of building specialized computer hardware for machine

380
00:28:40,760 --> 00:28:45,160
learning applications, you have an idea, you have a blank canvas that you can start

381
00:28:45,160 --> 00:28:49,600
from, maybe not as blank, maybe you start from some components that you want to use.

382
00:28:49,600 --> 00:28:54,880
So the idea is that you can decide how you put together this architecture.

383
00:28:54,880 --> 00:29:00,760
And there are a lot of interesting ways to characterize a platform in terms of latency

384
00:29:00,760 --> 00:29:03,320
and power before it's even built, right?

385
00:29:03,320 --> 00:29:09,760
This is what we've done for many years before, I mean, we haven't started, you know, we

386
00:29:09,760 --> 00:29:14,640
don't build computer systems by building one and then measuring power in latency.

387
00:29:14,640 --> 00:29:18,480
We know what those numbers will be at design time, right?

388
00:29:18,480 --> 00:29:21,480
So the same approach can be used in this case.

389
00:29:21,480 --> 00:29:27,640
I really think computer hardware designers, as well as machine learning model developers

390
00:29:27,640 --> 00:29:33,560
have to work together because that's where the strength comes from and that's where the

391
00:29:33,560 --> 00:29:38,800
generalizability comes from, you're going to be able to get the expertise from both

392
00:29:38,800 --> 00:29:43,000
and come up with ways to do the actual true co-design.

393
00:29:43,000 --> 00:29:48,560
So it's a hard problem because the search space will be, you know, the cross product of

394
00:29:48,560 --> 00:29:53,920
the two, but we've done a lot of progress on the computer hardware side, right?

395
00:29:53,920 --> 00:29:59,920
So a lot of them, especially on the edge device embedded systems, this is automated quite

396
00:29:59,920 --> 00:30:00,920
a bit.

397
00:30:00,920 --> 00:30:05,160
So we can use all the expertise that exists there and then incorporate some of those things

398
00:30:05,160 --> 00:30:08,760
in the machine learning design process and the co-design as well.

399
00:30:08,760 --> 00:30:14,720
So when you combine all these things, neuropath, single path, architecture search, the light

400
00:30:14,720 --> 00:30:20,840
neural networks, can you talk a little bit about the results you've seen, you know,

401
00:30:20,840 --> 00:30:26,600
what were the baselines, what did you, were you able to demonstrate, et cetera?

402
00:30:26,600 --> 00:30:27,600
Right.

403
00:30:27,600 --> 00:30:32,840
So in terms of, I mean, our models once built the power and latency models once built,

404
00:30:32,840 --> 00:30:37,480
they are just zero cost or constant cost, you know, you just plug in the parameters where

405
00:30:37,480 --> 00:30:40,600
the architecture configuration and you get the numbers, right?

406
00:30:40,600 --> 00:30:44,880
So that's, you know, you do maybe a lot of work in the beginning, but then it, it amortizes

407
00:30:44,880 --> 00:30:47,320
across multiple uses.

408
00:30:47,320 --> 00:30:54,080
The neural architecture search approach basically reduced the search time by three orders of

409
00:30:54,080 --> 00:30:55,320
money to them more.

410
00:30:55,320 --> 00:30:59,720
So we took it from days all the way to ours.

411
00:30:59,720 --> 00:31:01,600
And there are many reasons why that was possible.

412
00:31:01,600 --> 00:31:04,160
One was the way the search space was defined.

413
00:31:04,160 --> 00:31:10,200
The second was how we did the search and of course the fact that we shared these structures,

414
00:31:10,200 --> 00:31:15,360
parameters across different configurations helped quite a bit as well.

415
00:31:15,360 --> 00:31:18,880
So three orders of magnitude in the search, meaning training time.

416
00:31:18,880 --> 00:31:25,200
And then in the inference efficiency, if you look at the impact of quantization or pruning,

417
00:31:25,200 --> 00:31:30,480
you can get maybe two orders of magnitude efficiency, especially from quantization.

418
00:31:30,480 --> 00:31:33,080
So that's quite significant.

419
00:31:33,080 --> 00:31:39,200
We're actually looking now at custom hardware that, you know, post synthesis, post layout,

420
00:31:39,200 --> 00:31:43,320
how much of that will translate into the actual hardware.

421
00:31:43,320 --> 00:31:48,520
There's still going to be at least 40 to 50% savings that still translates because the

422
00:31:48,520 --> 00:31:52,720
tourism I need to that I mentioned looks only at the computation.

423
00:31:52,720 --> 00:31:56,080
There's also quite a bit in terms of storage that we need to capture.

424
00:31:56,080 --> 00:32:00,000
So I think there's quite a lot of good news.

425
00:32:00,000 --> 00:32:05,840
So overall, on the training search side, several orders of magnitude on the inference side,

426
00:32:05,840 --> 00:32:09,040
a lot of efficiency from quantization.

427
00:32:09,040 --> 00:32:15,680
On the training search side, do you look at like ablations, you mentioned several different

428
00:32:15,680 --> 00:32:20,840
characteristics or several different properties that go into producing your, you know, several

429
00:32:20,840 --> 00:32:25,280
order of magnitude, advantages and in train time.

430
00:32:25,280 --> 00:32:26,280
Is that all or nothing?

431
00:32:26,280 --> 00:32:28,720
Is there something magical about this combination?

432
00:32:28,720 --> 00:32:33,800
Or, you know, have you looked at the individual tricks that you've tried to apply and, you

433
00:32:33,800 --> 00:32:37,720
know, there's an individual, one of those gets you 80% of the way there.

434
00:32:37,720 --> 00:32:40,720
How did the different techniques come together?

435
00:32:40,720 --> 00:32:41,720
Right.

436
00:32:41,720 --> 00:32:47,440
So I actually think it's the combination thereof because there are other differentiable

437
00:32:47,440 --> 00:32:51,960
neural architecture search approaches like Dart's inspired.

438
00:32:51,960 --> 00:32:58,680
Once you do that explicit representation, unless you do this super network kind of approach,

439
00:32:58,680 --> 00:33:01,920
you're not going to get the savings.

440
00:33:01,920 --> 00:33:09,080
But at the same time, the ability to consider this specific building blocks allowed us to

441
00:33:09,080 --> 00:33:12,160
perform this comparison with mobile net V2 and V3, right?

442
00:33:12,160 --> 00:33:17,640
So I guess you could design your search space differently to make other types of comparisons.

443
00:33:17,640 --> 00:33:19,600
The results might be different.

444
00:33:19,600 --> 00:33:25,440
But I think the fact that we limit the search space and we also share these parameters across

445
00:33:25,440 --> 00:33:30,680
configurations made it for the fast search time.

446
00:33:30,680 --> 00:33:38,240
And so what is ahead in this general line of research for you and your research group?

447
00:33:38,240 --> 00:33:39,240
Yes.

448
00:33:39,240 --> 00:33:44,880
So we are actually, we have not completed the co-design dream yet, right?

449
00:33:44,880 --> 00:33:52,320
So because we do have the power, latency characterization, we have the, you know, neural network search.

450
00:33:52,320 --> 00:33:58,000
We put in the hardware metrics, but do we have a push button solution that we say, okay,

451
00:33:58,000 --> 00:34:04,440
my task is this robotic vision application or this ARVR application, and these are my constraints

452
00:34:04,440 --> 00:34:05,760
in hardware.

453
00:34:05,760 --> 00:34:10,600
And these are my specifications for accuracy for the machine learning model.

454
00:34:10,600 --> 00:34:13,520
Just build for me something that does that.

455
00:34:13,520 --> 00:34:15,080
We did not get there yet.

456
00:34:15,080 --> 00:34:19,160
What I think is needed right now is to actually, even your hardware constraints are relatively

457
00:34:19,160 --> 00:34:27,680
coarse, right, the power latency, things like that on a typical, an actual chip CPU, for

458
00:34:27,680 --> 00:34:35,840
example, you can get tons of metrics and capabilities from the chip itself about, you know, that

459
00:34:35,840 --> 00:34:41,200
may impact the way your search operates.

460
00:34:41,200 --> 00:34:42,800
That's absolutely right.

461
00:34:42,800 --> 00:34:48,040
And, you know, everything we've done looks at one machine learning model running on this

462
00:34:48,040 --> 00:34:51,520
piece of hardware, and that's never the case.

463
00:34:51,520 --> 00:34:55,240
There are many more things, even more machine learning tasks running.

464
00:34:55,240 --> 00:35:01,520
What maybe one does, speech recognition, one does image recognition, one does object detection.

465
00:35:01,520 --> 00:35:05,520
Maybe there's some sort of multimodal fusion that happens.

466
00:35:05,520 --> 00:35:08,880
That's a huge space that we're looking at, right?

467
00:35:08,880 --> 00:35:14,760
So, I think the, I guess the goal would be to, the ability to say, okay, these are, this

468
00:35:14,760 --> 00:35:18,200
is what I would like to run, and it's not just one machine learning model.

469
00:35:18,200 --> 00:35:19,200
Maybe it's multiple ones.

470
00:35:19,200 --> 00:35:21,640
Maybe it's multiple modalities.

471
00:35:21,640 --> 00:35:26,520
Maybe it's different tasks, maybe it's the same task with different inputs.

472
00:35:26,520 --> 00:35:30,320
How do I run this most efficiently for this particular application?

473
00:35:30,320 --> 00:35:35,480
And by application, I mean something that is not a platform is, like I said, it could

474
00:35:35,480 --> 00:35:38,720
be robotic or an AR, VR application.

475
00:35:38,720 --> 00:35:45,360
So you have the ability to define or design what the platform will be, or will look like.

476
00:35:45,360 --> 00:35:50,760
So Truco design will happen when we're able to close the loop, right?

477
00:35:50,760 --> 00:35:55,920
So we're able to say, okay, this is an initial version of the hardware I think might work.

478
00:35:55,920 --> 00:36:00,280
Based on this, I think your network or networks should look like this.

479
00:36:00,280 --> 00:36:03,880
However, when I do it again, it looks like the hardware needs to be tweaked.

480
00:36:03,880 --> 00:36:09,280
So it's going to be an iterative continuous process until you get to the right configuration.

481
00:36:09,280 --> 00:36:13,520
The question then stands, well, I'm going to build this.

482
00:36:13,520 --> 00:36:17,840
But what if I want to run an additional task on this piece of hardware?

483
00:36:17,840 --> 00:36:19,400
Will I be able to do so?

484
00:36:19,400 --> 00:36:22,480
So one is the ability to reach this code design.

485
00:36:22,480 --> 00:36:25,640
But the second one is programmability and flexibility.

486
00:36:25,640 --> 00:36:30,000
And that's where, you know, there's a balance that you need to, I mean, do you want your

487
00:36:30,000 --> 00:36:35,920
system to be able to run these new applications, or maybe it's the same task, but the data

488
00:36:35,920 --> 00:36:37,480
set is different, right?

489
00:36:37,480 --> 00:36:39,080
You need to retrain it.

490
00:36:39,080 --> 00:36:41,000
Does that affect the way your hardware looks like?

491
00:36:41,000 --> 00:36:46,200
So I think these are all interesting questions that we're still looking at.

492
00:36:46,200 --> 00:36:53,800
And it sounds like this vein of work is primarily operating under kind of traditional

493
00:36:53,800 --> 00:37:01,040
GPU-like assumptions, and there are all manner of other proposed directions that one might

494
00:37:01,040 --> 00:37:08,040
go with hardware, you know, from kind of graph-native implementations and other things.

495
00:37:08,040 --> 00:37:13,920
Do you think these ideas apply similarly to some of those other types of constructs?

496
00:37:13,920 --> 00:37:14,920
Yes.

497
00:37:14,920 --> 00:37:16,400
I think that's a nice one question.

498
00:37:16,400 --> 00:37:21,480
I think we're pretty much, I mean, if you look at the classic machine learning research

499
00:37:21,480 --> 00:37:24,760
papers, they do run on typical GPU platforms, right?

500
00:37:24,760 --> 00:37:27,040
This is where much of the training happens.

501
00:37:27,040 --> 00:37:33,560
This is where inference, I mean, maybe inference can be done on, or they provide results on

502
00:37:33,560 --> 00:37:36,480
other types of platforms.

503
00:37:36,480 --> 00:37:41,560
But I really think, so one is, okay, what are the other options for us to go to in terms

504
00:37:41,560 --> 00:37:43,600
of the hardware that we're looking at?

505
00:37:43,600 --> 00:37:46,520
So that's where accelerators come into the picture.

506
00:37:46,520 --> 00:37:48,760
And there's quite a bit of work on that end.

507
00:37:48,760 --> 00:37:52,520
On the other hand, what is beyond neural networks?

508
00:37:52,520 --> 00:37:58,440
I mean, neural networks, I think, are, I mean, it's just deep learning has become, I don't

509
00:37:58,440 --> 00:38:02,920
know that it's because it's on more understandable, I don't think it's that.

510
00:38:02,920 --> 00:38:09,840
I think it's because it offers quite a rich field of questions that are still unanswered.

511
00:38:09,840 --> 00:38:16,720
But there's quite a bit of other types of machine learning models or modalities that

512
00:38:16,720 --> 00:38:18,960
we have to look at, right?

513
00:38:18,960 --> 00:38:21,080
Is there any computational impact?

514
00:38:21,080 --> 00:38:24,320
Of course there is on those other ones.

515
00:38:24,320 --> 00:38:29,000
Is there a role that hardware, computer hardware designers or developers should play in

516
00:38:29,000 --> 00:38:30,000
that space?

517
00:38:30,000 --> 00:38:31,320
Of course they do.

518
00:38:31,320 --> 00:38:36,040
But I think right now, much of the, if you look at 90% of the work, I think it's just

519
00:38:36,040 --> 00:38:41,600
deep learning, especially vision and, you know, deep neural networks, neural architectures,

520
00:38:41,600 --> 00:38:48,200
research, things that are really well-defined and quite a bit of work happens there.

521
00:38:48,200 --> 00:38:54,320
I believe there's still quite a bit of unsolved and interesting questions that have yet to

522
00:38:54,320 --> 00:38:59,960
be answered in other types of machine learning that perhaps have not received that much attention

523
00:38:59,960 --> 00:39:03,880
because they're not as amenable to be run on GPUs, right?

524
00:39:03,880 --> 00:39:06,960
Maybe this is why, I mean, there's a chicken and egg.

525
00:39:06,960 --> 00:39:14,320
Those made deep learning, you know, on the visible end of our attention, but all the others

526
00:39:14,320 --> 00:39:18,840
are probably not receiving the attention because they don't really run well on GPUs which

527
00:39:18,840 --> 00:39:19,840
all of us have access to.

528
00:39:19,840 --> 00:39:25,080
So I think we can elevate those kinds of machine learning applications by looking at ways

529
00:39:25,080 --> 00:39:30,400
to make them more efficient and increase the size and their applicability and dissemination.

530
00:39:30,400 --> 00:39:34,800
So I think there's quite a bit of impetus in this field.

531
00:39:34,800 --> 00:39:38,800
It's really hard to say, I'm not going to look in the crystal ball and say, okay, in ten

532
00:39:38,800 --> 00:39:43,480
years, everything on the DNN or deep learning side will be solved and then we're going to

533
00:39:43,480 --> 00:39:46,880
look at, I don't know, probabilistic graphical models or something.

534
00:39:46,880 --> 00:39:48,640
I don't know what that might be.

535
00:39:48,640 --> 00:39:50,640
I was going to ask, right?

536
00:39:50,640 --> 00:39:51,640
Yes.

537
00:39:51,640 --> 00:39:52,640
Right.

538
00:39:52,640 --> 00:39:55,880
Well, my students asked me all the time, should I still work in this because I don't

539
00:39:55,880 --> 00:39:56,880
know.

540
00:39:56,880 --> 00:40:00,040
When I graduate, we'll also be able to find a job.

541
00:40:00,040 --> 00:40:01,040
Definitely.

542
00:40:01,040 --> 00:40:05,920
I mean, this is definitely going to be a hot space for high tech.

543
00:40:05,920 --> 00:40:12,800
Though I think this conversation is standing out as one of a couple in the past, a couple

544
00:40:12,800 --> 00:40:18,840
of weeks where there was this undercurrent or suggestion of us being a peak deep learning

545
00:40:18,840 --> 00:40:23,120
which has not really been much of a thing in my conversations.

546
00:40:23,120 --> 00:40:30,320
It's a little bit, you know, that's a lot to extrapolate off of two comments.

547
00:40:30,320 --> 00:40:31,320
Sure.

548
00:40:31,320 --> 00:40:36,080
But interesting that you're saying that and so probabilistic graphical models is one

549
00:40:36,080 --> 00:40:43,600
of the things that you think about as a, if not a contender, a co-existing thing, are

550
00:40:43,600 --> 00:40:48,640
there others that come to mind as, you know, you're worth?

551
00:40:48,640 --> 00:40:54,480
I mean, yeah, well, closer to neural networks, I mean, you have RNNs that can benefit from

552
00:40:54,480 --> 00:40:57,200
much of the things that we've talked about.

553
00:40:57,200 --> 00:41:02,800
But I think makes DNNs and actually RNNs as well or anything that has a deep learning

554
00:41:02,800 --> 00:41:07,360
component in it like, you know, when you look at deep reinforcement learning, it does rely

555
00:41:07,360 --> 00:41:12,480
on using or encoded the state action using deep neural networks.

556
00:41:12,480 --> 00:41:20,240
So I think the appeal of that is that it's a very well structured way to look at a problem,

557
00:41:20,240 --> 00:41:21,240
right?

558
00:41:21,240 --> 00:41:24,440
Because you understand or we think we understand where the components are, we don't

559
00:41:24,440 --> 00:41:26,280
really understand how they work.

560
00:41:26,280 --> 00:41:28,880
But we understand what the components do.

561
00:41:28,880 --> 00:41:33,760
So I think it's, people are still going to stain that space quite a bit.

562
00:41:33,760 --> 00:41:37,080
What I think needs to happen for others, and I just mentioned probabilistic graphical models

563
00:41:37,080 --> 00:41:39,960
because it's really different, right?

564
00:41:39,960 --> 00:41:42,520
It's not as structured.

565
00:41:42,520 --> 00:41:49,600
And you know, when people think of AI, there's many other things that, you know, are not

566
00:41:49,600 --> 00:41:55,840
quantizable as how many MAC operations can you do in parallel or efficiently, right?

567
00:41:55,840 --> 00:41:59,280
So there's more on the decision making side.

568
00:41:59,280 --> 00:42:01,800
And that's what makes it a true AI, right?

569
00:42:01,800 --> 00:42:07,840
So I think the gap between where we are now and where we could be is quite, quite wide.

570
00:42:07,840 --> 00:42:14,440
I think to get to see more interesting things happen in other types of machine learning applications

571
00:42:14,440 --> 00:42:20,040
is a need for those things to happen more efficiently.

572
00:42:20,040 --> 00:42:25,800
And maybe that need is not there yet because we can still perform those tasks using

573
00:42:25,800 --> 00:42:27,760
existing approaches.

574
00:42:27,760 --> 00:42:33,880
Another way to see an impetus in that space would be to have hardware that supports it.

575
00:42:33,880 --> 00:42:40,280
But maybe not because of them, because of some other type of application that is really

576
00:42:40,280 --> 00:42:41,440
requiring that.

577
00:42:41,440 --> 00:42:47,800
So I think machine learning was the lucky winner of the GPU revolution.

578
00:42:47,800 --> 00:42:52,480
GPUs were built in mind with, you know, parallelism.

579
00:42:52,480 --> 00:42:56,640
And we switched from the single core to multi-core, but then multi-core were not enough and

580
00:42:56,640 --> 00:42:59,280
then GPU came into the picture.

581
00:42:59,280 --> 00:43:07,000
So can we envision a trajectory like this for other machine learning applications absolutely?

582
00:43:07,000 --> 00:43:08,000
I just don't know.

583
00:43:08,000 --> 00:43:13,440
I mean, there's many other things that people are looking at from the hardware side.

584
00:43:13,440 --> 00:43:18,760
I know there's quite a bit of interest in removing the memory wall bottleneck, which has

585
00:43:18,760 --> 00:43:21,400
been there for quite a bit of time.

586
00:43:21,400 --> 00:43:25,840
But to be able to do that, to do processing in memory, or to do other kinds of advances

587
00:43:25,840 --> 00:43:28,760
in that field, you need help from the technology.

588
00:43:28,760 --> 00:43:34,760
So I think maybe the progress in that space might be slowed down because we don't really

589
00:43:34,760 --> 00:43:39,960
have a replacement for our current technology, although it's still working quite fine so

590
00:43:39,960 --> 00:43:40,960
far.

591
00:43:40,960 --> 00:43:47,840
So talking about exponential trends, I mean, Moore's law was supposed to double performance,

592
00:43:47,840 --> 00:43:52,440
you name it, transistor density, pick your favorite every couple of years or every year

593
00:43:52,440 --> 00:43:53,440
and a half.

594
00:43:53,440 --> 00:43:59,360
And it stopped, but it doesn't mean we stopped in making things more, you know, better and

595
00:43:59,360 --> 00:44:00,440
better and more efficient.

596
00:44:00,440 --> 00:44:05,920
So there's always something that we can do, and I'm really hopeful that's going to be

597
00:44:05,920 --> 00:44:08,320
the case in machine learning as well.

598
00:44:08,320 --> 00:44:09,320
Awesome.

599
00:44:09,320 --> 00:44:10,320
Awesome.

600
00:44:10,320 --> 00:44:13,200
Well, Deanna, thanks so much for taking the time to chat with us.

601
00:44:13,200 --> 00:44:15,280
It's great to learn a bit about what you're up to.

602
00:44:15,280 --> 00:44:16,280
Thank you.

603
00:44:16,280 --> 00:44:17,280
Thank you for having me.

604
00:44:17,280 --> 00:44:23,320
All right, everyone, that's our show for today.

605
00:44:23,320 --> 00:44:29,120
For more information on today's show, visit twomolai.com slash shows.

606
00:44:29,120 --> 00:44:50,240
As always, thanks so much for listening and catch you next time.

