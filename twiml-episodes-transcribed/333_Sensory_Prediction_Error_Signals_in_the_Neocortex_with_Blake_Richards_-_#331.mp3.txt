Welcome to the Twomel AI Podcast.
I'm your host Sam Charrington.
This week on the podcast, I'm happy to share just a few of the nearly 20 interviews I recorded
earlier this month at the 33rd annual NURRIPS conference.
If you've been waiting for the Twomel pendulum to swing from workflow and deployment back
over to AI and ML research, this is your time.
We've got some great interviews in store for you over the upcoming weeks.
Before we move on, I want to send a huge thanks to our friends at Shell for their support
of the podcast and their sponsorship of this NURRIPS series.
Shell has been an early adopter of a wide variety of AI technologies to support use cases
across retail, trading, new energies, refineries, exploration, and many more, and is doing
some really interesting things, but don't take it from me.
Microsoft CEO Satya Nadella recently noted that what's happening at Shell is pretty amazing.
They have a very deliberate strategy of using AI right across their operation from the drilling
operations to safety.
Last year, the company established the Shell.AI Residency Program, a two-year full-time
program, which allows data scientists and AI engineers to gain experience working on
a variety of AI projects across all Shell businesses.
If you're in a position to take advantage of an opportunity like this, I'd encourage
you to hit pause now and head over to Shell.AI to learn more, once again, that Shell.AI.
And now on to the show.
All right, everyone here in Vancouver for NURRIPS.
And I am with Blake Richards.
Blake is an assistant professor in the School of Computer Science and the Montreal Neurological
Institute at McGill University, as well as a core faculty member at Miele.
And you've also got an affiliation with C-FAR.
Yes, I'm a candidate C-FAR AI Chair and a member of C-FAR is Learning and Machines
and Brains Program.
Fantastic, fantastic.
Well, Blake, welcome to the C-FAR AI Podcast.
Thank you very much for having me.
Awesome.
So you are doing a talk here on sensory prediction error signals in the Neo Cortex.
Yes.
Let's just jump right into that.
What's the talk about?
Sure.
So, a lot of people have postulated for a long time that our brains, and in particular,
the Neo Cortex, the region concerned with higher order thought or functions, if you will,
is effectively an unsupervised learning machine.
It is there to make predictions about incoming stimuli and that it would use differences
between those predictions and the actual data that it receives in order to learn about
the structure of the world and develop a good internal model.
And although there have been many computational studies that have postulated this and this
idea has also informed artificial intelligence a great deal, the fact is that there isn't
a lot of direct evidence for it in the brain.
There are a few initial studies.
But myself and my collaborators, Joel Zalberberg at York University, Joshua Bengeo, also at
Milla in University of Malayal, and Tim Lilly crap that glue deep mind.
We put together a proposal to the Allen Brain Institute a couple of years ago to run some
experiments to explicitly look for some of the sorts of prediction error signals that
these kinds of models of unsupervised learning in the brain predict would be there.
So the Allen Institute has been running a series of studies doing what's called two photon
calcium imaging in mice.
This is basically a way to record the activity of many hundreds of neurons at once, as well
as their dendritic processes in a live animal.
And so we've got recordings of the brains of mice, primary visual cortex.
Well we expose them to stimuli that follow particular statistical patterns, which we then
violate occasionally.
And we have found evidence for very clear and really strong responses to those violations
of the expected stimulus.
And additionally, there are some interesting kind of breakdowns in terms of where those
signals appear in the cortical circuit.
And also some interesting data in terms of the way that it seems to be something that the
animals actually learn over multiple exposures to the stimuli.
So you're conditioning mice to expect some kind of response, then you kind of take a
left turn when they're expecting a right so to speak and you're observing what's going
on in their brains as a result.
And so what you found is what?
So what we've found is we've examined two different types of stimuli, one which is where
you've got a consistent visual flow in the screen as it were.
So there's these bricks that kind of drift across the screen in a particular direction.
And they're always consistent in that movement.
And then occasionally some of the bricks will start moving in the different direction than
the expected one.
Suttily or like pretty clearly, and like you notice it when you watch the stimuli yourself
very much so.
And for those stimuli, we see a massive response in a particular part of the neocortical
micro circuit.
The cells go nuts in response to this stimulus and this seems to happen right off the
bat with no training.
So that suggests that this particular type of violation of expected stimuli is something
that the circuit is hardwired to detect.
But we also have another set of stimuli where we present basically these random patches
of edges that are all sampled from where the orientation of the edges are all sampled
from a particular distribution.
And then occasionally we violate that expectation by sampling from a different distribution for
the orientations.
And when we present these stimuli to the animal, at first we don't see any responses to the
unexpected orientations.
But over multiple recording sessions, we start to see huge responses to the unexpected
orientations.
So what's interesting about this is it suggests that the circuit is able to learn as it were
to be surprised to particular types of stimuli.
And it might, at the same time, be hard-coded to respond to particular other types of violations.
We hypothesize that this might have to do with ultimately the evolutionary purpose of most
visual cortex, one of which would obviously be to help the most avoid predators.
So it's very important that you detect violations of visual flow in your visual field if you're
trying to avoid predators, because that's something you want to avoid potentially.
You want to see that hawk flying above or that's right, exactly.
But for the other type of stimuli that we're showing them, where it's these oriented
edges that can violate these patterns, what's interesting is that they don't show that response
right away, but they learn to show it.
And so that is some evidence that their neocortex is, in fact, a sort of generative model that
can learn the data distribution over time and learn to be surprised when data doesn't
actually adhere to that distribution.
In the first case, where you've got more of a stark difference in the visual pattern,
do they become desensitized to it over time?
We don't actually see any evidence for desensitization, which is interesting.
The signal continues to be very robust over three different days of recording sessions,
and each session is an hour long.
So even after many reputed exposures of this, they still seem to signal this very strongly,
which again suggests that for that particular type of stimulus, this is a hard-wired component
to certain extent.
Which is very consistent with the evolutionary, like if you've got desensitized hawks,
that'd probably be a bad thing if you're not exactly.
Exactly.
And so there was another element of this work, or at least one that you haven't gone
into this level of detail yet, that is talking about the hierarchical nature of inference
in these circuits, is that a kind of an ancillary result, or is that core to the model that
you've developed to understand this stuff?
Yeah, so the thing that I didn't mention is that what's interesting is that that second
type of surprise signal that we see, that the animals learn to be surprised to the orientation
of edges that occur in an unexpected way, we actually see that signal not in the neurons
themselves, but in the dendritic trees of the neurons, and in particular part of the
dendritic trees, that is the area...
That's right, it's being like the fingers that we see in our depiction of the nerves.
Yes, that's right, exactly.
All those little branches that come out of the dendrite out of the neurons, those are
dendrites, and those are the sites of synaptic inputs to real neurons.
But real, pure middle neurons in the neocortex, which is a particular type of neuron that comprises
75% to 80% of the neurons in the neocortex, and it's the key information processing cell
type in this circuit, these cells have one unique dendritic process called the apical dendrite,
which they send up...
Apical?
Yeah, apical.
And they kind of like a tree, because what they do is they send it up to the top, the surface
of the brain, almost like what the trunk of a tree does to get the leaves up to the sunlight.
But in this case, they send it up to the surface of the brain, and what they receive at this
location are the top-down inputs, so higher order information from other parts of the brain.
And our data suggests...
So that is actually where we see those surprise signals that are learned.
And from some additional analysis...
At the tops of the brain, or in this structure overall.
In this dendritic structure that is up at the top of the brain, and what that data suggests
and some of our other analyses suggest is that this surprise signal, that violated my
expectations signal that the animals learn, is being driven by top-down inputs.
So that suggests that the entire model that they have for the world that they're learning
is a hierarchical model, where it's actually the higher order parts of the network, if you
will, for machine learning people, you can think of it as the upper layers of the network
that are actually detecting the violation of the expected statistics, and then they are
communicating that back down the hierarchy to the lower layers of the network.
Is there a relationship between this hierarchical nature of inference that you've observed
and the idea of spiking in neural nets, where a signal becomes significant after a certain
number of times for lack of a scientific one?
You know where there's like a build-up of the signal and it crosses some threshold?
Potentially.
I mean, we do find that the surprise response that we see in this circuit takes around 300
milliseconds to appear after the initial surprising stimulus.
And to put that into perspective, that's a pretty long time.
Usually it takes around 50 milliseconds or so for visual stimuli to get into visual cortex.
So that means...
And this is the second case, as opposed to...
This is the second case where the animals learn this signal, that's right.
And what that suggests is that there is a whole build-up of activity.
There's additional stuff going on before the surprising stimulus gets noticed by the
circuit as it were.
Now that might be a build-up of spikes, but it also might just be because the information
has to be propagated up the hierarchy and then back down the hierarchy.
And each synaptic step takes an extra 10 milliseconds kind of thing.
So it suggests that there's potentially a fairly large hierarchy or very recurrent
circuits that are responsible for calculating this surprise signal.
Are you distinguishing or able to distinguish in your work something akin to learning versus
realizing or paying attention to or other things that may be happening?
So that's an interesting question.
The question of whether or not they're just paying more attention to it, we try to
get at by measuring a variety of behavioral signals, including, say, for example, the
dilation of their pupils.
And we don't have any evidence to suggest that they are more attentive when we see these
signals.
Their pupil dilation is no different from the other times that it occurred and we didn't
see the signal.
We also look at their running speed and we don't see any change in their running speed,
so it suggests that it's not like a treadmill kind of thing.
They're on a treadmill.
That's right.
So just that they're not like, I don't know, just like kind of wigged out and they respond
differently and this drives the response.
Instead, there seems to be some kind of build up of the calculation that implies learning.
But you may have just answered this.
Yes.
But is the amplitude of the response the same as the in the first case?
It's not surprised.
It's just they.
Right.
So the amplitude is the same as in the first case where we have that violation of visual
flow.
Okay.
But it's in a different part of the circuit.
So in the first case, it's actually the cell bodies that are signalling this, a particular
type of cell body.
And that suggests that in that first case, that's a calculation that's happening potentially
locally within the circuit.
Whereas in the second case, this is, as I said, the signal is in these dendrites that
are receiving information from higher up, the cortical hierarchy.
So suggesting that it's something that's part of the hierarchical calculations rather
than a local calculation.
Now that you've made this discovery, like, what's next in this line of research, what
does it tell you?
So an example of the kind of thing that this tells us and, you know, obviously one of
the things we're interested in is just understanding the brain.
And so, you know, this is evidence that indeed the brain has a hierarchical predictive model,
a generative model as it were.
And, you know, next steps will be trying to understand how this generative model works,
more fully explore new stimulus space, understand the type, the role of different cell types
and modulators in this model, et cetera.
But as well, one of the other things that we're very interested in is using the lessons that
we gain from neurobiology to inform new machine learning designs.
So one of the things that we're interested in exploring further is developing new unsupervised
learning models based on some of what we learn here.
And there are a few different avenues that we're interested in exploring this way.
So one is we think, and it's a little bit too complicated to go into, I think, here,
but we've got to at least try.
Okay.
So there is another type of cell in the circuit that is responsible for sending inhibitory
signals to these dendrites where we see these surprise responses.
And we have been trying to rack our brains about, okay.
So what is happening in a circuit?
Why would it learn this surprise?
And we wondered if one of the reasons it might learn this surprise is because there were
changes in this other cell type that were inhibiting the dendrites.
And in particular, what we've started to get interested in is that there might actually
be a connection with these results with a class of unsupervised learning models known
as contrast of predictive coding.
So contrast of predictive coding is one approach to doing these sorts of next frame prediction
models.
And what they do in contrast of predictive coding is technically they're trying to maximize
the mutual information between a higher order context variable and incoming stimuli.
But the way that they do this is basically they compare a top down prediction from up the
hierarchy with the data.
And then they compare that match to the extent to which the prediction matches other data
samples from the same distribution, but not the appropriate frame.
These are called negative samples.
And from a machine learning perspective, this has all sorts of advantages.
But one of the things that we got thinking about from a neuroscience perspective is what
if is potentially happening in the circuit is basically that this other cell type that
normally inhibits these dendrites is having to actually learn that this is no longer a
sort of negative sample.
And so we're starting to build ML models where what we do is we take a sort of contrast
of predictive approach, but we have alternative circuits that are learning the data distribution
for the negative samples and using that to compare to the positive samples.
And this has potential interesting applications for online learning because you could do this
now in a totally online manner real time having to live.
Yes.
And without having to store previous examples or anything like that, which might be interesting
for kind of later extensions of this work to edge computing in the field kind of applications.
Is this related to like the concept of forgetfulness in a sense, thinking of like forget
gates and things like that that have come up in conversations in the past?
Right.
That's interesting.
I mean, it's not directly related.
There have been people who have postulated that these other types of cells that I mentioned,
these inhibitory cells might be involved in something like forget gates and output gates.
And I think that's possible.
What we're exploring here in particular is the idea that these cells are as it were,
you can think of it more like a GAN scenario.
So the idea is that you're going to generate predictions from up in your hierarchy.
And then what you want to do to train your predictions is you want to not only have the
data that you're trying to predict, but also some negative samples that are going to
try to fool your predictions as it were.
And so then what you want to do is you want to train simultaneously your predictions to
get better at differentiating the true data from the false data that you're generating,
but you also want to train that false data generator to generate better, better, better
false data.
That's the GAN analogy.
Yeah.
That's right.
So we're thinking of this as more like that these inhibitory cells are effectively trying
to produce false stimuli to fool the predictive system.
And then the predictive systems job is to try to learn that this is false and what's
reality.
It's kind of amazing to relate all of the machinery that goes into a GAN to a single
cell.
Yeah.
Or a population of cells, but this is my work.
That's right.
Yes, quite.
Quite.
Is there an element of what's interesting here that in kind of traditional inference if
that makes any sense like that you have only positive signal and here you've got kind
of both positive and negative signal.
That's right.
That serves a really useful purpose for training.
And this is part of the reason that contrastive predictive coding works well.
In contrasting the positive samples to the negative samples, you can end up doing a lot
better at capturing more interesting distributions in your predictive system.
No, why is that?
Well, so there's a variety of reasons.
One reason is there are these, so the classic predictive models in neuroscience and also
some of the predictive models in machine learning.
What you do is you generate a prediction.
You compare the prediction to your incoming data and you take the difference.
And then that difference is your error signal and you can pass that back up the hierarchy
in order to do more training of your prediction.
So these are called predictive coding models and it's based on a particular theory from
some guy's name Rowan Ballard that goes back to 1999, but which a lot of neuroscientists
are interested in and a few machine learning people, you know, David Cox's group at IBM
has built the prednet, which is basically just a stacked predictive coding system.
And these systems can do interesting things and they can do frame prediction.
But arguably there are some difficulties with frame as in an image.
Yes, sorry.
So like you've got a movie and you feed in a bunch of frames of the movie and then the
job of the system is to predict the next frame of the movie.
And do you mention that to say that that's one of the things that they're good at or that's
one of the things that they're better at than other things?
Well, so this is what they're designed to do, these sorts of predictive coding models.
Got it.
But there's a slight problem with them, which is that you have this situation where if you've
got a, so if you have a unimodal probability distribution over your data, these models
can work very well.
But once you have complicated multimodal distributions, it starts to break down a little bit.
And the reason is that if you think about it this way, okay, so let's say I have a complicated
multimodal data distribution, I make a prediction and I get, I make a prediction that it's going
to be on a particular mode and then I get data from another mode.
And what I'm going to do then in these standard systems is I'm going to try to reduce the difference
between my prediction and reality.
But what that's going to do is that's going to bring me off the original mode that I predicted
into some in between space that might actually have very low probability.
So that's not actually what I want to do.
Instead what I want to understand is that I just sampled from the wrong mode there effectively
and I should have been predicting the other mode for that data.
So potentially what you want is you want to be able to contrast your predictions to not
only the incoming data you get, but to all the other negative data distributions from the
data that you have received to know basically whether or not the problem was that you were
in fact predicting a wrong part of the data as opposed to simply just not the right piece
of data.
And so by actually doing this contrastive thing you can end up doing a much better job
of making predictions with complicated probability distributions than you can with a standard
kind of just like take the difference and learn on that model.
So is that also imply that it's better when the distributions itself are shifting or
themselves are shifting as opposed to static distributions?
Potentially that's an interesting question.
I think that the, now obviously you have to be able to deal with shifting distributions
for doing movie prediction because necessarily the distribution for a frame is distributed
according to a conditional probability distribution based on the other frames and that's
what you're trying to learn.
I think there's the other interesting question though which is if you have a shifting dynamics.
So the way in which frames depend on past frames changes over time.
Can you learn that?
I am not sure that some of these standard systems could deal with that that might be require
additional machinery for such networks.
It's also an interesting question as to whether or not animals can do that because arguably,
I mean probably animals could but generally test for that.
Well, yeah, you see the funny thing is how would you test for that because generally
our world though it can be changing and though the actual kind of distributions of data
that we receive are hardly stationary, the dynamics in the world are relatively stationary.
So objects tend to move in consistent ways and it's not like the laws of physics occasionally
switch on us.
And so as a result, it's not clear that the brain has to be able to deal with shifting dynamics
necessarily.
But at the same time, things will happen in certain types of movies that are unexpected
in a real world but we still kind of expect them.
Yes, and we can get used to them.
And we get used to them, right?
That's right.
And so that's why I was about to correct myself as even though it seems like maybe animals
don't have to deal with that, our own intuition would be that we seem to be able to deal
with that because let's say if we play a video game where the laws of physics are indeed
a bit different, over time we will get used to that video game and we will kind of understand
its dynamics.
Right.
And so this gets back to the idea that, you know, and this is a discussion that we have
in the sort of neural AI intersection a lot is how much is hard-coded, how much is
innate, and what can we learn and what can we not learn, and even though it is like
indeniable that evolution endows us with a massive innate structure to help us understand
the world.
A lot of our core facilities also seem to be learnable and tunable.
And so even something as simple as kind of being able to predict what stimuli you're
going to receive in the near future.
Our data, for example, suggests that that's something tunable and it's possible that
also the dynamics of these of the world is tunable for our models.
Kind of going back to this particular research and the hierarchical inference result and
some of these other results.
How close are you to having an implementation of that in the machine learning world?
So it's something that we're working on and I've got a couple of students who are trying
to build these systems where the negative examples are being learned by an additional
network that's doing the off-distribution estimation.
And I think that we've made some progress on that.
We're still probably a ways away from it.
We're in the grand scheme of things, a small academic lab compared to the big ML outfits
in industry and stuff like that.
So it'll probably take us longer if I could take them, even just because of the compute
resources that we have to compete for.
But I think that hopefully within the year we'll have a working model of the basic thing
and we'll be able to say whether or not this actually works well in practice or if it's
just a bad idea.
Separating this particular result with the negative examples from the concept of hierarchical
instance, like how well has that idea been explored?
And I've got to imagine that there are examples of that or prior research into that idea or
is that novel in some way.
I think it's actually reasonably novel.
I haven't seen a lot of stuff this way for predictive models.
Obviously, there's a lot of work now in training adversarial systems where you generate sort
of negative data that you have to discriminate from positive data and that's just across
the amount of tons of this stuff, but using this specifically to do the predictions in
time series, I haven't seen yet, but I can't keep on top of all literature.
And so as I say this, I'm sure someone's publishing exactly this.
Right, right.
There's a talk happening right now.
Yeah, that's right.
That's right.
Yeah.
Nice.
So what are some of the other things that your live is working on?
So another area that we're really interested in is the use of memory systems for reinforcement
learning.
And this is again, motivated by the neuroscience literature.
So one of the systems like attention or...
So attention and memory are, you know, kind of intimately related, but let's maybe keep
it simple for a moment and just focus on classical memory systems.
And what I mean by that is to give you a sense.
So the region of the brain that is typically associated with what we call episodic memories
is the hippocampus.
Episodic memories just refer to actual memories of events from your life where you can say
stuff that happened.
You can say X occurred in this location with these people there at this time.
And that depends on this brain region, the hippocampus.
We know that from, you know, decades of neurology research.
People who have their hippocampus damaged can't form new episodic memories.
What the idea in applying reinforcement learning is, if in addition to remembering a kind
of a current cumulative reward, I can remember stuff that I did and stuff that happened.
That's right.
I should be able to converge faster to some kind of...
Yes, right.
So what's interesting is in the neuroscience field, there's been more and more evidence
suggesting that the hippocampus is involved in reinforcement learning in animals.
And indeed the hippocampus makes direct projections to some of the core circuits in a region
called the basal ganglia that seem to be key for reinforcement learning.
And various theoreticians have posited that the hippocampus in providing this sort of episodic
record could be used for bootstrapping reinforcement learning.
Basically...
The hippocampus predicts or does it just serve up experiences?
Is it just memory or is there some inference in there too?
That's a great question.
So the original proposal, which was made by Peter Dianne and Matti Lengiel back in the
mid-2000s, was that it just provides you with a record of explicit trajectories that
you've done through the state space, basically.
And they posted it by taking an explicit record of trajectories you've done, then you can
just rely upon like, okay, well, I did this last time and I got something okay so I can
just do it again.
And they showed that this could speed up reinforcement learning in the kind of early phases or just
after a change occurs.
But subsequent work in neuroscience has suggested that the hippocampus is really key for in
fact making predictions and making inferences and doing imagination, kind of doing forward
rollouts in your mind.
And so I think that...
How do you test for that?
Well, so the way that they test for this, you know, there's a bunch of different ways,
but fundamentally what you do is you record from the hippocampus well an animal or a person
is doing something and you train a decoder to take hippocampal activity and say what the
person was looking at or where the animal was in the environment.
And then you can show that when you then put the person back in this task or the animal
back in this environment and you have them go around and you're recording from their
hippocampus, there will be moments where your decoder says, oh, what's actually happening
right now is that the person is looking a few frames ahead in the sequence or the animal
is down the hallway to the right kind of thing.
And so that suggests that the activity in the hippocampus is basically following a forward
trajectory through space or time, which is pretty cool.
Yeah, that's amazing, totally.
And so there's also other evidence suggesting that the representations in the hippocampus
themselves are in fact predictive.
There's within reinforcement learning that this was another contribution from Peter
Diane.
There's a particular type of representation called successor representations, the successor
successor.
That's right.
And there are a way of representing state space that allows you to do one shot adaptation
to changes in the reward function.
And what's, the way they do that is basically they pull out the transition dynamics from
the MDP out of the expectation for your value calculation or rather pulls the reward out
of the expectation.
And that means that now when you redo your reward calculations, you don't need to worry
about averages, which speeds things up.
And so, so anyway, what's interesting is Kim Stockenfeld and Sam Gershman showed in
a paper, I guess, either two years ago or one year ago now, that the representations
in the hippocampus seem to adhere in many ways to successor representation type dynamics
and predictions.
And so what's interesting is successor representations are basically a way of representing the environment
in a predictive manner because how they do this is they represent each state according
to the expected future occupancy of that state from your current state.
So it's kind of saying that there's evidence that fundamentally the hippocampus isn't
just a kind of snapshot of scenes, but it's an inherently predictive representation
of this scene kind of happened and this thing kind of resulted from whatever action you
took.
Right.
So, so what's interesting is that that's roughly right, but I would rephrase it slightly.
I would say that it looks like the hippocampus is not just a record of what you've seen
or done, but it is a record that is encoded in a manner that also predicts what you are
going to now do in the future.
It's like taking your past to predict your future at a very high level representation
of the environment.
And this successor was a successor representation.
That's kind of the machine learning approximate of that or analog.
That's right.
Okay.
So this, you know, the successor representation is just an idea from machine learning.
Yeah.
And what's interesting is that it seems to predict a lot of data in the hippocampus.
Now, is the hippocampus actually a successor representation?
Sure.
I don't know, but I think the core idea that part of what the hippocampus is doing is providing
a sort of predictive map for the environment and something that would also allow you to
do more so that it's not just like functionally predicting, but it's representationally
predicting.
That's right.
It's representational.
That's pretty cool.
And so my lab is working on a variety of implementations of reinforcement learning that
takes advantage of these sorts of systems to then do much faster adaptation to changes
in the environment.
Okay.
So in particular, what we're building, because what's interesting right is you've got,
so the successor representation had been developed previously in machine learning, but then there's
a variety of interesting questions in terms of, well, okay, what if now you are explicitly
storing different memories of different successor representations for different contexts?
And furthermore, what if you can use those memories of different successor representations
for different environments in order to sort of load up your predictions for any new context?
We think that this should be able to help you to do reinforcement learning much more rapidly
and we have some initial data showing that that is indeed the case.
So kind of broadly, the idea is that we want to take the lessons that we're learning about
how memory systems in the brain work and use those to inform the design of new RL systems
that are better at adapting to changes in the environment.
Huh.
Interesting.
Interesting.
Sounds like it's still a little early or do you have an implementation?
So we do have an implementation in toy tasks that works really well and we can basically
take a specific thing or just toy tasks.
Toy tasks, meaning just all the standard kind of stuff.
So grid worlds, Atari games, these sorts of very simple tasks, right?
And what we find is that in very simple tasks, this approach of storing multiple different
successor representations for different contexts and also utilizing what we call
a sort of evolutionary initialization where we learn a sort of average successor representation
across contexts that we use to initialize all the memories, combining these two ideas
leads to RL systems that can do very rapid, just kind of like one-shot adaptation to changes
in the environment, which is cool.
But we need to see if changes are step-to-step or changes to the reward function, critical
changes to the reward.
Also changes to the task.
So if you drop the agent in a new task or within a given task, if you move where the rewards
are located or change the reward function in any way, that it can adapt quickly to this.
And it changes to the reward function inherent to kind of your typical Atari game or is
it...
So that's not something...
Yeah.
So typically we do this, I should say, when we look at changes to the reward function, we're
doing this in little environments that we've made.
And typically it's like foraging tasks where the agent has to go through a little maze
to find rewards and what we do is we change where the rewards are located.
And if you just take a standing where the reward is located, it doesn't necessarily dictate
a new reward function, does it?
Will it die about that correctly?
Well it does because the reward function is typically defined as, okay, for any particular
state and action pair, you're going to get a particular reward.
And so if you change to the location of the reward now, the state action pair that gets
you a reward is different than the one that it was previously.
And if you...
There are a variety of different ways to do this.
So it should be said that this result in and of itself is not super exciting.
Other people have shown ways to adapt to changes in the reward function.
But what we're hoping is that we'll be able to show...
It is early days, but you know, as we scale up, there will be able to show that in more
complicated environments, as you change the context and you drop the agent down into
different contexts, they will always be able to adapt very rapidly by using these sorts
of predictive memories to guide their updates.
Okay.
Yeah, I thought you were talking about like structural changes to the reward function
as opposed to...
I see.
Kind of parameterize.
No, just parameterize.
Okay.
Okay.
And that's what the successor representation is good at dealing with.
Got it.
You can update its parameters to the reward function in one shot very quickly with just a single
multiplicative calculation.
Okay.
Well, sounds like really interesting work.
Thank you.
Yeah, thanks for taking the time to share a bit about what you're up to.
My pleasure.
Thanks very much for having me.
Thank you.
All right, everyone.
That's our show for today.
For more information on today's guest or our NURPS podcast series, head over to Twemal
AI.com slash NURPS 2019.
Thanks once again to Shell for sponsoring this week's series.
Check out the Shell.AI Residency program by typing Shell.AI into your browser's address
bar.
Thanks so much for listening.
Happy Holidays and catch you next time.
