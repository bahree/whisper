WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.440
I'm your host Sam Charrington.

00:32.440 --> 00:35.980
This week's shows are drawn from some of the great conversations I had at the recent

00:35.980 --> 00:40.960
Nvidia GPU technology conference and they're brought to you by Dell.

00:40.960 --> 00:44.840
If you caught my tweets from GTC, you may already know that one of the announcements this

00:44.840 --> 00:49.320
year was a new reference architecture for data science work stations powered by high

00:49.320 --> 00:54.480
NGPUs and accelerated software such as Nvidia's Rapids.

00:54.480 --> 00:58.960
Dell was among the key partners showcased during the launch and offers a line of workstations

00:58.960 --> 01:03.040
designed for modern machine learning and AI workloads.

01:03.040 --> 01:07.720
To learn more about Dell precision workstations and some of the ways they're being used by customers

01:07.720 --> 01:12.320
in industries like media and entertainment, engineering and manufacturing, healthcare

01:12.320 --> 01:20.440
and life sciences, oil and gas and financial services, visit Dell EMC.com slash precision.

01:20.440 --> 01:23.040
Alright everyone, I am on the line with Gerald Kwan.

01:23.040 --> 01:27.040
Gerald is an assistant professor at UC Davis.

01:27.040 --> 01:30.280
Gerald, welcome to this week in machine learning and AI.

01:30.280 --> 01:32.800
Thanks for the invitation to interview with you.

01:32.800 --> 01:33.800
Absolutely, absolutely.

01:33.800 --> 01:38.560
I'm looking forward to diving into some of the work that you've been doing around applying

01:38.560 --> 01:41.640
deep learning to genomics.

01:41.640 --> 01:46.320
Before we do that, I'd love to hear a little bit about your background and how you started

01:46.320 --> 01:50.360
working in the intersection of those two fields.

01:50.360 --> 01:57.280
So I guess my career in computational biology started out in my first year undergrad actually

01:57.280 --> 02:02.560
where I was initially slated to enter a computer science undergraduate program and about

02:02.560 --> 02:07.720
two weeks before I started, I got a piece of paper in the mail from my university telling

02:07.720 --> 02:12.320
me that they started a new bioinformatics program and were wondering if I was interested

02:12.320 --> 02:13.320
in joining.

02:13.320 --> 02:17.200
And actually initially, I initially turned them down because I thought the idea of kind

02:17.200 --> 02:21.440
of studying biology with computers was, you know, it didn't seem that interesting.

02:21.440 --> 02:27.160
But after a couple semesters, I just happened to meet a professor doing some really interesting

02:27.160 --> 02:31.160
kind of high throughput data analysis, looking at proteins in their kind of three dimensional

02:31.160 --> 02:36.600
structures and he was interested in kind of predicting these 3D structures from their

02:36.600 --> 02:40.680
kind of linear protein sequence and I was just kind of peaked my interest in from there

02:40.680 --> 02:43.720
I just kind of, yeah, I never really looked back.

02:43.720 --> 02:48.400
And so do you currently sit in which department at Davis?

02:48.400 --> 02:54.160
So I'm in the department of Lecler and Cell Bio, which is a little bit of a, yeah, it's

02:54.160 --> 02:59.040
kind of an interesting, interesting place to be because I, you know, my undergrad gain

02:59.040 --> 03:05.360
was kind of in computer science bioinformatics and then I took a brief break and went to biochemistry,

03:05.360 --> 03:09.960
had to do masters in biochemistry before I went back to computer science for a PhD.

03:09.960 --> 03:13.680
And I think this kind of background where I have a little bit of biology and a little bit

03:13.680 --> 03:18.160
of computer science background kind of appealed to my department and so that's kind of how

03:18.160 --> 03:19.160
I ended up here.

03:19.160 --> 03:20.160
Awesome.

03:20.160 --> 03:21.160
Awesome.

03:21.160 --> 03:26.480
And so you were recently at the NVIDIA GTC conference where you gave a presentation

03:26.480 --> 03:34.520
on deep domain adaptation and generative models for single cell genomics.

03:34.520 --> 03:40.800
What's the main challenge that you're talking about in that presentation?

03:40.800 --> 03:47.440
Well, so basically in the field of kind of molecular biology, you know, these kind of genomics

03:47.440 --> 03:52.560
technologies are the biologist's way of kind of getting really high resolution snapshots

03:52.560 --> 03:55.800
of different kinds of like human tissues and so on.

03:55.800 --> 04:00.360
And the work that I spoke with specifically is kind of on this emerging field called

04:00.360 --> 04:04.680
single cell genomics where you could actually do these measurements on individual cells

04:04.680 --> 04:09.240
where it's kind of historically you had to kind of use like really big tissues in order

04:09.240 --> 04:12.080
to get enough sample to generate this data.

04:12.080 --> 04:16.240
And so it's kind of exciting because you know, because you can do measurements on these

04:16.240 --> 04:22.240
on these single cells, you can generate vast orders of magnitude more data than you could

04:22.240 --> 04:23.240
previously.

04:23.240 --> 04:28.320
And so now, whereas before when you look at kind of human data, you could only generate

04:28.320 --> 04:33.200
data for say like a couple hundred tissues or samples, now we can generate like millions

04:33.200 --> 04:35.480
of data for millions of samples.

04:35.480 --> 04:40.440
And so that it's really exciting because you can now we can sort of apply a lot of the

04:40.440 --> 04:44.000
kind of methods from like deep learning and so on because we have this large amount

04:44.000 --> 04:46.760
of data that we we didn't previously have.

04:46.760 --> 04:53.160
What are some of the things that you're trying to understand through these analyses or diseases

04:53.160 --> 04:57.760
that you're trying to identify or fight?

04:57.760 --> 05:00.360
We try to study a couple of diseases of interest.

05:00.360 --> 05:05.000
So these include things like cancer and Alzheimer's disease and schizophrenia.

05:05.000 --> 05:09.000
And so the basic premise is that you know, if you want to kind of understand how say

05:09.000 --> 05:15.400
a disease like schizophrenia works, you know, you kind of want to take say samples from

05:15.400 --> 05:19.680
brain tissues from sort of so-called healthy normal people and you want to take them

05:19.680 --> 05:23.680
from these, you know, say people with schizophrenia, and then you want to somehow be able to

05:23.680 --> 05:28.960
compare, you know, the data that you get from both of these groups of patients in order

05:28.960 --> 05:34.000
to figure out what changed when you know, when schizophrenia happens at different stages.

05:34.000 --> 05:39.840
And so again, in sort of this, in our work that we presented at the GTC, the tool that

05:39.840 --> 05:47.600
we developed was specifically designed to basically build models of genomic data in both

05:47.600 --> 05:52.680
kind of normal people and schizophrenia people and then kind of use techniques from domain

05:52.680 --> 05:57.520
adaptation to kind of understand how the two are related.

05:57.520 --> 06:02.480
And this is maybe a more domain specific question.

06:02.480 --> 06:10.720
But for something like schizophrenia, it strikes me as more of a systemic problem than

06:10.720 --> 06:14.520
a, you know, something that you would see in a single cell.

06:14.520 --> 06:19.480
How does what's the connection?

06:19.480 --> 06:20.480
So you're right.

06:20.480 --> 06:25.320
You know, complex diseases like schizophrenia have, you know, there's many factors that

06:25.320 --> 06:28.000
contribute to the development of schizophrenia.

06:28.000 --> 06:33.200
In our particular work, we look at, we try to look at whole regions of the brain and how

06:33.200 --> 06:36.960
these whole regions may change between normal people with schizophrenia.

06:36.960 --> 06:42.160
And so the reason why this single cell stuff is really interesting is because previously

06:42.160 --> 06:46.000
if you wanted to, like we know that in the brain there's like lots of different types

06:46.000 --> 06:48.400
of cells and tissues in there.

06:48.400 --> 06:52.520
And so previously when people did these kind of studies, they could only take say an entire

06:52.520 --> 06:57.560
brain kind of chop it up and then look at a snapshot of the whole brain and compare

06:57.560 --> 07:00.920
a whole normal versus whole schizophrenia.

07:00.920 --> 07:05.760
But now because we can measure things at single cell, we can take, we can measure like millions

07:05.760 --> 07:11.400
of cells in a single normal person, millions of cells in a single schizophrenia person,

07:11.400 --> 07:15.560
and then try to compare and say, okay, of these million cells I got from one normal, one

07:15.560 --> 07:19.880
schizophrenic person, which subset of these million cells is actually changing because

07:19.880 --> 07:26.360
not all, like not, you know, not everything in the brain will change when you get schizophrenia.

07:26.360 --> 07:31.520
And so we've talked about genomics here is the data that you're fundamentally looking

07:31.520 --> 07:38.760
at, is it sequence data or is it some kind of imaging data or a combination of the two?

07:38.760 --> 07:49.280
So the data that sort of comes from the biologist comes in the form of like DNA or RNA sequence

07:49.280 --> 07:53.200
from these patients, but that kind of gets converted in this kind of data preprocessing

07:53.200 --> 07:59.480
steps such that the data that we look at essentially is just, it just blows down to matrices

07:59.480 --> 08:04.440
where like columns of these matrices correspond to different cells from a patient and rows

08:04.440 --> 08:09.440
represent kind of different features of the cells that are measured through genomics.

08:09.440 --> 08:17.440
So a feature, an example of a feature being like a known sequence or something, the existence

08:17.440 --> 08:21.000
of a known sequence or a snip or something like that?

08:21.000 --> 08:25.120
So it actually corresponds to like a gene and so the idea is that when you look at a cell

08:25.120 --> 08:31.600
within, you know, a patient, you can measure, you can make many different types of measurements

08:31.600 --> 08:36.600
on the cells, but one common type of measurement is called a gene expression measurement.

08:36.600 --> 08:42.640
So the idea is that, you know, each cell has DNA in it, but DNA by itself usually acts

08:42.640 --> 08:44.360
as kind of just a storage mechanism.

08:44.360 --> 08:50.600
So for DNA to do anything, it actually, large parts of it get converted to this molecule

08:50.600 --> 08:51.600
called RNA.

08:51.600 --> 08:56.360
And so what we're measuring, what each features measuring is how much of this kind of active

08:56.360 --> 09:00.480
RNA molecule gets produced from each part of the DNA.

09:00.480 --> 09:04.680
And so that's how we get like a vector for each cell where each component corresponds

09:04.680 --> 09:08.840
to how much activity we see from a given part of the genome.

09:08.840 --> 09:09.840
Okay.

09:09.840 --> 09:15.080
And so how does, where does domain adaptation and generative models, where do those come

09:15.080 --> 09:19.160
into your workflow here?

09:19.160 --> 09:24.760
So we're, the GTC talk was, was basically discussing two different projects.

09:24.760 --> 09:30.240
So in terms of domain adaptation, the problem we were trying to solve there is that essentially

09:30.240 --> 09:37.680
the problem is when you kind of look at, when you look at these features of cells in normal

09:37.680 --> 09:44.440
versus schizophrenia people, the, and you do say, so one of the, one of the most common

09:44.440 --> 09:48.360
first tasks that people do when they, when they get this kind of data is they, they will

09:48.360 --> 09:53.200
do some, you know, dimensionality reduction and they'll sort of visualize the cells that

09:53.200 --> 09:55.680
they get from these normal schizophrenia people.

09:55.680 --> 10:00.400
And the first thing that you notice when you do this visualization is that all of the

10:00.400 --> 10:06.200
cells that you collect from normal people separate very distinctly from the cells that you

10:06.200 --> 10:08.040
get from schizophrenia people.

10:08.040 --> 10:12.280
Now this is kind of a problem because the, you know, your underlying goal is, you know,

10:12.280 --> 10:17.240
you have these different types of cells represented in the normal and the schizophrenia people.

10:17.240 --> 10:23.160
And you ideally want to kind of in an unsupervised way match the right cell types from the normal

10:23.160 --> 10:27.800
schizophrenia people so that you can figure out for each type of cell, how are they different

10:27.800 --> 10:29.800
across the two populations.

10:29.800 --> 10:37.240
And so we use, we basically perform domain, domain adaptation to take these cells that kind

10:37.240 --> 10:42.120
of look very different overall between normal schizophrenia people and kind of merge them

10:42.120 --> 10:47.320
together such that cells of the same type basically look very similar to each other.

10:47.320 --> 10:51.800
And so we can kind of match them across, across these two groups of people.

10:51.800 --> 10:56.760
So the adaptation really is in basically building model to say, okay, given this is what my data

10:56.760 --> 11:00.480
from the normal people look like, given this is what the data from the schizophrenia people

11:00.480 --> 11:01.480
look like.

11:01.480 --> 11:08.240
Can I kind of match them such that the distribution of my cells in this feature space basically

11:08.240 --> 11:09.240
overlap?

11:09.240 --> 11:16.600
And so when you say the data looks very different between these two groups, in what sense?

11:16.600 --> 11:21.080
I mean, in some sense, that's what you want if you're trying to build a classifier that

11:21.080 --> 11:26.840
can determine whether a given cell indicates schizophrenia, for example.

11:26.840 --> 11:30.840
But it sounds like that's not the part of the process that you're working on here.

11:30.840 --> 11:37.480
So I guess just to be more clear, there are both kind of very large scale differences

11:37.480 --> 11:40.240
between normal and schizophrenia people.

11:40.240 --> 11:46.000
And then there's very kind of smaller changes that are specific to each type of cell that

11:46.000 --> 11:48.240
is different between normal and schizophrenia people.

11:48.240 --> 11:55.720
And so in a typical kind of analysis, you want to first characterize what are those very

11:55.720 --> 12:01.640
big changes that cause these two different types of cells from these two types of people

12:01.640 --> 12:05.160
to reside in different regions of this feature space.

12:05.160 --> 12:10.000
And then conditioned on understanding what those big changes are, you want to then take

12:10.000 --> 12:14.360
away the effect of that big change such that now you can look at individual cell types

12:14.360 --> 12:18.280
and then ask, okay, for these individual groups of cells that are common between normal

12:18.280 --> 12:22.200
and schizophrenia, what's different about them?

12:22.200 --> 12:25.000
What's the approach for doing that?

12:25.000 --> 12:32.120
There's been some previous work published, basically these class of models called associative

12:32.120 --> 12:35.200
domain adaptation have previously been developed.

12:35.200 --> 12:39.760
And so in the associative domain adaptation problem, the problem that are trying to solve

12:39.760 --> 12:45.440
is that they basically envision that you have, say, two different data sets where you're

12:45.440 --> 12:47.480
trying to perform the same task.

12:47.480 --> 12:51.280
So they assume that you're trying to do classification in dataset one and classification in dataset

12:51.280 --> 12:52.280
two.

12:52.280 --> 12:55.720
And they assume that you have, you know, you have the same labels that you're trying to

12:55.720 --> 12:58.200
classify in both datasets.

12:58.200 --> 13:04.120
Now what the assumption that the original associative domain adaptation makes is that although the

13:04.120 --> 13:08.600
labels that you're trying to classify are the same, the distribution of the data is different.

13:08.600 --> 13:13.480
So given your feature space, the data in one dataset sits in a different region of the

13:13.480 --> 13:15.480
feature space than the other.

13:15.480 --> 13:22.320
And so this, their domain adaptation approach was designed to basically try to essentially

13:22.320 --> 13:27.960
learn kind of common features that are predictive of the same labels across these two different

13:27.960 --> 13:29.360
datasets.

13:29.360 --> 13:36.040
And so our approach is basically similar in the sense that we have, you know, we have data

13:36.040 --> 13:42.640
in sort of two different regions of the feature space, corresponding to normal and schizophrenia.

13:42.640 --> 13:48.560
But in our case, we're trying to do an unsupervised analysis, so we're trying to group cells that

13:48.560 --> 13:52.640
come from these two different domains, but we think represent the same cell type.

13:52.640 --> 14:00.600
And so we basically modify their domain app, original supervised domain adaptation approach

14:00.600 --> 14:05.480
to do unsupervised clustering across two datasets.

14:05.480 --> 14:12.600
So the way that the approach works is that it's a deep neural network and it looks, it

14:12.600 --> 14:15.600
looks for all intents and purposes like an autoencoder.

14:15.600 --> 14:21.600
So, you know, the first half of the network takes the data in the original feature space

14:21.600 --> 14:26.000
and it projects it down to a low dimensional manifold.

14:26.000 --> 14:29.600
And then the second half of the network takes the low dimensional manifold and projects

14:29.600 --> 14:32.560
it back into some high dimensional space.

14:32.560 --> 14:38.240
But kind of unlike a typical autoencoder where the loss function that you're optimizing,

14:38.240 --> 14:42.960
you know, can be something like C squared loss, sorry, squared error.

14:42.960 --> 14:47.880
What are and what what our loss function is and what the other domain adaptation loss

14:47.880 --> 14:53.760
function is, is it it actually, what we try to do is we try to find a low dimensional

14:53.760 --> 15:01.920
embedding of the different cells such that the marginal distribution of the cells in the

15:01.920 --> 15:06.640
latent space overlap between the two datasets as much as possible.

15:06.640 --> 15:12.400
And so there's no kind of notion of kind of squared loss or reconstruction loss that

15:12.400 --> 15:13.800
we're trying to optimize here.

15:13.800 --> 15:19.440
It's really just trying to match the marginal distributions of the cells in the low dimensional

15:19.440 --> 15:22.280
embedding space between the two different datasets.

15:22.280 --> 15:28.920
And so the embedding space that you're creating when you're trying to overlap the two, that's

15:28.920 --> 15:33.240
essentially the domain adaptation aspect of this, is that right?

15:33.240 --> 15:34.240
Exactly.

15:34.240 --> 15:38.680
Yeah, we're essentially adapting like one domain to another.

15:38.680 --> 15:44.960
We're distorting one set of cells to match the marginal distribution of the other.

15:44.960 --> 15:52.080
So where does the domain adaptation fit in in your overall kind of end to end experimentation?

15:52.080 --> 15:57.840
Well, so what we do is we take the two the cells from the two different groups.

15:57.840 --> 16:04.160
We do this adaptation of to bring them into the same region of the future space.

16:04.160 --> 16:08.800
And that then allows us to match for an individual cell by cell basis.

16:08.800 --> 16:14.920
We can say, okay, this cell in the normal person corresponds to exactly this normal cell

16:14.920 --> 16:17.080
in the schizophrenic person.

16:17.080 --> 16:22.200
And then we can, that then allows us downstream to then look at those individual cells paired

16:22.200 --> 16:26.700
across normal schizophrenic people and ask the question, how are they different in

16:26.700 --> 16:28.680
terms of the original features?

16:28.680 --> 16:35.720
And when you're talking about looking at these different types of cells pre and post adaptation,

16:35.720 --> 16:44.760
are you taking the output of the domain adaptation and using that to train a model or is it for

16:44.760 --> 16:46.520
more manual analysis?

16:46.520 --> 16:50.560
So one thing that I didn't quite discuss it as I said that we, you know, we trained

16:50.560 --> 16:55.720
this network to learn and embedding a shared embedding space for both kind of the normal

16:55.720 --> 17:00.200
and schizophrenic people, but what I didn't talk about is that the other half of the network

17:00.200 --> 17:06.160
is still a decoder and it's a decoder in the traditional sense where we learn the encoder

17:06.160 --> 17:07.160
separately.

17:07.160 --> 17:12.640
And then after we learn the shared embedding space, we train decoders to take cells from

17:12.640 --> 17:18.360
this shared embedding space and project them back into the normal versus schizophrenic

17:18.360 --> 17:19.360
data set.

17:19.360 --> 17:22.760
And so this is important for the following reason.

17:22.760 --> 17:29.400
So the reason why domain adaptation is necessary for this kind of genomic analysis is that

17:29.400 --> 17:32.480
in the ideal biological experiment, you would be able to take a normal cell, you would

17:32.480 --> 17:38.080
be able to kind of assay it's, you know, make those high dimensional measurements, you

17:38.080 --> 17:43.880
would then be able to kind of give it schizophrenic and give it schizophrenia in some sense and then measure

17:43.880 --> 17:45.600
its activity again.

17:45.600 --> 17:49.600
And so in that sense, you'd be able to take exactly the same cell but kind of before and

17:49.600 --> 17:55.560
after you've applied this phenotype, this disease phenotype, and then see exactly what change

17:55.560 --> 17:59.640
occurred in that cell do do this, do this stimulus.

17:59.640 --> 18:04.880
And so this isn't possible in real biology because to kind of, you know, make these high

18:04.880 --> 18:08.760
dimensional measurements, you have to kill the cell and so you can measure the same cell

18:08.760 --> 18:09.760
twice.

18:09.760 --> 18:15.720
But because we can train decoders which take any cell in this shared embedding space, project

18:15.720 --> 18:21.880
it to normal or schizophrenia, we can now kind of simulate, you know, what would happen

18:21.880 --> 18:26.000
if you took one cell and you gave it either, you know, you made it either normal cell or

18:26.000 --> 18:27.480
schizophrenia cell.

18:27.480 --> 18:33.200
And so now we can treat these projections as paired data and then ask on a personal basis,

18:33.200 --> 18:36.560
you know, how do they, how do they differ between between these two conditions?

18:36.560 --> 18:41.000
And so, so getting back to original question, we don't, you know, the downstream analysis

18:41.000 --> 18:45.720
is still kind of somewhat manual but we are kind of trying to do something a little bit

18:45.720 --> 18:50.800
smarter by using these decoders to kind of simulate these biological experiments that

18:50.800 --> 18:51.800
we can't do.

18:51.800 --> 18:52.800
Right.

18:52.800 --> 18:53.800
Right.

18:53.800 --> 19:00.120
But ultimately you're not trying to like train a classifier to determine, you know, given

19:00.120 --> 19:09.280
a cell, whether it's normal or schizophrenic or not, it's more using the domain adaptation

19:09.280 --> 19:18.320
to allow you to more kind of manually compare the characteristics of these two types of

19:18.320 --> 19:25.120
cells absent the kind of broad spectrum differences between the two.

19:25.120 --> 19:30.760
So that's the domain adaptation piece and then you, you're using generative models as

19:30.760 --> 19:32.480
a part of this as well.

19:32.480 --> 19:33.480
Where do they come in?

19:33.480 --> 19:34.480
Right.

19:34.480 --> 19:38.920
And so the idea behind these generative models is that one of the kind of really hot research

19:38.920 --> 19:45.720
areas in biology right now is to kind of try to understand how cells work together.

19:45.720 --> 19:46.720
Right.

19:46.720 --> 19:50.560
And so, you know, there's this kind of this big focus on, okay, can we, you know, do these

19:50.560 --> 19:54.520
genomic measurements on these individual cell single cell levels?

19:54.520 --> 19:58.480
But part of the thing is that when you look at, like, you know, a disease like cancer,

19:58.480 --> 20:03.520
for example, cancer is not really just kind of a collection of individual cancer cells

20:03.520 --> 20:04.520
doing their own thing.

20:04.520 --> 20:07.480
They're kind of, there's a lot of cross talk between them.

20:07.480 --> 20:11.160
They're kind of helping each other out to kind of, you know, metastasize and so on.

20:11.160 --> 20:17.040
And so the idea of our generative model is that we kind of want to, is that, you know,

20:17.040 --> 20:23.680
it's very easy nowadays to take cells in isolation and, you know, measure their genomics profile

20:23.680 --> 20:25.440
to see what they're doing.

20:25.440 --> 20:29.920
And it's also easy to take a collection of them together when they're working together

20:29.920 --> 20:33.560
and measure as a whole, you know, what they're doing.

20:33.560 --> 20:38.000
And so now what we're trying to do is we're trying to develop models where generative models

20:38.000 --> 20:43.840
where we have generative models for each individual cell based on measurements we make on single

20:43.840 --> 20:45.000
cell level.

20:45.000 --> 20:49.960
And then these component generative models essentially inform a very larger generative

20:49.960 --> 20:54.400
model which try to explain what happens when you put them all together, if that makes

20:54.400 --> 20:55.400
sense.

20:55.400 --> 20:58.600
And so we're basically learning these nested generative models to say, okay, what can

20:58.600 --> 21:04.920
we learn about cells in isolation and then can we explain how they work together and why

21:04.920 --> 21:08.760
that's different from just some of the individual components itself.

21:08.760 --> 21:10.600
Wow, there's a lot there.

21:10.600 --> 21:17.880
So these nested generative models, are they end to end trained or are you training them

21:17.880 --> 21:25.000
on a cell by cell basis and then kind of unsombling or aggregating to create the system

21:25.000 --> 21:26.000
level?

21:26.000 --> 21:32.560
We're basically training them end to end by optimizing.

21:32.560 --> 21:40.760
So basically, for example, the component generative models that kind of handle what generating

21:40.760 --> 21:44.880
individual cell types look like, those are influenced both by our data on the individual

21:44.880 --> 21:49.760
cells themselves as well as kind of the cells all together, whereas other components that

21:49.760 --> 21:54.680
take these, whereas the other pieces of the bigger model that take the individual components

21:54.680 --> 22:00.200
and somehow add them together, those are only influenced by the data on kind of the bulk

22:00.200 --> 22:01.760
cells, all of the cells together.

22:01.760 --> 22:06.240
And so we kind of try to optimize them all, we try to optimize the parameters of all

22:06.240 --> 22:09.640
of these parts of the network at the same time.

22:09.640 --> 22:16.440
And so is the starting the underlying data set that you're using to build up these generative

22:16.440 --> 22:17.440
models?

22:17.440 --> 22:23.000
Is it the same type of data that we discussed previously for the domain adaptation piece?

22:23.000 --> 22:29.160
Right, so the parts, the type of data used to train the individual generative models

22:29.160 --> 22:35.240
of each cell type are the same type of data as we just talked about with the domain adaptation.

22:35.240 --> 22:41.840
The type of data that we collect to measure how they're working together is actually from

22:41.840 --> 22:43.000
an older technology.

22:43.000 --> 22:46.880
So kind of genomics technologies that don't work on single cell levels, but they work

22:46.880 --> 22:50.840
on measuring genomics of a collection of cells together.

22:50.840 --> 22:56.960
And so we're kind of mixing data then from a newer generation and in older generation.

22:56.960 --> 23:00.240
What type of gen approach did you use for this?

23:00.240 --> 23:04.520
So we actually, we started out with variational auto encoders actually.

23:04.520 --> 23:08.560
We tried different types of different variants of variational auto coders, it turns out for

23:08.560 --> 23:13.600
this specific problem, in terms of performance, you know, this specific variant didn't seem

23:13.600 --> 23:16.040
to make such a big difference.

23:16.040 --> 23:21.400
So we are trying to, we are also trying more certainly to kind of combine.

23:21.400 --> 23:25.320
So I mean, the number of people have looked at this performance, basically use like deep

23:25.320 --> 23:32.160
graphical models, where again, like the generative models are, you know, they use proper probability

23:32.160 --> 23:35.360
distributions, but they also use neural networks in there as well.

23:35.360 --> 23:43.160
So the variational auto encoder, you started this project using variational auto encoders

23:43.160 --> 23:49.960
for your generative models, but then did you ultimately evolve to using more of a

23:49.960 --> 23:51.960
GAN type of a model?

23:51.960 --> 23:57.280
We actually have tried, we've tried GANs in the past, we found they were a little bit

23:57.280 --> 23:58.640
more difficult to train.

23:58.640 --> 24:04.320
And so part of the issue here, when, you know, part of the issue we have in the field of

24:04.320 --> 24:09.280
computation biology is that we're building these models in part to try to understand

24:09.280 --> 24:13.720
something about biology into these, but also a lot of these kind of problems.

24:13.720 --> 24:16.800
So this, the problem that we're trying to solve with these generative models is what's

24:16.800 --> 24:18.640
called deconvolution.

24:18.640 --> 24:23.600
And this, the problem of deconvolution arises in many different areas of biology as well.

24:23.600 --> 24:29.000
And so part of what we're trying to do is also develop kind of usable software than

24:29.000 --> 24:33.520
Ben Shindtis, Ben Shindtis biologists who may not have a lot of experience training

24:33.520 --> 24:38.000
neural nets, for example, can easily just kind of pick up and apply to their own problems.

24:38.000 --> 24:45.840
And so we found it using, at least, you know, when comparing VAE's versus GANs, the VAE

24:45.840 --> 24:50.440
based models were easier for other people to train and use on their own data, whereas

24:50.440 --> 24:55.520
the GANs seem to be much harder to get to work out of the box.

24:55.520 --> 24:56.520
Got it.

24:56.520 --> 25:01.920
And so I guess from a more practical perspective, we, our current models are based on VAE's

25:01.920 --> 25:05.880
just because it's easier to give to other people to use on their own problems.

25:05.880 --> 25:12.360
And so you've got the domain adaptation piece and the generative models.

25:12.360 --> 25:18.680
Are those, do those then come together as part of this pipeline, or are they just both

25:18.680 --> 25:25.200
tools in your tool bag that you used to study these cells and the conditions that create

25:25.200 --> 25:26.720
them?

25:26.720 --> 25:35.680
So they're basically two related but distinct tools that we're using on similar problems.

25:35.680 --> 25:43.640
So we're basically part of a project called the human cell Atlas, whose goal is in part

25:43.640 --> 25:49.480
to kind of characterize human tissues and organs at the single cell level and characterize

25:49.480 --> 25:52.800
how they look under normal and disease conditions.

25:52.800 --> 25:59.120
And so both of these tools, you know, one which enables comparison of data collected

25:59.120 --> 26:04.720
under different experiments, which is the domain adaptation and the other deconvolution,

26:04.720 --> 26:11.560
which is a tool to help us understand how do, you know, wire human tissues, not just collections

26:11.560 --> 26:13.320
of their individual components.

26:13.320 --> 26:19.520
I sort of see both of these tools as, you know, ways to study this, to study the same problem

26:19.520 --> 26:20.520
essentially.

26:20.520 --> 26:26.320
Are there examples you can share of insights that these tools ultimately led to?

26:26.320 --> 26:32.680
Up to now, we've basically been working mostly on kind of the development aspect of this

26:32.680 --> 26:38.040
tool, so we've been developing it and kind of validating that it works on, you know,

26:38.040 --> 26:39.440
datasets with known ground truth.

26:39.440 --> 26:44.720
And so now we're kind of in the, we're in the process of applying these tools in the

26:44.720 --> 26:45.800
number of different ways.

26:45.800 --> 26:51.520
And so to give an example of, you know, maybe a more practical application, this tool,

26:51.520 --> 26:56.800
we're working, you know, one, in one problem we're looking at, we're trying to understand

26:56.800 --> 27:04.120
how, for example, these malaria parasites, you know, transmit themselves across, you

27:04.120 --> 27:06.840
know, across various populations.

27:06.840 --> 27:12.000
And so, you know, the thing about malaria is that malaria is kind of an interesting parasite

27:12.000 --> 27:18.280
because it has this like life cycle where for single, basically for single parasite,

27:18.280 --> 27:25.000
at the end of this life cycle, it has to make a decision, you know, do I asexually reproduce

27:25.000 --> 27:30.280
or do I sexually reproduce and therefore, you know, which therefore leads to transmission.

27:30.280 --> 27:36.520
And so kind of understanding, you know, if you can get a good grasp of how these malaria

27:36.520 --> 27:41.960
parasites decide when and how to transmit to other hosts, then you could potentially

27:41.960 --> 27:43.840
develop therapies to stop it.

27:43.840 --> 27:49.400
And so that's in this specific problem we were working with some other scientists to,

27:49.400 --> 27:55.840
to basically compare malaria parasites that were undergoing kind of sexual or asexual

27:55.840 --> 28:00.680
reproduction and doing this domain adaptation to then ask the question, you know, at what

28:00.680 --> 28:05.920
point do these two different, do these malaria parasites, you know, going down two different

28:05.920 --> 28:07.720
fates change.

28:07.720 --> 28:13.360
And so by applying this tool, we identified a small set of genes, which now we think

28:13.360 --> 28:19.160
if we kind of delete these genes from the malaria or somehow inhibit them from working,

28:19.160 --> 28:21.640
we might be able to stop their transmission.

28:21.640 --> 28:26.080
But this kind of work is still, the downstream application of these tools is still kind

28:26.080 --> 28:30.240
of something that we're now just getting into now that we've kind of established that

28:30.240 --> 28:31.240
the tools are working.

28:31.240 --> 28:37.240
Well, certainly that the application to malaria could be hugely impactful if we're able

28:37.240 --> 28:44.120
to identify and kind of stop that whatever the sequence is, yeah, the transmission, obviously

28:44.120 --> 28:48.840
the transmission, but whatever the mechanism is that is causing the transmission, whether

28:48.840 --> 28:53.120
there are other things that you covered in your presentation that it would make sense

28:53.120 --> 28:55.560
to jump into.

28:55.560 --> 28:59.520
So I think, you know, one of the things that we briefly touched on that we're really

28:59.520 --> 29:08.280
excited about is kind of the use of the use of machine learning to do what people sometimes

29:08.280 --> 29:10.960
call like multimodal data integration.

29:10.960 --> 29:17.240
And so the idea here is that sort of in biology again, a lot of people including, including,

29:17.240 --> 29:22.480
you know, my research lab, you know, we do a lot of analyses at kind of like the DNA level,

29:22.480 --> 29:24.480
which is kind of very low level.

29:24.480 --> 29:29.760
And we, you know, our goal is always to somehow tie what happens at the low level to, you

29:29.760 --> 29:33.920
know, the big things like, you know, are you going to get the disease or not?

29:33.920 --> 29:39.920
But you know, tying these low level events to these high level things like disease incidents

29:39.920 --> 29:43.360
is sometimes really hard because there's a lot of steps between, you know, things that

29:43.360 --> 29:46.920
happen at DNA level and things that happen at, you know, the whole human level.

29:46.920 --> 29:53.840
So nowadays there's kind of more and more interest in people collecting data from the same kinds

29:53.840 --> 29:59.480
of cells or tissues that both kind of like the DNA level and at the whole cell level and

29:59.480 --> 30:03.240
at the kind of tissue level and then trying to link all of these things together such

30:03.240 --> 30:07.160
that we can instead of just trying to predict, okay, if something happened, if this event

30:07.160 --> 30:10.160
happens at DNA, you know, am I going to get disease?

30:10.160 --> 30:13.600
Now we can try to predict, okay, if something happens at the DNA level, does that somehow

30:13.600 --> 30:18.600
change how my, like, tissues are organized or how my brain is, like, structured and then

30:18.600 --> 30:20.640
how does that then kind of impact the season?

30:20.640 --> 30:26.760
So I think, you know, for example, we're working with some people at the Allendants

30:26.760 --> 30:31.640
too for brain science where they're kind of, they're doing some really cool experiments

30:31.640 --> 30:36.840
where they can take certain types of brain cells and they can use like electrophysiology

30:36.840 --> 30:42.720
or, you know, they can take pictures of the cell to look at their shape and then also

30:42.720 --> 30:45.400
measure what's happening at the genome level.

30:45.400 --> 30:51.880
And so now we're trying to build models which can basically tie together things at the

30:51.880 --> 30:59.240
DNA level, at the kind of electrophysiology level and at the imaging level to try to get

30:59.240 --> 31:04.600
a better understanding of how do events at the DNA change our risk of disease.

31:04.600 --> 31:10.200
And so this is, you know, this is kind of exciting because, you know, now we, you know, there's

31:10.200 --> 31:15.720
obviously tons and tons of work in the computer vision field for doing things like, you know,

31:15.720 --> 31:19.600
doing all things related to image processing and, you know, understanding, understanding

31:19.600 --> 31:20.600
images.

31:20.600 --> 31:24.800
And so now we can, you know, our goal over the next few years is to try to incorporate,

31:24.800 --> 31:30.040
you know, what's happening in, like, computer vision with, you know, our work in sort of genomics

31:30.040 --> 31:34.120
along with, you know, what people have been doing, modeling neuroscience data and try

31:34.120 --> 31:38.560
to put it all together to try to kind of really understand what's happening in biology

31:38.560 --> 31:39.560
and the human cell.

31:39.560 --> 31:44.760
Well, very interesting work, Gerald, thanks so much for taking the time to chat with us

31:44.760 --> 31:45.760
about it.

31:45.760 --> 31:46.760
Yep, definitely.

31:46.760 --> 31:47.760
Thanks.

31:47.760 --> 31:53.320
All right, everyone, that's our show for today.

31:53.320 --> 31:59.880
For more information on any of the shows in our GTC 2019 series, visit twimmaleye.com

31:59.880 --> 32:02.760
slash GTC19.

32:02.760 --> 32:05.560
Thanks again to Dell for sponsoring this series.

32:05.560 --> 32:10.320
Be sure to check them out at dellemc.com slash precision.

32:10.320 --> 32:37.080
As always, thanks so much for listening and catch you next time.

