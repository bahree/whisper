1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:21,920
I'm your host Sam Charington.

3
00:00:21,920 --> 00:00:25,160
Alright everyone, I am on the line with Mateo Kinazi.

4
00:00:25,160 --> 00:00:29,680
Mateo is an associate research scientist at Northeastern University.

5
00:00:29,680 --> 00:00:32,760
Mateo, welcome to the Twimal AI Podcast.

6
00:00:32,760 --> 00:00:34,280
Hi Sam, thank you for having me.

7
00:00:34,280 --> 00:00:36,600
It's great to be part of this show.

8
00:00:36,600 --> 00:00:37,600
Thank you.

9
00:00:37,600 --> 00:00:43,200
I'm looking forward to jumping into our conversation about some work that you've done recently

10
00:00:43,200 --> 00:00:48,480
to apply machine learning in particular word embeddings and word-to-vec to the physics

11
00:00:48,480 --> 00:00:49,480
research space.

12
00:00:49,480 --> 00:00:53,480
In fact, we've covered similar applications.

13
00:00:53,480 --> 00:01:00,280
First recently, the Twimal AI Podcast number 291 took a look at some applications of machine

14
00:01:00,280 --> 00:01:07,080
learning to identify new materials by looking at materials research publications.

15
00:01:07,080 --> 00:01:10,720
But very interested in learning a little bit more about your application.

16
00:01:10,720 --> 00:01:15,920
But before we do that, tell us a little bit about your background and how you came to work

17
00:01:15,920 --> 00:01:19,600
on kind of physics and machine learning.

18
00:01:19,600 --> 00:01:25,400
And you're also doing some work in computational epidemiology as well, if I understand correctly.

19
00:01:25,400 --> 00:01:26,920
Yeah, correct.

20
00:01:26,920 --> 00:01:31,840
So actually, my background is a bit strange in a sense because my former background is

21
00:01:31,840 --> 00:01:33,400
actually in economics.

22
00:01:33,400 --> 00:01:38,560
So I hold a PhD in economics from Santana University in Italy.

23
00:01:38,560 --> 00:01:42,280
But actually, during my career path, I've always worked, let's say, the intersection

24
00:01:42,280 --> 00:01:46,520
between I would say economics, physics, and computer science in a sense because I'm

25
00:01:46,520 --> 00:01:50,520
always being involved in this kind of field of research, which is called complex systems

26
00:01:50,520 --> 00:01:54,120
and applied network science, typically.

27
00:01:54,120 --> 00:01:59,200
And so what I've been working lately actually are essentially two main kind of broad area

28
00:01:59,200 --> 00:02:00,200
of research.

29
00:02:00,200 --> 00:02:04,880
One is, as you mentioned, now computational epidemiology, which means this idea of actually

30
00:02:04,880 --> 00:02:10,320
building these large-scale simulators that can allow us to kind of model the spread of

31
00:02:10,320 --> 00:02:12,400
a disease at the global scale.

32
00:02:12,400 --> 00:02:17,000
So you can imagine, essentially, what we're doing at our lab is kind of have this essentially

33
00:02:17,000 --> 00:02:22,080
complex piece of software that is able to kind of simulate in a realistic manner the evolution

34
00:02:22,080 --> 00:02:24,200
of an epidemic at the global scale.

35
00:02:24,200 --> 00:02:29,800
So imagine, let's say, I've been Zika starting a given point in Brazil and then spreading

36
00:02:29,800 --> 00:02:33,760
it on the world or having, for example, to try to model, let's say, the patterns of

37
00:02:33,760 --> 00:02:37,080
the seasonal flu, or the pandemic flu, let's say, in the United States.

38
00:02:37,080 --> 00:02:39,720
So this is the kind of research we are doing.

39
00:02:39,720 --> 00:02:44,200
And on the other hand, the other, let's say, focus of research was actually to work on

40
00:02:44,200 --> 00:02:46,840
a discipline, which is called science of science.

41
00:02:46,840 --> 00:02:51,840
And the idea there is essentially to kind of look under a microscope of how science works,

42
00:02:51,840 --> 00:02:56,600
meaning, for example, how it evolves over time, how collaborations occurs between different

43
00:02:56,600 --> 00:03:01,360
scientists and between different fields, how scientists speak their research problems,

44
00:03:01,360 --> 00:03:07,200
how they, for example, move across different institutions, how nations develop expertise

45
00:03:07,200 --> 00:03:09,360
in different fields of research and so on.

46
00:03:09,360 --> 00:03:13,200
So those are kind of my main two topics of research at the moment.

47
00:03:13,200 --> 00:03:17,120
How did you get started, in particular, on that science of science?

48
00:03:17,120 --> 00:03:22,040
What led you to start exploring that area in your research?

49
00:03:22,040 --> 00:03:27,120
Well, actually, that kind of comes in a sense from my economic background, because the

50
00:03:27,120 --> 00:03:31,280
reason why I was attracted to economics in the first place was this idea of actually

51
00:03:31,280 --> 00:03:37,080
trying to understand, as human behavior and all the interactions occurs between individuals.

52
00:03:37,080 --> 00:03:41,800
And so translating that to science of science, the idea is that, okay, you have all these

53
00:03:41,800 --> 00:03:46,640
many people working on producing new innovative research, you are trying to understand how

54
00:03:46,640 --> 00:03:51,640
you can, for example, accelerate science and, let's say, redirect funding, so you can

55
00:03:51,640 --> 00:03:55,400
get, let's say, the most out of it, and you can maybe, let's say, promote the specific

56
00:03:55,400 --> 00:04:00,400
set of expertise in specific institutions, or pressing the word, so how will you do that?

57
00:04:00,400 --> 00:04:05,000
And one way, which I approached the problem was actually to first try to understand how

58
00:04:05,000 --> 00:04:08,960
you can actually map a research space in a given field.

59
00:04:08,960 --> 00:04:12,120
In our case, we started actually looking at physics as a work.

60
00:04:12,120 --> 00:04:16,880
Let's say, baseline, if you wish, because I mean, the people with whom I'm working most

61
00:04:16,880 --> 00:04:20,120
of the time are actually people training physics or computer science, so that was kind

62
00:04:20,120 --> 00:04:22,720
of the natural way to go in a sense.

63
00:04:22,720 --> 00:04:25,520
Well, let's dig into this research.

64
00:04:25,520 --> 00:04:34,360
So you start with the goal of mapping the physics community via the research.

65
00:04:34,360 --> 00:04:41,520
Did you define it more specifically than that, or did you set out broadly to see what

66
00:04:41,520 --> 00:04:45,440
you could see by applying machine learning to the space?

67
00:04:45,440 --> 00:04:49,920
No, actually, we are using kind of a process definition given that this kind of, due to

68
00:04:49,920 --> 00:04:52,000
the kind of data we are using.

69
00:04:52,000 --> 00:04:56,800
So in our case, we are using publications from the American Physical Society.

70
00:04:56,800 --> 00:05:00,840
So essentially, we have a data set with all the publications, all the papers that were

71
00:05:00,840 --> 00:05:05,640
published in the journals of the American Physical Society, and we use that as our corpus

72
00:05:05,640 --> 00:05:07,160
in a sense.

73
00:05:07,160 --> 00:05:13,160
And the reason why we started looking at this is because there is, let's say, a very specific

74
00:05:13,160 --> 00:05:18,480
classification that allows you to kind of look at each paper and know if that paper, which

75
00:05:18,480 --> 00:05:22,400
kind of topics are covered in that specific paper, given that they have this classification

76
00:05:22,400 --> 00:05:27,000
that we different essentially cause that tells you, okay, this paper is about nuclear physics

77
00:05:27,000 --> 00:05:32,680
and specifically this subtopic of nuclear physics versus astrophysics or something different.

78
00:05:32,680 --> 00:05:37,680
And so the fact that we were able to kind of map simultaneously publications, authors,

79
00:05:37,680 --> 00:05:42,640
locations through the affiliations of the authors and topics, such topics, that's kind

80
00:05:42,640 --> 00:05:47,320
of what we needed to be able to generate and then infer this knowledge and research space

81
00:05:47,320 --> 00:05:48,640
in physics.

82
00:05:48,640 --> 00:05:57,640
So the technique that you applied is word-to-vec specifically or word embeddings more generally?

83
00:05:57,640 --> 00:06:02,040
The word embeddings, in a sense, that the model that we're actually using is star space,

84
00:06:02,040 --> 00:06:06,880
which was developed by Facebook Research AI Group.

85
00:06:06,880 --> 00:06:11,440
And but is one of, let's say, of the many, in a sense, variations of the word-to-vec approach.

86
00:06:11,440 --> 00:06:16,360
So the idea, in our case, is that we are treating each author as a bag of topics.

87
00:06:16,360 --> 00:06:20,200
So as a bag of research fields in which that author has worked.

88
00:06:20,200 --> 00:06:25,880
And then we use this bag of topics to kind of infer the embeddings for each specific

89
00:06:25,880 --> 00:06:31,200
research sub-area, in a sense, and create this mapping of the overall nor space by essentially

90
00:06:31,200 --> 00:06:36,320
looking at the expertise of authors to guide us on what it means to be, for two topics

91
00:06:36,320 --> 00:06:37,800
to be similar to each other.

92
00:06:37,800 --> 00:06:39,240
So that's the general idea.

93
00:06:39,240 --> 00:06:44,920
So for example, if we, let's say if we go back to the word-to-vec idea there, I mean,

94
00:06:44,920 --> 00:06:50,040
the main assumption is to use this distribution hypothesis idea, then the fact that essentially

95
00:06:50,040 --> 00:06:54,480
you can infer the meaning of a word by looking at the context of that word.

96
00:06:54,480 --> 00:06:58,080
And in our case, let's say the analogy in our case is that we're saying, okay, we actually

97
00:06:58,080 --> 00:07:00,840
observe the publication record of scientists.

98
00:07:00,840 --> 00:07:05,960
So we know what each scientist is able to actually produce in terms of research.

99
00:07:05,960 --> 00:07:11,120
And so we use that as our, let's say, way to infer what is the context of a specific

100
00:07:11,120 --> 00:07:12,120
sub-topic.

101
00:07:12,120 --> 00:07:15,760
So we're saying, okay, given that this author has published, for example, in nuclear physics,

102
00:07:15,760 --> 00:07:20,320
we can look at what other topics that author has published on, and we use that as a context

103
00:07:20,320 --> 00:07:24,600
to kind of predict the embedding for the former, for example.

104
00:07:24,600 --> 00:07:25,600
Okay.

105
00:07:25,600 --> 00:07:30,320
In that sense, does it mean that the approach that you've taken is more of a supervised type

106
00:07:30,320 --> 00:07:36,160
of an approach using the labels that were available in the journals for the different

107
00:07:36,160 --> 00:07:43,360
research than word-to-vec, or would you still consider it unsupervised or semi-supervised

108
00:07:43,360 --> 00:07:45,000
or something like that?

109
00:07:45,000 --> 00:07:49,800
So yeah, actually, I mean, it can be considered if you wish supervised learning in a sense

110
00:07:49,800 --> 00:07:51,720
that we do have the code.

111
00:07:51,720 --> 00:07:56,160
So we do have this Pax code that stands for Physics and Astronomy Classification Scheme,

112
00:07:56,160 --> 00:07:58,160
the data attached to each paper.

113
00:07:58,160 --> 00:07:59,160
Okay.

114
00:07:59,160 --> 00:08:03,120
But actually, the way in which we use them, they are as if they were simple keywords.

115
00:08:03,120 --> 00:08:07,600
In a sense that we are not trying to actually, for example, create an embedding of the author,

116
00:08:07,600 --> 00:08:12,000
given the labels that we have access to the author, we are actually treating each bag of

117
00:08:12,000 --> 00:08:14,960
labels in a sense as a standalone document.

118
00:08:14,960 --> 00:08:18,760
So for example, the idea is that you can, I mean, our idea actually is that you then you

119
00:08:18,760 --> 00:08:25,000
can apply this approach in general to any kind of keyword list that you have associated

120
00:08:25,000 --> 00:08:30,000
to papers, which can be even inferred, for example, semi-automatically from titles or abstracts

121
00:08:30,000 --> 00:08:31,360
of papers.

122
00:08:31,360 --> 00:08:35,880
But the reason why we chose to use this specific data set with this specific taxonomy already

123
00:08:35,880 --> 00:08:40,800
embedding into it was because this will give us a way to actually have a ground truth

124
00:08:40,800 --> 00:08:43,440
baseline to compare our results with.

125
00:08:43,440 --> 00:08:48,240
So the idea is that we know that this taxonomy exists, we know that we have these hierarchical

126
00:08:48,240 --> 00:08:53,280
schemes that tell us that physics is divided, for example, in 10 big sections, and then

127
00:08:53,280 --> 00:08:58,760
each section has many different sub-fields and so on, but we are not using that kind of

128
00:08:58,760 --> 00:09:00,400
information at training time.

129
00:09:00,400 --> 00:09:05,600
So the idea is that once we have our embedding, can we actually see that same, in a sense,

130
00:09:05,600 --> 00:09:07,160
classification scheme exposes?

131
00:09:07,160 --> 00:09:10,320
Can we use that to actually validate what we are going to observe?

132
00:09:10,320 --> 00:09:12,680
And that's why we're using it in that sense.

133
00:09:12,680 --> 00:09:13,680
Got it.

134
00:09:13,680 --> 00:09:19,480
Are you also using the full texts of the research papers or are you only using these keywords

135
00:09:19,480 --> 00:09:21,680
as kind of your document?

136
00:09:21,680 --> 00:09:26,760
No, we are only using these keywords because really our idea was to kind of just, in a sense,

137
00:09:26,760 --> 00:09:31,680
start with a minimal level of information we could get, in a sense, but yes, really

138
00:09:31,680 --> 00:09:35,600
I mean, our goal actually is try to kind of extend it beyond physics, actually, where

139
00:09:35,600 --> 00:09:38,920
we do not have any sense of classification scheme.

140
00:09:38,920 --> 00:09:42,640
And then at that point, yes, we will go towards the direction of using, for example, text

141
00:09:42,640 --> 00:09:47,320
of abstracts or titles to actually infer automatically, in a sense, these keywords.

142
00:09:47,320 --> 00:09:48,320
These keywords.

143
00:09:48,320 --> 00:09:49,320
Okay.

144
00:09:49,320 --> 00:09:54,720
And so with that as the backdrop, what are some of the things that you've been able

145
00:09:54,720 --> 00:10:01,040
to kind of identify within this embedding space that you were able to create?

146
00:10:01,040 --> 00:10:02,040
Yeah.

147
00:10:02,040 --> 00:10:07,920
So what we were able to do essentially was to kind of fingerprint the scientific production

148
00:10:07,920 --> 00:10:12,600
of cities and recover what is, let's say, an effect that is already observed in literature

149
00:10:12,600 --> 00:10:18,840
using different approaches, which is this idea of this so-called principle of relatedness.

150
00:10:18,840 --> 00:10:20,080
Principle of relatedness?

151
00:10:20,080 --> 00:10:21,080
Yes.

152
00:10:21,080 --> 00:10:22,080
That's correct.

153
00:10:22,080 --> 00:10:28,320
And so that's something that comes, let's say, from the economic geography literature

154
00:10:28,320 --> 00:10:30,160
in the first place.

155
00:10:30,160 --> 00:10:33,960
And that's the idea, for example, that if you look, let's say, the production and exports

156
00:10:33,960 --> 00:10:38,160
of the nation, so the country, and you look, let's say, what kind of product category is

157
00:10:38,160 --> 00:10:41,320
the export to their trade partners?

158
00:10:41,320 --> 00:10:46,080
You can kind of predict what kind of, what new categories they are going to be able to

159
00:10:46,080 --> 00:10:51,560
export next by looking at, by first, getting a measure of the relatedness between the

160
00:10:51,560 --> 00:10:53,720
different product categories.

161
00:10:53,720 --> 00:10:57,840
And then by essentially kind of checking where is the overall production, the overall level

162
00:10:57,840 --> 00:11:02,800
of expertise of the country, and then use that to kind of predict what is the next item

163
00:11:02,800 --> 00:11:04,520
you're going to export.

164
00:11:04,520 --> 00:11:09,720
And the idea there is that if you are working in a part of, let's say, this product space

165
00:11:09,720 --> 00:11:14,360
in which we have, let's say, you are able to, you have skills and expertise in different

166
00:11:14,360 --> 00:11:18,280
product categories, and you have a way to measure relatedness within another product category

167
00:11:18,280 --> 00:11:21,120
that will help you kind of predict.

168
00:11:21,120 --> 00:11:23,520
If that's where you're going to go next or not.

169
00:11:23,520 --> 00:11:27,360
And in our case, essentially, this kind of, the analogy in our case is that now at this

170
00:11:27,360 --> 00:11:29,360
point, we're looking at cities.

171
00:11:29,360 --> 00:11:33,800
So we kind of collect all the publications at the geographical level.

172
00:11:33,800 --> 00:11:38,240
And then we use our knowledge space, our research space, build using your buildings to measure

173
00:11:38,240 --> 00:11:43,240
this level of relatedness, and then kind of predict where each city would go next in

174
00:11:43,240 --> 00:11:46,480
terms of their scientific expertise.

175
00:11:46,480 --> 00:11:51,280
So for example, we might look at the publication records in a specific time window, let's say

176
00:11:51,280 --> 00:11:57,440
from 2000 to 2003, they were going to kind of predict using this embedding space, whether

177
00:11:57,440 --> 00:12:02,080
or not the specific city will have a so-called comparative advantage, a revealed comparative

178
00:12:02,080 --> 00:12:08,480
advantage in specific subfielding physics in the next time window, for example.

179
00:12:08,480 --> 00:12:13,360
And presumably, you were able to identify those kinds of trends with embedding space you

180
00:12:13,360 --> 00:12:14,360
created?

181
00:12:14,360 --> 00:12:15,360
Yes, that's correct.

182
00:12:15,360 --> 00:12:22,960
So what are some examples of any examples of cities and research topics come to mind?

183
00:12:22,960 --> 00:12:24,120
Yeah, me.

184
00:12:24,120 --> 00:12:28,560
So what we do essentially is, on one hand, what we do is, for example, look at specific

185
00:12:28,560 --> 00:12:34,480
cities and look at how their activities cluster in this embedded space.

186
00:12:34,480 --> 00:12:38,640
And so, for example, as we show in the paper, we can, let's say, see that cities like

187
00:12:38,640 --> 00:12:45,360
Brassos, for example, they have a clear level of expertise and expert knowledge in nuclear

188
00:12:45,360 --> 00:12:49,360
physics, for example, while you might have, let's say, other cities like Grenoble to

189
00:12:49,360 --> 00:12:52,320
instead specialize more on condensed matter physics.

190
00:12:52,320 --> 00:12:57,720
So this is what you can actually really, really observe by just using this embedding space

191
00:12:57,720 --> 00:13:03,360
and using an effort to kind of represent the interactions between different, between

192
00:13:03,360 --> 00:13:06,800
these different vectors representing the topics.

193
00:13:06,800 --> 00:13:11,360
And then what we show is that we can then use this model to kind of rank the probability

194
00:13:11,360 --> 00:13:13,760
of a given city to enter in a specific field.

195
00:13:13,760 --> 00:13:18,200
And they use that if you want as your classifier that will tell you whether yes or no whether

196
00:13:18,200 --> 00:13:22,040
or not that activity is going to be developed at the next time step.

197
00:13:22,040 --> 00:13:26,160
And for that, we kind of show that let's say, distribution of our predictions is actually

198
00:13:26,160 --> 00:13:31,240
better than a simple random model where, let's say, all these expertise will evolve randomly.

199
00:13:31,240 --> 00:13:39,720
And so is the model primarily good at identifying new research topics that are closely adjacent

200
00:13:39,720 --> 00:13:45,400
to existing research topics as a direction for a given city?

201
00:13:45,400 --> 00:13:51,520
Or is it, yeah, there's some ability to take more leaps if you all.

202
00:13:51,520 --> 00:13:55,720
So that's actually one of the things that are studied in the literature.

203
00:13:55,720 --> 00:14:01,440
And the idea is that when you build this space, you compute a so-called measure of knowledge

204
00:14:01,440 --> 00:14:02,520
density, in a sense.

205
00:14:02,520 --> 00:14:06,880
So imagine that now you have this very, in our case, very complex and dimensional space

206
00:14:06,880 --> 00:14:11,520
where you have different research topics, so different points placed in this and dimensional

207
00:14:11,520 --> 00:14:12,520
space.

208
00:14:12,520 --> 00:14:17,240
And you want to look if there is any specific area of this space where specific entity

209
00:14:17,240 --> 00:14:23,680
in our case, let's say, city has a high density of topics that are active.

210
00:14:23,680 --> 00:14:29,600
So what you show is that actually, that city is more, let's say, likely to be actually

211
00:14:29,600 --> 00:14:32,880
able to develop an expertise around that area.

212
00:14:32,880 --> 00:14:37,360
So in areas in which it actually did the density of points is high.

213
00:14:37,360 --> 00:14:41,720
And like, and that is also what is found in the literature using essentially also different

214
00:14:41,720 --> 00:14:42,720
techniques.

215
00:14:42,720 --> 00:14:44,480
And that is like a pretty robust result.

216
00:14:44,480 --> 00:14:48,600
The only, let's say, exception to that is that, for example, in the case of thing that

217
00:14:48,600 --> 00:14:54,160
was of, I think, economic activities, what they show is that if you are, for example, a

218
00:14:54,160 --> 00:14:58,760
country that is in an intermediate level of development, your more chances of actually

219
00:14:58,760 --> 00:15:01,880
jump to a different part of the space, in a sense.

220
00:15:01,880 --> 00:15:07,560
And there might be ways for you to kind of optimize your trajectory in the space precisely

221
00:15:07,560 --> 00:15:13,960
by kind of knowing what is them up around you and knowing where you have your level of expertise.

222
00:15:13,960 --> 00:15:18,960
You mentioned earlier that you used an algorithm that was developed at Facebook, what was that

223
00:15:18,960 --> 00:15:19,960
algorithm?

224
00:15:19,960 --> 00:15:22,320
The name is star space.

225
00:15:22,320 --> 00:15:23,320
Star space?

226
00:15:23,320 --> 00:15:24,320
Yes.

227
00:15:24,320 --> 00:15:26,320
And can you go into a little bit more detail?

228
00:15:26,320 --> 00:15:29,400
How does it differ from word to vac?

229
00:15:29,400 --> 00:15:30,640
Yeah, sure.

230
00:15:30,640 --> 00:15:34,400
So the main difference that here we actually don't have a neural network.

231
00:15:34,400 --> 00:15:40,680
So the embeddings are not trained as in word to vac as like an even layer in a very simple

232
00:15:40,680 --> 00:15:41,680
neural network.

233
00:15:41,680 --> 00:15:47,880
But in this case, instead, you kind of directly treat the problem as an optimization problem.

234
00:15:47,880 --> 00:15:52,520
So you have these metrics that represent essentially your dictionary.

235
00:15:52,520 --> 00:15:57,520
So in our case, let's say, all the different research topics that we have in our papers.

236
00:15:57,520 --> 00:16:01,680
And then the other dimension will be the actually embedding dimension that you picked.

237
00:16:01,680 --> 00:16:08,080
And then you have a loss function that kind of tries to optimize essentially these embeddings

238
00:16:08,080 --> 00:16:13,200
and optimize these metrics, but essentially allow you to play with essentially what is

239
00:16:13,200 --> 00:16:15,240
to be considered similar to each other.

240
00:16:15,240 --> 00:16:20,880
So essentially, the way the loss function is written is that you have a generator of

241
00:16:20,880 --> 00:16:25,720
positive and negative pairs, which will change depending on the kind of problem that you

242
00:16:25,720 --> 00:16:27,120
are trying to address.

243
00:16:27,120 --> 00:16:32,640
So for example, in our case, let's say we have a list of topics for a researcher.

244
00:16:32,640 --> 00:16:36,640
So the positive pairs will be, for example, the fact that I leave one of these topics

245
00:16:36,640 --> 00:16:38,280
out and that would be my target.

246
00:16:38,280 --> 00:16:40,800
So what I actually want to kind of predict.

247
00:16:40,800 --> 00:16:46,120
And then all the other topics within that pack will actually be something that I can use

248
00:16:46,120 --> 00:16:47,920
to generate these positive pairs.

249
00:16:47,920 --> 00:16:52,600
So these pairs that you have in our case, for example, are very high cosine similarity.

250
00:16:52,600 --> 00:16:57,240
And then on the other side, you have a negative pair generator, which instead kind of throws

251
00:16:57,240 --> 00:17:03,640
randomly other topics, for example, other bag of topics, or you already actually dictionary.

252
00:17:03,640 --> 00:17:07,080
And those are the ones in which you kind of want to stay away from, in a sense.

253
00:17:07,080 --> 00:17:11,560
And that is done in a similar way as it is done, or some implementation work to act using

254
00:17:11,560 --> 00:17:16,200
negative sampling, meaning that you indeed sample randomly what are, let's say, the negative

255
00:17:16,200 --> 00:17:20,720
cases, so that the points in the space from which you want to be far in a sense.

256
00:17:20,720 --> 00:17:25,960
And so all of this essentially is treated as just an optimization problem, which tries

257
00:17:25,960 --> 00:17:28,680
to directly learn these metrics.

258
00:17:28,680 --> 00:17:34,960
So do you think that a neural network-based approach would get you different results

259
00:17:34,960 --> 00:17:35,960
in any significant way?

260
00:17:35,960 --> 00:17:40,480
Is that something that you're interested in or not so much?

261
00:17:40,480 --> 00:17:45,880
Now, actually, I think they will probably give different results in a sense that what

262
00:17:45,880 --> 00:17:52,200
we are not playing with right now is actually this idea of having, in a sense, a time-spend

263
00:17:52,200 --> 00:17:56,480
list of topics, like, let's say, when you use, let's say, word-to-back in a word idea,

264
00:17:56,480 --> 00:18:00,520
is that you have, for example, this kind of rolling window that you define your context,

265
00:18:00,520 --> 00:18:01,520
right?

266
00:18:01,520 --> 00:18:04,880
So you can say, OK, look at two words before or afterwards.

267
00:18:04,880 --> 00:18:08,400
And then depending on whether you're using the continuous bag of words or the skip-gram

268
00:18:08,400 --> 00:18:10,720
approach, one is the target or the other.

269
00:18:10,720 --> 00:18:14,960
But essentially, the idea is that you have this kind of rolling window in your sentence

270
00:18:14,960 --> 00:18:16,640
that speaks up what is the context.

271
00:18:16,640 --> 00:18:21,400
Well, in our case, what we are doing is that we are treating everything as if all the

272
00:18:21,400 --> 00:18:23,600
production of a given scientist is the context.

273
00:18:23,600 --> 00:18:27,480
And there is no idea of, in a sense, temporal distance embedded yet.

274
00:18:27,480 --> 00:18:34,040
So one idea would be to maybe try to apply this algorithm maybe using a word, even just

275
00:18:34,040 --> 00:18:39,320
using a continuous bag of word implementation word-to-back, for example, to adapt this idea

276
00:18:39,320 --> 00:18:41,760
of having an early window.

277
00:18:41,760 --> 00:18:46,400
And the other thing is that, for example, if we're to use word-to-back in this skip-gram

278
00:18:46,400 --> 00:18:49,360
approach, then actually we'll give us something probably maybe that might be different

279
00:18:49,360 --> 00:18:51,280
than what we're observing right now.

280
00:18:51,280 --> 00:18:55,160
So of course, let's say, in the paper, what I show is that actually this method seems

281
00:18:55,160 --> 00:18:58,520
to have better results in the use cases they test.

282
00:18:58,520 --> 00:19:03,840
Then, for example, fast text, word-to-back, and I think also gloves, as far as I remember.

283
00:19:03,840 --> 00:19:06,440
But yes, it's definitely something we are willing to explore.

284
00:19:06,440 --> 00:19:10,680
So our idea is in the action not to try to kind of come up with what would be the best

285
00:19:10,680 --> 00:19:15,520
way of creating these embeddings, giving that we have a very specific problem in mind.

286
00:19:15,520 --> 00:19:19,720
So we are not teaming at creating embeddings that should work for, let's say, whatever

287
00:19:19,720 --> 00:19:24,800
task at hand, but actually the shoework for very specific, to test very specific ideas

288
00:19:24,800 --> 00:19:26,440
in a sense.

289
00:19:26,440 --> 00:19:34,560
Are you incorporating, in any way, some kind of waiting of a given researcher's volume

290
00:19:34,560 --> 00:19:41,080
of work or significance of work in a given area, or is the, their kind of bag of topics

291
00:19:41,080 --> 00:19:44,560
just based on anything that they've been active in?

292
00:19:44,560 --> 00:19:50,800
No, actually, it's based on anything that they've been active in, and that's actually

293
00:19:50,800 --> 00:19:55,080
by design, and that's something we wanted to do like that.

294
00:19:55,080 --> 00:19:59,400
And the reason for that is that, let's say, the other approaches that do not use embeddings

295
00:19:59,400 --> 00:20:03,160
to kind of address the same problem have that kind of filtering.

296
00:20:03,160 --> 00:20:10,000
So have this idea of actually first filtering, for example, what are the topics in which

297
00:20:10,000 --> 00:20:14,920
information or an institution or an entity have what is called a reveal comparative advantage?

298
00:20:14,920 --> 00:20:19,880
So which putting in your words would be something like, okay, identify where that scientist

299
00:20:19,880 --> 00:20:21,200
is most active on.

300
00:20:21,200 --> 00:20:25,680
So remove, for example, all the site papers that have topics that come out maybe of site

301
00:20:25,680 --> 00:20:29,560
projects or feeds the abandoned or something like that.

302
00:20:29,560 --> 00:20:33,280
Well in our case, what I said, no, we actually don't want to embed any of that information

303
00:20:33,280 --> 00:20:34,280
in our embeddings.

304
00:20:34,280 --> 00:20:39,440
So we want to try to see, okay, what if we are applying, we just keep and get a big pile

305
00:20:39,440 --> 00:20:44,160
of papers from an author, we just list all the topics in which has worked on, and train

306
00:20:44,160 --> 00:20:45,160
on that.

307
00:20:45,160 --> 00:20:49,880
So we don't for the filtering without trying to kind of pick the data to get the best

308
00:20:49,880 --> 00:20:50,880
results possible.

309
00:20:50,880 --> 00:20:55,560
Our idea was to show that you don't need to do that to have the same results that you

310
00:20:55,560 --> 00:20:59,360
have with a lot of this manual, let's say, checks and optimizations.

311
00:20:59,360 --> 00:21:05,000
Given that you're focused on a very specific task, what was your performance metric against

312
00:21:05,000 --> 00:21:06,440
that task?

313
00:21:06,440 --> 00:21:11,680
So okay, there's different in a sense checks and validations with it.

314
00:21:11,680 --> 00:21:16,040
So the first validation, which was not in a sense quantified, but was just in a sense

315
00:21:16,040 --> 00:21:21,920
of visual inspection, was what I was saying before, of actually comparing our results from

316
00:21:21,920 --> 00:21:24,760
this box classification scheme.

317
00:21:24,760 --> 00:21:29,360
So the idea is that what we did was that we obtained these embeddings, then we used

318
00:21:29,360 --> 00:21:34,920
them to create actually a research network space.

319
00:21:34,920 --> 00:21:38,600
In the idea, there is that we are borrowing, let's say, techniques and tools from network

320
00:21:38,600 --> 00:21:39,600
science.

321
00:21:39,600 --> 00:21:45,600
And at that point, what we're doing is that we are treating each topic as a node in a network

322
00:21:45,600 --> 00:21:50,960
where the nodes are connected to each other through edges and each hedge as a way has

323
00:21:50,960 --> 00:21:56,680
actually the amount of similarity relatedness to those two topics share, which in our case

324
00:21:56,680 --> 00:22:00,040
would simply be because of similarity of two topics.

325
00:22:00,040 --> 00:22:04,360
Then after building this network, we kind of use what are colleagues spring layouts

326
00:22:04,360 --> 00:22:08,600
in this kind of literature, which essentially the idea, the basic idea is that imagine that

327
00:22:08,600 --> 00:22:13,240
you have these balls into space that are representing your different topics, and you have these

328
00:22:13,240 --> 00:22:15,120
different edges connecting them.

329
00:22:15,120 --> 00:22:20,320
Now, imagine those edges are actually springs that have different force depending on the

330
00:22:20,320 --> 00:22:24,080
level of similarities of those nodes, so those entities.

331
00:22:24,080 --> 00:22:29,520
And so the strong of the similarity, the closer those nodes will be in your visual representation.

332
00:22:29,520 --> 00:22:34,320
And so we use that to build essentially this research network space.

333
00:22:34,320 --> 00:22:38,440
And in a sense, that is our way to kind of make the projection from the embedding space

334
00:22:38,440 --> 00:22:42,000
to a 2D dimensional plot in a sense.

335
00:22:42,000 --> 00:22:46,720
And by doing that, we can kind of clearly identify the 10 macro areas in which we know that

336
00:22:46,720 --> 00:22:52,520
Pax classification divides the different branches of physics.

337
00:22:52,520 --> 00:22:58,240
And that was kind of our first in a sense validation after running the embedding problem,

338
00:22:58,240 --> 00:23:02,520
because we knew that we weren't providing that kind of information to the algorithm.

339
00:23:02,520 --> 00:23:07,400
So we know that that kind of information of the existence of this hierarchical classification

340
00:23:07,400 --> 00:23:10,000
of topics was not used at training time.

341
00:23:10,000 --> 00:23:14,680
So that was our first way to benchmark ourselves and say, okay, look, actually our results

342
00:23:14,680 --> 00:23:16,960
in the look promising.

343
00:23:16,960 --> 00:23:22,680
And the second way in which we tested was actually to indeed measure the predictive power of

344
00:23:22,680 --> 00:23:23,680
the algorithm.

345
00:23:23,680 --> 00:23:29,120
And at that point, what we did was essentially to look at each city at a given point in time.

346
00:23:29,120 --> 00:23:34,520
So in a given time window, list the topics in which that city had a comparative advantage.

347
00:23:34,520 --> 00:23:39,760
So list the topics in which the research topics in which that city was particularly good at

348
00:23:39,760 --> 00:23:42,240
producing new research.

349
00:23:42,240 --> 00:23:46,960
And then try to predict what will happen next as if it was, let's say, a classifier problem

350
00:23:46,960 --> 00:23:51,040
where you have that you have one, see if you're going to develop in that new field and

351
00:23:51,040 --> 00:23:57,720
zero otherwise, and then use a standard metric like rock curve or something like that to

352
00:23:57,720 --> 00:24:02,720
actually kind of get a sense if our model was better than a random model when instead

353
00:24:02,720 --> 00:24:06,160
this evolution of expertise was occurring randomly.

354
00:24:06,160 --> 00:24:10,600
So those are the two ways in which we tested our results.

355
00:24:10,600 --> 00:24:18,520
And did you have any external comparisons available to you in the first of those two cases

356
00:24:18,520 --> 00:24:24,960
you're kind of comparing against common sense, but as you've mentioned previous research

357
00:24:24,960 --> 00:24:30,880
on the economic side, for example, are the results compatible such as you can kind of compare

358
00:24:30,880 --> 00:24:35,360
what you're seeing to what previous papers have shown?

359
00:24:35,360 --> 00:24:36,360
Yes.

360
00:24:36,360 --> 00:24:40,960
So for example, in terms of the kind of, in a sense, so let's say the map of the knowledge

361
00:24:40,960 --> 00:24:45,960
based with finding physics is actually very similar to the maps that are obtained.

362
00:24:45,960 --> 00:24:50,080
For example, by other researchers using different techniques, for example, looking at citation

363
00:24:50,080 --> 00:24:51,320
networks.

364
00:24:51,320 --> 00:24:55,040
So how different fields, subfields cite each other.

365
00:24:55,040 --> 00:24:58,680
And yes, actually, what we have found is that our results are well in line with what

366
00:24:58,680 --> 00:25:03,920
has been found using other methodologies and other approaches and actually even other

367
00:25:03,920 --> 00:25:06,080
data sets as far as I remember.

368
00:25:06,080 --> 00:25:08,520
And so that that was, let's say, confirm in a sense.

369
00:25:08,520 --> 00:25:13,640
So on the physics side, on the, let's say, more, let's say, theoretical side of whether

370
00:25:13,640 --> 00:25:17,560
or not we could like reproduce this principle of relatedness effect, in the dose in that

371
00:25:17,560 --> 00:25:22,640
case, we are getting results that are well in line with what has been found in the literature.

372
00:25:22,640 --> 00:25:23,640
What do you say this going?

373
00:25:23,640 --> 00:25:28,400
What's next in this line of inquiry for you and what do you think it opens up?

374
00:25:28,400 --> 00:25:32,720
Well, first of all, our idea is to actually extend this and this and we were already doing

375
00:25:32,720 --> 00:25:35,720
to other things rather than physics.

376
00:25:35,720 --> 00:25:39,680
So for example, now there is, let's say, a lot of data available to researchers in this

377
00:25:39,680 --> 00:25:40,680
field.

378
00:25:40,680 --> 00:25:45,120
So Google Scholar, Microsoft Academic Graph and other big databases that could allow you

379
00:25:45,120 --> 00:25:50,320
to actually extend this approach in a sense of scale, look at different disciplines.

380
00:25:50,320 --> 00:25:53,600
And then one, and we're already working on that, we already have some preliminary results

381
00:25:53,600 --> 00:25:54,600
on that.

382
00:25:54,600 --> 00:25:58,800
And for example, what we have seen is that by using the same approach in the Microsoft

383
00:25:58,800 --> 00:26:03,600
Academic Graph, which is essentially another of these bi-burematic data sets that kind

384
00:26:03,600 --> 00:26:08,200
of included at that point all the, all the different fields in science.

385
00:26:08,200 --> 00:26:12,280
And for example, using it there, we kind of can replicate fully that it's us we get in

386
00:26:12,280 --> 00:26:15,080
physics to actually all the other fields.

387
00:26:15,080 --> 00:26:22,680
But then actually our goal is to start using this embedding space as our, let's say, additional

388
00:26:22,680 --> 00:26:27,360
tool in our tool kit to actually try to something new.

389
00:26:27,360 --> 00:26:31,680
So for example, you know all these papers that, let's say, for example, crack the evolution

390
00:26:31,680 --> 00:26:34,240
of the semantics of a term over time, for example.

391
00:26:34,240 --> 00:26:38,640
So you can imagine that given that we have this space in which we can embed topics, but potentially

392
00:26:38,640 --> 00:26:43,200
also authors, fields, and so we can kind of try to understand, for example, how authors

393
00:26:43,200 --> 00:26:44,680
move in this space.

394
00:26:44,680 --> 00:26:48,520
Of, for example, can we find the same kind of analogies that we have in the classical,

395
00:26:48,520 --> 00:26:53,960
let's say, examples of King, Queen, man, woman, and so on.

396
00:26:53,960 --> 00:26:56,200
Arigmatics can we do the same things in science, for example.

397
00:26:56,200 --> 00:27:02,000
So can we kind of understand where they might be gaps into the, into science by actually

398
00:27:02,000 --> 00:27:03,360
try to play with these analogies?

399
00:27:03,360 --> 00:27:08,360
So try to see, for example, what happens if you use, or if you apply a specific, sub-fielding

400
00:27:08,360 --> 00:27:14,040
computer science to maybe a field in which that was never considered as kind of technique.

401
00:27:14,040 --> 00:27:18,680
So once that you get a sense of how to navigate this space, then you can try all these kind

402
00:27:18,680 --> 00:27:23,720
of scenarios and thought experiments that maybe can give you more of an intuition of how

403
00:27:23,720 --> 00:27:25,680
science really works underneath.

404
00:27:25,680 --> 00:27:30,080
So what is the actual structure of the knowledge that is kind of keeping everything together?

405
00:27:30,080 --> 00:27:36,640
It strikes me that it's possible that the, you know, the shifts that we see in research

406
00:27:36,640 --> 00:27:42,120
have less to do with changes happening, you know, in, you know, new research in a city,

407
00:27:42,120 --> 00:27:47,920
and more to do with changes in the way we describe our research and these tags that we applied

408
00:27:47,920 --> 00:27:48,920
to them.

409
00:27:48,920 --> 00:27:51,680
And I'm wondering if you've explored that at all.

410
00:27:51,680 --> 00:27:56,280
Knowing a sense of that, I mean, the way I think I understand your point is, is not

411
00:27:56,280 --> 00:28:01,000
a problem for us, so the way in which we define strength in a sense of a city in a specific

412
00:28:01,000 --> 00:28:07,160
topic is by using this concept borrowed from economics of trivial comparative advantage,

413
00:28:07,160 --> 00:28:12,400
which put in a very simple terms is like saying, okay, that your podcast, for example, if

414
00:28:12,400 --> 00:28:17,840
you compare it to other shows, as a comparative advantage in machine learning simply because

415
00:28:17,840 --> 00:28:22,600
they let's say the share of time that you devote to this topic compared to all the other

416
00:28:22,600 --> 00:28:28,760
possible topics that you might consider is larger than the share than the average share

417
00:28:28,760 --> 00:28:31,640
of podcasts that talk about machine learning.

418
00:28:31,640 --> 00:28:32,640
So it's a relative measure.

419
00:28:32,640 --> 00:28:38,960
So it's like saying, okay, imagine, for example, now from T2T plus 1, you have an explosion

420
00:28:38,960 --> 00:28:42,040
of papers using Word2Vac, for example.

421
00:28:42,040 --> 00:28:48,440
So let's say that your share of Word2Vac papers goes from 5% of the publications to 10%.

422
00:28:48,440 --> 00:28:49,440
Okay.

423
00:28:49,440 --> 00:28:53,040
That's not really going to affect the results because what we're going to look is what

424
00:28:53,040 --> 00:28:59,120
is your own, as a country, as a city, share of publication in that topic compared to what

425
00:28:59,120 --> 00:29:00,480
the Word is doing.

426
00:29:00,480 --> 00:29:07,880
So for example, if a 10% of your publications that were in Word2Vac in the Word was 5%, then

427
00:29:07,880 --> 00:29:13,400
you kind of have a value of this trivial comparative advantage that is true, which means that you

428
00:29:13,400 --> 00:29:17,440
are above average on how much you're publishing in that topic.

429
00:29:17,440 --> 00:29:21,520
If at the next time step, let's say that when, let's say, for example, Word2Vac publications

430
00:29:21,520 --> 00:29:25,040
increase, your share of publication doesn't change.

431
00:29:25,040 --> 00:29:31,320
So you still have that actual share of publications in Word2Vac papers is 10%, but now the Word

432
00:29:31,320 --> 00:29:36,440
share is 10% as well, then your kind of strength have decreased.

433
00:29:36,440 --> 00:29:41,600
So the effect of controlling for the actual absolute size of a different topic, sort of different

434
00:29:41,600 --> 00:29:47,640
fieldings, embedding in our definition of how we define, in a sense, strength and weakness

435
00:29:47,640 --> 00:29:51,040
or expertise in different fields of research.

436
00:29:51,040 --> 00:29:55,520
So what we observe is actually indeed a relative measure of how good you are with respect

437
00:29:55,520 --> 00:29:59,480
to all the other players in the arena in a sense.

438
00:29:59,480 --> 00:30:03,160
I think the scenario is trying to point out, and this may be way in the weeds and not

439
00:30:03,160 --> 00:30:06,560
at all irrelevant, but what I was trying to get at is, you know, what if we have this

440
00:30:06,560 --> 00:30:12,920
machine learning podcast and we talk about embeddings all the time, and then the term

441
00:30:12,920 --> 00:30:18,960
Word2Vac suddenly becomes more popular and so we start using that as opposed to another

442
00:30:18,960 --> 00:30:25,520
way of describing the same thing, or the field matures and we get more and more specific.

443
00:30:25,520 --> 00:30:32,560
Your approach, out of necessity, is really concerned with the way we're describing what

444
00:30:32,560 --> 00:30:37,520
we're doing and what we can learn about the way we describe what we're doing as opposed

445
00:30:37,520 --> 00:30:44,840
to what we're actually doing that your research isn't really involved in kind of mapping

446
00:30:44,840 --> 00:30:51,280
that to the cities themselves and trying to project, for example, economic value that

447
00:30:51,280 --> 00:30:59,560
comes out of any of these changes in research areas to validate the model in any way.

448
00:30:59,560 --> 00:31:06,800
So in terms of, let's say we, as we said, maybe we get more specific in like talking

449
00:31:06,800 --> 00:31:11,360
on, let's say, discussable specific topics, so we might kind of switch the way in which

450
00:31:11,360 --> 00:31:12,360
discuss that.

451
00:31:12,360 --> 00:31:19,040
That's actually the whole reason why we introduced the embedding idea, embedding model around

452
00:31:19,040 --> 00:31:25,880
this problem precisely because if your embeddings work well, then that kind of transition

453
00:31:25,880 --> 00:31:30,240
shouldn't really matter because, for example, if now you start from talking, I don't know,

454
00:31:30,240 --> 00:31:35,880
from word-to-back tool, let's say, using only glove, in theory, your, let's say, system

455
00:31:35,880 --> 00:31:39,160
so your, let's say, knowledge space should have already accounted for that.

456
00:31:39,160 --> 00:31:43,200
So it should have already placed, for example, the two vectors very close to each other.

457
00:31:43,200 --> 00:31:50,960
So I don't think that's actually a problem given how we are setting up the whole system,

458
00:31:50,960 --> 00:31:55,200
which is kind of why we decide to go with the embedding approach rather than, for example,

459
00:31:55,200 --> 00:32:00,080
do probability counts or quick current counts as others, what authors have been doing precisely

460
00:32:00,080 --> 00:32:02,520
because of the problem you're saying.

461
00:32:02,520 --> 00:32:06,840
And regarding the economic impact, actually, indeed, that's actually the next step in

462
00:32:06,840 --> 00:32:07,840
the system.

463
00:32:07,840 --> 00:32:13,680
That's actually why we want to kind of extend our approach to different, to different

464
00:32:13,680 --> 00:32:14,680
areas.

465
00:32:14,680 --> 00:32:18,080
For example, because another thing that we have been working on and we can actually,

466
00:32:18,080 --> 00:32:22,480
you can imagine doing is that you don't only have academic publications in the world,

467
00:32:22,480 --> 00:32:24,480
but for example, you also have patents.

468
00:32:24,480 --> 00:32:28,960
So you actually also have recordings of what, for example, cities, if we want to stick

469
00:32:28,960 --> 00:32:33,640
with the geographical dimension, what cities have been publishing both in terms, for example,

470
00:32:33,640 --> 00:32:37,840
of technology, so patent applications and also publications.

471
00:32:37,840 --> 00:32:43,280
And so you can imagine that if you actually are able to build this big knowledge space,

472
00:32:43,280 --> 00:32:47,960
that includes not only knowledge as publications, but actually, for example, technology, then

473
00:32:47,960 --> 00:32:51,600
maybe you're actually able to kind of throw a line between the expertise that you might

474
00:32:51,600 --> 00:32:58,720
be developing in a specific field, research-wise, and the kind of maybe patenting activity might

475
00:32:58,720 --> 00:33:00,760
open up for you in the future.

476
00:33:00,760 --> 00:33:04,160
And then the actually link between the patent and the activity and the actual economic value,

477
00:33:04,160 --> 00:33:07,960
I would say is pretty straightforward, and it's still out of work to do, but I mean,

478
00:33:07,960 --> 00:33:09,720
it's something that any way can be done.

479
00:33:09,720 --> 00:33:15,240
So that's actually the reason why we are kind of trying to refine this approach, because

480
00:33:15,240 --> 00:33:21,000
once you have this space that can tell you essentially how science works and how technology

481
00:33:21,000 --> 00:33:25,080
works as an extension depending if you put those patent applications in or not, then you

482
00:33:25,080 --> 00:33:27,800
are actually able to answer those kind of questions.

483
00:33:27,800 --> 00:33:28,800
Fantastic.

484
00:33:28,800 --> 00:33:32,840
Mateo, thanks so much for taking the time to chat with us about what you're up to.

485
00:33:32,840 --> 00:33:35,120
Thank you for having me.

486
00:33:35,120 --> 00:33:41,080
All right, everyone, that's our show for today.

487
00:33:41,080 --> 00:33:46,880
For more information on today's show, visit twomolai.com slash shows.

488
00:33:46,880 --> 00:33:53,880
As always, thanks so much for listening, and catch you next time.

