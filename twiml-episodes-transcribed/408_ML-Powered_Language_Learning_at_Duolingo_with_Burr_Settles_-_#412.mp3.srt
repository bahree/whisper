1
00:00:00,000 --> 00:00:12,800
All right, everyone. I've got Burr settles here. Burr is research director at Duolingo. Burr,

2
00:00:12,800 --> 00:00:20,080
welcome to the Twomo AI podcast. Thanks for having me, Sam. I am super excited to have an

3
00:00:20,080 --> 00:00:29,200
opportunity to chat with you. I am a bit of an avowed lingua file myself and a user of the Duolingo

4
00:00:29,200 --> 00:00:34,400
app, but it has been a while. I don't know what my points are, my XP's are at this point.

5
00:00:35,680 --> 00:00:42,800
But super excited to chat with you about some of the ways that Duolingo uses AI to deliver the

6
00:00:42,800 --> 00:00:48,000
app. Before we get into that, I'd love to hear a little bit about your personal journey,

7
00:00:48,960 --> 00:00:55,600
how you came to work on machine learning and AI and in language education in particular.

8
00:00:55,600 --> 00:01:04,400
Yeah, it's not a straightforward path, as one might imagine. So my introduction, well, I've always

9
00:01:04,400 --> 00:01:11,360
loved languages. My aunt was bilingual in English and French and worked as a translator. So she

10
00:01:11,360 --> 00:01:18,720
started me on French when I was pretty young. I've always enjoyed languages, never really thought

11
00:01:18,720 --> 00:01:23,840
to study it and certainly knew nothing about natural language processing when I was growing up.

12
00:01:23,840 --> 00:01:28,640
I went to a small liberal arts school where I double majored planned to double major in art,

13
00:01:28,640 --> 00:01:36,400
studio art and math. But then quickly kind of fell into computer science as kind of a mathematical

14
00:01:36,400 --> 00:01:42,320
art form. And then ended up going to grad school, thinking I was going to go into distributed

15
00:01:42,320 --> 00:01:47,280
computing because I thought that was cool at the time. Again, first year, took a machine learning

16
00:01:47,280 --> 00:01:56,880
course, fell in love with that. And then language still wasn't in the natural language processing

17
00:01:56,880 --> 00:02:01,520
wasn't really in the scope just yet. I finished my master's degree at the University of Wisconsin,

18
00:02:02,080 --> 00:02:07,920
and intended to kind of graduate and go get a job at that point. But then I met a girl,

19
00:02:07,920 --> 00:02:12,080
and she still had a year left in her program. And so I stuck around and started just taking

20
00:02:12,080 --> 00:02:20,720
linguistics and biology classes and then fell into a, I started doing research with a professor

21
00:02:20,720 --> 00:02:27,200
in the biostatistics department using machine learning to do information extraction on biomedical

22
00:02:27,200 --> 00:02:33,440
texts. And so for several years, I worked on that. So biomedical natural language processing

23
00:02:33,440 --> 00:02:41,760
was a pretty big thing in the mid-2000s. And during that process, the kinds of annotations,

24
00:02:41,760 --> 00:02:50,160
you need to train biomedical, you know, data sets for those models. It's extremely expensive,

25
00:02:50,160 --> 00:02:54,880
because you have to find people with PhD level knowledge of molecular biology and enough

26
00:02:54,880 --> 00:03:02,080
kind of linguistic sense to know like what semantic role labeling might look like in this domain.

27
00:03:02,800 --> 00:03:08,800
It was very slow and expensive, which then tipped my interest into active learning,

28
00:03:08,800 --> 00:03:14,960
which is machine learning algorithms that participate in their own training. So they can ask questions.

29
00:03:14,960 --> 00:03:20,320
The typical way this is done is you've got a bunch of unlabeled data as we did it in this case.

30
00:03:21,200 --> 00:03:27,520
You have an oracle in this case, a human expert who's very expensive. So you have them annotate

31
00:03:27,520 --> 00:03:33,680
a little bit of data, and then the model learns from that, and then it can quickly analyze all

32
00:03:33,680 --> 00:03:39,680
of the unlabeled data. And then say, this I'm pretty squared away on this and this. I understand

33
00:03:39,680 --> 00:03:44,160
these things, but here's some things that confuse me. Can you please label these for me? And then that

34
00:03:44,160 --> 00:03:50,240
way you can kind of steepen the learning curve or flatten the learning curve depending on how

35
00:03:50,240 --> 00:03:56,640
you're thinking about it. And so I switched my I did my PhD and that kind of work.

36
00:03:56,640 --> 00:04:03,760
And in general was just kind of interested, like that was where I did kind of the language

37
00:04:03,760 --> 00:04:08,960
and the natural language processing. And this interaction between humans and machines.

38
00:04:10,400 --> 00:04:16,560
This this interplay using figuring out how to best use a human resource and training machine

39
00:04:16,560 --> 00:04:23,120
learning systems was something that interested me a lot. Then I came to Carnegie Mellon as a post

40
00:04:23,120 --> 00:04:32,080
stock for a few years to work on this read the read the web project. And then the opportunity

41
00:04:32,080 --> 00:04:36,960
dolingo was spinning out of Carnegie Mellon as a company about the time I post stock wrapped up,

42
00:04:36,960 --> 00:04:43,600
which is almost eight years ago now. And I suddenly realized I was interested in flipping the

43
00:04:44,240 --> 00:04:48,960
flipping the script on that learning. And instead of figuring out how to best use people to

44
00:04:48,960 --> 00:04:54,080
teach machines. How can we best use machines to teach people. And the fact that it was kind of

45
00:04:54,080 --> 00:05:01,680
this intersection of AI computer science building apps. I've never been a pure academic. Like I've

46
00:05:01,680 --> 00:05:07,360
always wanted to build things that like create technology that you then built something with.

47
00:05:08,480 --> 00:05:13,360
And then language and cognitive science. It was just like the perfect perfect combination.

48
00:05:13,360 --> 00:05:20,640
So I've been here ever since. That's awesome. That's great. Are you also a language hobbyist and

49
00:05:20,640 --> 00:05:25,760
user of the app? Where did you kind of settle in on French and kind of push deep in that direction?

50
00:05:25,760 --> 00:05:31,760
I've I've dabbled in many languages. I've used the app to learn a bit of German and Spanish and

51
00:05:31,760 --> 00:05:38,640
Portuguese. But French is the only one I could probably have any sort of conversation.

52
00:05:38,640 --> 00:05:44,400
Yeah, I'm probably similar. Although my French is a lot rustier than I like it to be.

53
00:05:46,240 --> 00:05:53,200
I picked that up in in college. I picked up Spanish was like my high school, you know, high

54
00:05:53,200 --> 00:05:59,200
high school language. And it tends to be generally rusty. But it kind of comes out when it,

55
00:05:59,200 --> 00:06:07,200
you know, needs to. But I've also I'm a dabbler as well. And my typical pattern is to,

56
00:06:07,200 --> 00:06:16,720
you know, spend anything from a month to three months before a trip. Kind of, you know, cramming

57
00:06:16,720 --> 00:06:23,600
a language essentially. And then, you know, go use that knowledge to enjoy the trip and then kind

58
00:06:23,600 --> 00:06:28,160
of flush the cash immediately afterwards because it's really hard to keep that, you know, to get

59
00:06:28,160 --> 00:06:39,040
it to stick. But I've learned a little bit of Portuguese. Do Brasil from Duolingo, a little bit of

60
00:06:39,040 --> 00:06:48,880
Mandarin. What else on Duolingo? A little bit of Spanish, I think, with Duolingo. I think I'm

61
00:06:48,880 --> 00:06:59,200
missing missing one. I've also tried to teach myself a bit of umhark and a bit of Arabic quite a

62
00:06:59,200 --> 00:07:06,160
while ago. But it's it's always great to kind of chat with someone else to share that interest.

63
00:07:07,280 --> 00:07:12,320
Yeah, well, it's pretty amazing to work at a company where everybody shares that. I mean,

64
00:07:12,320 --> 00:07:17,040
there, I think there's only imagine. I think there are 30 different languages spoken

65
00:07:17,040 --> 00:07:24,400
among the employees here. So and most of them all not most. I think there's about 30 different

66
00:07:24,400 --> 00:07:32,400
languages that we teach too. Not the same overlap, but there are enough people who speak a variety

67
00:07:32,400 --> 00:07:37,920
of languages that within the company we have these language tables, like a German stomatish

68
00:07:37,920 --> 00:07:47,040
or top of the Portuguese and you know, people get together. Well, back when it was possible and kosher

69
00:07:47,040 --> 00:07:52,480
to like physically together. Right. Actually, that's extended. I think the Portuguese table.

70
00:07:53,680 --> 00:08:00,000
There's a line. Yeah, just everybody orders take out from like this Portuguese restaurant

71
00:08:00,000 --> 00:08:10,080
and then like thousand to a zoom room. Oh, very cool. But that's awesome. And so I could geek out on

72
00:08:10,960 --> 00:08:15,440
just the language side of this, but I'm not sure that many of the, you know, I have no idea how

73
00:08:15,440 --> 00:08:21,120
many of the listeners would be interested in that conversation. So maybe let's shift gears a

74
00:08:21,120 --> 00:08:32,480
little bit to talking about some of the ways that Duolingo uses AI. And it may not be a surprise

75
00:08:32,480 --> 00:08:38,000
to folks that know the some of the history of the company and like spinning out of Carnegie Mellon,

76
00:08:38,000 --> 00:08:44,640
but there's a bit of machine learning focus from the very beginning to the way the company

77
00:08:44,640 --> 00:08:52,640
approached, you know, this problem of language learning as well as the broader issue of building

78
00:08:52,640 --> 00:09:01,440
a business around it. Yeah, I mean, I was one of the first folks. And let's see, I think I was

79
00:09:01,440 --> 00:09:07,840
number 20 or so. I joined about about six months after it spun out. Okay. And so I started working

80
00:09:07,840 --> 00:09:16,000
on machine learning things from day one. And I think a lot of people, this is changing, but like

81
00:09:16,000 --> 00:09:21,680
three or four years ago, if I'd go to an LP or machine learning conference, people would be kind

82
00:09:21,680 --> 00:09:26,880
of like, how does, you know, Duolingo, how would that use any machine learning? And it's sort of

83
00:09:26,880 --> 00:09:32,320
like surprised people. It doesn't seem to be a surprise anymore, but the way we think about it is

84
00:09:32,320 --> 00:09:42,800
particularly for language education, but I think any kind of education. The best kind of education

85
00:09:42,800 --> 00:09:48,480
you can get is a one-on-one tutor. Okay. And but the problem with that is very few people have

86
00:09:48,480 --> 00:09:53,040
access to a good one-on-one tutor subject matter expert for whatever they're trying to learn.

87
00:09:54,800 --> 00:10:01,680
And so we believe the best way that you can scale that kind of experience is with AI. So it's not

88
00:10:01,680 --> 00:10:07,680
necessarily to replace good teachers or good tutors, but most people don't have access to them

89
00:10:07,680 --> 00:10:16,240
anyway. So by using AI, we can sort of democratize education, sort of. And then if you take that

90
00:10:16,240 --> 00:10:22,560
sort of approach and you think, okay, well, what do good tutors do? I claim that they've got

91
00:10:22,560 --> 00:10:27,040
three, at least three properties, important properties. One is that they know the content

92
00:10:27,040 --> 00:10:32,720
really well. In this case, the language that they're teaching and how different aspects of the

93
00:10:32,720 --> 00:10:40,400
language align to different kind of proficiency levels. Two is they know how to keep you engaged

94
00:10:40,400 --> 00:10:47,360
and excited with the material. And then three, and maybe most importantly, they have a way of

95
00:10:47,360 --> 00:10:52,240
getting inside your head. So because of this one-on-one time that they have with you, they see what you

96
00:10:52,240 --> 00:10:57,520
get right. They see what you get wrong. They see what you used to be getting wrong and you're

97
00:10:57,520 --> 00:11:02,720
starting to get right. They get it for how quickly you forget things. I mean, a lot of educational

98
00:11:02,720 --> 00:11:09,600
technology is focused on assessment or like a shorter term semester long sort of learning,

99
00:11:09,600 --> 00:11:16,800
not lifelong learning. So forgetting usually is not baked into the models. And so anyway,

100
00:11:16,800 --> 00:11:21,680
going back to those three things, the content, keeping you engaged and getting inside your head,

101
00:11:22,400 --> 00:11:28,320
we've essentially arranged our AI kind of research program here around those three things. So we've

102
00:11:28,320 --> 00:11:34,400
got machine learning projects going on in those three areas, broadly speaking. So we've got

103
00:11:34,400 --> 00:11:41,280
projects to help develop efficiently like high quality content that's aligned to a proficiency

104
00:11:41,280 --> 00:11:48,320
scale and then to keep people engaged and then also to model what people know so that we can

105
00:11:48,320 --> 00:11:56,240
personalize their experiences. And the story that I was thinking of and I wondered the

106
00:11:56,240 --> 00:12:03,440
extent to which it continues. It was from years ago, it was kind of this idea that

107
00:12:03,440 --> 00:12:14,880
Duolingo had this, you know, the company has a whole wanted to make an impact on language learning

108
00:12:14,880 --> 00:12:21,360
and the recognition was that, you know, yeah, there are a bunch of, you know, folks in wealthy

109
00:12:21,360 --> 00:12:26,480
countries that could like spend a bunch of time, you know, hobby learning languages, but the

110
00:12:26,480 --> 00:12:34,560
biggest impact on them from a global perspective was helping people at massive scale learn languages

111
00:12:34,560 --> 00:12:42,160
that would increase their economic position, you know, languages like English. And there was,

112
00:12:43,200 --> 00:12:48,720
I forget the specific technique or approach that was taken, but there were some things that were

113
00:12:48,720 --> 00:12:56,560
that were being done early on using machine learning to like ingest articles and identify,

114
00:12:58,240 --> 00:13:03,360
you know, turn those articles into into lessons. Again, kind of at a scale beyond the way

115
00:13:03,360 --> 00:13:09,840
a handcrafted, you know, Portuguese language track might work for the Duolingo that many of us

116
00:13:09,840 --> 00:13:15,120
hear. No, you know, does that, does anybody of that sound familiar? Am I making that up?

117
00:13:15,120 --> 00:13:20,000
You're not making that up. We've kind of pivoted since then. So the original business model

118
00:13:21,120 --> 00:13:29,760
for Duolingo, the idea was that we can give away a free education. So the app is free and it's

119
00:13:29,760 --> 00:13:34,640
still free. Yeah. You can go through an entire course without ever, you know, paying us anything.

120
00:13:36,160 --> 00:13:44,240
And the idea at the time was that as part of doing the lessons, some of the exercises that you did

121
00:13:44,240 --> 00:13:51,600
were translating documents. And a lot of it was Wikipedia articles. For a while, we actually did

122
00:13:51,600 --> 00:13:58,400
have clients like, I think CNN was one of our clients BuzzFeed would give us articles written

123
00:13:58,400 --> 00:14:04,880
in English that they wanted translated into their Latin American properties. And so we had,

124
00:14:04,880 --> 00:14:09,760
you know, tens or hundreds of thousands of Spanish speakers who were learning English on Duolingo

125
00:14:09,760 --> 00:14:14,880
that as part of their exercises were translating documents kind of crowdsourced to translating

126
00:14:14,880 --> 00:14:21,360
documents from English into Spanish reports of ease, which then we sold back to CNN and BuzzFeed

127
00:14:21,360 --> 00:14:27,440
and then they published on their website. That was like the original, you know, business model

128
00:14:27,440 --> 00:14:33,840
idea. It ended up not really to be sustainable. So we've kind of like shut that down and pivoted

129
00:14:33,840 --> 00:14:44,560
it, pivoted a bit. The primary business model now is twofold. One is in, I'm trying to remember

130
00:14:44,560 --> 00:14:54,160
exactly when 2014, 2015 or so, we launched the Duolingo English test, which is a high stakes

131
00:14:54,800 --> 00:15:01,040
English language proficiency exam. And the sort of test that if you're an international

132
00:15:01,040 --> 00:15:06,400
student wanting to study in the United States, you could take as proof of English proficiency.

133
00:15:07,200 --> 00:15:15,440
But the idea again, keeping with our mission of being as accessible as possible, it was a test

134
00:15:15,440 --> 00:15:21,760
that was much less expensive. It was, it's $49 and you can take it online anytime anywhere

135
00:15:22,400 --> 00:15:29,680
and it's a computer adaptive test. And so the idea there is that the education is free and then

136
00:15:29,680 --> 00:15:38,240
to certify what you've learned, that is a, a small fee. So that's part of the, the business model.

137
00:15:38,240 --> 00:15:43,840
And then another part of the business model is a subscription service that is not a paywall

138
00:15:43,840 --> 00:15:50,240
on the educational content. But it does unlock certain gamified, you know, features.

139
00:15:50,240 --> 00:16:00,240
Not things that, you know, make it easier. I actually am not familiar with all the sets of features

140
00:16:00,240 --> 00:16:05,360
in the offering right now, like there's a progress quiz that you can take to sort of like track

141
00:16:05,360 --> 00:16:11,200
your own learning over time. I remember the big thing, I remember the big thing for me being,

142
00:16:11,840 --> 00:16:17,760
I think if you are a pro subscriber, you can download your lessons.

143
00:16:17,760 --> 00:16:21,280
Right, offline. We're not on a plane and stuff like that.

144
00:16:21,280 --> 00:16:28,720
Or the subway. Yeah, we're on a subway. Maybe not so big a deal now, you know, with people

145
00:16:29,680 --> 00:16:35,360
quarantined at home. But when I was actively using the app, that was the one thing that

146
00:16:36,240 --> 00:16:39,040
I think maybe even made me do pro for a little while.

147
00:16:40,080 --> 00:16:46,160
Yeah, in fact, lockdown has been really great for us. And a lot of, you know, most of

148
00:16:46,160 --> 00:16:54,080
kind of like tech app most, all the metrics that we look at have at least doubled with the

149
00:16:54,080 --> 00:16:59,440
Duolingo English test. That is, that's gone up, I think 2000 per cent or something like that. Wow.

150
00:16:59,440 --> 00:17:06,400
All the test centers for all, for the traditional tests are closed. So basically, in many parts of

151
00:17:06,400 --> 00:17:12,240
the world, the Duolingo English test was the only option for English language proficiency testing

152
00:17:12,240 --> 00:17:18,400
for people who were hopeful to study in the United States and Canada. Well, so let's push a

153
00:17:18,400 --> 00:17:25,440
little bit deeper into those three areas. I think the first she mentioned was the content side.

154
00:17:26,880 --> 00:17:34,800
Yeah, so a few examples of things that we're doing there. I mentioned earlier proficiency

155
00:17:34,800 --> 00:17:39,680
standards. So there's something called the Common European Framework of Reference. This is a

156
00:17:39,680 --> 00:17:46,240
descriptive framework of the kinds of things you can do at various levels of proficiency.

157
00:17:46,880 --> 00:17:52,000
So it split up into these six levels. There's an A, B and C and then a high and low for each one

158
00:17:52,000 --> 00:17:57,840
of those. So A1 is super, super beginner, survival English or survival Spanish.

159
00:17:59,280 --> 00:18:08,480
And then as you move up, A2, B1, B2 is kind of the threshold for being like you could study

160
00:18:08,480 --> 00:18:16,720
probably in a or work, get a job in a company or university that, where that language is the

161
00:18:16,720 --> 00:18:21,200
medium. You'll probably still have an accent to make your medical errors and stuff, but you can

162
00:18:22,800 --> 00:18:26,240
converse abstractly in that language. And then the C levels are like

163
00:18:26,240 --> 00:18:39,760
much closer to fluent. And so we needed tools to help align our curricula to a standard like that.

164
00:18:39,760 --> 00:18:48,560
So we picked the CEFR for whatever reason. It was as good as any. And there have been lots of

165
00:18:48,560 --> 00:18:57,040
work building vocabulary profiles. This is an A1 level piece of vocabulary like brother and sister

166
00:18:57,040 --> 00:19:03,200
and Monday and Tuesday. And those are kind of A1 level words, whereas contributory,

167
00:19:03,200 --> 00:19:08,160
well actually it's not a word. Or is it? Cropuscular. That's like a C2 level word.

168
00:19:11,440 --> 00:19:15,760
So lots of work had gone into creating these profiles, but only for English.

169
00:19:15,760 --> 00:19:23,600
And we have our in-house language and curriculum developers had put a lot of work into aligning

170
00:19:23,600 --> 00:19:29,200
our English curricula to the CEFR. But we wanted to then duplicate that for French and Spanish.

171
00:19:29,200 --> 00:19:34,720
And the data resources just didn't exist. So we could train machine learning models to project

172
00:19:34,720 --> 00:19:41,440
arbitrary English words onto this onto a CEFR scale, just treating it like an ordinal regression

173
00:19:41,440 --> 00:19:51,440
problem. But we needed to be able to do that for other languages too. And so we had this idea

174
00:19:51,440 --> 00:19:58,880
of using multilingual word embeddings and language normalized frequencies and things like that,

175
00:19:58,880 --> 00:20:04,480
as features in a model that we can essentially train on English data and then make predictions

176
00:20:04,480 --> 00:20:12,400
on Spanish data or French data or Portuguese data or German data. And so we created that. And

177
00:20:12,400 --> 00:20:20,400
there's a tool. We lovingly called it CEPFR, some people pronounce it as CEPFR.

178
00:20:21,920 --> 00:20:29,440
We actually released this publicly. So if you go to CEFR.duelingo.com, you can explore the English

179
00:20:29,440 --> 00:20:36,080
and Spanish versions of that. Internally, we also support I think half a dozen or more languages.

180
00:20:36,640 --> 00:20:42,400
And our curriculum developers use that. We also have features called Duelingo Stories. We

181
00:20:42,400 --> 00:20:48,160
have Duelingo podcasts in French and Spanish. And we have some other kind of top secret things

182
00:20:48,160 --> 00:20:54,960
that we're experimenting with. And the teams that develop content for that are using those tools

183
00:20:54,960 --> 00:21:00,800
to help make sure that, hey, this is stuff we're targeting toward A1 beginner learners.

184
00:21:00,800 --> 00:21:06,240
This is stuff we're targeting toward B1 intermediate learners. And we use these tools to kind of

185
00:21:06,240 --> 00:21:12,560
check the content. And then also highlight the things that are maybe too advanced or too simple.

186
00:21:12,560 --> 00:21:16,320
And those can be nudged in either direction accordingly.

187
00:21:17,280 --> 00:21:23,520
Interesting. So how do you go about training a multilingual word embedding? Are you doing

188
00:21:23,520 --> 00:21:29,120
that based on kind of bi-directional word pairs in a bunch of languages? Or is it

189
00:21:30,320 --> 00:21:36,080
another approach? Yeah, when we were doing this work, we actually didn't train the embeddings

190
00:21:36,080 --> 00:21:42,960
ourselves. I forgot which embeddings we were using at the time. This was about two or three

191
00:21:42,960 --> 00:21:50,880
years ago. We were first building the CEFR tool. But we used some off-the-shelf embeddings. We used

192
00:21:50,880 --> 00:21:59,760
some frequencies from I think Wikipedia and movie subtitles. And that provided enough information

193
00:21:59,760 --> 00:22:07,920
to get pretty accurate scaleings and projections in English data. And then when we qualitatively

194
00:22:07,920 --> 00:22:14,720
inspected the projections in the predictions and the other languages, the content matter experts

195
00:22:14,720 --> 00:22:24,240
found them useful. And was the idea that you would develop a word list for given

196
00:22:25,920 --> 00:22:33,040
course or level of a course and then feed it into the CEFR tool. And it would give you an

197
00:22:33,040 --> 00:22:40,240
approach. It would try to predict the language level of the person who would be taking that

198
00:22:40,240 --> 00:22:45,200
who would successfully complete that level. Right. So a lot of our courses when we were first

199
00:22:45,200 --> 00:22:52,000
developing them, and when I say first developing them, I'm talking about like seven years ago.

200
00:22:52,000 --> 00:22:58,080
So the Spanish for English course was built by our co-founder and CEO Luis Fanon, who's from

201
00:22:58,080 --> 00:23:05,040
Guatemala and bilingual in English and Spanish, as well as some other Spanish-speaking

202
00:23:05,040 --> 00:23:13,600
engineers that we had very early on. Before the linguists and the people with classroom

203
00:23:13,600 --> 00:23:20,000
experience, for example, in Spanish came in. And before I even started, and I was the one who

204
00:23:20,000 --> 00:23:28,960
kind of introduced the CEFR to the company. So for years, we kind of had this like things were

205
00:23:28,960 --> 00:23:34,720
organized in case your listeners haven't actually used a link of lessons are organized into

206
00:23:34,720 --> 00:23:41,120
these skills, which can be their thematic or, you know, they're, you know, like at the restaurant.

207
00:23:41,120 --> 00:23:47,360
And so you learn a few words about ordering food at the restaurant or animals or family or

208
00:23:48,160 --> 00:23:54,560
or in some cases, you know, they're more grammatical in nature. So you'll learn about the past tense

209
00:23:54,560 --> 00:24:02,880
in this skill. And so we had these arbitrary skills that were just like, here are 25 animals.

210
00:24:02,880 --> 00:24:07,440
And no matter, even though you're a beginner, and you don't really need to know how to say

211
00:24:07,440 --> 00:24:13,360
Pangolin in, you know, we're going to teach it to you anyway just because it was just like this

212
00:24:13,360 --> 00:24:19,760
vocabulary core dump. So in the process of going through and revising the courses over time to

213
00:24:19,760 --> 00:24:27,680
make them aligned to the CEFR, tools like this are the kinds of things that we use to both inspire

214
00:24:27,680 --> 00:24:33,920
and sort of quality control check the content as it's being developed. Yeah.

215
00:24:36,240 --> 00:24:44,000
And are there other ways that ML is used in the content development and programming scheme I'm

216
00:24:44,000 --> 00:24:51,680
thinking of trying to think about, you know, either experiences with the app or issues with the app

217
00:24:51,680 --> 00:25:00,320
or I think one kind of recurring theme is or at least something that would be important

218
00:25:00,320 --> 00:25:08,640
for an app like this is kind of the relevancy of the terms and the constructs that you're

219
00:25:09,520 --> 00:25:14,240
learning. And you kind of alluded to this with the Pangolin

220
00:25:14,240 --> 00:25:22,640
content. You want the sentence that the app is teaching you to be representative of the kind of

221
00:25:22,640 --> 00:25:29,360
language that you see out in the wild. Like I imagine that one could create a tool that would

222
00:25:29,360 --> 00:25:42,000
sanity check a curricula like Duolingo's for, you know, relevancy. I remember a lot of folks

223
00:25:42,000 --> 00:25:50,640
might not know this, but you've got there are very active Duolingo forums and entire communities

224
00:25:50,640 --> 00:25:55,920
around like beta versions of languages and stuff like that that most people don't see in the app.

225
00:25:57,600 --> 00:26:03,040
And you'll see folks talking about how, you know, either complaining about the usage in a

226
00:26:03,040 --> 00:26:11,120
a particular question or phrase or, you know, complaining about the relevancy of, you know,

227
00:26:11,120 --> 00:26:15,200
given phrases and it seems like that's something you can develop a tool that would, you know,

228
00:26:15,200 --> 00:26:21,360
compare against, you know, popular media and determine the relevancy.

229
00:26:22,080 --> 00:26:27,760
Yeah, that's something that we have talked about. We actually haven't started a project using

230
00:26:27,760 --> 00:26:33,760
machine learning to do that. But a very similar adjacent thing is in the questions that we give you

231
00:26:33,760 --> 00:26:39,440
particularly the translation exercises and then also sometimes the transcription when you listen and

232
00:26:39,440 --> 00:26:43,440
try to transcribe what you heard. But anytime you're entering in something in the language,

233
00:26:44,560 --> 00:26:50,160
you might, like you may submit something that you're pretty sure you got right, but you're

234
00:26:50,160 --> 00:26:59,840
graded as wrong. And it's also, it's very difficult to capture all possible, you know, language is

235
00:26:59,840 --> 00:27:04,560
very flexible and expressive. So if you're given a prompt in English that can be translated into

236
00:27:04,560 --> 00:27:10,320
Spanish, you know, there's hundreds of valid, you know, Spanish translations of the sentence.

237
00:27:11,440 --> 00:27:17,040
So we write things essentially like regular expressions to sort of capture all the possible

238
00:27:17,040 --> 00:27:23,040
different ways that you can say it. But, you know, the content matter experts who develop this

239
00:27:23,680 --> 00:27:28,240
might miss some or might forget some of there's one particular synonym or, you know,

240
00:27:28,240 --> 00:27:34,880
idiomatic turn of phrase that they skip. So when you grade it wrong, there's a little report

241
00:27:34,880 --> 00:27:43,280
this button. And if you hit that button, then it goes into a big queue. And we get about,

242
00:27:43,280 --> 00:27:51,040
I think, a half a million of those a week. And so it's impossible for the content developers.

243
00:27:51,040 --> 00:27:58,960
And for some of the smaller courses, like Klingon and, you know, Irish Gaelic, there are, you know,

244
00:27:58,960 --> 00:28:06,320
volunteers who actually contributors who maintain. Yeah, we don't have Dothraki. We do have

245
00:28:06,320 --> 00:28:13,600
Hyvalurian. We do have Hyvalurian, which was actually created by David Peterson Patterson,

246
00:28:13,600 --> 00:28:18,400
the linguist who created those languages for Game of Thrones. He approached us and said,

247
00:28:18,400 --> 00:28:23,760
I love the Olingo. I love the incubator, which is the platform where we crowdsource these languages.

248
00:28:24,640 --> 00:28:30,240
I want to create a Hyvalurian course. So we were like, yeah, have at it.

249
00:28:30,240 --> 00:28:35,920
And there's also like, is there Elvish and a bunch of the languages from Lord of the Rings

250
00:28:35,920 --> 00:28:41,120
are? I don't think we have any Lord of the Rings languages. But people, you know,

251
00:28:41,120 --> 00:28:45,520
criticize us all the time for, you know, you're not teaching. For a long time, we didn't have an

252
00:28:45,520 --> 00:28:50,720
Arabic course. We launched that last summer. So people were kind of like, what is your, you know,

253
00:28:50,720 --> 00:28:54,800
you're teaching Klingon, but you're not teaching Arabic. And part of it is the fact that,

254
00:28:55,680 --> 00:29:00,640
you know, a rabid community approached us and volunteered to create the Klingon course, right? So

255
00:29:00,640 --> 00:29:08,160
um, um, but getting back to the, the reports, uh, when those come in, we have a machine learning

256
00:29:08,160 --> 00:29:14,960
model that will probably help prioritize, uh, which are the ones that are likely to be correct

257
00:29:14,960 --> 00:29:21,760
translations, uh, and this uses a combination of kind of like linguistic distance from the things

258
00:29:21,760 --> 00:29:27,520
we do already accept and what this particular submission was, as well as, you know, who was it

259
00:29:27,520 --> 00:29:33,680
that submitted it? Do they have a track record of submitting, you know, good things? And then, um,

260
00:29:35,360 --> 00:29:42,800
uh, and then also like historical data about what did and didn't get approved. And then,

261
00:29:43,840 --> 00:29:49,040
so we have this tool and it's been extremely important. For a lot of the mature courses,

262
00:29:49,040 --> 00:29:54,320
it's, it's less important, but when a new course, like Arabic, for example, when we first launched it,

263
00:29:54,320 --> 00:30:02,480
um, before Latin and Arabic and Scottish Gaelic, which were the first three courses to launch

264
00:30:02,480 --> 00:30:08,480
that we created after we created this tool, even though we didn't have any training data in those

265
00:30:08,480 --> 00:30:16,400
languages yet, it generalized well to those languages. Previous to that, it took about six months

266
00:30:16,400 --> 00:30:21,920
for a course, a new course to graduate out of beta, which among other things, like one of the key

267
00:30:21,920 --> 00:30:25,600
things there is that the number of reports that comes in is below a certain threshold.

268
00:30:27,280 --> 00:30:33,360
Yeah, the number of reports per session or something. And after, you know, this tool was available

269
00:30:33,360 --> 00:30:38,320
after those courses launched and they graduated from beta in like five weeks as of the last

270
00:30:38,320 --> 00:30:49,760
months. Yeah. Oh, very interesting. Um, and so that's the, the content side. I'm also wondering if

271
00:30:49,760 --> 00:30:58,800
there are applications of, you know, language models, GPT-3 types of models on the, the content

272
00:30:58,800 --> 00:31:03,440
side as well. Is that something you're looking at? Uh, potentially. We're, we're looking at those

273
00:31:03,440 --> 00:31:11,520
sorts of models not so much for content creation, but for some, uh, some features you might be seeing

274
00:31:11,520 --> 00:31:17,600
over the next six months or so that are more, uh, kind of interactive feedback on things.

275
00:31:17,600 --> 00:31:23,120
Uh, so currently a lot of the dualingo experiences, translating or transcribing these kind of

276
00:31:23,120 --> 00:31:31,040
rope things and you're not necessarily producing language, uh, spontaneously. Uh, and so we're working

277
00:31:31,040 --> 00:31:36,560
on some, uh, features that would allow you to do that and, and some language models that can give

278
00:31:36,560 --> 00:31:45,840
you feedback. Interesting. Um, this is maybe the last, I'm trying to remember the buckets was

279
00:31:45,840 --> 00:31:51,360
assessment separate from content? Well, assessment is sort of related to the, what I said at the

280
00:31:51,360 --> 00:31:56,560
beginning, I think was content, uh, you know, keeping you engaged with the material and then getting

281
00:31:56,560 --> 00:32:01,920
inside your head. Yeah. And assessment is related to that last one. So if you think about

282
00:32:03,920 --> 00:32:11,200
a good interactive tutor, they're constantly assessing you as they're teaching you. Yeah. Uh,

283
00:32:11,200 --> 00:32:20,080
and so a good interactive, uh, personalized adaptive language system or learning, uh,

284
00:32:20,080 --> 00:32:25,600
educational system of any platform, which is happening to be language, uh, would be assessing

285
00:32:25,600 --> 00:32:33,040
you as well. Um, so we have, and it's actually very related to active learning, which is the,

286
00:32:33,040 --> 00:32:42,000
what I did my PhD in that we talked about earlier. If you think about, um, uh, let's,

287
00:32:43,600 --> 00:32:49,840
uh, like an active learning system, uh, it can look at a bunch of unlabeled data and say,

288
00:32:49,840 --> 00:32:54,640
this I understand, this I understand, this confuses me, please label this. Uh,

289
00:32:54,640 --> 00:32:59,760
a core layer to that is if, if I'm a machine learning system that is trying to teach you something,

290
00:32:59,760 --> 00:33:07,280
I have a, a mental model of what you know, I think you understand this. I don't think you're ready

291
00:33:07,280 --> 00:33:12,640
for this yet. This is in the zone, uh, a proximal development is, is what it's sometimes called.

292
00:33:13,280 --> 00:33:19,760
Uh, so I'm going to give you, uh, material in this area. So it's right on the margin, you know,

293
00:33:19,760 --> 00:33:24,960
this is something that confuses me about what you know and don't. So it probably confuses you in

294
00:33:24,960 --> 00:33:32,240
terms of what you know and you don't. Um, and so we use those kinds of models both for personalized

295
00:33:32,240 --> 00:33:35,760
learning and for computer adaptive testing in the dual and go of most tests.

296
00:33:37,200 --> 00:33:43,840
And one of the things that has really changed language learning over the past, I don't know,

297
00:33:43,840 --> 00:33:51,520
10 years, let's say, for just the number out there is I think the, um, kind of the mainstreaming

298
00:33:51,520 --> 00:33:58,000
of this whole idea of space repetition and kind of systems, a ton of systems that are based on

299
00:33:58,000 --> 00:34:05,360
space repetition. Basically, uh, some system trying to, you know, learn what you know and focus

300
00:34:05,360 --> 00:34:11,200
your learning effort on the things that you don't know as opposed to, uh, you know, continuing

301
00:34:11,200 --> 00:34:18,400
and reinforce the things that you've probably already committed to memory. Um, and this idea that,

302
00:34:18,400 --> 00:34:22,720
you're describing a active learning kind of takes space repetition, you know, once that

303
00:34:22,720 --> 00:34:27,840
further, because it's building this mental model of what you know and what you probably need to

304
00:34:27,840 --> 00:34:34,720
be refreshed of or on, um, versus not as opposed to, uh, I think the original spaced

305
00:34:34,720 --> 00:34:39,440
repetition models were based on this. The idea of a deck of cards and you would put things in

306
00:34:39,440 --> 00:34:43,520
different places in this deck, uh, and that would dictate how often you see them.

307
00:34:43,520 --> 00:34:51,280
Um, yeah, actually, so, uh, I published a paper in 2016 in the association of computational

308
00:34:51,280 --> 00:34:58,640
linguistics about the space repetition model that we use, uh, uh, at Duolingo. Um, so it, that

309
00:34:58,640 --> 00:35:03,920
was actually one of my first projects back in 2013 when I first joined. So when I first joined,

310
00:35:03,920 --> 00:35:10,080
we had one of these flashcard algorithms that was, you know, designed in the 70s back when you

311
00:35:10,080 --> 00:35:17,440
had physical flashcards. So that's what was running in production to, um, to select the exercises

312
00:35:17,440 --> 00:35:21,840
that you would see when you did a practice. So there were, there are lessons where you're being

313
00:35:21,840 --> 00:35:26,560
introduced to new material in Duolingo and then there are practice sessions, uh, where you're

314
00:35:27,200 --> 00:35:32,880
reviewing a old material that can either be within a particular skill that you did or you can

315
00:35:32,880 --> 00:35:36,800
just like the whole course, everything that you've learned so far, you can practice all of it.

316
00:35:36,800 --> 00:35:46,000
Um, and so the, the, the algorithm that was used in production before I started was called the

317
00:35:46,000 --> 00:35:50,880
lightener system. And there's a few different flavors of it, but, but the one that was in production

318
00:35:50,880 --> 00:35:56,560
is this idea that you've got a different box. And there's the one day box and the two day box

319
00:35:56,560 --> 00:36:01,600
and the four day box and they grow up, go up exponentially in powers of two. Uh, and everything

320
00:36:01,600 --> 00:36:05,120
starts in the one day box. And then when you practice something, if you get it right,

321
00:36:05,120 --> 00:36:10,880
it graduates to the two day box. And then you get the way two days to review it. And then if you

322
00:36:10,880 --> 00:36:14,960
practice it again two days later and you get it right, then it can graduate to the four day box

323
00:36:14,960 --> 00:36:21,120
and, and so on and so forth. And, and that way you can, if you consistently keep doing well,

324
00:36:21,120 --> 00:36:25,840
you can push it off longer and longer. Uh, but then if you get it wrong, then it demotes back to

325
00:36:25,840 --> 00:36:32,240
the previous box. And so it cuts the, the spacing in half. Um, so that's what we had in production.

326
00:36:32,240 --> 00:36:38,080
You know, there was, there was some rationale for it, but I quickly realized that that algorithm is

327
00:36:38,080 --> 00:36:44,640
essentially, uh, two to the power of the number of times you got it right and the number of times

328
00:36:44,640 --> 00:36:53,040
you got it wrong. So you can turn that into, you know, a, a model that is like, number of times

329
00:36:53,040 --> 00:36:57,440
right as a feature with a coefficient of plus one and number of times wrong as a feature with

330
00:36:57,440 --> 00:37:02,960
a coefficient of minus one. And you can learn those plus ones and minus ones from data, rather than

331
00:37:02,960 --> 00:37:08,640
just like picking those kind of arbitrary numbers. Yeah. It turns out the Pimsler method, um,

332
00:37:09,520 --> 00:37:17,200
that he published a schedule, Pimsler published a schedule on 67, I think, uh, that also turns out

333
00:37:17,200 --> 00:37:23,600
to be a special case of this. So we, yeah, so we called it half-life regression because it's

334
00:37:23,600 --> 00:37:31,200
essentially a nonlinear regression on with this kind of inductive bias of this half-life.

335
00:37:31,200 --> 00:37:38,000
It's not exactly a slope, but like decay rate. Um, and, and turns out both the lightener system

336
00:37:38,000 --> 00:37:44,800
and the Pimsler method are special cases of that model. And we were able to fit a model

337
00:37:44,800 --> 00:37:51,280
two data and then put, we ran a controlled AB test putting that into production. Uh, and

338
00:37:51,280 --> 00:37:57,360
and to this day, it's probably one of the most impactful, impactful experiments that we've done.

339
00:37:57,360 --> 00:38:05,840
I think there was a 12% boost in, uh, in retention. And by that, I don't mean mental retention.

340
00:38:05,840 --> 00:38:11,120
I mean, like a user used the actual day and going back the next day. Got it.

341
00:38:11,120 --> 00:38:15,600
Or in the very early days in the first year or two of the company, that's the main metric that

342
00:38:15,600 --> 00:38:22,160
we looked at. Well, it can be really frustrating when you're, you've got some set block of time

343
00:38:22,160 --> 00:38:25,920
that you want to spend, you know, learning your language, whether you're on the bus or train,

344
00:38:25,920 --> 00:38:30,960
or whatever, or, you know, you just block out the time. And if you spend half of that time

345
00:38:30,960 --> 00:38:36,320
reviewing cards that you've already committed to memory, that is terribly frustrating.

346
00:38:36,880 --> 00:38:43,040
Yeah. And, and one thing that we're running into now is like, we really, that was an early kind

347
00:38:43,040 --> 00:38:46,880
of successive mine. And I've gone on to do other things and nobody's really been working on that

348
00:38:46,880 --> 00:38:53,040
for seven years. And, and, and our user base has changed. We have more courses now. We've got

349
00:38:53,920 --> 00:39:00,480
orders of magnitude more users now. Uh, the, the amount of content has changed. So that model that

350
00:39:00,480 --> 00:39:07,680
was in production probably is not ecologically valid anymore. Uh, we do have a research scientist

351
00:39:07,680 --> 00:39:12,800
who this past quarter has started revisiting those. And we've got some promising alternatives,

352
00:39:12,800 --> 00:39:18,240
um, that we're about to start running some experiments to try to improve on. So that it,

353
00:39:18,240 --> 00:39:23,040
it's still the case that some people, you know, hey, this is basics one at the beginning of the

354
00:39:23,040 --> 00:39:27,600
tree. Why is it telling me I need to practice this now? I know that stuff backward and forward. And

355
00:39:27,600 --> 00:39:32,560
so it was just an artifact of the fact that when we first fit the models, there were like,

356
00:39:32,560 --> 00:39:43,200
only 100,000 users and, you know, six courses. Yeah. Wow. Um, so you, you, you made this distinction

357
00:39:43,200 --> 00:39:52,320
between human or learner retention and, uh, the, your SaaS metric of retention. Um, I imagine

358
00:39:52,320 --> 00:39:59,600
though that you are also trying to understand the human learner metrics, you know, to what degree

359
00:39:59,600 --> 00:40:08,080
do you, um, you know, to what degree do you go after that? How difficult is it to go after that?

360
00:40:08,080 --> 00:40:13,440
And is there a role, you know, of machine learning and trying to understand, um, you know,

361
00:40:13,440 --> 00:40:20,240
the user experience, the learner experience? Yeah. That's a very, very important question. Uh,

362
00:40:20,240 --> 00:40:25,360
and it's a very difficult question. So most startups, most companies really, there,

363
00:40:25,360 --> 00:40:31,760
there's different families of, of metrics that you look at. All right. And all companies, of course,

364
00:40:31,760 --> 00:40:37,920
look at revenue. That's one that's super important. Uh, most apps and things also look at

365
00:40:37,920 --> 00:40:43,840
engagement. So how growth, you know, how many people are using it. But we have this additional

366
00:40:43,840 --> 00:40:49,040
family of metrics on learning, uh, that most other companies don't have. And it's easy to kind of

367
00:40:49,040 --> 00:40:53,840
look at what other companies are doing to, to optimize, you know, revenue and engagement.

368
00:40:53,840 --> 00:41:00,400
And there's nobody else to look to to really figure out how to, to measure and improve learning.

369
00:41:01,040 --> 00:41:08,480
And, and one way you could do that is, you know, by looking at which exercises they get

370
00:41:08,480 --> 00:41:12,880
right or wrong and are these exercises that they got wrong six months ago or they get in them

371
00:41:12,880 --> 00:41:18,960
right now. Um, and we do do some of that. And, but it's still very challenging because this,

372
00:41:18,960 --> 00:41:24,160
it's this endogenous reasoning, you know, uh, what you're showing them is sort of priming them on,

373
00:41:24,160 --> 00:41:31,360
on what they're, uh, they're getting right and wrong. But, uh, about a year ago, we created a,

374
00:41:31,360 --> 00:41:39,440
an efficacy team, um, within the company that has started to do some longer term sort of research.

375
00:41:39,440 --> 00:41:45,600
And just a month or two ago, we, we published a white paper. Uh, I think if you go to do

376
00:41:45,600 --> 00:41:52,000
lingo.com slash efficacy, you can find a summary of it and, and actually download the white paper.

377
00:41:52,560 --> 00:41:59,040
Um, the main results were essentially for learners of French and Spanish. So English speakers

378
00:41:59,040 --> 00:42:08,320
learning French and Spanish. Um, there is, uh, at reaching checkpoint five, uh, of the course,

379
00:42:08,320 --> 00:42:16,000
which is probably, uh, I don't know exactly how many skills that is, but it's, I'm going to guess

380
00:42:16,000 --> 00:42:23,760
it's on the order of 20 or 25 skills. Um, so after doing 25 skills and do a lingo, you're performing

381
00:42:23,760 --> 00:42:29,360
at least in terms of listening and reading, uh, at the level of somebody who's finished four or

382
00:42:29,360 --> 00:42:36,880
five college semesters. And so there's some, there's mounting evidence that this kind of personalization

383
00:42:36,880 --> 00:42:43,200
that we're doing, uh, through machine learning, uh, to build good content to, you know, create

384
00:42:43,200 --> 00:42:50,800
engagement and to personalize through getting inside your head, um, is working. And do you also

385
00:42:50,800 --> 00:42:56,560
using the, uh, using the separate tools that we talked about earlier, you should be all also

386
00:42:56,560 --> 00:43:06,640
able to relate that level five knowledge base to, uh, a safer level. Uh, yes, you, you could.

387
00:43:06,640 --> 00:43:12,560
I mean, the problem, the difficulty in doing that is you need to get parallel data to do some

388
00:43:12,560 --> 00:43:18,880
mapping and equating, uh, but that's definitely something you could do. I mean, an analogous, uh,

389
00:43:20,320 --> 00:43:25,440
thing we could maybe switch gears and talk about the doing the duolingo English tests because

390
00:43:25,440 --> 00:43:32,480
this is related, um, you know, it is an assessment. And the idea there was, uh, before we do that,

391
00:43:32,480 --> 00:43:42,720
another question on assessment, uh, in some of the tests, there are, um, if I remember correctly,

392
00:43:42,720 --> 00:43:49,840
there are, uh, assessments where you're speaking into the, into your device. And it is, um,

393
00:43:49,840 --> 00:43:54,880
essentially doing a speech recognition and trying to tell you if you've got that correct. And I'm

394
00:43:54,880 --> 00:44:02,400
curious how, um, you know, machine learning oriented that is or technically, you know, sophisticated

395
00:44:02,400 --> 00:44:11,600
that is, um, I remember, I don't remember when I was trying to learn a tonal language like

396
00:44:11,600 --> 00:44:16,640
Mandarin, like if it was really all that sophisticated at telling one tone from another, I know that

397
00:44:16,640 --> 00:44:21,440
when I got there, no one understood a word that I said. So clearly the tones I was not learning

398
00:44:21,440 --> 00:44:31,200
them all that well, uh, but, um, you know, how, how much ML is going into like the speech side of

399
00:44:31,200 --> 00:44:36,880
things. In the, in the duolingo learning app up until very recently, it was fairly rudimentary

400
00:44:36,880 --> 00:44:44,320
and, and to be honest, we were relying on, uh, like the on app services like the Siri and the

401
00:44:44,320 --> 00:44:52,560
Google, uh, ASR systems and, and also relying on, um, TTS that were third party provided. Uh, this,

402
00:44:52,560 --> 00:44:57,600
this year we've actually hired, uh, more people, uh, and in particular, somebody who used to work

403
00:44:57,600 --> 00:45:04,080
on the Siri team who's starting to beef that up. And so over, uh, where, where, we, we have reached

404
00:45:04,080 --> 00:45:10,720
the point where we've hit a ceiling, uh, of what we can do effectively in the learning app using

405
00:45:10,720 --> 00:45:14,960
third party tools. And now we're starting to build it out, uh, in-house with some promising early

406
00:45:14,960 --> 00:45:19,600
results. So you should see that starting to be pushed out into the app, particularly for the,

407
00:45:20,240 --> 00:45:26,480
the more popular courses like English for Spanish and Portuguese speakers or French and Spanish

408
00:45:26,480 --> 00:45:34,800
for English speakers. Um, uh, but on the, on the duolingo English test side of things, there are also

409
00:45:34,800 --> 00:45:42,560
speaking exercises that you have to do, um, both the sort of thing where you're, you're,

410
00:45:42,560 --> 00:45:47,920
you're given a prompt and you have to say this kind of scripted thing as well as just completely

411
00:45:47,920 --> 00:45:53,760
open ended, uh, exercises like you're given, uh, an image and you just have to talk about that

412
00:45:53,760 --> 00:46:03,360
picture for, uh, you know, two or three minutes, uh, there may be one or two minutes. Um, so for those,

413
00:46:03,360 --> 00:46:10,400
we do have more. That sounds like a really interesting NLP machine learning question answering,

414
00:46:10,400 --> 00:46:17,040
potentially type of, uh, challenge. Yeah, yeah. So it's, it's pretty challenging and another thing

415
00:46:17,040 --> 00:46:23,680
that is very important to us, uh, is to make sure that those models are fair across, you know,

416
00:46:23,680 --> 00:46:30,800
that, that, uh, you know, uh, males and females have different vocal registers. Uh, so if you have

417
00:46:30,800 --> 00:46:36,640
lopsided data in some sort of way, it could, you know, be automatically scoring, uh, one group,

418
00:46:37,200 --> 00:46:41,600
uh, with higher scores than the other group for irrelevant reasons, different accents,

419
00:46:41,600 --> 00:46:47,280
so that the speech recognition that is used as a part of that, uh, scoring algorithm features

420
00:46:47,280 --> 00:46:52,800
are extracted from the ASR. Uh, it might work better for some accents than others and you need to

421
00:46:52,800 --> 00:46:57,760
make sure that it's fair. Uh, so we've put a lot of effort into making sure that happens. We've

422
00:46:57,760 --> 00:47:04,160
done a lot of, uh, you know, what's called differential item functioning in the psychometrics, uh,

423
00:47:04,160 --> 00:47:10,800
literature, uh, make sure that the items are not behaving differently, uh, for different groups.

424
00:47:10,800 --> 00:47:14,880
So those are some of the challenges of building the test, but another challenge is,

425
00:47:14,880 --> 00:47:24,080
uh, your traditional, uh, test, and this is true for like, all kinds of educational high-stakes tests.

426
00:47:24,080 --> 00:47:30,160
They're usually done at a test center, uh, and so that means by definition, there's only a few of

427
00:47:30,160 --> 00:47:34,960
them. They're in certain cities. Uh, there's a finite number of seats that are available. They're

428
00:47:34,960 --> 00:47:39,760
not open every day. More expensive to take and deliver. They're more expensive to take and deliver,

429
00:47:39,760 --> 00:47:44,960
and there's this, and the assumption is because it's in this place, it is, you know,

430
00:47:44,960 --> 00:47:50,240
there are security protocols that are sort of in place. Right. The day off will work usually

431
00:47:50,240 --> 00:47:55,360
because they're, you know, businesses. Right. But if you're, if you're from like,

432
00:47:56,560 --> 00:48:03,920
if you're in the rural Amazon, right, then that means you have to take a 14-hour bus ride into,

433
00:48:03,920 --> 00:48:07,680
I don't know, Sao Paulo or something to take the tests, probably spend the night,

434
00:48:07,680 --> 00:48:11,840
and then if there's a trucker strike that shuts down the highway for a day, the day before you're

435
00:48:11,840 --> 00:48:18,320
going to take your test, you're out of luck. Yeah. And so we, we needed, it was part of our goals in

436
00:48:18,320 --> 00:48:23,520
creating this test that is something that anybody anywhere, you know, could take, uh, as, I mean,

437
00:48:23,520 --> 00:48:31,040
as long as they had an internet connected device. Um, that also means that, you know, one way to

438
00:48:31,040 --> 00:48:40,000
cheat on a traditional kind of test is, uh, you know, you take the test and then you, you somehow,

439
00:48:40,000 --> 00:48:45,840
you know, circulate the items that were in that test online so that in very short order,

440
00:48:45,840 --> 00:48:51,200
anybody, you know, who is taking an administration with them in the next 48 hours has access to

441
00:48:51,200 --> 00:48:57,760
the questions and that can get better scores. Uh, this by definition is a test that is in an

442
00:48:57,760 --> 00:49:05,760
uncontrolled environment. So our strategy in combating that was to make it a computer adaptive test

443
00:49:05,760 --> 00:49:13,600
with a huge number of items. Um, the only way to scale it, like, and the reason that makes it

444
00:49:13,600 --> 00:49:19,440
more secure is when you go in and take the test, like, you're, every time you take it, if you take

445
00:49:19,440 --> 00:49:24,960
the test 10 times, it's going to be a different set of questions every time. I think the item exposure

446
00:49:24,960 --> 00:49:33,360
rate is, uh, less than a half a percent, maybe less than a tenth per percent. Yeah, you would have

447
00:49:33,360 --> 00:49:40,000
to take the test a thousand times to see the same item again on average. Uh, and the way we accomplish

448
00:49:40,000 --> 00:49:46,080
that is through machine learning. So all of, all of the items are automatically generated and then we

449
00:49:46,080 --> 00:49:54,880
use the same, we're very similar techniques to create the, the CEFR line tools. We also then project

450
00:49:54,880 --> 00:50:02,880
the items that we automatically generate onto a CEFR line scale, um, using machine learning.

451
00:50:02,880 --> 00:50:10,080
So this is a B2 level question and this is an A1 level question. Um, and then we can adaptively,

452
00:50:10,080 --> 00:50:14,560
you know, we'll start out giving you a B1 level question. If you do well on that, then we'll jump

453
00:50:14,560 --> 00:50:21,280
up to a B2 or a C2, C1 level question. And if you do not as well on that, you know, we'll back off

454
00:50:21,280 --> 00:50:29,200
to a B2 level question and we can zero in on your language proficiency. And when you're starting

455
00:50:29,200 --> 00:50:37,600
this process and generating, uh, questions or texts, um, what type of generation are we talking

456
00:50:37,600 --> 00:50:44,960
about generation from, you know, straight from the model or generation via selection from in the

457
00:50:44,960 --> 00:50:51,440
wild text that you know are valid and make sense and what all goes into that. So right now it's a lot

458
00:50:51,440 --> 00:50:56,560
of the latter, what you just said. Um, it, it depends on the item type. So there are several

459
00:50:56,560 --> 00:51:01,680
different item types. We actually just published a paper this year in transactions of the association

460
00:51:01,680 --> 00:51:08,240
of computational linguistics laying out the approach to the first version of the test. So that paper

461
00:51:08,240 --> 00:51:14,560
even though it was just published a few months ago is already obsolete. Uh, but, um, but there are

462
00:51:14,560 --> 00:51:20,000
several different item types, uh, that go back decades in the language testing literature that

463
00:51:20,000 --> 00:51:28,480
are things that are easy to produce automatically and to grade automatically. Um, and so we'll, we'll

464
00:51:28,480 --> 00:51:35,360
take texts from naturally occurring sources, authentic texts is what the language assessment folks

465
00:51:35,360 --> 00:51:42,400
would say. Uh, and then one of the items is called the C test where every other word you remove

466
00:51:42,400 --> 00:51:47,920
the second half of the word and then the task is to fill in the missing blanks, uh, which seems

467
00:51:47,920 --> 00:51:53,600
like a really simple task really, really hard to do if you don't know the language. Um, and,

468
00:51:53,600 --> 00:52:02,960
and what we can do then is we have a pretty good model to rank order. This is a, an A2 level text

469
00:52:02,960 --> 00:52:13,120
because the language in it is pretty concrete. Uh, it's, it's pretty, um, what, what's, what's

470
00:52:13,120 --> 00:52:18,880
the word I'm looking for? Um, there's not a lot of abstract ideas. It's like informational. And

471
00:52:18,880 --> 00:52:24,880
then as you move up, uh, the levels that can become more academic and more, not only does the

472
00:52:24,880 --> 00:52:30,320
vocabulary become more sophisticated, but the content, you know, it's trying to argue of viewpoint

473
00:52:30,320 --> 00:52:38,400
or, uh, make some abstract cons, you know, uh, discuss abstract concepts. Uh, so those are the

474
00:52:38,400 --> 00:52:44,640
sorts of things that the ML models pick up on. And we use these authentic text to create the

475
00:52:44,640 --> 00:52:51,600
items and then the ML models to project them onto the scale. And then we use adaptive, kind

476
00:52:51,600 --> 00:53:00,720
of active learning type algorithms to then search through that space, um, to, to, uh, efficiently

477
00:53:00,720 --> 00:53:10,560
figure out where you belong on that scale. Interesting. Um, so that's, uh, kind of content and assessment,

478
00:53:10,560 --> 00:53:17,280
a little bit of getting in your head, um, engagement. Yeah, spend a few minutes on that.

479
00:53:18,080 --> 00:53:26,400
Yeah. Uh, so this is probably where to date we've invested the least, uh, and I'm looking forward to,

480
00:53:26,400 --> 00:53:31,600
you know, uh, growing the team and, and doing more work in this. But a good example of that is,

481
00:53:32,720 --> 00:53:38,160
um, we, we send out these, uh, if you've used the dual and go app, you've probably gotten and

482
00:53:38,160 --> 00:53:42,800
maybe been annoyed by, you know, these push notifications that are practice reminders to remind you,

483
00:53:42,800 --> 00:53:49,360
use the app, you know, or you'll lose your streak or something like that. Um, and so we actually

484
00:53:49,360 --> 00:53:57,200
use, uh, machine learning to determine what message to send you when we, we send you those.

485
00:53:57,200 --> 00:54:02,560
We also actually experimented with using machine learning to figure out when to send those to you.

486
00:54:02,560 --> 00:54:10,400
Not, and we had fairly accurate models, uh, that could figure out, you know, that some people

487
00:54:10,400 --> 00:54:17,360
have different patterns on the weekends versus the weekdays or, uh, or whatnot. It turns out that

488
00:54:17,360 --> 00:54:22,880
those, that those, those timing models, even though they were very accurate, they just didn't

489
00:54:23,520 --> 00:54:28,160
make a difference in terms of the metrics compared to some pretty good heuristics that we'd

490
00:54:28,160 --> 00:54:33,280
developed that were simple heuristics. And so in the end, we actually are not using machine learning

491
00:54:33,280 --> 00:54:37,600
to do the timing because it wasn't worth the technical debt of the machine learning. But we do

492
00:54:37,600 --> 00:54:44,240
use it to pick the content and we actually have a KDD paper this year, um, about that. It's a

493
00:54:44,240 --> 00:54:50,320
banded algorithm that's actually a novel algorithm because there are two things in using banded

494
00:54:50,320 --> 00:54:56,480
algorithms for push notifications, at least the way we do it, that are kind of out of the box.

495
00:54:56,480 --> 00:55:04,160
And one of them is that not every template that we could send you is, is viable at every time.

496
00:55:05,040 --> 00:55:12,000
So we might want to send you a message that says, um, you know, uh, keep, keep your streak

497
00:55:12,560 --> 00:55:16,880
or, or don't forget, uh, your streak, which is like the number of consecutive days that you've

498
00:55:16,880 --> 00:55:21,680
been using Duolingo. But if your streak is one or zero or something, we don't want to send you

499
00:55:21,680 --> 00:55:29,440
that, uh, that particular one. Uh, or another one is like, if you are on the leaderboard,

500
00:55:29,440 --> 00:55:33,840
if you're in a certain position in the leaderboard, we might send you a message about that,

501
00:55:34,480 --> 00:55:40,000
but it's only eligible if you're on a leaderboard. That's right. And so that's messes up the statistics,

502
00:55:40,640 --> 00:55:46,080
a little bit. A universe of, uh, what, how big is the message space? I would have imagined that

503
00:55:46,080 --> 00:55:51,760
that would be relatively small, like half a dozen and machine learning wouldn't be all that interesting

504
00:55:51,760 --> 00:55:58,400
as a way to optimize these messages. No, we have hundreds. And we have hundreds. And they also

505
00:55:58,400 --> 00:56:04,160
are translated into all the different languages. Because remember, we've got, uh, about half of

506
00:56:04,160 --> 00:56:09,840
people using Duolingo are learning English from a variety of other languages from, you know,

507
00:56:09,840 --> 00:56:17,760
like, uh, you know, Arabic to Spanish to Chinese. Um, so, so we have to localize all of those. And

508
00:56:17,760 --> 00:56:25,760
the messages culturally perform differently, uh, you know, for, for different groups. Sure.

509
00:56:26,880 --> 00:56:31,120
And so that's one problem is the eligibility criteria, which kind of screws up the statistics

510
00:56:31,120 --> 00:56:37,680
a little bit. And then the other problem is this novelty effect, where, uh, if we send you the same,

511
00:56:37,680 --> 00:56:43,040
like the bandit algorithm figures out, oh, yeah, time for your Spanish lesson. That's the number one

512
00:56:43,040 --> 00:56:48,960
that we should always send that. Right. Very quickly, you'll burn out on that message. Uh, and so

513
00:56:48,960 --> 00:56:54,080
we've had to introduce kind of this cognitive penalty, uh, which is very related to space

514
00:56:54,080 --> 00:57:00,480
repetition. We actually borrowed a lot of the same work from, um, uh, seven years ago on space

515
00:57:00,480 --> 00:57:06,880
repetition and baked it into this bandit model. And so to us, it seemed like it was a, it was a novel

516
00:57:06,880 --> 00:57:13,040
algorithm. So we submitted a paper in it, uh, presented it, I guess last month. Oh, well. And in

517
00:57:13,040 --> 00:57:21,920
terms of the, um, you know, comparison to relatively simple, heuristic type of an approach,

518
00:57:21,920 --> 00:57:26,080
like, how do you characterize, I imagine you're characterized that in terms of engagement lift. Like

519
00:57:26,080 --> 00:57:34,320
what is that, you know, how significant, uh, is the difference of the, uh, uh,

520
00:57:34,320 --> 00:57:39,760
machine learning algorithms and heuristic approach? Uh, I think it was, uh, don't, you know,

521
00:57:39,760 --> 00:57:45,600
don't send a, a streak message if there's no streak, don't send, uh, you know, the other one,

522
00:57:45,600 --> 00:57:55,600
if that doesn't apply, you know, equally, I believe what we had been doing before. Uh, it wasn't

523
00:57:55,600 --> 00:58:01,520
a heuristic as much as like these are all the things that are eligible at this time. Uh, pick one

524
00:58:01,520 --> 00:58:06,080
at random. That's what we were doing. Yeah. Which gave us, you know, really good kind of like

525
00:58:06,080 --> 00:58:13,760
training data to start with because it was a relatively, uh, representative sample. Um,

526
00:58:15,600 --> 00:58:24,000
but we, in general, you try to, with, uh, in production, kind of industry, machine learning,

527
00:58:24,000 --> 00:58:34,800
use heuristics where possible until, until as long as you can fit it in your head. That's,

528
00:58:34,800 --> 00:58:40,000
that's a good heuristic, you know, heuristic can fit in your head or, or, uh, you don't have to

529
00:58:40,000 --> 00:58:46,960
draw a flow chart for somebody else to understand it. Uh, and if it's doing well, stick with that for

530
00:58:46,960 --> 00:58:52,720
now, uh, if it gets to the point where you can't keep it in your head or you have to draw a flow chart

531
00:58:52,720 --> 00:58:57,840
to explain it to somebody else, that's when you should start using machine learning. Uh,

532
00:58:57,840 --> 00:59:02,640
or when you've iterated on the heuristics to the point where you're just getting diminishing

533
00:59:02,640 --> 00:59:09,120
returns, usually like whatever branches are in that heuristic, turn those into features and

534
00:59:09,120 --> 00:59:14,080
apply machine learning, uh, and you'll usually get a lift. At least that's been our experience.

535
00:59:14,080 --> 00:59:22,960
So in this particular case, um, you know, we hadn't really tried any heuristics beyond, uh, randomly

536
00:59:22,960 --> 00:59:28,160
sampling because none of them made sense that wouldn't become immediately very complicated.

537
00:59:29,040 --> 00:59:33,920
So machine learning seemed like the way to go. Interesting, interesting. Any other things that

538
00:59:33,920 --> 00:59:40,080
you're doing on the engagement front? Uh, that's probably the best example to talk about now,

539
00:59:40,080 --> 00:59:48,560
all right. Yeah. I think we're going on for a while. So I don't know. Yeah, no, that is, uh,

540
00:59:48,560 --> 00:59:55,040
I just popped my, uh, my timer up here. We have been, um, but definitely a fascinating conversation.

541
00:59:55,040 --> 01:00:01,280
And, uh, you know, like I said at the beginning, I could continue on, uh, and definitely, but, um,

542
01:00:01,280 --> 01:00:09,440
um, we, uh, want to be conscious of our listeners, uh, attention span as well. Um, but it has been

543
01:00:10,240 --> 01:00:16,800
great chatting with you about some of the things that Duolingo's doing, uh, to, you know, help people

544
01:00:16,800 --> 01:00:22,160
learn languages using machine learning and AI. Well, thanks for having me, Sam. I'm a fan of the

545
01:00:22,160 --> 01:00:34,320
podcast. So it's pleasure to be here. Awesome. Thanks so much for.

