1
00:00:00,000 --> 00:00:16,000
Hey, everyone. I am here with Jonathan LaRou. Jonathan is a senior principal research scientist

2
00:00:16,000 --> 00:00:21,920
and Mitsubishi Electric Research Laboratories on Merle. Jonathan, welcome to the Twimal AI

3
00:00:21,920 --> 00:00:25,040
podcast. Hi, thanks for having me.

4
00:00:25,040 --> 00:00:33,960
So we'll be digging into your work in the speech and audio field broadly. And to get us

5
00:00:33,960 --> 00:00:38,680
cut up, I'd love to have you share a little bit about your background and how you came to

6
00:00:38,680 --> 00:00:41,200
work on applying ML to those problems.

7
00:00:41,200 --> 00:00:47,580
Sure, yeah. So my background actually started with mathematics. I studied math, the pure

8
00:00:47,580 --> 00:00:55,420
math for a while until my master's and then did a master's with Cedric Villaini, who later

9
00:00:55,420 --> 00:01:01,940
got the fields metal. And I was supposed to do my PhD with him. But I wanted to do a gap

10
00:01:01,940 --> 00:01:07,060
here in China. So I need to, because I love, I studied Chinese and I like languages.

11
00:01:07,060 --> 00:01:11,020
So after spending a year in China and I got back, he said, I'm going to Stanford. He

12
00:01:11,020 --> 00:01:14,780
met as well as spending over here somewhere else. So I decided to go to Japan because I met

13
00:01:14,780 --> 00:01:21,140
friends there in China, Japanese friends there. And I started studying Japanese and then

14
00:01:21,140 --> 00:01:28,460
I realized that maybe math, pure math wasn't actually so much my thing. And I fought, I looked

15
00:01:28,460 --> 00:01:34,460
around what I could do and I thought that math and language is kind of mixed well. And

16
00:01:34,460 --> 00:01:40,180
I also, I was really into music and, you know, I said playing with in a band with friends

17
00:01:40,180 --> 00:01:44,940
and I thought like kind of the intersection of all his interests was the speech and audio

18
00:01:44,940 --> 00:01:50,300
field. And I got introduced to a professor at University of Tokyo, who was in there.

19
00:01:50,300 --> 00:01:56,060
And so I was able to do my PhD with him and with a professor in Paris. So that's kind of

20
00:01:56,060 --> 00:02:00,500
how I got into the field of speech and audio. And I'm really happy I did.

21
00:02:00,500 --> 00:02:06,180
That's awesome. That's awesome. And a lot of your work is focused on this classical problem

22
00:02:06,180 --> 00:02:12,700
in the field called the cocktail party problem. Can you tell us a little bit about the background

23
00:02:12,700 --> 00:02:18,420
of that problem and, you know, some of the ways that you've kind of built your research

24
00:02:18,420 --> 00:02:19,900
program around it?

25
00:02:19,900 --> 00:02:27,740
Sure. So yeah, I worked and my core of my work is centered around a social separation, audio

26
00:02:27,740 --> 00:02:34,020
social separation. And in the field, the kind of the holy grail is the so-called cocktail

27
00:02:34,020 --> 00:02:41,220
party problem, which was kind of stated in that name by Cherry in 1953. And it's kind

28
00:02:41,220 --> 00:02:46,500
of, if you imagine yourself in a cocktail party and we humans have this amazing ability

29
00:02:46,500 --> 00:02:52,980
that we are able to entertain a conversation and kind of to focus on pretty much any given

30
00:02:52,980 --> 00:02:58,260
sort of sound that we wanted to focus on. And despite all the background noises and the

31
00:02:58,260 --> 00:03:03,580
reverberation and all these interferences that are happening in this very cluttered

32
00:03:03,580 --> 00:03:12,460
acoustic scene. And so what we've been interested in in my group over the years has been to

33
00:03:12,460 --> 00:03:20,340
kind of tackle this problem, to chop at it. And it's first by working on speech enhancement,

34
00:03:20,340 --> 00:03:28,340
which is the separation of speech from noise. And then later on we worked on speech separation,

35
00:03:28,340 --> 00:03:37,420
which is separation of speech from speech. And this was kind of a big milestone because speech

36
00:03:37,420 --> 00:03:40,860
and noise have very different characteristics. And so you can kind of use this at your

37
00:03:40,860 --> 00:03:45,100
advantage to separate them from each other. You can use machine learning to learn these

38
00:03:45,100 --> 00:03:51,900
characteristics and then kind of classify one and one from the other. When it comes to

39
00:03:51,900 --> 00:03:58,180
speech from speech, then it wasn't clear really how you would handle this. I mean,

40
00:03:58,180 --> 00:04:04,980
because they have, by definition, the same kind of characteristics. So the main challenge was

41
00:04:04,980 --> 00:04:12,100
how to deal with that problem. And so that's kind of where we started developing methods to do

42
00:04:12,100 --> 00:04:20,580
this efficiently using deep learning. Nice. And is the ability to separate speech from noise or

43
00:04:20,580 --> 00:04:29,300
speech is that a necessary prerequisite for a machine to be able to attend to a particular speech

44
00:04:29,300 --> 00:04:36,100
signal strikes me that we don't necessarily separate we attend to. It's a good question. I mean,

45
00:04:36,100 --> 00:04:41,540
it's actually a very good question, whether we humans actually effectively separate in the brain or

46
00:04:41,540 --> 00:04:51,300
not. And I think there is some some hints that we are actually to some extent separating the

47
00:04:51,300 --> 00:04:56,820
signals. Okay. And maybe we're not separating all the signals. It's more like an attention mechanism

48
00:04:56,820 --> 00:05:02,020
where, you know, like the higher part of the brain kind of guide the lower parts of the brain to

49
00:05:02,820 --> 00:05:08,660
decide which features which parts of the sound to focus on and to cancel other ones. So we're not

50
00:05:08,660 --> 00:05:15,060
completely separating a scene as a machine maybe potentially could do. But there is some hints

51
00:05:15,060 --> 00:05:22,020
that in neuroscience that's kind of kind of part of the process. It's not necessary. You're right.

52
00:05:22,020 --> 00:05:28,740
Like you could totally design a speech recognition or somebody's speech recognition algorithm that

53
00:05:28,740 --> 00:05:35,140
does not explicitly separate before recognizing noisy speech or even multi speaker speech. And

54
00:05:35,140 --> 00:05:40,260
that's actually something that we did do. And we're not the only ones. So we,

55
00:05:42,420 --> 00:05:48,020
basically, even nowadays, I would say the state of the art in like kind of single channel,

56
00:05:48,740 --> 00:05:54,580
noisy speech recognition does not necessarily involve enhancement. It's sometimes surprising.

57
00:05:54,580 --> 00:06:01,380
Even though it sounds much, much better, it's often a better strategy to just include more noise

58
00:06:01,380 --> 00:06:06,980
in your training data and to let the neural network that does the transcription, you know,

59
00:06:07,860 --> 00:06:12,740
be robust to noise inherently by the version of training instead of like, you know,

60
00:06:13,460 --> 00:06:17,940
thinking that you're doing the network of favor by removing the noise when you're actually

61
00:06:17,940 --> 00:06:24,740
probably introducing some artifacts that are bothering the system. So it's not always the best

62
00:06:24,740 --> 00:06:31,380
strategy. But it, I mean, going forward, I think as the enhancement algorithms get better and

63
00:06:31,380 --> 00:06:37,700
better, it's likely that it will become probably probably the state of the art to include some form

64
00:06:37,700 --> 00:06:43,940
of enhancement. Got it. Got it. Maybe a good way to talk through your work in the field is to start

65
00:06:43,940 --> 00:06:52,660
with a recent paper. And as you kind of describe what you're doing there, you can talk through some

66
00:06:52,660 --> 00:07:00,500
of the work that's led up to it. And so that recent paper is a new formulation of the cocktail

67
00:07:00,500 --> 00:07:06,180
party problem that you call the cocktail fork problem. What does that mean? Yeah, so that's,

68
00:07:06,180 --> 00:07:13,620
we tried to be a little bit cute there. And so basically this project started with myself,

69
00:07:13,620 --> 00:07:18,500
not being an English native speaker and trying to listen to movies and not always being able to

70
00:07:18,500 --> 00:07:24,020
catch a dialogue with all the special effects and the music. And I was like, and I really like

71
00:07:24,020 --> 00:07:29,380
to be able to just, you know, separate the dialogue and enhance it. And some some smart TVs,

72
00:07:29,380 --> 00:07:33,860
you know, claim they can do this and they can do some extent, but they mostly rely on, you know,

73
00:07:33,860 --> 00:07:41,140
some form of equalization. They don't actually separate the sources. And so we thought, well,

74
00:07:41,140 --> 00:07:45,860
you know, we have all these separation technology, they can separate speech and speech. And now

75
00:07:45,860 --> 00:07:52,900
nowadays we also can separate, you know, multiple types of sounds from each other, like for like a car,

76
00:07:52,900 --> 00:07:59,940
from a, from a belt, for example. And it was kind of a natural thing to try to see, well, can we

77
00:07:59,940 --> 00:08:06,580
just, you know, formulate this problem as to separating complex acoustic scene into like speech,

78
00:08:06,580 --> 00:08:13,540
music and sound effects or sound events. And that's basically what we did. Like we,

79
00:08:13,540 --> 00:08:21,860
it's like carefully crafted a dataset that of that uses these free types of sources and try to

80
00:08:21,860 --> 00:08:30,820
replicate with as much realism as we could the mixing process of like real soundtracks, like movie

81
00:08:30,820 --> 00:08:36,420
or TV show soundtracks. So in terms of loudness or in terms of amount of overlap. And to be

82
00:08:36,420 --> 00:08:43,380
able to training set basically. And then we trained our kind of somewhat standard algorithms

83
00:08:45,060 --> 00:08:50,100
for separation. And we treat them a little bit in order to account for the specificity of a task.

84
00:08:51,300 --> 00:08:58,100
So that we called it the cocktail fork problem because there's free outputs. And cocktail

85
00:08:58,100 --> 00:09:06,340
forks happen to have three outputs, free branches. So if it wasn't a nice hint at the cocktail party

86
00:09:06,340 --> 00:09:10,020
problem to, to call it the cocktail fork problem because it's a, it's a tiny, it's a tiny,

87
00:09:10,020 --> 00:09:19,300
easy version of the cocktail party problem. And so the, given that you started with a

88
00:09:19,300 --> 00:09:29,860
kind of an inorganic training set, you, you created your training set from his component pieces,

89
00:09:30,500 --> 00:09:34,820
you know, thus bypassing one of the, you know, might otherwise be the hardest part of,

90
00:09:34,820 --> 00:09:42,980
of training a model here. I imagine that the, on the back end, the generalization was a big

91
00:09:42,980 --> 00:09:49,540
challenge in building the model. It turned out that it actually generalizes pretty well. So we,

92
00:09:50,420 --> 00:09:55,540
the demos that we put online, we're trained on, only on this synthetic data set. And, and we,

93
00:09:55,540 --> 00:10:01,060
we, we show examples that we got just from taking YouTube videos like movie trailers or TV shows

94
00:10:01,060 --> 00:10:07,780
and applying the network pretty much as, and exactly as it was. And it, it, it does make mistakes.

95
00:10:07,780 --> 00:10:12,820
But it does generalize pretty well. And actually an interesting thing is, at first, we,

96
00:10:13,540 --> 00:10:18,180
we trained the network at 16 kilohertz something rate. So we, we downsample the signals. So,

97
00:10:18,900 --> 00:10:25,860
because it was lighter weight easier and faster to train and to try things. And interestingly,

98
00:10:25,860 --> 00:10:33,540
we, we first used a wrong version of the speech data set, but was downsampled with a slightly

99
00:10:33,540 --> 00:10:38,740
too harsh band pass filter, like low pass filter. And it was, so it was leading out a bit of,

100
00:10:38,740 --> 00:10:45,380
of frequency, of the higher frequencies of speech. And when we applied it to, to these real videos,

101
00:10:45,380 --> 00:10:49,060
we, there was a lot of bleed from the speed, the high frequencies of speech, a very

102
00:10:49,060 --> 00:10:54,180
squeaky speech in the other sources like, and we're like, what, what's going on? And, and actually,

103
00:10:54,180 --> 00:10:59,380
we realized that actually we, the version of the speech data we had was not full band. And,

104
00:10:59,380 --> 00:11:03,700
and when we, we trained our models on, on a better quality data that it just went away.

105
00:11:04,740 --> 00:11:08,500
So kind of interesting that like, if you, if you listened to it, you wouldn't notice,

106
00:11:09,300 --> 00:11:14,820
but the network definitely was not able to learn how to separate the high frequencies for speech.

107
00:11:15,460 --> 00:11:21,220
Oh, wow. It did the model. Is the model language agnostic?

108
00:11:22,420 --> 00:11:25,940
So we only trained it on English, but we found that these,

109
00:11:25,940 --> 00:11:32,180
these speech separation models are pretty language agnostic. A few years back in 2017,

110
00:11:32,180 --> 00:11:40,660
we had a, we did a, a demo of speech separation. And that was trained on 30 hours of English,

111
00:11:40,660 --> 00:11:47,860
you know, world-sweet journal, red sentences speech. And we were, this was a live demo in Japan.

112
00:11:47,860 --> 00:11:54,340
And, and we were separating, you know, English from Japanese for, and Japanese from French,

113
00:11:54,340 --> 00:11:59,700
right? Like, people who visited the demo tried every crazy combination. And it worked pretty well.

114
00:11:59,700 --> 00:12:03,140
I mean, it's really, it's not picking up on the language content. It's really the acoustics.

115
00:12:04,180 --> 00:12:11,460
And when it's, when it's separating, how clean is the separation of the language? If you,

116
00:12:11,460 --> 00:12:16,260
you listen to the separated language, do you hear artifacts of whatever might be going on in

117
00:12:16,260 --> 00:12:22,500
the background or... So it depends how you trained. Yeah, back in the back then, we only used

118
00:12:22,500 --> 00:12:30,020
clean speech for training. And actually, it is still a challenge in the field to, to expand,

119
00:12:30,020 --> 00:12:36,580
like to extend these methods to, to work well on noisy and especially, especially in reverberant

120
00:12:36,580 --> 00:12:44,020
conditions. So like the, the data set that we put out in 2016, when we first came up with this

121
00:12:44,020 --> 00:12:48,180
method called deep clustering, that kind of, kind of revived the field of speech separation. Because

122
00:12:48,180 --> 00:12:52,660
the first one that uses deep, use deep learning, you know, way that it could separate unknown

123
00:12:52,660 --> 00:12:57,380
speakers. You didn't have to train on a specific pair of speakers. It could be anybody. And

124
00:12:58,340 --> 00:13:03,380
we created a data set to do this based on the Warsaw Journal. And this is still used to this day,

125
00:13:03,380 --> 00:13:11,060
but it's kind of completely beaten up, like the performance on that clean data set has become

126
00:13:11,060 --> 00:13:18,500
so amazing that it's, it's pointless. It's like super clean over 20 dB. And so there's no,

127
00:13:19,380 --> 00:13:25,540
I think we solved that task, not we, but the field. The community has solved that task. And

128
00:13:25,540 --> 00:13:31,140
the challenge now is to moving to more challenging scenarios with noise and especially with reverberation.

129
00:13:31,140 --> 00:13:37,460
So we put out a couple data sets to encourage for a community to, to move on to that. We call them

130
00:13:37,460 --> 00:13:46,820
WAM, the Wall Street Journal, WSJ hipster ambient mixtures. And it's like, like the band WAM.

131
00:13:48,580 --> 00:13:54,340
And it's, people are now using this, this data set as well, to try, try the algorithm in a

132
00:13:54,340 --> 00:13:59,060
more challenging scenario. The performance is, of course, much worse in reverberant and noisy

133
00:13:59,060 --> 00:14:03,620
conditions. And when you say reverberant and noisy, you're talking about when the training,

134
00:14:03,620 --> 00:14:10,580
the speech training data has noise. Both for training and the tests. So typically, like if you

135
00:14:10,580 --> 00:14:16,980
don't train on data that has noise and reverberation, that could be a completely fail. But even if

136
00:14:16,980 --> 00:14:21,780
you include noise and reverberation in your training data, the performance is clearly not as good

137
00:14:21,780 --> 00:14:27,140
as in clean conditions, because the noise, but especially the reverberation really smears the

138
00:14:27,140 --> 00:14:32,180
characteristics of the speech, everything that is normally in a spectrogram. So in a time frequency

139
00:14:32,180 --> 00:14:37,380
representation of your signal, where you show the energy at each time in frequency, speech is very,

140
00:14:37,380 --> 00:14:41,140
very clean. You can see the patterns. But as soon as you introduce reverberation, everything

141
00:14:41,140 --> 00:14:50,180
gets smeared. And so it becomes much harder to first to, for the network to identify the structure.

142
00:14:50,180 --> 00:14:56,180
And then there's more overlap between, because everything gets like mushed. So there's more overlap

143
00:14:56,180 --> 00:15:02,340
between the different sound sources. So you imagine like, let's say imagine like on a spectrogram,

144
00:15:02,340 --> 00:15:08,900
typically you have stripes when you have harmonic speech tends to be, there's a lot of harmonic parts

145
00:15:08,900 --> 00:15:13,700
in speech and these show up as stripes on a spectrogram. So you imagine stripes and you have two

146
00:15:13,700 --> 00:15:20,900
sets of stripes. If they are very clean, you can probably identify them and then make a mask that

147
00:15:20,900 --> 00:15:25,700
cancel one of them and only keeps one. But if both stripes are kind of smeared and become

148
00:15:25,700 --> 00:15:30,420
like kind of mushed, then there's a lot of portion but overlap and it's also harder to see the

149
00:15:30,420 --> 00:15:37,540
stripes in the first place. So it makes it makes it difficult both to identify structure and it's

150
00:15:37,540 --> 00:15:41,620
it's, it's, it is actually, there actually is more overlap between the structures.

151
00:15:42,340 --> 00:15:52,820
So when you, so the way you've set up the, the cocktail fork problem and maybe this is general

152
00:15:52,820 --> 00:16:01,460
to my question is general to other cocktail party problem applications. You've got speech and you've

153
00:16:01,460 --> 00:16:08,420
got, you know, other, you know, background things. It could be other speech or it could be other,

154
00:16:09,780 --> 00:16:16,020
you know, the movie soundtrack and sound effects and the like. And then you pass it through this

155
00:16:16,020 --> 00:16:24,900
model and you have a speech stream that comes out on the other end. And I, I guess independent of

156
00:16:24,900 --> 00:16:33,460
whether you are working with clean speech or noisy speech on the back end, I'm curious how clean

157
00:16:33,460 --> 00:16:47,940
the output speeches or I'm trying to, I have a, a picture in my head of something that is doing

158
00:16:50,260 --> 00:16:55,140
I don't even know how to describe it like you, some other application that's similar to this kind

159
00:16:55,140 --> 00:17:00,340
of separation, but I don't remember where it's from. But like you get the speech, but there's also kind

160
00:17:00,340 --> 00:17:05,860
of this warbling noise in the background that there's a, in, in, in like kind of more,

161
00:17:06,740 --> 00:17:13,860
more conventional, I would say, separation methods. There's this artifact that's kind of like

162
00:17:15,460 --> 00:17:19,700
that are kind of added to the speech. That's something that's typical of deep learning

163
00:17:19,700 --> 00:17:26,580
based methods. We don't observe that much actually. Interestingly, yes, there's still artifacts,

164
00:17:26,580 --> 00:17:32,980
I mean, but they don't sound musical mode is not such an issue in deep learning based methods.

165
00:17:32,980 --> 00:17:40,100
And I'm not sure I have a good insight for why. But yeah, it's sort of like these methods like

166
00:17:40,100 --> 00:17:44,740
introduce sort of a spread out noise across the structure, but doesn't really have a structure,

167
00:17:44,740 --> 00:17:50,660
but then that kind of creates that kind of musical noise. I would say the artifacts in deep learning

168
00:17:50,660 --> 00:17:57,380
based methods are, I mean, most of the time it's really failure to separate like, or you get,

169
00:17:58,100 --> 00:18:02,340
when you're doing speech separation in an era that we very often get is that the speaker,

170
00:18:03,540 --> 00:18:06,900
if you listen to each, because you have two speakers and you separate them,

171
00:18:07,860 --> 00:18:12,180
the speakers actually separate that at any time instant, they're pretty well separated, but,

172
00:18:12,180 --> 00:18:17,460
but halfway through, the other than makes a mistake in how it's stitched thing and then it switches

173
00:18:17,460 --> 00:18:23,780
the speakers. And so if you listen with headphones, you're like, you suddenly have the speakers

174
00:18:23,780 --> 00:18:30,420
to switch sides, they're separated, and that can happen multiple times. So this kind of speaker

175
00:18:30,420 --> 00:18:37,620
permutation problem is one of the issues that we have typical issues that these systems have.

176
00:18:38,740 --> 00:18:42,180
And so people have tried to come up with methods to alleviate that.

177
00:18:42,180 --> 00:18:51,300
But yeah, that's a, invest to a lot of artifacts. It tends to work pretty well,

178
00:18:51,300 --> 00:18:57,060
except when it completely fails. That's a pretty typical thing in deep learning, I would say.

179
00:18:57,860 --> 00:19:06,420
And you've talked a little bit about the spectrograph representation of speech.

180
00:19:06,420 --> 00:19:12,340
Do these models tend to work like in the frequency domain or a time domain, or like,

181
00:19:12,340 --> 00:19:18,100
how do you think about the way they're working? So there are multiple approaches in speech

182
00:19:18,100 --> 00:19:23,940
separation. Historically, a lot of them were based in the time frequency domain. So what we do

183
00:19:23,940 --> 00:19:30,260
is we take, you know, short snippets of signals, we call frames. And so we just put a window

184
00:19:31,140 --> 00:19:35,060
a little bit of signal and do a four-year transform to analyze a frequency content,

185
00:19:35,060 --> 00:19:41,700
of that's more window. And so that gives us a vector. And then we have a bunch of these vectors,

186
00:19:41,700 --> 00:19:50,100
and this is what makes the spectrogram. And historically, people only mostly treated the,

187
00:19:50,820 --> 00:19:54,900
these numbers are all complex numbers, by the way. The free transform gives you a complex number

188
00:19:54,900 --> 00:20:02,420
at each time frequency. And historically, people have looked mainly at the magnitude at the length

189
00:20:02,420 --> 00:20:11,060
of that vector, another of that complex number. And the phase part, the reangle of the complex

190
00:20:11,060 --> 00:20:14,980
number was kind of left as it is because it was thought to be pretty difficult tomorrow.

191
00:20:15,940 --> 00:20:22,340
And if you look at a picture of a magnitude spectrogram, so where you don't consider afraid,

192
00:20:22,340 --> 00:20:26,660
the phase looks very random, but the magnitude looks very nice with these stripes that I was

193
00:20:26,660 --> 00:20:30,420
mentioning. So it feels like you can do something about it. You can do machine learning on it.

194
00:20:30,420 --> 00:20:36,500
And that's what people have been doing. And still to this day, there's a lot of methods that only

195
00:20:36,500 --> 00:20:43,700
handle the magnitude part, like for example, with some transform, like a frequency transform,

196
00:20:43,700 --> 00:20:49,060
like a, what we call mail transform, to gather some frequencies according to some filters.

197
00:20:49,060 --> 00:20:55,380
And that's what is used in, in speech recognition, for example. But, so we started with that.

198
00:20:55,380 --> 00:20:58,980
And for many years, most of the methods were based in the time frequency domain,

199
00:20:58,980 --> 00:21:07,060
nowadays, the state of your methods, either deal, now include the phase, they include both

200
00:21:07,940 --> 00:21:13,540
the model of a full complex number in the time frequency domain, or they ditch, they do not

201
00:21:14,180 --> 00:21:19,460
explicitly use a time frequency transform. They go straight from the time, the waveform,

202
00:21:19,460 --> 00:21:27,060
the time domain signal to the desired time domain signal. Inplicitly, they are kind of learning

203
00:21:27,060 --> 00:21:36,180
the time frequency transform. So instead of using the usual Fourier transform, they use a filter bank,

204
00:21:36,180 --> 00:21:42,180
a bank of filters, and they let the network figure out what these filters should be. So it's very

205
00:21:42,180 --> 00:21:47,300
similar. And it's implicitly based, still based in some form of time frequency representation,

206
00:21:47,300 --> 00:21:52,020
but it's fully learned. So we see both types of methods. Can you talk a little bit about the

207
00:21:52,020 --> 00:22:00,260
the model and architecture that you used in the cocktail fork problem? Sure. So that more

208
00:22:00,260 --> 00:22:06,100
actually, we took from kind of state of the art model from music separation called Trust and Mix.

209
00:22:07,460 --> 00:22:16,100
It was introduced by Sony. And, and it's based on a recurrent neural network,

210
00:22:16,100 --> 00:22:25,620
a bi-directional long short term RNN, so BLCM. The its characteristic, what kind of makes it a

211
00:22:25,620 --> 00:22:32,340
bit different is that in their model, you start from a single, start from the mixture. And then

212
00:22:32,340 --> 00:22:40,260
you have multiple branches that have different, different transforms, different layers

213
00:22:40,260 --> 00:22:48,980
that go through that transformers signal in ways that the network can decide. And then they

214
00:22:48,980 --> 00:22:55,700
average this transform. And this goes through some BLSTM layers. And then they average again,

215
00:22:56,340 --> 00:23:04,100
the outputs. And it's kind of this averaging that's between various branches that is the

216
00:23:04,100 --> 00:23:14,020
special sauce. And so we took this. And the only thing that we changed is that we thought that

217
00:23:14,020 --> 00:23:20,180
in the case of a cocktail fork problem, the signals that we deal with are so speech,

218
00:23:20,180 --> 00:23:26,820
sound effects, and music. And they have kind of different dynamics. They, like some very,

219
00:23:26,820 --> 00:23:31,860
especially with sound effects, you may have very short, impulsive sounds like bangs or claps,

220
00:23:31,860 --> 00:23:42,980
or you may have long harmonic sounds like sirens, for example. And the way you do your time

221
00:23:42,980 --> 00:23:48,660
frequency transform kind of dictates what characteristics of the signal you can focus on.

222
00:23:49,460 --> 00:23:57,220
So if you use a very impulsive sound, you want a very short window in order to be able to

223
00:23:57,220 --> 00:24:03,700
really analyze the frequency content of that very short, impulsive sound. If you have a long

224
00:24:03,700 --> 00:24:09,940
sinusoidal, you want a longer window in order to get a more fine-grained, like to narrow down

225
00:24:09,940 --> 00:24:15,860
the frequency, exact frequency. Otherwise, it's going to be more like a blurry representation.

226
00:24:15,860 --> 00:24:22,260
So the longer the window, the more fine-grained frequency resolution you get, the worse time resolution

227
00:24:22,260 --> 00:24:27,940
you get. Everything gets more blurred in the time direction. And vice versa. So depending on the

228
00:24:27,940 --> 00:24:35,060
characteristic of signal, your choice of window length is going to impact what you can see in the

229
00:24:35,060 --> 00:24:44,180
signal. So what we decided to do is that we already had several branches in the Sony

230
00:24:44,180 --> 00:24:50,980
cross end mix algorithm. And we decided to specialize each branch to have a different window length.

231
00:24:50,980 --> 00:24:56,500
So to make it able to focus on different characters, to kind of nudge it, to focus on different

232
00:24:56,500 --> 00:25:03,460
characteristics of the signal. So that was the main twist on that architecture that we had. And it

233
00:25:03,460 --> 00:25:06,900
did bring a small bit of significant improvement on performance.

234
00:25:07,700 --> 00:25:13,060
And does that window length end up being a hyperparameter that you have to tune for each of your

235
00:25:13,060 --> 00:25:23,780
sources? So yes, in the sense that we manually tried various combinations and looked at which one

236
00:25:23,780 --> 00:25:30,820
performed the best on a development set. Yes. We just tried some variety within a reasonable

237
00:25:32,020 --> 00:25:39,060
set of parameters. There's not many things to try. Got it. You kind of characterize this as like a

238
00:25:39,060 --> 00:25:47,620
toy problem or a subset of the cocktail party problem. Going back to the broader problem,

239
00:25:48,340 --> 00:25:55,460
how do you characterize where we are with regard to solving that problem with machine learning

240
00:25:55,460 --> 00:26:01,540
or deep learning? And what are some of the big research directions and challenges that folks are

241
00:26:01,540 --> 00:26:07,860
working on? Yeah. I would say it's a smaller version, but it's not the list. It's pretty,

242
00:26:07,860 --> 00:26:13,140
I would say it's an important application that is kind of surprising to me that nobody actually

243
00:26:13,140 --> 00:26:22,580
looked at this earlier, because it sounds like it should be useful for analyzing YouTube videos,

244
00:26:22,580 --> 00:26:26,580
for example. If you want to know what's going on in a YouTube video and maybe sometimes somebody

245
00:26:26,580 --> 00:26:31,220
speaking, but you really want to listen more to the sound events, then you would like to separate

246
00:26:31,220 --> 00:26:35,540
the speech from a sound events. And most, and sometimes you have music in the background where you

247
00:26:35,540 --> 00:26:43,220
don't care about. So we think of it as an important pre-processing step that we plan to use

248
00:26:43,220 --> 00:26:51,540
ourselves in our projects for this kind of multimodal content analysis. But to answer your question,

249
00:26:52,580 --> 00:27:00,420
I think the field is progressing very fast in the capacity that we have to separate general scenes.

250
00:27:00,420 --> 00:27:08,900
I'm very excited about the progress that's been made. And you know, there's a lot of work that's

251
00:27:08,900 --> 00:27:20,580
been done on general sound separation sound events. I feel like what is maybe what the challenges

252
00:27:20,580 --> 00:27:30,740
are, are the first, the realism of data, like how realistic can you, can you get, can you make

253
00:27:30,740 --> 00:27:38,020
your data? And because it's still very important currently for these algorithms to train in a

254
00:27:38,020 --> 00:27:44,420
supervised manner. And we're only seeing, starting seeing methods that can, that can be trained

255
00:27:44,420 --> 00:27:51,140
without super, like supervised data. So what I mean by this is the typical way to train these

256
00:27:51,140 --> 00:27:57,780
methods is to get a bunch of sounds, mix them together, and this is your input, and then you ask

257
00:27:57,780 --> 00:28:03,540
the algorithm to come up with the separated, isolated sources, which you already have,

258
00:28:04,100 --> 00:28:09,300
because you mix them together. So that's easy. The downside of that is that that mixture may not be

259
00:28:09,300 --> 00:28:14,340
very realistic. So, you know, we make some efforts to do it, but it's never going to be a real thing.

260
00:28:15,540 --> 00:28:22,340
And so recently some, some groups have developed unsupervised methods where you only have the

261
00:28:22,340 --> 00:28:28,020
mixture. And you don't have, you can't, you can't ask the algorithm to be as close as possible

262
00:28:28,020 --> 00:28:32,980
to the isolated source, because they don't exist. You don't have them. And so one of the methods,

263
00:28:32,980 --> 00:28:37,860
for example, was developed by my former colleague, John Hershey, and his colleagues at Google,

264
00:28:37,860 --> 00:28:47,540
Scott Wisdom, is called mix it. So what they do is they take two mixtures and they mix them together.

265
00:28:48,500 --> 00:28:55,060
And so they have the original mixtures. And what they have the algorithm do is very, very simple

266
00:28:55,060 --> 00:29:02,900
bit of a clever is they have, they ask the algorithm to separate all the sources in this mixture

267
00:29:02,900 --> 00:29:11,460
of mixture. And you have many, you have many sources. And then they judge how well this happened

268
00:29:11,460 --> 00:29:18,500
by picking the best combination that comes back to each of the mixtures. And because the algorithm

269
00:29:18,500 --> 00:29:22,820
doesn't know which sources mix with each other, it has to separate all of them in order to be

270
00:29:22,820 --> 00:29:27,460
able to reconstruct the mixtures. So that's very simple and works pretty well.

271
00:29:27,460 --> 00:29:33,300
So I think these kind of methods and another word that we also did

272
00:29:34,420 --> 00:29:43,620
called weekly supervised source separation, where we trained a system where the reference

273
00:29:43,620 --> 00:29:50,260
signal, the only labeling we had was wherever a type of a sound of a particular type was present

274
00:29:50,260 --> 00:29:55,460
or not in the mixture. So imagine that you want separate dogs from cats from bells from cars.

275
00:29:55,460 --> 00:30:04,180
And your data is mixtures of these sound classes. And in each mixture, which sources are present?

276
00:30:04,180 --> 00:30:10,660
Like is there a dog? Yes, maybe somewhere as a dog and somewhere as a car. But that's it. That's

277
00:30:10,660 --> 00:30:18,660
all you know. And so we devised an algorithm to train the separation under that constraint. And

278
00:30:18,660 --> 00:30:25,620
the idea is that you have a separator come up with separate sources and then you use a classifier

279
00:30:25,620 --> 00:30:31,860
that looks at each source and should tell you there's a dog in there when there should be a dog.

280
00:30:31,860 --> 00:30:39,540
And if it detects a cat, then you did something wrong. So that's kind of one of those methods that

281
00:30:39,540 --> 00:30:45,060
try to reduce the amount of labeling that is needed to train these methods. And that's very

282
00:30:45,060 --> 00:30:52,260
important because if you go in the field and you record something, you may be able to ask someone

283
00:30:52,260 --> 00:30:58,100
to tell you which sources are present, but it's impossible to get the actual real isolated sources.

284
00:30:59,620 --> 00:31:04,900
So these methods that kind of reduce the amount of labeling that's needed to train separation

285
00:31:04,900 --> 00:31:12,260
systems are pretty important to going forward. And I would say maybe yeah, I was going to suggest

286
00:31:12,260 --> 00:31:20,260
another challenge in the separation. Before we move on to the next challenge, are there methods

287
00:31:23,380 --> 00:31:28,340
that approach it from weak supervision from the perspective of, hey, I've got

288
00:31:29,700 --> 00:31:35,380
set of training data that is, you know, it's speech and isolation. I know it's speech and

289
00:31:35,380 --> 00:31:40,820
isolation. I have sound effects, you know, to you know, put in the context of this cocktail fork

290
00:31:40,820 --> 00:31:48,020
problem, sound effects and isolation, music and isolation, but not providing the mixture.

291
00:31:52,020 --> 00:31:56,260
Well, yeah, I think there are such like based on like auto encoders, for example,

292
00:31:57,140 --> 00:32:02,500
there are methods that try to do that. Now the question is like, if you have these sounds, why not

293
00:32:02,500 --> 00:32:13,460
mix them? So that's that's maybe one of a downside of this. But yeah, I mean like in a sense,

294
00:32:13,460 --> 00:32:19,540
if you're able to train auto encoders on each source type, then it's pretty modular. Like if you

295
00:32:19,540 --> 00:32:24,020
want to, if you have a speech autoencoder, for example, this this network is only able to reconstruct

296
00:32:24,020 --> 00:32:31,300
speech. And then you have a music autoencoder, then you can combine them to kind of separate speech

297
00:32:31,300 --> 00:32:37,940
from music. And so if they are these methods, typically, they don't work as well as the best,

298
00:32:39,620 --> 00:32:47,540
the artets of methods that are not based on like generating the output. I think your implicit

299
00:32:47,540 --> 00:32:55,780
responses that a bad mixture is going to be better than no mixture at all. I mean, I'm pretty

300
00:32:55,780 --> 00:33:00,660
sure that the auto encoders are going to go further, but at this point, yes, they don't work as well,

301
00:33:01,540 --> 00:33:06,740
as as this kind of more training based on mixtures.

302
00:33:08,580 --> 00:33:14,100
Of course, yeah, that's and there's another type of seniority that is interesting is if you

303
00:33:14,100 --> 00:33:23,300
actually have example of speech and noise, and you have some time to remember the exact

304
00:33:23,300 --> 00:33:28,740
scenario. So you have example of speech and noise, and you have example of speech, but you don't

305
00:33:28,740 --> 00:33:36,260
have example of noise and isolation. Then you can still kind of do some tricks to train methods

306
00:33:36,260 --> 00:33:40,340
to to separate the speech from the noise. So you don't have, you have a mixture of speech and

307
00:33:40,340 --> 00:33:45,860
noise, but you don't have the corresponding isolate around truth. But you're sort of what speech

308
00:33:45,860 --> 00:33:51,300
sound like. So you can then come up with some tricks to to use these data to train a method,

309
00:33:51,300 --> 00:33:55,860
a separation system. So you're about to mention the next challenge.

310
00:33:55,860 --> 00:34:00,660
I mean, something that is kind of what I'm excited about that we started a little bit working

311
00:34:00,660 --> 00:34:09,460
on still and we need to go further, I think is this idea of hierarchy in in sounds. So we published

312
00:34:09,460 --> 00:34:15,940
one paper on hierarchical source separation, where we try to argue that, you know, source

313
00:34:15,940 --> 00:34:23,940
separation sometimes is actually a bit of an ear pose problem. Because let's say you're in a bar

314
00:34:23,940 --> 00:34:29,860
and you're talking with people and there's a band playing in the background and you say,

315
00:34:29,860 --> 00:34:38,740
well, okay, separate that scene. What does it exactly mean? Like if you're saying, if your problem

316
00:34:38,740 --> 00:34:43,540
is separated separate speech from noise, that's pretty well posed or speech from speech that's

317
00:34:43,540 --> 00:34:49,620
pretty well posed. But as soon as you get into more complex scenarios, you ask to separate the

318
00:34:49,620 --> 00:34:54,900
scene, do you want to put all the people talking together in one group? Do you want to put the band

319
00:34:54,900 --> 00:35:00,980
as one coherent source or do you want each instrument in the band? You want each person separately?

320
00:35:00,980 --> 00:35:08,020
So it's kind of a hierarchical structure of the sound within a scene. And this hasn't been

321
00:35:08,020 --> 00:35:16,740
I could even envision trying and pose some kind of spatial hierarchy, like the people that may

322
00:35:16,740 --> 00:35:21,780
be talking to one another because they're closer than more distant people. Exactly. Yeah, that's

323
00:35:21,780 --> 00:35:28,660
another type of hierarchy. Exactly. So there's many different cues that can be that we can use

324
00:35:28,660 --> 00:35:35,140
when we're in that scene to kind of make sense of it. But this hasn't been, I mean, the spatial

325
00:35:35,140 --> 00:35:42,340
information have been used with microphone array methods, but maybe not exactly in that, in that

326
00:35:42,340 --> 00:35:50,340
with that angle, if I may say so. And so we had a, we tried to argue that paper, but like

327
00:35:50,340 --> 00:35:55,460
people should start trying to think about social separation as a hierarchical, we have a hierarchical

328
00:35:55,460 --> 00:36:04,420
angle. And the first thing that we did in that topic was to consider a music source separation

329
00:36:05,380 --> 00:36:12,420
with a hierarchy of instruments. And so a slightly different idea of hierarchy, but basically,

330
00:36:12,420 --> 00:36:24,820
we were saying that if a user wants to create a system to separate a particular instrument

331
00:36:24,820 --> 00:36:32,980
in a music mixture, and you allow the user to give you an example of what they want.

332
00:36:34,500 --> 00:36:39,700
And so they give you a sound snippet of what they want. And so let's say you have a rock band

333
00:36:39,700 --> 00:36:46,740
and they give you an acoustic guitar snippet. What do they mean? Do they want the acoustic guitar

334
00:36:46,740 --> 00:36:53,460
in that recording? If there is one or nothing else, do they want any guitar electric acoustic?

335
00:36:53,460 --> 00:37:00,500
Do they want any instrument that is kind of harmonic? So it's it's not exactly what they mean. So

336
00:37:02,100 --> 00:37:08,260
or so we thought, well, why not consider that hierarchy? Like so when you the user

337
00:37:08,900 --> 00:37:15,140
queries with guitar or acoustic guitar, you you're going to have a network estimate all these

338
00:37:15,140 --> 00:37:21,300
all these levels of hierarchy. So is there an acoustic guitar if no silence? If there is there any

339
00:37:21,300 --> 00:37:28,020
guitar, then you you you output the mixture of all the guitars in that recording. And then

340
00:37:28,020 --> 00:37:31,780
above is all the kind of harmonic instruments and then the drums on the other side.

341
00:37:33,060 --> 00:37:41,700
And we found that if you train an network to do this, it can actually help it train better with

342
00:37:41,700 --> 00:37:48,660
less data at the fine grain level. So it was able. So for example, if you you don't have a lot of

343
00:37:48,660 --> 00:37:55,700
tracks with acoustic guitar, you have a lot of tracks with electric guitar. The you can network

344
00:37:55,700 --> 00:38:02,980
and leverage that and kind of learn the structure and can improve its performance on acoustic guitars

345
00:38:02,980 --> 00:38:09,940
by learning to separate together the other guitars during train. And do you think is that an

346
00:38:09,940 --> 00:38:17,940
instance of multitask learning? Is that part of why that's working? I think that's it's

347
00:38:17,940 --> 00:38:24,820
some form of multitask learning. Yes. But it's in that case, it's it's a bit more specific in

348
00:38:24,820 --> 00:38:32,180
that the tasks are really related to each other. So we actually impose constraint that the the

349
00:38:32,180 --> 00:38:38,100
output at the upper level should include the output at the lower level. So for example, if you

350
00:38:38,100 --> 00:38:47,620
have electric guitar and acoustic guitar outputs, the their parent is we impose and let's say you use

351
00:38:47,620 --> 00:38:55,220
you do the separation by using a mask. So a mask is what we typically use I mean very often use

352
00:38:55,220 --> 00:39:01,300
for separation. It's numbers typically between zero and one that we apply we multiply at each

353
00:39:01,300 --> 00:39:08,020
time and frequency and we say to zero if we want to basically turn turn there this part of this

354
00:39:08,020 --> 00:39:13,860
time frequency energy off and we put it to one if we want all of it. And so you can apply a mask to

355
00:39:13,860 --> 00:39:20,820
shut off the things you don't want and keep the others. So one way to impose a hierarchy go

356
00:39:20,820 --> 00:39:25,700
consternate say well I have two masks for the electric and acoustic guitar and I'm going to impose

357
00:39:25,700 --> 00:39:31,860
that the apparent mask is at least as large as the max of the two. And so that kind of

358
00:39:31,860 --> 00:39:35,700
imposes a hierarchy course structure and this regularization helps the network train a little bit

359
00:39:35,700 --> 00:39:42,980
there. Nice. Any other challenges come to mind? Or areas that you're excited about?

360
00:39:43,940 --> 00:39:50,580
Let's see I mean I'm also excited on the audiovisual aspects. So not only Sanone but the

361
00:39:50,580 --> 00:39:57,300
what I mean in my team what we're really trying to do is to make sense of a complex audiovisual

362
00:39:57,300 --> 00:40:04,260
scene. And so using using vision with our partners in the computer vision group at Merl and

363
00:40:04,260 --> 00:40:12,020
trying to type this with sound. So we did some work on trying to use vision as an auxiliary

364
00:40:12,020 --> 00:40:18,420
feature to improve sound source separation and in particular try to model how objects interact

365
00:40:18,420 --> 00:40:24,100
with each other to create sound in a scene like think of a bang a stick banging on a pot.

366
00:40:25,540 --> 00:40:34,900
And so this kind of visual cues can help you separate better. So yeah I'm very excited about

367
00:40:34,900 --> 00:40:41,460
trying to I mean our ultimate goal in my team is what we call total transcription.

368
00:40:42,500 --> 00:40:47,780
So it's like if you have a complex acoustic scene is to completely transcribe what's going on.

369
00:40:47,780 --> 00:40:52,580
So if somebody speaks you transcribe the speech. You may also be interested in the emotion and how

370
00:40:52,580 --> 00:40:58,660
they say it. And if there's music you you could separate the music potentially transcribe it as

371
00:40:58,660 --> 00:41:04,260
well. If you have sound events you could you know try to detect them, localize them in 3D space as

372
00:41:04,260 --> 00:41:09,300
well. So that's kind of that more holistic approach that I'm pretty excited about.

373
00:41:09,860 --> 00:41:16,100
Nice. Well Jonathan thanks so much for sharing with us a bit about what you're working on.

374
00:41:16,100 --> 00:41:18,100
Thank you for having me.

