WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:31.940
I'm your host Sam Charrington.

00:31.940 --> 00:36.660
You are invited to join us for the very first Twimblecon conference which will focus on

00:36.660 --> 00:41.200
the tools, technologies and practices necessary to scale the delivery of machine learning

00:41.200 --> 00:43.720
and AI in the enterprise.

00:43.720 --> 00:48.600
The event will be held October 1st and 2nd in San Francisco and early bird registration

00:48.600 --> 00:58.840
is open today at twimblecon.com, again that's twimblecon.com, I can't wait to see you there.

00:58.840 --> 01:04.680
Alright everyone, I am on the line with Judy Gesoya, Judy is an interventional radiology

01:04.680 --> 01:10.960
fellow at the Daughter Institute at Oregon Health and Science University as well as a co-organizer

01:10.960 --> 01:15.680
of Black and AI, Judy, welcome to this week in machine learning and AI.

01:15.680 --> 01:18.280
Thank you Sam, I'm very excited to be here.

01:18.280 --> 01:24.720
It is great to have you on the show and I'm really looking forward to diving into this

01:24.720 --> 01:34.040
conversation about the intersection of radiology and AI but before we do that, how did

01:34.040 --> 01:37.840
you come to start working in this space?

01:37.840 --> 01:46.200
So I had some interest and experience already working in digital health in medicine.

01:46.200 --> 01:52.200
I just a little bit of background about me, I went to medical school in Kenya and during

01:52.200 --> 01:57.800
that time I was taking care of HIV patients and it was really difficult to sort of just

01:57.800 --> 02:04.200
use paper and Excel spreadsheets and so I gravitated to sort of walking on an open source

02:04.200 --> 02:11.840
medical record system and so I started walking, you know, deeply deploying, programming and

02:11.840 --> 02:14.280
understanding how these systems are deployed.

02:14.280 --> 02:22.800
And so at that point I did move to the US to do masters in clinical informatics and during

02:22.800 --> 02:29.200
that point I decided that I wanted to get a clinical specialization just because the

02:29.200 --> 02:33.840
truth is you get a little more respect if you have a clinical background, especially

02:33.840 --> 02:38.840
when talking to clinicians, but also that I realized that what I was passionate about

02:38.840 --> 02:44.400
which was building health systems that I use by doctors would be very difficult if you

02:44.400 --> 02:46.880
are not using them yourself.

02:46.880 --> 02:54.760
So I subsequently went to Indiana University for my radiology residency and the program

02:54.760 --> 02:59.840
that allowed me to grow a lot, I had lots of opportunities to work on informatics on

02:59.840 --> 03:06.680
a national level and so when I was attending the American College of Radiology Anum meeting

03:06.680 --> 03:11.920
and that's around also when Joff Hinton, you know, publicly say that we should stop

03:11.920 --> 03:18.120
training radiologists, you know, there was this sort of force and just uprising of physicians

03:18.120 --> 03:24.440
who could do radiology and computer science to figure out why, you know, what was this

03:24.440 --> 03:25.440
deep planning?

03:25.440 --> 03:26.440
Why was it a threat?

03:26.440 --> 03:32.480
And so I would say it was just right place, right time that I got involved into this sort

03:32.480 --> 03:38.000
of this fantastic intersection of tech and medicine.

03:38.000 --> 03:46.720
And that uprising as you call it resulted in a paper that you're the lead author on called

03:46.720 --> 03:52.840
Frenesis of AI and Radiology, superhuman meets natural stupidity.

03:52.840 --> 03:59.200
I think poking fun at this idea that AI will put radiologists out of business.

03:59.200 --> 04:05.640
Yes, you know, some, it was very, very, first of all, you know, in medicine we don't publish

04:05.640 --> 04:13.600
an archive, you know, and I have, so it's really such a big shift to be honest and one

04:13.600 --> 04:19.720
of my really good friends who's, Satoshi, who's on the paper and we've known each other

04:19.720 --> 04:26.520
on global health and work as good collaborators right now and, you know, it took me a lot

04:26.520 --> 04:31.920
of evenings to try and explain to him what radiology does or drag him to the reading room

04:31.920 --> 04:36.720
and just, you know, explain to him and just show him the workflow.

04:36.720 --> 04:42.400
And, you know, at this, when we came to sort of this advert and, I mean, Andrew M. was

04:42.400 --> 04:46.960
saying, you know, we're doing better than radiologists every, I mean, every article every

04:46.960 --> 04:53.640
week just talked about how they had better performance, you know, and it was just sort

04:53.640 --> 05:01.080
of like almost like, okay, maybe you should focus your efforts on maybe these areas that

05:01.080 --> 05:04.680
would actually make you make us all better.

05:04.680 --> 05:11.880
And to be honest, I think someone who's also an engineer, you get a little bit cocky when

05:11.880 --> 05:13.320
you start to solve problems.

05:13.320 --> 05:19.800
I think you always, it's very easy and I've suffered from this to dismiss domain expertise

05:19.800 --> 05:24.840
and decide, you know, I can do this or all, that's just a few lines of code.

05:24.840 --> 05:29.540
And so these people was sort of like, to be honest, we were hoping that it would get

05:29.540 --> 05:35.600
debated at one of the debates in machine learning conferences and maybe we get to debate

05:35.600 --> 05:39.280
it today at this weekend machine learning.

05:39.280 --> 05:46.560
That's funny because we just recently hosted our first debate, if you want to call it,

05:46.560 --> 05:55.560
not a formal debate, but a discussion around some recent announcements by open AI on

05:55.560 --> 05:56.560
there.

05:56.560 --> 06:02.080
I'm not going to go into the details there, but it was really interesting and we'd like

06:02.080 --> 06:03.080
to do more of it.

06:03.080 --> 06:09.080
So we'll talk offline about who the right people at the table should be for this debate

06:09.080 --> 06:11.480
and maybe pull something together.

06:11.480 --> 06:17.920
But it sounds like you're saying that part of the problem is that the people that are

06:17.920 --> 06:24.320
out there proclaiming that we've solved radiology don't really understand radiology.

06:24.320 --> 06:25.320
Absolutely.

06:25.320 --> 06:27.760
Can you elaborate on that?

06:27.760 --> 06:31.360
Have you figured out what it is that they don't understand or just that they don't understand

06:31.360 --> 06:32.360
it?

06:32.360 --> 06:36.520
Well, they just, they don't understand it.

06:36.520 --> 06:43.120
And this is not also, to be honest, both sides can be very quirky.

06:43.120 --> 06:48.960
The physician's medicine is very hierarchical, you wait for your turn.

06:48.960 --> 06:53.560
In fact, I remember my teachers would say medicine is a club, we decide when and who

06:53.560 --> 06:54.560
joins it.

06:54.560 --> 07:01.680
So there's also the sort of like the nonchalant comfort, but I'm a doctor, what do you

07:01.680 --> 07:03.240
mean you can do my work?

07:03.240 --> 07:05.240
But there's also the other side of engineers.

07:05.240 --> 07:10.480
And so I think what's missing is people who sit at the intersection of both, you know,

07:10.480 --> 07:16.480
when you can easily describe things to the engineers and also sort of bring a little

07:16.480 --> 07:19.840
bit of reason to the physicians.

07:19.840 --> 07:26.520
And you know, I have sort of, back times with a little bit of stigma actually, because

07:26.520 --> 07:31.080
in medicine, if you do anything but clinical work, now it's being embraced, but it was

07:31.080 --> 07:34.560
always assumed that you're not good enough to be a doctor.

07:34.560 --> 07:36.720
You know, you're not doing well.

07:36.720 --> 07:39.720
If you do research, they're like, oh, yeah, he's good at research, but he's a terrible

07:39.720 --> 07:40.720
doctor.

07:40.720 --> 07:45.440
If you do IT or deep learning, you know, people expect you to not do a good job.

07:45.440 --> 07:50.200
And so you have to fast fight that bias and stigma and do a very good clinical job as

07:50.200 --> 07:53.640
a doctor so that people get that respect from you.

07:53.640 --> 07:59.920
So there's all these sort of soft skills that you have to stimulate, even before we can

07:59.920 --> 08:04.840
have a candid conversation where we have, you know, sort of engineers embedded in the healthcare

08:04.840 --> 08:12.120
system to innovate, you know, and also to address problems that actually make an impact.

08:12.120 --> 08:22.640
And so, for example, you know, we, we notice a little bit, so I think on online some

08:22.640 --> 08:31.600
Altman recently wrote on CNBC that, you know, he'd rather have computer radiologists, more

08:31.600 --> 08:38.080
than human radiologists, and you have these sort of people posting every, every week.

08:38.080 --> 08:43.480
And it takes a long time to be a radiologist, you know, and it's, I think a little bit,

08:43.480 --> 08:48.200
I was having a conversation with a friend of mine, you become emotionally attached.

08:48.200 --> 08:52.240
And it's not just an attack on a profession, it becomes an attack on you.

08:52.240 --> 08:59.520
You know, you're not good enough, you know, or you're indispensable, you know, dispensable.

08:59.520 --> 09:05.760
And that can be a really big challenge also just to reconcile that personal conflict.

09:05.760 --> 09:09.760
And, you know, I'm glad that radiology community has completely moved from this.

09:09.760 --> 09:14.600
I mean, if you look at the last year, RSNN, you money challenged the top groups, the top

09:14.600 --> 09:21.440
10 groups, what are radiologists, you know, and if you, we have a new radiology AI journal

09:21.440 --> 09:29.720
that's online, that's, you know, good quality, pure reviewed, and it's focused on AI.

09:29.720 --> 09:35.120
I mean, so it's amazing what in three years, you know, the rapid, that uprising,

09:35.120 --> 09:39.520
the rapid transformation that came to sort of a little bit guard the profession,

09:39.520 --> 09:45.520
but also to bring a little bit of common sense in discussing what's the best way forward,

09:45.520 --> 09:50.200
because the truth is AI is coming, you know, we can't just control and make sure

09:50.200 --> 09:53.880
that you're not wasteful and we get the benefit of it.

09:53.880 --> 10:01.320
It sounds like you're saying on one hand, kind of the, this constant, you know,

10:01.320 --> 10:09.520
derision of radiologists relative to AI, you know, isn't empathetic and it's not productive,

10:09.520 --> 10:12.280
but also that is wrong.

10:12.280 --> 10:19.920
And the examples that people try to out to demonstrate that AI has exceeded

10:19.920 --> 10:25.360
the capability of radiologists are, well, you tell me how you characterize them,

10:25.360 --> 10:32.720
you know, cherry-picked or incomplete or, you know, not representative of what's actually

10:32.720 --> 10:35.200
happening in radiology.

10:35.200 --> 10:40.560
And maybe as a, you know, taking a step back from that question, maybe a good way to get

10:40.560 --> 10:46.480
there is to have you explain, you know, when you think of, you know, radiology and kind

10:46.480 --> 10:53.200
of, in its broadest sense in what radiologists are doing, that image classifiers aren't

10:53.200 --> 10:57.400
doing, you know, maybe we need to understand that more broadly.

10:57.400 --> 10:58.400
Yeah.

10:58.400 --> 11:05.000
So, I think you have to think about the sort of like the pixel level, pixel walk, which

11:05.000 --> 11:09.800
is like the computer vision, can you diagnose pneumonia, can you diagnose a fracture, can

11:09.800 --> 11:12.280
you diagnose a brain bleed?

11:12.280 --> 11:19.880
And, you know, that's, I think a lot of people have focused on that, you know, because one

11:19.880 --> 11:26.600
radiologist, especially if you've always digitized our, you know, our studies and their report.

11:26.600 --> 11:30.840
So as long as you have access to the data, to be honest, you don't need to be a radiologist

11:30.840 --> 11:34.920
to start to, you know, do some of this work.

11:34.920 --> 11:38.720
But, you know, some, I think there'll be two things.

11:38.720 --> 11:43.960
I think impactful AI will be embedded in healthcare institutions.

11:43.960 --> 11:49.520
In fact, today, if I was, my advice to sort of healthcare managers, instead of buying

11:49.520 --> 11:55.880
purchasing AI algorithms, I would say rent them or give them a place within your institution

11:55.880 --> 11:59.040
to innovate and do the work and experiment.

11:59.040 --> 12:04.960
And the reason is, I mean, your witness to this, the technology changes so much every

12:04.960 --> 12:11.880
week, you know, and for you to sort of, if you understand the purchasing process of medicine,

12:11.880 --> 12:16.440
it takes a long time by the time you, I mean, implementations of electronic medical records,

12:16.440 --> 12:18.440
system ticks, months and years.

12:18.440 --> 12:23.680
So if my implementation of your algorithm ticks, you know, nine months, by the time I'm

12:23.680 --> 12:29.200
using it, it's updated, unless you have sort of new ways, and especially if you have to

12:29.200 --> 12:33.360
get FDA approval for every algorithm, you're not going to go back in nine months before

12:33.360 --> 12:36.400
you've made your money to get a new approval.

12:36.400 --> 12:44.080
And so I see a big role, honestly, a big, if I was to say, what do I think are the big

12:44.080 --> 12:47.400
grand challenges for AI in medicine?

12:47.400 --> 12:49.800
It's really the workflow processes.

12:49.800 --> 12:52.120
And you've, I mean, I'm a big fan of the podcast.

12:52.120 --> 12:59.080
You had a few guests recently, like Microsoft talking about the notes, you know, enabling

12:59.080 --> 13:06.000
the voice transcription on the notes for doctors, because physicians spend a lot of time at home

13:06.000 --> 13:08.720
trying to catch up with documentation.

13:08.720 --> 13:12.800
And we also have things around population health management.

13:12.800 --> 13:20.360
So for example, I had a patient in May last year that I took care of and unfortunately

13:20.360 --> 13:26.360
we had a congenital blood disease ended up getting institutional disease.

13:26.360 --> 13:32.360
And this patient of mine would come to the hospital three times a week, because that's all

13:32.360 --> 13:36.400
all dialysis patients come for dialysis three times a week.

13:36.400 --> 13:41.560
So literally someone in the healthcare system, so this patient, unfortunately, 10 years

13:41.560 --> 13:44.720
ago, he had had an IVC filter.

13:44.720 --> 13:49.080
This is, you can think about this like an inverted umbrella, which you put in your vein to

13:49.080 --> 13:52.200
sort of catch the cloth so that they don't go to your lungs.

13:52.200 --> 13:57.880
And he had displaced, you know, after an accident, and it was never removed.

13:57.880 --> 14:02.880
If you said IVC filters, there are lots of lawyers who like to sue about this.

14:02.880 --> 14:09.800
And but the challenging part was, you know, these filters are placed in multiple institutions,

14:09.800 --> 14:11.720
some of them are not placed under imaging.

14:11.720 --> 14:16.040
So they just, it's just like, you know, it's like the wild west, you know, about where

14:16.040 --> 14:17.920
these filters drop in.

14:17.920 --> 14:24.480
And at the same time, why couldn't we, we must have had a, you know, an abdomen radiograph

14:24.480 --> 14:26.400
that was done for something different.

14:26.400 --> 14:31.320
And, you know, the person reading it, interpreting it may not really realize, okay, this is an

14:31.320 --> 14:35.600
institutional disease patient who probably should not be having this old type of filter

14:35.600 --> 14:37.800
that should affect their veins.

14:37.800 --> 14:40.400
But you know, sort of like that addition of value.

14:40.400 --> 14:43.920
So maybe the X-ray was done for constipation or something else.

14:43.920 --> 14:47.920
So they focused on the constipation and they answered the question, the clinical question

14:47.920 --> 14:50.200
that was asked before performing the study.

14:50.200 --> 14:55.480
But with an AI running on the background, and this is a project that we are working on,

14:55.480 --> 15:03.520
with Adam, my colleague at Emory, is to create an automatic, it's a computer detection problem.

15:03.520 --> 15:06.080
And we create an automatic filter detector.

15:06.080 --> 15:11.160
We run this every night, you know, at the end of the day with all our radiographs.

15:11.160 --> 15:16.600
And if a patient has a filter, is anticoagulated, we send a message to their doctor and say,

15:16.600 --> 15:20.200
hey, the next time they're coming in, you know, you may want to talk to them about these

15:20.200 --> 15:25.520
IVC filter, refer them to a clinic to get this evaluated for removal.

15:25.520 --> 15:31.880
So literally a decision that it's not really anyone's fault, but just sort of, we're so

15:31.880 --> 15:37.640
narrow-minded and focused on the clinical problem at hand or for the day, we completely

15:37.640 --> 15:41.960
miss the bigger picture of this patient, and that means he can never get a renal transplant

15:41.960 --> 15:47.560
because he doesn't have the right veins, the old scar done after the IVC filter placement.

15:47.560 --> 15:53.640
So this type of clinical questions, they actually come out naturally when you're embedded

15:53.640 --> 15:55.840
in the healthcare system.

15:55.840 --> 16:02.800
You know, I can go on and on with some really low-hanging fruits, and you know, it's very

16:02.800 --> 16:09.840
difficult if you're not sort of bringing this, you know, cross-pollination of teams to

16:09.840 --> 16:11.960
sort of answer these questions.

16:11.960 --> 16:16.880
And that was actually, I would say, if you read this paper, the biggest thing is actually

16:16.880 --> 16:23.720
the table one, landing passes in radiology, which is something that has been well-studied

16:23.720 --> 16:27.200
by informaticians even before this era of deep planning.

16:27.200 --> 16:31.720
And even give you some examples of things that you could do as an application that would

16:31.720 --> 16:36.360
definitely augment and value to the radiologists.

16:36.360 --> 16:38.200
Let's walk through that table.

16:38.200 --> 16:42.080
What are some of the big biases that come up?

16:42.080 --> 16:48.160
So for example, you could have a confirmation bias.

16:48.160 --> 16:53.520
This is, for example, I may say, you know, man, this is pneumonia, and you keep searching

16:53.520 --> 16:54.520
for evidence.

16:54.520 --> 16:58.360
You know, you go back to the electronic medical record system, you call the doctor, you're

16:58.360 --> 17:00.120
like, oh, you think this is pneumonia, right?

17:00.120 --> 17:08.720
So you have sort of this preformed bias, and an AI tool could probably scout all the information

17:08.720 --> 17:13.960
that's relevant already about that patient and present it in a consumable way for the radiologists

17:13.960 --> 17:14.960
to interpret.

17:14.960 --> 17:21.280
It could also give, you know, best if the AI tool was sort of not blind to the previous

17:21.280 --> 17:22.280
studies.

17:22.280 --> 17:28.840
It could say, you know, Judy, some of these studies that have been done in the past, which

17:28.840 --> 17:35.080
have a similar sort of pixel appearance like this, were interpreted by these five radiologists

17:35.080 --> 17:39.000
who, and we definitely know this high variability, but we said most of the radiologists thought

17:39.000 --> 17:43.680
that this was something else like tuberculosis, not pneumonia.

17:43.680 --> 17:50.320
And so it just gives you, you can get differential alternative diagnosis and, you know, then

17:50.320 --> 17:55.800
contextually be like, okay, maybe it was a pneumonia or just raised the possibility

17:55.800 --> 18:00.600
of both diagnosis, which I think is a big value.

18:00.600 --> 18:06.880
Something else that happens a lot is this satisfaction of report or satisfaction of such,

18:06.880 --> 18:10.000
and actually satisfaction of report which is different.

18:10.000 --> 18:17.520
So for example, if you saw the previous radiologists who read a study said, oh, you know, I have,

18:17.520 --> 18:19.040
this is cancer.

18:19.040 --> 18:23.040
And if you read the report before looking at the images to form, first form your mental

18:23.040 --> 18:28.560
representation and understanding of what the problem is, you could say, you could perpetuate

18:28.560 --> 18:31.560
that diagnosis sometimes even when it's wrong.

18:31.560 --> 18:39.760
And so one, some of the deep learning techniques that can be helped here is, for example, volumetric

18:39.760 --> 18:45.120
just measurement, you know, you could measure the size of the otter, the size of a tumor

18:45.120 --> 18:46.120
of multiple studies.

18:46.120 --> 18:52.040
I mean, that, that task, specifically, is actually kind of repetitive and tedious for

18:52.040 --> 18:53.040
our radiologists.

18:53.040 --> 18:58.560
It's easier for me to have these representations, you know, on images and presented easily,

18:58.560 --> 19:01.680
I'm like, okay, that's, that looks like accurate measurements.

19:01.680 --> 19:03.720
And the computer is, we know this.

19:03.720 --> 19:08.240
It's very accurate at determining volume and maybe even more consistent that radiologists.

19:08.240 --> 19:16.400
And so it can sort of help instead of just me glancing and being like what we call eyeballing

19:16.400 --> 19:18.640
and being like, oh, that tumor looks the same size.

19:18.640 --> 19:24.960
It can help sort of even give more additional findings or additional data points to be like,

19:24.960 --> 19:30.600
yeah, I can tell you that this tumor grew by 20%, which, you know, even just understanding

19:30.600 --> 19:34.840
maybe an event, a timeline of the patient, maybe they got changed the chemotherapy three

19:34.840 --> 19:40.640
months ago, can have an impact more than, because 20% change in a tumor in a patient

19:40.640 --> 19:45.400
on chemotherapy versus one who's not on chemotherapy, completely means different things.

19:45.400 --> 19:48.800
So there's always this sort of things that are going on.

19:48.800 --> 19:52.960
It's not just sitting at a computer, looking at the image and be like, okay, this is pneumonia,

19:52.960 --> 19:55.280
there's always this clinical context.

19:55.280 --> 20:03.920
And I feel if you had an AI assistant or whoever an assistant at the back who was consistently,

20:03.920 --> 20:11.280
you know, knows the correct information to send to you because the EMR, that is the electronic

20:11.280 --> 20:17.000
medical record system is another monstrous dump of information would just present you

20:17.000 --> 20:19.680
just in time intervention that made sense.

20:19.680 --> 20:27.200
I think those are some of the true augmentation and true actually benefits we can rip from AI.

20:27.200 --> 20:36.960
And some other examples could be, you know, the satisfaction of such a mention and this

20:36.960 --> 20:42.160
because sometimes you just see the tumor. So for example, maybe we usually get some studies

20:42.160 --> 20:50.000
around cities of the neck and, you know, you go in there and you see a big fracture,

20:50.000 --> 20:54.560
maybe, you know, compressing the cord. I mean, that's very dramatic and drastic and you

20:54.560 --> 20:57.960
call the ordering doctor and you tell them, you know, I see a big fracture.

20:57.960 --> 21:04.360
And you know, this is probably a patient who just has, you know, came in because of trauma.

21:04.360 --> 21:10.520
But you completely miss the big cancer that's just at the edge of the film, you know, on

21:10.520 --> 21:14.960
the upper lungs because you, you know, you're happy. You're like, okay, I found the findings.

21:14.960 --> 21:18.600
This patient is going to get treated this way. And those incidentals are really the places

21:18.600 --> 21:25.920
where radiologists shine. But the AI could learn our blind spots and just ask, you know,

21:25.920 --> 21:31.480
have just like another system that helps me be more cognizant of my blind spots. And

21:31.480 --> 21:35.960
this is maybe not something documented, but this is how we learn, you know, every time,

21:35.960 --> 21:40.840
you know, what you've missed and what you've caught and you start getting better and, you

21:40.840 --> 21:45.600
know, like my such pattern every time I look at the city abdomen is to repetitively look

21:45.600 --> 21:51.000
through the pancreas because those pancreatic tumors are very subtle and, you know, are

21:51.000 --> 21:56.080
places where you can make a huge difference if you have early diagnosis. So I think we

21:56.080 --> 22:01.680
have real good areas. And the reason why I actually just focused on buyers is because

22:01.680 --> 22:09.920
this, we know this exists. We know that there's a big cost of medical errors and that there's

22:09.920 --> 22:18.040
a pretty, pretty big opportunity to save costs and also to reduce death because medical

22:18.040 --> 22:22.440
errors is the fat living cost of death in the United States.

22:22.440 --> 22:34.000
It strikes me that this whole area of bias applied to radiology and AI is paradoxical in

22:34.000 --> 22:42.200
the sense that you could easily argue that the existence of bias and human and radiologists

22:42.200 --> 22:50.960
is the reason why we should be kind of all rooting for AI's to take over and for all these

22:50.960 --> 22:58.560
stories about, you know, AI's up-performing radiologists to be true. At the same time,

22:58.560 --> 23:04.760
these AI's are all trained on, you know, data sets that were labeled by humans and the

23:04.760 --> 23:11.240
biases that you've outlined, you mentioned, you know, three or four here in the paper

23:11.240 --> 23:17.000
you've got, you know, eight to ten, you know, these are all kind of embedded in our training

23:17.000 --> 23:22.880
data sets. So, you know, the example you mentioned with an image that's got, you know, the

23:22.880 --> 23:27.920
big compression fracture, but, you know, is labeled as, you know, perhaps in a training

23:27.920 --> 23:33.880
data set is labeled as such, but not labeled for the, you know, the cancer that's at the

23:33.880 --> 23:39.920
fringe. You know, we're kind of baking that into the AI's we're building today.

23:39.920 --> 23:44.480
How do you kind of pick apart that paradox and is that something that you covered in the

23:44.480 --> 23:47.960
paper or in your research or thinking about this generally?

23:47.960 --> 23:56.800
So actually, we haven't covered this and I think, you know, this fairness and transparency

23:56.800 --> 24:02.880
for ideology. First of all, this is a huge, huge, huge, huge area. Actually, this is

24:02.880 --> 24:09.360
one of the areas I'm going to be focusing on as I begin my faculty position later this

24:09.360 --> 24:16.760
year in July. And one is, even just the explainability, you know, like, why did it get here? You

24:16.760 --> 24:21.760
know, you know, this idea of the black box AI, and this is not a problem, you know, we

24:21.760 --> 24:28.960
always know that image net is the sort of the parent that all these AI tools at this

24:28.960 --> 24:34.680
and computer vision are trained in for ideology. And the moment you say, yeah, we started

24:34.680 --> 24:41.360
from dogs, cats and aeroplanes and cars. I mean, the radiologist and it goes back to the,

24:41.360 --> 24:45.360
you know, the other discussion we had during this session where I said this a little bit

24:45.360 --> 24:50.600
of soft power because when you say, oh, man, we started from cats and dogs, then you're

24:50.600 --> 24:57.240
very far from replacing me or this, I mean, how do you come back to grayscale imaging,

24:57.240 --> 25:04.480
which is radiology? But, you know, so there's one area about a gap in having more explainability.

25:04.480 --> 25:11.120
But the issue around even bias, even North, the application and the downstream consequences

25:11.120 --> 25:18.720
on policy and patient outcomes, the bias, I see radiology and I have to tell you that the

25:18.720 --> 25:23.280
engineers who are working on this are not looking at these sort of questions, as far as

25:23.280 --> 25:30.800
I know of, why maybe the AI's that will start to build help us open our eyes to the biases

25:30.800 --> 25:36.480
that exist in medicine. We know this, but maybe it's that maybe the AI revolution will

25:36.480 --> 25:42.720
help us sort of, you know, open this, open up these biases and come up with systemic

25:42.720 --> 25:51.400
changes that actually end up improving our patients. And so, for example, there is, you

25:51.400 --> 25:56.640
know, with their hospital scoring and the surgical scoring, people get very dinged for

25:56.640 --> 26:03.600
re-admissions within 30 days or performing surgeries with poor outcomes. So, what the

26:03.600 --> 26:10.000
doctors do, this is not documented, but you know it, is you self-select, you know. So,

26:10.000 --> 26:15.520
if and also they sell selection for specialties. So, there's a reason why most of my calls,

26:15.520 --> 26:19.920
I get them in the middle of the night and at that point, the excuse is that the patient

26:19.920 --> 26:25.440
is not a good surgical candidate for a procedure because, you know, it's the middle of the night.

26:25.440 --> 26:29.840
And so, we have all these subtle things that you as a doctor in the healthcare system,

26:29.840 --> 26:37.840
you kind of know, but the people building these AI tools are a little blinded too. And we don't

26:37.840 --> 26:44.560
talk, to be honest, we don't talk about them as biases in, you know, in sort of like the

26:44.560 --> 26:50.720
stricter sense and sort of like this new discipline around bias and fairness and accountability

26:50.720 --> 26:58.880
in AI, there just things that happen, you know. And so, maybe we will reap benefits, but only if we

26:58.880 --> 27:06.800
start to look at the correct sort of information and again, work with multidisciplinary teams

27:06.800 --> 27:13.760
that we can be able to address this issue. And specifically, if you look at, for example,

27:13.760 --> 27:18.160
the engineer who gets, let's say you go, you buy a bunch of data and you come in and say,

27:18.160 --> 27:22.880
oh, I trained on pneumonia, how will they know that they completely use the wrong data set

27:22.880 --> 27:26.960
if they're not working with radiologists? So, I think it's going to be sort of this

27:28.400 --> 27:35.600
union of both teams realizing that no, I'm knows it all. And we will discover things that

27:35.600 --> 27:42.800
are embarrassing and things that forces to rethink how we provide healthcare. But it's only going

27:42.800 --> 27:47.520
to come if the hype is not, I'm going to replace our radiologists because you will just replace

27:47.520 --> 27:52.720
our radiologists with another radiologist. Maybe one who doesn't get tired because they can keep going

27:52.720 --> 27:59.920
as long as they have electricity. But the subtle biases will be perpetuated and maybe amplified

27:59.920 --> 28:05.200
because I mean, if there's no right now, we don't have a framework for continuous monitoring

28:05.200 --> 28:11.520
or mandates for continuous monitoring in production. And we will continue to experience those

28:11.520 --> 28:19.680
things. So, I know I was in a presentation where the Harvard group had trained an AI to detect

28:19.680 --> 28:26.640
breast cancers. And you know, they got collaborators. I think this would be a big deal to validate

28:26.640 --> 28:34.160
the algorithm around Detroit. And it really caught a lot of cancers in Detroit. And they

28:34.160 --> 28:42.000
paused the question, why do we think it caught a lot of cancers in Detroit? And the reason was the

28:42.000 --> 28:48.240
patients who come to the Harvard healthcare system come for regular mammograms for screening

28:48.240 --> 28:54.560
Ali. And the Detroit patients came in late when they had the diagnosis, you know, head alarm

28:54.560 --> 28:59.920
and they already had cancer. So, if you didn't understand who was coming to the healthcare system,

28:59.920 --> 29:06.560
you'd be bragging how your AI is performing better, yet it just represents how healthcare is

29:06.560 --> 29:11.840
delivered and the health-seeking behaviors of the two different populations in both areas.

29:12.400 --> 29:20.320
So, the paper kind of invokes this idea of natural stupidity in the title. And you outline

29:21.120 --> 29:26.560
kind of three behaviors that you label as kind of representative of this natural stupidity,

29:26.560 --> 29:34.400
kind of the things that you see AI researchers talking about when they're not in close

29:34.400 --> 29:38.480
collaboration with subject matter experts. Can you outline those for us?

29:39.120 --> 29:46.000
Yeah, so we sort of picked up some areas and one was wishful, you know,

29:46.000 --> 29:53.840
in the Monics. And I think this has been amplified honestly by the media. And also like when leaders

29:53.840 --> 30:00.800
of group street, you know, that, oh, we are much better or we are the same as, you know, any

30:00.800 --> 30:07.760
specialist. And so, for example, one of the things that came up actually around the check

30:07.760 --> 30:15.520
net pneumonia paper was that there are many types of pneumonia. And actually, as someone who's

30:15.520 --> 30:22.640
walked in global health and developing countries, that when you take care of HIV patients,

30:22.640 --> 30:27.040
it's very different type of pneumonia. There's clinic on pneumonia and radiologic pneumonia.

30:27.040 --> 30:32.080
So by just saying pneumonia, you know, everyone will be like, oh man, that's really amazing. But

30:33.680 --> 30:39.280
when it comes to sort of like rational, just reading a negative chest x-ray, for example,

30:39.280 --> 30:45.440
on a HIV patient, does not mean that it's negative for disease. You could say they're not finding,

30:45.440 --> 30:51.440
not radiologic findings, but that does not always translate to negative for disease.

30:51.440 --> 30:56.640
Can you elaborate on this distinction between clinical pneumonia and radiological pneumonia?

30:57.200 --> 31:05.440
So, for example, radiologic would have a finding. So you may have like a consolidation that

31:05.440 --> 31:11.280
would be pretty obvious. So for example, a low-burning pneumonia or multifocus pneumonia. But the patient

31:11.280 --> 31:16.960
may, may, depending on the time or also when, where they are in terms of antibiotic treatment,

31:16.960 --> 31:22.480
may not have radiologic findings because of the sort of like the spectrum of disease. Or if they

31:22.480 --> 31:28.320
immunosuppressed to not monitor response that presents as opacities that you can see on a

31:28.320 --> 31:33.520
radiograph, they still are sick and you can have lab tests and just the clinical appearance of

31:33.520 --> 31:39.120
the picture and the history to help you diagnose, you know, clinical pneumonia. But at the same time,

31:39.120 --> 31:44.800
their chest x-ray will be negative. So if your algorithm is only looking at one facet,

31:44.800 --> 31:49.440
just the x-ray, which is maybe probably the only data available for you to look at,

31:49.440 --> 31:55.600
then you would completely mislabel these patients as, you know, pneumonia,

31:56.720 --> 32:01.840
negative for pneumonia, yet they have pneumonia. So not all pneumonia patients will have positive

32:01.840 --> 32:08.240
findings on a radiograph. Got it, got it. And so, advertising your marketing, your

32:08.240 --> 32:16.000
accomplishment as detecting, you know, some high percent of pneumonia without stating that

32:16.000 --> 32:22.560
it's radiologic pneumonia, which is a subset of kind of the broader things that a radiologist needs

32:22.560 --> 32:28.400
to be able to identify or that the medical community rather needs to be able to identify

32:29.200 --> 32:33.440
is misleading. That's what you're getting out there. Yes. And so, what was the next one?

32:33.440 --> 32:40.880
So, the next one is this assumption that human performance is decotomous. So, in this example,

32:40.880 --> 32:48.000
using this sort of statement, we can say that you either diagnose pneumonia as positive or negative.

32:48.000 --> 32:55.920
Okay. And a little bit, this is a bias of how the AI is allowed to give probabilities. You know,

32:55.920 --> 33:01.680
I may think that maybe 80 percent probability that this is pneumonia. But when you look at the

33:01.680 --> 33:08.000
performance, it's been compared to you, you just say, well, radiology, did you diagnose pneumonia

33:08.000 --> 33:14.800
or non pneumonia? We do, we completely, or me, that we also depend on probabilities. And those

33:14.800 --> 33:20.880
probabilities don't come out as a percentage. But for example, they varied a study and say,

33:20.880 --> 33:25.840
you know, finding some most consistent or suggestive of malignancy and less likely infectious

33:25.840 --> 33:31.840
etiology. That statement to a radiologist has a probability. And even for the referring doctor,

33:31.840 --> 33:37.280
we'll know, okay, I should really make sure that there's no cancer here before, you know, going

33:37.280 --> 33:42.480
the rabbit hole of infectious disease. And this one is says, okay, let's compare human performance

33:42.480 --> 33:50.000
and AI. But you have to also factor in the probabilities of of of certainty of diagnosis

33:50.000 --> 33:55.680
that the human performance look at. And so you can't just treat it as a zero, one

33:56.560 --> 34:02.240
phenomenon for human performance, but have a little more variability when you think about the

34:02.240 --> 34:08.480
machine performance. The other one we talked about was this idea of training with secondary data.

34:08.480 --> 34:15.440
And, you know, I have to give full disclosure here that I really love to study how humans

34:15.440 --> 34:22.560
work with technology, not just AI. And we know that data is a problem. The big institutions

34:22.560 --> 34:28.080
have the data. But even when you have the data, it's not well labeled. And like I think you

34:28.080 --> 34:34.080
exactly said, you may have a city for compression fracture, which was dictated as such. But if it

34:34.080 --> 34:41.840
or completely omitted the tumor incidental tumor, then you perpetuate sort of those biases.

34:41.840 --> 34:51.360
But this idea of secondary data, I think is we say that it's not enough for you to come up and

34:51.360 --> 34:56.400
just use secondary data without a prospective validation trial where your head to head with

34:56.400 --> 35:03.200
the radiologist in clinical practice and say I'm better than pneumonia. You know, and the examples

35:03.200 --> 35:09.520
that give here are, for example, you could say it's not unusual when you're reading at 7 a.m.

35:09.520 --> 35:15.120
you're reading the ICU films. You just say stable and stable. It doesn't mean normal. You know,

35:15.120 --> 35:21.680
and if your algorithm doesn't your NLP tool doesn't recognize this, then you could this,

35:21.680 --> 35:26.800
especially these groups, tend to be classified as negative. But you could have a patient who's

35:26.800 --> 35:31.920
very sick and tubated with bilateral gestives and pneumothorax. But what I'm trying to communicate

35:31.920 --> 35:38.160
to the ICU doctor is that nothing has changed. The tubes and lines are in stable position.

35:38.160 --> 35:44.160
The appearance, the opacity on the chest structure are stable. So it helps them determine,

35:44.160 --> 35:50.320
okay, is the patient, you know, getting better? But that's not the only metric that they use.

35:50.320 --> 35:55.280
But you know, when I'm looking at tubes and lines, I'm checking how they moved in position so that

35:55.280 --> 36:04.000
they need to be in position because that's sort of the purpose of an ICU team. And so to say that

36:04.000 --> 36:12.400
you're looking at a one time, you know, look where you see some radiologists three or four. Have

36:12.400 --> 36:17.440
them look at studies and then you come up and say, man, I can do better than radiologists. I think

36:17.440 --> 36:23.280
is inaccurate, which, you know, which radiologists are you doing better than the chest experts? Are you

36:23.280 --> 36:30.160
doing better than the resident who's just touching out to be a radiologist? I think relying on secondary

36:30.160 --> 36:37.520
data is not bad, but I don't think it gives us enough mandate and enough platform to say that

36:37.520 --> 36:44.880
we are better than a certain profession. And we need and calls for a calm and discipline to

36:44.880 --> 36:51.040
identify that we do need prospective trials to figure out how the human machine assemblage walks

36:51.520 --> 36:58.800
to to augment or be better than radiologists. Yeah, I think what I'm hearing here is that

36:58.800 --> 37:06.560
it's well, several things, but one is that, you know, when you think about kind of the capabilities

37:06.560 --> 37:14.400
of these algorithms, what really happens on the ground, the multiple parties involve the actual

37:14.400 --> 37:25.600
nuance of these diagnoses, they don't necessarily translate to very well to kind of the way some of

37:25.600 --> 37:36.880
these academic studies have been presented, but also that, you know, as a community of radiologists,

37:36.880 --> 37:41.840
you're not, you know, it's not like kind of sticking your head in the sand and, you know, waiting

37:41.840 --> 37:47.200
for this AI thing to go away. You know, what I'm seeing at least from you, you know, maybe you're

37:47.200 --> 37:54.320
way more sophisticated than your colleagues, but it sounds like the field as a whole is recognizing

37:54.320 --> 38:01.520
that AI can be a valuable tool. You know, let's try to work together to apply it to the problem

38:01.520 --> 38:09.120
in a more sophisticated way, the better reflects the way, you know, what that community really needs.

38:09.760 --> 38:17.120
Absolutely. Yeah, and I mean, so one of the things that are surprising was that the American

38:17.120 --> 38:23.360
College of Radiology, which represents, you know, a lot of radiology has over 20,000 members

38:23.360 --> 38:28.640
came together and set up a data science institute. And one of the things that's there, you know,

38:28.640 --> 38:34.160
they have been walking on is setting up these writing use cases. And honestly, those can bring

38:34.160 --> 38:39.600
a little bit of calm to say, okay, if I want to walk on this, maybe these, these are done by

38:40.640 --> 38:47.440
several people, several radiologists, and they come together and say, hey, this would make

38:47.440 --> 38:54.000
much more sense if I was walking on this. And, and you know, I think that's been a really good

38:54.000 --> 38:59.840
way to sort of bring some calm. And you actually, there can be a win-win, you know, you can

38:59.840 --> 39:05.200
build things that we need and we'll use them. And sort of aligning yourself with sort of these

39:06.160 --> 39:12.640
initiatives and not, you know, going around the world and doing your own innovations,

39:12.640 --> 39:18.800
I think can be a win for an entrepreneur. The other thing that, you know, for radiology,

39:18.800 --> 39:24.480
what we've started to do, I've been doing this from December 2017, is organized a monthly

39:24.480 --> 39:30.800
journal club on AI for the residents and the fellows. And we also have some faculty radiologists

39:30.800 --> 39:37.040
who join in. And this has been fantastic. And just bringing those dialogues, bringing

39:37.040 --> 39:43.920
engineers and bringing radiologists together and looking at papers and digesting this material.

39:43.920 --> 39:49.760
And I think it's been, personal, it's been a fantastic experience to do this. But also to see,

39:49.760 --> 39:55.280
also sort of like an understanding, right now, it's, you know, I know this hype and there'll be

39:55.280 --> 40:01.280
a new person who posts something. But it's more of like, okay, I think I can't start to see the role

40:01.280 --> 40:07.520
that I can play and where this can be beneficial. And I think that's, that's great. And I also see

40:07.520 --> 40:14.240
a change in tone, even in terms of the new, the newer leaders when they speak, the recently

40:14.240 --> 40:21.520
launched Coursera course by Andrew and I was listening to it on my flight yesterday. And you know,

40:21.520 --> 40:27.360
he literally shows what machine learning can do and what he cannot do with an example from

40:27.360 --> 40:33.840
radiology and states. Machine learning can learn from 10,000 just extra images or, you know,

40:33.840 --> 40:40.000
just trying to say that from a large number. But machine learning cannot learn like a radiologist

40:40.000 --> 40:45.200
who can just have three or four images of pneumonia and a short blob of text. And it's just this,

40:45.920 --> 40:51.680
sort of learning that occurs. And some it's a little difficult to describe how actually that

40:51.680 --> 41:00.240
learning occurs because the way we do it is you go in, you start to walk on an apprenticeship model

41:01.040 --> 41:08.240
and you all of a sudden are left and call at night and you just don't realize how much you've

41:08.240 --> 41:11.840
learned when the other doctors come in to ask you a question and you're like, oh, I think,

41:12.480 --> 41:17.680
I think this is, you know, this is this or this is this and just brings your confidence.

41:17.680 --> 41:26.000
I would, in fact, we are going to do this actually. We're going to start to maybe use some of the

41:26.000 --> 41:31.840
techniques that we are seeing, for example, when people use YouTube to teach games and try and

41:31.840 --> 41:40.160
get representations of the learning process, some of the tasks that can be of how especially

41:40.160 --> 41:48.720
radiology residents learn and bring this human machine assemblage that is based within the system

41:48.720 --> 41:55.200
that keeps learning and knows when to augment. And I think this will start will be probably one of

41:55.200 --> 42:00.880
the earliest efforts to understand the future of work where radiologists work with AI.

42:01.680 --> 42:08.480
Sounds like at least in what you're seeing being published by Andrew, there's maybe a broader

42:08.480 --> 42:13.280
perspective or some recognition that there are some things that radiologists are good at.

42:13.840 --> 42:22.000
Yeah. Well, Judy, thanks so much for taking the time to chat with me. It's been great discussing

42:22.000 --> 42:31.920
your perspective on the way AI is impacting radiology and the kinds of things we need to move forward.

42:31.920 --> 42:38.160
Absolutely. Thank you so much for inviting me. I'm a big fan.

42:40.720 --> 42:45.600
All right, everyone. That's our show for today. For more information about today's show,

42:45.600 --> 42:53.040
visit twimmolay.com. Be sure to visit twimmolcon.com for information or to register for Twimmolcon AI

42:53.040 --> 43:05.120
platforms. As always, thanks so much for listening and catch you next time.

