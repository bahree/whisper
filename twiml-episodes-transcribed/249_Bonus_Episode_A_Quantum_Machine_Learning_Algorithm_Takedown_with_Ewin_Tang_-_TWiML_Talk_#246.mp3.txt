Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host Sam Charrington.
Alright gang, in this special bonus episode of the podcast, I'm joined by Ewin Tang, a
PhD student in the theoretical computer science group at the University of Washington.
In our conversation, Ewin and I dig into her paper, A Quantum Inspired Classical Algorithm
for Recommendation Systems, which took the Quantum Computing community by storm last summer.
Now we haven't called out a nerd alert interview in quite a long time, but this interview inspired
us to dust off that designation, so get your notepad ready.
And now on to the show.
Alright everyone, I am on the line with Ewin Tang, Ewin is a PhD student at the University
of Washington in the theoretical computer science group.
Ewin, welcome to this week in machine learning and AI.
Hi thanks, thanks for having me.
I'm really glad we finally got a chance to connect.
I think I first reached out to you back in the summer when you came on my radar for your,
I think it was your undergrad thesis around, was this like July or so?
Yeah, that sounds about right, yeah.
So I'm not going to explain it, I'm going to let you explain it, but before we do that,
tell us a little bit about your background, what you're up to now, and how your interests
in machine learning evolved.
Right now I'm like a first year grad student.
So last year, and the four years before that I was at the UT Austin University of Texas
at Austin.
And initially actually starting my undergrad, I was a math major, so I was like interested
in doing some more much more theoretical stuff, but then I sort of realized how interesting
computer science was and how a lot of it has a lot more tangible applications that seem
really interesting.
And that's what led me to take Scott Erinzen's course in quantum information theory, so
quantum computing type stuff.
And from there, I kind of just stumbled into the field of quantum machine learning.
So I've done a little bit of research in like a machine learning type things or sort
of like the theoretical side, but this sort of work within quantum machine learning was
just brought up because I wanted to do a thesis and undergrad thesis advised by him, and
then that was a problem that he suggested.
And so that basically was, he sort of phrased it to me as like a problem of actually solving
a, like a lower bound.
So technically it, it wouldn't have been like machine learning per se if I actually got
that result.
But since the result I got was now algorithm, it's now I guess kind of machine learning.
But yeah, there is at least started my interest in the field.
I'm working on that a little bit here at UW, and I'm also working on some other things
with my advisor, James Lee.
And so the headlines I remember from back in the summer were like this major kind of
quantum computing advance, which were, which I took to be that there were these this
class of machine learning quantum machine learning algorithms that were to a large degree
considered to be a big part of the justification for quantum computing, at least by some people.
And your thesis basically showed how well you didn't really need quantum computers to
do this.
Is that, you know, how, how close is that to describing the thesis and can you kind of put it
in your own words and maybe give us an overview of it?
Yeah, that sounds, that sounds actually pretty accurate.
So the general idea of my thesis is that there's this field of algorithms called like quantum
algorithms, you know, that do that solve machine learning tasks.
And usually this is like very basic tasks like linear algebra type things.
And the reason people were so excited about them when they were like when they were released
is that they produce exponential speed ups for these linear algebra problems.
So basically, if you're familiar with quantum computing, the sort of the highest, the best
type of speed up that you can get is an exponential speed up.
And that's, for example, what we believe occurs for like factoring and like shows algorithm
and the problems that quantum computing is known for solving.
And so an exponential speed up would be great.
But it kind of has some caveats in the sense that it, these algorithms assume some very
strong assumptions about their input.
Actually they say, okay, if you give me all of the data in a quantum state, so that means
like if you give me everything in like superposition basically, then I can do it exponentially fast.
Then I can, you know, do your machine learning incredibly fast.
However, doing this is actually creating these quantum states is actually a hard task
computationally. And so people didn't really were kind of skeptical about whether you could
actually implement these things, you know, people weren't sure whether you could, whether
even when the subroutines, when these like linear algebra routines are fast, that just
getting to that point would take so much time that it wouldn't have been worth the effort
to begin with.
So that's the sort of conflict that's sort of motivating the work.
So this, my thesis was about a particular algorithm.
It's an algorithm by Jordan Karen Edison, a new form per cache called quantum recommendation
systems.
And it purports to like give an exponential speed up for the problem of like recommending
products to users like, like what Netflix or Amazon do.
And the interesting thing about that algorithm is that it actually has, it actually has
relatively few assumptions.
So it doesn't assume that you, that you, that you can just implement quantum states out
of thin air.
It gives like a protocol, it gives like a data structure for you to construct these
things.
And further, a lot of the previous quantum algorithms were sort of trying to solve problems
that weren't being studied classically.
So these previous quantum algorithms were just constructing problems that they believed
could be solved faster with quantum computing, and then they justify their usefulness.
But the nice thing about this algorithm is that it had been studied previously, and then
people had not been able to get even close to as fast as what the quantum algorithm achieved.
So basically this recommendation system algorithm is, was one of the best examples of quantum
machine learning that we had.
And it had an exponential, it was exponentially faster than all previous classical algorithms
that we knew of.
Is this recommendation system or algorithm that they proposed is it based on matrix factorization
and is that why did they kind of inherit the exponential speed up of some matrix factorization
algorithms, or is there something kind of fundamental to their approach that gave them
that exponential speed up?
Yeah, yeah.
So it's basically like a matrix factorization.
Yeah.
It's essentially what happens is that you want, so your recommendation matrix is some
like low rank matrix, basically that just the implication is that people base their preferences
on some small number of attributes that something has.
So maybe you judge an Amazon product by its popularity, its price, and then the
number of stars or something.
And so you have relatively few attributes which tends to lead to low rank matrices.
And the general idea is that you want to factor this matrix, like compute the SVD, and
this will separate your users into user classes, basically.
So you can say, okay, these are like prototypical users, and then these are sort of recommendations
for prototypical users.
And thereby, if you give me a user, I can look at it and say, how similar is it to these
users that I already know the recommendations for, and then compute the recommendations
accordingly.
And so this is basically what the quantum recommendation system does.
It does all of this in like the quantum superposition world, like it doesn't actually explicitly write
down any of these things, but it's able to do that without writing anything down, and
so it can do it a lot faster than it would be to write anything down.
So I'm curious, maybe we can hit pause on the thesis conversation.
I had several interviews about quantum stuff, and to be honest, I don't really understand
it still.
And so that's probably the case for at least a few people in the audience as well.
I'm curious, in your words, how do you think of this whole quantum computing stuff?
How do you explain it to people?
It's kind of hard because I'm not really that good at quantum computing intuition myself.
So I think the way that I like to think about it at least when I'm like, at least the way
that it's relevant to my research is that it's useful to think of quantum superposition.
If you think of like saying a quantum superposition of a state, usually you can like pretty well
approximate the meaning by replacing like quantum superposition with probability distribution.
So like generally what I like to do is whenever I hear about like quantum computing is just
about setting up these probability distributions in the right way, and these probability distributions
are souped up so that you can have these like interference.
So you can have probabilities cancel each other out.
Yeah.
And basically it seems like a lot of the like exponential type speed ups especially come
from this type of like way of thinking about it.
So roughly we can think of a computer that's made up of not buses and cells and gates
that deal with ones and zeros but rather those similar kinds of things that deal with probability
distributions and weird ones that interact in weird ways.
Right.
So yeah.
If you think about like like, yeah, so it's useful a lot to think of like, okay, like a probability
like a like a quantum superposition of zero and one is essentially just like a probability
distribution like a coin flip picking zero or one each half the time.
And obviously this fails sometimes because whenever you want to like do some cancellation,
you might have a situation where you know technically it's okay, so you have a one
fourth probability of one thing and then you add that to one quarter probability of the
same thing and then you get zero somehow.
But a lot of the techniques that are used in quantum in quantum computing are basically
similar to what similar to their like classical analogs similar to what you would do to manipulate
probability distributions.
And so then back to your thesis, you, there's this recommendation algorithm based on matrix
factorization and you started out by trying to identify what initially we believe that
the the quantum algorithm that they provided that the quantum recommendation system algorithm.
You could prove that it was exponentially faster than any classical algorithm.
And so what this would do is basically say show, hey, we found an example where quantum
can help exponentially in machine learning.
And so this is like a, this is something that people had already believed.
So there was an existing proof that this algorithm couldn't be done in classical computers.
We wanted to show such a proof.
Okay.
Got it.
We wanted to actually show that you could, you could not do the same thing classically.
And people sort of already believed this because of how strong the results looked.
And so this is, this would have been like the, I guess the final nail in the coffin.
But the thing is that, well, I worked on it for a while and I kind of got nowhere.
So like, this is for my undergrad thesis.
I guess I had like, you know, nine ten months to dissolve it and like halfway through I had,
you know, nothing.
I didn't have any approach to allow around the things that I tried didn't seem to work.
And gradually I came to realize that the reason the strategies didn't work was because
the recommendation algorithm can actually be mimicked with a classical algorithm.
What was it that told you that?
There are a couple of factors that led me to realize this.
So, so the first factor was basically that whenever I tried to prove some sort of lower
bound, tried to prove that classical algorithms couldn't do the problem, I would run into
some sort of roadblock.
And then in analyzing what these roadblocks were further, you could basically, I could
basically sort of figure out what were the parts that seemed that the classical algorithm
could do.
So, so it's kind of like a push and pull.
If you want to design a classical algorithm, you have to try to do it without hitting any
roadblocks that because the problems that you come up with are hard, whereas if you're
trying to prove a lower bound, you're trying to show hardness, the roadblocks are actually
things that the, that the, that a classical algorithm would find easy.
Essentially, all of the roadblocks that I found were basically, were indications for
what could be done for a, and a classical algorithm.
And so, so that was one factor, basically just that research into lower bounds gives
you some understanding of like how, where the hardness of the problem is and how you
could avoid it.
And the other factor was I, I early into the research I found this paper by says Alan
Fries, Robbie canon and Santosh Vampala, it's a machine learning type algorithm, basically
it does low rank matrix factorization and it does it in time independent of the input
size.
So, basically what that means is like no matter, no matter the dimension, it can do it
just as fast.
And it does this with some, some assumptions, so it is, namely it assumes that you can
sample somehow from the entries of this matrix, this input matrix.
And the interesting thing is that these input assumptions, these sampling assumptions that
are crucial for the algorithm were also present for the recommendation system, the recommendation
system model.
So you recall that we don't get the matrix just to like read from like we don't get like,
we don't just get like RAM access memory to a random access memory to the matrix, the
matrix that contains like user product preferences.
We also get it in a data structure.
So in order for the quantum algorithm to be able to prepare the superposition that
it wants, it designs a data structure.
And this data structure classically also allows for you to perform the exact type of samples
that you would need to do this fast classical algorithm.
Now just from this algorithm, it's not obvious at all what really to do with it.
It kind of shows that you could do it and then you could factor the matrix and stops there
when in fact for the recommendation system problem, you need to do something with those,
do something with those vectors that you find.
And it's not obvious how you would do that, but it was an indication that said, okay,
you can do some things in this model.
Some of the tasks that you might have thought were hard classically.
You might need to rethink those, those problems.
So a big part of your thesis was coming up with this classical algorithm that was analogous
to this quantum algorithm.
You mentioned that you'd found some prior work that had similar constraints as the algorithm
did in the quantum side.
What did you need to do to get from the prior work to the algorithm that corresponded to
the quantum side?
So it's kind of interesting.
I wasn't familiar with any of this, so this is like in the area of the radical recommendation
system literature.
There's this set of papers that basically talk about how to like theoretically model
recommendation systems and do like that sort of analysis.
So they use the name recommendation system instead of like, I think people use collaborative
filtering more recently, is that that's on right?
But yeah, so there were a set of papers that basically in like the early 2000s to late
90s that basically laid out the entire model for recommendation systems.
And this is the model that is used for my algorithm and for the quantum algorithm as
well.
And they actually came up with something.
It's basically the same thing as what Frie's canon and been polited.
They basically found this prior work and then sort of got stuck because what you have
to do is just to lay out the groundwork.
What we want to do is we want to give a sublinear algorithm for this recommendation system
problem.
And because it is a sublinear algorithm, this means that we can't read all of the matrix
in all of the input in.
And so if you imagine your input as being a set of preferences that users have for particular
products, it even means that we can't list out all of the use that we have or list out
all of the products we have.
We don't have enough time to do that because we want to give like a logarithmic time algorithms.
It's the time that we are allotted is so small that we can't do much.
And the problem with that is that we're dealing with like a linear algebra problem.
And so what this prior work outputs is it outputs the singular vectors of the input matrix.
But it can't output the singular vectors because the vectors are the length of the matrix.
And so you can't do that in enough time.
Does that make sense?
I think so.
Yeah.
Yeah.
So you don't have enough time to output the whole vector.
So you have to describe what the vector looks like in, you have to give what they call
a succinct description.
And so what they do is they can't say, here's the vector that we give as output because
to write out all of that down it takes too long.
And so what you say is you say, look at this set of users and then perform some linear
combination or some like you do some vector addition or something on these users.
And that's going to be your singular vector.
So you've described the vector that you want to output based on the vectors that you were
given.
And that way you can avoid the whole process of giving out each entry one by one.
But that's a problem if you actually want to use this result because now you can't
actually read the vector.
You have to deal with all of this stuff beforehand and that stuff takes a lot of time presumably,
right?
And essentially you run into this issue where you really want to write your vectors down
but you can't because you don't have enough time.
And the quantum algorithm sort of foregoes this issue by just having everything in a quantum
state.
And so in like if you have something in like a quantum superposition or something like
that, you don't have to write anything down.
So that was the main issue from going from the prior work to the recommendation system
algorithm.
And so what ended up happening, what I ended up doing is figuring out, okay, you can avoid
this problem by looking at the quantum, like by taking quantum super, super positions.
And so maybe you can actually do the same thing with the distribution.
So you can avoid the problem of writing everything down by using a probability distribution.
And what I mean by that is basically to say the following, using the fact that we can sample
high-weight entries of basically anything that we want about the input matrix.
So you know, basically I can get the user product preferences that matter most.
I can sample randomly, so I can just pick a random, a bunch of random samples from the matrix,
from the input matrix.
And use that to give a decent approximation of the vectors that I want.
What's interesting is that basically you can do this through the entire process.
So basically once you have this succinct description, you can actually use that for sampling.
So even though I haven't written anything down about the vector, I can actually still
sample from it.
It's basically not obvious, but you can actually do it.
And so once you have these samples, you can sample from the high-weight entries of these
vectors.
You can go the rest of the way.
You can do whatever you want to do.
And for the recommendation system case, basically what you want to do is you want to perform
some projection operation.
You want to project your user vector onto these matrix singular vectors.
And it turns out that you can do that using these sampling strategies.
And that basically gives you the algorithm.
You can do everything without writing anything down.
And so you take poly logarithm at time.
So you take a sublinear amount of time.
The quantum algorithm is, you said invariant with the length.
So it's like big of one kind of time for this recommendation system.
And so you needed to do it in something sublinear.
Right.
So the quantum algorithm is like big of log n, something like that.
So that's like logarithmic time, anything that's like that to some powers, like called
poly logarithmic time.
So what we achieve is we achieve poly logarithmic time.
So we achieve something like log n squared or cubed or something like that.
Okay.
Initially we thought that the quantum algorithm could do log n and the classical algorithm
could do only like O of n.
And so I brought the time down from O of n to O of like log squared n.
So it's like a like exponential improvement.
And is this something that you envision would actually be used or deployed in some recommendation
system?
Or is it more the kind of theoretical implementations on, you know, the what it means for this quantum
algorithm and the theoretical analysis of this classical algorithm?
I mean, I certainly hope it could be useful in the future.
So the thing is like because this algorithm wasn't really intended to be like, it was just
something that I like hacked together and it like because because for my theoretical
purposes, anything that's like even close to the quantum algorithm would have worked.
But I didn't think that hard about trying to optimize everything about the algorithm.
So I'm not sure if it would actually work in practice is something that I still want
to try to figure out.
But I do think that it would be possible for somebody to like clean up the algorithm
and actually get a version that works a lot better and then maybe is competitive with
other techniques.
I think what's done in practice right now is sort of limited by the like the amount of
time that these factorization techniques take.
So I think if you look at like what recommendation systems do in practice, they have to like update
regularly.
So they have to perform the algorithm every week or every, I don't know, I don't know
how often they do it.
But because you have to take so much time out to actually just sit down and compute the
things that you want to compute, it just that if you could speed up that problem, you
could probably help a lot with the flow of the I guess recommendation system.
I guess I guess I'm not the right person to ask about like the classical what it means
for like, you know, practice.
But I can at least say that for the quantum machine learning side of things, it's pretty
interesting that you can actually just simulate all of these things with simulate all of these
quantum algorithms with classical sampling techniques.
And did you a big part of your algorithm is replacing knowing the input vector with the
distribution of the input vector?
Did you characterize at all how that replacement impacts the ultimate result or performance
of the recommendation system?
I guess my thinking is that when you replace the full vector with some distribution, that's
essentially like injecting noise or adding noise to this recommendation system.
Yeah.
And I'm just wondering if you looked at all at what the impact of that noise is on the
performance of the recommendation system theoretically?
Actually, so the noise is quite bad from a theoretical perspective.
So in fact, so the sampling algorithm that I mentioned, the classical sampling algorithm,
the prior work was released in, I believe, 2007.
And since then, there have been a lot of like improvements on this algorithm that basically
say, okay, these sampling assumptions were too weak because they added too much, they
sort of, we can basically do better just by assuming a little bit more.
And so a lot of these more recent algorithms follow the line of, okay, let's assume that
our matrix is a sparser, okay, let's assume that we're able to read our matrix with
some number, some number of times.
So like, we're basically streamed all the matrix entries and then we're supposed to
develop an algorithm out of that.
And so in these models, you can actually do, you can improve your error quite a bit.
Um, it turns out that for the, it turns out that for the recommendation system problem,
a lot of noise is okay, that's also kind of just because we have strong assumptions
for how well, how good our data is.
So even, even way back in like 2002, we had to assume theoretically that our data was
really, really nice in order for us to give any recommendation algorithm.
So we're still kind of operating off of some of these assumptions.
So like if you, if you look at my paper, um, like there are a lot of really weird assumptions
that I have to cover in like the first, like the first 10 pages is to like sort of set
the groundwork and these are things that you could weaken presumably or that you would
have to weaken in practice.
And so I'm not sure how the noise would impact that.
It's not something that it looked at, but it does impact it quite a bit.
Do you have a sense for the ultimate impact on the quantum machine learning kind of community
or our research based on the, the results that you came up with?
So, so there have been a couple of developments since this, uh, this paper actually there.
So since this algorithm was, okay, I call it like de-quantize, basically, I mean, like
you took the quantum out of it, right?
So you, you gave a classical algorithm corresponding to it.
Um, there have been, uh, three more quantum machine learning algorithms that have been
essentially like this, this kind of de-quantized, uh, that have been de-quantized.
So I guess, so I guess to like, um, a bystander looks like quantum machine learning is like
birding to the ground or something.
Like, um, like all of these algorithms are going down one by one, but I think I've talked
to like some quantum machine learning researchers who were like, uh, who basically said, yeah,
um, we didn't really believe that these algorithms were that strong anyway.
So it's, it's good that you're, uh, that you're, um, showing that, you know, like that
these are, um, being de-quantized because, uh, essentially, there is a lot of hype in
quantum machine learning and these types, like, these hyper, hype is spurred by, um, papers
that claim a lot of exponential speedups.
Um, and so, so basically, uh, these classical algorithms, there have been some classical
algorithms that try to say, okay, these exponential speedups seem really hard to achieve in practice
just because these are easy tasks classically anyways.
So you have to, um, basically, you, you should, you, this should be like something stronger
that you can do quantumly or, um, in order to actually get a exponential speedup that's,
uh, I guess, stronger, uh, depending on what your model is, some exponential speedups
are weaker and some are stronger, I guess.
And so we're just, right now, I guess, uh, this, this work is sort of, serves to separate
some of the wheat from the, the shaft, I guess.
Are there currently existing strong use cases for the, these quantum algorithms or, or
strong algorithms, or are they, these, you know, have we, uh, dequantized a bunch of
them and, you know, do we need to now find the, the new strong cases?
I think that question, like, the answer to that question definitely depends on like, it's
like an ongoing topic of discussion.
So I think, um, like, personally, I don't think that there's any quantum of human learning
algorithm that has like a, like a strong theoretical backing for being good, um, for being like,
forgiving exponential speedups, um, but I don't know, maybe that could change depending
on the day.
Uh, there is one algorithm, there, there, there's still hope and that hope is, uh, because
a lot of the algorithms that remain are based on this procedure for matrix inversion.
So you may have heard of this quantum algorithm by Harrow, Hasidim, and Lloyd, that inverts
a, um, a matrix in time poly logarithmic in the dimension.
And this algorithm is actually, it, it's what we call BQP complete, um, so this is like
a complexity class that basically says, if you can de-quantize this algorithm, then you
can de-quantize all of quantum computing, um, and so basically we believe it's hard, uh,
we believe it actually does give exponential speedups in like certain circumstances.
However, the thing is that, uh, the, the situations for which we can devise exponential
speedups are very contrived.
Um, so, you know, if your input data happens to be exactly this matrix, then you can get
an exponential speedup, for example, um, but of course, in practice, you want to be able
to apply this, uh, apply this to just any matrix that you have.
And we know for a fact that, or essentially we, we, we can't do this for any arbitrary
matrix.
We can't get a speedup for any matrix, at least it doesn't seem that way right now.
And so it's kind of like a, like a balancing act.
So you have to try to find like the machine learning problem that's just hard enough to
be out of reach for classical computers, but not too hard so that quantum computers can't
solve them.
So you need to be in like that sweet spot.
And I guess the final thing is that even if we could find such a thing, the work would
still be somewhat speculative because we don't actually know whether we can build quantum
memory.
Uh, so right now we have quantum computers and they can like do circuits and things like
that.
Um, but for a lot of these machine learning algorithms to work, we assume that we have
some really strong, like, I guess, the RAM, they call it quantum RAM, Q RAM.
Uh, we have some really strong RAM that holds all our data and that we can do some like,
we can, we can query to and, um, construct states from quickly.
And so this is, uh, additionally is something that we don't know whether it works in practice.
So, so it works for classical computers, but we're not sure whether it works for quantum
computers.
And this is something that an experimentalist would have to confirm, I guess.
It's all a little bit speculative, but it's, it's interesting to think about all the
different scenarios that could happen, right?
It sounds like it's, uh, you know, considering that you, that so many of these algorithms
have been de-quantized in, uh, just a few months since your paper, uh, was published.
It, it sounds like it's a very fast moving area.
And there's, I didn't realize that the, that when we talk about these quantum algorithms
and quantum computers, that, you know, there's this huge missing piece, which is this quantum
RAM.
I didn't realize that that wasn't part of what we, you know, when we talk about quantum
computers, that that wasn't more of a complete package, but, uh, it sounds like there's
still a ton of work to do in this space.
Right.
It's, it's interesting, actually, like, um, some quantum algorithms don't need memory, if
that makes sense.
So, so, for example, you can, you can, do, you can run short as algorithm without requiring
memory because basically you can, for, for whatever number I give you, you can give
me a circuit or a quantum circuit that basically simulates the RAM.
So, I can construct, you can give me the input and I can construct the RAM quickly.
So, let's say, like, you gave me a number four and you wanted me to construct it.
Well, basically what I can do in the terms of like the, what, what, what the, what we
care about for, like quantum algorithms, what you can do is just devise a circuit that
just outputs four.
It sounds like you're saying that we kind of cheat, right?
Like we, if you, if you have some given number or a vector, you could show that there exists
some quantum circuit, you know, that could serve as a RAM that will return this, that could
store this number, but we don't really have a fully functioning system for getting that
number into memory and then retrieving it back.
Right.
So, what I'm saying is, like, you can hard-coded it.
Yeah.
Yeah.
Yeah.
And the thing is that for algorithms that are faster than like O of N, so they're sublitting
your algorithms, you can't hard-code something like that in because it takes too much time
to read it.
And so, you really need the RAM, whereas for other algorithms, you maybe can get away without
using the RAM.
Well, awesome.
And even thanks so much for taking the time to walk me through what you've done and kind
of the broader implications.
It's, you know, it's, as we just said, it's an interesting space.
And because of that, I keep, I'll keep kind of coming back to it and, you know, banging
my head against it a little bit and see if anything manages to permeate.
But certainly what you've done, you know, you got a lot of publicity for your results
back in the summer and it's really interesting work and I appreciate you taking the time
to walk us through it.
Yeah.
No problem.
Thanks for having me.
All right, everyone.
That's our show for today.
For more information on Ewin or any of the topics covered in this episode, visit twimmelai.com
slash talk slash 246.
As always, thanks so much for listening and catch you next time.
