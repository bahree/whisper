1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:37,720
I'm your host Sam Charrington.

4
00:00:37,720 --> 00:00:43,120
One of them mentioned that one of the things he misses is giveaways and he's right.

5
00:00:43,120 --> 00:00:47,000
It's been way too long since we've done a giveaway here on the show.

6
00:00:47,000 --> 00:00:53,000
So did he know though that we had one in the works that we're excited to kick off today.

7
00:00:53,000 --> 00:00:57,640
The O'Reilly Artificial Intelligence Conference is returning to New York in April and we

8
00:00:57,640 --> 00:01:02,760
have one free conference pass ready to give away to a lucky listener.

9
00:01:02,760 --> 00:01:10,160
To enter, first go to twimbleai.com slash AI and Y giveaway to access the entry form.

10
00:01:10,160 --> 00:01:14,120
Next, choose any or all of the nine ways to enter.

11
00:01:14,120 --> 00:01:18,400
The more entries you earn, the higher your chances to win.

12
00:01:18,400 --> 00:01:24,400
And finally for three bonus entries, answer the question at the bottom of the entry box.

13
00:01:24,400 --> 00:01:29,120
The giveaway ends soon so be sure to get your entries in while you can.

14
00:01:29,120 --> 00:01:33,120
Now about today's show.

15
00:01:33,120 --> 00:01:38,200
We're joined by Kevin Tran, PhD student in the Department of Chemical Engineering at Carnegie

16
00:01:38,200 --> 00:01:40,200
Mellon University.

17
00:01:40,200 --> 00:01:45,640
Kevin's research focuses on creating and using active learning workflows to perform density

18
00:01:45,640 --> 00:01:51,560
functional theory or DFT simulations, which are used to screen for new catalysts for a

19
00:01:51,560 --> 00:01:54,640
myriad of materials applications.

20
00:01:54,640 --> 00:01:59,760
In our conversation, we explore the challenges surrounding one such application, the creation

21
00:01:59,760 --> 00:02:05,520
of renewable energy fuel cells, which is discussed in his recent nature paper, active learning

22
00:02:05,520 --> 00:02:12,840
across intermetallics to guide discovery of electro catalysts for CO2 reduction and H2

23
00:02:12,840 --> 00:02:14,340
evolution.

24
00:02:14,340 --> 00:02:18,820
We dig into the role and need for good catalysts in this application, the role that quanta

25
00:02:18,820 --> 00:02:23,400
mechanics plays in finding them and how Kevin uses machine learning and optimization

26
00:02:23,400 --> 00:02:26,600
to predict electro catalyst performance.

27
00:02:26,600 --> 00:02:30,200
Okay, let's do it.

28
00:02:30,200 --> 00:02:37,320
All right, everyone, I am on the line with Kevin Tran. Kevin is a PhD student in the Department

29
00:02:37,320 --> 00:02:41,200
of Chemical Engineering at Carnegie Mellon University.

30
00:02:41,200 --> 00:02:43,960
Kevin, welcome to this week in machine learning and AI.

31
00:02:43,960 --> 00:02:46,160
Thank you for having me, Sam.

32
00:02:46,160 --> 00:02:51,720
So Kevin, you're in chemical engineering, you've practiced chemical engineering.

33
00:02:51,720 --> 00:02:55,920
How did you come to get involved in the application of machine learning?

34
00:02:55,920 --> 00:03:00,080
I kind of fell into it with the research that I'm doing.

35
00:03:00,080 --> 00:03:06,960
And so we are looking for new materials and we need an intelligent way to screen them.

36
00:03:06,960 --> 00:03:11,240
And machine learning is actually one of the tools that we found that apparently does

37
00:03:11,240 --> 00:03:12,240
that pretty well.

38
00:03:12,240 --> 00:03:16,520
You're using that in your graduate work, have you used machine learning prior to grad

39
00:03:16,520 --> 00:03:17,520
school?

40
00:03:17,520 --> 00:03:18,520
I have not actually.

41
00:03:18,520 --> 00:03:23,480
So everything that we're doing now is kind of learned on the spot, so to say.

42
00:03:23,480 --> 00:03:27,480
And so it's new for us and we start a lot of room for improvement, but so far it's

43
00:03:27,480 --> 00:03:28,480
working pretty well.

44
00:03:28,480 --> 00:03:29,480
Awesome.

45
00:03:29,480 --> 00:03:33,880
So tell us a little bit about the problem that you're trying to solve there.

46
00:03:33,880 --> 00:03:39,760
Yeah, so at a high level, it's really about sustainable energy, right?

47
00:03:39,760 --> 00:03:43,320
And that's actually the reason that I started doing this research.

48
00:03:43,320 --> 00:03:50,120
And so the idea is to help make what we call solar fuels and solar chemicals.

49
00:03:50,120 --> 00:03:54,080
So what that means is we take energy dry from the sun.

50
00:03:54,080 --> 00:03:56,800
So let's say solar cells.

51
00:03:56,800 --> 00:04:01,680
And one of the problems right now is that when we take solar cells and make electricity

52
00:04:01,680 --> 00:04:07,480
out of that, if we have a lot generated during the day and we can't use it, then it's kind

53
00:04:07,480 --> 00:04:08,480
of wasted, right?

54
00:04:08,480 --> 00:04:12,520
So there are a lot of people doing really nice research into figuring out how to store

55
00:04:12,520 --> 00:04:14,480
that energy.

56
00:04:14,480 --> 00:04:20,720
So the reason is one method, the method that we are interested in is storing it in chemical

57
00:04:20,720 --> 00:04:21,720
bonds.

58
00:04:21,720 --> 00:04:28,000
So what we would do is we would take something like carbon dioxide and water.

59
00:04:28,000 --> 00:04:32,240
Take that and the electricity we get from the sun and convert that into more valuable

60
00:04:32,240 --> 00:04:36,960
fuels or chemicals like methane or ethylene or hydrogen.

61
00:04:36,960 --> 00:04:41,800
And then in turn, we can use those for whatever application that we want.

62
00:04:41,800 --> 00:04:48,880
And so part of that problem to actually turn those chemicals into more valuable chemicals

63
00:04:48,880 --> 00:04:52,280
is that we need good catalyst to do so.

64
00:04:52,280 --> 00:04:58,600
And right now, our research focuses on finding good catalyst to do those reactions.

65
00:04:58,600 --> 00:05:03,880
And it involves a lot of computer simulations that we have to do intelligently.

66
00:05:03,880 --> 00:05:08,640
And so we're using machine learning to help us decide which simulations to do it early.

67
00:05:08,640 --> 00:05:15,080
OK, so to dig into that a little bit more, you're trying to store solar energy and chemical

68
00:05:15,080 --> 00:05:16,080
bonds.

69
00:05:16,080 --> 00:05:19,120
And some ways that sounds a little bit like what a battery is doing also.

70
00:05:19,120 --> 00:05:22,440
Yeah, it's fundamentally a chemical thing.

71
00:05:22,440 --> 00:05:29,640
Now, or you could, you know, while the sun is up, use the solar energy to kind of power

72
00:05:29,640 --> 00:05:37,960
some chemical reaction and that's kind of where your need for some kind of catalyst comes

73
00:05:37,960 --> 00:05:38,960
into play.

74
00:05:38,960 --> 00:05:40,720
Yeah, yeah, exactly.

75
00:05:40,720 --> 00:05:46,800
So the setup we have actually looks very similar to a battery, right, or the physical setups.

76
00:05:46,800 --> 00:05:54,400
And so if you think of a car battery, right, it has a solution and it has an anode and

77
00:05:54,400 --> 00:05:58,480
a cathode as in more or less two metal rods on either end.

78
00:05:58,480 --> 00:06:04,680
And it can either generate electricity or it can, you can put energy electricity into

79
00:06:04,680 --> 00:06:10,280
it to store the energy in more or less chemical form.

80
00:06:10,280 --> 00:06:16,640
And so our system, what it does is instead of storing the energy in the battery itself,

81
00:06:16,640 --> 00:06:23,120
it generates methane and fuels from carbon dioxide and the water that is in the battery itself

82
00:06:23,120 --> 00:06:26,200
or in this case, it's the electrical chemical cell.

83
00:06:26,200 --> 00:06:29,360
You just described it as an electrical chemical cell.

84
00:06:29,360 --> 00:06:30,360
Yes.

85
00:06:30,360 --> 00:06:36,720
And so it's not like you could use that energy to power some kind of machine to, you know,

86
00:06:36,720 --> 00:06:42,080
extract some other compound or fuel, but rather you're doing this all chemically.

87
00:06:42,080 --> 00:06:43,080
Yes.

88
00:06:43,080 --> 00:06:44,080
Yep.

89
00:06:44,080 --> 00:06:45,080
That's the idea.

90
00:06:45,080 --> 00:06:49,160
The issue there is in order to perform that reaction and a large scale that could help

91
00:06:49,160 --> 00:06:50,760
a lot of people.

92
00:06:50,760 --> 00:06:54,800
We needed to be fast and efficient and affordable.

93
00:06:54,800 --> 00:06:59,040
And the catalyst that we have right now can meet some of those criteria, but not a lot

94
00:06:59,040 --> 00:07:00,040
of them.

95
00:07:00,040 --> 00:07:04,760
And so we're looking for more materials that we can scale up to a commercial scale.

96
00:07:04,760 --> 00:07:06,720
And that's the problem we have right now.

97
00:07:06,720 --> 00:07:10,160
In order to do this, you need these catalysts.

98
00:07:10,160 --> 00:07:14,040
How do you measure the performance of one of these catalysts?

99
00:07:14,040 --> 00:07:15,040
Yes.

100
00:07:15,040 --> 00:07:20,760
So there are a lot of metrics for performance that experimentals usually look at, one of

101
00:07:20,760 --> 00:07:26,160
which we call activity, but the idea is how fast it can actually drive the reaction.

102
00:07:26,160 --> 00:07:34,720
So let's say one catalyst can transform one kilojoule of energy and it would take maybe

103
00:07:34,720 --> 00:07:37,920
a few seconds and another catalyst, it could take hours.

104
00:07:37,920 --> 00:07:42,600
So we want the catalyst that could take a few seconds to actually do that reaction.

105
00:07:42,600 --> 00:07:46,360
Another example is the efficiency.

106
00:07:46,360 --> 00:07:51,400
So just because you put one kilojoule of energy in does not mean you get one kilojoule out.

107
00:07:51,400 --> 00:07:52,920
It's often much less than that.

108
00:07:52,920 --> 00:07:59,240
And so certain catalysts are more efficient about transferring that energy than others.

109
00:07:59,240 --> 00:08:03,560
And there's often a balance between those two and even all the properties such as how

110
00:08:03,560 --> 00:08:05,640
expensive the material is, right?

111
00:08:05,640 --> 00:08:09,400
And so there's a lot of things that we plan to play with and look at.

112
00:08:09,400 --> 00:08:13,320
But the work we have right now, since we're just starting out, only looks at the first

113
00:08:13,320 --> 00:08:17,080
thing that I mentioned, which is how fast the reaction can go.

114
00:08:17,080 --> 00:08:20,280
What's the space of possible catalysts look like?

115
00:08:20,280 --> 00:08:21,680
It's a good question.

116
00:08:21,680 --> 00:08:27,320
So one of the issues that the field has right now is that in order to solve this problem,

117
00:08:27,320 --> 00:08:30,800
we're more or less looking at most of the periodic table, right?

118
00:08:30,800 --> 00:08:35,200
And so there are a lot of elements there, a lot of iterations that go through.

119
00:08:35,200 --> 00:08:40,120
And even if you choose, let's say, two metals, they stole the question of what ratio do

120
00:08:40,120 --> 00:08:42,400
you have between those two metals, right?

121
00:08:42,400 --> 00:08:45,160
What if you have three metals, what ratios do you have there?

122
00:08:45,160 --> 00:08:52,280
And so what we have is a really rough first pass of looking at two or three metal common

123
00:08:52,280 --> 00:08:58,240
issues, sometimes four, for about 30 different elements on the periodic table for select

124
00:08:58,240 --> 00:09:01,600
combinations of ratios between them.

125
00:09:01,600 --> 00:09:05,200
And so in that sense, it sounds kind of like a constrained optimization type of a problem.

126
00:09:05,200 --> 00:09:11,920
You're trying to figure out how to get the greatest yield from this set of materials that

127
00:09:11,920 --> 00:09:12,920
you're working with.

128
00:09:12,920 --> 00:09:14,560
Yeah, exactly.

129
00:09:14,560 --> 00:09:20,800
And it's kind of a difficult problem because the search space explodes geometrically as

130
00:09:20,800 --> 00:09:23,200
you add more materials, right?

131
00:09:23,200 --> 00:09:26,240
And more composition blends.

132
00:09:26,240 --> 00:09:30,800
And so what we have is even though it feels large and seems large to us, it's really the

133
00:09:30,800 --> 00:09:33,800
tip of the iceberg of what we could do, right?

134
00:09:33,800 --> 00:09:37,040
At what level are you modeling this?

135
00:09:37,040 --> 00:09:42,640
Like are you modeling this at and forgive my ignorance here if I'm not asking this correctly?

136
00:09:42,640 --> 00:09:48,920
And are you kind of when you're searching for high yield catalysts?

137
00:09:48,920 --> 00:09:55,480
Are you modeling atomic interactions, subatomic interactions and molecular interactions?

138
00:09:55,480 --> 00:09:59,320
Or could you do all of this, you know, what you need at the level of, you know, just kind

139
00:09:59,320 --> 00:10:01,880
of the things you might see on a periodic table?

140
00:10:01,880 --> 00:10:02,880
Yeah, yeah.

141
00:10:02,880 --> 00:10:09,320
So what we are doing specifically is modeling things at the atomic level.

142
00:10:09,320 --> 00:10:16,080
And so there's a type of theory in chemistry, it's called density functional theory.

143
00:10:16,080 --> 00:10:22,640
So what this does, it more or less uses quantum mechanics to take a set of atoms and predict

144
00:10:22,640 --> 00:10:25,480
the properties of those atoms.

145
00:10:25,480 --> 00:10:31,560
And using those properties, those are actually indicative of the performance of the catalyst.

146
00:10:31,560 --> 00:10:37,920
And so we take a catalyst, we perform destiny functional theory simulations really around

147
00:10:37,920 --> 00:10:39,560
how the atoms interact with each other.

148
00:10:39,560 --> 00:10:45,480
And that will tell us if that catalyst or those atoms really will perform well in electrochemical.

149
00:10:45,480 --> 00:10:46,480
So.

150
00:10:46,480 --> 00:10:50,280
And so how long do the simulations take to run?

151
00:10:50,280 --> 00:10:55,240
The issue here is that these simulations take anywhere from hours to days or the big

152
00:10:55,240 --> 00:10:58,120
ones sometimes even weeks to run a single one.

153
00:10:58,120 --> 00:10:59,120
Oh, wow.

154
00:10:59,120 --> 00:11:04,360
Can you give us a high level understanding of how these DFT simulations work?

155
00:11:04,360 --> 00:11:10,280
What are they doing to determine ultimately whether these catalysts are going to be high

156
00:11:10,280 --> 00:11:11,280
yield?

157
00:11:11,280 --> 00:11:12,280
Yeah, yeah.

158
00:11:12,280 --> 00:11:17,720
So to answer that, we can back up a second to talk about how the catalysts work.

159
00:11:17,720 --> 00:11:23,040
So let's take for example, we want to turn carbon dioxide into methane, right?

160
00:11:23,040 --> 00:11:27,120
So it's CO2 going to CH4.

161
00:11:27,120 --> 00:11:28,280
That doesn't happen in one step.

162
00:11:28,280 --> 00:11:36,400
That happens in a series of chemical reactions and we know from chemistry experts that the

163
00:11:36,400 --> 00:11:40,160
elementary reaction we call it, they're a smaller reaction that matters most to convert

164
00:11:40,160 --> 00:11:48,120
CO2 to methane is actually adding one hydrogen onto carbon monoxide.

165
00:11:48,120 --> 00:11:53,480
And so what that means is how strongly that carbon monoxide binds onto the catalyst is

166
00:11:53,480 --> 00:11:54,720
really important.

167
00:11:54,720 --> 00:12:00,080
So let's say carbon monoxide binds onto the catalyst very strongly.

168
00:12:00,080 --> 00:12:01,800
So it will react quickly.

169
00:12:01,800 --> 00:12:06,040
But once it's done reacting, it'll actually just stay there, never come off and you will

170
00:12:06,040 --> 00:12:08,320
never actually have any product.

171
00:12:08,320 --> 00:12:13,360
Conversely, if carbon monoxide binds to weekly, it will never go on to the catalyst in

172
00:12:13,360 --> 00:12:16,640
the first place and therefore it will never react.

173
00:12:16,640 --> 00:12:21,120
So there's kind of a Goldilocks effect where the catalyst needs to have a sweet spot of

174
00:12:21,120 --> 00:12:22,800
energy in the middle.

175
00:12:22,800 --> 00:12:25,360
So that's where density functional theory comes in.

176
00:12:25,360 --> 00:12:27,200
We call it DFT.

177
00:12:27,200 --> 00:12:33,320
And so given a configuration of atoms and how the carbon monoxide is sitting on a surface

178
00:12:33,320 --> 00:12:40,480
of a catalyst, DFT can tell us how strongly that carbon monoxide is sticking to the surface.

179
00:12:40,480 --> 00:12:43,240
And that is indicative of how well it's going to perform.

180
00:12:43,240 --> 00:12:50,280
Presumably, you're using a simulator because the laws that govern this are either too

181
00:12:50,280 --> 00:12:59,120
ill-defined to apply directly or are at too granular a level to apply directly.

182
00:12:59,120 --> 00:13:00,120
That's interesting.

183
00:13:00,120 --> 00:13:05,880
So when we call them simulations, we just say that because it's what it feels like.

184
00:13:05,880 --> 00:13:10,680
But what we're actually using is quantum mechanics and so we know the laws that govern how everything

185
00:13:10,680 --> 00:13:11,680
interacts.

186
00:13:11,680 --> 00:13:17,000
The reason that takes so long is because it's really just a math problem solving a lot

187
00:13:17,000 --> 00:13:20,080
of partial differential equations simultaneously.

188
00:13:20,080 --> 00:13:22,320
And that just takes a really long time.

189
00:13:22,320 --> 00:13:29,640
You recently published a paper on your work in this field in nature, nature catalysis.

190
00:13:29,640 --> 00:13:33,880
And you're using active learning to help solve this problem.

191
00:13:33,880 --> 00:13:36,320
Where does active learning come into play?

192
00:13:36,320 --> 00:13:37,320
Yeah.

193
00:13:37,320 --> 00:13:41,360
So we have a set of elements that we want to look at, right?

194
00:13:41,360 --> 00:13:45,880
And they can have different varying composition space as we thought about before.

195
00:13:45,880 --> 00:13:52,360
And we want to actually keep running simulations to keep finding new materials for experimental

196
00:13:52,360 --> 00:13:54,440
list to test out.

197
00:13:54,440 --> 00:13:59,000
So our whole workflow is designed around the idea of we want to continuously find new

198
00:13:59,000 --> 00:14:01,760
materials for other people to look at.

199
00:14:01,760 --> 00:14:06,560
So that's where the active part comes in because it feels almost in there, right?

200
00:14:06,560 --> 00:14:12,000
So the more simulations we have, the more data we have, and the more data we have, the

201
00:14:12,000 --> 00:14:17,520
more we get a better idea of how to what sites to perform next.

202
00:14:17,520 --> 00:14:24,160
So in this case, what we're doing is, let's say we start out with a small database of a

203
00:14:24,160 --> 00:14:26,400
few hundred simulations.

204
00:14:26,400 --> 00:14:32,240
We use machine learning on those as a training set.

205
00:14:32,240 --> 00:14:37,840
And from there we can predict how well the other catalyst that we have not simulated are,

206
00:14:37,840 --> 00:14:39,840
how well they will perform.

207
00:14:39,840 --> 00:14:44,960
And once we have an idea of that, we can pick the ones that we think will perform well

208
00:14:44,960 --> 00:14:50,400
as per the machine learning model, and actually just do the simulations, get more data,

209
00:14:50,400 --> 00:14:53,520
and then perform their questions again, and just continue that loop.

210
00:14:53,520 --> 00:14:57,760
And that's how we're using active learning in a sense to find new materials.

211
00:14:57,760 --> 00:15:03,560
You've got this database of materials that you've run these simulations on, and they're

212
00:15:03,560 --> 00:15:08,080
characterized by their different kind of material properties.

213
00:15:08,080 --> 00:15:12,240
Well, actually, maybe you can talk a little bit about how they're characterized.

214
00:15:12,240 --> 00:15:16,720
Are they, you know, is it strictly at the atomic level or are there other material

215
00:15:16,720 --> 00:15:21,480
properties that you're using to ultimately become features in your machine learning

216
00:15:21,480 --> 00:15:22,480
models?

217
00:15:22,480 --> 00:15:31,240
Yeah, so what we do is we look at the location of where the carbon monoxide is sitting

218
00:15:31,240 --> 00:15:32,800
on the surface.

219
00:15:32,800 --> 00:15:37,880
So if you can imagine how the atoms are set up, the carbon monoxide could be bonded to,

220
00:15:37,880 --> 00:15:44,080
let's say, one plinum atom on a surface, and that plinum atom could be bound to maybe

221
00:15:44,080 --> 00:15:46,000
eight other plinum atoms.

222
00:15:46,000 --> 00:15:54,080
So what we feed as features to our aggression model is how many atoms that the carbon monoxide

223
00:15:54,080 --> 00:15:55,080
is bound to?

224
00:15:55,080 --> 00:15:57,440
So in this case, it'll be one plinum.

225
00:15:57,440 --> 00:16:03,240
How many atoms that are in the next nearest neighbor or the next outer shell?

226
00:16:03,240 --> 00:16:06,120
So in this case, it'll be eight plinums.

227
00:16:06,120 --> 00:16:09,400
We also look at the identity, the chemical identity of those atoms.

228
00:16:09,400 --> 00:16:11,080
So in this case, it'll be platinum.

229
00:16:11,080 --> 00:16:16,600
If there's more than one metal, we consider, let's say, this plinum and aluminum.

230
00:16:16,600 --> 00:16:18,240
We consider the aluminum too.

231
00:16:18,240 --> 00:16:20,760
We also look at the chemical properties of those elements.

232
00:16:20,760 --> 00:16:26,880
So platinum and aluminum have what we call electronegativity, which is how much they

233
00:16:26,880 --> 00:16:28,760
like electrons.

234
00:16:28,760 --> 00:16:30,440
And we look at that.

235
00:16:30,440 --> 00:16:34,720
And we also look at what we call the atomic number.

236
00:16:34,720 --> 00:16:37,640
So where it is on the periodic table.

237
00:16:37,640 --> 00:16:44,880
And one of the interesting features that we have is the average binding energy of that

238
00:16:44,880 --> 00:16:47,160
absorbate on the material.

239
00:16:47,160 --> 00:16:52,960
So in this sense, let's say we already have a database of maybe 20 calculations of platinum.

240
00:16:52,960 --> 00:16:56,880
And we take an average of those binding energies.

241
00:16:56,880 --> 00:16:59,400
And that number is associated with platinum.

242
00:16:59,400 --> 00:17:03,200
It kind of gives a feel for how strongly it binds the palatine in general.

243
00:17:03,200 --> 00:17:07,200
We extend, we take that number and we feed it into our alloys.

244
00:17:07,200 --> 00:17:13,640
And so when we have a material that has aluminum platinum, it has a feel for how strongly

245
00:17:13,640 --> 00:17:15,440
it binds the platinum and aluminum.

246
00:17:15,440 --> 00:17:19,040
And it sort of averages those together in a way when we do the regression.

247
00:17:19,040 --> 00:17:20,040
Okay.

248
00:17:20,040 --> 00:17:24,520
And that's that number that you described earlier is wanting to have that Goldilocks effect.

249
00:17:24,520 --> 00:17:26,720
That one needs to be kind of just right.

250
00:17:26,720 --> 00:17:27,720
Exactly.

251
00:17:27,720 --> 00:17:32,680
And so you've got all of these as features that come out of the materials that you're

252
00:17:32,680 --> 00:17:42,040
using, the simulations themselves are those only used as your, your labels, your answers

253
00:17:42,040 --> 00:17:47,000
or are you also gaining information from those simulations that you're using as feature

254
00:17:47,000 --> 00:17:49,520
input?

255
00:17:49,520 --> 00:17:57,720
So the simulations that we're performing effectively give us the labels for our regression models.

256
00:17:57,720 --> 00:18:05,400
At the same time, the, so those labels are the binding energy of carbon dioxide on a

257
00:18:05,400 --> 00:18:12,800
particular site on a catalyst, a site being a location where the carbon dioxide could

258
00:18:12,800 --> 00:18:15,520
just attach onto the catalyst.

259
00:18:15,520 --> 00:18:20,720
I was mainly curious whether the simulation was just giving you that, you know, that answer

260
00:18:20,720 --> 00:18:27,280
that binding energy that you use as labels or if the simulation was also kind of illuminating

261
00:18:27,280 --> 00:18:35,440
other characteristics of the, the material pairs or combinations, you know, they're qualitatively

262
00:18:35,440 --> 00:18:40,320
or quantitatively that you could also use as, you know, input signal.

263
00:18:40,320 --> 00:18:41,320
Sure.

264
00:18:41,320 --> 00:18:44,520
So the simulations themselves really just give us the labels.

265
00:18:44,520 --> 00:18:50,600
If we want to look for more holistic information around what materials might work well, we actually

266
00:18:50,600 --> 00:18:53,320
just look at our database of simulations.

267
00:18:53,320 --> 00:18:59,040
So I guess for each individual one, we don't really clean much chemical information from

268
00:18:59,040 --> 00:19:05,880
that for from seeing patterns arise in how many simulations work well given a certain

269
00:19:05,880 --> 00:19:09,560
alloy, we clean information from there.

270
00:19:09,560 --> 00:19:16,560
You've got this regression model that essentially allows you to take unknown combinations of

271
00:19:16,560 --> 00:19:24,240
materials and guess what the binding energy will be without running through these really

272
00:19:24,240 --> 00:19:29,280
long simulations, is that that kind of the general goal there?

273
00:19:29,280 --> 00:19:30,280
Yeah.

274
00:19:30,280 --> 00:19:31,280
Yeah.

275
00:19:31,280 --> 00:19:32,280
Exactly.

276
00:19:32,280 --> 00:19:33,880
So that's what our surrogate model is doing, our machine learning model.

277
00:19:33,880 --> 00:19:34,880
Yep.

278
00:19:34,880 --> 00:19:41,400
And then you're using that to allow you to kind of constrain the search space for new elements.

279
00:19:41,400 --> 00:19:46,760
How do you tie that surrogate model into the active learning piece or the piece that you're

280
00:19:46,760 --> 00:19:49,360
using to constrain your search space?

281
00:19:49,360 --> 00:19:50,360
Yeah.

282
00:19:50,360 --> 00:19:57,600
So the very naive way to do this is to take the model, evaluate across our entire search

283
00:19:57,600 --> 00:20:04,000
space and pick the materials and the adoption sites that actually would work best.

284
00:20:04,000 --> 00:20:05,000
But there's the idea of.

285
00:20:05,000 --> 00:20:10,760
And if I can interrupt that, sure, should even be an improvement over running these simulations

286
00:20:10,760 --> 00:20:11,760
every time.

287
00:20:11,760 --> 00:20:12,760
Right?

288
00:20:12,760 --> 00:20:13,760
Yeah.

289
00:20:13,760 --> 00:20:16,840
It's an intelligent way of doing the simulations, right?

290
00:20:16,840 --> 00:20:21,480
So what people do in the current field is they use their intuition to decide what

291
00:20:21,480 --> 00:20:23,720
materials to simulate next.

292
00:20:23,720 --> 00:20:24,720
We're here.

293
00:20:24,720 --> 00:20:31,280
We're actually using machine learning predictions to make that decision instead of human intuition.

294
00:20:31,280 --> 00:20:36,400
How well do you trust the surrogate model when you run it and it tells you that something

295
00:20:36,400 --> 00:20:40,200
should have this kind of just right bonding energy?

296
00:20:40,200 --> 00:20:45,600
Do you trust it or do you use that to determine what you should actually simulate?

297
00:20:45,600 --> 00:20:47,240
So that's a really good question.

298
00:20:47,240 --> 00:20:51,320
At the current state of the model, I trust it with a grain of salt, right?

299
00:20:51,320 --> 00:20:52,920
Does that make sense?

300
00:20:52,920 --> 00:20:57,160
But what I really trust are the simulations.

301
00:20:57,160 --> 00:21:02,760
And so the whole point of our workflow isn't necessarily to make machine learning model

302
00:21:02,760 --> 00:21:05,200
that can predict everything perfectly.

303
00:21:05,200 --> 00:21:10,280
This is really to have the machine learning model decide what simulations to run next,

304
00:21:10,280 --> 00:21:13,640
which will build our database in an intelligent way.

305
00:21:13,640 --> 00:21:14,640
Right.

306
00:21:14,640 --> 00:21:15,640
Right.

307
00:21:15,640 --> 00:21:21,120
And so you were starting to walk through that process and how you applied the machine learning

308
00:21:21,120 --> 00:21:22,880
given your surrogate model.

309
00:21:22,880 --> 00:21:23,880
Yeah.

310
00:21:23,880 --> 00:21:24,880
Yeah.

311
00:21:24,880 --> 00:21:28,560
And so I think we discussed the naive way to do it, which is to use the model to find the

312
00:21:28,560 --> 00:21:31,880
best candidate and to just run that.

313
00:21:31,880 --> 00:21:36,880
But if you do that, you run in, there's the idea of exploration versus exploitation.

314
00:21:36,880 --> 00:21:38,320
I'm not sure if you're familiar with.

315
00:21:38,320 --> 00:21:39,320
Mm-hmm.

316
00:21:39,320 --> 00:21:40,760
Yeah, we talk about that quite a bit on the show.

317
00:21:40,760 --> 00:21:41,760
Yeah, yeah.

318
00:21:41,760 --> 00:21:43,160
And so that's pure exploitation.

319
00:21:43,160 --> 00:21:49,520
And so there's a decent amount of research into ways to balance the two.

320
00:21:49,520 --> 00:21:56,600
For our workflow, we had a quick and dirty way to try and balance this where we actually

321
00:21:56,600 --> 00:22:01,800
made a Gaussian distribution that is centered at our optimal point, right?

322
00:22:01,800 --> 00:22:07,920
So let's say our optimal point is maybe 0.1 for the binding energy.

323
00:22:07,920 --> 00:22:12,960
So we made a Gaussian distribution centered there with a certain standard deviation.

324
00:22:12,960 --> 00:22:19,640
And for our search space, we used that distribution to assign a probability of how of selecting

325
00:22:19,640 --> 00:22:22,680
a new candidate material to simulate.

326
00:22:22,680 --> 00:22:28,320
So let's say if something was at 0.1 or the model thought it was exactly at 0.1 or optimal,

327
00:22:28,320 --> 00:22:32,000
then it would assign the highest probability to picking that.

328
00:22:32,000 --> 00:22:35,720
And the further out you go from that space, the lower the probability gets.

329
00:22:35,720 --> 00:22:41,160
And so once we assign an array of probabilities to our possible search space, then we just

330
00:22:41,160 --> 00:22:43,000
picked them at random.

331
00:22:43,000 --> 00:22:47,720
So in a way, we would focus on the area that we're interested in.

332
00:22:47,720 --> 00:22:52,720
But still, in a way, stochastically choose other materials that we might not normally

333
00:22:52,720 --> 00:22:53,720
choose.

334
00:22:53,720 --> 00:22:58,880
Just retaining some of that explore characteristic.

335
00:22:58,880 --> 00:22:59,880
Exactly.

336
00:22:59,880 --> 00:23:00,880
Yeah.

337
00:23:00,880 --> 00:23:01,880
Okay.

338
00:23:01,880 --> 00:23:05,120
And then the active learning piece of this is what specifically?

339
00:23:05,120 --> 00:23:13,000
So the active learning piece is the process of iteratively deciding new points to simulate

340
00:23:13,000 --> 00:23:14,480
for us, right?

341
00:23:14,480 --> 00:23:17,160
And so we have this loop constantly going.

342
00:23:17,160 --> 00:23:24,080
So the active part is the fact that we're doing a regression and then a prediction and

343
00:23:24,080 --> 00:23:29,680
then a query or simulation, in our case, to get us more data, which yields another

344
00:23:29,680 --> 00:23:31,320
regression, et cetera, et cetera.

345
00:23:31,320 --> 00:23:35,160
So the active part is the iterative nature of our workflow.

346
00:23:35,160 --> 00:23:40,160
So you're running this kind of in serial.

347
00:23:40,160 --> 00:23:50,600
And then each time you produce this distribution of possible materials to simulate based on

348
00:23:50,600 --> 00:23:55,240
the tests that you've done previously, you pick one, you run that.

349
00:23:55,240 --> 00:24:01,240
And then you, based on the result, you put that in your distribution, you pick another

350
00:24:01,240 --> 00:24:09,280
one, you run that and you keep feeding these results back into your database, your distribution

351
00:24:09,280 --> 00:24:10,880
of things to try.

352
00:24:10,880 --> 00:24:11,880
Exactly.

353
00:24:11,880 --> 00:24:12,880
Exactly.

354
00:24:12,880 --> 00:24:13,880
Very cool.

355
00:24:13,880 --> 00:24:14,880
Very cool.

356
00:24:14,880 --> 00:24:20,520
And so how do you characterize the results that you saw with the paper?

357
00:24:20,520 --> 00:24:21,520
Yeah.

358
00:24:21,520 --> 00:24:25,240
And so that's an interesting question because we've been playing with around with different

359
00:24:25,240 --> 00:24:29,200
methods to actually analyze our database.

360
00:24:29,200 --> 00:24:35,280
So we, our main three methods that we thought of, we actually put inside the paper, one

361
00:24:35,280 --> 00:24:44,360
is to just make a list of materials that are within an acceptable range of performance

362
00:24:44,360 --> 00:24:45,560
and give that to experimentalists.

363
00:24:45,560 --> 00:24:48,360
And so we have that in the paper.

364
00:24:48,360 --> 00:24:55,360
Another way is to create a, almost a heat map, a two-dimensional heat map of what materials

365
00:24:55,360 --> 00:24:56,640
would work well.

366
00:24:56,640 --> 00:24:59,320
So on one axis would be one set of elements.

367
00:24:59,320 --> 00:25:04,600
So Luminon, Platinum, Gold, Copper, and the other axis is those same elements.

368
00:25:04,600 --> 00:25:09,680
And so each point would be a blend of those elements.

369
00:25:09,680 --> 00:25:13,640
So the grid would be maybe choose Copper and Luminon.

370
00:25:13,640 --> 00:25:22,480
And from there we can color code the grid according to the fraction of the binding sites that

371
00:25:22,480 --> 00:25:23,760
would work well.

372
00:25:23,760 --> 00:25:26,520
And so that was another way that we could look at it.

373
00:25:26,520 --> 00:25:32,440
And the third way is actually simply using Teasney to cluster all of the points that

374
00:25:32,440 --> 00:25:38,600
we've done simulations on, and then color code them by their energy and look for clusters

375
00:25:38,600 --> 00:25:44,240
that have the appropriate energy for us and look for the themes within those clusters

376
00:25:44,240 --> 00:25:47,560
and recommend that those materials be studied.

377
00:25:47,560 --> 00:25:54,280
And so you've got these three methods, how do you know that this method is helping you?

378
00:25:54,280 --> 00:26:02,400
Are you finding, are you able to, you know, ultimately is it the number of kind

379
00:26:02,400 --> 00:26:08,760
of candidate combinations that you're able to find per unit of your own time as a researcher

380
00:26:08,760 --> 00:26:12,200
or is there some other kind of fundamental measure here?

381
00:26:12,200 --> 00:26:14,600
Yeah, that's a good question.

382
00:26:14,600 --> 00:26:20,520
The two that I kind of lean towards are the number of candidates we find over time.

383
00:26:20,520 --> 00:26:23,640
So we actually have a plot of that in the paper as well.

384
00:26:23,640 --> 00:26:28,240
And over time we can see that number rising and when I was getting a weaning according

385
00:26:28,240 --> 00:26:32,080
to the things the way we modify our workflow.

386
00:26:32,080 --> 00:26:37,640
The other way is really, I guess the real way is to see if any of the candidates we have

387
00:26:37,640 --> 00:26:42,080
actually work and in an experimental setup.

388
00:26:42,080 --> 00:26:49,240
And so for that case, experimental validation, we do have collaborators that are testing

389
00:26:49,240 --> 00:26:55,280
some of these materials and we actually have another paper in review right now where

390
00:26:55,280 --> 00:27:01,880
our collaborators tested one of the things that we've suspected to be performing well.

391
00:27:01,880 --> 00:27:02,880
And it turns out it did.

392
00:27:02,880 --> 00:27:05,320
And so that worked out perfectly in our minds.

393
00:27:05,320 --> 00:27:07,640
And so we're really excited to get that paper out.

394
00:27:07,640 --> 00:27:11,160
And you might be seeing that in the next few months or so.

395
00:27:11,160 --> 00:27:12,160
Nice.

396
00:27:12,160 --> 00:27:15,720
And it doesn't sound like it would be enough to just demonstrate one.

397
00:27:15,720 --> 00:27:21,000
There's some notion of kind of demonstrating that this is a, you know, more efficient or

398
00:27:21,000 --> 00:27:27,120
more repeatable process than, you know, what folks usually do.

399
00:27:27,120 --> 00:27:34,560
And that goes back to have you have you produced enough candidates and had those go through

400
00:27:34,560 --> 00:27:39,520
the experimental process in order to be able to make claims around that.

401
00:27:39,520 --> 00:27:40,520
Yeah.

402
00:27:40,520 --> 00:27:44,920
So this is the candidates that we found so far.

403
00:27:44,920 --> 00:27:46,720
We found them relatively recently.

404
00:27:46,720 --> 00:27:52,920
So our, we are still in the process of testing them with experimental collaborators.

405
00:27:52,920 --> 00:27:58,240
One thing that does make us feel better about our workflow is that I'd say on the order

406
00:27:58,240 --> 00:28:03,760
of 40 to 60% of the candidates that we have found have already been shown in the literature

407
00:28:03,760 --> 00:28:10,200
to work well for what we're looking for, but ended up not being used for other reasons

408
00:28:10,200 --> 00:28:11,680
that we're not looking at right now.

409
00:28:11,680 --> 00:28:15,400
And so that kind of gives us a gut check to say that, hey, the things that our workflow

410
00:28:15,400 --> 00:28:18,880
are finding are things that people have already looked at.

411
00:28:18,880 --> 00:28:24,520
And so hopefully the things that we haven't experimentally tested yet might be good candidates

412
00:28:24,520 --> 00:28:25,520
too.

413
00:28:25,520 --> 00:28:26,520
Oh, that's right.

414
00:28:26,520 --> 00:28:32,880
Because you're only looking at one of many important categories that you need to, or

415
00:28:32,880 --> 00:28:37,840
properties that you need to consider to actually commercialize something.

416
00:28:37,840 --> 00:28:46,640
So your workflow is spinning out candidates that according to your criteria are good ones.

417
00:28:46,640 --> 00:28:51,440
But you know, may not, you know, may have already been proven to be not commercially viable

418
00:28:51,440 --> 00:28:53,480
for other reasons.

419
00:28:53,480 --> 00:28:54,480
Exactly.

420
00:28:54,480 --> 00:28:55,480
Exactly.

421
00:28:55,480 --> 00:28:58,720
And so that's where some of our research might be going in the future to start looking

422
00:28:58,720 --> 00:29:03,600
at those other properties as well so that we can really trim down our list and reduce

423
00:29:03,600 --> 00:29:07,880
the chances of giving a bad recommendation to an experimentalist.

424
00:29:07,880 --> 00:29:14,600
And so how do you envision scaling your workflow to these multiple criteria?

425
00:29:14,600 --> 00:29:18,280
So that's something that we're actually looking to now.

426
00:29:18,280 --> 00:29:20,360
And we're still hoping that out.

427
00:29:20,360 --> 00:29:28,120
But the rough idea is to find the Pareto front of multiple objectives and to try and balance

428
00:29:28,120 --> 00:29:31,440
where we want to be on that Pareto front, if you're familiar with that term.

429
00:29:31,440 --> 00:29:32,440
A library on that.

430
00:29:32,440 --> 00:29:34,680
How do you go about doing that practically?

431
00:29:34,680 --> 00:29:35,680
Yeah.

432
00:29:35,680 --> 00:29:42,160
So practically, we would first have to find another property that we'd be interested

433
00:29:42,160 --> 00:29:49,360
in, let's say stability of the catalyst or how long it'll stay there because if a catalyst

434
00:29:49,360 --> 00:29:53,840
operates for a few hours and then degrades into something else and stops working, then

435
00:29:53,840 --> 00:29:55,600
that's not a good catalyst, right?

436
00:29:55,600 --> 00:29:59,640
So that's one of the things that we're going to be looking into in the future.

437
00:29:59,640 --> 00:30:05,720
And our simulation, our simulations can actually get a handle on how stable something is.

438
00:30:05,720 --> 00:30:10,320
And so what we would do is we would have a metric for what we call activity or how fast

439
00:30:10,320 --> 00:30:13,440
the reaction goes and we would have a metric for stability.

440
00:30:13,440 --> 00:30:17,360
And for all the candidates that we want to look at, we could evaluate how well it performs

441
00:30:17,360 --> 00:30:19,080
in each.

442
00:30:19,080 --> 00:30:20,920
And I mean, there's no free lingerie.

443
00:30:20,920 --> 00:30:26,440
And so each candidate is probably going to perform well on one and maybe not for the other.

444
00:30:26,440 --> 00:30:36,040
And so what we call a Pareto front is where let's say you have a certain activity, right?

445
00:30:36,040 --> 00:30:40,040
And you find, let's say you want to have the ideal activity of point one.

446
00:30:40,040 --> 00:30:44,120
We can find the candidates that have that idea, that ideal activity, but also are the

447
00:30:44,120 --> 00:30:45,120
most stable.

448
00:30:45,120 --> 00:30:48,400
And so a lot of give us one answer for that particular activity.

449
00:30:48,400 --> 00:30:52,520
And then we can take a step to say, okay, let's look around materials that have an activity

450
00:30:52,520 --> 00:30:59,480
of point two a little further away and of that, find the subset that are the most stable.

451
00:30:59,480 --> 00:31:05,760
And then we can continue that across the spectrum of activities and find a list of materials

452
00:31:05,760 --> 00:31:10,360
that are active and generally more stable than other candidates.

453
00:31:10,360 --> 00:31:12,080
And we would still get a list from there.

454
00:31:12,080 --> 00:31:17,080
But using that, that we could down select our material set even further.

455
00:31:17,080 --> 00:31:21,520
What else do you plan on doing the kind of further this research line?

456
00:31:21,520 --> 00:31:22,520
Yeah.

457
00:31:22,520 --> 00:31:27,800
So other things we're looking at are more intelligent ways to select the next experiments

458
00:31:27,800 --> 00:31:29,760
or the next simulations.

459
00:31:29,760 --> 00:31:36,880
And so what I told you was the Gaussian selection.

460
00:31:36,880 --> 00:31:41,600
So right now we're actually having conversations with machine learning people at Carnegie Mellon

461
00:31:41,600 --> 00:31:49,880
University to figure out what type of algorithms might be best suited for our application.

462
00:31:49,880 --> 00:31:51,360
So there's that.

463
00:31:51,360 --> 00:31:55,880
And another way is to actually simply improve their regression methods that we're using

464
00:31:55,880 --> 00:32:01,320
right now to see if we can get better serial models that can be more intelligent about

465
00:32:01,320 --> 00:32:05,640
what this light didn't have a better prediction rate.

466
00:32:05,640 --> 00:32:12,840
With the first of those kind of an alternative to Gaussian selection is there some intuition

467
00:32:12,840 --> 00:32:19,000
that you're pursuing as to what methods might be better or what might the characteristics

468
00:32:19,000 --> 00:32:20,560
of better methods be.

469
00:32:20,560 --> 00:32:21,560
Yeah.

470
00:32:21,560 --> 00:32:24,880
So again, this is something that we're just starting to dabble in.

471
00:32:24,880 --> 00:32:29,880
So we haven't flushed the ideas out, but a lot of the active learning literature that

472
00:32:29,880 --> 00:32:36,120
we've seen so far is really centered around Bayesian prediction and processes.

473
00:32:36,120 --> 00:32:42,520
And so that's something that we might look into to model our system with and maybe start

474
00:32:42,520 --> 00:32:44,800
selecting new candidates with.

475
00:32:44,800 --> 00:32:53,320
And on the second direction that you mentioned, improved models, what are you thinking there

476
00:32:53,320 --> 00:32:58,320
or what directions are you looking at?

477
00:32:58,320 --> 00:32:59,320
Yeah.

478
00:32:59,320 --> 00:33:02,680
So that kind of has an interaction with other things we're looking at.

479
00:33:02,680 --> 00:33:09,120
Like I said, if we end up using Bayesian statistics to do the active learning, well then that

480
00:33:09,120 --> 00:33:11,960
means our regression is going to be Bayesian based.

481
00:33:11,960 --> 00:33:17,560
If we don't end up doing that, I've toyed with the idea of actually using neural networks

482
00:33:17,560 --> 00:33:19,280
to do these predictions.

483
00:33:19,280 --> 00:33:22,600
But there's a decent amount of overhead work that we need to go into that.

484
00:33:22,600 --> 00:33:25,960
So we're still thinking about that, not sure if we want to go that direction.

485
00:33:25,960 --> 00:33:31,360
But using neural networks is probably, if we choose to go that route, probably what

486
00:33:31,360 --> 00:33:32,360
we would do.

487
00:33:32,360 --> 00:33:33,360
Okay.

488
00:33:33,360 --> 00:33:39,920
And when you say overhead, what are you specifically speaking of computational or the

489
00:33:39,920 --> 00:33:45,680
effort that goes into just learning, you know, how to build out the models or what?

490
00:33:45,680 --> 00:33:53,400
I would say the effort going into building the models because the way in which we turn

491
00:33:53,400 --> 00:33:57,480
our system into features really matters a lot.

492
00:33:57,480 --> 00:34:01,840
And that's really very active area of research in the field right now.

493
00:34:01,840 --> 00:34:06,720
But it requires a lot of intuition, a lot of luck, and a lot of time to fit these models

494
00:34:06,720 --> 00:34:10,680
well, especially when we're looking at search spaces as large as ours.

495
00:34:10,680 --> 00:34:15,880
Well, Kevin, thanks so much for taking some time to share with us what you're working

496
00:34:15,880 --> 00:34:16,880
on.

497
00:34:16,880 --> 00:34:17,880
It's really cool stuff.

498
00:34:17,880 --> 00:34:18,880
Of course.

499
00:34:18,880 --> 00:34:19,880
You're welcome.

500
00:34:19,880 --> 00:34:24,960
All right, everyone, that's our show for today.

501
00:34:24,960 --> 00:34:31,000
For more information on Kevin or any of the topics covered in this show, visit twimmelai.com

502
00:34:31,000 --> 00:34:34,000
slash talk slash 238.

503
00:34:34,000 --> 00:34:41,760
Remember to enter our AI conference ticket giveaway at twimmelai.com slash AI and Y giveaway.

504
00:34:41,760 --> 00:35:10,280
And of course, as always, thanks so much for listening and catch you next time.

