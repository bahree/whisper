Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting
people, doing interesting things in machine learning and artificial intelligence.
I'm your host, Sam Charrington, hey what's up everyone, before we jump into today's
show, big news on the Twimble Con front.
I am excited to announce that in response to strong attendee and speaker interest, we
have supersized our agenda.
What does that mean?
Well we've added a second track, doubling the number of great sessions and speakers that
we can accommodate.
And we're unveiling part of that expanded agenda today for the first time at twimblecon.com
slash speakers.
Check it out and then hit that big green register today button to join us and learn from
our experts how your organization can eliminate the barriers to building effective machine
learning models and getting them into production.
Hope to see you there.
All right everyone, I am on the line with Kate Darling.
Kate is a research specialist at the MIT Media Lab.
Kate, welcome to this week in machine learning and AI.
Thanks for having me.
I'm really looking forward to this conversation.
We met not too long back at the AWS Remarers Conference or Amazon Remarers Conference where
you did a great presentation on some of your research into human and robot interactions.
And I'm really looking forward to diving into that.
But let's start with the kind of broad brush look at the field.
You're a leading expert in robot ethics.
What exactly is robot ethics and why is it important?
That's a great question.
So robot ethics sounds very science fictiony, very, you know, I robot blade runner-esque.
But when I talk about robot ethics, what I really mean is the ethical use of robotic technologies.
And part of that is, you know, how we integrate technology into the workforce, how we think
about responsibility for harm in all sorts of contexts, whether that's automated weapons
systems or automated vehicles or other types of automated technology.
But the main thing I'm interested in in the sphere of robot ethics is the social aspect
of integrating robots that seem very lifelike to people.
So I'm really interested in the ways that people treat robots like they're alive, even
though they know that they're just machines.
And what sort of ethical issues can arise from that?
And how did you get interested in this field, what's your background and what led you to
this area of focus?
So I originally studied law and social sciences and I did law and economics and intellectual
property, but I think that my interest has always been in how systems shape behavior.
So, you know, if you look at the laws of system or economics is a system, or now I'm really
focused on technology as a system, and how it shapes human behavior.
And I think what really got me interested in robots in particular was this one moment
where I was I was in law school and I bought this baby dinosaur robot called a pleo, they
don't make them anymore, but it was this really cool toy that had this kind of lifelike
behavior.
And one of the things it did was mimic pain very well.
So like if you held it up by the tail, it had a tilt sensor and it knew that it was upside
down, so it would start to cry and squirm around.
It's super cute and it was really cool like for the toy that it was at the time, so I would
show it off to people and I would be like hold it up by the tail, see what it does.
And people would hold it up, and after a while it started to bother me when they held
it up too long, and I would tell them to put it back down, I'd say that's enough now.
And that was really interesting to me because I knew exactly how the toy worked, but I still
felt this empathy for it when it was crying.
And I was like that's weird.
And then I started looking into this more and I discovered the whole field of human robot
interaction that looks at how people interact with robotic technology.
A lot of it is studies that border on psychology and people's tendency to treat these machines
like living things and how to tweak that.
So I got very interested in that, and I started coming at it though from the perspective
of okay clearly we do this, but what does this mean in the broader context of a society
where we're increasingly integrating robots into shared spaces?
You have several of these PLEO robots to this day.
I think you showed some photos of them in your presentation and we'll make sure to either
include or link to some of those photos in the show notes.
Yeah, they're very cute.
So you kind of mentioned the increasing role of robots in our day to day lives as we're
transitioning from world where they, you know, the primary experience that most folks
had with robots was if they work with them like industrial types of robots and now we're
starting to see these robots that are in stores guiding people around or your robot baristas,
things like that.
How is the study of human robot interaction practically applied to, you know, this new
world that we're evolving into?
Well yeah, so like you said, you know, we're very familiar with industrial robots.
We've had those for a long time, but what's happening right now is that robots are coming
into workplaces and households and public spaces, you know, stores.
And right now the technology is still very crude.
I know like my mom recently had an encounter with a robot in stop and shop that she was
not happy with.
She was like, creepy and it's beeping and I don't like it, but I think it's a matter of
time before the design gets better and more compelling and people actually, you know, start
to accept robots in their shared spaces.
I think it's inevitable that this is going to happen. I also think it's inevitable that
we're going to be working more with robotic technology because I know the media likes to
talk about how robots are taking all the jobs.
And that's true in some cases, jobs that are very, very easily automated.
But in most cases, robots aren't good replacements for humans and they have a very different
type of skill set.
So what is actually happening is that we're going to see more technology that people have
to work with and human robot interaction helps to study how people interact with that technology
and how to design it in a way that they might trust it or even enjoy working with it instead
of just saying, oh, this machine is threatening to me or it doesn't work, it made a mistake
and I don't like it.
Human robot interaction is oftentimes about designing the technology in a way that is
more palatable to people and that people might even like to work with.
And it's also about finding use cases for the technology that we might not even have.
So it's not just about workplace integration, but there are also some applications in
health and education that are really interesting where we're starting to see social robots being
used as replacements for animal therapy, for example, in context where we can't use
real animals or robots that are working with autistic children and engaging them in ways
that we haven't seen before.
So a lot of pretty cool things happening in human robot interaction.
So I think it's a very useful field of study for this day and age.
One of the aspects of human robot interaction that you study is, or at least the result of
it is kind of this exploration of human empathy in those kind of scenarios and you've done
a number of experiments to explore that, including I think involving these PLEO robots.
Can you talk a little bit about some of the experiments that you've done?
Yeah, sure.
So the PLEO, I haven't actually done any scientific experiments with the PLEO robots because they're
very expensive, and the experiments I've done usually involve destroying the robot.
But what inspired the experimental work was a workshop that I did with five of these
baby dinosaur PLEOs with my friend, Honest Gossult, where we took the baby dinosaur robots.
We made five groups of people.
These were all adults.
They were at a conference.
We had like five teams of six people each, and each team got a robot, and they named it,
and they had to interact with it and play with it for like 45 minutes.
And then we unveiled a hammer and a hatchet, and we told them to torture and kill the robots.
And it was really interesting.
Like we...
It seems like you're getting uncomfortable just at the thought of torturing and killing these
PLEOs.
Well, it was really, really interesting to see that people were more uncomfortable than
I expected them to be.
We thought that some people would be like, yeah, sure, it's just a robot.
I'll take this hatchet to it, and some people would be like, no, don't do it.
And instead, in this particular group, everyone refused to even hit the robots.
So we actually had to improvise in the workshop, and at some point we were like, okay, you
can save your team's robot if you destroy another team's robot, and they tried to do that,
and they couldn't do that either.
And finally, we threatened to destroy all the robots, unless someone took a hatchet to
one of them.
And it was this very half-joking, half-serious discomfort that people felt when the robot
kind of got destroyed by this hatchet.
And there was actually a moment of silence in the room for the fallen robot.
So it was just this very interesting, very dramatic, very not-scientific experiment day that
we had.
And that inspired some later research that I did at MIT with Palachnondi in Cynthia, Brazil.
And for those experiments, we weren't using cute baby dinosaur robots in part because of
the cost, like I mentioned, but in part also because we wanted to choose something that
people don't immediately bond with and respond to.
So we chose hex bugs, which are this toy.
It's small, it moves around in a really lifelike way, like a bug.
And we had people come into the lab and smash them with mallets, and we wanted to know two
things.
We wanted to know, would people hesitate more if we gave the hex bug a name and kind of
a backstory?
So if we said, this is Frank, and Frank's favorite color is red, and he likes to play.
And the other thing we wanted to know was whether people's hesitation correlated in any way
to their natural tendencies for empathy.
So we did this psychological empathy test with them, and we found that people who scored
low on the test for empathic concern, they would hesitate much less than the other people
they would just hit Frank.
And the people who scored very high on the empathic concern test would hesitate much more
or even refuse to hit the hex bugs.
So it was, you know, it was a little study, but it was kind of interesting because it
indicates that, you know, we might even be able to measure people's empathy using robots,
which is kind of like a weird turn on the VoicConf test from Blade Runner, and I know if you're
familiar with that one.
I don't remember the details of it.
So in Blade Runner, you have robots that look just like humans, and so to tell whether
someone is a robot or a human, they do this empathy test where they tell them these stories
and see how they react to them.
And so our version of that is, we can see how empathic you are as a human by telling
you stories about a robot and seeing how you react to that.
So it's kind of, it was, it's fun.
Was it your presentation that showed a video of kids and a, like a mall security robot
and some of the dynamics that occurred or did I see that separately?
Or do you know the video that I'm referring to?
I know this video.
I did, I have not shown this video, but I have talked about the study.
This is the one in Japan, right?
I don't remember where it takes place, but the basic idea was they were, at least the
thing that I remember is they were, you know, if they were multiple kids or no parents
around, they were identifying the situations in which, you know, kids would come to abuse
the security robot and, you know, things like multiple kids around and no parents were
kind of key indicators.
Yeah.
And they ended up, it was so funny, they, because the paper is essentially about the solution
that they found for preventing the security mall robot from getting beat up by the kids,
which is they made it avoid people below a certain height and move towards taller people.
They figure, okay, if there is an adult nearby, they'll intervene and stop the kids from
like verbally and physically abusing the robot, which is kind of funny.
It's really like, the video is funny, it's like terrifying and funny at the same time.
And do you interpret, when you see that video, do you interpret it through the lens of kind
of the empathy results of some of your experiments?
Well, so my question when I look at that is, if you're a parent, do you intervene to stop
your kid from beating up the robot for reasons other than just respecting property?
Like, is there a reason to worry that your kid might, you know, learn that it's okay
to treat something that responds in a life like way violently?
And could that like translate to their behavior towards other children or animals or things
that are alive?
And so that's, we don't know the answer to that by any means.
It's something that I think needs to be explored, but I also wonder about it in the context
of adults even, you know, how muddled is it in our subconscious to treat something that's
designed to respond in a really life like way violently?
Is that a healthy outlet for violent behavior or is that, as my friend once said, training
your cruelty muscles?
We don't know.
It's the same question as violence in video games, except that for video games, we seem
to have landed on, you know, adults can compartmentalize when it's on a screen.
Children were not so sure about, so we restrict it there.
But robots bring this to a very new, very visceral level because of the physicality.
We're very physical creatures.
There's lots of research that shows that we respond differently to something in our
space than to something on a screen.
And so it seems like we might want to ask that question again.
Well, also, with video games, the question comes up more often than not when the thing that
we're being violent against is another human or a thing that's supposed to be a human.
Whereas we don't often see humanoid robots kind of roaming around in real life.
Very few of us have the opportunity to interact with things that are anywhere close to
humanoid robots.
That is true. It's funny how people always leap to humanoid robots.
I think we have this tendency to constantly compare robots to humans, and I also admit
that a lot of people are trying to build humanoid robots.
That is definitely a fascination that is there.
But when I think about life robots, I think about all sorts of different designs.
I don't know if you've seen the baby seal robot that they use with dementia patients.
No.
It's super cute. It's been around for a long time, at least a decade.
It's used as a therapeutic device in nursing homes.
It's this baby harpsial that is furry and you pet it and it doesn't talk to you or do
anything that might disappoint your expectations like a humanoid robot would because they're
just not good yet.
It just responds to your touch and makes these little sounds.
It's very effective.
I think that even though we're not in a place where we have robots that look like in
Westworld or Blade Runner where we can't tell the difference, we are starting to see
design that we certainly treat like a living thing, even though it's clearly not smart.
And so part of your thesis perhaps is that our interactions with these things speak to
empathy in the same way that our interactions with animals speak to some kind of fundamental
empathy even though they're not human.
Yeah.
I actually think animals are really, really great analogy in this context.
Obviously animals are alive and they experience things and they feel pain which robots absolutely
do not.
But I think one of the commonalities here is that throughout history, we've treated
most animals like tools and products and not really cared about their inner worlds.
And then there are just some animals that we've kind of bonded with and made our companions
and treated with more kindness.
And if you look at the history of the animal rights movement, this doesn't seem to really
have anything to do with inherent biological criteria.
It has more to do with what we relate to.
People didn't care about whales until the moment that someone recorded them singing.
And suddenly you have to save the whales movement.
And this huge movement started back in the 70s because suddenly these were creatures
that we could relate to.
And when I look at how, yeah, I actually just met the guy who discovered whale song.
It was very, I was very, very much fan-girling.
It was amazing.
That's awesome.
But when I look at how we treat robots, I can see this going in a very similar direction
where we treat most of them like tools and products and some of them.
We really want to treat our pets and I think we're going to start to do that.
You mentioned earlier trust in the human robot trust relationship and it reminded me of
my conversation with Ayanna Howard back in February of last year, number 110 podcast.
And we, one of the things that she talked about with some of her research in that area
and how humans would, you know, essentially blindly follow, you know, these robots like
for example, robots that are supposed to guide them out of a burning building, you know,
in spite of the fact that the robots are doing, you know, they're obviously broken in some
way.
Like they're banging into a wall or something like that and the humans waiting around
for them to, you know, give them guidance.
Have you, is that research you're familiar with and how does that relate to any of your
work?
Yeah, so yeah, I love Ayanna's work and it's, it's very interesting.
There is something that we call automation bias that is this trust that people place
in robots or AI systems because in certain cases we place a lot of trust in these systems
because we assume that they have the right answers and that they're not biased and that
they're not going to, you know, make a mistake, the way that we trust a calculator to add
numbers.
So Madeline Ellish is someone who's done work on taking some of that research and looking
at how it applies societally in the world and also on a policy level because she's, for
example, what's that name again?
Madeline Ellish.
Madeline Ellish, okay.
She's at data and society in New York, I believe.
She loved her work, you should totally interview her.
She, so she's looked at, for example, the fact that when there are a machine and a human
working together, so you have a human in the loop and something goes wrong that was totally
the machine's fault, the human usually gets blamed for it.
So we have this over reliance on, on machines that is in some cases really unwarranted and
it's not really clear what to do about that because technology keeps getting more and
more complex.
And so sometimes you can't have that education or that transparency that really lets people
understand the limitations of the technology.
But I also think that it's, some of it is because we currently, maybe thanks to science
fiction and pop culture, kind of overestimate what the technology can do.
I see that a lot that people kind of think that we are much further along in robotics
and artificial intelligence development than we actually are.
And I, I think that's also a problem of science communications and the media in general.
So it's definitely an issue that needs to be addressed.
Yeah, and this comes up in some of my conversations with folks on the business or enterprise side,
just in thinking about how to address kind of statistical literacy and numeracy within
organizations and trying to raise the level of understanding of, you know, as we're adopting
more and more of these ML and AI systems within organizations, you know, what it means
that these systems are, you know, based on statistical models and pattern matching in
our probabilistic, and many organizations are spending a lot of energy trying to come
up with new and innovative ways to educate folks that don't kind of think about systems
in this way.
It kind of sounds like we're going to need to do this on a broader societal scale as these
systems get more and more integrated into the way things work.
Oh, yeah, for sure.
I mean, there's been, so there's this group called the Personal Robots Group at the
Media Lab that I do a lot of work with and one of their students, Blakely Payne, is working
on an AI ethics curriculum for middle schoolers.
And it's really great like she's going to middle schools and she's doing these exercises
with the kids where, for example, like they look at, you know, a YouTube recommendation
algorithm and then they have to, you know, put themselves in the place of the different stakeholders
in the system and not only understand how it works, but who it works for.
And so they really learn to kind of question what's going on and think critically about
it in a really good way, but they, you know, they're getting overwhelmed with requests
from people like all over the place, you know, companies, you know, everyone is like,
how do we adapt this to use in our organization because we need this too?
Like this isn't just for middle schoolers, like everyone needs this education right now.
And so they're trying to scale as fast as they can, but it is, you know, there's a massive
amount of interest in this and I think it's, it's very important and people are starting
to realize that.
Interesting.
So some of your work, maybe going back to the empathy conversation relates to the different
ways that humans anthropomorphize robots and the different implications of that.
And there's some policy implications that you've looked at.
Can you talk a little bit about that sphere?
Yeah.
So my main interest is the anthropomorphism of robots.
So people, you know, projecting life like qualities onto the robots.
And there are some pundits in technology ethics who claim that this is a bad thing and that
we need to discourage it.
And I, I, I see some concerns that we might want to have about the fact that people treat
robots like their lives.
So for example, if, if, you know, the persuasive design folks get their hands on robots and,
you know, start to try to manipulate people for, you know, corporate interest and try to
get them to, you know, buy products and services or reveal more personal data than they would
ever willingly enter into a database through, you know, interacting with, you know, a social
robot in the home, for example.
Then I think that's maybe a consumer protection issue that we might want to think about a little
bit.
You know, there's a lot of privacy concerns, manipulation concerns, but I don't think
that it's inherently a bad thing because there are so many great use cases for this as well.
And so those are, I mean, those are some questions that are popping up, but I'm also interested
in, you know, this question that we touched on earlier of, you know, is it a bad thing
for people to treat life like objects in a violent way?
We don't have the answer to that, but it's already coming up in some policy questions.
So for example, there's some discussion of whether sex robots are something that should
be banned because they encourage certain behaviors in people or whether there's something
that should be encouraged because they, you know, are an outlet for behaviors that we,
you know, don't want to be levied against real people.
So it's, there's, there are the, you know, policy questions being thrown around without
any actual, you know, evidence behind them and a lot of moral panic, especially in the
area of sex technology that, you know, are already becoming very relevant.
And I, I do think that if we found evidence that, you know, it's somehow desensitizing
to people to be violent towards robots that we might need to have some legislation that
says, you know, you can't torture a certain kind of robot in the same way that, you know,
we don't allow torture of animals, even though we still allow people to, you know, kill
animals and do it.
But you can't, you know, set a kitten on fire and throw it, you know, in a bag.
And, you know, there are certain things that would, that just we're not comfortable with.
And this might be a problem.
But there are totally different reasons behind that, right?
But the kitten, it's because we, the kitten is a creature that, you know, experiences
pain and all of that kind of thing with the robot, the rationale would need to be much
more about us than, you know, the target of our violence.
Absolutely.
And that's why I think it should be purely evidence-based.
Like, if we have evidence that is desensitizing to people, then, you know, maybe we need
a solution for it.
But that said, I am also not convinced that when it comes to animals that we truly, truly
care about their pain, because there are certain animals that we're perfectly willing to torture
for our own benefit.
Like it's obvious that, you know, animals like chickens feel pain, but, you know, we keep
them in these, you know, cooped up spaces.
We chop off their beak so that they can't peck each other's eyes out because they're
going crazy in these cooped up spaces.
We torture plenty of animals for our own benefit, even though we know that, you know, there's
no biological difference between a horse and a cow that would justify us eating one of
them in America and not the other.
And yet, people are appalled when we suggest eating horse meat here.
I think there are a lot of reasons that we protect animals that are actually about us.
And as much as we don't like to hear that, I think that the way that we're interacting
with robots is kind of making that very apparent.
Interesting.
And this part of the conversation reminds me of this device that I saw at CES earlier
this year, Bot Boxer, it's like this robotic boxing training thing.
And, you know, the device as presented was basically a punching bag that kind of, you
know, had some behaviors in it to evade you and that kind of thing.
But it makes me wonder, like, if the thing was, you know, one of these humanoid punching
bags and made painful sounds, like what the implications are, it needs to be this discussion.
It creates all kinds of interesting thought experiments.
It does.
Oh, yeah, that is interesting.
But also, you know, if it's not, if we're not subconsciously treating that boxing punching
bag like a human opponent, then it's not a good training device.
So, you know, it, to some extent, that relies on this kind of subconscious treating it like
a person.
And, you know, I think that, you know, there are plenty of situations where, like, I'm,
I would never argue that, you know, because boxing exists as a sport that people who box,
you know, might be more likely to hit other people.
I think we're very good at compartmentalizing in that type of way.
It is, you know, with children, it is, there are some questions.
You know, I have a toddler myself.
And so, I'm starting to have to deal with things like, if he pulls the robot cat's tail,
we have a robot cat at home.
Do I stop him from doing that because I don't want him to learn that it's okay to pull
a real cat's tail?
And there are a lot of stories, even with, like, the very primitive kind of voice assistance
we have today, you know, there are stories of older kids where parents are like, you
know, Amazon's Alexa is turning my child into an asshole because she doesn't require
you to say, please, and thank you, you can just bark commands at her.
And so, my kid now thinks that's okay to do to anyone.
And there were so many complaints to Amazon that they actually released a feature that
you can turn on where Alexa will require, please, and thank you.
So, we don't have scientific evidence that it has an effect on kids' behavior that translates
to, you know, how they interact with the world, but there's some anecdotal concern at
least that we might want to be a little bit careful because interacting with these machines
is subconsciously like interacting with, you know, a social agent, like another person.
Yeah, that reminds me a little bit of, you know, don't let your kids play with fake cell
phones or your animals for that matter because then they'll just destroy your real ones.
But, you know, it seems like that, at least in that case, like there's some age in which
you age range, in which you have to worry about that, but at some point there's a level
of maturity in which they realize the difference between this plastic thing that kind of looks
like a phone and the actual real phone that you get upset about if they throw around.
Oh, sure.
Kids are smart.
Right, right.
And we have a lot of kids, like a lot of the roboticists who work in the meaty lab have kids,
and their kids come into the lab and they totally know how the robots work and that the robots
don't feel anything and yada yada, and yet they're still treating them like friends.
And even the roboticists will do it, like it's really funny, like, you know, um, our
setting of the hex bugs, our participants were mostly MIT undergrad, so people who have
a lot of tech literacy, and we still found an effect.
And I've seen, you know, I've seen effects in myself and others who know perfectly well
how the robots work and will still kind of treat them.
It's very hard to not go with that instinct to treat the robot like an agent because
we're biologically hardwired to perceive the type of movement and interaction that these
robots have as a social interaction, and so we like immediately slip into treating the
robot like, you know, an agent instead of an object, and even adults who are completely
tech literate aren't immune from that effect.
So, you know, kids are smart, like, they know, but, you know, my workshop with the adults
who refuse to harm the robotic baby dinosaur shows that even if you know, it might not
matter.
Right, right, right.
I'm curious what's the example of the most, you know, either the most, you know, human
life or kind of empathy producing or just interesting in general, kind of robots that
you've, you know, seen out there.
I'm imagining it's not Sophia.
Yeah.
I was just going to say, yeah, I think the humanoid ones are actually not as empathy and
gendering because there's this thing in robotic design that some people call the uncanny valley.
I call it expectation management where if you're comparing, like, say you take a humanoid
robot or my shitty cat robot that I have at home, like, these are things that are designed
to try and look as closely to a human or as closely to a cat as possible.
So when they behave, you're expecting them to behave exactly like the thing that they're
trying to mimic and as soon as they don't do that, they make some, like, different movement
or, you know, utterance, it breaks the illusion and you're no longer suspending your disbelief
and it kind of disappoints your expectations.
And so I think that the most successful social robots actually have very different shapes
and forms.
I think that Baby Dinosaur works because no one's ever actually interacted with a Baby
Dinosaur before.
So it's kind of like, you know, I'll believe that a Baby Camerasaurist moves in this way.
You know, I'll totally imagine that.
But even like, have you seen Gibo at all?
Which one is Gibo?
To Gibo.
Gibo, unfortunately, the company no longer exists, but they had this very successful kickstart
or Indiegogo where it's, it's just, it's one of those home assistant robots like Amazon
Alexa, except it had, it looks a little bit like a Pixar lamp.
It has my body and the face and has this animated like circle that just, it's very, very simple
and very, very compelling, like the little movements that it makes, it just makes you feel
like you're interacting with, you know, something that has a character instead of just an object.
And those I think are, those are the most successful robots in kind of engendering people's empathy.
Have you explored anything related to these telepresence robots?
Like the, the kind of iPad on a Segway, things that people can use to be remotely present
in meetings and that kind of thing?
Yeah, so I haven't done any work on that, but I have heard some interesting stories about
how people, you know, people will Skype in or whatever you do to the telepresence robot.
And if someone in that physical space comes up and like picks up the robot and moves it
because of the way, people have reported that they feel like violated, like physically
violated, even though they're only Skyping into it.
Wow.
Or if somebody's standing too close to it or whatever.
So I think there's a lot to unpack there and explore, especially as these get more robot
likes so far they haven't, they've been kind of like just remote controlled, you know,
iPads on wheels, but I think as they develop more autonomous capabilities, it'll be interesting
to see, you know, the intersection of people's feelings of autonomy while they're, you
know, quote unquote, inside the thing and how, you know, the robot is interacting with
the world.
So that's definitely a very interesting area, I think.
Yeah, as you describe that, I can almost, that it becomes almost tangible for me, that
feeling of kind of anxiety of being, you know, someone just kind of, you know, quote unquote
man handling my virtual self also makes me think about like, one of my thing, I guess
the zoom is kind of the example that I'm thinking of if you like, we've got one zoom
account that we share among a few people and like it'll kick people out of a room if
somebody comes in and like takes over it.
And I'm thinking about like the, you know, multiple humans, like struggling to take
control over this virtual robot and like, you know, all kinds of issues related to like
the hierarchy of being able to use this thing.
Yeah, I mean, I'm telling you, the intersection of psychology and how we're using technology
is really, I think it's so fascinating.
Yeah, yeah, it is.
So what are the things that are kind of on top of your list of, you know, exciting things
that are happening, really interesting work, you know, besides the stuff that you're
doing that folks should check out?
I'm personally really excited, this is not my own work.
There have been some advances in the research with autistic children.
So a long time ago, researchers and social robots and social robotics found out that autistic
kids will respond really well to social robots and they'll engage with them more than they
will with, you know, an adult, a teacher, a caregiver.
But not only that, the kids, when you bring them into a room and have them interact with
the robot, they'll also interact more with the person who's in the room with them.
So suddenly, they'll be making more eye contact, answering more questions than they were before.
So it's a really great kind of facilitating device and they're not quite sure how or
why this works.
But what's interesting going on right now is that just last year, they published the first
long-term study where they actually put robots in the homes of children who are on the
spectrum and did a longer-term study where the kids were interacting with the robot for
half an hour every day together with their caregiver.
And then after about a month, they saw like a dramatic increase in the social skills
that they were looking to kind of encourage in the kids.
And so like really thousands and thousands of dollars worth of therapy just from, you
know, interacting with this robot.
So I think there's so much promise in that area and I'm really excited to see what other
work comes out of there.
That's pretty fascinating.
You said they're not sure kind of how or why any hints or indications.
Well, I think so the leading researcher in that space, Brian Scuzzelotti, who's at Yale,
he thinks that it's because the kids view the robot as a social actor.
So something that they engage with socially, but it doesn't come with the baggage of another
child or another adult, you know, because they also understand that it's a robot.
So that's their best guess for why this is having a pretty strong effect on these kids.
But it's pretty cool because it's something it's a tool that we haven't, you know, had previously
and now we do.
Cool.
Anything else that we should check out?
I don't think so.
You should check out Blakely's AI ethics curriculum.
Okay.
Yeah.
Yeah.
We'll definitely link to that.
Very cool work.
And then you mentioned earlier, the possible opportunities to kind of abuse the human robot
interaction and persuasive design, like have you seen anything there that kind of, you
know, put your, put your radar up or that folks should be aware of as kind of a negative
example of how this kind of technology or the interactions are being taken advantage
of?
So I haven't seen anything.
I, you know, I don't think that social robots right now are, you know, pervasive enough
for this to be a problem, but I, I see it on the horizon because, you know, if you think
about it, you know, a sexual robot that has in-app purchases, you know, is that okay or
is that too manipulative or I can think of a million examples or even just so Sony came
out with a new robot dog, the Ibo, which I really want when I'm probably going to get one.
But they're not only are they very expensive, but they require a monthly subscription to
the cloud services.
Oh, wow.
I don't know what functionality you lose if you stop paying for that, but it's kind of
interesting because we know from the older Ibo is that people really treated these robots
like a pet and like they're a part of the family.
And so that does seem like a little bit of maybe manipulating an emotional connection
if you're going to charge a monthly fee to keep this robot going, you know what I mean?
Yeah, you can imagine the, the, uh, uh, loss prevention emails, you know, don't let your
Ibo die kind of, oh my gosh, yes, they should hire you as a consultant.
Oh, yeah, um, yeah, that doesn't, that seems to be on the other side of some line.
Uh, well, Kate, thanks so much for taking the time to chat really, really fascinating conversation
and, uh, I'm looking forward to following along with your work.
Thanks so much.
All right, everyone, that's our show for today.
For more information on today's show, visit twomolai.com slash shows.
Make sure you head over to twomolcan.com to learn more about the Twomolcan AI Platforms
Conference.
As always, thanks so much for listening and catch you next time.
