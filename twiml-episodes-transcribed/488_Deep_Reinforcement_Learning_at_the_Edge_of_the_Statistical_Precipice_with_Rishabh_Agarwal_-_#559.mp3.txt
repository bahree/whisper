All right, everyone. I am here with Rishabh Agarwal. Rishabh is a research scientist at Google Brain.
Rishabh, welcome to the Twimal AI podcast. I'm looking forward to digging into our conversation. We'll
be talking about your work broadly, as well as your NURBS 2021 outstanding paper, deep reinforcement
learning at the edge of these statistical precipice. Great title. But before we dive into that,
I'd love to have you share a little bit about your background and how you came to work in RL.
Back in undergrad, I think deep mind, we released their Atari pencils. And as an undergrad,
it was really cool to see like, oh, we can play these games without any sort of hard coding and you
can just learn how to play these games. So it combined like two of the interests I had back then,
which is playing games and being able to do that automatically. So like I decided to take like
ML courses back then in undergrad and then started to get interested in more and more in research.
And I tried a bunch of different things before coming to RL. And I think it really happened. So my
bachelor thesis was on learning how to play Scrabble. And basically Scrabble, I think the best
agent in Scrabble is something from MIT, which uses some sort of heuristics and Monte Carlo research.
And we were thinking, well, can we replace these heuristics with a learned agent? Because think about
like it's basically hard coding some sort of strategies to do when you are in a certain position.
And we're saying, it can we learn these sort of things? Because we can collect data from multiple
self-play games and whatnot. So in some sense, what we were trying was to try and do something like
AlphaGo, but on Scrabble. And this was like around the same time. It's just that we didn't have
the same amount of compute. So we had to resort to like imitation learning and other kind of approaches.
But this is how I started doing RL. It didn't really Google. But after that, I applied to a bunch of
these residency programs at these different companies and got into Jeffenton's team. And I think
one thing with Jeffenton said me, told me it was very like interesting. He said, well, I've seen
a lot of people with a lot of people in solidarity before they start research. Your case was interesting
because you have done a lot of research and you have failed. So you know how research can be.
And this is something I think you would need in a researcher. That research can be quite hard.
A lot of the times. And it's better to know or seek failures early rather than later on.
Because if you don't see failures early on, then you'll think that research is easy and you just
have to like do some things in published papers. But it can be quite tricky. And he was like,
he was like, this is the kind of quality I was looking for. At least in researchers trying to
find out. That was good to know that. Well, someone to appreciate failures too. So I worked there
as a resident in the brain team for a year in Toronto. And there I worked on Meliado, I think.
And after that, I moved to Montreal and I'm doing art research still because I like doing art.
I think I pride moving away, but art is just too exciting too. So I just keep coming back to
all of you. You mentioned that you tried a few areas of research besides RL before settling
into it. What were some of those? So I think the first research project I had done was something
about disambiguation, which is think about like a lot of these things are already solved by large
language models these days. But think about let's say I say I'm going to the bank. Now this could
be a river bank or it could be a bank where you collect money, right? And without actually additional
information, you can't really say which bank were you talking about. Most likely the bank
where you go to the bit money, but it might be the river bank too. And we're thinking about,
well, how do you actually figure out or how do you come up with word embeddings that can give you
some sort of probabilistic sort of embeddings, which are like depending on the future sentences or
future words you see, you will change your embedding in some sense. So it's not a static embedding.
It depends on where this word is. So this is the first thing. Then I think dabbled a little bit
in meta learning or how do we classify I think thanks quickly because back I think it was 2016
and back then meta learning was starting to get popular. And it was interesting to see if we can
generalize like one shot or two shots. So things like I think if I give you images of docs and
cats and now any generalize one shot to let's say different breeds of docs, then what is in your
dataset given like a few examples. So these are the sort of things. But I think the aspect of
RL which was much more interesting was that the agent can learn from its own mistakes rather than a
static data. And I think that was something which is quite cool because this is something we as
humans also do that if we don't know how to solve the problem, we try to figure it out, collect
some data maybe and then just solve it or like make attempts at doing so. But this is not something
which happened in happens in supervised learning. So I think that was the aspect which made me think
about plus we used we play usually games in RL. So that was another interesting thing. And within
the realm of RL is there a particular area that you like to focus on nowadays? I have in the past
focused on offline reinforcement learning a lot. But more recently what I've been trying to think
about is that a lot of the problems we tackle are sort of we have let's say a sort of problem,
we learn an agent and forget about it and we try to improve and come up with another agent.
But a real world problem is not like that. A real world problem remains in existence for a few years
and it's almost always the case that you don't forget about your previous agents. You always try to
build up on top of that. So think about let's say you have a few recommendations system and
the algorithm recommend you videos. Now let's say you want to you have another algorithm
which can learn like better recommendations. You wouldn't really just throw away your existing
policy or agent or whatever behavior you have learned and try to learn everything from scratch.
What you really do is you try to build on top of whatever existing policy or agent you have.
Now this is good and this is mostly the case for applications but in research we don't really
follow this sort of protocol. Like really we have we have a benchmark we try to come up with an
agent and resolve this and we try to improve on that benchmark and we're saying well why don't we
really do this in research. So this is something I've been trying to focus on that how do we come up
with sort of problem settings and methods which can build on existing things and these
existing systems does not have to be like human demonstration. They might be just like your
existing agents which not so think of like an example which I think which I quite like is think
of Minecraft and think of some agent which has some basic capabilities to do build basic structures
in this Minecraft let's say build a house or something like that. Now if you take that agent
can it expand quite a lot and can it do like much more than just building a basic house.
Things like take existing agents and improve them significantly as opposed to learning new agents
entirely from scratch and I think this is valuable if your problem either takes a long amount of time
or it's just not feasible to do things again and again. So if I think if I don't know if you know
so opening I had this project on Dota where they basically spent 10 months of training to get
the agent to beat the canvas just there. Now the thing was in their paper they have like an
interesting figure where they say we never really started from scratch because if we did
they would have taken us like five to six times more like months like apparently more than 40
months to just get this agent and as I was really expensive to do so. So what they always did was
whenever they had to and the reason they had to do this was because their environments kept
in changing so their observation space or things like whatever their input is it kept in changing
and they had to make sure that their model still works for these changes so and they could not
really afford to like just throw away the model. So what they kept on doing was they kept adding
more components to their existing model so that it can reuse whatever they have learned
but also handle the existing change or the new changes which come into their pipeline.
So that was like an interesting use case of this sort of setup but this is more of a one-off
example of research I think because they have to tackle practical problem they had to do this
but if your benchmarks will just take like I don't know a day to tackle probably people
are not interested in but I still think there is value in this sort of research paradigm of
building on top of existing agents or this is something I think I've been excited about for a while
now. Nice nice and you're speaking about it broadly have you thought through kind of a taxonomy
of approaches like it has echoes of transfer to it it has echoes of hierarchy like open AI
has echoes of like initialization even you know taken simply. Yeah so this is a pretty broad topic
because what do you transfer really like it depends on for example what do you transfer do you have
a policy do you have a value function do you have a model do you have an exploration strategy
also depends on how you transfer it and what is given to you and what not so there are a lot of
these things really it's like a much bigger paradigm it's just that what we're saying is typically
in research what we do most of the times is we start tabular asa but that's something we don't
have to do like at least for a lot of the problems we don't do tabular asa at least in practice
so why are we doing that in research and we should be moving away from this so I think in our head
it's really broad but we probably start from focusing on a narrow like problem and then hopefully
the community thinks this is valuable and maybe the pick it up from there got it sounds like
you're early and you haven't specifically or narrowly formulated a problem statement just yet
we do have something but I think it is so it's related to let's say I guess we had some desiderata
which was let's say give you a policy or an agent and I have some and I have to use enough policy
agent or something which can exploit existing data because that's something which is useful now
can I quickly recover this for policies performance or agents performance while actually continuing
learning and I think the interesting aspect here is this policy is not optimal in any sense it's
just some sort of good policy it can get you some good rewards of behavior but really you want to
improve on it so the dependence of your agent on this policy or age or this existing let's call
a teacher should be limited like at some point you know that you should have to learn on your own
like kind of like how we take learn anything we take lessons and then we learn out of on
we just don't keep taking lessons forever similarly here
yeah so think of it like your instructor right like your swimming instructor teaches you
swimming but let's say you went on to become like a gold medalist or something so I think you
definitely learned a lot on your own rather than just relying on your instructor all the time
so something like that you have to let go of your instructor so that you keep learning on your own
because you know that maybe you can do better than your instructor and so that's the sort of
I think concrete setup you have in but yeah I still in the works and still thinking about what to do
with this the paper that I mentioned earlier DRL at the edge of the statistical precipice as I
mentioned at one and outstanding paper award at this past noreps but it wasn't necessarily the
paper that you were trying to write when you started out on your research program how did that
paper come to be yeah there's an interesting backstory so I think what happened was as you
find a use this benchmark called Atari 100k which is an interesting benchmark where they say that
oh we'll train an agent for 100k interactions which is approximately two to three hours of human
gameplay and this is the amount of time which the human agent was given or the professional human
gameplay was given to get like some sort of score on all these games so this is a baseline
they use when we compare the respective humans on these games and the benchmark said okay let's
do this let's actually train our own agents to this this much amount of time and see how well can
we compare against human or how well we do compare to humans so so this is good this is like a good
benchmark because it gives you like an estimate of how sample efficient the agent can be or like
how fast can your agent learn so this is all good and I was coming up with like maybe a better
approach to do this sort of thing and my agent I like get some sort of variation I thought maybe
I should just increase the number of random seats so that I see less variation in my results I
double the seats I still see a lot of variation I kept doubling the seats and at some point I think
I was at 30 seats and I was still like the variation is huge and it's all over the place and
but then I looked at the existing literature and I saw everyone who used three random seats and
they were comparing the results and claims soda and whatnot and in my own results what I was seeing
was that there were seats which led to really worse performance and their seats which were
beating state of the art are depending on what seats I end up using right and this was concerning
so for a moment I thought maybe the implication is that the the published results that you were
referring to are kind of cherry picking runs particularly well rather than that I was more like
it's unclear if the published results actually hold if you ran a lot more evaluations so it's like
it's unclear either they underperformed their actual reported results or overperformed them so that
was something which is not clear but so initially I thought maybe it's so I think the interesting artifact
which happened in my own work was that if I kept adding more seats my numbers or results kept going
up and I'll tell you the story about why that happened to but it was just like interesting to see
that I went from 3 to 5 to 10 to 20 to 30 am I just I'll skip improving and I was like I have
not done any like change here really the only changes evaluation what is happening so initially
I thought maybe it's just with the method I'm evaluating so there were two parts here one was just
to skip this benchmark and go to some better benchmark and think about it and the other was
maybe there is something wrong going on here and people have been using this benchmark and
probably they will because it is an interesting one so why not try to fix it so at this
crossword we decided let's just stop the existing project we are doing and think about this other
problem because it seems more fundamental and this is I think something in research which we should
learn that we should always keep an eye out for interesting opportunities if you're really trying
to go somewhere it's okay to take like a turn or something along the way if that looks more
interesting to you like it's totally okay to give up on that so we decided let's let's focus
on this other thing so what I did was I had a bunch of baselines already coded up for the
like the existing published results so I thought maybe I don't trust my own baseline let me just
ask the code from the authors themselves or whatever their open source code was and let's just
try to evaluate this code and funny enough the same sort of things happen that the uncertainty
results was pretty huge and for some of these methods I again so basically the more run number
of random seats I used the better the results were and what was this issue and then I think
this hit me basically what are we doing is we have an algorithm we were alluding it on a bunch of
task and aggregating the results to do something and in Atari at least what we use is something called
median score so if you have let's say I don't know 20 tasks what you use is the performance on the
median tasks in some sense and this is like a simple statistical fact that if you take median
off expectations or median of averages it's not going to come out to be the same as average of
medians in some sense so basically there's a difference between if you evaluate let's say a few
seats calculate the average score on each of the task and compute the median this is one quantity
the other quantity is you evaluate infinite number of seats you really get the true average
score you get on each of the task and then take the median this is another quantity so the funny
thing is if you repeat the first process again and again which is you have few seats you
evaluate the aggregate performance and then just keep repeating this process to take an average
in some sense expectation this quantity is not equivalent to the the true quantity you get about
because this is a bias estimator now this is I think well known statistical fact but the funny
thing was the size of the bias can be as large as 30% of your performance itself so it's possible
that some of the results were just like reported just cause of the bias or you really didn't know
the what's happening and this is actually what we found what was it in the the problem that you
were solving that made that bias always an underestimate of the actual performance yeah so this
was actually an artifact of the algorithm so what happened was really like I really didn't tune
my hyperparameters tackle so it hadn't overpitted and because of that it was actually like under
performing when I use few seats and if I run more seats it seemed to be a bit of performing but
for some other algorithms what we found that the direction of the bias was in the other direction
which is it could be positive or negative depending on the algorithm so for some algorithms if I
add more seats the performance went down as opposed to improving so that this made it even worse
because if it was in the same direction at least you can say in algorithms we didn't prove
with more seats now it was oh some algorithms actually become worse and some algorithms
become better and the thing is the bias is actually reasonably large just to not say what's
happening so this is one of the issues but it was not actually the big issue the biggest issue was
the uncertainty in these results was pretty high so what actually happened here was like I have
as I've seen like my results were changing quite a lot and sometimes the difference in the results
from three seats and let's say something like 20 seats was as large as the difference between
the previous soda and the baseline so it was a huge gap to see like maybe this gap can make
your conclusions sort of yeah like it's unclear if your conclusions are published method actually
was better or wasn't due to just statistical fluctuations and that was an issue so we thought well
this is not so expensive benchmark and I am at Google so we do have a reasonable model compute
let's just try to use that compute to see how much fluctuation there is and that led to like the
first finding of the paper that there is an enormous fluctuation and most of the existing methods
actually over reported their results were like overestimated in some sense except one method
for funnily enough which actually underestimated so there's also one method which was like if I
evaluated on more seats it results would have been much better it says that they didn't do so but the
interesting bit was that most of the results were actually quite overlapping at least in terms of
these meetings scores and it was unclear if there is a improvement and all of these were
published papers in the last couple of years so people said ICML is running on and some of them
were actually at near its 2012 but were there implementations published or did you have to
engineer their implementations so most of them were actually published except one for which we
had implemented so this was really not so we're trying to like remove all sorts of
confounding factors that this is our issues it was really we just took their open source code
and just ran it on a lot of seats and we said oh well there is actually a lot of overlap and what
not and so this was the problem part of things and I think it was sort of at least a lot of people
in researches and other low of these problems that there are problems when we compare our methods
and there's a lot of fluctuations in what not so this was the first part but then I think we thought
about it and realized that well we were at Google and we could have we launched these 100 seats
for all these methods but this is not computationally tractable for anyone outside Google and even
at Google it was pretty expensive to do so each time let's say you have to compare a method you
wouldn't be launching I don't know 100 seats and it also comes like and this was like a simple
benchmark or at least and computationally relatively inexpensive benchmark if you had let's say
harder benchmark some let me give you some estimates so for Atari games like the Atari 57 benchmark
it takes about 1000 GPUs for computing like a single result aggregated across all the tasks
and so this is about if you launch all the like runs in parallel or one of all the games it will
take you about three four days with like existing open source libraries so that is a lot of time
just get like one result and that one regret you're talking about training an agent for a particular
game in the Atari suite so no it's like training an agent for the standard 200 million frames
for all on all the games and doing like let's say something like five seats so let's say this is
the minimum you're going for this will take about three four days assuming you have done
parallelization also now you can't really expect researchers to run 10 seats at 20 seats a lot
more seats because it quickly becomes a lot more expensive but the thing is at least like in
reviews we often see this like we often get requests from like reviewers saying well we've only
validated in five seats why don't you run five more seats and it's very like hard to like convince
them otherwise but it is really expensive and I think this was the problem I also had at the back
of my mind because I have gotten reviews like these and I couldn't just say hey it's really
expensive and it'll take like five days just to get five more seats and so how do I convince
that these results are statistically robust while not running a lot more seats because that's not
really the solution and I think this problem will become worse as we move on to harder benchmarks
because these are like bench sort of games are actually 20 year old I think a 40 year old I don't
exactly remember but they are pretty old and now if you think about let's say more recent and
harder problems let's say you think of something like StarCraft then suddenly evaluating even just a
handful of seats is pretty hard so so I think the problem of few seats and like really expensive
benchmarks is going to stay at least more and more researchers would end up using fewer seats
and more difficult problems curious appears a correlation between the the number of seats the
reviewers asking for and the size of their company like the Google review were asking for more seats
if that is possible although we did have one flood in the paper where we showed the correlation between
as time passed and the number of seats we have used while our benchmarks have become more complicated
so I think back in the 80s we are using something like 30 seats because our benchmarks were really
simple maybe mountain card card like really simple simulated problems and now I think as we
introduced the satari benchmark we started using like fewer and fewer seats and more recently
I think you've been using just three to five seats that's the sort of standard idea that it's out
of necessity as opposed to negligence so yeah it's not really yeah it's not really people are
going to hide it's just that it's it is expensive to evaluate these models and like get like these
results so people run whatever like the prior work follows so okay so we had this mindset that oh
people are going to evaluate few seats and they still want to compare across these algorithms
now how can we do this robustly and this is the question we had the solution we went on to
something which we already doing computer science but not in machine learning which is they just
plot the distributions of the score you get across all the tasks and all the seats so think
about it for again so what are they doing is so this is commonly done for comparing optimization
software which is how fast they're able to solve the problem so what they do is you have a bunch
of problems they run their software or algorithm on all these problems they get times for each
of them and now they just plot the distribution of how fast they were in this like to solve the
problem and this is distribution meaning something like the CDF of all the times they got across
all the problems and all the runs and this is school for multiple reasons but I'll cover that
but we thought oh why don't we do this in RL also that basically the nice thing at least in
RL typically is that when we have a bunch of tasks we usually have some notion of score comparisons
across tasks which is at least for Atari for example we use human score human normalized scores
which is all the scores are relatively humans so we often compute that's why we are able to compute
aggregate scores also which is what does it mean to aggregate scores across two different games
which might have really different score ranges so think about I don't know pinball versus something
like pong the score in pong goes up to 21 while in pinball it can be a million or something like that
now you really just can't compare their scores but if you normalize the scores that's a relative
to humans now it makes sense to actually compare scores and pong in the pinball you can say how bad
I'm doing with respect to humans on both of these games now this is a cool property so we thought
well why don't we just mix all the scores we get and just compute like the distribution
and this gives you something called performance profiles now there are a bunch of cool things about
these profiles these profiles are nothing fancy these are really just the CDFs of the distribution
you would get if you were to mix all the scores now the nice thing about these things is that the
area under these profiles would be actually the mean score you get across all the games
similarly if you compute let's say a draw line at y equal to 0.5 then you'll get the median
the point where these curves into second and you can also similarly get the any sort of percentile
so because it is just giving you the entire distribution in a single picture and the other
nice thing about these profiles is that you don't really have to like write out a big table with
like a bunch of methods and let's say Atari has 57 tasks so what your table would look like is 57
sort of rows and then for each row you have multiple columns and oftentimes what happens is if
you have more than 4.5 methods you will just not report standard deviation or variance because
space mistakes and whatnot and then it's even harder to compare because some methods are doing
better in some class and not on others so these tables end up going in the appendix most of the time
and like really what you end up reporting is in the mean risk mean paper is some sort of aggregate
measures but the aggregation heights a lot of the things so we said well why don't we report these
sort of distributions and this was the first sort of solution the the solution to the problem of
how do we do better but the actual solution was more or less something like we should report
the statistical uncertainties we have when we run these algorithms so think about this whenever
you're running an algorithm on a bunch of tasks what you're trying to evaluate is a random variable
because the performance really depends on the randomness of whatever thing you evaluated now each
time you change your random seed your comments would changes so really what you have is a random
variable and you're trying to evaluate it using a finite number of samples or seeds in this case
right so what you should be really reporting is also what is the uncertainty in this variable
which is if you were to repeat your experiment that's a with a different random seed
what what is the result you expect and now the thing is some people say well maybe we'll just
fix the random seeds and that's okay and I'm saying well it's not really okay because let's say I
go to a different hardware then suddenly just fixing random seeds put in really fix the randomness
and also I think the bigger reason against just fixing random seed being the solution is
why should I prefer seed 1 2 3 over seed I don't know 42 or something like that so these are the
problems we said well let's report uncertainties and now the nice thing again by mixing all the tasks
and the runs you still get some a decent sample size which is let's say I have a single task
and five seeds really reporting uncertainty five seeds you don't really do much because
you just have five samples to sort of report some sort of measure of what is the variation
but let's say I have five seeds and 10 tasks I have 50 samples now and now you can actually think
about doing something with these 50 samples and reporting like uncertainty now starts to make sense
and for that we went on to something called statistical bootstrapping which is the I think the
first time I use bootstrapping in a different context then and then the way we typically use
another but it was basically I guess without going into details of what it is doing but the main
question we are trying to answer was we have finite data and we are trying to report evaluate
a random variable we're saying we should also report what is the uncertainty we have which is if
we were to evaluate seem algorithm again with maybe different seeds or under different
random conditions what are we expected to get so this is why we enter into the solution of confidence
intervals and there's a fun fact which I learned through while going through this so apparently
in statistics also so there were other options also things like p-values and whatnot which people
use in other areas but the main general of statistics in the US actually bans thresholding p-values
so so people often say my result is significantly statistically significant or not
seem to be really significant but I think there's a catch there which is it's like you say oh if
my p-value is less than 0.5 then it's statistically significant but if my p-value is 0.51 then it is
like not significant and that is an issue the other thing is your improvement or whatever your
reporting is actually statistically significant but not practically significant maybe it's really
trivial gain and I don't really care about if it's significant because it's like I don't know
it's a 0.001 percent improvement or whatever you're doing so really what do you want to see is
what is your effect size in some sense like what are the possibilities of gains you expect without
thresholding and then if you really care about statistical significance you can also glean that
from a confidence intervals but I saw like at least in rest of the statistics and other areas
also there was a push for confidence intervals as opposed to using these p-values plus p-values
are also harder to grok like these are something which makes this paper harder to read I think in
some sense so that's why we went on with the solution of confidence intervals and so does the
confidence interval you're asking researchers to publish these confidence intervals in conjunction
with their RL experiments is that replacing asking them to also publish the distributions or
they complement one another yeah so they are both complemented so apparently your distribution
node also have a confidence interval so the funny thing is think about it which is if you repeat
your experiment with a different set of seats your distribution would also change so in some
sense your CDS and this is why I think we say don't use something like or that's why you didn't
post something like a box plot because your distribution itself has uncertainty and if you
plot the CDS you can also plot the confidence bands around your CDS so really like reporting
uncertainties is one of the key things we are saying don't report these point estimates so what
people typically do is report a single number which is saying this is my aggregate result across
my benchmark we're saying oh no no there is actually some uncertainty in your aggregate result
and it can be larger it can be small so it's better to just report this uncertainty also so that
we have some idea of how high variance this algorithm is and how significant maybe the improvement is
so yeah so think about basically any sort of result you obtain from a finite number of seats
is a random variable at the end of the day and now there is some uncertainty associated with
so there is some sort of confidence interval associated with that and that's why I think so
any yeah so this is the sort of view we take that if you just think start thinking about random
variables it's a lot of things make sense so maybe I'll present like one metric which we do
proposes so think about the question of comparing to algorithms really what you have is you have
two random variables and you have a bunch of samples from both of them and you're trying to ask
me which random variable is better than the other now if you really if you think about it this way
in the natural thing which will come to me my end is well what is the probability that this
random variable is better than the other random variable and that's it you really just can compute
this problem now what you have is you really you don't have the distributions you just have the
empirical distributions but there is a way to compare the empirical distributions which is
which is you just to pair by some comparisons for whatever points you have and you get a probability
now here's the fun bit this probability itself is a random variable because if you repeat your
experiment your probability changes so there is a confidence band even though this is a probability
there is a confidence band around this probability but this this metric is a cool measure because
it directly asks the question okay what is the probability that you're baseline or your method
beats the baseline like if you're saying or claiming soda you can just evaluate this probability
itself it does not care so the downside is it does not care about the fixed size which is your method
can be 1% better or 100% better they'll still have the probability of 1 but at least ask this
there a question of okay am I doing better than the baseline and gives you like an estimate of
what is the likelihood that this is happening and you can glean a lot of the things they're
surprisingly so okay so this was the second metric and I think the third metric we realized that
so proposing or plotting distribution is a school and that is good for qualitative comparisons but
it's unlikely that a method is or like a new algorithm is going to outperform another method
in distribution across all the tasks and all the samples it's likely that these distributions
would overlap at certain point and all these things would happen so eventually I think or
research as a result with some sort of aggregate comparisons in some sense so we thought well
okay let's talk about aggregate metrics here which is what do we use we use mean or median
and this is common this is common across machine learning that whenever you have a benchmark
yeah there are these these are the two sort of metrics which are really prevalent everywhere
that either we report the mean performance or the median performance now at least in RL I think
mean is not so common the reason being there are few tasks which can be really high performing
or you're algorithm can be really high performing on some of these tasks and mean is just
dominated by these outlier tasks so let's say you get a score below one on almost all the tasks
except one where you get a score of 100 now mean score is just dominated by that one task so
typically in RL we perform this median score because it's more robust but the fun thing about median
is if I set the score to be zero and half of my tasks the median score wouldn't really change
and so what kind of robust measure is this where I can just really just crash in half of my tasks
and it wouldn't really reflect any sort of scenes so these aggregate metrics are somewhat misleading
so here again we look back let's statistics and we said well what we can do is we can really
again think about we have I don't know maybe five seats and 10 to ask the 15 numbers
we can compute something like the interquartial mean which is like somewhere in between the mean
and the median which is the mean of the middle 50 percent of the numbers you have so the middle
to the five runs and now this is this has like best of the both properties of the mean and the median
it cares about all the tasks in some sense and it also is robust to outliers like median
but it has nicer properties in with now so this is something that was like in general that when
you are trying to report aggregate benchmark performance there's something you can do but
that's it the caveat is that the aggregate really hides a lot of the information which was given
to you by the distribution which is aggregate is talking about a specific property so maybe I'll
give you one sort of anecdote related to this so Natari at least typically what people have
focused on is median scores or median normalized scores now the funny thing is we have algorithms
which are improving on the median normalized scores but we said let's come up with this metric called
which is a better version of mean which is optimality gap which is how far are we doing with respect
to the humans so rather than really talking about what is your average performance let's talk about
how far you are from the humans so lower is better so zero means you're really close to the humans
and one means you're really random like you're as worse as you can be so so this is a metric
be computed for all these existing algorithms and finally enough what we saw that a lot of these
recent algorithms were improving in terms of their median performance or mean performance but they
are doing worse in terms of their closeness to human performance which is in some sense they're
getting better on a lot of the games but they're also doing worse than humans on a lot of these
harder games possibly so that was something interesting and this is what happens I think when we
really focus on a single metric in some sense and that's a downside of like aggregation but I do
think aggregation is sort of a necessary ego because it's really hard to compare I don't know if
you have a lot of tasks and results it's really hard to compare their distributions or these
tables and people would ever want to report some sort of aggregate measure but aggregation has
this harms I was just going to jump in and ask since publishing this have you seen the community
kind of take up these additional measures and include them in their own research and publications
yeah so we have seen some pickup still pretty early to say what could happen but I do think
I at least have seen some pickup of this recently and like some of these I clear submissions and
some of the new to submissions themselves and I think the the reason the community would like
to pick this up is because it's really not asking you to do anything extra other than just
evaluating your results more thoroughly and we also really is actually a pretty good open-source
library so you just give me your row numbers for doing this so I think the reason we did that was
because we realized if you just say these are all the things you do it takes some work to actually
implement something and people are not going to do that if that's something additional they have
to do so we thought well let's just come up with a library so that you give me your numbers
and it'll just do the thing for you we even release the plotting scripts because I think we got
like some apprace for the kind of plots we had in the paper and we thought it might be better
actually if other researchers can use these kind of plots also and so the library you just kind of
call it in your training loop and it's recording your no and I thought you were doing that so it's
like you do whatever you're doing in your training loop at the end of the day you were reporting
performance also send these numbers to the library it'll just give you a plot pack and then
dad you can put directly in your paper without any like changes sort of so that's the night so
I think we tried to make it as easy for people to use this now there's a bigger question about
is there any incentive for researchers to do this because that's a tricky one but I do believe
I think a lot of us are at least trying to do good science and if people are aware of these sort
of issues they will go towards using them a lot of us are trying to publish and I guess those
people may be more worried about oh if I report these uncertainties my results don't look as good
as they did without these but the realities like these are the uncertainties are really just
telling you oh this is the randomness you have in your reporter to serve can you maybe qualify
or characterize the when you look at the existing algorithms did you
did you find anything particularly surprising or dramatic in the reported results or was it kind
of like there's a lot of variation here and you don't really know but it's not something that kind
of fundamentally changes the way you think about a particular algorithm I think some of the rankings
were even flipped in the because of the uncertainties that you see that the uncertainties for one
method are really like much larger than the other and it's likely the other thing I think you'll think
is a lot of the changes we were able to publish are actually not really resulting in an improvement
it's just that happened that there were random fluctuations and you ended up getting it's almost
like terrific but not exactly but it is something along those lines that there were enough random
fluctuations in the setup that it's likely that you benefited from that and because of that the
paper was able to be if like if you look at the empirical results only then it's possible that
the method is not really doing any better than soda you're saying that some percentage of papers
didn't really need to be papers in some sense yeah but I guess those are the papers which really
depend on just the empirical evaluation I would say most papers really are not here's my method
here's the 10 different benchmarks I evaluate them my method is better except I don't think those
are the kind of papers or at least a lot of the papers are like that papers really have a lot of
going on they have some sort of argument why this is a good algorithm or why this paper is worth
publishing and I think the empirical results are only the supporting evidence for all that I don't
think we're trying to sell the algorithm as being the best but we're trying to say oh here's a good
method and here's another argument for why this is a good method in some sense so I don't think
there's any downside of reporting uncertainties is just that I think the maybe as an author you might
feel that the reviewers would not like this if my method is not really beating everything straight
out of the like I don't know it's like they feel hiding the sort of information is better than
actually presenting it but the sort of reality is that if someone else was to run their algorithm
then they need to know what is the variation is in their method because it's possible I'm not
able to reproduce the results and the reason was as simple as oh I ran on a different GPU so this
happened on I think with one of the things I was trying so what happened is at least intensive
low in jacks you can't really set the random seed because there are some operations on GPU
which have randomness and which can't really be fixed now this was the only change in my code pace
I evaluated 100 seeds for a single method twice and what I saw that was the correlation between
the results was something around minus 0.2 to 0.2 which was like really tiny correlation basically
just like this saying all the seeds for really just one change of GPU randomness made a random
ness really a big issue eventually in metasels so this is a huge change even with like small randomness and maybe
it's like a butterfly effect another that initially you collect different data and then you different
update and what not and it expands but this sort of issues I think after our paper and I think
concurrent to our paper there were other works also which point out similar issues in vision
and natural language processing in fine tuning these pre-trained models they say that it really
depends on what exact model you use and all these other hypotheticals we set up so it's just
like I'm saying reporting uncertainty is more true to reality it might make the results look
slightly worse but this is what the reality looks like and now if you deliberately try to hide
reality that is like so I think ignorance is okay like I think most of the researchers were really
sort of not aware that these were the issues because some of our own papers had reported these
point estimates which is we said here's a benchmark here's an algorithm here's the result we get
and we just compared them but now we are aware that there is a huge uncertainty in these sort of
evaluations so we would be more willing to report these sort of things the showcase okay what is
the uncertainty or at least try to make sure that our results are robust statistically that is
like and that's I think this is also one of the reasons we see these papers that say oh here are
here we evaluated a bunch of these soda models and nothing really holds up in these applications
actually on a controlled experiment and the reason is it's just because a lot of the times you're
not reporting these sort of uncertainty measures and we thought like these methods were actually
better but they really were not in some sense yeah one maybe one point I want to say is like
so I think the actual incentive for doing all this is doing good and reproducible science
but I don't think that's really a strong incentive the better incentive would be if conferences
for example try to enforce this and they do to some extent so if you look at Neurif's checklist
for example they do ask that have you reported error bars and now the funny thing is people do
report yes to that question but also not report error bars when it comes to things like aggregate
metrics because like it's harder to think of oh this is an overall result across all the task
what is the error bar mean here in some sense and is the does the effect of the kind of randomness
we're talking about is it outsized in RL due to the kind of repetitive nature of it relative
to you know vision and LP other things yeah yeah I feel so there are like two kind of
yeah so I feel that is the case and there are two sources of it one is when you collect your own
data so it depends really on where you are at and if there was some randomness it will sort of
activate more over time because this randomness affected what was the initial data you collected
and then the second source is we are learning from our own predictions in some sense when we're
whenever we're doing off-false learning with Q learning and if you're so it really highly depends
on what your initial estimates of whatever the function your learning was so and I think both of
these amplify like the randomness so definitely I think it's more prevalent but then again there are
cases like I was talking about NLP earlier so when you do fine tuning a pre-trained model they find
there is actually a huge uncertainty there again depending on for example simple things like
what was the order of the samples you point you down over was the exact point you used to fine
you so there are like probably so I think any area where these sort of statistical considerations can
have large influence it should be the case that we report uncertainties so I I don't know where
did I see this but I saw like a paper where they said that even on image net if I just
framed a resonant model the variation is something around plus minus 1% so it's a Gaussian
distribution but depending on the seed I pick my result can be 1% or better or 1%
force and this is a huge number if we are talking about image net because people publish results
with like 0.1% of differences but yeah coming back to incentives I do think there's
it's unclear what are the actual incentives for researchers to probably or at least to do
these things because if it so rigor makes publications or harder to publish in some sense and
sometimes it does make it easier if your results are really great but most of the scenarios it will
have problems and researchers might not want to be more rigorous so that they can publish easily
so there is this trade-off I think but shouldn't really be a trade-off but if you're really going
for I want to publish this paper then I think you may not want to report uncertainties and work not
so that's something we are not clear on. Where does the these uncertainty measures that
that you're proposing here how do they fit into kind of a broader imagining that others have
published things that they also think that papers should all include and and how many of those
how long is that list and you know where does this fit in that list in terms of potential impact
on the field do you think. Right so I think there were a lot of things we took into consideration
when thinking about these metrics and we said let's try to do or at least build on what people
already actually do so people already actually report some sort of aggregate performance measures
at least in RL and we said what's the minimal change we can do rather than going to like complicated
statistical tests and what all those things people already want to report some sort of standard deviation
and oftentimes they do so if you use mean for example it's easy to compute the standard deviation
now the thing is when you think about median people haven't reported any sort of uncertain
measure and the reason is it's unclear what the standard deviation would look like or how would
you compute it unless you are willing to use that's a more advanced statistical would sharpen tools
so I think this was the reason or at least this is maybe one of the reasons that we typically
don't report uncertainty measures because we already do if we are able to do them it's just that
once your metric is not as straightforward as mean then I think it's become unclear how do you
actually calculate the variance and because the only way to do so is to simulate your distribution
and see what the variance looks like and so I think my and at least our understanding of the tools
we propose is are also some of these tools are already in use in computer science like I've been
saying like the sort of motivation for these performance profiles came from a paper published
in I think 2000 which is I think the standard for optimization software in computer science
they already use some of these tools and these are like tried and tested tools it's just that
maybe we can also adopt them and now the I think the maybe the sort of the question you're asking
for that I feel there might be better ways around reporting uncertainties for example because
we we just propose like the simplest possible measures which we can do and we tested their
like correctness on some of the tasks but it's possible that there might be better ways to report
uncertainty which are more correct in some sense or let's say theoretical guarantees around this
also I think some of the problems which we don't tackle is let's say I give you a single task
and you have really have three proceeds what do you do like I don't think our paper really talks
anything about that but that is a common setup I don't really have a benchmark I just have two
three tasks and I want to compare these algorithms what do I do to do like better comparisons in some
sense and that still is an open problem I think but with regards to where does it fit I would say
it's really the high level message of the paper is we should be reporting interval estimates of
things and here are some tools which we have developed where you can use but there might be other
ways also to do this and please feel free to do so I don't think you should be using these
specific tools if you feel that's not the right way to go but you should be reporting some measure
of uncertainty this is the high level message the maybe the only advantage of the tools is you
don't really have to do anything extra than what you have been doing like no extra logging no extra
sort of seed evaluation would not it is really geared towards a few seed and like multitasking
awesome well we shall thanks so much for taking us through that very cool work and
congrats again on the award
