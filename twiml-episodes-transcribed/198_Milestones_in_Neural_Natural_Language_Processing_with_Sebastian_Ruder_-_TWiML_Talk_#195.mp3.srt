1
00:00:00,000 --> 00:00:16,320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

2
00:00:16,320 --> 00:00:21,480
people, doing interesting things in machine learning and artificial intelligence.

3
00:00:21,480 --> 00:00:31,400
I'm your host Sam Charrington.

4
00:00:31,400 --> 00:00:34,600
In this episode we're joined by Sebastian Rooter.

5
00:00:34,600 --> 00:00:38,960
Sebastian is a PhD student studying natural language processing at the National University

6
00:00:38,960 --> 00:00:44,440
of Ireland and a research scientist at Text Analysis Startup, Alien.

7
00:00:44,440 --> 00:00:49,120
In our conversation, Sebastian and I discussed recent milestones in neural NLP, including

8
00:00:49,120 --> 00:00:52,480
multi-task learning and pre-trained language models.

9
00:00:52,480 --> 00:00:57,680
We also discussed the use of attention-based models, tree RNNs and LSTMs and memory-based

10
00:00:57,680 --> 00:00:58,680
networks.

11
00:00:58,680 --> 00:01:04,320
Finally, Sebastian walks us through his recent UML fit paper, short for universal language

12
00:01:04,320 --> 00:01:09,060
model fine-tuning for text classification, which he co-authored with Jeremy Howard

13
00:01:09,060 --> 00:01:15,800
afast.ai, who I interviewed in episode 186.

14
00:01:15,800 --> 00:01:19,420
Before we dive into the conversation, I'd like to send a huge thanks to our friends at

15
00:01:19,420 --> 00:01:22,440
IBM for their sponsorship of this show.

16
00:01:22,440 --> 00:01:27,440
Interested in exploring code patterns leveraging multiple technologies, including ML and AI,

17
00:01:27,440 --> 00:01:29,440
then check out IBM Developer.

18
00:01:29,440 --> 00:01:34,600
With more than 100 open source programs, a library of knowledge resources, developer advocates

19
00:01:34,600 --> 00:01:40,400
ready to help, and a global community of developers, what in the world will you create?

20
00:01:40,400 --> 00:01:45,760
Live in at IBM.biz slash ML AI podcast, and be sure to let them know that Twimmel sent

21
00:01:45,760 --> 00:01:46,760
you.

22
00:01:46,760 --> 00:01:50,320
And now onto the show.

23
00:01:50,320 --> 00:01:53,320
All right, everyone.

24
00:01:53,320 --> 00:01:56,200
I am on the line with Sebastian Rooter.

25
00:01:56,200 --> 00:02:01,160
Sebastian is a PhD student studying natural language processing at the National University

26
00:02:01,160 --> 00:02:05,360
of Ireland, as well as a research scientist at Alien.

27
00:02:05,360 --> 00:02:08,160
Sebastian, welcome to this week in Machine Learning and AI.

28
00:02:08,160 --> 00:02:09,160
Hey, Sam.

29
00:02:09,160 --> 00:02:10,360
Thanks for having me.

30
00:02:10,360 --> 00:02:12,880
Yeah, I'm really excited about having you on the show.

31
00:02:12,880 --> 00:02:15,560
We've been trying to coordinate this for a while.

32
00:02:15,560 --> 00:02:20,120
And so thank you once again, that you've been doing some really interesting work at kind

33
00:02:20,120 --> 00:02:25,760
of the intersection of deep learning and natural language processing that I'm looking forward

34
00:02:25,760 --> 00:02:27,120
to diving into.

35
00:02:27,120 --> 00:02:30,640
But before we do that, I'd love to learn a bit about your background.

36
00:02:30,640 --> 00:02:35,920
You've been studying natural language processing and computational linguistics in particular

37
00:02:35,920 --> 00:02:38,400
since undergrad days.

38
00:02:38,400 --> 00:02:42,560
How did you get interested in into this field of study?

39
00:02:42,560 --> 00:02:48,240
Yeah, so yes, I said I did my undergrad in computational linguistics at the University

40
00:02:48,240 --> 00:02:50,520
of Highlandberg in Germany.

41
00:02:50,520 --> 00:02:57,440
And computational linguistics really is still quite a small field, or at least when I was

42
00:02:57,440 --> 00:03:01,680
studying that, there wasn't that much interest in it.

43
00:03:01,680 --> 00:03:09,560
And so how I initially got into that was that I was kind of doing high school, really interested

44
00:03:09,560 --> 00:03:13,320
in both mathematics and languages as well.

45
00:03:13,320 --> 00:03:17,360
So I really liked kind of learning different languages, but at the same time kind of the

46
00:03:17,360 --> 00:03:20,200
analytical side of math really appealed to me.

47
00:03:20,200 --> 00:03:26,440
And so I really was looking for something that like for some fields that were I could manage

48
00:03:26,440 --> 00:03:30,000
to combine both of those maybe quite different areas.

49
00:03:30,000 --> 00:03:35,240
And then in doing research, I really found that computational linguistics was really the

50
00:03:35,240 --> 00:03:41,200
field which is really at the intersection of yeah, like computer science and linguistics.

51
00:03:41,200 --> 00:03:48,160
And so for me, I tried that out and really seemed to be the perfect fit for me personally.

52
00:03:48,160 --> 00:03:53,480
And a lot of the kind of my classmates at that time, what I really liked about computational

53
00:03:53,480 --> 00:03:59,960
linguistics was that it seemed to because you kind of need such a like a peculiar

54
00:03:59,960 --> 00:04:02,360
set of interests.

55
00:04:02,360 --> 00:04:11,040
So there were a lot of people in that subject with like a variety of different interests,

56
00:04:11,040 --> 00:04:15,160
people who were interested in kind of other things, social sciences, practical linguistics

57
00:04:15,160 --> 00:04:16,160
as well.

58
00:04:16,160 --> 00:04:20,680
So really had kind of a very wide variety broad set of backgrounds.

59
00:04:20,680 --> 00:04:26,680
And I think that made studying and working in the field particularly interesting for

60
00:04:26,680 --> 00:04:27,680
me as well.

61
00:04:27,680 --> 00:04:31,440
Tell us a little bit about your graduate work.

62
00:04:31,440 --> 00:04:36,640
And yeah, so I did kind of the undergraduate in that there.

63
00:04:36,640 --> 00:04:42,520
And then afterwards I was really looking for, I really wanted to stay in the field as

64
00:04:42,520 --> 00:04:47,280
well, but I was looking for to get some industry experience first.

65
00:04:47,280 --> 00:04:54,120
But then in kind of researching for a job, I came across a program kind of an industrial

66
00:04:54,120 --> 00:05:00,080
PhD program where it could combine research and kind of industry work as well.

67
00:05:00,080 --> 00:05:02,400
And that really appealed to me.

68
00:05:02,400 --> 00:05:06,520
And yeah, I decided to go in that direction, consequently.

69
00:05:06,520 --> 00:05:12,400
And then doing my graduate work, I was really been looking for something initially I started

70
00:05:12,400 --> 00:05:15,440
working on kind of a lot of the applications of NLP.

71
00:05:15,440 --> 00:05:22,840
So as it's often used in the industry setting, like working on different applications of

72
00:05:22,840 --> 00:05:28,560
text classification, like sentiment analysis, or different forms of information extraction.

73
00:05:28,560 --> 00:05:34,000
And then kind of working on those very downstream tasks, I realized that actually the most kind

74
00:05:34,000 --> 00:05:39,200
of the challenge that permitted those or what was really at the heart of solving them was

75
00:05:39,200 --> 00:05:45,160
really because we had to create text classification applications for a lot of different domains,

76
00:05:45,160 --> 00:05:48,760
different languages, different data from different customers as well.

77
00:05:48,760 --> 00:05:54,920
So I really realized that actually we in order to solve those challenges better, we actually

78
00:05:54,920 --> 00:05:59,760
need to have better methods and better algorithms to learn from limited amounts of label data.

79
00:05:59,760 --> 00:06:04,680
So really having either models that analyze better or better ways to learn from either

80
00:06:04,680 --> 00:06:10,720
distant or distant supervision or from unlabeled data to.

81
00:06:10,720 --> 00:06:17,680
So this kind of idea of how can we more efficiently use label data or how can we learn from additional

82
00:06:17,680 --> 00:06:23,840
sources of supervision has really driven a lot of my interest and really a lot of my graduate

83
00:06:23,840 --> 00:06:24,840
work.

84
00:06:24,840 --> 00:06:30,600
And you're you're finishing up your PhD kind of any day now, right, you're on your last

85
00:06:30,600 --> 00:06:31,600
year?

86
00:06:31,600 --> 00:06:36,240
Yeah, I'm in my last year some currently in the process of just writing up my thesis and

87
00:06:36,240 --> 00:06:40,680
putting those different projects on which I worked on it together essentially.

88
00:06:40,680 --> 00:06:45,480
And I guess the last background question you are also working as a research scientist

89
00:06:45,480 --> 00:06:50,840
at Alien, which it sounds like is through this industrial PhD program?

90
00:06:50,840 --> 00:06:53,000
Yeah, exactly.

91
00:06:53,000 --> 00:06:55,280
And what does Alien do?

92
00:06:55,280 --> 00:07:01,960
So Alien is focused on the natural language for us in startup, which is mainly focused

93
00:07:01,960 --> 00:07:07,040
on analyzing and providing services around news.

94
00:07:07,040 --> 00:07:12,400
So we have some kind of a different number of products, one which aims at aggregating

95
00:07:12,400 --> 00:07:18,480
news and enriching news with different forms of kind of semantic information like entities,

96
00:07:18,480 --> 00:07:24,880
but also relations and so and sentiment to doing things like named entry recognition and

97
00:07:24,880 --> 00:07:28,120
to linking sentiment analysis around those.

98
00:07:28,120 --> 00:07:33,280
And then there's kind of a separate set of services which are which developers can integrate

99
00:07:33,280 --> 00:07:37,920
into their own applications to kind of use NLP in their own services.

100
00:07:37,920 --> 00:07:43,120
And then finally, we also do a fair amount of consulting to developing kind of specialized

101
00:07:43,120 --> 00:07:46,240
applications for particular customers.

102
00:07:46,240 --> 00:07:49,000
Okay, very cool.

103
00:07:49,000 --> 00:07:58,320
And so maybe to kind of dive into some of your work in NLP, you recently wrote I think

104
00:07:58,320 --> 00:08:05,360
on your blog post about kind of the history of natural language processing from your perspective.

105
00:08:05,360 --> 00:08:13,920
Maybe kind of walk us through the, maybe not the full history, but kind of the interesting

106
00:08:13,920 --> 00:08:18,640
recent historical developments in the field.

107
00:08:18,640 --> 00:08:19,640
Sure, yeah.

108
00:08:19,640 --> 00:08:26,040
So that blog post kind of arose in conjunction or as a byproduct of a session of natural

109
00:08:26,040 --> 00:08:30,840
language processing that we organized at the deep learning level, which is kind of a

110
00:08:30,840 --> 00:08:36,480
big initiative kind of a summer school like event in South Africa that kind of sought

111
00:08:36,480 --> 00:08:41,480
to bring together and strengthen the African machine learning community.

112
00:08:41,480 --> 00:08:47,160
And for a session of natural language processing, we essentially wanted to give kind of both

113
00:08:47,160 --> 00:08:52,480
beginners as well as maybe people who've already worked with NLP and overview of maybe

114
00:08:52,480 --> 00:08:58,320
some of the most important milestones that are still relevant today or that they can

115
00:08:58,320 --> 00:09:05,200
still like knowing them would still kind of help them tackle ongoing challenges.

116
00:09:05,200 --> 00:09:13,520
And in compiling this kind of this overview, Herman Camper, who co-organized the session

117
00:09:13,520 --> 00:09:19,800
with me, we essentially started out because we really wanted to have to get something

118
00:09:19,800 --> 00:09:25,000
that not only reflects our opinion, but also some opinions of a larger set of experts.

119
00:09:25,000 --> 00:09:30,200
So we started out just sending emails to a number of experts, NLP, just to get a bit

120
00:09:30,200 --> 00:09:35,040
more, more suggestions and more ideas.

121
00:09:35,040 --> 00:09:39,440
And then essentially we compiled kind of the eight milestones that were mentioned.

122
00:09:39,440 --> 00:09:42,400
And then we found relevant into a list here.

123
00:09:42,400 --> 00:09:49,600
And yeah, and so just because the main kind of category of methods are currently being

124
00:09:49,600 --> 00:09:56,040
used in NLP at this point, are really mostly using neural network-based approaches.

125
00:09:56,040 --> 00:10:01,920
So because of that, we mainly focused on kind of approaches or milestones that have relevance

126
00:10:01,920 --> 00:10:07,560
to that, although in the post there's also a lot of other kind of more traditional milestones

127
00:10:07,560 --> 00:10:13,720
that led up to that research in which kind of a lot of research later on is building on.

128
00:10:13,720 --> 00:10:19,480
Yeah, so to think so, essentially the milestones we arrived at were kind of the

129
00:10:19,480 --> 00:10:27,880
first neural language models that were proposed in 2001, kind of as a something that has been

130
00:10:27,880 --> 00:10:35,320
in many different ways kind of built upon and or co-opted by subsequent approaches because

131
00:10:35,320 --> 00:10:40,440
a lot of different tasks like sequence-to-sequence learning or even learning word embeddings can

132
00:10:40,440 --> 00:10:46,160
be expressed as some sort of variation of just doing language modeling.

133
00:10:46,160 --> 00:10:51,480
And something that is becoming more prevalent these days is multi-task learning, which

134
00:10:51,480 --> 00:10:59,520
was first proposed for NLP in 2008, as kind of a very milestone, very seminal paper of

135
00:10:59,520 --> 00:11:01,360
Colourbird and Destiny.

136
00:11:01,360 --> 00:11:05,680
And that paper also got a test of time award at ICNL this year.

137
00:11:05,680 --> 00:11:07,520
And then probably what was that paper called?

138
00:11:07,520 --> 00:11:12,680
Yeah, so it's like a unified architecture of four-netual language processing, I think.

139
00:11:12,680 --> 00:11:16,600
Yeah, deep neural networks with multi-task learning, I think that's the full title.

140
00:11:16,600 --> 00:11:20,840
Yeah, so that's definitely, if you haven't got that paper yet, that's really worth reading.

141
00:11:20,840 --> 00:11:27,200
And I'm surprised to hear that one was 10 years ago, multi-task learning seems to be just

142
00:11:27,200 --> 00:11:29,200
getting popular.

143
00:11:29,200 --> 00:11:36,000
Well, I should say I'm reading a lot more about it over the past, maybe six to nine months.

144
00:11:36,000 --> 00:11:41,080
So in that paper in particular, that introduced kind of a lot of different techniques like

145
00:11:41,080 --> 00:11:47,240
using CNNs for text, for instance, or even using something like unsuverse learning of

146
00:11:47,240 --> 00:11:48,240
word embeddings.

147
00:11:48,240 --> 00:11:50,160
That was in 2008.

148
00:11:50,160 --> 00:11:55,640
And only about five years later, people were really starting to use word embeddings.

149
00:11:55,640 --> 00:12:00,680
And even nowadays, yeah, so now like 10 years later, people are actually really starting

150
00:12:00,680 --> 00:12:02,480
to pick up multi-task learning.

151
00:12:02,480 --> 00:12:08,480
So in many ways, that particular paper was kind of a bit ahead of its time and not really

152
00:12:08,480 --> 00:12:16,640
widely kind of regarded or not a lot of people kind of took inspiration from that paper

153
00:12:16,640 --> 00:12:17,640
at that time.

154
00:12:17,640 --> 00:12:21,800
And then kind of probably what most of the experts we were talking to mentioned really

155
00:12:21,800 --> 00:12:28,200
as one of the biggest recent milestones was the use of word embeddings that were pre-trained

156
00:12:28,200 --> 00:12:36,440
and learned in unsuverse way on a large corpus so that really started out with word-to-vec.

157
00:12:36,440 --> 00:12:41,080
And kind of a whole separate branch of research that I've kicked off of really trying to

158
00:12:41,080 --> 00:12:45,200
better understand what these word embeddings capture.

159
00:12:45,200 --> 00:12:51,320
And word embeddings really are still very widely used, even word-to-vec is still often

160
00:12:51,320 --> 00:12:54,160
used in papers these days.

161
00:12:54,160 --> 00:13:00,600
And then kind of the last milestones you mentioned were essentially just the more prevalent use

162
00:13:00,600 --> 00:13:03,000
of using neural networks for NLP.

163
00:13:03,000 --> 00:13:07,880
And mainly then kind of in three different types, so essentially the main types people

164
00:13:07,880 --> 00:13:13,600
are using these days are recurrent neural networks and mainly these long short-term memory

165
00:13:13,600 --> 00:13:21,480
networks which are kind of a modification on this classic recurrent neural network.

166
00:13:21,480 --> 00:13:27,120
Then convolutional neural networks have been proposed for NLP and also used in particular

167
00:13:27,120 --> 00:13:33,760
for some text classification tasks like sentiment analysis.

168
00:13:33,760 --> 00:13:39,520
They have been used quite often and now more recently also for different tasks like machine

169
00:13:39,520 --> 00:13:43,880
translation for instance because people have realized that using these conclusions you

170
00:13:43,880 --> 00:13:52,840
can actually capture more efficiently long term dependencies in your data by using kind

171
00:13:52,840 --> 00:13:57,560
of wider receptive fields with daily latest convolutions in your models.

172
00:13:57,560 --> 00:14:03,760
So there's been some recent scene end-based models for machine translation.

173
00:14:03,760 --> 00:14:09,880
And then one particular category that I find quite interesting in NLP is that you have

174
00:14:09,880 --> 00:14:15,920
because linguistically if you look at a sentence it really, its sentence can be kind of split

175
00:14:15,920 --> 00:14:16,920
up hierarchically.

176
00:14:16,920 --> 00:14:22,080
So you have words or even like going on the sub word level you have different more themes

177
00:14:22,080 --> 00:14:27,720
that compose into a word and then you have words that form different clauses or chunks

178
00:14:27,720 --> 00:14:28,720
in a sentence.

179
00:14:28,720 --> 00:14:33,760
So instead of processing a sentence sequentially you can have models that take its input

180
00:14:33,760 --> 00:14:38,120
or act upon this tree structure as well.

181
00:14:38,120 --> 00:14:43,440
And these have been kind of called historically like recursive neural networks because they

182
00:14:43,440 --> 00:14:49,760
kind of recursively from the bottom up starting from the individual words builds the representation

183
00:14:49,760 --> 00:14:55,000
of the entire sentence using some sort of composition function.

184
00:14:55,000 --> 00:14:59,320
And more recently they're also called tree RNNs or tree LSTMs.

185
00:14:59,320 --> 00:15:04,760
So you can have an LSTM that doesn't act on a sequence but on entire tree.

186
00:15:04,760 --> 00:15:08,120
How much are those used and how well do they work?

187
00:15:08,120 --> 00:15:15,920
Yeah, so it really depends on the kind of tasks you use so they can be kind of useful if

188
00:15:15,920 --> 00:15:21,680
you want to, for instance, incorporate so for machine translation some of these some

189
00:15:21,680 --> 00:15:27,400
modifications using these kind of three-based methods have been proposed essentially to

190
00:15:27,400 --> 00:15:31,560
incorporate a bit more syntactic structure into the model.

191
00:15:31,560 --> 00:15:36,760
So just to make it a bit easier for the model to model this type of structure which helps

192
00:15:36,760 --> 00:15:42,360
when translating into certain languages or maybe if you translate from one language

193
00:15:42,360 --> 00:15:50,120
where, for instance, the verb comes, so with the object comes first to an language where

194
00:15:50,120 --> 00:15:55,000
this, where the position of object is reversed, for instance.

195
00:15:55,000 --> 00:16:02,720
So in some sense this can help you model yeah, some syntactic relations and or yeah another

196
00:16:02,720 --> 00:16:07,320
task like for sentiment, there's been some datasets where you can have kind of supervision

197
00:16:07,320 --> 00:16:12,680
at the word level where these for which task these have been originally proposed.

198
00:16:12,680 --> 00:16:18,520
So there's been a few things in general still if you have like an arbitrary NP task, probably

199
00:16:18,520 --> 00:16:26,040
like a bidirectional LSTM will still get you further probably but I would guess or I really

200
00:16:26,040 --> 00:16:33,040
hope to see, I personally would hope to see more of these models that can actually incorporate

201
00:16:33,040 --> 00:16:38,520
some linguistic information in kind of feel as a future direction.

202
00:16:38,520 --> 00:16:45,160
So I think this maybe while it's maybe not the most performant model yet, I think it's

203
00:16:45,160 --> 00:16:52,440
really a kind of useful way or useful kind of method or kind of class of methods to

204
00:16:52,440 --> 00:16:53,440
think about.

205
00:16:53,440 --> 00:16:59,480
And more recently you can have people have started using kind of graph convolution networks

206
00:16:59,480 --> 00:17:06,240
which basically apply like a scene in the convolution operation on on this tree as well.

207
00:17:06,240 --> 00:17:10,400
And they've shown some good results for yeah, for tasks we have some dependencies like

208
00:17:10,400 --> 00:17:16,160
semantic role labeling for instance, like semantic what like semantic role labeling,

209
00:17:16,160 --> 00:17:22,920
which is yeah, which is kind of a classic LP task and basically a form of so it's also

210
00:17:22,920 --> 00:17:29,720
people also refer to it as shallow semantic parsing and it essentially comes kind of from

211
00:17:29,720 --> 00:17:38,280
a and has like an underlying theory in that you have different frames of sentences.

212
00:17:38,280 --> 00:17:44,320
So depending what works, you use each verb put a vocal different frame and then you would

213
00:17:44,320 --> 00:17:51,080
essentially try to label the arguments of that verb that coca with that frame.

214
00:17:51,080 --> 00:17:55,720
So it's kind of a more linguistically informed or a classic linguistic task that people

215
00:17:55,720 --> 00:17:56,960
have been evaluating on.

216
00:17:56,960 --> 00:18:00,680
The next milestone that you pick up is sequence sequence models, right?

217
00:18:00,680 --> 00:18:02,880
Yes, exactly.

218
00:18:02,880 --> 00:18:06,400
Those have been popping up all over the place, not just NLP.

219
00:18:06,400 --> 00:18:11,520
Yeah, so it's really kind of like, yeah, like in doing that review, I looked back at

220
00:18:11,520 --> 00:18:15,920
the paper again and they in their paper, I think mainly evaluated on machine translation,

221
00:18:15,920 --> 00:18:21,520
which is I think really the kind of the biggest application these kinds of models have been

222
00:18:21,520 --> 00:18:23,760
applied to.

223
00:18:23,760 --> 00:18:28,000
But then, yeah, so for context sequence, the sequence models are essentially just a general

224
00:18:28,000 --> 00:18:31,920
kind of framework that tries to map an input sequence so it can be words, but can also

225
00:18:31,920 --> 00:18:39,320
be any other type of symbols to an output sequence using an encoder neural network that encodes

226
00:18:39,320 --> 00:18:44,240
this input sequence into a fixed access vector and then output sequence that's given that

227
00:18:44,240 --> 00:18:47,160
vector generates the target sequence.

228
00:18:47,160 --> 00:18:52,320
And yeah, and this has already been proposed in 2015 for machine translation, but since

229
00:18:52,320 --> 00:18:58,760
then basically a lot of applications in NLP, like as soon as you try to generate a sentence,

230
00:18:58,760 --> 00:19:04,720
most of these models would try to use kind of a sequence, a sequence model under the hood.

231
00:19:04,720 --> 00:19:10,960
So these have been very popular for things like chatbots or even in settings where you

232
00:19:10,960 --> 00:19:15,560
don't have an input sequence as an input, but just kind of another representation.

233
00:19:15,560 --> 00:19:21,160
Like for image captioning, for instance, you can imagine that you have a CNN model that

234
00:19:21,160 --> 00:19:27,400
generates a representation of an image and then based on that, you would use that as

235
00:19:27,400 --> 00:19:32,840
initialization for your LSTM and you would then try to generate the caption of that image.

236
00:19:32,840 --> 00:19:38,920
So this kind of framework has really turned out to be very versatile and really applicable

237
00:19:38,920 --> 00:19:40,600
in a lot of scenarios.

238
00:19:40,600 --> 00:19:43,120
You also mentioned attention-based methods.

239
00:19:43,120 --> 00:19:45,600
How do those fit into NLP?

240
00:19:45,600 --> 00:19:51,360
Yeah, so attention, probably like everyone who's kind of aware of sequence sequence model

241
00:19:51,360 --> 00:19:53,640
probably has also heard of attention.

242
00:19:53,640 --> 00:20:01,280
So attention was kind of the probably the main improvement that has really allowed machine

243
00:20:01,280 --> 00:20:10,400
translation models to exceed and overcome or outperform classic phrase-based or statistical

244
00:20:10,400 --> 00:20:13,080
machine translation models.

245
00:20:13,080 --> 00:20:16,720
And attention really, so as I mentioned before, in classic sequence to sequence learning,

246
00:20:16,720 --> 00:20:22,240
you would try to have a model that compresses the input sequence into a fixed size vector.

247
00:20:22,240 --> 00:20:27,400
But that, if you can imagine for machine translation, if you have a very long sentence or

248
00:20:27,400 --> 00:20:33,040
even entire paragraph that you might want to translate to, it can be very challenging

249
00:20:33,040 --> 00:20:37,240
or it can place a lot of computation burden on the model to try to compress the entire

250
00:20:37,240 --> 00:20:41,640
meaning of this paragraph into a fixed size vector.

251
00:20:41,640 --> 00:20:46,560
And attention essentially allows the model to kind of overcome this bottleneck.

252
00:20:46,560 --> 00:20:51,640
So in addition to having this fixed size vector, at every step of generating an output

253
00:20:51,640 --> 00:20:57,080
symbol, the model can also look back at kind of the input sequence and at the different

254
00:20:57,080 --> 00:20:59,160
hidden states of the input sequence.

255
00:20:59,160 --> 00:21:04,360
And essentially, it allows the model to, instead of having to compress everything into

256
00:21:04,360 --> 00:21:12,960
one state, essentially remember kind of the most similar past states, which then makes

257
00:21:12,960 --> 00:21:17,120
it easier for the model to generate the relevant output.

258
00:21:17,120 --> 00:21:22,400
And yeah, and attention also is really something that has turned out to be quite flexible.

259
00:21:22,400 --> 00:21:29,000
So that is really now used, I think in most sequences to sequence learning tasks as well,

260
00:21:29,000 --> 00:21:35,880
that has also been used in, for instance, in image captioning, so you could imagine that

261
00:21:35,880 --> 00:21:42,280
instead of kind of paying attention to different parts of your input sequence, when you just

262
00:21:42,280 --> 00:21:47,640
try to generate a caption or individual words with some image, you can also pay particular

263
00:21:47,640 --> 00:21:52,960
attention to certain parts of that image that might be relevant for generating that next

264
00:21:52,960 --> 00:21:53,960
word.

265
00:21:53,960 --> 00:22:00,640
Or yeah, generally so I think that now this is really common, the attention because this

266
00:22:00,640 --> 00:22:08,880
idea of just computing kind of a weight or kind of a weighted average of your input sequence

267
00:22:08,880 --> 00:22:11,840
or of certain states based on the similarity.

268
00:22:11,840 --> 00:22:17,960
So the doctorate to your current state really has allowed kind of models that can access

269
00:22:17,960 --> 00:22:25,360
memory based on similarity to kind of current state, or also these recent, basically kind

270
00:22:25,360 --> 00:22:30,960
of state of the app models for machine translation, which is a transformer, which essentially

271
00:22:30,960 --> 00:22:37,920
uses self attention, so is the attention not applied to previous sequence, but to the sequence

272
00:22:37,920 --> 00:22:44,120
itself to essentially allow the model to kind of look at and consider the surrounding

273
00:22:44,120 --> 00:22:50,080
words and the surrounding context to improve the representation of the current word.

274
00:22:50,080 --> 00:22:55,360
And by using multiple layers, this can be done multiple times so that you can have a kind

275
00:22:55,360 --> 00:23:01,160
of a more contextually sensitive and a hopefully more expressive or better representation of

276
00:23:01,160 --> 00:23:02,880
each word in your sentence.

277
00:23:02,880 --> 00:23:09,800
Is this related at all to the representations of networks like wavenet where you've got

278
00:23:09,800 --> 00:23:17,920
like these hierarchical layers that are folding upwards, and does that kind of relate to

279
00:23:17,920 --> 00:23:19,280
wavenet, for example?

280
00:23:19,280 --> 00:23:26,040
Yeah, so basically in kind of these conditional sequence models, you get, you can model like

281
00:23:26,040 --> 00:23:31,480
the kind of longer dependencies between different words, but essentially stacking these conditional

282
00:23:31,480 --> 00:23:33,080
layers on top of each other.

283
00:23:33,080 --> 00:23:41,440
So as you go higher, each word will be able to kind of access or consider kind of words

284
00:23:41,440 --> 00:23:47,520
that have become more, that have become, come earlier in the sentence.

285
00:23:47,520 --> 00:23:48,520
Yeah.

286
00:23:48,520 --> 00:23:54,760
So with just a few layers, if you use stylish convolutions, you can have more, you can,

287
00:23:54,760 --> 00:23:59,440
you're able to incorporate almost the entire sequence length.

288
00:23:59,440 --> 00:24:04,400
And with attention, essentially you can do that instead, you don't need to stack layers

289
00:24:04,400 --> 00:24:09,640
at all because the attention mask or this attention operation allows you to directly look

290
00:24:09,640 --> 00:24:12,640
at all of the words in the sentence.

291
00:24:12,640 --> 00:24:18,920
So essentially you kind of have this, you have like a global context, essentially every

292
00:24:18,920 --> 00:24:24,440
layer, whereas in this conditional one, you have a local context, which can very quickly

293
00:24:24,440 --> 00:24:27,360
scale to a global context.

294
00:24:27,360 --> 00:24:32,520
So it's an attention layer intuitively kind of makes it easier to model this global context,

295
00:24:32,520 --> 00:24:37,280
but on the other hand, you miss out on some of this local information again, because

296
00:24:37,280 --> 00:24:46,120
your mask or your attention operation is kind of irrespective of the position of the words.

297
00:24:46,120 --> 00:24:51,840
So because of that, people typically use kind of an embedding of the position or some

298
00:24:51,840 --> 00:24:56,520
function that indicates where the word in the sentence is situated to be able to kind

299
00:24:56,520 --> 00:24:58,960
of model this local context again.

300
00:24:58,960 --> 00:25:04,440
When you describe attention, you're talking about this attention mask, which essentially

301
00:25:04,440 --> 00:25:08,760
kind of weights where you're looking at in the input vectors, is that the right way to

302
00:25:08,760 --> 00:25:10,280
think about it?

303
00:25:10,280 --> 00:25:11,280
Exactly.

304
00:25:11,280 --> 00:25:16,040
So you generally have some, some attention weights that you, that you compute, and then

305
00:25:16,040 --> 00:25:17,360
you do softmax over that.

306
00:25:17,360 --> 00:25:23,440
So in the end, you get some, you essentially get kind of a probability distribution that

307
00:25:23,440 --> 00:25:30,160
tells you how relevant each of the words is or how similar they are to your current input.

308
00:25:30,160 --> 00:25:35,400
And then, and then in attention, the way these attention weights are computed that can

309
00:25:35,400 --> 00:25:39,720
differ a bit, and that usually includes some number of learned parameters so that model

310
00:25:39,720 --> 00:25:47,520
can kind of learn to pay attention or with regard to different aspects potentially.

311
00:25:47,520 --> 00:25:52,960
And so that the next milestone on your list are memory-based networks, and that's not

312
00:25:52,960 --> 00:25:55,760
a term that I've heard used.

313
00:25:55,760 --> 00:25:57,280
What is that referring to?

314
00:25:57,280 --> 00:25:58,280
Yeah.

315
00:25:58,280 --> 00:26:03,840
So we use that to refer to basically kind of a variety of different methods that essentially

316
00:26:03,840 --> 00:26:06,880
have some sort of memory mechanism, yeah.

317
00:26:06,880 --> 00:26:14,920
So essentially these methods are kind of very similar to attention in a way in that you

318
00:26:14,920 --> 00:26:20,640
can kind of see attention as essentially using the past instance of your model essentially

319
00:26:20,640 --> 00:26:22,760
as a memory or like a lookup table.

320
00:26:22,760 --> 00:26:27,480
So by using attention in a model essentially has like a memory that is the size of the

321
00:26:27,480 --> 00:26:29,600
input sequence.

322
00:26:29,600 --> 00:26:37,600
And these other methods are similar in a way, but some of them have kind of in addition

323
00:26:37,600 --> 00:26:43,800
a kind of a not only that you're able to read from this memory, but you can also write

324
00:26:43,800 --> 00:26:45,760
back at the memory.

325
00:26:45,760 --> 00:26:51,800
So probably I think deep-mind, they have, you know, researchers from deep-mind probably

326
00:26:51,800 --> 00:26:57,480
have produced most of these models that fall under this category with some coming from

327
00:26:57,480 --> 00:26:59,000
fair as well.

328
00:26:59,000 --> 00:27:04,600
And yeah, probably one of the most common ones is kind of this neural touring machine,

329
00:27:04,600 --> 00:27:10,000
which essentially which essentially tries to mimic kind of the classic touring machine,

330
00:27:10,000 --> 00:27:15,040
which has like this tape where you can read and write to and which essentially tries to

331
00:27:15,040 --> 00:27:21,280
use this concept and implement that in a neural network by having a memory where you

332
00:27:21,280 --> 00:27:27,240
can basically read and write to as well, but with a mechanism that is very similar to kind

333
00:27:27,240 --> 00:27:32,360
of attention that you can basically access content based on this on similarity, but you

334
00:27:32,360 --> 00:27:39,960
can also have some location based addressing in that you kind of know, okay, you've stored

335
00:27:39,960 --> 00:27:44,440
this information in a certain part of your memory, so you can add access that based

336
00:27:44,440 --> 00:27:47,600
on location as well.

337
00:27:47,600 --> 00:27:54,400
And these types of models really are kind of, I've found it kind of useful to include

338
00:27:54,400 --> 00:28:00,920
this, this particular category of models, because I think this, so firstly because this

339
00:28:00,920 --> 00:28:06,600
concept of memory is kind of closely tied to attention, but also because I think going

340
00:28:06,600 --> 00:28:12,840
forward having some sort of memory or some sort of kind of additional capacity of the

341
00:28:12,840 --> 00:28:19,840
model to access additional information in some form or another, so for instance having

342
00:28:19,840 --> 00:28:25,560
access in some way to a knowledge base that you can, that you can kind of query from

343
00:28:25,560 --> 00:28:34,400
or write to as well, or having some kind of more expressive way of storing, remembering

344
00:28:34,400 --> 00:28:40,000
like reading, storing facts that you've read about and then recalling them.

345
00:28:40,000 --> 00:28:45,960
I think going forward and really to really think we can probably talk about that later

346
00:28:45,960 --> 00:28:52,360
in more depth as well, but really to kind of have models that can kind of reason and

347
00:28:52,360 --> 00:28:59,040
have better reason abilities, we need some sort of memory of something, some mechanism

348
00:28:59,040 --> 00:29:01,360
that comes at least close to that.

349
00:29:01,360 --> 00:29:09,400
It does seem a bit like the previous milestones that you've mentioned, the impacts of those

350
00:29:09,400 --> 00:29:16,160
are fairly clear with these memory-based networks, it's a bit more speculative, do you

351
00:29:16,160 --> 00:29:17,160
feel that way?

352
00:29:17,160 --> 00:29:23,240
Yeah, I definitely agree with that, so most of these methods have either been evaluated

353
00:29:23,240 --> 00:29:31,040
on either kind of synthetic tasks, like counting, sorting, things like that, or on, so kind

354
00:29:31,040 --> 00:29:39,120
of the baby dataset from Facebook AI research, which is essentially a synthetically

355
00:29:39,120 --> 00:29:45,240
generated dataset for reading comprehension, so essentially they generate kind of stories

356
00:29:45,240 --> 00:29:51,880
which consist of different sentences, which involve multiple actors and kind of objects

357
00:29:51,880 --> 00:29:56,960
as people moving a ball or bringing a ball to different rooms in the end, the question

358
00:29:56,960 --> 00:29:59,280
would be, okay, where's the ball?

359
00:29:59,280 --> 00:30:06,120
And so most of these methods have been incorporated on this dataset, which is quite synthetic

360
00:30:06,120 --> 00:30:12,240
in the way it was created, so in practice people haven't really seen them scale too well

361
00:30:12,240 --> 00:30:16,920
to kind of more realistic datasets, yeah, so I think it's really safe to say that so far

362
00:30:16,920 --> 00:30:21,920
kind of the impact was probably limited, but I think kind of this, yeah, this mechanism

363
00:30:21,920 --> 00:30:24,720
I think is promising for the future.

364
00:30:24,720 --> 00:30:30,560
And so that brings us to 2018, the last milestone on your list, which is pre-trained language

365
00:30:30,560 --> 00:30:36,440
models, and more generally the idea of being able to apply transfer learning, which we've

366
00:30:36,440 --> 00:30:43,400
seen a ton of success with, and the image domain to NLP.

367
00:30:43,400 --> 00:30:48,280
And you've done quite a bit in that space, you know, tell us about what you've seen in

368
00:30:48,280 --> 00:30:54,760
that space that kind of inspired you to start working in that area.

369
00:30:54,760 --> 00:31:00,680
Yeah, so for me personally, so maybe I came on that from two directions.

370
00:31:00,680 --> 00:31:06,080
So first in my personal research, as I've worked on different tasks and kind of developing

371
00:31:06,080 --> 00:31:11,600
different, like domain annotation, multi-tasking methods for different tasks, that were like

372
00:31:11,600 --> 00:31:16,040
usually task specific and try to incorporate some features of the target task, kind of

373
00:31:16,040 --> 00:31:20,480
the logical or kind of the natural progression in that was really trying to work on kind of

374
00:31:20,480 --> 00:31:25,760
this more general inductive transfer learning setting, essentially, where you try to use some

375
00:31:25,760 --> 00:31:31,360
sort of pre-trained knowledge and generalize or use that to do better on an arbitrary target

376
00:31:31,360 --> 00:31:32,680
task, essentially.

377
00:31:32,680 --> 00:31:39,400
So in that context, you really want to have some pre-training task or some existing information

378
00:31:39,400 --> 00:31:45,360
that is as general as possible, so that it will be likely useful for kind of any number

379
00:31:45,360 --> 00:31:47,640
of target task, essentially.

380
00:31:47,640 --> 00:31:52,920
And then in the second direction, I've also been really following a lot of the progress

381
00:31:52,920 --> 00:31:55,360
made in computer vision.

382
00:31:55,360 --> 00:32:05,120
And just really been fascinated or I found it really cool to see that how transfer learning

383
00:32:05,120 --> 00:32:12,480
was really kind of commonplace there and we used a lot in practice by practitioners in

384
00:32:12,480 --> 00:32:22,320
computer vision that was really very easy to build models or to just collect a few hundred

385
00:32:22,320 --> 00:32:26,840
images for different classes that you care about, use a pre-trained model, find you those

386
00:32:26,840 --> 00:32:29,920
on those images and you could really already get reasonable performance.

387
00:32:29,920 --> 00:32:34,840
And I found that really kind of a factor that I think made it easy for a lot of people

388
00:32:34,840 --> 00:32:38,720
to just have an experiment or develop their own computer vision models.

389
00:32:38,720 --> 00:32:44,520
And I really wanted to have something or I really found thought it would be very useful

390
00:32:44,520 --> 00:32:50,440
to have something similar for a natural language as well, where you can similarly create

391
00:32:50,440 --> 00:32:56,920
collective view, 100 label examples and train your own models for your own task that

392
00:32:56,920 --> 00:32:58,240
you care about.

393
00:32:58,240 --> 00:33:05,040
And kind of in thinking about that, essentially I started kind of talking with Jeremy

394
00:33:05,040 --> 00:33:09,680
Howard from FASDI as well, which been thinking along similar directions.

395
00:33:09,680 --> 00:33:16,880
And concurrently there's been some related work coming out from the Institute for Artificial

396
00:33:16,880 --> 00:33:18,600
Intelligence as well.

397
00:33:18,600 --> 00:33:25,640
And so it really started to crystallize that kind of a like the ideal or useful pre-trained

398
00:33:25,640 --> 00:33:31,720
task for that would really be language modeling because that's already been used as a kind

399
00:33:31,720 --> 00:33:38,720
of a successful variant with Goetheveg, because all Goetheveg does essentially kind of

400
00:33:38,720 --> 00:33:43,400
a variation of language modeling, but instead of using that just to initialize the first

401
00:33:43,400 --> 00:33:48,520
layer of our models with Goetheveg meetings, why don't we just pre-trained a language

402
00:33:48,520 --> 00:33:53,920
model and use that to initialize the remaining parameters of our models.

403
00:33:53,920 --> 00:33:59,440
And that really kind of was, yeah, like this overall direction wasn't really like, wasn't

404
00:33:59,440 --> 00:34:05,280
completely new, so in 2015 there was like the first paper which actually used language

405
00:34:05,280 --> 00:34:08,920
models and then fine-tuned those on the target task.

406
00:34:08,920 --> 00:34:13,400
And we essentially kind of took that a step further, added like a step where we pre-trained

407
00:34:13,400 --> 00:34:20,520
that on a general domain purpose and then proposed some other techniques to improve the fine-tuning

408
00:34:20,520 --> 00:34:26,760
performance or to kind of to retain as much information as possible doing this fine-tuning

409
00:34:26,760 --> 00:34:27,760
process.

410
00:34:27,760 --> 00:34:31,320
I guess when we're talking about transfer learning, there's two pieces, one is being

411
00:34:31,320 --> 00:34:39,240
able to use models that are created by third parties, presumably on other data sets and

412
00:34:39,240 --> 00:34:44,760
the other element is disability to kind of fine-tune it to meet your specific needs.

413
00:34:44,760 --> 00:34:50,800
With kind of that in mind, like do you consider word embeddings to be transfer learning,

414
00:34:50,800 --> 00:34:54,200
you know, certainly we share like glove vectors and things like that, they definitely meet

415
00:34:54,200 --> 00:35:00,120
that first criteria, but maybe not the second, is that the key distinction that you make?

416
00:35:00,120 --> 00:35:05,840
So I definitely think which using like pre-trained vulnerabilities is one form of transfer learning.

417
00:35:05,840 --> 00:35:13,800
So it's one way or kind of a very simple way to use existing pre-trained knowledge.

418
00:35:13,800 --> 00:35:19,880
And kind of using pre-trained language models in the asset is really kind of an additional

419
00:35:19,880 --> 00:35:28,800
step or you basically use kind of that knowledge and for additional input in your model either

420
00:35:28,800 --> 00:35:34,040
to influence more and more layers or you use kind of these richer, these embeddings from

421
00:35:34,040 --> 00:35:40,840
language models which just capture richer relations than you could capture with word embeddings.

422
00:35:40,840 --> 00:35:45,600
So both are kind of forms of transfer learning, but I think using language models scores kind

423
00:35:45,600 --> 00:35:48,920
of a step further than just using word embeddings.

424
00:35:48,920 --> 00:35:53,880
When you say language models, what specifically are you referring to?

425
00:35:53,880 --> 00:35:58,800
When I think of language models, I think of things like LDA, is that what you're referring

426
00:35:58,800 --> 00:36:01,840
to or are you speaking about them more broadly?

427
00:36:01,840 --> 00:36:07,600
So LDA, so latent, usually allocation is an example of topic modeling, so topic modeling

428
00:36:07,600 --> 00:36:13,160
is typically just used for like the expiration where you have like a corpus and you want

429
00:36:13,160 --> 00:36:17,360
to find out what people are talking about in there, so that generally gives you like

430
00:36:17,360 --> 00:36:20,040
a list of topics.

431
00:36:20,040 --> 00:36:23,600
And so language modeling when people talk about language modeling is really, so language

432
00:36:23,600 --> 00:36:29,400
modeling really is the task of you have a sequence of words like a sentence and you want

433
00:36:29,400 --> 00:36:31,960
to predict the next word in that sequence.

434
00:36:31,960 --> 00:36:40,200
So it's a very clearly defined task because you only need kind of the raw text data essentially.

435
00:36:40,200 --> 00:36:43,840
So given the text you can always see, you always know what the next word is that you want

436
00:36:43,840 --> 00:36:48,840
to predict, so you can very easily train this kind of model on a large amount of data.

437
00:36:48,840 --> 00:36:54,280
The typical approach to solving this kind of task would be like an LSTM.

438
00:36:54,280 --> 00:37:00,200
Yeah, so the typical model really this days, so the more traditional model, so stepping

439
00:37:00,200 --> 00:37:05,320
back a bit, the more traditional model would be kind of using basically using Ngrams,

440
00:37:05,320 --> 00:37:15,400
so using kind of different windows of words and then computing the next word or predicting

441
00:37:15,400 --> 00:37:22,640
the next word based on how what words you've seen co-cur with your existing Ngrams essentially.

442
00:37:22,640 --> 00:37:28,480
And then these days, yeah, you have kind of a typical language model would be in LSTM,

443
00:37:28,480 --> 00:37:31,800
maybe with multiple layers.

444
00:37:31,800 --> 00:37:36,040
And yeah, so that would kind of be the most typical one, people have used other models as

445
00:37:36,040 --> 00:37:42,040
well using memory or kind of other variations on this classic LSTM, but recently actually

446
00:37:42,040 --> 00:37:47,120
people showed that if you're just a bit more careful about tuning the hyper parameters

447
00:37:47,120 --> 00:37:52,960
of your LSTM, you can actually get very good performance just using a very classic LSTM

448
00:37:52,960 --> 00:37:54,160
for language modeling.

449
00:37:54,160 --> 00:38:01,760
And so the method that you came up with with Jeremy is called ULM Fit, tell us a little

450
00:38:01,760 --> 00:38:08,240
bit about how it works and the approach you took there.

451
00:38:08,240 --> 00:38:14,760
Sure, so yeah, so our intuition was really with that method that instead of initializing

452
00:38:14,760 --> 00:38:18,400
only the first layer of the model with word embeddings, we really want to have something

453
00:38:18,400 --> 00:38:23,760
with which we can initialize our entire model so that we in the end can take our pre-tuned

454
00:38:23,760 --> 00:38:28,600
language model and we just need to put a new layer, like a new layer for the classification

455
00:38:28,600 --> 00:38:30,600
task on top.

456
00:38:30,600 --> 00:38:36,840
And essentially our kind of method repose here consists of three steps, so we first just

457
00:38:36,840 --> 00:38:42,400
train kind of a classic LSTM language model on a large general domain copus, so we really

458
00:38:42,400 --> 00:38:48,040
want to have a kind of a copus that can capture very general domain knowledge.

459
00:38:48,040 --> 00:38:53,720
And for that, we just use kind of a copus subset of Wikipedia that is quite commonly used

460
00:38:53,720 --> 00:38:57,880
for language model evaluation and training as well.

461
00:38:57,880 --> 00:39:02,640
And then in the second step, you want to get this language model to kind of learn about

462
00:39:02,640 --> 00:39:07,920
some of the characteristics of the data or of the copus you actually care about.

463
00:39:07,920 --> 00:39:12,840
So in the second step, we then find you in this language model, so still the same language

464
00:39:12,840 --> 00:39:18,760
model, only we can just continue training it on data of our new classification task.

465
00:39:18,760 --> 00:39:23,840
So for instance, if you want to do a movie review classification based on positive or negative

466
00:39:23,840 --> 00:39:31,640
reviews, we would just fine tune that and continue training it on movie reviews of that domain.

467
00:39:31,640 --> 00:39:39,840
And for that step, essentially, we realize that we don't really want to train the model,

468
00:39:39,840 --> 00:39:43,600
like use too high of learning rate or you want to kind of be careful in how you train

469
00:39:43,600 --> 00:39:48,360
the model so that you don't actually accidentally override the information that your general

470
00:39:48,360 --> 00:39:51,360
language model has already captured before.

471
00:39:51,360 --> 00:39:55,960
So we essentially proposed kind of two different techniques, certain learning rate schedule

472
00:39:55,960 --> 00:40:02,280
and using different learning rates for different layers, essentially for the model to allow to

473
00:40:02,280 --> 00:40:06,880
retain as much of the general information from the general language model as possible.

474
00:40:06,880 --> 00:40:13,840
And kind of only adapt the higher layers of the model for the current, the new domain.

475
00:40:13,840 --> 00:40:19,200
Because our intuition was with that, that from computer vision, people have shown that

476
00:40:19,200 --> 00:40:25,120
essentially the low layers of the model really capture the most general information.

477
00:40:25,120 --> 00:40:31,080
So in the lowest layers, you really have information about the edges and kind of very general

478
00:40:31,080 --> 00:40:32,080
features.

479
00:40:32,080 --> 00:40:36,000
And as you go higher, the information gets more specific to the task for image net you

480
00:40:36,000 --> 00:40:40,640
would have the higher layers really responding to particular parts of dogs or dog noses

481
00:40:40,640 --> 00:40:45,920
even so the information as you go higher in the model gets more task specific.

482
00:40:45,920 --> 00:40:50,160
And in a P people actually now recently have started to show that there's like a similar

483
00:40:50,160 --> 00:40:54,600
hierarchy of information in your model that was trained in text.

484
00:40:54,600 --> 00:40:58,440
So because of that, our intuition was really that we want to retain as much of the information

485
00:40:58,440 --> 00:41:02,320
in the lower layers as possible and then use higher learning rates for the higher layers

486
00:41:02,320 --> 00:41:06,200
to adapt them to a large extent.

487
00:41:06,200 --> 00:41:11,040
And then in the final step, we essentially remove the softmax layer that was used for the

488
00:41:11,040 --> 00:41:17,200
language modeling objective and replace that with layer, so just softmax cross entropy

489
00:41:17,200 --> 00:41:22,400
layer that just tries to predict the target classes for our task.

490
00:41:22,400 --> 00:41:28,720
And we just train that new layer on top of together with the remaining language model.

491
00:41:28,720 --> 00:41:33,120
And here in addition to the two previous techniques, we also propose to make it a bit easier

492
00:41:33,120 --> 00:41:38,560
for the model to adapt the top layer to the, which is randomly initialized to the other

493
00:41:38,560 --> 00:41:41,080
parameters which have already been trained.

494
00:41:41,080 --> 00:41:44,800
So we initially only train the top layer and freeze the remaining layers.

495
00:41:44,800 --> 00:41:50,760
And then at the second epoch, I'm freeze another layer from the top down to make it kind

496
00:41:50,760 --> 00:41:56,000
of easier for the model to adapt the new parameters to the existing ones that have already been

497
00:41:56,000 --> 00:41:57,000
trained.

498
00:41:57,000 --> 00:42:06,120
There's a lot there. So first, let's maybe start by noting that the method is specifically

499
00:42:06,120 --> 00:42:12,280
kind of targeting this classification type of a task as opposed to predicting the next

500
00:42:12,280 --> 00:42:15,320
word or a generation type of task.

501
00:42:15,320 --> 00:42:18,760
Where do you see those classification tasks come up?

502
00:42:18,760 --> 00:42:19,760
Yes.

503
00:42:19,760 --> 00:42:25,960
So we decided to initially focus on classification because we thought that was kind of the most

504
00:42:25,960 --> 00:42:29,680
or those had like the most real world applications.

505
00:42:29,680 --> 00:42:30,680
So yeah.

506
00:42:30,680 --> 00:42:38,680
So from, yeah, kind of a lot of applications from, yeah, classifying chat logs, customer

507
00:42:38,680 --> 00:42:45,720
requests, requests in different categories routing those to different relevant entities

508
00:42:45,720 --> 00:42:52,520
in legal classifying different legal documents, depending on different, yeah, different fields.

509
00:42:52,520 --> 00:42:59,440
Yeah. So I think classification is really something as soon as you have some sort of text

510
00:42:59,440 --> 00:43:03,600
that is generated and you want to have some information that you want to filter out that

511
00:43:03,600 --> 00:43:10,160
can be expressed as a classification task if you just are able just to define certain

512
00:43:10,160 --> 00:43:13,360
number of categories that you want to express.

513
00:43:13,360 --> 00:43:17,720
Or similarly, a lot of other tasks like even doing sequence labeling or so can also be

514
00:43:17,720 --> 00:43:24,160
expressed as classification. So we found that by focusing on classification first, we

515
00:43:24,160 --> 00:43:29,720
can really cover a lot of, a lot of the real world applications of NLP.

516
00:43:29,720 --> 00:43:35,400
And then you mentioned that recent research has shown that in neural networks that are

517
00:43:35,400 --> 00:43:40,080
kind of language models, there's a similar phenomenon to what we've seen in computer

518
00:43:40,080 --> 00:43:43,400
vision where the lower layers are more general.

519
00:43:43,400 --> 00:43:48,160
On the computer vision side, we've got some intuitive ways of explaining that and talking

520
00:43:48,160 --> 00:43:54,000
about low level features like edges and textures and things like that as being found in these

521
00:43:54,000 --> 00:44:01,240
lower layers. Is there a similar clean way to explain what we're seeing in the NLP side?

522
00:44:01,240 --> 00:44:06,440
Yeah. So there's not, so it's a bit, so in computer vision, it's always nice that

523
00:44:06,440 --> 00:44:10,920
you can actually get some sort of like intuitive feeling by visualizing that in NLP, it's not

524
00:44:10,920 --> 00:44:18,040
exactly as straightforward. But it's still somewhat intuitive. So so far, I think there's

525
00:44:18,040 --> 00:44:24,160
been kind of two ways in which people have shown that there's some hierarchy first when

526
00:44:24,160 --> 00:44:29,840
you do multi-task loading with different little p-task. So multi-task loading is when you

527
00:44:29,840 --> 00:44:34,560
share, when you have one model that performs multiple tasks at the same time. And then

528
00:44:34,560 --> 00:44:41,120
if you have your model, if you train a model with some kind of higher level or more semantic

529
00:44:41,120 --> 00:44:47,560
tasks, like named entry recognition, and if you train that together with another task,

530
00:44:47,560 --> 00:44:52,400
it's more syntactic. So kind of a more lower level linguistically, like a part of speech

531
00:44:52,400 --> 00:44:57,720
tagging, then people have shown that actually information that is relevant for the part

532
00:44:57,720 --> 00:45:01,520
of speech tagging tasks, so information that kind of maximizes the performance in that

533
00:45:01,520 --> 00:45:06,600
task is actually captured at lower level lower layers of model, whereas for named entry

534
00:45:06,600 --> 00:45:12,680
recognition, the information would be contained on a higher level layer. And then more recently

535
00:45:12,680 --> 00:45:21,400
in this kind of in some papers from AI2, where they had so in their L-mold paper and then

536
00:45:21,400 --> 00:45:26,800
in the follow-up paper as well, they showed that in having these embeddings that were

537
00:45:26,800 --> 00:45:35,360
pre-trained or trained in a language model, if you use just, if you use lower layer lower

538
00:45:35,360 --> 00:45:40,520
level layers of your model, then those actually give you the best performance for lower level

539
00:45:40,520 --> 00:45:48,280
tasks, like part of speech tagging, whereas for higher level tasks, the most, the best information

540
00:45:48,280 --> 00:45:53,800
is contained in higher layers of model. So, so far, yeah, so really the best way to look

541
00:45:53,800 --> 00:45:58,680
at kind of a downstream task that encodes some sort of some level of hierarchy, and then

542
00:45:58,680 --> 00:46:04,480
basically generalize from that. So, yeah, so, so far, we haven't really, but there has

543
00:46:04,480 --> 00:46:09,880
not been kind of more intuitive or like a more easily accessible way by just defining

544
00:46:09,880 --> 00:46:14,040
a task that kind of encodes some assumption or something that you want to measure and

545
00:46:14,040 --> 00:46:17,880
then measure the performance of different layers of the model on that task.

546
00:46:17,880 --> 00:46:26,280
And then you mentioned that the ULMFIT method includes some specific guidance around like

547
00:46:26,280 --> 00:46:30,840
setting your learning rates and learning rate scheduling and things like this. On the

548
00:46:30,840 --> 00:46:38,600
computer vision side, these types of things are often done very iteratively via experimentation

549
00:46:38,600 --> 00:46:44,520
to determine, you know, what the right learning rates are and how to schedule your learning

550
00:46:44,520 --> 00:46:50,440
rates or apply learning rates to different layers is what you're saying here that you can

551
00:46:50,440 --> 00:46:55,320
be more prescriptive about it because of some characteristic of the problem or just that

552
00:46:55,320 --> 00:46:58,840
you should take that kind of an approach when training these models.

553
00:46:58,840 --> 00:47:04,520
Um, yeah, so we try to come up with a, so I mean, the way we arrived at these methods,

554
00:47:04,520 --> 00:47:09,400
also, and there's some explanation and trial and error on kind of our validation data as well,

555
00:47:09,400 --> 00:47:15,640
um, but recently or kind of the parameters we arrived at, we found them to perform well on

556
00:47:16,200 --> 00:47:21,720
different or kind of a variety of text classification problems. So it's really kind of more of

557
00:47:21,720 --> 00:47:27,960
the guidance of these are like good rules of thumb or like a good range of parameters that

558
00:47:27,960 --> 00:47:33,880
give, um, that give good results in general, but then obviously for, for particular domains or

559
00:47:33,880 --> 00:47:39,240
particular tasks, it might still be useful to do some, um, some fine-tuning of these parameters or

560
00:47:39,240 --> 00:47:44,360
to slightly change them and see, um, particularly playing around with like the learning rate,

561
00:47:44,360 --> 00:47:48,840
and, um, kind of the number of epochs you train your model, um, can still give you boost in

562
00:47:48,840 --> 00:47:55,960
performance. And so what kind of results have you seen with this approach? Um, so, so with this,

563
00:47:55,960 --> 00:48:00,200
um, so with our approach that we proposed, we essentially were able to outperform the state of

564
00:48:00,200 --> 00:48:06,520
the art on, um, kind of the number of YT studied text classification datasets. And that was, um,

565
00:48:06,520 --> 00:48:11,880
particularly kind of exciting encouraging for us, um, because on a lot of these datasets, um,

566
00:48:12,600 --> 00:48:18,680
some of the architectures were really had either a lot of feature or architecture engineering,

567
00:48:18,680 --> 00:48:25,240
whereas we, um, really used a very simple, um, uh, LSM with just different numbers of dropout,

568
00:48:25,240 --> 00:48:31,320
and, um, only this, um, kind of pre-training step, um, essentially. Um, so it was really encouraging

569
00:48:31,320 --> 00:48:36,200
that, um, yeah, this very simple model was really able to outperform the state of the art on,

570
00:48:36,200 --> 00:48:42,280
on a variety of benchmarks. Um, and then I think to me personally, uh, very exciting was just to see

571
00:48:42,280 --> 00:48:48,120
that this type of approach, um, even trained on, um, limited number of examples. So we basically did

572
00:48:48,120 --> 00:48:56,040
some inflation studies, so we trained it on, um, smaller training exercises, going from 100 to, um,

573
00:48:56,040 --> 00:49:01,960
yeah, two in different steps to 1,000, 10,000, uh, number of examples. And we really saw that just

574
00:49:01,960 --> 00:49:07,080
by virtue of using this pre-trained, um, information and pre-training language model,

575
00:49:07,080 --> 00:49:12,760
we were actually really able to, um, outperform, kind of, training a model from scratch on the same

576
00:49:12,760 --> 00:49:18,440
number of examples by an order of magnitudes, um, or we were able to reach the same performance as a

577
00:49:18,440 --> 00:49:23,560
model that was trained from scratch on kind of an order of magnitude more data. And I think this,

578
00:49:23,560 --> 00:49:29,160
this particular finding that's something that, um, not only we, but, um, kind of other researchers

579
00:49:29,160 --> 00:49:34,040
working in one of a similar direction of using language modeling have found. And, uh, yeah, for,

580
00:49:34,040 --> 00:49:38,600
for me personally, I think that's very encouraging, um, because I think this will really help to unlock,

581
00:49:38,600 --> 00:49:44,840
um, a lot of the kind of potential for NLP and just make it easier for people to use it on, on their

582
00:49:44,840 --> 00:49:49,720
own data sets and just should make it easier for people to, to just collect a small number of

583
00:49:49,720 --> 00:49:54,920
examples and then train and apply these kind of models to their data. Are there any qualitative

584
00:49:54,920 --> 00:50:01,640
comparisons you can make between transfer learning and computer vision and transfer learning

585
00:50:01,640 --> 00:50:08,920
in NLP based on this approach? So, um, so I think maybe one, one comparison is that what we

586
00:50:08,920 --> 00:50:13,000
initially tried and what is, um, kind of commonly done computer vision is that you only train,

587
00:50:13,000 --> 00:50:18,040
kind of, freeze your, um, internet work and you only train like the, the top mostly in front

588
00:50:18,040 --> 00:50:23,240
scratch or you're only on trees, like a couple of the, the top layers of the model, um, but really for

589
00:50:23,240 --> 00:50:30,520
us in, um, at least at the moment, because the models people are using right now are still

590
00:50:30,520 --> 00:50:35,400
quite a bit more shallow than, um, kind of typical models like resonant or dense net that you

591
00:50:35,400 --> 00:50:40,600
would use for transfer learning computer vision. Um, so we really still found it useful to kind of

592
00:50:40,600 --> 00:50:47,400
train, um, the entire model or to fine tune the entire model. At the same time, we've seen or in the

593
00:50:47,400 --> 00:50:53,480
community, um, kind of one of the most promising approaches is, um, this Elmo approach from AI2

594
00:50:53,480 --> 00:50:57,960
and they actually use our, take kind of an orthogonal approach where they don't fine tune the

595
00:50:57,960 --> 00:51:02,920
model, but use the embeddings from the language model, um, kind of as fixed features in a,

596
00:51:02,920 --> 00:51:07,560
in a, um, kind of in a separate model that is still trained from scratch. So you kind of have your

597
00:51:07,560 --> 00:51:12,280
existing, um, architecture and you just add the embedding that you get from your language model

598
00:51:12,280 --> 00:51:19,640
for each word as an additional feature, um, as input, um, and they, they, um, yeah, we're able to

599
00:51:19,640 --> 00:51:24,600
achieve kind of very good and serial fair performance on, um, a large variety of tasks as well.

600
00:51:24,600 --> 00:51:31,480
And, um, kind of reasonably comparing, so in some ongoing work comparing against the

601
00:51:31,480 --> 00:51:38,520
approach, we actually find that, um, they're very kind of those, so our fine tuning approach versus

602
00:51:38,520 --> 00:51:43,400
just using the fixed features like an Elmo is quite, uh, actually is about like a similar performance.

603
00:51:43,400 --> 00:51:49,960
Um, whereas in computer vision, really the, um, kind of the prevalent or the, uh, current paradigm

604
00:51:49,960 --> 00:51:55,320
really seems that people are just, um, fine tuning these models, instead of using them as fixed features.

605
00:51:55,320 --> 00:52:01,560
So I think it's still, um, yeah, so I think we'll still, um, so that's an ongoing research direction

606
00:52:01,560 --> 00:52:07,000
essentially, um, to see what is kind of the best way going forward really to do this sort of fine

607
00:52:07,000 --> 00:52:13,000
tuning in LP. Do we want to use, uh, do we want to use fine tuning or do we want to use fixed features

608
00:52:13,000 --> 00:52:17,960
or maybe a combination of the two and what are actually, yeah, what are like the pros and cons of

609
00:52:17,960 --> 00:52:24,840
that? And in your work, did you train from scratch, your pre-trained language model that you then

610
00:52:24,840 --> 00:52:30,280
use later on to apply to different test code? Did you, uh, was that already available for you?

611
00:52:31,000 --> 00:52:37,240
Um, so we, we trained on language model from scratch. So we trained, uh, yeah, we, we tried

612
00:52:37,240 --> 00:52:41,640
or experimented with different ways to train the initial language model as well, um, yeah,

613
00:52:41,640 --> 00:52:45,480
just because we want to observe the effect of the data, um, on the language model.

614
00:52:45,480 --> 00:52:51,080
Um, but, yeah, there's like different types of, um, tasks like, um, so we trained on the

615
00:52:51,080 --> 00:52:56,360
subset of Wikipedia, but Elmo, for instance, was trained on kind of a larger, uh, news corpus.

616
00:52:56,920 --> 00:53:02,600
Um, and, um, they use, I think like a pre-trained language model that was available online,

617
00:53:02,600 --> 00:53:07,880
and more recently, there's been some work from, uh, OpenAI, where they also trained a language model

618
00:53:07,880 --> 00:53:13,240
that was based on this transformer architecture from machine translation on an even, uh, larger corpus.

619
00:53:13,240 --> 00:53:20,280
Um, so, and it's still not entirely clear how those different, what the actual impact or

620
00:53:20,760 --> 00:53:26,760
how much of a benefit it gives you when training on like a larger corpus. Um, I think one hypothesis

621
00:53:26,760 --> 00:53:32,600
might be that training on training a more expressive model on more data, as we've kind of seen all

622
00:53:32,600 --> 00:53:37,480
the history of deep learning that might work better in the long term, um, but we still need to kind

623
00:53:37,480 --> 00:53:43,240
of figure out what actually the, uh, the best ways to do that. And out of curiosity, what, uh, kind

624
00:53:43,240 --> 00:53:47,560
of order of magnitude were you experiencing in terms of training time for these models?

625
00:53:48,440 --> 00:53:54,120
Um, so in terms of training the initial, so training the first, uh, uh, language model on the

626
00:53:54,120 --> 00:54:00,760
large corpus, or fine tuning the language model, or, uh, well, both. Um, so yeah, so it kind of

627
00:54:00,760 --> 00:54:06,600
depends on your, um, like on the approach. So when we trained our language model, um, because the

628
00:54:06,600 --> 00:54:11,240
initial, like, the spring training set, because you have, um, a very, like, a lot of tokens,

629
00:54:11,800 --> 00:54:15,640
a large corpus that you train on, that's usually the most extensive step. So for us,

630
00:54:15,640 --> 00:54:21,560
in our early experiments, we trained that out for, like, 24 hours on one, uh, GPU, essentially,

631
00:54:21,560 --> 00:54:27,240
and then usually the fine tuning, um, and the final training set would be a lot faster, so maybe

632
00:54:27,240 --> 00:54:32,280
it would be like, uh, one hour, depending on the training set size, um, in the open AI paper,

633
00:54:32,280 --> 00:54:36,920
because they trained an even larger model on even more data, they trained for about a month.

634
00:54:37,880 --> 00:54:42,120
So it can really, if you scale that up, um, that can take a lot longer, but then fine tuning would

635
00:54:42,120 --> 00:54:47,160
probably, um, take similar long, maybe a bit longer, because, because model is steeper, um,

636
00:54:47,160 --> 00:54:52,440
and recently, kind of in the fast AI library, um, with some ongoing experience from computer vision,

637
00:54:53,560 --> 00:54:59,080
they've kind of integrated some methods where you can train, um, which are kind of, um,

638
00:54:59,080 --> 00:55:05,000
this, like, one technique called, uh, super convergence, that has shown, um, some good results for

639
00:55:05,000 --> 00:55:09,080
computer vision, where you essentially have a, uh, particular learning rate schedule, and you

640
00:55:09,080 --> 00:55:14,280
train a model with a very high learning rate, um, and in doing that, and if you're kind of careful

641
00:55:14,280 --> 00:55:22,200
about how you use the schedule, you can get to very high performance, uh, very fast, um, so using,

642
00:55:22,200 --> 00:55:26,920
like a technique like that might also, um, for language modeling, might also allow you to train your

643
00:55:26,920 --> 00:55:32,760
language models, um, yeah, significantly faster. With word embeddings, we've kind of, you know,

644
00:55:32,760 --> 00:55:39,000
those of matured and we've gotten to the point that there are, you know, multiple options for

645
00:55:39,640 --> 00:55:45,720
word embedding vectors that you can more or less use interchangeably. Is that the case for

646
00:55:45,720 --> 00:55:52,760
these pre-trained language models? Like, can you use either the open AI language models or what

647
00:55:52,760 --> 00:55:58,600
you've published kind of interchangeably in building out models and just experiment to see what

648
00:55:58,600 --> 00:56:06,040
works best for you? Or are they, are the, the language models more, you know, intertwined to

649
00:56:07,000 --> 00:56:14,360
how they are intended to be used? Um, yeah, so in a sense, you can still, um, you can basically kind

650
00:56:14,360 --> 00:56:22,440
of use them interchangeably at this point, um, although so we've recently seen, or there's been

651
00:56:22,440 --> 00:56:28,120
a recent paper which showed, okay, there's maybe some, um, differences in the representation that

652
00:56:28,120 --> 00:56:34,520
these models learn so that on, on some tasks, actually having, um, using, like an LSTM in contrast

653
00:56:34,520 --> 00:56:39,080
with a transformer as language model might actually give you better performance, but then you have

654
00:56:39,080 --> 00:56:45,080
the transformer which might be maybe more efficient than the LSTM because it doesn't require this

655
00:56:45,080 --> 00:56:52,440
temporal dependency, but this is again, like still very preliminary work, um, so I don't think, yeah,

656
00:56:52,440 --> 00:56:57,640
so I don't think at this point we really have a great understanding yet for which type of tasks,

657
00:56:58,520 --> 00:57:05,160
these particular models work, um, work really well, um, but I think what you want, so as long as you

658
00:57:05,160 --> 00:57:10,840
have a model, like a language model that is very expressive, that achieves kind of a good, um,

659
00:57:10,840 --> 00:57:15,640
performance on your data, then I think generally you would probably expect to get similar

660
00:57:15,640 --> 00:57:20,440
performance, and then probably going forward, there might be some particular guidance on maybe

661
00:57:20,440 --> 00:57:25,560
if you have a sort of, like, reasoning class or, um, something which requires maybe more

662
00:57:25,560 --> 00:57:31,320
long term, um, dependencies, maybe you won't rather want to use a transformer, um, but that's

663
00:57:31,320 --> 00:57:36,040
still a bit early stage to give kind of these more, um, more concrete rules of them.

664
00:57:36,040 --> 00:57:43,640
If we cover it all, there is a cover on ULM fit, uh, and, you know, and or, you know, what's next in

665
00:57:43,640 --> 00:57:49,720
kind of that research direction? Um, yeah, so, so I think in that, in that research direction,

666
00:57:50,360 --> 00:57:56,840
I think there's, like, a lot of interesting directions, so first I don't think we've yet,

667
00:57:56,840 --> 00:58:01,560
kind of come to the ceiling of what we can achieve, um, using language models in NLP,

668
00:58:01,560 --> 00:58:07,000
so I think, so I personally think that, um, because of the results I've seen so far are really

669
00:58:07,000 --> 00:58:11,400
encouraging and really significant improvements of what we've seen before. Um, people are going to,

670
00:58:12,040 --> 00:58:15,880
um, start using more and more instead of using pre-treatment meetings, using, um,

671
00:58:15,880 --> 00:58:20,120
pre-treatment language models in their own applications. So I think these will really kind of

672
00:58:20,120 --> 00:58:26,280
become a, uh, a mainstay in NLP going forward, at least for the foreseeable future. Um,

673
00:58:26,280 --> 00:58:32,360
and in, in that, I think there's still a lot of questions on how you actually, um, how you can,

674
00:58:33,160 --> 00:58:37,160
kind of compress and capture as much information in your language model as before,

675
00:58:37,160 --> 00:58:41,720
as possible, what are like the best language models to use, how you can incorporate these in that.

676
00:58:41,720 --> 00:58:46,840
So just, um, kind of, uh, maximizing the benefit from using this language modeling task,

677
00:58:46,840 --> 00:58:53,320
I think is kind of a need to, like, need a direction that will still or should still give us some

678
00:58:53,320 --> 00:58:59,240
boost in some tasks. Um, and then probably in the, in the more long term, um, and, and then kind

679
00:58:59,240 --> 00:59:05,880
of maybe tied to the former, um, just understanding what actually, um, this or how far we can actually

680
00:59:05,880 --> 00:59:10,680
go using these language models, um, because language modeling might, um, while it's still, while it

681
00:59:10,680 --> 00:59:15,320
gives us a boost on some tasks, it's still a similar objective to what we've been doing with,

682
00:59:15,320 --> 00:59:21,000
where to back. And just kind of this idea of language modeling of just, um, predicting the next word

683
00:59:21,000 --> 00:59:27,640
based on its previous words, um, is while it should give you kind of some, um, to capture some

684
00:59:27,640 --> 00:59:33,880
relations in text, there's still a lot of things that it can't capture. So it's, um, yeah, for

685
00:59:33,880 --> 00:59:38,920
instance, it's really hard to capture, um, kind of more like real world knowledge, actually about

686
00:59:38,920 --> 00:59:45,880
how the, how the world works or how different, um, kind of entities or things in the, in the real

687
00:59:45,880 --> 00:59:50,920
world, um, kind of relate to each other. It's really hard, it's often not mentioned in text, and then

688
00:59:50,920 --> 00:59:56,040
only like, uh, very rarely. So it's really hard to capture something like that, which would be

689
00:59:56,040 --> 01:00:00,920
useful for us like quest answering or reading comprehension with just using language model. So I

690
01:00:00,920 --> 01:00:07,320
think one really interesting direction going forward is really how we can, um, incorporate, um,

691
01:00:07,320 --> 01:00:11,800
information that allows us to do better sort of reasoning and natural language understanding,

692
01:00:11,800 --> 01:00:17,640
either in order to augment our language model, or, um, as additional forms of pre-training maybe,

693
01:00:17,640 --> 01:00:23,960
or maybe additional forms of knowledge has, um, like in the memory from a knowledge base,

694
01:00:23,960 --> 01:00:30,440
or through some other module that we can then, um, use or leverage in our, uh, downstream tasks.

695
01:00:31,400 --> 01:00:37,720
Well, Sebastian, thanks so much for taking the time to chat about this. Uh, it's really interesting

696
01:00:37,720 --> 01:00:42,040
work you're doing, and I'm looking forward to kind of keeping up with how it evolves.

697
01:00:42,760 --> 01:00:45,000
Uh, cool and congrats. Yeah, thanks for having me. Thank you.

698
01:00:48,840 --> 01:00:53,960
All right, everyone, that's our show for today. For more information on Sebastian or any of the

699
01:00:53,960 --> 01:01:00,440
topics covered in this episode, visit twimmelai.com slash talk slash 195.

700
01:01:01,480 --> 01:01:06,280
If you're a fan of the show, and you haven't already done so, or you're a new listener, and you

701
01:01:06,280 --> 01:01:12,600
like what you hear, please head over to your Apple or Google podcast app, and leave us a five-star

702
01:01:12,600 --> 01:01:17,640
rating and review. Your reviews help inspire us to create more and better content, and they help

703
01:01:17,640 --> 01:01:23,320
new listeners find the show. Thanks again to our friends at IBM for their sponsorship of this

704
01:01:23,320 --> 01:01:31,480
episode. Be sure to check out IBM developer at IBM.biz slash MLAI podcast. As always,

705
01:01:31,480 --> 01:01:39,720
thanks so much for listening, and catch you next time.

