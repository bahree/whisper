1
00:00:00,000 --> 00:00:13,400
Welcome to the Twimal AI Podcast.

2
00:00:13,400 --> 00:00:16,880
I'm your host, Sam Charrington.

3
00:00:16,880 --> 00:00:23,840
Hey, what's up everyone?

4
00:00:23,840 --> 00:00:28,320
I am super excited to bring you this interview with my friend, Emmanuel Amazon, whose

5
00:00:28,320 --> 00:00:34,120
new book, Building Machine Learning Powered Applications, just hit bookstores.

6
00:00:34,120 --> 00:00:38,880
In our conversation, as in the book, we explore how to approach machine learning projects

7
00:00:38,880 --> 00:00:44,960
systematically, from idea all the way to working product, including formulating your problem

8
00:00:44,960 --> 00:00:51,240
and creating a plan, building a working pipeline and initial data set, evolving your models,

9
00:00:51,240 --> 00:00:54,120
and deploying and monitoring them.

10
00:00:54,120 --> 00:00:58,120
These are the same concepts covered in a new study group we're launching starting this

11
00:00:58,120 --> 00:01:05,320
Saturday, February 22nd, around the AI Enterprise Workflow Specialization on Coursera.

12
00:01:05,320 --> 00:01:09,960
If you're an aspiring or practicing data scientist or ML developer, and you want to level

13
00:01:09,960 --> 00:01:14,760
up on the broader set of skills required to deliver models and business settings, I really

14
00:01:14,760 --> 00:01:16,560
encourage you to join us.

15
00:01:16,560 --> 00:01:23,200
To learn more about the study group, visit twimalai.com slash AIEW, where you can catch a recorded

16
00:01:23,200 --> 00:01:27,080
session I held with Ray Lopez, the instructor for the courses.

17
00:01:27,080 --> 00:01:32,040
There, you'll also find instructions for joining the study group, which is totally free,

18
00:01:32,040 --> 00:01:36,000
and how you can get a free month of access to Coursera to boot.

19
00:01:36,000 --> 00:01:41,120
Once again, the website is twimalai.com slash AIEW.

20
00:01:41,120 --> 00:01:46,040
For more on the interview and a link to the book, visit the show notes page at twimalai.com

21
00:01:46,040 --> 00:01:49,560
slash talk slash 349.

22
00:01:49,560 --> 00:01:56,040
And now on to the show.

23
00:01:56,040 --> 00:02:01,880
Alright everyone, I am on the line with Emanuel Amazing Emanuel is a machine learning engineer

24
00:02:01,880 --> 00:02:02,880
at Stripe.

25
00:02:02,880 --> 00:02:08,280
Emanuel, welcome finally, I should say, to the twimalai podcast.

26
00:02:08,280 --> 00:02:09,280
How are you my friend?

27
00:02:09,280 --> 00:02:10,280
Thanks for having me.

28
00:02:10,280 --> 00:02:11,280
I'm great.

29
00:02:11,280 --> 00:02:12,280
How are you?

30
00:02:12,280 --> 00:02:13,280
I am doing well.

31
00:02:13,280 --> 00:02:14,280
I'm doing well.

32
00:02:14,280 --> 00:02:18,720
So Emanuel and I have known each other for at least a couple years now, maybe more.

33
00:02:18,720 --> 00:02:25,360
When you met, when you were at insight, you were responsible for the insight data science

34
00:02:25,360 --> 00:02:27,360
AI program.

35
00:02:27,360 --> 00:02:29,280
What was your official responsibility?

36
00:02:29,280 --> 00:02:34,440
Because I've also, I've interviewed Ross on the show as well with insight.

37
00:02:34,440 --> 00:02:35,440
Yeah.

38
00:02:35,440 --> 00:02:36,440
What was your official role there?

39
00:02:36,440 --> 00:02:41,680
So I had a role very similar to Ross's, Ross was my equivalent in New York.

40
00:02:41,680 --> 00:02:44,520
So I was leading the AI program there.

41
00:02:44,520 --> 00:02:49,440
So we have a sort of professional education programs, fellowships in many different domains,

42
00:02:49,440 --> 00:02:55,120
state science, state engineering, and we have one in AI and so I was leading that one.

43
00:02:55,120 --> 00:02:59,840
And so usually we start these interviews by having folks share a little bit about their

44
00:02:59,840 --> 00:03:00,840
journey.

45
00:03:00,840 --> 00:03:07,640
Why don't you tell us how you got to Stripe and perhaps more importantly, how you got

46
00:03:07,640 --> 00:03:09,440
to being a published author.

47
00:03:09,440 --> 00:03:10,440
Congratulations.

48
00:03:10,440 --> 00:03:13,280
Yeah, happy to.

49
00:03:13,280 --> 00:03:20,520
So I started off, so I think like many people in this field, my fashion firm, I started

50
00:03:20,520 --> 00:03:24,480
with Jeff Hinton's Coursera classes way back in the day.

51
00:03:24,480 --> 00:03:25,480
Okay.

52
00:03:25,480 --> 00:03:29,400
I feel like for most reports, either Jeff Hinton or Andrew Eng that got them started.

53
00:03:29,400 --> 00:03:34,480
Team Andrew here, nothing against Jeff.

54
00:03:34,480 --> 00:03:38,480
And then after that, I started my career, my professional career, I did data science at

55
00:03:38,480 --> 00:03:43,280
the start up in the Bay Area, which got acquired by zip car later.

56
00:03:43,280 --> 00:03:46,800
I spent two years there and actually after that, I joined insight.

57
00:03:46,800 --> 00:03:51,480
So after having been a data scientist, I joined a role that was much more about professional

58
00:03:51,480 --> 00:03:53,680
education and mentorship.

59
00:03:53,680 --> 00:03:59,480
And so for a couple years at insight, I mentored in aggregate, it was over a hundred fellows

60
00:03:59,480 --> 00:04:04,760
that were PhDs and engineers that wanted to transition and get a job in like the field

61
00:04:04,760 --> 00:04:06,280
of machine learning.

62
00:04:06,280 --> 00:04:11,720
So that was amazing and through my work there, I learned a lot about what it takes to transition

63
00:04:11,720 --> 00:04:14,720
and what it takes to build successful ML projects.

64
00:04:14,720 --> 00:04:17,880
A lot of that is in the book you mentioned.

65
00:04:17,880 --> 00:04:22,040
After a couple of years there, I went to go back towards something more of a nice C role

66
00:04:22,040 --> 00:04:24,000
for me, more of an individual contributor.

67
00:04:24,000 --> 00:04:30,000
And so I went at Stripe because it sort of had the perfect blend of what I was looking

68
00:04:30,000 --> 00:04:34,680
for in a role, which was it blends very heavy and challenging machine learning with sort

69
00:04:34,680 --> 00:04:38,800
of very heavy engineering requirements, which I think is where the field is going in

70
00:04:38,800 --> 00:04:39,800
general.

71
00:04:39,800 --> 00:04:41,520
And so I wanted to do more of that.

72
00:04:41,520 --> 00:04:46,400
We haven't mentioned the title of the book yet, but it is building machine learning powered

73
00:04:46,400 --> 00:04:50,480
applications going from idea to product.

74
00:04:50,480 --> 00:04:52,680
When did the book become available?

75
00:04:52,680 --> 00:04:55,120
The book became available last week.

76
00:04:55,120 --> 00:04:56,120
Nice.

77
00:04:56,120 --> 00:04:57,120
Nice.

78
00:04:57,120 --> 00:05:00,960
And so I have one of the first copies of it right here in my hand and it was signed by

79
00:05:00,960 --> 00:05:01,960
you.

80
00:05:01,960 --> 00:05:02,960
Thank you very much.

81
00:05:02,960 --> 00:05:08,400
So the book has got this subtitle going from idea to product.

82
00:05:08,400 --> 00:05:11,240
Is it a conceptual book, a technical book?

83
00:05:11,240 --> 00:05:12,240
Yeah.

84
00:05:12,240 --> 00:05:13,480
That's a good question.

85
00:05:13,480 --> 00:05:16,800
The title comes from the desire scope of the book.

86
00:05:16,800 --> 00:05:23,200
So the desire scope is really to give tools to aspiring engineers and data scientists

87
00:05:23,200 --> 00:05:29,200
to go from sort of either a VM has an idea or you have an idea to you have something in

88
00:05:29,200 --> 00:05:34,840
production that is actually being used by real people.

89
00:05:34,840 --> 00:05:38,000
And it's a bit of a blend of conceptual and technical.

90
00:05:38,000 --> 00:05:41,000
It is technical in a sense that there are many code examples.

91
00:05:41,000 --> 00:05:45,000
There is a set of notebooks that accompany the book and there's actually an entire prototype

92
00:05:45,000 --> 00:05:47,560
application that we build together throughout the book.

93
00:05:47,560 --> 00:05:49,600
And at the end of the book, you can just, it has a GitHub repo.

94
00:05:49,600 --> 00:05:51,160
You can go and try it out.

95
00:05:51,160 --> 00:05:55,880
But it's also conceptual in a sense that a lot of these topics are more about how you

96
00:05:55,880 --> 00:05:59,840
frame problems than just like copying code off of Stack Overflow.

97
00:05:59,840 --> 00:06:04,520
And so there's interviews with data science leaders that have none of this sort of thing.

98
00:06:04,520 --> 00:06:08,320
There's an entire section about like data ethics and how you think about shipping models

99
00:06:08,320 --> 00:06:10,200
and when you shouldn't, shouldn't.

100
00:06:10,200 --> 00:06:12,400
So it's a bit of a hybrid book in that sense.

101
00:06:12,400 --> 00:06:13,400
Nice.

102
00:06:13,400 --> 00:06:14,400
Nice.

103
00:06:14,400 --> 00:06:19,360
I flipped through it and saw some of my favorite folks in here, Monica Burgatti and Rob

104
00:06:19,360 --> 00:06:22,800
Monroe and I guess we know some of the same people.

105
00:06:22,800 --> 00:06:23,800
Yeah.

106
00:06:23,800 --> 00:06:24,800
Yeah.

107
00:06:24,800 --> 00:06:25,800
Small world.

108
00:06:25,800 --> 00:06:31,000
There's things that I noticed and I haven't gone through them in a lot of detail.

109
00:06:31,000 --> 00:06:36,160
I mentioned to you that like I literally just got this out of the mail room here.

110
00:06:36,160 --> 00:06:43,040
But the structure of the book is that you develop a sample application.

111
00:06:43,040 --> 00:06:47,920
As you mentioned and the sample app is predictive text.

112
00:06:47,920 --> 00:06:50,880
How did you pick that app for context?

113
00:06:50,880 --> 00:06:51,880
Yeah.

114
00:06:51,880 --> 00:06:55,680
That was one of, first of all, that's a really good question because that was one of the

115
00:06:55,680 --> 00:07:00,120
parts that I went over the most times and just changed my mind, changed my mind very many

116
00:07:00,120 --> 00:07:03,040
times about which application should be the running example.

117
00:07:03,040 --> 00:07:06,680
And in fact, I had a conversation with Monica Burgatti where I pitched her on one of my

118
00:07:06,680 --> 00:07:11,600
initial ideas and she told me it was a terrible idea and I should definitely not what wasn't.

119
00:07:11,600 --> 00:07:18,480
I wanted to do something that was like it would listen to like politicians speeches and

120
00:07:18,480 --> 00:07:22,000
then compare with how they vote and tell you whether like what they were saying in their

121
00:07:22,000 --> 00:07:24,360
speeches sort of aligned with how they were actually voting.

122
00:07:24,360 --> 00:07:26,360
Like a fact checker kind of thing.

123
00:07:26,360 --> 00:07:27,360
Yeah.

124
00:07:27,360 --> 00:07:29,360
Like some sort of automatic fact checker.

125
00:07:29,360 --> 00:07:30,360
It would have been timely.

126
00:07:30,360 --> 00:07:31,360
Right.

127
00:07:31,360 --> 00:07:32,360
It would have been timely.

128
00:07:32,360 --> 00:07:33,360
That's what I was thinking.

129
00:07:33,360 --> 00:07:35,480
But it would have also been pretty hard.

130
00:07:35,480 --> 00:07:41,120
The sort of reconciliation of what's true and like what's not true very quickly gets

131
00:07:41,120 --> 00:07:43,320
into the realm of opinion.

132
00:07:43,320 --> 00:07:48,280
And if you add to like the errors of a machine learning model to the nuanced worldview,

133
00:07:48,280 --> 00:07:51,360
it's like definitely one of those examples of apps that could do more harm than good.

134
00:07:51,360 --> 00:07:57,360
So Monica talked to me off the ledge on that one, but what were some of your other ideas?

135
00:07:57,360 --> 00:08:01,840
Well, I consider doing initially sort of computer vision examples because does are always

136
00:08:01,840 --> 00:08:07,080
the more striking to necessarily be newer folks to the field like it's, you know, an image

137
00:08:07,080 --> 00:08:13,280
is like a very powerful example, but I was about 1000 or it's exactly, but I felt like

138
00:08:13,280 --> 00:08:17,360
they were sort of overused and most amount tutorials are some sort of that computer vision

139
00:08:17,360 --> 00:08:18,360
thing nowadays.

140
00:08:18,360 --> 00:08:22,680
So I want to do something different and I want to do tabular data because that's what

141
00:08:22,680 --> 00:08:28,640
I think what most people do in their day to day for most companies.

142
00:08:28,640 --> 00:08:33,560
But I felt like there was less room for a standalone product there, like sort of like bring

143
00:08:33,560 --> 00:08:38,920
your own tabular data has less less of a ring to it than like bring your own writing.

144
00:08:38,920 --> 00:08:45,520
So I ended up settling on an LP and then I wanted to do something that was most ML products

145
00:08:45,520 --> 00:08:46,520
aren't just one model.

146
00:08:46,520 --> 00:08:51,120
They're not just, you know, like you have a model that solves your use case perfectly.

147
00:08:51,120 --> 00:08:52,120
And then you just chip it.

148
00:08:52,120 --> 00:08:57,440
They're usually a combination of sort of heuristics and rules and models and engineering work.

149
00:08:57,440 --> 00:08:59,560
And so I wanted to probably then reflect that.

150
00:08:59,560 --> 00:09:05,320
And when I was thinking of products that did that today, sort of writing and assisting

151
00:09:05,320 --> 00:09:08,840
people to write better is a crucial example of that where you have you can check for grammar

152
00:09:08,840 --> 00:09:12,800
and that's just rules or you can check for vocabulary or for a variety of things and then

153
00:09:12,800 --> 00:09:16,040
you can also help them improve their style and that's more something that you can learn

154
00:09:16,040 --> 00:09:17,040
with them out.

155
00:09:17,040 --> 00:09:20,760
So it was a nice blend that sort of reflects what happens in the real world, I think.

156
00:09:20,760 --> 00:09:24,600
So does that mean that somewhere in this book there are lots of rejects?

157
00:09:24,600 --> 00:09:25,600
No.

158
00:09:25,600 --> 00:09:31,360
We've chosen not to go down that path, but there could be.

159
00:09:31,360 --> 00:09:36,280
The book starts with very simple, so like instead of regular expressions, it's simple

160
00:09:36,280 --> 00:09:37,280
word counts.

161
00:09:37,280 --> 00:09:38,960
Like, oh, how many adverbs are you using?

162
00:09:38,960 --> 00:09:39,960
Are you using?

163
00:09:39,960 --> 00:09:41,680
And a little too much for that sort of stuff.

164
00:09:41,680 --> 00:09:48,160
And so what's the overall kind of path or structure through the book?

165
00:09:48,160 --> 00:09:53,560
And kind of more importantly, what does it say about the way that you think folks need

166
00:09:53,560 --> 00:09:55,920
to approach these kinds of projects?

167
00:09:55,920 --> 00:09:56,920
Yeah.

168
00:09:56,920 --> 00:10:01,400
The book is broadly separated to four stages.

169
00:10:01,400 --> 00:10:06,560
I think generally makes sense for most ML projects.

170
00:10:06,560 --> 00:10:10,600
And I think a lot of time people focus a lot on training models and then you talk to

171
00:10:10,600 --> 00:10:14,960
experienced data scientists and you hear, right, oh, 95% of the job is they're looking

172
00:10:14,960 --> 00:10:18,320
at the data and shipping the model, not really training the model.

173
00:10:18,320 --> 00:10:22,800
And so this book purposely sort of almost ignores training models, it just assumes that

174
00:10:22,800 --> 00:10:26,840
you can figure that part out with the really good courses around.

175
00:10:26,840 --> 00:10:30,920
And so the four approaches are sort of, or the four parts are going from whatever your

176
00:10:30,920 --> 00:10:36,360
goal is, your product, what your company's doing, what you want to do to an ML approach

177
00:10:36,360 --> 00:10:40,960
into a plan for that ML approach, because I think at insight and as of course, I've seen

178
00:10:40,960 --> 00:10:44,600
just many projects fail just because it's the wrong ML approach.

179
00:10:44,600 --> 00:10:48,800
And if you had just chosen a slightly different approach, you'd be in a much better spot.

180
00:10:48,800 --> 00:10:54,000
The second step is sort of building your MVP and that's something that was definitely

181
00:10:54,000 --> 00:10:55,000
the motto at insight.

182
00:10:55,000 --> 00:10:59,240
And I think that insight applied very well was sort of encouraging people to start extremely

183
00:10:59,240 --> 00:11:05,400
simple and build a full project before they go diving down the rabbit hole of research.

184
00:11:05,400 --> 00:11:09,040
The third part is I think one of the ones where I sat down with the most fellows over

185
00:11:09,040 --> 00:11:13,480
my time at insight, which is like, how do you debug models more often like, if your model

186
00:11:13,480 --> 00:11:17,800
is either not working or if it's working, but the performance isn't sufficient, how do

187
00:11:17,800 --> 00:11:20,040
you know what you should do next?

188
00:11:20,040 --> 00:11:23,000
How can you sort of take a deep dive into what your model is doing, what your data looks

189
00:11:23,000 --> 00:11:27,400
like to actually decide what you do in your next iteration cycle?

190
00:11:27,400 --> 00:11:32,080
And then the fourth step is sort of like deployment, monitoring and the concerns that come

191
00:11:32,080 --> 00:11:36,240
with showing the real world to a model and a model to the real world.

192
00:11:36,240 --> 00:11:43,560
Yeah, I am really appreciating all of the focus that kind of this real world ML and AI has

193
00:11:43,560 --> 00:11:48,680
been getting over the, I don't know if I would say past year, past couple of years.

194
00:11:48,680 --> 00:11:55,000
I mean, it's something that we've spent a lot of time focusing on and led us to produce

195
00:11:55,000 --> 00:12:02,000
the TwilmoCon AI platforms conference last week to kind of talk to how folks in real

196
00:12:02,000 --> 00:12:05,480
organizations are tackling these broader problems.

197
00:12:05,480 --> 00:12:09,320
But even just over the weekend on Twitter, like I'm seeing tweets all the time, like, hey,

198
00:12:09,320 --> 00:12:13,120
it's not just about the model, it's not just about the model anymore.

199
00:12:13,120 --> 00:12:18,640
And there seems to be a kind of growing recognition that, you know, not so much recognition, but

200
00:12:18,640 --> 00:12:25,480
more really appreciation that that's the case and that kind of the broader workflow that

201
00:12:25,480 --> 00:12:32,440
takes you from an idea or a problem identification to getting a solution in production is, you

202
00:12:32,440 --> 00:12:37,800
know, multifaceted and involves much more than just training up a model.

203
00:12:37,800 --> 00:12:38,800
Right.

204
00:12:38,800 --> 00:12:43,200
And to be fair, right, it used to be that just training up a model was pretty hard and

205
00:12:43,200 --> 00:12:46,920
maybe you need a team of people that understood sort of the internals deeply.

206
00:12:46,920 --> 00:12:51,760
But now, because the tooling has evolved so much, it actually is the case that the tooling

207
00:12:51,760 --> 00:12:56,280
so good and the courses are so good to train models that that becomes relatively simpler

208
00:12:56,280 --> 00:12:57,280
than the rest.

209
00:12:57,280 --> 00:12:58,280
Right.

210
00:12:58,280 --> 00:12:59,280
Right.

211
00:12:59,280 --> 00:13:04,080
Yeah, we're actually launching a study group in just a couple of weeks now.

212
00:13:04,080 --> 00:13:09,880
We do these study groups as part of our community where we'll do online courses together and

213
00:13:09,880 --> 00:13:14,960
we've done a bunch of fast AI courses and Stanford courses.

214
00:13:14,960 --> 00:13:22,080
But we're doing one starting with kind of an intro webinar on the 15th of February and

215
00:13:22,080 --> 00:13:28,600
then continuing on after that on this AI enterprise workflow course.

216
00:13:28,600 --> 00:13:33,720
This really interesting, unlike any kind of formal, this is on Coursera, unlike any formal

217
00:13:33,720 --> 00:13:38,800
courses I've seen, this one touches on a bunch of the things that you cover in the book.

218
00:13:38,800 --> 00:13:41,960
So, you know, how do you structure data collection?

219
00:13:41,960 --> 00:13:46,160
How do you kind of visualize and analyze your data explored in an exploratory mode and

220
00:13:46,160 --> 00:13:49,040
kind of test hypotheses?

221
00:13:49,040 --> 00:13:55,600
How do you identify data biases in the process of collecting your data?

222
00:13:55,600 --> 00:14:01,040
You mentioned using multiple models, you know, how do you use multiple models together

223
00:14:01,040 --> 00:14:06,080
with heuristics in order to build a solution and then, you know, unit testing?

224
00:14:06,080 --> 00:14:10,800
Like, when do you see that come up in a machine learning course, I'm both never.

225
00:14:10,800 --> 00:14:14,800
And then monitoring a model in production, deploying models with microservices.

226
00:14:14,800 --> 00:14:19,480
So, I'm really looking forward to that and I'll be putting a link into our show notes.

227
00:14:19,480 --> 00:14:23,960
So, anyone else that wants to take this course with me can do so.

228
00:14:23,960 --> 00:14:30,760
But it sounds like you are also of the belief that, you know, this is where the field needs

229
00:14:30,760 --> 00:14:37,960
to go in terms of actually getting value out of machine learning, kind of thinking about

230
00:14:37,960 --> 00:14:39,440
it more holistically.

231
00:14:39,440 --> 00:14:41,360
Yeah, I think so.

232
00:14:41,360 --> 00:14:47,680
I think researches like the class you described are really some of the most valuable classes

233
00:14:47,680 --> 00:14:52,560
or lessons that everyone can learn right now because this comes from a couple of things

234
00:14:52,560 --> 00:14:57,040
which is ad zip code, ads drive, and to some extent, and insight as well.

235
00:14:57,040 --> 00:15:01,000
You notice that the people that are able to contribute the fastest are sort of the ones

236
00:15:01,000 --> 00:15:04,640
that have the appreciation for the whole workflow.

237
00:15:04,640 --> 00:15:08,760
In fact, a lot of the times you'll see companies sort of be scope out projects for people

238
00:15:08,760 --> 00:15:12,560
that are like maybe interning, they're only here for a few months, and those projects

239
00:15:12,560 --> 00:15:14,480
are like just training the model, right?

240
00:15:14,480 --> 00:15:15,480
It's like they've done everything.

241
00:15:15,480 --> 00:15:18,920
They've like thought about the product, they've decided that for this product, this is

242
00:15:18,920 --> 00:15:22,480
the model we need, this is the data we have, they've prepared the data, they've decided

243
00:15:22,480 --> 00:15:25,520
how they're going to serve the model, and they're like, hey, you know, you have, you're

244
00:15:25,520 --> 00:15:28,720
here only for eight weeks or something, here's like a model training.

245
00:15:28,720 --> 00:15:33,840
And so while that's, you know, a fun project, it turns out that if you want to have a meaningful

246
00:15:33,840 --> 00:15:38,440
impact doing all of these other parts is what's going to be most helpful to your

247
00:15:38,440 --> 00:15:42,240
career at a larger company, or even if you're building your own startup, right, to just

248
00:15:42,240 --> 00:15:43,800
actually getting it out the door.

249
00:15:43,800 --> 00:15:49,560
Yeah, I love that you're only 45 pages into this book before you're into the build your

250
00:15:49,560 --> 00:15:54,600
own end-to-end or build your first end-to-end pipeline chapter.

251
00:15:54,600 --> 00:15:57,880
And that chapter in the middle of that chapter is like testing.

252
00:15:57,880 --> 00:15:58,880
Yeah.

253
00:15:58,880 --> 00:16:02,720
So there's, at the end of the first chapter, I'll also introduce you with Monica Regatti,

254
00:16:02,720 --> 00:16:07,880
and she has this great concept that she talks about, she talks about the impact bottleneck.

255
00:16:07,880 --> 00:16:11,920
And sometimes, one of the examples she gives, she says like, well, sometimes your ML might

256
00:16:11,920 --> 00:16:16,920
be perfect, but your product is still dead, you're still dead in the water, because you've

257
00:16:16,920 --> 00:16:22,080
sort of, you know, kind of misunderstood a fundamental need of like how your users would

258
00:16:22,080 --> 00:16:23,480
actually interact with it.

259
00:16:23,480 --> 00:16:27,560
And this is more of maybe like a general, you know, software engineering, like first you

260
00:16:27,560 --> 00:16:32,520
should talk to users before you build it, but it's also a good ML tip to say like, well,

261
00:16:32,520 --> 00:16:38,680
you should, as quickly as possible, get to the point where you can show the like UI to

262
00:16:38,680 --> 00:16:42,640
a friend or a user, have them try it, and then it gives out results.

263
00:16:42,640 --> 00:16:45,960
Even if you don't have a good model, even if it's a heuristic, just because then you might

264
00:16:45,960 --> 00:16:48,920
notice that like, oh, they're using it in a completely different way than you thought.

265
00:16:48,920 --> 00:16:50,800
And that's super complicated model that you thought you needed.

266
00:16:50,800 --> 00:16:51,800
You actually don't need it at all.

267
00:16:51,800 --> 00:16:52,800
You need something else.

268
00:16:52,800 --> 00:17:00,360
Yeah, that's kind of applying the lean startup type of approach, which I don't even

269
00:17:00,360 --> 00:17:04,320
know if it deserves a net, or if it has a net, you know, we need to call it by a name

270
00:17:04,320 --> 00:17:05,320
anymore.

271
00:17:05,320 --> 00:17:09,880
It's kind of how we do things with MVPs, but it's kind of applying that approach to flesh

272
00:17:09,880 --> 00:17:14,600
out some of the, you know, where it's just a bad idea.

273
00:17:14,600 --> 00:17:15,600
Exactly.

274
00:17:15,600 --> 00:17:21,280
And I think that approach is like more and more valuable as the iteration time goes up,

275
00:17:21,280 --> 00:17:22,280
right?

276
00:17:22,280 --> 00:17:25,840
So like, if you don't show it as someone, but it takes you, you know, 50 minutes to build,

277
00:17:25,840 --> 00:17:26,840
you've lost 50 minutes.

278
00:17:26,840 --> 00:17:27,840
That's okay.

279
00:17:27,840 --> 00:17:30,520
So if you don't show it as someone and you need to train a model and get the data and

280
00:17:30,520 --> 00:17:34,000
that whole process is going to take you two months, then you've lost a lot of time by

281
00:17:34,000 --> 00:17:35,000
not doing it.

282
00:17:35,000 --> 00:17:40,440
You know, what I'm curious about is there's, you know, the steps that you need to do to

283
00:17:40,440 --> 00:17:42,280
kind of build this sample app.

284
00:17:42,280 --> 00:17:47,520
And then there's kind of the broader, you know, things that you need to think about to

285
00:17:47,520 --> 00:17:53,360
apply this methodology to your own problems.

286
00:17:53,360 --> 00:17:54,360
Right.

287
00:17:54,360 --> 00:17:58,480
So things like walking folks through building the end pipeline for this app or acquiring

288
00:17:58,480 --> 00:17:59,480
a data set.

289
00:17:59,480 --> 00:18:04,200
Like, how do you make sure it's tangible enough that, you know, you're moving, you're making

290
00:18:04,200 --> 00:18:08,640
progress on your example application, but also broad enough that they can take it and

291
00:18:08,640 --> 00:18:09,640
run with it.

292
00:18:09,640 --> 00:18:10,640
Yeah.

293
00:18:10,640 --> 00:18:12,520
I think that's a, that's a really great question.

294
00:18:12,520 --> 00:18:15,960
It's really hard to think about how you balance that.

295
00:18:15,960 --> 00:18:21,160
In fact, the reason, a big part of the reason for having a sample app is to hold myself

296
00:18:21,160 --> 00:18:26,080
accountable so that my advice would actually be practical and the methods that I gave were

297
00:18:26,080 --> 00:18:27,080
actually true.

298
00:18:27,080 --> 00:18:28,080
Right.

299
00:18:28,080 --> 00:18:30,040
If I, if I tell you something that's actually not going to work, then I'm going to have

300
00:18:30,040 --> 00:18:32,840
a hard time demonstrating to you that it works on this toy example.

301
00:18:32,840 --> 00:18:33,840
Yeah.

302
00:18:33,840 --> 00:18:38,560
And so most chapters are structured in a way where I try to give broad, like, this is

303
00:18:38,560 --> 00:18:44,080
how you generally look for a data set and try to explore it.

304
00:18:44,080 --> 00:18:48,560
Like, for example, you generally, like, if you have a current approach right, because

305
00:18:48,560 --> 00:18:52,080
you've built your plan in the previous chapter and you're saying, well, like, we're going

306
00:18:52,080 --> 00:18:55,920
to, we're going to use this current approach, trying to think of problems that have sort

307
00:18:55,920 --> 00:18:59,800
of the same kind of approach, even if they're in a completely different domain.

308
00:18:59,800 --> 00:19:05,800
So maybe you're doing, you know, like, some, like, reaction prediction for molecules,

309
00:19:05,800 --> 00:19:08,400
but maybe that looks just like text translation in a way, right?

310
00:19:08,400 --> 00:19:12,280
You're predicting a sequence because in other sequence, you can try to find that form,

311
00:19:12,280 --> 00:19:16,120
that first data set, the like translation data set, see if your approach works on that

312
00:19:16,120 --> 00:19:18,000
and then change to your current data set.

313
00:19:18,000 --> 00:19:19,960
That's sort of like a general, how you do it.

314
00:19:19,960 --> 00:19:24,040
And then I illustrate it as an example with, okay, well, for this ML editor thing, this

315
00:19:24,040 --> 00:19:27,520
is what, like, these are the data sets we're actually going to use.

316
00:19:27,520 --> 00:19:29,800
So for most concepts, I try to do both.

317
00:19:29,800 --> 00:19:36,320
The couple of examples or chapters that we've talked about, like, what are the, the broad

318
00:19:36,320 --> 00:19:41,280
principles that folks need to be thinking about when they're building their first pipelines,

319
00:19:41,280 --> 00:19:43,480
when they're acquiring their initial data sets?

320
00:19:43,480 --> 00:19:44,480
Yeah.

321
00:19:44,480 --> 00:19:49,480
So I think we skipped a little bit over the first section, but I think there's, there's

322
00:19:49,480 --> 00:19:53,200
a broad principle in the first section of, like, going for a product to an ML approach

323
00:19:53,200 --> 00:19:57,000
that I generally find really valuable and find that, I don't know, I think everybody

324
00:19:57,000 --> 00:19:58,000
should do.

325
00:19:58,000 --> 00:20:02,240
Kind of sets the tone for the rest of it, it sounds like, yeah.

326
00:20:02,240 --> 00:20:08,560
Which is that for, for the same product goal, you have, again, many ways to do it, many

327
00:20:08,560 --> 00:20:10,440
ways to tackle that problem using machine learning.

328
00:20:10,440 --> 00:20:16,520
So an example I use is, let's say that you're, you know, a retailer and you have an online

329
00:20:16,520 --> 00:20:20,520
catalog of items and you want to help people when they're, when they're typing something

330
00:20:20,520 --> 00:20:25,360
in the search bar to find the category or the types of items that, you know, they, they

331
00:20:25,360 --> 00:20:27,000
would want to buy.

332
00:20:27,000 --> 00:20:29,760
One way to do this, and maybe the simplest way to think about it is you're like, oh, well,

333
00:20:29,760 --> 00:20:33,040
somebody's writing something in the search bar, I'm going to try to, like, autocomplete

334
00:20:33,040 --> 00:20:34,440
the rest of what they're going to say, right?

335
00:20:34,440 --> 00:20:39,080
So if they type hands, maybe you autocomplete, like, handbag or something, right?

336
00:20:39,080 --> 00:20:45,000
A slightly, maybe like, different approach is they type something and you try to identify

337
00:20:45,000 --> 00:20:47,840
which words in their query are relevant to products.

338
00:20:47,840 --> 00:20:52,160
So if they're like, I want, you know, like, a handbag of this brand, like, you try to

339
00:20:52,160 --> 00:20:55,200
find the name of the brand.

340
00:20:55,200 --> 00:20:59,200
And then a third approach that's even simpler is they type anything in that bar and all

341
00:20:59,200 --> 00:21:03,440
you do is you try to like classify it and like, is it about handbags, is it about jeans,

342
00:21:03,440 --> 00:21:05,640
is it about, you know, shoes or something else?

343
00:21:05,640 --> 00:21:10,480
And those three approaches, all would have a slightly different UI, right?

344
00:21:10,480 --> 00:21:13,320
You would build sort of the way you show results and the way you show suggestions slightly

345
00:21:13,320 --> 00:21:14,320
differently.

346
00:21:14,320 --> 00:21:19,080
But they all are basically each successive approach is like an order of magnitude easier

347
00:21:19,080 --> 00:21:22,600
to do than the one before, especially if you don't have much data, right?

348
00:21:22,600 --> 00:21:26,240
If you don't have much data, that final approach building a classifier is something where

349
00:21:26,240 --> 00:21:30,720
you can label data for a few hours and you'll have a classifier that's decent.

350
00:21:30,720 --> 00:21:33,880
If you want to do the sort of like, not a distraction part, that's going to take you

351
00:21:33,880 --> 00:21:36,560
maybe a couple of days, but you can probably get something that's pretty good.

352
00:21:36,560 --> 00:21:40,640
If you want to go do the full like language model approach of predicting the next tokens,

353
00:21:40,640 --> 00:21:43,880
that's going to require a lot of data and you're also going to have a lot more variance

354
00:21:43,880 --> 00:21:47,720
where like, sometimes you might predict some crazy things and unless you have the engineering

355
00:21:47,720 --> 00:21:52,160
resources to add that filtering layer on top, that'll make it very hard to shift.

356
00:21:52,160 --> 00:21:56,920
And so I think like a question that generally people should ask is like, what is the absolute

357
00:21:56,920 --> 00:22:02,800
simplest model in that bucket of model and I give sort of like a hierarchy of models?

358
00:22:02,800 --> 00:22:06,680
What is the simplest model that you could use that could solve what you're currently

359
00:22:06,680 --> 00:22:07,920
trying to do?

360
00:22:07,920 --> 00:22:11,080
And then if that model's too simple and it's not good enough, it's fine.

361
00:22:11,080 --> 00:22:14,240
You can improve on it, but you'll have spent you know, a couple hours building it instead

362
00:22:14,240 --> 00:22:17,240
of a couple months being building a model that doesn't work.

363
00:22:17,240 --> 00:22:21,720
And so with that in mind, we go into building the pipeline and acquiring the data set, what

364
00:22:21,720 --> 00:22:23,880
are some of the broad principles there?

365
00:22:23,880 --> 00:22:30,120
Yeah, so I think there's this idea that I would say for any data project that you tackle,

366
00:22:30,120 --> 00:22:34,240
any new project, you should spend like a couple hours looking at the data.

367
00:22:34,240 --> 00:22:37,640
And the couple hours is definitely the minimum time you should spend looking at the day.

368
00:22:37,640 --> 00:22:38,640
You could spend a lot of more time.

369
00:22:38,640 --> 00:22:43,600
That's fine, but you should never spend less than a few hours looking at the data.

370
00:22:43,600 --> 00:22:44,920
Looking at the data means a few things.

371
00:22:44,920 --> 00:22:50,200
And I found that in my experience of insight, often people think about it in terms of aggregates,

372
00:22:50,200 --> 00:22:51,200
right?

373
00:22:51,200 --> 00:22:54,120
They're like, okay, well, now I'm looking at this database of like texts, what's the

374
00:22:54,120 --> 00:22:58,360
average length of a sentence, you know, how many different words are there.

375
00:22:58,360 --> 00:23:02,800
And that's fine, and that's something that you should do, both to find errors or things

376
00:23:02,800 --> 00:23:03,800
that surprise you.

377
00:23:03,800 --> 00:23:07,560
You're like, oh, I was looking at like a database of tweets and somehow like all the tweets

378
00:23:07,560 --> 00:23:10,000
are one word long, just probably something wrong.

379
00:23:10,000 --> 00:23:14,000
But you should also actually look at individual examples, and that's something that Rob

380
00:23:14,000 --> 00:23:17,960
Monroe actually talks about a lot, where a lot of these individual examples will be the

381
00:23:17,960 --> 00:23:20,440
source for which model you end up choosing, right?

382
00:23:20,440 --> 00:23:23,280
So you have your product goal on one side, you've said like, okay, well, maybe like a classifier

383
00:23:23,280 --> 00:23:25,080
is a single thing I can start with.

384
00:23:25,080 --> 00:23:30,280
But then looking at the data will give you your initial set of like features or types

385
00:23:30,280 --> 00:23:35,720
of model that you think could reasonably capture what you're trying to identify in this dataset.

386
00:23:35,720 --> 00:23:40,400
And so in the chapter, I go into, well, how can you look at individual examples in a

387
00:23:40,400 --> 00:23:41,400
dataset?

388
00:23:41,400 --> 00:23:42,400
That's hard, right?

389
00:23:42,400 --> 00:23:46,120
If you have a dataset with like 10,000 examples or 10 million, you're not going to look

390
00:23:46,120 --> 00:23:47,120
at all of them.

391
00:23:47,120 --> 00:23:52,560
And so there's some methods where you can basically use some NLP approaches or some

392
00:23:52,560 --> 00:23:55,320
approaches from other domains to embed your data.

393
00:23:55,320 --> 00:23:59,760
And then once you have this sort of like map of all your data points, you look into each

394
00:23:59,760 --> 00:24:04,080
specific sub area and you're like, oh, like this sub area, you know, for me, for the

395
00:24:04,080 --> 00:24:07,200
example, it was questions on Stack Overflow.

396
00:24:07,200 --> 00:24:11,320
So this area of questions, like they all seem to be about, you know, the English language.

397
00:24:11,320 --> 00:24:15,160
And so they use a lot of like maybe complex words and they have like longer sentences.

398
00:24:15,160 --> 00:24:18,840
Oh, this area, you know, is all about like non-native speakers, that sort of stuff.

399
00:24:18,840 --> 00:24:22,480
And so then it helps you like build features where it's like, okay, well, how can I identify

400
00:24:22,480 --> 00:24:24,840
a good question in this area or in that area?

401
00:24:24,840 --> 00:24:31,680
Do you have a sense for how you know when you have done enough looking at your data in

402
00:24:31,680 --> 00:24:39,520
aggregate or as individual items that you're, you know, you're ready to move on?

403
00:24:39,520 --> 00:24:47,760
Is there a feeling there or a sense or a checklist or something that beyond just, hey, I've,

404
00:24:47,760 --> 00:24:52,800
you know, looking at my watch here, the two hours is up, I can move on to the fun stuff.

405
00:24:52,800 --> 00:24:57,600
Like what, what should you have taken away from that experience?

406
00:24:57,600 --> 00:25:03,800
Yeah, that's something that I think obviously is going to depend on your data sense.

407
00:25:03,800 --> 00:25:04,800
So you're right.

408
00:25:04,800 --> 00:25:05,800
Sometimes it's going to be two hours.

409
00:25:05,800 --> 00:25:07,280
Sometimes it's maybe going to be two days.

410
00:25:07,280 --> 00:25:10,200
Sometimes you're going to realize that something's very wrong and you, you shouldn't move

411
00:25:10,200 --> 00:25:11,200
on, right?

412
00:25:11,200 --> 00:25:14,440
If you realize that something's off, you should go back to the drawing board and sort of

413
00:25:14,440 --> 00:25:18,520
like get another data set or look at why your data is looking all weird.

414
00:25:18,520 --> 00:25:23,040
But in the case where you are ready to move on to a model, I'd say that that's when you

415
00:25:23,040 --> 00:25:28,120
have a strong hypothesis about how your model will actually do its work.

416
00:25:28,120 --> 00:25:32,560
So you know, we've talked about like the retailer that like you type queries and then it tells

417
00:25:32,560 --> 00:25:34,080
you like, oh, is it a handbag?

418
00:25:34,080 --> 00:25:35,160
Is it like something else?

419
00:25:35,160 --> 00:25:39,920
Well, if you look at a bunch of examples and you realize that really there's, you know,

420
00:25:39,920 --> 00:25:46,080
only two or three ways that people ever say handbag or jeans that the vocabulary is like

421
00:25:46,080 --> 00:25:47,840
pretty simple.

422
00:25:47,840 --> 00:25:49,440
That sentences are usually pretty short.

423
00:25:49,440 --> 00:25:54,800
You can say like, okay, well, for this approach, like, you know, sort of like a simple bag

424
00:25:54,800 --> 00:25:59,360
of words with like word counts will work because I'm reasonably confident that like all

425
00:25:59,360 --> 00:26:05,320
I need to know is like what, which of these three words is in the sentence.

426
00:26:05,320 --> 00:26:09,280
More generally, I think what I'm trying to say is when you build your first model, you

427
00:26:09,280 --> 00:26:12,360
should already have the part goal, that section one.

428
00:26:12,360 --> 00:26:17,000
And then a hypothesis about how your model, like, what will make your model succeed?

429
00:26:17,000 --> 00:26:20,120
That's like the part two of looking at the data set because then once you've trained

430
00:26:20,120 --> 00:26:23,720
your model, what you want to do is you want to check your results against your assumption.

431
00:26:23,720 --> 00:26:28,200
You want to say like, okay, well, I thought that like because sentences are short, you know,

432
00:26:28,200 --> 00:26:32,760
the model would be easily able to pick up on different words, but it turns out that

433
00:26:32,760 --> 00:26:35,000
it's not, or it turns out that it's not performing on these.

434
00:26:35,000 --> 00:26:38,160
And then you can go back to the data, right, and say like, okay, well, let's look at the

435
00:26:38,160 --> 00:26:41,720
examples that my model got wrong and like, why did it get these wrong?

436
00:26:41,720 --> 00:26:43,240
And then make a new hypothesis.

437
00:26:43,240 --> 00:26:47,520
And that's the fastest way and generally the most, the best way to iterate because it'll

438
00:26:47,520 --> 00:26:52,120
actually let you understand how your model is working, which once you're ready to deploy

439
00:26:52,120 --> 00:26:54,640
it, you'd much rather be like, oh, well, I'm pretty sure that this is how my model's

440
00:26:54,640 --> 00:26:58,120
making decisions, rather than like, I trained a model, I got a really high score.

441
00:26:58,120 --> 00:27:00,280
I hope everything goes well.

442
00:27:00,280 --> 00:27:05,360
Do you talk about explainability in the context of models?

443
00:27:05,360 --> 00:27:10,280
That's something that is getting a lot of attention for folks that are, you know, particularly

444
00:27:10,280 --> 00:27:13,880
working in business or enterprise types of environments.

445
00:27:13,880 --> 00:27:14,880
Yeah.

446
00:27:14,880 --> 00:27:17,120
This is a topic that it gets here.

447
00:27:17,120 --> 00:27:19,160
A lot of attention, a lot of debate.

448
00:27:19,160 --> 00:27:22,200
There's generally a few ways to look at explainability.

449
00:27:22,200 --> 00:27:24,120
I say there's a lot of attention.

450
00:27:24,120 --> 00:27:29,000
And I'm asking primarily from the perspective of having a sense of what your model is doing

451
00:27:29,000 --> 00:27:31,320
so that you can better debug it.

452
00:27:31,320 --> 00:27:32,320
Yeah.

453
00:27:32,320 --> 00:27:33,720
So there's two parts to that.

454
00:27:33,720 --> 00:27:38,560
One part is to have a sense and to debug it, you need to look at like the internals of

455
00:27:38,560 --> 00:27:39,560
your model.

456
00:27:39,560 --> 00:27:43,120
And then you can look at like its feature importance, its coefficients, that sort of stuff that

457
00:27:43,120 --> 00:27:46,040
gives you some sense of explainability.

458
00:27:46,040 --> 00:27:51,120
But then there's actually looking at individual data examples or even like multiple data

459
00:27:51,120 --> 00:27:55,360
examples, but trends in results on examples, which I think gives you a much better sense

460
00:27:55,360 --> 00:27:56,360
of explainability.

461
00:27:56,360 --> 00:28:03,080
And what I mean by that is looking at what are the like 10 examples where you're, let's

462
00:28:03,080 --> 00:28:08,880
say your classifier was the most confident that it was class one, but your class where

463
00:28:08,880 --> 00:28:10,040
it was wrong.

464
00:28:10,040 --> 00:28:12,520
What are the ones where it was most confident of the other class?

465
00:28:12,520 --> 00:28:17,080
What are the ones where it was the most unsure, like looking at those examples and seeing

466
00:28:17,080 --> 00:28:23,080
like, oh, you know, it seems like every time there's a question that's 16 sentences long,

467
00:28:23,080 --> 00:28:26,840
the model just doesn't know what's going on, it just gives up.

468
00:28:26,840 --> 00:28:28,640
That gives you one sense of explainability.

469
00:28:28,640 --> 00:28:31,640
And the final sense of explainability that I think a lot of people talk about is black

470
00:28:31,640 --> 00:28:32,640
box explainers.

471
00:28:32,640 --> 00:28:37,680
They're like shop values or lime and we actually use those pretty heavily in the book because

472
00:28:37,680 --> 00:28:41,920
you can use them to power suggestions for your users.

473
00:28:41,920 --> 00:28:45,520
So again, the example of a case study is like something that's going to help you write

474
00:28:45,520 --> 00:28:46,520
better.

475
00:28:46,520 --> 00:28:48,560
So you give it something you wrote, a question you've wrote.

476
00:28:48,560 --> 00:28:52,400
And then we have our model that predicts whether you wrote something basically good or bad

477
00:28:52,400 --> 00:28:54,080
for some definition of good or bad.

478
00:28:54,080 --> 00:28:59,480
And then we use lime to say like, oh, you know, we said that this question was like 60%

479
00:28:59,480 --> 00:29:04,520
good, these are the features that if you were to change them, would push your questions

480
00:29:04,520 --> 00:29:06,120
towards the positive class more, right?

481
00:29:06,120 --> 00:29:09,080
Well, it seems like your question's much too long.

482
00:29:09,080 --> 00:29:12,400
So if you cut it down, actually, like we would have said that it was much better that

483
00:29:12,400 --> 00:29:13,400
sort of stuff.

484
00:29:13,400 --> 00:29:17,560
So you can use explainability to power sort of user-facing suggestions.

485
00:29:17,560 --> 00:29:18,560
Interesting.

486
00:29:18,560 --> 00:29:19,560
Yeah.

487
00:29:19,560 --> 00:29:20,560
Is that done commonly?

488
00:29:20,560 --> 00:29:22,560
Do you see that a lot?

489
00:29:22,560 --> 00:29:24,640
I see that in a few cases.

490
00:29:24,640 --> 00:29:30,560
But I think there's no example is perfect, and so the emulator definitely has a few things

491
00:29:30,560 --> 00:29:34,080
where it's like, well, how wildly applicable is that?

492
00:29:34,080 --> 00:29:38,800
But out of companies that do that sort of stuff that do help people write better, actually

493
00:29:38,800 --> 00:29:46,320
have an interview with Chris Harland, who is from a textio, where they do that for general

494
00:29:46,320 --> 00:29:48,640
job postings and job communication.

495
00:29:48,640 --> 00:29:54,360
And they do mention using sort of similar methods, at least to surface potential features,

496
00:29:54,360 --> 00:29:59,440
because whenever you're doing writing recommendations, the real challenge is that you want your users

497
00:29:59,440 --> 00:30:00,440
to understand them.

498
00:30:00,440 --> 00:30:04,520
It's explainability becomes crucial because if I give you anything and you don't understand

499
00:30:04,520 --> 00:30:07,400
why I'm recommending it to you, then you're not going to use my product.

500
00:30:07,400 --> 00:30:10,640
And so for a subset of ML products, explainability isn't just a bonus.

501
00:30:10,640 --> 00:30:12,480
It's what the product is.

502
00:30:12,480 --> 00:30:16,440
You mentioned Lime for folks that want to learn more about that.

503
00:30:16,440 --> 00:30:24,520
You can check out my seventh interview ever with Carlos Guestrin back in October of 2016.

504
00:30:24,520 --> 00:30:28,440
What are some of the other things that come up from a debugging perspective?

505
00:30:28,440 --> 00:30:32,360
I think from the debugging perspective, we end up going back to the iteration loop conversation

506
00:30:32,360 --> 00:30:38,800
we had earlier, which is one debugging tip that actually Ross gave me, which you interviewed

507
00:30:38,800 --> 00:30:42,160
earlier, and that is used widely on site and elsewhere.

508
00:30:42,160 --> 00:30:47,960
I've seen it use the industry is when your model doesn't work, not just when it's, when

509
00:30:47,960 --> 00:30:52,360
you're not happy with this current score, but when something is not working, you should

510
00:30:52,360 --> 00:30:57,400
cut down your data set to one or two examples and then get your model to work.

511
00:30:57,400 --> 00:31:02,400
Like by working, what I mean is just get your model to train and then output predictions.

512
00:31:02,400 --> 00:31:06,480
They'll be completely random because you're overfitting on a couple examples.

513
00:31:06,480 --> 00:31:07,480
But that alone.

514
00:31:07,480 --> 00:31:09,240
But if you can't do that.

515
00:31:09,240 --> 00:31:10,240
Exactly.

516
00:31:10,240 --> 00:31:11,240
Exactly.

517
00:31:11,240 --> 00:31:16,720
And so there's sort of like that that's helped so many fellows and honestly, like colleagues

518
00:31:16,720 --> 00:31:21,400
at AdFuse companies where like if every training run takes four hours and at the end, you

519
00:31:21,400 --> 00:31:25,640
have like, you know, the shape mismatch or something like it is the most infuriating process

520
00:31:25,640 --> 00:31:26,640
you can go through.

521
00:31:26,640 --> 00:31:33,600
So should you just start there or should you regress there when things aren't working?

522
00:31:33,600 --> 00:31:37,560
I think there's nothing wrong with sort of starting with the Hail Mary of like I'm going

523
00:31:37,560 --> 00:31:38,840
to write my code.

524
00:31:38,840 --> 00:31:41,040
This should work.

525
00:31:41,040 --> 00:31:44,040
And then if it doesn't, then I think the first step is going there.

526
00:31:44,040 --> 00:31:48,880
Like if you've written your first training loop and like something's very wrong, the first

527
00:31:48,880 --> 00:31:53,520
step is like, okay, you know, you go to your first line and you like take X train equals

528
00:31:53,520 --> 00:31:57,640
like X train, period two, like you just take a couple examples and then you run it again

529
00:31:57,640 --> 00:31:59,400
and you try to get that to work.

530
00:31:59,400 --> 00:32:02,320
And in fact, in debugging, I mentioned like three steps.

531
00:32:02,320 --> 00:32:06,440
The first step is that, which I call debugging the wiring, like making sure that data can

532
00:32:06,440 --> 00:32:08,240
go back and forth.

533
00:32:08,240 --> 00:32:13,040
And I think the first step is debugging training performance.

534
00:32:13,040 --> 00:32:16,640
Once you've debugged that first aspect, what you want to see is like, can I over fit

535
00:32:16,640 --> 00:32:17,840
on my data set?

536
00:32:17,840 --> 00:32:22,440
And so if you take a data set of like, you know, 2000 examples, can you train a model

537
00:32:22,440 --> 00:32:24,640
that becomes very good on this data set?

538
00:32:24,640 --> 00:32:26,720
Once more, this model isn't going to be good in production because you've trained it

539
00:32:26,720 --> 00:32:27,720
to just be good on data.

540
00:32:27,720 --> 00:32:28,720
It's already seen.

541
00:32:28,720 --> 00:32:32,240
But again, if you can't do that, you're probably not going to be able to train a model

542
00:32:32,240 --> 00:32:36,240
that learns general things at all if it can't learn local features.

543
00:32:36,240 --> 00:32:38,400
And then finally, you debug generalization.

544
00:32:38,400 --> 00:32:43,200
And so these three successive steps are our steps, and honestly, I think most program

545
00:32:43,200 --> 00:32:47,320
directors and insight, for example, are very used to like helping their fellows walk

546
00:32:47,320 --> 00:32:49,280
through each of these steps successfully.

547
00:32:49,280 --> 00:32:52,440
But I haven't seen as many resources just sort of like outline them.

548
00:32:52,440 --> 00:32:56,280
So if you just follow that recipe, usually you'll debug your models much, much faster.

549
00:32:56,280 --> 00:33:01,880
It's like the hierarchy of like what you should debug in three steps.

550
00:33:01,880 --> 00:33:04,800
What categories of models are covered in the book?

551
00:33:04,800 --> 00:33:11,120
Is it, you mentioned TF IDF, and you also mentioned that you're composing multiple models

552
00:33:11,120 --> 00:33:16,400
are using both kind of traditional Python models,

553
00:33:16,400 --> 00:33:22,760
scikit-learn-ish types of models, or are you doing deep learning as well?

554
00:33:22,760 --> 00:33:27,840
What's the portfolio look like?

555
00:33:27,840 --> 00:33:31,040
That's a, let me think, how many?

556
00:33:31,040 --> 00:33:32,880
We have a bat.

557
00:33:32,880 --> 00:33:36,400
We have three models that are used in the main example.

558
00:33:36,400 --> 00:33:41,640
And then I'd say like probably half a dozen examples of other models as like sort of separate

559
00:33:41,640 --> 00:33:42,640
from the case study.

560
00:33:42,640 --> 00:33:48,120
It's like this is how you would use VGG to extract features from images, for example.

561
00:33:48,120 --> 00:33:52,480
For the main application, the models we use are pretty simple.

562
00:33:52,480 --> 00:33:55,120
It starts with a heuristic, that's not even a model.

563
00:33:55,120 --> 00:33:57,560
So you haven't written a birth version yet?

564
00:33:57,560 --> 00:33:58,560
No, I have not.

565
00:33:58,560 --> 00:34:02,520
I have purposefully stayed away from birth.

566
00:34:02,520 --> 00:34:13,080
I don't have anything against more recent approaches, of course, they're breakthroughs.

567
00:34:13,080 --> 00:34:15,360
But I think one, they're covered enough.

568
00:34:15,360 --> 00:34:19,800
I think if you talk to somebody that's either new to the field or even working in LPN,

569
00:34:19,800 --> 00:34:24,760
you ask them, out of all of the blog posts and research articles on things you've read

570
00:34:24,760 --> 00:34:30,120
recently, how many of them were about deep language models, and they'd probably say 85%.

571
00:34:30,120 --> 00:34:34,000
So I didn't feel like I needed to sort of add my break onto that cathedral.

572
00:34:34,000 --> 00:34:40,320
At the same time, a lot of the advice about building practical applications apply regardless

573
00:34:40,320 --> 00:34:42,480
of the model that you use.

574
00:34:42,480 --> 00:34:47,640
And so I felt that for readers, it was best if I kept with models that were as simple

575
00:34:47,640 --> 00:34:51,600
as possible because I didn't want to just add complexity for the sake of adding complexity.

576
00:34:51,600 --> 00:34:55,480
I think if you were to use birth or something more complicated, you could probably get just

577
00:34:55,480 --> 00:34:58,080
a better performance metric on some of these models.

578
00:34:58,080 --> 00:35:02,640
But to the point of this book, that's something that you do in iteration number four, number

579
00:35:02,640 --> 00:35:04,040
five, number six, number seven.

580
00:35:04,040 --> 00:35:08,400
The book shows you the first three iterations, which is one, you build your select first

581
00:35:08,400 --> 00:35:13,280
sub at a crappy heuristic, number two, you build a simple model, number three, you build

582
00:35:13,280 --> 00:35:16,360
a slightly more complicated model, and number four, actually, and I guess this is spoiler

583
00:35:16,360 --> 00:35:17,360
alert.

584
00:35:17,360 --> 00:35:20,000
But you look at your more complicated model and you realize it's too complicated and you

585
00:35:20,000 --> 00:35:23,800
remove some of the useless features to make, so the final model we use in prototype.

586
00:35:23,800 --> 00:35:29,840
So you wouldn't be following your own advice if you jump right into Burton chapter two.

587
00:35:29,840 --> 00:35:30,840
Exactly.

588
00:35:30,840 --> 00:35:34,360
If somebody wants to write a sequel to this book, I think there's like a lot of rooms to

589
00:35:34,360 --> 00:35:39,280
try, Bert and GPT-2 and other more complicated approaches, but I think, yeah, in general,

590
00:35:39,280 --> 00:35:42,720
even in what would that be called, building more complicated, machine learning pattern

591
00:35:42,720 --> 00:35:43,720
applications?

592
00:35:43,720 --> 00:35:46,760
Yeah, it would be called building applications the third year.

593
00:35:46,760 --> 00:35:51,240
When you join a team at a company, the first year they're doing this, and then if the

594
00:35:51,240 --> 00:35:55,560
team's been around for like 10 years, they're just throwing anything they can at the wall,

595
00:35:55,560 --> 00:35:58,800
they're like, oh, let's try it, Bert, you know, let's try anything.

596
00:35:58,800 --> 00:36:03,960
We didn't cover the feature importance of you spend quite a bit of time on feature importance.

597
00:36:03,960 --> 00:36:05,720
How do you see that coming up?

598
00:36:05,720 --> 00:36:11,040
Yeah, I think in this case, right, it was especially useful because we make suggestions for

599
00:36:11,040 --> 00:36:12,720
users based on the features of our application.

600
00:36:12,720 --> 00:36:19,280
So that application specific as opposed to a general step in the process?

601
00:36:19,280 --> 00:36:20,280
Good question.

602
00:36:20,280 --> 00:36:26,600
The importance is a step for debugging, where especially if you're doing a, sort of if you're

603
00:36:26,600 --> 00:36:30,320
using a model that has many features either because it's a deep learning model and just

604
00:36:30,320 --> 00:36:33,840
does its own feature generation, or you know, you just have like a tabular data set with

605
00:36:33,840 --> 00:36:38,160
thousands of features, looking at feature importance can help you check the assumptions

606
00:36:38,160 --> 00:36:41,880
that we talked about, meaning like you've made assumptions, you said like, well, because

607
00:36:41,880 --> 00:36:45,080
length of a question is important, it'll be an important feature.

608
00:36:45,080 --> 00:36:48,360
And then you look at your features and it's not, you know, it's like the importance is

609
00:36:48,360 --> 00:36:50,840
zero, then your assumption was wrong.

610
00:36:50,840 --> 00:36:56,120
And so as part of your iteration cycle, looking at feature importance is really valuable.

611
00:36:56,120 --> 00:36:59,320
The specific chapter you're talking about, which is using feature importance to make

612
00:36:59,320 --> 00:37:04,280
recommendations, I would say is the only chapter in the whole book that's very, very specific

613
00:37:04,280 --> 00:37:08,800
to the, to the emulator, it's sort of, it was actually reviewers of the books that like

614
00:37:08,800 --> 00:37:12,480
the book is great, but I feel like we should have a deep dive into the, the sort of like

615
00:37:12,480 --> 00:37:14,800
emulator at some point that's like wraps things up.

616
00:37:14,800 --> 00:37:18,200
And so this is, this is the books attempt at that of like, okay, well, let's take everything

617
00:37:18,200 --> 00:37:21,560
together and actually get the emulator to like a, a ready product.

618
00:37:21,560 --> 00:37:25,920
And then the last part of the book goes into deployment and modern, monitoring.

619
00:37:25,920 --> 00:37:28,960
What are the, the key takeaways there?

620
00:37:28,960 --> 00:37:29,960
Yeah.

621
00:37:29,960 --> 00:37:33,760
So there's, there's like three main aspects that are covered there.

622
00:37:33,760 --> 00:37:39,960
One is the, just the, the ethics of deployment and the things that you should think about

623
00:37:39,960 --> 00:37:41,520
when you deploy them on models.

624
00:37:41,520 --> 00:37:45,560
I, this chapter is mostly about resources that I share.

625
00:37:45,560 --> 00:37:49,400
There's actually an excellent, an excellent free or highly book about data ethics that I

626
00:37:49,400 --> 00:37:50,880
linked to in this, in this chapter.

627
00:37:50,880 --> 00:37:56,040
And there's, there's a lot of, there's a big body of work, but I try to just give some,

628
00:37:56,040 --> 00:38:00,760
some aspects that you might want to, want to think about based on recent research.

629
00:38:00,760 --> 00:38:06,400
Then the other two aspects are just what is the engineering work around models, both

630
00:38:06,400 --> 00:38:09,440
as you deploy them and once you've deployed them.

631
00:38:09,440 --> 00:38:16,280
And so a lot of takeaways from the engineering work are for most complicated models.

632
00:38:16,280 --> 00:38:20,520
So I think famously the Google smart reply.

633
00:38:20,520 --> 00:38:25,160
So Google smart replies, you get an email and they suggest those three responses that you

634
00:38:25,160 --> 00:38:26,160
could use.

635
00:38:26,160 --> 00:38:28,520
And not the new version where you press tab and they're just all going to place the one

636
00:38:28,520 --> 00:38:33,440
where I just suggest, sort of, yeah, responses you can just click on.

637
00:38:33,440 --> 00:38:37,920
That model is a relatively complicated model and because of that, it fails on a non-zero

638
00:38:37,920 --> 00:38:39,120
number of emails.

639
00:38:39,120 --> 00:38:43,320
And so before running that model, they have what's called a filtering model, which is a

640
00:38:43,320 --> 00:38:47,600
much simpler model, which the goal of that model is to say, like, should we run our model

641
00:38:47,600 --> 00:38:48,600
or not?

642
00:38:48,600 --> 00:38:50,480
And so I cover like tricks like these tricks, right?

643
00:38:50,480 --> 00:38:55,320
For example, like, can you, when should you decide that it's worthwhile to build a first

644
00:38:55,320 --> 00:38:59,960
model that's like a much simpler one that will save you compute time and save you from

645
00:38:59,960 --> 00:39:04,240
showing ridiculous results to your users if, if like, this input is not suited for your

646
00:39:04,240 --> 00:39:05,960
complex model?

647
00:39:05,960 --> 00:39:11,760
And then the last chapter goes into sort of like CICD and monitoring for ML and like what

648
00:39:11,760 --> 00:39:14,840
you can look at when you're like, maybe testing or when you're putting a new model in production

649
00:39:14,840 --> 00:39:18,720
of a lot of companies that started doing what's called like shadow deployments where like

650
00:39:18,720 --> 00:39:22,720
you put a model into shadow, which means that it's just like a real model except that

651
00:39:22,720 --> 00:39:26,400
you don't actually use what it produces, but it's sort of like a final way to test it.

652
00:39:26,400 --> 00:39:27,400
Yeah.

653
00:39:27,400 --> 00:39:28,400
Nice.

654
00:39:28,400 --> 00:39:36,480
So, you know, I referred back to the Twoma kind of platform's conference and some of

655
00:39:36,480 --> 00:39:44,040
the writing that I've been doing on AI, ML platforms and kind of open source and commercial

656
00:39:44,040 --> 00:39:48,600
products that like enable you to build out these workflows and kind of manage these workflows

657
00:39:48,600 --> 00:39:49,600
for you.

658
00:39:49,600 --> 00:39:56,040
I'm assuming that you're not like building all of this in one of those environments.

659
00:39:56,040 --> 00:40:00,280
How are you like stringing together the pieces that you're doing?

660
00:40:00,280 --> 00:40:03,600
Is it all kind of standard vanilla Python?

661
00:40:03,600 --> 00:40:06,240
Are you building on vanilla Python?

662
00:40:06,240 --> 00:40:13,360
Are you using any particular kind of approach to make this modular or are you worried about

663
00:40:13,360 --> 00:40:14,360
that?

664
00:40:14,360 --> 00:40:18,600
You know, how are you pulling this all these workflow components together?

665
00:40:18,600 --> 00:40:23,080
I mean, there is a world where, you know, in like a few years, something comes out.

666
00:40:23,080 --> 00:40:27,360
It's like the TensorFlow, but just for all of machine learning, maybe it'll be TensorFlow.

667
00:40:27,360 --> 00:40:28,360
And this is all obsolete.

668
00:40:28,360 --> 00:40:31,080
Or if you're just like, well, you could just, you know, like write one line of this to

669
00:40:31,080 --> 00:40:33,760
serve new framework.

670
00:40:33,760 --> 00:40:35,520
And I'm willing to take that risk.

671
00:40:35,520 --> 00:40:41,120
In the current state, I feel like while there's many useful tools, it wouldn't necessarily

672
00:40:41,120 --> 00:40:43,880
be what readers are looking for.

673
00:40:43,880 --> 00:40:47,880
Like they're not necessarily looking to learn how to use Scoopflow or, you know, how to

674
00:40:47,880 --> 00:40:53,920
use Airflow for service scheduling DAGs. So I kept things pretty lightweight where most

675
00:40:53,920 --> 00:40:55,720
of it is in raw Python.

676
00:40:55,720 --> 00:40:59,760
You know, there's simple unit testing, Jupyter notebooks to illustrate concepts.

677
00:40:59,760 --> 00:41:04,160
And then for the serving side, built a simple flask app with some examples of like how

678
00:41:04,160 --> 00:41:06,560
you cache requests.

679
00:41:06,560 --> 00:41:10,000
And that I think serves the purposes of the book.

680
00:41:10,000 --> 00:41:14,280
Well, whenever possible, I've added sort of links to resources where it's like, well,

681
00:41:14,280 --> 00:41:18,960
if you want to know more about how you, for example, build models on device, like you might

682
00:41:18,960 --> 00:41:21,520
want to check out this resource.

683
00:41:21,520 --> 00:41:25,560
But for the actual code examples, again, I wanted to, because the book already has the

684
00:41:25,560 --> 00:41:29,600
tall order of covering all the machine learning, I felt like I went a bit pretty focused.

685
00:41:29,600 --> 00:41:31,400
Cool.

686
00:41:31,400 --> 00:41:37,680
So once should we expect the, you know, volume two second edition, you're next, your

687
00:41:37,680 --> 00:41:42,760
next book after a long vacation.

688
00:41:42,760 --> 00:41:47,000
Yeah, I mean, depending on how well this book goes, it was really, it's actually a really

689
00:41:47,000 --> 00:41:49,240
enjoyable process to write it with O'Reilly.

690
00:41:49,240 --> 00:41:52,480
The process of writing a book, you know, I would recommend to no one that's horrible.

691
00:41:52,480 --> 00:41:56,520
But O'Reilly made it as unhorrible as possible.

692
00:41:56,520 --> 00:41:57,520
So nice.

693
00:41:57,520 --> 00:41:58,520
Awesome.

694
00:41:58,520 --> 00:42:02,160
Well, Emmanuel is great catching up with you.

695
00:42:02,160 --> 00:42:05,640
Congratulations on getting this book published.

696
00:42:05,640 --> 00:42:09,160
We've been chatting about it for a bit, at least conceptually.

697
00:42:09,160 --> 00:42:14,600
You know, that it was something that you're laboring under and I'm super excited to, you

698
00:42:14,600 --> 00:42:18,720
know, see it, have an opportunity to hold it in my hand and chat with you about it on

699
00:42:18,720 --> 00:42:19,720
the show.

700
00:42:19,720 --> 00:42:20,720
Awesome.

701
00:42:20,720 --> 00:42:21,720
Thank you, Sam.

702
00:42:21,720 --> 00:42:22,720
All right.

703
00:42:22,720 --> 00:42:23,720
Thank you.

704
00:42:23,720 --> 00:42:24,720
All right, everyone.

705
00:42:24,720 --> 00:42:26,720
That's our show for today.

706
00:42:26,720 --> 00:42:33,520
To learn more about Emmanuel or his new book, visit twomalai.com slash talk slash 349.

707
00:42:33,520 --> 00:42:38,920
For more information on the AI Enterprise workflow study group, visit twomalai.com slash

708
00:42:38,920 --> 00:42:39,920
AIEW.

709
00:42:39,920 --> 00:42:46,120
Of course, if you like what you hear on the podcast, we would be very grateful if you

710
00:42:46,120 --> 00:42:50,680
subscribe, rate, and review the show on your favorite pod catcher.

711
00:42:50,680 --> 00:42:51,680
All right.

712
00:42:51,680 --> 00:43:18,440
Thanks so much for listening and catch you next time.

