WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.120
I'm your host Sam Charrington.

00:32.120 --> 00:38.000
Today we are joined by Andrew Trask, PhD student at the University of Oxford and leader of

00:38.000 --> 00:40.320
the Open Mind project.

00:40.320 --> 00:45.440
Open Mind is an open source community focused on researching, developing and promoting tools

00:45.440 --> 00:51.840
for secure, privacy preserving, value aligned, artificial intelligence.

00:51.840 --> 00:56.760
Andrew and I caught up back at Noreps to dig into why Open Mind is important and to explore

00:56.760 --> 01:03.160
some of the basic research and technologies supporting private decentralized data science.

01:03.160 --> 01:09.480
We touch on ideas such as differential privacy, secure, multi-party computation, and how

01:09.480 --> 01:14.360
these come into play and for example federated learning.

01:14.360 --> 01:19.240
Before we jump in, I'd like to join Pegasystems this episode's sponsor and inviting you to

01:19.240 --> 01:25.680
meet me at the MGM Grand in Las Vegas from June 2nd to 5th at Pegaworld, the company's

01:25.680 --> 01:29.040
annual digital transformation conference.

01:29.040 --> 01:34.240
Pegasystems puts AI at the center of its customer engagement software so that every customer

01:34.240 --> 01:38.720
touchpoint on every channel is optimized in real time.

01:38.720 --> 01:43.800
So the customers find each interaction relevant and timely, whether a sales call, a marketing

01:43.800 --> 01:46.520
campaign, or a customer service chat.

01:46.520 --> 01:51.760
At Pegaworld, you'll hear great stories of AI applied to the customer experience at

01:51.760 --> 01:54.360
real Pegacustomers.

01:54.360 --> 01:59.600
The event is a great way to learn from a who's who of the Fortune 500, and of course, I'll

01:59.600 --> 02:02.160
be there speaking as well.

02:02.160 --> 02:08.800
To register, visit Pegaworld.com and use the promo code TWIMIL-19 when you sign up for

02:08.800 --> 02:10.120
$200 off.

02:10.120 --> 02:13.600
Again, that's TWIMIL-19, it's as easy as that.

02:13.600 --> 02:17.120
Hope to see you there, and now on to the show.

02:17.120 --> 02:20.840
All right, everyone.

02:20.840 --> 02:25.600
I am here in Montreal at the NURPS conference, and I am with Andrew Trask.

02:25.600 --> 02:32.000
Andrew is a PhD student at Oxford, as well as leader of the Open Source Open Mind project.

02:32.000 --> 02:34.600
Andrew, welcome to this week in Machine Learning and AI.

02:34.600 --> 02:35.600
Awesome.

02:35.600 --> 02:36.600
Thank you.

02:36.600 --> 02:37.600
Great to be here.

02:37.600 --> 02:38.600
Awesome.

02:38.600 --> 02:39.920
Why don't we get started by having you tell us a little bit about your background?

02:39.920 --> 02:46.120
How did you get involved in machine learning and privacy in the intersection of those two?

02:46.120 --> 02:51.680
Yeah, so I went to undergrad at a liberal arts college in Tennessee called Belmont University.

02:51.680 --> 02:58.960
And while I was there, I was originally a music student, and I stumbled into a CS course

02:58.960 --> 03:04.000
dabbled around, and there was an AI course, and that got me really into machine learning.

03:04.000 --> 03:09.080
So as soon as I, there's just really great professor named Dr. Hooper, as many great

03:09.080 --> 03:10.080
stories start.

03:10.080 --> 03:14.720
I really compelling professor, teaching really exciting concepts, and I trained my first

03:14.720 --> 03:19.080
neural nets, and got them to converge, and got a paper into undergraduate conference.

03:19.080 --> 03:23.880
And at that point, I was pretty committed on the machine learning front.

03:23.880 --> 03:31.200
After that, I joined a local company called Digital Reasoning that does deep learning and

03:31.200 --> 03:34.080
AI at scale on private data sets.

03:34.080 --> 03:35.080
Very natural, actually, right?

03:35.080 --> 03:36.080
Yeah, they're natural.

03:36.080 --> 03:38.080
Yeah, so Belmont is also a natural.

03:38.080 --> 03:39.080
Okay.

03:39.080 --> 03:40.080
Got it.

03:40.080 --> 03:41.080
Yeah.

03:41.080 --> 03:45.080
And that's when I really got exposed to the power of doing machine learning on data that

03:45.080 --> 03:46.080
you don't have access to.

03:46.080 --> 03:52.080
So I mean, they work with a lot of government agencies, like the intelligence coming across

03:52.080 --> 03:53.080
them.

03:53.080 --> 03:54.080
Oh, that's awesome.

03:54.080 --> 03:56.080
I'll throw out that story.

03:56.080 --> 04:00.000
I got the dabble in research as well as product management there, and really just came to

04:00.000 --> 04:04.600
appreciate how challenging it is to do machine learning and deep learning on unseen data.

04:04.600 --> 04:10.800
So when I left to do a PhD, I had already a very good appreciation for that, and also

04:10.800 --> 04:15.640
some sort of high-level understanding of the challenges of that environment.

04:15.640 --> 04:22.440
And then, during my first year at Oxford, the first year of PhD is often very exploratory.

04:22.440 --> 04:27.000
And one of the first things that came across was homeomorphic encryption, which just to

04:27.000 --> 04:28.520
me was absolutely magical.

04:28.520 --> 04:31.280
I couldn't believe that you could do computation on encrypted data.

04:31.280 --> 04:36.360
And I wrote a blog post, sort of just trying to jerry rig some homeomorphic encryption

04:36.360 --> 04:39.400
code that I found with training and neural net.

04:39.400 --> 04:43.320
And I got a tremendous amount of positive feedback from it, ended up chatting with some

04:43.320 --> 04:45.520
folks at the Future Humanity Institute in Oxford.

04:45.520 --> 04:50.120
So I don't know if you know Nick Bostrom and kind of the safety work that they do.

04:50.120 --> 04:54.000
And it just became really clear that there was a really exciting research opportunity here

04:54.000 --> 04:59.120
that one was underserved, so there weren't many people working on it, and secondarily,

04:59.120 --> 05:04.000
and perhaps more importantly, that there was good reason to believe we can make a lot

05:04.000 --> 05:08.520
of progress in a relatively short period of time on a topic with pretty significant

05:08.520 --> 05:09.520
social gains.

05:09.520 --> 05:12.080
And so that at the beginning of PhD is a no-brainer.

05:12.080 --> 05:15.360
Like you actually want to jump on those kind of opportunities.

05:15.360 --> 05:18.640
And since then, what I've discovered is that tons of great workers going on in sort

05:18.640 --> 05:23.000
of the field of cryptography, tons of great workers going on in the field of statistics,

05:23.000 --> 05:24.920
and of course in machine learning and deep learning.

05:24.920 --> 05:29.960
But these three communities, they don't necessarily talk as much as they should.

05:29.960 --> 05:34.760
And so by simply trying to become as familiar as fast as possible with sort of the core

05:34.760 --> 05:40.360
primitives of these three different fields, and by just trying to combine them in ways

05:40.360 --> 05:44.760
that solve important use cases, I think we've already been able to make quite a bit of progress

05:44.760 --> 05:47.000
and expect to be so in the future.

05:47.000 --> 05:48.480
So that's kind of where I'm at.

05:48.480 --> 05:49.480
Nice, nice.

05:49.480 --> 05:56.240
Here, some kind of core concepts, maybe from the cryptography side, that it makes sense

05:56.240 --> 06:02.200
to kind of talk about, like mention homomorphic encryption that are fundamental to all the

06:02.200 --> 06:07.000
rest of the stuff you're doing, or it's better to go top down.

06:07.000 --> 06:12.160
Yeah, so I mean, the three core concepts that we deal with, two of which are from cryptography,

06:12.160 --> 06:15.200
are differential privacy and secure multi-party computation.

06:15.200 --> 06:19.520
So and then there's federated learning from the machine learning side.

06:19.520 --> 06:23.640
So those three things kind of make up what I think is the future of privacy-reserving

06:23.640 --> 06:25.400
machine learning.

06:25.400 --> 06:30.680
So differential privacy is concerned with, I suppose your viewers already know this,

06:30.680 --> 06:35.200
in the context of machine learning, differential privacy is focused on allowing models to sort

06:35.200 --> 06:39.720
of learn from a training data set without them accidentally learning information we don't

06:39.720 --> 06:40.720
want them to learn.

06:40.720 --> 06:42.720
Like, you know, private training data itself, for example.

06:42.720 --> 06:43.720
Yeah, yeah, exactly.

06:43.720 --> 06:46.600
Or it's any specific features about a specific person, right?

06:46.600 --> 06:52.000
So it might, a good example might be, you want a model to learn to identify cancer in

06:52.000 --> 06:59.760
radiology scans without it memorizing which individual person has cancer, right?

06:59.760 --> 07:02.120
And so yeah, differential privacy is super important.

07:02.120 --> 07:05.120
That one is already, there's already good community form between that and the machine

07:05.120 --> 07:06.120
learning community.

07:06.120 --> 07:08.840
It's really exciting to see that develop.

07:08.840 --> 07:13.680
Homomorphic encryption and secure multi-party computation are a little less known, in particular

07:13.680 --> 07:19.040
secure multi-party, I'll just say secure NPC from now on, for short, because it's such

07:19.040 --> 07:21.040
a wordy thing.

07:21.040 --> 07:24.400
Homomorphic encryption, I think, is more intuitive for people because it's sort of a, it's

07:24.400 --> 07:27.080
typically laid out in the form of a standard encryption.

07:27.080 --> 07:31.280
So you have a public key and a private key, and you'll use the private key to encrypt

07:31.280 --> 07:32.280
something, right?

07:32.280 --> 07:35.680
So I can encrypt the number five, and then I can give it to you.

07:35.680 --> 07:39.280
And while it's in its encrypted state, you know, while you have it, and I can't see it,

07:39.280 --> 07:41.520
you can do arithmetic with that number, right?

07:41.520 --> 07:43.920
And you could do computation with that number.

07:43.920 --> 07:47.360
And then you can give the result back to me, still encrypted, and I can decrypt the result.

07:47.360 --> 07:51.280
And so that's interesting from machine learning standpoint, because I can take, say, all

07:51.280 --> 07:54.960
the parameters of my model, so a model, you know, a neural net, for example, is just a big

07:54.960 --> 07:56.120
collection of numbers.

07:56.120 --> 07:57.720
I can encrypt all of them.

07:57.720 --> 08:01.600
I can give them to you, and then you can do prediction or training, or anything else

08:01.600 --> 08:04.320
that you'd like to do with a machine learning model.

08:04.320 --> 08:08.360
And then, so if you do training, for example, you can give me back the resulting model.

08:08.360 --> 08:12.760
And you were able to, it's sort of a one-way information flow, where you're able to make

08:12.760 --> 08:14.760
the model smarter, but you're not actually able to use it.

08:14.760 --> 08:19.040
So there's lots of interesting commercial use cases that protect privacy in that context.

08:19.040 --> 08:20.680
The one, and some people have heard about this.

08:20.680 --> 08:24.080
So homework encryption is sort of mildly well-known.

08:24.080 --> 08:26.840
But the problem is, is that it's very, very slow.

08:26.840 --> 08:32.280
So I mean, it's several, meaning more than five orders of magnitude slower than typical,

08:32.280 --> 08:35.280
what we call plain text computation, what we do normally.

08:35.280 --> 08:38.600
And this is where NPC really starts to become interesting.

08:38.600 --> 08:43.680
So there's, security NPC is a slightly different setup.

08:43.680 --> 08:47.960
Instead of trying to encrypt a number, we actually split it into what's called shares.

08:47.960 --> 08:51.600
So let's say we want to encrypt the number five.

08:51.600 --> 08:55.280
I can take the number five, I can split it into two shares, we'll say a two and a three,

08:55.280 --> 08:56.280
right?

08:56.280 --> 08:58.320
And I can give you the three, and I can hang on to the two.

08:58.320 --> 09:01.360
So the three is your share, the two is my share.

09:01.360 --> 09:05.720
Now we could restore this number five by just adding these two shares together, if that's

09:05.720 --> 09:06.720
an instance.

09:06.720 --> 09:11.600
But the interesting thing about security NPC is that while it's in this state, neither of

09:11.600 --> 09:14.920
us know what the number is, it's hidden between us, right?

09:14.920 --> 09:18.520
And you may have no idea that this was a five.

09:18.520 --> 09:20.440
But we can compute over it.

09:20.440 --> 09:25.240
So let's say we want it to multiply at times two, like if you multiply your share at times

09:25.240 --> 09:29.160
two, I multiply my share at times two, then this hidden value that's between us was

09:29.160 --> 09:31.160
also multiplied by two.

09:31.160 --> 09:33.520
And this is better than homeomorphic encryption for two reasons.

09:33.520 --> 09:35.640
One, it's much faster, typically.

09:35.640 --> 09:40.320
So there's a recent paper show and you could train a deep cognet that's only 20 times slower

09:40.320 --> 09:43.760
than the normal, which I know that sounds mildly annoying to anyone who's training deep

09:43.760 --> 09:48.680
known that's, but it's totally different than like 10,000 or 100,000 times slower, right?

09:48.680 --> 09:57.400
And is it faster only in aggregate or for each individual participant?

09:57.400 --> 10:02.880
So it is, in this specific setup, it's two or three party computations.

10:02.880 --> 10:09.400
So we've split this deep neural net, all the parameters in it into shares that three

10:09.400 --> 10:11.240
parties own.

10:11.240 --> 10:13.200
And in that case, it's faster in aggregate.

10:13.200 --> 10:17.280
So we typically try to measure it in the context of a use case because different functions.

10:17.280 --> 10:18.680
So you imagine this is a protocol, right?

10:18.680 --> 10:24.240
And I described how to multiply this hidden value five by two, but addition is different

10:24.240 --> 10:27.680
from multiplication, which is different from comparison operators, which is different

10:27.680 --> 10:31.920
than how you bootstrap sine and ray-lew and like, sigmoid and all these kinds of things.

10:31.920 --> 10:37.000
So we typically try to look at the end use case and just sort of give a high level empirical

10:37.000 --> 10:39.560
results for what the slowdown is.

10:39.560 --> 10:43.480
But the interesting thing is, one more on that, if it's based on homeomorphic encryption,

10:43.480 --> 10:45.720
how is it end up being faster than homeomorphic encryption?

10:45.720 --> 10:50.560
Oh, so it's not always based on homeomorphic encryption.

10:50.560 --> 10:51.960
Sometimes there's a homeomorphic encryption component.

10:51.960 --> 10:54.360
They kind of steal ideas from each other.

10:54.360 --> 10:58.280
But the cool thing is that this is not a form of encryption.

10:58.280 --> 11:02.560
It actually, it's a protocol for sharing.

11:02.560 --> 11:05.640
So encryption is not necessarily involved.

11:05.640 --> 11:06.640
It's more...

11:06.640 --> 11:07.640
Yeah.

11:07.640 --> 11:11.600
So it's funny because it has an API that's very similar to encryption to the extent that

11:11.600 --> 11:16.200
I can take a value and I can make it so that a group of people can't see it, right?

11:16.200 --> 11:17.680
But we can still compute over it.

11:17.680 --> 11:21.600
So to that extent, it can do everything that homeomorphic encryption can do.

11:21.600 --> 11:24.000
But it has two specific properties that are different.

11:24.000 --> 11:25.000
Well, so three.

11:25.000 --> 11:30.360
One, it's a lot faster because you basically are exchanging heavy compute, typically,

11:30.360 --> 11:33.600
which is homeomorphic encryption for a more network overhead.

11:33.600 --> 11:36.680
So whereas with homeomorphic encryption, I could just send you an encrypted asset and

11:36.680 --> 11:38.520
you can do things with it.

11:38.520 --> 11:40.880
With NPC, we have to be online the whole time.

11:40.880 --> 11:45.800
It's a protocol for how we can exchange messages in a way to compute functions on hidden

11:45.800 --> 11:47.680
data.

11:47.680 --> 11:49.920
And there's two properties that we really like.

11:49.920 --> 11:51.720
One, it gets this privacy property.

11:51.720 --> 11:55.160
But secondarily, and this is different from homeomorphic encryption, everyone has a private

11:55.160 --> 11:56.480
key.

11:56.480 --> 11:57.480
And this creates...

11:57.480 --> 12:02.000
It's a lot more like shared ownership of a data structure instead of just encryption.

12:02.000 --> 12:05.600
So in this particular case, if I take a neural net, I take all of its parameters and I

12:05.600 --> 12:09.640
split it, split each parameter into four shares, then I have four governors.

12:09.640 --> 12:11.960
I have four owners for this model.

12:11.960 --> 12:17.240
And any one of those four people could say, oh, I'm not willing to perform computation

12:17.240 --> 12:19.760
with you guys on this data point.

12:19.760 --> 12:24.280
Because I haven't been paid or because I don't believe in it or for whatever reason, right?

12:24.280 --> 12:28.600
And that's fundamentally different from homeomorphic encryption, which kind of defaults to a single

12:28.600 --> 12:30.480
owner who has a single private key.

12:30.480 --> 12:31.480
Okay.

12:31.480 --> 12:35.200
So yeah, we really like that sort of distributed governance apparatus.

12:35.200 --> 12:37.200
Okay.

12:37.200 --> 12:39.200
And so what does OpenMind come in?

12:39.200 --> 12:40.200
Yeah.

12:40.200 --> 12:44.640
So OpenMind is basically about lowering the barrier to entry to being able to use these

12:44.640 --> 12:45.640
tools.

12:45.640 --> 12:51.440
As I described a minute ago, these communities have kind of lived in separate worlds.

12:51.440 --> 12:56.520
So the crypto community has their own toolkits, typically in C++, the machine learning community

12:56.520 --> 13:00.560
has their own toolkits that's like, you know, PyTorch and TensorFlow.

13:00.560 --> 13:03.080
And statistics has their own, it's like R and all these kinds of things.

13:03.080 --> 13:08.920
So the problem is that if I want to do deep learning with security and PC, I either

13:08.920 --> 13:14.840
have to take an NPC framework and write all my autograd and my layer types and all

13:14.840 --> 13:19.400
stuff from scratch, or I have to go to a deep learning framework and know all the cryptography

13:19.400 --> 13:20.400
to be able to add.

13:20.400 --> 13:28.320
And as it turns out, this is prohibitively difficult and nearly most papers that I come across,

13:28.320 --> 13:32.480
people write some big amorphous C++ blog from scratch.

13:32.480 --> 13:35.480
And then it becomes difficult to repurpose, difficult for industry to pick up.

13:35.480 --> 13:40.520
So what OpenMind is all about is about installing these cryptography primitives inside of the

13:40.520 --> 13:41.840
major deep learning toolkits.

13:41.840 --> 13:44.120
So PyTorch and TensorFlow being the first two.

13:44.120 --> 13:48.960
So that people in the machine learning community don't have to really know anything about cryptography

13:48.960 --> 13:49.960
to be able to use them.

13:49.960 --> 13:54.360
Or if you're able to do research on them to be able to study these different protocols.

13:54.360 --> 13:59.880
So in particular, in one example, if that is in PyTorch, we have this new tensor type

13:59.880 --> 14:02.000
that's an NPC shared tensor.

14:02.000 --> 14:06.160
So it feels like a normal tensor has the same API, but every time you add two tensors

14:06.160 --> 14:10.640
together, under the hood it actually sends messages to multiple different machines and

14:10.640 --> 14:11.960
does the protocol correctly.

14:11.960 --> 14:15.960
This is the multi-party aspect of it.

14:15.960 --> 14:18.160
Is it like widely distributed?

14:18.160 --> 14:23.960
Like you might think of the parties representing actual people that are in different corners

14:23.960 --> 14:30.920
of the earth, or is it are the parties typically like computational processes that might be local

14:30.920 --> 14:34.040
to an environment and under the same control?

14:34.040 --> 14:37.000
Like how does it typically, what's the typical topology?

14:37.000 --> 14:38.520
Great question.

14:38.520 --> 14:43.320
So for the purpose of development, we've got nice sort of what we call virtual workers

14:43.320 --> 14:47.280
that will simulate it on one machine so that you can build protocols and that kind of thing.

14:47.280 --> 14:55.280
But in the context of a more real world use case, there's different philosophies on how

14:55.280 --> 14:56.280
it will actually roll out.

14:56.280 --> 15:00.840
So the thing to know is that the more people you have governing a data point, the more

15:00.840 --> 15:04.600
people have to touch it for it to be decrypted or for it to be used in computation.

15:04.600 --> 15:09.000
But meaning the more owners it has, the slower the computation is going to go.

15:09.000 --> 15:14.840
So our vision is that most of privacy-preserving machine learning will operate with two owners

15:14.840 --> 15:18.440
at a time because that's sort of the fastest version of it.

15:18.440 --> 15:23.360
And this is where, as I mentioned, I may have mentioned this before, we really see a

15:23.360 --> 15:28.120
trifecta of like three core technologies coming together, secure multi-party computation

15:28.120 --> 15:31.520
being one of them, and then federated learning and differential privacy being the other.

15:31.520 --> 15:36.840
So the nice thing about federated learning, as it's a protocol that allows me as a model

15:36.840 --> 15:42.120
owner to train a model on a highly distributed data set by performing training with one

15:42.120 --> 15:43.360
person at a time.

15:43.360 --> 15:48.720
So I'll send a model to someone and it will train there for a while.

15:48.720 --> 15:51.960
I think I could do this with 10 people in parallel.

15:51.960 --> 15:56.640
But the nice thing is that I should probably come back to that.

15:56.640 --> 15:59.880
So the short answer is I see it happening two people at a time.

15:59.880 --> 16:04.400
And when you combine it with these other technologies, that actually works out to be quite

16:04.400 --> 16:05.400
performed.

16:05.400 --> 16:07.400
When do you want to go back to the federated?

16:07.400 --> 16:08.400
Oh, yeah.

16:08.400 --> 16:10.640
It's like an interesting topic.

16:10.640 --> 16:15.720
How well developed is that in the literature, the models for federated machine learning.

16:15.720 --> 16:20.200
I've seen a lot of work on distributed training.

16:20.200 --> 16:27.000
And then more recently in the context of like IoT and like edge devices, federated training

16:27.000 --> 16:32.080
in that sense, where you are trying to do some amount of learning kind of at the edge,

16:32.080 --> 16:37.240
but also share the model back to some centralized thing so that it could incorporate learning

16:37.240 --> 16:43.120
from other edge devices, but also so that it becomes like the master model.

16:43.120 --> 16:47.920
And then you ship out updates back out to the other edge devices.

16:47.920 --> 16:52.840
I don't know if the same kinds of techniques apply or what the overlap might be there.

16:52.840 --> 16:53.840
They absolutely do.

16:53.840 --> 16:57.440
If federated learning is out of these three things, like federated learning, multi-party

16:57.440 --> 17:01.200
computation and differential privacy, federated learning is the most mature.

17:01.200 --> 17:05.920
I mean, to that extent, so if you have a smartphone and you open it up and you go to

17:05.920 --> 17:10.760
text someone, you know, try to recommend the next word for your text, that model for

17:10.760 --> 17:15.360
both, to my knowledge, to both Apple and Google is trained using federated learning.

17:15.360 --> 17:19.480
So to that extent, federated learning is already deployed on, you know, billions of devices

17:19.480 --> 17:21.480
around the planet.

17:21.480 --> 17:24.520
So to that extent, it's quite robust.

17:24.520 --> 17:27.520
And the nice thing about federated learning, so I guess for anyone listening that doesn't

17:27.520 --> 17:31.440
know what federated learning is, typically when you're training a machine learning model,

17:31.440 --> 17:37.080
you would aggregate all the data to sort of one central server, then you'd initialize

17:37.080 --> 17:40.280
a random model and then you'd train it on that data, right?

17:40.280 --> 17:43.880
So federated learning kind of flips the script and says instead of bringing all the data

17:43.880 --> 17:47.680
to the model in one location, you're going to bring the model out to the data wherever

17:47.680 --> 17:48.680
it already lives.

17:48.680 --> 17:51.840
And the general goal is that whoever owns the data doesn't want to have to send it to

17:51.840 --> 17:52.840
someone.

17:52.840 --> 17:55.440
They want to own and maintain the only copy.

17:55.440 --> 17:58.240
So in this particular case, this means that, you know, if I've got a billion phones,

17:58.240 --> 18:03.000
I as a central server, we'll then send the model with some instructions on how to train

18:03.000 --> 18:06.400
it down to individual data owners.

18:06.400 --> 18:10.240
And this is where the nice thing about that is it does two things.

18:10.240 --> 18:11.880
One, it protects the privacy of the data.

18:11.880 --> 18:15.480
And two, it typically actually results in less communication than sending the data to

18:15.480 --> 18:16.480
the cloud.

18:16.480 --> 18:22.760
Look at the original paper on federated learning from Brendan McMahon.

18:22.760 --> 18:27.040
They, I think they decided 10 to 100x less network overhead and sending the data sets to

18:27.040 --> 18:28.040
the cloud.

18:28.040 --> 18:31.240
I mean, it depends on how big the data set is, but often it can actually be less network

18:31.240 --> 18:32.760
intensive.

18:32.760 --> 18:36.760
And so as you can imagine, this is where MPC comes in.

18:36.760 --> 18:40.520
So the big, the nice thing here is you protect the privacy of the data, but the problem is

18:40.520 --> 18:44.920
is that you send a plain text copy of the model to potentially thousands or millions of

18:44.920 --> 18:45.920
people, right?

18:45.920 --> 18:49.880
And if this is your $10 million healthcare model, well, then you'll be, you're sort of at

18:49.880 --> 18:55.440
risk of any one of these millions of people stealing it and trying to monetize it for themselves.

18:55.440 --> 18:57.920
This is where secure MPC becomes interesting.

18:57.920 --> 19:02.080
So instead of sending the model, you would MPC share the model, and they would MPC share

19:02.080 --> 19:06.840
their data set, and I would perform sort of encrypted training with each individual person

19:06.840 --> 19:11.840
in parallel, and then only aggregate their gradients sort of at the end.

19:11.840 --> 19:15.840
This is why federated learning allows you to do MPC sort of two people at a time, which

19:15.840 --> 19:17.160
is quite a more optimal.

19:17.160 --> 19:25.920
What's the kind of user experience of OpenMind is that like system level packets that they're

19:25.920 --> 19:28.680
using, or is it more framework level?

19:28.680 --> 19:32.440
Is the typical user a developer or a data scientist?

19:32.440 --> 19:35.120
How do you kind of look at the world from that perspective?

19:35.120 --> 19:36.120
Yeah.

19:36.120 --> 19:39.600
So speaking from today, it's mostly the framework level.

19:39.600 --> 19:45.800
So our target audience today is primarily sort of machine running researchers.

19:45.800 --> 19:51.720
Basically a scientist, perhaps working at enterprise wanting to do a pilot in privacy

19:51.720 --> 19:53.120
preserving machine learning.

19:53.120 --> 19:58.360
So the vast majority of the code we've written is extending PyTorch Intensive Flow.

19:58.360 --> 20:02.600
So if you're not a PyTorch Intensive Flow user, you don't have any, then we're not quite

20:02.600 --> 20:03.880
ready for you yet.

20:03.880 --> 20:08.840
I think in the future, this will likely get packaged more into sort of, here's your federated

20:08.840 --> 20:09.840
learning system.

20:09.840 --> 20:13.120
Here's how you spin up the server, and this is where all your data adapters will attach

20:13.120 --> 20:16.600
to that kind of thing, but we still have quite a bit of code to write until we get there.

20:16.600 --> 20:23.760
So is it extending your PyTorch Intensive Flow in an analogous way to the way different

20:23.760 --> 20:27.240
distributed training methods are working?

20:27.240 --> 20:28.240
Yeah.

20:28.240 --> 20:36.000
So for PyTorch, for example, the first thing we built was a remote execution protocol.

20:36.000 --> 20:42.120
So for anyone who knows PyTorch that's listening, it allows you to take a tensor, which is

20:42.120 --> 20:46.520
a tensor as a simple list of numbers or nested list of numbers.

20:46.520 --> 20:50.840
So if we have a texture called like x equals, you know, some tensor, maybe 5, 0 or something

20:50.840 --> 20:56.760
like that, I can go x.send and send it to some machine and put an address for that machine.

20:56.760 --> 21:00.000
And then what gets returned to me is a pointer to that tensor.

21:00.000 --> 21:04.520
And then whatever that pointer has the exact same API as a tensor normally would.

21:04.520 --> 21:06.480
So it feels like a normal tensor.

21:06.480 --> 21:10.160
But the nice thing is that then we can use this underlying primitive to coordinate remote

21:10.160 --> 21:14.400
executions and coordinate higher level protocols like security PC.

21:14.400 --> 21:18.760
So most of it's built on top of that, kind of a remote execution paradigm.

21:18.760 --> 21:19.760
But then the API.

21:19.760 --> 21:24.240
And that was standard already existing in both PyTorch Intensive Flow, something analogous

21:24.240 --> 21:25.240
to it.

21:25.240 --> 21:28.680
So PyTorch Intensive Flow have not had pointers in the past, so that's a new thing for

21:28.680 --> 21:29.680
us.

21:29.680 --> 21:30.680
Okay.

21:30.680 --> 21:35.880
But the thing that we try to keep the same is we want that pointer to have basically have

21:35.880 --> 21:38.920
the exact same API as if the data was on your local machine, right?

21:38.920 --> 21:39.920
Okay.

21:39.920 --> 21:44.240
So we want to make it so that you don't have to relearn a new framework or really any

21:44.240 --> 21:48.320
other paradigms to be able to use things like federated learning or NPC.

21:48.320 --> 21:52.360
And that's why our NPC tensor feels like it's a normal tensor.

21:52.360 --> 21:55.960
It feels like it's on your device, has the same API and protocol and you can do the same

21:55.960 --> 21:56.960
things.

21:56.960 --> 22:02.200
You can call back propagate on a lot of remote tensors.

22:02.200 --> 22:07.400
But it's under the hood, it's actually doing all the protocol for you, such that you

22:07.400 --> 22:09.320
don't have to learn that protocol yourself.

22:09.320 --> 22:10.320
Yeah.

22:10.320 --> 22:16.800
And you're using this API or to get the pointers, you're specifying the number of parties

22:16.800 --> 22:19.960
and IP addresses or something like that, or?

22:19.960 --> 22:20.960
Yeah.

22:20.960 --> 22:25.440
So we do have the ability to spin up servers, which we call workers, that are connected via

22:25.440 --> 22:28.960
you know, sockets or web sockets.

22:28.960 --> 22:31.960
And what you'll get is actually a pointer to that worker on the client side.

22:31.960 --> 22:36.080
So we typically name them Bob and Alice canonically, just because that's sort of the crypto literature.

22:36.080 --> 22:37.560
Yeah, exactly.

22:37.560 --> 22:41.800
So we have a big set of tutorials, actually, we just listed this shows how the whole protocol

22:41.800 --> 22:42.800
goes.

22:42.800 --> 22:46.840
So if you go right and get hub slash examples slash, so the PICIF library is the library

22:46.840 --> 22:47.840
I'm referring to.

22:47.840 --> 22:50.720
PICIF slash examples slash tutorials will have that.

22:50.720 --> 22:57.800
But yeah, so if I'm going to, if I'm going to send it, I'll go X, my variable, dot send.

22:57.800 --> 23:01.640
And then in parentheses, I can put in one or more workers that I want to send it to and

23:01.640 --> 23:02.640
return to that.

23:02.640 --> 23:03.640
Okay.

23:03.640 --> 23:07.960
And then I pass in a list of people, a list of pointers to their workers that I want

23:07.960 --> 23:09.240
to NPC share it.

23:09.240 --> 23:10.240
Yeah.

23:10.240 --> 23:11.800
And that's the sort of the encrypted version.

23:11.800 --> 23:17.080
Do you also have kind of flexibility over whether you want to use encryption or just NPC

23:17.080 --> 23:23.200
or whether you want to layer in differential privacy or is the framework making those kinds

23:23.200 --> 23:24.360
of decisions for me?

23:24.360 --> 23:25.360
No.

23:25.360 --> 23:27.520
So the framework is totally agnostic to those decisions.

23:27.520 --> 23:33.600
So obviously we have higher level libraries that try to give you nice defaults.

23:33.600 --> 23:39.960
But the bulk of the framework is this low level protocol for allowing you to either design

23:39.960 --> 23:44.480
your own sort of remote execution strategies or to pull one of the off the shelf ones that

23:44.480 --> 23:49.560
you provide, like federated averaging or different kinds of NPC protocols.

23:49.560 --> 23:51.280
I think we have two different NPC protocols.

23:51.280 --> 23:52.280
Okay.

23:52.280 --> 23:55.920
And what are the distinctions between the different protocols?

23:55.920 --> 24:01.800
So they're APIs primarily, so and so the first one we implemented was called speeds.

24:01.800 --> 24:09.920
So speeds is a protocol that allows you to do, in our case, addition and multiplication.

24:09.920 --> 24:12.800
And then we extended it with this new protocol called Secure and In, which allows you to

24:12.800 --> 24:14.680
do comparison operators.

24:14.680 --> 24:19.440
So to compare two encrypted numbers, as it turns out, we can bootstrap sort of the rest

24:19.440 --> 24:22.280
of the deep learning API just from those three core primitives.

24:22.280 --> 24:27.760
How mature would you say is the, you know, the particular, the project and kind of the

24:27.760 --> 24:33.800
ecosystem around it, like are, are you finding folks extra, is it like, you know, your personal

24:33.800 --> 24:36.800
effort mostly that's kind of pushing along, or are there other folks that are kind of

24:36.800 --> 24:38.560
jumping in and contributing?

24:38.560 --> 24:39.560
How's that been going?

24:39.560 --> 24:41.600
So that part's actually been going really well.

24:41.600 --> 24:46.400
So I think open-mind Slack team at this point has around 3,400 people in it.

24:46.400 --> 24:47.400
Wow.

24:47.400 --> 24:48.400
Yeah.

24:48.400 --> 24:49.400
Oh, wow.

24:49.400 --> 24:53.000
And I think we're up to 170 or 180 people who have contributed code.

24:53.000 --> 24:54.000
Oh.

24:54.000 --> 24:57.640
So to that extent, like, I think a lot of people get that privacy is important at these

24:57.640 --> 25:02.160
tools are a really compelling answer, or they could be in a long run and that by building

25:02.160 --> 25:05.400
better research tools and bringing these two kind of crypto and machine learning community

25:05.400 --> 25:09.640
together, that we can make a lot of progress, and then it's worth their nights and weekends

25:09.640 --> 25:11.920
to work on.

25:11.920 --> 25:19.280
That being said, I would describe PICIF today as kind of an alpha, alpha level project.

25:19.280 --> 25:23.400
So PICIF, again, is the extension deep to PyTorch Intensive Flow.

25:23.400 --> 25:26.840
Open Mind is just kind of the general community name.

25:26.840 --> 25:31.080
So yeah, PICIF is kind of an alpha phase where it's got all the features that we would

25:31.080 --> 25:36.920
like for it to have, but they're not necessarily like a product ready state, and they're also

25:36.920 --> 25:37.920
using an older version.

25:37.920 --> 25:42.560
It was in the particular case of PyTorch, they're using 0.3.1.

25:42.560 --> 25:47.760
So the main push at the moment is to upgrade to PyTorch 1.0, which was recently released,

25:47.760 --> 25:55.280
as well as to really make them robust enough that the institutions can do kind of pilots

25:55.280 --> 25:58.600
and practice use cases and these kinds of things, and we're hoping to release that around

25:58.600 --> 25:59.600
February and March.

25:59.600 --> 26:05.680
Are there any examples of use cases that you can talk about?

26:05.680 --> 26:10.600
So we are in the midst of crafting several early pilots, but I don't think they're quite

26:10.600 --> 26:12.440
public yet, but I'm very excited about them.

26:12.440 --> 26:17.280
We also do have several, I think about a half dozen firms that are in the process of

26:17.280 --> 26:22.680
putting together research grants for people to work on PICIF in Open Mind full-time, which

26:22.680 --> 26:27.920
will greatly help both their internal adoption as well as the health of the library, which

26:27.920 --> 26:29.880
I'm excited about.

26:29.880 --> 26:33.200
And so where do you see this going?

26:33.200 --> 26:38.880
What are some of the next steps for the community and your research in general?

26:38.880 --> 26:44.600
For my personal research, I think I want to sort of stay focused on whatever is needed

26:44.600 --> 26:47.560
for the steps to become mainstream and why they adopted.

26:47.560 --> 26:51.680
At the moment, that's raising awareness and lowering the barrier to entry for tools

26:51.680 --> 26:57.560
that already exist and conducting research to solve the holes that still miss in the

26:57.560 --> 26:58.560
technologies.

26:58.560 --> 27:01.760
There are some sub-problems, especially in differential privacy that have not yet been

27:01.760 --> 27:08.640
solved to the extent that the widest use cases could be implemented today.

27:08.640 --> 27:12.600
So in the next three to six months, I think that it's still mostly a research and open-source

27:12.600 --> 27:14.680
tool push.

27:14.680 --> 27:18.000
In about a six-month to 18-month timeline, I think that we're going to start seeing

27:18.000 --> 27:19.720
considerable enterprise adoption.

27:19.720 --> 27:26.840
So there's already some interesting movement happening in Europe as kind of an echo to GDPR.

27:26.840 --> 27:32.640
There's a handful of startups in the US that I've seen growing up in this space.

27:32.640 --> 27:36.560
And so I think probably the six to 18-month timeline is when enterprises which already

27:36.560 --> 27:39.640
have the data will be interested in moving into this space.

27:39.640 --> 27:44.760
You mentioned some kind of point challenges on the differential privacy side that need

27:44.760 --> 27:49.520
to be solved to better facilitate all of this.

27:49.520 --> 27:53.080
Are there some examples of those that you can share?

27:53.080 --> 27:58.440
So from what I've observed, there's two different kinds of differential privacy right?

27:58.440 --> 28:01.600
There's local and there's global.

28:01.600 --> 28:06.360
Local differential privacy focuses on actually adding noise to each individual row of

28:06.360 --> 28:10.520
data to make sure that each individual row of data is protected before you even do anything

28:10.520 --> 28:11.520
to it.

28:11.520 --> 28:15.560
But as you can imagine, if you have a million data points, that means you're adding a ton

28:15.560 --> 28:19.040
of noise to the data set as a whole.

28:19.040 --> 28:22.800
And you need to have like a lot of people in order to be able to get interesting insights

28:22.800 --> 28:26.320
about it because you've had to add so much noise to each individual data point.

28:26.320 --> 28:31.880
Global differential privacy allows you to compute some function on the data and then only

28:31.880 --> 28:35.920
add noise to the output, which can actually be relatively small amounts depending on what

28:35.920 --> 28:38.640
that function is.

28:38.640 --> 28:44.760
So one thing I have yet to see in deep learning literature is a global differential privacy

28:44.760 --> 28:52.520
technique that really fits the business use case that people want to use it for.

28:52.520 --> 28:56.360
The best one that I know of that comes to mind is the Pate algorithm, which I think if

28:56.360 --> 28:58.920
people know different differential privacy and deep learning is probably the one that

28:58.920 --> 29:00.840
they'd be most familiar with.

29:00.840 --> 29:07.000
This is a really innovative approach wherein you split a data set into 10 different buckets

29:07.000 --> 29:08.000
or in different buckets.

29:08.000 --> 29:12.400
I'll just use 10 as an example and you train a model on each different one that's a private

29:12.400 --> 29:13.400
model.

29:13.400 --> 29:17.800
And then there's this clever trick where you can use these models to annotate a second

29:17.800 --> 29:23.280
data set that is a public data set in a way that all of the labels, you can sort of enforce

29:23.280 --> 29:26.640
differential privacy by the way that you label this second data set using these private

29:26.640 --> 29:32.240
models such that you then train another model on this sort of synthetic, synthetically labeled

29:32.240 --> 29:33.240
data set.

29:33.240 --> 29:35.000
I think that one is Nicholas Paprono.

29:35.000 --> 29:36.000
Yeah, that's right.

29:36.000 --> 29:38.520
We did a podcast interview with him as well.

29:38.520 --> 29:39.520
Oh, excellent.

29:39.520 --> 29:40.520
Yeah.

29:40.520 --> 29:45.160
That's probably the most famous and most successful general purpose deep learning for differential

29:45.160 --> 29:47.240
privacy algorithm that we have.

29:47.240 --> 29:52.560
But the problem is that it requires this, the second data set that you already have.

29:52.560 --> 29:57.680
And in practice in industry, this is not usually the case, right?

29:57.680 --> 30:04.080
So if I'm a hospital network and I've got say 50,000 examples that are unlabeled, the

30:04.080 --> 30:09.080
friction for me to go to 10 other hospitals and train models with all of them and synthesize

30:09.080 --> 30:13.600
this new model just to like annotate my data set and train my secondary model is it's

30:13.600 --> 30:14.600
pretty rare use case.

30:14.600 --> 30:18.000
Typically, it's cheaper, easier for them to just pay some of the annotated, right?

30:18.000 --> 30:20.560
It's a little more straightforward and it's not as experimental, not kind of thing.

30:20.560 --> 30:25.680
So if we can figure out a way where you can have a similar level of global differential

30:25.680 --> 30:30.760
privacy like this, actually even it's debatable whether this is global or local, it's actually

30:30.760 --> 30:37.920
it's probably closer to local, which doesn't have this core assumption that would that would

30:37.920 --> 30:43.800
be a pretty big breakthrough and it would be the kind of thing where that's the form

30:43.800 --> 30:49.480
of a DP technique that would be general purpose, I think.

30:49.480 --> 30:54.960
And I would very much like to see sort of someone push through on that front, but obviously

30:54.960 --> 30:59.680
lots of people are trying and we'll hopefully we'll have something in the next little while.

30:59.680 --> 31:05.960
Any pointers or thoughts or folks that want to dig more into this area, where should they

31:05.960 --> 31:12.800
start looking for ways to get up to speed on these three pillars or your project?

31:12.800 --> 31:16.880
Yeah, so I mean the three activities that we do and open mind hopefully are designed

31:16.880 --> 31:18.240
to make that as easy as possible.

31:18.240 --> 31:22.960
So we build open source tools, we create learning resources and we build community through

31:22.960 --> 31:25.400
like hackathons and through the Slack channel.

31:25.400 --> 31:30.600
So the first thing that I'd recommend is join the Slack team and kind of scroll through

31:30.600 --> 31:31.960
the articles people are posting.

31:31.960 --> 31:38.680
So you can go to slack.openmind.org, openmind is spelled OPE in I in ED.

31:38.680 --> 31:43.520
Yeah, I happen to general discussion, you know, say hello and then I just finished writing

31:43.520 --> 31:49.640
a nine length notebook tutorial walking from the very basics of kind of remote execution

31:49.640 --> 31:54.480
through federated learning and secure NPC, it'll be extended with differential privacy

31:54.480 --> 31:55.480
in a bit.

31:55.480 --> 31:58.760
And the real goal of that tutorial series is just to walk someone who you know just knows

31:58.760 --> 32:04.880
in this case PyTorch through in a very, you know, hands-on kind of way how these techniques

32:04.880 --> 32:05.880
work.

32:05.880 --> 32:06.880
Awesome.

32:06.880 --> 32:07.880
Yeah.

32:07.880 --> 32:08.880
Awesome.

32:08.880 --> 32:09.880
Well, Andrew, thanks so much for taking the time to chat with me.

32:09.880 --> 32:10.880
It was great to meet you.

32:10.880 --> 32:11.880
Great to meet you too.

32:11.880 --> 32:17.160
All right, everyone, that's our show for today.

32:17.160 --> 32:23.760
For more information on Andrew or any of the topics covered in this episode, visit twimmelai.com

32:23.760 --> 32:27.240
slash talk slash 241.

32:27.240 --> 32:30.720
As always, thanks so much for listening and catch you next time.

