WEBVTT

00:00.000 --> 00:16.320
Hello and welcome to another episode of Twimble Talk, the podcast why interview interesting

00:16.320 --> 00:21.480
people, doing interesting things in machine learning and artificial intelligence.

00:21.480 --> 00:32.960
I'm your host Sam Charrington.

00:32.960 --> 00:38.520
While at the Nips conference back in December, I attended the Black and AI workshop and dinner

00:38.520 --> 00:42.160
and had a chance to meet a bunch of amazing people.

00:42.160 --> 00:46.000
This week on the show, we'll be highlighting some of the great work being done by folks

00:46.000 --> 00:48.320
in this community.

00:48.320 --> 00:53.800
In this episode, I'm joined by Mustafa Cise, research scientist at Facebook AI Research

00:53.800 --> 00:56.600
Lab or Fair in Paris.

00:56.600 --> 01:01.640
Mustafa's broad research interests include the security and safety of AI systems, and

01:01.640 --> 01:06.440
we spend some time discussing his work on adversarial examples and systems that are robust

01:06.440 --> 01:08.720
to adversarial attacks.

01:08.720 --> 01:13.560
More broadly, we discuss the role of bias in datasets and explore his vision for models

01:13.560 --> 01:17.960
that can identify these biases and adjust the way they train themselves in order to

01:17.960 --> 01:21.160
avoid taking them on.

01:21.160 --> 01:27.400
Before we dive in, we want to hear about your experiences with home and personal AI.

01:27.400 --> 01:31.120
So we launched my AI video contest last week.

01:31.120 --> 01:37.000
Please head on over to twimbleai.com slash myai and take a few minutes to submit your thoughts

01:37.000 --> 01:42.560
on the role AI is playing in your home and personal life, your favorite example of home

01:42.560 --> 01:47.320
or personal AI, the AI that you really want to see in your lifetime.

01:47.320 --> 01:50.120
Or just where you see this all going.

01:50.120 --> 01:55.560
The entries with the most likes will win great prizes, including a Cosmo and a Lighthouse,

01:55.560 --> 02:00.720
both of which we discussed on our AI and consumer electronic series last week.

02:00.720 --> 02:03.760
And now on to the show.

02:03.760 --> 02:08.680
Alright everyone, I am on the line with Mustafa Cise.

02:08.680 --> 02:14.000
Mustafa is a research scientist at Facebook AI Research Lab and we had the pleasure of

02:14.000 --> 02:21.200
meeting at the recent nips event and particularly we hung out quite a bit at the black and AI,

02:21.200 --> 02:23.400
both the workshop and the dinner.

02:23.400 --> 02:25.560
Mustafa, it is great to reconnect with you.

02:25.560 --> 02:26.560
How are you doing?

02:26.560 --> 02:27.560
I'm great.

02:27.560 --> 02:28.560
How about you?

02:28.560 --> 02:29.560
Fantastic.

02:29.560 --> 02:30.560
Fantastic.

02:30.560 --> 02:37.320
So the tradition here is for me to give you an opportunity to introduce yourself to

02:37.320 --> 02:38.320
the audience.

02:38.320 --> 02:42.720
So why don't you tell us a little bit about your background and how you got interested

02:42.720 --> 02:44.360
in AI?

02:44.360 --> 02:45.360
Sure.

02:45.360 --> 02:46.360
So I'm Mustafa.

02:46.360 --> 02:49.760
I was born and raised in Senegal in West Africa.

02:49.760 --> 02:54.120
If you have never been there, you should, it's a fantastic country, just a little bit of

02:54.120 --> 02:55.120
ad.

02:55.120 --> 02:56.960
Nice.

02:56.960 --> 03:01.760
So I was born and raised there and I did most of my education there.

03:01.760 --> 03:07.920
I went to the University Gaston Berger, where I studied mathematics and physics.

03:07.920 --> 03:11.800
And it's during one of these courses that I really got interested into AI.

03:11.800 --> 03:15.400
It was an algorithmic course where we had a project.

03:15.400 --> 03:22.560
The task was to design an AI, basically, to solve the game of AOLA, which is a strategic

03:22.560 --> 03:29.480
game, a bit like Checkers or chess, but very popular in West Africa.

03:29.480 --> 03:34.880
And most of the systems that were designed by the students were based on a rule base.

03:34.880 --> 03:38.080
If you do it, if then else rules, basically.

03:38.080 --> 03:46.960
So I really wanted to design a system that learned from different games and learns to solve

03:46.960 --> 03:49.000
the game, basically.

03:49.000 --> 03:51.200
So that's how I got into AI.

03:51.200 --> 03:58.520
I started watching videos on YouTube and teaching myself some of the basic algorithms.

03:58.520 --> 03:59.520
Okay.

03:59.520 --> 04:00.520
Awesome.

04:00.520 --> 04:01.520
Awesome.

04:01.520 --> 04:08.000
And so how did you make your way from there to your current role at Facebook AI research?

04:08.000 --> 04:12.400
So, after that experience, I really enjoyed it.

04:12.400 --> 04:18.480
And I kind of realized that this was what I wanted to do for the rest of my life.

04:18.480 --> 04:21.000
I could contribute in this area.

04:21.000 --> 04:29.160
So I decided to go abroad because there were now not an advanced course or degree in AI

04:29.160 --> 04:30.480
in the university.

04:30.480 --> 04:34.120
So I went to France to study for a master's.

04:34.120 --> 04:40.640
So I studied, I spent the first year in Paris at the University of Paris in Marie Curie.

04:40.640 --> 04:46.040
Then I spent the second year at the University of Montreal.

04:46.040 --> 04:49.520
Then I came back to Paris to do a PhD there.

04:49.520 --> 04:56.920
And then after I graduated from my PhD, I did a one-year postdoc and after that postdoc,

04:56.920 --> 05:00.080
I came to Facebook AI research.

05:00.080 --> 05:01.080
So it's been...

05:01.080 --> 05:03.160
You're based in New York cities, all right?

05:03.160 --> 05:07.080
Actually, I'm based in Paris even though I spent some time in New York sometimes.

05:07.080 --> 05:08.080
Oh, okay.

05:08.080 --> 05:09.080
But I'm based in Paris.

05:09.080 --> 05:14.960
So we have a live in Paris with about 30 researchers and engineers.

05:14.960 --> 05:15.960
Oh, nice.

05:15.960 --> 05:16.960
Yeah.

05:16.960 --> 05:17.960
Nice.

05:17.960 --> 05:23.080
And so maybe we can start out by having you tell us a little bit about your research interests

05:23.080 --> 05:25.400
and the kind of things you're working on nowadays.

05:25.400 --> 05:26.400
Yeah.

05:26.400 --> 05:32.040
So I'm interested in AI and machine learning at large.

05:32.040 --> 05:37.000
But these days I spend most of my time working on trust in AI.

05:37.000 --> 05:48.120
So by trust, I mean all the topics pertaining to safety and security in AI, fairness and

05:48.120 --> 05:51.280
biases and also interpretability.

05:51.280 --> 05:56.320
I think these are very important topics that are gaining some momentum now but that have

05:56.320 --> 05:59.720
been overlooked for some time.

05:59.720 --> 06:08.160
And it's very important that as a community, we focus on building systems that can be trusted

06:08.160 --> 06:14.880
and that can be used broadly to say.

06:14.880 --> 06:18.080
So that's what I spend most of my time working on right now.

06:18.080 --> 06:19.880
Okay.

06:19.880 --> 06:25.200
Can you maybe give me some examples of your recent research you've done in these areas?

06:25.200 --> 06:26.200
Sure.

06:26.200 --> 06:34.320
So in the area of security and safety, I have worked on understanding the topic of adversarial

06:34.320 --> 06:35.720
examples.

06:35.720 --> 06:43.720
So adversarial examples are examples that the model, the models of an AI system sees or

06:43.720 --> 06:51.320
hears or depending on the modality, but that makes the model behave in a completely different

06:51.320 --> 06:59.360
way that it should behave that is in a completely unexpected way.

06:59.360 --> 07:05.400
So these are malicious examples and they tell us a lot about how the models that we are

07:05.400 --> 07:10.840
building right now are kind of not very well understood.

07:10.840 --> 07:17.000
So I have done some work with colleagues in understanding how these adversarial examples

07:17.000 --> 07:23.360
arise and how to generate them for speech recognition or for semantic segmentation which

07:23.360 --> 07:29.280
are very challenging tasks, but which have applications in personal assistance or in self-driving

07:29.280 --> 07:30.680
cars.

07:30.680 --> 07:35.360
And it's very important to be able to create these adversarial examples because that's

07:35.360 --> 07:41.000
the first step in order to evaluate the robustness of an AI.

07:41.000 --> 07:45.160
So then we moved on working on defenses against these adversarial examples.

07:45.160 --> 07:50.560
We have also done some work in this area which have been recently published with some

07:50.560 --> 07:57.240
colleagues here and another line of work is also this area of biases and fairness.

07:57.240 --> 08:05.040
So recently we have shown in a study that the models basically the deep learning models

08:05.040 --> 08:10.440
that you use to train to recognize images when you train them on a popular benchmark

08:10.440 --> 08:13.080
data set which is imagined at.

08:13.080 --> 08:19.760
They learn quite some biased decision making process and this is somehow unexpected because

08:19.760 --> 08:24.160
most of the time the data set is considered as balance, but there are explanations for

08:24.160 --> 08:25.160
these.

08:25.160 --> 08:32.160
So we have observed some interesting biases ranging from racial biases for example and

08:32.160 --> 08:34.200
all this is described in the paper.

08:34.200 --> 08:35.200
Okay.

08:35.200 --> 08:41.240
Yeah, I think I've talked about adversarial examples in my newsletter but I don't think

08:41.240 --> 08:46.640
we've really dug into it in the podcast, at least not in a lot of detail.

08:46.640 --> 08:53.040
So maybe let's start there and for folks who aren't familiar with that whole field,

08:53.040 --> 08:58.280
do you have some favorite examples of adversarial examples?

08:58.280 --> 08:59.280
Sure.

08:59.280 --> 09:08.360
So imagine you have a system that is supposed to tell you what is in a picture.

09:08.360 --> 09:15.240
So if you present the system with a picture it says, this is a dog or this is a cat etc.

09:15.240 --> 09:21.320
So it is possible to inject to the picture that you're presenting to the image.

09:21.320 --> 09:28.200
Some very structured noise that is imperceptible to the human eye but that will change the

09:28.200 --> 09:29.840
decision of the system.

09:29.840 --> 09:36.160
So you can have two seemingly identical pictures of dog of the same dog and for one of them

09:36.160 --> 09:40.240
the system will say this is a dog and for the second one the system will say this is

09:40.240 --> 09:44.280
a cat or this is a car or any other thing.

09:44.280 --> 09:51.320
So this is kind of intriguing because these systems are very accurate normally and we tend

09:51.320 --> 09:56.760
to compare them to humans because they are very accurate and when we measure their accuracy

09:56.760 --> 10:02.720
we say hey, usually we say hey look the system is as accurate as a human but on the flip side

10:02.720 --> 10:08.160
a human is very robust to these kind of perturbations like no matter how much perturbation you

10:08.160 --> 10:14.200
add to an example if a human looks at two pictures that are similar she will be able

10:14.200 --> 10:20.200
to say that this is a dog and this is a dog as well but if the system can be tricked

10:20.200 --> 10:27.200
by these slight modifications that are imperceptible to the human eyes it means something.

10:27.200 --> 10:33.480
It says a lot about the current understanding or the lack of understanding that we have

10:33.480 --> 10:39.240
about the behavior of these models and it makes it an interesting topic to study.

10:39.240 --> 10:45.160
So this is basically the high level explanation of adversarial examples.

10:45.160 --> 10:53.880
By the way I gave an example speaking about topics or speaking about images, image recognition

10:53.880 --> 11:00.160
but I think that's what most of us associate with adversarial examples with scenes a bunch

11:00.160 --> 11:03.800
of these but it's not just images.

11:03.800 --> 11:04.800
Absolutely.

11:04.800 --> 11:11.280
So we have shown in a recent paper a generic method called Houdini that allows you to generate

11:11.280 --> 11:17.080
adversarial examples not only for images but also for speech recognition task where you

11:17.080 --> 11:23.520
can add some noise in an audio file such that humans cannot distinguish the two audio

11:23.520 --> 11:30.720
files but a speech recognition system will be completely interpreted to audio files in

11:30.720 --> 11:32.280
a completely different way.

11:32.280 --> 11:34.720
So it's a very broad topic.

11:34.720 --> 11:35.720
Oh wow.

11:35.720 --> 11:42.120
So the example that comes to mind for me is if you can create an audio file that says

11:42.120 --> 11:49.800
one thing but interprets the system as saying okay Google you could really mess with people's

11:49.800 --> 11:53.680
their virtual assistants and things like that that they've got around their home.

11:53.680 --> 11:54.680
Sure.

11:54.680 --> 11:59.920
That's one one scenario you could imagine definitely.

11:59.920 --> 12:01.720
That's one scenario you could imagine.

12:01.720 --> 12:10.040
To what extent are the current adversarial examples that work in general like to what

12:10.040 --> 12:16.040
extent is that model specific meaning is the work that's happening around adversarial

12:16.040 --> 12:25.120
examples and defenses is it all explicitly impact specific types of models you know deep

12:25.120 --> 12:29.240
neural networks or specific types of deep neural networks or specific architectures

12:29.240 --> 12:35.680
of networks or is it a broader phenomenon that can apply more generally.

12:35.680 --> 12:41.400
So I would say that this is a broad phenomenon that applies to different class of model

12:41.400 --> 12:45.520
branching from deep neural networks of course people talk about it because it's the most

12:45.520 --> 12:50.360
familiar family of functions right now but it also applies to separate vector machines

12:50.360 --> 12:53.120
or to decision trees and all sorts of things.

12:53.120 --> 12:59.160
That being said the kind of model that you that the specific type of model that you're

12:59.160 --> 13:07.080
that you're using can have properties that make it less robust to adversarial examples.

13:07.080 --> 13:11.640
This is true across the different family of models but depending on the conditioning

13:11.640 --> 13:17.800
and the nice some nice properties that the model may have they may be more or less robust

13:17.800 --> 13:19.840
to adversarial examples.

13:19.840 --> 13:27.160
However for the defenses there are defenses that exploit the nature of the model sometimes

13:27.160 --> 13:32.520
but there are also some defenses that are agnostic to the type of model that you consider.

13:32.520 --> 13:38.880
For example we recently proposed in a paper defenses against adversarial examples that

13:38.880 --> 13:45.800
are just based on transformations of the input in order to remove the noise that has

13:45.800 --> 13:49.720
been injected to the input.

13:49.720 --> 13:54.800
So this is typically completely agnostic to the type of model that you consider.

13:54.800 --> 13:56.800
Okay.

13:56.800 --> 14:03.560
These adversarial examples they you know they're maliciously created what's the process

14:03.560 --> 14:06.600
generally for creating them.

14:06.600 --> 14:13.320
So the process for creating an adversarial example is actually the inverse process for

14:13.320 --> 14:15.000
training the model.

14:15.000 --> 14:21.760
So when you train the model you show to the model an image of a dog and you measure how

14:21.760 --> 14:25.120
well the model recognizes that this is a log.

14:25.120 --> 14:33.640
This is a dog and you reinforce that decision if it is a positive one and or you you kind

14:33.640 --> 14:40.320
of encourage the model to change its decision if the decision is not good.

14:40.320 --> 14:45.240
So when you create an adversarial example you do the reverse process and the way you

14:45.240 --> 14:51.600
do it is that we show an example of a dog to the model and when the model says yes this

14:51.600 --> 15:02.760
is a dog then you calculate from the model the right direction in which you need to move

15:02.760 --> 15:09.680
the example in the input space in order to change slowly the decision of the model which

15:09.680 --> 15:17.280
means that your base on the decision of the model in order to know how to trick it.

15:17.280 --> 15:20.440
So it's the opposite way of training the model.

15:20.440 --> 15:27.040
So the subtlety here is that changing the decision of the model does not require a big

15:27.040 --> 15:29.600
change in the input space.

15:29.600 --> 15:36.320
In fact if you consider images at the pixel level that change is always almost imperceptible

15:36.320 --> 15:43.840
by a human eye but it is surprisingly sufficient for flipping the decision of a model.

15:43.840 --> 15:50.400
You know there are technologies in the case of images and audio as well where you can have

15:50.400 --> 15:56.600
like these digital watermarks where you are doing similar things right you are placing

15:56.600 --> 16:03.600
some kind of watermark I guess in that case it is not noise per se but you are altering

16:03.600 --> 16:12.240
the image in a way that is imperceptible to the human observer but has some meaning to

16:12.240 --> 16:14.080
the system.

16:14.080 --> 16:17.920
I guess I'm just sharing that as kind of an example of how that process is.

16:17.920 --> 16:25.880
It's very similar it's not the same thing but it's very similar that you manage to put

16:25.880 --> 16:33.120
in the input some information that makes the model behave in some way but that is basically

16:33.120 --> 16:38.800
imperceptible and you are definitely right pointing to the watermarks because it's very

16:38.800 --> 16:40.880
similar in spirit.

16:40.880 --> 16:47.640
And so there are a bunch of folks that are researching these adversarial examples.

16:47.640 --> 16:53.760
What's the current research thrust is it trying to identify different ways of generating

16:53.760 --> 17:00.440
them or have we all kind of is there you know only you know that general way that you

17:00.440 --> 17:05.760
describe to create them and everyone is working on defense or what.

17:05.760 --> 17:09.400
So there are different rules for generating adversarial examples but they basically exploit

17:09.400 --> 17:12.320
the same information which is the gradient.

17:12.320 --> 17:16.480
In one way or another it can be direct when you have access to the full information

17:16.480 --> 17:22.320
about the model or it can be direct where you kind of train a substitute of a model and

17:22.320 --> 17:26.640
generate the gradient from it and transfer the adversarial example to another model.

17:26.640 --> 17:32.040
So there are a few different attacks that do exist but for now I have to say that attackers

17:32.040 --> 17:37.280
have an edge because it's much more difficult to full a model than to protect it from adversarial

17:37.280 --> 17:38.280
examples.

17:38.280 --> 17:43.080
However in my opinion adversarial examples are not just interesting from a security point

17:43.080 --> 17:44.080
of view.

17:44.080 --> 17:51.040
Of course this is very important because it's critical when you want to use these powerful

17:51.040 --> 17:57.760
and attractive deep learning models for example in certain types of applications.

17:57.760 --> 18:04.680
But another aspect of the adversarial example is that they tell us something about the models

18:04.680 --> 18:08.480
that we love and that we use on a daily basis.

18:08.480 --> 18:14.600
They say that there are things that we do not understand because this behavior is

18:14.600 --> 18:17.200
sort of pathological.

18:17.200 --> 18:23.720
So they trigger interesting research questions beyond the security perspective but on the

18:23.720 --> 18:28.520
very nature of these models and the learning algorithms that we use.

18:28.520 --> 18:33.560
And we have also found with other colleagues that adversarial examples can be used for

18:33.560 --> 18:39.480
other purposes and in our case it was to exhibit biases that the model may have learned

18:39.480 --> 18:43.760
from the data which was quite interesting to see.

18:43.760 --> 18:47.320
Can you elaborate on that and what the implications are?

18:47.320 --> 18:53.720
When you have a model so basically machine learning models or you design a parametric

18:53.720 --> 19:00.880
model you take some data you apply a learning rule and you train the model from the data.

19:00.880 --> 19:09.720
So you can expect that if your model is trained to optimize for some criteria it will behave

19:09.720 --> 19:16.160
very well if the learning algorithm is good, if you have sufficient data, if the model

19:16.160 --> 19:25.200
that you have chosen is powerful enough that it will behave in some expected way, right?

19:25.200 --> 19:29.960
But so many your input data is follows a similar distribution.

19:29.960 --> 19:35.960
Absolutely, that's the back hypothesis, very good observation.

19:35.960 --> 19:44.680
So the point here is that your model may have captured some regularities that are present

19:44.680 --> 19:52.000
in the data but that you may not notice because when you test the model or when you evaluate

19:52.000 --> 19:58.840
it you are not using the right criteria that could show you the broad scope of everything

19:58.840 --> 20:00.800
your model have learned.

20:00.800 --> 20:03.920
So I can give you an example.

20:03.920 --> 20:11.760
So we have found that, for example, if you take a popular model like ResNet, residual networks

20:11.760 --> 20:18.680
which are very popular in computer vision and you train them on ImageNet using all the

20:18.680 --> 20:24.880
sophisticated learning algorithms then you will achieve some excellent performance as far

20:24.880 --> 20:30.680
as accuracy is concerned but when you look deeply into why the model predicts what it

20:30.680 --> 20:33.600
predicts you can see some very funny things.

20:33.600 --> 20:40.040
So for example we found out that if you consider the class the category basketball.

20:40.040 --> 20:47.520
So for some reason the model tend to consider that if you show an input with a black person

20:47.520 --> 20:52.960
to the model it will predict that this is a picture whose corresponding category is

20:52.960 --> 21:00.760
basketball and this is no matter whether it is a basketball or not doesn't matter.

21:00.760 --> 21:05.560
And the reason is the model has picked up some biases in the data.

21:05.560 --> 21:13.680
So that suggests this actually and it's true also for different sort of biases.

21:13.680 --> 21:19.240
It almost makes me think that like all models over fit, it's just some of them over

21:19.240 --> 21:26.400
fit too much but you know the other ones are overfitting in ways that you know might produce

21:26.400 --> 21:32.720
acceptable results until you know we're throwing a curve ball and see something that we didn't

21:32.720 --> 21:33.720
expect.

21:33.720 --> 21:34.720
Absolutely.

21:34.720 --> 21:37.520
This is a very good observation.

21:37.520 --> 21:42.880
So many of the models which if they are not properly regularized or if the last function

21:42.880 --> 21:52.440
the objective, the criteria that you're optimizing for does not take into account these problems

21:52.440 --> 21:57.680
they may well learn different sort of biases from the data.

21:57.680 --> 22:00.880
And these biases are numerous and diverse by the way.

22:00.880 --> 22:09.480
So I always put it this way I think you are what you eat and for models for the models

22:09.480 --> 22:14.160
the data sets they're trained on right now are just junk food.

22:14.160 --> 22:19.140
So we should we should not expect them to behave differently.

22:19.140 --> 22:25.240
If we don't endow the models with the ability to pick precisely the examples they should

22:25.240 --> 22:29.600
learn from and what they should learn and what they should not learn.

22:29.600 --> 22:34.320
Just the way we should we we do it with the kids for example where the kid takes something

22:34.320 --> 22:42.160
and puts it in her mouth if it is something that a kid can eat you probably can let her

22:42.160 --> 22:48.920
do but if it is not then you will stop her and say hey you cannot do this and as adult

22:48.920 --> 22:52.920
as well sometimes we say oh I can eat this I cannot eat that because there is there is

22:52.920 --> 22:57.960
a lot of fat here there is not enough there is a lot of sugar here etc.

22:57.960 --> 23:05.360
So we should endow the models with the ability to do some self criticism and introspection

23:05.360 --> 23:10.360
to say I should learn from this data set I should learn from this data point or I should

23:10.360 --> 23:13.720
not learn from this data point etc.

23:13.720 --> 23:19.000
So these are very fascinating questions I spend a lot of time thinking and working

23:19.000 --> 23:21.000
on these days.

23:21.000 --> 23:28.240
So I wanted to drill down into that last point you made about endowing the model with

23:28.240 --> 23:34.480
this ability to kind of discriminate between good and bad data but before we do that we

23:34.480 --> 23:42.120
kind of got to the bias by way of the adversarial examples and I think you were making a connection

23:42.120 --> 23:46.640
there that I didn't fully catch and I want to make sure that if that is what you were

23:46.640 --> 23:48.800
trying to do that we make that clear.

23:48.800 --> 23:54.920
Sure so the reason why adversarial examples can be interesting to exhibit the biases

23:54.920 --> 24:00.760
the model may have learned is actually very simple so when you learn a classifier basically

24:00.760 --> 24:07.520
what you are trying to do is to find a decision boundary to draw a line that says that this

24:07.520 --> 24:12.560
side of the line is the cat and the other side of the line is the dogs.

24:12.560 --> 24:16.960
This is very simple and you can visualize it very well if the model is linear but if

24:16.960 --> 24:20.800
the model is non-linear it is much more challenging.

24:20.800 --> 24:28.000
And the way adversarial examples are built it is by taking a point of a specific class

24:28.000 --> 24:36.160
and moving it slowly but in a very straightforward way to the other side of the decision boundary.

24:36.160 --> 24:39.680
So not all examples are created equal.

24:39.680 --> 24:46.480
If an example is close to the decision boundary in the latent space then you will not need

24:46.480 --> 24:52.960
to move it a lot in order to take it to the other side of the decision boundary.

24:52.960 --> 24:58.080
And if an example is far from the decision boundary you will have a hard time taking it

24:58.080 --> 25:03.000
to the decision boundary to the other side of it meaning that you will have to add a

25:03.000 --> 25:05.920
lot of noise, a significant amount of noise.

25:05.920 --> 25:11.560
But the examples that are close to the decision boundary are those examples for which the

25:11.560 --> 25:17.640
model have learned a representation that is not very robust when you consider the concept

25:17.640 --> 25:19.440
they should belong to.

25:19.440 --> 25:26.960
So if you take a picture of a dog and for which you can flip the decision of the model very

25:26.960 --> 25:34.040
easily by adding very slightly very small amount of noise that means that the model is

25:34.040 --> 25:38.760
not very sure about what this is and that is called criticism.

25:38.760 --> 25:40.480
So this is a criticism for the model.

25:40.480 --> 25:45.440
It's something that the model barely knows what it is but it's not very sure.

25:45.440 --> 25:51.720
But if you take a picture of a dog and you add a lot of noise and still you have a hard

25:51.720 --> 25:57.800
time flipping the decision of the model then the model is pretty confident what this is.

25:57.800 --> 26:03.400
So and that is what the model considers as being prototypical.

26:03.400 --> 26:11.680
So after learning from some data you can take another dataset, some test dataset and

26:11.680 --> 26:16.800
and by using the adversary the procedure for generating adversarial examples you can

26:16.800 --> 26:22.840
see how much noise you should add to some data points in order to change the decision

26:22.840 --> 26:30.080
of the classifier and use that as a proxy of how prototypical an example is to the model.

26:30.080 --> 26:33.960
So the examples that are prototypical to the model to what the model have learned about

26:33.960 --> 26:39.040
a given concept will be hard to change their decision.

26:39.040 --> 26:41.840
But for the others it will be easy to change their decision.

26:41.840 --> 26:46.120
And if you look at the prototypical example they will tell you what what the model has

26:46.120 --> 26:49.320
really learned what it is very confident about.

26:49.320 --> 26:54.960
And that's that's actually what we used and in order to discover what what the model considered

26:54.960 --> 26:57.680
really as being pictures of basketball.

26:57.680 --> 27:02.000
And when we looked at it it was all pictures of basketball basically with just blood

27:02.000 --> 27:04.600
persons inside and never white persons.

27:04.600 --> 27:09.160
And when we looked at pictures of that or criticisms which is you know some pictures

27:09.160 --> 27:13.840
that the model classifies pretty well but it's not very sure about it it's at the edge

27:13.840 --> 27:24.240
of its knowledge then it's very it's pictures that are mostly populated with white persons.

27:24.240 --> 27:32.120
And all this is basically validated on different categories and different classes.

27:32.120 --> 27:38.560
Similarly we found that when the model considers us being prototypical offered an image belonging

27:38.560 --> 27:45.600
to the category ping pong is basically a picture with an Asian in it.

27:45.600 --> 27:52.480
So so so after when people see this kind of things they will say hey the model is is racist.

27:52.480 --> 27:56.840
In fact a model cannot be racist because it's not intelligent the models that we build

27:56.840 --> 27:59.320
are accurate but they are not intelligent.

27:59.320 --> 28:04.640
And whatever they learn whatever they whatever behavior they exhibit they just learned

28:04.640 --> 28:06.560
it from the data they were trained on.

28:06.560 --> 28:11.240
So somehow these models are the mirror of what we are.

28:11.240 --> 28:17.320
So they they just tell us something about the process we use to collect the data to

28:17.320 --> 28:23.160
train the model and sometimes they even tell us something about those who collected that

28:23.160 --> 28:26.000
data and those who built the model.

28:26.000 --> 28:31.640
So so I will take this occasion to emphasize on something that is very important regarding

28:31.640 --> 28:35.880
diversity in in our community.

28:35.880 --> 28:42.920
It's actually critical and not just because it's fashionable or it's because it sounds cool.

28:42.920 --> 28:49.080
It's critical that as a community we become more open and more diverse because the models

28:49.080 --> 28:56.440
that we build and the data sets these models learn from if we want to have a broad impact

28:56.440 --> 29:03.440
at the global level worldwide and build technology that is representative of the human

29:03.440 --> 29:09.600
beings on this planet and not just a specific population which tends to be westerns and

29:09.600 --> 29:16.240
North Americans it's important that we become more open and more diverse so that everybody

29:16.240 --> 29:25.040
has the tools and and the the techniques to build the technology to solve its own problems.

29:25.040 --> 29:32.400
Well everyone has the tools and techniques but also those organizations that are building

29:32.400 --> 29:38.280
you know these types of systems you know have a more natural inclination to structure

29:38.280 --> 29:44.040
their data sets for example so that they're more robust.

29:44.040 --> 29:50.760
Actually I'm not even sure that everybody has the tools and techniques because let's

29:50.760 --> 29:55.280
say let's consider the machine learning community for example.

29:55.280 --> 30:01.080
So this community every year organizes conferences which the tool of the most important ones being

30:01.080 --> 30:09.720
Nips and ICML and they are basically every year organized in a western country either in

30:09.720 --> 30:16.200
Europe or in the US with some exceptions ICML has been organized previously in China for

30:16.200 --> 30:23.640
example and last year in Australia which I consider being a western country as well.

30:23.640 --> 30:30.360
So it is very difficult for people from other parts of the world to get into the community

30:30.360 --> 30:38.280
just because the places where these gatherings happen are not easily accessible to them

30:38.280 --> 30:44.760
and that's one thing right that's one thing and when you are when you are European or

30:44.760 --> 30:48.280
American you may not see these things because you have the right passport that allows you

30:48.280 --> 30:53.480
to go everywhere but when you are not then then you can see this you know I just give you

30:53.480 --> 31:00.280
an example this year I had two papers accepted for publication at ICML it was in Australia

31:00.280 --> 31:04.840
I could not go there because they did not grant me a visa.

31:04.840 --> 31:09.640
For many people this is they don't even believe it when you say it but it's true and it's

31:09.640 --> 31:19.240
just like the random the normal the routine for many people for many people so that's one

31:19.240 --> 31:25.640
thing and a second thing is that even for those who manage to attend these conferences

31:25.640 --> 31:32.120
if they are not established in a lab that is in one of these countries it can be very difficult

31:32.120 --> 31:39.320
to do some research some you know interesting and important research because most of what we do

31:39.320 --> 31:47.560
right now requires huge amount of computational resources which are not available in many countries

31:47.560 --> 31:57.400
so we need to do something so you know there are various initiatives to make the community more

31:57.400 --> 32:05.800
open and more inclusive some of which I can name are the women in machine learning and also the

32:05.800 --> 32:12.920
black in AI which we organize this year we also have this initiative called Data Science Africa

32:12.920 --> 32:19.080
which we organize every year it's a summer school where we teach machine learning to local

32:19.080 --> 32:25.000
students chat with very popular so far so they're initiatives but I think we need much more

32:25.880 --> 32:31.320
I also throw in a shout out to the deep learning in Dava which happened in South Africa for the

32:31.320 --> 32:37.400
first time last year and they're planning a follow on this year as well yes yes that that also is

32:37.400 --> 32:45.320
an is an interesting initiative that was very helpful in this direction as well interesting so

32:46.360 --> 32:53.800
and I guess in that way all of the you know I think that that that kind of ties together all of

32:53.800 --> 33:00.440
the various things that you work on that may seem like separate and distinct areas of research

33:00.440 --> 33:06.680
they're not really separate they they fit in what I think as a person I should be doing so I

33:06.680 --> 33:12.120
am committed to build an axiological artificial intelligence and by axiological I mean

33:12.760 --> 33:17.880
an artificial intelligence that is aligned with the value of the society in which it operates

33:19.000 --> 33:25.000
and and to be aligned with the value of the society in which you operate means that you were

33:25.640 --> 33:31.960
aware of the biases that you should avoid but also you were aware of the utility that you can

33:31.960 --> 33:38.360
have the problems that you may be you may solve and also the guarantees of level of safety and

33:38.360 --> 33:46.600
security so that's that's the general scope of what gives me busy right now which brings us back

33:47.400 --> 33:53.960
to that previous comment that I kind of put a bookmarker on and that was kind of the distinction

33:53.960 --> 34:07.000
you made between us as the creators of the AI being aware of bias in data sets and factors like

34:07.000 --> 34:14.680
that versus the models themselves being aware of these things and evolving through training

34:14.680 --> 34:20.520
or design or other things to I guess the way you put it was be selective about the data they

34:20.520 --> 34:26.040
consume can you elaborate on on that and the kind of work you're seeing happen there so

34:26.680 --> 34:33.080
so what I said is that you are what you eat and the data the models are trained on right now is

34:33.080 --> 34:43.240
basically junk food so what I meant is that we do not put a lot of effort into our cells first

34:43.240 --> 34:50.040
looking into this data and seeing what the model what should be there and what should not be there

34:50.040 --> 34:56.520
so for example if you collect the data that that serves to train a model which will be used

34:57.080 --> 35:03.000
worldwide then you make sure that that data is representative of the population worldwide

35:03.000 --> 35:08.200
that's that's that's the basic first step to do I get that part I thought you also maybe this is

35:08.200 --> 35:12.920
where you're getting with the second step I thought you were also suggesting that you're also

35:12.920 --> 35:21.080
exploring ways to teach the models yes to know yes that's that's that's where I'm coming so that's

35:21.080 --> 35:27.320
so the first part is the data part itself actually there is the degree zero which which even comes

35:27.320 --> 35:36.760
from before the the data part the zero the zero step step zero is to make sure that the people

35:36.760 --> 35:45.320
who work on the data and on the model are diverse as possible because only that way they will be

35:45.320 --> 35:51.080
able to notice these biases and of course the first step after that is to make sure that the

35:51.080 --> 35:59.480
data itself is representative of the population but then after that which which a more little

35:59.480 --> 36:06.680
bit more technical question is the models should be in doubt with the ability to select what they

36:06.680 --> 36:13.000
should learn from and what they should not learn from and what what kind of features they should be

36:13.000 --> 36:23.800
invariant of and and this is a this is a less explored territory because there is definitely

36:23.800 --> 36:31.960
some work in the area of making the models invariant to gender or invariant to race etc but I think

36:31.960 --> 36:38.440
we should we should in order to design models that are intelligent but that are free from the

36:38.440 --> 36:46.520
biases we should just take inspiration from ourselves you know we do we humans do some do

36:46.520 --> 36:54.280
introspection basically we when we see a book and we read the book we don't believe everything in

36:54.280 --> 37:00.360
the book right we say this makes sense this doesn't make sense we criticize many of the things

37:00.360 --> 37:07.080
and that's also part of learning but right now that learning is completely positively supervised

37:07.080 --> 37:13.400
which means that when you show when you show an example to a model the the label it is associated

37:13.400 --> 37:19.560
with is exactly what you want to learn from so we should we should end out the model to be able

37:19.560 --> 37:25.960
to to be able to criticize the data it is learning from and to select what it should learn from

37:25.960 --> 37:31.400
according to some criteria and what it should not learn from according to other criteria and

37:31.400 --> 37:37.400
and to me this is an excited an exciting direction research direction which which has not been

37:37.400 --> 37:44.840
much explored I have to say but but it's something that we should work more on are there any examples

37:44.840 --> 37:51.640
of of this even very simple ones that have been explored in the research so recently I published

37:51.640 --> 37:57.800
a paper with a colleague of mine where we try to do something like this we try to we try but but

37:57.800 --> 38:07.320
for a model that is already learned we we try to design a method that allows an operator to take

38:07.320 --> 38:14.840
the model and apply the algorithm to in order to in order to to exhibit the biases of that

38:14.840 --> 38:21.160
that the devices that have been learned by the model but then one could one could imagine that

38:21.160 --> 38:28.280
this algorithm is applied recursively to the model while the model is being trained and that's

38:28.280 --> 38:33.800
where it gets interesting it's like again type of training something like that something like that

38:33.800 --> 38:42.040
not not not necessarily again because in this way is I don't see it really as being invariant

38:42.040 --> 38:51.320
something but but but really as being able to criticize itself and also to to decide which data

38:51.320 --> 38:56.120
it should learn from and which which one it should not learn from so more in an active learning

38:56.120 --> 39:03.160
fashion but it's a bit more complicated than that so so it's it's it's it's an interesting

39:03.720 --> 39:07.960
and challenging research direction I'm very excited about what were the results of that

39:07.960 --> 39:13.160
that paper how far did you get with it so what we thought is some of the results that I mentioned

39:13.160 --> 39:18.440
before so we thought that if you take a state of your computer vision model image classification

39:18.440 --> 39:25.000
model and train it on a most standard data you will see stuff appearing that are not expected

39:25.000 --> 39:29.480
for example you will see that the model considers if you basically if you show it a picture of

39:29.480 --> 39:34.920
Barack Obama it would classify as basketball and we have we have real examples like that in the

39:34.920 --> 39:39.400
paper and if you show it a picture of the president of China it would classify it as ping pong

39:40.200 --> 39:47.240
and and and this can be shocking but it's just it's just some biases that are present in the data

39:47.240 --> 39:52.840
in fact there is something very interesting that we show that actually we found out which is

39:53.560 --> 40:00.120
we we thought that this was due to the imbalance in the in the class basketball for example

40:00.120 --> 40:05.960
which we thought that the class basketball ball only contained pictures with mostly black people

40:05.960 --> 40:10.840
but when we I've assumed this whole time that that's what you're uh that that was what you were

40:10.840 --> 40:17.000
saying actually that's not what happened so yeah so we we we we we we computed the statistics and

40:17.000 --> 40:22.920
what we found out is that the class basketball basically contains as many pictures with white

40:22.920 --> 40:27.400
people as pictures with black people so if you take a picture the probability that there is white

40:27.400 --> 40:33.960
people is roughly the same as a as a as a probability that you see a black person in it but what

40:33.960 --> 40:39.880
happened actually is that if you maybe black people are much you know less much more underrepresented

40:39.880 --> 40:44.520
in your data center so they're more uniquely associated absolutely absolutely so it's this is

40:44.520 --> 40:49.080
it's a mutual information problem so if you look at the if you look at just the first order

40:50.600 --> 40:55.240
observations then you may not really see what's happening but if you look at the problem more

40:55.240 --> 41:00.840
broadly then you see that it's exactly what you what you what you said so that uh even in the

41:00.840 --> 41:06.840
data set black people are more uniquely associated with the class basketball which is a bit problematic

41:06.840 --> 41:12.200
because uh again it say something about how the data set was collected and I'm sure that the

41:12.200 --> 41:18.120
people who collected the data set they did put at the time a lot of care to make uh you know

41:18.120 --> 41:24.760
everything uh as correct as possible but still you can see the type of uh biases that can

41:24.760 --> 41:32.840
arise uh from the data set yeah and so so we're talking about algorithms that can learn to identify

41:32.840 --> 41:39.160
these uh these issues and you mentioned the ping pong and basketball examples in that research

41:39.160 --> 41:45.640
did your algorithm you know were these kind of handpicked examples or did your supervisory

41:45.640 --> 41:50.840
algorithm find these examples within the first model yeah it's it's it's really not handpicked

41:50.840 --> 41:55.000
actually I gave them I gave these examples because it's racial biases and people know what it means

41:55.880 --> 42:01.160
but uh but we found other things for example when for the class traffic light we found out that the

42:01.160 --> 42:08.120
model whenever there is in an image you a blue sky with a bar you know some some black bar in it

42:08.120 --> 42:12.920
the the model will always predict traffic light just because doesn't matter whether there is

42:12.920 --> 42:17.720
traffic light or not it just because in most pictures with traffic light you see the this you

42:17.720 --> 42:24.680
know this middle this iron bar with in a blue sky or something so it's it's really you know

42:24.680 --> 42:29.640
it's really a lot of different biases but I just I just give these two as examples because

42:29.640 --> 42:37.480
people can relate with it and know exactly what that means can you envision like or or at least

42:37.480 --> 42:44.760
I'm envisioning as you're describing this maybe um some kind of meta annotation process where

42:44.760 --> 42:50.600
you know you you you initially annotate your data but then you run and you train a model and then

42:50.600 --> 42:55.560
you kind of apply this algorithm to the model and it shows you all of these biases and then you

42:55.560 --> 43:00.920
have this meta annotation step where folks are identifying whether these biases are valid or

43:00.920 --> 43:05.720
something like that and using that to retrain your model do you envision something like that actually

43:05.720 --> 43:10.040
I will take it one step further I will I will remove even the human in the second loop

43:10.040 --> 43:16.040
if things are done if things are done properly ideally what I would like to see is that a model

43:16.040 --> 43:22.920
learn from the data in an initial step and itself criticizes itself and says I should not have

43:22.920 --> 43:29.880
learned this and that and then goes in a second round of learning and in a be more careful

43:30.440 --> 43:38.360
what it learns from which data point etc and iteratively produces a final model that is at least

43:38.360 --> 43:46.440
um less biased because you never can have a model that is completely free from all biases that's

43:46.440 --> 43:53.640
just not possible um but you can mitigate it and and in my opinion that's something that we

43:53.640 --> 43:59.880
definitely can do this question maybe getting further ahead of you know where you are with this

43:59.880 --> 44:04.680
research but you know given and given this exact any of these examples really but this example of

44:04.680 --> 44:13.880
the blue sky and the bar and the the traffic light the way we train these examples or these models

44:13.880 --> 44:21.000
rather uh you know what what do you envision the mechanism being inside that training loop

44:21.000 --> 44:27.720
that allows the model to even know you know what the thing is that it's supposed to be

44:27.720 --> 44:35.880
uh paying attention to so there are two things here the first is the the learning rule that we're

44:35.880 --> 44:41.000
using so currently most of these models are trained in a completely supervised way so we say hey

44:41.000 --> 44:47.160
this is an image of a traffic light and then the model kind of learn to figure out these things

44:47.160 --> 44:55.400
uh so I think that we should reduce the supervision that's that's one one thing and reducing the

44:55.400 --> 45:02.440
supervision having a learning algorithm that needs less supervision will make definitely the

45:02.440 --> 45:07.960
algorithm more robust because it will have access to much more data which is unsupervised and

45:07.960 --> 45:15.000
it will the labeling process itself is very noisy in general so the the model may be exposed to

45:15.000 --> 45:23.960
less noise and it it may less overfit so if the model is able to to leverage more of the unsupervised

45:23.960 --> 45:29.960
data it it may end up less overfitting so that's one thing the second thing is that

45:30.600 --> 45:35.720
the objective functions that the model is optimizing should be designed in a way

45:36.840 --> 45:44.760
that makes the model avoid learning some biases and that's that's where the that's also one

45:44.760 --> 45:51.160
change that needs to happen in the way we supervise the model so so so I just give you an example so

45:51.160 --> 45:57.240
right now the way we trained if you take a any state of the other image that model it was trained

45:58.360 --> 46:06.760
in examples that have pictures and label and the and the model has no idea how the labels are

46:06.760 --> 46:13.640
related to each other it it ended up somehow figuring it out and I should not even say figuring it

46:13.640 --> 46:20.040
out but it ended up you know producing representations yes representations where cats are

46:20.040 --> 46:25.800
close to dogs etc but it's not really the case so your model can make completely stupid mistakes

46:26.520 --> 46:32.920
so if a cat is misclassified as a dog it's it's a wrong decision but it's still fine because

46:32.920 --> 46:38.520
semantically we know that they are closer to each other than a cat being misclassified as a plane

46:38.520 --> 46:47.880
for example and and you know so far we have not managed to train the models to to really take

46:47.880 --> 46:53.320
into account this decision in order to improve their accuracy and the type of errors they take

46:53.320 --> 46:59.960
so there are a lot of attempts in the literature but in my opinion has not been very convincing

46:59.960 --> 47:08.120
so there is a lot of things to explore in these directions yeah it's it sounds like

47:09.080 --> 47:17.000
there's definitely a lot to explore and furthermore these you know the kind of the direction

47:17.000 --> 47:22.760
that you're proposing is a fairly as dramatic too strong you know a dramatic shift from kind

47:22.760 --> 47:30.760
of the way we train these models today well at some point we just need to to change paradigms and

47:30.760 --> 47:37.320
that's that's how that's how research works I'm not claiming that what I'm saying is the right

47:37.320 --> 47:44.040
thing to do I mean it's probably not but I feel that I observed that we're reaching the limits

47:44.040 --> 47:49.800
of most of the the learning algorithms and the models that we're currently using can offer

47:50.360 --> 47:56.680
and probably what we take us to the next level is not in the box in which we're thinking right now

47:56.680 --> 48:05.400
so we should probably take a step back and and look at the the very principles of the learning

48:05.400 --> 48:11.080
algorithms that we're using and think them in a way that can take us to the next level

48:11.080 --> 48:17.720
awesome awesome well I think that sounds like a good place to wrap up Mustafa thank you so much

48:17.720 --> 48:24.920
for taking the time to chat with us any final words that you'd like to add so thank you very much

48:24.920 --> 48:29.640
for inviting me it has been a pleasure to discuss with you I just would like to mention a last

48:29.640 --> 48:37.800
thing which is there is a lot of research currently in the area of bias and fairness and and

48:37.800 --> 48:45.400
and all of these very interesting and fascinating topics but there is also one thing that in my

48:45.400 --> 48:51.160
opinion is a bit overlooked in the community which is fairness and bias is not only in the models

48:51.160 --> 48:57.400
that we design or the datasets the models learn from it starts with the problems that we consider

48:58.680 --> 49:05.160
if we start by considering problems that are important for a specific population in the world

49:05.160 --> 49:10.760
and just focus on solving these type of problems the end resource is the datasets that we'll be

49:10.760 --> 49:16.440
using and the models that will be that will be learning from these datasets will obviously be biased

49:17.320 --> 49:25.160
so I think as a community we should also open ourselves to consider problems that are of interest

49:25.160 --> 49:34.360
at the global level and I will just give you an example to conclude where there is definitely

49:35.400 --> 49:41.880
it's definitely an exciting direction all the work happening in self-driving cars etc

49:43.080 --> 49:49.160
and a lot of resources is being poured into this and it's important for the for humanity in general

49:49.160 --> 49:55.960
I think but right now this is important for a very very tiny percentage of the population of

49:55.960 --> 50:02.920
this planet and there are challenging challenges out there where as a community we could have a huge

50:02.920 --> 50:09.000
impact but we are still overlooking and we should open ourselves to that and that I just would like

50:09.000 --> 50:15.400
to conclude by mentioning this but thank you very much for inviting me to this podcast I was

50:15.400 --> 50:24.440
very happy to discuss with you absolutely thank you so much thank you all right everyone that's

50:24.440 --> 50:31.560
our show for today remember we want to hear from you on AI in the home and in our personal lives

50:31.560 --> 50:38.440
head on over to twimmalei.com slash my AI to share your thoughts for more information on Mustafa

50:38.440 --> 50:45.000
or any of the topics covered in this episode head on over to twimmalei.com slash talk slash 108

50:45.000 --> 51:10.280
thanks for listening and catch you next time

